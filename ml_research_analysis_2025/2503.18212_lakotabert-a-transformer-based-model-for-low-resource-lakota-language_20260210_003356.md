---
ver: rpa2
title: 'LakotaBERT: A Transformer-based Model for Low Resource Lakota Language'
arxiv_id: '2503.18212'
source_url: https://arxiv.org/abs/2503.18212
tags:
- lakota
- language
- lakotabert
- languages
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LakotaBERT, the first transformer-based language
  model for the critically endangered Lakota language, to support language revitalization
  efforts. The authors compiled a bilingual Lakota-English corpus of 105K sentences
  from diverse sources and developed a masked language modeling approach using RoBERTa
  architecture.
---

# LakotaBERT: A Transformer-based Model for Low Resource Lakota Language

## Quick Facts
- arXiv ID: 2503.18212
- Source URL: https://arxiv.org/abs/2503.18212
- Reference count: 31
- First transformer-based model for Lakota language

## Executive Summary
This paper introduces LakotaBERT, the first transformer-based language model for the critically endangered Lakota language, to support language revitalization efforts. The authors compiled a bilingual Lakota-English corpus of 105K sentences from diverse sources and developed a masked language modeling approach using RoBERTa architecture. Trained on a dataset of 8.8 million words, LakotaBERT achieved 51.48% accuracy, 0.56 precision, and 0.51 MRR, demonstrating performance comparable to other language models despite limited resources. The model captures Lakota's complex morphology and polysynthetic structure, providing a foundation for future NLP tasks like translation and text generation. The work sets a precedent for using AI to preserve endangered indigenous languages and highlights the potential for real-world applications in education and cultural preservation.

## Method Summary
LakotaBERT employs masked language modeling (MLM) with RoBERTa architecture trained on a bilingual Lakota-English corpus of 105K sentences. The model uses byte pair encoding (BPE) with 52K vocabulary to handle Lakota's agglutinative morphology, masks 15% of tokens during training, and processes sequences of 512 tokens through 12 transformer layers with 768 hidden dimensions. Training utilized 92K steps with batch size 128 on NVIDIA A100 80GB GPU, incorporating both Lakota and English text to learn contextual representations through self-supervised objectives.

## Key Results
- Achieved 51.48% MLM accuracy, 0.56 precision, and 0.51 MRR on masked token prediction
- Demonstrated reasonable performance despite data scarcity (8.8M words total vs. billions for typical transformers)
- Showed ability to understand both Lakota and English through bilingual training approach

## Why This Works (Mechanism)

### Mechanism 1
Masked language modeling enables contextual representation learning from limited monolingual data without requiring parallel corpora. The model masks 15% of input tokens and learns to predict them from surrounding context, forcing it to capture syntactic and semantic relationships. For Lakota's agglutinative morphology, MLM may help the model learn that suffix patterns correlate with grammatical functions.

### Mechanism 2
Byte Pair Encoding (BPE) tokenization handles morphological complexity better than word-level approaches for agglutinative languages. BPE iteratively merges frequent character pairs into subword units, creating a vocabulary that balances character-level flexibility with word-level efficiency. This allows the model to capture meaningful morphemes without requiring the exact word in training data.

### Mechanism 3
Training on bilingual Lakota-English corpus provides implicit alignment signals even without explicit parallel supervision. The shared RoBERTa architecture processes both languages, potentially learning transferable representations where English patterns inform Lakota predictions through shared positional encoding and attention patterns.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: LakotaBERT's primary training objective; understanding how 15% masking creates self-supervision is essential for interpreting loss curves and evaluation metrics
  - Quick check question: Given the sentence "The cat sat on the ___", would MLM predict a single word or a distribution over vocabulary?

- **Concept: Subword Tokenization (BPE)**
  - Why needed here: Handles Lakota's agglutinative morphology where single words contain multiple morphemes; vocabulary size of 52K suggests aggressive subword splitting
  - Quick check question: If the vocabulary contains "un", "happiness", and "##ness", how would BPE tokenize "unhappiness"?

- **Concept: Transformer Attention Mechanisms**
  - Why needed here: LakotaBERT uses 12-head multi-head attention; understanding scaled dot-product attention is prerequisite for debugging prediction failures
  - Quick check question: Why does the attention formula divide QK^T by √d_k before softmax?

## Architecture Onboarding

- **Component map:** Raw Lakota/English text → BPE tokenizer (52K vocab) → 512-token sequences → 12 transformer layers → MLM predictions
- **Critical path:** OCR extraction (Tesseract) from PDF sources → text with potential errors → bilingual corpus construction → BPE tokenization → 15% random masking → RoBERTa pre-training → contextual embeddings
- **Design tradeoffs:** Larger vocab (52K) vs. smaller for coverage but sparser gradients; bilingual vs. monolingual training with potential English interference; single ground truth evaluation simpler but may penalize valid synonyms
- **Failure signatures:** High CER (0.43) relative to baselines suggests character-level prediction struggles with morphological complexity; precision (0.56) exceeds accuracy (51.48%) → model makes confident but sometimes wrong predictions; BLEU of 0.09 indicates translation-like outputs are weak
- **First 3 experiments:**
  1. Ablation on masking ratio: Test 10%, 15%, 20% masking on held-out Lakota sentences to find optimal self-supervision signal for low-resource conditions
  2. Vocabulary size sweep: Compare 30K, 52K, 70K BPE vocabularies; measure OOV rate and CER to validate assumption that larger vocab helps agglutinative morphology
  3. Monolingual vs. bilingual training: Train Lakota-only model on 2.9M Lakota words; compare MLM accuracy and F1 to current bilingual setup to isolate English interference/transfer effects

## Open Questions the Paper Calls Out

- **Open Question 1:** Can leveraging transfer learning from related Siouan languages (e.g., Dakota) significantly improve LakotaBERT's performance compared to training solely on the Lakota-English corpus?
- **Open Question 2:** To what extent does the noise introduced by Tesseract OCR during data extraction degrade the model's ability to learn the complex agglutinative morphology of the Lakota language?
- **Open Question 3:** How does the "single ground truth" evaluation assumption bias the performance metrics for a polysynthetic language where multiple valid outputs may exist for a masked context?
- **Open Question 4:** What specific multimodal data sources and architectures are required to effectively integrate speech-to-text capabilities into the current LakotaBERT framework?

## Limitations

- Data scarcity severely limits model capability with only ~105K sentences (8.8M words total)
- Single-ground-truth evaluation methodology may underestimate true capability for polysynthetic languages
- Bilingual training approach introduces uncertainty about whether English data aids or hinders Lakota learning
- OCR noise from Tesseract extraction may propagate errors into the model's understanding of morphological patterns

## Confidence

- **High Confidence:** Technical implementation details (RoBERTa architecture, BPE tokenization, MLM training procedure) are clearly specified and follow established practices
- **Medium Confidence:** Reported evaluation metrics are plausible given data constraints, though single-ground-truth approach may understate model capability
- **Low Confidence:** Claims about bilingual benefits and model's ability to capture complex Lakota morphology are weakly supported by current evidence

## Next Checks

1. **Ablation study on training data composition:** Train separate models on monolingual Lakota data vs. current bilingual corpus, then compare MLM performance and translation-like output quality to quantify impact of English interference/transfer
2. **Multi-reference evaluation:** Create test set with multiple valid completions for masked tokens in Lakota sentences, then evaluate whether model's top predictions include semantically equivalent alternatives to ground truth
3. **Downstream task validation:** Apply LakotaBERT to practical language revitalization tasks like text completion for language learners or morphological analysis, measuring whether high MLM accuracy translates to useful real-world performance