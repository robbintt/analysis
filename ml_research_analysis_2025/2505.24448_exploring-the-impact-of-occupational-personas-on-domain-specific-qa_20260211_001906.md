---
ver: rpa2
title: Exploring the Impact of Occupational Personas on Domain-Specific QA
arxiv_id: '2505.24448'
source_url: https://arxiv.org/abs/2505.24448
tags:
- persona
- personas
- performance
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how occupational personas influence domain-specific
  question-answering performance in large language models. The authors introduce two
  types of personas: Profession-Based Personas (PBPs) representing domain expertise
  and Occupational Personality-Based Personas (OPBPs) reflecting cognitive tendencies.'
---

# Exploring the Impact of Occupational Personas on Domain-Specific QA

## Quick Facts
- arXiv ID: 2505.24448
- Source URL: https://arxiv.org/abs/2505.24448
- Reference count: 17
- Primary result: Profession-based personas (PBPs) like "scientist" slightly improve domain QA accuracy, while occupational personality-based personas (OPBPs) like "artistic person" often degrade performance by imposing cognitive constraints.

## Executive Summary
This study investigates how occupational personas influence domain-specific question-answering performance in large language models. The authors introduce two types of personas: Profession-Based Personas (PBPs) representing domain expertise and Occupational Personality-Based Personas (OPBPs) reflecting cognitive tendencies. Experiments across biology, chemistry, and physics datasets show that PBPs like "scientist" slightly improve accuracy, while OPBPs such as "artistic person" often degrade performance even when semantically related. Analysis of neuron activations reveals that personas affect knowledge retrieval more in later model layers, with PBPs engaging domain-relevant knowledge and OPBPs imposing cognitive constraints. The findings suggest persona relevance alone does not guarantee improved performance, and that nuanced distinctions in persona representations may better guide LLMs in domain-specific reasoning.

## Method Summary
The study uses zero-shot inference on MMLU benchmark subsets (biology, chemistry, physics) with GPT-3.5-turbo and Llama-3-8B-Instruct. Two persona types are tested: PBPs (Scientist, Biologist, Chemist, Physicist, Artist) and OPBPs based on Holland's Occupational Themes (Brief and Descriptive versions). Each persona is evaluated with 3 system prompt variants × 3 random seeds across 3 domains, totaling 27 runs per persona. Accuracy is the primary metric, with Confession Rate (CR) and Abstention Rate (AR) measuring persona self-awareness and knowledge limitation acknowledgment. Answer extraction uses regex pattern matching on final responses.

## Key Results
- PBPs like "scientist" and "biologist" improve accuracy by 2-4% on biology and chemistry tasks
- OPBPs (e.g., "artistic person") degrade performance, with Descriptive OPBPs showing the largest drops
- Physics tasks show no improvement from any persona, with some degrading performance
- Neuron activation analysis reveals personas affect later transformer layers more than earlier ones
- High AR with OPBPs indicates models self-censor knowledge access due to perceived persona limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Profession-Based Personas (PBPs) slightly improve domain QA by activating domain-relevant knowledge in feed-forward layers.
- Mechanism: PBPs like "scientist" or "biologist" directly associate with domain expertise tokens, triggering knowledge retrieval pathways in later transformer layers. The model treats profession nouns as direct pointers to domain knowledge stored in FFN key-value memories.
- Core assumption: FFN layers store factual knowledge accessible via semantic associations.
- Evidence anchors:
  - [abstract]: "PBPs like 'scientist' slightly improve accuracy... PBPs engaging domain-relevant knowledge"
  - [Section 5.2]: "persona assignments may play a role in shaping the model's decision-making and response formulation in the final stages"
  - [corpus]: Related work confirms expert personas do not reliably improve factual accuracy—consistent with "slight" gains observed here.
- Break condition: When domain requires quantitative reasoning (physics showed no PBP improvement in GPT-3.5); when model lacks domain knowledge entirely.

### Mechanism 2
- Claim: Occupational Personality-Based Personas (OPBPs) degrade performance by imposing cognitive constraints that restrict knowledge access.
- Mechanism: OPBPs (e.g., "artistic person") activate behavioral trait associations rather than knowledge pointers. Detailed descriptions (Descriptive style) amplify this constraint by reinforcing role limitations, leading to higher abstention rates.
- Core assumption: Models encode personality traits as behavioral priors that can override knowledge retrieval.
- Evidence anchors:
  - [abstract]: "OPBPs often degrade performance, even when semantically related to the task... impose cognitive constraints"
  - [Table 5]: Artist persona abstention example—"As an artist, I must confess that my expertise lies in the realm of creativity... I am unable to provide a detailed answer"
  - [Section 4.1]: "Artistic persona scored the lowest and had the highest AR... this persona constrained the model's ability to access relevant knowledge"
- Break condition: Brief OPBPs without detailed descriptions show less degradation; models with better personality-knowledge separation (GPT-4o mini showed near-zero AR).

### Mechanism 3
- Claim: Personas influence knowledge retrieval primarily in later layers, not early syntactic processing.
- Mechanism: Layer-wise analysis shows personas reduce neuron engagement in early layers (lexical/syntactic) but cause significant divergence in final layers where domain knowledge integration and response formulation occur.
- Core assumption: Transformer layers have functional specialization—early for syntax, late for knowledge/reasoning.
- Evidence anchors:
  - [Section 5.2]: "In earlier layers... persona assignments appear to reduce overall neuron engagement... in later layers, a noticeable divergence emerges, particularly in the final layer"
  - [Figure 4 description]: Layer-wise neuron impact shows divergence concentrated in layers 28-32 for Llama-3-8B
  - [corpus]: Insufficient external validation—this layer-wise persona effect needs replication across architectures.
- Break condition: Effect may vary by model architecture; Llama-70B showed different activation patterns than 8B.

## Foundational Learning

- Concept: **Holland's Occupational Themes (RIASEC)**
  - Why needed here: OPBPs are constructed from this framework—understanding that "Investigative" vs "Artistic" reflects cognitive styles (not expertise) explains why OPBPs fail where PBPs succeed.
  - Quick check question: Would "Investigative Person" (OPBP) or "Scientist" (PBP) better activate domain knowledge for biology QA?

- Concept: **Feed-Forward Networks as Key-Value Memories**
  - Why needed here: The neuron activation analysis assumes FFNs store retrievable knowledge; understanding this explains why persona tokens can "point" to stored domain facts.
  - Quick check question: Which layer type (attention vs FFN) primarily handles factual knowledge retrieval per Geva et al. 2021?

- Concept: **Confession Rate vs Abstention Rate**
  - Why needed here: These metrics distinguish persona acknowledgment from knowledge refusal—high CR with low AR suggests role-play without constraint; high AR indicates self-censorship.
  - Quick check question: If a "Chemist" persona on physics questions shows high CR but low AR, what does this indicate about the model's behavior?

## Architecture Onboarding

- Component map: Input: Persona instruction (system) + Question (user) → Early Layers (1-20): Lexical/syntactic processing—persona reduces ambiguity → Middle Layers (21-28): Semantic integration → Late Layers (29-32): Knowledge retrieval via FFN, persona-driven divergence → Output: Domain answer with optional confession/abstention

- Critical path: Persona token → FFN knowledge pointers (late layers) → Domain fact retrieval → Answer generation. OPBPs redirect this path toward behavioral constraints instead of knowledge.

- Design tradeoffs:
  - PBP specificity (e.g., "Biologist" vs "Scientist"): More specific = better domain activation but worse cross-domain transfer
  - Brief vs Descriptive OPBPs: Brief = less performance drop; Descriptive = stronger behavioral constraint
  - No persona baseline: Often competitive; persona gains are marginal (~2-4%)

- Failure signatures:
  - High AR with domain-mismatched personas (e.g., Biologist on physics: 22.44% AR in GPT-3.5)
  - Physics tasks: No persona outperforms all personas—suggests quantitative reasoning resists persona-based activation
  - Descriptive OPBPs in Llama-70B: Catastrophic drops (e.g., Realistic Descriptive: 46.76% vs 71.53% baseline in biology)

- First 3 experiments:
  1. **Baseline comparison**: Run MMLU subset (biology/chemistry/physics) with no persona, "Scientist" PBP, and "Scientific Person" OPBP—quantify the PBP vs OPBP gap.
  2. **Layer-wise activation analysis**: Extract neuron impacts at layers 1, 16, and 32 for Scientist vs Artist personas on identical questions—verify late-layer divergence.
  3. **Cross-domain AR test**: Apply Biologist, Chemist, Physicist personas to each domain; measure CR/AR—confirm domain-mismatch triggers knowledge self-censorship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do personas differentially affect performance on reasoning-intensive versus knowledge-retrieval tasks?
- Basis in paper: [explicit] The authors note that "prompts designed for reasoning tasks do not necessarily improve performance on knowledge-intensive tasks" and call for "systematic classification of tasks into reasoning and domain knowledge categories."
- Why unresolved: MMLU tasks conflate both abilities, making it impossible to isolate whether personas help retrieve stored knowledge or structure reasoning steps.
- What evidence would resolve it: Experiments on benchmark suites that separately measure pure factual recall versus multi-step reasoning, using matched personas across task types.

### Open Question 2
- Question: Why do physics tasks respond differently to persona-based prompting compared to biology and chemistry, and does this pattern generalize to other quantitative domains?
- Basis in paper: [inferred] The authors observe an "unanticipated trend" where no persona outperformed baselines in physics, and suggest "physics often involves abstract concepts and mathematical reasoning that may not be as effectively captured through persona-based prompts."
- Why unresolved: The study only tested three scientific domains and did not systematically vary the reasoning type or abstractness of tasks.
- What evidence would resolve it: Controlled experiments across additional quantitative domains (e.g., mathematics, computer science) with analysis of whether abstract reasoning load mediates persona effectiveness.

### Open Question 3
- Question: How can persona representations be designed to impose helpful cognitive constraints rather than limiting access to relevant knowledge?
- Basis in paper: [explicit] The authors conclude that "persona relevance alone does not guarantee effective knowledge utilization" and that OPBPs "may impose cognitive constraints that hinder optimal knowledge application."
- Why unresolved: The study demonstrates that some personas harm performance but does not identify the specific features or prompt structures that distinguish beneficial from detrimental constraints.
- What evidence would resolve it: Systematic variation of persona description components (e.g., explicit strengths, cognitive style descriptors, task-relevant competencies) to identify which elements improve versus degrade knowledge retrieval.

## Limitations

- Sample Selection and Domain Specificity: The study uses fixed sample counts from MMLU without specifying exact question selection, raising concerns about domain specificity and generalizability, particularly for physics where no persona improved performance.
- Persona Representation Granularity: Only the Artistic persona's full description is provided; the other five personality types' descriptions must be sourced externally, potentially introducing variability in OPBP construction.
- Architecture-Specific Effects: Layer-wise analysis is only validated on Llama-3-8B-Instruct; the pattern needs cross-architecture verification to confirm it's not model-specific.

## Confidence

- High Confidence: The core empirical findings about performance degradation with OPBPs (especially descriptive versions) and the observation that persona relevance doesn't guarantee improvement.
- Medium Confidence: The mechanistic explanation linking persona tokens to FFN key-value memory access and the layer-wise divergence pattern.
- Low Confidence: The claim that physics specifically resists persona effects due to quantitative reasoning requirements.

## Next Checks

1. **Cross-Domain Persona Transfer Test**: Apply each domain-specific PBP (Biologist, Chemist, Physicist) to all three domains and measure AR/CR to validate whether domain-mismatched personas consistently trigger self-censorship.

2. **Architecture Replication Study**: Replicate the layer-wise neuron activation analysis on at least one additional model architecture (e.g., GPT-4o mini or another Llama variant) to test whether late-layer persona divergence is architecture-specific.

3. **PBP Specificity Gradient**: Test a spectrum of PBPs from general (Scientist) to specific (Molecular Biologist) on biology questions, measuring accuracy and neuron activation to quantify the tradeoff between domain specificity and cross-domain transferability.