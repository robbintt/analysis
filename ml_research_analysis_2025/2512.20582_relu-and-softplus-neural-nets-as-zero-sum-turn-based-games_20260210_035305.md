---
ver: rpa2
title: Relu and softplus neural nets as zero-sum turn-based games
arxiv_id: '2512.20582'
source_url: https://arxiv.org/abs/2512.20582
tags:
- game
- neural
- relu
- state
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a game-theoretic interpretation of ReLU
  and Softplus neural networks, representing them as zero-sum turn-based stopping
  games. The key result shows that the output of a ReLU network equals the value of
  a corresponding game where the input serves as terminal reward, and evaluating the
  network corresponds to running the Shapley-Bellman backward recursion for this game.
---

# Relu and softplus neural nets as zero-sum turn-based games

## Quick Facts
- arXiv ID: 2512.20582
- Source URL: https://arxiv.org/abs/2512.20582
- Reference count: 4
- Key outcome: This paper establishes a game-theoretic interpretation of ReLU and Softplus neural networks, representing them as zero-sum turn-based stopping games. The key result shows that the output of a ReLU network equals the value of a corresponding game where the input serves as terminal reward, and evaluating the network corresponds to running the Shapley-Bellman backward recursion for this game.

## Executive Summary
This paper presents a novel game-theoretic framework that interprets ReLU and Softplus neural networks as zero-sum turn-based stopping games. The central result demonstrates that the forward evaluation of a ReLU network is mathematically equivalent to the backward Shapley-Bellman recursion for the value of a corresponding game. This correspondence enables new applications including bounds propagation from input to output, verification using optimal policies as certificates, and interpreting training as an inverse game problem. The framework extends to Softplus networks through entropic regularization of the ReLU game, where the optimal policy becomes a Gibbs distribution.

## Method Summary
The method establishes a correspondence between ReLU/Softplus neural networks and zero-sum turn-based stopping games. For a network with L layers, each neuron i at layer l is represented by two game states (l,i+) and (l,i-) corresponding to Max and Min players. The discount factor γ^l_i is the sum of absolute weights, and transition probabilities are derived from normalized weights. Terminal rewards are set to the input vector. The Shapley-Bellman backward recursion computes game values V^l_{i+}(x) that equal the network output f(x)_i. For Softplus networks, entropy regularization is added to the game, resulting in a Gibbs distribution as the optimal policy.

## Key Results
- The forward evaluation of a ReLU neural network equals the backward Shapley-Bellman recursion for a corresponding zero-sum stopping game
- Linearity regions of the network correspond to regions where the pair of optimal policies for both players remains constant
- Softplus networks are represented as entropy-regularized versions of ReLU games, with the optimal policy becoming a Gibbs distribution
- The game representation provides an order-preserving lift of the network map, enabling abstract interpretation approaches

## Why This Works (Mechanism)

### Mechanism 1: Shapley-Bellman Equivalence
The forward evaluation of a ReLU network is mathematically equivalent to the backward recursion of a specific zero-sum game. The ReLU function $\max(0, x)$ can be represented by a turn-based "stopping" decision where the input serves as terminal reward and network layers serve as game stages. The Shapley-Bellman backward recursion produces the same values as the forward neural net pass. This breaks for non-piecewise linear activations like standard Sigmoid.

### Mechanism 2: Policy-Induced Linearity Regions
The linear regions of a ReLU network correspond one-to-one with the optimal policies of the corresponding game. A ReLU network is piecewise linear, and in the game view, players choose "continue" or "stop" based on the sign of the weighted sum of future values. A specific input triggers a specific optimal policy pair. As long as this policy pair remains optimal, the function behaves linearly. A change in optimal policy signals a boundary crossing into a new linearity region.

### Mechanism 3: Entropic Regularization as Smoothing
Softplus networks are the entropically regularized versions of the ReLU game. By adding the log-probability of the policy to the reward (entropy term), the "hard" max/min decisions of the ReLU game are smoothed into probabilistic decisions. The Softplus function $\tau \log(1 + e^{x/\tau})$ arises naturally as the value function of this perturbed game, where the optimal policy becomes a Gibbs distribution rather than a deterministic stop/continue.

## Foundational Learning

- **Shapley-Bellman Operator**
  - Why needed: This is the recursive engine used to solve the game, required to understand why a backward game pass equals a forward network pass
  - Quick check: How does the discount factor $\gamma$ in the Shapley equation relate to the weights in the neural network?

- **Zero-Sum Turn-Based Stopping Games**
  - Why needed: The paper maps neurons to game states where players either "stop" (yield 0) or "continue" (accumulate reward)
  - Quick check: In this context, does the "stop" action correspond to a ReLU killing a signal (outputting 0) or the signal passing through?

- **Gibbs Distribution (Boltzmann Policy)**
  - Why needed: This explains the Softplus extension where deterministic policies become probabilistic Gibbs distributions
  - Quick check: What happens to the Gibbs distribution in the limit as the temperature $\tau \to 0$?

## Architecture Onboarding

- **Component map**: Input Layer ($L$) -> Terminal Rewards in the game; Neurons ($l, i$) -> Game States (split into Max state $(l, i+)$ and Min state $(l, i-)$); Weights ($W^l_{i,j}$) -> Transition Probabilities (normalized by sum of absolute weights); Biases ($b^l_i$) -> Instantaneous Rewards; Forward Pass -> Backward Shapley Recursion

- **Critical path**: The mapping of weights to transition probabilities. You must normalize weights $W$ such that $\sum |W_{ij}| = \gamma$ (discount factor) to ensure valid probabilities. This scaling is vital; without it, the game analogy fails.

- **Design tradeoffs**: The "Game View" sacrifices computational speed (simulating a game is generally slower than matrix multiplication) for interpretability and verification power (policies as certificates). Use this abstraction for analysis/verification, not for accelerating inference.

- **Failure signatures**:
  - Invalid Probabilities: If weights are not properly normalized to determine $P_{i+,j+}$ and $P_{i+,j-}$, the game analogy fails
  - Sign Confusion: A negative weight implies a player switch (Max $\to$ Min). Incorrect sign handling breaks the correspondence

- **First 3 experiments**:
  1. Manual Verification: Take a 2-neuron, 2-layer ReLU net. Construct the game tree manually. Compute the Shapley value and confirm it matches the network output
  2. Policy Visualization: Run inputs from different linearity regions through the game solver and visualize the resulting optimal policies $(\pi^*, \sigma^*)$ to verify they correspond to active/inactive ReLU patterns
  3. Softplus Limit Check: Implement the entropic game for a small Softplus net and verify that as $\tau \to 0$, the game values converge to the ReLU net values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the game-theoretic representation be extended to support abstract interpretation over relational domains (e.g., polyhedra or octagons) rather than just non-relational interval boxes?
- Basis in paper: [explicit] Remark 7 states, "We leave it for further work to extend the present approach to more expressive, relational, domains."
- Why unresolved: The current framework propagates bounds using the coordinate-wise order, which corresponds to "boxes" in abstract interpretation, a method known to yield coarse invariants
- What evidence would resolve it: A derivation showing how the Shapley operator can be lifted to handle linear inequalities between input variables, or a practical algorithm computing polyhedral reachable sets via game policies

### Open Question 2
- Question: How can the "inverse game problem" formulation be utilized to create efficient or interpretable alternatives to standard gradient-based training?
- Basis in paper: [explicit] The authors state in the Introduction that "training the neural network becomes an inverse game problem."
- Why unresolved: While the paper establishes the theoretical equivalence between network parameters and game parameters, it does not propose or analyze algorithms to solve this inverse problem
- What evidence would resolve it: The development of an inverse reinforcement learning or inverse game theory algorithm that recovers network weights from the input-output value pairs

### Open Question 3
- Question: Can the correspondence be generalized to architectures that lack a strictly sequential layer structure, such as ResNets with skip connections?
- Basis in paper: [inferred] The ReLU net game is defined with states and transitions mirroring a strict layer-by-layer structure
- Why unresolved: The definition of state transitions depends on weights connecting layer $l$ to $l+1$; it is unclear how the state space or transition probabilities would represent bypass connections or parallel paths
- What evidence would resolve it: A formal construction of a "ResNet game" where skip connections are modeled as specific transitions or actions, preserving the Shapley-Bellman equivalence

## Limitations
- The framework lacks empirical validation beyond a single 3-layer toy example, with no systematic testing on standard benchmark architectures or datasets
- The game-theoretic representation sacrifices computational efficiency for interpretability and verification power
- Practical implementation details for computing optimal policies efficiently at scale are not addressed

## Confidence
- **High Confidence**: The Shapley-Bellman equivalence between ReLU networks and zero-sum stopping games (Theorem 1)
- **Medium Confidence**: The policy-induced linearity region interpretation
- **Medium Confidence**: The entropic regularization framework for Softplus

## Next Checks
1. Implement the game-theoretic framework on standard architectures (LeNet, ResNet variants) and benchmark datasets (MNIST, CIFAR-10) to validate practical utility and computational overhead

2. For a trained network, extract optimal policies from the game representation and verify their effectiveness as verification certificates for robustness properties

3. Systematically vary the temperature parameter τ in Softplus networks and measure the convergence rate to ReLU behavior, comparing with standard temperature-based smooth activation functions