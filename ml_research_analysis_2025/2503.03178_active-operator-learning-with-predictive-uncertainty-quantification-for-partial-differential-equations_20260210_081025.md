---
ver: rpa2
title: Active operator learning with predictive uncertainty quantification for partial
  differential equations
arxiv_id: '2503.03178'
source_url: https://arxiv.org/abs/2503.03178
tags:
- network
- uncertainty
- learning
- training
- deeponet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for predictive uncertainty quantification
  in deep operator networks (DeepONets) using a single network approach that introduces
  minimal computational overhead during training and inference. The authors developed
  a framework that calibrates uncertainty estimates to model errors observed during
  training, using a log-likelihood loss function to train the network to output parameters
  for a predictive probability distribution.
---

# Active operator learning with predictive uncertainty quantification for partial differential equations

## Quick Facts
- arXiv ID: 2503.03178
- Source URL: https://arxiv.org/abs/2503.03178
- Authors: Nick Winovich; Mitchell Daneker; Lu Lu; Guang Lin
- Reference count: 40
- Key outcome: Single-network approach for predictive uncertainty quantification in DeepONets with >5x inference speedup and unbiased, non-skewed uncertainty estimates

## Executive Summary
This paper introduces a method for predictive uncertainty quantification in deep operator networks (DeepONets) using a single network approach that introduces minimal computational overhead during training and inference. The authors developed a framework that calibrates uncertainty estimates to model errors observed during training, using a log-likelihood loss function to train the network to output parameters for a predictive probability distribution. They also introduced an optimized inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation times by more than a factor of five. The method was validated on linear and nonlinear partial differential equation (PDE) problems, showing that the predictive uncertainty estimates are unbiased, non-skewed, and accurately reflect observed model errors across different levels of problem complexity.

## Method Summary
The method trains a DeepONet to output both mean predictions and uncertainty estimates by splitting the final layers of branch and trunk networks into separate pathways for each output type. The network is trained using negative log-likelihood loss, which calibrates uncertainty estimates to observed model errors during training. For inference, trunk outputs are precomputed and a sparse placement matrix is used to efficiently map predictions to structured output arrays, achieving significant speedups. The framework was validated on Poisson and diffusion-reaction equations, showing unbiased uncertainty estimates and improved active learning performance compared to deterministic baselines.

## Key Results
- Predictive uncertainty estimates are unbiased (bias centered at zero) and non-skewed (skew magnitudes below 0.2) across test problems
- Inference time reduced by more than a factor of five using precomputed trunk outputs and sparse placement matrix
- Active learning framework with uncertainty-guided sampling yields 8.2% improvement in wave equation and 5.3% in diffusion-reaction equation compared to deterministic baselines
- Framework extends to Fourier Neural Operators with external uncertainty quantification network

## Why This Works (Mechanism)

### Mechanism 1
Calibrating network outputs as parameters of a predictive distribution via negative log-likelihood loss yields uncertainty estimates aligned with observed model errors. The network outputs μ(x) and log σ(x) are interpreted as mean and standard deviation of a Gaussian predictive distribution. During training, the NLL loss penalizes both prediction error and poorly calibrated uncertainty—forcing the network to output larger σ when errors are high and smaller σ when predictions are accurate.

### Mechanism 2
Precomputing trunk outputs and using a sparse placement matrix reduces inference time by >5x without retraining. The trunk network outputs depend only on evaluation coordinates x, not on input functions (f,g). For fixed grids, trunk outputs T are computed once post-training. A sparse placement matrix P maps vectorized predictions to structured arrays, eliminating redundant forward passes and manual indexing.

### Mechanism 3
Decoupling mean and uncertainty predictions in the final layers improves uncertainty calibration while preserving mean accuracy. The final two layers of branch and trunk networks split into parallel pathways—one for mean (b, t) and one for uncertainty (b_σ, t_σ). This allows distinct basis functions for uncertainty, rather than forcing uncertainty to reuse mean-prediction features.

## Foundational Learning

- **Concept: DeepONet architecture (branch/trunk decomposition)**
  - Why needed here: The entire framework builds on DeepONet's factorization—branch processes input functions, trunk processes evaluation coordinates, output is their inner product.
  - Quick check question: Can you explain why trunk outputs can be precomputed while branch outputs cannot?

- **Concept: Aleatoric vs. epistemic uncertainty**
  - Why needed here: The paper acknowledges its method primarily captures aleatoric uncertainty; understanding this distinction prevents misapplication to scenarios requiring epistemic modeling.
  - Quick check question: Would this framework detect when the model is queried on an entirely new PDE family it was never trained on?

- **Concept: Active learning acquisition functions**
  - Why needed here: The framework uses uncertainty to guide data acquisition; understanding UCB/exploration-exploitation tradeoffs is essential for reproducing BO and AL results.
  - Quick check question: In the BO experiments, what happens if ε (exploration weight) is set too low?

## Architecture Onboarding

- **Component map:** Branch network: 5 FC layers × 50 units → split into mean head (b) and uncertainty head (b_σ); Trunk network: 5 FC layers × 30 units → split into mean head (t) and uncertainty head (t_σ); Output: μ = ⟨b, t⟩, log σ = ⟨b_σ, t_σ⟩ (N = 150 basis functions); For FNO: Main FNO backbone + standalone UQ network

- **Critical path:** 1) Verify error distribution normality via histogram; 2) Implement NLL loss with log-σ parameterization; 3) Split final layers of branch/trunk for decoupled predictions; 4) For inference: precompute trunk outputs and placement matrix once

- **Design tradeoffs:** Integrated UQ (DeepONet): Lower overhead, faster inference, but requires architecture modification; External UQ (FNO): Easier to add to existing models, but 10.6% training overhead; Ensemble methods: Best calibration in AL experiments (15.4% improvement) but 1308% training overhead

- **Failure signatures:** Training instability/overflow: Small σ predictions cause NLL explosion → rescale inputs or clip minimum σ; Poor OoD generalization: Uncertainty doesn't increase for harder examples → likely insufficient training data diversity; Low R² between predicted uncertainty and observed error (e.g., 0.29 for FNO in wave equation): Indicates miscalibration, may need more data or architecture tuning

- **First 3 experiments:** 1) Sanity check: Poisson equation with homogeneous BC—if L2 relative error > 2%, UQ integration is harming accuracy; 2) Uncertainty calibration test: Train on length-scales {0.20, 0.30}, test on {0.15, 0.35}—verify uncertainty increases for shorter length-scales; 3) Inference speed benchmark: Compare naïve loop evaluation vs. precomputed trunk + placement matrix on a 128×128 grid; target >3x speedup

## Open Questions the Paper Calls Out

### Open Question 1
How can aleatoric and epistemic uncertainty be rigorously disentangled within a deterministic, single-pass operator learning framework? The proposed framework relies on deterministic training regimes that make modeling epistemic uncertainty difficult without the computational overhead of ensembles or Bayesian methods.

### Open Question 2
To what extent does the predictive uncertainty framework generalize to PDE systems featuring discontinuities, shocks, or variable coefficients? The validation experiments were limited to diffusion-reaction and wave equations, which generally possess smoother solutions than systems with shocks or discontinuous parameters.

### Open Question 3
What are the specific architectural or data-driven factors that cause the degradation of uncertainty calibration in complex, irregular-domain problems? It is unclear if the drop is caused by the "discrete-trunk" modification, the high dimensionality, or the active learning sampling density.

## Limitations
- The Gaussian NLL loss assumption may fail for highly skewed or multimodal error distributions, potentially requiring alternative predictive distributions
- Framework's performance on problems with sharp gradients or discontinuities remains untested
- External validation of the decoupled uncertainty architecture is limited, with weak corpus support for this specific design choice

## Confidence

**High Confidence:** Predictive uncertainty calibration mechanism (Mechanism 1) and optimized inference strategy (Mechanism 2) - both have direct experimental validation in the paper with quantitative metrics.

**Medium Confidence:** Decoupled mean/uncertainty architecture (Mechanism 3) - supported by ablation studies but lacks strong external corpus validation.

**Medium Confidence:** Active learning framework extension - demonstrated but performance relative to ensembles suggests room for improvement.

## Next Checks

1. Test framework on a problem with known skewed error distribution (e.g., nonlinear diffusion with varying coefficients) to verify Gaussian assumption holds.

2. Evaluate performance when evaluation locations vary between queries to confirm precomputation speedup claims break down as expected.

3. Implement ensemble-based uncertainty quantification on the same problems to benchmark against the 15.4% improvement reported in AL experiments.