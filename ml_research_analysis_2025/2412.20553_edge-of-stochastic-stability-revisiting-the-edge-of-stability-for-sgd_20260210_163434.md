---
ver: rpa2
title: 'Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD'
arxiv_id: '2412.20553'
source_url: https://arxiv.org/abs/2412.20553
tags:
- batch
- mini-batch
- sharpness
- size
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work bridges the gap between the Edge of Stability (EoS)\
  \ phenomenon observed in full-batch methods and mini-batch Stochastic Gradient Descent\
  \ (SGD). While EoS identifies a regime where the largest eigenvalue of the full-batch\
  \ Hessian stabilizes around 2/\u03B7, this breaks down for mini-batch methods due\
  \ to per-step curvature changes."
---

# Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD

## Quick Facts
- arXiv ID: 2412.20553
- Source URL: https://arxiv.org/abs/2412.20553
- Reference count: 40
- Primary result: Introduces Edge of Stochastic Stability (EoSS), showing Batch Sharpness stabilizes at 2/η for SGD, not full-batch λmax.

## Executive Summary
This work bridges the gap between the Edge of Stability (EoS) phenomenon observed in full-batch methods and mini-batch Stochastic Gradient Descent (SGD). While EoS identifies a regime where the largest eigenvalue of the full-batch Hessian stabilizes around 2/η, this breaks down for mini-batch methods due to per-step curvature changes. The authors introduce the Edge of Stochastic Stability (EoSS), showing that Batch Sharpness—the expected directional curvature of mini-batch Hessians along stochastic gradients—stabilizes at 2/η for SGD. This metric governs the instability threshold for SGD, not the full-batch λmax. Empirically, Batch Sharpness undergoes progressive sharpening and stabilizes near 2/η across architectures and batch sizes. Interventions (changing η or batch size) trigger catapults when Batch Sharpness crosses this threshold, confirming its role as the operational stability comparator. Consequently, smaller batches and larger step sizes lead to flatter minima, as λmax is suppressed below Batch Sharpness. This reveals that classical stability arguments fail for SGD and that "GD + noise" models cannot capture its unique dynamics.

## Method Summary
The authors propose and validate a new stability criterion for mini-batch SGD by tracking Batch Sharpness—the expected Rayleigh quotient of mini-batch Hessians along stochastic gradients. They conduct controlled experiments on CIFAR-10 and SVHN with varying batch sizes, step sizes, and architectures (MLP, CNN, ResNet variants), measuring Batch Sharpness, full-batch λmax, and GNI over time. The key empirical approach involves estimating directional curvature via Hessian-vector products and validating the 2/η threshold through mid-training interventions that induce catapults when Batch Sharpness is saturated.

## Key Results
- Batch Sharpness stabilizes near 2/η for SGD, not the full-batch λmax as in EoS.
- Smaller batch sizes and larger step sizes lead to flatter minima (lower λmax) due to the Batch Sharpness-λmax gap.
- Interventions (increasing η or decreasing batch size) trigger catapults only after Batch Sharpness reaches 2/η.
- GNI ≈ 2/η is a signature of noise-driven Type-1 oscillations, distinct from curvature-driven Type-2 catapults.

## Why This Works (Mechanism)

### Mechanism 1: Batch Sharpness as the Stability Limit
- **Claim:** Mini-batch SGD operates at the "Edge of Stochastic Stability" (EoSS) where Batch Sharpness hovers near $2/\eta$, rather than the full-batch Hessian $\lambda_{\max}$.
- **Mechanism:** In full-batch GD, $\lambda_{\max}$ stabilizes at $2/\eta$. For SGD, the stability boundary is determined by **Batch Sharpness**—the expected directional curvature of mini-batch Hessians along stochastic gradients. Theorem 1 suggests that on the quadratic approximation, Batch Sharpness $\ge 2/\eta$ is a sufficient condition for instability (catapults).
- **Core assumption:** The dynamics can be locally approximated by a quadratic model, and mini-batch gradients serve as a valid Lyapunov function.
- **Evidence anchors:**
  - [abstract]: Defines EoSS and Batch Sharpness as the stabilizing quantity.
  - [section 6.1]: Theorem 1 proves Batch Sharpness crossing $2/\eta$ forces a catapult on the quadratic approximation.
  - [corpus]: Weak direct evidence; related work "Optimizer Dynamics at the Edge of Stability" discusses EoS but with differential privacy, not the Batch Sharpness metric.
- **Break condition:** If the loss landscape cannot be locally approximated as quadratic or if higher-order terms dominate, the $2/\eta$ threshold may fail to predict catapults.

### Mechanism 2: Suppression of Full-Batch Sharpness via Mini-batch Gaps
- **Claim:** Smaller batch sizes and larger step sizes lead to flatter minima (lower $\lambda_{\max}$) as a byproduct of EoSS saturation.
- **Mechanism:** Batch Sharpness stabilizes at $2/\eta$ independent of batch size. However, a "gap" exists between Batch Sharpness and $\lambda_{\max}$ due to the misalignment of per-sample Hessians. This gap forces $\lambda_{\max}$ to settle at a lower level than the stability threshold, a phenomenon suppressed by larger batch sizes.
- **Core assumption:** Progressive sharpening of Batch Sharpness halts once it reaches $2/\eta$.
- **Evidence anchors:**
  - [section 7]: Discusses the fate of $\lambda_{\max}$ and the gap induced by batch size.
  - [figure 8]: Visualizes the critical batch size crossover where SGD dynamics shift to GD-like behavior.
  - [corpus]: No direct evidence for this specific suppression mechanism in the provided neighbors.
- **Break condition:** If the batch size exceeds the "critical batch size," the gap closes, and $\lambda_{\max}$ converges to the full-batch EoS value ($2/\eta$).

### Mechanism 3: Distinguishing Type-1 (Noise) and Type-2 (Curvature) Oscillations
- **Claim:** Standard loss oscillations in SGD (Type-1) are ubiquitous noise artifacts and do not signal instability; true instability (Type-2) is signaled by "catapults" triggered by crossing Batch Sharpness thresholds.
- **Mechanism:** GNI (Gradient-Noise Interaction) stabilizes near $2/\eta$ even in convex settings as a certificate of noise-driven oscillation (Type-1). In contrast, Type-2 oscillations occur only when Batch Sharpness saturates and perturbations (e.g., increasing $\eta$) trigger divergence.
- **Core assumption:** Observing a "loss spike of sufficient size" equates to a catapult on the quadratic model.
- **Evidence anchors:**
  - [section 5]: Explicitly differentiates Type-1 and Type-2 oscillations.
  - [proposition 1]: Shows GNI $\approx 2/\eta$ is typical for stable noise-driven orbits.
  - [corpus]: "Full-Batch Gradient Descent Outperforms One-Pass SGD" distinguishes full-batch from SGD dynamics, supporting the need for distinct analysis models.
- **Break condition:** If Hessian statistics change non-linearly per step, GNI may fail as a reliable noise proxy.

## Foundational Learning

### Concept: Rayleigh Quotient (Directional Curvature)
- **Why needed here:** Batch Sharpness is defined as the *expected Rayleigh quotient*. Understanding that this measures curvature specifically in the direction of the update step (gradient) is vital, as opposed to the maximum eigenvalue ($\lambda_{\max}$) which looks at the worst-case direction.
- **Quick check question:** Does Batch Sharpness measure the steepest cliff on the loss landscape or the steepness of the specific path the optimizer is taking?

### Concept: Lyapunov Stability in Optimization
- **Why needed here:** The paper reframes stability from "monotone descent" to "absence of divergence" using Lyapunov functions (specifically the norm of mini-batch gradients).
- **Quick check question:** Why is the squared norm of the mini-batch gradient ($\|\nabla L_B\|^2$) a better Lyapunov candidate for SGD stability than the parameter distance ($\|\theta - \theta^*\|^2$)?

### Concept: Progressive Sharpening
- **Why needed here:** This is the driving force that pushes Batch Sharpness up until it hits the $2/\eta$ ceiling. Without this dynamic, the system would never reach the "Edge."
- **Quick check question:** What happens to the progressive sharpening phase once Batch Sharpness hits the $2/\eta$ boundary?

## Architecture Onboarding

### Component map:
- **Instability Comparator:** Batch Sharpness $\approx 2/\eta$ (replaces $\lambda_{\max} \approx 2/\eta$)
- **Oscillation Diagnostics:** GNI (Type-1 noise) vs. Catapults/Loss Spikes (Type-2 curvature)
- **Curvature Objects:** Full-batch Hessian ($\lambda_{\max}$) vs. Mini-batch Hessian ($H_B$) distribution

### Critical path:
Implementing the Batch Sharpness estimator (requires Hessian-vector products) and distinguishing it from standard Trace or $\lambda_{\max}$ calculations.

### Design tradeoffs:
- **Small Batch Sizes:** Induce flatter minima (lower $\lambda_{\max}$) but increase variance/instability risk (more Type-1 noise)
- **Large Step Sizes:** Accelerate progressive sharpening but lower the saturation ceiling ($2/\eta$ is lower), potentially finding flatter minima faster

### Failure signatures:
- **False Instability:** High loss oscillations (GNI $\approx 2/\eta$) without catapults—this is Type-1 noise, not EoSS failure
- **Divergence:** Consistent Batch Sharpness $> 2/\eta$ leading to parameter explosion

### First 3 experiments:
1.  **Validation of Threshold:** Train a simple MLP on CIFAR-10 with fixed $\eta$; plot Batch Sharpness vs. epoch to confirm stabilization at $2/\eta$.
2.  **Hyperparameter Perturbation:** Mid-training, double the learning rate $\eta$. Verify that a "catapult" (loss spike) occurs *only* after Batch Sharpness has saturated at the original $2/\eta$.
3.  **Batch Size Ablation:** Train identical models with varying batch sizes (e.g., 32, 128, 512). Observe if smaller batches result in lower final $\lambda_{\max}$ while Batch Sharpness remains at $2/\eta$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Edge of Stochastic Stability (EoSS) framework and the saturation of Batch Sharpness generalize to adaptive optimizers like Adam?
- **Basis in paper:** [explicit] Section 9 (Limitations) states the authors "have not analyzed momentum-based or adaptive methods (e.g. Adam)."
- **Why unresolved:** The theoretical and empirical analysis is restricted to constant-step-size mini-batch SGD; Adam modifies gradient directions and effective step sizes, potentially altering the instability criterion.
- **What evidence would resolve it:** Empirical tracking of Batch Sharpness during Adam training to see if it stabilizes at a threshold derived from Adam's effective step size.

### Open Question 2
- **Question:** Does the Batch Sharpness stabilization phenomenon persist when training with Cross-Entropy loss rather than Mean Squared Error (MSE)?
- **Basis in paper:** [explicit] Section 9 notes, "Our experiments are run with MSE, a complete study of training with cross entropy is needed."
- **Why unresolved:** All presented empirical results rely exclusively on MSE, and the behavior of directional curvature and $\lambda_{\max}$ suppression may differ under classification losses.
- **What evidence would resolve it:** Replicating the EoSS experiments (tracking Batch Sharpness vs. $2/\eta$) on standard classification benchmarks using Cross-Entropy loss.

### Open Question 3
- **Question:** What is the precise mathematical mechanism underlying the self-stabilization of SGD at the Edge of Stochastic Stability?
- **Basis in paper:** [explicit] Section 3.2 asks "Which of these quantities are actually selected by the SGD dynamics?" and Section 9 lists "establishing the self-stabilization mechanism... for SGD" as future work.
- **Why unresolved:** While the paper identifies Batch Sharpness as the saturating quantity, it does not fully derive the progressive sharpening dynamics that drive the system to this specific edge.
- **What evidence would resolve it:** A theoretical derivation showing how the SGD dynamics self-correct to maintain Batch Sharpness $\approx 2/\eta$ during the training trajectory.

## Limitations
- The quadratic approximation assumption may break down in highly non-convex regimes or when higher-order curvature effects dominate.
- The precise definition and measurement of "catapults" remains heuristic; alternative definitions might yield different instability thresholds.
- The role of architecture-specific factors (e.g., depth, skip connections) in shaping Batch Sharpness trajectories is not fully explored.

## Confidence
- **High Confidence:** The empirical observation that Batch Sharpness stabilizes near $2/\eta$ across architectures and batch sizes is well-supported by the presented data.
- **Medium Confidence:** The theoretical connection between Batch Sharpness exceeding $2/\eta$ and the occurrence of catapults on the quadratic model is logically sound but relies on idealized assumptions.
- **Low Confidence:** The mechanism by which smaller batch sizes lead to flatter minima via the Batch Sharpness-$\lambda_{\max}$ gap is plausible but lacks rigorous proof and could be influenced by other confounding factors.

## Next Checks
1. **Generalization Across Architectures:** Verify the EoSS phenomenon on architectures not tested in the paper (e.g., Vision Transformers, RNNs) to confirm its broad applicability.
2. **Critical Batch Size Dependence:** Systematically map the critical batch size threshold where $\lambda_{\max}$ begins to track Batch Sharpness, and test if this varies predictably with network width or depth.
3. **Alternative Lyapunov Functions:** Experiment with different Lyapunov candidates (e.g., exponential moving averages of gradients) to see if they provide a more robust stability certificate for SGD.