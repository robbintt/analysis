---
ver: rpa2
title: Opportunities and Barriers for AI Feedback on Meeting Inclusion in Socioorganizational
  Teams
arxiv_id: '2601.11750'
source_url: https://arxiv.org/abs/2601.11750
tags:
- feedback
- meeting
- participants
- meetings
- organizational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Emily, an AI agent designed to improve meeting
  inclusion by facilitating feedback exchange using the Induced Hypocrisy Procedure.
  The agent solicits feedback after meetings and delivers it before subsequent meetings,
  prompting participants to reflect on value-behavior inconsistencies.
---

# Opportunities and Barriers for AI Feedback on Meeting Inclusion in Socioorganizational Teams

## Quick Facts
- **arXiv ID:** 2601.11750
- **Source URL:** https://arxiv.org/abs/2601.11750
- **Reference count:** 40
- **Primary result:** Emily, an AI agent using Induced Hypocrisy Procedure, improved speaking time balance and meeting quality in lab studies, but organizational barriers limited adoption in field deployment.

## Executive Summary
This paper presents Emily, an AI agent designed to improve meeting inclusion by facilitating feedback exchange using the Induced Hypocrisy Procedure (IHP). The agent solicits feedback after meetings and delivers it before subsequent meetings, prompting participants to reflect on value-behavior inconsistencies. A within-subjects lab study (n=28) demonstrated that Emily made speaking times more balanced and improved meeting quality. A field study (n=10) at a consulting firm revealed that while the agent influenced behavior when participants engaged deeply, organizational barriers led most to use it for personal reflection rather than feedback exchange. The work highlights the potential of IHP for meeting AI systems while demonstrating the critical importance of considering organizational context in designing AI tools for workplace collaboration.

## Method Summary
The study employed a mixed-methods approach with lab and field deployments of Emily, an AI agent built on a React/Node/Express platform using Jitsi for video. The agent uses GPT-4o-mini with structured prompts to facilitate feedback exchange via the Induced Hypocrisy Procedure: after meetings, it solicits feedback based on speaking time data; before subsequent meetings, it delivers anonymized feedback and guides users through IHP by having them set goals and reflect on past failures. The lab study (n=28) used a within-subjects design comparing control (no agent) to treatment (full IHP flow), measuring speaking time balance and meeting quality. The field study (n=10) deployed the system in a consulting firm over 3-4 weeks, examining real-world adoption barriers through interviews and system logs.

## Key Results
- Lab study: Emily significantly improved speaking time balance (Gini coefficient) and meeting quality compared to control conditions
- Field study: Agent influenced behavior when participants engaged deeply with the IHP conversation, but organizational barriers (hierarchy, time pressure, feedback norms) limited broader adoption
- Most field participants used Emily for personal reflection rather than feedback exchange, citing concerns about feedback credibility and organizational context

## Why This Works (Mechanism)

### Mechanism 1: Induced Hypocrisy Procedure (IHP)
- **Claim:** If participants are guided through a two-step conversation where they first endorse an inclusive behavior norm, then recall past failures to uphold it, cognitive dissonance motivates behavior change.
- **Mechanism:** Normative salience (committing to a goal) + transgression salience (recalling past inconsistency) → cognitive dissonance → pressure to align future behavior with stated values.
- **Core assumption:** Users will experience discomfort from the value-behavior gap and will resolve it by changing behavior rather than rationalizing.
- **Evidence anchors:**
  - [abstract] "When delivering feedback, the agent uses the Induced Hypocrisy Procedure, a social psychological technique that prompts behavior change by highlighting value-behavior inconsistencies."
  - [section] Figure 3 shows system instructions guiding the agent: "Emily asks the user to set a goal... then asks them to reflect on a previous time when they did not meet the goal."
  - [corpus] Corpus does not contain strong independent validation of IHP in meeting contexts; this paper provides novel empirical support.
- **Break condition:** Users rush through the reflection without genuine engagement (observed in field study P9: "I was trying to get through the questions").

### Mechanism 2: AI as Psychologically Safer Disclosure Recipient
- **Claim:** If users give feedback to an AI agent rather than directly to colleagues, fear of negative evaluation decreases, enabling more honest disclosure.
- **Mechanism:** Reduced interpersonal risk → lower anticipated conflict → increased willingness to share critical feedback.
- **Core assumption:** Users attribute lower social consequences to AI-mediated feedback than face-to-face exchange.
- **Evidence anchors:**
  - [abstract] "AI agents can facilitate feedback exchange by being psychologically safer recipients."
  - [section] "Prior work has shown that disclosing difficult information to virtual agents is easier than disclosing directly to other humans [59]."
  - [corpus] Related paper "Observe, Ask, Intervene" found similar disclosure patterns but struggled to influence behavior.
- **Break condition:** Feedback recipients distrust the feedback because they cannot assess its credibility or source context (field study P9: "it would maybe be taken personally or discounted if it's a one-off situation").

### Mechanism 3: Persistent Goal Visibility During Meetings
- **Claim:** If approved goals remain visible in a sidebar throughout the meeting, they maintain salience and increase likelihood of behavioral follow-through.
- **Mechanism:** Environmental cue → sustained attention to goal → increased self-monitoring during interaction.
- **Core assumption:** Visual persistence translates to behavioral adherence; users notice and act on the cue.
- **Evidence anchors:**
  - [section] "Upon approval, the goal and reflection appear in a persistent sidebar panel... This panel persists throughout the subsequent meeting to ensure it remains salient."
  - [section] P9: "the AI agent recommended participation balance... having that right there... I definitely was more mindful of it during the meeting."
  - [corpus] Weak corpus evidence; no related papers directly test persistent goal visibility in meetings.
- **Break condition:** Users habituate to the sidebar and stop noticing it, or meetings are too fast-paced for reflection.

## Foundational Learning

- **Concept:** Induced Hypocrisy Procedure (IHP)
  - **Why needed here:** This is the primary behavioral intervention mechanism; understanding it is essential for debugging why the agent works (or doesn't).
  - **Quick check question:** Can you explain why IHP requires both normative commitment and transgression recall, rather than just one step?

- **Concept:** Cognitive Dissonance Theory
  - **Why needed here:** Explains the psychological pressure driving behavior change; helps anticipate when users might resist or rationalize instead of change.
  - **Quick check question:** If a user responds to transgression recall by saying "that wasn't my fault," what does this suggest about dissonance resolution?

- **Concept:** Sociotechnical Gap (Ackerman)
  - **Why needed here:** The field study revealed organizational barriers (hierarchy, time pressure, feedback norms) that the technical system couldn't bridge.
  - **Quick check question:** Why might a system succeed in a lab study but fail in organizational deployment despite identical functionality?

## Architecture Onboarding

- **Component map:** Meeting data collection -> Agent orchestration -> IHP conversation flow -> Goal persistence -> Feedback delivery
- **Critical path:**
  1. Meeting ends → collect speaking time per participant
  2. Emily initiates private feedback solicitation (inclusion-focused prompts)
  3. User approves feedback → anonymized, routed to recipient
  4. Before next meeting → Emily delivers feedback via IHP (normative + transgression salience)
  5. User approves goal → persisted in sidebar during meeting

- **Design tradeoffs:**
  - Anonymity protects givers but reduces credibility for receivers (P7: "do I care about this feedback or not?")
  - Agent pushes back on shallow answers (effective for reflection) but creates friction in time-pressed environments
  - Female-presenting agent increases compliance (per prior work) but may reinforce gendered labor expectations

- **Failure signatures:**
  - Users speed through pre-meeting conversation → no genuine reflection → no behavior change
  - Feedback declined due to context mismatch → agent falls back to generic participation advice
  - Junior employees benefit; senior employees see no value → adoption stalls at leadership level

- **First 3 experiments:**
  1. **Timing intervention:** Move IHP conversation to a scheduled low-pressure time (e.g., virtual commute) rather than immediately pre-meeting; measure engagement depth and goal quality.
  2. **Feedback credibility cues:** Add context about whether feedback is recurrent vs. one-off, and aggregate themes across meetings; test impact on receiver trust and action.
  3. **Role-aware framing:** Tailor agent behavior based on organizational role (coach mode for juniors, feedback-framing assistant for managers); compare adoption rates across hierarchy levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the behavioral changes induced by the Induced Hypocrisy Procedure (IHP) persist over time without repeated intervention?
- Basis in paper: [explicit] The authors state, "Notably, we did not measure long-term behavior change, and there is little longitudinal research on the Induced Hypocrisy Procedure."
- Why unresolved: The study design involved immediate pre-meeting interventions and did not track whether participants maintained inclusive behaviors in subsequent meetings where the agent was absent.
- What evidence would resolve it: A longitudinal field study tracking participant behavior and meeting balance weeks or months after the initial IHP intervention is withdrawn.

### Open Question 2
- Question: How can IHP-based systems prevent the enforcement of behavioral homogeneity when the underlying peer feedback reflects specific biases?
- Basis in paper: [explicit] The authors ask, "The question of which norms warrant enforcement through cognitive dissonance is therefore critical... feedback would then cause IHP to pressure behavior change that enforces homogeneity rather than supporting genuine inclusion."
- Why unresolved: The system relies on peer feedback to define norms, but the authors did not implement mechanisms to distinguish between constructive inclusion feedback and feedback that merely enforces conformity to a dominant communication style.
- What evidence would resolve it: Algorithmic auditing of feedback content combined with user studies measuring perceived psychological safety and cultural expression within diverse teams.

### Open Question 3
- Question: Can non-gendered agent personas achieve the same effectiveness in IHP interventions as female-presenting personas?
- Basis in paper: [explicit] The authors note that "future work is needed to identify non-gendered cues that could support the effectiveness of IHP agents without relying on gendered representations."
- Why unresolved: The current system used a female persona ("Emily") based on prior literature suggesting higher compliance, leaving the efficacy of neutral or non-humanoid designs untested.
- What evidence would resolve it: A comparative study measuring behavior change compliance rates across conditions using gendered versus non-gendered or abstract agent avatars.

### Open Question 4
- Question: How does the effectiveness of AI-mediated feedback differ in organizations with rigid hierarchies compared to the "feedback-forward" culture observed in the field study?
- Basis in paper: [inferred] The authors acknowledge the limitation that "The consulting firm’s... relatively feedback-forward attitude may not reflect more traditional or rigid organizational structures where feedback exchange faces different barriers."
- Why unresolved: The field study was conducted at a single small consulting firm with a specific collaborative culture; it is unknown if the tool would fail or succeed differently in traditional, siloed, or strictly hierarchical corporate environments.
- What evidence would resolve it: Deployment of the system across multiple organizations with varying Hofstede cultural dimensions or power distance indices to correlate organizational structure with adoption rates.

## Limitations
- Field study sample size (n=10) is small and limited to one consulting firm, making generalization difficult
- Speaking time metric may not fully capture inclusion quality (e.g., chat participation, non-verbal engagement not measured)
- Lab study artificial setting may not capture real organizational dynamics including power structures and established communication patterns

## Confidence
- **High Confidence:** Induced Hypocrisy Procedure improves speaking time balance and meeting quality in controlled settings; AI agents are psychologically safer for feedback exchange; organizational context significantly impacts AI tool effectiveness
- **Medium Confidence:** AI agents are psychologically safer recipients for difficult feedback than humans; persistent goal visibility improves behavioral adherence; normative commitment + transgression recall are both necessary for IHP
- **Low Confidence:** Speaking time balance directly correlates with meeting inclusion quality; identified field study barriers are representative of broader contexts; female-presenting AI agents consistently increase compliance

## Next Checks
1. **Organizational Context Generalization Study:** Deploy Emily across 3-5 organizations with different sizes, cultures, and hierarchical structures to validate whether the identified barriers (hierarchy, time pressure, feedback norms) are consistent patterns or firm-specific phenomena.

2. **Behavioral Mechanism Validation:** Conduct think-aloud protocols during IHP conversations to directly observe whether users experience cognitive dissonance and how they resolve it, distinguishing between genuine behavior change and rationalization strategies.

3. **Multi-modal Inclusion Measurement:** Expand metrics beyond speaking time to include chat participation, non-verbal cues (when video is on), and post-meeting survey data to validate whether speaking balance truly captures meeting inclusion quality.