---
ver: rpa2
title: Quantifying Logical Consistency in Transformers via Query-Key Alignment
arxiv_id: '2502.17017'
source_url: https://arxiv.org/abs/2502.17017
tags:
- reasoning
- logical
- heads
- baseline
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation strategy for logical reasoning
  in large language models (LLMs) that leverages query-key alignments within transformer
  attention heads. The authors propose computing a QK-score as the dot product between
  query vectors of candidate answers and key vectors of statements, identifying specific
  attention heads that reliably assess logical consistency.
---

# Quantifying Logical Consistency in Transformers via Query-Key Alignment

## Quick Facts
- arXiv ID: 2502.17017
- Source URL: https://arxiv.org/abs/2502.17017
- Reference count: 36
- Introduces QK-score method for evaluating logical reasoning in LLMs via attention head alignment

## Executive Summary
This paper presents a novel evaluation strategy for logical reasoning in large language models that leverages query-key alignments within transformer attention heads. The method computes a QK-score as the dot product between query vectors of candidate answers and key vectors of statement tokens, identifying specific attention heads that reliably assess logical consistency. Through extensive experiments on multiple logical reasoning benchmarks, the approach demonstrates improved robustness against distractors and reasoning depth compared to baseline probability-based methods. The evaluation covers models ranging from 1.5B to 70B parameters, showing that selected heads maintain stable performance across varying reasoning depths and generalize well to cross-dataset evaluation.

## Method Summary
The method extracts a QK-score from carefully chosen attention heads by computing the dot product between query vectors of candidate answer tokens ("true"/"false") and key vectors of statement-end tokens. A single forward pass through the frozen LLM captures all query and key vectors, which are then used to compute QK-scores for all (layer, head) pairs. The best-performing head is selected on a calibration set of ~600 balanced samples, then used to compare QK-scores for candidate answers during inference. This efficient procedure avoids extensive model modifications or head-by-head ablation studies common in prior work, offering a scalable alternative for identifying logical reasoning components in transformers.

## Key Results
- Selected attention heads achieve over 10% accuracy improvement compared to baseline probability-based methods on logical reasoning benchmarks
- Heads maintain stable performance across reasoning depths (1-5 hops) and distractor counts, functioning as "verification anchors"
- The method shows consistent performance across model sizes from 1.5B to 70B parameters
- Cross-dataset evaluation reveals 3-4 heads that maintain superiority across format changes and reasoning types

## Why This Works (Mechanism)

### Mechanism 1: Query-Key Alignment Captures Semantic Logical Consistency
The dot product between query vectors of candidate answer tokens and key vectors of statement-end tokens encodes logical validity signals that outperform final-layer probability predictions. Attention heads compute query-key alignment scores representing semantic relationships between tokens, with specific heads learning to align valid inference conclusions with their corresponding statement representations. This bypasses later-stage processing that may introduce biases or obscure reasoning signals. Core assumption: Logical validity is encoded in the semantic alignment space of certain attention heads, independent of positional encodings or final-layer transformations.

### Mechanism 2: Head Selection Identifies "Verification Anchor" Circuits
A small subset of attention heads function as logical consistency verifiers that maintain stable performance across reasoning depths and distractor counts. Different attention heads specialize in different computational roles, with the calibration procedure identifying heads that consistently correlate QK-scores with logical validity. These heads appear to implement a verification function rather than the primary reasoning computation—they check whether a conclusion follows from premises. Core assumption: The calibration dataset's logical structure is representative of the target distribution; head specialization is sufficiently consistent across similar reasoning patterns.

### Mechanism 3: Single-Pass Head Enumeration Enables Scalable Analysis
Computing QK-scores across all heads in a single forward pass provides a tractable alternative to ablation-based interpretability methods while maintaining discriminative power. Standard attention computation already generates all query and key vectors, with QK-score extraction adding only a dot product operation per head per candidate answer. This avoids the O(layers × heads) forward passes required by ablation studies, making the method applicable to 70B+ parameter models. Core assumption: Query-key interactions are sufficient for capturing logical validity; attention weights and value vectors are not necessary for this specific assessment.

## Foundational Learning

- **Concept: Transformer Attention Query-Key Computation**
  - Why needed here: The entire method depends on understanding that Q·K^T computes semantic similarity scores before softmax normalization and value mixing
  - Quick check question: Given query vector q ∈ R^d and key vector k ∈ R^d, what does a high dot product q^T k indicate before the softmax operation is applied?

- **Concept: Logical Inference and Deductive Reasoning Rules**
  - Why needed here: The benchmarks test specific deduction patterns (Modus Ponens, Modus Tollens, syllogisms). Understanding these patterns is necessary to interpret why certain heads specialize in logical verification
  - Quick check question: For the premises "All A are B" and "X is an A," what conclusion follows via Modus Ponens, and how would you construct a false conclusion to test a model's discrimination ability?

- **Concept: Calibration vs. Evaluation Data Splits**
  - Why needed here: The method requires a held-out calibration set for head selection, separate from evaluation data. Improper splitting would inflate results through selection overfitting
  - Quick check question: If you select the best-performing head on your evaluation set, why would your reported accuracy be optimistically biased? How does the paper's 600-sample calibration split address this?

## Architecture Onboarding

- **Component map:**
  ```
  Input: (context c, statement s, candidate answers a0="true", a1="false")
     ↓
  Tokenization & Embedding
     ↓
  Transformer Layers (l = 1...L)
     └─ Multi-Head Attention (h = 1...H per layer)
         ├─ Query projection: q(l,h)_token = W_q · hidden_state
         ├─ Key projection: k(l,h)_token = W_k · hidden_state
         └─ [EXTRACTED] QK-score: S(l,h) = q(l,h)_answer^T · k(l,h)_statement_end
     ↓
  Calibration Phase: Select head (l*, h*) maximizing accuracy
     ↓
  Inference: Compare S(l*,h*)(c,s,"true") vs S(l*,h*)(c,s,"false")
  ```

- **Critical path:**
  1. Tokenize input ensuring statement-end token and answer tokens ("true"/"false") are identifiable
  2. Run single forward pass, capturing all query and key vectors
  3. Extract QK-scores for candidate answers across all (layer, head) pairs
  4. Use calibration-identified head to compare QK-scores and select answer

- **Design tradeoffs:**
  - **Calibration size vs. selection quality:** Paper uses 600 balanced samples; smaller calibration risks selecting spurious heads, larger calibration improves selection but requires more labeled data
  - **Head specialization vs. generalization:** Heads selected on Modus Ponens may not transfer to proof-by-contradiction; paper tests only two rule sets and notes this limitation
  - **Format sensitivity:** True/false format outperforms yes/no format in cross-dataset tests; answer token choice affects which heads are discriminative

- **Failure signatures:**
  - QK-scores near zero for both candidates → head has no logical specialization for this input type
  - Reversed accuracy (invalid inferences scored higher) → observed for head (22, 16) in DeepSeek-R1 on PARARULE Plus; suggests head encodes logical structure but with inverted polarity
  - High variance across reasoning depths → head is not a stable verification anchor; discard from selection
  - Calibration accuracy ~50% → likely class imbalance or insufficient signal; expand and rebalance calibration set

- **First 3 experiments:**
  1. **Reproduce in-domain results:** Implement QK-score extraction on Qwen2.5-1.5B-Instruct with PrOntoQA-OOD (Modus Ponens, depth 1-3). Target: identify 3+ heads exceeding baseline by >5% on held-out evaluation split of 1000 samples
  2. **Ablation on calibration size:** Systematically vary calibration set size (100, 200, 400, 600, 800 samples) while keeping evaluation fixed. Measure selection stability (do the same heads remain in top-5?) and accuracy degradation
  3. **Cross-format robustness test:** Take heads selected on true/false format, evaluate on identical logical structures formatted as yes/no questions. Quantify format sensitivity gap and identify if certain heads show cross-format stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do logical consistency capabilities within attention heads evolve during the transition from base pre-trained models to instruction-tuned versions?
- **Basis in paper:** [explicit] The authors state they "leave for the future work" the question of "if and how capabilities for logic in attention heads change during the transition from base to fine-tuned version."
- **Why unresolved:** The study exclusively evaluated Instruct- and Chat-tuned models, omitting base models
- **What evidence would resolve it:** A comparative analysis of QK-scores in base versus fine-tuned checkpoints of the same model on identical logical benchmarks

### Open Question 2
- **Question:** Are specific attention heads specialized for distinct logical principles (e.g., Modus Tollens vs. Disjunctive Syllogism), or do they function as general logical reasoners?
- **Basis in paper:** [explicit] The authors note that experiments were limited to two sets of deduction rules and suggest "the ‘scope’ of certain attention heads is limited to different sets of logical principles."
- **Why unresolved:** The experimental scope was restricted primarily to Modus Ponens and composed rules, leaving the specialization of heads for other logical operators untested
- **What evidence would resolve it:** Mapping head performance (QK-score accuracy) across a granular dataset isolating specific logical operators to identify functional specialization

### Open Question 3
- **Question:** Can QK-scoring be effectively integrated with Chain-of-Thought (CoT) prompting to assess the coherence of intermediate reasoning steps?
- **Basis in paper:** [explicit] The conclusion suggests future work should "explore synergy with chain-of-thought prompting," as current CoT methods "lack mechanisms to assess the coherence of these logical transitions."
- **Why unresolved:** The current method evaluates the final logical validity in a single forward pass but has not been applied to validate the intermediate transitions generated by CoT
- **What evidence would resolve it:** A method that applies QK-scoring to intermediate steps in a CoT chain and benchmarks task accuracy against standard CoT

## Limitations

- **Dataset Generalization Fragility:** Cross-dataset performance is inconsistent, with only 3-4 selected heads maintaining superiority across format changes (true/false vs yes/no) and reasoning types
- **Mechanism Specificity Uncertainty:** Unclear whether selected heads implement genuine logical verification or encode statistical correlations between statement structure and answer tokens
- **Calibration Representativeness Concerns:** The 600-sample calibration set is relatively small, and heads selected on limited rule sets may not generalize to other logical operations

## Confidence

**High Confidence:** The core QK-score extraction methodology is sound and reproducible. The computational efficiency claims are well-supported by the single-pass design versus ablation-based approaches.

**Medium Confidence:** The claim that selected heads function as "verification anchors" is partially supported but undermined by cross-dataset inconsistencies. The scalability benefits are demonstrated but only within tested model families.

**Low Confidence:** Claims about the semantic nature of QK-alignment encoding logical validity are not conclusively proven. The method may be capturing format-specific artifacts rather than genuine logical reasoning circuits.

## Next Checks

1. **Cross-Format Transfer Robustness:** Systematically test the same logical statements across multiple formats (true/false, yes/no, correct/incorrect) to quantify format sensitivity. Measure whether any heads show consistent performance across formats, indicating genuine logical reasoning versus format-specific correlations.

2. **Logical Rule Generalization:** Expand beyond Modus Ponens and Modus Tollens to test heads on syllogism variations, proof-by-contradiction, and inductive reasoning patterns. This would validate whether selected heads truly implement general logical verification or are specialized to specific deduction rules.

3. **Ablation on Head Importance:** Perform partial ablations on the selected verification heads to determine if their removal degrades logical reasoning accuracy more than random heads. This would directly test whether these heads are functionally important for logical consistency versus merely correlating with correct answers.