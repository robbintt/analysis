---
ver: rpa2
title: 'The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor
  Attacks'
arxiv_id: '2512.10402'
source_url: https://arxiv.org/abs/2512.10402
tags:
- backdoor
- attack
- trigger
- data
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Eminence, a backdoor attack that exploits\
  \ feature boundary ambiguity to achieve high attack success with minimal poisoning\
  \ (\u22640.01%) and negligible clean accuracy loss (<0.5%). The core idea is to\
  \ identify and poison sparse decision boundary regions, where relabeling a few samples\
  \ induces disproportionate misclassification."
---

# The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks

## Quick Facts
- arXiv ID: 2512.10402
- Source URL: https://arxiv.org/abs/2512.10402
- Reference count: 40
- Primary result: Achieves >90% ASR with ≤0.01% poisoning and <0.5% clean accuracy loss by exploiting feature boundary ambiguity

## Executive Summary
This paper introduces Eminence, a novel backdoor attack that exploits the ambiguous boundary regions between class prototypes in feature space. Rather than optimizing triggers to resemble specific classes, Eminence forces poisoned samples into low-density inter-class regions where minimal relabeling causes disproportionate misclassification. The attack achieves high success rates (>90%) with extremely low poisoning rates (≤0.01%) while maintaining near-clean accuracy, and demonstrates robustness against both input-based and model-based defenses across white-box, gray-box, and black-box scenarios.

## Method Summary
Eminence works by first optimizing a universal trigger pattern using a surrogate model to minimize intra-trigger feature variance, forcing poisoned samples into ambiguous boundary regions between class prototypes. The trigger is an additive noise pattern constrained by ℓ∞ norm. During training, sparse samples (≤0.01% of data) are selected, the trigger is applied, and these samples are relabeled to the target class. The attack leverages influence functions and margin analysis to show that relabeling samples in these low-density boundary regions induces exponential gains in attack success while bounding clean accuracy degradation. The method is designed to be transferable across architectures by targeting geometric relationships between prototypes rather than specific feature values.

## Key Results
- Achieves >90% ASR across white-box, gray-box, and black-box scenarios
- Maintains <0.5% clean accuracy drop with ≤0.01% poisoning rate
- Outperforms state-of-the-art methods on CIFAR-10/100, TinyImageNet, and transformer architectures
- Demonstrates resistance to input-based detection and model-based defenses

## Why This Works (Mechanism)

### Mechanism 1: Feature Collapse into Ambiguous Margins
The attack optimizes a trigger to minimize intra-trigger variance, forcing poisoned samples into low-density "ambiguous boundary regions" between class prototypes rather than specific class clusters. This works because the batch contains mixed classes, creating implicit tension that prevents collapse onto a single class prototype. The surrogate model must preserve semantic structure for this to work. Break condition: surrogate model has collapsed features or batch size too small.

### Mechanism 2: Boundary Absorption via Exponential Leverage
Relabeling sparse samples in the ambiguous margin induces disproportionate shifts in the decision boundary. The model's uncertainty allows small gradient updates (scaled by inverse Hessian) to shift the hyperplane significantly. Theoretically, the probability of the boundary "snapping" to the target class grows exponentially with the number of poisoned samples. Break condition: high-density noise in the margin region.

### Mechanism 3: Universal Transferability via Prototype Geometry
Attacks targeting geometric inter-class regions generalize better across architectures than class-specific feature attacks. By targeting the geometric relationship between prototypes rather than absolute feature values, the trigger exploits structural similarities shared across different model initializations and architectures. Break condition: victim model uses fundamentally different inductive bias.

## Foundational Learning

- **Influence Functions & The Hessian**: Used to theoretically bound parameter drift from poisoned samples. Understanding the Hessian explains why margin samples have high leverage through movement in directions of low curvature. *Quick check*: How does loss landscape curvature affect impact of single gradient step from margin sample?

- **Class Prototypes & Metric Learning**: The "Ambiguous Boundary Region" is defined relative to class prototypes. Understanding feature clustering around means is essential to visualize trigger placement in the inter-cluster band. *Quick check*: If two classes have high prototype overlap, how does the width of the ambiguous band change?

- **Projected Gradient Descent (PGD)**: The trigger optimization is constrained by ℓ∞ bound for stealth. The appendix treats this as a PGD problem on trigger parameters. *Quick check*: What happens if you remove the projection operator in trigger optimization?

## Architecture Onboarding

- **Component map**: Surrogate Model -> Trigger Network -> Optimization Loop -> Injection Pipeline

- **Critical path**:
  1. **Surrogate Setup**: Load pre-trained surrogate model and extract penultimate layer as feature space
  2. **Trigger Synthesis**: Run Algorithm 1 to output universal trigger pattern
  3. **Execution**: Select boundary samples, apply trigger, relabel (dirty) or keep label (clean), inject into training set

- **Design tradeoffs**:
  - **Noise Weight (α)**: Higher α yields higher ASR but reduces visual stealth
  - **Sample Selection**: Exact sparse margin targeting is optimal but requires estimation; random selection is baseline
  - **Surrogate Fidelity**: White-box (identical architecture) guarantees geometric alignment; Black-box relies on universality of prototype geometry

- **Failure signatures**:
  - **High Clean Accuracy Drop**: Trigger collapsed into high-density region rather than sparse margin
  - **Low ASR**: Trigger features didn't collapse sufficiently or surrogate/victim geometry mismatch too high
  - **Detection by Scale-up/Beatrix**: Trigger pattern too distinct or perturbation scale inconsistent

- **First 3 experiments**:
  1. **Margin Visualization**: Generate t-SNE plots of triggered features vs. clean prototypes to verify inclusion in ambiguous band
  2. **Poison Rate Ablation**: Plot ASR vs. number of poisons to check predicted exponential growth curve
  3. **Transferability Test**: Optimize trigger on ResNet-18; test attack success on ViT and CCT to verify black-box robustness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Theoretical foundation relies on assumptions about feature space geometry that may not hold universally
- Core claim about consistent prototype separability across architectures lacks direct mechanistic validation
- "Low-density boundary" assumption is critical but difficult to verify in practice
- Influence function analysis assumes convex or near-convex loss landscape that may not hold for deep networks

## Confidence

**High Confidence**:
- ASR >90% at poison rates ≤0.01% empirically validated across multiple datasets and architectures
- Visual stealth demonstrated through provided examples
- Resistance to input-based detection methods shown through comparative experiments

**Medium Confidence**:
- Exponential relationship between poison count and ASR theoretically derived but not exhaustively validated
- Universal transferability claim empirically supported but geometric mechanism not directly measured
- Defense-resistance claims based on comparisons to specific baselines

**Low Confidence**:
- Exact boundary density estimation method for optimal sample selection not specified
- Interaction between trigger optimization and different training dynamics not characterized

## Next Checks

1. **Margin Density Verification**: For a given target pair of classes, extract features from surrogate model, compute prototype locations, and empirically measure density of clean samples in the ambiguous band B_ε to compare with theoretical low-density assumption.

2. **Influence Landscape Mapping**: For poisoned samples in margin vs. dense regions, measure actual parameter change Δθ after single gradient step to verify margin samples produce disproportionately larger changes per unit poison.

3. **Cross-Architecture Prototype Alignment**: Take same source dataset, extract penultimate-layer features from ResNet-18, ViT, and CCT, and compute pairwise angular distances between class prototypes to quantify geometric consistency enabling transferability.