---
ver: rpa2
title: 'Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned
  and Open Challenges'
arxiv_id: '2511.15652'
source_url: https://arxiv.org/abs/2511.15652
tags:
- parking
- learning
- agent
- tasks
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates continual reinforcement learning (CRL)\
  \ in cyber-physical systems through experiments in an autonomous driving parking\
  \ environment. The agent is trained sequentially on four parking scenarios (perpendicular,\
  \ 25\xB0 diagonal, 50\xB0 diagonal, and parallel) using Proximal Policy Optimisation\
  \ (PPO)."
---

# Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges

## Quick Facts
- arXiv ID: 2511.15652
- Source URL: https://arxiv.org/abs/2511.15652
- Authors: Kim N. Nolle; Ivana Dusparic; Rhodri Cusack; Vinny Cahill
- Reference count: 15
- Primary result: Sequential CRL training causes catastrophic forgetting in autonomous parking, partially mitigated by EWC but still degrades previous task performance

## Executive Summary
This paper investigates continual reinforcement learning (CRL) in cyber-physical systems through experiments in an autonomous driving parking environment. Using Proximal Policy Optimisation (PPO) trained sequentially on four parking scenarios, the authors demonstrate catastrophic forgetting where previously learned skills degrade as new tasks are learned. While Elastic Weight Consolidation (EWC) provides partial mitigation, it fails to prevent performance degradation in earlier tasks. The results reveal fundamental limitations of neural networks for CRL, including shared weights causing forgetting, limited capacity, and hyperparameter sensitivity across tasks.

## Method Summary
The study uses PPO with a neural network policy to learn four sequential parking tasks (perpendicular, 25° diagonal, 50° diagonal, parallel) in a simulated environment. Each task is trained for 500k steps sequentially with fixed hyperparameters tuned on the first task. EWC is implemented to mitigate forgetting by computing Fisher information matrices after each task and applying quadratic penalties to important weights during subsequent training. Success rates are measured for all four tasks throughout training, with results averaged over five runs. The study also examines the impact of task order by reversing the training sequence.

## Key Results
- Catastrophic forgetting occurs during sequential training, with success rates dropping to near zero on previous tasks when switching to new ones
- EWC reduces but does not eliminate forgetting, with previous task performance stabilizing below original levels
- Neural network capacity limitations are evident, with first-layer weights showing highest importance and task order affecting learning success
- Parallel parking fails to learn with shared hyperparameters, requiring different configurations than easier tasks

## Why This Works (Mechanism)

### Mechanism 1: Elastic Weight Consolidation (EWC) for Forgetting Mitigation
EWC constrains updates to weights identified as important for previous tasks via Fisher information penalties. While this reduces catastrophic forgetting, the agent still doesn't retain original performance levels in previous tasks. The partial success suggests that while important weight identification helps, the diagonal Fisher approximation and weight overlap across tasks limit effectiveness.

### Mechanism 2: Sequential On-Policy Training with Shared Network Weights
PPO's on-policy gradient updates overwrite task-specific representations when training sequentially. The same neural network weights are repurposed for new tasks, destroying previously learned representations. This weight interference is fundamental to on-policy methods without replay mechanisms.

### Mechanism 3: Transfer and Interference via First-Layer Representations
First-layer weights receive highest Fisher importance scores, suggesting they encode critical input features. When trained sequentially, the first task's first-layer representations persist and bias subsequent learning. This creates both potential transfer benefits and negative interference depending on task order.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**:
  - Why needed here: Core RL algorithm used; understanding its on-policy nature explains why sequential training causes complete overwriting rather than gradual adaptation
  - Quick check question: Can you explain why on-policy algorithms like PPO are more susceptible to catastrophic forgetting than off-policy methods with replay buffers?

- **Catastrophic Forgetting / Interference**:
  - Why needed here: The central problem investigated; distinguishing between weight-level interference and representation-level interference is essential for interpreting EWC results
  - Quick check question: Why does constraining "important" weights via EWC only partially reduce forgetting rather than eliminating it?

- **Fisher Information Matrix for Weight Importance**:
  - Why needed here: EWC's theoretical foundation; understanding that Fisher information approximates parameter sensitivity to output changes clarifies why it may miss task-critical weights in deep networks
  - Quick check question: What assumptions does the diagonal Fisher approximation make, and when might it fail to capture true weight importance?

## Architecture Onboarding

- **Component map**:
  Environment (parking simulator with 4 scenarios) -> PPO agent (neural network policy) -> EWC module (Fisher information computation) -> Training loop (sequential task presentation)

- **Critical path**:
  1. Define reward function and state/action abstractions for first task
  2. Hyperparameter search on first task (perpendicular parking used as baseline)
  3. Train task 1, compute and store Fisher information for EWC
  4. Sequentially train tasks 2-4 with EWC penalty active
  5. Evaluate all tasks periodically during training to measure forgetting

- **Design tradeoffs**:
  - Shared hyperparameters across tasks vs. per-task tuning (paper used shared, but parallel parking required different settings)
  - EWC regularization strength: too high blocks new learning, too low fails to protect old tasks
  - Network capacity: larger networks may reduce overlap but increase sample complexity and training cost
  - Task order: easier tasks first may establish useful representations, but this is domain-dependent

- **Failure signatures**:
  - Success rate drops to ~0 immediately after task switch (catastrophic, not gradual forgetting)
  - EWC reduces but does not prevent drop; previous task performance plateaus below original level
  - Parallel parking fails to learn entirely with shared hyperparameters
  - Reversed task order (parallel first) causes complete failure across all tasks

- **First 3 experiments**:
  1. **Baseline sequential training without EWC**: Train PPO on all 4 parking scenarios sequentially with fixed hyperparameters; measure success rate for all tasks throughout. Expect to observe clear catastrophic forgetting pattern.
  2. **EWC integration with hyperparameter sweep**: Add EWC, sweep regularization coefficient (λ) across [1, 10, 100, 1000]; assess whether any setting substantially preserves previous task performance without blocking new learning.
  3. **Task order ablation**: Reverse training order (parallel → 50° → 25° → perpendicular) to test whether first-task bias explains capacity utilization issues; compare final multi-task performance to original order.

## Open Questions the Paper Calls Out
None

## Limitations
- Neural network architecture details are unspecified, making it difficult to assess whether capacity limitations are due to model size or fundamental CRL constraints
- Reward function and termination conditions are not fully described, which could affect learning dynamics across different parking scenarios
- EWC hyperparameter settings are not reported, limiting reproducibility and understanding of why regularization only partially mitigates forgetting
- Task complexity differences between perpendicular and parallel parking are not quantified, making it unclear whether failure to learn parallel parking is due to inherent difficulty or hyperparameter mismatch

## Confidence

- **High confidence**: Catastrophic forgetting occurs in sequential CRL training of related CPS tasks; sequential training with shared hyperparameters fails on more complex tasks
- **Medium confidence**: EWC provides partial mitigation but does not eliminate forgetting; first-layer representations show task-specific importance and order-dependent bias
- **Low confidence**: The parallel parking failure is primarily due to hyperparameter mismatch rather than fundamental capacity limitations

## Next Checks
1. Perform ablation study on network capacity by training with varying hidden layer sizes (e.g., [64, 128, 256, 512] units per layer) to determine if increased capacity reduces forgetting
2. Implement per-task hyperparameter tuning for each parking scenario and measure whether this eliminates the parallel parking failure mode while maintaining acceptable multi-task performance
3. Test alternative forgetting mitigation approaches (e.g., replay buffers, meta-learning) alongside EWC to determine if the observed partial success is specific to EWC or represents a general CRL limitation in CPS domains