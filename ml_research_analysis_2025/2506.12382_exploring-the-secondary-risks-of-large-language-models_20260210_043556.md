---
ver: rpa2
title: Exploring the Secondary Risks of Large Language Models
arxiv_id: '2506.12382'
source_url: https://arxiv.org/abs/2506.12382
tags:
- risks
- arxiv
- secondary
- response
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces secondary risks as a novel class of non-adversarial\
  \ LLM failures that emerge during benign interactions, often evading standard safety\
  \ mechanisms. To enable systematic evaluation, the authors define two risk primitives\u2014\
  excessive response and speculative advice\u2014based on information-theoretic length\
  \ bounds and logical precondition inference."
---

# Exploring the Secondary Risks of Large Language Models

## Quick Facts
- arXiv ID: 2506.12382
- Source URL: https://arxiv.org/abs/2506.12382
- Reference count: 40
- Key outcome: Secondary risks (excessive response and speculative advice) are novel, prevalent, transferable LLM failures that evade standard safety mechanisms, with SecLens achieving up to 75.82% attack success rate.

## Executive Summary
This work introduces secondary risks as a novel class of non-adversarial LLM failures that emerge during benign interactions, often evading standard safety mechanisms. To enable systematic evaluation, the authors define two risk primitives—excessive response and speculative advice—based on information-theoretic length bounds and logical precondition inference. They propose SecLens, a black-box, multi-objective search framework that elicits these risks by optimizing task relevance, risk activation, and linguistic plausibility. Extensive experiments on 16 models show that secondary risks are prevalent, transferable across model families, and modality-independent, with SecLens achieving up to 75.82% attack success rate—significantly outperforming baselines. These results underscore the urgent need for enhanced safety mechanisms to address subtle yet harmful LLM behaviors in real-world deployments.

## Method Summary
The authors introduce SecLens, a black-box multi-objective evolutionary search framework designed to systematically elicit secondary risks in LLMs. SecLens optimizes candidate prompts using a composite fitness function balancing risk induction, task compliance, and stealth, employing semantics-guided crossover and mutation operators. The framework is evaluated on SecRiskBench, a benchmark of 650 prompts across 8 categories and 16 subtypes, using 16 diverse models. Results show SecLens achieves up to 75.82% attack success rate, significantly outperforming baselines and demonstrating the prevalence and transferability of secondary risks across model families and modalities.

## Key Results
- Secondary risks are prevalent across 16 models, with SecLens achieving up to 75.82% attack success rate
- Secondary risks transfer across model families and modalities, indicating fundamental vulnerabilities
- SecLens outperforms baselines by 10-15 percentage points, with few-shot guidance accelerating convergence by 2-3x

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive response risk emerges when conditional entropy H(Y|X) grows beyond the information-theoretic minimum required for task completion.
- Mechanism: The model generates additional content beyond G_expected, appending potentially harmful material Re via concatenation (G_exce = G_expected ⊕ Re). This manifests as semantic drift where outputs remain syntactically valid but introduce unwarranted information, stereotypes, or biased conclusions.
- Core assumption: LLMs trained with RLHF may develop length-response bias and reward-hacking behaviors during PPO that systematically inflate output length beyond task necessity.
- Evidence anchors:
  - [abstract]: Risk primitives defined using "information-theoretic length bounds"
  - [section 3.2, Eq. 2]: Formal characterization as G_exce = G_expected ⊕ Re
  - [section 5]: Links to "Long-Response Bias" in RLHF training data and PPO reward accumulation patterns
  - [corpus]: Weak direct support; corpus focuses on jailbreaks and embedding attacks, not non-adversarial entropy growth
- Break condition: If temperature is reduced to near-deterministic levels and excessive response rate remains constant, entropy-based explanation is insufficient.

### Mechanism 2
- Claim: Speculative advice risk occurs when mutual information I(Y;X) shifts away from the expected input-output dependency, causing trajectory divergence onto unintended paths.
- Mechanism: The model infers unstated user intent and produces recommendations that depart from the original request trajectory (G_spec = G_expected ⇝ Δs). This represents a fundamental misalignment where the model's value projections override explicit user constraints.
- Core assumption: Current alignment mechanisms (RLHF, DPO, RLAIF) are "shallow and not sufficiently powerful" to prevent speculative reasoning in ambiguous or open-ended scenarios.
- Evidence anchors:
  - [abstract]: Defined via "logical precondition inference"
  - [section 3.2, Eq. 3]: Formalized as trajectory shift G_expected ⇝ Δs
  - [section 5]: Cites insufficient robustness of value alignment mechanisms (referencing Ji et al., 2024)
  - [Appendix I, Table 19]: Mutual information decreases from ~0.93 to ~0.82 after prompt optimization, confirming dependency shift
  - [corpus]: Limited support; corpus papers address adversarial attacks rather than intent misalignment
- Break condition: If mutual information shift is not observed (I(Y;X) remains stable) but speculative advice still occurs, information-theoretic framing is incomplete.

### Mechanism 3
- Claim: SecLens discovers secondary risks via multi-objective evolutionary search that maximizes risk induction while maintaining task compliance and stealth.
- Mechanism: Population-based optimization evaluates candidate prompts using composite fitness F(x) = w_risk·R(f_θ(x),x) + w_task·TASKSCORE - w_stealth·DETECTSCORE. Semantics-guided crossover and mutation operators explore the prompt space while preserving linguistic plausibility through dependency parsing alignment.
- Core assumption: Secondary risk triggers exist as discoverable regions in the benign prompt space that can be systematically found through guided evolutionary search rather than random exploration.
- Evidence anchors:
  - [abstract]: "black-box, multi-objective search framework that elicits these risks by optimizing task relevance, risk activation, and linguistic plausibility"
  - [section 3.3, Eq. 6-7]: Formal fitness function and selection mechanism
  - [section 4.2, Table 2]: Achieves 67-76% attack success rates across 16 models, outperforming random/tuning/MCTS baselines by 10-15 percentage points
  - [Appendix D]: Convergence proof under elitist selection, diversity preservation, and ergodic variation assumptions
  - [corpus]: Indirect support; corpus includes black-box jailbreak methods (AutoBreach, MCTS) but not specifically for non-adversarial risk discovery
- Break condition: If optimized prompts fail to transfer across model families (transferability <30%), mechanism may be overfitting to specific model quirks rather than discovering universal vulnerabilities.

## Foundational Learning

- Concept: Information Theory (Shannon Entropy H(X), Conditional Entropy H(Y|X), Mutual Information I(X;Y))
  - Why needed here: Core theoretical framework distinguishing excessive response (entropy growth) from speculative advice (mutual information shift). Without this, risk primitives remain ad-hoc categories.
  - Quick check question: Given that excessive response correlates with entropy and speculative advice with mutual information shift, what would a hybrid risk exhibiting both patterns suggest about the underlying failure mode?

- Concept: Evolutionary Multi-Objective Optimization (Pareto Front, Fitness Functions, Selection Pressure)
  - Why needed here: SecLens operates as a multi-objective evolutionary algorithm balancing competing objectives. Understanding trade-offs between risk induction, task completion, and stealth is essential for interpreting results.
  - Quick check question: If w_task is increased relative to w_risk, would you expect attack success rates to increase or decrease, and why?

- Concept: RLHF Training Dynamics (Reward Hacking, Length Bias, Preference Modeling Limitations)
  - Why needed here: Paper hypothesizes secondary risks emerge from RLHF artifacts (length-response bias, reward accumulation strategies). This frames risks as systemic training issues rather than model bugs.
  - Quick check question: If secondary risks stem from RLHF, would models trained with DPO (which avoids explicit reward modeling) show reduced vulnerability?

## Architecture Onboarding

- Component map: Seed prompts -> Population Generator (crossover + mutation) -> Fitness Evaluator (risk × task - stealth) -> Selection Mechanism -> Optimized prompts

- Critical path:
  1. Seed prompt initialization with few-shot guidance -> establishes search direction
  2. Population generation via crossover/mutation -> explores semantic neighborhood
  3. Fitness evaluation (risk × task - stealth) -> ranks candidates
  4. Selection of top-k -> preserves best candidates
  5. Termination when risk threshold exceeded OR max generations T reached
  Failure at step 1 causes slow convergence; failure at step 3 causes false positives (non-risky prompts scored high).

- Design tradeoffs:
  - Weights (w_risk=1, w_task=0.2, w_stealth=0.1): Prioritizes risk discovery over task completion; ablations show <1% variance across configurations
  - Population size N vs. query budget: Paper uses O(NT)=O(B) complexity; SecLens achieves 34.6 avg queries vs. MCTS's 52.8
  - Black-box vs. white-box: Trade-off between applicability (black-box works on proprietary models) and efficiency (white-box could use gradient information)
  - LLM evaluator choice: GPT-4o (99% human correlation) vs. others (90-95%); higher correlation improves reliability but increases cost

- Failure signatures:
  - Semantic drift in prompts: Optimized prompts diverge from original intent (CLIP similarity <0.85 indicates broken semantic preservation)
  - Evaluator disagreement: Low Pearson correlation between LLM and human scores suggests evaluation prompt needs refinement
  - Overfitting to specific models: Transferability <30% indicates prompts exploit model-specific quirks rather than universal vulnerabilities
  - Degenerate solutions: High fitness but grammatically invalid or obviously adversarial prompts (DETECTSCORE spike)

- First 3 experiments:
  1. Baseline comparison: Run SecLens, Random, Tuning, and MCTS on 3 models (GPT-4o, Claude-3.7, Gemma-2-27b) with identical query budgets; expect SecLens to achieve 5-10 percentage point improvement over MCTS
  2. Ablation study: Remove few-shot guidance (0-shot) and measure convergence speed; expect 2-3× slower convergence per Figure 3
  3. Transferability test: Optimize prompts on GPT-4o, apply to 5 other models without re-optimization; expect 35-45% success rate (vs. 60-70% in-source) to confirm cross-model vulnerability

## Open Questions the Paper Calls Out
None

## Limitations
- Model coverage and evaluation bias: Study excludes smaller, task-specific, or highly specialized models; LLM-based evaluators may introduce circular bias
- Theoretical framing vs. empirical grounding: Information-theoretic mechanisms are proposed but not directly validated through entropy calculations
- Safety mechanism evaluation gap: Effectiveness of standard safety mechanisms against secondary risks is asserted but not empirically tested

## Confidence
- High confidence: Experimental methodology and finding that secondary risks transfer across model families and modalities
- Medium confidence: Information-theoretic risk primitive definitions and RLHF artifact hypotheses
- Low confidence: Distinction between secondary risks and existing jailbreak techniques requires more rigorous categorization

## Next Checks
1. Compute and report actual conditional entropy H(Y|X) for excessive response cases and verify correlation with risk severity
2. Apply SecLens to models with different safety training paradigms (RLHF vs. DPO vs. Constitutional AI) to test causal relationship
3. Deploy optimized prompts in controlled environments with human-in-the-loop evaluation to measure false positive rates and actual harm potential<|end_of_text|><|begin_of_text|>- State "Reproducible with At Most One Armchair Decision"