---
ver: rpa2
title: 'GENIE: Generative Note Information Extraction model for structuring EHR data'
arxiv_id: '2501.18435'
source_url: https://arxiv.org/abs/2501.18435
tags:
- data
- terms
- genie
- clinical
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GENIE is a generative model for structuring unstructured clinical
  notes in electronic health records. It performs end-to-end extraction of medical
  terms, assertion statuses, body locations, modifiers, values, units, and purposes
  using a fine-tuned small-scale LLM (Llama-3.1-8B-Instruct).
---

# GENIE: Generative Note Information Extraction model for structuring EHR data

## Quick Facts
- arXiv ID: 2501.18435
- Source URL: https://arxiv.org/abs/2501.18435
- Reference count: 0
- Key outcome: Single-pass LLM extracts structured medical data from EHR notes with 0.837 F1 for phrase extraction

## Executive Summary
GENIE is a generative model that replaces traditional multi-module NLP pipelines for structuring unstructured clinical notes. Using a fine-tuned Llama-3.1-8B-Instruct model, it performs end-to-end extraction of medical terms, assertion statuses, body locations, modifiers, values, units, and purposes in a single forward pass. The model processes entire paragraphs and outputs structured JSON, achieving competitive accuracy while running on consumer hardware.

## Method Summary
GENIE fine-tunes Llama-3.1-8B-Instruct on 180K MIMIC-III discharge report samples, where inputs are raw clinical paragraphs and outputs are structured JSON containing all extracted attributes. The data pipeline uses GPT-4o for annotation (locations, modifiers, values, purposes) and a separate Llama-2-7b model for assertion status, integrated via position-based matching. Training uses 3 epochs with cross-entropy loss, and inference runs on RTX 3090 24GB VRAM.

## Key Results
- Phrase extraction F1: 0.837 on human-labeled test data
- Assertion status accuracy: 0.912, location accuracy: 0.867
- Outperforms traditional tools like cTAKES and MetaMap
- Handles abbreviation restoration and word sense disambiguation
- Runs locally on consumer hardware (RTX 3090, 24GB VRAM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single fine-tuned LLM can replace multi-module NLP pipelines for EHR structuring with competitive accuracy.
- Mechanism: By training on input-output pairs where inputs are raw clinical paragraphs and outputs are structured JSON containing all extracted attributes, the model learns to perform NER, assertion classification, relation extraction, and normalization in a single forward pass—leveraging the LLM's pre-trained contextual understanding.
- Core assumption: The model's pre-trained knowledge of clinical language and text structure transfers sufficiently to the structured extraction task without requiring task-specific architectural components.
- Evidence anchors:
  - [abstract] "GENIE processes entire paragraphs in a single pass, extracting entities, assertion statuses, locations, modifiers, values, and purposes with high accuracy"
  - [section] Page 7: "It performs NER and generates each term's text (with automatic standardization for abbreviations), semantic type, assertion status, locations, modifiers, values, units, and purposes as JSON format in a single pass"
  - [corpus] Paper "Leveraging LLMs for Structured Data Extraction" (FMR=0.55) similarly finds single-model approaches effective for clinical extraction, suggesting this is a convergent pattern
- Break condition: If output JSON schema changes frequently or requires attributes not seen during training, the generative approach may hallucinate or omit fields.

### Mechanism 2
- Claim: GPT-assisted annotation combined with ontology-based term matching produces viable training labels without extensive manual labeling.
- Mechanism: Forward maximum matching (trie search) against BIOS ontology identifies candidate terms; GPT-4o annotates attributes (locations, modifiers, values, purposes) via carefully-engineered prompts; a separate fine-tuned Llama-2-7b model handles assertion labels using i2b2/VA data. These are aggregated with positional matching.
- Core assumption: GPT-generated labels are sufficiently accurate for training purposes, and errors are random rather than systematically biased.
- Evidence anchors:
  - [abstract] "Using a robust data preparation pipeline and fine-tuned small scale LLMs, GENIE achieves competitive performance"
  - [section] Page 11: "the annotation of these attributes are performed by GPT... Through iterative prompt engineering, we find the following prompt effective and stable"
  - [corpus] Corpus lacks direct evidence on GPT annotation quality for clinical data; this is a gap requiring validation
- Break condition: Systematic GPT annotation errors (e.g., consistently mismatched attributes) will propagate into model behavior; post-hoc verification is limited.

### Mechanism 3
- Claim: Training on raw EHR with expanded abbreviation targets enables the model to learn context-dependent word sense disambiguation.
- Mechanism: Input retains original abbreviations (e.g., "RA", "LLL"); target output contains context-appropriate expansions (e.g., "Left Lower Lobe"). The model learns to disambiguate from surrounding context during fine-tuning.
- Core assumption: The training distribution covers sufficient examples of each abbreviation sense for the model to learn the mapping.
- Evidence anchors:
  - [abstract] "handles abbreviation restoration and word sense disambiguation"
  - [section] Page 17: "the model successfully extracts 'hypoxemia'... and assigns 'Left Lower Lobe' as the location, inferring it from the abbreviation 'LLL'"
  - [section] Page 10: "With raw EHR as the input and expanded abbreviations in the annotated output, GENIE will learn WSD automatically"
  - [corpus] Related papers (SimSUM, ClinStructor) do not explicitly address abbreviation disambiguation; this mechanism lacks external validation
- Break condition: Rare abbreviation senses unseen during training will likely be incorrectly resolved; the paper acknowledges some incorrect restorations (Page 18).

## Foundational Learning

- Concept: **JSON-structured generation for NLP tasks**
  - Why needed here: GENIE outputs structured data directly; understanding constrained generation and JSON formatting as a target is essential.
  - Quick check question: Can you explain why generative JSON output differs from token classification, and what post-processing it eliminates?

- Concept: **Transfer learning and fine-tuning paradigms**
  - Why needed here: The model is Llama-3.1-8B-Instruct fine-tuned on 180K samples; understanding what knowledge transfers vs. requires training data is critical.
  - Quick check question: What types of clinical knowledge would you expect a pre-trained LLM to already possess vs. require fine-tuning to learn?

- Concept: **EHR structuring task decomposition (NER, assertion, relation extraction)**
  - Why needed here: The paper integrates multiple classical NLP tasks; understanding each component's output format and evaluation metric is needed to interpret results.
  - Quick check question: For a sentence "Patient denies chest pain," what would NER, assertion, and relation extraction each output?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Line break restoration + abbreviation expansion (GPT-assisted, training-time only)
  - Term recognition: Forward maximum matching against BIOS v3 ontology (36M terms)
  - Attribute annotators: GPT-4o (locations, modifiers, values, purposes) + fine-tuned Llama-2-7b (assertion status)
  - Data integration: Position-based matching of terms to attributes; JSON formatting
  - GENIE model: Fine-tuned Llama-3.1-8B-Instruct (3 epochs, 180K samples, lr=2e-5)
  - Inference: Raw EHR text → GENIE → JSON output (single pass)

- Critical path: Data annotation quality → training sample integrity → model fine-tuning → inference fidelity. Errors in GPT annotation or integration mismatches directly degrade model output.

- Design tradeoffs:
  - Single-model simplicity vs. potential task-specific accuracy loss (assertion accuracy dropped from 97% in dedicated model to 91.2% in GENIE)
  - Consumer hardware accessibility (RTX 3090, 24GB VRAM) vs. model scale constraints (8B parameters)
  - Comprehensive attribute extraction vs. evaluation difficulty (purpose attribute not numerically evaluated)

- Failure signatures:
  - Attribute mismatches during integration: dropped training samples (Page 12)
  - Over-prediction of "present" assertion status: observed bias (Page 15)
  - Non-standardized value/unit formats in output: "mmHg" vs. "millimeters of mercury" (Page 18)
  - Incorrect acronym restorations for rare senses (Page 18)

- First 3 experiments:
  1. **Inference test on sample MIMIC note**: Run GENIE on a held-out discharge summary paragraph; verify JSON parses correctly and compare extracted terms against manual review for precision/recall.
  2. **Ablation by attribute**: Mask individual attribute fields in training; re-run to quantify each attribute's contribution to overall F1 and identify interdependencies.
  3. **Cross-institution generalization**: Test on non-MIMIC clinical notes (if available); measure F1 drop to assess domain transfer and identify systematic failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a rule-based or constrained decoding intervention effectively mitigate the model's observed bias toward predicting "present" assertion status, particularly for conditional or historical terms?
- Basis in paper: [explicit] The authors note in the Results and Discussion that "the GENIE model exhibits a bias toward predicting 'present'," hypothesizing it may stem from "improper post-processing of mismatched status results or existing error patterns in the original i2b2 database."
- Why unresolved: While the bias is identified, the specific correction mechanism—whether it requires data re-balancing, post-processing rules, or architecture changes—is not implemented or validated in the current study.
- What evidence would resolve it: Ablation studies showing a reduction in false positive "present" predictions after applying targeted data augmentation or specific post-processing constraints.

### Open Question 2
- Question: To what extent does incorporating a passage-specific acronym table during data preparation improve the accuracy of word sense disambiguation (WSD) and reduce acronym restoration errors?
- Basis in paper: [explicit] In the Limitations section, the authors propose that "A passage-specific acronym table generated by GPT will be constructed to refine term extraction results," suggesting this as a solution to current restoration errors.
- Why unresolved: The current model learns WSD implicitly from context, but still suffers from "ambiguity in acronyms" leading to "phrase extraction errors." The proposed table-based refinement is a future plan, not a tested feature.
- What evidence would resolve it: Comparative evaluation of the model's performance on acronym-heavy sections with and without the integration of the dynamic acronym table.

### Open Question 3
- Question: Does cross-validation using diverse offline LLMs during the annotation pipeline significantly reduce the noise in training data for attributes like location and purpose?
- Basis in paper: [explicit] The authors state that for attributes generated by GPT, "it is impractical to verify the correctness of each training data entry," and propose that "cross-validation using other offline LLMs will be explored for verifying term attributes."
- Why unresolved: The current training data relies largely on single-source GPT generation, which admits errors (e.g., dislocation of attributes). The utility of a multi-model consensus verification step remains hypothetical.
- What evidence would resolve it: Comparison of model fine-tuned on single-source annotations vs. model fine-tuned on consensus-verified annotations, specifically measuring the reduction in attribute dislocation errors.

### Open Question 4
- Question: Can a standardization post-processor or unified constraint effectively resolve the inconsistency in extracted values and units (e.g., formatting variations like "mmHg" vs "millimeters of mercury")?
- Basis in paper: [explicit] The authors acknowledge that "extracted values and units are not consistently unified into a standard format," listing variations in scientific notation and acronyms as a limitation that undermines downstream use.
- Why unresolved: The model currently generates these formats based on the variability present in the GPT-augmented training data, and no specific normalization layer has been added to the pipeline yet.
- What evidence would resolve it: Evaluation of the model's raw JSON output against a standardized value/unit ontology to see if consistency scores improve after implementing a deterministic normalization layer.

## Limitations
- GPT annotation quality uncertainty: No quantitative validation of GPT-generated training labels
- Purpose attribute extraction not evaluated numerically
- Rare abbreviation senses may be incorrectly resolved
- Value/unit standardization inconsistencies in output

## Confidence

**High Confidence**: The core claim that GENIE achieves competitive accuracy on standard EHR structuring metrics (F1=0.837 for phrase extraction, assertion accuracy=0.912) is well-supported by human-labeled test data evaluation.

**Medium Confidence**: The claim that GPT-assisted annotation combined with ontology matching produces viable training labels without extensive manual labeling. While the mechanism is described and the model performs well, direct validation of annotation quality is absent from the paper.

**Low Confidence**: The claim that training on raw EHR with expanded abbreviation targets enables robust context-dependent word sense disambiguation. The paper provides examples but lacks systematic evaluation of abbreviation resolution accuracy or comparison to dedicated WSD approaches.

## Next Checks

1. **Quantitative GPT annotation validation**: Create a small manually-labeled gold standard (100-200 phrases) and compare GPT-4o's attribute annotations against it. Calculate precision/recall for each attribute type to establish the baseline error rate entering the training pipeline.

2. **Cross-institution generalization test**: Evaluate GENIE on clinical notes from a different institution or specialty than MIMIC-III. Measure performance degradation and analyze failure modes to assess domain transfer limitations and identify systematic weaknesses in the model's clinical language understanding.

3. **Ablation study on attribute interdependencies**: Systematically remove individual attribute fields from the training data (e.g., train without locations, without values/units) and measure the impact on overall F1 and on other attribute predictions. This would reveal whether certain attributes are critical for context or whether the model can learn them independently.