---
ver: rpa2
title: Multimodal Arabic Captioning with Interpretable Visual Concept Integration
arxiv_id: '2510.03295'
source_url: https://arxiv.org/abs/2510.03295
tags:
- arabic
- image
- captioning
- visual
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLCAP is an Arabic image captioning framework that integrates CLIP-based
  visual label retrieval with multimodal text generation. Unlike prior work, it grounds
  captioning in interpretable Arabic visual concepts extracted using three multilingual
  encoders (mCLIP, AraCLIP, Jina V4).
---

# Multimodal Arabic Captioning with Interpretable Visual Concept Integration

## Quick Facts
- **arXiv ID**: 2510.03295
- **Source URL**: https://arxiv.org/abs/2510.03295
- **Reference count**: 9
- **Primary result**: Achieved best BLEU-1 (5.34%) and cosine similarity (60.01%) on ImageEval 2025 Arabic image captioning task

## Executive Summary
This paper presents VLCAP, a novel Arabic image captioning framework that integrates interpretable visual concept grounding with multimodal text generation. Unlike prior work, VLCAP builds a hybrid vocabulary from training captions and translated general domain labels, then uses top-k retrieved Arabic visual labels to construct prompts for vision-language models. The approach leverages three multilingual CLIP-based encoders (mCLIP, AraCLIP, Jina V4) and two VLMs (Qwen-VL, Gemini Pro Vision), yielding six encoder-decoder configurations. Results show strong performance across multiple evaluation metrics, with mCLIP + Gemini Pro Vision achieving the best BLEU-1 and cosine similarity, while AraCLIP + Qwen-VL obtained the highest LLM-judge score. The framework demonstrates how culturally aware captioning can be achieved through interpretable visual concept integration.

## Method Summary
VLCAP employs a two-stage pipeline for Arabic image captioning. First, it builds a hybrid Arabic visual vocabulary by extracting frequent content words from training captions (after removing Arabic stopwords, numbers, and short tokens) and merging them with 21K Visual Genome labels translated to Arabic. For each test image, the system retrieves top-k Arabic visual labels using CLIP-based encoders through cosine similarity. Second, it constructs an Arabic prompt containing the retrieved labels and passes it with the image to a vision-language model (Qwen-VL or Gemini Pro Vision) for caption generation. The framework explores six encoder-decoder configurations, comparing different combinations of multilingual encoders and VLMs to identify optimal performance.

## Key Results
- mCLIP + Gemini Pro Vision achieved best BLEU-1 (5.34%) and cosine similarity (60.01%)
- AraCLIP + Qwen-VL obtained highest LLM-judge score (36.33%)
- VLCAP ranked first in cosine similarity and second in LLM-as-a-judge among participating teams
- Demonstrated strong performance in semantic adequacy and human preference metrics

## Why This Works (Mechanism)
VLCAP succeeds by grounding image captioning in interpretable Arabic visual concepts rather than relying solely on end-to-end generation. The hybrid vocabulary approach combines task-specific training data with general domain knowledge, providing richer semantic context. By using multiple multilingual CLIP variants, the system can better capture culturally relevant visual concepts specific to Arabic contexts. The prompt engineering strategy of incorporating retrieved labels directly into the generation prompt guides the VLM to produce more accurate and contextually appropriate captions. This interpretable approach allows for better control over the generation process and provides transparency in how visual concepts are translated into textual descriptions.

## Foundational Learning
- **Arabic text normalization**: Required to standardize text for consistent vocabulary extraction and cosine similarity computation. Quick check: Verify normalization removes diacritics, normalizes whitespace, and handles Arabic-specific characters consistently.
- **Multilingual CLIP variants**: Different multilingual encoders (mCLIP, AraCLIP, Jina V4) capture visual concepts with varying effectiveness across languages. Quick check: Compare embedding spaces and similarity distributions across encoders using a validation set.
- **Vision-Language Model prompting**: The quality of generated captions heavily depends on prompt formulation and label integration strategy. Quick check: A/B test different prompt templates with fixed encoder-VLM pairs to measure impact on BLEU-1 scores.
- **Hybrid vocabulary construction**: Combining task-specific and general domain knowledge improves coverage of visual concepts. Quick check: Measure vocabulary overlap between training captions and Visual Genome labels to assess complementarity.
- **Cosine similarity for retrieval**: Used to rank visual labels based on their relevance to image embeddings. Quick check: Plot precision-recall curves for top-k label retrieval using different encoders.
- **Evaluation metrics for Arabic**: BLEU-1 measures n-gram overlap while cosine similarity captures semantic similarity; LLM-judge provides human-like assessment. Quick check: Correlate metric scores with manual evaluation to validate their effectiveness.

## Architecture Onboarding

**Component Map**: Image → CLIP Encoder → Top-k Arabic Labels → Prompt Construction → VLM → Arabic Caption

**Critical Path**: The core pipeline consists of image embedding generation → label retrieval → prompt construction → caption generation. The bottleneck is typically the CLIP-based label retrieval step, as embedding computation and similarity search must be efficient for batch processing.

**Design Tradeoffs**: 
- Using multiple encoders increases computational cost but improves robustness
- Hybrid vocabulary construction adds translation overhead but provides richer semantic coverage
- Top-k label retrieval (25-30) balances informativeness with prompt conciseness
- API-based VLMs eliminate training complexity but introduce dependency on external services

**Failure Signatures**:
- Poor BLEU-1 scores indicate issues with label relevance or prompt formulation
- Low cosine similarity suggests embedding mismatch between image and text spaces
- High variance in LLM-judge scores may indicate inconsistent prompt quality
- Systematically irrelevant labels point to translation quality issues or embedding space misalignment

**First Experiments**:
1. Baseline: Generate captions using VLMs without retrieved labels to measure baseline performance
2. Encoder comparison: Test all three CLIP variants with fixed VLM to identify best retriever
3. Label ablation: Vary k (5, 10, 15, 20, 25, 30) to find optimal number of retrieved labels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the experimental design and results.

## Limitations
- Key implementation details remain underspecified, including exact Arabic prompt template and translation pipeline for 21K labels
- The experimental design conflates model choice and prompt strategy effects, making it difficult to isolate individual contributions
- No external validation on other Arabic image datasets to demonstrate generalizability
- The choice of 25-30 retrieved labels appears heuristic rather than empirically validated

## Confidence

| Claim | Label |
|-------|-------|
| Experimental results and metric comparisons | High |
| Method description and overall pipeline validity | Medium |
| Full reproducibility and external generalizability | Low |

## Next Checks
1. Reconstruct the Arabic prompt template and test sensitivity by varying label inclusion strategies (top-k, random k, filtered k) to measure impact on generation quality.
2. Audit the quality of the 21K translated Visual Genome labels by sampling and manual evaluation, checking for semantic drift or mistranslation that could affect retrieval performance.
3. Reproduce the cosine similarity metric on a held-out validation set, ensuring Arabic text normalization and TF-IDF steps match the reported process to verify metric computation.