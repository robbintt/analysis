---
ver: rpa2
title: 'RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual
  Knowledge Graphs in Medicine'
arxiv_id: '2410.13987'
source_url: https://arxiv.org/abs/2410.13987
tags:
- gene
- disease
- textual
- medical
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RiTeK is a new dataset for evaluating complex reasoning over medical
  textual knowledge graphs (TKGs). It addresses limitations of existing datasets by
  incorporating richer topological structures, diverse relational templates, and complex
  textual descriptions validated by medical experts.
---

# RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs in Medicine

## Quick Facts
- arXiv ID: 2410.13987
- Source URL: https://arxiv.org/abs/2410.13987
- Reference count: 40
- Primary result: RiTeK is a medical TKG benchmark that exposes current retrieval systems' limitations in handling integrated textual-relational queries across six complex topological structures.

## Executive Summary
RiTeK addresses a critical gap in medical knowledge graph evaluation by introducing a dataset that requires models to perform complex reasoning over textual knowledge graphs (TKGs). Unlike existing benchmarks focused on either pure relational structure or textual retrieval, RiTeK synthesizes queries that integrate both modalities, requiring models to traverse graph structures while applying textual constraints on entity attributes. The dataset covers 68 relational templates in RiTeK-PharmKG and 58 in RiTeK-ADint across six topological structures, from simple 1-hop to complex 3-hop with constraints. Evaluation of 11 retrieval models reveals that current methods struggle with the integration of textual and relational information, particularly when queries embed attribute constraints within medical terminology.

## Method Summary
The RiTeK construction pipeline begins with two source knowledge graphs (PharmKG and ADint), enriches entities with textual descriptions from UMLS, Ensembl, and Mondo Disease Ontology, then synthesizes realistic user queries through a five-step process: relational template construction across six topological structures, textual property extraction using GPT-4, query combination that merges relational and textual information, multi-LLM answer filtering for validation, and expert evaluation. The resulting benchmark evaluates retrieval models on their ability to traverse graph structures while applying textual constraints, using Exact Match and ROUGE-1 metrics. The evaluation includes both supervised and zero-shot methods, with particular focus on how different architectures handle the textual-relational integration challenge.

## Key Results
- Current retrieval methods struggle with textual-relational integration, with G-retriever showing limited ability to handle complex constraints embedded in query text.
- GNN-RAG and GCR achieve highest supervised performance (F1 ~50-57) but rely on shortest-path assumptions that miss indirect reasoning patterns.
- GPT-4 zero-shot baseline achieves only F1 11.03, demonstrating the difficulty of the task and the importance of specialized retrieval methods.
- Pure LLM methods tend to hallucinate answers when graph traversal fails, highlighting the need for structured retrieval approaches.

## Why This Works (Mechanism)

### Mechanism 1: Textual-Relational Integration Creates Novel Retrieval Challenges
- Claim: Combining textual entity descriptions with relational structure in queries exposes limitations in current retrieval systems that process these modalities separately.
- Mechanism: The RiTeK construction pipeline extracts textual properties from candidate answers (e.g., characteristics of "Hypersensitivity") and merges them with relational templates (e.g., "Anti-Bacterial Agents causes pathologic function"), forcing models to jointly reason over both modalities rather than treating them independently.
- Core assumption: Real-world medical queries require simultaneous understanding of what entities are (textual semantics) and how they relate (graph structure).
- Evidence anchors:
  - [abstract] "synthesize realistic user queries integrating diverse topological structures, relational information, and complex textual descriptions"
  - [section 4.2.2] "The core idea is to intertwine relational information and textual properties within the queries"
  - [corpus] Limited corpus evidence; related work on TKGQA exists but RiTeK is the first medical-specific TKG benchmark with this integration level
- Break condition: If models can process textual and relational information through unified representations (e.g., GNN-RAG with verbalized paths), this mechanism's explanatory power diminishes.

### Mechanism 2: Topological Complexity Increases Reasoning Depth Requirements
- Claim: Expanding from 1-2 hop reasoning to 6 topological structures (including 3-hop, constrained multi-hop, and converging paths) exposes the brittleness of methods that assume simple linear chains.
- Mechanism: The six reasoning topologies (1-hop, 2-hop, 3-hop, and constrained variants, plus two-to-one convergence) require models to track multiple reasoning states, apply intermediate constraints, and handle branching/fusion in the graph—all absent in simpler benchmarks.
- Core assumption: Medical domain reasoning frequently involves multi-step dependencies where intermediate entities must satisfy both structural and semantic constraints.
- Evidence anchors:
  - [abstract] "incorporating richer topological structures, diverse relational templates, and complex textual descriptions"
  - [section 5.2] Results show ToT and GoT underperform (F1 13.42 vs. KAR 27.50 zero-shot), indicating that structured prompting alone cannot compensate for lack of graph-based reasoning
  - [corpus] Related TKGQA work (Plan of Knowledge, MemoTime) focuses on temporal reasoning but does not systematically vary topological complexity
- Break condition: If a model can internally construct and traverse graph structures without explicit retrieval, topological complexity may not correlate with retrieval difficulty.

### Mechanism 3: Domain-Specific Constraints Amplify Precision Requirements
- Claim: Medical attribute constraints (e.g., "disease characterized by overexpression in alb") filter answers more aggressively than general-domain constraints, making partial retrieval insufficient.
- Mechanism: In RiTeK, queries embed attribute constraints within textual descriptions (extracted via GPT-4 from entity documents). A retrieved path is correct only if the final entity's textual properties satisfy the constraint—requiring semantic alignment between query and retrieved content, not just structural matching.
- Core assumption: Medical terminology has high synonymy and specificity, so surface-level keyword matching fails; semantic understanding of constraint text is necessary.
- Evidence anchors:
  - [section 5.2] "G-retriever's ability to handle complex relational constraints, particularly when the answer's attributes are embedded in the query, is limited"
  - [section 5.3.2] Case study shows MCTS incorrectly retrieves "Juvenile Huntington Disease" because it fails to match the subtle constraint about "biomarkers linked to npy"
  - [corpus] DoctorRAG and RAR² papers focus on medical RAG but do not systematically evaluate constraint-based retrieval over structured graphs
- Break condition: If retrieval models can perform fine-grained semantic matching over entity attributes (e.g., via dense passage retrieval on textual properties), constraint-based filtering may become tractable without explicit graph reasoning.

## Foundational Learning

- **Textual Knowledge Graphs (TKGs)**
  - Why needed here: RiTeK's central data structure; each node has both relational edges and textual descriptions.
  - Quick check question: Given a triple `(Drug, treats, Disease)` where Disease has textual description "a neurodegenerative condition affecting motor control," how would you represent this in a TKG?

- **Multi-hop Reasoning Topologies**
  - Why needed here: The benchmark includes 6 topologies; understanding these is prerequisite to analyzing model failures.
  - Quick check question: For a 2-hop query with constraint (e.g., Gene → activates → Gene → targets → Disease where Disease type = inflammatory), what intermediate states must a retrieval system track?

- **Retrieval-Augmented Generation (RAG) over Graphs**
  - Why needed here: The evaluation compares graph-based retrievers (G-retriever, GNN-RAG, KAR) against pure LLM baselines.
  - Quick check question: How does retrieving a subgraph (e.g., via PCST in G-retriever) differ from retrieving document chunks in standard RAG? What additional reasoning does it enable?

## Architecture Onboarding

- **Component map:**
  Source KGs (PharmKG, ADint) -> TKG Construction (enrich entities with UMLS/Ensembl/Mondo text) -> Query Synthesis Pipeline: 1. Relational Template Construction (6 topologies) 2. Textual Property Extraction (GPT-4 from entity docs) 3. Query Combination (merge relational + textual) 4. Answer Filtering (LLM-based validation) 5. Expert Evaluation -> Benchmark Datasets (RiTeK-PharmKG, RiTeK-ADint) -> Retrieval Models (11 evaluated: Random Walk, MCTS, ToT, GoT, TOG, G-retriever, KAR, GNN-RAG, GCR, etc.) -> Evaluation (Exact Match, ROUGE-1)

- **Critical path:**
  1. Query synthesis is the bottleneck—incorrect merging of textual properties with relational templates creates noisy training signals.
  2. Retrieval model performance depends on successfully traversing multi-hop paths while applying textual constraints at the final node.
  3. GNN-RAG and GCR achieve highest supervised performance (F1 ~50-57), but require pre-constructed indices and shortest-path assumptions.

- **Design tradeoffs:**
  - **Topological coverage vs. evaluation cost:** 6 structures provide comprehensive coverage but require more model variants to analyze per-structure performance.
  - **Synthetic queries vs. realism:** GPT-4-generated queries enable scale but may not capture the full diversity of clinician phrasing; expert evaluation mitigates but does not eliminate this.
  - **Retrieval-based vs. pure-LLM approaches:** Retrieval methods outperform (GNN-RAG F1 50.73 vs. GPT-4 F1 11.03), but add latency and index maintenance overhead.

- **Failure signatures:**
  - **Textual-relational misalignment:** Model retrieves structurally correct path but entity attributes do not match query constraints (e.g., retrieving "Alzheimer's Disease" instead of "Schizophrenia" in case study).
  - **Hallucination on rare relations:** Pure LLM methods invent answers (e.g., "Autism Spectrum Disorder") when graph traversal fails.
  - **Shortest-path bias:** GNN-RAG relies on shortest paths, missing correct answers reachable via longer or indirect routes.

- **First 3 experiments:**
  1. **Baseline establishment:** Run GPT-4 (zero-shot) and G-retriever (few-shot) on all 6 topological structures separately; report per-structure F1 to identify which topologies cause the largest performance drop.
  2. **Constraint ablation:** Remove textual constraints from queries (keep only relational structure); compare retrieval performance to quantify the difficulty introduced by attribute filtering.
  3. **Retriever comparison:** On the 3-hop and 2-hop-with-constraint structures, compare path-based retrievers (G-retriever, GNN-RAG) against traversal-based methods (TOG, MCTS); analyze whether explicit graph traversal compensates for lack of neural retrieval.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the inclusion of multiple topic entities and image modalities impact the performance of retrieval systems on medical Textual Knowledge Graphs?
  - Basis in paper: [explicit] The authors state in the Limitations section that "RiTeK is currently limited to queries that involve only a single topic entity" and suggest future work should "incorporate additional modalities, such as images."
  - Why unresolved: The current dataset construction and experiments strictly utilize single-entity, text-only inputs.
  - What evidence would resolve it: Evaluation of advanced retrieval architectures on an extended version of RiTeK containing multi-entity queries and imaging data.

- **Open Question 2**
  - Question: What is the optimal strategy for determining the value of top-n documents and their ordering in Knowledge-Aware Retrieval (KAR) for complex medical reasoning?
  - Basis in paper: [explicit] The authors note that KAR relies on top-n relevant documents, but "determining an appropriate value for n and the optimal order in which to select documents is non-trivial."
  - Why unresolved: The paper identifies this hyperparameter sensitivity as a gap but does not propose a method to automate or optimize this selection.
  - What evidence would resolve it: An ablation study defining an adaptive algorithm for n and ordering that consistently outperforms static baselines.

- **Open Question 3**
  - Question: Can graph-based retrieval methods be modified to effectively capture reasoning information in complex or indirect graph structures beyond shortest paths?
  - Basis in paper: [explicit] The results discussion notes that GNN-RAG relies primarily on shortest paths, which causes it to "overlook critical reasoning information embedded in more complex or indirect graph structures."
  - Why unresolved: Current graph neural approaches prioritize efficiency, potentially missing medically relevant but indirect dependencies.
  - What evidence would resolve it: A novel GNN architecture that successfully aggregates longer reasoning chains to improve Exact Match scores on the RiTeK benchmark.

## Limitations

- **Dataset construction bias:** The use of GPT-4 for query synthesis and validation may introduce systematic biases in how medical queries are formulated and evaluated.
- **Limited expert validation:** Only 10 queries per source graph were evaluated by medical experts, which may not capture the full diversity of medical reasoning patterns.
- **Indexing strategy dependency:** Evaluation was conducted on pre-constructed indices without systematic exploration of how different indexing approaches affect retrieval performance.

## Confidence

- **High confidence:** The core contribution of creating a medical TKG benchmark with integrated textual and relational information is well-supported by the construction methodology and baseline results.
- **Medium confidence:** The claim that current retrieval methods struggle with textual-relational integration is supported but could be influenced by the specific models and prompts used in evaluation.
- **Medium confidence:** The topological complexity findings are robust, but the relative difficulty of different structures may vary depending on indexing strategies not explored in the current evaluation.

## Next Checks

1. **Expert validation expansion:** Conduct systematic evaluation of a larger sample (e.g., 50+ queries) by medical experts to verify the realism and difficulty of synthesized queries across all six topological structures.
2. **Cross-dataset generalization:** Test the best-performing retrieval models (GNN-RAG, GCR) on held-out subsets of the TKGs to assess whether performance degradation occurs on unseen relation types or topologies.
3. **Indexing strategy ablation:** Compare retrieval performance using different indexing approaches (e.g., full subgraph vs. entity-level indexing) to determine whether current performance bottlenecks stem from the retrieval algorithm or the underlying data representation.