---
ver: rpa2
title: 'When LLMs Struggle: Reference-less Translation Evaluation for Low-resource
  Languages'
arxiv_id: '2501.04473'
source_url: https://arxiv.org/abs/2501.04473
tags:
- language
- translation
- llms
- pairs
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating machine translation
  quality for low-resource language pairs without reference translations, known as
  quality estimation (QE). The authors comprehensively evaluate large language models
  (LLMs) in zero-shot, few-shot/in-context learning, and instruction fine-tuning scenarios,
  proposing a novel annotation guidelines-based prompt (AG-prompt) to improve performance.
---

# When LLMs Struggle: Reference-less Translation Evaluation for Low-resource Languages

## Quick Facts
- **arXiv ID**: 2501.04473
- **Source URL**: https://arxiv.org/abs/2501.04473
- **Reference count**: 14
- **Primary result**: Prompt-based LLM approaches are outperformed by encoder-based fine-tuned QE models, with OpenChat achieving highest correlation scores (0.45-0.62 Spearman) across low-resource language pairs.

## Executive Summary
This paper comprehensively evaluates large language models (LLMs) for reference-less quality estimation (QE) of machine translation in low-resource language pairs. The authors propose a novel annotation guidelines-based prompt (AG-prompt) and compare zero-shot, few-shot, and instruction fine-tuned approaches against encoder-based baselines. Results demonstrate that while LLMs show promise, they underperform encoder-based models due to tokenization issues and errors with named entities and transliteration. The study highlights the need for improved cross-lingual capabilities in LLMs for evaluation tasks.

## Method Summary
The paper evaluates three settings: (1) Zero-shot prompting with GEMBA, TE, and proposed AG prompts, (2) In-context learning with 3/5/7 examples, and (3) Instruction fine-tuning using LoRA adapters. Models tested include Gemma-7B, Llama-2-7B/13B, and OpenChat-3.5-7B, compared against TransQuest and CometKiwi encoder baselines. Training uses Unified Multilingual Training (UMT) combining 8 language pairs versus Independent Language-Pair Training (ILT). Performance is measured via Spearman correlation between predicted and human DA scores on WMT22/23 QE datasets.

## Key Results
- AG-prompt improves zero-shot QE performance across most language pairs compared to baseline prompts
- UMT training outperforms ILT for most low-resource language pairs, suggesting cross-lingual transfer benefits
- OpenChat-3.5-7B achieves highest Spearman correlation scores (0.45-0.62) among LLM approaches
- Encoder-based models (TransQuest/CometKiwi) significantly outperform all LLM approaches (0.60-0.86 Spearman)
- Tokenization issues disproportionately affect morphologically rich low-resource languages like Marathi, Tamil, and Telugu

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Annotation guidelines-based prompts improve zero-shot QE by providing structured evaluation criteria
- **Mechanism:** AG-prompt explicitly defines scoring ranges (0-100), evaluation dimensions, and reasoning steps, reducing output format failures and anchoring model behavior to human protocols
- **Core assumption:** LLMs can transfer instruction-following capabilities to novel cross-lingual tasks when evaluation criteria are explicit
- **Evidence anchors:** AG-prompt achieved highest scores in zero-shot setting for most language pairs; reduces output parsing failures
- **Break condition:** Near-zero pre-training exposure to target language makes structured prompting insufficient

### Mechanism 2
- **Claim:** UMT outperforms ILT because cross-lingual transfer compensates for per-language data scarcity
- **Mechanism:** Combined training data exposes adapter to diverse error patterns and scoring distributions, learning generalizable quality signals
- **Core assumption:** Quality estimation signals share cross-lingual regularities that transfer across language families
- **Evidence anchors:** UMT performs better for most low-resource pairs; cross-lingual transfer benefits documented in CUTE dataset
- **Break condition:** Typologically distant language pairs in UMT may cause negative transfer

### Mechanism 3
- **Claim:** Tokenization discrepancies degrade cross-lingual semantic matching, affecting morphologically rich languages
- **Mechanism:** LLM tokenizers fragment non-Latin scripts into excessive tokens, introducing noise in semantic alignment between source and translation
- **Core assumption:** Semantic matching for QE relies on comparable token-level representations across source and target
- **Evidence anchors:** Token counts for low-resource non-English languages deviate significantly from word counts; affects semantic matching
- **Break condition:** Shared script between source and target (e.g., Estonian-English) mitigates tokenization issues

## Foundational Learning

- **Concept:** Quality Estimation (QE) vs. Reference-based Evaluation
  - **Why needed here:** QE predicts translation quality scores without reference translations, requiring cross-lingual semantic understanding rather than string matching
  - **Quick check question:** Can you explain why QE is modeled as regression rather than classification?

- **Concept:** Direct Assessment (DA) Scoring
  - **Why needed here:** Target variable is human-annotated DA scores (mean of 3+ annotators), introducing subjectivity the model must approximate
  - **Quick check question:** Why might averaging multiple annotator scores reduce noise in training signal?

- **Concept:** Spearman Correlation for Evaluation
  - **Why needed here:** Paper reports Spearman ρ (rank correlation) between predicted and human scores, measuring ordinal agreement rather than absolute error
  - **Quick check question:** Why is rank correlation preferred over mean squared error for evaluating QE models?

## Architecture Onboarding

- **Component map:** Source text + translated text -> AG-prompt formatting -> LLM/encoder backbone -> LoRA adapter (fine-tuned) -> Regex score extraction -> Spearman correlation evaluation
- **Critical path:** 1) Tokenize source+translation jointly, 2) Format with AG-prompt including scoring rubric, 3) Generate with temperature=0, 4) Extract numeric score via regex, 5) Compute Spearman correlation
- **Design tradeoffs:** LLMs offer flexible prompting but underperform encoders; UMT improves generalization but requires managing 8 language pairs; 4-bit quantization reduces memory with slight accuracy trade-off
- **Failure signatures:** No score generated (TE prompt drops >10% rows), negative correlation (inverse scoring), high variance (temperature >0), Latin-script target advantage
- **First 3 experiments:** 1) Zero-shot AG-prompt on Et-En vs En-Ta to confirm ρ gap, 2) Tokenization audit: compute token/word ratio for target language pair, 3) LoRA adapter pilot: train on single pair, compare zero-shot vs fine-tuned Spearman

## Open Questions the Paper Calls Out
- Can replacing text-generation with regression head-based adapters improve reliability and accuracy of LLM-based QE?
- To what extent does adapting tokenization for low-resource languages mitigate semantic matching deficiencies?
- Does scaling model size beyond 13B parameters overcome tokenization and syntactic limitations identified in smaller LLMs?

## Limitations
- Study restricted to models smaller than 14B parameters due to resource constraints
- Tokenization impact quantified descriptively rather than through controlled experiments
- No statistical significance testing for correlation differences between training paradigms
- Encoder baseline configurations not fully specified for reproduction

## Confidence
- **Prompt-based QE improvements**: Medium confidence - consistent gains but limited absolute performance
- **UMT vs ILT comparison**: Medium confidence - demonstrates trends without statistical validation
- **Tokenization impact on low-resource languages**: Low confidence - identified as problem but not quantitatively measured
- **LLM vs encoder performance gap**: High confidence - clear empirical demonstration of encoder superiority
- **Named entity handling issues**: Medium confidence - observed but not systematically analyzed

## Next Checks
1. Perform paired T-tests comparing AG-prompt vs baseline prompts and UMT vs ILT configurations with p<0.05 significance threshold
2. Measure token-to-word ratio discrepancies and compute correlation degradation as function of tokenization inflation rate
3. Train separate adapters for language families (Dravidian vs Indo-Aryan vs Estonian) to test cross-lingual signal generalization