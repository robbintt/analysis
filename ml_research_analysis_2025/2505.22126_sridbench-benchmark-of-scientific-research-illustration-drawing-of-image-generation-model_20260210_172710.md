---
ver: rpa2
title: 'SridBench: Benchmark of Scientific Research Illustration Drawing of Image
  Generation Model'
arxiv_id: '2505.22126'
source_url: https://arxiv.org/abs/2505.22126
tags:
- generation
- scientific
- image
- science
- gpt-4o-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SridBench, the first benchmark for evaluating
  image generation models on scientific research illustration tasks. It collects 1,120
  high-quality instances from peer-reviewed papers across 13 disciplines in natural
  and computer sciences.
---

# SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model

## Quick Facts
- arXiv ID: 2505.22126
- Source URL: https://arxiv.org/abs/2505.22126
- Reference count: 33
- Introduces first benchmark for evaluating image generation models on scientific research illustration tasks

## Executive Summary
This paper introduces SridBench, the first benchmark for evaluating image generation models on scientific research illustration tasks. It collects 1,120 high-quality instances from peer-reviewed papers across 13 disciplines in natural and computer sciences. Each instance includes an illustration, caption, and related section text. The benchmark evaluates generated images along six dimensions: completeness and accuracy of textual information, diagrammatic structural integrity, diagrammatic logic, cognitive readability, and aesthetic feeling. Experiments show that current state-of-the-art models like GPT-4o-image still fall short of human performance, particularly in text and visual clarity and scientific correctness. The study highlights the need for further advances in reasoning-driven visual generation for scientific contexts.

## Method Summary
The SridBench benchmark was constructed by collecting 1,120 high-quality scientific illustrations from peer-reviewed papers across 13 disciplines in natural and computer sciences. Each instance includes the original illustration, its caption, and related section text from the source paper. The evaluation framework assesses generated images across six dimensions: completeness and accuracy of textual information, diagrammatic structural integrity, diagrammatic logic, cognitive readability, and aesthetic feeling. Human experts and MLLM-based evaluators (GPT-4o) were used to rate the quality of generated illustrations, with scores ranging from 1 (lowest) to 5 (highest).

## Key Results
- GPT-4o-image achieved only "fair" level scores (2-3) across most evaluation dimensions
- Major weaknesses identified in text completeness/accuracy and scientific correctness
- Models frequently produce hallucinations, missing elements, and text omissions
- Human experts significantly outperform current models in all six evaluation dimensions

## Why This Works (Mechanism)
The benchmark works by providing a standardized evaluation framework for scientific illustration generation tasks. By using real scientific illustrations from peer-reviewed papers as ground truth, it captures the complexity and domain-specific requirements of scientific communication. The multi-dimensional evaluation approach ensures comprehensive assessment beyond simple visual similarity, focusing on functional aspects like textual accuracy and logical coherence that are critical for scientific communication.

## Foundational Learning

1. **Scientific illustration evaluation criteria**
   - Why needed: Scientific illustrations require precise textual information and logical coherence beyond general visual quality
   - Quick check: Verify all six dimensions are explicitly defined and measurable

2. **Multi-modal learning for scientific content**
   - Why needed: Models must understand both visual and textual scientific concepts to generate accurate illustrations
   - Quick check: Confirm models receive both image and text inputs during evaluation

3. **Domain-specific knowledge grounding**
   - Why needed: Scientific illustrations often contain specialized symbols, notations, and conventions
   - Quick check: Validate dataset covers diverse scientific disciplines and illustration types

## Architecture Onboarding

**Component Map:** Image Generation Model -> Illustration Output -> Multi-dimensional Evaluator (MLLM + Human) -> Score Vector

**Critical Path:** Prompt Input → Image Generation → Visual-Text Alignment Check → Structural Integrity Assessment → Scientific Accuracy Verification → Quality Score

**Design Tradeoffs:** 
- Generalist models vs. domain-specific fine-tuning
- Visual quality vs. textual accuracy emphasis
- Automated evaluation vs. human expert assessment

**Failure Signatures:**
- Text hallucinations and omissions
- Structural inconsistencies in diagrams
- Scientific inaccuracies in labeled elements
- Poor logical flow between visual components

**First 3 Experiments:**
1. Test baseline models with standardized prompts on subset of 100 illustrations
2. Compare human vs. MLLM evaluation consistency on 50 randomly selected samples
3. Evaluate impact of prompt engineering variations on generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reasoning-driven visual generation capabilities be advanced to close the substantial performance gap between current models (e.g., GPT-4o-image) and human experts in scientific illustration tasks?
- Basis in paper: The conclusion states: "How to improve the generation ability of the image generation model in the task of strong inference should be the focus of the next researchers."
- Why unresolved: Current models achieve only "fair" level scores; GPT-4o-image shows common errors like missing elements, text omissions, and scientific inaccuracies.
- What evidence would resolve it: A model achieving scores of 4+ across all six dimensions with reduced hallucination rates.

### Open Question 2
- Question: What specific architectural or training improvements are needed to address the persistent weaknesses in textual information completeness and accuracy in generated scientific illustrations?
- Basis in paper: The paper identifies "the lack of text and visual information and scientific errors are the main bottlenecks of GPT-4o-image."
- Why unresolved: Text accuracy scores remain below average even for the best model; generated illustrations frequently contain incomplete or incorrect text.
- What evidence would resolve it: Targeted ablation studies showing improved text rendering through specific modifications to model architecture or training procedures.

### Open Question 3
- Question: How can automated evaluation metrics be refined to better align with human expert judgment, particularly regarding completeness and accuracy of textual information?
- Basis in paper: The paper notes GPT-4o as evaluator "is still slightly overrated in terms of completeness and accuracy compared to human expert ratings."
- Why unresolved: Current MLLM-based evaluation shows systematic bias, potentially masking true model limitations.
- What evidence would resolve it: Development of evaluation protocols showing higher correlation with human expert scores across all six dimensions.

## Limitations
- Relatively small scale (1,120 instances) compared to general image generation benchmarks
- Potential sampling bias from the selected disciplines
- Subjective nature of human evaluation across six dimensions
- Focus on illustration drawing rather than full scientific figure generation

## Confidence
- Core methodology: Medium
- Performance gap claims: Medium
- Evaluation framework validity: Medium
- Generalizability to other scientific domains: Low

## Next Checks
1. Conduct cross-validation studies using different prompt templates to assess robustness of model performance rankings
2. Expand evaluation to include additional scientific domains and more diverse illustration types (schematics, graphs, photos)
3. Implement automated metrics that correlate with human judgments to enable scalable benchmarking beyond small-scale human evaluation