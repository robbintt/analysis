---
ver: rpa2
title: Predicting Implicit Arguments in Procedural Video Instructions
arxiv_id: '2505.21068'
source_url: https://arxiv.org/abs/2505.21068
tags:
- step
- semantic
- where
- verb
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Implicit-VidSRL, a dataset designed to benchmark
  multimodal models on inferring implicit arguments in procedural videos. The authors
  focus on understanding elliptical instructions in cooking videos by identifying
  predicate-argument structures ({verb, what, where/with}) and predicting implicit
  entities, such as ingredients, that are contextually inferred.
---

# Predicting Implicit Arguments in Procedural Video Instructions

## Quick Facts
- **arXiv ID:** 2505.21068
- **Source URL:** https://arxiv.org/abs/2505.21068
- **Reference count:** 40
- **Primary result:** iSRL-Qwen2-VL achieves 17% relative F1 improvement for implicit what-arguments and 14.7% for where/with-arguments over GPT-4o

## Executive Summary
This paper introduces Implicit-VidSRL, a multimodal benchmark for inferring implicit arguments in procedural cooking videos. The authors tackle the challenge of understanding elliptical instructions where entities like ingredients are omitted but contextually inferable. By annotating YouCook2 and Tasty datasets with semantic role labels and using GPT-4o to generate silver-standard training data, they create a dataset of 2,545 semantic frames across 231 videos. The proposed iSRL-Qwen2-VL model, fine-tuned for semantic role labeling, significantly outperforms GPT-4o on implicit argument prediction tasks, demonstrating that explicit SRL supervision improves contextual reasoning and entity tracking in long procedural sequences.

## Method Summary
The authors create Implicit-VidSRL by annotating cooking videos from YouCook2 and Tasty with semantic frames containing {verb, what, where/with} triples. They use GPT-4o with chain-of-thought prompting to generate silver-standard training data (~18K next-step samples from ~2.5K videos). The iSRL-Qwen2-VL model is built by fine-tuning Qwen2-VL-7B-Instruct via LoRA (rank=8) on this data, optimizing for implicit argument prediction in a cloze task format. The model is evaluated on two tasks: predicting implicit arguments for given verbs, and generating next-step instructions with semantic frames. Video inputs are processed as 8-frame clips at 224px resolution, with ≤320 frames per video.

## Key Results
- iSRL-Qwen2-VL achieves 17% relative F1 improvement for implicit what-arguments over GPT-4o
- 14.7% relative F1 improvement for implicit where/with-arguments
- Model shows more robust performance on later semantic positions in long sequences
- Ablation confirms importance of LoRA fine-tuning for semantic role labels

## Why This Works (Mechanism)
The approach works by explicitly training a multimodal model to recognize and predict implicit arguments through semantic role labeling supervision. By converting elliptical instructions into structured {verb, what, where/with} frames and using chain-of-thought prompting during silver data generation, the model learns to track entities across frames and infer contextually missing information. The combination of visual and textual inputs allows the model to disambiguate between visually similar entities and leverage compositional context for prediction.

## Foundational Learning

**Semantic Role Labeling (SRL):** Understanding predicate-argument structures in language
- Why needed: Core task is identifying {verb, what, where/with} triples and predicting missing entities
- Quick check: Can identify subject, object, and modifier roles in simple sentences

**Multimodal Fusion:** Combining visual and textual representations for joint reasoning
- Why needed: Must integrate video frames with instruction text to infer implicit entities
- Quick check: Model can correctly predict "salt" when seeing a person sprinkling something white

**Chain-of-Thought Prompting:** Breaking down complex reasoning into intermediate steps
- Why needed: GPT-4o needs structured reasoning to generate accurate silver-standard annotations
- Quick check: Model produces step-by-step reasoning before final answer

**LoRA Fine-tuning:** Parameter-efficient adaptation of large models
- Why needed: Efficiently specialize Qwen2-VL for implicit argument prediction task
- Quick check: Fine-tuned model shows improved performance on SRL task

## Architecture Onboarding

**Component Map:** GPT-4o (silver data generation) -> Qwen2-VL-Instruct (base model) -> LoRA fine-tuning -> iSRL-Qwen2-VL (final model)

**Critical Path:** Video frames + instruction text → CLIP-like encoder → LoRA-adapted transformer → {verb, what, where/with} prediction

**Design Tradeoffs:** Simplified {verb, what, where/with} scheme vs. full PropBank-style annotations; silver-standard data vs. expensive gold annotation; LoRA efficiency vs. full fine-tuning

**Failure Signatures:** 
- Text bias: Predicting text entities even when visual context suggests otherwise
- Temporal decay: Performance degradation on later semantic positions
- Tool confusion: Mistaking cooking tools for implicit arguments in where/with role

**First Experiments:**
1. Verify silver-standard generation with provided CoT prompts and 5 in-context examples
2. Test visual-only vs. text+visual performance to diagnose modality bias
3. Evaluate F1-score decay across semantic positions to assess temporal tracking

## Open Questions the Paper Calls Out

**Domain Generalization:** Can the approach extend beyond cooking to assembly, medical, or DIY domains? The current annotation scheme assumes state transformations that may not hold for other procedural data.

**Silver Data Quality:** What is the impact of GPT-4o-generated annotation errors on model performance? The method relies entirely on automated SRL without validation of error propagation.

**Long-sequence Tracking:** How to improve implicit argument prediction for longer procedural sequences where entity tracking becomes challenging? All models show performance degradation as sequence length increases.

**Annotation Completeness:** Would including tools and spatial-temporal context in the annotation scheme improve downstream task performance? The current focus on ingredients excludes salient information that models currently confuse.

## Limitations

- Relies entirely on GPT-4o-generated silver-standard annotations without gold validation
- Performance degrades on later semantic positions in long sequences
- Limited to cooking domain; may not generalize to other procedural tasks
- Simplified annotation scheme may miss important semantic information

## Confidence

**High:** Relative improvements over GPT-4o are well-supported by direct comparisons and ablation studies
**Medium:** Missing details in silver-standard generation and potential confounding factors in data sources
**Low:** Claims about generalization to other domains are speculative without empirical validation

## Next Checks

1. **Replicate silver-standard annotation pipeline** using provided CoT prompts and in-context examples to verify F1-score improvements

2. **Test visual-only vs. text+visual performance** on a subset of the test set to isolate multimodal input impact

3. **Evaluate temporal entity tracking** by measuring F1-score decay across semantic frame positions in long videos