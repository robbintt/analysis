---
ver: rpa2
title: Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement
  Learning
arxiv_id: '2511.17598'
source_url: https://arxiv.org/abs/2511.17598
tags:
- policy
- equation
- optimal
- theorem
- r-lvn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Non-stationary and Varying-discounting
  Markov Decision Process (NVMDP) framework to address limitations of classical stationary
  MDPs in non-stationary environments and finite-horizon tasks. The framework extends
  non-stationary MDPs by allowing time- and transition-dependent discount rates, enabling
  explicit shaping of optimal policies without altering state space, action space,
  or reward structure.
---

# Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.17598
- Source URL: https://arxiv.org/abs/2511.17598
- Reference count: 40
- Key outcome: NVMDP framework extends non-stationary MDPs with time- and transition-dependent discount rates, enabling trajectory shaping without altering reward structure

## Executive Summary
This paper introduces the Non-stationary and Varying-discounting Markov Decision Process (NVMDP) framework to address limitations of classical stationary MDPs in non-stationary environments and finite-horizon tasks. The framework extends non-stationary MDPs by allowing time- and transition-dependent discount rates, enabling explicit shaping of optimal policies without altering state space, action space, or reward structure. The authors establish theoretical foundations including state-value and action-value recursions, matrix representations, optimality conditions, and policy improvement theorems, and adapt dynamic programming, Q-learning, and generalized Q-learning algorithms to NVMDPs with convergence guarantees.

## Method Summary
The NVMDP framework treats policies as collections of time-specific sub-policies and allows discount rates to vary with time and transitions. The framework extends classical MDP theory by modifying the Bellman equations to accommodate time-varying dynamics and discount factors. Key algorithms include NVMDP-Q-learning (Algorithm 3) which maintains time-indexed Q-tables and updates using transition-specific discount rates, and Generalized Q-learning which tracks multiple optimal action candidates per state-time pair. For function approximation, the paper extends the Policy Gradient Theorem and TRPO policy improvement bound to NVMDPs. The empirical validation uses a non-stationary gridworld environment where classic Q-learning fails due to time-varying wind dynamics, while NVMDP-based methods successfully learn optimal trajectories.

## Key Results
- NVMDP-Q-learning successfully learns optimal trajectories in non-stationary gridworld where classical Q-learning fails
- Varying discount rates (γ > 1) effectively shape trajectories by encouraging avoidance of specific states without changing reward structure
- Empirical results validate NVMDPs as theoretically sound and practically effective framework requiring only minor algorithmic modifications compared to classical approaches

## Why This Works (Mechanism)

### Mechanism 1: Time-Indexing for Policy Consistency
The framework treats the global policy π as a sequence (π₀, π₁, ...) where each πₜ is optimized for the specific dynamics pₜ active at that time step. By indexing action-values Qₜ(s,a) by time, the Bellman recursion propagates values backward through time steps, allowing the agent to learn that "Action A is optimal in State S at time 5" while "Action B is optimal in State S at time 10," solving the failure mode of classical Q-learning in non-stationary settings.

### Mechanism 2: Discount-Based Trajectory Shaping
By allowing γₜ₊₁(s,a,s') to vary, the framework controls the weight of future rewards based on the specific transition taken. Setting γ = 1.02 for transitions entering specific cells with negative expected rewards amplified the magnitude of the future negative return, effectively penalizing those paths and encouraging avoidance.

### Mechanism 3: Finite-Horizon via Implicit Truncation
Instead of modifying the reward to 0 at the horizon H, the framework sets the discount rate γₜ₊₁(·) = 0 for all t ≥ H-1. This mathematically forces the value recursion to zero at the boundary, creating a mathematically rigorous "soft" termination that satisfies the NVMDP assumptions.

## Foundational Learning

### Concept: Markov Property vs. Stationarity
- Why needed here: To distinguish between "the future depends only on the current state" (Markov) and "the rules don't change over time" (Stationarity). The paper exploits the fact that an environment can be Markov but non-stationary.
- Quick check question: Does a non-stationary MDP violate the Markov property? (Answer: No, it violates the stationarity assumption of the dynamics)

### Concept: Bellman Optimality Equation
- Why needed here: The paper modifies this fundamental recursion. You must understand the standard form Q(s,a) = r + γ max Q(s',a') to understand why moving γ inside the expectation (Eq. 30) is significant.
- Quick check question: Why does the discount factor γ move inside the expectation in the NVMDP Bellman equation?

### Concept: Temporal Difference (TD) Learning
- Why needed here: The proposed algorithms (Algorithms 3 & 4) are modifications of TD control methods (Q-learning).
- Quick check question: How does the "target" in the Q-learning update change when the discount rate depends on the specific transition just taken?

## Architecture Onboarding

### Component map:
NVMDP Agent (maintains Qₜ(s,a) tensor) -> Environment (returns s', r, γₜ₊₁) -> Q-table update (uses γₜ₊₁ in Bellman update) -> Memory (stores H × |S| × |A| values)

### Critical path:
1. Initialize Q-tables (possibly zero)
2. Start episode at t=0
3. At step t, observe s, select a
4. Observe s', r, and compute/retrieve γₜ₊₁(s,a,s')
5. Update Q̂ₜ(s,a) using Q̂ₜ₊₁(s', ·) and the specific γₜ₊₁
6. Increment t

### Design tradeoffs:
- **Storage vs. Generality**: Using state-time augmentation (t,s) allows using standard Q-learning code but explodes the state space. The NVMDP framework handles this explicitly but requires distinct storage for every time step.
- **Variance vs. Shaping Control**: Using γ > 1 is a powerful tool for shaping but introduces higher variance in learning (Section 5.3).

### Failure signatures:
- **Classic Q-learning failure**: The agent oscillates or fails to reach the goal in "Tricky Gridworld" because it tries to learn a single policy for wind dynamics that change with time
- **Value Explosion**: If γ > 1 is used without accumulated discount decay, Q-values may diverge to infinity

### First 3 experiments:
1. **Tricky Gridworld Replication**: Implement the "Tricky Gridworld" and verify that Classic Q-learning fails to improve returns, establishing the baseline non-stationarity problem
2. **NVMDP-Q Convergence**: Implement Algorithm 3 (NVMDP-Q-learning) on the same gridworld with constant γ=0.999 to verify that time-indexing allows the agent to learn the periodic wind patterns
3. **Discount-Based Shaping**: Configure the discount to γ=1.02 for transitions entering a specific "danger" cell and verify if the agent successfully avoids that cell despite the reward being structurally identical to other cells

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical convergence guarantees for NVMDPs when utilizing function approximation?
- Basis in paper: Section 6 states, "we did not address the theoretical foundations of convergence guarantees under function approximation," noting this remains an open direction.
- Why unresolved: The paper provides convergence proofs for tabular methods (Theorem 3.3) but only extends theorems like Policy Gradient and TRPO bounds for function approximation without proving algorithm convergence.
- What evidence would resolve it: Formal proofs establishing convergence conditions for gradient-based methods (like NVMDP-PPO) in continuous spaces.

### Open Question 2
- Question: Can the NVMDP framework be effectively extended to continuous state or action spaces?
- Basis in paper: Section 6 explicitly lists "continuous state or action spaces" as aspects not yet explored in the current work.
- Why unresolved: The theoretical foundations in Section 2 assume finite state and action spaces (Lemma 2.1, Theorem 2.4), requiring summations that do not directly translate to integrals in continuous domains.
- What evidence would resolve it: A theoretical extension of the matrix representation and optimality conditions to continuous probability measures, along with empirical validation.

### Open Question 3
- Question: How can NVMDPs be formulated to incorporate risk-aware objectives?
- Basis in paper: Section 6 identifies "formulations incorporating risk-aware RL" as an unexplored direction.
- Why unresolved: The experiments in Section 5 demonstrate that varying discount rates (e.g., γ > 1) increase the variance of action-value estimates, but the paper does not formalize this variance within a risk-constraint framework.
- What evidence would resolve it: A modified NVMDP objective function that explicitly optimizes for risk metrics (e.g., Conditional Value at Risk) alongside cumulative reward.

## Limitations
- Empirical validation is limited to a single gridworld environment, restricting generalizability claims
- Scalability to large state spaces remains untested as Q-table storage scales with H × |S| × |A|
- Discount-based trajectory shaping lacks external validation and relies heavily on specific reward sign assumptions

## Confidence

- **High Confidence**: Theoretical foundations (state-value/action-value recursions, policy improvement theorems, TRPO bound extension) - supported by formal proofs and consistent with classical MDP theory
- **Medium Confidence**: Algorithm convergence guarantees for DP and Q-learning variants - proof-based but limited empirical validation
- **Low Confidence**: Discount-based trajectory shaping effectiveness - demonstrated only in one synthetic environment with specific reward structure

## Next Checks
1. **Generalization Test**: Apply NVMDP-Q-learning to a continuous control environment (e.g., Mujoco) with function approximation to verify the Policy Gradient Theorem extension and assess scalability
2. **Discount Rate Sensitivity**: Systematically vary γ > 1 values in the gridworld to quantify the tradeoff between trajectory shaping effectiveness and value variance, as warned in Section 5.3
3. **Non-Stationary Baseline Comparison**: Compare NVMDP performance against alternative non-stationary RL approaches (e.g., meta-learning, context detection) on the same environments to establish relative effectiveness