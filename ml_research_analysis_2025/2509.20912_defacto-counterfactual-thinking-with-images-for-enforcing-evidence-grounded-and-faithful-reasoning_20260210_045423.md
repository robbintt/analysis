---
ver: rpa2
title: 'DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded
  and Faithful Reasoning'
arxiv_id: '2509.20912'
source_url: https://arxiv.org/abs/2509.20912
tags:
- reasoning
- answer
- visual
- evidence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring evidence-grounded
  and faithful reasoning in multimodal language models (MLLMs), particularly when
  they use "thinking with images" for reasoning. The authors propose DeFacto, a counterfactual
  reasoning framework that enforces consistency between visual evidence and final
  answers.
---

# DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning

## Quick Facts
- arXiv ID: 2509.20912
- Source URL: https://arxiv.org/abs/2509.20912
- Reference count: 15
- Key outcome: Introduces DeFacto, a counterfactual reasoning framework for multimodal models that improves evidence-grounded reasoning through three training paradigms and reinforcement learning optimization

## Executive Summary
This paper addresses the critical challenge of ensuring evidence-grounded and faithful reasoning in multimodal language models that use visual thinking. The authors propose DeFacto, a counterfactual reasoning framework that enforces consistency between visual evidence and final answers through three complementary training paradigms: positive supervision, counterfactual abstention, and random masking. The approach includes automatic construction of a large-scale counterfactual dataset (DeFacto-100K) and GRPO-based reinforcement learning optimization. Experiments demonstrate substantial improvements in both answer accuracy and reasoning faithfulness across diverse benchmarks, establishing a stronger foundation for interpretable multimodal reasoning.

## Method Summary
DeFacto introduces a counterfactual reasoning framework that trains multimodal models to maintain consistency between visual evidence and their answers. The method employs three complementary training paradigms: (1) positive supervision where models learn to select correct evidence regions and provide accurate answers, (2) counterfactual abstention where masked evidence regions teach the model to output "Unknown", and (3) random masking of irrelevant regions to prevent shortcut learning. The authors develop an automated language-guided evidence construction pipeline that localizes question-relevant regions and constructs positive, counterfactual, and random variants, creating the DeFacto-100K dataset. Models are further optimized with GRPO-based reinforcement learning using three complementary rewards promoting correct answering, structured reasoning, and consistent evidence selection.

## Key Results
- DeFacto substantially improves both answer accuracy and reasoning faithfulness on diverse benchmarks
- The framework establishes stronger evidence-grounded reasoning through counterfactual training paradigms
- Automatic construction of DeFacto-100K dataset (100k images) enables large-scale training
- GRPO-based reinforcement learning with three complementary rewards enhances model performance

## Why This Works (Mechanism)
The framework works by explicitly training models to establish causal relationships between visual evidence and reasoning outcomes. By using counterfactual scenarios where evidence is masked, the model learns to recognize when sufficient evidence is absent and appropriately abstains from answering. The random masking of irrelevant regions prevents the model from relying on spurious correlations or shortcut learning. The reinforcement learning component with multiple reward signals ensures the model balances correctness, reasoning structure, and evidence consistency simultaneously.

## Foundational Learning
- **Counterfactual reasoning**: Understanding how models behave under hypothetical scenarios where evidence is altered or absent; needed because standard training doesn't teach models to recognize missing evidence; quick check: can the model correctly abstain when key visual information is removed
- **Multimodal evidence grounding**: The process of linking visual regions to textual reasoning; needed to ensure models base answers on actual visual content rather than text-only reasoning; quick check: does the model consistently point to relevant evidence regions
- **Reinforcement learning for reasoning**: Using reward signals to shape complex reasoning behaviors; needed to balance multiple objectives (accuracy, structure, consistency); quick check: do reward curves show stable convergence across all three objectives
- **Evidence localization**: Automatically identifying relevant visual regions for questions; needed for scalable dataset construction; quick check: what is the precision/recall of the localization pipeline
- **Visual thinking in MLLMs**: The capability of multimodal models to generate and reason with visual information; needed as the foundation for image-based reasoning; quick check: can the model generate coherent visual thoughts before answering
- **GRPO optimization**: Group Relative Policy Optimization for reinforcement learning; needed to effectively optimize multiple competing objectives; quick check: does GRPO outperform standard PPO on this multi-objective task

## Architecture Onboarding

**Component Map**: Input Image/Question/Answer -> Evidence Localization (DINO-X, RPN, OCR) -> Counterfactual Dataset Construction -> Three Training Paradigms -> GRPO Optimization -> Faithful Reasoning Model

**Critical Path**: The most critical sequence is: Evidence Localization → Counterfactual Dataset Construction → All Three Training Paradigms → GRPO Optimization. Without accurate evidence localization, the counterfactual training paradigms cannot function properly, and without all three paradigms working together, the model cannot learn robust evidence-grounded reasoning.

**Design Tradeoffs**: The framework trades computational efficiency for reasoning faithfulness. The three training paradigms and reinforcement learning optimization require significant computational resources compared to standard supervised training. The automatic evidence construction pipeline sacrifices some precision for scalability, potentially introducing noise that could affect learning quality.

**Failure Signatures**: 
- Poor evidence localization leading to incorrect counterfactual examples
- Over-abstention where the model refuses to answer even when evidence is sufficient
- Inconsistent reasoning where the model's visual thoughts don't align with final answers
- Reward hacking where the model optimizes for rewards without genuine understanding

**Three First Experiments**:
1. Ablation study removing each training paradigm individually to measure their individual contributions to faithfulness
2. Error analysis of the evidence localization pipeline measuring precision and recall across different image types
3. Evaluation on tasks requiring compositional reasoning to test generalization beyond straightforward visual evidence matching

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several critical uncertainties emerge from the methodology and results.

## Limitations
- Performance regressions on document and text-centric tasks (DocVQA -5.2%, WTQ -1.2%, TextVQA -3.2%) suggest the framework may not generalize well to text-dense scenarios
- The DeFacto-100K dataset relies entirely on automated evidence localization without validation of localization accuracy or completeness
- The framework's robustness to input quality variations and real-world noisy queries is not thoroughly examined

## Confidence
- High confidence: The core methodology for counterfactual reasoning and evidence-grounded training is technically sound and well-motivated
- Medium confidence: The empirical improvements on benchmark datasets are convincing but may not fully generalize to broader applications
- Medium confidence: The automatic evidence construction pipeline is effective but quality concerns remain unaddressed

## Next Checks
1. Conduct ablation studies systematically removing each of the three training paradigms (positive supervision, counterfactual abstention, random masking) to quantify their individual contributions and potential redundancies
2. Perform detailed error analysis on the DeFacto-100K dataset construction pipeline, measuring precision and recall of evidence region localization across different image types and question categories
3. Evaluate the framework's performance and reasoning faithfulness on tasks requiring compositional reasoning, temporal understanding, or abstract concepts that go beyond straightforward visual evidence matching