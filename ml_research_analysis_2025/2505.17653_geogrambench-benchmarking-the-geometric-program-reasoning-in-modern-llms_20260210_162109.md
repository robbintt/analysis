---
ver: rpa2
title: 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs'
arxiv_id: '2505.17653'
source_url: https://arxiv.org/abs/2505.17653
tags:
- code
- reasoning
- geometric
- label
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Program-to-Geometry task and GeoGramBench
  benchmark to evaluate LLMs' ability to translate procedural drawing code into geometric
  spatial reasoning. The benchmark comprises 500 carefully curated geometry problems
  organized by a three-level taxonomy based on geometric complexity.
---

# GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs

## Quick Facts
- **arXiv ID:** 2505.17653
- **Source URL:** https://arxiv.org/abs/2505.17653
- **Reference count:** 40
- **Primary result:** Even advanced LLMs achieve less than 50% accuracy on the highest complexity level of geometric program reasoning

## Executive Summary
This paper introduces the Program-to-Geometry task and GeoGramBench benchmark to evaluate LLMs' ability to translate procedural drawing code into geometric spatial reasoning. The benchmark comprises 500 carefully curated geometry problems organized by a three-level taxonomy based on geometric complexity. Extensive evaluation of 17 frontier LLMs reveals significant deficiencies in this capability, with even the most advanced models achieving less than 50% accuracy on the highest complexity level. The findings highlight unique challenges in program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning.

## Method Summary
GeoGramBench is a zero-shot evaluation benchmark consisting of 500 geometry problems with embedded Asymptote or Matplotlib code. Problems are organized into three complexity levels: Primitive Recognition (102 problems), Local Relation Composition (279 problems), and Global Abstract Integration (119 problems). The benchmark uses a standardized prompt template with chain-of-thought reasoning and evaluates models using mean accuracy across 8 samples per problem at temperature 0.6. Answer extraction is performed via parsing \boxed{} format, with GPT-4o-assisted fallback when necessary. The evaluation covers 17 frontier LLMs ranging from 1.5B to 32B parameters, testing both zero-shot performance and CoT effectiveness.

## Key Results
- Models achieve ~85% accuracy on Primitive Recognition but drop below 50% on Global Abstract Integration
- Performance correlates with model scale (32B > 7B > 1.5B) but even top models fail at highest complexity
- Chain-of-thought reasoning rarely corrects spatial misinterpretations, often resulting in repetitive symbolic reasoning loops
- Code parsing is largely language-agnostic within procedural paradigms (<1% difference between Asymptote and Matplotlib)

## Why This Works (Mechanism)

### Mechanism 1
LLMs can extract local geometric primitives from procedural code but struggle with compositional integration. The model parses code tokens → identifies geometric primitives (points, lines, arcs) → constructs local substructures (z₁, z₂, ...) → attempts integration into a coherent global representation (Z₁). The bottleneck appears at the integration stage where small local errors propagate.

Core assumption: Internal representations exist as discrete geometric substructures that must be composed, analogous to scene graph construction in vision models.

Evidence anchors:
- [abstract] "even the most advanced models achieve less than 50% accuracy at the highest abstraction level"
- [section 6] RQ1 shows models achieve ~85% on Primitive Recognition; RQ2 shows sharp drops (e.g., GPT-o1: 86.76% → 76.02% → 43.35% across levels)
- [corpus] Limited direct corpus support; neighbor papers (CoRe, EquiBench) address code reasoning generally, not specifically geometric-spatial integration.

Break condition: When local feature extraction errors exceed a threshold, global integration fails entirely. Evidence: Figure 5 shows single direction-assignment errors cascade into complete spatial confusion.

### Mechanism 2
Performance degradation is driven by geometric complexity, not code syntax familiarity. The model processes code identically across languages but fails when spatial abstraction demands increase.

Core assumption: LLMs have learned transferable code parsing that is largely language-agnostic within procedural paradigms.

Evidence anchors:
- [abstract] "three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity"
- [appendix A] QwQ-32B shows <1% accuracy difference between Asymptote and Matplotlib on equivalent problems (47.5% vs 46.3% on AIME24)
- [corpus] Neighbor papers on code reasoning (CoRe, ThrowBench) focus on control flow and semantics; none specifically test geometric-spatial translation.

Break condition: If syntax were the bottleneck, language translation would cause measurable drops. It does not (<1%), confirming spatial abstraction as the limiting factor.

### Mechanism 3
Chain-of-thought reasoning produces verbal refinement but rarely corrects spatial misinterpretations. CoT triggers iterative self-reflection loops ("Let me check again") → models re-examine code algebraically → but lack mechanisms to update spatial mental models → verbal coherence without spatial correction.

Core assumption: Current CoT patterns optimize for textual consistency, not for maintaining and refining geometric world models.

Evidence anchors:
- [abstract] "unique challenges posed by program-driven spatial reasoning"
- [section 6 RQ3] "CoT trajectories rarely correct or update internal geometric understanding... CoT may lead LLMs fall into repetitive symbolic reasoning"
- [corpus] No corpus neighbor directly addresses CoT for spatial reasoning; spatiotemporal benchmark (STARK) noted but not CoT-specific.

Break condition: When spatial relationships require non-textual mental simulation, CoT iteration loops without convergence. Evidence: Figure 5 shows repeated self-questioning without resolution.

## Foundational Learning

- Concept: **Procedural geometry code semantics** (Asymptote/matplotlib drawing primitives)
  - Why needed here: Models must translate `draw(A--B--C--cycle)` into a mental triangle representation; without this, all downstream reasoning fails.
  - Quick check question: Given `pair A=(0,0), B=(2,0), C=(1,1.732); draw(A--B--C--cycle)`, what shape is constructed and what is its approximate area?

- Concept: **Spatial composition vs. aggregation**
  - Why needed here: The paper's taxonomy distinguishes local element recognition from global structure integration; models fail at the latter.
  - Quick check question: If code specifies two circles with centers at (0,0) and (3,0), both radius 2, can you determine if they intersect and where?

- Concept: **Coordinate frame transformations**
  - Why needed here: Global Abstract Integration problems involve rotation, projection, 3D-to-2D mapping—all requiring coordinate transformations.
  - Quick check question: Given a point at (1,0,0) in 3D, what are its 2D coordinates after orthographic projection onto the xy-plane?

## Architecture Onboarding

- Component map: Input text + code -> Code parsing -> Primitive extraction -> Spatial representation (local z₁, z₂... → global Z₁) -> Reasoning -> Answer extraction

- Critical path:
  1. Code parsing → primitive extraction (RQ1, ~85% success)
  2. Local composition → spatial integration (RQ2, degradation begins)
  3. Global abstraction → multi-step reasoning (RQ3, <50% for top models)

  Bottleneck confirmed at step 2→3 transition.

- Design tradeoffs:
  - **Language-agnostic vs. specialized**: Paper shows <1% gap between Asymptote and Matplotlib; investing in language-specific parsing is low-yield
  - **CoT depth vs. spatial grounding**: Longer CoT does not correlate with spatial accuracy; need explicit geometric verification mechanisms
  - **Model scale**: Clear scale-dependent performance (32B > 7B > 1.5B), but even frontier models <50% at Abstract level

- Failure signatures:
  - "Let me check again" loops without spatial correction (Figure 5)
  - Correct algebraic manipulation but wrong spatial assumptions
  - Direction confusion (CW vs CCW) cascading to region misidentification
  - Coordinate extraction without geometric relationship understanding

- First 3 experiments:
  1. **Probing local extraction accuracy**: Isolate primitive recognition by evaluating on synthetic code with single elements; measure per-primitive accuracy (point, line, circle, arc).
  2. **Testing composition degradation**: Create controlled pairs differing only in number of elements (2 vs 4 vs 8); plot accuracy vs. element count to quantify integration scaling.
  3. **Comparing CoT with explicit visualization prompts**: Run same problems with "first sketch the diagram mentally" prompts vs. standard CoT; measure if explicit spatial framing improves Abstract-level accuracy.

## Open Questions the Paper Calls Out

- **Can RL enhance spatial reasoning?** Can reinforcement learning (RL) or other targeted training strategies explicitly enhance the spatial reasoning and abstraction capabilities of LLMs on Program-to-Geometry tasks? The authors plan to explore RL as well as other targeted training strategies to explicitly enhance spatial reasoning and abstraction in LLMs. This remains unresolved as the current work focuses on evaluation rather than proposing or testing fine-tuning methods.

- **Does the framework generalize?** Do the framework and insights derived from GeoGramBench generalize to broader domains where procedural descriptions interact with spatial or relational reasoning? While the current focus is geometry, the framework may generalize to broader domains where procedural descriptions interact with spatial or relational reasoning. This remains untested as the paper restricts its scope to mathematical geometry.

- **What architectural mechanisms are needed?** What specific architectural mechanisms are required to overcome the integration bottleneck where small errors in local geometric parsing prevent the formation of a globally consistent spatial representation? The paper identifies the symptoms (failure to compose elements) but does not propose a solution for maintaining global consistency during the interpretation of procedural code.

## Limitations

- Synthetic vs. Natural Integration: The benchmark focuses on procedural geometry code rather than natural language descriptions of spatial relationships, potentially limiting generalizability to real-world geometric reasoning scenarios.
- Single-Language Focus: Despite showing minimal language-switching effects, the benchmark uses primarily Asymptote code, which may not capture the full spectrum of geometric specification languages used in practice.
- Static Evaluation: The zero-shot evaluation approach cannot measure how models would perform with fine-tuning or few-shot adaptation specifically for geometric reasoning tasks.

## Confidence

- **High Confidence** (4+ strong evidence anchors):
  - Models show significant performance degradation at higher geometric complexity levels
  - Chain-of-thought reasoning does not effectively correct spatial misinterpretations
  - Code parsing is largely language-agnostic within procedural paradigms

- **Medium Confidence** (2-3 evidence anchors):
  - The hypothesis about local-to-global geometric integration bottlenecks
  - Performance correlates with model scale but even frontier models struggle
  - CoT patterns optimize for textual consistency over spatial world model refinement

- **Low Confidence** (0-1 evidence anchors):
  - The specific hypothesis about internal geometric representation as discrete substructures
  - The claim that geometric complexity is the primary limiting factor versus other potential bottlenecks

## Next Checks

1. **Controlled Composition Scaling Experiment**: Create synthetic problems varying only in the number of geometric elements (2, 4, 8, 16) while holding primitive types constant. Measure accuracy decay curve to quantify integration scaling limits and validate whether the bottleneck is compositional rather than recognition-based.

2. **Cross-Paradigm Generalization Test**: Adapt 50 benchmark problems to use natural language geometric descriptions instead of procedural code. Evaluate whether models maintain similar performance gaps, testing if the difficulty stems from code parsing or fundamental geometric reasoning limitations.

3. **Geometric Visualization Prompting Study**: Run identical problems with explicit "first sketch the diagram mentally" prompts versus standard CoT. Compare accuracy improvements, particularly at the Global Abstract Integration level, to determine if explicit spatial framing can overcome the verbal reasoning limitation.