---
ver: rpa2
title: 'FedMuon: Accelerating Federated Learning with Matrix Orthogonalization'
arxiv_id: '2510.27403'
source_url: https://arxiv.org/abs/2510.27403
tags:
- local
- muon
- preprint
- learning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedMuon, a federated learning optimizer that
  explicitly incorporates matrix orthogonalization to improve convergence and communication
  efficiency. Existing federated optimizers neglect the geometric structure of weight
  matrices, leading to ill-conditioned updates and slow convergence.
---

# FedMuon: Accelerating Federated Learning with Matrix Orthogonalization

## Quick Facts
- **arXiv ID:** 2510.27403
- **Source URL:** https://arxiv.org/abs/2510.27403
- **Reference count:** 40
- **Primary result:** Introduces FedMuon optimizer that achieves linear convergence rate O(√(L∆σ²ₗ/(SKR) + L∆/R)) without data heterogeneity assumptions, reducing communication rounds by up to 3.5× while improving accuracy on vision and language models.

## Executive Summary
FedMuon introduces a federated learning optimizer that explicitly incorporates matrix orthogonalization to improve convergence and communication efficiency. Unlike existing optimizers that treat weight matrices as flattened vectors, FedMuon applies SVD-based orthogonalization during local training, combined with local-global alignment to reduce client drift and momentum aggregation for temporal consistency. The method achieves significant reductions in communication rounds while improving test accuracy compared to strong baselines like FedAvg, SCAFFOLD, and FedLADA, particularly for Transformer architectures under non-IID conditions.

## Method Summary
FedMuon extends the Muon optimizer to federated settings by applying Newton-Schulz iterations for matrix orthogonalization, maintaining global momentum across rounds, and interpolating local updates with global directions to mitigate client drift. The server aggregates both model updates and momentum matrices from clients, while clients initialize local momentum with the global state and apply low-rank SVD compression (5% retention) for efficient communication. This approach preserves the geometric structure of weight matrices while reducing communication overhead, achieving faster convergence without requiring strong data heterogeneity assumptions.

## Key Results
- **Communication efficiency:** Reduces communication rounds by 2.4×-3.5× compared to FedAvg while achieving higher accuracy on CIFAR-100 and Tiny ImageNet
- **Convergence guarantees:** Establishes linear convergence rate O(√(L∆σ²ₗ/(SKR) + L∆/R)) without relying on data heterogeneity assumptions
- **Transformer performance:** Demonstrates significant improvements on BERT fine-tuning tasks (SST-2, QQP) with 96.2% accuracy versus 94.8% for FedAvg

## Why This Works (Mechanism)

### Mechanism 1: Matrix Orthogonalization via Newton-Schulz
The optimizer applies 5 iterations of Newton-Schulz algorithm to momentum matrix $M$, approximating $(MM^T)^{-1/2}M$ to force singular values toward 1. This spectral normalization prevents pathological amplification of weight matrix directions, stabilizing condition number and accelerating convergence. The orthogonalization ensures updates span full row/column space rather than collapsing into dominant directions.

### Mechanism 2: Local-Global Alignment
Modifies local update rule to $x_{k+1} = x_k - \eta [(1-\alpha) \text{LocalMuon} + \alpha \Delta_{Global}]$, where $\alpha$ anchors client to server's consensus direction. This mitigates "client drift" in non-IID settings where local preconditioners would otherwise rotate gradients into misaligned subspaces, correcting direction mismatches caused by client-specific data geometries.

### Mechanism 3: Momentum Aggregation & Compression
Persists momentum across rounds by aggregating and broadcasting it rather than resetting to zero, preserving temporal velocity critical for convergence. Uses SVD compression retaining top 5% of singular values/vectors to manage communication cost of transmitting large matrix states, leveraging the low-rank nature of momentum matrices.

## Foundational Learning

- **Concept: Condition Number & Hessian Geometry**
  - **Why needed here:** Core thesis is that element-wise optimizers worsen condition number of weight matrices, slowing convergence
  - **Quick check question:** If Hessian has eigenvalues 1000 and 0.001, how does this constrain maximum safe step size?

- **Concept: Client Drift in Federated Learning**
  - **Why needed here:** Local Muon baseline fails in non-IID settings because local optimizers over-fit to local data distributions, causing global model divergence
  - **Quick check question:** In non-IID setting, why might average of client gradients point in direction that decreases local losses but increases global loss?

- **Concept: Singular Value Decomposition (SVD)**
  - **Why needed here:** Used for orthogonalization mechanism (Newton-Schulz approximates SVD) and communication compression of momentum state
  - **Quick check question:** If you zero out all but largest singular value of matrix, how does geometric transformation change?

## Architecture Onboarding

- **Component map:** Client -> Local Model/Momentum -> Newton-Schulz Orthogonalizer -> Alignment Mixer -> Server -> Aggregator/Momentum Aggregator -> Global Delta Calculator

- **Critical path:**
  1. Broadcast: Server sends global weights $x^r$, global momentum $\bar{M}^r$, and global delta $\Delta^r_G$ to selected clients
  2. Init: Client sets local momentum $M_0 = \bar{M}^r$ (warm start)
  3. Local Step (K iterations): Compute Grad -> Update Momentum -> Orthogonalize (Newton-Schulz) -> Apply Aligned Update (Eq. 3)
  4. Compress & Upload: Client performs SVD on final momentum, keeps top-k singular values/vectors, sends $(\Delta x, M_{compressed})$ to server
  5. Aggregate: Server averages updates to generate $x^{r+1}$ and $\bar{M}^{r+1}$

- **Design tradeoffs:**
  - Orthogonalization Overhead: ~5% compute increase vs. significantly fewer communication rounds
  - Compression Fidelity: 5% rank retention reduces comm by 95% but assumes momentum is low-rank
  - Alignment Strength ($\alpha$): High $\alpha$ stabilizes global convergence but may dampen local adaptation

- **Failure signatures:**
  - Stagnation in Local Muon: Without alignment on non-IID data, local loss drops rapidly but global accuracy flattens
  - Oscillation: If $\alpha$ is too low (<0.25) in non-IID settings, training becomes unstable
  - Information Loss: If SVD compression rank is too aggressive, convergence speed degrades to standard SGD levels

- **First 3 experiments:**
  1. IID Validation: Train Local Muon vs. Local AdamW on IID data to confirm base benefit of matrix orthogonalization
  2. Non-IID Stress Test: Train FedMuon vs. Local Muon on Dir-0.1 (high heterogeneity). Verify removing alignment causes global model failure
  3. Communication Efficiency: Compare FedMuon (SVD compressed) vs. FedMuon (full) on ViT-Tiny. Plot accuracy vs. bytes transmitted

## Open Questions the Paper Calls Out

- **Question:** Can the local-global alignment and momentum aggregation mechanisms of FedMuon be effectively extended to other structure-aware or adaptive optimizers, such as LAMB or Lion?
  - **Basis in paper:** Conclusion states authors believe FedMuon "inspires future extensions to related methods such as LAMB or Lion"
  - **Why unresolved:** Current paper only validates approach using Muon optimizer; interaction between FedMuon's specific matrix orthogonalization and update rules of LAMB or Lion remains unexplored
  - **What evidence would resolve it:** Empirical results from experiments replacing Local Muon with LAMB or Lion within FedMuon framework on same non-IID benchmarks

- **Question:** Can computational overhead of Newton-Schulz iteration be further reduced to better suit resource-constrained edge devices?
  - **Basis in paper:** Page 4 notes matrix orthogonalization results in "about 5% higher computation time compared to AdamW," which may be significant for low-power clients
  - **Why unresolved:** While paper addresses communication efficiency, additional local computation time is accepted as trade-off without exploring lower-cost approximations
  - **What evidence would resolve it:** Convergence analysis using fewer than five Newton-Schulz iterations or alternative orthogonalization methods, demonstrating maintained accuracy with reduced FLOPs

## Limitations

- Claims about matrix orthogonalization benefits rely on empirical validation rather than comprehensive theoretical guarantees for deep neural networks
- Convergence analysis assumes specific Hessian properties that may not hold universally across model architectures
- 5% SVD compression threshold is presented as effective but lacks systematic sensitivity analysis across different model scales and tasks

## Confidence

- **High confidence:** Experimental results demonstrating communication efficiency improvements over baselines (FedAvg, SCAFFOLD, FedLADA) are robust and well-documented with multiple datasets and model types
- **Medium confidence:** Theoretical convergence rate derivation is mathematically sound within its assumptions, though practical tightness of bound across heterogeneous FL scenarios requires further validation
- **Medium confidence:** Mechanism claims about condition number stabilization and client drift mitigation are supported by ablation studies, but specific contributions of orthogonalization versus other design choices are difficult to disentangle cleanly

## Next Checks

1. **Convergence sensitivity analysis:** Systematically vary orthogonalization rank and alignment coefficient α across broader range of non-IID Dirichlet parameters to map stability boundaries of FedMuon

2. **Cross-architecture generalization:** Test FedMuon on recurrent networks (LSTMs/GRUs) and small CNNs to determine whether orthogonalization benefits extend beyond Transformers to architectures with different weight matrix characteristics

3. **Communication cost breakdown:** Measure and report exact communication cost (bytes transmitted per round) for FedMuon with full momentum versus compressed momentum across different compression ranks to validate claimed 95% reduction is maintained across model scales