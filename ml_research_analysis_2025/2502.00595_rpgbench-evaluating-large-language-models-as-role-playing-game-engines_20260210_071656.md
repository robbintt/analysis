---
ver: rpa2
title: 'RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines'
arxiv_id: '2502.00595'
source_url: https://arxiv.org/abs/2502.00595
tags:
- game
- human
- personality
- type
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RPGBench is the first benchmark designed to evaluate large language
  models as text-based role-playing game engines. It introduces two core tasks: Game
  Creation, where models design playable games with verifiable mechanics, and Game
  Simulation, where models simulate interactive gameplay.'
---

# RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines

## Quick Facts
- arXiv ID: 2502.00595
- Source URL: https://arxiv.org/abs/2502.00595
- Reference count: 40
- Key outcome: First benchmark evaluating LLMs as text-based RPG engines with Game Creation and Game Simulation tasks

## Executive Summary
RPGBENCH introduces a novel framework for evaluating large language models as text-based role-playing game engines. The benchmark defines two core tasks: Game Creation, where models design playable games with verifiable mechanics, and Game Simulation, where models simulate interactive gameplay. Using a BFS-based validity checker and an LLM-as-a-judge framework, RPGBench provides both objective metrics for mechanical correctness and subjective evaluations for narrative quality, revealing significant gaps between LLM capabilities and human-level game design.

## Method Summary
RPGBENCH employs a two-stage evaluation pipeline. In the Game Creation task, LLMs generate RPGs following a strict JSON schema that defines game worlds, characters, state variables, events, and termination conditions. A BFS-based validity checker verifies that games are playable by ensuring all events are reachable and both success and failure terminations are achievable. The Game Simulation task then uses validated games, running 10-round simulations where the LLM generates event plans, narrations with action choices, and state updates. An LLM-as-a-judge framework evaluates subjective metrics including interestingness and role-playing quality, while objective metrics measure mechanical consistency.

## Key Results
- State-of-the-art LLMs can generate engaging narratives but struggle with implementing consistent, verifiable game mechanics
- The BFS validity checker effectively filters out games with unreachable events or impossible win/loss conditions
- Objective metrics (mechanical correctness) and subjective evaluations show both alignment and discrepancies, highlighting the complexity of evaluating LLM-driven game simulation

## Why This Works (Mechanism)
The benchmark works by constraining game representation to a finite-state machine model with explicit state variables and deterministic transitions. This formalization enables automated verification through BFS reachability analysis, transforming the subjective task of game evaluation into a mix of objective mechanical checks and controlled subjective assessments via LLM judging. The JSON schema provides a structured interface between the creative LLM generation and the mechanical validation processes.

## Foundational Learning

- **Finite-State Machines (FSMs) for Narrative**
  - Why needed here: The benchmark represents games as discrete states and transitions, requiring understanding of state graphs for validation
  - Quick check question: Can you draw the state diagram for a simple game with two variables (Health: 0-10, Gold: 0-10) where the player wins if they collect 5 Gold and loses if Health reaches 0?

- **Large Language Model (LLM) as a Component**
  - Why needed here: The system treats LLM as a component with specific inputs and structured outputs, not as a magic box
  - Quick check question: What are the three distinct outputs the LLM must produce for each round in the Game Simulation task?

- **Formal Verification (Reachability)**
  - Why needed here: The benchmark's innovation is proving games are playable via automated reachability analysis
  - Quick check question: In the BFS algorithm, what two conditions must be true for a game to be declared "valid"?

## Architecture Onboarding

- **Component map**: The system consists of two main pipelines. The Game Creation Pipeline takes a character prompt, queries an LLM, parses the JSON output, and runs it through the BFS Validity Checker. The Game Simulation Pipeline takes a validated game, initializes a state, and enters a loop: 1) A Simulated Player picks an action, 2) The LLM Engine generates an Event Plan, Narration, and State Update, 3) The Evaluator logs the turn. After the simulation ends, the Scoring Module calculates metrics using the logged data and a separate LLM Judge.

- **Critical path**: For an engineer, the most critical and non-obvious component is the BFS Validity Checker (Algorithm 1 in the paper). Implementing this correctly—including parsing conditional expressions, applying effects, and handling state bounds—is the foundational step. All evaluation depends on a pool of valid games.

- **Design tradeoffs**: The primary tradeoff is between game expressiveness and evaluability. The JSON schema is a major constraint; complex mechanics like inventory systems or free-form dialogue are simplified into a handful of numerical variables. This makes verification tractable but limits the complexity of the games that can be represented.

- **Failure signatures**: Common failure modes include 1) JSON Format Errors: The LLM outputs malformed JSON. 2) Validity Errors: The game is impossible to win (no success state reachable) or trivially easy (win condition met at start). 3) Simulation Drift: The LLM's narrative ignores the state variables, leading to inconsistent storytelling.

- **First 3 experiments**:
  1. Baseline Sanity Check: Implement a random player agent for the Game Simulation task. Run it on a set of valid games for a baseline model (e.g., GPT-4o). This establishes the lower bound for the Mechanics Score (MEC), as a random agent will often make suboptimal choices that don't advance the game.
  2. Validity Checker Stress Test: Task an LLM with generating games of increasing complexity (e.g., by specifying more state variables or events). Plot the Valid-Check Pass Rate (VCR) against game complexity to identify the point of failure for the model's planning capabilities.
  3. Judge Alignment Study: Run the human evaluation protocol on a small subset of games and compare the results with the LLM-as-a-judge scores. Calculate the correlation coefficients (Pearson/Kendall) for metrics like Interestingness (INT) to quantify the alignment before relying on the automatic evaluator for large-scale experiments.

## Open Questions the Paper Calls Out

- **Can agent-based player simulation frameworks (using strategic, goal-directed agents) reveal different failure modes in LLM game engines compared to the current random-action selection approach?**
  - Basis in paper: The conclusion states "future work could focus on expanding the character pool and exploring agent-based simulation framework."
  - Why unresolved: The current framework uses random action selection from candidate choices, which may not stress-test game mechanics as thoroughly as intentional, strategic gameplay would.
  - What evidence would resolve it: Implementing agent-based players with various strategies (optimization-driven, narrative-driven, adversarial) and comparing the types and rates of mechanical errors uncovered.

- **Why do LLM-as-judge and human evaluations show negative correlation for personality consistency (PER) despite reasonable alignment on other subjective metrics like interestingness?**
  - Basis in paper: Table 9 shows Pearson correlation of -0.691 between automatic and human PER scores, which the authors call "a fundamental mismatch between how LLM-based and human evaluators assess personality consistency through TIPI."
  - Why unresolved: The paper documents the discrepancy but does not determine whether the issue lies in TIPI methodology, LLM judge calibration, or human annotator understanding of Big Five traits.
  - What evidence would resolve it: Systematic ablation studies comparing alternative personality assessment methods, annotator training interventions, and different LLM judge prompting strategies.

- **How does game mechanic accuracy degrade as simulation length extends beyond 10 rounds, and what architectural modifications could maintain long-horizon consistency?**
  - Basis in paper: Table 5 shows MEC declining from 0.693 to 0.668 as rounds increase from 10 to 25, and the authors note "growing difficulty in maintaining coherent game mechanics within a longer context."
  - Why unresolved: The paper limits main experiments to 10 rounds for computational feasibility but does not investigate the root causes of degradation or potential mitigations.
  - What evidence would resolve it: Extended simulations with context management techniques (summarization, state compression, explicit memory mechanisms) compared against baseline performance trajectories.

## Limitations
- The constrained JSON schema simplifies complex RPG mechanics into numerical variables, excluding many real-world game design elements
- The LLM-as-a-judge framework introduces potential bias and may not fully align with human preferences
- The benchmark assumes deterministic state updates and does not account for stochasticity in LLM outputs beyond the specified temperature parameter

## Confidence

- **High Confidence**: The BFS-based validity checker correctly identifies reachable states and verifies termination conditions in games that conform to the JSON schema. The Mechanics Score (MEC) reliably measures objective game state changes when the LLM follows the specified format.
- **Medium Confidence**: Subjective metrics like Interestingness (INT) and Factual Consistency (FAC) are moderately aligned with human judgments, as evidenced by the reported Pearson correlation coefficients (e.g., 0.48 for INT). However, the exact correlation may vary with different judge prompts or human evaluator pools.
- **Low Confidence**: The benchmark's ability to generalize to more complex game mechanics (e.g., inventory systems, dialogue trees) is uncertain, as these are excluded by design. The long-term stability of LLM-generated game narratives under extended play is also unverified.

## Next Checks

1. **Schema Stress Test**: Systematically increase the number of state variables and events in generated games, then measure the Valid Check Rate (VCR) to identify the complexity threshold where validity drops significantly.
2. **Judge Prompt Sensitivity**: Run the same set of games through multiple variations of the LLM judge prompts (e.g., different instructions for INT or FAC) and compare the resulting score distributions to assess prompt sensitivity.
3. **Human-LLM Alignment Replication**: Conduct a small-scale human evaluation study on a subset of games, then compute correlation coefficients (Pearson/Kendall) for each subjective metric to verify the reported alignment levels.