---
ver: rpa2
title: 'DiffDenoise: Self-Supervised Medical Image Denoising with Conditional Diffusion
  Models'
arxiv_id: '2504.00264'
source_url: https://arxiv.org/abs/2504.00264
tags:
- denoising
- noise
- diffusion
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffDenoise is a self-supervised medical image denoising method
  that addresses the challenge of preserving high-frequency details while removing
  noise. The approach leverages conditional diffusion models trained on noisy images,
  using outputs from a pretrained Blind-Spot Network as conditioning inputs.
---

# DiffDenoise: Self-Supervised Medical Image Denoising with Conditional Diffusion Models

## Quick Facts
- **arXiv ID**: 2504.00264
- **Source URL**: https://arxiv.org/abs/2504.00264
- **Reference count**: 40
- **Primary result**: Outperforms state-of-the-art self-supervised denoising on medical images, preserving high-frequency details

## Executive Summary
DiffDenoise addresses the challenge of self-supervised medical image denoising by leveraging conditional diffusion models trained exclusively on noisy images. The method uses a pretrained Blind-Spot Network (BSN) as a conditioning signal to guide the diffusion model in recovering high-frequency details lost to noise. A novel Stabilized Reverse Diffusion Sampling (SRDS) technique averages outputs from symmetric noise initialization pairs to suppress artifacts and stabilize convergence. The approach demonstrates superior performance over existing self-supervised methods while preserving fine anatomical structures across multiple medical imaging modalities.

## Method Summary
DiffDenoise employs a three-stage pipeline: (1) Pretrain a BSN (PUCA for iid noise or LG-BPN for spatially correlated noise) to generate low-noise conditioning signals, (2) Train a conditional diffusion model on noisy images using BSN outputs as conditioning, and (3) Apply SRDS during inference by averaging outputs from symmetric noise pairs, then distill to a deterministic NAFNet for fast deployment. The method requires no clean reference images and achieves state-of-the-art performance while preserving high-frequency details.

## Key Results
- Achieves higher PSNR and SSIM than existing self-supervised denoising methods on FastMRI, COVID Chest X-Ray, and M4Raw datasets
- Preserves high-frequency details better than conventional self-supervised approaches
- Demonstrates robust performance across different noise types including Gaussian, Poisson, Gamma, and spatially correlated noise
- Knowledge distillation step improves PSNR by 0.15-0.36 dB

## Why This Works (Mechanism)

### Mechanism 1: Conditional Diffusion on Noisy Manifolds
The diffusion model learns the score function of noisy data distributions. When conditioned on BSN outputs, it focuses on recovering high-frequency details rather than denoising from scratch. The method assumes noise distributions are symmetric around zero, allowing the score function of noisy data to approximate that of clean data.

### Mechanism 2: Stabilized Reverse Diffusion Sampling (SRDS)
SRDS initializes reverse diffusion with symmetric noise pairs (ε and -ε) and averages the outputs. This technique cancels out approximation errors in the learned score function, stabilizing convergence toward the clean data manifold and suppressing artifacts.

### Mechanism 3: Knowledge Distillation for Hallucination Suppression
A deterministic NAFNet network is trained to mimic the diffusion model's successful restorations while filtering out stochastic variations and hallucinations. This provides faster inference while maintaining denoising quality.

## Foundational Learning

- **Blind-Spot Networks (BSN) & J-Invariance**: BSNs predict pixels using only neighbors to separate signal from noise. Why needed: DiffDenoise uses BSN outputs as conditioning signals. Quick check: If noise is spatially correlated, does a small-blind-spot BSN effectively remove it?

- **Denoising Diffusion Implicit Models (DDIM)**: DDIM enables deterministic sampling with fewer steps. Why needed: Critical for SRDS mechanism requiring deterministic behavior. Quick check: Why is DDIM's deterministic property essential for symmetric noise pairing?

- **Knowledge Distillation**: Training a student network to mimic a teacher's behavior. Why needed: Final distillation step filters hallucinations and accelerates inference. Quick check: Does the student network learn to predict noise or clean image?

## Architecture Onboarding

- **Component map**: BSN Pretraining → Conditional Diffusion Training → SRDS Dataset Generation → NAFNet Training
- **Critical path**: BSN pretraining → diffusion training → SRDS dataset generation → NAFNet training. Final quality depends heavily on SRDS dataset generation.
- **Design tradeoffs**: BSN blind spot size affects noise handling vs. detail preservation; inference speed vs. quality between diffusion model and distilled NAFNet; SRDS doubles diffusion inference cost.
- **Failure signatures**: Over-smoothing from aggressive BSN conditioning; hallucinations without distillation; correlation artifacts from wrong blind-spot size.
- **First 3 experiments**: 1) Conditioning ablation (identity vs. BSN), 2) SRDS validation vs. standard DDIM, 3) Noise symmetry test with asymmetric synthetic noise.

## Open Questions the Paper Calls Out

1. **Training optimization**: How to condense the three-stage pipeline to reduce GPU memory usage and training time? The authors note this as a future direction.
2. **Extreme noise performance**: Does the method maintain performance on noise levels outside tested ranges? Authors haven't tested on other datasets with different noise levels.
3. **Noise distribution violations**: How does performance degrade with asymmetric noise distributions? The method assumes symmetric noise around zero.

## Limitations
- High computational overhead from three-stage pipeline and SRDS doubling diffusion sampling
- Performance may degrade with asymmetric or highly correlated noise violating symmetry assumptions
- Requires careful hyperparameter tuning for BSN blind spot size and diffusion architecture

## Confidence

- **High confidence**: Conditional diffusion models effectiveness for medical denoising (supported by ablation studies and metrics)
- **Medium confidence**: SRDS mechanism's artifact suppression (theoretical reasoning and Figure 4 evidence)
- **Medium confidence**: Knowledge distillation's hallucination suppression (PSNR improvements but limited ablation evidence)

## Next Checks

1. Apply DiffDenoise to synthetic noise with non-zero mean to test symmetry assumption break conditions
2. Measure BSN conditioning quality by comparing BSN outputs to ground truth across noise types
3. Systematically vary diffusion sampling steps to observe hallucination emergence in critical anatomical regions