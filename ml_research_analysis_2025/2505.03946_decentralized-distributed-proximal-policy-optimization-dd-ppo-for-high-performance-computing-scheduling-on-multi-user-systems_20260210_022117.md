---
ver: rpa2
title: Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance
  Computing Scheduling on Multi-User Systems
arxiv_id: '2505.03946'
source_url: https://arxiv.org/abs/2505.03946
tags:
- scheduling
- policy
- dd-ppo
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to job scheduling in high-performance
  computing (HPC) environments using decentralized distributed proximal policy optimization
  (DD-PPO). Traditional rule-based scheduling algorithms are challenged by the increasing
  heterogeneity and scale of HPC systems.
---

# Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems

## Quick Facts
- **arXiv ID**: 2505.03946
- **Source URL**: https://arxiv.org/abs/2505.03946
- **Reference count**: 40
- **Primary result**: DD-PPO scheduler improves HPC job scheduling performance across four optimization objectives compared to rule-based and existing RL algorithms.

## Executive Summary
This paper introduces a novel decentralized distributed proximal policy optimization (DD-PPO) algorithm for HPC job scheduling. Traditional rule-based scheduling algorithms struggle with increasing heterogeneity and scale of modern HPC systems. The proposed approach leverages large-scale distributed training across multiple workers without requiring parameter synchronization at every step, enabling enhanced scalability, training efficiency, and sample utilization. The scheduler is trained on a dataset containing over 11.5 million real HPC job traces spanning six years, demonstrating improved performance across multiple scheduling objectives compared to both rule-based and existing RL-based approaches.

## Method Summary
The DD-PPO scheduler uses decentralized gradient synchronization across multiple workers to enable scalable training without parameter server bottlenecks. Each worker independently collects experiences and computes local gradients, which are periodically synchronized and applied to shared parameters. The algorithm employs a clipped surrogate objective from PPO to prevent large policy deviations during distributed updates. Training is performed on over 11.5 million real HPC job traces using a custom SchedGym environment based on OpenAI Gym. The method is validated against rule-based schedulers and existing RL algorithms across four key optimization objectives: average waiting time, average turnaround time, average bounded slowdown, and resource utilization.

## Key Results
- DD-PPO achieves superior scheduling performance compared to rule-based schedulers (FCFS, SJF, F1) across all four optimization objectives
- The method outperforms standard PPO algorithm in both scheduling performance and resource utilization
- Population-based training and hyperparameter fine-tuning significantly enhance DD-PPO's performance
- Training on large-scale real workload traces improves generalization compared to smaller datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decentralized gradient synchronization enables scalable training across multiple workers without bottlenecking on a central parameter server.
- Mechanism: Each worker independently collects experiences and computes local gradients. Gradients are summed across all N workers at regular intervals (not every step), then applied to shared parameters. This increases effective batch size and reduces gradient variance.
- Core assumption: Periodic synchronization intervals are short enough that stale gradients do not destabilize policy updates.
- Evidence anchors:
  - [abstract] "supports large-scale distributed training across multiple workers without requiring parameter synchronization at every step"
  - [section 4.1] "each worker independently collects experiences and computes gradients, which are then periodically synchronized without relying on a centralized parameter server"
- Break condition: If synchronization intervals become too long relative to policy update frequency, gradient staleness may cause divergence or unstable learning curves.

### Mechanism 2
- Claim: Training on large-scale real workload traces (11.5M+ jobs) improves generalization to unseen HPC environments.
- Mechanism: Diverse job characteristics (sizes, arrival patterns, runtime distributions) from six years of production traces expose the model to a wide policy space, reducing overfitting to narrow workload patterns.
- Core assumption: Historical workload patterns are sufficiently representative of future scheduling scenarios in target systems.
- Evidence anchors:
  - [abstract] "validation dataset leveraged over 11.5 million real HPC job traces"
  - [section 6] "combining DD-PPO with large-scale datasets results in a more generalized and effective model than a standard PPO-based approach trained on the same dataset"
- Break condition: If deployed system workloads diverge significantly from training distribution (e.g., different job mix, hardware topology), learned policies may not transfer.

### Mechanism 3
- Claim: The PPO clipped surrogate objective stabilizes training by preventing large policy deviations during distributed updates.
- Mechanism: The objective uses a clipping function `clip(r_t(θ), 1-ε, 1+ε)` to constrain the probability ratio between new and old policies, limiting the magnitude of each update step regardless of advantage magnitude.
- Core assumption: Constraining per-step policy changes to a bounded range (controlled by ε, typically 0.2) leads to more consistent convergence than unconstrained gradient ascent.
- Evidence anchors:
  - [section 3.3] "The clipping mechanism is crucial as it prevents excessively large policy updates, thereby enhancing training stability compared to other RL approaches"
  - [section 4.1] DD-PPO "retaining the stability and performance improvements of PPO"
- Break condition: If ε is set too small, learning becomes excessively slow; if too large, instability can re-emerge, particularly under distributed gradient variance.

## Foundational Learning

- **Actor-Critic Methods**
  - Why needed here: The framework uses separate policy (actor) and value (critic) neural networks. The actor selects jobs to schedule; the critic estimates expected cumulative reward for a job sequence. Understanding this separation is essential for debugging training dynamics.
  - Quick check question: Given equation (5) `δ_t = R_t + γV(s_{t+1}) - V(s_t)`, how does the TD-error δ_t influence the policy gradient update?

- **Proximal Policy Optimization (PPO)**
  - Why needed here: DD-PPO extends standard PPO; the clipped surrogate objective (equation 7) is the core loss function. Understanding clipping and the probability ratio `r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)` is prerequisite.
  - Quick check question: If the advantage A_t is positive but `r_t(θ)` exceeds `1 + ε`, what is the effective gradient signal under the clipped objective?

- **Markov Decision Processes (MDPs) for Scheduling**
  - Why needed here: The HPC scheduling problem is formalized as an MDP with states (observable jobs, system resources), actions (job selection), and rewards (scheduling metrics). Defining these components correctly is critical for environment design.
  - Quick check question: What state features would you include to capture both queue dynamics and resource availability for a multi-user HPC system?

## Architecture Onboarding

- **Component map**:
  - Job traces (Falcon/Lemhi) -> SchedGym environment -> State observation (waiting jobs) -> Policy network (32→16→8) -> Action (job selection) -> Environment state update -> Reward calculation -> Local gradient computation -> Periodic gradient synchronization -> Shared parameter update

- **Critical path**:
  1. Load job traces (Falcon/Lemhi for training; Lublin-256/SDSC-SP2 for evaluation)
  2. SchedGym initializes cluster state, presents waiting jobs as state observation
  3. Policy network scores each job → softmax → probability distribution → action (job selection)
  4. Environment executes action, updates cluster state, computes reward
  5. Each distributed worker accumulates experience batch, computes local gradients
  6. Gradients summed across N workers, applied to shared parameters (equation 8)
  7. PBT periodically adjusts hyperparameters across worker population

- **Design tradeoffs**:
  - **Synchronization frequency vs. training speed**: Less frequent sync reduces communication overhead but risks gradient staleness
  - **Dataset scale vs. compute requirements**: 11.5M traces improve generalization but demand distributed infrastructure
  - **Network capacity vs. inference latency**: Smaller networks (Table 1) enable faster scheduling decisions but may limit policy expressiveness
  - **Reward shaping vs. objective alignment**: Reward must directly reflect target metric; bounded slowdown uses `max((w_j + e_j) / max(e_j, 10), 1)` to avoid short-job bias

- **Failure signatures**:
  - **Gradient staleness**: Learning curves show high variance or divergence; may indicate sync interval too long
  - **Poor generalization**: Strong training performance but degraded metrics on SDSC-SP2 or Lublin-256; suggests overfitting to Falcon/Lemhi patterns
  - **Resource utilization underperformance**: Table 3 shows PPO slightly edges DD-PPO on utilization (0.61 vs. 0.65) on Lublin-256; may indicate reward function not fully aligned with utilization objective
  - **Hyperparameter sensitivity**: DD-PPO acknowledged to have "greater sensitivity to hyperparameters" (Section 4.1); PBT failure can manifest as unstable episode rewards

- **First 3 experiments**:
  1. **Baseline reproduction**: Run DD-PPO vs. PPO vs. FCFS/SJF/F1 on Lublin-256 (1,024 jobs, 10 iterations) measuring average bounded slowdown; verify Table 3 results
  2. **Ablation study**: Disable PBT, then disable fine-tuning, then both; compare reward curves and final metrics against full configuration (replicate Figures 7-8)
  3. **Scaling analysis**: Vary worker count (e.g., 2, 4, 8, 16) while holding total samples constant; measure wall-clock training time and final policy performance to assess near-linear scaling claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training other RL algorithms (e.g., DQN, SAC) on the 11.5 million job trace dataset yield similar generalization improvements as seen with DD-PPO?
- **Basis in paper:** [explicit] The conclusion states future work involves "utilizing our large dataset on other RL HPC scheduling algorithms to quantify the impact on their performance."
- **Why unresolved:** The study focused exclusively on comparing DD-PPO against standard PPO and rule-based methods, leaving the scalability of other RL architectures unexplored.
- **What evidence would resolve it:** Comparative benchmarks of alternative RL algorithms trained on the same large-scale dataset.

### Open Question 2
- **Question:** Can the DD-PPO scheduler effectively transfer learned policies to live, heterogeneous HPC systems?
- **Basis in paper:** [inferred] The introduction posits heterogeneity as a key challenge for rule-based systems, yet the training data (Falcon/Lemhi) is described as coming from "homogeneous systems," and validation is conducted solely in simulation.
- **Why unresolved:** The "sim-to-real" gap is not addressed; the model's ability to handle real-world system noise, failures, or hardware heterogeneity absent from the training logs remains unproven.
- **What evidence would resolve it:** Performance metrics collected from a live deployment on a heterogeneous cluster.

### Open Question 3
- **Question:** What modifications are required to ensure DD-PPO consistently outperforms PPO in resource utilization across diverse datasets?
- **Basis in paper:** [inferred] The authors note in the results that for the SDSC-SP2 dataset, "the PPO algorithm slightly outperforms our method" regarding resource utilization.
- **Why unresolved:** While DD-PPO excels in wait time and slowdown, the specific dynamics causing it to lag in utilization on certain workloads are not analyzed.
- **What evidence would resolve it:** An ablation study focusing on reward shaping specifically for utilization on the SDSC-SP2 trace.

## Limitations

- **Simulation-only validation**: The method is validated exclusively in simulation environments, leaving the "sim-to-real" gap unaddressed for deployment on actual HPC systems
- **Potential overfitting to training distribution**: Despite using large-scale traces, the method may not generalize well to HPC environments with significantly different job characteristics or hardware configurations
- **Hyperparameter sensitivity**: DD-PPO exhibits greater sensitivity to hyperparameters compared to standard PPO, requiring careful tuning and potentially limiting practical applicability

## Confidence

- **Decentralized gradient synchronization effectiveness**: High confidence based on mathematical framework and empirical validation
- **Large-scale dataset generalization benefits**: Medium confidence due to reliance on historical workload representativeness
- **PPO clipping stability mechanism**: High confidence anchored in the mathematical clipping framework
- **Near-linear scaling with worker count**: High confidence for tested scale, but less validated across diverse cluster sizes

## Next Checks

1. **Distributional Robustness**: Test DD-PPO on synthetic or real workloads with deliberately altered job characteristics (e.g., longer average runtimes, different arrival rates) to assess policy transfer beyond training distribution.

2. **Synchronization Stress Test**: Systematically vary the gradient synchronization interval and measure its impact on training stability and final policy performance to identify the threshold for gradient staleness effects.

3. **Architecture Sensitivity**: Compare DD-PPO performance using alternative network architectures (e.g., wider layers, different depths) to determine if the current design is critical or if simpler models suffice.