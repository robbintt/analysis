---
ver: rpa2
title: 'AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem
  Solving'
arxiv_id: '2510.21436'
source_url: https://arxiv.org/abs/2510.21436
tags:
- optimization
- mathematical
- problem
- problems
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AutoOpt, an end-to-end automated framework
  for solving optimization problems from images. The core innovation is a three-module
  pipeline: M1 uses a hybrid CNN-Transformer model to convert images into LaTeX, M2
  fine-tunes a small LLM to generate PYOMO code from LaTeX, and M3 employs a bilevel
  optimization-based decomposition method (BOBD) to solve the generated model.'
---

# AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving

## Quick Facts
- **arXiv ID:** 2510.21436
- **Source URL:** https://arxiv.org/abs/2510.21436
- **Authors:** Ankur Sinha; Shobhit Arora; Dhaval Pujara
- **Reference count:** 40
- **Primary result:** 94.20% success rate on held-out optimization problems using a three-module pipeline (Image→LaTeX→PYOMO→solution)

## Executive Summary
This paper introduces AutoOpt, an end-to-end automated framework for solving optimization problems from images. The core innovation is a three-module pipeline: M1 uses a hybrid CNN-Transformer model to convert images into LaTeX, M2 fine-tunes a small LLM to generate PYOMO code from LaTeX, and M3 employs a bilevel optimization-based decomposition method (BOBD) to solve the generated model. The framework is trained and evaluated using AutoOpt-11k, a novel dataset of over 11,000 optimization problems with handwritten and typeset images, LaTeX, and PYOMO representations. The M1 module achieves a BLEU score of 96.70 and a character error rate of 0.0286, outperforming ChatGPT, Gemini, and Nougat. BOBD consistently delivers high-quality solutions across diverse test problems, outperforming standard IP and GA methods. The framework achieves a 94.20% success rate on held-out problems, demonstrating its effectiveness in automating the full optimization pipeline with minimal human intervention.

## Method Summary
AutoOpt is an end-to-end automated pipeline converting optimization problem images to solutions via three modules: M1 (Image→LaTeX) uses a hybrid CNN-Transformer encoder and mBART decoder; M2 (LaTeX→PYOMO) fine-tunes DeepSeek-Coder 1.3B on instruction-style pairs; M3 (PYOMO→solution) employs a bilevel optimization-based decomposition method (BOBD) with GA upper level and convex solver lower level. The framework is trained on AutoOpt-11k (11,554 images: 5,070 handwritten, 6,484 typeset) with LaTeX labels and 1,018 with PYOMO scripts, split 80/10/10 (train/val/test).

## Key Results
- M1 module achieves BLEU score of 96.70 and character error rate of 0.0286, outperforming ChatGPT, Gemini, and Nougat
- AutoOpt-M2 achieves BLEU 88.25 and CER 0.0825 on LaTeX→PYOMO translation
- BOBD consistently outperforms IP and GA across 10 test problems (TP1-TP10) with various complexities
- Framework achieves 94.20% success rate on held-out problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hybrid CNN-Transformer encoder outperforms either architecture alone for Mathematical Expression Recognition (MER) from optimization problem images.
- **Mechanism:** ResNet-101 captures local fine-grained features (symbol shapes, stroke patterns) while Swin Transformer captures long-range spatial dependencies (superscripts, subscripts, matrices). These streams are fused via a learned gating scalar α, prepended to patch embeddings, and decoded by an mBART autoregressive decoder initialized from NOUGAT weights.
- **Core assumption:** Mathematical programs require both local symbol recognition and global 2D spatial understanding, which single-architecture models cannot jointly optimize.
- **Evidence anchors:**
  - [section 3.1] Ablation study shows DL3 (CNN+Transformer) achieves BLEU 96.70 and CER 0.0286, vs. DL1 (CNN only) BLEU 16.10/CER 0.8812 and DL2 (Transformer only) BLEU 95.51/CER 0.0440.
  - [abstract] "M1 module achieves a BLEU score of 96.70 and a character error rate of 0.0286, outperforming ChatGPT, Gemini, and Nougat."
  - [corpus] VisTIRA paper confirms VLMs underperform on visual math due to modality gap, validating the need for specialized architectures.
- **Break condition:** Performance degrades significantly if either CNN or Transformer branch is removed, or if the gating mechanism α remains at zero initialization (no fusion).

### Mechanism 2
- **Claim:** A small fine-tuned LLM (1.3B parameters) can reliably translate LaTeX optimization formulations into executable PYOMO code when trained on aligned LaTeX-PYOMO pairs.
- **Mechanism:** DeepSeek-Coder 1.3B is fine-tuned on instruction-style data mapping LaTeX to PYOMO scripts. The two-stage design (Image→LaTeX→PYOMO) provides a human-verifiable intermediate checkpoint, reducing error propagation.
- **Core assumption:** The semantic structure of optimization problems (variables, objectives, constraints) is sufficiently regular that modest model scale with targeted fine-tuning outperforms prompting larger general-purpose LLMs.
- **Evidence anchors:**
  - [section 3.2] AutoOpt-M2 achieves BLEU 88.25 and CER 0.0825 on 80/20 split of 1,018 LaTeX-PYOMO pairs.
  - [section 3] "A two-step design enhances the ease of verification, as the intermediate LaTeX output serves as a human-readable checkpoint."
  - [corpus] OR-LLM-Agent and Table2LaTeX-RL papers similarly show task-specific fine-tuning outperforms prompting, but neither addresses full optimization pipelines.
- **Break condition:** Performance collapses if training data lacks semantic diversity (e.g., only linear problems), or if the LaTeX-to-code mapping requires domain knowledge not captured in 1,018 samples.

### Mechanism 3
- **Claim:** Decomposing a single-level optimization problem into a bilevel structure via learned variable classification allows simultaneous use of exact solvers (for convex subproblems) and metaheuristics (for non-convex global search).
- **Mechanism:** A Logistic Regression Variable Classification Model (LR-VCM) learns which variables to fix at the upper level (sampled via Genetic Algorithm) vs. optimize at the lower level (solved via interior-point or LP methods). The decomposition is re-learned periodically during optimization.
- **Core assumption:** Many complex optimization problems have latent decomposable structure where fixing some variables renders the subproblem tractable for classical solvers.
- **Evidence anchors:**
  - [section 3.3, Appendix D] BOBD consistently outperforms IP and GA across 10 test problems (TP1-TP10) with various complexities, achieving near-zero deviation in 11 runs while IP/GA often converge to local optima or infeasible solutions.
  - [section 3.3] "BOBD method, which is a hybrid approach, yields better results on complex test problems compared to common approaches, like interior-point algorithm and genetic algorithm."
  - [corpus] Corpus lacks direct comparisons to bilevel decomposition methods; related work focuses on LLM-based solver generation, not hybrid algorithmic decomposition.
- **Break condition:** Performance degrades if variables are misclassified (LR-VCM fails), or if the problem lacks exploitable decomposable structure (e.g., fully coupled non-convex objectives).

## Foundational Learning

- **Concept:** Mathematical Expression Recognition (MER)
  - **Why needed here:** Module M1 must handle 2D spatial relationships (subscripts, fractions, matrices) that standard OCR cannot capture.
  - **Quick check question:** Can you explain why a line-by-line OCR like Tesseract fails on multi-line mathematical formulations with nested structures?

- **Concept:** Transfer Learning with Vision-Language Models
  - **Why needed here:** AutoOpt-M1 initializes ResNet from ImageNet and Swin/mBART from NOUGAT weights, leveraging pre-trained representations for faster convergence on the smaller AutoOpt-11k dataset.
  - **Quick check question:** What is the risk of using NOUGAT weights (trained on scientific documents) for handwritten mathematical expressions?

- **Concept:** Bilevel Optimization
  - **Why needed here:** Module M3 reformulates single-level problems into nested upper/lower structures, requiring understanding of how variable classification affects tractability.
  - **Quick check question:** Given an optimization problem with mixed convex/non-convex terms, how would you decide which variables belong to the upper vs. lower level?

## Architecture Onboarding

- **Component map:** Image preprocessing (resize to 768×1024, contrast enhancement, unsharp masking) → Hybrid encoder (ResNet-101 + Swin Transformer with learnable gating α) → mBART decoder → LaTeX output → DeepSeek-Coder 1.3B (fine-tuned) → PYOMO script → LR-VCM (variable classification) → GA (upper level sampling) + Convex solver (lower level) → Solution

- **Critical path:** M1 accuracy is the bottleneck; LaTeX errors propagate to M2 and M3. The 94.20% framework success rate is constrained by M1's 97.14% reliability × M2's 91.75% reliability.

- **Design tradeoffs:**
  - Two-stage M1→M2 vs. end-to-end image-to-PYOMO: Two-stage enables human verification but adds latency and potential error accumulation.
  - Small LLM (1.3B) vs. large: Smaller model enables deployment but limits generalization to out-of-distribution problem types.
  - BOBD vs. pure GA or IP: BOBD is slower (seconds to minutes vs. seconds for IP) but consistently finds better solutions on complex problems.

- **Failure signatures:**
  - M1 fails on heavily degraded handwritten images or unusual notation styles not in AutoOpt-11k.
  - M2 produces syntactically correct but semantically wrong PYOMO if LaTeX omits critical problem details (e.g., parameter values).
  - M3 returns infeasible solutions if LR-VCM misclassifies variables or the problem lacks decomposable structure.

- **First 3 experiments:**
  1. **Reproduce M1 ablation:** Train DL1 (CNN-only), DL2 (Transformer-only), and DL3 (hybrid) on AutoOpt-11k subset to verify BLEU/CER improvements from fusion.
  2. **Test M2 generalization:** Fine-tune M2 on linear problems only, then evaluate on non-linear problems to measure out-of-distribution degradation.
  3. **Stress test M3 decomposition:** Apply BOBD to a fully coupled non-convex problem (no latent structure) and compare solution quality/compute time against pure GA and IP.

## Open Questions the Paper Calls Out

- **Question:** How can the framework be adapted to effectively handle ill-defined or incomplete optimization problems?
  - **Basis in paper:** [explicit] Section 4 explicitly lists "handling ill-defined optimization problems effectively" as a key limitation and a direction for future research.
  - **Why unresolved:** The current pipeline requires a mathematically complete formulation to generate executable PYOMO code; ill-defined problems lack the necessary structure for the M2 and M3 modules to function without error.
  - **What evidence would resolve it:** A module capable of inference or user-interaction to resolve ambiguities (e.g., missing constraints) before code generation, validated on a dataset of incomplete formulations.

- **Question:** Can the AutoOpt framework process optimization problem definitions that span multiple pages or images?
  - **Basis in paper:** [explicit] Section 4 identifies "handling optimization problem definitions that span multiple pages or images" as a specific limitation to be addressed in future work.
  - **Why unresolved:** The current M1 module (Image_to_Text) is designed to process single images, lacking the context-aggregation mechanisms required to stitch together fragmented mathematical formulations.
  - **What evidence would resolve it:** An extension of the M1 architecture that utilizes multi-modal context or page-ordering to generate a unified LaTeX representation from a sequence of images.

- **Question:** Does the Bilevel Optimization based Decomposition (BOBD) method outperform specialized solvers for specific complex problem classes?
  - **Basis in paper:** [inferred] Section 3.3 states the authors "do not make any claims of our BOBD implementation being better than any specialized optimization technique," despite outperforming general IP and GA methods.
  - **Why unresolved:** The evaluation is limited to general-purpose baselines (Interior Point, Genetic Algorithms), leaving the performance gap between BOBD and state-of-the-art problem-specific solvers unknown.
  - **What evidence would resolve it:** Benchmark comparisons of the BOBD module against specialized solvers (e.g., dedicated non-convex or mixed-integer solvers) on the test problems TP1-TP10.

## Limitations

- The framework's performance depends heavily on the quality and diversity of the AutoOpt-11k dataset, which may not capture all real-world optimization problem variations
- The two-stage design introduces error propagation that compounds across modules, with no mechanism to recover from errors in earlier stages
- The BOBD method's effectiveness relies on exploitable decomposable structure that may not exist in all optimization problems

## Confidence

**High Confidence Claims:**
- The hybrid CNN-Transformer architecture for MER significantly outperforms single-architecture approaches, as evidenced by the ablation study (BLEU 96.70 vs 16.10 and 95.51) and comparison with general-purpose VLMs like ChatGPT and Gemini.
- The fine-tuned DeepSeek-Coder 1.3B model reliably translates LaTeX to PYOMO code for problems within the training distribution, achieving BLEU 88.25 and CER 0.0825.
- BOBD consistently outperforms standard IP and GA methods on the 10 test problems, demonstrating the effectiveness of hybrid decomposition approaches for complex optimization.

**Medium Confidence Claims:**
- The framework's 94.20% success rate on held-out problems represents robust end-to-end performance, though this metric depends heavily on the specific problem distribution in the test set.
- The preprocessing pipeline (resize to 768×1024, contrast enhancement, unsharp masking) is sufficient for handling the image quality variation in AutoOpt-11k, but may not generalize to all real-world scenarios.

**Low Confidence Claims:**
- The framework's generalization to optimization problems outside the AutoOpt-11k distribution (e.g., problems with novel notation, extremely large scale, or different mathematical structures) remains untested.
- The scalability of BOBD to problems with thousands of variables or constraints is not evaluated, raising questions about computational tractability in real-world applications.

## Next Checks

1. **Distribution Shift Test:** Evaluate AutoOpt on a held-out set of optimization problems from different domains (e.g., operations research textbooks, academic papers) that were not used in training. Measure performance degradation as a function of domain distance from AutoOpt-11k to quantify generalization limits.

2. **Extreme Scale Evaluation:** Apply AutoOpt to optimization problems with significantly larger scale (e.g., 1000+ variables, 5000+ constraints) than those in AutoOpt-11k. Measure computational runtime, memory usage, and solution quality to assess practical scalability constraints.

3. **Robustness to Image Quality Variation:** Systematically degrade image quality (blur, noise, low resolution) for test problems in AutoOpt-11k and measure M1 performance degradation. This would reveal the framework's sensitivity to real-world image capture conditions and inform requirements for input preprocessing or model robustness improvements.