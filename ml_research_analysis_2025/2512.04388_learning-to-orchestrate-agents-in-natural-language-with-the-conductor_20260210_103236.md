---
ver: rpa2
title: Learning to Orchestrate Agents in Natural Language with the Conductor
arxiv_id: '2512.04388'
source_url: https://arxiv.org/abs/2512.04388
tags:
- conductor
- performance
- arxiv
- reasoning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Conductor, a 7B language model trained\
  \ via reinforcement learning to automatically coordinate and prompt-engineer a pool\
  \ of powerful LLM workers. By outputting natural-language workflows that specify\
  \ subtasks, worker assignments, and information access, the Conductor learns to\
  \ combine diverse models\u2019 strengths through emergent strategies like verification\
  \ and refinement."
---

# Learning to Orchestrate Agents in Natural Language with the Conductor

## Quick Facts
- arXiv ID: 2512.04388
- Source URL: https://arxiv.org/abs/2512.04388
- Reference count: 40
- This paper introduces the Conductor, a 7B language model trained via reinforcement learning to automatically coordinate and prompt-engineer a pool of powerful LLM workers.

## Executive Summary
This paper introduces the Conductor, a 7B language model trained via reinforcement learning to automatically coordinate and prompt-engineer a pool of powerful LLM workers. By outputting natural-language workflows that specify subtasks, worker assignments, and information access, the Conductor learns to combine diverse models' strengths through emergent strategies like verification and refinement. On challenging reasoning benchmarks (LiveCodeBench, GPQA), the Conductor achieves state-of-the-art results, surpassing both individual workers and prior multi-agent baselines at a fraction of the cost. Extensions allow the Conductor to adapt to arbitrary agent pools and employ recursive topologies for test-time scaling. The work demonstrates that small models can effectively orchestrate collective intelligence, opening new avenues for collaborative AI systems.

## Method Summary
The Conductor is a 7B language model trained via GRPO (a variant of PPO) to output natural language specifications of workflows that coordinate multiple worker LLMs. It generates structured outputs containing model IDs, subtasks, and access lists, which are parsed and executed to solve reasoning problems. Training uses binary correctness rewards without intermediate supervision, allowing strategies like verification and refinement to emerge naturally. The approach supports arbitrary worker pools, recursive self-calling for test-time scaling, and both in-domain and out-of-distribution evaluation across math, code, and science benchmarks.

## Key Results
- On LiveCodeBench, the Conductor achieves 50.7% accuracy, outperforming best individual workers (43.2%) and prior multi-agent baselines (42.7%)
- On GPQA-Diamond, the Conductor reaches 34.4% accuracy, surpassing both individual workers (26.8%) and prior methods (32.3%)
- The Conductor delivers state-of-the-art performance across all 7 reasoning benchmarks while using only 7B parameters and a fraction of the compute cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end RL enables discovery of coordination strategies without manual design
- Mechanism: The Conductor model (7B parameters) outputs natural language specifications of workflows—subtasks, worker assignments, and communication topologies—and receives binary reward signals based on final answer correctness. Through GRPO optimization, strategies like task decomposition, verification loops, and role specialization emerge as high-reward behaviors.
- Core assumption: The reward signal from final answer correctness provides sufficient gradient information for the model to learn complex multi-step coordination strategies.
- Evidence anchors:
  - [abstract] "powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization"
  - [section 3.1] "Training the Conductor with this simple recipe, we observe the emergence of problem breakdowns and prompt-engineered subtasks that match the strengths of each worker"
  - [corpus] Related work HCPO (hierarchical conductor-based policy optimization) provides complementary evidence that conductor-based coordination improves multi-agent RL, though in a different domain

### Mechanism 2
- Claim: Natural language as the output medium enables unconstrained workflow specification
- Mechanism: Rather than selecting from predefined topologies or routing decisions, the Conductor generates free-form text parsed into three lists (model IDs, subtasks, access lists). This allows arbitrary DAG structures, custom prompts per worker, and adaptive strategies unavailable to classifier-based routers.
- Core assumption: The base LLM's language generation capabilities transfer to producing structured, executable workflow specifications.
- Evidence anchors:
  - [abstract] "learns not only to design targeted communication topologies... but also to prompt engineer focused instructions"
  - [section 3.1] "This design lets the Conductor freely craft tailored subtasks and communication strategies across its workers, allowing the specification of agentic workflows ranging from simple best-of-N and sequential chain-like topologies to parallelizable arbitrary tree-structured approaches"
  - [corpus] Related work on multi-agent LLM grounding notes natural language enables interpretable coordination, but introduces overhead; corpus evidence on expressiveness vs. structure tradeoffs is limited

### Mechanism 3
- Claim: Recursive self-calling provides tunable test-time compute scaling
- Mechanism: The Conductor can designate itself as a worker, receiving its previous workflow output as context. This enables iterative refinement where the model critiques and revises its own orchestration strategy, allocating more compute to challenging problems.
- Core assumption: The Conductor has sufficient meta-reasoning to recognize when prior coordination was suboptimal and can productively revise.
- Evidence anchors:
  - [abstract] "allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling"
  - [section 4.4, Table 2] Conductor-Recursive improves from 37.8 to 40.0 on BigCodeBench; Figure 10 shows adaptive redistribution of worker selection in recursive rounds
  - [corpus] Limited direct evidence; related work on test-time scaling focuses on single-model chain-of-thought rather than recursive multi-agent orchestration

## Foundational Learning

- Concept: **Reinforcement learning with verifiable rewards (RLVR)**
  - Why needed here: The Conductor training uses GRPO with correctness-based rewards; understanding policy gradients, advantage estimation, and reward shaping is essential to debug training dynamics.
  - Quick check question: Given a batch of 64 rollouts per question with rewards [1, 0.5, 1, 0.5, ...], how would you compute the advantage for each sample?

- Concept: **Multi-agent coordination patterns**
  - Why needed here: The Conductor must discover strategies like sequential planning→execution→verification, parallel exploration with aggregation, and role specialization; familiarity with these patterns helps interpret learned behaviors.
  - Quick check question: For a code generation task requiring both algorithm design and strict output formatting, what coordination topology might balance correctness and constraint satisfaction?

- Concept: **Prompt engineering and instruction following**
  - Why needed here: The Conductor's primary output is prompt-engineered subtasks for workers; understanding how instructions shape LLM behavior is critical to evaluating whether discovered strategies are sensible.
  - Quick check question: If a worker consistently fails at verification tasks despite correct reasoning, what prompt modifications might improve reliability?

## Architecture Onboarding

- Component map:
  - Conductor model (7B LLM) -> Workflow executor -> Worker pool (7 LLMs) -> Reward compute -> GRPO update

- Critical path:
  1. System prompt design -> defines output format and few-shot examples
  2. Rollout generation -> Conductor produces workflows, workers execute
  3. Reward computation -> final answer matching against ground truth
  4. GRPO update -> advantage-normalized policy gradient with optional KL penalty
  5. Evaluation -> in-domain (MATH500, MMLU, RLPR, LiveCodeBench) and OOD (AIME25, GPQA-Diamond, BigCodeBench)

- Design tradeoffs:
  - Workflow length limit (5 steps): Constrains compute cost but may truncate useful strategies; paper finds average converged length ~3 steps
  - Worker pool randomization during training: Improves generalization to arbitrary pools but slows convergence on any specific pool
  - OOD few-shot examples vs. in-distribution: Paper finds OOD examples improve exploration by preventing reward hacking; counterintuitive but empirically validated (Appendix B.2)
  - Constrained vs. unconstrained evaluation: Constraining worker reasoning budgets (4096 tokens, minimal reasoning) reduces cost but lowers individual worker performance; Conductor's relative advantage increases

- Failure signatures:
  - Format parse failures: Reward set to 0; high initial rates expected, should decrease rapidly
  - Worker context overflow: Access list "all" with 5 workers × 4096 tokens exceeds context; paper uses constrained setting to mitigate
  - Reward hacking: Model discovers trivial strategies (e.g., always select single best worker) that achieve moderate reward without learning coordination; addressed by OOD few-shot conditioning
  - Recursive non-termination: Without max recursion depth, model may loop; paper sets explicit limits

- First 3 experiments:
  1. Coldstart validation: Train on 100 samples with verbose logging of parsed workflows; verify format compliance reaches >90% within 20 iterations and that subtasks are non-trivial (not just "solve the problem")
  2. Worker ablation: Fix all worker selections to GPT-5 only; compare performance to full pool access. Expect degraded performance on tasks where GPT-5 is suboptimal (BigCodeBench, GPQA-Diamond per Table 10), confirming coordination value
  3. OOD generalization test: Train on MATH+MMLU only, evaluate zero-shot on LiveCodeBench. Target: >50% of full-training performance, indicating transfer of coordination strategies across domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Conductor framework effectively coordinate workers across multiple modalities (vision, robotics, biology) while using natural language as the unifying interface?
- Basis in paper: [explicit] The authors state: "an exciting, unexplored extension is to go beyond LLMs alone, introducing workers with expertise in other modalities...allowing the Conductor to use natural language as an expressive unifying interface and tackle increasingly ambitious human challenges in fields such as biology, robotics, and beyond."
- Why unresolved: All experiments in this work use only text-based LLM workers; no multi-modal coordination was tested.
- What evidence would resolve it: Demonstrating Conductor performance on multi-modal benchmarks (e.g., VQA, embodied AI tasks) with specialized non-text workers in the pool.

### Open Question 2
- Question: Does allowing fine-grained topology control (specifying arbitrary visibility per agent) yield meaningful performance gains with larger Conductor models?
- Basis in paper: [explicit] Appendix B.8 notes: "We leave revisiting the complex, fine-grained control over coordination topology to future work, where we hope that with a larger and more intelligent Conductor beyond 7B, additional performance gains may be unlocked through discovery of powerful topologies."
- Why unresolved: The 7B Conductor showed no significant gains from fine-grained control, but the authors hypothesize larger models may benefit.
- What evidence would resolve it: Training a 14B+ Conductor with the fine-grained access list scheme and comparing against the binary all/[] scheme.

### Open Question 3
- Question: How does Conductor performance scale with the size and diversity of the training dataset beyond the 960 problems used?
- Basis in paper: [inferred] The training dataset comprises only 960 problems from four domains, and convergence was rapid (200 iterations). The relationship between training scale and emergent coordination strategies is unexplored.
- Why unresolved: Small training sets may limit the diversity of coordination strategies learned, potentially leaving performance gains unrealized.
- What evidence would resolve it: Systematic evaluation of Conductor performance when trained on 5K, 10K, and 50K problems with analysis of strategy diversity.

## Limitations

- The approach requires access to multiple powerful worker models, limiting applicability when only a single model is available
- Fixed 5-step workflow limit could truncate optimal strategies for complex problems
- Reliance on binary correctness rewards may limit learning of nuanced coordination strategies when intermediate verification is valuable but not explicitly rewarded

## Confidence

- **High** confidence in observed performance improvements over baselines, supported by extensive benchmarking across 7 reasoning tasks
- **Medium** confidence in emergence of sophisticated coordination strategies (verification, refinement, role specialization), as these behaviors are inferred from output analysis
- **Medium-Low** confidence in OOD generalization and cost-effectiveness claims, as these depend heavily on specific worker pools and evaluation conditions

## Next Checks

1. **Transfer to novel worker pools**: Evaluate Conductor trained on the 7-worker pool when given access to a different set of 3-5 workers with varying capabilities. Target: >50% accuracy on held-out reasoning tasks without retraining.

2. **Zero-shot reasoning transfer**: Test Conductor on completely unseen problem domains (e.g., chemistry, logic puzzles) where no training data existed. Target: 30% accuracy on a held-out benchmark with no fine-tuning.

3. **Resource-constrained deployment**: Measure performance when each worker has limited reasoning budget (e.g., 2048 tokens instead of 4096) and when only 2-3 workers are available. Target: Relative performance advantage maintained within 10% of full-ensemble results.