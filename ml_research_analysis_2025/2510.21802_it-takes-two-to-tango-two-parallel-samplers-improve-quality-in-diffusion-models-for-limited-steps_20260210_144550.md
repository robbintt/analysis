---
ver: rpa2
title: 'It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion
  Models for Limited Steps'
arxiv_id: '2510.21802'
source_url: https://arxiv.org/abs/2510.21802
tags:
- image
- steps
- quality
- diffusion
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SE2P, a simple method to improve the quality
  of images generated by diffusion models when using a limited number of denoising
  steps. The method uses two parallel processors that run the denoising process with
  consecutive time steps, and integrates their latent predictions through a convex
  combination weighted by a mixing parameter and a variance scaling factor.
---

# It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion Models for Limited Steps

## Quick Facts
- **arXiv ID:** 2510.21802
- **Source URL:** https://arxiv.org/abs/2510.21802
- **Reference count:** 40
- **Primary result:** SE2P improves image quality for diffusion models under limited denoising steps (10-100) using two parallel processors

## Executive Summary
This paper proposes SE2P, a simple method to improve the quality of images generated by diffusion models when using a limited number of denoising steps. The method uses two parallel processors that run the denoising process with consecutive time steps, and integrates their latent predictions through a convex combination weighted by a mixing parameter and a variance scaling factor. This approach is plug-and-play, model-agnostic, and does not require additional fine-tuning or external models. The method was tested on multiple diffusion models (DDPM, Latent Diffusion, Diffusion Transformers, and Stable Diffusion) across different settings (pixel/latent space, conditional/unconditional) and step counts (10-100).

## Method Summary
SE2P operates by running two parallel denoising processors at consecutive timesteps (t_k and t_k+1) during the diffusion sampling process. Processor 0 computes a prediction estimate from its output at t_k+1 and projects it back to step t_k. This predictor is then convexly combined with Processor 1's latent at step t_k using mixing parameter γ. The method introduces a variance scaling factor ρ that modulates the noise injected in the predictor. Both processors start with identical initial noise and share random seeds during parallel denoising. The integration step is performed at each denoising iteration, and the method requires 2× model evaluations per step but no additional training.

## Key Results
- SE2P consistently improves image quality metrics (MUSIQ, CLIP-IQA, NIMA) across multiple diffusion models and step counts
- Qualitative analysis shows improvements in contrast, brightness, and sharpness compared to baseline
- Human evaluation (33 participants, 25 pairwise comparisons per model) confirms SE2P outperforms baseline in most cases
- Naive integration of latents without prediction leads to quality loss, validating the predictive integration mechanism
- Adding more than two processors does not improve results and can degrade quality

## Why This Works (Mechanism)

### Mechanism 1: Predictive Latent Integration
Integrating a *predicted* latent (from step t_k+1 projected to step t_k) with the current latent improves image quality compared to naive direct integration. Processor 0 computes a prediction estimate x̂_0 from the noisy latent at t_k+1, then uses this to construct x̂_pred at step t_k. This predictor is convexly combined with the latent from Processor 1 at step t_k via mixing parameter γ. The prediction step introduces structured information about the denoising trajectory that direct averaging lacks.

### Mechanism 2: Variance Scaling as Quality Modulator
Scaling the injected noise variance (by factor ρ²) in the predictor enables control over image quality attributes like contrast and sharpness. The x̂_pred computation includes a noise term ρ · √β̃_t_k · ε. Increasing ρ beyond 1 amplifies stochasticity in the integration step, which correlates with increased contrast and vividness. Decreasing ρ is necessary at higher step counts to avoid degradation.

### Mechanism 3: Two-Processor Sufficiency
Two parallel processors capture the useful integration signal; adding more processors introduces destructive interference rather than cumulative benefit. The integration between just two consecutive steps (t_k and t_k+1) provides a local correction signal. Extending to P processors integrates predictions across wider time gaps, which leads to quality degradation rather than improvement.

## Foundational Learning

- **DDPM Denoising Process (Eq. 2)**: SE2P operates entirely within the DDPM scheduler framework; understanding x_{t-1} = f(x_t, ε_θ, t) is prerequisite to understanding where the integration happens.
  - *Quick check*: Can you explain why the denoising step requires both the model prediction ε_θ(x_t, t) AND injected noise?

- **Jump Sampling / Step Scheduling**: The method is explicitly designed for "limited denoising steps" (N ≪ 1000) with approximately equally spaced timesteps.
  - *Quick check*: If N=10 with T=1000 total diffusion steps, what are the denoising timesteps?

- **x̂_0 Estimation from Noisy Latent**: The predictor x̂_pred is derived from an estimate of the clean image x̂_0. This is a standard DDPM operation but central to SE2P's mechanism.
  - *Quick check*: Given x_t at timestep t, how would you estimate x_0?

## Architecture Onboarding

- **Component map**: Processor 0 -> Processor 1 -> Integration -> Output
- **Critical path**:
  1. Initialize both processors with same x_N-1 ~ N(0, I)
  2. For each denoising step k (in parallel): compute v^(j) = ε_θ(x_k^(j), t_k^(j)) for j ∈ {0,1}
  3. Compute x̂_0 from Processor 0's output (reuses stored v^(0))
  4. Construct x̂_pred = μ̃_pred + ρ · √β̃ · ε
  5. Integrate: x_k^(1) = γ · x_k^(1) + (1-γ) · x̂_pred
  6. Synchronize: x_k^(0) = x_k^(1)
  7. Continue to step k-1

- **Design tradeoffs**:
  - Compute vs. quality: Requires 2× model evaluations per step (parallelizable), no quality gain in high-step regime
  - ρ tuning burden: Optimal ρ varies by model architecture AND step count; no closed-form solution provided
  - Model agnosticism: Works across U-Net, DiT, pixel-space, latent-space, conditional/unconditional—BUT effectiveness varies

- **Failure signatures**:
  - Oversaturation/brightness blowout: ρ too high for step count
  - Image washout/blur: γ too low (predictor dominates) OR using naive integration
  - Semantic drift: More pronounced at higher step counts; SE2P can change image content
  - Artifact emergence: Particularly in DiT with high ρ values

- **First 3 experiments**:
  1. Baseline comparison at N=20: Run standard DDPM scheduler vs. SE2P with ρ=1.35, γ=0.015 on CelebA-HQ. Measure MUSIQ/CLIP-IQA scores and visually inspect contrast/brightness changes.
  2. Ablation validation: Implement naive integration (direct convex combination of latents without predictor) and confirm quality degradation matches paper's Figure 8/9 pattern.
  3. Step-count sensitivity: Test ρ ∈ {0.5, 1.0, 1.35, 1.55} at N ∈ {10, 20, 40, 80, 100} on a single model. Identify where SE2P gains diminish and where ρ recalibration is needed.

## Open Questions the Paper Calls Out

- **Theoretical underpinnings**: What are the theoretical underpinnings explaining how the integration of a latent and its prediction enhances sample quality, and why does this lead to increased semantic changes at higher step counts?
- **Non-image modalities**: Can the SE2P method be effectively adapted for diffusion models operating on non-image modalities such as video, 3D objects, or audio?
- **Adaptive variance scaling**: Is there a dynamic or adaptive mechanism for setting the variance scaling parameter (ρ) that maintains quality improvements without requiring manual tuning for different step counts?

## Limitations

- Variance scaling parameter ρ requires careful per-model, per-step-count tuning with no theoretical guidance
- Performance gains are inconsistent across architectures, with Stable Diffusion showing the weakest improvements
- Human evaluation sample size (33 participants) is relatively small
- Method can cause semantic drift in generated images, particularly at higher step counts
- No benefit in high-step regimes and requires additional computational overhead

## Confidence

- **High Confidence**: The core mechanism of predictive latent integration is well-supported by the ablation study showing naive integration degrades quality
- **Medium Confidence**: The variance scaling mechanism is supported by empirical parameter tuning but lacks theoretical justification
- **Low Confidence**: The human evaluation findings are based on a limited sample size and single pairwise comparison study

## Next Checks

1. **Ablation validation**: Implement and test the naive integration baseline (direct latent averaging without predictor) to confirm it degrades quality as claimed
2. **Variance scaling sensitivity**: Systematically test ρ across a broader range of step counts (10-100) on a single model to map the degradation boundary
3. **Semantic drift quantification**: Measure content changes between baseline and SE2P outputs using CLIP similarity to systematically quantify the observed semantic drift effect