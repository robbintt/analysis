---
ver: rpa2
title: In-Context Semi-Supervised Learning
arxiv_id: '2512.15934'
source_url: https://arxiv.org/abs/2512.15934
tags:
- transformer
- manifolds
- learning
- manifold
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces in-context semi-supervised learning (IC-SSL),
  where a Transformer uses both labeled and unlabeled contextual data to learn geometry-aware
  representations for prediction. The authors propose a two-stage end-to-end Transformer:
  the first stage constructs a discrete Laplacian and computes an Eigenmap from unlabeled
  data, while the second stage performs in-context learning for categorical outcomes
  via gradient descent.'
---

# In-Context Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2512.15934
- Source URL: https://arxiv.org/abs/2512.15934
- Reference count: 40
- Primary result: A two-stage Transformer learns geometry-aware representations from both labeled and unlabeled data, achieving state-of-the-art semi-supervised learning on manifold-structured datasets.

## Executive Summary
This paper introduces in-context semi-supervised learning (IC-SSL), where a Transformer leverages both labeled and unlabeled contextual data to learn geometry-aware representations for prediction. The proposed approach consists of two stages: first, constructing a discrete Laplacian and computing its Eigenmap from unlabeled data using RBF and linear attention layers; second, performing in-context learning for categorical outcomes via functional gradient descent implemented through attention. Across synthetic manifolds, product manifolds, and high-dimensional image manifolds generated with Stable Diffusion, the model consistently outperforms strong baselines, especially in low-label regimes, and demonstrates robust out-of-distribution generalization.

## Method Summary
The method uses a two-stage end-to-end Transformer architecture. The first stage (TF_rep) computes a discrete Laplacian via RBF-activated attention to form pairwise affinities, then extracts the bottom-k eigenvectors using linear attention with block power iteration. The second stage (TF_sup) implements functional gradient descent in an RKHS for categorical outcomes, where each attention layer performs one GD step using kernel-weighted label residuals. The model is trained end-to-end to optimize classification loss, allowing it to deviate from strict spectral algorithms when beneficial. Experiments use synthetic manifolds in R^3, product manifolds, image manifolds from Stable Diffusion, and ImageNet100 features.

## Key Results
- Consistently outperforms strong baselines on synthetic and product manifolds, especially at low label ratios (3-15%)
- Achieves higher accuracy and separation than standard Transformer baselines on ImageNet100
- Demonstrates robust out-of-distribution generalization when trained on diverse manifold types
- Shows a phase transition in representation quality tied to learning dynamics, with critical thresholds around 1200 training samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A Transformer can compute a discrete graph Laplacian and its bottom-k eigenmap in a single forward pass, enabling geometry-aware representation learning from unlabeled data.
- **Mechanism:** Stage 1 uses RBF-activated attention to compute pairwise affinities (forming adjacency matrix A), then linear attention with a block power-iteration structure to extract eigenvectors. The RBF attention directly implements the kernel exp(−‖x(i)−x(j)‖²/2h), while subsequent layers perform QR orthogonalization and subspace iteration.
- **Core assumption:** Data lies on or near a low-dimensional manifold where local Euclidean distances meaningfully approximate intrinsic geometry; the Laplacian Eigenmap captures structure relevant to downstream classification.
- **Evidence anchors:**
  - [abstract] "the first stage constructs a discrete Laplacian and computes an Eigenmap from unlabeled data"
  - [Section 3.1] "Lemma 1... there exists a simple Transformer TFL, which can compute the discrete Laplacian L̂"; "Lemma 2 shows that Zout³ ≈ [v₁...vₖ], where vᵢ is the ith smallest eigenvector"
  - [corpus] Related work (Cheng et al. 2025) shows Transformers can compute positional embeddings during inference, but corpus lacks direct evidence for Laplacian eigenmap computation in end-to-end semi-supervised ICL.
- **Break condition:** If the affinity graph is disconnected or k exceeds the number of connected components, the eigenmap loses discriminative power; if RBF bandwidth h is mismatched to data scale, local structure is not captured.

### Mechanism 2
- **Claim:** The second-stage Transformer implements functional gradient descent (GD) in an RKHS for categorical outcomes, with each layer performing one GD step using kernel-weighted label residuals.
- **Mechanism:** The update f(i)ℓ₊₁ = f(i)ℓ + α Σⱼ₌₁ᵐ [wᵧⱼ − E(w|f(j)ℓ)] κ(ϕ(i),ϕ(j)) is computed via attention: the kernel κ becomes the attention score, and the residual term (label embedding minus expected category) becomes the value. An MLP approximates E(w|f) via softmax.
- **Core assumption:** The target function f(ϕ) resides in an RKHS; categorical labels follow a softmax model with learnable category embeddings w_c; labeled samples are sufficiently representative of local manifold neighborhoods.
- **Evidence anchors:**
  - [abstract] "the second stage performs in-context learning for categorical outcomes via gradient descent"
  - [Section 3.2] "The form of the update in (6) is the same as that considered in previous Transformer ICL research... and it naturally aligns with a self-attention layer"
  - [corpus] von Oswald et al. (2023) and follow-ups establish GD-as-attention for regression; Wang et al. (2025) extends to categorical outcomes—this paper builds on that line.
- **Break condition:** If the kernel κ fails to capture manifold proximity (e.g., wrong bandwidth), label information does not propagate correctly to unlabeled points; if label noise is high, GD amplifies errors.

### Mechanism 3
- **Claim:** End-to-end training of both stages yields representations that outperform offline spectral methods because the Transformer can deviate from the strict algorithm when beneficial, learning data-adaptive Laplacian-like structures.
- **Mechanism:** The two-stage construction serves as an inductive bias/initialization, but joint optimization allows the model to learn composite Laplacians (multiple RBF bandwidths), non-RBF affinities, or representations that directly optimize classification loss rather than spectral fidelity.
- **Core assumption:** The spectral construction is a useful starting point but not optimal; gradient-based optimization can find better representations when guided by task loss; sufficient training diversity exists to generalize.
- **Evidence anchors:**
  - [Section 1.1] "the modular construction serves as a strong mechanistic inductive bias/initialization rather than a hard constraint, and joint optimization can deviate from and improve upon the strict two-stage algorithm when beneficial"
  - [Section 4.1] Orig+E2E-ICL outperforms Eig+ICL (which uses ground-truth eigenvectors), suggesting learned representations are task-optimized
  - [corpus] Li et al. (2025) prove depth or looping enables polynomial estimators for unlabeled data, supporting the need for multi-layer architectures—consistent with this paper's findings.
- **Break condition:** With insufficient training tasks (datasets), the model may underfit or overfit to seen manifolds; end-to-end training may collapse representation learning if the ICL head is too weak.

## Foundational Learning

- **Concept: Graph Laplacian and Laplacian Eigenmaps**
  - **Why needed here:** The entire first stage is built on constructing a discrete approximation to the Laplace-Beltrami operator and extracting its eigenvectors as manifold-aware features.
  - **Quick check question:** Given points x₁,...,xₙ, can you write down the RBF affinity matrix A, the degree matrix D, and the normalized Laplacian L̂ = I − AD⁻¹? What do the bottom eigenvectors of L̂ represent geometrically?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS) and Kernel Methods**
  - **Why needed here:** The ICL stage assumes f(ϕ) ∈ RKHS and uses kernel κ(ϕ(i),ϕ(j)) to propagate label information; understanding how kernels induce feature spaces is essential.
  - **Quick check question:** If κ is an RBF kernel, what does κ(ϕ(i),ϕ(j)) → 0 imply about the relationship between points i and j? How does kernel regression relate to the GD update in Eq. (6)?

- **Concept: Attention-as-Algorithm Implementation**
  - **Why needed here:** The paper explicitly constructs attention layers that implement Laplacian computation (RBF attention) and power iteration (linear attention); understanding how matrix operations map to attention is key.
  - **Quick check question:** Can you explain how W^V Z · κ(W^Q Z, W^K Z) in attention can implement either (a) an affinity matrix multiplication, or (b) a power-iteration step?

## Architecture Onboarding

- **Component map:** Raw coordinates X → TF_L (Laplacian module) → TF_ϕ (Eigenmap module) → TF_sup (ICL head) → Class probabilities
- **Critical path:**
  1. Augment raw tokens with zero-padding → Z₀
  2. TF_L computes L̂ via RBF attention on pairwise distances
  3. TF_ϕ performs block power iteration on L̂ → eigenmap features ϕ(i)
  4. Concatenate ϕ(i) with label embeddings w_y (zero for unlabeled)
  5. TF_sup alternates attention (kernel update) and MLP (expectation computation)
  6. Final layer outputs probabilities; loss on unlabeled positions
- **Design tradeoffs:**
  - **RBF vs. linear attention:** RBF captures locality for Laplacian; linear enables power iteration—mixing incorrectly breaks both
  - **Depth:** More layers in TF_ϕ improve eigen convergence but increase training cost; too few iterations yield noisy eigenvectors
  - **Parameter sharing:** MLP in TF_sup uses tied weights across layers (same E(w|f) function)—more principled but less flexible
  - **Offline vs. in-context Laplacian:** End-to-end is more accurate but requires training; offline Eig+ICL is zero-shot but suboptimal (Table 1, Figure 3)
- **Failure signatures:**
  - **Accuracy plateaus early:** Check RBF bandwidth h—too large ignores local structure; too small yields disconnected graph
  - **OOD transfer fails:** Model may overfit to training manifolds; verify training diversity (multiple manifold types)
  - **Representation collapse:** Eigenmap features constant → initialization M may be poorly set; check normalization in TF_ϕ
  - **ICL head does not improve:** Kernel κ may not match data; try different bandwidths or switch RBF → linear
- **First 3 experiments:**
  1. **Sanity check on synthetic manifold (e.g., sphere):** Train and test on same manifold with 10% labels; verify Orig+E2E-ICL ≫ Orig+ICL. If not, debug TF_L→TF_ϕ pipeline by visualizing learned ϕ(i) vs. ground-truth eigenvectors.
  2. **Ablation on RBF bandwidth:** Fix all else, vary h ∈ {0.1, 1, 10, 100} on cylinder manifold; plot accuracy vs. h. Expect inverted-U shape; identify optimal range for target data scale.
  3. **OOD generalization test:** Train on {sphere, cone, torus, Swiss-roll}, test on cylinder. Compare Orig+E2E-ICL (OOD) vs. Orig+E2E-ICL (ID). If OOD ≪ ID, increase training manifold diversity or regularize.

## Open Questions the Paper Calls Out

- **Question:** To what extent does the end-to-end trained Transformer maintain the strict two-stage spectral construction (Laplacian + Eigenmap) versus converging to a functionally similar but algorithmically distinct solution?
- **Basis in paper:** [explicit] Page 2 states that the modular construction serves as an inductive bias, and that "joint optimization can deviate from and improve upon the strict two-stage algorithm when beneficial."
- **Why unresolved:** While the paper proves a construction exists and shows alignment with spectral features, it does not verify if the final optimized weights strictly implement the theoretical Laplacian/Eigenmap operations or if the model finds a "shortcut" that mimics the output.
- **What evidence would resolve it:** A layer-wise mechanistic analysis (probing) of the trained $TF_{rep}$ module to verify if it explicitly computes graph Laplacians and power iterations, rather than just producing aligned embeddings.

## Limitations

- The approach assumes data lies on or near a low-dimensional manifold where local Euclidean distances meaningfully approximate intrinsic geometry; this assumption may fail for high-dimensional, non-manifold data.
- The computational cost of repeatedly constructing and eigendecomposing large affinity matrices may become prohibitive for large-scale, high-dimensional datasets with millions of points.
- The paper does not explore robustness to label noise or adversarial perturbations, leaving open questions about practical reliability.

## Confidence

**High confidence** in the general framework: The two-stage architecture (Laplacian construction + ICL head) is well-motivated and consistent with prior work on Transformers as algorithmic executors.

**Medium confidence** in empirical claims: The reported performance gains over baselines are promising, but the lack of full implementation details and the limited scope of experiments prevent full verification.

**Low confidence** in exact mechanism: While the paper claims the Transformer computes the Laplacian and Eigenmap in a single forward pass, the precise mapping from attention layers to matrix operations is not fully specified.

## Next Checks

1. **Debug Eigenmap Construction:** Implement the TF_rep module (RBF attention → Laplacian, linear attention → power iteration) on a simple manifold (e.g., cylinder). Visualize the learned eigenfeatures ϕ(i) against ground-truth eigenvectors. If they do not align, the issue likely lies in the attention-to-matrix-operation mapping.

2. **Ablate RBF Bandwidth:** Systematically vary the RBF bandwidth α in TF_rep on cylinder manifold. Plot accuracy vs. α. Expect an inverted-U curve, with optimal α matching the typical inter-point distance on the manifold. This tests whether the Laplacian construction is sensitive to scale.

3. **Test OOD Generalization:** Train on a diverse set of manifolds (sphere, cone, torus, Swiss-roll), then test on a held-out manifold (cylinder). Compare in-distribution (ID) vs. out-of-distribution (OOD) accuracy. If OOD << ID, increase training diversity or add regularization to encourage transferable representations.