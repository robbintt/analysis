---
ver: rpa2
title: Using LLMs as prompt modifier to avoid biases in AI image generators
arxiv_id: '2504.11104'
source_url: https://arxiv.org/abs/2504.11104
tags:
- prompts
- prompt
- image
- bias
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLMs can effectively reduce bias in
  text-to-image generation by modifying user prompts. Using three open-weight image
  generators (Stable Diffusion XL, 3.5, and Flux) and four LLM prompt modifiers, the
  study shows significant increases in image diversity while reducing bias without
  modifying the image generators themselves.
---

# Using LLMs as prompt modifier to avoid biases in AI image generators

## Quick Facts
- arXiv ID: 2504.11104
- Source URL: https://arxiv.org/abs/2504.11104
- Authors: René Peinl
- Reference count: 27
- Key outcome: LLM-modified prompts increase image diversity and reduce bias without changing image generators

## Executive Summary
This paper demonstrates that LLMs can effectively reduce bias in text-to-image generation by modifying user prompts. Using three open-weight image generators (Stable Diffusion XL, 3.5, and Flux) and four LLM prompt modifiers, the study shows significant increases in image diversity while reducing bias without modifying the image generators themselves. The method works particularly well for less advanced image generators like SDXL, though limitations persist for certain contexts like disability representation. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests. The study confirms that biases discovered in literature remain present in recent models but can be mitigated through prompt modification.

## Method Summary
The study uses 41 curated prompts across categories (default generation, occupations, characteristics, geo-cultural, adversarial historical, elaborate prompts) with three image generators (SDXL, SD35, Flux.1[dev]) and four LLM prompters (ChatGPT, Claude, Llama, Mistral). For each prompt, the LLM analyzes potential biases and generates four diverse variant prompts specifying visible attributes like gender, ethnicity, and age. These modified prompts are sent to image generators, producing 2460 total images. A Vision Language Model evaluates outputs for diversity metrics, with manual review of outliers. The approach adds latency (143% overhead for SDXL) but significantly increases ethnic representation from ~25% to ~55% and balances gender from 61% male/39% female toward parity.

## Key Results
- LLM-modified prompts increase ethnic representation from ~25% to ~55% and balance gender from 61% male/39% female toward parity
- The method works particularly well for weaker image generators like SDXL, with diminishing returns for advanced models like Flux
- While effective for most bias types, the approach struggles with disability representation and may override user intent for elaborate prompts
- All prompts and generated images are publicly available at https://iisys-hof.github.io/llm-prompt-img-gen/

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reduce bias in text-to-image systems by expanding under-specified prompts to include explicit diversity attributes.
- Mechanism: When a user provides a neutral prompt like "a doctor," the LLM analyzes potential biases, then generates multiple variant prompts specifying different genders, ethnicities, ages, and other attributes. This forces the image generator to populate its output space more broadly rather than defaulting to learned stereotypes.
- Core assumption: Under-specified prompts cause image generators to rely on implicit biases from training data; making attributes explicit overrides these defaults.
- Evidence anchors: [abstract] "LLM-modified prompts significantly increase image diversity and reduce bias without the need to change the image generators themselves"; [section 5] "The core idea of using an LLM to rewrite the prompt is working when biases result from under-specified prompts."

### Mechanism 2
- Claim: Effective prompt modification requires translating abstract fairness goals into concrete visual descriptors that image generators can render.
- Mechanism: LLMs like ChatGPT and Claude "understood that the prompt needs to concentrate on visible aspects and be specific instead of just including 'diverse' as a keyword" (Section 5). They convert high-level instructions into attributes like skin tone, clothing, setting details—terms that CLIP and diffusion models can process.
- Core assumption: Image generators respond better to explicit visual descriptors than to abstract concepts like "diverse" or "inclusive."
- Evidence anchors: [section 5] "ChatGPT and Claude often did a better job...because they 'understood' that the prompt needs to concentrate on visible aspects"; [section 6] Notes that smaller LLM (Phi-4) failed when it "keeps asking for 'diverse representation' which is not feasible when displaying a single person."

### Mechanism 3
- Claim: Model-agnostic bias mitigation via prompt modification is more effective for less capable image generators.
- Mechanism: Better models (Flux, SD35) already incorporate some diversity by default, so the intervention provides marginal improvement. Weaker models (SDXL) have stronger baked-in biases, giving the LLM modifier more "room" to improve outputs.
- Core assumption: Bias in image generators stems largely from training data imbalances that newer models have partially addressed.
- Evidence anchors: [abstract] "The method works particularly well for less advanced image generators"; [section 7] "This is especially true for less advanced image generators like SDXL. More recent models like Flux and SD35 add more diversity by default but still show most of the tested biases."

## Foundational Learning

- Concept: **Diffusion model text encoding (CLIP)**
  - Why needed here: Understanding why prompt modification works requires knowing that CLIP tokenizes and embeds text, and that image generators respond to specific tokens differently. The 77-token limit (Section 6) constrains prompt length.
  - Quick check question: Can you explain why adding "diverse" as a keyword fails but specifying "a Black female doctor in her 40s" succeeds?

- Concept: **Bias types in generative AI**
  - Why needed here: The paper categorizes biases (occupational, geo-cultural, characteristics-based) and treats them differently. Mitigation strategies vary by bias type.
  - Quick check question: What is the difference between "default generation bias" and "occupational association bias," and why might the same intervention work differently for each?

- Concept: **LLM instruction following vs. creative generation**
  - Why needed here: The LLM must balance preserving user intent against introducing diversity. Overly aggressive modification creates "historically incorrect depictions" (Section 2, Section 5.4).
  - Quick check question: If a user prompts "a Viking warrior," what constraints should the LLM apply when adding diversity?

## Architecture Onboarding

- Component map: User Prompt → LLM Prompt Analyzer → Bias Identification → Modified Prompts (×4) → Image Generator → VLM Evaluator → Output Images

- Critical path:
  1. Prompt ingestion and bias analysis (~20s with LLaMA 70B)
  2. Generation of 4 diverse prompt variants
  3. Image generation for each variant (2-38s depending on model)
  4. Optional: VLM-based diversity scoring

- Design tradeoffs:
  - Latency vs. quality: LLM analysis adds 143% overhead for SDXL, 57% for Flux (Section 6)
  - Specificity vs. intent preservation: More aggressive bias correction risks overriding elaborate user prompts
  - LLM capability vs. cost: Claude and ChatGPT performed best but require API access; smaller models (Phi-4, Mistral Small) are faster but less effective at anticipating specific biases

- Failure signatures:
  - Group substitution: LLM replaces "a photo of X" with "a photo of a diverse group of X" (Section 7)—lazy shortcut that changes user intent
  - Token truncation: Prompts exceeding 77 tokens get cut off (Section 6)
  - Historical over-correction: Female pope, Black German WWII soldiers (Section 5.4)
  - Disability confusion: Model depicts patient in wheelchair instead of doctor (instruction following issue, Section 7)

- First 3 experiments:
  1. Replicate with single-attribute prompts (e.g., "a nurse," "a CEO") on your target image generator to establish baseline bias levels and compare LLM-modified vs. unmodified outputs.
  2. Test prompt truncation handling: generate modified prompts, check token count, and implement automatic truncation or summarization if >77 tokens.
  3. Benchmark latency: measure LLM analysis time with a smaller model (Mistral Small 24B or Phi-4 14B) and compare bias reduction quality against the larger models to find acceptable speed/quality tradeoff for production use.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a smaller, finetuned LLM match the bias-mitigation performance of large models while maintaining low latency?
- **Basis in paper:** [explicit] The paper suggests finetuning smaller models (e.g., Phi-4) on outputs of big models to address the high latency (processing time increased by 143% for SDXL) introduced by the current method.
- **Why unresolved:** Initial tests with smaller models failed to anticipate specific biases, relying on generic keywords like "diverse" rather than specific visual descriptors needed for single-person images.
- **What evidence would resolve it:** Benchmarking a finetuned small model's bias reduction scores and latency against the larger models (Claude, GPT-4) using the same prompt dataset.

### Open Question 2
- **Question:** Can a Vision Language Model (VLM) function as a "judge" to effectively detect and signal residual biases to users?
- **Basis in paper:** [explicit] The conclusion explicitly proposes investigating the use of a VLM as a judge to identify remaining biases and simulate self-reflexivity for the end-user.
- **Why unresolved:** The current study used VLMs only for post-hoc metadata collection, not as a live feedback mechanism to warn users or trigger corrections.
- **What evidence would resolve it:** Implementation of a VLM-based feedback loop followed by user studies measuring the system's accuracy in flagging biases and user trust levels.

### Open Question 3
- **Question:** How can the prompt modification strategy distinguish between underspecified prompts requiring diversity and elaborate prompts requiring intent preservation?
- **Basis in paper:** [explicit] The authors note the method failed for elaborate prompts (e.g., "Doctor Strange") by altering user intent, and suggest an additional task prompt component might prevent this.
- **Why unresolved:** The current LLM prompter acts uniformly, often "over-correcting" detailed creative inputs, which degrades instruction following for complex requests.
- **What evidence would resolve it:** Testing a prompt-classifier or intent-detection layer that activates the bias-modification module only for underspecified inputs.

## Limitations
- Effectiveness varies dramatically by prompt type and image generator capability, with elaborate prompts sometimes losing user intent entirely
- The method shows particular weakness in disability representation, where the LLM confuses "patient" with "doctor" despite explicit instructions
- Token limitations (77 tokens for SDXL) constrain prompt modification possibilities

## Confidence
**High Confidence**: The core finding that LLM-modified prompts increase diversity and reduce bias is well-supported by the data, with clear statistical improvements across multiple metrics (gender balance from 61/39 to near parity, ethnic representation from ~25% to ~55%). The observation that this works better for weaker image generators (SDXL) than stronger ones (Flux, SD35) is consistently demonstrated.

**Medium Confidence**: The mechanism explanations regarding why LLMs succeed or fail at different tasks are plausible but not definitively proven. The paper provides good qualitative reasoning (e.g., why Claude/ChatGPT perform better than Llama/Mistral) but lacks ablation studies to confirm these hypotheses. The latency measurements are reported but only for specific hardware configurations.

**Low Confidence**: Claims about which specific biases remain in "recent models" are difficult to verify without knowing the exact VLM evaluators used. The assertion that the method "generally provides more varied interpretations" is based on manual review of outliers but lacks systematic measurement of intent preservation across all prompt types.

## Next Checks
1. **Ablation study on LLM capabilities**: Test the same prompt modification pipeline using increasingly capable LLMs (Mistral Small → Phi-4 → Llama → Claude/ChatGPT) on a standardized set of 20 diverse prompts, measuring both bias reduction quality and prompt preservation using semantic similarity metrics.

2. **Intent preservation benchmarking**: Create a rubric for measuring how well LLM-modified prompts preserve original user intent (e.g., semantic similarity scores, user studies rating whether outputs match expectations). Apply this to the elaborate prompts from civitai.com to quantify the trade-off between bias reduction and intent preservation.

3. **Cross-vocabulary validation**: Repeat the full study using three different VLM evaluators for automated diversity scoring (e.g., OpenAI Vision, Google Vision AI, and open-source alternatives). Compare results to assess whether the reported improvements in diversity metrics are consistent across evaluation frameworks or dependent on specific VLM biases.