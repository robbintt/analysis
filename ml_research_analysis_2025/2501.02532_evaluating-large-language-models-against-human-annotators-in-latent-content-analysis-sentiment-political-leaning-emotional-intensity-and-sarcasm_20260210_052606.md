---
ver: rpa2
title: 'Evaluating Large Language Models Against Human Annotators in Latent Content
  Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm'
arxiv_id: '2501.02532'
source_url: https://arxiv.org/abs/2501.02532
tags:
- llms
- human
- content
- sentiment
- sarcasm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared seven LLMs (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini,
  Gemini, Llama-3.1, Mixtral) against 33 human annotators in latent content analysis
  across sentiment, political leaning, emotional intensity, and sarcasm detection.
  Humans and LLMs each analyzed 100 curated sentences, with LLMs evaluated at three
  time points for consistency.
---

# Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm

## Quick Facts
- arXiv ID: 2501.02532
- Source URL: https://arxiv.org/abs/2501.02532
- Reference count: 10
- Seven LLMs (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, Gemini, Llama-3.1, Mixtral) compared against 33 human annotators in content analysis

## Executive Summary
This study directly compares seven large language models against 33 human annotators across four latent content analysis tasks: sentiment, political leaning, emotional intensity, and sarcasm detection. Using 100 curated sentences, the research evaluates both human and machine performance through reliability metrics including Krippendorff's alpha, temporal consistency, and inter-model agreement. The findings reveal that LLMs can achieve performance comparable to or exceeding human annotators in certain tasks while maintaining superior consistency over time, though both groups struggle with the inherently ambiguous nature of sarcasm detection.

The research establishes that GPT-4 variants, particularly GPT-4o-mini, match or exceed human performance in sentiment and political leaning analysis, with LLMs demonstrating excellent temporal stability (ICCs 0.981-0.998). However, significant differences emerge in emotional intensity ratings, where humans consistently assign higher values than LLMs (3.44 vs 3.19, p<0.001). The study suggests that while LLMs offer promising capabilities for content analysis, human expertise remains essential for nuanced emotional interpretation, particularly in tasks requiring subtle contextual understanding.

## Method Summary
The study employed a controlled comparison design where 33 human annotators and seven LLMs each analyzed 100 curated English sentences across four latent content dimensions. Human annotators provided ratings on 5-point Likert scales for each dimension, while LLMs were evaluated at three separate time points to assess temporal consistency. Reliability was measured using Krippendorff's alpha for inter-annotator agreement, with additional metrics including Cohen's kappa for human-human agreement, intraclass correlation coefficients (ICCs) for temporal stability, and Spearman correlation for inter-model agreement. The dataset was specifically curated to include examples of sentiment, political leaning, emotional intensity, and sarcasm to ensure adequate representation across all four analysis dimensions.

## Key Results
- High reliability in sentiment analysis (Krippendorff's alpha: 0.95 for both humans and LLMs)
- LLMs showed superior temporal stability (ICCs 0.981-0.998) and internal consistency in emotional intensity (alpha: 0.85)
- Humans rated emotional intensity significantly higher than LLMs (3.44 vs 3.19, p<0.001)
- Both humans and LLMs struggled with sarcasm detection (alpha: 0.25)

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively process and analyze latent content dimensions through their trained understanding of language patterns, contextual relationships, and semantic structures. The mechanisms underlying this performance include the models' ability to recognize sentiment-bearing words, detect political framing and ideological cues, assess emotional valence through linguistic markers, and identify sarcastic intent through contextual incongruities. The temporal consistency observed in LLMs suggests that their reasoning processes are stable and reproducible, unlike human annotators who may be influenced by subjective factors or fatigue. The superior inter-model agreement among LLMs (rho: 0.92-0.99) indicates that these models have converged on similar interpretive frameworks for content analysis tasks.

## Foundational Learning
1. **Krippendorff's alpha** - Measures inter-rater reliability for categorical data
   *Why needed*: Quantifies agreement between annotators beyond chance
   *Quick check*: Values range from 0 (no agreement) to 1 (perfect agreement)

2. **Intraclass Correlation Coefficient (ICC)** - Assesses consistency of measurements over time
   *Why needed*: Evaluates temporal stability of model outputs
   *Quick check*: Values closer to 1 indicate higher consistency

3. **Cohen's kappa** - Measures agreement between two raters
   *Why needed*: Provides specific human-human agreement metrics
   *Quick check*: Adjusts for agreement occurring by chance

4. **Spearman correlation** - Measures monotonic relationship between ranked variables
   *Why needed*: Assesses agreement patterns between different models
   *Quick check*: Values range from -1 to 1, with higher positive values indicating stronger agreement

5. **Latent content analysis** - Method for extracting underlying meanings from text
   *Why needed*: Goes beyond surface-level features to analyze deeper semantic content
   *Quick check*: Requires understanding of context, tone, and implicit messaging

6. **Likert scale measurement** - Ordinal rating system for subjective assessments
   *Why needed*: Provides standardized framework for subjective content evaluation
   *Quick check*: Typically uses 5-point scales (1=low to 5=high)

## Architecture Onboarding

**Component Map**: Human Annotators -> Content Analysis Tasks -> Reliability Metrics -> Performance Comparison; LLMs -> Content Analysis Tasks -> Reliability Metrics -> Performance Comparison

**Critical Path**: Data curation → Human annotation → LLM processing → Reliability measurement → Comparative analysis → Result interpretation

**Design Tradeoffs**: 
- Small dataset (100 sentences) enables controlled comparison but limits generalizability
- Three time points for LLM evaluation balances temporal assessment with practical constraints
- 5-point Likert scales provide granularity while maintaining interpretability
- Single language (English) ensures consistency but reduces cross-linguistic applicability

**Failure Signatures**:
- Low Krippendorff's alpha (<0.40) indicates poor inter-annotator agreement
- Inconsistent ICC values suggest temporal instability in LLM outputs
- Large discrepancies between human and LLM ratings may indicate systematic interpretation differences
- High variance in individual model performance suggests model-specific limitations

**3 First Experiments**:
1. Replicate analysis with 500 sentences across multiple domains to assess scalability
2. Cross-linguistic validation using the same methodology with non-English content
3. Blind validation study where experts evaluate LLM outputs without source knowledge

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Small sample size of 100 curated sentences may not represent real-world content diversity
- English-language focus limits generalizability to other linguistic contexts
- Only short-term temporal consistency assessed, leaving long-term reliability uncertain
- Sarcasm detection task shows poor reliability for both humans and LLMs, indicating fundamental challenges

## Confidence
- **High**: Sentiment analysis reliability (alpha: 0.95) and political leaning analysis (alpha: 0.80 for LLMs)
- **Medium**: Emotional intensity analysis due to significant human-LLM rating discrepancies (3.44 vs 3.19, p<0.001)
- **Low**: Sarcasm detection reliability (alpha: 0.25) for both humans and LLMs

## Next Checks
1. Replicate the study with a larger and more diverse dataset spanning multiple domains and languages to assess generalizability
2. Conduct a longitudinal study to evaluate LLM consistency over extended periods beyond the current three-time-point assessment
3. Implement a blind validation study where human experts evaluate LLM-generated annotations without knowing their source to better assess practical equivalence of human and machine analysis