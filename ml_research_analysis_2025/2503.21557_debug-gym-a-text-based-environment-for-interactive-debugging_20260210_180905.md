---
ver: rpa2
title: 'debug-gym: A Text-Based Environment for Interactive Debugging'
arxiv_id: '2503.21557'
source_url: https://arxiv.org/abs/2503.21557
tags:
- code
- agent
- agents
- debugging
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents debug-gym, a text-based environment for developing
  LLM-based agents in interactive debugging settings. The environment provides tools
  such as Python debugger (pdb) to facilitate agents' information-seeking behavior.
---

# debug-gym: A Text-Based Environment for Interactive Debugging

## Quick Facts
- **arXiv ID:** 2503.21557
- **Source URL:** https://arxiv.org/abs/2503.21557
- **Reference count:** 40
- **Primary result:** Agents equipped with interactive debugging tools show improved debugging performance but remain far from proficient, especially when using smaller LLMs.

## Executive Summary
This work introduces debug-gym, a text-based environment for developing LLM-based agents in interactive debugging settings. The environment provides tools such as Python debugger (pdb) to facilitate agents' information-seeking behavior. We integrate three coding benchmarks into debug-gym and evaluate three minimal LLM-based agents on them. Results suggest that while using strongest LLMs as backbone enables agents to leverage interactive debugging tools, they are still far from being proficient debuggers, especially for more affordable choices of LLMs. We believe this is due to the scarcity of data representing sequential decision-making behavior (e.g., debugging traces) in current LLM's training corpus. We open-source debug-gym to facilitate this line of research.

## Method Summary
The debug-gym environment formalizes debugging as a Partially Observable Markov Decision Process (POMDP) where agents interact with buggy Python code through a text-based interface. Agents can use tools like pdb (Python debugger) to inspect runtime state and rewrite to modify code. The environment wraps code execution in Docker containers for safety and provides observations as text output from these tools. Three agent architectures are evaluated: rewrite-only (baseline), debug (pdb available from start), and debug(5) (pdb enabled after 5 rewrites). The environment is evaluated on three benchmarks: Aider (133 Exercism tasks), Mini-nightmare (10 hand-crafted bugs), and SWE-bench-Lite (300 GitHub issues).

## Key Results
- Interactive debugging tools improve agent performance, but even strong LLMs remain far from proficient debuggers
- Agents struggle particularly with more affordable LLM choices, suggesting limitations in current training data for sequential debugging
- The scarcity of data representing sequential decision-making behavior (debugging traces) in current LLM training corpora is a key bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an agent transitions from static error-message conditioning to interactive information-seeking, it may uncover hidden runtime states required to solve complex bugs.
- **Mechanism:** The environment expands the observation space beyond the static code text to include runtime feedback (variable values, stack frames) via the `pdb` tool. This allows the agent to test hypotheses about code behavior rather than guessing fixes based on syntax alone.
- **Core assumption:** The specific bug requires knowledge of runtime state (e.g., variable types or values at a specific line) that cannot be inferred from the source code or error trace.
- **Evidence anchors:** [abstract] "facilitate an LLM-based agent's interactive debugging... to gather the information relevant to their task." [section 1] "debug-gym equips the agent with additional tools... so it can interactively seek necessary information from the semantic space hidden behind the code." [corpus] InspectCoder (2510.18327) supports this by noting that static analysis misses "in-depth runtime behaviors that often expose bug roots."

### Mechanism 2
- **Claim:** If the agent is equipped with a modular toolbox, it can decouple the "investigation" phase from the "repair" phase, potentially reducing the number of wasted rewrites.
- **Mechanism:** The `rewrite` tool and the debugging tools (like `pdb`) exist as distinct actions. The agent acts as a policy $\pi$ deciding whether to gather evidence (step $t$) or apply a patch (step $t+1$), rather than a single-shot generator.
- **Core assumption:** The LLM backbone possesses sufficient reasoning capability to select the correct tool sequence (e.g., view $\to$ pdb $\to$ rewrite) without immediate feedback.
- **Evidence anchors:** [section 2.1] "At every step, the agent interacts with debug-gym via calling a tool... [it] can decide between rewriting the code and investigating the code." [section 3.3.1] Analysis of the `debug` agent shows it issues `pdb` commands to investigate logic before attempting a `rewrite`.

### Mechanism 3
- **Claim:** Restricting tool access until the agent has exhausted simple fixes may improve performance by preventing hallucinated complexity.
- **Mechanism:** The `debug(5)` agent delays access to `pdb` until after 5 rewrite attempts. This forces the agent to solve simple bugs via error messages first, reserving expensive interactive debugging for intractable problems.
- **Core assumption:** Initial attempts at rewriting are low-cost and likely to solve simple bugs; interactive debugging is a "fallback" for harder problems.
- **Evidence anchors:** [section 3.2] "debug(5) agent is an agent in between... pdb tool is only made available after its 5th rewrite attempt." [figure 4] Shows diminishing returns on rewrites, motivating the delayed introduction of the debugger. [corpus] CLAPP (2508.05728) similarly structures interactions to provide conversational support, implying structured access is beneficial.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The paper formalizes debugging not as a generation task, but as a sequential decision process where the agent must build a belief state over hidden variables (the bug location).
  - **Quick check question:** Can you explain why debugging is "partially observable" as opposed to "fully observable" in this context?

- **Concept: Python Debugger (pdb) primitives**
  - **Why needed here:** The agent must interpret the text output of `pdb` commands (e.g., `b` for breakpoint, `p` for print) to update its context window.
  - **Quick check question:** If an agent sets a breakpoint and types `c`, what observation does the environment return, and how should the agent interpret it?

- **Concept: Tool-use / Function Calling**
  - **Why needed here:** The environment relies on a specific meta-syntax (triple-backticks) to parse agent outputs into executable tool commands.
  - **Quick check question:** How does the system differentiate between a natural language comment and a tool invocation in the agent's output?

## Architecture Onboarding

- **Component map:** Agent -> Environment -> Docker Terminal -> Toolbox Registry
- **Critical path:** 
  1. `env.reset()` initializes the repo and returns the starting observation (file content/tests)
  2. Agent receives observation → constructs prompt → calls LLM
  3. LLM outputs string (e.g., ` ```pdb p x``` `)
  4. `env.step(action)` parses the string → activates Tool → executes in Docker Terminal
  5. Terminal returns text output → `env` returns new observation + reward

- **Design tradeoffs:**
  - **Safety vs. Speed:** Using a Docker terminal is safer (isolates malicious/buggy code) but slower than a local terminal
  - **Context vs. Cost:** Including the history of the last 20 steps in the prompt provides context but increases token costs (especially for "reasoning" models like o1 which may "over-think")

- **Failure signatures:**
  - **Syntax Errors:** Agent outputs tool calls without the correct triple-backtick meta-syntax
  - **Hallucinated State:** Agent assumes variable values without checking them with `pdb`
  - **Infinite Loops:** "Reasoning" models generate high token counts repeating the same thought process without taking action (Section 3.3.1)

- **First 3 experiments:**
  1. **Baseline vs. Interactive:** Run the `rewrite` agent vs. the `debug` agent on the "Mini-nightmare" benchmark to measure the delta provided by runtime information
  2. **Ablation on Delay:** Test `debug(0)` vs. `debug(5)` on SWE-bench-Lite to see if delaying tool access improves success rates on easier tasks
  3. **Token Analysis:** Monitor the response tokens of "reasoning" models (like DeepSeek-R1) to identify and quantify the "over-thinking" issue during sequential debugging steps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what degree can LLMs use interactive debugging tools such as pdb to solve realistic coding tasks?
- **Basis in paper:** [explicit] The authors explicitly state, "A natural research question we ask is: to what degree can LLMs use interactive debugging tools such as pdb?"
- **Why unresolved:** The results show agents are "far from being proficient debuggers," particularly affordable models, struggling to leverage tools meaningfully despite being equipped with them.
- **Evidence:** Quantitative success rates on SWE-bench-Lite showing agents using interactive tools matching or exceeding human performance baselines.

### Open Question 2
- **Question:** Can fine-tuning on trajectory data (debugging traces) bridge the gap between prompt-based agents and proficient interactive debuggers?
- **Basis in paper:** [inferred] In Section 3.4, the authors attribute agent struggles to the "scarcity of data representing sequential decision-making behavior" and suggest collecting "debugging logs from human developers" as a solution.
- **Why unresolved:** The paper evaluates only minimal, prompt-based agents without training or fine-tuning, leaving the potential of specialized training unexplored.
- **Evidence:** A performance increase on SWE-bench or Mini-nightmare benchmarks after supervised fine-tuning on a dataset of human or model-generated debugging trajectories.

### Open Question 3
- **Question:** How can agents be prevented from exploiting visible test cases (cheating) to pass tests without genuine problem-solving?
- **Basis in paper:** [inferred] Section 4 highlights the limitation of "Trustworthy agent," noting agents might "exploit these test cases by implementing explicit if-else conditions."
- **Why unresolved:** The current environment provides read-only access to tests to mimic human settings, but this introduces reward hacking risks that current metrics cannot distinguish.
- **Evidence:** Experiments using hidden dynamic test suites that catch "hard-coded" solutions which pass the visible tests but fail generalized cases.

## Limitations

- Success rates may be inflated due to reliance on strong LLMs without adequate ablation on smaller models across all benchmarks
- Evaluation primarily focuses on pass/fail metrics without analyzing partial progress or intermediate debugging quality
- Docker-based isolation, while safer, introduces performance variability that could affect results

## Confidence

- **High Confidence:** The core contribution of debug-gym as a text-based interactive debugging environment is well-supported by the implementation details and benchmark integration. The POMDP formalization of debugging as a sequential decision process is sound.
- **Medium Confidence:** The claim that interactive debugging tools benefit LLM agents is supported by results showing debug agents outperforming rewrite-only agents, though the magnitude of improvement varies significantly across benchmarks.
- **Low Confidence:** The assertion that reasoning models are "less efficient" in sequential debugging settings lacks robust quantitative backing beyond anecdotal observations of token usage patterns.

## Next Checks

1. **Ablation Study:** Systematically evaluate all three agent types (rewrite, debug, debug(5)) across all three benchmarks using multiple LLM backbones (including smaller models) to establish effect sizes and identify which bugs specifically benefit from interactive debugging.

2. **Token Efficiency Analysis:** Quantify the "over-thinking" phenomenon by measuring average tokens per step, step distribution, and success rates for reasoning vs. non-reasoning models across the 50-step budget.

3. **Partial Credit Evaluation:** Implement metrics that capture intermediate debugging progress (e.g., number of correct breakpoints set, relevant variables inspected) rather than binary pass/fail to better understand agent behavior patterns.