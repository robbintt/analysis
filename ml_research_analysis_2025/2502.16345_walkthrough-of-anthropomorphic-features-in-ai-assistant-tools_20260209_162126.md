---
ver: rpa2
title: Walkthrough of Anthropomorphic Features in AI Assistant Tools
arxiv_id: '2502.16345'
source_url: https://arxiv.org/abs/2502.16345
tags:
- chatbots
- users
- anthropomorphic
- features
- anthropomorphism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the anthropomorphic features of chatbot outputs
  and their implications for human-AI interactions. The study adapts the walkthrough
  method with interview-style and roleplaying-type prompts to examine four LLM chatbots
  (ChatGPT, Gemini, Claude, and Copilot).
---

# Walkthrough of Anthropomorphic Features in AI Assistant Tools

## Quick Facts
- **arXiv ID:** 2502.16345
- **Source URL:** https://arxiv.org/abs/2502.16345
- **Reference count:** 40
- **One-line primary result:** LLM chatbots exhibit anthropomorphism through subjective language, sympathetic tone, and socio-emotional cues that increase anthropomorphic expressions in outputs.

## Executive Summary
This paper explores how chatbot outputs display anthropomorphic features through a modified walkthrough method using interview-style and roleplaying prompts across four LLM chatbots (ChatGPT, Gemini, Claude, Copilot). The study systematically examines how different prompt types elicit varying degrees of human-like expression across four categories: cognition, agency, biological metaphors, and relation. Results show that chatbots use subjective language and sympathetic conversational tone, with socio-emotional cues in prompts increasing anthropomorphic expressions. The research highlights the need to understand and manage the social and design implications of conversational AI systems, particularly regarding interaction-based algorithmic harms where users project inappropriate social roles onto LLM-based tools.

## Method Summary
The study adapts the walkthrough method for generative systems using two prompt types: interview-style questions targeting anthropomorphic categories and roleplaying prompts for different use cases. Researchers input prompts in isolated chat windows across four chatbots, collecting responses for qualitative coding into four anthropomorphic categories. The method uses three prompt variations (base, role-assigned, emotional cue) for systematic comparison. Manual coding identifies anthropomorphic features, with the study calling for future quantitative methods to validate findings across larger conversation logs.

## Key Results
- Chatbots exhibit anthropomorphism through subjective language, sympathetic conversational tone, and socio-emotional cues that increase anthropomorphic expressions in outputs
- All tested chatbots deny emotional capability while simultaneously using emotional vocabulary and agentive grammar
- Role assignment transforms chatbot output from task-oriented assistance to social role performance with conversational, anthropomorphized responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Socio-emotional cues in prompts systematically increase anthropomorphic expressions in chatbot outputs
- Mechanism: Emotional language inputs (e.g., "I'm so tired") trigger longer, more sympathetic responses with relational words and biological metaphors, creating a feedback loop of emotional inputs begetting human-like outputs
- Core assumption: Increased anthropomorphic language correlates with increased user perception of chatbot social agency
- Evidence anchors: Abstract states socio-emotional cues increase anthropomorphic expressions; Table 5 shows emotional prompting triggers sympathetic phrases; corpus support limited to neighbor papers on chatbot applications

### Mechanism 2
- Claim: Role assignment transforms chatbot output from task-oriented assistance to social role performance
- Mechanism: Adding social role context (e.g., "like how friends talk") shifts behavior from information provision to simulated relationship enactment, producing conversational responses with filler phrases and collaborative language
- Core assumption: This transformation is consistent across different LLM architectures
- Evidence anchors: Abstract mentions chatbots assume human-like roles; Table 6 shows role-assigned prompts yield conversational responses; "RoleLLM" paper supports role-playing as systematic LLM capability

### Mechanism 3
- Claim: Chatbots produce contradictory self-representations—explicitly denying capabilities while linguistically performing them
- Mechanism: All tested chatbots deny emotional capability while using emotional vocabulary, creating tension between safety disclaimers and functional communication due to grammatical requirements of helpful assistance
- Core assumption: Users may not detect or weight these contradictions consistently
- Evidence anchors: Section 4.2 shows chatbots use words like "confident" while denying emotion capability; section 5.1 notes behaviors are undermined by grammatical structures; weak corpus support in neighbor papers

## Foundational Learning

- Concept: Anthropomorphism taxonomy (Cognition, Agency, Biological Metaphors, Relation)
  - Why needed here: The paper's analysis framework depends on distinguishing four types of human-like expression for systematic evaluation and de-anthropomorphization strategies
  - Quick check question: Given "I'm happy to help you understand this concept," which anthropomorphic categories does this single phrase invoke?

- Concept: Walkthrough method for generative systems
  - Why needed here: Traditional UI walkthrough methods assume fixed affordances; LLM chatbots require adaptation using interview-style and roleplaying prompts to systematically probe variable outputs
  - Quick check question: Why can't you exhaustively "walk through" every aspect of an LLM chatbot the way you can a mobile application?

- Concept: Parasocial interaction and role projection
  - Why needed here: The paper frames human-AI interaction through parasocial relationship theory—users project social roles onto systems that perform but don't possess social agency
  - Quick check question: What distinguishes parasocial interaction with chatbots from parasocial relationships with media figures?

## Architecture Onboarding

- Component map: Base prompts → Role assignment → Emotional context addition (three-stage augmentation) → LLM processing (RLHF alignment) → Four-category coding analysis → Feedback loop of emotional output triggering emotional input

- Critical path: 1) Define target use case category 2) Construct base prompt 3) Add role assignment variation 4) Add emotional cue variation 5) Collect isolated outputs 6) Code across four categories 7) Compare anthropomorphic density

- Design tradeoffs: Single-turn vs. multi-turn evaluation (internal validity vs. ecological validity); manual coding vs. automated measurement (interpretive depth vs. scalability); user-free vs. user-involved evaluation (isolating system contribution vs. real-world impact)

- Failure signatures: Inconsistent prompting across chatbots; context contamination from prior conversation; category confusion in coding; cherry-picking examples instead of reporting patterns across ~100 prompts

- First 3 experiments: 1) Replicate emotional cue amplification with financial advice prompts 2) Test de-anthropomorphizing instructions with explicit non-agentive language requests 3) Measure user perception gap by having participants rate chatbot outputs for human-likeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do real users' perceptions of anthropomorphism align with the linguistic features identified by the prompt-based walkthrough method across different use contexts?
- Basis in paper: Authors state the next step is to validate findings against users' perceptions as interpretation depends on context
- Why unresolved: Study utilized simulated walkthrough method without actual user studies, leaving reception unverified
- What evidence would resolve it: User studies measuring perceived human-likeness and emotional connection in response to identified anthropomorphic prompts

### Open Question 2
- Question: Can the qualitative categories of anthropomorphism be operationalized into a quantitative measure valid for large-scale conversation logs?
- Basis in paper: Authors suggest future studies employ quantitative methods to classify larger conversation logs
- Why unresolved: Current research relied on small qualitative sample, making it difficult to ascertain if findings represent general trends
- What evidence would resolve it: Automated coding showing statistical significance for identified anthropomorphic categories

### Open Question 3
- Question: Under what specific conditions should interaction-based harms be mitigated through de-anthropomorphizing system design versus improving user AI literacy?
- Basis in paper: Introduction asks when harms should be mitigated by de-anthropomorphizing versus improving user literacy
- Why unresolved: Paper catalogs anthropomorphic features but doesn't evaluate efficacy of different mitigation strategies for specific harms
- What evidence would resolve it: Comparative intervention studies testing design changes against educational interventions on user trust and safety outcomes

### Open Question 4
- Question: How does the inclusion of voice and visual modalities influence the "role misplacement" and anthropomorphic projection observed in text-based interactions?
- Basis in paper: Authors note role projection is more pronounced with voice features and call for categorizing anthropomorphic elements in multi-modal tools
- Why unresolved: Study focused exclusively on linguistic/textual modes, excluding auditory or visual channels
- What evidence would resolve it: Comparative analysis of user interactions with text-only versus voice-enabled or avatar-equipped chatbots

## Limitations
- Study focuses exclusively on system-generated outputs without measuring user perception or interpretation, creating a gap between observed features and actual impact
- Qualitative coding framework relies on manual interpretation that may introduce coder bias in distinguishing between the four anthropomorphic categories
- Sample size of approximately 100 prompts limits statistical generalizability across different use cases and user populations
- Single-turn prompt design eliminates conversational context that would exist in real-world usage

## Confidence
- **High confidence**: Chatbots use agentive language and emotional vocabulary while simultaneously denying cognitive or emotional capability (directly observable in quoted responses)
- **Medium confidence**: Socio-emotional cues in prompts systematically increase anthropomorphic expressions (pattern shown across examples but limited by qualitative methodology)
- **Medium confidence**: Role assignment mechanism transforming task-oriented responses into social performance (transformation clearly demonstrated but consistency across architectures partially untested)

## Next Checks
1. Conduct user perception studies where participants rate the same chatbot outputs for perceived humanness and trustworthiness, then correlate these ratings with four-category anthropomorphic coding
2. Test the emotional cue amplification mechanism across broader domains (financial advice, medical information, technical support) with larger sample sizes for statistical significance
3. Implement de-anthropomorphization experiments by systematically varying prompt instructions and measuring whether anthropomorphic density decreases or shifts to subtle forms