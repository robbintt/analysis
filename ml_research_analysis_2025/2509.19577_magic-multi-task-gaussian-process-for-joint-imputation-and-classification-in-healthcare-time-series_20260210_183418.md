---
ver: rpa2
title: 'MAGIC: Multi-task Gaussian process for joint imputation and classification
  in healthcare time series'
arxiv_id: '2509.19577'
source_url: https://arxiv.org/abs/2509.19577
tags:
- time
- magic
- series
- data
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGIC is a unified framework for joint imputation and classification
  in healthcare time series data. It uses a hierarchical multi-task Gaussian process
  coupled with functional logistic regression to simultaneously perform class-informed
  missing value imputation and label prediction.
---

# MAGIC: Multi-task Gaussian process for joint imputation and classification in healthcare time series

## Quick Facts
- arXiv ID: 2509.19577
- Source URL: https://arxiv.org/abs/2509.19577
- Authors: Dohyun Ku; Catherine D. Chong; Visar Berisha; Todd J. Schwedt; Jing Li
- Reference count: 40
- Primary result: Unified framework for joint imputation and classification in healthcare time series data using hierarchical multi-task Gaussian processes

## Executive Summary
MAGIC is a unified framework for joint imputation and classification in healthcare time series data. It uses a hierarchical multi-task Gaussian process coupled with functional logistic regression to simultaneously perform class-informed missing value imputation and label prediction. The model employs Taylor expansion approximations to handle intractable likelihood components and uses an EM algorithm with block coordinate optimization for parameter estimation. In two healthcare applications—post-traumatic headache recovery prediction and ICU mortality prediction—MAGIC achieved superior predictive accuracy compared to existing methods. The framework enables real-time and accurate predictions with limited samples, facilitating early clinical assessment and treatment planning.

## Method Summary
MAGIC jointly models imputation and classification through a hierarchical multi-task Gaussian process where each patient's time series is decomposed as y_i(t) = μ_{z_i}(t) + δ_i(t) + ε_i(t). Class-specific mean processes μ_0(t) and μ_1(t) are estimated along with individual processes δ_i(t). The model uses an EM algorithm where the E-step computes posterior expectations of latent class means, and the M-step updates kernel hyperparameters and logistic regression coefficients via block coordinate ascent. Taylor expansion approximations handle intractable likelihood expectations, and B-spline basis functions reduce functional logistic regression to finite-dimensional optimization.

## Key Results
- MAGIC achieved superior predictive accuracy compared to existing methods in post-traumatic headache recovery and ICU mortality prediction tasks
- The framework enables real-time and accurate predictions with limited samples, facilitating early clinical assessment
- Classification accuracy improves with class-informed imputation, demonstrating the value of joint optimization over decoupled pipelines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-specific mean processes enable imputation that leverages outcome-relevant structure, improving both missing value recovery and downstream classification.
- Mechanism: Each patient's time series is decomposed as y_i(t) = μ_{z_i}(t) + δ_i(t) + ε_i(t), where μ_{z_i}(t) is a class-specific GP prior. During imputation, the posterior mean at unobserved times conditions on both observed values and the estimated class mean, pulling reconstructions toward class-consistent trajectories rather than generic interpolations.
- Core assumption: The underlying class-conditional trajectories are sufficiently distinct and smoothly varying; patients within a class share temporal structure beyond individual variation.
- Evidence anchors: [abstract] "hierarchical multi-task Gaussian process coupled with functional logistic regression to simultaneously perform class-informed missing value imputation and label prediction"; [section 4.1, Eq. 9-11] Defines hierarchical decomposition and class-specific GP priors; [corpus] Related work on Deep GPs for multi-stream imputation (arXiv:2505.12076) corroborates GP-based sharing across related signals, but does not incorporate class labels directly.
- Break condition: If classes have near-identical temporal trajectories, or if within-class variance vastly exceeds between-class variance, the class-specific prior provides minimal discriminative signal.

### Mechanism 2
- Claim: Joint optimization of imputation and classification parameters prevents error propagation from decoupled two-step pipelines.
- Mechanism: The EM algorithm iteratively (E-step) computes posterior expectations of latent class means μ_0, μ_1 given current parameters, then (M-step) updates kernel hyperparameters θ_0, θ_1, θ, noise σ², and logistic coefficients β via block coordinate ascent on the Q-function. Imputation and classification losses are coupled through the Q-function, so each iteration refines both tasks simultaneously.
- Core assumption: The block coordinate scheme reaches a useful stationary point; the likelihood surface is sufficiently well-behaved for L-BFGS-B sub-optimization.
- Evidence anchors: [section 4.2.3] "every limit point of the sequence generated by the MAGIC optimization algorithm is a stationary point" under monotonicity, continuity, and space-filling conditions; [section 4.2.2, Eq. 28-31] Explicit block decomposition separating θ_0, θ_1, β, and (θ, σ²) subproblems; [corpus] No direct corpus validation of this specific block scheme; general EM convergence (Meng & Rubin, 1993) is cited.
- Break condition: If the Q-function has poor curvature or highly coupled blocks, coordinate ascent may stall at poor local optima; initialization sensitivity increases.

### Mechanism 3
- Claim: Taylor expansion approximates the intractable label-likelihood expectation with bounded error, enabling tractable gradient-based optimization.
- Mechanism: The term E[log(1 + exp(x_i^T β))] is approximated via second-order Taylor expansion around E[x_i^T β] = U_i, yielding a closed-form involving U_i and Var[x_i^T β] = V_i. The remainder is O(||β_1||_2³), motivating the ℓ₂ penalty on β_1 to control approximation error.
- Core assumption: The linear predictor x_i^T β is approximately Gaussian (sub-Gaussian suffices); β_1 remains small enough for higher-order terms to be negligible.
- Evidence anchors: [section 4.2.2, Proposition 2-4] Derives U_i, V_i, and shows remainder bounded by O(Var[x_i^T β]^{3/2}) = O(||β_1||_2³); [section 4.2.2] "we include an ℓ₂-penalty on β_1 in the optimization, which ensures that the higher-order terms in the Taylor expansion remain small"; [corpus] Weak direct corpus support; standard practice in variational/approximate inference but not validated for this specific GP-logistic coupling.
- Break condition: If β_1 grows large (weak regularization) or V_i is large (high posterior uncertainty), the approximation degrades and optimization may diverge or yield biased estimates.

## Foundational Learning

- Concept: Gaussian Process Regression and Kernel Design
  - Why needed here: Core building block for modeling temporal dependencies and computing posterior distributions over missing values; RBF kernel hyperparameters control smoothness and extrapolation.
  - Quick check question: Given observations y at times t, write the posterior mean at new time t* under a GP prior with mean m(t) and RBF kernel k(t, t').

- Concept: Expectation-Maximization (EM) for Latent Variable Models
  - Why needed here: μ_0 and μ_1 are latent; EM iteratively computes their posterior (E-step) and maximizes expected complete-data likelihood (M-step).
  - Quick check question: Explain why EM guarantees non-decreasing likelihood under regularity conditions, and name one common failure mode.

- Concept: Functional Logistic Regression and Basis Expansions
  - Why needed here: Maps imputed time series to class probabilities via β_0 + ∫ β_1(t) f_i(t) dt; B-spline basis reduces infinite-dimensional coefficient to finite K-dimensional vector.
  - Quick check question: If β_1(t) is expanded in K B-spline basis functions, how many parameters does the functional logistic regression add? What happens if K is too large relative to sample size?

## Architecture Onboarding

- Component map:
  - Data layer: Irregularly sampled time series Y with binary labels Z; preprocessing handles time binning and normalization
  - GP prior layer: Class-specific mean processes μ_0(t), μ_1(t) with RBF kernels (hyperparameters θ_0, θ_1); individual processes δ_i(t) with shared kernel (θ)
  - Imputation module: GP posterior mean/covariance at unobserved times (Eq. 14); uses current μ_{z_i} estimates
  - Classification module: Functional logistic regression with B-spline basis (β) operating on imputed curves f_i(t)
  - Optimization loop: EM with block coordinate M-step (θ_0 → θ_1 → β → (θ, σ²)); L-BFGS-B per block; convergence when ||Θ^{(r)} - Θ^{(r-1)}|| < ε

- Critical path:
  1. Initialize Θ⁰ (kernel params, noise, β, mean priors m_0, m_1)
  2. E-step: Compute posterior (m̃_0, K̃_0), (m̃_1, K̃_1) via Eq. 21
  3. M-step: Solve four subproblems sequentially (Eq. 28-31)
  4. Check convergence; if not converged, return to step 2
  5. Prediction: For new sample, compute class-conditional GP posterior (Eq. 33), construct f_new(t), evaluate logistic probability (Eq. 35)

- Design tradeoffs:
  - Basis dimension K: Larger K captures finer temporal detail but risks overfitting; paper uses cubic B-splines but does not tune K systematically
  - Regularization λ on β_1: Controls Taylor approximation error vs. underfitting classification boundary
  - Roughness penalty R on μ_0, μ_1: Smoother class means improve stability with sparse data but may miss sharp class differences
  - Computational cost: GP covariance inversion is O(n³) per sample; total cost scales with number of time points and EM iterations

- Failure signatures:
  - Imputation yields near-constant or highly oscillatory curves: Check kernel hyperparameters (θ_v, θ_l may be poorly scaled); verify smoothing penalty R is active
  - Classification accuracy near chance: Inspect class mean posteriors (m̃_0, m̃_1)—if nearly identical, classes may not differ temporally; verify β is not collapsing to intercept
  - EM does not converge: Monitor Q-function values; check L-BFGS-B bounds and gradient norms; initialization may be poor or data too sparse
  - AUC improves but MSE degrades (or vice versa): Joint objective may favor one task; consider rebalancing penalty terms

- First 3 experiments:
  1. Reproduce simulation study (Section 5): Generate sinusoidal data with known missing ratios (0.5–0.8), compare MAGIC vs. SGP+logistic and MTGP+logistic on AUC and MSE; verify MAGIC's improvement increases with missingness
  2. Ablate class-informed imputation: Replace class-specific μ_{z_i} with global mean μ; measure performance drop to quantify contribution of class-informed prior
  3. Sensitivity to β_1 regularization: Vary λ across {0.001, 0.01, 0.1, 1.0} on PTH or ICU data; plot AUC, MSE, and ||β_1||_2 vs. λ to identify stable operating region

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation-specific hyperparameters (B-spline basis dimension K, initialization values, regularization strength λ) are not fully specified, making exact reproduction challenging
- The Taylor expansion approximation's accuracy across different data regimes is theoretically bounded but empirically untested
- Practical convergence behavior and sensitivity to initialization require empirical verification beyond theoretical guarantees

## Confidence

- **High Confidence**: The core methodological framework (hierarchical MTGP + functional logistic regression + EM optimization) is clearly specified and theoretically sound. The simulation setup and comparison baselines are well-defined.
- **Medium Confidence**: The Taylor expansion approximation and its bounded error are derived rigorously, but empirical validation of approximation quality across different data regimes is limited.
- **Medium Confidence**: The convergence proofs for the block coordinate EM algorithm are provided, but practical convergence behavior and sensitivity to initialization require empirical verification.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary K (B-spline basis dimension), λ (β₁ regularization), and initial θ values across multiple runs on both simulation and real datasets to map the stability landscape.

2. **Approximation Error Validation**: For a fixed dataset, compare the Taylor-approximated Q-function values against Monte Carlo estimates of the true expectation to quantify approximation error as a function of Vᵢ and β₁ magnitude.

3. **Ablation Studies on Class-Specific Priors**: Replace class-specific mean processes μ₀(t), μ₁(t) with a single global mean process and measure the degradation in both AUC and MSE to quantify the contribution of class-informed imputation.