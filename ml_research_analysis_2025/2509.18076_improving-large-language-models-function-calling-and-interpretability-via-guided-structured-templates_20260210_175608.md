---
ver: rpa2
title: Improving Large Language Models Function Calling and Interpretability via Guided-Structured
  Templates
arxiv_id: '2509.18076'
source_url: https://arxiv.org/abs/2509.18076
tags:
- function
- reasoning
- prompting
- template
- thought
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving function-calling
  accuracy and interpretability in large language models. It introduces a structured,
  template-based reasoning framework that guides models through explicit step-by-step
  reasoning processes for tool use.
---

# Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates

## Quick Facts
- arXiv ID: 2509.18076
- Source URL: https://arxiv.org/abs/2509.18076
- Reference count: 32
- Authors: Hy Dang; Tianyi Liu; Zhuofeng Wu; Jingfeng Yang; Haoming Jiang; Tao Yang; Pei Chen; Zhengyang Wang; Helen Wang; Huasheng Li; Bing Yin; Meng Jiang
- Primary result: Template-based reasoning improves function-calling accuracy by 3–12% relative to free-form CoT across multiple models and benchmarks

## Executive Summary
This work addresses the challenge of improving function-calling accuracy and interpretability in large language models. It introduces a structured, template-based reasoning framework that guides models through explicit step-by-step reasoning processes for tool use. The method is implemented through both prompting strategies and supervised fine-tuning on a synthetic dataset (ToolGT) generated using guided templates. Experimental results demonstrate consistent performance improvements across multiple model families and benchmarks: prompting strategies achieve 3–12% relative gains over baselines, while fine-tuning further enhances accuracy and robustness. The approach also increases interpretability by providing clear, structured reasoning chains, making model decisions more transparent and reliable for real-world applications.

## Method Summary
The method introduces guided-structured templates for function calling, consisting of a 7-step reasoning curriculum that models follow to generate function calls. The approach has two implementations: prompting (embedding the template into inference prompts) and fine-tuning (training models on a synthetic dataset called ToolGT). ToolGT is constructed via a 3-stage pipeline: (1) GPT-4o-mini generates reasoning chains for (query, tools, ground truth function call) triples using the template, (2) an LLM regenerates function calls from the reasoning to validate consistency, and (3) samples pass through Exact Match, AST structural comparison, and LLM-based semantic verification filters. Models are then fine-tuned on the filtered dataset using standard supervised learning with 8192-token context.

## Key Results
- Template-prompting improves function-calling accuracy by 3–12% relative to free-form CoT across BFCL and Nexus benchmarks
- Fine-tuning on ToolGT dataset enables smaller models (e.g., Mistral-7B) that fail at template-prompting to achieve 24.52 absolute gain in accuracy
- The structured reasoning approach provides interpretable decision-making chains while maintaining or improving task performance
- Template-based fine-tuning shows consistent gains on non-nested tasks but may degrade performance on complex nested scenarios due to training data limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured templates outperform free-form Chain-of-Thought for function-calling tasks.
- Mechanism: Templates enforce discrete reasoning stages (identify functions → assess relevancy → examine documentation → extract parameters → type conversion → draft call → revalidate), constraining the solution space and reducing reasoning drift that occurs in unconstrained CoT.
- Core assumption: Function-calling tasks have inherent structure that benefits from explicit decomposition rather than free-form reasoning.
- Evidence anchors:
  - [abstract] "our analysis reveals that free-form CoT is insufficient and sometimes counterproductive for structured function-calling tasks"
  - [section 4.1] GPT-4o-FC achieves 80.26 with Template vs. 78.83 with CoT; LLaMA-3-70B achieves 77.78/47.40 with Template vs. 75.75/46.10 with CoT across BFCL/Nexus
  - [corpus] Related work on Think-Augmented Function Calling (FMR=0.526) supports embedded reasoning for parameter accuracy, but corpus evidence for template superiority specifically is limited
- Break condition: Models heavily pre-trained on direct function-calling (e.g., Qwen-2.5-14B) may show degraded BFCL performance when reasoning is injected, though Template degrades less than CoT (-4.05 vs -15.08 absolute).

### Mechanism 2
- Claim: Fine-tuning internalizes structured reasoning, enabling models that fail at template prompting to succeed.
- Mechanism: Supervised fine-tuning on template-guided reasoning chains creates learned associations between query patterns and structured reasoning steps, removing inference-time prompting dependency while preserving reasoning benefits.
- Core assumption: Smaller models lack sufficient instruction-following capability to adhere to complex templates at inference time but can learn these patterns through gradient updates.
- Evidence anchors:
  - [section 4.2] Mistral-7B-Instruct-v0.3 improves from 46.88 (Template-prompting, worse than No-Thought 57.55) to 71.40 (Template-training) on BFCL—a 24.52 absolute gain
  - [section 4.1] Analysis attributes Mistral-7B prompting failures to "difficulty in following structured reasoning templates and formatting issues—due to limited instruction tuning"
  - [corpus] Related work on ToolPRM (FMR=0.526) suggests inference scaling benefits, but corpus does not directly validate fine-tuning internalization hypothesis
- Break condition: Training data must cover target task complexity; models fine-tuned on simple nested scenarios (ToolACE) showed degradation on complex Nexus tasks (-7% relative for Qwen No-Thought).

### Mechanism 3
- Claim: Multi-stage verification filters low-quality training samples, improving fine-tuning effectiveness.
- Mechanism: Two-stage filtering (AST/EM structural comparison + LLM semantic judgment) removes reasoning chains that produce incorrect or non-equivalent function calls, preventing reward hacking where plausible-looking reasoning leads to wrong outputs.
- Core assumption: Ground-truth function calls represent the correct distribution, and semantic equivalence can be reliably judged by LLMs.
- Evidence anchors:
  - [section 2.3] 11,488 single-turn samples filtered to 10,830 structured reasoning samples (94.1% retention)
  - [section 2.3] "This filtering process ensures that the final fine-tuning dataset includes high-quality examples, covering valid responses—supporting robust reasoning"
  - [corpus] No direct corpus validation of verification pipeline effectiveness; related work ToolRM (outcome reward models) is cited but not directly comparable
- Break condition: LLM judges may accept semantically plausible but practically incorrect alternatives; EM/AST miss order-independent correct calls without AST fallback.

## Foundational Learning

- Concept: **Joint modeling p(r, y|x, T)** — generating both reasoning chain r and function call y conditioned on query x and tools T, rather than directly predicting y.
  - Why needed here: The paper's core contribution extends function-calling from p(y|x, T) to p(r, y|x, T), making reasoning explicit and interpretable.
  - Quick check question: Can you explain why adding reasoning r to the objective improves transparency without sacrificing accuracy?

- Concept: **Curriculum learning / structured scaffolding** — guiding models through predetermined reasoning stages rather than unconstrained exploration.
  - Why needed here: The template design (7 steps from identification to revalidation) is explicitly "curriculum-inspired," decomposing complex function-calling into learnable sub-tasks.
  - Quick check question: What is the difference between CoT's "think step-by-step" and the paper's 7-step curriculum?

- Concept: **AST (Abstract Syntax Tree) comparison** — parsing function calls into tree structures for semantic equivalence checking beyond string matching.
  - Why needed here: The training sample filtering uses AST to accept parameter-order variations and optional-parameter differences that EM would incorrectly reject.
  - Quick check question: Why would `func(a=1, b=2)` and `func(b=2, a=1)` fail EM but pass AST comparison?

## Architecture Onboarding

- Component map:
  - Template-Prompting Module: Embeds 7-step curriculum into prompts (Identification → Relevancy → Documentation → Parameter Extraction → Conversion → Draft → Revalidate)
  - ToolGT Construction Pipeline (3 stages): Stage-1 generates reasoning chains with GPT-4o-mini + template + ground truth; Stage-2 validates by regenerating function calls from reasoning; Stage-3 filters via EM/AST + LLM judge
  - Supervised Fine-Tuning: Standard SFT on filtered (query, tools, reasoning, function-call) quadruples with 8192-token context

- Critical path:
  1. Start with Template-Prompting on target model to establish baseline
  2. If model shows formatting/instruction-following failures (like Mistral-7B), proceed to fine-tuning
  3. Construct domain-specific ToolGT variant if target tasks differ from BFCL/Nexus distribution
  4. Fine-tune and evaluate on held-out test set before deployment

- Design tradeoffs:
  - **Template complexity**: "Detail" template performs best overall but "Simple" excels on Relevancy tasks—consider adaptive selection
  - **Prompting vs. fine-tuning**: Prompting adds ~200-500 tokens per query; fine-tuning requires 15 GPU-hours (8×A100) but removes inference overhead
  - **Training data coverage**: ToolACE lacks complex nested scenarios—models fine-tuned on it degraded on Nexus; allocate budget for compositional task synthesis

- Failure signatures:
  - **Format drift**: Model outputs reasoning outside `<THINKING>` tags or function calls outside `<FUNCTION>` tags—mitigate with format-constrained decoding
  - **Reasoning-action mismatch**: Reasoning chain correct but function call wrong (Stage-2 catches this); or plausible reasoning leading to hallucinated parameters (Stage-3 LLM judge catches some)
  - **Overfitting to simple patterns**: Performance drops on nested/parallel calls (Qwen: 38.57→35.72 Nexus) despite BFCL gains—indicates training distribution mismatch

- First 3 experiments:
  1. **Establish prompting baseline**: Run No-Thought, CoT, and Template-Prompting on your target model using BFCLv2 non-executable subset; expect 3-12% relative gain from Template
  2. **Diagnose failure modes**: If Template-Prompting underperforms No-Thought (like Mistral-7B), analyze outputs for formatting errors and instruction-following failures; this signals need for fine-tuning
  3. **Fine-tune with verification ablation**: Train on ToolGT with EM-only vs. EM+AST+LLM filtering on a held-out validation set; measure function-call accuracy to quantify verification contribution (paper does not ablate this directly)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive framework dynamically select the optimal template complexity (e.g., Simple vs. Detail) based on specific task characteristics to maximize performance?
- Basis in paper: [explicit] The authors state in the Limitations section that different templates offer advantages in specific scenarios, suggesting a "promising direction for developing models that can adaptively select and apply the most suitable template."
- Why unresolved: The ablation study (Table 3) shows that while the "Detail" template is generally best, the "Simple" template occasionally outperforms it on specific subtasks like Relevancy.
- What evidence would resolve it: A dynamic routing mechanism that selects templates per query and outperforms static template baselines across all BFCL and Nexus sub-categories.

### Open Question 2
- Question: Does the structured reasoning framework generalize to multi-turn conversation settings where tool state and history must be maintained?
- Basis in paper: [explicit] The authors acknowledge in the Limitations that current experiments are restricted to single-turn tasks and that future work must "incorporate multi-turn tool, function calling settings."
- Why unresolved: The current methodology and ToolGT dataset construction filter multi-turn dialogues into single-turn instances, leaving the efficacy of structured templates across conversational turns untested.
- What evidence would resolve it: Evaluation results on the BFCLv3 multi-turn benchmark using a variant of ToolGT designed for sequential interaction.

### Open Question 3
- Question: To what extent does the lack of training data covering nested and compositional tool use contribute to performance degradation in complex scenarios?
- Basis in paper: [inferred] The authors hypothesize in Section 5.2 that the performance drop in fine-tuned Qwen and LLaMA models on the Nexus benchmark is due to ToolACE’s "limited coverage of complex, nested function-call scenarios."
- Why unresolved: The study observes that fine-tuning improves non-nested tasks but causes regression in overall scores, suggesting the model overfits to simpler patterns due to data limitations.
- What evidence would resolve it: Creating a dataset explicitly featuring nested calls and demonstrating that fine-tuning on it eliminates the performance regression on complex Nexus subsets.

## Limitations
- Heavy reliance on synthetic dataset generation via GPT-4o-mini, with effectiveness dependent on LLM-as-a-judge quality for filtering
- Performance degradation on complex nested scenarios after fine-tuning on simpler ToolACE data due to training distribution mismatch
- BFCLv2 evaluation protocol and ToolACE conversion methodology contain unspecified details affecting reproducibility

## Confidence
- **High Confidence**: Template-prompting superiority over free-form CoT (3-12% relative gains consistently observed across multiple model families and benchmarks)
- **Medium Confidence**: Fine-tuning effectiveness for smaller models that fail at template-prompting (24.52 absolute gain for Mistral-7B documented, but limited to one model)
- **Medium Confidence**: Structured reasoning interpretability benefits (qualitative improvement demonstrated but not systematically quantified)

## Next Checks
1. **Ablation of Training Data Verification**: Fine-tune on ToolGT subsets using only EM filtering vs. EM+AST+LLM filtering to quantify the contribution of each verification stage to final accuracy gains.

2. **Nested Scenario Generalization Test**: Create a synthetic test set with complex nested/parallel function calls (beyond BFCL/Nexus complexity) to measure whether fine-tuning on simple ToolACE data actually degrades performance on compositional tasks.

3. **Real-World Contamination Check**: Evaluate template-prompting and fine-tuned models on a held-out dataset from a different domain or collected post-publication to assess whether gains transfer beyond the BFCL/Nexus distribution.