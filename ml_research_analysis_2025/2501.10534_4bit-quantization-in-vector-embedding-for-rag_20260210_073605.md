---
ver: rpa2
title: 4bit-Quantization in Vector-Embedding for RAG
arxiv_id: '2501.10534'
source_url: https://arxiv.org/abs/2501.10534
tags:
- quantization
- vectors
- similarity
- vector
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory limitations of Retrieval-Augmented
  Generation (RAG) systems, which require large amounts of memory to store high-dimensional
  embedding vectors. The proposed solution is to use 4-bit quantization to reduce
  the memory requirements of these vectors while maintaining retrieval accuracy.
---

# 4bit-Quantization in Vector-Embedding for RAG

## Quick Facts
- arXiv ID: 2501.10534
- Source URL: https://arxiv.org/abs/2501.10534
- Reference count: 32
- 4-bit quantization with group size ≤128 achieves higher retrieval accuracy than HNSW while using 8x less memory

## Executive Summary
This paper addresses the memory limitations of RAG systems by proposing 4-bit quantization for embedding vectors. The approach uses group-wise quantization where high-dimensional vectors are partitioned into fixed-size groups, each with its own scaling factor. This enables INT4 precision without catastrophic accuracy loss. The study demonstrates that 8-bit quantization maintains accuracy with only slight degradation, while 4-bit quantization with group size 128 or less achieves higher accuracy than the HNSW algorithm, a state-of-the-art approximate nearest neighbor search method.

## Method Summary
The method employs symmetric linear quantization with the formula x_q = Clamp(Round[x/S]) where S = max(|x|)/(2^(b-1)). For INT4 quantization, high-dimensional vectors are split into groups of fixed size (32, 64, 128, or 256 elements), with each group computing its own scaling factor. This group-wise approach preserves local value distributions better than global scaling. The paper evaluates quantization effects on cosine similarity (RMSE vs FP32 baseline), retrieval accuracy (overlap ratio with baseline top-10 results), and semantic similarity correlation. Experiments use dbpedia-openai-1M-1536-angular dataset, STS datasets (sts-metb, str-2022, SICK), and bge-large-en-v1.5 embedding model.

## Key Results
- 4-bit quantization with group size ≤128 achieves higher retrieval accuracy than HNSW algorithm
- 8-bit quantization maintains accuracy with only slight degradation (83% retrieval accuracy)
- INT4 with group size 32 achieves 67% retrieval accuracy vs baseline; PQ configurations all fall below 10% accuracy
- INT4 correlation coefficient 0.8285 (ratio 0.9592 vs FP32) compared to PQ[32,256] at 0.6391 (ratio 0.7400)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-wise quantization enables INT4 precision without catastrophic accuracy loss by adapting scale factors to local value distributions within each vector.
- Mechanism: Instead of applying a single global scale to an entire high-dimensional vector (e.g., 1536 dimensions), the vector is partitioned into fixed-size groups (e.g., 32, 64, 128 elements). Each group computes its own scaling factor S = max(|x|) / (2^(b-1)), allowing local value ranges to be preserved. This reduces the quantization loss |x - x_dq| because outlier values in one region don't dominate the scale for unrelated dimensions.
- Core assumption: Embedding vector dimensions exhibit non-uniform value distributions where local scaling better preserves relative magnitudes than global scaling.
- Evidence anchors:
  - [section III.D] "Directly quantizing high-dimensional vectors as an entirety with the same quantization scales can significantly degrade the accuracy in INT4 quantization. In the case of group-wise quantization, a high-dimensional vector would be split into several groups with same group size."
  - [Table I] Shows RMSE increasing with group size: INT4 with group size 32 has RMSE 0.0048, while group size 256 has RMSE 0.0324—nearly 7x worse.
  - [corpus] Weak direct evidence for group-wise specifically in vector embeddings; most corpus papers focus on LLM weight quantization (LUQ, R2Q, ParetoQ) rather than embedding quantization.
- Break condition: If embedding dimensions are highly correlated or have near-uniform distributions, group-wise provides minimal benefit over global scaling.

### Mechanism 2
- Claim: INT4 quantization preserves semantic similarity relationships significantly better than Product Quantization (PQ) for retrieval tasks.
- Mechanism: Symmetric linear quantization maps continuous float values to 16 discrete integer levels (-8 to 7) while preserving the relative ordering of values. PQ instead subdivides vectors into sub-vectors and replaces each with a centroid ID, which fundamentally alters distance computations by introducing discretization errors in the distance metric itself rather than just the vector representation.
- Core assumption: Cosine similarity computation on dequantized vectors approximates FP32 similarity well enough that ranking order is preserved for top-k retrieval.
- Evidence anchors:
  - [Table III] SICK dataset shows INT4 correlation coefficient 0.8285 (ratio 0.9592 vs FP32) compared to PQ[32,256] at 0.6391 (ratio 0.7400)—a ~22 percentage point gap.
  - [Table II] INT4 with group size 32 achieves 67% retrieval accuracy vs baseline; PQ configurations all fall below 10% accuracy (stated in Section V: "the overlap ratio...was less than 0.1").
  - [corpus] No direct corpus comparison; corpus papers focus on LLM quantization rather than embedding comparison methods.
- Break condition: If downstream tasks require absolute similarity scores rather than relative rankings, INT4's ~4% correlation degradation may become problematic.

### Mechanism 3
- Claim: INT4 with group size ≤128 achieves higher retrieval accuracy than HNSW approximate nearest neighbor search while using 8x less memory than FP32.
- Mechanism: Exact KNN search on quantized vectors trades precision for memory while maintaining exact comparison logic. HNSW trades accuracy for speed via graph traversal that may miss true neighbors. The paper shows that quantization error at appropriate group sizes introduces less ranking disruption than HNSW's approximation.
- Core assumption: The HNSW configuration used (M=64, ef=50) is representative of typical production deployments; vector database fits in reduced memory without additional indexing overhead.
- Evidence anchors:
  - [Figure 7 description] "the dotted line represents the accuracy achieved by the HNSW algorithm (M=64 and ef=50)...INT4 with a group size of 128 or less yields higher accuracy than the HNSW algorithm."
  - [Table II] INT4 group size 128 achieves 45% accuracy, group size 64 achieves 56%, group size 32 achieves 67%—all above the HNSW baseline line in Figure 7.
  - [corpus] H1B-KV and LogQuant papers explore binary/sub-4-bit quantization for KV caches, suggesting ongoing interest in extreme compression, but don't address embedding retrieval directly.
- Break condition: If specialized INT4 hardware instructions are unavailable (noted in Section VI), expected speed gains may not materialize, reducing the tradeoff advantage over HNSW.

## Foundational Learning

- **Concept: Symmetric Linear Quantization**
  - Why needed here: The paper's entire method relies on mapping FP32 values to INT4 using the formula x_q = Clamp(Round[x/S]) where S = max(|x|)/(2^(b-1)). Understanding this transform is essential for implementing and debugging quantization.
  - Quick check question: Given a vector [0.5, -0.3, 0.9, -0.1] and 4-bit quantization, what is the scaling factor S and what values does the quantized vector contain?

- **Concept: Cosine Similarity vs Euclidean Distance**
  - Why needed here: The paper evaluates retrieval using cosine similarity, which measures angle between vectors rather than magnitude. Quantization affects both metrics differently, and the choice determines whether normalization is required before quantization.
  - Quick check question: If two vectors have cosine similarity 0.8 in FP32, and quantization changes one vector's magnitude by 10% while preserving direction, what happens to their Euclidean distance vs cosine similarity?

- **Concept: Exact vs Approximate Nearest Neighbor Trade-offs**
  - Why needed here: The paper positions INT4+KNN against HNSW. Understanding why HNSW sacrifices accuracy (graph traversal may miss true neighbors) versus why quantization sacrifices accuracy (representation precision loss) clarifies when each approach is appropriate.
  - Quick check question: For a 1M vector database requiring 99% recall@10, would you choose HNSW with high ef parameter or quantized exact KNN? What if memory is constrained to 2GB?

## Architecture Onboarding

- **Component map:**
  [Raw Documents] → [Embedding Model (e.g., bge-large-en-v1.5)] → [FP32 Embedding Vectors (1024-4096 dim)] → [Group-wise Quantizer] → [INT4/INT8 Quantized Vectors + Scale Factors per Group] → [Vector Database Storage] → [Query Time: Quantize Query → Dequantize for Similarity → Rank Results]

- **Critical path:**
  1. Choose group size based on accuracy requirements (32=high accuracy, 128=acceptable accuracy, 256=degraded)
  2. Pre-compute per-group scale factors during indexing
  3. Store both quantized vectors AND scale factor metadata (overhead: dim/group_size × sizeof(float))
  4. At query time: quantize query using same group structure, compute similarities, dequantize top-k for final ranking if needed

- **Design tradeoffs:**
  - **Group size 32 vs 128:** 32 preserves more accuracy (67% vs 45% retrieval overlap) but requires 4x more scale factor storage
  - **INT4 vs INT8:** INT4 gives 2x better compression but accuracy drops from 83% (INT8) to 28-67% (INT4 depending on group size)
  - **Quantized exact KNN vs HNSW:** Quantized KNN gives deterministic results; HNSW gives variable quality depending on graph construction but faster search without INT4 hardware support

- **Failure signatures:**
  - Retrieval accuracy drops below HNSW baseline → group size likely too large (try reducing from 256 to 64)
  - Correlation with FP32 below 0.90 → check if embedding model outputs values outside expected range; may need calibration dataset
  - Memory savings less than expected → scale factor overhead consuming gains; verify group count × 4 bytes is small relative to vector data

- **First 3 experiments:**
  1. **Baseline calibration:** On your embedding dataset, compute pairwise cosine similarities for 1000 random vectors in FP32, BF16, INT8, and INT4 (group sizes 32, 64, 128). Plot RMSE vs FP32 to verify paper's Table I pattern applies to your data.
  2. **Retrieval accuracy test:** Split your corpus 90/10, select 10 diverse query vectors from the 90% split, retrieve top-10 from 10% using each quantization method, measure overlap with FP32 baseline. Target: INT4 g=64 should achieve >50% overlap.
  3. **Production memory validation:** Load your full corpus with INT4 g=64 quantization, measure actual RAM usage including scale factors. Verify: (original_size / 8) + scale_overhead < expected_target. Compare against HNSW index memory for same corpus.

## Open Questions the Paper Calls Out
- What is the actual search latency speedup from INT4 quantization, particularly with and without native hardware INT4 support? The paper states that measuring actual impact on searching speed was not done in this work.
- How well does group-wise INT4 quantization generalize across diverse embedding models and dimensions (512–4096)? The experiments use only two embedding configurations (1024 and 1536 dimensions).
- Does the retrieval accuracy degradation from 4-bit quantization measurably impact end-to-end RAG task performance (e.g., answer correctness, hallucination rates)? The paper evaluates retrieval overlap but does not measure downstream LLM response quality.

## Limitations
- Performance may vary significantly with different vector dimensions or embedding architectures beyond the 1536 dimensions tested
- The computational overhead of group-wise scaling factors and impact on query latency in production systems is not addressed
- The selection of 10 "near-orthogonal" query vectors for retrieval accuracy testing is not precisely specified, which could affect reproducibility

## Confidence

**High Confidence**: The RMSE degradation patterns with increasing group sizes (INT4 g=32 at 0.0048 vs g=256 at 0.0324) are well-supported by quantitative measurements in Table I. The retrieval accuracy comparisons with HNSW baseline are clearly demonstrated in Figure 7 with specific accuracy percentages.

**Medium Confidence**: The claim that INT4 with group size ≤128 achieves higher accuracy than HNSW is supported but depends on the specific HNSW configuration (M=64, ef=50) chosen. Different parameter settings could yield different results. The semantic similarity correlation improvements over PQ are shown but limited to three specific datasets.

**Low Confidence**: The assertion that this method is broadly applicable to "RAG systems" is not fully substantiated, as the evaluation focuses on a single retrieval task and dataset. The memory savings calculations don't account for the overhead of storing per-group scale factors in production scenarios.

## Next Checks

1. **Dataset Generalization Test**: Apply the 4-bit quantization method to a different embedding dataset with varying dimensions (e.g., 768, 2048) and compare RMSE/retrieval accuracy patterns to verify if the group size recommendations (≤128 for acceptable accuracy) hold across dimensions.

2. **Production Memory Validation**: Implement the complete pipeline including scale factor storage overhead, measure actual memory consumption on a production-sized corpus (100M+ vectors), and compare against the theoretical 8x savings claim to validate real-world applicability.

3. **Latency Impact Analysis**: Benchmark query latency for INT4 quantized search versus FP32 and HNSW baselines, measuring the tradeoff between memory savings and search speed, particularly focusing on whether the lack of INT4 hardware support significantly impacts performance as mentioned in Section VI.