---
ver: rpa2
title: 'HI-TransPA: Hearing Impairments Translation Personal Assistant'
arxiv_id: '2511.09915'
source_url: https://arxiv.org/abs/2511.09915
tags:
- speech
- arxiv
- hi-transpa
- multimodal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HI-TransPA addresses communication barriers faced by hearing-impaired
  individuals by introducing an instruction-driven audio-visual personal assistant
  that jointly processes indistinct speech and lip dynamics. The method employs a
  comprehensive multimodal preprocessing pipeline with facial landmark detection and
  stabilization, combined with a quality-aware curriculum learning strategy that trains
  first on high-confidence samples and progressively incorporates harder cases.
---

# HI-TransPA: Hearing Impairments Translation Personal Assistant

## Quick Facts
- arXiv ID: 2511.09915
- Source URL: https://arxiv.org/abs/2511.09915
- Reference count: 30
- Primary result: Achieves CS=0.79, CER=27% on HI-Dialogue test set, outperforming baseline ASR models and generic Omni-Models

## Executive Summary
HI-TransPA introduces an instruction-driven audio-visual personal assistant that jointly processes indistinct speech audio and lip dynamics to improve communication for hearing-impaired individuals. The system employs a quality-aware curriculum learning strategy that first trains on high-confidence samples before progressively incorporating harder cases, combined with a novel unified 3D-Resampler for efficient lip dynamics encoding. Experiments on a purpose-built HI-Dialogue dataset demonstrate state-of-the-art performance with significant error reduction compared to audio-only approaches.

## Method Summary
HI-TransPA uses Qwen2.5-Omni-3B as base model with SigLIP Vision Encoder and a novel 3D-Resampler (64 learnable queries) for visual processing. Training follows a three-stage alignment: general visual alignment on external datasets, AVSR co-adaptation on Chinese-LiPS, then instruction fine-tuning with quality-aware curriculum learning. Samples are partitioned into accept/reject sets using composite quality scores (S_comp ≥ 0.55), with 3 epochs on accept set followed by 5 epochs on reject set. The system supports both /translate and /chat modes through instruction tuning.

## Key Results
- Achieves Comprehensive Score (CS) of 0.79, significantly outperforming baseline ASR models and generic Omni-Models (CS 0.65-0.67)
- Reduces character error rate (CER) from 42% to 27% through joint audio-visual processing
- Ablation shows visual modality improves CS from 0.64 to 0.70 and reduces CER from 46% to 37%
- Curriculum learning improves CS from 0.70 to 0.79 and reduces CER from 37% to 27%

## Why This Works (Mechanism)

### Mechanism 1
Joint audio-visual processing reduces transcription errors by providing complementary disambiguation cues. Indistinct speech audio fused with lip dynamics enables both translation and dialogue within a single multimodal framework. When audio alone is ambiguous (phoneme confusions typical in hearing-impaired articulation), visual lip configurations constrain possible interpretations, reducing substitution and deletion errors. Core assumption: Hearing-impaired speakers produce lip movements that retain meaningful correspondence to intended phonemes even when acoustic output is distorted.

### Mechanism 2
Quality-aware curriculum learning improves generalization to heterogeneous, noisy hearing-impaired speech by establishing stable multimodal alignment before exposure to hard samples. Samples scored via composite audio quality (ASR confidence + SNR) and video quality (motion magnitude) are partitioned into accept (S_comp ≥ 0.55) and reject sets. Stage 1 trains on accepted samples for 3 epochs; Stage 2 continues on rejected samples for 5 epochs. This easy-to-hard schedule prevents early training instability from noisy gradients.

### Mechanism 3
The unified 3D-Resampler enables efficient encoding of high-frame-rate lip dynamics, preserving spatiotemporal cues critical for phoneme discrimination that generic vision encoders lose. SigLIP Vision Encoder produces patch tokens Z_patch from lip video, and the 3D-Resampler applies cross-attention with N_q = 64 learnable queries to compress the token sequence to Z_fused ∈ R^(Nq×D_llm), retaining essential temporal dynamics while reducing computational load.

## Foundational Learning

- **Audio-Visual Speech Recognition (AVSR)**: Why needed - HI-TransPA fuses audio and lip visual streams; understanding how modalities complement each other is essential for debugging fusion failures. Quick check: Given an audio-only model with 40% CER on distorted speech, what error types (substitutions, deletions) would you expect lip information to reduce most?

- **Curriculum Learning**: Why needed - The quality-aware two-stage training directly depends on understanding how easy-to-hard schedules affect gradient stability and convergence. Quick check: If Stage 1 (clean data) trains for 3 epochs and Stage 2 (noisy data) for 5 epochs, what would happen if you reversed the order?

- **Cross-Attention and Token Resampling**: Why needed - The 3D-Resampler uses learned queries to compress spatiotemporal tokens; understanding cross-attention clarifies what information is retained vs. discarded. Quick check: With 64 learnable queries attending to ~1000+ video patch tokens, what inductive bias does this architecture impose on temporal compression?

## Architecture Onboarding

- **Component map**: Raw audio + stabilized lip video → SigLIP encoder → 3D-Resampler → Z_fused → Qwen2.5-3B → Token generation for /translate or /chat outputs
- **Critical path**: 1) Video → facial landmark detection (468 keypoints) → lip crop stabilization → SigLIP encoder → 3D-Resampler → Z_fused (64 tokens) 2) Audio → audio encoder → audio hidden states 3) Z_fused + audio hidden states + text tokens → LLM → output
- **Design tradeoffs**: 3B parameter model vs. 7B for inference efficiency; curriculum learning compensates for capacity. 64 resampler queries balance compression efficiency vs. temporal detail retention; risk of under-representing fine lip motion. Threshold 0.55 for accept/reject partition is heuristic; higher threshold yields cleaner Stage 1 data but fewer samples.
- **Failure signatures**: High CER with high EmbSim indicates model captures semantics but misses literal phonemes - check audio encoder alignment. Large gap between /translate and /chat quality suggests instruction tuning may have overfitted to one mode. CS drops on specific speakers indicates quality scoring may have systematically rejected their samples from Stage 1.
- **First 3 experiments**: 1) Ablate visual modality: Run audio-only variant on HI-Dialogue test set; expect CS ~0.64, CER ~46%. Confirms visual contribution. 2) Vary curriculum schedule: Train with reversed order (hard→easy) and compare CS; expect instability or degraded performance. Validates curriculum hypothesis. 3) Resampler query count sweep: Test N_q ∈ {32, 64, 128} on validation set; plot CS vs. inference latency. Identifies compression sweet spot.

## Open Questions the Paper Calls Out

- **Generalization to broader population**: Can HI-TransPA generalize effectively to the broader hearing-impaired population given the limited demographic diversity of the training data? The model's state-of-the-art performance is demonstrated on a test set derived from the same small group of six volunteers, leaving its robustness to diverse physiological speech patterns and etiologies unknown. Evaluation on an external, larger-scale dataset containing speakers with varying degrees and types of hearing loss not seen during training would resolve this.

- **Qualitative dialogue factors**: How can the model be optimized for qualitative dialogue factors like empathy and fluency, which were omitted from the current evaluation? While the "Chat mode" generates contextually relevant responses, optimizing solely for the Comprehension Score (CS) may not ensure the natural or empathetic interaction required for a "personal assistant." A human or model-based evaluation specifically rating the social appropriateness, tone, and fluency of generated dialogue responses would resolve this.

- **Severe occlusion and extreme poses**: How does the system perform under severe visual occlusion or extreme head poses that were excluded during data curation? Real-world "daily communication" often involves partial face coverings or erratic movement; by filtering these out, the model's practical robustness in unconstrained environments remains untested. Ablation studies testing the model's degradation curves when synthetic occlusion or extreme pose noise is applied to visual input would resolve this.

## Limitations

- Quality-aware curriculum learning depends on heuristic thresholds (S_comp ≥ 0.55) without sensitivity analysis showing how different thresholds affect performance
- The unified 3D-Resampler represents a novel architectural choice but lacks ablation studies varying query counts or comparisons to alternative spatiotemporal compression methods
- The HI-Dialogue dataset, while purpose-built, is relatively small (9,673 samples) compared to general speech datasets, raising questions about robustness and generalization

## Confidence

- **High confidence**: The core empirical claim that HI-TransPA achieves state-of-the-art performance with CS=0.79, CER=27% on the HI-Dialogue test set. Experimental methodology and metric definitions are clearly specified.
- **Medium confidence**: The claim that joint audio-visual processing reduces transcription errors for hearing-impaired speech. Ablation studies show performance drops when removing visual modality, but analysis of specific error types affected is lacking.
- **Medium confidence**: The curriculum learning hypothesis that easy-to-hard training schedules improve robustness. Two-stage training shows performance gains, but controlled experiments comparing alternative schedules are absent.
- **Low confidence**: The architectural claim that the unified 3D-Resampler is "critical for accurate interpretation." Performance improvements over generic Omni-Models are shown, but no ablation study isolates the 3D-Resampler's contribution.

## Next Checks

1. **Error type analysis**: Run the audio-only and audio-visual variants on HI-Dialogue test set and categorize errors by type (substitutions, deletions, insertions). If visual modality primarily reduces substitutions related to labial phonemes (b, p, m) but not other sound classes, this would support the mechanism that lip information disambiguates specific phonetic confusions.

2. **Curriculum schedule ablation**: Train three variants: (a) standard curriculum (easy→hard), (b) reversed curriculum (hard→easy), and (c) random order mixing all samples. Compare CS and CER on validation set after equivalent total training time. If reversed curriculum shows degraded performance or training instability, this would validate the easy-to-hard hypothesis.

3. **Resampler capacity sweep**: Test HI-TransPA with 3D-Resampler query counts of 32, 64, and 128 on validation set. Plot CS vs. inference latency to identify the point of diminishing returns. If CS plateaus between 64-128 queries while latency increases linearly, this would indicate that 64 queries represent a reasonable compression-accuracy tradeoff.