---
ver: rpa2
title: 'Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts
  on Hundred-Billion-Scale MoE'
arxiv_id: '2512.07710'
source_url: https://arxiv.org/abs/2512.07710
tags:
- reward
- reasoning
- training
- learning
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles inefficiencies in scaling reinforcement learning\
  \ (RL) for hundred-billion-parameter Mixture-of-Experts (MoE) models, specifically\
  \ addressing zero-variance prompts that waste rollouts, unstable importance sampling,\
  \ and train\u2013inference discrepancies. To solve these, the authors propose a\
  \ unified RL framework that includes (1) Multi-Stage Zero-Variance Elimination to\
  \ prune non-informative prompts and enhance GRPO stability; (2) Entropy Importance\
  \ Sampling Policy Optimization (ESPO) to adaptively balance token- and sequence-level\
  \ sampling; (3) Router Replay to align training and inference router decisions;\
  \ (4) a high-throughput RL system with FP8 rollouts, overlapped reward computation,\
  \ and length-aware scheduling."
---

# Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE

## Quick Facts
- arXiv ID: 2512.07710
- Source URL: https://arxiv.org/abs/2512.07710
- Authors: Anxiang Zeng; Haibo Zhang; Hailing Zhang; Kaixiang Mo; Liang Yao; Ling Hu; Long Zhang; Shuman Liu; Shuyi Xie; Yanshi Li; Yizhang Chen; Yuepeng Sheng; Yuwei Huang; Zhaochen Xu; Zhiqiang Zhou; Ziqin Liew
- Reference count: 3
- Primary result: CompassMax-V3-Thinking achieves SEA average of 86.41 across seven languages and 85.79 in-house e-commerce domain average

## Executive Summary
This work tackles inefficiencies in scaling reinforcement learning (RL) for hundred-billion-parameter Mixture-of-Experts (MoE) models, specifically addressing zero-variance prompts that waste rollouts, unstable importance sampling, and train–inference discrepancies. To solve these, the authors propose a unified RL framework that includes (1) Multi-Stage Zero-Variance Elimination to prune non-informative prompts and enhance GRPO stability; (2) Entropy Importance Sampling Policy Optimization (ESPO) to adaptively balance token- and sequence-level sampling; (3) Router Replay to align training and inference router decisions; (4) a high-throughput RL system with FP8 rollouts, overlapped reward computation, and length-aware scheduling. Empirically, the resulting CompassMax-V3-Thinking model shows strong performance, with a SEA average of 86.41 across seven languages, an in-house e-commerce domain average of 85.79, and a general ability average of 76.01. These results demonstrate that the proposed methods enable stable, efficient RL training at scale, yielding both higher reasoning quality and production-level robustness.

## Method Summary
The approach addresses three core RL training challenges in hundred-billion-scale MoE models: (1) zero-variance prompts that waste rollouts by proposing Multi-Stage Zero-Variance Elimination with expanded exploration, reward reshaping, and stochastic advantage estimation; (2) unstable importance sampling through Entropy Importance Sampling Policy Optimization (ESPO) that groups tokens by entropy and applies adaptive clipping; (3) train-inference discrepancies via Router Replay that logs and reuses inference-time router decisions during training. The framework is implemented in a two-phase RL pipeline starting from a cold-start Long-CoT SFT model, followed by domain-specific model merging via TIES, and finally two-phase GRPO training with ESPO, Router Replay, and FP8 optimizations. The system achieves high throughput through overlapped reward computation and length-aware load balancing.

## Key Results
- CompassMax-V3-Thinking achieves SEA average of 86.41 across seven languages
- In-house e-commerce domain average of 85.79 demonstrates strong domain-specific reasoning
- General ability average of 76.01 shows robust performance across reasoning tasks
- Multi-Stage ZVP Elimination reduces zero-variance rate by 17% through expanded exploration
- Router Replay reduces train-inference log-prob discrepancy from 10^-3 to 10^-4, stabilizing RL training

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage filtering and reshaping of zero-variance prompts preserves gradient signal and accelerates convergence. Three-stage intervention: (1) expand exploration space by increasing sampling size N during rollout to naturally reduce uniform reward groups; (2) reshape rewards using pass-rate-based scoring plus length/repetition penalties; (3) inject controlled stochasticity into advantage estimation for remaining zero-variance groups (RL-ZVP). This maintains useful gradient signal where naive dynamic sampling would discard rollouts entirely. Core assumption: Zero-variance prompts indicate wasted computation rather than solved tasks; variance can be induced through targeted intervention without corrupting learning signal. Evidence anchors: [abstract] "zero-variance prompts that waste rollouts...Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization"; [section 2.4.1] "experimental results show that expanding the exploration space reduced the overall zero-variance rate by 17%".

### Mechanism 2
Entropy-grouped importance sampling with adaptive clipping stabilizes long-horizon policy optimization better than uniform sequence-level or token-level treatment. ESPO decomposes each sequence into entropy-coherent token groups, assigns independent importance sampling ratios per group using length-normalized sequence ratios with stop-gradient, and applies entropy-adaptive clipping bounds (ε_τ = α × average_entropy / log|V|). High-entropy tokens receive independent treatment rather than being averaged with predictable tokens. Core assumption: High-entropy tokens correspond to decision points requiring more granular credit assignment; 80/20-style concentration of learning signal in minority tokens holds for MoE reasoning models. Evidence anchors: [abstract] "entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics"; [section 2.4.2] "ESPO decomposes each sequence into entropy-coherent token groups and performs policy optimization at the token-group level".

### Mechanism 3
Replaying inference-time router decisions during training log-probability computation eliminates train-inference mismatch that causes RL collapse in MoE models. During vLLM rollout, log all MoE router gating decisions for each token. During Megatron training phase, directly reuse these logged decisions instead of recomputing router selections. This ensures the expert paths used for probability computation match those used during generation. Core assumption: The primary source of train-inference log-probability discrepancy in MoE models is nondeterministic router decisions, not floating-point precision alone; determinism in routing is sufficient for stable importance sampling. Evidence anchors: [abstract] "Router Replay to align training and inference router decisions to mitigate train–infer discrepancies"; [section 2.4.4] "reducing the discrepancy from the order of 10^-3 to 10^-4 and effectively stabilizing RL training".

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: The paper's zero-variance problem is specific to group-based advantage estimation where all samples in a group receive identical rewards.
  - Quick check question: Given 8 rollouts for a prompt with rewards [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], what is the advantage for each sample?

- **Concept: Importance Sampling in RL**
  - Why needed here: ESPO's core contribution is modifying how importance ratios are computed and clipped; understanding standard PPO/GSPO approaches is prerequisite.
  - Quick check question: Why does the importance sampling ratio ρ = π(a|s) / π_old(a|s) become unstable over long token sequences?

- **Concept: MoE (Mixture of Experts) Routing**
  - Why needed here: Router Replay addresses a MoE-specific failure mode; need to understand how top-k gating decisions affect token representations.
  - Quick check question: In a top-2 MoE layer with 64 experts, what information must be logged during inference to exactly reproduce the forward pass during training?

## Architecture Onboarding

- **Component map**:
  [Cold-Start SFT] → [Model Merge with Domain Checkpoint] → [Thinking Base] → [Phase 1 RL: Code/Math/Instruction] → [Phase 2 RL: + E-commerce/Tool/QA] → [CompassMax-V3-Thinking]

- **Critical path**: Rollout generation dominates runtime (~61% per Figure 5); FP8 quantization and load balancing target this bottleneck directly. Reward computation overlap provides marginal gains once rollout is optimized.

- **Design tradeoffs**:
  - Larger N (sampling size) reduces zero-variance rate but linearly increases rollout cost; paper found optimal N through monitoring ZV rate vs compute.
  - FP8 rollout trades precision for 1.6× throughput; requires validation that quality degradation is acceptable.
  - TIES merging was chosen over TA/DARE for noise elimination but may not preserve all domain capabilities equally.

- **Failure signatures**:
  - Zero-variance rate >10%: Indicates exploration space too narrow or prompts too easy/hard uniformly.
  - Advantage flipping near mean rewards: Switch from BT-style reward model to GenRM with ternary classification.
  - RL collapse after O(100) steps with sudden loss spike: Check train-inference log-prob discrepancy; enable Router Replay.
  - Straggler workers in rollout: Length distribution has long tail; enable length-aware scheduling.

- **First 3 experiments**:
  1. **Profile baseline**: Run single RL iteration with profiling enabled (Figure 5 style); identify which stage dominates and whether zero-variance rate is problematic (>5%).
  2. **Ablate Router Replay**: Train two runs with/without Router Replay on same seed; measure log-prob discrepancy and training stability (steps before collapse or divergence).
  3. **Validate ESPO vs GSPO**: On a held-out reasoning benchmark, compare convergence speed and final performance between entropy-grouped vs uniform importance sampling; monitor gradient variance.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal trade-off curve for weighting long-CoT capabilities against domain-specific skills during model merging in Mixture-of-Experts architectures? Basis in paper: [explicit] Section 2.3 states that the weight of the long-CoT model affects merging results significantly and notes this is the "first systematic exploration," implying the exact parameterization for different domains remains an open investigation. Why unresolved: The paper provides empirical insights for their specific case but does not offer a theoretical or generalized rule for determining optimal merge weights across varying model capacities or domains. What evidence would resolve it: Ablation studies showing performance retention across a spectrum of weighting coefficients for both reasoning and domain-specific tasks.

### Open Question 2
Does the Router Replay mechanism introduce memory or communication bottlenecks that limit scalability to contexts significantly longer than the 32k tokens evaluated? Basis in paper: [inferred] Section 2.4.4 describes logging and reusing gating decisions for "all tokens" to stabilize training. While effective for stability, the storage overhead for router logs in extreme-length contexts is not quantified. Why unresolved: The paper verifies stability and discrepancy reduction but does not profile the memory footprint or latency impact of the logging mechanism itself as context length scales. What evidence would resolve it: Profiling data on memory usage and throughput for Router Replay specifically on 128k+ context windows compared to standard inference.

### Open Question 3
Does the ESPO entropy-adaptive clipping threshold impede convergence speed during the final stages of training where policy entropy naturally collapses? Basis in paper: [inferred] Section 2.4.2 defines the clipping bound $\epsilon_\tau$ as proportional to the token group entropy. In late-stage training where models become deterministic (low entropy), the clipping behavior changes dynamically, potentially affecting gradient flow. Why unresolved: The paper demonstrates improved stability and overall performance but does not analyze if the adaptive clipping slows down the final "fine-tuning" phase where entropy is low. What evidence would resolve it: A learning curve analysis specifically isolating the gradient norms and convergence speed of ESPO versus fixed-clipping baselines in the low-entropy training regime.

## Limitations

- Evaluation relies heavily on in-house benchmarks (e-commerce domain average, general ability average) that are not publicly accessible for independent verification
- The relative contribution of each proposed component (ZVP elimination, ESPO, Router Replay) to final performance is not systematically ablated or quantified
- Computational cost-benefit analysis is incomplete - total compute cost for full RL pipeline relative to baseline approaches is not quantified

## Confidence

**High Confidence**: The core mechanisms of Multi-Stage Zero-Variance Elimination and Router Replay are technically sound and address well-documented failure modes in RL for MoE models. The entropy-grouped importance sampling (ESPO) follows established principles of adaptive credit assignment.

**Medium Confidence**: The empirical improvements shown in SEA average (86.41) and general ability average (76.01) are likely real but difficult to independently verify without access to proprietary benchmarks.

**Low Confidence**: The paper presents the methods as a unified framework without isolating individual effects through systematic ablation studies.

## Next Checks

1. **Component Ablation Study**: Run controlled experiments isolating each proposed method (Multi-Stage ZVP Elimination, ESPO, Router Replay) on a public MoE model (e.g., Mixtral-8x7B) using standard benchmarks like HumanEval and Big-Bench Hard. Measure individual contribution to convergence speed and final performance to quantify the 80/20 effect of each component.

2. **Zero-Variance Rate Analysis**: Systematically vary the exploration parameter N and measure zero-variance rate reduction vs. computational overhead. Plot ZVP rate against rollout cost to identify the optimal trade-off point and validate the claimed 17% reduction from exploration expansion.

3. **Train-Inference Log-Probability Discrepancy Measurement**: Implement Router Replay on a standard MoE setup and measure the L2 distance between vLLM and Megatron log-probabilities before and after Router Replay across multiple sequences. Quantify the reduction from 10^-3 to 10^-4 and correlate this reduction with training stability metrics (steps to collapse, gradient variance).