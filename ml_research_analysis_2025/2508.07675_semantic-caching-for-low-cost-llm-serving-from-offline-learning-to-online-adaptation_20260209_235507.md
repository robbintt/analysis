---
ver: rpa2
title: 'Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online
  Adaptation'
arxiv_id: '2508.07675'
source_url: https://arxiv.org/abs/2508.07675
tags:
- cache
- cost
- caching
- query
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing inference costs
  for large language models (LLMs) through semantic caching, which leverages semantic
  similarity between queries rather than exact matching to retrieve cached responses.
  The core method introduces a principled framework that combines offline learning
  and online adaptation for semantic cache eviction under unknown query arrival probabilities
  and serving costs.
---

# Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation

## Quick Facts
- **arXiv ID**: 2508.07675
- **Source URL**: https://arxiv.org/abs/2508.07675
- **Reference count**: 35
- **Primary result**: Achieves sublinear regret with only O(log log T) cache switches, demonstrating 11.75% average regret improvement over baselines on synthetic datasets

## Executive Summary
This paper addresses the challenge of reducing inference costs for large language models (LLMs) through semantic caching, which leverages semantic similarity between queries rather than exact matching to retrieve cached responses. The authors introduce a principled framework that combines offline learning and online adaptation for semantic cache eviction under unknown query arrival probabilities and serving costs. For the offline setting, they develop CUCB-SC, which estimates parameters from historical data with uncertainty penalties and uses a reverse greedy algorithm. For the online setting, they propose CLCB-SC-LS, which balances exploration and exploitation through stage-based cache switching with optimism in the face of uncertainty, while controlling switching costs.

## Method Summary
The core method involves estimating query arrival probabilities and serving costs from historical data or online observations, then using these estimates to decide whether to serve a query from the LLM or retrieve a semantically similar cached response. The offline approach uses upper confidence bounds (UCB) to handle uncertainty in parameter estimates, while the online approach employs stage-based switching with lower confidence bounds (LCB) for exploration. The cache selection problem is formulated as minimizing expected loss over a supermodular function, enabling the use of reverse greedy algorithms for approximation.

## Key Results
- CLCB-SC-LS achieves sublinear regret bounds of O(√(mT log(mT) log logT)) with only O(log log T) cache switches
- Experiments on synthetic datasets demonstrate at least 11.75% improvement over baselines in average regret
- CLCB-SC-LS reduces cache switches by up to 90.91% and running time by up to 85.40% while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reverse greedy with supermodularity provides provable approximation to the NP-hard optimal cache selection problem.
- **Mechanism**: The loss function ℓ(M; p, c, d) = Σ p(q)·min{c(q), d(q,M)} is non-increasing and supermodular in M. The reverse greedy algorithm exploits this by starting with all queries cached and iteratively removing the least beneficial one, providing an approximation factor of e^(β-1)/β where β = c/(1-c).
- **Core assumption**: The distance function d and cost structure yield curvature c < 1; semantic similarity correlates with response interchangeability.
- **Evidence anchors**: [section III]: Lemma 2 proves the loss function is non-increasing and supermodular; Theorem 1 provides the approximation guarantee.
- **Break condition**: If curvature approaches 1 (highly non-submodular structure), approximation degrades; if semantic distance poorly predicts response utility, the min{c(q), d(q,M)} decision rule fails.

### Mechanism 2
- **Claim**: Upper confidence bounds (UCB) used as pessimistic cost estimates in offline learning provide finite-sample suboptimality guarantees by penalizing uncertainty.
- **Mechanism**: In the offline setting, the algorithm estimates arrival probabilities p̂ and costs ĉ from historical data. Crucially, it computes ¯c(q) = ĉ(q) + √(log(6mn/δ)/2Nc(q)) as an upper bound on true cost. This pessimism principle counters high variance from limited observations.
- **Core assumption**: Historical data covers queries with sufficient frequency; cost observations are sub-Gaussian; the data collection policy doesn't heavily bias which costs are observed.
- **Evidence anchors**: [section IV]: Theorem 2 provides the suboptimality bound; Algorithm 2 Line 5 shows the UCB computation.
- **Break condition**: If ν(q) is extremely small for some queries (rarely observed), the bound degrades; if noise is heavy-tailed rather than sub-Gaussian, concentration fails.

### Mechanism 3
- **Claim**: Stage-based cache switching with lower confidence bound (LCB) exploration achieves Õ(√(mT)) regret while limiting switches to O(m log log T).
- **Mechanism**: The online algorithm CLCB-SC-LS uses optimism for exploration: it computes LCB estimates ¯ct(q) = ĉt(q) - √(log(4mT³)/2Nc,t(q)), which underestimate true costs, encouraging the agent to query the LLM more often (exploration). Stage-based switching partitions time by observation counts.
- **Core assumption**: Query arrivals follow a stationary distribution p; costs have bounded variance; the regret target is against the best fixed cache in hindsight.
- **Evidence anchors**: [section V]: Theorem 3 gives regret bound; Lemma 7 bounds switches; Algorithm 3 Lines 5-11 define stage triggers.
- **Break condition**: Non-stationary query distributions break the fixed-optimal-cache benchmark; if LCB causes excessive LLM queries, serving cost dominates.

## Foundational Learning

- **Concept: Supermodular/submodular set functions**
  - Why needed here: The theoretical foundation relies on proving the loss function is supermodular, which justifies reverse greedy's approximation guarantee.
  - Quick check question: Can you explain why diminishing marginal returns (submodularity for increasing functions, supermodularity for decreasing functions) enable greedy algorithms to approximate optimal set selection?

- **Concept: Combinatorial bandits with partial feedback**
  - Why needed here: The online setting involves selecting a cache (a combinatorial action) and receiving feedback only on queried items, not cached ones.
  - Quick check question: How does partial feedback in caching (no cost observation when using cached responses) differ from full-feedback bandits, and why does optimism help?

- **Concept: Embedding spaces and semantic distance metrics**
  - Why needed here: The method depends on representing queries as vectors and defining d(q₁, q₂) such that semantically similar queries have small distances.
  - Quick check question: Given two queries with Euclidean embedding distance 0.3 vs 0.8, what does the decision rule min{c(q), d(q,M)} imply for serving cost c(q) = 0.5?

## Architecture Onboarding

- **Component map**: [Incoming Query qt] -> [Embedding Generator e: X → R^de] -> [Distance Computation: d(qt, M)] -> [Decision: min{¯ct(qt), d(qt, Mt)}] -> [LLM Call OR Cache Return] -> [Feedback Update] -> [Stage Check]

- **Critical path**:
  1. Embedding generation latency directly adds to request latency—must be fast or precomputed for anticipated queries.
  2. Distance computation over cache M requires efficient nearest-neighbor search (vector DB with ANN index).
  3. Stage-based switching logic determines when to recompute cache; errors here cause either excessive switching (cost overhead) or stale cache (high regret).

- **Design tradeoffs**:
  - Cache size k: Larger k reduces mismatch cost but increases memory and vector search overhead.
  - Embedding dimension de: Higher dimensions capture finer semantics but slow distance computation.
  - Stage threshold tuning: Aggressive switching reduces regret faster but incurs more LLM calls for cache population.
  - Distance metric: Euclidean vs cosine—paper claims results hold for any metric, but semantic alignment varies by domain.

- **Failure signatures**:
  - Cold-start degradation: Early rounds have high ¯ct uncertainty, causing excessive LLM queries; expect high initial regret converging over ~√(mT) rounds.
  - Semantic drift: If embedding model changes, distances become inconsistent; cached responses may become semantically mismatched.
  - Query distribution shift: The fixed-cache benchmark assumes stationary p; sudden distribution changes cause regret spikes until stages accumulate new observations.
  - Stage computation overflow: The stage trigger involves cumulative sums; for very long T, numerical precision in threshold computation matters.

- **First 3 experiments**:
  1. Reproduce Figure 2(a) on your query set: Implement Reverse Greedy with known p, c, d and compare loss vs. cache size k against brute-force optimal (for small m) and LFU baseline.
  2. Offline learning validation: Generate synthetic historical data with controlled observation rates ν(q) and noise levels; run CUCB-SC and plot suboptimality gap vs. dataset size n.
  3. Online switching behavior: Deploy CLCB-SC-LS with logging of (a) cumulative regret over time, (b) number of cache switches, (c) fraction of queries served from cache.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can offline pre-training be effectively integrated with online adaptation to create hybrid caching systems that improve robustness and efficiency?
- **Basis in paper**: [explicit] The conclusion states, "A promising future direction is to explore hybrid approaches that integrate offline training with online adaptation for more robust and efficient caching."
- **Why unresolved**: The paper currently treats offline and online settings as separate frameworks; combining them introduces challenges in balancing pre-computed static knowledge with dynamic real-time updates.
- **What evidence would resolve it**: A unified algorithm that initializes using historical logs and continuously adapts with provable regret bounds comparable to the online setting.

### Open Question 2
- **Question**: How does the framework perform on real-world LLM query traces characterized by non-stationary distributions or an infinite query vocabulary?
- **Basis in paper**: [inferred] The evaluation relies exclusively on "synthetic datasets" (Section VI), and the model assumes a "finite set of m distinct queries" (Section II.A) with known embeddings.
- **Why unresolved**: Production LLM workloads often exhibit concept drift and prompts outside a pre-defined set, which may violate the theoretical assumptions of fixed query sets and static embeddings used in the proofs.
- **What evidence would resolve it**: Experimental results on public LLM serving logs (e.g., ShareGPT) or theoretical extensions that bound regret under an infinite or evolving query space.

### Open Question 3
- **Question**: Can the theoretical guarantees be maintained if the semantic distance metric does not perfectly correlate with user utility?
- **Basis in paper**: [inferred] The model assumes a distance function where "smaller value indicates stronger semantic similarity" and normalizes the cost scaling (γ=1) (Section II.A).
- **Why unresolved**: The framework assumes the embedding distance is a perfect proxy for response utility (mismatch cost). In practice, semantic embeddings may fail to capture nuance (e.g., negation), potentially causing the cache to serve incorrect answers while theoretically minimizing loss.
- **What evidence would resolve it**: A robustness analysis or theoretical bounds that account for noise or misalignment between the embedding distance and the true utility cost.

## Limitations
- Theoretical guarantees rely on fixed query sets and stationary distributions, which may not hold in production environments with concept drift.
- Performance depends on the quality of semantic embeddings and their correlation with response utility, which may not hold for all query types.
- The framework assumes costs are sub-Gaussian and observations are unbiased, which may not reflect real-world cost distributions.

## Confidence
- **High Confidence**: The supermodular optimization framework (Mechanism 1) and the general structure of the online learning algorithm (Mechanism 3) are theoretically well-grounded with clear mathematical proofs provided.
- **Medium Confidence**: The empirical results showing 11.75% regret improvement are based on synthetic data with controlled parameters. Real-world deployment would face query distributions and semantic similarity structures that may deviate from synthetic assumptions.
- **Low Confidence**: The claim that semantic caching "reduces inference costs" assumes cached responses are semantically equivalent to fresh LLM responses. If embedding-based similarity doesn't capture functional equivalence, this could introduce errors not reflected in the loss function.

## Next Checks
1. **Ablation on embedding models**: Test whether the caching performance degrades when using different embedding models (e.g., OpenAI embeddings vs. sentence transformers) to validate that the method is robust to embedding choice, not tuned to one specific model.

2. **Non-stationary query distribution test**: Evaluate CLCB-SC-LS under time-varying query arrival probabilities p_t to verify the algorithm's performance degrades gracefully rather than catastrophically when the fixed-cache benchmark assumption breaks.

3. **Semantic equivalence validation**: Implement a human evaluation pipeline to assess whether cached responses are truly semantically equivalent to what the LLM would generate, measuring accuracy degradation vs. cost savings trade-off in real-world applications.