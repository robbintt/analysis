---
ver: rpa2
title: Probabilistic Forecasting via Autoregressive Flow Matching
arxiv_id: '2503.10375'
source_url: https://arxiv.org/abs/2503.10375
tags:
- flow
- forecasting
- autoregressive
- training
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Autoregressive Flow Matching (AFM), a probabilistic
  forecasting method that combines autoregressive modeling with flow matching to predict
  multivariate time series. AFM decomposes the forecasting problem into a sequence
  of conditional distributions, each modeled via a shared flow that transforms a simple
  base distribution into the next observation distribution conditioned on past observations
  and covariates.
---

# Probabilistic Forecasting via Autoregressive Flow Matching

## Quick Facts
- arXiv ID: 2503.10375
- Source URL: https://arxiv.org/abs/2503.10375
- Reference count: 40
- Introduces Autoregressive Flow Matching (AFM), achieving 93-98% error reduction compared to non-autoregressive baselines on extrapolation tasks

## Executive Summary
This paper introduces Autoregressive Flow Matching (AFM), a novel probabilistic forecasting method that combines autoregressive modeling with flow matching to predict multivariate time series. AFM decomposes the forecasting problem into a sequence of conditional distributions, each modeled via a shared flow that transforms a simple base distribution into the next observation distribution conditioned on past observations and covariates. The method leverages flow matching to learn these transformations efficiently without requiring simulation or handcrafted noise schedules.

The autoregressive structure enables AFM to handle variable forecasting horizons, extrapolate beyond training data, and provide well-calibrated uncertainty estimates while simplifying the optimization problem. Empirical evaluation on both simulated dynamical systems and real-world datasets shows AFM achieves state-of-the-art performance, particularly excelling at extrapolation where it demonstrates 93-98% error reduction compared to non-autoregressive baselines.

## Method Summary
AFM treats time series forecasting as a sequence of conditional probability distributions, where each step conditions on all previous observations and covariates. Instead of modeling the full joint distribution directly, AFM uses a shared flow matching network to transform a simple base distribution into the conditional distribution for each time step. This autoregressive decomposition allows the model to handle variable forecasting horizons and extrapolate beyond training data by leveraging the sequential nature of time series. The flow matching approach learns to transform between distributions without requiring simulation or handcrafted noise schedules, making training more stable and efficient compared to traditional normalizing flows.

## Key Results
- Achieves state-of-the-art performance with average CRPS of 0.042 on Electricity data (vs 0.045 for second-best)
- Demonstrates superior extrapolation capabilities with 93-98% error reduction compared to non-autoregressive baselines
- Shows strong performance across multiple datasets: 0.009 CRPS on Exchange and 0.284 on Solar data

## Why This Works (Mechanism)
AFM works by decomposing the complex multivariate forecasting problem into a sequence of simpler conditional distributions. Each conditional distribution is learned via flow matching, which transforms a simple base distribution into the target distribution without requiring expensive simulations. The autoregressive structure means each prediction conditions on all previous observations, allowing the model to capture temporal dependencies naturally. By sharing the same flow network across all time steps, AFM learns a general transformation that can be applied repeatedly, enabling it to handle variable forecasting horizons and extrapolate beyond the training distribution.

## Foundational Learning

**Flow Matching**: A method for learning transformations between probability distributions without requiring simulation. Needed because traditional normalizing flows require expensive sampling during training. Quick check: Can transform between distributions efficiently without simulation.

**Autoregressive Modeling**: Decomposing joint distributions into sequential conditional distributions. Needed to handle the temporal structure of time series naturally. Quick check: Each prediction conditions on all previous observations.

**Conditional Distributions**: Modeling probability distributions conditioned on past observations and covariates. Needed because forecasting requires incorporating historical information. Quick check: Model P(x_t | x_{<t}, c) for each time step.

## Architecture Onboarding

**Component Map**: Base Distribution -> Flow Matching Network -> Conditional Distribution -> Autoregressive Prediction Loop

**Critical Path**: Historical observations and covariates are fed into the shared flow matching network, which transforms a base distribution into the conditional distribution for the next time step. This process repeats autoregressively to generate the full forecast sequence.

**Design Tradeoffs**: The autoregressive structure simplifies optimization but introduces sequential computation during inference. Flow matching eliminates the need for simulation but requires careful network design to capture temporal dependencies effectively.

**Failure Signatures**: Poor performance on short sequences may indicate insufficient conditioning information. Degraded extrapolation suggests the model hasn't learned the underlying dynamics. High uncertainty in predictions may indicate inadequate flow network capacity.

**3 First Experiments**: 
1. Test flow matching on synthetic distributions to verify basic transformation capability
2. Evaluate autoregressive predictions on simple time series to confirm temporal modeling
3. Compare uncertainty estimates against ground truth on known dynamical systems

## Open Questions the Paper Calls Out
None

## Limitations
- Autoregressive inference is inherently sequential, potentially limiting real-time applications
- Performance depends on the capacity of the shared flow network to capture complex temporal dependencies
- May struggle with very long-range dependencies due to the autoregressive structure

## Confidence
- State-of-the-art results on real-world datasets: High
- Demonstrated extrapolation capabilities: High
- Theoretical foundations of flow matching approach: Medium
- Scalability to very long time series: Low

## Next Checks
1. Verify AFM's uncertainty calibration on synthetic datasets with known ground truth distributions
2. Test performance on extremely long sequences to assess scalability limits
3. Compare inference speed against non-autoregressive baselines for real-time applications