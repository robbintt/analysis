---
ver: rpa2
title: Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts
arxiv_id: '2512.02486'
source_url: https://arxiv.org/abs/2512.02486
tags:
- uni00000013
- domain
- dynamics
- droco
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of dual (train-time and test-time)
  robustness in cross-domain offline RL, where policies must handle dynamics shifts
  in both training and deployment. The authors propose the Dual-RObust Cross-domain
  Offline RL (DROCO) algorithm, which introduces a novel robust cross-domain Bellman
  (RCB) operator to enhance test-time robustness while maintaining train-time robustness.
---

# Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts

## Quick Facts
- arXiv ID: 2512.02486
- Source URL: https://arxiv.org/abs/2512.02486
- Reference count: 40
- Primary result: DROCO achieves 14.0% improvement over OTDF baseline on D4RL benchmarks

## Executive Summary
This paper addresses the challenge of dual-robustness in cross-domain offline RL, where policies must handle dynamics shifts during both training (train-time robustness) and deployment (test-time robustness). The authors propose the Dual-Robust Cross-domain Offline RL (DROCO) algorithm, which introduces a novel robust cross-domain Bellman (RCB) operator to enhance test-time robustness while maintaining train-time conservatism. By incorporating dynamic value penalty and Huber loss, DROCO mitigates value overestimation or underestimation issues. Extensive experiments across kinematic and morphology shift scenarios demonstrate superior performance compared to strong baselines.

## Method Summary
DROCO introduces a robust cross-domain Bellman (RCB) operator that applies standard Bellman updates to target domain data while using robust Bellman updates (infimum over Wasserstein uncertainty sets) for source domain data. The method employs an ensemble of dynamics models trained on limited target data to approximate the uncertainty set, enabling practical implementation. Value overestimation or underestimation is mitigated through a dynamic penalty term and Huber loss. The policy is extracted using exponential advantage-weighted regression. The algorithm maintains conservative Q-learning for train-time robustness while preparing for test-time perturbations through worst-case value estimation.

## Key Results
- Total normalized score of 1105.2 across D4RL benchmarks, significantly outperforming second-best method OTDF by 14.0%
- Enhanced robustness to test-time dynamics perturbations across multiple perturbation types and intensity levels
- Effective handling of both kinematic shifts (e.g., min/max joint angles) and morphology shifts (e.g., mass/dimension changes)

## Why This Works (Mechanism)

### Mechanism 1: Robust Cross-Domain Bellman Operator
The RCB operator achieves dual robustness by applying robust Bellman updates to source domain data while using standard Bellman updates on target domain data. For source transitions, it computes the infimum of value estimates over a Wasserstein uncertainty set U_ε(s'), preventing value overestimation from out-of-distribution dynamics. This worst-case optimization prepares the policy for test-time perturbations while maintaining conservatism on training data.

### Mechanism 2: Practical RCB via Ensemble Dynamics Modeling
The intractable dynamics uncertainty set is approximated using an ensemble of learned target domain dynamics models. DROCO trains N dynamics models on limited target data via maximum likelihood, with the ensemble prediction set {s'_1, ..., s'_N} approximating sampling from the uncertainty set around P_tar. The infimum over these predictions provides a practical worst-case estimate without requiring access to the true uncertainty set.

### Mechanism 3: Dynamic Value Penalty with Huber Loss
Value estimation errors from the infimum operator are corrected through adaptive penalization and robust loss functions. A penalty term u(s,a,s') measures the gap between observed next-state value and worst-case ensemble prediction, subtracted (scaled by β) from the Bellman target. The Huber loss transitions from ℓ² to ℓ¹ when TD error exceeds threshold δ, providing robustness to outliers from value estimation errors.

## Foundational Learning

- **Concept: Robust Bellman Operator in RL**
  - Why needed: The RCB operator extends standard robust RL theory to cross-domain settings. Understanding that robust RL optimizes worst-case performance via infimum over an uncertainty set is essential.
  - Quick check: Can you explain why taking an infimum over a dynamics uncertainty set produces a conservative value estimate?

- **Concept: Wasserstein Distance and Uncertainty Sets**
  - Why needed: The paper uses Wasserstein uncertainty sets rather than (s,a)-rectangular or d-rectangular sets. Understanding the dual reformulation that converts dynamics perturbations to state perturbations is critical.
  - Quick check: Why does the Wasserstein uncertainty set enable a closed-form dual reformulation while other uncertainty sets do not?

- **Concept: Offline RL with Conservative Q-Learning**
  - Why needed: DROCO builds on IQL's expectile regression for in-sample value learning. The policy extraction uses advantage-weighted regression. Understanding why offline RL needs conservatism is foundational.
  - Quick check: Why does IQL's expectile regression produce an in-sample optimal value function rather than the true optimal?

## Architecture Onboarding

- **Component map:**
  - Ensemble Dynamics Model (N neural networks) → Q-Network and V-Network → Policy Network
  - Dynamics models predict next states from target data; Q/V networks learn robust values; policy extracts from Q/V

- **Critical path:**
  1. Train ensemble dynamics models on D_tar for 100K steps
  2. Sample batches from both D_src and D_tar
  3. Compute value penalty using ensemble predictions
  4. Update V via expectile regression (τ=0.7)
  5. Update Q with Huber loss on source data + ℓ² on target data
  6. Extract policy via AWR with inverse temperature β_IQL=3.0
  7. Total: 1M gradient steps

- **Design tradeoffs:**
  1. ε selection: Larger ε increases test-time robustness but reduces clean performance
  2. β coefficient: β<1 mitigates underestimation (preferred); β>1 increases conservatism
  3. Huber threshold δ: Larger δ keeps ℓ² longer (more stable); smaller δ transitions to ℓ¹ earlier (more robust)
  4. Ensemble size N: Not sensitive (N=3,5,7,9 all perform similarly); default is 7

- **Failure signatures:**
  1. Severe Q-value underestimation: β too large or ensemble predictions too pessimistic; reduce β toward 0.1
  2. Training instability: δ too small; increase to 30-50
  3. Poor test-time robustness despite good clean performance: ensemble models may not cover target dynamics support
  4. Degraded performance on clean target: β may be too large or ε too large; try β≤1.0

- **First 3 experiments:**
  1. Sanity check on D4RL: Train on halfcheetah-medium with 100% target data using standard IQL
  2. Ablation on ensemble dynamics: Compare N=1 vs N=7 ensemble members on halfcheetah-kinematic-expert
  3. Hyperparameter sweep for β and δ: Sweep β∈{0.1, 0.5, 1.0, 1.2} and δ∈{5, 10, 30, 50} on hopper-kinematic-medium

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the dual-robustness framework be extended to complex navigation and manipulation tasks like Antmaze or Adroit?
  - Basis: Appendix A.1 states enabling cross-domain RL to succeed in such challenging settings remains an open problem
  - Why unresolved: Structural barriers in navigation and dexterous manipulation make policy adaptation difficult
  - What evidence would resolve it: Successful application to Antmaze or Adroit benchmarks with kinematic/morphology shifts

- **Open Question 2:** How can the optimal trade-offs for the penalty coefficient (β) and transition threshold (δ) be automated?
  - Basis: Section 5.3 and Appendix E.5 demonstrate dataset-dependent sensitivity to β and δ, requiring manual sweeping
  - Why unresolved: Lack of adaptive mechanism limits "plug-and-play" capability
  - What evidence would resolve it: Meta-learning or adaptive scheme that dynamically adjusts β and δ during training

- **Open Question 3:** How does DROCO perform when the ensemble dynamics model fails to accurately cover the support of target domain transitions?
  - Basis: Section 4.3 discusses using ensemble model to approximate uncertainty set, but acknowledges potential overfitting with limited data
  - Why unresolved: Sensitivity to dynamics model quality remains unclear
  - What evidence would resolve it: Ablation study showing performance degradation with reduced ensemble size or insufficient target data

## Limitations

- Reliance on limited target domain data to train ensemble dynamics models may not fully capture true target dynamics support
- Theoretical guarantees assume Lipschitz continuity of Q-function and proper ε specification, which may not hold for neural networks
- Cross-validation procedure for selecting ε is computationally expensive and may not generalize to unseen perturbation types

## Confidence

- **High confidence**: Empirical performance claims on D4RL benchmarks (1105.2 total score, 14.0% improvement) are well-supported by extensive ablations and sensitivity analyses
- **Medium confidence**: Theoretical robustness guarantees under idealized conditions may not fully translate to practical neural network implementations
- **Low confidence**: Assumption that ensemble dynamics models trained on 10% of D4RL data can reliably approximate Wasserstein uncertainty sets for complex morphology shifts

## Next Checks

1. **Robustness to extreme data scarcity**: Evaluate DROCO when target domain data is reduced from 10% to 1% or 0.1% of D4RL datasets to identify minimum viable data requirement

2. **Generalization to unseen perturbation types**: Test DROCO on novel perturbation combinations (e.g., combined kinematic and morphology shifts) not included in original evaluation suite

3. **Alternative uncertainty set constructions**: Compare Wasserstein-based uncertainty sets against (s,a)-rectangular and d-rectangular alternatives in terms of both theoretical guarantees and empirical robustness performance