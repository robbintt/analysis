---
ver: rpa2
title: Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial
  Masking
arxiv_id: '2505.20023'
source_url: https://arxiv.org/abs/2505.20023
tags:
- agent
- trajectories
- task
- teacher
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training effective autonomous
  agents using smaller open-source large language models (LLMs). Current methods often
  rely on expert trajectories from powerful closed-source models, which can lead to
  performance plateaus and error propagation.
---

# Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking

## Quick Facts
- arXiv ID: 2505.20023
- Source URL: https://arxiv.org/abs/2505.20023
- Reference count: 21
- One-line primary result: STeP achieves up to 22% improvement on unseen tasks vs. training on expert trajectories alone

## Executive Summary
This paper introduces STeP (Self-Reflected Trajectories and Partial Masking), a method to train smaller open-source LLMs as autonomous agents by synthesizing error-reflection-correction sequences and employing partial masking during training. Instead of relying solely on expert trajectories from powerful closed-source models, STeP uses a larger teacher model to generate self-reflected trajectories that include explanations of errors and corrective actions. Experiments on ALFWorld, WebShop, and SciWorld tasks show that LLaMA2-7B-chat trained with self-reflected trajectories achieves significant improvements—up to 22% on unseen tasks—using less training data than training exclusively on expert trajectories.

## Method Summary
STeP is a three-stage pipeline that trains smaller LLMs as autonomous agents. First, a base agent is trained on 50% of expert trajectories using standard SFT. Second, this base agent is deployed on held-out tasks with a larger teacher model monitoring its actions; when errors occur, the teacher generates real-time reflections and corrections in ReAct format, creating self-reflected trajectories. Third, the original LLM is retrained from scratch on a combination of expert and self-reflected trajectories, with a partial masking strategy that prevents the model from internalizing incorrect steps. The method is evaluated on ALFWorld, WebShop, and SciWorld, showing improved performance on both seen and unseen tasks compared to baselines.

## Key Results
- LLaMA2-7B trained with self-reflected trajectories outperforms training on expert trajectories alone by up to 22% on unseen tasks.
- Partial masking prevents error propagation and improves final agent performance.
- STeP uses less training data (fewer expert trajectories) than baselines while achieving superior results.
- Ablation studies confirm that both self-reflection and partial masking contribute to the gains.

## Why This Works (Mechanism)

### Mechanism 1
Training on error-reflection-correction sequences enables models to learn recovery behaviors, not just successful trajectories. The teacher model monitors the base agent in real-time, generating reflections and corrections when errors occur. These "self-reflected trajectories" are incorporated into training, teaching the student model how to recover rather than only imitating success. Core assumption: Error patterns are learnable and generalizable; exposure to recovery patterns reduces cascading failures at inference time. Evidence: Teacher model offers error content, reason, reflection, and correct action; agentDistill addresses agent distillation but not error-recovery learning. Break condition: If teacher reflections are noisy or inconsistent, the model may learn spurious error patterns; paper excludes trajectories with >1-2 erroneous steps.

### Mechanism 2
Partial masking prevents the model from internalizing incorrect thought-action pairs while preserving context for correction learning. During SFT, incorrect steps are masked in the loss computation—the model sees the full trajectory but only receives gradients from correct steps and reflection-correction steps. This avoids reinforcing wrong actions while keeping the sequential context intact. Core assumption: Context surrounding errors is informative; masking loss rather than removing steps preserves trajectory coherence. Evidence: Loss function includes δ_i multiplier; related agent distillation papers do not discuss granular masking within trajectories. Break condition: If masking is too aggressive, the model may lose too much signal; paper limits error steps per trajectory to 1-2.

### Mechanism 3
A two-stage curriculum—first building a base agent on expert trajectories, then augmenting with self-reflected data—mitigates catastrophic forgetting and improves data efficiency. Stage 1 trains on D1 (50% of golden trajectories) to create a base agent capable of basic task execution. Stage 2 combines D1 + self-reflected trajectories D_r to retrain from the original LLM, not from the base agent, preserving prior capabilities while adding recovery skills. Core assumption: The base agent must be competent enough to generate plausibly recoverable trajectories; retraining from base LLM prevents overfitting to error-heavy data. Evidence: Sequential training caused performance drops; combining D1 + D_r from base LLM worked better. Break condition: If D1 is too small, the base agent fails too often, producing low-quality self-reflected data.

## Foundational Learning

- **ReAct Prompting (Thought-Action-Observation loops)**: All trajectories are formatted in ReAct style. The model must generate a reasoning thought before each action, and observations provide environment feedback. Quick check: Can you explain why interleaving thoughts with actions helps an agent correct itself mid-trajectory?
- **Supervised Fine-Tuning (SFT) for Behavioral Cloning**: The entire STeP pipeline uses SFT at two stages—first to create the base agent, then to train the final agent with partial masking. Quick check: What is the risk of naively fine-tuning on trajectories containing errors without any masking or filtering?
- **Cascading Errors in Sequential Decision-Making**: The paper explicitly frames the problem as error propagation—one mistake can trap an agent in failure loops. Understanding this motivates the reflection-correction design. Quick check: In a multi-step agent task, why might learning from only successful trajectories fail to prevent cascading errors?

## Architecture Onboarding

- Component map: Base LLM -> Teacher LLM -> Environment -> Trajectory Store -> Partial Masking Layer
- Critical path: 1) Train base agent on D1 (50% golden trajectories) via SFT; 2) Run base agent on D2 tasks with teacher monitoring; teacher flags errors and generates reflections/corrections; 3) Filter self-reflected trajectories (≤1-2 errors for WebShop, ≤2 for others); 4) Retrain base LLM (not base agent) on D1∪Dr with partial masking enabled.
- Design tradeoffs: Teacher model choice (larger teacher yields better reflections but higher cost); error tolerance in filtering (stricter improves quality but reduces quantity); masking vs. removal (masking preserves context but increases sequence length).
- Failure signatures: Base agent too weak (generates incoherent actions, teacher corrections dominate, low-quality D_r); over-masking (model never learns to handle errors); catastrophic forgetting (training only on D_r without D1 causes drops).
- First 3 experiments: 1) Baseline comparison (golden-only vs. STeP) on ALFWorld/WebShop/SciWorld test sets; 2) Ablation on masking (with vs. without partial masking) on same D_r; 3) Teacher sensitivity (70B vs. 110B) to generate D_r and track downstream performance.

## Open Questions the Paper Calls Out
- Can the base LLM agent be trained effectively using self-generated feedback (self-reflection) rather than relying on a distinct, more powerful teacher model for error correction?
- Why does the Partial Masking strategy reduce performance on certain tasks like ALFWorld, and does this indicate a trade-off between preventing error learning and preserving beneficial exploration?
- How can the real-time reflection and correction prompts be improved to ensure the quality and appropriateness of synthetic trajectories?

## Limitations
- Relies on a large teacher model for real-time error detection and correction, raising scalability concerns.
- Partial masking may not be optimal for all tasks, as evidenced by performance drops on ALFWorld when masking is applied.
- The method does not address cross-task generalization; benefits are shown within-task but not across domains.

## Confidence
- High confidence: Experimental results demonstrating STeP's superiority over golden-only baselines (+9-22% average reward) are robust, with multiple ablations isolating the contributions of self-reflection and partial masking.
- Medium confidence: The mechanism by which partial masking prevents learning incorrect steps is theoretically sound, but implementation details (token-level mask alignment, error step detection) are not fully specified.
- Medium confidence: The two-stage curriculum is supported by ablation, but the optimal base agent quality threshold is not explored.

## Next Checks
1. **Teacher robustness check**: Vary the teacher model size (70B vs. 110B) and measure self-reflected trajectory quality (count, reflection coherence) and downstream agent performance to quantify teacher sensitivity.
2. **Mask alignment audit**: Instrument the training pipeline to log per-step loss values and gradient norms; verify that error-tagged steps contribute zero loss and that no step-level misalignment occurs between mask labels and token positions.
3. **Cross-task transfer test**: Train STeP on one task domain (e.g., ALFWorld) and evaluate zero-shot on a different domain (e.g., WebShop) to measure generalization beyond in-domain test sets.