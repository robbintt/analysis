---
ver: rpa2
title: Asymptotically optimal regret in communicating Markov decision processes
arxiv_id: '2505.18064'
source_url: https://arxiv.org/abs/2505.18064
tags:
- optimal
- lemma
- unif
- have
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of achieving asymptotically optimal\
  \ regret in communicating Markov decision processes under average reward, where\
  \ the goal is to minimize the expected regret Reg(T;M,\u039B) = Tg\u2217 - E[\u2211\
  Rt] relative to the optimal policy. The main result is the development of an algorithm\
  \ called ECoE that achieves regret K(M)log(T) + o(log(T)), matching the lower bound\
  \ established by Boone and Maillard (2025) for consistent algorithms."
---

# Asymptotically optimal regret in communicating Markov decision processes

## Quick Facts
- **arXiv ID**: 2505.18064
- **Source URL**: https://arxiv.org/abs/2505.18064
- **Reference count**: 40
- **Primary result**: ECoE* algorithm achieves regret K(M)log(T) + o(log(T)), matching the lower bound for consistent algorithms in communicating MDPs.

## Executive Summary
This paper presents the first algorithm achieving asymptotically optimal regret in communicating Markov decision processes (MDPs) under average reward. The ECoE* algorithm matches the lower bound K(M)log(T) + o(log(T)) established by Boone and Maillard (2025) for consistent algorithms. The key innovation is a regularization approach that smooths the discontinuous optimal pair structure, enabling stable estimation of near-optimal exploration policies from empirical data. The algorithm dynamically balances exploration (gathering information on sub-optimal pairs), co-exploration (ensuring all optimal pairs are visited sufficiently), and exploitation (playing optimally when information is sufficient).

## Method Summary
The ECoE* algorithm operates in communicating MDPs with unknown transition and reward distributions. It maintains an MLE estimate of the environment and uses a "leveling transform" to smooth the boundary between optimal and sub-optimal pairs. The algorithm makes decisions based on a generalized log-likelihood ratio test and a "square trick" to efficiently balance exploration across disconnected components of optimal pairs. A carefully tuned regularization hyperparameter stabilizes estimation despite the discontinuity of K(M) with respect to the model. The algorithm achieves K(M)log(T) + o(log(T)) regret by ensuring sub-optimal pairs are visited no more than µ*(z)log(T)/C(µ*,M) + o(log(T)) times, where µ* is a special optimal exploration measure.

## Key Results
- ECoE* achieves regret K(M)log(T) + o(log(T)), matching the lower bound for consistent algorithms
- The algorithm explicitly tracks the constant K(M) through regularization, enabling near-optimal exploration
- Establishes that the lower bound K(M) is tight for communicating MDPs
- Provides complete regret analysis showing sub-optimal visit counts scale as µ*(z)log(T)/C(µ*,M) + o(log(T))

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Coexploration-Exploitation (ECoE) Framework
The algorithm achieves asymptotically optimal regret by dynamically balancing three distinct behavioral modes. At each phase start, it estimates optimal pairs and enters Exploration to gather data on sub-optimal pairs if information is insufficient (via GLR test), triggers Co-exploration to navigate the state space when optimal pairs are under-visited, and enters Exploitation to maximize immediate reward when sufficient information exists. This framework relies on the communicating assumption allowing transitions between any states under some policy.

### Mechanism 2: Regularized Estimation via the Leveling Transform
The algorithm estimates the complex, discontinuous regret constant K(M) from noisy empirical data by "smoothing" the MDP structure. The leveling transform artificially increases rewards for "nearly optimal" pairs (those with small Bellman gaps), effectively smoothing the boundary between optimal and sub-optimal sets. This allows for stable estimation of an exploration policy π⁻ despite the discontinuity of K(M) and optimal pairs Z** with respect to model parameters.

### Mechanism 3: The Square Trick for Travel Efficiency
The algorithm minimizes the overhead cost of switching between disconnected components of optimal pairs. Instead of standard doubling tricks causing Ω(log T) switches, the Square Trick compares visit counts of the current component to the square root of minimum counts in other components. It forces travel only when this balance is violated, reducing switching cost to o(log T).

## Foundational Learning

**Concept**: Communicating vs. Ergodic MDPs
- **Why needed here**: The paper relaxes the standard "ergodic" assumption to the weaker "communicating" assumption. In communicating MDPs, a policy might be transient in some states (not visiting them infinitely often), whereas an ergodic policy visits all states infinitely often.
- **Quick check question**: Does the MDP allow for a policy that stays in a subset of states forever without visiting others? (If yes, it is communicating but potentially non-ergodic).

**Concept**: The Bellman Gap (Optimality Gap)
- **Why needed here**: The algorithm classifies pairs based on their Bellman gap Δ*(z). Understanding that Δ*(z) = 0 denotes "weak optimality" is crucial for the leveling transform.
- **Quick check question**: If a pair has a Bellman gap of 0, is it guaranteed to be played infinitely often by an optimal policy? (No, it is only "weakly optimal"; it might be transient).

**Concept**: Regret Lower Bounds (K(M))
- **Why needed here**: The entire architecture is built to match a specific constant K(M) derived from a lower bound. This constant represents the "cost of information" relative to the "gain loss" of exploration.
- **Quick check question**: Does minimizing regret always mean minimizing the number of sub-optimal plays? (Yes, in this formulation, regret is the sum of Bellman gaps over time).

## Architecture Onboarding

**Component map**: Estimator (ˆMt) -> Leveler (Z**_{ε_lvl}(ˆMt)) -> Oracle (K¯ε(M), μ*_¯ε) -> GLR Test -> Controller (Square Trick) -> Exploration/Co-exploration/Exploitation

**Critical path**: The "Leveling" step is the bottleneck for stability. If the threshold ε_lvl is misconfigured, the Oracle solves the wrong problem, leading to catastrophic exploration policies.

**Design tradeoffs**: The paper uses an "Oracle" for computing K(M), acknowledging computational difficulty. A practical implementation requires an approximate solver for the constrained optimization problem defining K(M). The "Square Trick" adds complexity but is necessary to avoid the log(T) travel penalty of standard episodic algorithms.

**Failure signatures**:
- Stagnation: If the "Square Trick" thresholds are too high, the agent may refuse to travel to distant optimal components, failing co-exploration.
- Oscillation: If ε_lvl is too large, the set of "leveled optimal pairs" fluctuates wildly, causing the exploitation policy to jump erratically.
- Panic Loops: Frequent "Panic" signals indicate the estimated model supports are incorrect, triggering unnecessary exploration episodes.

**First 3 experiments**:
1. Verify the Leveling Effect: Run the algorithm on a deterministic MDP with small perturbations. Confirm that small noise does not flip the classification of optimal pairs.
2. Isolate the Square Trick: Compare the "Square Trick" against a standard "Doubling Trick" on an MDP with two disconnected optimal loops. Plot the number of travels over time.
3. Stress Test Regularization: Slowly decay ε_lvl and measure the stability of the estimated exploration measure μ*_¯ε. Look for convergence to the true K(M).

## Open Questions the Paper Calls Out

**Open Question 1**: Can an asymptotically optimal version of the ECoE algorithm be implemented that runs in reasonable time without relying on oracles for computations like K(M)? The current analysis assumes oracle capabilities to compute complex optimization problems and GLR tests, which are generally NP-hard or computationally intensive. Resolution would require a modified algorithm with provably polynomial time complexity for its subroutines.

**Open Question 2**: Can the asymptotic optimality results be extended to weakly communicating MDPs or environments where the pair space Z is unknown? The algorithm and lower bound analysis currently rely on the "communicating" assumption and known structure of Z. Resolution would require extending the lower bound K(M) and developing an algorithm that operates effectively in weakly communicating settings or with unknown state spaces.

**Open Question 3**: Can the upper bound for the regret lower bound constant K(M) be tightened to |Z|D(M)²Δ_g(M)⁻¹? The current analysis provides a bound of order |Z|D(M)³Δ_g(M)⁻², which is looser than the conjectured bound. Resolution would require a formal proof establishing the conjectured upper bound.

## Limitations

- The algorithm assumes oracle access to compute K(M) and exploration tests, which are computationally difficult and limit practical implementation
- The theoretical framework relies on Bernoulli rewards and product-form model space assumptions that may not generalize to all communicating MDPs
- The paper does not extensively validate the effectiveness of regularization hyperparameters and decay schedules across diverse MDP structures

## Confidence

**High Confidence**: The theoretical foundation for the regret lower bound K(M) and the asymptotic optimality of the ECoE* algorithm are well-established within the paper's stated assumptions.

**Medium Confidence**: The effectiveness of the "leveling transform" and "square trick" as practical mechanisms for stabilizing estimation and reducing travel costs is demonstrated, but the paper relies on the reader's trust in stated hyperparameters and the oracle solver.

**Low Confidence**: The paper does not provide a detailed discussion of the computational complexity of the convex optimization problem or the scalability of the algorithm to large state-action spaces, which are critical for practical implementation.

## Next Checks

1. **Computational Scalability**: Implement a benchmark suite of communicating MDPs with varying state-action space sizes to empirically measure the computational time and memory usage of the ECoE* algorithm, particularly focusing on the convex optimization step for K(M).

2. **Hyperparameter Sensitivity**: Conduct a systematic study of the algorithm's performance across a grid of regularization hyperparameters (ε_lvl, ε_unif, etc.) to identify regions of stability and potential failure modes.

3. **Generalization Beyond Bernoulli Rewards**: Modify the ECoE* algorithm to handle a broader class of reward distributions (e.g., Gaussian) and validate its regret performance on a set of communicating MDPs with non-Bernoulli rewards.