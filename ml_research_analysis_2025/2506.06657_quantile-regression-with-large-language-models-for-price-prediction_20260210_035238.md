---
ver: rpa2
title: Quantile Regression with Large Language Models for Price Prediction
arxiv_id: '2506.06657'
source_url: https://arxiv.org/abs/2506.06657
tags:
- price
- regression
- quantile
- prediction
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates probabilistic regression using Large Language
  Models (LLMs) for text-to-distribution price prediction tasks. The authors propose
  a novel quantile regression approach with multi-quantile heads attached to LLMs,
  enabling production of full predictive distributions rather than point estimates.
---

# Quantile Regression with Large Language Models for Price Prediction

## Quick Facts
- arXiv ID: 2506.06657
- Source URL: https://arxiv.org/abs/2506.06657
- Reference count: 29
- One-line primary result: Multi-quantile regression heads on Mistral-7B achieve 16.86% MAPE on Amazon products, 6.3% on used cars, and 21.2% on boats while producing well-calibrated distributions.

## Executive Summary
This paper proposes a novel approach for probabilistic regression using Large Language Models (LLMs) to predict full price distributions from unstructured text. The authors attach multi-quantile regression heads to the final hidden layer of Mistral-7B, enabling the model to produce K=200 quantiles that approximate the conditional price distribution. Through extensive experiments on three diverse datasets (Amazon products, used cars, and boats), they demonstrate that fine-tuned decoder-only models with quantile heads significantly outperform traditional approaches including point regression, embedding-based methods, and few-shot learning. The approach achieves superior point estimate accuracy (MAPE 16.86-6.3%) and produces well-calibrated distributions with calibration errors of 0.04-0.07.

## Method Summary
The method involves attaching a quantile head to the final hidden state of an LLM (Mistral-7B) to predict K=200 quantiles of the price distribution. The model is fine-tuned using smoothed pinball loss with LoRA adapters (rank=192, alpha=384) and trained on log-transformed prices. The quantile head uses delta encoding with cumulative sum to ensure monotonicity. Inputs consist of concatenated text fields (title, description, attributes) that are tokenized and passed through the LLM backbone. The model outputs a full distribution via quantile interpolation, with the median (τ=0.5) serving as the point estimate.

## Key Results
- Mistral-7B-Quantile achieves MAPE of 16.86% for Amazon products, 6.3% for used cars, and 21.2% for boats
- The quantile approach outperforms point regression (Mistral-7B-Point) by 4-19 MAPE points across datasets
- Model produces well-calibrated distributions with calibration errors of 0.04-0.07 and sharp confidence intervals
- Fine-tuned decoder models consistently outperform encoder architectures, embedding methods, and few-shot approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attaching multi-quantile regression heads to the LLM's final hidden layer enables full predictive distribution estimation while improving point estimate accuracy.
- Mechanism: The quantile head `g(·; ϕ)` maps the final hidden state `h_T` to K quantiles `(q̂_τ1, ..., q̂_τK)`. Training with smoothed pinball loss across K=200 quantile levels provides a discrete approximation to the CRPS integral, a strictly proper scoring rule that recovers the ground truth conditional distribution.
- Core assumption: The LLM's hidden representations contain sufficient signal about price-relevant features that can be refined through end-to-end gradient updates.
- Evidence anchors: [abstract] "propose a novel quantile regression approach that enables LLMs to produce full predictive distributions"; [section 6.3] "multi-quantile approach... summing over multiple τ provides a discrete approximation to the integral of pinball losses over τ in (0,1). This integral corresponds to CRPS which is a strictly proper scoring rule"; [corpus] Weak direct corpus support for this specific LLM-quantile coupling; related work (arXiv:2507.15079) uses quantile averaging for electricity prices but without LLM integration.
- Break condition: If hidden representations lack price-relevant semantic structure (e.g., purely numeric inputs without textual context), the quantile head cannot extract distributional signal.

### Mechanism 2
- Claim: The median from quantile regression outperforms direct point regression due to outlier robustness and implicit regularization from non-median quantile losses.
- Mechanism: The τ=0.5 quantile loss is inherently robust to outliers (asymmetric gradient structure). When training multiple quantiles simultaneously, non-median losses constrain the model to perform well across the full distribution, acting as regularization for the median prediction task.
- Core assumption: The underlying price distribution has meaningful structure beyond the conditional mean; outliers represent noise rather than signal.
- Evidence anchors: [section 6.1] "Mistral-7B-Quantile (MAPE 16.86%) substantially outperforms Mistral-7B-Point (MAPE 20.81%)"; [section 6.3] "quantile approach yields better point estimates through: first, τ=0.5 quantile loss's inherent robustness to outliers, and second... non-median quantile losses effectively serve as regularization"; [corpus] No direct corpus validation for this multi-task regularization claim in LLM contexts.
- Break condition: If the true price distribution has heavy tails where extreme values are informative rather than noisy, median-focused training may underfit important price dynamics.

### Mechanism 3
- Claim: Decoder-only fine-tuned models outperform encoder-only, embedding-based, and few-shot approaches because end-to-end gradient updates adapt representations to the specific text-to-distribution mapping.
- Mechanism: Fine-tuning with LoRA (rank=192, alpha=384) updates both the quantile head and the underlying LLM representations, allowing the model to learn domain-specific pricing features from heterogeneous text. Encoder models freeze representations; embedding approaches decouple text understanding from distribution estimation.
- Core assumption: Pre-trained decoder representations can be efficiently adapted to numerical distribution prediction without catastrophic forgetting.
- Evidence anchors: [abstract] "Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods"; [section 6.1] "fine-tuned Mistral-7B-Quantile model notably outperforms other approaches... MAPE of 6.3% for Used Cars vs. 235% for embedding baselines"; [corpus] Corpus neighbors focus on traditional quantile methods (arXiv:2509.14113) without LLM comparison.
- Break condition: If training data is too small (<1K samples, per Figure 4 scaling), representation updates may overfit; few-shot or frozen embeddings become competitive.

## Foundational Learning

- Concept: **Pinball Loss (Quantile Loss)**
  - Why needed here: Core objective function; understanding its asymmetric gradient structure explains why quantile regression handles outliers differently than MSE.
  - Quick check question: For τ=0.7, should the loss penalize over-prediction or under-prediction more heavily when the true value is above the predicted quantile?

- Concept: **Calibration Error and CRPSS**
  - Why needed here: Evaluation metrics for distributional quality; CE measures coverage match while CRPSS measures full distribution quality relative to a reference.
  - Quick check question: If a model's 90th percentile prediction covers 85% of true values, is it over-confident or under-confident?

- Concept: **LoRA Fine-Tuning**
  - Why needed here: Practical implementation; the paper uses specific LoRA configurations (rank=192) that affect gradient flow to the quantile head.
  - Quick check question: With LoRA, are the original LLM weights updated, or only low-rank adapters?

## Architecture Onboarding

- Component map:
  Input: Tokenized text (title, description, attributes) → LLM backbone (Mistral-7B) → Final hidden state h_T → Quantile head g(·; ϕ) → K=200 quantiles → (Optional) Delta encoding for monotonicity
- Critical path:
  1. Data preprocessing: Log-transform prices; clean labels (LLM-assisted filtering removed 10-100K samples per dataset)
  2. Model setup: Attach quantile head to Mistral-7B; configure LoRA (rank=192)
  3. Training: AdamW, lr=1e-6, weight decay=0.01
  4. Inference: Extract τ=0.5 for point estimates; use full K quantiles for distribution
- Design tradeoffs:
  - K (quantile count): K=200 plateaued; K<50 underfits distribution shape; K>500 no improvement
  - Architecture: Decoder (Mistral-7B) best but expensive; encoder (XLM-R-Large) competitive on small datasets (Boats: 22.7% vs 21.2% MAPE)
  - Smoothing parameter α: 0.01 balances gradient stability vs. loss fidelity
- Failure signatures:
  - Non-monotonic quantiles (q̂_τ90 < q̂_τ80): Indicates need for delta encoding
  - High CE (>0.15) with low CRPSS: Model is mis-calibrated; check label noise
  - Few-shot MAPE >200% (Cars dataset): In-context learning fails on heterogeneous features; requires fine-tuning
- First 3 experiments:
  1. **Baseline sanity check**: Train Mistral-7B-Point (MSE loss) vs. Mistral-7B-Quantile on 10K Amazon samples; expect quantile median to beat point estimate by ~4 MAPE points
  2. **Scale ablation**: Train on {1K, 10K, 100K} samples; plot MAPE curve (should decrease from ~40% to ~24%)
  3. **Calibration diagnostic**: Compute CE across quantile levels; if CE >0.1, investigate whether certain τ levels systematically over/under-cover

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do decoder-only architectures provide intrinsic advantages over encoder-only models for probabilistic regression when parameter counts are matched?
- Basis in paper: [explicit] The authors state they "defer a more thorough investigation that isolates and quantifies the precise architectural effects on point prediction accuracy to future research" after observing mixed results between decoders (Mistral) and encoders (XLM-R).
- Why unresolved: The study compared decoder models (e.g., Mistral-7B) against smaller encoder models (e.g., XLM-R-Large), confounding the effects of architecture with model scale.
- What evidence would resolve it: A controlled ablation study evaluating decoder and encoder architectures of identical sizes on the same regression benchmarks.

### Open Question 2
- Question: Does the LLM-based quantile regression approach generalize effectively to non-price text regression tasks?
- Basis in paper: [explicit] The authors note that while they believe the approach would generalize, "we did not evaluate this on other general regression domains or non-price prediction tasks" such as sentiment analysis or readability scoring.
- Why unresolved: The empirical validation was restricted to price estimation datasets (Amazon, Cars, Boats), leaving performance on other continuous targets unknown.
- What evidence would resolve it: Application of the quantile head methodology to diverse text-to-value datasets (e.g., financial forecasting, text readability).

### Open Question 3
- Question: How does model performance and calibration scale when applying quantile regression heads to LLMs larger than 7B parameters?
- Basis in paper: [explicit] The authors list as a limitation: "First, we did not fine-tune LLMs larger than 7B parameters in size."
- Why unresolved: It is unclear if the performance gap between fine-tuned models and baselines persists, narrows, or widens with increased model capacity (e.g., 13B, 70B models).
- What evidence would resolve it: Fine-tuning larger foundation models with the proposed quantile heads and comparing the resulting MAPE and Calibration Error rates against the Mistral-7B baseline.

## Limitations

- **Scalability and Generalizability**: The model's reliance on textual features means its effectiveness may vary significantly depending on the richness and relevance of available text descriptions across different domains.
- **Computational Costs**: The fine-tuning approach requires substantial computational resources, particularly for larger LLM architectures, with no exploration of the performance-cost tradeoff across different model sizes.
- **Label Quality Dependence**: The study relies on LLM-assisted label correction which inherits potential systematic biases from the correction LLM, and the impact of different label cleaning strategies on final performance isn't thoroughly investigated.

## Confidence

**High Confidence**: The core finding that quantile regression with multi-quantile heads attached to LLMs produces better calibrated distributions and point estimates than traditional approaches is well-supported by the experimental results across three diverse datasets.

**Medium Confidence**: The explanation for why quantile regression outperforms point regression (outlier robustness and implicit regularization) is plausible but not definitively proven through controlled experiments specifically designed to isolate these mechanisms.

**Low Confidence**: The claim about specific LoRA hyperparameters (rank=192, alpha=384) being optimal is based on single configuration experiments rather than systematic hyperparameter search, making these values likely dataset-specific rather than generally applicable.

## Next Checks

1. **Domain Transfer Experiment**: Fine-tune the same Mistral-7B-Quantile model on a completely different domain (e.g., housing prices or stock prices) with minimal hyperparameter tuning to assess true generalization capability beyond the three studied datasets.

2. **Label Quality Impact Study**: Conduct controlled experiments varying the proportion of erroneous labels in the training data (0%, 10%, 20%, 50%) to quantify how sensitive the model is to label noise, both with and without LLM-assisted cleaning.

3. **Scaling Law Analysis**: Systematically vary dataset sizes (1K, 10K, 100K, 500K) across all three domains while keeping all other hyperparameters fixed to map out the full scaling behavior and identify potential diminishing returns or breakpoints in performance improvement.