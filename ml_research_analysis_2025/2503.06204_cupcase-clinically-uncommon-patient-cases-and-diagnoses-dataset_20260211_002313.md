---
ver: rpa2
title: 'CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset'
arxiv_id: '2503.06204'
source_url: https://arxiv.org/abs/2503.06204
tags:
- case
- diagnosis
- medical
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CUPCase, a dataset of 3,562 real-world clinical
  case reports containing rare and complex diagnoses, extracted from the BMC Journal
  of Medical Case Reports. Unlike existing medical benchmarks derived from exam questions,
  CUPCase captures the complexity of real patient cases, including uncommon presentations
  and unexpected treatment responses.
---

# CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset

## Quick Facts
- arXiv ID: 2503.06204
- Source URL: https://arxiv.org/abs/2503.06204
- Authors: Oriel Perets; Ofir Ben Shoham; Nir Grinberg; Nadav Rappoport
- Reference count: 8
- GPT-4o achieves 87.9% accuracy on CUPCase multiple-choice tasks, outperforming clinical LLMs

## Executive Summary
This paper introduces CUPCase, a dataset of 3,562 real-world clinical case reports containing rare and complex diagnoses, extracted from the BMC Journal of Medical Case Reports. Unlike existing medical benchmarks derived from exam questions, CUPCase captures the complexity of real patient cases, including uncommon presentations and unexpected treatment responses. The dataset includes both multiple-choice questions with semantically similar distractors and open-ended diagnoses. When evaluated on state-of-the-art LLMs, GPT-4o achieved the highest performance—87.9% accuracy on multiple-choice and 0.764 BERTScore F1 on open-ended tasks—outperforming clinical LLMs like Meditron-70B and MedLM-Large. Even with only 20% of case text, GPT-4o retained over 87% of its diagnostic accuracy, highlighting its potential for early clinical decision support. CUPCase provides a reproducible benchmark for evaluating LLMs on realistic, complex clinical scenarios.

## Method Summary
The dataset was constructed by extracting case reports from BMC Journal of Medical Case Reports (2012-2020), preprocessing to remove explicit diagnosis mentions while retaining clinical presentation text, and generating semantically similar distractors using JINA AI embeddings and ICD-10-CM descriptions. The resulting dataset contains 3,562 cases with multiple-choice and open-ended diagnosis tasks. Evaluation used zero-shot prompting across multiple LLMs including GPT-4o, Meditron-70B, MedLM-Large, and Llama variants. Multiple-choice accuracy was measured by selecting the highest probability option, while open-ended tasks used BERTScore F1 against ground truth diagnoses. Bootstrap sampling (8 samples of 500 cases each) provided statistical validation.

## Key Results
- GPT-4o achieved the highest accuracy with 87.9% on multiple-choice tasks
- Open-ended diagnosis performance: GPT-4o scored 0.764 BERTScore F1, significantly outperforming clinical LLMs (Meditron-70B: 0.598, MedLM-Large: 0.593)
- Even with only 20% of case text, GPT-4o retained over 87% of its diagnostic accuracy
- General-purpose LLMs (GPT-4o, Llama3-70B Instruct) outperformed clinical LLMs on both task types

## Why This Works (Mechanism)

### Mechanism 1
Real-world case reports provide diagnostic complexity absent from exam-derived benchmarks. BMC Journal of Medical Case Reports publishes cases that deviate from textbook abstractions—rare diseases, uncommon presentations, unexpected treatment responses. These reports maintain chronological case presentations that mirror actual clinical encounters, unlike exam questions which test canonical knowledge patterns. Core assumption: Case presentation text, stripped of explicit diagnosis mentions, retains sufficient clinical signal for diagnostic inference.

### Mechanism 2
Semantic similarity distractor generation creates meaningful diagnostic discrimination. JINA AI embeddings encode diagnoses into 768-dim vectors. Cosine similarity against ICD-10-CM descriptions yields top-4 matches; ranks 2-4 become distractors. High BERTScore F1 (mean 68-69.5%) between distractors and correct diagnoses confirms semantic proximity, forcing models to discriminate fine-grained clinical distinctions rather than eliminate obviously wrong options. Core assumption: Embedding similarity correlates with clinical confusability.

### Mechanism 3
General-purpose LLMs outperform clinical LLMs on complex diagnosis due to broader knowledge integration. GPT-4o achieved 87.9% MC accuracy vs. Meditron-70B's 85.55% and MedLM-Large's 84.40%. In open-ended tasks, gap widened: GPT-4o 0.764 BERTScore vs. Meditron-70B's 0.598. Error analysis suggests model mistakes sometimes mirror physician errors—potentially indicating reasoning rather than retrieval failure. Core assumption: Performance differences stem from training data breadth rather than benchmark contamination.

## Foundational Learning

- Concept: Clinical case report structure
  - Why needed here: Understanding "case presentation" vs. "final diagnosis" sections is essential for dataset construction logic.
  - Quick check question: Why must follow-up treatment information be removed alongside explicit diagnosis mentions?

- Concept: ICD-10-CM hierarchical coding
  - Why needed here: Distractor generation relies on semantic similarity across a standardized diagnosis taxonomy.
  - Quick check question: Why might cosine similarity on code descriptions fail to capture clinical confusability?

- Concept: Zero-shot evaluation
  - Why needed here: Long case texts (avg 938 tokens) preclude few-shot prompting; understanding zero-shot limitations is critical for interpreting results.
  - Quick check question: What tradeoffs exist between zero-shot evaluation authenticity and potential performance gains from few-shot examples?

## Architecture Onboarding

- Component map:
BMC Journal → Python extraction script → Raw case presentations → GPT-4o-mini preprocessing (diagnosis removal + extraction) → Clean case text + Final diagnosis label → JINA AI embeddings ← ICD-10-CM embeddings (94,766 codes) → Cosine similarity ranking → Top-4 matches → CUPCase-MC (1 correct + 3 distractors) + CUPCase-Open (case + diagnosis label)

- Critical path:
  1. Preprocessing validation: 1% manual samples validated extraction quality; 14% still contained diagnosis mentions, requiring second pass. This validation loop is production-critical.
  2. Distractor quality: Manual check confirmed no correct answers leaked into distractors.
  3. Tokenizer alignment: Llama3-8B tokenizer used for cumulative token analysis.

- Design tradeoffs:
  - API dependency: GPT-4o-mini preprocessing introduces cost and reproducibility concerns vs. rule-based extraction.
  - Single-source corpus: BMC-only limits specialty coverage; 18.4% Oncology, 10% Infectious Disease, with under 1% in Radiology, Pathology.
  - Open-source models preferred for clinical deployment: Llama3-70B Instruct achieves 85.7% (2.2% below GPT-4o) but runs locally, addressing privacy concerns.
  - Text-only evaluation: Images present in dataset but unused; multimodal evaluation deferred.

- Failure signatures:
  - Contamination risk: "we cannot be certain that the dataset case presentations were not included in the training data"
  - Distractor quality degradation: If embedding model poorly captures medical semantics, distractors become trivially wrong or impossibly close.
  - Specialty imbalance: Performance may not generalize to underrepresented disciplines (Anesthesiology 0.22%, Vascular Surgery 0.11%).

- First 3 experiments:
  1. Reproduction baseline: Run provided evaluation code on Llama3-8B-Instruct to validate MC accuracy (~80%) and BERTScore (~0.51) match reported values.
  2. Contamination probe: Check whether GPT-4o performance drops on held-out 2021+ BMC cases (post training cutoff assumption).
  3. Specialty stratification: Evaluate accuracy by discipline to test whether Oncology (18.4% of data) drives overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
Does the inclusion of visual data (images) alongside text in the CUPCase dataset significantly improve the diagnostic accuracy of multi-modal LLMs compared to text-only models? Basis: The authors explicitly state in the "Future Work" section that a natural future direction is to evaluate multi-modal language models on the multi-modal dataset. Unresolved because the current study evaluated models using only the text from case presentations. What evidence would resolve it: Benchmarking multi-modal models (e.g., GPT-4o vision capabilities) on the full image-text pairs of CUPCase and comparing performance against the reported text-only baselines.

### Open Question 2
How does the diagnostic performance of LLMs using limited clinical information (e.g., the first 20% of case tokens) compare to the diagnostic accuracy of human physicians provided with the same partial data? Basis: The authors note that this approach "offers insights into the models' ability to formulate correct diagnoses with limited information" and suggest future work to explore model superiority or inferiority compared to physicians. Unresolved because the paper demonstrates GPT-4o's performance but does not provide a comparative baseline of human clinical reasoning under the same information constraints. What evidence would resolve it: A blinded user study where human clinicians diagnose cases based on the first 20% of the text, with results compared to the model's specific performance on those same truncated inputs.

### Open Question 3
Can a mixture-of-experts (MoE) approach, utilizing models fine-tuned for specific clinical domains, outperform general-purpose LLMs on complex or uncommon case diagnoses? Basis: Based on error analysis showing that model mistakes often require domain-specific expertise, the authors suggest a mixture-of-experts approach may improve diagnostic performance. Unresolved because the current evaluation shows general-purpose models outperforming clinical LLMs, but it did not test an ensemble or MoE architecture specifically designed to route cases to domain specialists. What evidence would resolve it: Constructing an MoE system using the clinical variants evaluated in the paper and measuring its aggregate accuracy against GPT-4o on the CUPCase dataset.

## Limitations
- Dataset contamination risk: Uncertainty about whether case presentations appeared in LLM training data, particularly for GPT-4o and MedLM-Large
- Single-source corpus: Restricting to BMC Journal of Medical Case Reports creates specialty bias (18.4% Oncology, 10% Infectious Disease) and limits generalizability
- Distractor quality dependency: Semantic similarity via embeddings assumes cosine similarity correlates with clinical confusability, which may not hold for rare diseases

## Confidence
- High confidence: GPT-4o's superior performance (87.9% MC accuracy, 0.764 BERTScore F1) over both clinical LLMs and open-source alternatives is robust given consistent methodology and bootstrap validation
- Medium confidence: The benchmark's ability to discriminate clinical reasoning quality depends on the assumption that case presentation text contains sufficient diagnostic signal without explicit diagnosis mentions
- Low confidence: Claims about GPT-4o's potential for early clinical decision support (87% accuracy with 20% text) extrapolate from zero-shot performance without accounting for real-world deployment factors

## Next Checks
1. Contamination probe: Evaluate whether GPT-4o performance degrades significantly on held-out 2021+ BMC cases (post-training cutoff) compared to 2012-2020 cases to assess memorization vs. reasoning
2. Specialty stratification: Compute accuracy and BERTScore by medical specialty to determine if performance is driven by overrepresented domains (Oncology) and identify underperforming specialties for targeted improvement
3. Distractor validation study: Have clinical experts rate semantic similarity and clinical confusability of distractors to validate whether embedding-based similarity captures meaningful diagnostic distinctions rather than trivial semantic relationships