---
ver: rpa2
title: 'OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded
  Agentic Coding'
arxiv_id: '2601.10343'
source_url: https://arxiv.org/abs/2601.10343
tags:
- instruction
- checklist
- user
- code
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OctoBench, a benchmark designed to evaluate
  how well language models follow heterogeneous instructions in repository-grounded
  agentic coding environments. The benchmark includes 34 environments, 217 tasks,
  and 7,098 checklist items across three scaffold types (Claude Code, Kilo, and Droid),
  focusing on persistent, multi-source constraints that arise in real-world coding
  agents.
---

# OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding

## Quick Facts
- **arXiv ID:** 2601.10343
- **Source URL:** https://arxiv.org/abs/2601.10343
- **Reference count:** 40
- **Key outcome:** Large gap between per-check compliance (79.75–85.64%) and end-to-end success (9.66–28.11%) reveals models' limited reliability in scaffold-aware instruction following.

## Executive Summary
This paper introduces OctoBench, a benchmark designed to evaluate how well language models follow heterogeneous instructions in repository-grounded agentic coding environments. The benchmark includes 34 environments, 217 tasks, and 7,098 checklist items across three scaffold types (Claude Code, Kilo, and Droid), focusing on persistent, multi-source constraints that arise in real-world coding agents. Using an automated observation-and-scoring toolkit, the authors measure instruction-following compliance through binary checklist items, disentangling task-solving from rule-following. Experiments on eight representative models reveal a large gap between per-check compliance (CSR: 79.75–85.64%) and end-to-end success (ISR: 9.66–28.11%), indicating that high partial compliance does not guarantee full execution reliability. Models show varying robustness across scaffold types and instruction categories, with skill constraints being particularly challenging. The study also finds that external supervisory signals can improve instruction following, and that instruction-following capability generally degrades with interaction length, except for top models like Claude-Opus-4.5. The benchmark and toolkit are released to support reproducible evaluation and future model development.

## Method Summary
The authors benchmark instruction-following compliance by running LLMs through scaffold-aware coding environments (Claude Code v2.0.69, Kilo v0.10.2, Droid v0.42.2) in isolated Docker containers. Each of the 217 task instances is run 3 times (30-min timeout) with full trajectory logging via a proxy. Checklists of 7,098 binary items capture compliance across seven instruction-source categories (system prompt, user query, config files, etc.). An ensemble of three judge models (GPT-5.1, Claude-Sonnet-4.5, Gemini-3-Pro) independently scores each checklist item; scores are averaged to compute per-instance ISR (all-or-nothing) and per-check CSR (average pass rate). The methodology explicitly decouples task completion from rule-following and normalizes all scaffolds to a unified message/tool format for scoring consistency.

## Key Results
- Large CSR–ISR gap: models achieve 79.75–85.64% per-check compliance but only 9.66–28.11% end-to-end success.
- Instruction-following capability degrades with interaction length, except for top models like Claude-Opus-4.5.
- Skill.md constraints are hardest to follow; compliance varies significantly across scaffold types (Claude Code vs. Kilo vs. Droid).
- External supervisory signals can substantially improve instruction-following compliance.

## Why This Works (Mechanism)
OctoBench works by isolating instruction-following as a measurable dimension separate from task-solving. The scaffold-aware checklist approach ensures persistent constraints (from config files, tool schemas, memory) are explicitly tracked across multi-turn interactions. Automated trajectory logging and LLM-as-a-judge scoring enable scalable, reproducible evaluation without manual annotation overhead. The use of ensemble judging mitigates individual judge biases and increases scoring reliability. By running each instance multiple times and across different scaffold types, the benchmark captures both variability and robustness in model behavior.

## Foundational Learning
- **Scaffold-aware evaluation:** Needed to capture real-world constraints in agentic coding; quick check: verify checklist items map to actual scaffold elements.
- **Checklist-based compliance scoring:** Enables granular measurement of instruction-following beyond binary task success; quick check: confirm checklist items are objective and decidable.
- **Ensemble LLM judging:** Reduces variance and bias in automated scoring; quick check: ensure judges agree on >90% of items in validation sample.
- **Trajectory normalization:** Unifies diverse scaffold logs for consistent scoring; quick check: confirm all scaffolds convert to same {meta, tools, messages} schema.
- **Isolated Docker execution:** Guarantees reproducibility and prevents cross-contamination; quick check: verify container isolation and timeout enforcement.
- **Multiple runs per instance:** Captures model stochasticity and interaction variability; quick check: confirm 3-run averaging is applied before ISR/CSR calculation.

## Architecture Onboarding
- **Component map:** Docker container → Proxy logger → Trajectory normalizer → LLM judge panel → ISR/CSR aggregator
- **Critical path:** Model execution → Proxy-trajectory capture → Unified format conversion → Checklist evaluation → Score aggregation
- **Design tradeoffs:** Isolated containers vs. resource overhead; automated judging vs. potential scoring errors; multiple runs vs. evaluation time.
- **Failure signatures:** Low ISR with high CSR indicates partial compliance without task completion; scaffold-specific ISR variance points to robustness gaps; CSR drop over turns signals interaction fatigue.
- **First experiments:** (1) Run a single instance across all three scaffolds and compare ISR/CSR; (2) Execute 5 instances with and without external supervision to measure compliance gain; (3) Plot ISR vs. turn count for a top model to verify degradation trend.

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend on proprietary model APIs (GPT-5.1, Claude-Sonnet-4.5, Gemini-3-Pro) which may not be publicly available.
- Cross-scaffold variance and conditional constraint handling require careful manual auditing to avoid false negatives.
- Degradation-over-time trends are based on limited interaction-length granularity; extended episodes may be needed for robust validation.

## Confidence
- **High:** Benchmark framework design, Docker isolation, trajectory normalization, ensemble judging methodology.
- **Medium:** Comparative model rankings, cross-scaffold variance claims, degradation-over-time trends.
- **Low:** Claims requiring proprietary API access beyond current release.

## Next Checks
1. Release and publicly host the benchmark dataset and Docker images so that independent labs can reproduce the CSR/ISR measurements without reliance on proprietary APIs.
2. Perform a stratified human audit of checklist judgments (e.g., 5% sample per scaffold type) to confirm that the ensemble LLM scoring achieves >95% alignment with objective ground-truth items.
3. Test the degradation trend by running extended interaction episodes (e.g., 100+ turns) on a subset of instances and measuring ISR at fixed intervals to verify the reported fatigue effect for models other than Claude-Opus-4.5.