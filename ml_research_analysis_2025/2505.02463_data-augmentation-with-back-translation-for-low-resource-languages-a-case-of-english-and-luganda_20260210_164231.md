---
ver: rpa2
title: 'Data Augmentation With Back translation for Low Resource languages: A case
  of English and Luganda'
arxiv_id: '2505.02463'
source_url: https://arxiv.org/abs/2505.02463
tags:
- translation
- data
- luganda
- datasets
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles the challenge of low-resource machine translation
  for English-Luganda by employing back translation to generate synthetic data from
  monolingual corpora. The authors construct bilingual and monolingual datasets from
  multiple sources, including web-crawled news text, and apply both standard, iterative,
  and incremental back translation techniques.
---

# Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda

## Quick Facts
- arXiv ID: 2505.02463
- Source URL: https://arxiv.org/abs/2505.02463
- Authors: Richard Kimera; Dongnyeong Heo; Daniela N. Rim; Heeyoul Choi
- Reference count: 40
- Primary result: >10 BLEU point improvement over benchmarks (40.25 Eng→Lug, 44.25 Lug→Eng)

## Executive Summary
This study demonstrates how back translation can mitigate bilingual data scarcity for English-Luganda machine translation by generating synthetic parallel sentences from monolingual corpora. The authors develop a custom "OurBT" approach that strategically selects and combines monolingual datasets to maximize BLEU score gains. Through iterative back translation techniques, they achieve substantial improvements - over 10 points in both translation directions compared to previous benchmarks, reaching 40.25 BLEU for Eng→Lug and 44.25 for Lug→Eng.

## Method Summary
The approach trains a baseline Transformer model on ~98K bilingual sentences, then generates synthetic parallel data by translating monolingual corpora in both languages. The custom OurBT algorithm evaluates multiple monolingual dataset combinations by training intermediate models and selecting those yielding highest validation BLEU scores. This curated synthetic data is merged with bilingual data for final training. Iterative back translation is applied 3-4 times, with each cycle using the improved model to regenerate synthetic data, progressively refining translation quality.

## Key Results
- Achieved 40.25 BLEU (Eng→Lug) and 44.25 BLEU (Lug→Eng), exceeding previous benchmarks by over 10 points
- OurBT approach with Mozilla Common Voice, Makerere Text, and YouTube news headlines yielded best Eng→Lug BLEU of 35.94 before iterations
- Iterative back translation progression: Eng→Lug BLEU improved from 29.67 (baseline) to 40.25 after 3 iterations
- Additional metrics (ChrF2, TER) confirm enhanced translation quality, particularly important for morphologically rich Luganda

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Back translation mitigates bilingual data scarcity by generating synthetic parallel sentences from monolingual corpora.
- Mechanism: Train initial NMT model on limited bilingual data → translate monolingual sentences in target language → pair synthetic translations with original monolingual text → merge synthetic pairs with bilingual data → retrain model on expanded dataset.
- Core assumption: Synthetic parallel data provides useful training signal even when translation quality is imperfect.
- Evidence anchors:
  - [abstract] "demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora"
  - [section 1] "This standard technique involves training a model using bilingual data, it then uses the model to generate synthetic data by translating monolingual data. Finally, we merge the synthetically generated data with the bilingual data"
  - [corpus] Related papers confirm data augmentation effectiveness for African MT (avg neighbor FMR=0.401), though specific BT validation varies by language pair.
- Break condition: Synthetic data introduces systematic errors that amplify across training; model degrades when source-side synthetic text is incoherent.

### Mechanism 2
- Claim: Strategic selection of monolingual datasets by BLEU-guided evaluation outperforms naive concatenation.
- Mechanism: For each monolingual subset, generate synthetic pairs → train candidate model → evaluate BLEU on validation set → select highest-scoring dataset combinations → retrain final model on selected synthetic data only.
- Core assumption: BLEU score on validation set reliably indicates which monolingual sources provide domain-relevant signal.
- Evidence anchors:
  - [abstract] "OurBT approach strategically selects and combines monolingual datasets to maximize BLEU score gains"
  - [section 3.2.2] Algorithm 1: "Select models and datasets with the highest BLEU scores for final training"
  - [section 4.3] "OurBT was very instrumental in helping us select the best dataset combinations... Mozilla common voice, Makerere Text and Speech, and Youtube news headlines" achieved best Eng2Lug BLEU (35.94) vs StandardBT (32.29)
  - [corpus] Limited direct evidence on dataset selection strategies; related work focuses on augmentation existence rather than curation methods.
- Break condition: Validation set domain mismatch causes selection of datasets that don't generalize to test distribution.

### Mechanism 3
- Claim: Iterative back translation progressively refines model quality through repeated synthetic data generation cycles.
- Mechanism: Train model → generate synthetic data → retrain → use improved model to regenerate synthetic data → repeat until BLEU plateaus.
- Core assumption: Each iteration produces higher-quality synthetic data because the translating model has improved.
- Evidence anchors:
  - [abstract] "applying Iterative and Incremental Back translation techniques"
  - [section 4.3, Table 6] Eng2Lug BLEU progression: 29.67 (baseline) → 35.94 (OurBT) → 37.96 (iter 1) → 39.31 (iter 2) → 40.25 (iter 3)
  - [section 4.3, Table 7] Lug2Eng BLEU progression: 32.92 (baseline) → 39.97 (OurBT) → 44.25 (iter 3)
  - [corpus] Iterative approaches mentioned in related low-resource MT literature but comparative iteration data sparse.
- Break condition: Model collapse when synthetic data amplifies errors; diminishing returns after convergence.

## Foundational Learning

- Concept: Transformer Architecture (encoder-decoder with self-attention)
  - Why needed here: All experiments use Transformer base model (N=6, d_model=512, d_ff=2048, h=8); understanding attention is essential for debugging translation quality.
  - Quick check question: Why does self-attention help with agglutinative languages like Luganda?

- Concept: Byte-Pair Encoding (BPE) Subword Tokenization
  - Why needed here: Vocabulary set to 10,000 subword units; critical for handling Luganda's agglutinative morphology where single words carry multiple morphemes.
  - Quick check question: How does BPE help prevent OOV issues in morphologically rich languages?

- Concept: Evaluation Metrics Beyond BLEU (ChrF, TER, SacreBLEU)
  - Why needed here: Paper explicitly argues BLEU alone is inadequate for morphologically rich languages; ChrF captures character n-gram overlap better suited for agglutinative morphology.
  - Quick check question: Why would two translations with identical BLEU scores have different ChrF2 scores?

## Architecture Onboarding

- Component map: Bilingual corpus (merged from 4 sources, ~98K sentences) + Monolingual corpora (English: ~1.28M sentences; Luganda: ~333K sentences) → Selenium web crawler → text cleaning (remove hyperlinks, code-mixed text, special characters) → BPE tokenization (10,000 subword units) → Transformer base model (6 encoder/decoder layers, 512 dim, 8 attention heads, dropout 0.1) → Adam optimizer, batch size 1000 → early stopping at 40 epochs → BT pipeline (baseline model → beam search decoding of monolingual data → synthetic pair creation → model retraining)

- Critical path:
  1. Merge and split bilingual data (train/test/val); create "newtest" validation set from news domain
  2. Train baseline bilingual model → record BLEU/ChrF2/TER
  3. Run OurBT: translate each monolingual subset independently, evaluate, select best combinations by BLEU
  4. Run IteBT: 3-4 iterations of synthetic data regeneration until convergence
  5. Final evaluation on all metrics

- Design tradeoffs:
  - Bible corpus inclusion: +32K sentences but archaic vocabulary ("thee," "believeth") causes context mismatch; paper shows removal improves BLEU (39.05→53.77 Eng2Lug)
  - Dataset quantity vs. domain match: Smaller context-aligned datasets (YouTube news: 5K-20K sentences) outperformed larger generic sources
  - Iteration depth: 3 iterations yielded +10.58 BLEU gain but training cost multiplies linearly

- Failure signatures:
  - BLEU decreases after IncBT (Table 5: Lug2Eng dropped 44.36→35.65 with Bible text): monolingual data context misaligned with evaluation domain
  - ChrF2 significantly exceeds BLEU: morphological correctness present but word-level n-gram matching penalized
  - TER increases after BT: synthetic data introduces fluency errors requiring more edits

- First 3 experiments:
  1. **Baseline establishment**: Train bilingual model with/without Bible text; measure impact of domain mismatch on default test set
  2. **Standard BT comparison**: Apply StandardBT to single monolingual dataset combination; quantify minimal gain (+2.49 BLEU for Eng2Lug)
  3. **OurBT validation**: Run dataset selection algorithm across all monolingual sources; confirm BLEU improvement (35.94) over StandardBT before proceeding to iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can leveraging linguistic similarities among African languages through multilingual NMT or unsupervised approaches outperform the current semi-supervised back-translation benchmarks for English-Luganda?
- Basis in paper: [explicit] The conclusion states that "Future work will explore multilingual methods, language models and unsupervised NMT, leveraging linguistic similarities among African languages."
- Why unresolved: The current study focused strictly on semi-supervised back-translation (BT) techniques using monolingual corpora for a single language pair, without comparing against multilingual or unsupervised baselines.
- What evidence would resolve it: Comparative experiments evaluating multilingual pivot-based models or unsupervised NMT against the established OurBT benchmarks on the same test sets.

### Open Question 2
- Question: Does the "OurBT" algorithm specifically improve translation quality by enhancing the model's contextual representation of diverse datasets, rather than merely acting as a data scaling method?
- Basis in paper: [inferred] The authors state, "Our assumption is that when ourBT is applied, the model learns more contextual representation from each dataset that is used," but provide only BLEU score correlations rather than a causal analysis.
- Why unresolved: The improvement in scores is observed, but the internal mechanism—whether the strategic selection forces better contextual generalization or simply adds volume—remains an unverified hypothesis.
- What evidence would resolve it: An ablation study analyzing embedding clusters or attention maps to verify if the model captures distinct contextual features from the curated mini-datasets compared to standard BT.

### Open Question 3
- Question: What is the optimal ordering strategy for merging diverse monolingual mini-datasets during incremental back-translation to maximize performance gains?
- Basis in paper: [inferred] The paper notes, "Even though we define no specific order in which these mini-datasets should be combined," suggesting the sequence is currently determined empirically rather than theoretically.
- Why unresolved: While the authors found that datasets with similar contextual structures performed well, they did not determine if the sequence of ingestion (e.g., easy-to-hard or domain-specific ordering) impacts the final model convergence.
- What evidence would resolve it: Systematic experiments varying the ingestion order of the identified monolingual datasets (e.g., Mozilla, Makerere, YouTube) to measure sensitivity to sequencing.

## Limitations

- Results heavily depend on specific validation set composition ("newtest" augmented with news pairs), raising questions about generalizability
- BLEU-guided dataset selection may introduce overfitting to validation distribution rather than true test performance
- Single language pair focus limits generalizability claims to other low-resource language pairs
- Lack of explicit error analysis prevents understanding which translation phenomena improved most

## Confidence

- **High Confidence**: BLEU score improvements (40.25/44.25 vs previous benchmarks) are well-documented with clear methodology and reproducible results
- **Medium Confidence**: Effectiveness of OurBT dataset selection strategy is supported by empirical results but lacks theoretical justification for why BLEU-guided selection works
- **Medium Confidence**: Iterative BT progression is demonstrated but the diminishing returns after 3 iterations suggest potential optimization issues
- **Low Confidence**: Generalization claims to other low-resource languages are not empirically validated beyond this single language pair

## Next Checks

1. **Domain Transfer Test**: Apply the trained model to standard Luganda test sets without news augmentation to verify BLEU scores remain above 40; this validates whether "newtest" artificially inflated results
2. **Error Analysis Validation**: Conduct human evaluation on 100 translated sentences to identify whether improvements come from fluency, adequacy, or morphological correctness rather than just n-gram matching
3. **Cross-Lingual Replication**: Apply the same OurBT methodology to a different low-resource pair (e.g., English-Kinyarwanda) to test whether dataset selection via validation BLEU generalizes beyond English-Luganda