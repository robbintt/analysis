---
ver: rpa2
title: Learning a Pessimistic Reward Model in RLHF
arxiv_id: '2505.20556'
source_url: https://arxiv.org/abs/2505.20556
tags:
- reward
- policy
- dataset
- pessimistic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PET, a novel pessimistic reward fine-tuning
  method for RLHF that learns a reward model robust to reward hacking without requiring
  KL regularization. PET uses adversarial training against rejection sampling to produce
  a pessimistic reward model that provides trustworthy evaluations.
---

# Learning a Pessimistic Reward Model in RLHF

## Quick Facts
- **arXiv ID:** 2505.20556
- **Source URL:** https://arxiv.org/abs/2505.20556
- **Reference count:** 38
- **Primary result:** PET achieves 98.3% positive sentiment likelihood on IMDB vs 95% for best baseline, outperforming state-of-the-art RLHF methods.

## Executive Summary
This work introduces PET, a pessimistic reward fine-tuning method for RLHF that learns a robust reward model without requiring KL regularization. PET uses adversarial training against rejection sampling to create a reward model that's skeptical of its own high-scoring outputs, preventing reward hacking. The key insight is that pessimism can be baked into the reward model itself, eliminating the need for explicit regularization during policy optimization. Experiments on TL;DR summarization and IMDB datasets show PET improves rejection sampling performance and enables high-performing policies with large KL divergence from the dataset.

## Method Summary
PET trains a reward model to be pessimistic by minimizing the score gap between rejection sampling policies and a reference policy while maintaining prediction accuracy on the preference dataset. The method solves a minimax problem where the reward model learns to give low relative scores to policies that would be selected by rejection sampling on that same reward. This creates a self-consistent pessimistic estimator. During downstream policy optimization, the pessimistically fine-tuned reward model eliminates the need for KL regularization because it already penalizes out-of-distribution exploitation. The approach combines standard reward modeling, PET fine-tuning via adversarial rejection sampling, and PPO policy optimization without KL constraints.

## Key Results
- PET achieves 98.3% positive sentiment likelihood on IMDB vs 95% for best baseline
- PET-trained policies can achieve high KL divergence (114.0) while maintaining high win rates (40.8%)
- Proxy-reward policies with high KL (192.6) collapse to win rate 7.2%, demonstrating PET prevents reward hacking
- PET improves rejection sampling performance compared to standard reward models

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Training Against Rejection Sampling Policies
PET induces pessimism by training the reward model to minimize the score gap between rejection sampling policies and a reference policy, while maintaining prediction accuracy on the preference dataset. The method solves a minimax problem where the reward model learns to give low relative scores to policies that would be selected by rejection sampling on that same reward. This creates a self-consistent pessimistic estimator—the reward model learns to be skeptical of outputs it would itself rate highly through the RS process. Proposition 2.1 shows that the gradient through the RS policy is zero at the optimum, simplifying implementation.

### Mechanism 2: Pessimism Transfer from Reward Model to Policy Optimization
A pessimistically fine-tuned reward model eliminates the need for KL regularization during downstream policy optimization, because the reward model itself already penalizes out-of-distribution exploitation. Standard KL regularization penalizes any policy far from the reference, regardless of quality. PET's pessimistic reward instead learns to specifically penalize policies that achieve high proxy rewards through spurious features not supported by the dataset. Table 2 shows PET-trained policies can achieve high KL (114.0) while maintaining high win rate (40.8%), whereas proxy-reward policies with high KL (192.6) collapse (win rate 7.2%).

### Mechanism 3: Dataset Coverage as the Theoretical Safety Boundary
The theoretical guarantee bounds performance degradation by the coverage coefficient C_μD, meaning PET policies cannot dramatically underperform any well-covered RS policy. The coverage coefficient measures how much the reward model's relative scoring can deviate from the true reward for a given policy pair. By constraining solutions to reward models with low dataset prediction loss, PET ensures that covered policies are evaluated accurately. The bound scales as O(√(log(N_ε)/N)), improving with dataset size.

## Foundational Learning

- **Concept: Rejection Sampling (Best-of-N) as Policy Optimization**
  - Why needed here: PET uses RS as its inner optimization loop; understanding that RS is a valid policy (not just an inference trick) is essential.
  - Quick check question: Can you explain why sampling N responses and selecting the highest-reward one constitutes a stochastic policy, and compute its computational cost?

- **Concept: The Reward Hacking / Over-optimization Problem**
  - Why needed here: The entire motivation for PET rests on understanding why high proxy reward ≠ high true quality.
  - Quick check question: Given a proxy reward model with 85% correlation to true preferences, why might a policy achieving 99th percentile proxy scores actually be worse than one at the 90th percentile?

- **Concept: Minimax/Adversarial Training Dynamics**
  - Why needed here: PET frames pessimism as a game between reward model and policy; instability is a known failure mode.
  - Quick check question: In GAN-style adversarial training, what happens if the discriminator trains much faster than the generator? How might this analogy apply to PET?

## Architecture Onboarding

- **Component map:** Reward Model -> PET Fine-tuning (Adversarial RS) -> Pessimistic Reward -> PPO Policy Optimization
- **Critical path:**
  1. Train proxy reward model to convergence on preference dataset D
  2. Initialize PET with proxy reward, run T iterations of adversarial RS training
  3. Export pessimistic reward; run PPO with β_KL=0 (or very small)
- **Design tradeoffs:**
  - **n (RS samples):** Higher n improves RS policy quality but increases PET compute. Paper uses n=64 due to resource limits; principled choice depends on expected policy quality gap.
  - **β (pessimism coefficient):** Controls prediction loss vs. pessimism tradeoff. Paper uses 1/β=0.1. Too high → over-pessimism, excluding good policies; too low → insufficient robustness.
  - **Temperature gap (π_0 vs π_ref):** Larger gap increases RS exploration but may generate low-quality candidates that waste capacity.
- **Failure signatures:**
  - **Reward model prediction loss increases significantly during PET:** Indicates over-pessimism; increase β.
  - **PET-trained policy performs well on proxy reward but poorly on ground truth:** Suggests pessimism insufficient; decrease β or increase n.
  - **Training instability / oscillating losses:** RS policy changing too rapidly; consider smaller learning rate or gradient clipping on reward model.
- **First 3 experiments:**
  1. **Ablation on β:** Train PET with β ∈ {0.05, 0.1, 0.2, 0.5} on a held-out validation split. Plot: prediction loss, RS win rate vs. reference, and downstream PPO performance. Identify the knee point where prediction loss begins degrading.
  2. **Coverage diagnosis:** For your target task, compute the coverage coefficient C_μD for the SFT policy vs. π_ref. If coverage is poor (>10³), consider expanding the preference dataset before running PET.
  3. **Baseline comparison on proxy vs. pessimistic reward:** Run PPO (no KL) on both the proxy reward and PET reward. Expected result: proxy version should show reward hacking (high proxy reward, low ground truth); PET version should maintain alignment. If both fail, pessimism transfer is not working.

## Open Questions the Paper Calls Out

- **Question:** Can the PET framework be effectively extended to the online RLHF setting where the agent actively collects preference data?
  - **Basis in paper:** The authors state in Section 6 that the scope is limited to the offline setting, while Section 5.3 distinguishes online RLHF as a distinct paradigm where data distribution is actively controlled.
  - **Why unresolved:** The theoretical guarantees rely on dataset coverage coefficients, which are static in the offline setting but might behave unpredictably under dynamic distribution shifts during online exploration.
  - **What evidence would resolve it:** Empirical evaluation of PET in an iterative online learning loop, measuring stability and sample efficiency compared to online versions of DPO or PPO.

- **Question:** Does the PET method maintain its robustness and performance advantages when scaled to LLMs significantly larger than the 1B parameter Pythia model used in the experiments?
  - **Basis in paper:** In Section 6, the authors acknowledge that "due to limited computational resources, the evaluation is performed on a subset of RLHF tasks based on LLMs with relatively small size."
  - **Why unresolved:** The dynamics of reward hacking and over-optimization may change qualitatively with model scale, and the efficiency of the adversarial training loop might degrade on larger architectures.
  - **What evidence would resolve it:** Benchmarking PET on standard 7B or 70B scale models (e.g., Llama) on complex alignment tasks to verify if the benefits over KL-regularized PPO persist.

## Limitations

- The theoretical coverage guarantees rely heavily on the assumption that the dataset distribution covers the policy space well, but empirical validation of coverage coefficients across tasks is limited.
- The computational cost of rejection sampling with n=64 remains a practical barrier, and the paper doesn't fully explore how much pessimism degrades with smaller n values.
- The claim that pessimism transfers perfectly from PET-trained reward to PPO policy assumes no distribution shift, which isn't rigorously tested.

## Confidence

- **High confidence:** PET improves rejection sampling performance and achieves competitive results on IMDB (98.3% positive sentiment likelihood). The minimax formulation and its gradient properties are mathematically sound.
- **Medium confidence:** The claim that PET eliminates need for KL regularization is supported by Table 2 but could benefit from more extensive ablation across different KL strengths and tasks. The mechanism of pessimism transfer from reward to policy is plausible but not exhaustively validated.
- **Low confidence:** The theoretical coverage bound's practical relevance across diverse domains isn't demonstrated. The specific choice of n=64 and β=10 lacks principled justification beyond empirical tuning.

## Next Checks

1. **Coverage coefficient validation:** Compute and report C_μD for your target task before running PET. If coverage exceeds 10³, expand the preference dataset or consider alternative methods.
2. **Ablation on rejection sampling scale:** Run PET with n ∈ {16, 32, 64, 128} while monitoring prediction loss and downstream PPO performance. Plot the tradeoff curve to find the knee point.
3. **KL-regularized vs. unregularized comparison on same reward:** Train PPO with and without KL on both proxy and PET rewards. Expected: proxy+KL performs well, proxy+no KL shows reward hacking, PET+no KL maintains alignment. If PET+no KL still shows hacking, the pessimism transfer mechanism needs investigation.