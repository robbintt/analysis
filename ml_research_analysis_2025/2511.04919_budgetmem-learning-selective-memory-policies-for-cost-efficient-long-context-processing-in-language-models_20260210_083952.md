---
ver: rpa2
title: 'BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context
  Processing in Language Models'
arxiv_id: '2511.04919'
source_url: https://arxiv.org/abs/2511.04919
tags:
- arxiv
- memory
- compression
- documents
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BudgetMem addresses the high computational cost of processing long
  contexts in large language models by selectively retaining only high-salience content
  under explicit memory budgets. The method operates at the chunk level, using interpretable
  features (entity density, TF-IDF, position bias, discourse markers) to make keep-or-discard
  decisions, then indexing retained chunks with BM25 for retrieval.
---

# BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models

## Quick Facts
- **arXiv ID:** 2511.04919
- **Source URL:** https://arxiv.org/abs/2511.04919
- **Reference count:** 40
- **Primary result:** Achieves 72% memory savings with 1% F1 degradation on medium documents using interpretable chunk-level salience scoring

## Executive Summary
BudgetMem addresses the computational cost of processing long contexts in large language models by selectively retaining only high-salience content under explicit memory budgets. The method operates at the chunk level, using interpretable features (entity density, TF-IDF, position bias, discourse markers) to make keep-or-discard decisions, then indexing retained chunks with BM25 for retrieval. This differs from token-level compression and standard RAG systems that store all content. Experiments on 750 question-answer pairs across short (237 tokens), medium (5K-10K tokens), and very long (50K-100K tokens) documents show that BudgetMem achieves 72% memory savings with only 1% F1 degradation on medium documents. On ultra-long NarrativeQA texts, it reaches 87% of LLMLingua's F1 performance (0.0351 vs 0.0402) without requiring neural compression models, training, or GPU-based compression.

## Method Summary
BudgetMem implements a pre-query filtering approach that makes irreversible discard decisions at ingestion time. Documents are chunked into 150-token segments with 30-token overlap, then each chunk is scored using weighted features: entity density (0.2, spaCy NER), TF-IDF importance (0.2), position bias (0.15, U-shaped), numerical density (0.15), discourse markers (0.1), and question presence (0.1). The top 30% of chunks by score are retained and indexed with BM25. For retrieval, top-3 chunks are selected per query and passed to Llama-3.2-3B-Instruct (FP16, greedy, max 100 tokens) to generate answers. The entire system runs training-free on a $10/month Google Colab instance.

## Key Results
- Achieves 72% memory savings with only 1% F1 degradation on medium documents (5K-10K tokens)
- Reaches 87% of LLMLingua's F1 performance (0.0351 vs 0.0402) on ultra-long NarrativeQA texts
- Outperforms TF-IDF-only baseline by 3.5 F1 points (0.8042 vs 0.7689) on synthetic academic papers
- Runs entirely on a $10/month Google Colab instance without requiring GPU-based compression or training

## Why This Works (Mechanism)

### Mechanism 1: Chunk-Level Preservation of Semantic Coherence
Operating at chunk (paragraph) level rather than token level preserves enough semantic structure to maintain retrieval effectiveness, even when discarding 70% of content. Complete paragraphs retain subject-verb-object relationships and topical unity that scattered high-importance tokens lose. BM25 keyword matching works on intact passages but fails on "word soup" from token-level pruning. Core assumption: Important information clusters within paragraphs rather than distributing uniformly across text. Evidence: Token-level TF-IDF scored F1 < 0.001 (catastrophic), while chunk-level with identical scoring achieved F1 = 0.0351—same budget, same features, different granularity.

### Mechanism 2: Multi-Feature Salience Approximates Neural Importance
Hand-crafted features (entity density, TF-IDF, position bias, discourse markers) capture ~87% of neural compressor performance without any training or GPU inference. Named entities signal factual content; TF-IDF captures distinctive vocabulary; position bias exploits document structure (introductions/conclusions); discourse markers flag organizational content. Fixed weights (0.2, 0.2, 0.15, 0.15, 0.1, 0.1) were set once on dev set. Core assumption: Document importance correlates with these surface-level features consistently across domains. Evidence: BudgetMem outperforms TF-IDF-only baseline by 3.5 F1 points (0.8042 vs 0.7689), confirming multi-feature value.

### Mechanism 3: Budget-Constrained Pre-Query Filtering
Making irreversible discard decisions at ingestion time (before any query arrives) reduces storage by 72% while accepting controlled quality loss. Unlike standard RAG that stores all chunks, BudgetMem keeps only top ⌊r·M⌋ chunks by salience (r=30% default). The BM25 index is built only on retained set S, so discarded content is permanently inaccessible. Core assumption: Future queries will target high-salience content predictable from document structure. Evidence: Budget sensitivity shows 30-40% retention as sweet spot: 60-72% memory savings for 1-3% F1 loss.

## Foundational Learning

- **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: Core salience feature; must understand that high TF-IDF means distinctive vocabulary within document context
  - Quick check question: Would a word appearing in every chunk have high or low TF-IDF score for that document?

- **BM25 Sparse Retrieval**
  - Why needed here: Retrieval mechanism after filtering; must understand it matches query keywords to chunk keywords without neural embeddings
  - Quick check question: Why would BM25 work better on complete paragraphs than on token-pruned fragments?

- **Chunk Overlap in RAG Systems**
  - Why needed here: 30-token overlap between 150-token chunks is standard practice; prevents boundary information loss
  - Quick check question: If chunk boundaries split a named entity in half, how would overlap help retrieval?

## Architecture Onboarding

- **Component map:**
Document → [Chunker: 150 tok, 30 overlap] → Chunks → [Salience Scorer] → [Budget Filter: keep top 30%] → [BM25 Index] ← stored ← Query → [BM25 Retrieval: top-3 chunks] → [LLM: Llama-3.2-3B] → Answer

- **Critical path:** Salience scoring quality → chunk retention decisions → BM25 retrieval effectiveness → final answer quality. Errors compound; bad scoring cannot be recovered at retrieval time.

- **Design tradeoffs:**
  - Chunk size (150 tokens): Smaller = finer control but more boundary fragmentation; larger = more coherence but coarser filtering
  - Budget ratio (30%): Lower = more savings but higher risk; higher = diminishing returns above 50%
  - BM25 vs neural retrieval: BM25 maintains training-free property but may miss semantic matches

- **Failure signatures:**
  - Token-level performance (F1 ~0.001): Granularity error—switched to chunk-level but pipeline still token-based somewhere
  - Short document degradation (9.7% F1 drop): Expected behavior—uniform salience means aggressive pruning loses content
  - Multi-hop question failure: Architectural limitation—distributed information across pruned regions

- **First 3 experiments:**
  1. Validate chunking integrity: Run single document through chunker, verify overlap working, check token counts match expected (should be M chunks where M ≈ (doc_length - overlap) / (chunk_size - overlap))
  2. Feature calibration check: Score chunks manually on a sample document, verify entity density (spaCy NER) and TF-IDF (sklearn) produce reasonable distributions; flag if all chunks score identically
  3. Budget sweep baseline: On 10 documents, test budgets [10%, 30%, 50%, 90%] and plot F1 vs savings; should replicate paper's finding that 30% is near-optimal (if not, feature weights may need domain adjustment)

## Open Questions the Paper Calls Out

**Can a hybrid compression approach combining chunk-level selection with token-level refinement within retained chunks close the 13% F1 gap with LLMLingua while preserving interpretability?**
Applying token-level refinement within retained chunks could capture the best of both granularity levels—preserving coherent passages while trimming intra-chunk redundancy. Evidence would come from experiments applying LLMLingua or similar token-level compression only to chunks that pass BudgetMem's salience threshold, measuring whether F1 approaches LLMLingua's 0.0402 on NarrativeQA.

**Can learned write policies trained on supervised signals (which chunks were retrieved for correct answers) close the performance gap with neural compressors while remaining cheaper than full neural compression?**
Training a lightweight classifier to predict chunk utility from supervised signals—such as which chunks were actually retrieved for correct answers—could close the gap with LLMLingua. Evidence would come from training a simple classifier (e.g., logistic regression) on chunk-level labels derived from retrieval success, then comparing F1 against both the hand-tuned baseline and LLMLingua.

**Does BudgetMem generalize to tasks beyond extractive QA, such as summarization, multi-hop reasoning, and non-English text?**
Our evaluation covers extractive QA on English text; generalization to summarization, multi-hop reasoning, or other languages remains untested. Evidence would come from evaluation on GovReport (summarization), Qasper (scientific QA requiring multi-hop reasoning), and LongBench (multi-task benchmark), plus experiments on translated documents.

**How should per-document budgets be adaptively set based on content complexity to optimize the quality-efficiency tradeoff?**
Per-document budget selection based on content complexity could improve efficiency on heterogeneous corpora. Evidence would come from developing a content complexity metric (e.g., salience score variance, information density) and correlating optimal budget ratios with this metric across diverse documents.

## Limitations
- Irreversible filtering permanently discards 72% of document content, making it inaccessible for queries targeting low-salience information
- Fixed feature weights calibrated once on a small development set may not generalize across diverse document types and domains
- Evaluation limited to extractive QA on English text; generalization to summarization, multi-hop reasoning, and non-English text remains untested

## Confidence

**High Confidence Claims:**
- Chunk-level vs token-level granularity makes >1000x difference in F1 performance (0.0351 vs <0.001)
- 30% retention budget provides optimal tradeoff (72% savings for 1-3% F1 loss)
- BudgetMem achieves 87% of LLMLingua's F1 without requiring neural models or GPU inference

**Medium Confidence Claims:**
- Hand-crafted features capture sufficient salience for general document types
- Position bias with U-shaped formula meaningfully contributes to salience scoring
- BM25 retrieval on retained chunks provides adequate answer coverage

**Low Confidence Claims:**
- The synthetic academic papers corpus represents real-world long-document scenarios
- Fixed feature weights generalize across domains without tuning
- 72% memory savings at 1% F1 cost is acceptable for production deployment

## Next Checks

**Corpus Generalization Test:** Apply BudgetMem to three additional document types not in the original evaluation: legal contracts (structured but dense), news articles (inverted pyramid structure), and technical documentation (hierarchical organization). Measure whether the 30% budget still provides optimal tradeoff, or if feature weights need domain-specific adjustment. Track feature importance per domain to identify which salience signals are universal vs domain-specific.

**Query Coverage Analysis:** For each test document, enumerate all possible questions that could be asked, then simulate queries targeting: (a) high-salience content (should succeed), (b) medium-salience content (may fail), (c) low-salience content (expected to fail). Measure the fraction of potential information needs that become inaccessible due to pruning. This quantifies the true cost of irreversible filtering beyond average F1 metrics.

**Adaptive Budget Validation:** Implement a dynamic budget allocation where documents with high salience variance get aggressive pruning (30%) while uniform documents get conservative pruning (70%). Compare against fixed 30% across all documents. This tests whether the uniform-budget assumption holds or if document-specific calibration improves the recall-quality tradeoff.