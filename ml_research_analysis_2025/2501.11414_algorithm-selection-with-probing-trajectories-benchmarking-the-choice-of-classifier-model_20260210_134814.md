---
ver: rpa2
title: 'Algorithm Selection with Probing Trajectories: Benchmarking the Choice of
  Classifier Model'
arxiv_id: '2501.11414'
source_url: https://arxiv.org/abs/2501.11414
tags:
- algorithm
- data
- trajectories
- classifier
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks 17 time-series classifiers for algorithm
  selection in black-box optimisation, using probing-trajectories as input data. The
  study contrasts with prior work that found classifier choice less critical when
  using tabular data, revealing that for time-series trajectories, the choice of classifier
  significantly impacts performance.
---

# Algorithm Selection with Probing Trajectories: Benchmarking the Choice of Classifier Model

## Quick Facts
- arXiv ID: 2501.11414
- Source URL: https://arxiv.org/abs/2501.11414
- Reference count: 40
- Primary result: Feature-based and interval-based classifiers outperform deep learning models for algorithm selection using time-series trajectories, with tuned Summary classifiers achieving 6% gain over ELA features.

## Executive Summary
This paper benchmarks 17 time-series classifiers for algorithm selection in black-box optimization, using probing-trajectories as input data. The study reveals that for time-series trajectories, the choice of classifier significantly impacts performance, contrasting with prior work that found classifier choice less critical when using tabular data. Best-performing models are feature-based (e.g., Summary) and interval-based (e.g., Time Series Forest), which outperform kernel-based and deep learning models. In leave-one-instance-out validation, tuned Summary classifiers achieve a 6% gain over ELA features with a similar budget, while tuned Time Series Forest gives a 2% gain with 7x fewer function evaluations.

## Method Summary
The authors benchmark 17 time-series classifiers on algorithm selection tasks using probing trajectories from continuous black-box optimization problems. They use the BBOB benchmark suite with trajectories from CMA-ES, PSO, and DE algorithms. The study compares performance across different validation settings (leave-one-instance-out and leave-one-problem-out) and evaluates both default and tuned classifier configurations. Trajectory types (Best, Current, All) and generation lengths (2, 4, 7) are varied to assess their impact on classification accuracy.

## Key Results
- Feature-based (Summary) and interval-based (Time Series Forest) classifiers outperform kernel-based and deep learning models on time-series trajectory data
- Tuned Summary classifiers achieve 6% gain over ELA features with similar budget in LOIO validation
- Tuned Time Series Forest achieves 2% gain with 7x fewer function evaluations compared to Summary
- Only 4 models achieve â‰¥90% accuracy on 11 of 24 functions in LOPO validation
- CNN and kernel models often collapse to predicting only the majority class (Dummy classifier performance)

## Why This Works (Mechanism)

### Mechanism 1: Algorithm-Centric Trajectory Encoding
Probing trajectories provide superior signal for algorithm selection compared to static instance features by encoding dynamic search behavior. A short probing run generates a time-series of objective values that captures the rate of improvement and search dynamics specific to that algorithm-instance pair. The initial search trajectory contains sufficient discriminative information to predict performance over a full run.

### Mechanism 2: Inductive Bias of Interval and Feature-Based Models
Time-series classifiers relying on interval sampling or statistical summarization outperform deep learning and kernel methods on optimization trajectory data. Optimization trajectories are often noisy and short, making feature-based models (extracting robust statistics) and interval-based models (analyzing random subsequences) more robust to noise and data scarcity than deep learning models which require large datasets.

### Mechanism 3: Parameter Transferability via Trajectory Structure
Hyperparameter configurations optimized for one algorithm's trajectories can be successfully transferred to classifiers trained on trajectories from different algorithms. This implies that the structural characteristics of the time-series (scale, noise, trend) share similarities across different optimizers on the same problem class, allowing a universal tuned configuration to capture these patterns regardless of the specific source algorithm.

## Foundational Learning

- **Black-Box Optimization & ELA**: Understanding ELA extracts static features (e.g., curvature, multimodality) is critical to grasp why a dynamic "trajectory" approach represents a paradigm shift. Quick check: Can you explain why a static feature vector might fail to capture the "stickiness" of local optima that a trajectory might reveal?

- **Time-Series Classification (TSC) Taxonomy**: The paper evaluates 17 classifiers requiring distinction between feature-based (Summary), interval-based (TSF), distance-based (kNN), and DL-based (CNN/LSTM) to understand the performance hierarchy. Quick check: Why would a Time Series Forest be more robust to overfitting on a dataset of 600 samples than a Transformer model?

- **Cross-Validation Contexts (LOIO vs. LOPO)**: Results differ significantly between Leave-One-Instance-Out and Leave-One-Problem-Out validation. Understanding LOPO as "generalization to unseen problem types" is critical for interpreting the 61% accuracy drop. Quick check: If a model achieves high LOIO accuracy but fails LOPO, is it learning the problem structure or simply memorizing instance-specific noise?

## Architecture Onboarding

- **Component map**: BBOB Benchmark -> Probing Layer (Best/Current/All trajectories) -> Model Layer (17 classifiers) -> Evaluation Layer (LOIO/LOPO)
- **Critical path**: Generate "Best" trajectory from CMA-ES (2 generations) -> Tune Summary or TSF classifier using Irace (1000-5000 evaluations) -> Train on full training set -> Validate on hold-out instance/function
- **Design tradeoffs**: Budget vs. Accuracy (Tuned Summary gives 6% gain over ELA with similar budget vs. TSF gives 2% gain with 7x fewer evaluations), Model Complexity (DL models failed with defaults and are expensive to tune), Trajectory Type (Best optimal for short budgets, Current may need longer runs)
- **Failure signatures**: CNN and Kernel models collapsing to Dummy classifier (predicting only majority class), Hard Functions (F15, F20, F16 unlearnable in LOPO with max accuracy < 24%), Low corpus relevance for trajectory-based AS approach
- **First 3 experiments**:
  1. Baseline Validation: Reproduce LOIO result using default TSF on CMA-ES "Best" trajectory to verify 80-85% accuracy range
  2. Budget Sensitivity: Compare accuracy of Tuned Summary vs. Tuned TSF on 70-evaluation budget to validate "7x fewer evaluations" claim
  3. Hard Function Inspection: Isolate F15 and F20, visualize trajectories of three algorithms to determine if they are visually inseparable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the superior performance and transferability of feature-based and interval-based classifiers generalize to combinatorial optimization domains and other continuous benchmarks?
- Basis in paper: The authors state "Obvious next steps include extending the benchmarking approach to other datasets both in continuous and combinatorial domains."
- Why unresolved: Current study restricted to BBOB suite which may possess specific landscape characteristics that favor winning models
- What evidence would resolve it: Replication of benchmarking methodology on diverse combinatorial problems (SAT, TSP) and alternative continuous benchmarks

### Open Question 2
- Question: How do time-series regressors compare to classifiers for algorithm selection when predicting numerical performance metrics rather than discrete labels?
- Basis in paper: Conclusion notes "benchmarking regressors for time-series inputs should also be undertaken in future" as study focused solely on classification
- Why unresolved: Algorithm selection often framed as regression task (predicting solution quality), but unknown if classifier findings transfer to regression models
- What evidence would resolve it: Comparative study evaluating ranking performance of time-series regression models against classifiers benchmarked in this paper

### Open Question 3
- Question: Can deep learning models outperform feature-based classifiers if their architectures are extensively fine-tuned rather than used with default settings?
- Basis in paper: Discussion notes deep learning models performed poorly but "often require a lot of fine tuning... we only used off-the-shelf architectures"
- Why unresolved: Unclear if poor performance is inherent to short trajectory data or simply result of sub-optimal hyperparameters
- What evidence would resolve it: Benchmark where deep learning architectures are optimized specifically for these trajectories, achieving accuracy competitive with Summary or TSF

## Limitations
- Corpus relevance is weak with average FMR 0.30, suggesting limited external validation for trajectory-based AS approach
- Hard problems (F15, F20, F16) show persistent low accuracy (<24% in LOPO) across all models, questioning fundamental limits of trajectory-based AS
- Hyperparameter transfer assumption across algorithms lacks rigorous ablation study validation

## Confidence
- **High confidence**: Classifier performance hierarchy (feature/interval-based > deep learning/kernel) and general superiority of tuned models over defaults
- **Medium confidence**: Mechanism of trajectory encoding capturing dynamic search behavior is plausible but not rigorously proven
- **Low confidence**: Transferability of hyperparameters across different algorithms (CMA-ES to DE/PSO) is asserted but not deeply validated

## Next Checks
1. Ablation on trajectory length: Systematically vary probing budget (2, 4, 7 generations) for each trajectory type to quantify interaction between budget and trajectory type
2. Cross-algorithm hyperparameter transfer: Conduct formal ablation study where classifiers are tuned on each algorithm's trajectories and tested on others to quantify cost of transfer vs. algorithm-specific tuning
3. Hard function diagnosis: Isolate F15, F20, and F16, visualize and statistically compare three algorithms' trajectories to determine if signal is truly ambiguous or if different trajectory encoding could resolve issue