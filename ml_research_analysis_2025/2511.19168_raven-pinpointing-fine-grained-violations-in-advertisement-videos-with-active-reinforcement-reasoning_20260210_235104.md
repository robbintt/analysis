---
ver: rpa2
title: 'RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with
  Active Reinforcement Reasoning'
arxiv_id: '2511.19168'
source_url: https://arxiv.org/abs/2511.19168
tags:
- learning
- violation
- reasoning
- active
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of fine-grained violation detection
  in video advertisements, which is critical for content moderation in the digital
  economy. The authors propose RAVEN++, a novel framework that improves upon the existing
  RAVEN model by introducing three key innovations: active reinforcement learning
  for dynamic adaptation to sample difficulty, fine-grained violation understanding
  through hierarchical reward functions and reasoning distillation, and progressive
  multi-stage training combining knowledge injection, curriculum-based passive RL,
  and active RL.'
---

# RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning

## Quick Facts
- arXiv ID: 2511.19168
- Source URL: https://arxiv.org/abs/2511.19168
- Authors: Deyi Ji; Yuekui Yang; Liqun Liu; Peng Shu; Haiyang Wu; Shaogang Tang; Xudong Chen; Shaoping Ma; Tianrun Chen; Lanyun Zhu
- Reference count: 18
- Primary result: RAVEN++ significantly outperforms RAVEN and general-purpose LLMs in fine-grained violation understanding, reasoning, and generalization for video advertisement content moderation.

## Executive Summary
RAVEN++ addresses fine-grained violation detection in video advertisements through a three-stage progressive training framework. The system introduces active reinforcement learning for dynamic sample routing between SFT and RL buffers, hierarchical reward decomposition with Tversky distance for imbalanced categories, and curriculum-based reward scheduling. Extensive experiments on public and proprietary datasets, including online A/B testing, demonstrate significant improvements over existing approaches, achieving up to 9.9% better temporal interval localization accuracy with statistical significance (p < 0.02).

## Method Summary
RAVEN++ employs a three-stage progressive training approach: (1) Knowledge Injection via SFT using Qwen-72B-generated QA pairs for video summaries and violation rules, (2) Curriculum-based Passive RL with GRPO using six reward components across three phases, and (3) Active RL with dynamic SFT/RL buffer routing based on sample difficulty. The system uses a hierarchical reward structure including format, violation, major category, sub-category (with Tversky distance), temporal grounding, and reasoning consistency rewards, with progressive weight scheduling to prevent early optimization collapse.

## Key Results
- Achieves up to 9.9% improvement in temporal interval localization accuracy compared to RAVEN
- Demonstrates statistically significant performance gains (p < 0.02) across multiple metrics
- Outperforms general-purpose LLMs and specialized models in fine-grained violation understanding and reasoning capabilities
- Shows strong generalization ability through OOD testing on held-out violation categories

## Why This Works (Mechanism)

### Mechanism 1: Active Buffer-Based Sample Routing
During active RL, samples where the model fails at violation detection or major category classification are routed to SFT buffer for knowledge supplementation. Samples where major category is correct but sub-category, temporal grounding, or reasoning deviate are routed to RL buffer with higher weights. This dynamic routing improves data efficiency by 6% grounding improvement over passive RL alone, based on the principle that SFT is more effective for tasks beyond current model scope while RL excels at refining existing capabilities.

### Mechanism 2: Hierarchical Reward Decomposition with Tversky Distance
The six-component reward system includes Tversky distance for major/sub-category rewards, which generalizes Dice coefficient by allowing flexible weighting of false positives vs. false negatives. This asymmetric formulation improves learning on rare sub-categories, demonstrated by improvements in major category precision/recall from 0.860/0.812 to 0.869/0.820. Reasoning reward uses BERT semantic similarity against Qwen-72B chain-of-thought outputs.

### Mechanism 3: Curriculum-Based Reward Weight Scheduling
Progressive shifting of reward weights across three phases prevents early optimization collapse: Phase 1 [1,1,0.5,0.3,0,0.1] → Phase 2 [0.5,0.5,1,1,0,0.5] → Phase 3 [0.2,0.2,1,1,1,0.5]. This curriculum approach ensures stable temporal grounding learning after prerequisite mastery of simpler tasks, with consistent gains observed across training stages.

## Foundational Learning

- **Reinforcement Learning with Reward Shaping**
  - Why needed here: RAVEN++ uses GRPO-based RL with six-component rewards; understanding how rewards shape policy is essential for debugging.
  - Quick check question: Can you explain why Tversky distance would help with imbalanced sub-categories compared to standard IoU?

- **Curriculum Learning Principles**
  - Why needed here: The three-phase training relies on easy-to-hard task progression; misapplication causes optimization failure.
  - Quick check question: What would happen if temporal grounding (Phase 3) was trained from the start with full weight?

- **Knowledge Distillation from Larger Models**
  - Why needed here: Reasoning reward uses Qwen-72B outputs as supervision signal via BERT similarity.
  - Quick check question: How does reasoning distillation differ from standard answer distillation?

## Architecture Onboarding

- Component map: Knowledge Injection (SFT on augmented QA pairs from Qwen-72B) → Passive RL (curriculum-weighted rewards on large-scale noisy data) → Active RL (dynamic SFT/RL buffer routing on small-scale precise data) → Deployment (online hard sample feedback loop)

- Critical path: Stage 1 knowledge injection is prerequisite for stable RL; skipping it causes reasoning inconsistency issues noted in original RAVEN.

- Design tradeoffs:
  - Small-scale precise data vs. large-scale noisy: Active RL uses minimal precise data efficiently but requires accurate annotation.
  - SFT vs. RL ratio: SFT for gaps, RL for refinement; misclassification of sample type wastes capacity.
  - Reward weight tuning: Manual phase schedules work but may not generalize across violation types.

- Failure signatures:
  - Format errors in output → Stage 1 incomplete or Phase 1 weights too low.
  - Reasoning contradiction with conclusion → Reasoning reward weight insufficient or Qwen-72B distillation data missing.
  - Poor OOD performance → Knowledge injection inadequate; overfitting to training distribution.

- First 3 experiments:
  1. Ablate single reward components (set each λ to 0 one at a time) to verify claimed contributions from Table 7.
  2. Test phase timing sensitivity: Vary epoch counts per phase to find optimal transition points for your dataset.
  3. OOD validation split: Train on 3 categories, test on 2 held-out categories (following Table 5 protocol) before deployment consideration.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical RL hyperparameters (learning rates, batch sizes, KL penalty coefficients, clip ranges) are not specified, preventing exact reproduction
- Tversky distance parameters and boundary alignment reward formula details are unspecified
- Phase transition criteria for curriculum learning are not defined
- Limited direct corpus validation of Tversky distance in RL reward functions specifically

## Confidence

- **High confidence**: Claims about overall performance improvements (9.9% temporal interval accuracy, p < 0.02 significance) supported by extensive experiments
- **Medium confidence**: Claims about individual mechanism contributions (active RL adding +6% grounding, Tversky distance improvements) supported by ablation studies but lack direct comparison to alternatives
- **Low confidence**: Claims about curriculum learning effectiveness based on internal timing rather than systematic sensitivity analysis

## Next Checks

1. Ablate individual reward components by setting each λ to 0 one at a time to verify claimed contributions from Table 7 and test sensitivity to reward weight configurations.
2. Conduct systematic phase timing sensitivity analysis by varying epoch counts per curriculum phase to identify optimal transition points and test robustness to timing variations.
3. Implement OOD validation protocol by training on 3 violation categories and testing on 2 held-out categories to assess true generalization capability before deployment consideration.