---
ver: rpa2
title: 'Reward Model Perspectives: Whose Opinions Do Reward Models Reward?'
arxiv_id: '2510.06391'
source_url: https://arxiv.org/abs/2510.06391
tags:
- alignment
- demographic
- page
- reward
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates social biases in reward models (RMs),
  which are used to align language models to human preferences. The authors introduce
  a framework to measure the alignment of RM opinions across demographic groups, and
  evaluate RMs on three datasets: OPINIONQA (opinion alignment), BBQ and STEREOSET
  (stereotyping).'
---

# Reward Model Perspectives: Whose Opinions Do Reward Models Reward?

## Quick Facts
- arXiv ID: 2510.06391
- Source URL: https://arxiv.org/abs/2510.06391
- Reference count: 40
- One-line primary result: Reward models exhibit consistent sociodemographic biases in relative alignment, favoring certain demographic groups over others regardless of model architecture.

## Executive Summary
This paper investigates social biases in reward models (RMs), which are used to align language models to human preferences. The authors introduce a framework to measure the alignment of RM opinions across demographic groups, and evaluate RMs on three datasets: OPINIONQA (opinion alignment), BBQ and STEREOSET (stereotyping). They find that absolute alignment varies across models, but relative alignment—the ranking of demographic groups—is consistent, with certain groups (e.g., Southerners, those with less education) consistently ranked higher. RMs also exhibit different patterns of stereotyping, with some favoring harmful stereotypes more than others. Attempts to steer RMs toward target demographics through prompting had little effect. These findings highlight the need to consider RM biases during alignment training to avoid propagating social biases in deployed models.

## Method Summary
The study evaluates seven open-source reward models on four datasets (BBQ, OPINIONQA, PRISM, STEREOSET) using a framework that measures opinion alignment with demographic groups. RMs are treated as multiple-choice classifiers that score each option and select the highest-scoring one. The alignment metric compares the model's opinion distribution (via softmax over rewards) to human distributions using Jensen-Shannon or Wasserstein distances. The authors also test in-context steering through demographic prompts and analyze stereotyping patterns. Statistical tests include Friedman tests for rank differences and Spearman's rank correlation for consistency.

## Key Results
- Absolute alignment scores vary across models, but relative rankings of demographic groups remain consistent across different RM architectures
- RMs systematically favor certain demographic groups (e.g., Southern US residents, those with less education) over others
- Some RMs prefer harmful stereotypes while others favor refusals or unrelated responses
- In-context steering through demographic prompts has minimal impact on mitigating these biases

## Why This Works (Mechanism)

### Mechanism 1: Relative Preference Inheritance
If a reward model (RM) is initialized from a base LM or trained on narrow preference data, it appears to encode consistent relative rankings that favor specific sociodemographic groups (e.g., higher alignment with lower education levels) regardless of the specific RM architecture. The RM learns a fixed "opinion distribution" ($D_M$) that reflects the statistical priors of its training data. Because preference learning relies on relative rankings rather than absolute scores, these implicit demographic biases persist across different model checkpoints and sizes. The preference data or base model weights contain spurious correlations between text features and specific demographic viewpoints. [abstract], [section 6.1], [corpus]: *Reward Models Inherit Value Biases from Pretraining*

### Mechanism 2: Representation Rigidity in Steering
In-context learning (steering) via demographic prompts likely fails to alter RM preferences because the reward signal is derived from a single forward pass over the (prompt, response) pair, treating the persona description as noise rather than a conditional constraint. Unlike autoregressive generation where attention can focus on persona tokens, the RM produces a scalar $r(x, y)$ where the "opinion" is deeply baked into the model's hidden states. The steering prompt fails to shift the decision boundary significantly because the RM's value function is over-fitted to its training distribution. The RM's attention mechanism does not grant sufficient weight to the persona prefix to override the pre-trained preference weights. [abstract], [section 6.3], [corpus]: Weak/missing direct evidence in neighbors regarding the failure mode of steering specifically in RMs vs LLMs

### Mechanism 3: Surface Form Generalization
RMs systematically reward stereotypes or refusals not because they "believe" the content, but because stereotypical text often exhibits higher perplexity/fluency patterns or correlates with the "helpful" signal found in standard preference datasets (e.g., OpenAssistant). RMs optimize for the likelihood of the "chosen" response in the training set. If stereotypes or specific refusal styles were over-represented or marked as "better" in the preference data (e.g., for safety or assertiveness), the RM learns to reward the surface form of these outputs. The Bradley-Terry optimization objective captures preference magnitude via surface-level features rather than deep semantic reasoning. [abstract], [section 6.2], [corpus]: *Reward Models Identify Consistency, Not Causality*

## Foundational Learning

- **Concept: Bradley-Terry Model**
  - **Why needed here:** This is the mathematical foundation of the RM. The paper assumes $P(y_1 \succ y_2) \propto \exp(r(x, y_1))$. Understanding this helps explain why RMs output scalars and how rankings are derived.
  - **Quick check question:** If two responses have rewards of 1.0 and 2.0, what is the probability that the RM prefers the second response?

- **Concept: Alignment Metrics (Wasserstein vs. Jensen-Shannon)**
  - **Why needed here:** The paper distinguishes between ordinal (Wasserstein) and non-ordinal (Jensen-Shannon) opinions to measure the "distance" between model and human distributions.
  - **Quick check question:** Why would you use Wasserstein distance for a Likert scale (1-5) but Jensen-Shannon for a multiple-choice question about favorite colors?

- **Concept: Absolute vs. Relative Alignment**
  - **Why needed here:** The paper's core insight is that while absolute scores vary, *relative* unfairness is consistent. Understanding this distinction is key to interpreting the results.
  - **Quick check question:** If Model A aligns with Group X at 0.8 and Group Y at 0.6, and Model B aligns with X at 0.5 and Y at 0.3, which model is "fairer" regarding relative bias?

## Architecture Onboarding

- **Component map:** Prompt ($x$) + Response ($y$) -> Pre-trained Transformer Backbone -> Linear Head -> Scalar Reward $r(x,y)$ -> Softmax Normalization -> Opinion Distribution ($D_M$) -> Distance Function -> Alignment Metric
- **Critical path:** The calculation of the Alignment Metric $A(D_1, D_2; Q)$. You must correctly normalize rewards per question (softmax) before computing the distance to the ground-truth demographic distributions.
- **Design tradeoffs:**
  - **Evaluation:** Using BBQ/StereoSet is efficient but proxies complex social dynamics with synthetic or simplified examples.
  - **Steering:** Prepending prompts is computationally cheap but, as shown, ineffective. Training Intervention (e.g., DPO on balanced data) is expensive but potentially more robust.
- **Failure signatures:**
  - **Reward Hacking:** The RM gives high rewards to "Unrelated" text (absurdities) on StereoSet (Section 6.2, Figure 6), indicating a failure to ground the reward in semantic relevance.
  - **Refusal Bias:** Some RMs (like Pythia) predict "Unknown" (refusals) excessively, failing to distinguish between genuine ambiguity and bias.
- **First 3 experiments:**
  1. **Sanity Check:** Re-run the BBQ evaluation on a target RM (e.g., OpenAssistant/reward-model-deberta-v3-base). Check the confusion matrix for "Stereotyped" vs. "Unstereotyped" labels.
  2. **Steering Null Test:** Take the "BIO" steering prompt for "Female" and "Male" on a sample of OpinionQA. Calculate the Wasserstein distance between the steered distributions. Verify if the difference is statistically significant (Wilcoxon test as per Section F.3).
  3. **Correlation Analysis:** Compute Spearman's rank correlation of alignment scores across 3 different RMs for the "Income" demographic in OpinionQA. Verify if the relative ranking of income brackets remains constant (as claimed in Section 6.1).

## Open Questions the Paper Calls Out

- **Question:** Do the sociodemographic biases and relative alignment rankings identified in reward models (RMs) causally propagate to the final behaviors of language models fine-tuned via RLHF?
- **Basis in paper:** [inferred] The authors note in the Limitations section: "For future work, we would like to train RMs and LMs to measure the downstream performance on our datasets to gain a deeper understanding of the social biases of RMs in language modeling."
- **Why unresolved:** This study evaluates RMs as static classifiers of preference data. It does not execute the full RLHF pipeline to observe if the specific biases measured in the RM (e.g., favoring Southern US perspectives) persist in the generated text of the aligned language model.
- **What evidence would resolve it:** An experiment training language models using the studied RMs and evaluating the resulting models using the same opinion distribution and stereotype metrics applied to the RMs in this paper.

## Limitations
- Limited real-world demographic coverage: The study relies on synthetic or simplified datasets that may not fully capture the complexity of real-world demographic perspectives
- Unknown transfer to generation tasks: While the paper establishes that RMs exhibit demographic biases in scoring, it remains unclear how these biases manifest when RMs are used to fine-tune or guide LLM generation
- Steering effectiveness threshold ambiguity: The paper concludes steering prompts have minimal effect, but the absolute magnitude of steering (0.064-0.148 alignment improvement) is not contextualized

## Confidence
- **High Confidence**: The finding that relative alignment rankings are consistent across RMs (Section 6.1). This is supported by statistical tests (Friedman, Spearman's correlation) and multiple datasets.
- **Medium Confidence**: The claim that RMs inherit biases from pretraining (Section 6.2). While the evidence from stereotype patterns is strong, the mechanism linking pretraining to preference learning is not fully established.
- **Low Confidence**: The assertion that in-context steering is ineffective (Section 6.3). The paper only tests a limited set of prompt templates, and the null result could stem from suboptimal prompt engineering rather than fundamental architectural limitations.

## Next Checks
1. **Transfer validation**: Evaluate the same RMs on a generation task (e.g., summarization or dialogue) where demographic perspectives matter. Compare the demographic representation in generated text against human references to verify if scoring biases transfer to output generation.
2. **Prompt engineering ablation**: Systematically vary steering prompt templates (persona placement, length, detail level) and measure alignment changes. This would clarify whether the null steering result is due to prompt limitations or fundamental architectural constraints.
3. **Demographic correlation mapping**: For each RM, compute the correlation between demographic group alignment scores across all three datasets (OpinionQA, BBQ, STEREOSET). This would validate whether the same demographic groups are consistently ranked across different bias types and evaluation paradigms.