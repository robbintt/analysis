---
ver: rpa2
title: Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained
  Large Language Models as Tutors With Advice Reusing
arxiv_id: '2509.08329'
source_url: https://arxiv.org/abs/2509.08329
tags:
- learning
- arxiv
- reinforcement
- https
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of pre-trained large language models
  (LLMs) as tutors to accelerate reinforcement learning (RL) convergence, employing
  a student-teacher architecture with advice reusing. Across 54 configurations involving
  three RL algorithms (DQN, PPO, A2C), three environments (Blackjack, Snake, Connect
  Four), and three LLMs (Llama, Vicuna, DeepSeek), results show that LLM tutoring
  significantly accelerates RL convergence while maintaining comparable optimal performance.
---

# Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing

## Quick Facts
- **arXiv ID**: 2509.08329
- **Source URL**: https://arxiv.org/abs/2509.08329
- **Authors**: Lukas Toral; Teddy Lazebnik
- **Reference count**: 40
- **Primary result**: LLM tutoring significantly accelerates RL convergence while maintaining comparable optimal performance

## Executive Summary
This study introduces a novel approach to accelerate reinforcement learning convergence by employing pre-trained large language models as external tutors. The proposed student-teacher architecture integrates LLMs (Llama, Vicuna, DeepSeek) with standard RL algorithms (DQN, PPO, A2C) across three environments (Blackjack, Snake, Connect Four). The method uses zero-shot prompting to generate action advice and implements an advice reuse mechanism through state hashing. Results demonstrate that LLM tutoring substantially reduces training time while maintaining comparable final performance, though advice reuse introduces convergence instability. DeepSeek generally outperformed other LLMs despite longer inference times.

## Method Summary
The approach employs a student-teacher architecture where LLMs provide action guidance to RL agents through zero-shot prompting. The LLM consultation probability decays linearly from 1.0 to 0.1 over training, allowing the agent to transition from imitation to reinforcement learning. Advice reuse is implemented via state hashing, storing LLM suggestions in a dictionary with a budget limiting reuse frequency. The system uses three RL algorithms (DQN, PPO, A2C) and three environments, with LLMs deployed locally through Ollama. Training involves 54 configurations across different algorithm-environment-LLM combinations, with performance measured through cumulative reward and a Relative RL Convergence Metric.

## Key Results
- LLM tutoring significantly accelerates RL convergence across all 54 configurations while maintaining comparable optimal performance
- DeepSeek generally outperformed other LLMs despite having the highest inference latency (29.94s average response time)
- Advice reuse improved training efficiency but introduced convergence instability and less stable reward dynamics
- Weak correlation (r=0.24, p>0.05) between LLM size and tutoring effectiveness
- Substantial time savings in simpler environments despite longer inference times

## Why This Works (Mechanism)

### Mechanism 1: Externalized Policy Initialization via Zero-Shot Reasoning
Pre-trained LLMs function as an external "cortex," providing stronger initial policies than random exploration. Instead of initializing with random weights, the system queries an LLM that uses its pre-trained world knowledge to map textual state descriptions to probable actions. This bridges the gap in sparse reward environments by bypassing inefficient random exploration phases.

### Mechanism 2: Stale Advice Caching (Reusing)
A dictionary stores state hashes mapped to LLM-suggested actions with a budget limiting reuse. When the agent re-enters a hashed state and the budget > 0, the action is reused without querying the LLM. This reduces inference latency but risks reinforcing suboptimal actions in environments where state-action relationships change over time.

### Mechanism 3: Probabilistic Knowledge Weaning
A decaying probability schedule transitions the agent from imitation (following tutor) to reinforcement (learning from environment feedback). The probability decreases linearly from 1.0 to 0.1, forcing the agent to rely on its own learned policy as training progresses. This prevents catastrophic forgetting while allowing the agent to accumulate successful trajectories during high-guidance phases.

## Foundational Learning

**Concept: Exploration vs. Exploitation**
Why needed here: The core value proposition replaces inefficient random exploration with informed exploration. Understanding the exploration bottleneck is critical to evaluate LLM latency trade-offs.
Quick check question: In a sparse reward environment like Montezuma's Revenge, why would random exploration fail where an LLM might succeed?

**Concept: Markov Decision Processes (MDP) & State Hashing**
Why needed here: The advice reuse mechanism relies on identifying unique states to cache advice. Understanding hashable states and potential collisions is critical for debugging cache logic.
Quick check question: If two game states are visually similar but numerically distinct (one pixel difference), will the hash-based advice cache function efficiently? What is the risk?

**Concept: Zero-Shot Prompting**
Why needed here: The LLM acts as a tutor without prior examples of the game being played. Understanding how to craft prompts that transform raw observations into solvable problems is essential.
Quick check question: How would you convert the raw numerical output of a gymnasium environment (e.g., a 10x10 grid) into a prompt that preserves spatial relationships for an LLM?

## Architecture Onboarding

**Component map**: RL Agent (Student) -> LLM Tutor (Teacher) -> Advice Cache -> Action Selector -> Environment

**Critical path**: 
1. Get raw state from gymnasium
2. Serialize state into prompt template
3. Compute hash(state) and check dictionary O
4. Query LLM if cache miss, parse <action> tag, update O
5. Execute action and step environment
6. Standard RL backprop + decay P

**Design tradeoffs**: 
- Latency vs. Sample Efficiency: LLM inference adds seconds per step; caching mitigates this but reduces advice diversity
- Stability vs. Speed: Advice reuse accelerates wall-clock time but introduces convergence instability

**Failure signatures**:
- Parsing errors: LLM generates invalid format; log events to ensure prompt effectiveness
- Budget stagnation: High budget with mediocre advice causes bad action loops
- State hash collision: Distinct states mapping to same key apply wrong advice

**First 3 experiments**:
1. Baseline Verification: Run RL algorithm without LLM support to establish random exploration baseline
2. Tutor Integration (No Cache): Integrate LLM with P=1.0â†’0.1 decay but disable caching; compare wall-clock time to baseline
3. Cache Ablation: Enable advice reusing with budget of 3; monitor convergence instability and verify wall-clock time decrease

## Open Questions the Paper Calls Out

**Open Question 1**: How do adaptive advice reuse strategies (e.g., Q-change or decaying probability) compare to the budget-based method in mitigating convergence instability?
- Basis: Authors explicitly propose exploring advanced reuse strategies like Q-change or decay to address instability
- Why unresolved: Study only implemented fixed ReBudget approach
- Evidence needed: Comparative results showing convergence curves for adaptive vs. fixed budget methods

**Open Question 2**: Does tutoring effectiveness scale to high-dimensional environments when using state-of-the-art cloud-hosted LLMs?
- Basis: Discussion suggests extending to complex, higher-dimensional environments using powerful cloud-hosted LLMs
- Why unresolved: Limited to simple environments and smaller locally hosted models due to resource constraints
- Evidence needed: Performance benchmarks in high-dimensional environments using models like GPT-4

**Open Question 3**: To what extent does domain-specific fine-tuning improve reliability and validity of LLM advice compared to zero-shot prompting?
- Basis: Authors acknowledge zero-shot setting likely limited effectiveness and caused occasional invalid suggestions
- Why unresolved: Study did not isolate variable of model fine-tuning
- Evidence needed: Comparative analysis of advice validity rates between zero-shot and domain-fine-tuned LLM tutors

## Limitations
- Weak correlation (r=0.24, p>0.05) between LLM size and effectiveness suggests larger models don't guarantee better tutoring
- Convergence instability with advice reuse lacks detailed analysis of failure modes or recovery strategies
- Focus on simple environments (Blackjack, Snake, Connect Four) without testing on complex or continuous state-space domains

## Confidence

**High Confidence**: Core finding that LLM tutoring accelerates RL convergence is well-supported by experimental results across multiple algorithms and environments.

**Medium Confidence**: Effectiveness ranking of different LLMs is reasonably supported but could benefit from additional statistical validation given small sample size.

**Low Confidence**: Generalizability to more complex environments remains unproven; weak correlation between model size and performance suggests sensitivity to uncontrolled factors.

## Next Checks
1. Conduct t-tests or ANOVA across 54 configurations to verify performance differences are statistically significant
2. Test methodology on challenging environments like Atari games or continuous control tasks to assess scalability
3. Isolate contribution of each component (LLM tutoring, advice reuse, probabilistic weaning) through controlled ablation studies