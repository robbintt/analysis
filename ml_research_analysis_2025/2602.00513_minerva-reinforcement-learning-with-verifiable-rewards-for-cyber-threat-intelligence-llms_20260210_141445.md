---
ver: rpa2
title: 'Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence
  LLMs'
arxiv_id: '2602.00513'
source_url: https://arxiv.org/abs/2602.00513
tags:
- technique
- prompt
- training
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Minerva, a reinforcement learning framework
  with verifiable rewards for cyber threat intelligence (CTI) tasks. Minerva leverages
  CTI's structured standards and canonical identifiers to enable deterministic verification
  of model outputs, allowing scalable training without human preference models.
---

# Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs

## Quick Facts
- arXiv ID: 2602.00513
- Source URL: https://arxiv.org/abs/2602.00513
- Reference count: 40
- Key outcome: Minerva improves accuracy and robustness over supervised fine-tuning baselines on 12 CTI benchmarks using verifiable-reward RL

## Executive Summary
Minerva introduces a reinforcement learning framework that leverages the structured nature of cyber threat intelligence (CTI) to enable deterministic verification of model outputs. By replacing learned preference models with programmatic verifiers, Minerva scales RL training for CTI tasks without human annotation. The framework's core innovation, MinervaRL, addresses reward sparsity through answer-conditioned reasoning traces and periodic distillation, achieving consistent improvements across multiple LLM backbones on 12 CTI benchmarks.

## Method Summary
Minerva trains LLMs on CTI tasks using GRPO with deterministic verifiers that score outputs via exact match, structured partial credit, or set-based overlap against canonical identifiers. MinervaRL adds answer-conditioned reasoning (ACR) for hard prompts and periodic distillation of verified traces back to answer-free prompts. The system uses N=8 rollouts per prompt, K=4 ACR traces at intervals of I=10 steps, and distillation with learning rate scale γ=0.05. Four LLM backbones are evaluated: Llama-3.1-8B, Llama-3.2-3B, Qwen3-8B, and Qwen3-4B.

## Key Results
- Consistent accuracy improvements over supervised fine-tuning baselines on 12 CTI benchmarks
- Reduced fraction of prompts with no verified rollouts (from ~0.45 to ~0.25)
- Higher policy entropy during training leading to better response quality
- Effective across four different LLM backbones (Llama-3.1-8B, Llama-3.2-3B, Qwen3-8B, Qwen3-4B)

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Verification Replaces Learned Preference Models
- Verifiable rewards enable scalable RL for structured CTI outputs without human annotation
- Each CTI task is paired with a programmatic verifier scoring completions via exact match, structured partial credit, or set-based overlap against canonical identifiers
- Assumes CTI's structured standards provide sufficiently dense, unambiguous targets for reward shaping

### Mechanism 2: Answer-Conditioned Reasoning (ACR) Mitigates Reward Sparsity
- Providing ground-truth labels during training increases probability of generating verified trajectories under limited rollout budgets
- ACR constructs prompts revealing the gold label and requests short justifications to make verified traces more likely
- Assumes conditioning on correct answers elicits reasoning traces that generalize back to answer-free prompts

### Mechanism 3: Periodic Distillation Expands Empirical Support
- Distilling verified ACR traces back to answer-free prompts raises policy's success probability above detectability thresholds
- Every 10 steps, accepted traces are collected and used for SFT updates on original prompts
- Assumes a single SFT step produces nontrivial increase in answer-level probability

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables RLVR without training a critic by estimating advantages from groups of samples per prompt
  - Quick check question: Can you explain how GRPO computes relative advantages from N=8 rollouts per prompt without a value function?

- **Reward Sparsity in On-Policy RL**
  - Why needed here: Understanding why zero-reward batches stall learning is critical to appreciating ACR's contribution
  - Quick check question: If p(correct answer) = 0.01 and k=8 rollouts, what is the probability of observing zero successes?

- **Policy Entropy and Exploration**
  - Why needed here: MinervaRL maintains higher entropy than GRPO alone, preventing premature collapse to narrow response patterns
  - Quick check question: Why does higher entropy matter when optimizing sparse, verifiable rewards?

## Architecture Onboarding

- **Component map**: Actor policy (πθ) -> EMA teacher (πφ) -> Task verifiers -> ACR buffer (P) -> Two-stage filter -> Distillation buffer (Q) -> Actor policy updates
- **Critical path**: Sample batch → N=8 GRPO rollouts → verify → update actor → buffer hard prompts → generate K=4 ACR traces → filter → enqueue → SFT update
- **Design tradeoffs**: Rollout budget N=8 vs. compute; distillation interval I=10 vs. overhead; LR scale γ=0.05 optimal, 0.10 destabilizes; threshold τq=0.5 for filtering
- **Failure signatures**: Zero-solve plateau (>0.4 fraction) → increase K or lower τq; entropy collapse → reduce γ; leakage in traces → tighten heuristic filters
- **First 3 experiments**: 1) Baseline GRPO vs. MinervaRL on Minerva-CTI; 2) Distillation LR sweep (γ ∈ {0.01, 0.02, 0.05, 0.10}); 3) ACR ablation (GRPO with N=12 rollouts vs. full MinervaRL)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unswept hyperparameters (distillation interval I, per-interval batch cap M, EMA teacher decay α) impact MinervaRL's stability and final performance?
- Basis: Authors note they only studied distillation learning-rate scale γ due to compute constraints, leaving other key hyperparameters unexplored
- What evidence would resolve it: Systematic ablation study varying I, M, and α across different LLM backbones

### Open Question 2
- Question: Can MinervaRL effectively transfer to multilingual CTI tasks and region-specific reporting styles?
- Basis: Current dataset and training pipeline are English-centric, creating uncertainty about generalization to other languages
- What evidence would resolve it: Application to non-English threat intelligence corpora with corresponding multilingual benchmarks

### Open Question 3
- Question: Do stronger LLM-judge filters provide significant quality advantage over the lightweight TextCNN classifier?
- Basis: Current implementation prioritized efficiency (TextCNN) over potential higher accuracy of LLM-based filtering
- What evidence would resolve it: Comparative evaluation of accepted trace quality and downstream performance using LLM judge vs. TextCNN

## Limitations

- Deterministic verification only works for structured CTI outputs with canonical identifiers; subjective assessments cannot be verified
- Requires multiple LLM rollouts per prompt (N=8), making it computationally expensive
- Training assumes CTI tasks have sufficient canonical targets for reward shaping

## Confidence

- **High Confidence**: Deterministic verification using CTI standards is feasible and improves training scalability; MinervaRL reduces zero-solve fraction; γ=0.05 is optimal
- **Medium Confidence**: ACR traces effectively generalize back to answer-free prompts; EMA teacher with decay 0.995 is optimal; K=4 traces provides sufficient learning signal
- **Low Confidence**: Exact GRPO implementation details without full pseudocode; TextCNN performance without labeled training data; optimal filtering thresholds across diverse CTI domains

## Next Checks

1. **GRPO Implementation Verification**: Implement complete GRPO algorithm with group-relative advantages and verify against paper's described behavior, particularly relative advantage computation and KL regularization

2. **TextCNN Filtering Validation**: Train TextCNN filter using described LLM judge methodology, test on held-out traces from multiple backbones, and verify τq=0.5 threshold provides claimed quality-throughput tradeoff

3. **ACR Transfer Generalization**: Systematically evaluate ACR trace generalization by testing model performance on answer-free prompts before and after distillation, measuring whether reasoning patterns actually transfer versus simple answer memorization