---
ver: rpa2
title: 'LANID: LLM-assisted New Intent Discovery'
arxiv_id: '2503.23740'
source_url: https://arxiv.org/abs/2503.23740
tags:
- intent
- lanid
- zhang
- intents
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LANID, a framework that leverages Large Language
  Models (LLMs) to improve new intent discovery in task-oriented dialogue systems.
  The key innovation is using LLMs to generate contrastive training signals from selectively
  sampled utterance pairs, which are then used to fine-tune lightweight encoders via
  triplet loss.
---

# LANID: LLM-assisted New Intent Discovery

## Quick Facts
- **arXiv ID**: 2503.23740
- **Source URL**: https://arxiv.org/abs/2503.23740
- **Reference count**: 0
- **Primary Result**: Achieves up to 87.64% NMI and 78.51% accuracy in semi-supervised settings on Banking and StackOverflow datasets

## Executive Summary
This paper introduces LANID, a framework that leverages Large Language Models (LLMs) to improve new intent discovery in task-oriented dialogue systems. The key innovation is using LLMs to generate contrastive training signals from selectively sampled utterance pairs, which are then used to fine-tune lightweight encoders via triplet loss. Two sampling strategies—KNN-based and DBSCAN-based—are proposed to ensure representative and informative data selection. The method outperforms state-of-the-art baselines on three benchmark datasets (Banking, StackOverflow, M-CID) in both unsupervised and semi-supervised settings, achieving up to 87.64% NMI and 78.51% accuracy in semi-supervised settings. The approach balances LLM-guided semantic enhancement with computational efficiency, addressing scalability and privacy concerns while maintaining strong performance.

## Method Summary
LANID operates through a two-stage process: selective sampling followed by contrastive learning. First, it employs either KNN-based or DBSCAN-based sampling to identify representative utterance pairs from unlabeled data. These pairs are then processed by an LLM to generate contrastive training signals, including hard negative examples and cluster-aware similarity scores. The LLM outputs are used to construct triplet loss functions that fine-tune lightweight encoders, which can subsequently be deployed for efficient inference. The framework is designed to work in both unsupervised settings (where no labels exist) and semi-supervised settings (where a small amount of labeled data is available), with the semi-supervised variant showing superior performance across all benchmark datasets.

## Key Results
- Achieves 87.64% NMI and 78.51% accuracy on Banking dataset in semi-supervised setting
- Outperforms all baseline methods on StackOverflow dataset with 85.14% NMI
- Maintains strong performance in unsupervised settings, exceeding previous SOTA by 5-8 percentage points on average

## Why This Works (Mechanism)
The framework leverages LLMs' superior semantic understanding to create high-quality contrastive signals that capture nuanced intent relationships. By selectively sampling utterance pairs rather than using all possible combinations, it balances the depth of LLM analysis with computational efficiency. The triplet loss training objective ensures that the lightweight encoder learns to preserve the semantic distinctions identified by the LLM, resulting in better clustering performance than approaches that rely solely on LLM embeddings or traditional unsupervised methods.

## Foundational Learning

**Contrastive Learning**: Why needed - To learn meaningful representations by pulling similar samples together and pushing dissimilar ones apart; Quick check - Verify that positive pairs are semantically similar and negative pairs are meaningfully different

**Triplet Loss**: Why needed - To create relative distance constraints between anchor, positive, and negative samples; Quick check - Ensure margin parameter is appropriately set to prevent trivial solutions

**DBSCAN Clustering**: Why needed - To identify dense regions of similar utterances for representative sampling; Quick check - Validate that epsilon and min_samples parameters are tuned for each dataset's density characteristics

**k-Nearest Neighbors Sampling**: Why needed - To select utterance pairs that are locally representative of the data distribution; Quick check - Confirm that k value captures sufficient local variation without being dominated by noise

## Architecture Onboarding

**Component Map**: Raw utterances -> Sampling Strategy (KNN/DBSCAN) -> LLM for Contrastive Signal Generation -> Triplet Loss Training -> Lightweight Encoder -> Clustering/Classification

**Critical Path**: Sampling -> LLM Processing -> Encoder Fine-tuning -> Evaluation

**Design Tradeoffs**: The framework trades some LLM inference cost for significantly improved downstream clustering performance, while the selective sampling strategy mitigates the computational burden compared to using all possible utterance pairs.

**Failure Signatures**: Poor sampling strategy leads to unrepresentative pairs; insufficient LLM quality results in weak contrastive signals; improper triplet loss configuration causes collapsed embeddings or inability to separate classes.

**First Experiments**:
1. Compare KNN vs DBSCAN sampling performance on a held-out validation set to determine optimal strategy for each dataset
2. Test different LLM models (GPT-3.5 vs GPT-4) to assess impact on contrastive signal quality and overall performance
3. Evaluate the effect of varying the number of sampled pairs on both computational cost and clustering accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of LLM-based contrastive signal generation remains significant for very large-scale deployments
- Performance heavily depends on base LLM quality and may not generalize equally well across domains with different linguistic characteristics
- Selective sampling strategies require careful parameter tuning, and suboptimal choices could lead to poor representative pair selection
- Assumes availability of sufficient unlabeled data for meaningful clustering, which may not hold in data-scarce scenarios
- Evaluation focuses primarily on clustering metrics without extensive analysis of practical deployment considerations

## Confidence

**High Confidence**: Claims regarding performance improvements over baseline methods on the three benchmark datasets, particularly the NMI scores of 87.64% and 85.14% on Banking and StackOverflow datasets respectively in semi-supervised settings.

**Medium Confidence**: Generalizability claims across different domains and languages, as the evaluation is limited to English datasets with specific domain characteristics.

**Medium Confidence**: Computational efficiency claims, given that while selective sampling reduces overhead, comprehensive benchmarking against production-scale systems is not provided.

## Next Checks
1. **Cross-domain Transferability Test**: Evaluate LANID on at least three additional domains (e.g., healthcare, retail, travel) with different linguistic patterns to assess robustness beyond the current banking, technical Q&A, and customer service domains.

2. **Scaling Analysis**: Conduct experiments measuring inference time, memory usage, and LLM API costs as dataset size scales from 1K to 100K+ utterances, comparing against both traditional methods and alternative LLM-based approaches.

3. **Longitudinal Stability Assessment**: Implement a temporal validation where models are trained on data from one time period and evaluated on data from subsequent periods (e.g., 3-6 month intervals) to measure performance degradation and identify potential drift patterns in intent distributions.