---
ver: rpa2
title: 'KAC: Kolmogorov-Arnold Classifier for Continual Learning'
arxiv_id: '2503.21076'
source_url: https://arxiv.org/abs/2503.21076
tags:
- learning
- uni00000013
- continual
- functions
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KAC, a Kolmogorov-Arnold Classifier designed
  to address catastrophic forgetting in continual learning. KAC replaces conventional
  linear classifiers with a KAN-based architecture that uses learnable Radial Basis
  Functions (RBF) as spline functions, improving compatibility with class-incremental
  learning tasks.
---

# KAC: Kolmogorov-Arnold Classifier for Continual Learning

## Quick Facts
- **arXiv ID**: 2503.21076
- **Source URL**: https://arxiv.org/abs/2503.21076
- **Reference count**: 40
- **Primary result**: KAC achieves 1-25% accuracy improvements over baselines in class-incremental learning by replacing linear classifiers with RBF-based KAN architecture

## Executive Summary
This paper introduces KAC (Kolmogorov-Arnold Classifier), a novel classifier architecture designed to address catastrophic forgetting in continual learning. KAC replaces conventional linear classifiers with a KAN-based architecture that uses learnable Radial Basis Functions (RBF) as spline functions, improving compatibility with class-incremental learning tasks. The key insight is that RBFs provide better approximation capability than B-splines, reducing the need for extensive backbone updates and enabling more stable feature space learning. Experiments across multiple datasets demonstrate consistent performance improvements, particularly in long-sequence and fine-grained classification scenarios.

## Method Summary
KAC replaces linear classifiers in existing prompt-based continual learning methods with a Kolmogorov-Arnold Network architecture using Radial Basis Functions. The method uses a pre-trained ViT backbone (frozen or lightly tuned), applies Layer Normalization to embeddings, then passes them through N Gaussian RBF activations (N=4) with centers evenly distributed in [-2, 2] and σ=1. A learnable weight matrix W projects the RBF-activated features to class logits, expanding as new classes are added. The architecture removes the linear shortcut present in standard KANs, as experiments showed it degraded performance.

## Key Results
- Achieves 1-25% accuracy improvements over baseline methods across various continual learning benchmarks
- Particularly effective in long-sequence tasks (20+ incremental steps) where forgetting typically accumulates
- Outperforms linear classifiers on ImageNet-R, CUB200, CIFAR-100, and DomainNet datasets
- Shows superior performance in fine-grained classification scenarios

## Why This Works (Mechanism)

### Mechanism 1
Replacing B-splines with Radial Basis Functions (RBFs) enables more effective high-dimensional function approximation, reducing the need for backbone updates. Standard B-splines in KAN suffer from the curse of dimensionality on high-dimensional data, forcing large disruptive updates to the backbone. RBFs create an inherent Gaussian mixture structure that is easier to fit, allowing the model to adapt to new classes with smaller, more localized parameter changes, thereby stabilizing the feature space.

### Mechanism 2
Learnable, class-specific activations enable local plasticity by selectively updating feature channels. Unlike linear classifiers where weight updates affect all connected features, KAC uses learnable RBF activations that can specialize to different input value ranges for each channel. When a new task is learned, the model can adjust weights and activation profiles for new classes on a subset of channels, leaving the activation profiles for old classes undisturbed.

### Mechanism 3
The architecture improves stability by mapping features to a Gaussian process-like space. The RBF formulation projects each channel into a Gaussian mixture distribution, and the final class prediction is a weighted combination of these distributions. This structure conforms to a Gaussian Process with an additive kernel, providing a strong long-range structure for approximation and making the decision boundary more robust and less prone to drift from small perturbations.

## Foundational Learning

- **Kolmogorov-Arnold Networks (KAN)**: KAC is built on the KAN principle of placing learnable univariate functions (edges) instead of fixed activations (nodes). Understanding this shift from MLPs is crucial. Quick check: Can you explain how a KAN layer differs from a standard MLP linear layer?

- **Catastrophic Forgetting & Class-Incremental Learning (CIL)**: The entire design of KAC is motivated by the stability-plasticity dilemma in CIL. You must understand why a standard linear classifier struggles when new classes are added sequentially. Quick check: What happens to a linear classifier's weights for old classes when trained exclusively on data from new classes?

- **Radial Basis Functions (RBF)**: The paper's key innovation is replacing KAN's default B-splines with RBFs. Understanding their Gaussian-like, localized response is key to the method. Quick check: How does an RBF's output change as an input feature moves away from its center point?

## Architecture Onboarding

- **Component map**: Input Image -> ViT Backbone -> Layer Norm -> (x - c)^2 / sigma -> Gaussian RBF Activation -> W (linear projection) -> Logits
- **Critical path**: Input Image -> ViT Backbone -> Layer Norm -> (x - c)^2 / sigma -> Gaussian RBF Activation -> W (linear projection) -> Logits
- **Design tradeoffs**:
  - RBF vs. B-spline: The paper explicitly rejects B-splines for high-dimensional CIL due to poor fitting and resulting backbone drift. RBFs are chosen for their Gaussian structure and better approximation properties.
  - Number of RBFs (N): This is a key hyperparameter. Too few reduces expressivity; too many increases computation and parameter count with diminishing returns. The paper finds N=4 or N=8 to be optimal.
  - No Linear Shortcut: Unlike original KAN, KAC removes the silu(x) linear shortcut, as experiments showed it degraded performance.
- **Failure signatures**:
  - Performance Drop vs. Linear Baseline: If you see this, check if you are using B-splines (deprecated in this context) or if the number of RBFs is too low.
  - Severe Forgetting on Long Sequences: May indicate the backbone is being updated too aggressively, violating the stability assumption.
- **First 3 experiments**:
  1. Sanity Check: Implement a single KAC layer (LayerNorm + RBF + Linear) and verify it can overfit a small, static dataset. This validates the core function approximator.
  2. Baseline Comparison: Take an existing CIL method like L2P. Replace only its final linear classifier with KAC, keeping all hyperparameters the same. Run on a 5-step ImageNet-R split to verify the reported accuracy gain.
  3. Ablation on RBF Count: Using the setup from experiment 2, sweep the number of RBFs (e.g., N=1, 2, 4, 8, 16) on a 20-step benchmark to confirm the paper's finding that N=4 is optimal and that gains are not just from parameter count.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does the compatibility of KAC vary significantly across different prompt-based continual learning methods (e.g., CODAPrompt vs. CPrompt), and does the prompt selection mechanism interfere with the RBF classifier's locality? The paper observes this discrepancy in experimental results but does not provide theoretical or empirical analysis of why the underlying prompt strategies interact differently with the Kolmogorov-Arnold structure.

- **Open Question 2**: Why does the inclusion of a linear shortcut (SiLU), which is standard in KAN architectures, lead to performance degradation in KAC? The authors remove the component to improve results but do not fully explain why the residual connection—typically beneficial for gradient flow—harms the specific stability-plasticity balance required in this class-incremental setting.

- **Open Question 3**: How does KAC perform on non-pre-trained backbones or low-resolution datasets where feature representations are less saturated? The paper relies heavily on pre-trained ViT-B/16 backbones and notes that on CIFAR-100, KAC shows little improvement or slight drops because "performance tends to be saturated" for pre-trained backbones on such low-resolution data.

## Limitations

- Focuses exclusively on replacing linear classifiers in prompt-based methods, limiting generalizability to other continual learning approaches
- No experiments compare KAC against non-prompt-based continual learning methods like Elastic Weight Consolidation
- Computational overhead of RBF activation is not quantified, which could be significant for deployment
- Performance on very long sequences (>20 tasks) isn't thoroughly evaluated

## Confidence

- **Low**: The central claims about catastrophic forgetting reduction rely heavily on a single architectural change (RBF vs. B-spline) without extensive ablation on backbone stability
- **Medium**: The empirical improvements are well-documented across multiple datasets, but the magnitude of gains (1-25%) varies significantly by task
- **Medium**: The claim that learnable RBF activations enable local plasticity is theoretically sound but lacks direct visualization evidence

## Next Checks

1. **Ablation on Backbone Updates**: Run KAC with and without frozen backbone on a 20-task sequence to isolate whether improvements come from classifier design or reduced backbone updates.

2. **Generalization Beyond Prompts**: Implement KAC in a non-prompt-based method like Elastic Weight Consolidation to test if improvements transfer beyond the prompt-based framework.

3. **Parameter Efficiency Analysis**: Compare parameter growth and FLOPs between KAC (N=4) and linear classifiers across increasing class counts to quantify the computational cost of improved performance.