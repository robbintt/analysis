---
ver: rpa2
title: 'Mem-Rec: Memory Efficient Recommendation System using Alternative Representation'
arxiv_id: '2305.07205'
source_url: https://arxiv.org/abs/2305.07205
tags:
- embedding
- mem-rec
- dlrm
- size
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose MEM-REC, a memory-efficient embedding table
  representation for deep learning recommendation models that uses Bloom filters and
  hashing to encode categorical features with two cache-friendly embedding tables.
  The first table contains raw embeddings, while the second (much smaller) table contains
  weights to scale raw embeddings and improve discriminative capability.
---

# Mem-Rec: Memory Efficient Recommendation System using Alternative Representation

## Quick Facts
- arXiv ID: 2305.07205
- Source URL: https://arxiv.org/abs/2305.07205
- Reference count: 4
- The authors propose MEM-REC, a memory-efficient embedding table representation for deep learning recommendation models that uses Bloom filters and hashing to encode categorical features with two cache-friendly embedding tables.

## Executive Summary
MEM-REC is a memory-efficient embedding table representation for deep learning recommendation models that leverages Bloom filters and hashing to encode categorical features. The method uses two cache-friendly embedding tables: one containing raw embeddings and a second, much smaller table containing weights to scale raw embeddings and improve discriminative capability. This approach significantly reduces memory footprint and improves embedding latency, compressing the MLPerf Criteo Terabyte benchmark DLRM model by 2900x and achieving up to 3.4x faster embeddings while maintaining the same AUC as the full uncompressed model.

## Method Summary
MEM-REC replaces standard embedding tables with a dual-encoder architecture. The Token Encoder maps categorical features to a Bloom filter vector, which selects columns from a large embedding table that are pooled (summed) to create a raw embedding. The Weight Encoder uses a separate Bloom filter to generate a scalar weight from a smaller table. The final embedding is the element-wise product of the raw embedding and this weight scalar. This compression allows embedding tables to fit entirely within CPU Last Level Cache (LLC), converting a memory-bandwidth-bound problem into a cache-friendly one while maintaining accuracy through the dual-encoder mechanism.

## Key Results
- Compresses MLPerf Criteo Terabyte benchmark DLRM model by 2900x
- Achieves up to 3.4x faster embeddings while maintaining same AUC as full model
- Reduces memory footprint from hundreds of GBs to fit within CPU LLC (48MB)

## Why This Works (Mechanism)

### Mechanism 1
Mapping high-cardinality categorical features into a low-dimensional binary vector space using Bloom filters drastically reduces memory footprint compared to one-hot encoding. The Token Encoder uses k hash functions to map a categorical input x to a sparse binary vector φ(x) of size d. Instead of storing m embeddings, the system stores a compressed table M of size d ≪ m. The embedding is generated by pooling columns of M corresponding to active indices in φ(x). Core assumption: model can tolerate information loss inherent in approximate set membership representations where distinct tokens may map to identical indices (collisions).

### Mechanism 2
A secondary "Weight Encoder" recovers discriminative capability lost due to hash collisions by applying data-dependent scaling factor. A separate, smaller Bloom filter and embedding table generate scalar weight α(x) for each token. Final embedding is element-wise product: z(x) = α(x)Mφ(x). Even if two tokens collide in token table, they are assigned different magnitudes, allowing downstream neural network to distinguish them. Core assumption: downstream MLP can untangle feature interactions based on magnitude differences for colliding tokens.

### Mechanism 3
Aggressive compression allows embedding tables to fit entirely within CPU Last Level Cache (LLC), converting memory-bandwidth-bound problem into cache-friendly one. Standard DLRM tables reside in DRAM (100s of GBs), causing high latency due to random access. MEM-REC compresses tables to sizes compatible with LLC (e.g., 48MB). This minimizes expensive DRAM accesses and reduces embedding latency. Core assumption: hash computation overhead is significantly lower than latency of DRAM access.

## Foundational Learning

- **Bloom Filters**: Why needed: Core data structure for MEM-REC. Understand how hash functions set bits in bit-vector and trade-off between vector size (d), number of hashes (k), and false positive rates (collisions). Quick check: If you increase number of hash functions k without increasing vector size d, does false positive probability increase or decrease?

- **Hash Collisions**: Why needed: Primary drawback of "hashing trick" used here is that different categorical values map to same index. MEM-REC is specifically architected to mitigate this. Quick check: In standard hashing trick, if Token A and Token B map to same index, what happens to their learned representations?

- **Memory Hierarchy (DRAM vs. LLC)**: Why needed: Performance gain of MEM-REC relies on fitting embedding tables into Last Level Cache (LLC) to avoid memory bandwidth wall. Quick check: What is typical order-of-magnitude latency difference between accessing data in CPU LLC versus main memory (DRAM)?

## Architecture Onboarding

- **Component map**: Input categorical features (x) -> Two parallel Bloom filter paths (Token Encoder + Weight Encoder) -> Token Table (Large, fits in LLC) + Weight Table (Small, fits in L2) -> Sum pooling of selected vectors -> Dot product with weight scalar -> Final embedding passed to DLRM interaction layers

- **Critical path**: 1. Receive categorical ID. 2. Compute k indices for Token Encoder and k' indices for Weight Encoder (pure compute). 3. Lookup indices in respective tables (memory access). 4. Pool vectors (Sum) and multiply by weight scalar.

- **Design tradeoffs**: Size (d) vs. Accuracy: Smaller d saves memory but increases collisions. Hashes (k) vs. Latency: More hashes reduces collisions but requires accessing more rows in table, increasing memory access parallelism. Training Speed: MEM-REC requires 6 epochs vs DLRM's 1 for convergence on Criteo-TB.

- **Failure signatures**: Accuracy Drop (>1%): Indicates d is too small or collision resolution failed. Slower than Baseline: Indicates hash computation overhead exceeds memory access savings. Slow Convergence: Observed behavior requiring more epochs.

- **First 3 experiments**: 1. Hyperparameter Sweep (d, k, k'): Replicate Figure 5/6/7 on subset of data to find "knee" of accuracy curve for specific dataset. 2. Iso-AUC Compression Check: Compare memory size required for MEM-REC to achieve same AUC as uncompressed DLRM baseline. 3. Cache Miss Profiling: Use profiler to verify LLC misses drop significantly compared to baseline, confirming system-level mechanism.

## Open Questions the Paper Calls Out

1. How does MEM-REC perform on hardware accelerators like GPUs or TPUs compared to server-class CPUs evaluated? [explicit] Authors plan to study architectural implications on different hardware platforms.

2. Can software pipelines be specifically optimized to exploit MEM-REC's cache-friendly behavior? [explicit] Authors plan to explore leveraging cache-friendliness to design more efficient pipelines.

3. Can training procedure be optimized to reduce number of epochs required for convergence? [inferred] Paper notes MEM-REC requires 6 epochs vs baseline's 1, attributing this to lack of extensive hyper-parameter tuning.

## Limitations

- Collision tolerance threshold is unclear; precise bounds on Bloom vector size d that preserves AUC within 1% of baseline are not quantified
- Weight encoder dependency creates single point of failure; if d' is too small or weight table cannot be accessed quickly, mechanism fails
- Hash function sensitivity is significant; collision distributions depend heavily on specific hash implementation choice

## Confidence

- **High Confidence**: Memory compression claim (2900x) is well-supported by described mechanism and consistent with Bloom filter theory
- **Medium Confidence**: 3.4x embedding latency improvement is plausible given cache-fitting premise but depends on hardware specifics
- **Low Confidence**: Accuracy preservation claim (same AUC as uncompressed model) is demonstrated empirically but collision tolerance threshold creates uncertainty about generalizability

## Next Checks

1. **Collision Rate Analysis**: Implement MEM-REC with controlled hash functions and measure actual collision rate for various d values on Criteo-Kaggle dataset. Plot AUC degradation as function of collision probability to establish practical tolerance threshold.

2. **Weight Encoder Sensitivity Test**: Systematically vary d' (weight table size) while holding d constant. Measure both AUC and inference latency to quantify trade-off between discriminative power recovery and weight encoder's contribution to critical path.

3. **Cross-Dataset Generalization**: Apply MEM-REC to non-Criteo dataset (e.g., MovieLens) with different vocabulary characteristics. Compare compression ratios and accuracy preservation to establish whether observed performance gains generalize beyond Criteo benchmark.