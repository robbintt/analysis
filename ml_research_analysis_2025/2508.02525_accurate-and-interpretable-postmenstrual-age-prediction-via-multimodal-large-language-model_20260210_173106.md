---
ver: rpa2
title: Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large
  Language Model
arxiv_id: '2508.02525'
source_url: https://arxiv.org/abs/2508.02525
tags:
- weeks
- cortical
- training
- postmenstrual
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal large language model (MLLM) approach
  for accurate and interpretable postmenstrual age (PMA) prediction from neonatal
  brain MRI. The authors adapt Qwen2.5-VL-7B-Instruct using parameter-efficient fine-tuning
  (LoRA) on four-channel cortical surface projection maps derived from neonatal MRI.
---

# Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large Language Model

## Quick Facts
- **arXiv ID:** 2508.02525
- **Source URL:** https://arxiv.org/abs/2508.02525
- **Reference count:** 28
- **Primary result:** MAE of 1.10 weeks (95% CI: 0.78–1.52) on validation set using multimodal LLM for PMA prediction from neonatal brain MRI

## Executive Summary
This paper presents a multimodal large language model (MLLM) approach for accurate and interpretable postmenstrual age (PMA) prediction from neonatal brain MRI. The authors adapt Qwen2.5-VL-7B-Instruct using parameter-efficient fine-tuning (LoRA) on four-channel cortical surface projection maps derived from neonatal MRI. A key innovation is using different prompts for training (regression) and inference (explanatory output), enabling the model to generate clinically grounded explanations alongside PMA predictions. The model achieves a mean absolute error of 1.10 weeks (95% CI: 0.78–1.52) on the validation set, approaching state-of-the-art accuracy. User studies with clinicians confirm high interpretability and clinical acceptability of the model's explanations. This work demonstrates how MLLMs can balance predictive accuracy with transparency, offering a promising direction for trustworthy AI in perinatal neuroscience.

## Method Summary
The authors fine-tune Qwen2.5-VL-7B-Instruct with LoRA (r=16, α=32) targeting vision attention layers to predict PMA from 4-channel cortical projection maps (thickness, curvature, myelination, sulcal depth). Training uses a regression-only prompt, while inference employs an explanatory prompt for zero-shot explanation generation. The model is trained on 425 samples from the dHCP dataset for 3 epochs, achieving MAE of 1.10 weeks on 51 validation samples.

## Key Results
- Achieved MAE of 1.10 weeks (95% CI: 0.78–1.52) on validation set
- R² = 0.821 indicates strong correlation between predictions and ground truth
- User studies show high interpretability and clinical acceptability of explanations
- Systematic errors observed at age distribution extremes (very preterm and post-term samples)

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Training-Inference Prompting
- Claim: Using structurally different prompts at training vs. inference enables the same model to learn precise regression while generating zero-shot explanations.
- Mechanism: During training, a minimal prompt forces the model to output only a numeric PMA value, concentrating gradient updates on the regression task. At inference, a richer prompt leverages the model's pre-trained instruction-following capacity to produce explanations without having seen exemplars during fine-tuning.
- Core assumption: The base model's instruction-following capabilities generalize across prompt structures, allowing zero-shot explanation generation that remains grounded in learned visual features.
- Evidence anchors:
  - [abstract] "By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference."
  - [Section 2.2.1-2.2.2] Training prompt requests only numerical value; inference prompt requests "Predicted Postmenstrual Age" followed by "Explanation" in structured format.
  - [corpus] No direct corpus evidence for this specific decoupling strategy; similar prompt engineering appears in general MLLM literature but not validated for medical regression tasks.
- Break condition: If explanations are factually inconsistent with input images (hallucination), the decoupling has failed—indicating the model is not grounding text in visual features.

### Mechanism 2: Vision-Specific Low-Rank Adaptation
- Claim: Restricting LoRA to vision encoder components preserves language reasoning while adapting visual representations to neonatal MRI patterns.
- Mechanism: LoRA injects trainable rank-16 decomposition matrices into attention layers (q_proj, v_proj, qkv, proj) of the ViT encoder only. The LLM decoder remains frozen, retaining general reasoning while the visual pathway learns domain-specific features.
- Core assumption: The distribution shift from natural images to 4-channel cortical projection maps can be captured through low-rank updates without requiring full model adaptation.
- Evidence anchors:
  - [Section 2.1.2] "LoRA works by freezing the pre-trained model weights and injecting trainable rank-decomposition matrices into specified layers."
  - [Appendix A.1.1] "Target Modules: We applied LoRA...specifically targeting query, key, value, and projection layers...This configuration resulted in only ~0.2% of the total model parameters being trainable."
  - [corpus] Corpus papers use various PEFT approaches; M-TabNet applies transformers to multimodal neonatal data but does not report LoRA-specific configurations for comparison.
- Break condition: If fine-tuned model performs worse than zero-shot on held-out distributions, the low-rank constraint is too restrictive for the feature space complexity.

### Mechanism 3: Multi-Channel Cortical Maturation Encoding
- Claim: Four complementary cortical metrics jointly encode developmental stage information that single-modality inputs cannot capture.
- Mechanism: Cortical thickness, curvature, myelination, and sulcal depth each follow distinct developmental trajectories. Their joint representation allows the model to learn correlated maturational patterns rather than relying on any single proxy.
- Core assumption: 2D spherical projections preserve sufficient spatial and structural information for age prediction despite losing 3D volumetric detail.
- Evidence anchors:
  - [Section 3.2] "The four channels correspond to different metrics of cortical maturation: Cortical Thickness, Cortical Curvature, Cortical Myelination, Sulcal Depth."
  - [Section 4] "Limitations of this work include the use of 2D projections, which inevitably leads to some loss of information compared to a full 3D analysis."
  - [corpus] CINeMA paper uses multimodal perinatal brain representation; Equivariant Spherical CNNs paper addresses similar neonatal dMRI challenges, supporting the utility of structured multi-channel inputs.
- Break condition: If model errors cluster at distribution extremes (very preterm, post-term), the cortical metrics may lack discriminative power at those ages—observed in Sample #6 (26.9 weeks) and Sample #13 (44.4 weeks) failure cases.

## Foundational Learning

- **Vision-Language Model Architecture**
  - Why needed here: Understanding how Qwen2.5-VL couples a ViT encoder with an LLM decoder clarifies why LoRA on vision components affects feature extraction without disrupting text generation.
  - Quick check question: Can you explain why freezing the LLM decoder while adapting the ViT encoder preserves instruction-following ability?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: The entire fine-tuning strategy depends on LoRA's rank-decomposition approach; misconfiguring rank or alpha directly impacts model capacity and convergence.
  - Quick check question: What happens to the effective update magnitude if rank=16 and alpha=32? (Answer: scaled by α/r = 2)

- **Cortical Surface Projections**
  - Why needed here: Interpreting model explanations requires knowing what each of the four input channels represents neuroanatomically.
  - Quick check question: Which cortical metric would you expect to show the most dramatic change between 27 and 45 weeks PMA?

## Architecture Onboarding

- **Component map:** Raw dHCP MRI -> 4-channel cortical projections (240×320×4) -> Qwen2.5-VL-7B-Instruct ViT encoder (LoRA-adapted) -> Vision-language merger -> Frozen LLM decoder -> Numeric PMA + free-text explanation

- **Critical path:**
  1. Load pre-trained Qwen2.5-VL-7B-Instruct weights
  2. Apply LoRA adapters to vision attention layers (q_proj, v_proj, qkv, proj)
  3. Format training samples with regression-only prompt
  4. Compute loss only on assistant response tokens (mask system/user tokens)
  5. Train 3 epochs, lr=5e-5, batch_size=1
  6. At inference, switch to explanation prompt and parse structured output

- **Design tradeoffs:**
  - 2D projections vs. 3D volumes: Lower memory/compute but information loss acknowledged by authors
  - Vision-only LoRA vs. full-model LoRA: Preserves language capabilities but may limit cross-modal alignment refinement
  - Regression-only training vs. joint training with explanations: Simpler optimization but zero-shot explanation quality is not directly supervised

- **Failure signatures:**
  - Extreme age errors: Model systematically overestimates very preterm (Sample #6: +3.8 weeks) and underestimates post-term (Sample #13: -3.7 weeks) due to training distribution skew
  - Hallucinated explanations: If explanations reference features not present in input, the vision-language grounding has degraded
  - High variance across bootstrap samples (95% CI: 0.78–1.52 weeks) indicates sensitivity to test set composition

- **First 3 experiments:**
  1. **LoRA ablation:** Test rank ∈ {4, 8, 16, 32} while holding alpha=2×rank constant; monitor both MAE and explanation quality to find the inflection point where regression performance plateaus but language capabilities remain intact.
  2. **Prompt sensitivity:** Evaluate whether adding explanation exemplars to training (few-shot) improves explanation faithfulness without degrading regression accuracy, comparing against the current zero-shot approach.
  3. **Distribution robustness:** Stratify validation by age quartiles; if errors concentrate at extremes as the paper suggests, experiment with age-weighted sampling or synthetic augmentation for underrepresented age ranges.

## Open Questions the Paper Calls Out

- **Question 1:** Would incorporating textual clinical metadata (gestational history, birth reports, clinical measurements) as additional input modalities improve PMA prediction accuracy and explanation quality?
  - Basis in paper: [explicit] The authors state in Future Works: "textual features from patient's medical records such as gestational history, birth reports and other clinical measurements, can be potentially included as part of the input prompt in addition to the visual features."
  - Why unresolved: Current work uses only visual cortical projection maps; multimodal fusion with structured clinical text remains unexplored for this task.
  - What evidence would resolve it: A comparative study where models are trained with and without clinical text inputs, measuring changes in MAE and explanation clinical relevance scores.

- **Question 2:** Would processing full 3D volumetric MRI data instead of 2D cortical surface projections significantly improve prediction accuracy?
  - Basis in paper: [explicit] The authors acknowledge as a limitation: "the use of 2D projections, which inevitably leads to some loss of information compared to a full 3D analysis."
  - Why unresolved: Computational constraints and MLLM architecture favor 2D inputs; no comparison with 3D-based approaches was conducted.
  - What evidence would resolve it: Direct benchmarking against 3D CNN or 3D transformer baselines on the same cohort, with statistical comparison of MAE.

- **Question 3:** Are the model's textual explanations faithful representations of its decision-making process, or post-hoc rationalizations that may not reflect actual features used for prediction?
  - Basis in paper: [inferred] The model generates explanations at inference time using a different prompt than training, leveraging pre-trained knowledge. The authors note explanations "require further validation by clinical experts to confirm their diagnostic utility" but do not assess faithfulness.
  - Why unresolved: The decoupled training/inference prompt strategy means explanations are zero-shot generations not directly supervised during fine-tuning.
  - What evidence would resolve it: Systematic perturbation studies where input features are masked or altered, measuring whether explanations appropriately change; or attention visualization correlated with mentioned features.

## Limitations

- **Cortical projection pipeline uncertainty:** The exact methodology for generating 4-channel cortical surface projections from raw dHCP MRI data is not fully specified, relying on external references without implementation details.
- **Age distribution skew:** The model shows systematic errors at distribution extremes (very preterm and post-term samples), with failure cases suggesting insufficient representation in these ranges during training.
- **Zero-shot explanation validity:** While explanations are clinically acceptable in user studies, the zero-shot approach has no direct supervision. The quality and faithfulness of explanations could degrade if the base model's instruction-following capabilities are less robust than assumed.

## Confidence

- **High confidence:** The core regression performance (MAE = 1.10 weeks, R² = 0.821) is well-supported by the validation results and methodology. The LoRA configuration and training procedure are explicitly detailed.
- **Medium confidence:** The decoupled prompting strategy (regression-only training, explanatory inference) is logically sound and grounded in MLLM literature, but its effectiveness for medical regression tasks is not directly validated against supervised explanation training.
- **Low confidence:** The clinical interpretability assessment relies on subjective user studies without detailed methodology or statistical validation of explanation quality across clinicians.

## Next Checks

1. **Distribution robustness test:** Stratify validation MAE by PMA quartiles (very preterm, early preterm, late preterm, term) to confirm whether errors indeed concentrate at extremes as suggested by the failure cases. If confirmed, implement age-weighted sampling or synthetic augmentation for underrepresented ranges.
2. **LoRA configuration sweep:** Systematically test rank parameters (4, 8, 16, 32) while monitoring both MAE and explanation quality. Identify the inflection point where regression performance plateaus but language capabilities remain intact, providing evidence for optimal parameter efficiency.
3. **Explanation faithfulness evaluation:** Design an automated check comparing explanation content against input image features. For example, verify that mentions of specific cortical features (thickness, curvature, etc.) correlate with actual values in the corresponding projection channels, detecting potential hallucination.