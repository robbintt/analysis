---
ver: rpa2
title: 'AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit'
arxiv_id: '2509.21597'
source_url: https://arxiv.org/abs/2509.21597
tags:
- speech
- datasets
- deepfake
- real
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AUDDT, a unified benchmark toolkit for evaluating
  audio deepfake detectors across 28 diverse datasets. AUDDT addresses the challenge
  of inconsistent evaluation setups by automating dataset preparation, label standardization,
  and model benchmarking.
---

# AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit

## Quick Facts
- **arXiv ID:** 2509.21597
- **Source URL:** https://arxiv.org/abs/2509.21597
- **Authors:** Yi Zhu; Heitor R. Guimarães; Arthur Pimentel; Tiago Falk
- **Reference count:** 40
- **Primary result:** Unified benchmark toolkit evaluating audio deepfake detectors across 28 datasets, revealing performance gaps from 42% to 96% accuracy

## Executive Summary
AUDDT addresses the challenge of inconsistent evaluation setups in audio deepfake detection by providing a unified benchmark toolkit that automates dataset preparation, label standardization, and model benchmarking across 28 diverse datasets. The toolkit enables comprehensive performance analysis across real-world conditions including different languages, accents, perturbations, and generative methods. Using a pretrained Wav2vec2-XLSR-AASIST detector, the study demonstrates significant performance variance across datasets, with accuracies ranging from 42% to 96%, and reveals that existing detectors struggle with diffusion-based deepfakes, in-the-wild speech, and perturbations like replay attacks and neural codec artifacts.

## Method Summary
AUDDT provides a standardized pipeline for evaluating audio deepfake detectors by automating dataset download, label alignment, preprocessing (16kHz resampling, amplitude normalization, 4-second duration alignment), and evaluation across 28 heterogeneous datasets. The toolkit wraps user models in an `AudioDeepfakeDetector` class, applies consistent preprocessing, and computes multiple metrics including EER, accuracy, TPR, TNR, and AUC-ROC. The baseline evaluation uses a Wav2vec2-XLSR-AASIST detector fine-tuned on ASVspoof2019, demonstrating how the toolkit reveals performance gaps across different perturbation types, generative methods, and dataset characteristics.

## Key Results
- W2V-AASIST achieves 90%+ accuracy on ASVspoof variants but only 8.7% on PLAYBACK-ATTACKS and near chance-level on SpoofCeleb
- Vocoder-only datasets (WaveFake, CVoiceFake) show 2-10% accuracy, indicating detectors may use vocoder artifacts as detection shortcuts
- Enhanced real speech is misclassified as fake 82% of the time, confirming sensitivity to neural processing artifacts
- Significant performance variance (42%-96% accuracy) across datasets reveals category-specific failure modes

## Why This Works (Mechanism)

### Mechanism 1
Standardized multi-dataset evaluation exposes performance gaps invisible to narrow benchmarking. By normalizing labels, sampling rates, and evaluation protocols across 28 heterogeneous datasets, AUDDT eliminates protocol variance as a confounding factor, forcing the model to confront diverse generative architectures, languages, and perturbations simultaneously, revealing category-specific failure modes. Core assumption: Performance variance across datasets reflects true generalization gaps rather than evaluation artifacts (assuming standardization is correct). Evidence: [abstract] "accuracies ranging from 42% to 96%" across datasets using the same pretrained detector.

### Mechanism 2
Category decomposition isolates specific detector vulnerabilities to guide targeted improvements. AUDDT groups datasets by 10 attributes (perturbation type, generative method, language, in-the-wild status, vocoder/codec usage, etc.). Performance stratification across these categories reveals which cues the model relies on (e.g., vocoder artifacts) versus where it fails (e.g., diffusion-generated speech). Core assumption: Category labels accurately reflect underlying acoustic characteristics that detectors respond to. Evidence: [section IV.A] Vocoder-only datasets show 2-10% accuracy, indicating detectors may use vocoder artifacts as shortcuts.

### Mechanism 3
Real-world deployment gaps stem from compounded distribution shifts absent in curated datasets. Real-world audio combines multiple perturbations (codec compression, denoising, microphone artifacts, room acoustics) that rarely co-occur in lab datasets. Detectors trained on clean studio speech overfit to narrow artifact signatures and misclassify benign neural processing or environmental variations. Core assumption: The gap between in-lab and real-world performance is primarily due to acoustic/processing distribution shifts. Evidence: [section IV.A] Near chance-level performance on SpoofCeleb (in-the-wild) vs. 90%+ on ASVspoof (studio-quality).

## Foundational Learning

- **Equal Error Rate (EER) vs. fixed-threshold accuracy:** AUDDT reports both but uses fixed 0.5 threshold for binary metrics. EER-based accuracy represents an upper bound with test-set knowledge, while fixed-threshold accuracy reflects realistic deployment. Quick check: Why would a model with low EER still have poor real-world accuracy if the deployment threshold is poorly chosen?

- **Self-supervised speech representations (Wav2Vec2):** The baseline detector uses Wav2vec2-XLSR as a frontend. These models learn acoustic features from unlabeled speech and are fine-tuned for downstream tasks. Their generalization depends on pretraining diversity. Quick check: What types of acoustic cues might a Wav2Vec2 model learn that transfer to deepfake detection, versus what it might miss?

- **Vocoder and neural codec artifacts:** The paper shows detectors flag vocoder-processed real speech as fake (2-10% accuracy), suggesting reliance on vocoder artifacts. Neural codecs introduce different artifacts. Understanding this distinction explains why "bonafide" samples with neural processing may be misclassified. Quick check: If a detector flags vocoder-resynthesized real speech as fake, what does this imply about its learned decision boundary?

## Architecture Onboarding

- **Component map:** Datasets (bash download scripts) → standardized label files (bonafide/spoof) → optional subsetting by group → preprocessing (resampling, normalization, duration alignment) → model wrapper (`AudioDeepfakeDetector` class) → inference loop → evaluation metrics (EER, accuracy, TPR, TNR, AUC-ROC) → LaTeX tables and PDF reports → configuration (YAML)

- **Critical path:** 1. Install AUDDT with minimal dependencies 2. Place model script and checkpoint in `models/` folder 3. Create detector wrapper inheriting from `AudioDeepfakeDetector` 4. Configure YAML with target datasets/groups and evaluation settings 5. Run automated pipeline: download → prepare labels → infer → report

- **Design tradeoffs:** Fixed vs. adaptive threshold (AUDDT uses fixed 0.5 for realistic deployment simulation, but this may understate potential performance); Binary vs. multi-class labels (current system uses binary, but some datasets contain nuanced categories); Snapshot vs. living benchmark (paper acknowledges rapid dataset evolution; AUDDT is a snapshot but aims for community-sustained updates)

- **Failure signatures:** 1. Dependency conflicts (user's model environment clashes with AUDDT requirements—mitigated by minimal dependencies) 2. Label mismatches (some datasets lack explicit labels or have non-standard formats—customizable preparation scripts address this) 3. Category conflation (in-the-wild datasets may contain unmarked perturbations, making category attribution noisy) 4. Duration mismatch (audio files shorter than target duration are zero-padded, potentially introducing artifacts)

- **First 3 experiments:** 1. Baseline sanity check: Run W2V-AASIST on ASVspoof2019-LA (in-domain) to verify pipeline reproduces expected 90%+ accuracy 2. Category stress test: Evaluate the same detector on all perturbation-grouped datasets to identify which perturbations cause the largest drops 3. Shortcut diagnosis: Test on vocoder-only datasets and enhanced speech to quantify false positive rates from neural artifacts

## Open Questions the Paper Calls Out

### Open Question 1
How can audio deepfake detectors be architected to distinguish malicious synthetic speech from benign speech processed by neural enhancement or codec models? Basis: [explicit] The authors report that the baseline detector misclassifies 82% of enhanced real speech as fake, suggesting models rely on "neural artifacts" as shortcuts rather than detecting actual spoofing. Unresolved because current models struggle to disentangle artifacts caused by "unharmful" neural processors from those generated by malicious TTS or VC systems. Evidence to resolve: Development of a model that maintains high detection accuracy on spoofed data while achieving TNR > 90% on vocoded and enhanced speech datasets.

### Open Question 2
What standardized validation protocols are required to transform static benchmarks like AUDDT into a "living benchmark" that keeps pace with generative model evolution? Basis: [explicit] The authors state the need to make AUDDT a "living benchmark" sustained by community contributions, acknowledging that the rapid pace of deepfake research renders any static study "just a snapshot in time." Unresolved because new generative architectures and datasets emerge faster than manual benchmarking pipelines can be updated. Evidence to resolve: A functioning community-driven pipeline that automatically integrates and validates new deepfake datasets into the existing framework while maintaining label consistency.

### Open Question 3
What specific acoustic features or training paradigms are necessary to bridge the distribution gap between studio-quality training data and "in-the-wild" deepfakes? Basis: [inferred] The paper notes that the baseline model achieves near chance-level accuracy on the in-the-wild dataset SpoofCeleb, despite high performance on curated datasets. Unresolved because the dominance of scripted, clean speech in existing datasets prevents models from learning to handle layered variations of noise, microphone quality, and compression. Evidence to resolve: Identification of robust features or training methodology that significantly improves accuracy on in-the-wild subsets without compromising performance on standard benchmarks.

## Limitations
- Dataset standardization risks: Some datasets lack explicit metadata, potentially introducing noise into category-based analysis
- Model dependency constraints: Study relies on single pretrained baseline, limiting generalizability of conclusions about detection failures
- Real-world deployment gap: Despite comprehensive in-lab benchmarking, specific deployment contexts remain underspecified

## Confidence
- **High confidence:** Standardized multi-dataset evaluation reveals true generalization gaps (supported by consistent performance variance across 28 datasets using identical protocols)
- **Medium confidence:** Category decomposition accurately isolates detector vulnerabilities (dependent on category label accuracy and absence of confounding factors)
- **Medium confidence:** Real-world deployment failures stem primarily from distribution shifts rather than infrastructure issues (based on current evidence but lacks direct deployment validation)

## Next Checks
1. Dataset quality audit: Manually verify label accuracy and metadata completeness for vocoder-only and enhanced speech categories showing near-zero accuracy
2. Cross-model replication: Apply AUDDT's standardized evaluation to at least two additional pretrained detectors with different architectures
3. Controlled perturbation study: Generate synthetic test sets that systematically combine perturbations to quantify compound effects on detection performance