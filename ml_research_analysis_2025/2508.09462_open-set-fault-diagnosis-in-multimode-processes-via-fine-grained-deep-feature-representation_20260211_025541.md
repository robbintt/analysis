---
ver: rpa2
title: Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature
  Representation
arxiv_id: '2508.09462'
source_url: https://arxiv.org/abs/2508.09462
tags:
- fault
- diagnosis
- unknown
- health
- faults
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses open-set fault diagnosis in multimode industrial
  processes, where samples from the same health state often exhibit multicluster distributions
  across different operating modes, making it difficult to construct compact decision
  boundaries and effectively identify unknown faults. To tackle this, the authors
  propose a Fine-Grained Clustering and Rejection Network (FGCRN) that combines multiscale
  depthwise convolution, bidirectional GRU, and temporal attention to extract discriminative
  features.
---

# Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation

## Quick Facts
- arXiv ID: 2508.09462
- Source URL: https://arxiv.org/abs/2508.09462
- Reference count: 40
- Primary result: FGCRN achieves 98.03% accuracy, 1.28% FAR, and 1.12% FRR on open-set fault diagnosis in multimode processes.

## Executive Summary
This paper addresses the challenge of open-set fault diagnosis in multimode industrial processes, where samples from the same health state exhibit multicluster distributions across different operating modes. The proposed Fine-Grained Clustering and Rejection Network (FGCRN) combines multiscale depthwise convolution, bidirectional GRU, and temporal attention to extract discriminative features. A distance-based loss function enhances intra-class compactness, while unsupervised clustering constructs multiple fine-grained feature representations per health state. Extreme value theory models the distance between samples and their cluster centroids to estimate rejection probabilities for unknown fault identification. Experiments on TE Process, CSTR, and a real industrial process demonstrate superior performance compared to existing open-set recognition methods.

## Method Summary
FGCRN processes multivariate time windows through a multiscale depthwise convolution (MSDC) branch with parallel convolutions (kernels 3,5,7,9) followed by bidirectional GRU and temporal attention. The classifier uses softmax over k known classes. During training, a distance-based loss minimizes Mahalanobis distance between samples and their cluster centroids. After training, K-means++ partitions each health state into M clusters (equal to operating modes), and EVT fits Weibull distributions to distance tails for rejection. The method requires hyperparameter tuning for distance loss weight λ, EVT tail ratio α, and assumes known operating mode count M.

## Key Results
- FGCRN achieves 98.03% average accuracy, 1.28% FAR, and 1.12% FRR across TE Process, CSTR, and real industrial datasets.
- Ablation study shows fine-grained clustering (M=2) reduces FAR by 10.7% compared to single-cluster representation.
- Distance-based loss function improves accuracy by 5.09% over baseline without compactness enhancement.

## Why This Works (Mechanism)

### Mechanism 1: Fine-Grained Cluster Representation for Multimode Distributions
Constructing multiple cluster centroids per health state improves unknown fault rejection compared to single-category prototypes. K-means++ partitions each health state into M sub-clusters, creating mode-specific decision boundaries. During inference, samples are assigned to the nearest cluster within their predicted class, and rejection probability is computed against that specific cluster's distribution rather than a coarse class-wide representation.

### Mechanism 2: Distance-Based Loss for Intra-Class Compactness
Adding Mahalanobis distance loss compresses feature distributions, tightening decision boundaries for better unknown rejection. The loss term directly minimizes distance between correctly classified samples and their assigned cluster centroids, pulling mode-specific samples closer together and reducing overlap between known and unknown fault regions in feature space.

### Mechanism 3: EVT-Based Tail Modeling for Unknown Rejection
Fitting Weibull distributions to distance tails provides calibrated rejection probabilities for unknown faults. After training, Mahalanobis distances from correctly classified samples to cluster centroids are computed. The tail (top α largest distances) is fitted to a Weibull distribution, and the CDF converts new sample distances to rejection probabilities—large distances yield high rejection scores.

## Foundational Learning

- **Concept: Open-Set Recognition vs. Closed-Set Classification**
  - Why needed: Traditional fault diagnosis assumes all test classes are known; this method must simultaneously classify known health states and reject novel fault types.
  - Quick check: Given a sample with high softmax probability but large distance to all cluster centroids, should it be classified or rejected?

- **Concept: Extreme Value Theory (EVT)**
  - Why needed: EVT models rare events (large distances) rather than assuming Gaussian distributions for all samples, providing better-calibrated tail probabilities.
  - Quick check: Why fit only the tail (top α distances) rather than the full distance distribution when estimating rejection thresholds?

- **Concept: Multimode Process Structure**
  - Why needed: Industrial systems operate under varying conditions; the same fault manifests differently across modes, creating multi-cluster distributions per health state.
  - Quick check: If you have 3 operating modes and 5 known fault types, how many cluster centroids should the fine-grained module produce (assuming M=3)?

## Architecture Onboarding

- **Component map:** Input → MSDC → BiGRU → TAM → Classifier → (Training) Cross-entropy + Distance loss; (Post-training) K-means++ → EVT Weibull fitting
- **Critical path:** Training—input → MSDC → BiGRU → TAM → classifier → cross-entropy + distance loss. Post-training—correctly classified samples → K-means++ clustering → EVT fitting. Inference—input → features → classify → compute distance to assigned cluster → EVT rejection probability → accept/reject.
- **Design tradeoffs:** Cluster count M: Higher M captures finer mode structure but risks sparse clusters; paper sets M equal to known operating mode count. Distance loss weight λ: Controls compactness vs. classification accuracy tradeoff. Tail ratio α: Larger α stabilizes Weibull fit but may include inliers.
- **Failure signatures:** High FAR (>50%): Single-cluster representation or insufficient distance loss—check M matches mode count. High FRR: EVT threshold too strict or distance loss over-compressing known variance. Cluster imbalance: Some centroids have few samples—may require mode balancing or reduced M.
- **First 3 experiments:**
  1. Ablation on M: Test M ∈ {1, 2, 4, 6} on TE process; expect M=2 optimal, M=1 to show FAR spike.
  2. Distance loss weight sweep: Vary λ ∈ {0.1, 0.5, 1.0, 2.0}; plot ACC/FAR/FRR to find plateau point.
  3. Cross-mode generalization: Train on TE modes {1,4}, test on {2,5} to validate cluster transferability.

## Open Questions the Paper Calls Out

- **Question:** How can the framework be extended to perform unsupervised differentiation among various types of unknown faults rather than grouping them into a single rejection class?
  - Basis: The Conclusion states the model categorizes all unknown faults into a single "unknown" category without performing a more detailed unsupervised classification.
  - Why unresolved: The current FGCRN architecture utilizes EVT solely to model the tail distribution of distances for rejection, lacking a mechanism to cluster or classify the outliers into distinct novel categories.

- **Question:** Can an automated incremental learning strategy be developed to incorporate newly identified faults into the known set without degrading performance on previously learned health states?
  - Basis: The authors identify the need for developing automated incremental learning-based fault diagnosis models for multimode processes as future work.
  - Why unresolved: The current method is static; once unknown faults are labeled, the model requires retraining. The paper does not explore how to update the fine-grained feature representations or the EVT Weibull models dynamically.

- **Question:** How does the performance of the fine-grained feature representation module vary if the number of operating modes (clusters M) is unknown or estimated incorrectly during the K-means++ initialization?
  - Basis: Algorithm 2 and the experimental setup assume the number of modes M is known a priori.
  - Why unresolved: In real-world "uncertain-mode" scenarios, the exact number of operating modes may not be available. If M is misspecified, the resulting cluster centroids may not align with true mode distributions, potentially degrading the EVT rejection accuracy.

## Limitations

- Hyperparameter details (λ, ε, d₀, window length T, EVT tail ratio α) are not fully specified, preventing exact reproduction.
- Claims about multiscale depthwise convolution with specific kernel-wise normalization lack comparative ablation evidence in the paper.
- The assumption that K-means++ with M clusters aligns perfectly with operating modes may not hold in noisy or poorly sampled mode regions.

## Confidence

- **High:** Overall performance metrics (98.03% ACC, 1.28% FAR, 1.12% FRR) and general superiority over baselines.
- **Medium:** The effectiveness of the distance-based loss function and fine-grained clustering mechanism, due to missing hyperparameter and design justification details.
- **Low:** The specific implementation of the multiscale depthwise convolution branch (kernel-wise BN vs SAIN) and its necessity over standard CNN backbones.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically sweep λ (distance loss weight) and α (EVT tail ratio) on TE process to quantify their impact on FAR and FRR.
2. **Cluster Count Ablation:** Evaluate M ∈ {1, 2, 4, 6} on both datasets to confirm the optimal M equals the number of operating modes and to observe FAR degradation when M=1.
3. **MSDC Necessity Test:** Replace the MSDC branch with a standard 1D CNN backbone and compare performance to isolate the contribution of kernel-wise normalization choices.