---
ver: rpa2
title: 'HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems:
  A Case Study'
arxiv_id: '2507.17118'
source_url: https://arxiv.org/abs/2507.17118
tags:
- safety
- failure
- systems
- driving
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HySAFE-AI addresses safety analysis challenges in end-to-end AI-based
  autonomous driving systems by adapting traditional methods (FMEA, FTA) to account
  for foundation model characteristics. The framework introduces architectural transparency
  requirements and AI-specific failure taxonomies to enable systematic safety evaluation
  of latent-space-driven architectures.
---

# HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study

## Quick Facts
- arXiv ID: 2507.17118
- Source URL: https://arxiv.org/abs/2507.17118
- Reference count: 40
- Primary result: Safety analysis framework for AI-based autonomous driving that reduces RPN values by up to 189 points through fused architecture with safety monitors

## Executive Summary
HySAFE-AI introduces a hybrid safety analysis framework that adapts traditional FMEA and FTA methods to address the unique challenges of end-to-end AI-based autonomous driving systems. The framework establishes architectural transparency requirements and develops AI-specific failure taxonomies to enable systematic safety evaluation of latent-space-driven architectures. By introducing safety-aware components like policy monitors, safety evaluators, and arbitration logic, the framework demonstrates effectiveness through substantial reductions in Risk Priority Numbers while maintaining compatibility with existing automotive safety standards.

## Method Summary
The framework operates through a systematic methodology: first establishing architectural transparency as a precondition for analysis, then applying multi-level abstraction analysis across latent space components to identify failure propagation paths. AI failure modes are mapped to traditional FMEA guidewords through a three-level taxonomy (guideword → AI failure mode → domain manifestation), enabling structured risk assessment. The framework conducts FMEA with expert-judged severity/occurrence/detection values to calculate RPN, constructs FTA identifying single points of failure, and implements safety-aware components (Policy Monitor, Safety Evaluator, Plan Arbitrator) to reduce detection difficulty. A final FMEA reassessment verifies RPN reductions, with the fused architecture achieving up to 189-point reductions in key failure modes.

## Key Results
- Identified AI-specific failure modes including quantization-induced hallucinations (RPN 252), temporal reasoning failures (RPN 252), and dataset staleness (RPN 216)
- Demonstrated RPN reductions of up to 189 points through mitigation strategies incorporating safety monitors
- Developed three-level mapping between FMEA guidewords and AI failure modes for systematic risk assessment
- Established architectural transparency requirements enabling analysis of otherwise "closed-box" foundation model systems

## Why This Works (Mechanism)

### Mechanism 1: Architectural Transparency as Safety Analysis Precondition
- Claim: Meaningful safety analysis of foundation model-based systems requires decomposing "closed-box" models into analyzable architectural entities.
- Mechanism: HySAFE-AI establishes architectural transparency by requiring visibility into FM system architecture across layers (encoder, latent diffusion, U-Net, decoder, trajectory planner) and representational dimensions, enabling systematic tracing of failure propagation paths that would otherwise be opaque.
- Core assumption: Foundation models can be structurally decomposed into components with identifiable interfaces even if internal parameter interactions remain uninterpretable.
- Evidence anchors:
  - [abstract]: "introduces architectural transparency requirements and AI-specific failure taxonomies to enable systematic safety evaluation of latent-space-driven architectures"
  - [section]: "Our safety analysis approach requires sufficient visibility into FM system architecture across different layers and representational dimensions. While FMs remain operationally 'closed-box,' systematic safety analysis necessitates access to architectural entities that enable identification of failure propagation paths."
  - [corpus]: Weak direct support; neighboring papers address LLM agent architectures and governance but not safety-specific architectural decomposition.
- Break condition: If foundation model components cannot be bounded with defined inputs/outputs, failure propagation paths become untraceable and the analysis reverts to black-box treatment.

### Mechanism 2: AI-Specific Failure Taxonomy Mapping to Traditional FMEA Guidewords
- Claim: Generalized AI failure modes can be systematically mapped to established FMEA guidewords, enabling structured risk assessment while preserving compatibility with safety engineering practices.
- Mechanism: The framework creates a three-level mapping: FMEA guidewords (e.g., "incorrect value") → AI Failure Modes (e.g., hallucination, quantization effects) → Domain-Specific Manifestations (e.g., phantom pedestrians in autonomous driving). This bridges AI model behavior with traditional safety engineering.
- Core assumption: AI failure behaviors fit within the discrete categorization framework that FMEA assumes, despite foundation models operating in continuous latent spaces.
- Evidence anchors:
  - [abstract]: "AI-specific failure modes identified include quantization-induced hallucinations, temporal reasoning failures, and dataset staleness, with RPN values ranging from 40-252"
  - [section]: Table III systematically maps failure modes—for example, "Quantization-Induced Hallucination (Incorrect Value)" manifests as "Imaginary obstacles on driving path, phantom pedestrians crossing highway" with RPN=252.
  - [corpus]: Limited corpus support; no neighboring papers specifically address AI failure taxonomy development.
- Break condition: If AI failures manifest as continuous statistical deviations rather than discrete states, FMEA guideword mapping loses precision and risk prioritization becomes unreliable.

### Mechanism 3: Fused Architecture with Independent Safety Monitors
- Claim: Integrating run-time safety mechanisms (Policy Monitor, Safety Evaluator, Arbitrator) with foundation model outputs can substantially reduce risk priority numbers through pre-validation.
- Mechanism: The fused architecture inserts three safety-aware components between trajectory generation and vehicle control: (1) Policy Monitor performs OOD detection and uncertainty quantification, (2) Safety Evaluator applies physics-based consistency checks, and (3) Plan Arbitrator selects trajectories passing both validations. Detection difficulty (D) decreases from 3-4 to 1, directly reducing RPN.
- Core assumption: Safety-critical failures in foundation model outputs are detectable by independent monitors operating on different principles (statistical uncertainty vs. physics constraints).
- Evidence anchors:
  - [abstract]: "demonstrates effectiveness through mitigation strategies that reduce RPN values by up to 189 points, incorporating safety-aware components like policy monitors, safety evaluators, and arbitration logic"
  - [section]: Table V shows RPN reductions—for Latent Denoiser quantization-induced hallucinations: D reduced 4→1, RPN reduced 252→63 (delta -189); for Temporal Reasoning Failure: D reduced 4→1, RPN reduced 252→63 (delta -189).
  - [corpus]: Weak corpus support; neighboring papers discuss agentic AI and control-theoretic governance but not runtime safety monitor architectures for foundation models.
- Break condition: If safety monitors introduce latency exceeding real-time constraints (noted in conclusion as a tradeoff), or if novel failure modes evade both uncertainty quantification and physics checks, the fused architecture provides false confidence.

## Foundational Learning

- Concept: FMEA (Failure Modes and Effects Analysis) and Risk Priority Number (RPN)
  - Why needed here: HySAFE-AI extends FMEA for AI systems; the entire risk quantification depends on understanding RPN = Severity × Occurrence × Detection (1-10 scale each).
  - Quick check question: Given S=9, O=7, D=4, can you calculate RPN and explain why reducing D from 4 to 1 changes RPN from 252 to 63?

- Concept: Foundation Model Latent Space Operations
  - Why needed here: The identified failure modes (quantization-induced hallucinations, temporal reasoning failures) arise specifically from operations in compressed latent representations; understanding VAE encoders, diffusion denoising, and space-time factorized transformers is essential.
  - Quick check question: What happens to feature representation fidelity when a latent denoiser is quantized from FP16 to INT4, and why might this produce "plausible but non-existent objects"?

- Concept: ISO 26262 and ISO/PAS 8800 Safety Standards
  - Why needed here: HySAFE-AI is explicitly designed for compatibility with automotive functional safety standards; the framework extends rather than replaces these foundations.
  - Quick check question: Why does the paper state that IEC 61508 and ISO 26262 "lack AI-specific guidance," and what gap does ISO/PAS 8800 attempt to address?

## Architecture Onboarding

- Component map:
  Reference E2E Architecture: Multi-view camera input → Encoder (32× spatial, 8× temporal compression) → Latent Diffusion/U-Net backbone (with causal temporal attention) → Latent Denoiser (DDIM/flow matching) → Decoder (video reconstruction) → Trajectory Planner (MLP waypoints) + Text Conditioning (CLIP embeddings) throughout
  Fused Architecture adds: Policy Monitor (uncertainty quantification, OOD detection, consistency checks) + Safety Evaluator (physics-based trajectory validation) + Plan Arbitrator (trajectory selection logic)

- Critical path:
  Sensor input → Encoder → Latent space processing (where quantization-induced hallucinations and temporal reasoning failures originate) → Trajectory output → Safety evaluation chain (Policy Monitor AND Safety Evaluator must pass) → Arbitrator selection → Vehicle control

- Design tradeoffs:
  - Quantization (INT4/FP4) for deployment efficiency vs. precision loss causing hallucinations (RPN 252→63 after mitigation)
  - E2E unified architecture simplicity vs. opacity preventing traditional component-level FMEA
  - Safety monitor comprehensiveness vs. latency overhead in time-critical driving scenarios (explicitly noted as a tradeoff in conclusion)
  - Mixed-precision architecture complexity vs. uniform precision simplicity

- Failure signatures:
  - Quantization-induced hallucinations: Imaginary obstacles, phantom pedestrians (RPN 252, highest severity S=9, highest occurrence O=7)
  - Temporal reasoning failures: Mispredicted motion in adverse weather (RPN 252, detection difficulty D=4)
  - Dataset staleness: Missing new road participants like e-scooters (RPN 216, detection difficulty D=4)
  - Constraint adherence failures: Physically impossible maneuvers at highway speeds (RPN 150)

- First 3 experiments:
  1. Baseline FMEA on reference architecture: Document component-level failure modes using Table III template, assign S/O/D values based on expert judgment, calculate RPN for prioritization.
  2. Quantization sensitivity analysis: Compare hallucination rates and uncertainty calibration between FP16, INT8, and INT4 variants of the Latent Denoiser under OOD inputs.
  3. End-to-end latency budget measurement: Instrument the fused architecture to quantify computational overhead of Policy Monitor + Safety Evaluator + Arbitrator chain against real-time driving constraints (e.g., <100ms planning cycle).

## Open Questions the Paper Calls Out

- Question: How can the HySAFE-AI methodology be expanded from the provided partial illustration to a comprehensive analysis covering the full spectrum of end-to-end (E2E) autonomous driving failure modes?
- Basis in paper: [explicit] The authors list "comprehensive analysis of E2E ADS failure modes" as a primary objective for future work.
- Why unresolved: The current study acts as a case study using a partial FMEA/FTA illustration rather than an exhaustive evaluation of the complex E2E stack.
- What evidence would resolve it: A complete FMEA/FTA catalog covering all system components and a validation of the analysis against a wider range of driving scenarios.

- Question: To what extent does the introduction of run-time safety mechanisms (policy monitors, safety evaluators) in the fused architecture impact the latency and real-time performance required for safety-critical driving?
- Basis in paper: [inferred] The conclusion notes that the fused architecture introduces "computational overhead, potentially impacting latency, a critical factor in time-sensitive driving scenarios."
- Why unresolved: While the paper demonstrates a reduction in RPN, it does not quantify the trade-off regarding inference time or computational cost introduced by the added safety layers.
- What evidence would resolve it: Benchmarks comparing the inference latency and resource consumption of the baseline E2E model against the fused safety architecture.

- Question: How can the Risk Priority Number (RPN) assessments, currently derived from expert judgment, be validated or replaced with empirical, data-driven quantification methods?
- Basis in paper: [inferred] The paper states that "RPN values are based on expert judgment," introducing subjectivity into the severity, occurrence, and detection ratings.
- Why unresolved: Expert judgment provides a subjective baseline for risk, but objective validation is necessary to ensure the assigned risk values (e.g., Occurrence=7 for quantization hallucinations) reflect real-world statistical behavior.
- What evidence would resolve it: A correlation analysis comparing expert-assigned RPNs with actual failure rates observed during simulation or operational testing.

## Limitations
- Effectiveness depends on expert-assigned S/O/D values without transparent scoring rubrics, making reproducibility challenging
- Implementation details for safety monitors remain underspecified, particularly uncertainty quantification algorithms and physics-based validation rules
- Architectural transparency requirement assumes foundation models can be meaningfully decomposed into analyzable components, which may not hold for deeply entangled transformer architectures

## Confidence
- **High confidence**: The mathematical relationship RPN = S × O × D and the general concept of adding independent safety monitors to reduce detection difficulty
- **Medium confidence**: The AI-specific failure taxonomy and its mapping to FMEA guidewords, as these classifications appear systematic but lack extensive validation against real-world incident data
- **Medium confidence**: The fused architecture design with Policy Monitor, Safety Evaluator, and Arbitrator components, though specific implementation details remain unclear

## Next Checks
1. **RPN scoring reproducibility test**: Have three independent safety engineers assign S/O/D values for the same failure modes using only the information provided in the paper, then calculate inter-rater agreement and compare resulting RPN distributions.
2. **Safety monitor implementation validation**: Implement a minimal prototype of the Policy Monitor using uncertainty quantification methods (e.g., Monte Carlo dropout) and Safety Evaluator using simple physics constraints, then measure actual latency overhead and detection rates on synthetic failure cases.
3. **Dependency analysis**: Systematically test whether the claimed RPN reductions hold when failure modes exhibit statistical dependencies, particularly between quantization-induced hallucinations and temporal reasoning failures that may share common root causes in the latent space processing chain.