---
ver: rpa2
title: Customizing the Inductive Biases of Softmax Attention using Structured Matrices
arxiv_id: '2509.07963'
source_url: https://arxiv.org/abs/2509.07963
tags:
- attention
- width
- matrices
- scale
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes structured matrices to customize the inductive
  biases of softmax attention, addressing two limitations: low-rank bottlenecks and
  lack of distance-dependent compute bias. The authors introduce scoring functions
  based on high-rank Block Tensor-Train (BTT) and Multi-Level Low Rank (MLR) matrices,
  which improve performance on in-context regression tasks by mitigating information
  loss.'
---

# Customizing the Inductive Biases of Softmax Attention using Structured Matrices

## Quick Facts
- **arXiv ID:** 2509.07963
- **Source URL:** https://arxiv.org/abs/2509.07963
- **Reference count:** 40
- **Primary result:** Structured matrices (BTT, MLR) in attention resolve low-rank bottlenecks and introduce distance-dependent locality bias, improving performance on regression and language modeling.

## Executive Summary
This paper addresses two fundamental limitations of standard softmax attention: the low-rank bottleneck that compresses high-dimensional inputs, and the lack of distance-dependent compute bias that makes learning local patterns inefficient. The authors propose replacing the standard low-rank bilinear scoring matrix with structured alternatives - Block Tensor-Train (BTT) for high-rank expressivity and Multi-Level Low-Rank (MLR) for hierarchical locality. These modifications enable transformers to better handle high-dimensional features and data with strong local dependencies, while maintaining computational efficiency through structured matrix factorizations.

## Method Summary
The method replaces standard attention's scoring function s(x,x') = x^T W_Q W_K^T x' with structured matrices. For high-dimensional inputs, Bilinear BTT and Bilinear MLR use tensor decompositions to create full-rank scoring matrices while maintaining O(D^1.5) complexity. For locality-biased tasks, MLR attention computes scores as a hierarchical sum of low-rank matrices with varying block sizes, allocating higher rank to local interactions. The framework is unified under Multi-Level Block Tensor Contraction (MLBTC), which generalizes these approaches and allows architectural search for optimal inductive biases.

## Key Results
- BTT and Bilinear MLR resolve low-rank bottlenecks in in-context regression, achieving near-perfect performance when standard attention fails.
- MLR attention with hierarchical locality improves scaling laws on OpenWebText language modeling and ETT time-series forecasting.
- The unified MLBTC framework provides a design space for customizing attention inductive biases between expressivity and locality.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing the standard low-rank bilinear scoring matrix with Block Tensor Train (BTT) or Bilinear MLR structures resolves the information bottleneck for high-dimensional inputs.
- **Mechanism:** Standard attention computes scores via $W_Q W_K^T$, a rank-$r$ matrix. If input dimension $d_{input} > r$, information is lost. BTT and Bilinear MLR use structured decompositions to create high-rank (or full-rank) scoring matrices while maintaining $O(D^{1.5})$ or similar efficiency, allowing the model to process high-dimensional features without compression loss.
- **Core assumption:** The target task (e.g., in-context regression) relies on fine-grained, high-dimensional features that cannot be compressed into a low-dimensional subspace without performance degradation.
- **Evidence anchors:**
  - [Section 2.2] Identifies the low-rank bottleneck where transformers fail unless head dimension $r \approx d_{input}$.
  - [Section 4] Shows Bilinear BTT significantly outperforms standard attention on in-context regression for high-dimensional inputs.
  - [Corpus] "MonarchAttention" supports the viability of structured matrices for sub-quadratic attention, though focuses on efficiency over rank.
- **Break condition:** If the task features are naturally low-dimensional or compressible, the added complexity of BTT yields diminishing returns over standard attention.

### Mechanism 2
- **Claim:** Structuring the attention matrix as Multi-Level Low Rank (MLR) encodes a distance-dependent compute bias, improving efficiency on data with locality.
- **Mechanism:** MLR attention sums multiple low-rank matrices with varying block sizes. Fine-grained blocks capture local interactions (high rank allocated to neighbors), while coarse blocks capture global interactions (low rank allocated to distant tokens). This mimics the efficiency of sliding windows while retaining global context.
- **Core assumption:** The data exhibits locality (e.g., language, time-series), where local token dependencies are more complex and relevant than long-range ones.
- **Evidence anchors:**
  - [Section 3.4] Equation 9 defines the scoring function where rank accumulates for tokens in closer proximity ($d(j,j')$).
  - [Figure 4b] Shows MLR attention outperforms Sliding Window Attention (SWA) on OpenWebText by allowing graded, hierarchical interactions rather than hard cutoffs.
  - [Corpus] "Hierarchical Self-Attention" parallels this, addressing multi-scale problems, though via different mechanics.
- **Break condition:** If a task requires dense, uniform high-resolution reasoning across all token ranges (e.g., certain algorithmic tasks or global retrieval), the low-rank approximation for distant pairs may drop critical information.

### Mechanism 3
- **Claim:** The Multi-Level Block Tensor Contraction (MLBTC) framework unifies these approaches, allowing customizable trade-offs between expressivity (rank) and efficiency (compute).
- **Mechanism:** MLBTC generalizes the tensor contraction orders, proving that both BTT (for rank) and MLR (for locality) are specific instances of a broader structured matrix family. This allows architectural search to find the optimal inductive bias.
- **Core assumption:** There is no single "optimal" attention mechanism; the structure must match the data geometry.
- **Evidence anchors:**
  - [Section 3.3] Definition 3.1 formally defines MLBTC and shows how it subsumes BTT and MLR.
  - [Abstract] Highlights that this customization addresses significant shortcomings of standard attention.

## Foundational Learning

- **Concept: Bilinear Forms & Rank**
  - **Why needed here:** Standard attention is $x^T (W_Q W_K^T) y$. The rank of the matrix $W_Q W_K^T$ limits the expressivity of the interaction. To understand the paper, you must see attention not just as "Query-Key dot product" but as a constrained matrix operation.
  - **Quick check question:** If a head dimension is 64 but the input dimension is 1024, what is the maximum rank of the standard attention score matrix?

- **Concept: Structured Matrix Factorization**
  - **Why needed here:** The core proposal replaces dense matrices with decompositions (Tensor Trains, Low-Rank blocks). You need to distinguish between "sparse" (zeros) and "structured" (efficient factorization with high rank).
  - **Quick check question:** How can a matrix be "full rank" in terms of its output space but "low parameter" in terms of storage?

- **Concept: Inductive Bias (Locality)**
  - **Why needed here:** The paper argues that standard attention is agnostic to token distance. You need to understand why forcing the model to learn local patterns from scratch is inefficient compared to baking it in via architecture (MLR).
  - **Quick check question:** Why does a "sliding window" (sparse) differ from "distance-dependent compute" (MLR)?

## Architecture Onboarding

- **Component map:**
  - **Input:** Sequence $X$.
  - **Standard:** $W_Q, W_K$ project to low rank $r$.
  - **Proposed (Bilinear):** Replace $W_Q, W_K$ with structured factors (e.g., $L_k, R_k$ in BTT/MLR) defined in Eq 6/7.
  - **Proposed (Distance Bias):** Compute score $S$ via hierarchical summation (Eq 9) rather than a single dot product.
  - **Output:** Softmax($S$) $X$.

- **Critical path:**
  1.  **Implementation:** Efficient tensor contraction is non-trivial. Reference Appendix D for optimal contraction orders (e.g., Eq 32 for MLR).
  2.  **Stability:** This architecture is sensitive to initialization. Use **Maximal Update Parameterization (µP)** (Section 3.5, Appendix E) for stable feature learning across widths.
  3.  **Normalization:** Apply QK LayerNorm (Appendix E.2) to prevent instability in the structured projections.

- **Design tradeoffs:**
  - **BTT vs. MLR:** Use **Bilinear BTT** for tasks requiring maximum expressivity with high-dimensional inputs (e.g., scientific computing, regression). Use **MLR Attention** for tasks with strong locality patterns (e.g., language, time-series forecasting).
  - **Efficiency:** MLR reduces KV cache size for autoregressive generation (Section 3.4), making it more viable for inference than full-rank dense attention.

- **Failure signatures:**
  - **Training Divergence:** Likely due to initializing structured weights without µP scaling or missing QK LayerNorm.
  - **Slower than Standard:** Using naive matrix multiplication instead of the optimized contraction orders in Appendix D.
  - **Regression Failure:** If using standard attention on high-dim inputs, check if the error stays flat (bottleneck). If using BTT/MLR, check if the rank parameters are effectively restricted.

- **First 3 experiments:**
  1.  **Rank Bottleneck Validation:** Replicate the in-context regression task (Figure 3a) comparing 8-head standard attention vs. Bilinear BTT to verify the information bottleneck resolution on high-dim inputs.
  2.  **Locality Scaling:** Train a small language model on OpenWebText (Figure 4) comparing Standard vs. MLR (8 levels) vs. Sliding Window to validate the scaling laws and locality bias.
  3.  **Stability Test:** Sweep learning rates for a wide MLR model (width 1024) with and without µP (Figure 6) to confirm stable hyperparameter transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the generalized Multi-Level Block Tensor Contraction (MLBTC) class provide superior inductive biases or efficiency compared to its specific sub-cases (BTT and MLR)?
- **Basis in paper:** [explicit] The authors introduce MLBTC in Section 3.3 to unify BTT and MLR, stating, "We hope future work will explore the benefits and inductive biases of MLBTC’s greater flexibility in attention and elsewhere."
- **Why unresolved:** The paper restricts experiments to BTT and MLR matrices, leaving the properties of the broader MLBTC family unexplored.
- **What evidence would resolve it:** Empirical comparisons of task performance and training speed between specific MLBTC configurations and the BTT/MLR baselines.

### Open Question 2
- **Question:** Does applying structured matrices to the value-output projection ($W_V W_O^\top$) yield performance improvements similar to those observed in the scoring function?
- **Basis in paper:** [explicit] The discussion notes, "Similar techniques could be applied to the other part of each attention head, defined by the low-rank matrix $W_V W_O^\top$."
- **Why unresolved:** The study focuses exclusively on modifying the attention scoring function, leaving the value path untouched.
- **What evidence would resolve it:** Experiments replacing the dense value projection with structured variants (e.g., MLR) and measuring the impact on regression or language modeling loss.

### Open Question 3
- **Question:** Does using different structured matrix types for different attention heads promote a better division of labor and improve overall model accuracy?
- **Basis in paper:** [explicit] The authors suggest that "different structures could be used for different heads... to promote a better division of labor."
- **Why unresolved:** The implemented models use a homogeneous structure (e.g., all heads use MLR) rather than a heterogeneous mixture.
- **What evidence would resolve it:** Ablation studies comparing models with mixed head structures (e.g., some BTT, some MLR) against models with uniform head structures.

## Limitations
- Narrow empirical scope: validation limited to synthetic regression and two real-world tasks (language, time-series)
- Efficiency claims depend on optimal tensor contraction ordering that may not be hardware-portable
- Does not address multimodal data, non-autoregressive generation, or tasks requiring dense global reasoning

## Confidence
- **High Confidence:** Low-rank bottleneck identification and resolution through BTT/MLR structures; theoretical unification under MLBTC
- **Medium Confidence:** Locality bias claims for MLR attention; need ablation studies to isolate distance-dependent computation effects
- **Medium Confidence:** Efficiency improvements; FLOPs analysis provided but actual wall-clock comparisons absent

## Next Checks
1. **Cross-Domain Generalization:** Test BTT and MLR attention on a multimodal task (e.g., image-text alignment) to verify the low-rank bottleneck resolution extends beyond synthetic regression to real-world high-dimensional features.

2. **Locality Ablation:** Create a controlled experiment where MLR attention is modified to remove the distance-dependent compute bias while maintaining the same FLOPs budget. Compare performance on language modeling to isolate the effect of hierarchical locality.

3. **Hardware Portability:** Implement MLR attention using standard PyTorch operations (avoiding specialized tensor contraction libraries) and measure the actual training/inference speed on both GPU and CPU to validate the practical efficiency claims.