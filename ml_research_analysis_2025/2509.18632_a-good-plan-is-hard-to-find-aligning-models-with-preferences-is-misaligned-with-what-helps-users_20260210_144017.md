---
ver: rpa2
title: 'A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned
  with What Helps Users'
arxiv_id: '2509.18632'
source_url: https://arxiv.org/abs/2509.18632
tags:
- users
- plans
- user
- plan
- helpful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that user preferences reflect
  helpfulness in LLM-generated plans. It introduces Planorama, an interface where
  126 users solve 300 multi-step questions with LLM plans, collecting 4388 executions
  and 5584 comparisons.
---

# A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users

## Quick Facts
- arXiv ID: 2509.18632
- Source URL: https://arxiv.org/abs/2509.18632
- Reference count: 40
- User preferences poorly predict which LLM-generated plans actually help users complete tasks

## Executive Summary
This paper challenges the fundamental assumption in alignment research that user preferences reflect helpfulness when evaluating LLM-generated plans. Through Planorama, an interface where 126 users solve 300 multi-step questions with LLM plans, the study collected 4388 execution traces and 5584 preference comparisons. Using Item Response Theory to measure actual helpfulness (accuracy + time), the results show that preferred plans and helpful plans poorly align - user/model preferences predict helpfulness with accuracy below 63%. This gap cannot be explained by individual user variance and is driven by surface-level cues like brevity and question similarity that predict preferences but not actual utility. The findings suggest that aligning LLMs with helpfulness requires feedback from real user interactions rather than just preferences.

## Method Summary
The study deployed Planorama, a web interface where users solve multi-step questions (150 GSM8k math + 150 MuSiQue/MQuAKE trivia) using generated plans. Researchers created 600 diverse plans (2 per question) using GPT-4o, Claude-3 Opus, Qwen-2 72B, and LLaMA-3 405B. Users executed these plans while researchers collected traces and preference comparisons. IRT (Bayesian 2PL accuracy model + log-normal time model, NUTS sampling) computed plan helpfulness controlling for user skill. ReACT agents executed plans for model evaluations, and 6 reward models plus GPT-4o-as-judge provided preference signals. The critical analysis compared agreement between preference proxies and IRT-based helpfulness.

## Key Results
- User/model preferences predict helpfulness with accuracy <63%, near random baseline
- Plan features like brevity and question-plan similarity predict preferences (R² > 0.12) but not helpfulness (R² ≈ 0)
- Execution errors dominate plan failures, showing correctness doesn't ensure helpfulness
- Users perform similarly on preferred vs. dispreferred plans, ruling out individual variance

## Why This Works (Mechanism)

### Mechanism 1: Preference-Helpfulness Decoupling
Standard preference signals poorly predict task success because preferences aggregate into a signal shaped by surface-level heuristics rather than execution-grounded utility. Reward models trained on such preferences reinforce these biases. The core assumption is that users cannot accurately forecast which plan will help them before execution.

### Mechanism 2: Surface Cue Bias in Preference Formation
Preferences are driven by shallow features unrelated to helpfulness, including inverse verbosity bias (preferring shorter plans), question-plan similarity bias, and structure bias. Users and models exhibit these biases through cognitive shortcuts rather than task-relevant judgment.

### Mechanism 3: Execution Errors Dominate Plan Failures
Correct plans can still be unhelpful due to ambiguity, execution complexity, or user/tool errors. This means verifiable reward signals like correctness are insufficient for alignment - helpfulness requires signals beyond propositional correctness such as clarity, simplicity, and engagement.

## Foundational Learning

- **Concept: Item Response Theory (IRT)**
  - Why needed here: IRT disentangles plan helpfulness from user skill, enabling fair comparison of plan quality across heterogeneous users
  - Quick check question: Why would averaging accuracy/time across users conflate plan quality with user skill?

- **Concept: Preference vs. Ground-Truth Feedback in RLHF**
  - Why needed here: This paper critiques the core RLHF assumption that preferences proxy helpfulness. Understanding RLHF's feedback loop clarifies why misalignment propagates
  - Quick check question: What happens to a model trained via RLHF if user preferences systematically diverge from task success?

- **Concept: ReACT Agent Framework**
  - Why needed here: ReACT provides a model-based proxy for plan helpfulness; understanding its reasoning-acting-observing loop is critical for interpreting GPT-Helpful results
  - Quick check question: How does ReACT differ from directly prompting an LLM for answers without intermediate steps?

## Architecture Onboarding

- **Component map:**
  Planorama Interface -> IRT Helpfulness Scorer -> ReACT Agent -> Reward Models
  (Question display -> Bayesian inference -> GPT-4o executor -> 6 RMs + GPT-4o-as-judge)

- **Critical path:** Generate diverse plan pairs → Deploy via Planorama → Collect execution traces and comparisons → Compute IRT helpfulness → Compare agreement between preferences and helpfulness

- **Design tradeoffs:**
  - IRT controls for skill but requires sufficient data per user (≥8 executions/plan in this study)
  - Random plan assignment avoids confounds but prevents same-user comparison of both plans for a question
  - ReACT search truncation (first paragraph + top-5 sentences) balances context length vs. information

- **Failure signatures:**
  - Preference-helpfulness agreement near 50% (random) indicates proxy failure
  - High RM agreement with user preference but low agreement with helpfulness suggests adversarial helpfulness
  - Users succeeding equally on preferred/dispreferred plans rules out individual variance as explanation

- **First 3 experiments:**
  1. Replicate with domain experts: Test if preference-helpfulness gap shrinks for users with high task familiarity
  2. Train RM on execution outcomes: Fine-tune a reward model on IRT-derived helpfulness scores rather than preferences; evaluate alignment
  3. Ablate surface features: Generate plans controlling for brevity and question overlap; measure if preference-helpfulness gap narrows

## Open Questions the Paper Calls Out

- **Does the misalignment between user preferences and actual helpfulness persist in subjective, hard-to-verify domains like creative writing or coding?**
  - The authors state in the Limitations the need to "develop protocols for measuring response helpfulness in harder-to-verify tasks such as writing" to confirm if the observed misalignment is a general issue. The current study is restricted to verifiable math and trivia; subjective tasks lack binary success metrics (accuracy/speed).

- **Can LLM agents utilizing persona-based prompting or improved architectures accurately simulate human execution errors to predict plan helpfulness?**
  - Section 10 asks if "persona-based prompting with LLM agents improves simulations for predicting which responses best helps users." Current agents (ReACT) fail to predict human success; improving simulation fidelity could reduce the high cost of collecting real user interaction data.

- **Do reward models fine-tuned on objective helpfulness signals predict user success better than standard preference-trained reward models?**
  - Section 10 suggests examining "if RMs fine-tuned on helpfulness can generalize across domains." Standard RMs prioritize surface-level preferences (brevity) rather than execution utility, leading to adversarial helpfulness.

## Limitations

- The study uses artificial tasks rather than real-world planning scenarios where context and stakes differ
- 126 participants may not represent diverse user populations; recruitment details remain unspecified
- Reliance on ReACT agents assumes faithful simulation of human execution behavior
- IRT framework requires sufficient data per user that may not generalize to sparse feedback scenarios

## Confidence

- High confidence: The empirical finding that preference signals poorly predict helpfulness (agreement <63%) is robust given the large sample size (5584 comparisons, 4388 executions) and convergent evidence across multiple preference proxies
- Medium confidence: The attribution of preference-helpfulness decoupling to surface-level heuristics is supported by regression analysis but relies on the assumption that these features fully capture the bias mechanism
- Low confidence: The generalizability of findings to real-world planning scenarios where users have longer-term goals, multiple dependencies, and varying levels of domain expertise

## Next Checks

1. **Domain Expert Replication**: Test whether the preference-helpfulness gap persists for expert users solving domain-specific planning tasks (e.g., software development planning, medical treatment planning) where task familiarity is high

2. **Reward Model Retraining**: Fine-tune a reward model directly on the IRT-derived helpfulness scores rather than preference comparisons. Measure whether this model better predicts user success than preference-based RMs

3. **Longitudinal Deployment Study**: Deploy the Planorama interface in a real-world setting (e.g., educational course planning tool) for one month. Compare preference-helpfulness alignment before and after users gain experience with the tool