---
ver: rpa2
title: The Material Contracts Corpus
arxiv_id: '2504.02864'
source_url: https://arxiv.org/abs/2504.02864
tags:
- contracts
- contract
- agreements
- agreement
- parties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Material Contracts Corpus (MCC) is a publicly available dataset
  containing over one million contracts filed with the SEC between 2000-2023. The
  corpus categorizes contracts by agreement type (e.g., employment, security, M&A)
  using a fine-tuned LLaMA-2 model achieving 95% accuracy, and links contracts to
  specific parties.
---

# The Material Contracts Corpus

## Quick Facts
- arXiv ID: 2504.02864
- Source URL: https://arxiv.org/abs/2504.02864
- Reference count: 8
- The Material Contracts Corpus (MCC) contains over one million contracts filed with the SEC between 2000-2023

## Executive Summary
The Material Contracts Corpus (MCC) is a publicly available dataset containing over one million contracts filed with the SEC between 2000-2023. The corpus categorizes contracts by agreement type (e.g., employment, security, M&A) using a fine-tuned LLaMA-2 model achieving 95% accuracy, and links contracts to specific parties. The dataset enables empirical research on contract design and legal language trends. Key findings show employment and security agreements dominate SEC filings, contract lengths and readability complexity have increased over time, and financial institutions appear most frequently as contract parties. The MCC is accessible for bulk download and online search at https://mcc.law.stanford.edu/.

## Method Summary
The MCC is constructed from 3.5 million EDGAR filings, filtering for exhibits 2, 10, and 99 with agreement-related descriptions, yielding 1.25 million filed contracts. After deduplication by URL, 1.04 million unique contracts remain. Agreement types are classified using a fine-tuned LLaMA-2 7B model with LoRA parameter-efficient fine-tuning on 1,993 hand-labeled contracts, achieving 95% accuracy. Party identification uses a RoBERTa-based NER model followed by fuzzy string matching and acronym matching to link entity variants. Document types (amendment, restatement, joinder, termination) are classified using keyword matching.

## Key Results
- Fine-tuned LLaMA-2 model achieves 95% accuracy and 0.95 weighted F1 for contract classification
- Entity linking pipeline achieves 100% recall and 96% precision for party identification
- Employment and security agreements comprise the largest share of SEC filings; contract lengths and readability complexity have increased over time

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a relatively small LLM (7B parameters) with LoRA achieves high classification accuracy on contract types. The authors add a classification layer atop a pre-trained LLaMA-2 7B model and use Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning on 1,993 hand-labeled contracts. The model outputs scores for 8 agreement categories; the highest score is selected. Five-fold cross-validation yielded 95% accuracy and 0.95 weighted F1.

### Mechanism 2
Named Entity Recognition followed by fuzzy string matching enables scalable party identification and linking across contract variants. A RoBERTa-based NER model extracts organization/individual entities; generic terms (e.g., "board of directors") are filtered. Fuzzy string matching (Levenshtein ratio) and acronym matching group variant names (e.g., "JPMC" ≈ "JP Morgan Chase"). Transitivity links entity sets. Manual review filters false matches.

### Mechanism 3
Simple keyword matching outperforms fine-tuned LLMs for binary document-type classification (amendment, restatement, joinder, termination). Keyword matching on initial contract text classifies documents into four binary categories. The authors report that an LLM "failed to achieve similar performance" on this task.

## Foundational Learning

- **SEC EDGAR structure and exhibit codes**: Understanding the source data governance is critical for filtering and interpreting metadata. Quick check: Which exhibit codes indicate "material contracts" and why might exhibit 99 require additional filtering?
- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**: The classification model uses LoRA to adapt a 7B-parameter LLM with limited compute. Quick check: What is the trade-off between LoRA rank and representation capacity?
- **Named Entity Recognition (NER) and entity normalization**: Party identification depends on extracting and linking named entities across documents. Quick check: Why might high recall be prioritized over precision in this pipeline?

## Architecture Onboarding

- **Component map**: Data Ingestion: EDGAR filings → exhibit extraction (codes 2, 10, filtered 99) → 1.25M filed contracts → deduplication by URL (1.04M unique) → Classification: LLaMA-2 7B + LoRA (1,024 tokens) → 8-category agreement type; keyword matcher → 4 binary document types → Entity Pipeline: RoBERTa NER → filter → fuzzy/acronym linking → transitive entity groups → manual review → Metadata: Filing form, file type (.htm, .txt, .pdf), amendment status, year, SIC code, state of incorporation
- **Critical path**: Accurate exhibit filtering → reliable classification → party linking → downstream research use. Errors in exhibit selection propagate through all analyses.
- **Design tradeoffs**: No text-based deduplication beyond URL; statistics use 1.25M filed contracts to preserve filing metadata; LLM classifier limited to 1,024 tokens; longer contracts truncated; Entity linking thresholds set manually; not validated on held-out data
- **Failure signatures**: Classification F1 drops for rare categories ("Other": 0.79) → may require stratified sampling or additional labels; 4% of contracts include extraneous entities; 1.2% of tags are false positives → acceptable for search, problematic for precise counts; 2023 data incomplete (collection ended Q1) → year-over-year comparisons require normalization
- **First 3 experiments**: 1) Baseline validation: Sample 100 random contracts; manually verify classification, party extraction, and document-type labels against ground truth. 2) Token-window sensitivity: Classify a subset of contracts using first 512 vs. 1,024 tokens to assess truncation impact. 3) Entity linking precision audit: Manually review 50 high-frequency entity groups (e.g., top 10 parties) for false merges; adjust fuzzy threshold if error rate exceeds 5%.

## Open Questions the Paper Calls Out

- **Open Question 1**: How would comprehensive text-based deduplication alter the composition and trends of the corpus compared to the current URL-based method? The authors state that the corpus represents partial deduplication because "there is no further de-duplication based on the text of the agreement."
- **Open Question 2**: Which specific contractual provisions or sections are driving the observed increase in document length and readability complexity? The paper documents a trend of increasing length and Flesch-Kincaid grade levels but does not perform clause-level analysis to identify the structural causes.
- **Open Question 3**: Can the classification accuracy for the "Other" category be improved by expanding the taxonomy beyond the eight broad labels? The fine-tuned model achieves a lower F1 score (0.79) for the "Other" category, which the authors hypothesize contains "unique" contracts unsuitable for broad grouping.

## Limitations
- Token truncation impact: The LLaMA-2 classifier is limited to 1,024 tokens, potentially reducing accuracy for complex agreements
- Manual review limitations: Entity linking relied on manual review for only a sample of 100 contracts
- 2023 data incompleteness: Collection ended in Q1 2023, making year-over-year comparisons for that year unreliable

## Confidence
- Contract classification accuracy (95% accuracy, 0.95 F1): High confidence
- Entity extraction recall (100%) and precision (96%): Medium confidence
- Trend analyses (increasing contract length/readability, party frequency): Medium confidence

## Next Checks
1. Classification robustness test: Randomly sample 100 contracts from the full corpus and manually verify agreement type classifications against ground truth to confirm the 95% accuracy holds in production
2. Token window sensitivity analysis: Classify a subset of contracts using first 512 vs. 1,024 tokens to quantify the impact of truncation on agreement type detection
3. Entity linking precision audit: Manually review the top 10 most frequent entity groups (e.g., financial institutions) for false merges, adjusting fuzzy matching thresholds if error rate exceeds 5%