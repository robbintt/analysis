---
ver: rpa2
title: Expanding Foundational Language Capabilities in Open-Source LLMs through a
  Korean Case Study
arxiv_id: '2509.03972'
source_url: https://arxiv.org/abs/2509.03972
tags:
- korean
- language
- arxiv
- llama-3-motif
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Llama-3-Motif, a 102 billion parameter language
  model specifically designed to enhance Korean language capabilities while maintaining
  strong English performance. The model expands on the Llama 3 architecture using
  advanced training techniques including LlamaPro for depth expansion and Masked Structure
  Growth for width expansion, enabling scalable model growth without altering the
  core Transformer architecture.
---

# Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study

## Quick Facts
- arXiv ID: 2509.03972
- Source URL: https://arxiv.org/abs/2509.03972
- Reference count: 19
- Key outcome: Llama-3-Motif achieves KMMLU score of 64.74, outperforming existing Korean specialized models by 9-40% and achieving results comparable to GPT-4

## Executive Summary
This paper presents Llama-3-Motif, a 102 billion parameter language model specifically designed to enhance Korean language capabilities while maintaining strong English performance. The model expands on the Llama 3 architecture using advanced training techniques including LlamaPro for depth expansion and Masked Structure Growth for width expansion. Trained using the MoAI platform across GPU clusters with a carefully curated dataset containing approximately 194 billion tokens balanced between Korean and English content, Llama-3-Motif demonstrates effective scaling of language models for less-resourced languages while providing insights into efficient model scaling and advanced AI infrastructure utilization.

## Method Summary
The model expands Llama-3-70B through LlamaPro depth expansion (adding 20% more layers) and Masked Structure Growth (MSG) width expansion, yielding ~102B parameters. Pre-training uses 194B tokens with a 9:1 Korean-to-English ratio, followed by SFT with NEFTune and KTO alignment. The training infrastructure utilizes the MoAI platform on AMD MI250 GPUs. Data undergoes aggressive filtering, removing 83% of samples but only 40% of text by volume, indicating strict quality control.

## Key Results
- KMMLU score of 64.74, outperforming existing Korean specialized models by 9-40% and achieving results comparable to GPT-4
- KorMedMCQA benchmark score of 83.34 for medical knowledge
- Demonstrated effective cross-lingual scaling while preserving English capabilities
- Successfully mitigated KTO-induced toxicity through logit caching strategy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Progressive model scaling via depth and width expansion allows parameter growth (70B to 102B) while preserving the functional knowledge of the base model.
- **Mechanism:** The system utilizes LlamaPro to insert new layers (depth) and Masked Structure Growth (MSG) to widen hidden dimensions. Crucially, MSG uses a masking mechanism to neutralize new neurons initially, preventing them from disrupting the base model's forward pass, and gradually introduces them during training.
- **Core assumption:** The base model's weights contain transferable representations that remain valid even as the architecture changes, provided the new parameters are integrated gradually.
- **Evidence anchors:** [section] "The expanded model was initialized by implementing LlamaPro’s depth expansion... followed by the width expansion facilitated by MSG which utilizes a masking mechanism... Initially, the newly added parameters are masked." (Page 2)
- **Break condition:** If the unmasking rate of new parameters is too fast relative to the learning rate, the model suffers catastrophic forgetting of the base English capabilities.

### Mechanism 2
- **Claim:** A high-ratio (9:1) target-language-to-source-language data composition drives domain adaptation while retaining cross-lingual reasoning.
- **Mechanism:** By flooding the continual pre-training phase with 194 billion tokens skewed heavily toward Korean (filtered for quality), the model is forced to allocate representational capacity to Korean syntax and semantics, while the 10% English data and base weights act as a regularizer to maintain original proficiency.
- **Core assumption:** The model has sufficient excess capacity (thanks to the 102B scale) to learn the new language distribution without overwriting the existing English distribution.
- **Evidence anchors:** [section] "The dataset maintained a carefully calibrated ratio of approximately 9:1 between Korean and English content." (Page 3)
- **Break condition:** If the English data ratio drops too low or the Korean data quality is insufficient (noise), the model may experience "language drift," where English syntactic structures degrade.

### Mechanism 3
- **Claim:** Kahneman-Tversky Optimization (KTO) enables effective alignment in low-resource languages by removing the dependency on hard-to-collect paired preference data.
- **Mechanism:** Unlike Direct Preference Optimization (DPO) which requires paired "chosen/rejected" outputs, KTO uses unpaired binary signals (desirable/undesirable). This widens the pool of usable training data, which is critical for languages where high-quality annotation is expensive.
- **Core assumption:** The binary signal of "desirability" provides a sufficient gradient for the model to converge on human-aligned behavior without the relative ranking used in DPO.
- **Evidence anchors:** [section] "KTO uses unpaired preference data with a binary signal... unlike DPO that learns them in pairs, which makes data collection more feasible." (Page 5)
- **Break condition:** If the "desirable" threshold is not calibrated correctly, the model optimizes for the proxy signal (e.g., length or politeness markers) rather than actual helpfulness or truthfulness.

## Foundational Learning

- **Concept: Masked Structure Growth (MSG) / Network Expansion**
  - **Why needed here:** The paper relies on expanding a 70B model to 102B. You cannot simply append random weights; you must understand how to initialize new parameters so they don't destroy the model's existing function.
  - **Quick check question:** If I add a new layer to a Transformer, should I initialize it to zero or to the mean of the previous layers to preserve the initial output distribution?

- **Concept: Preference Optimization (DPO vs. KTO)**
  - **Why needed here:** The paper selects KTO over the standard DPO. Understanding the difference between paired preferences (DPO) and binary outcomes (KTO) is necessary to debug alignment quality.
  - **Quick check question:** Can I train a model with DPO if my dataset only contains "good" responses and no "bad" responses for a specific prompt?

- **Concept: Continual Pre-training**
  - **Why needed here:** The model is not trained from scratch; it is adapted from Llama 3. Understanding the trade-offs between learning rate warmup and data mixing ratios is vital to prevent forgetting.
  - **Quick check question:** When continuing training on a new language, should the learning rate be the same as the pre-training peak LR, or significantly lower?

## Architecture Onboarding

- **Component map:** Base: Llama-3-70B (Transformer) -> Expansion: +26B parameters via LlamaPro (Depth) + MSG (Width) -> Pre-training: 194B token corpus (9:1 Korean:English) on MoAI Platform (AMD MI250) -> Post-training: SFT (NEFTune) -> Alignment (KTO)

- **Critical path:**
  1. Initialize Llama-3-70B
  2. Apply LlamaPro to expand depth (layers)
  3. Apply MSG to expand width (hidden/FFN dimensions) with masking
  4. Train on Korean/English mix (Pre-training)
  5. Apply NEFTune + KTO (Alignment)

- **Design tradeoffs:**
  - **Data Filtering:** Aggressive filtering removed 83% of samples but only 40% of text, implying strict quality control at the cost of discarding diverse but lower-quality web text
  - **Alignment Method:** Chose KTO over DPO/PPO to save data collection costs, accepting the risk of "naïve tuning" increasing toxicity (which they mitigated via hyperparameter caching)

- **Failure signatures:**
  - **Toxicity Spike:** The authors explicitly note that naïve KTO tuning increased toxicity
  - **English Degradation:** A known risk when using a 9:1 non-English/English ratio; must monitor English benchmarks during training

- **First 3 experiments:**
  1. **Expansion Ablation:** Implement depth expansion only vs. width expansion only on a small proxy model (e.g., Qwen 1.8B) to verify the claimed performance delta before scaling to 70B
  2. **Data Ratio Stress Test:** Train the expanded model on 10:0 vs. 9:1 vs. 8:2 Korean/English ratios to verify the "English retention" assumption does not collapse
  3. **Alignment Safety Check:** Run KTO with and without the specific "logit caching" trick mentioned to reproduce the toxicity issue and confirm the mitigation strategy works

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the LlamaPro + Masked Structure Growth (MSG) expansion methodology transfer effectively to other mid-sized base models (e.g., Qwen-72B, Mistral) beyond the Llama-3-70B family?
- **Basis in paper:** [inferred] The authors selected LlamaPro and MSG based on preliminary experiments using Qwen 1.8B (Section 2), a 1.8B parameter model, to guide decisions for scaling a 70B model to 102B. This 50× scale gap raises questions about whether findings from small-scale experiments generalize.
- **Why unresolved:** The paper does not validate whether the expansion methodology performs similarly when applied to different architectural families or whether the small-scale proxy experiments were representative of large-scale behavior.
- **What evidence would resolve it:** Comparative experiments applying the same expansion methodology to other 70B-class base models and reporting resulting benchmark performance and training stability.

### Open Question 2
- **Question:** What is the relationship between Korean-to-English pre-training data ratios and the Korean-English performance trade-off curve?
- **Basis in paper:** [inferred] The paper states a "carefully calibrated ratio of approximately 9:1" (Section 4.1) without providing ablation studies or justification for this specific ratio.
- **Why unresolved:** The authors do not explore whether this ratio is optimal or how different ratios affect both Korean performance gains and English capability preservation. Other Korean LLM papers (e.g., Hyperclova X) may use different ratios.
- **What evidence would resolve it:** Ablation experiments training models with different Korean-to-English ratios (e.g., 7:3, 8:2, 9:1, 10:0) and evaluating both KMMLU and standard English benchmarks.

### Open Question 3
- **Question:** Can the 20% depth expansion + width expansion strategy generalize to other under-resourced languages with similar training data constraints?
- **Basis in paper:** [explicit] The conclusion states the work "provides valuable insights that can be leveraged in the development of similar models for other under-resourced languages."
- **Why unresolved:** The methodology is demonstrated only for Korean, which has unique characteristics (agglutinative morphology, SOV word order, large web corpus available). It remains unclear whether similar expansion ratios and training data volumes would suffice for truly low-resource languages with limited web data.
- **What evidence would resolve it:** Replication studies applying the same expansion methodology to other mid-to-low resource languages (e.g., Vietnamese, Thai, or Swahili) with comparable data budgets.

### Open Question 4
- **Question:** How does the observed KTO-induced toxicity increase manifest across different hyperparameter settings, and are there more robust mitigation strategies?
- **Basis in paper:** [inferred] The authors note "naïve hyperparameter tuning with KTO resulted in a notable increase in toxicity" (Section 5) but address it only via caching policy model logits without quantifying the problem or exploring alternative solutions.
- **Why unresolved:** The toxicity increase is mentioned but not measured, characterized, or compared across hyperparameter configurations. The caching solution addresses computational efficiency but may not address the underlying cause of toxicity amplification.
- **What evidence would resolve it:** Systematic evaluation of toxicity metrics (e.g., REALTOXICITYPROMPTS, Korean-specific toxicity benchmarks) across different KTO hyperparameter settings and comparison with alternative alignment methods.

## Limitations

- The paper does not provide comprehensive English benchmark results beyond KMMLU, creating uncertainty about full English capability preservation
- Critical training hyperparameters (learning rates, batch sizes, training steps) are not specified, limiting reproducibility
- The toxicity mitigation strategy via logit caching is mentioned but not quantitatively validated

## Confidence

**High Confidence**: The architectural innovations (LlamaPro and MSG) are technically sound and the expansion from 70B to 102B parameters is verifiable through model architecture inspection. The dataset curation approach (9:1 ratio, aggressive filtering) is clearly described and reproducible.

**Medium Confidence**: The KMMLU score of 64.74 and its comparison to other Korean models appears methodologically sound, but the absence of standard English benchmark results limits confidence in the cross-lingual claims. The toxicity mitigation through logit caching is plausible but insufficiently validated.

**Low Confidence**: The KTO alignment methodology's effectiveness is questionable given the minimal detail on preference data collection and the acknowledged toxicity issues. Without paired preference data and only binary signals, the quality of alignment in low-resource languages remains uncertain.

## Next Checks

1. **English Capability Preservation Test**: Run Llama-3-Motif on standard English benchmarks (MMLU, BIG-bench) alongside KMMLU to empirically verify that the 9:1 Korean-to-English pre-training ratio does not degrade English performance. This directly tests the "maintaining strong English performance" claim.

2. **KTO Safety Validation**: Implement a controlled experiment comparing KTO with and without logit caching across multiple toxicity evaluation frameworks (RealToxicityPrompts, Perspective API) to quantify the actual reduction in toxic outputs and validate the mitigation strategy's effectiveness.

3. **Architecture Scaling Ablation**: Create and evaluate depth-only and width-only expansion variants of Llama-3-70B on Korean language tasks to determine the individual contribution of LlamaPro versus MSG to the final 9-40% performance improvement over existing models.