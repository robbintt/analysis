---
ver: rpa2
title: 'Fara-7B: An Efficient Agentic Model for Computer Use'
arxiv_id: '2511.19663'
source_url: https://arxiv.org/abs/2511.19663
tags:
- fara-7b
- tasks
- task
- data
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fara-7B is a small, native computer-use agent that achieves strong
  performance on web-based tasks despite its compact size. It was trained using FaraGen,
  a synthetic data generation system that produces verified trajectories for multi-step
  web tasks at low cost.
---

# Fara-7B: An Efficient Agentic Model for Computer Use

## Quick Facts
- arXiv ID: 2511.19663
- Source URL: https://arxiv.org/abs/2511.19663
- Authors: Ahmed Awadallah; Yash Lara; Raghav Magazine; Hussein Mozannar; Akshay Nambi; Yash Pandya; Aravind Rajeswaran; Corby Rosset; Alexey Taymanov; Vibhav Vineet; Spencer Whitehead; Andrew Zhao
- Reference count: 19
- Fara-7B is a small, native computer-use agent that achieves strong performance on web-based tasks despite its compact size

## Executive Summary
Fara-7B is a 7B parameter computer use agent that achieves competitive performance on web-based tasks through high-quality synthetic training data. It uses screenshot-only perception and outputs low-level actions directly via predicted coordinates, without relying on accessibility trees or DOM parsing. The model outperforms other 7B models and is competitive with much larger frontier models on benchmarks like WebVoyager, Online-Mind2Web, DeepShop, and the newly introduced WebTailBench.

## Method Summary
Fara-7B was trained via supervised fine-tuning on Qwen2.5-VL-7B using 145,603 synthetic trajectories (1.8M total samples) generated by FaraGen, a multi-agent pipeline that proposes tasks, solves them with orchestrated agents, and verifies successful trajectories. The model takes screenshots and browser metadata as input, then outputs thoughts and actions including absolute coordinates for clicks and scrolls. Training uses AdamW optimizer with cosine learning rate schedule, gradient clipping, and truncated history context (N=3 observations).

## Key Results
- Achieves 73% accuracy on WebVoyager benchmark
- Outperforms other 7B models and is competitive with much larger frontier models
- Shows high grounding accuracy (86.7-89.3%) on ScreenSpot-V2 benchmark
- Demonstrates strong safety performance with high refusal rates for harmful tasks

## Why This Works (Mechanism)

### Mechanism 1
High-quality synthetic trajectories can substitute for scarce human CUA data at scale. FaraGen's closed-loop system (Task Proposal → Task Solving → Verification) generates diverse, verified trajectories. Multi-agent orchestration enables recovery from errors and loop detection, producing demonstrations that encode multi-step reasoning patterns.

### Mechanism 2
Screenshot-only perception with direct coordinate prediction generalizes better than accessibility tree scaffolding. Fara-7B ingests raw pixels and outputs absolute coordinates for grounded actions, eliminating dependency on brittle DOM parsing. Training uses SoM-annotated data during collection but discards scaffolding at inference.

### Mechanism 3
Distilling multi-agent reasoning into a single model preserves planning capabilities while reducing inference overhead. Training extracts WebSurfer's thoughts and actions from Magentic-One trajectories, implicitly encoding Orchestrator signals into Fara-7B's outputs without runtime coordination.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) on trajectory data**: Fara-7B is trained via SFT on 1.8M samples. Understanding loss computation on action tokens (including coordinates) is essential. *Quick check*: Can you explain why backpropagation is limited to actions with corresponding observations when using truncated history?

- **Visual grounding / coordinate prediction**: The model predicts absolute pixel coordinates for click actions. Understanding how VLMs output spatial locations is critical for debugging localization failures. *Quick check*: How does Qwen2.5-VL's grounding convention represent coordinates in the output vocabulary?

- **LLM-as-judge verification**: Both data filtering and evaluation rely on LLM verifiers. Understanding their failure modes (e.g., hallucination, inconsistency) is necessary for quality assessment. *Quick check*: What are the three verifier types in FaraGen, and which one specifically targets screenshot-response consistency?

## Architecture Onboarding

- **Component map**: URL Sources (ClueWeb22/Tranco) → Task Proposal → Task Solving (Orchestrator + WebSurfer) → Verification → Training Data
- **Critical path**: Task Proposal quality → Verifier filtering accuracy → Trajectory diversity → SFT data quality → Fara-7B grounding/reasoning capability. The 145K trajectories across 70K domains is the bottleneck resource.
- **Design tradeoffs**: History truncation (N=3 observations) reduces memory/compute but risks losing long-horizon context; SFT-only training is simpler than RL but may limit self-correction capabilities; screenshot-only inference eliminates accessibility tree brittleness but may struggle on sites with poor visual affordances.
- **Failure signatures**: Looping (repeated actions without progress), critical point crossing (proceeding past payment/login without user confirmation), and hallucinated coordinates (clicking non-interactive regions).
- **First 3 experiments**: 1) Run Fara-7B on WebVoyager using provided inference harness; 2) Test N=1, 2, 3, 5 observation history to measure accuracy vs token cost tradeoff; 3) Sample 100 trajectories to compare Alignment/Rubric/Multimodal verifier outputs against human judgment.

## Open Questions the Paper Calls Out
None

## Limitations

- The 145K synthetic trajectories represent a critical bottleneck, and the paper provides limited detail on data quality distribution across samples
- Fara-7B's performance improvements may be driven by synthetic data quality rather than architectural choices, as direct comparisons to SFT-trained versions with different data sources are not provided
- Grounding accuracy of 86.7-89.3% on ScreenSpot-V2 may not fully translate to real-world websites with diverse visual designs and interactive elements

## Confidence

- **High Confidence**: Fara-7B's competitive performance on established benchmarks and the basic efficacy of the synthetic data generation pipeline
- **Medium Confidence**: The claim that screenshot-only perception with coordinate prediction generalizes better than accessibility tree scaffolding
- **Medium Confidence**: The distillation of multi-agent reasoning into single-agent performance

## Next Checks

1. **Verifier Quality Analysis**: Sample 100 trajectories from FaraGen's output and independently evaluate each verifier's performance against human judgment to quantify false positive and false negative rates for each verifier type.
2. **Data Quality Impact**: Train a baseline Fara-7B using a random 50% subset of the verified trajectories and compare performance on WebVoyager to isolate the impact of data quantity vs quality filtering.
3. **Grounding Robustness Test**: Evaluate Fara-7B on a custom benchmark of 50 web pages specifically designed with visually similar but functionally distinct elements to measure systematic localization errors.