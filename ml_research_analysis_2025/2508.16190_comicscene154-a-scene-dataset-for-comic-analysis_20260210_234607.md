---
ver: rpa2
title: 'ComicScene154: A Scene Dataset for Comic Analysis'
arxiv_id: '2508.16190'
source_url: https://arxiv.org/abs/2508.16190
tags:
- comics
- scene
- comic
- dataset
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ComicScene154, a manually annotated dataset
  for scene-level narrative segmentation in comics, aiming to advance computational
  narrative understanding in this multimodal medium. The dataset contains 154 pages
  from four public-domain comics, annotated to mark the start of each scene based
  on narrative arcs.
---

# ComicScene154: A Scene Dataset for Comic Analysis

## Quick Facts
- arXiv ID: 2508.16190
- Source URL: https://arxiv.org/abs/2508.16190
- Authors: Sandro Paval; Ivan P. Yamshchikov; Pascal Meißner
- Reference count: 16
- Primary result: Introduced ComicScene154 dataset with 154 annotated comic pages; human agreement pk≈0.17; multi-modal models struggle to improve beyond random segmentation (pk≈0.42 vs random≈0.43)

## Executive Summary
This paper introduces ComicScene154, a manually annotated dataset for scene-level narrative segmentation in comics, aiming to advance computational narrative understanding in this multimodal medium. The dataset contains 154 pages from four public-domain comics, annotated to mark the start of each scene based on narrative arcs. A key contribution is the establishment of a scene segmentation benchmark, using a two-step pipeline involving a multi-modal model and an LLM for refinement. The results show that while human annotators achieve reasonable agreement (pk≈0.17), the multi-modal model struggles to improve beyond random segmentation, highlighting the inherent subjectivity and challenges in defining scenes. Despite limitations, ComicScene154 provides a valuable resource for developing and evaluating scene segmentation methods, with potential applications extending beyond comics to other multimodal narrative forms.

## Method Summary
The method defines scenes as plot-based semantic units following Rao et al. (2020), then annotates 154 comic pages from 4 public-domain comics with scene start boundaries. The dataset includes panel coordinates and reading order. Evaluation uses the pk metric (sliding window disagreement) with k=3 derived from average scene lengths. A two-step pipeline is established: (1) a multi-modal model (gemini-2.0-flash-thinking-exp) processes page images with panel metadata to predict scene boundaries, and (2) a reasoning LLM refines these predictions. The method is validated through human annotation agreement and baseline model performance across 100 total iterations (10 initial + 10 refinement per comic).

## Key Results
- Human annotators achieved pk≈0.17 average agreement, indicating "notable agreement" but also subjectivity
- Multi-modal model performance: pk≈0.42-0.47 across comics, only marginally better than random baseline
- Best refined iterations improved to pk≈0.34-0.41, but large performance variations persisted
- Fixed window size k=3 derived from averaging scene lengths across annotators

## Why This Works (Mechanism)

### Mechanism 1: Scene-as-Semantic-Unit Definition
Defining scenes as plot-based semantic units (rather than physical divisions) provides a principled basis for narrative segmentation, though the approach exposes inherent subjectivity. Following Rao et al. (2020), scenes are defined as units where "an overarching task is pursued by a certain cast of characters" with temporal/spatial coherence. This abstraction maps video scene segmentation concepts onto discrete comic panels, treating narrative arcs as the atomic unit rather than pages. Core assumption: Narrative structure exists at a level between individual panels and complete stories, and human readers share enough conceptual alignment to make this annotatable.

### Mechanism 2: pk Metric Adaptation from Text Segmentation
The pk metric, borrowed from semantic text segmentation, provides a meaningful measure of boundary alignment, with window size k=3 computed from average scene lengths. pk measures proportion of sliding windows incorrectly segmented. For two segmentations, it counts disagreements on whether pairs of panels fall in the same segment. k=3 was derived empirically from average scene lengths across annotators. Core assumption: The metric transfers meaningfully from continuous text to discrete visual sequences, and the computed window size generalizes across comic genres.

### Mechanism 3: Two-Step Multi-modal + LLM Refinement Pipeline
A cascade approach (vision model → reasoning LLM) can marginally improve scene segmentation, but current models struggle to exceed random baselines significantly. Step 1 uses Gemini-2.0-flash-thinking-exp to process page images with panel coordinates and predict boundaries. Step 2 refines via reasoning LLM. Best iterations achieve pk≈0.34 vs. random≈0.42-0.46. Core assumption: Multi-modal models can extract narrative-relevant visual features, and LLM reasoning can impose coherence on noisy initial predictions.

## Foundational Learning

- **Concept: Closure (in comics)**
  - Why needed here: The paper explicitly identifies closure—the cognitive process of bridging narrative gaps between panels—as what makes comics challenging for computational analysis. Understanding this explains why scene segmentation differs from video shot detection.
  - Quick check question: Can you explain why panel-level features alone are insufficient for scene boundary detection?

- **Concept: pk Metric (Beeferman's Probability of Disagreement)**
  - Why needed here: This is the sole evaluation metric. Understanding how sliding windows work and why k matters is essential for interpreting all reported results.
  - Quick check question: If annotator A marks scenes at panels [1, 5, 10] and annotator B marks [1, 6, 12] in a 15-panel story, would pk increase or decrease if k were larger?

- **Concept: Narrative Arcs vs. Physical Divisions**
  - Why needed here: The paper's core argument is that page-level processing ignores narrative structure. Tasks like summarization or entity tracking would benefit from semantic boundaries.
  - Quick check question: Why might segmenting by pages introduce "superfluous content" or "omit key story details" for downstream tasks?

## Architecture Onboarding

- Component map: Raw Comic Pages → Panel Extraction (coordinates + reading order) → Annotation Interface (Boolean: scene-start per panel) → [Baseline Pipeline: Multi-modal Model → LLM Refiner] → pk Evaluation (window k=3)

- Critical path: Panel extraction with correct reading order is foundational—errors here corrupt all downstream annotation and evaluation. The panel numbering protocol ensures consistency.

- Design tradeoffs:
  - **Dataset scale vs. narrative completeness**: 154 pages from 4 comics prioritizes full stories over sample diversity. Random panels wouldn't support scene annotation.
  - **Public domain vs. modern comics**: Constrained to Golden Age (1940s-60s) styles; may not transfer to contemporary storytelling.
  - **Annotator agreement vs. task definition**: Moderate human agreement (pk≈0.17) validates the task's feasibility while exposing subjectivity.

- Failure signatures:
  - Model outputs show high self-consistency across iterations but poor alignment with human labels → suggests models capture *some* structure but not narrative-defined scenes
  - Large variance between best and average iterations (Table 5: 0.34 vs. 0.39) → sensitivity to prompt/output randomness
  - One annotator (Western Love, Tester 2: pk=0.37) segmented into much shorter scenes → subjective interpretation variance

- First 3 experiments:
  1. **Panel-level feature ablation**: Test whether adding explicit features (character presence, location changes, dialogue speakers) improves multi-modal predictions beyond raw image input.
  2. **k-sensitivity analysis**: Re-evaluate all baselines with k ∈ {2, 4, 5} to determine if metric choice drives observed differences.
  3. **Cross-comic transfer**: Train/fine-tune on 3 comics, test on the 4th to assess generalization across genres (Humor, Heroes, Fantasy, Love).

## Open Questions the Paper Calls Out

- **Can scene segmentation models trained on the ComicScene154 dataset effectively generalize to contemporary comics or Japanese manga, given the stylistic and structural differences inherent in Golden Age data?**
  - Basis in paper: [explicit] The "Limitations" section states the dataset is composed largely of Golden Age comics, which may limit applicability to modern works or manga due to differences in artistic style and narrative complexity.
  - Why unresolved: The current dataset only covers public-domain comics from 1940–1960, and the authors explicitly excluded manga due to structural discrepancies.
  - What evidence would resolve it: Benchmarking a model trained on ComicScene154 against a dataset of modern comics or manga (e.g., Manga109) and comparing performance metrics.

- **Can narrative-based scene segmentation in comics be utilized to effectively reduce data complexity and improve performance on video scene segmentation tasks?**
  - Basis in paper: [explicit] The Introduction and Conclusion suggest that treating comics as an abstraction for continuous data like movies could preserve narrative structure while reducing complexity.
  - Why unresolved: While proposed as a potential application, the paper only provides benchmarks for comic data and does not experimentally validate this cross-domain transfer.
  - What evidence would resolve it: A study applying comic-trained segmentation models to sampled frames from video datasets and evaluating the alignment with ground-truth video scene boundaries.

- **Does segmenting comics by narrative arcs rather than physical pages yield measurable improvements in downstream tasks such as story summarization or character re-identification?**
  - Basis in paper: [inferred] The Introduction posits that narrative segmentation could better support these tasks by omitting superfluous content, but the paper only establishes a segmentation benchmark without testing these downstream applications.
  - Why unresolved: The utility of this specific segmentation method for the mentioned tasks remains a theoretical claim rather than an empirically proven outcome.
  - What evidence would resolve it: Comparative experiments evaluating the accuracy of summarization or entity tracking models when using scene-level segments versus page-level segments.

## Limitations

- Dataset scale (154 pages from 4 comics) limits generalizability across comic styles and genres
- Golden Age public-domain comics may not reflect contemporary narrative structures or artistic conventions
- Human annotator agreement reveals inherent subjectivity in scene definition that affects both evaluation and model development
- pk metric represents a novel adaptation to visual media without corpus validation of its appropriateness for discrete panel sequences

## Confidence

- **High confidence** in task definition and dataset construction methodology
- **Medium confidence** in multi-modal baseline results and their interpretation
- **Medium confidence** in the claim that scene segmentation is "inherently subjective" based on human agreement alone

## Next Checks

1. **Metric sensitivity analysis**: Evaluate the multi-modal baseline across k∈{2,4,5} to determine if observed performance differences are artifacts of the fixed window size choice.

2. **Feature ablation study**: Compare the multi-modal model's performance using only raw image input versus input augmented with explicit narrative features (character presence, location changes, speaker attribution) to identify whether current models miss critical semantic signals.

3. **Cross-comic generalization test**: Perform leave-one-comic-out evaluation to assess whether models trained on 3 comics can successfully segment scenes in the held-out comic, measuring both performance drop and consistency patterns across genres.