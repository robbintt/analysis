---
ver: rpa2
title: 'Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study
  on Lithuanian History'
arxiv_id: '2501.09154'
source_url: https://arxiv.org/abs/2501.09154
tags:
- languages
- llama3
- language
- history
- nordic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multilingual large language models (LLMs)
  on Lithuanian and general history knowledge using a multiple-choice question-answering
  task. The dataset was created by translating Lithuanian history exam questions into
  Nordic, Baltic, and other languages using GPT-4o and DeepL, followed by manual quality
  evaluation.
---

# Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History

## Quick Facts
- arXiv ID: 2501.09154
- Source URL: https://arxiv.org/abs/2501.09154
- Authors: Yevhen Kostiuk; Oxana Vitman; Łukasz Gagała; Artur Kiulian
- Reference count: 8
- Key outcome: GPT-4o consistently outperformed all other models across all language groups in Lithuanian history knowledge evaluation

## Executive Summary
This study evaluates multilingual large language models on Lithuanian and general history knowledge using a multiple-choice question-answering task. The dataset was created by translating Lithuanian history exam questions into Nordic, Baltic, and other languages using GPT-4o and DeepL, followed by manual quality evaluation. Eight models were tested, including GPT-4o, various Llama and Qwen variants, Mistral Nemo, and Nordic fine-tuned models. Results show GPT-4o consistently outperformed all other models across all language groups, with slightly better results for Baltic and Nordic languages.

## Method Summary
The study used the EXAMS dataset (Hardalov et al., 2020) containing 550 Lithuanian history exam questions, with image-dependent questions removed. Questions were translated from Lithuanian to English using GPT-4o, then from English to target languages (Nordic: Danish, Finnish, Swedish; Baltic: Estonian, Latvian; others: Ukrainian, Arabic, English). Manual quality validation was performed on 100 samples per language with 77% average approval. Eight models were evaluated using 4-shot prompting in target languages with accuracy scoring via letter extraction (A/B/C/D). The evaluation framework used Ollama for open-source models and OpenAI API for GPT-4o.

## Key Results
- GPT-4o achieved 0.87–0.88 accuracy across Baltic and Nordic language groups, significantly outperforming all other models
- Larger open-source models (QWEN2.5 72b, LLaMa3.1 70b) performed well but showed weaker alignment with Baltic languages
- Nordic fine-tuned models failed to surpass multilingual models, indicating shared cultural context alone does not guarantee better performance
- Smaller models demonstrated significant gaps in Lithuanian history knowledge despite Lithuanian being part of the Baltic language group

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale-dependent cross-lingual knowledge alignment — larger models exhibit more consistent factual knowledge retrieval across linguistically diverse prompts
- Mechanism: Higher parameter count provides greater capacity for distributed multilingual knowledge representations, reducing performance variance across language groups
- Core assumption: Parameter scale correlates with multilingual embedding quality; causation not proven
- Evidence anchors: Larger open-source models performed well but showed weaker Baltic alignment; smaller models demonstrated gaps with LT-related alignment

### Mechanism 2
- Claim: Regional fine-tuning does not automatically transfer cultural-historical knowledge — linguistic adaptation ≠ domain knowledge injection
- Mechanism: Nordic-specific models were trained on regional languages but did not outperform general multilingual models on regional history, suggesting fine-tuning for fluency does not inherently encode cultural knowledge
- Core assumption: Fine-tuning corpora focused on language fluency rather than regional historical content
- Evidence anchors: Nordic fine-tuned models failed to surpass multilingual models; authors note shared cultural context alone does not guarantee better performance

### Mechanism 3
- Claim: Cultural proximity hypothesis rejected — linguistic/geographic relatedness does not predict cross-lingual knowledge transfer for low-resource languages
- Mechanism: Authors hypothesized Baltic/Nordic languages would perform better due to shared historical context; results showed Nordic outperforming Baltic even for Lithuanian-specific questions
- Core assumption: Training data representation, not cultural proximity, drives factual knowledge access
- Evidence anchors: Nordic outperformed Baltic languages even for LT-related questions; findings highlight shared context does not guarantee better performance

## Foundational Learning
- Concept: Cross-lingual knowledge consistency
  - Why needed here: Central to understanding why models retrieve facts differently when prompted in different languages; explains Baltic language performance gaps
  - Quick check question: If a model knows "Vilnius is the capital of Lithuania" in English, will it answer correctly in Lithuanian? This paper shows smaller models often fail this transfer
- Concept: Low-resource language representation
  - Why needed here: Baltic languages (Lithuanian, Latvian, Estonian) are underrepresented in training corpora, leading to weaker alignment even for region-specific content
  - Quick check question: Does scaling alone solve low-resource performance, or is targeted data curation required? Evidence suggests both matter
- Concept: Fine-tuning vs. knowledge injection
  - Why needed here: Distinguishes language fluency from domain knowledge; explains why Nordic fine-tuned models failed to improve regional history performance
  - Quick check question: If you fine-tune a model on Swedish text, does it learn Swedish history? Not necessarily — depends on corpus content

## Architecture Onboarding
- Component map: Lithuanian source → GPT-4o/DeepL translation to EN → pivot translation to 9 languages → manual quality validation → 4-shot prompting in target language → answer parsing → accuracy aggregation
- Critical path: Dataset translation quality → prompt formatting → model inference → answer parsing → accuracy aggregation by language group (NRD/BLT/MLT) and question type (LT-related vs. general)
- Design tradeoffs: Translation via pivot increases error propagation but leverages stronger translation models; 4-shot prompting improves format compliance but introduces variance; strict letter-extraction parsing penalizes verbose models
- Failure signatures: Baltic languages consistently underperform except GPT-4o (training data gap); Mistral Nemo anomalously low (0.36–0.41) across all groups (possible prompt incompatibility); GPT-SW3 126m outperforms larger variants on English LT-related questions (possible overfitting)
- First 3 experiments:
  1. Reproduce with native-language questions: Test whether original Lithuanian questions yield higher accuracy for Baltic languages to isolate translation vs. training-data effects
  2. Controlled regional fine-tuning: Fine-tune LLaMa3 8b on Baltic history corpora in Baltic languages to test knowledge injection vs. fluency-only adaptation
  3. Cross-question-type analysis: Compare LT-related vs. general history performance delta across model scales to quantify scale-dependent cultural knowledge gaps

## Open Questions the Paper Calls Out
- What specific fine-tuning strategies and targeted datasets are required to bridge the performance gap for Baltic languages in LLMs? The conclusion explicitly calls for targeted datasets and fine-tuning strategies to improve LLM alignment with less-resourced languages.
- To what extent do translation artifacts in the benchmark dataset contribute to the lower performance observed in smaller models? The methodology relied on automated translation with only partial manual validation, and smaller models showed specific weaknesses in Baltic languages compared to English.
- Why does region-specific pre-training (e.g., Nordic models) fail to enhance performance on shared regional history compared to general multilingual models? The authors note that shared cultural or historical context alone does not guarantee better performance, as Nordic fine-tuned models underperformed.

## Limitations
- Translation-dependent evaluation framework introduces potential semantic drift and fidelity loss, particularly for Baltic languages
- Strict single-letter parsing may underestimate model capabilities by marking multi-letter responses as incorrect
- Nordic fine-tuned models tested were not explicitly trained on regional history content, limiting conclusions about cultural knowledge transfer
- LT-related vs general history classification relies on qualitative criteria without automated verification

## Confidence
- High Confidence: GPT-4o outperforming all other models across all language groups and question types
- Medium Confidence: Scale-dependent multilingual knowledge alignment (correlation vs causation unclear)
- Medium Confidence: Rejection of cultural proximity hypothesis (limited by lack of history-specific fine-tuning in tested models)
- Low Confidence: Translation quality impacts on Baltic language performance (extent of contribution unclear)

## Next Checks
1. Controlled history corpus fine-tuning experiment: Fine-tune a base model (e.g., LLaMa3 8b) on Baltic history corpora in Baltic languages to isolate knowledge injection effects from language fluency adaptation
2. Translation quality impact analysis: Compare model performance on original Lithuanian questions versus translated versions across all language groups
3. Cross-lingual knowledge consistency testing: Implement parallel evaluation where models are tested on the same factual questions across all languages to quantify cross-lingual knowledge transfer consistency