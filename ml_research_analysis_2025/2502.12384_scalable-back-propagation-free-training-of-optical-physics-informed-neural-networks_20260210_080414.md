---
ver: rpa2
title: Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural
  Networks
arxiv_id: '2502.12384'
source_url: https://arxiv.org/abs/2502.12384
tags:
- training
- photonic
- neural
- error
- optical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training physics-informed
  neural networks (PINNs) in real-time on energy-constrained edge devices by leveraging
  photonic computing accelerators. The authors propose a completely back-propagation-free
  (BP-free) framework to enable scalable training of real-size PINNs on integrated
  photonic platforms.
---

# Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2502.12384
- Source URL: https://arxiv.org/abs/2502.12384
- Reference count: 40
- Key outcome: A back-propagation-free framework for training physics-informed neural networks on photonic hardware, achieving 42.7× reduction in device count and 1.64-second training time for solving the Black-Scholes equation.

## Executive Summary
This work presents a novel approach to train physics-informed neural networks (PINNs) on energy-constrained edge devices using photonic computing accelerators. The authors develop a completely back-propagation-free framework that combines a sparse-grid Stein derivative estimator with tensor-train compressed zeroth-order optimization. This method significantly reduces the number of photonic devices required for training large-scale PINNs, enabling real-time, low-power learning systems for physical AI and digital twins.

## Method Summary
The proposed method eliminates the need for back-propagation in PINN training by using a sparse-grid Stein derivative estimator to approximate derivatives in loss evaluation. This is combined with tensor-train compressed zeroth-order optimization to improve scalability and convergence. The approach leverages photonic computing accelerators to implement the training process, significantly reducing the number of photonic devices required compared to traditional BP-based methods.

## Key Results
- Achieves competitive accuracy compared to standard BP-based training methods across various PDEs
- Demonstrates 42.7× reduction in photonic device count for solving the Black-Scholes equation
- Achieves 1.64-second training time on photonic platforms for the tested problem

## Why This Works (Mechanism)
The method works by replacing traditional back-propagation with a sparse-grid Stein derivative estimator, which approximates the necessary derivatives without requiring gradient computation through the network. The tensor-train compression then reduces the computational complexity of the optimization process, making it feasible to implement on photonic hardware with limited resources.

## Foundational Learning
- **Physics-Informed Neural Networks (PINNs)**: Neural networks that incorporate physical laws into their training process - needed for solving PDEs and modeling physical systems
- **Stein Derivative Estimator**: A method for approximating derivatives without explicit differentiation - needed to eliminate back-propagation
- **Tensor-Train Decomposition**: A technique for compressing high-dimensional tensors - needed to reduce computational complexity
- **Photonic Computing Accelerators**: Hardware that uses light for computation - needed for energy-efficient training
- **Zeroth-Order Optimization**: Optimization methods that don't require gradient information - needed for the BP-free approach

## Architecture Onboarding
Component Map: Input PDE -> PINN Architecture -> Sparse-Grid Stein Estimator -> Tensor-Train Optimization -> Photonic Hardware
Critical Path: PDE formulation → PINN construction → Derivative estimation → Optimization → Hardware implementation
Design Tradeoffs: Accuracy vs. device count, training time vs. hardware complexity
Failure Signatures: Loss of convergence, increased approximation error, hardware resource constraints
First Experiments:
1. Validate derivative estimation accuracy on simple test functions
2. Test tensor-train compression effectiveness on synthetic data
3. Benchmark hardware resource requirements on a small-scale photonic platform

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Real-world scalability on actual photonic hardware remains unverified
- Generalization across diverse PDE classes is not fully demonstrated
- Energy efficiency claims are based on simulations rather than measured power consumption

## Confidence
High: Mathematical framework and algorithmic innovations, competitive accuracy claims
Medium: Scalability claims, device count reduction validation
Low: Real-time and energy efficiency assertions, hardware implementation feasibility

## Next Checks
1. Implement the BP-free training framework on an integrated photonic chip to measure actual training time and energy consumption
2. Test the method on a diverse set of PDEs with varying complexity and boundary conditions to assess generalizability
3. Conduct a comprehensive analysis of the method's performance under different noise levels and data quality scenarios to evaluate robustness