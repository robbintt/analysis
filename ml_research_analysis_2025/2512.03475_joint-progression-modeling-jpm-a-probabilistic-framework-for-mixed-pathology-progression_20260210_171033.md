---
ver: rpa2
title: 'Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology
  Progression'
arxiv_id: '2512.03475'
source_url: https://arxiv.org/abs/2512.03475
tags:
- data
- progression
- disease
- mixed
- rankings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Joint Progression Modeling (JPM) is a probabilistic framework for
  modeling mixed-pathology disease progression by combining partial rankings from
  individual diseases into a coherent aggregate progression. The framework uses probabilistic
  ranking models (Mallows, Bradley-Terry, Plackett-Luce, and Pairwise) to construct
  a prior over joint progressions, which is integrated with an Event-Based Model likelihood
  for inference.
---

# Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression

## Quick Facts
- arXiv ID: 2512.03475
- Source URL: https://arxiv.org/abs/2512.03475
- Reference count: 40
- Primary result: JPM improves ordering accuracy by ~21% over single-disease EBM baseline in mixed-pathology progression modeling

## Executive Summary
Joint Progression Modeling (JPM) addresses the challenge of modeling disease progression in mixed-pathology cohorts by aggregating partial rankings from individual diseases into a coherent joint progression. The framework uses probabilistic ranking models (Mallows, Bradley-Terry, Plackett-Luce, and Pairwise) to construct a prior over joint progressions, which is integrated with an Event-Based Model likelihood for inference. JPM demonstrates near-perfect separation between true and random rankings, positive calibration across variants, and improved ordering accuracy in synthetic experiments.

## Method Summary
JPM combines partial orderings from individual disease datasets with mixed-pathology data using a probabilistic framework. The method first estimates partial rankings from single-disease cohorts using SA-EBM, then constructs a prior distribution over joint progressions using one of four ranking models (Mallows, Bradley-Terry, Plackett-Luce, or Pairwise). This prior is integrated with an Event-Based Model likelihood through MCMC sampling to infer the most probable aggregate progression sequence. The framework operates on cross-sectional data and uses 20,000 MCMC iterations with 500 burn-in steps for inference.

## Key Results
- All JPM variants achieve near-perfect separation (AUROC ≈ 1.0) between true and random rankings
- Calibration is positive for all variants (Spearman's ρ > 0.58), with Mallows performing best
- JPM improves ordering accuracy by ~21% over single-disease EBM baseline in synthetic experiments
- NACC results show Mallows variant produces progression sequences most consistent with AD-VaD comorbidity literature

## Why This Works (Mechanism)

### Mechanism 1: Rank Aggregation as Data Augmentation
JPM overcomes data sparsity in mixed-pathology cohorts by structurally constraining the search space using "partial rankings" derived from larger single-disease datasets. The framework treats individual disease trajectories as partial orderings and aggregates these into a prior distribution over the joint progression, effectively transferring statistical strength from data-rich single-disease cohorts to the data-poor mixed cohort. The core assumption is that the biological progression of the comorbidity is an aggregation of the individual disease progressions, rather than a completely novel trajectory caused by interactions.

### Mechanism 2: Energy-Based Separation
The model distinguishes valid joint progressions from noise by assigning low "energy" to rankings that respect the partial orders, ensuring the ground truth is statistically distinguishable. JPM defines an energy function E(σ|S) where rankings consistent with input partials receive lower energy (higher probability). This creates a steep energy landscape where the true progression has significantly lower energy than random permutations. The core assumption is that the ground truth ordering satisfies the majority of constraints implied by the partial rankings.

### Mechanism 3: Bayesian Inference via MCMC
Integrating the ranking-based prior with an Event-Based Model likelihood via MCMC sampling improves ordering accuracy compared to likelihood-only inference. The inference algorithm samples from the posterior P(σ|D, S) ∝ P(D|σ) · P(σ|S). The prior P(σ|S) acts as a regularizer, guiding the MCMC sampler away from permutations that fit the data locally but violate the known sequence of individual diseases. The core assumption is that the EBM likelihood is accurate but under-determined due to small sample size in mixed cohorts.

## Foundational Learning

- **Concept: Event-Based Models (EBMs)**
  - Why needed here: JPM is a wrapper around EBM logic. You must understand how EBMs stage patients and estimate sequences from cross-sectional data to understand what JPM is improving.
  - Quick check: Can you explain how an EBM estimates a sequence of biomarker events without longitudinal data?

- **Concept: Probabilistic Ranking Models (Mallows, Plackett-Luce)**
  - Why needed here: These are the engines of the JPM prior. Understanding how they calculate probability based on distance (Mallows) or sequential choice (PL) is required to select the right variant.
  - Quick check: How does the Mallows model calculate the probability of a specific ranking relative to a central ordering?

- **Concept: Metropolis-Hastings MCMC**
  - Why needed here: Inference in JPM is not analytical; it relies on sampling. Understanding the proposal mechanism (swapping items) and acceptance criteria is vital for debugging convergence.
  - Quick check: In the context of JPM, what constitutes a "proposal" in the MCMC chain?

## Architecture Onboarding

- **Component map:** Single-disease datasets -> SA-EBM (partial rankings) -> Ranking model (Mallows/PL/BT/Pairwise) -> Energy function -> MCMC sampler -> Aggregate ranking
- **Critical path:** Estimating the partial rankings from single-disease data is the first dependency. If these are wrong (high variance), the Prior Engine will be misspecified, and the MCMC may converge to a local minimum inconsistent with biology.
- **Design tradeoffs:**
  - Mallows vs. Plackett-Luce: Mallows offers better calibration and sharpness for real-world data, but PL is simpler to sample from
  - Sharpness vs. Generalization: High sharpness helps when mixed data is very sparse but hurts if the true mixed progression deviates slightly from the partials
- **Failure signatures:**
  - Low Sharpness (Kendall's W < 0.5): Indicates high conflict or low overlap in input partial rankings
  - Poor Calibration (Spearman's ρ ≈ 0): Indicates the energy function does not correlate with distance to ground truth
- **First 3 experiments:**
  1. Run the generative algorithm with known partials to verify the "Separation" metric (AUROC ≈ 1.0) before touching real data
  2. Synthetic test: Inject artificial conflict into partial rankings and measure the drop in "Sharpness"
  3. Run JPM-Mallows on the NACC AD-VaD subset and verify the positioning of vascular biomarkers matches the paper's finding

## Open Questions the Paper Calls Out

- **Open Question 1:** How can JPM be extended to incorporate disease subtypes, given its current assumption of a single shared progression? [explicit] "Future work should explore how to incorporate subtypes such as in SuStaIn (Young et al., 2018) into the JPM framework."
- **Open Question 2:** How can JPM be adapted to handle continuous event sequences where its current performance struggles? [explicit] "JPM's performance struggles when event sequences are continuous. Integrating the approach taken by the Temporal EBM (Wijeratne et al., 2023) could help with this issue."
- **Open Question 3:** How can one identify or model mixed-pathology scenarios where the joint progression is fundamentally different from the combination of individual diseases? [explicit] "Some disease interactions might result in a disease progression that is completely different from the individual disease progressions. In these cases, the JPM would be inappropriate to apply."
- **Open Question 4:** What metrics or heuristics can reliably guide the selection of a specific JPM variant (e.g., Mallows vs. Plackett-Luce) for a given real-world dataset? [explicit] "Reconciling the synthetic results and real-world application is an important direction for future work."

## Limitations

- Core assumption that mixed-pathology progression is a simple aggregation of individual disease orderings remains empirically untested
- Framework's performance with very small mixed cohorts (n=37 in NACC VaD-AD) may not generalize to rarer comorbidities
- Choice between ranking model variants lacks a principled selection criterion based on observable data characteristics

## Confidence

- **High confidence:** AUROC separation metric and ordering accuracy improvements (~21%) on synthetic data; MCMC framework is well-specified
- **Medium confidence:** Real-world NACC results showing vascular pathology positioning; calibration and sharpness metrics showing variant sensitivity
- **Low confidence:** Generalizability to comorbidities beyond AD-VaD; biological validity of treating interactions as simple aggregations

## Next Checks

1. **Prior Sensitivity Analysis:** Systematically vary the θ parameter in the Mallows model and measure its effect on ordering accuracy and calibration metrics across synthetic datasets with known ground truth
2. **Cross-Validation on Mixed Cohorts:** Implement k-fold cross-validation on the mixed-pathology NACC cohort to quantify variance in progression ordering and assess stability beyond single-seed runs
3. **Interaction Stress Test:** Generate synthetic datasets where diseases interact synergistically (non-additive ordering changes) and measure how JPM's accuracy degrades compared to datasets with purely additive interactions