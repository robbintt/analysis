---
ver: rpa2
title: 'ROCM: RLHF on consistency models'
arxiv_id: '2503.06171'
source_url: https://arxiv.org/abs/2503.06171
tags:
- reward
- consistency
- generation
- diffusion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ROCM (Reinforcement Learning from Human Feedback
  on Consistency Models), addressing the slow generation and training inefficiency
  of diffusion models when incorporating RLHF. The key idea is to leverage consistency
  models, which enable single-step or few-step generation, and optimize the RLHF objective
  directly using reparameterization, avoiding complex policy gradient methods.
---

# ROCM: RLHF on consistency models

## Quick Facts
- arXiv ID: 2503.06171
- Source URL: https://arxiv.org/abs/2503.06171
- Authors: Shivanshu Shekhar; Tong Zhang
- Reference count: 40
- Primary result: ROCM achieves competitive or superior performance to diffusion-based RLHF methods across multiple reward models and metrics while being faster to train.

## Executive Summary
ROCM addresses the slow generation and training inefficiency of diffusion models when incorporating reinforcement learning from human feedback (RLHF). The method leverages consistency models' ability to generate images in 1-8 steps and optimizes the RLHF objective directly using reparameterization, avoiding complex policy gradient methods. Distributional regularization using various f-divergences (KL, JS, Hellinger, Fisher) prevents reward hacking and improves training stability. Experiments show ROCM achieves competitive or superior performance compared to baseline methods across multiple automated metrics and human evaluation while demonstrating faster training and better efficiency than diffusion-based methods.

## Method Summary
ROCM fine-tunes consistency models with RLHF using direct reward optimization via the reparameterization trick plus f-divergence regularization. The method uses 8-step consistency models (LCM Dreamshaper v7) with classifier-free guidance (ω=7.5) and LoRA (rank=16, α=32). Training employs 4,000 text prompts from Pick-a-Pic V1 dataset with batch size=1 on 2×A6000 GPUs at learning rate 6×10⁻⁵. The loss function combines reward model output with distributional regularization between current and reference models. Different f-divergences (KL, JS, Hellinger, Fisher) are tested with specific β values tuned per divergence type.

## Key Results
- ROCM achieves competitive or superior performance to RLCM, DDPO, D3PO, and DPOK across PickScore, CLIPScore, HPSv2, Aesthetic Score, BLIPScore, and ImageReward metrics
- Distributional regularization prevents reward hacking, with human preference peaking at optimal β values before declining when overfitting occurs
- Training efficiency is significantly improved compared to diffusion-based methods, converging in fewer GPU hours
- Different f-divergences produce distinct visual styles and failure modes, with no single divergence consistently dominating

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting from zero-order (policy gradients) to first-order (direct backpropagation) via reparameterization trick improves training efficiency and stability
- **Mechanism:** Standard RLHF uses PPO with sampling-based gradient estimates. ROCM exploits that consistency model sampling is deterministic given parameters θ and noise ε, enabling direct backpropagation through the generation steps
- **Core assumption:** Reward model R(·) is differentiable with respect to generated image x₀
- **Evidence:** Abstract states "leverages first-order gradients, making it more efficient and less sensitive to hyperparameter tuning"; Section 3 describes reformulation from zero-order to first-order optimization

### Mechanism 2
- **Claim:** Distributional regularization via f-divergences prevents reward hacking where models generate out-of-distribution images to trick reward models
- **Mechanism:** Loss includes penalty term β D(πθ || πθ_ref) that pulls policy back toward reference model distribution, constraining generation to valid image manifold
- **Core assumption:** Reference model πθ_ref defines valid "safe zone" of image distributions where straying correlates with degraded visual quality
- **Evidence:** Abstract mentions "incorporating distributional regularization to enhance training stability and prevent reward hacking"; Section 4.2 shows reward increases while human preference declines when β is too low

### Mechanism 3
- **Claim:** Consistency Models enable feasible end-to-end backpropagation by drastically shortening generation trajectory
- **Mechanism:** Diffusion models require storing gradients for 20-50+ steps (memory-prohibitive), while CMs map noise to data in 1-8 steps making backpropagation tractable
- **Core assumption:** Short trajectory preserves enough computation graph fidelity for useful gradient signals to propagate from final image back to parameters
- **Evidence:** Section 1 states CMs "can produce competitive results within 4-8 steps"; Section 3 shows gradient calculation using backpropagation through generation steps

## Foundational Learning

- **Concept:** Reparameterization Trick
  - **Why needed here:** Mathematical bridge treating stochastic sampling as deterministic function suitable for gradient descent
  - **Quick check question:** Can you explain why moving randomness from "inside" a network to "outside" allows for backpropagation?

- **Concept:** f-Divergence (KL, JS, Hellinger)
  - **Why needed here:** Choice of regularization distance as hyperparameter; understanding differences helps explain varying overfitting styles
  - **Quick check question:** If β is set too low causing reward hacking, which term in loss equation is effectively being ignored?

- **Concept:** Consistency Models vs. Diffusion Models
  - **Why needed here:** Efficiency gain rests on specific generative properties of CMs (forcing fθ(xt,t) = fθ(xt',t'))
  - **Quick check question:** Why does enforcing "consistency" across time steps lead to fewer required sampling steps compared to iterative denoising?

## Architecture Onboarding

- **Component map:** Text Prompt c + Gaussian Noise ε → Consistency Function fθ → Image x₀ → Reward Model R → Loss Aggregator (R - β·D)

- **Critical path:**
  1. Trajectory Generation: Forward pass generates image, must be recorded for backprop
  2. Divergence Calculation: Calculate distance between current and reference model predictions over steps
  3. Joint Optimization: Single backward pass updates θ to increase reward and decrease divergence

- **Design tradeoffs:**
  - Choice of Divergence: JS requires sampling (slower/higher variance) while KL/Hellinger have closed-form Gaussian solutions (faster)
  - β tuning: Too high = no learning (stays at base model); Too low = reward hacking (noisy images)

- **Failure signatures:**
  - Reward Hacking: Validation reward rises, human evaluation scores drop; images look high-contrast to game reward classifier
  - Mode Collapse: Same image generated for different prompts (weak regularization or prompt-agnostic reward)
  - OOM: Backpropagating through 8 steps of large UNet/Transformer; reduce batch size or steps

- **First 3 experiments:**
  1. Sanity Check (No Regularization): Run ROCM with β=0 to verify rapid quality degradation confirming reward model overfitting
  2. β Sweep: Linear sweep of β values to find "sweet spot" peak where reward score and human preference align
  3. Baseline Comparison: Compare training time and final score against PPO-based baseline like RLCM

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can ROCM be extended to handle non-differentiable reward functions such as compressibility or incompressibility metrics?
- **Basis in paper:** [explicit] Limitations section states "A limitation of our approach is that it requires differentiable reward signals, as it is first-order and relies on gradient-based optimization. Therefore, it is not directly applicable to tasks involving non-differentiable rewards, such as compressibility or incompressibility, where policy-gradient methods are still necessary."
- **Why unresolved:** Reparameterization trick requires gradient flow through reward model, fundamentally limiting applicability
- **What evidence would resolve it:** Hybrid approach combining ROCM with policy gradient estimators for non-differentiable reward components

### Open Question 2
- **Question:** Is there a principled, automated method for selecting regularization strength β without manual hyperparameter search?
- **Basis in paper:** [inferred] Figure 4 shows β critically controls trade-off between reward maximization and reward hacking with optimal values varying across regularization types; authors manually tuned these values
- **Why unresolved:** No theoretical guidance or adaptive mechanism for β selection provided; current approach requires empirical search
- **What evidence would resolve it:** Adaptive β schedule based on divergence measurements during training or theoretical bounds relating β to reward model capacity

### Open Question 3
- **Question:** What criteria determine optimal choice of f-divergence for given reward model and task?
- **Basis in paper:** [inferred] Figure 6 shows different divergences produce distinct visual styles and failure modes; Table 1 shows mixed results with no single divergence consistently dominating
- **Why unresolved:** Paper empirically compares divergences but provides no theoretical framework for predicting which will work best
- **What evidence would resolve it:** Theoretical analysis connecting divergence properties to reward landscape characteristics or meta-learning approach predicting optimal divergence

## Limitations
- Computational efficiency claims based on comparisons with diffusion models, but direct comparisons with other efficiency-focused methods like DPOK are limited
- Choice of f-divergence and β parameter appears critical and task-dependent with no robust method for selection beyond grid search
- Method assumes differentiable reward model and would fail with non-differentiable reward functions common in practice
- Evaluation comprehensive in automated metrics but human study based on pairwise comparisons rather than absolute quality assessment

## Confidence
- **High Confidence:** Core mechanism of using reparameterization trick for first-order gradient optimization of RLHF objectives is well-grounded and mathematically sound
- **Medium Confidence:** Effectiveness of distributional regularization in preventing reward hacking is empirically validated but generalizability of specific divergence choices requires further validation
- **Low Confidence:** Claim of "superior" performance compared to all baselines should be viewed with caution as differences are not always statistically significant and performance is task-dependent

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Conduct systematic study on how choice of f-divergence and regularization coefficient β affect performance across wider range of tasks and reward models
2. **Real-World Utility Assessment:** Evaluate generated images on practical downstream tasks to assess absolute quality and utility beyond automated metrics
3. **Scalability and Robustness Testing:** Test method on larger consistency models and with different types of reward functions (including non-differentiable ones) to assess scalability and robustness