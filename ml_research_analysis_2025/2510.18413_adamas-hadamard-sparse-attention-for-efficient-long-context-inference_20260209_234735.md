---
ver: rpa2
title: 'Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference'
arxiv_id: '2510.18413'
source_url: https://arxiv.org/abs/2510.18413
tags:
- adamas
- attention
- hadamard
- token
- transform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adamas addresses the efficiency challenge of long-context inference
  in large language models by proposing a sparse attention mechanism that reduces
  the quadratic complexity of self-attention. The core idea is to combine the Hadamard
  transform with bucketization and 2-bit compression to produce compact query and
  key representations, then use a lightweight Manhattan-distance estimator to select
  the most relevant key-value pairs dynamically at the token level.
---

# Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference

## Quick Facts
- **arXiv ID:** 2510.18413
- **Source URL:** https://arxiv.org/abs/2510.18413
- **Reference count:** 16
- **Primary result:** Achieves up to 4.4× self-attention and 1.5× end-to-end speedups on 32K-length sequences while maintaining accuracy comparable to full attention.

## Executive Summary
Adamas introduces a sparse attention mechanism that dramatically improves long-context inference efficiency in large language models. By combining the Hadamard transform with 2-bit quantization and Manhattan-distance-based candidate selection, the method reduces the quadratic complexity of self-attention to near-linear while preserving accuracy. The approach is training-free and dynamically selects the most relevant tokens at the token level rather than using static patterns or coarse page-level selection. Experiments demonstrate that Adamas matches full attention accuracy with as few as 64 tokens and achieves up to 8× higher sparsity than state-of-the-art methods.

## Method Summary
Adamas transforms queries and keys using the Hadamard transform, then applies bucketization and 2-bit compression to create compact representations. These compressed codes are used with a lightweight Manhattan distance estimator to identify the top-k most relevant key-value pairs for each query. This token-level dynamic selection avoids the inefficiencies of static attention patterns and page-level selection used in prior methods. The approach is training-free and leverages existing model parameters while modifying the KV cache to store compressed representations alongside original values.

## Key Results
- Matches full attention accuracy with as few as 64 tokens and becomes nearly lossless at 128 tokens
- Achieves up to 4.4× self-attention and 1.5× end-to-end speedups on 32K-length sequences
- Demonstrates up to 8× higher sparsity than prior state-of-the-art approaches
- Reports perplexity even lower than full attention in some conditions
- Supports sequence lengths up to 100K tokens with consistent performance

## Why This Works (Mechanism)

### Mechanism 1: Hadamard Transform Enables Aggressive Quantization
- Claim: Applying the Hadamard transform to queries and keys suppresses outlier values, allowing accurate 2-bit quantization that would otherwise destroy information.
- Mechanism: The Hadamard transform redistributes variance more evenly across dimensions, preventing extreme-magnitude features from dominating the distribution. This smoothing effect enables bucketization into just 4 levels {0,1,2,3} with minimal information loss.
- Core assumption: Query and key vectors in pretrained LLMs contain outlier features that dominate their distributions, making direct low-bit quantization ineffective.
- Evidence anchors:
  - [abstract] "Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations"
  - [section 3.1] "The Hadamard transform mitigates this issue by redistributing variance more evenly across dimensions and suppressing extreme outliers"
  - [section 4.4] "Adamas w/o Hadamard... achieving near-zero scores across multiple datasets under small token budgets"

### Mechanism 2: Manhattan Distance on Compressed Codes Approximates Attention Weights
- Claim: The negative Manhattan distance computed on 2-bit bucketized codes serves as a lightweight proxy for Q-K dot-product similarity, enabling efficient top-k candidate selection without floating-point attention computation.
- Mechanism: Since bucketization preserves relative ordering within each dimension, the L1 distance between compressed codes correlates with the dot-product similarity in the original space. This allows candidate pre-selection using only integer bitwise operations.
- Core assumption: Bucket thresholds capture sufficient rank-order information to identify top-k relevant keys.
- Evidence anchors:
  - [abstract] "leverages Manhattan-distance estimation for efficient top-k selections"
  - [section 3.2] "we approximate similarity by the negative Manhattan distance: sim(bHQ, bHK) ≈ −∥bHQ − bHK∥₁"
  - [corpus] Related sparse attention methods (SSA, SALE) also use low-bit estimation, confirming this is an active research direction

### Mechanism 3: Token-Level Granularity Achieves Higher Effective Sparsity
- Claim: Selecting individual tokens rather than fixed-size pages reduces token redundancy, enabling 8× higher sparsity ratios than page-level methods while maintaining accuracy.
- Mechanism: Page-level methods (e.g., Quest) select entire pages of tokens, introducing irrelevant KV pairs that dilute attention on truly relevant tokens. Token-level selection directly retrieves the most relevant k tokens regardless of their position.
- Core assumption: The computational cost of fine-grained selection is offset by reduced attention computation on irrelevant tokens.
- Evidence anchors:
  - [abstract] "achieves up to 8× higher sparsity than prior state-of-the-art approaches while maintaining accuracy comparable to full attention"
  - [section 4.2] "Quest achieves competitive performance when the token budget exceeds 1024, but its accuracy drops sharply below 512"
  - [corpus] DAM paper confirms static/predefined masks "fail to capture heterogeneous attention patterns"

## Foundational Learning

- **Hadamard Transform**
  - Why needed here: Understanding how this orthogonal transform achieves O(d log d) computation while enabling outlier suppression, compared to naive O(d²) matrix multiplication.
  - Quick check question: Why does the paper use the recursive Kronecker construction rather than computing H_d directly?

- **Sparse Attention Patterns**
  - Why needed here: Distinguishing between static patterns (StreamingLLM's sliding window), page-level dynamic selection (Quest), and token-level dynamic selection (Adamas) clarifies the design space.
  - Quick check question: What is the fundamental tradeoff between selection granularity and estimation overhead?

- **KV Cache in Autoregressive Decoding**
  - Why needed here: Adamas modifies what is stored in the KV cache (adding 2-bit compressed Hadamard vectors), so understanding baseline KV caching is essential.
  - Quick check question: Why does the paper report 1/16 increase in cache size from storing bHK?

## Architecture Onboarding

- **Component map:**
  Query → Hadamard Transform → Bucketization → Manhattan Estimation → Top-k → Sparse Attention
  KV Cache: Original K,V + Compressed bHK

- **Critical path:** Query → Hadamard Transform → Bucketization → Manhattan Estimation (bottleneck per ablation Table 4) → Top-k → Sparse Attention

- **Design tradeoffs:**
  - 2-bit vs 3-bit bucketization: 3-bit slightly improves accuracy but increases storage 50%; diminishing returns favor 2-bit
  - L1 vs L2 distance: L2 better for single-document reasoning; L1 more robust for multi-document integration
  - Token budget vs accuracy: 64 tokens matches full attention on some tasks; 128 provides near-lossless performance

- **Failure signatures:**
  - Perplexity spikes with token budget <64: insufficient context retained
  - Near-zero F1 scores without Hadamard transform: quantization destroys information
  - Retrieval accuracy degrades at extreme sequence lengths (>100K) with small budgets: candidate pool too sparse

- **First 3 experiments:**
  1. Replicate the ablation in Figure 6: run Adamas with and without Hadamard transform on a single LongBench dataset to verify the outlier suppression mechanism.
  2. Profile kernel-level latency breakdown (replicating Figure 5) to identify whether Manhattan estimation or sparse attention dominates runtime at your target sequence length.
  3. Test token budget sweep on your specific use case: start at 512 and halve until perplexity degrades >5% to find your minimal viable budget.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that the perplexity results below full attention baselines warrant further investigation and that the exact bucketization thresholds need specification.

## Limitations
- The counterintuitive claim of perplexity lower than full attention in some settings requires independent verification
- The 8× sparsity improvement claim lacks direct corpus validation
- Bucketization thresholds {B1, B2, B3} are not specified, creating a critical gap for faithful reproduction
- No evaluation of robustness to extremely long sequences (>100K) with small token budgets

## Confidence

- **High confidence**: The Hadamard transform's role in enabling 2-bit quantization without information loss is well-supported by ablation studies showing catastrophic failure without it. The mechanism of token-level selection providing higher effective sparsity than page-level methods is theoretically sound and partially validated.
- **Medium confidence**: The Manhattan distance estimator's approximation quality on compressed codes is reasonable given the correlation between bucketized ordering and original similarity, but the 2-bit compression approach lacks extensive independent validation. The claimed 4.4× self-attention speedup appears achievable based on kernel profiling, though end-to-end gains may vary with implementation.
- **Low confidence**: The counterintuitive perplexity results below full attention baselines require independent verification. The exact 8× sparsity improvement magnitude needs reproduction, as related work validates the approach but not this specific claim.

## Next Checks

1. **Replicate the Hadamard ablation**: Run Adamas with and without the Hadamard transform on a single LongBench dataset (e.g., GovReport) to verify the mechanism's necessity and quantify the accuracy drop when quantization destroys outlier information.

2. **Profile kernel bottlenecks**: Implement the custom CUDA kernels or equivalent operations and measure the runtime breakdown between Manhattan distance estimation and sparse attention computation at your target sequence length (e.g., 32K) to identify the actual performance bottleneck.

3. **Test perplexity behavior**: Independently verify the claim of lower perplexity than full attention by running perplexity evaluation on PG19 across multiple token budgets (64, 128, 512, 1024) and compare against a properly implemented full attention baseline.