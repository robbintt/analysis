---
ver: rpa2
title: How Small Transformation Expose the Weakness of Semantic Similarity Measures
arxiv_id: '2509.09714'
source_url: https://arxiv.org/abs/2509.09714
tags:
- semantic
- similarity
- code
- metrics
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research exposes fundamental flaws in semantic similarity
  metrics used across software engineering. The study evaluated 18 metrics on controlled
  text and code transformations, revealing catastrophic failures: embedding methods
  like CodeBERT and BERTScore incorrectly identified semantic opposites as similar
  up to 99.9% of the time.'
---

# How Small Transformation Expose the Weakness of Semantic Similarity Measures

## Quick Facts
- arXiv ID: 2509.09714
- Source URL: https://arxiv.org/abs/2509.09714
- Reference count: 40
- Semantic similarity metrics fail catastrophically on controlled transformations, with embedding methods incorrectly identifying semantic opposites as similar up to 99.9% of the time

## Executive Summary
This research reveals fundamental flaws in semantic similarity metrics widely used across software engineering. The study systematically evaluated 18 metrics on controlled text and code transformations, exposing catastrophic failures where embedding methods like CodeBERT and BERTScore incorrectly identified semantic opposites as similar up to 99.9% of the time. The findings demonstrate that switching from cosine to Euclidean distance improved code embedding performance by 24-66%, suggesting computational choices may overshadow model capabilities. LLM-based approaches (GPT-4o, DeepSeek-V3) demonstrated superior semantic discrimination, correctly identifying differences with scores of 0.00-0.29 compared to embedding failures at 0.82-0.99.

## Method Summary
The study evaluated 18 semantic similarity metrics across controlled transformations of text and code. Researchers systematically applied small modifications including synonym replacement, word reordering, variable renaming, and semantic opposites to test metric robustness. The evaluation measured metric performance using correlation coefficients against ground truth similarity scores. Both embedding-based methods (CodeBERT, BERTScore) and LLM-based approaches (GPT-4o, DeepSeek-V3) were tested. Distance computation methods (cosine vs Euclidean) were compared to assess their impact on metric performance.

## Key Results
- Embedding methods failed catastrophically, incorrectly identifying semantic opposites as similar up to 99.9% of the time
- Switching from cosine to Euclidean distance improved code embedding performance by 24-66%
- LLM-based approaches (GPT-4o, DeepSeek-V3) demonstrated superior semantic discrimination with scores of 0.00-0.29 versus embedding failures at 0.82-0.99

## Why This Works (Mechanism)
The study reveals that semantic similarity metric failures stem from fundamental mismatches between how metrics compute similarity and what constitutes true semantic equivalence. Embedding methods rely on vector space representations that can capture syntactic similarity but fail to distinguish semantic opposites when vectors remain close in embedding space. The distance metric choice (cosine vs Euclidean) significantly impacts performance, suggesting that the underlying representation quality may be adequate but the similarity computation method is flawed. LLM-based approaches leverage contextual understanding and reasoning capabilities that enable better discrimination of semantic differences, though their performance depends heavily on prompt engineering quality.

## Foundational Learning
- **Semantic similarity metrics**: Methods for measuring meaning equivalence between text/code - needed to understand evaluation framework; quick check: identify cosine similarity formula
- **Embedding representations**: Vector space encodings of text/code - needed to grasp why vector proximity doesn't guarantee semantic similarity; quick check: explain CodeBERT's training objective
- **Distance metrics**: Computational methods for measuring vector similarity (cosine vs Euclidean) - needed to understand performance variation; quick check: compare cosine vs Euclidean properties
- **LLM contextual understanding**: Large language models' ability to reason about meaning - needed to explain superior discrimination; quick check: describe prompt engineering impact
- **Semantic transformation types**: Controlled modifications testing metric robustness - needed to understand evaluation methodology; quick check: list synonym replacement and semantic opposites
- **Ground truth similarity scoring**: Reference standards for evaluating metric accuracy - needed to interpret correlation coefficient results; quick check: explain perfect vs poor correlation

## Architecture Onboarding
Component Map: Input Text/Code -> Transformation Generator -> 18 Similarity Metrics -> Distance Computation -> Similarity Score -> Ground Truth Comparison

Critical Path: Transformation generation → Metric computation → Distance calculation → Similarity scoring → Accuracy evaluation

Design Tradeoffs: Embedding methods prioritize computational efficiency over semantic discrimination, while LLM approaches trade speed for superior meaning understanding. Cosine distance favors angular relationships while Euclidean captures absolute vector differences.

Failure Signatures: Embedding methods consistently score semantic opposites as highly similar (0.82-0.99), indicating vector representations fail to capture semantic distinctions. LLM methods show low scores (0.00-0.29) on differences but may struggle with subtle semantic variations.

First Experiments:
1. Test embedding methods with Euclidean distance on semantic opposites to verify 24-66% performance improvement
2. Compare LLM performance across different prompt engineering strategies for the same transformations
3. Evaluate metric performance on real-world code repositories with naturally occurring semantic differences

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on controlled transformations rather than real-world codebases, limiting generalizability to practical software engineering scenarios
- Performance differences between distance metrics suggest computational choices may overshadow model capabilities
- Study doesn't fully isolate representation quality from distance metric effects in explaining observed failures

## Confidence
- **High confidence**: Embedding methods' failure on semantic opposites (99.9% error rate) - clear, reproducible finding with controlled inputs
- **Medium confidence**: LLM superiority claims - results show better discrimination but performance varies across transformation types
- **Medium confidence**: Distance metric impact - 24-66% improvement suggests computational choices matter but doesn't fully isolate representation quality

## Next Checks
1. Replicate findings on diverse real-world code repositories with multiple transformation types to assess practical applicability
2. Conduct ablation studies comparing embedding representations with different distance metrics to disentangle representation quality from computational choices
3. Test additional LLM models and prompt variations to determine whether observed superiority represents model capability or specific prompting strategies