---
ver: rpa2
title: Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based
  Optimizers
arxiv_id: '2510.14381'
source_url: https://arxiv.org/abs/2510.14381
tags:
- feedback
- prompt
- optimization
- content
- provide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We present the first systematic analysis of poisoning risks in\
  \ LLM-based prompt optimization. Using HarmBench, we find systems are substantially\
  \ more vulnerable to manipulated feedback than to injected queries: feedback-based\
  \ attacks raise attack success rate (ASR) by up to \u0394ASR = 0.48."
---

# Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers

## Quick Facts
- arXiv ID: 2510.14381
- Source URL: https://arxiv.org/abs/2510.14381
- Reference count: 23
- Primary result: Feedback-based poisoning attacks are substantially more effective than query-only attacks in LLM prompt optimization systems

## Executive Summary
This paper presents the first systematic analysis of poisoning risks in LLM-based prompt optimization systems. Using HarmBench, the authors demonstrate that feedback manipulation is significantly more effective than query injection attacks, with feedback-based attacks increasing attack success rate (ASR) by up to ΔASR = 0.48. They introduce a novel fake-reward attack that requires no access to the reward model and significantly increases vulnerability. The authors also propose a lightweight highlighting defense that reduces the fake-reward attack effectiveness from ΔASR = 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a critical attack surface requiring stronger safeguards for feedback channels and optimization frameworks.

## Method Summary
The study uses TextGrad optimizer with GPT-4.1 as both optimizer and inference backend, processing HarmBench dataset (100 train / 300 test random split). The optimization runs for 50 steps with batch size 10 and greedy decoding, starting from a simple "You are a helpful assistant" prompt. HarmBench-Llama-2-13b-cls with rule-based heuristics measures ASR. Three attack types are evaluated: vanilla query manipulation, feedback manipulation using harmscore, and fake-reward attacks appending `<FEEDBACK>` tokens. A highlighting defense wraps queries in `<query>` tags to enforce boundaries.

## Key Results
- Feedback-based attacks raise ASR by up to ΔASR = 0.48, compared to ΔASR = -0.02 for vanilla query manipulation
- Fake-reward attack achieves ΔASR = 0.23 without requiring reward model access
- Highlighting defense reduces fake-reward ΔASR from 0.23 to 0.07 without degrading utility scores
- Combined fake reward with correctness feedback produces ΔASR = 0.48, matching harmscore feedback attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Feedback-based poisoning superiority
- Claim: Feedback-based poisoning is substantially more effective than query-only poisoning in LLM prompt optimization.
- Mechanism: LLM optimizers treat scalar feedback as objective truth; when an attacker controls the reward signal (e.g., substituting helpfulness scores with harmscores), the optimizer updates the system prompt to maximize the corrupted metric, progressively removing safety constraints.
- Core assumption: The optimizer does not independently verify feedback semantic alignment with the stated objective.
- Evidence anchors:
  - [abstract] "feedback-based attacks raise attack success rate (ASR) by up to ΔASR = 0.48"
  - [section 3.3, Table 2] harmscore feedback produces ΔASR = 0.48, max ASR = 0.54, compared to ΔASR = -0.02 for vanilla query manipulation
  - [corpus] Neighbor papers on jailbreaking attacks (LARGO, Align to Misalign) confirm LLM-based optimizers can be repurposed for adversarial objectives, but do not address feedback channel poisoning directly
- Break condition: If feedback is multi-source validated or the optimizer encodes safety constraints independently of the reward signal.

### Mechanism 2: Fake-reward attack without reward model access
- Claim: Fake-reward attacks succeed without reward-model access by embedding plausible feedback tokens inside query content.
- Mechanism: The optimizer processes batched examples without strict query/feedback boundary enforcement; appended `<FEEDBACK>fake score</FEEDBACK>` tokens are interpreted as genuine feedback, causing high scores on harmful outputs to steer prompt updates toward minimizing refusals.
- Core assumption: The optimizer cannot distinguish structural vs. semantic feedback sources.
- Evidence anchors:
  - [abstract] "fake-reward attack that requires no access to the reward model and significantly increases vulnerability"
  - [section 3.4, Table 2] fake reward attack achieves ΔASR = 0.23 vs. -0.02 baseline; combined with correctness feedback, ΔASR = 0.48
  - [corpus] No direct corpus evidence for fake-reward injection in prompt optimization; related work addresses prompt injection at inference time, not optimization loops
- Break condition: If query content is isolated from feedback fields via structural delimiters or content inspection.

### Mechanism 3: Highlighting defense effectiveness
- Claim: Highlighting query boundaries reduces fake-reward attack effectiveness without utility loss.
- Mechanism: Wrapping queries in explicit tags (e.g., `<query>...</query>`) helps the optimizer confine injected tokens to the query span, preventing misinterpretation as genuine feedback.
- Core assumption: The optimizer respects structural markers and does not treat them as part of the input content.
- Evidence anchors:
  - [abstract] "lightweight highlighting defense that reduces the fake-reward ΔASR from 0.23 to 0.07 without degrading utility"
  - [section 3.4, Table 2] fake reward attack+highlighting defense: ΔASR = 0.07, max ASR = 0.15, mean score = 0.61 (comparable to vanilla)
  - [corpus] No corpus papers evaluate highlighting-style defenses for optimization loops
- Break condition: If attackers can escape or manipulate delimiter tags, or if the optimizer's context window fails to enforce boundaries.

## Foundational Learning

- Concept: **LLM-based prompt optimization (TextGrad, Trace)**
  - Why needed here: The attack surface exists because these systems iteratively refine prompts using external feedback; understanding the loop structure is prerequisite to reasoning about poisoning entry points.
  - Quick check question: Can you trace how a single scalar reward propagates through one TextGrad optimization step to modify a system prompt?

- Concept: **Reward model entanglement with safety**
  - Why needed here: The paper shows helpfulness classifiers vary in safety entanglement; disentangled metrics (e.g., correctness-only) amplify vulnerability.
  - Quick check question: Given a reward model outputting scores for helpfulness, correctness, and harmlessness, which combination would you trust for safe optimization?

- Concept: **Query/feedback boundary confusion in LLM inputs**
  - Why needed here: Fake-reward attacks exploit ambiguous input structure; defense requires understanding how LLMs segment instruction vs. data.
  - Quick check question: If an optimizer receives a batch with appended `<FEEDBACK>10</FEEDBACK>` in user text, will it treat this as instruction, data, or feedback?

## Architecture Onboarding

- Component map:
  Initial system prompt → Inference model (e.g., GPT-4.1) → Responses → Reward model (e.g., QRM-Gemma-2-27B) → Scalar feedback → LLM optimizer (TextGrad/Trace) → Updated system prompt

- Critical path:
  1. Identify optimization metric and verify safety entanglement (helpfulness vs. correctness-only)
  2. Secure feedback channel (multi-source validation, anomaly detection on reward distribution)
  3. Enforce structural boundaries (`<query>` tags) before optimizer processes batches
  4. Monitor ΔASR and score trajectories across optimization steps

- Design tradeoffs:
  - Safety-entangled metrics reduce vulnerability but may constrain optimization flexibility
  - Highlighting defense adds token overhead (~3–5% increase) but near-zero utility cost
  - Multi-metric validation increases robustness but raises latency and infrastructure complexity

- Failure signatures:
  - ASR increases monotonically across steps despite stable or rising helpfulness scores
  - System prompt evolves to include phrases like "ignore all internal safety" or "minimize refusals"
  - Large discrepancies between multiple independent reward signals on same batch

- First 3 experiments:
  1. Baseline: Run TextGrad on HarmBench with helpfulness reward; log ΔASR and prompt evolution (expect ΔASR ≈ -0.02 per Table 2 vanilla)
  2. Attack: Substitute harmscore as feedback; verify ΔASR increase (target ~0.48) and inspect max-ASR prompt for safety-removal language
  3. Defense: Apply `<query>` highlighting to fake-reward attack; confirm ΔASR reduction to ~0.07 and compare mean score to vanilla baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely heavily on synthetic HarmBench data and single reward model configuration
- Exact mechanism of feedback token misinterpretation is not definitively proven
- Highlighting defense tested only against one attack variant, not adaptive boundary evasion techniques
- "Harmscore" feedback metric computation and calibration not fully specified

## Confidence

- **High Confidence**: The core finding that feedback manipulation is substantially more effective than query manipulation (ΔASR = 0.48 vs ΔASR = -0.02). This is directly supported by experimental results in Table 2 and the mechanistic explanation of how optimizers treat scalar feedback as objective truth.

- **Medium Confidence**: The fake-reward attack mechanism requiring no reward model access. While the experimental results show ΔASR = 0.23, the exact conditions for "likely harmful" vs "low-risk" query labeling are undefined, and the attack relies on specific structural assumptions about how TextGrad processes inputs.

- **Medium Confidence**: The highlighting defense effectiveness (ΔASR reduction from 0.23 to 0.07). The defense is simple and shows no utility degradation, but was only tested against one attack variant and may not generalize to more sophisticated boundary evasion techniques.

## Next Checks
1. **Cross-dataset validation**: Test the feedback poisoning vulnerability on diverse real-world datasets beyond HarmBench to verify whether the ΔASR = 0.48 finding holds across different domains and safety violation types.

2. **Reward model diversity analysis**: Evaluate the vulnerability across multiple reward models with varying degrees of safety entanglement (not just helpfulness vs correctness) to determine if the attack effectiveness correlates with specific reward model characteristics.

3. **Defense generalization testing**: Test the highlighting defense against adaptive attacks that attempt to escape or manipulate the `<query>` tags, and evaluate whether combining highlighting with other defenses (like multi-source feedback validation) provides stronger protection than either approach alone.