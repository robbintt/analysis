---
ver: rpa2
title: Exploring Depth Generalization in Large Language Models for Solving Recursive
  Logic Tasks
arxiv_id: '2512.02677'
source_url: https://arxiv.org/abs/2512.02677
tags:
- depth
- generalization
- recursive
- length
- recursion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates depth generalization in transformers, a
  problem where models fail to generalize to deeper recursive structures than seen
  during training, even when handling longer non-nested sequences well. The authors
  hypothesize that this limitation stems from transformers' inability to maintain
  stack-like behavior, essential for resolving nested dependencies.
---

# Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks

## Quick Facts
- arXiv ID: 2512.02677
- Source URL: https://arxiv.org/abs/2512.02677
- Reference count: 38
- Primary result: Proposed looped locate-and-replace pipeline significantly outperforms end-to-end transformers on depth generalization, maintaining ~67% accuracy at depth 12 vs ~52% for baseline on Boolean algebra tasks.

## Executive Summary
This paper investigates depth generalization in transformers, a problem where models fail to generalize to deeper recursive structures than seen during training, even when handling longer non-nested sequences well. The authors hypothesize that this limitation stems from transformers' inability to maintain stack-like behavior, essential for resolving nested dependencies. To address this, they propose a looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents using a locator model to identify solvable subexpressions and a replacer model to evaluate them. Experiments on Boolean algebra, propositional logic, and recursive arithmetic tasks show that the method significantly outperforms end-to-end transformers, with accuracy dropping much more slowly as recursion depth increases.

## Method Summary
The Looped Locate-and-Replace (LLR) pipeline uses two specialized GPT-2 models: a locator that identifies base-case subexpressions using ALiBi positional encoding, and a replacer that evaluates these subexpressions using NoPE encoding. The input expression is reversed, the locator identifies end positions of evaluable patterns, the replacer generates reduced expressions, and the process repeats iteratively until completion or max iterations reached. The method is evaluated on three synthetic tasks in postfix notation: Boolean algebra evaluation, propositional logic truth tables, and recursive arithmetic, showing significantly better depth generalization than standard end-to-end transformers.

## Key Results
- Depth generalization shows strong correlation (PCC = -0.9210) between recursion depth and accuracy, while sequence length shows weak correlation (PCC = -0.2068)
- LLR maintains ~67% accuracy at depth 12 on Boolean algebra vs ~52% for end-to-end transformers
- ALiBi positional encoding achieves 96% accuracy vs 56% for absolute PE/RoPE in the locator model
- Error accumulation across loop iterations shows gradual decay rather than exponential collapse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth generalization failure correlates strongly with recursion depth, not sequence length.
- Mechanism: Recursive problems require sequential depth-wise computation where each layer in a transformer can only process approximately one level of recursion. The hidden state visualizations show correct predictions occur at progressively later layers as problem depth increases, creating a hard architectural ceiling.
- Core assumption: Transformers resolve nested dependencies layer-by-layer rather than in parallel.
- Evidence anchors:
  - [abstract] "depth generalization...refers to the number of nested levels in a hierarchical problem"
  - [section] Table 1 shows PCC(depth, accuracy) = -0.9210 vs PCC(length, accuracy) = -0.2068
  - [corpus] Related work on compositional generalization (COGS/ReCOGS) confirms transformers struggle with structural generalization
- Break condition: If tasks could be reformulated to flatten hierarchy or use parallelizable decomposition, this mechanism would not apply.

### Mechanism 2
- Claim: The looped locate-and-reduce pipeline mitigates depth decay by constraining each forward pass to base-case evaluations only.
- Mechanism: The locator identifies subexpressions that can be evaluated immediately (rightmost patterns in postfix notation). The replacer evaluates only these base cases and rewrites the expression, reducing depth by exactly one. Repeating this loop bounds the effective depth any single forward pass must handle.
- Core assumption: Base-case patterns are reliably detectable; errors accumulate slowly across iterations.
- Evidence anchors:
  - [abstract] "decomposes recursive problems into manageable subcomponents"
  - [section] Figure 7 shows LLR maintains ~67% accuracy at depth 12 vs ~52% for end-to-end on Boolean algebra
  - [corpus] LADDER and Mixture-of-Recursions papers similarly show recursive decomposition aids generalization
- Break condition: If locator fails to identify correct boundaries, or if replacer introduces errors that compound multiplicatively across loops.

### Mechanism 3
- Claim: Architectural decoupling with specialized positional encodings improves pattern localization.
- Mechanism: ALiBi for locator captures relative distances useful for identifying base-case boundaries. NoPE for replacer leverages pure autoregressive generation without position interference. Reversing input sequences positions base-cases first, reducing interference from non-evaluable prefixes.
- Core assumption: Pattern matching and generation have conflicting positional requirements.
- Evidence anchors:
  - [section] Figure 8 shows ALiBi achieves 96% accuracy vs 56% for absolute PE/RoPE
  - [section] "NoPE...outperforms alternatives like RoPE and ALiBi for copying tasks"
  - [corpus] Weak direct evidence; corpus papers do not address positional encoding for recursion
- Break condition: If tasks require absolute position information, or if autoregressive generation order matters for correctness.

## Foundational Learning

- Concept: **Postfix (Reverse Polish) Notation**
  - Why needed here: All tasks use postfix format where operators follow operands (e.g., `1 1 +` not `1 + 1`). Understanding this is essential for parsing the paper's examples and implementing the replacer logic.
  - Quick check question: Given `2 3 4 + *`, what is the evaluation order?

- Concept: **RASP (Restricted Access Sequence Processing)**
  - Why needed here: The paper uses RASP to formally prove transformers cannot efficiently implement match-and-replace operations. This provides the theoretical justification for the two-model architecture.
  - Quick check question: Why can't a transformer access `seq[i]` where `i` is a variable?

- Concept: **Stack-Based Evaluation**
  - Why needed here: Recursive evaluation fundamentally requires push/pop operations on intermediate states. The paper's central claim is that transformers lack this inductive bias.
  - Quick check question: How many stack operations are needed to evaluate `((1 + 2) * (3 - 4))`?

## Architecture Onboarding

- Component map:
  - Input expression -> Reverser -> Locator (GPT-2 + ALiBi + binary classification) -> Boundary detection -> Slicer -> Reverser -> Replacer (GPT-2 + NoPE + LM head) -> Reduced expression -> Reverser -> Loop controller

- Critical path:
  1. Input expression → reverse sequence
  2. Locator identifies end positions of evaluable base cases
  3. Slice input at marked boundaries
  4. Replacer generates output for each segment (evaluate base cases, copy rest)
  5. Concatenate segments → reverse back → check termination
  6. Loop until done

- Design tradeoffs:
  - ALiBi vs inverse-absolute PE: Inverse-absolute achieves 99.9% but requires re-computing positions during generation (inefficient). ALiBi chosen for autoregressive compatibility.
  - Two models vs one: Decoupling allows independent optimization but doubles parameters/training.
  - Reversing input: Improves replacer accuracy but adds preprocessing overhead.

- Failure signatures:
  - Accuracy drops sharply after depth ~8 in arithmetic tasks (near 0%)
  - Locator misses boundaries on longer-than-training sequences
  - Replacer hallucinates tokens when base-case pattern is corrupted
  - Loop exceeds max iterations (set to 12 in experiments)

- First 3 experiments:
  1. Replicate the Boolean algebra depth sweep (depths 5-12) comparing end-to-end GPT-2 vs LLR; verify PCC pattern.
  2. Ablate the reverser: run replacer on non-reversed inputs; expect accuracy drop per Figure 6 intuition.
  3. Stress-test locator generalization: train on max depth 5, evaluate boundary detection accuracy at depths 8-10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What explicit architectural modifications (beyond scaling or positional encodings) can enable true stack-like behavior in transformers for recursive reasoning?
- Basis in paper: [explicit] Authors state "depth generalization may demand architectural innovations beyond standard positional biases or data scaling, such as explicit memory mechanisms or syntactic scaffolding."
- Why unresolved: The paper demonstrates the limitation but does not implement or test specific architectural solutions like explicit memory or stack augmentation.
- What evidence would resolve it: Systematic comparison of transformer variants augmented with explicit stack/memory mechanisms on depth generalization benchmarks.

### Open Question 2
- Question: Why does increasing model size have negligible impact on depth generalization, given that larger models might be expected to develop more sophisticated recursive strategies?
- Basis in paper: [explicit] Authors report "accuracy difference between small 4-layer configurations and large 8-layer architectures never exceeds 1% at any depth level," calling this "surprising."
- Why unresolved: The paper hypothesizes "surface-level pattern matching" but does not mechanistically explain why additional capacity fails to enable deeper recursion.
- What evidence would resolve it: Mechanistic interpretability analysis comparing how different-sized models process recursive structures at the attention head level.

### Open Question 3
- Question: Do the depth generalization findings on synthetic logic tasks transfer to natural language domains with recursive structures (e.g., nested clauses, coreference chains)?
- Basis in paper: [inferred] All experiments use controlled synthetic domains; applicability to real-world NLP tasks like parsing or code generation is claimed but untested.
- Why unresolved: The paper mentions applications to "code generation, automated theorem proving, parsing" but evaluates only on synthetic arithmetic, Boolean, and propositional logic tasks.
- What evidence would resolve it: Evaluation of depth generalization on natural language benchmarks with controllable syntactic nesting depth.

## Limitations
- Theoretical RASP analysis assumes exact positional access requirements that may not fully capture transformer behavior in practice
- Locator model's boundary detection relies heavily on ALiBi positional encoding without thorough exploration of alternatives
- Evaluation limited to synthetic postfix notation tasks; real-world recursive structure performance unproven

## Confidence
- **High Confidence**: The core empirical finding that depth generalization fails dramatically in standard transformers (PCC = -0.9210 for depth vs accuracy) is well-supported by the experimental data across multiple task types.
- **Medium Confidence**: The claim that the looped locate-and-replace architecture fundamentally solves depth generalization is supported but limited to synthetic tasks. The error accumulation mechanism is plausible but not rigorously quantified.
- **Low Confidence**: The theoretical RASP proof's practical implications for transformer design are asserted but not fully validated against empirical observations. The choice of ALiBi/NoPE combination appears empirically effective but lacks theoretical justification.

## Next Checks
1. **Ablation on Position Encoding**: Systematically test the locator and replacer with various positional encoding combinations (RoPE, absolute PE, sinusoidal) to quantify the contribution of ALiBi/NoPE beyond just binary success/failure.

2. **Error Propagation Analysis**: Track accuracy per iteration across the loop to empirically measure error accumulation rates and determine if the gradual decay is logarithmic, linear, or exponential.

3. **Real-World Recursive Tasks**: Evaluate the pipeline on naturally occurring recursive problems (e.g., code parsing, mathematical expression simplification from real data) to test generalization beyond synthetic postfix notation.