---
ver: rpa2
title: 'JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation
  and Scene Segmentation'
arxiv_id: '2505.10057'
source_url: https://arxiv.org/abs/2505.10057
tags:
- knowledge
- student
- distillation
- multi-task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JointDistill, an adaptive multi-task distillation
  approach for jointly solving depth estimation and scene segmentation. Unlike static
  knowledge transfer from multiple teachers, the method dynamically adjusts the knowledge
  amount from each teacher based on the student model's current learning ability,
  using periodic feedback from a validation dataset.
---

# JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation

## Quick Facts
- arXiv ID: 2505.10057
- Source URL: https://arxiv.org/abs/2505.10057
- Authors: Tiancong Cheng; Ying Zhang; Yuxuan Liang; Roger Zimmermann; Zhiwen Yu; Bin Guo
- Reference count: 40
- Primary result: Adaptive multi-task distillation with feedback-driven weights and trajectory-based knowledge preservation improves joint depth estimation and scene segmentation by 9.04% (Cityscapes) and 2.83% (NYU-v2) over state-of-the-art.

## Executive Summary
JointDistill introduces an adaptive multi-task distillation approach that dynamically adjusts knowledge transfer weights from single-task teachers based on student validation performance, avoiding the pitfalls of static distillation. By employing a knowledge trajectory mechanism that records essential teacher attention points over time, the method prevents catastrophic forgetting while maintaining high efficiency. Evaluated on Cityscapes and NYU-v2 benchmarks, JointDistill significantly outperforms existing methods in both segmentation and depth estimation tasks with lower storage and computational requirements.

## Method Summary
JointDistill trains a single multi-task student model using two single-task teacher models (segmentation and depth estimation). A connector module fuses teacher features with dynamic weights that are adjusted based on student validation performance feedback. The method employs a knowledge trajectory mechanism that records top-k attention map peaks from teachers over time to guide student learning and prevent forgetting. The student is trained with a combined loss including task-specific losses, logit-based distillation, and trajectory-based distillation, with weight updates occurring at validation intervals.

## Key Results
- Achieves 9.04% average task improvement on Cityscapes benchmark
- Achieves 2.83% average task improvement on NYU-v2 benchmark
- Reduces memory usage by 14% and training time by 14% compared to EATA baseline
- Improves depth estimation Rel Err from 36.83 to 23.97 with adaptive KD alone

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Driven Adaptive Weight Adjustment
Dynamically adjusts knowledge transfer weights from each teacher based on student validation performance yields better multi-task learning than fixed weights. The system divides training into timeframes and at each validation interval measures student performance per task compared to teacher baseline to compute feedback scores. Teacher weights are updated via gradient descent, increasing weight for tasks where student shows strong progress and reducing it where student struggles. Core assumption is that validation set performance reflects true task-specific learning ability and learning dynamics are temporally non-uniform across tasks.

### Mechanism 2: Trajectory-Based Knowledge Preservation
Recording essential knowledge as attention-based trajectories over time prevents catastrophic forgetting with lower storage cost than parameter replay. At each timeframe, features are converted to 2D attention maps and top-k pixel locations with highest attention values are extracted as essential points. Consecutive points form trajectories and student is constrained to follow connector module's trajectories using L1 distance. Core assumption is that high-attention spatial locations capture task-critical knowledge and trajectory alignment preserves learning history without storing full model snapshots.

### Mechanism 3: Connector-Mediated Multi-Teacher Fusion
A trainable connector module that fuses multiple teacher features with task-specific heads enables coherent knowledge transfer to a unified student. Single-task teacher features are concatenated and processed through Conv-BN-ReLU layers, then passed through task-specific connector heads weighted by adaptive weights. The output serves as soft targets for student distillation via KL divergence. Core assumption is that teacher features are sufficiently aligned in representation space that concatenation and fusion can produce meaningful combined knowledge.

## Foundational Learning

- **Concept: Multi-Task Learning (MTL) with Shared Backbone**
  - Why needed here: JointDistill trains single student with shared backbone and task-specific heads; understanding tradeoffs (parameter efficiency vs. task interference) is essential
  - Quick check question: Can you explain why naive MTL often underperforms single-task models, and what role gradient conflict plays?

- **Concept: Knowledge Distillation (Logit-Based)**
  - Why needed here: Method transfers knowledge via KL divergence between student and connector logits; understanding soft targets and temperature scaling is prerequisite
  - Quick check question: Why do soft targets (logits) often transfer more information than hard labels?

- **Concept: Attention Mechanisms for Feature Importance**
  - Why needed here: Trajectory construction relies on converting features to 2D attention maps and extracting peak locations
  - Quick check question: How would you convert a CNN feature map (C×H×W) to a 2D spatial attention map, and what does a high-attention location signify?

## Architecture Onboarding

- **Component map:** Teachers (DeepLabV3-ResNet-101 for segmentation; separate depth estimator) -> Connector Module (feature concatenation → Conv-BN-ReLU×2 → Task-specific heads with dynamic weights) -> Student (shared backbone ResNet-50 + task-specific heads) -> Validation Set (15% held-out for periodic feedback) -> Trajectory Buffer (stores top-k essential points per timeframe)

- **Critical path:** 1) Forward pass through teachers → extract features and logits 2) Connector fuses features → produces weighted soft targets 3) Student forward pass → produces logits 4) Compute combined loss and connector distillation loss 5) At validation intervals: compute feedback scores → update weights → update trajectory buffer

- **Design tradeoffs:** k (trajectory points): higher k captures more knowledge but increases storage; paper uses k=10. α (weight sensitivity): controls how aggressively weights shift; paper finds [1, 2] optimal. Validation frequency: too frequent → noisy feedback; too infrequent → slow adaptation. Trajectory length t: longer t preserves more history but increases memory.

- **Failure signatures:** Weight collapse: one weight → 0 (task ignored); check feedback score variance. Trajectory drift: student attention maps diverge from connector; monitor trajectory loss. Stagnant performance: validation metrics plateau early; may indicate α too low or validation set too small.

- **First 3 experiments:** 1) Reproduce baseline: Train naive MTL student on Cityscapes to confirm reported mIoU ~0.8149, Rel Err ~36.8. 2) Ablate Adap-KD alone: Add feedback-based weight adjustment without trajectory; verify Rel Err drops to ~24. 3) Vary trajectory length t: Test t ∈ {3, 5, 10, 20} to find stability point; monitor trajectory loss and final mIoU.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does performance change when using more sophisticated multi-modal fusion strategies (beyond simple scaled dot-product attention) to combine multi-task student with textual prompts? The authors state they combined image feature with text features via scaled dot-product but believe further improvement could be achieved with advanced fusion.

- **Open Question 2:** To what extent is self-adaptive distillation mechanism sensitive to size and distribution of validation set used for performance feedback? Method relies on splitting 15% of training data to create validation set for calculating feedback scores, but does not analyze performance if this set is very small or slightly out-of-distribution.

- **Open Question 3:** Does "knowledge trajectory" mechanism, which relies on sparse top-k essential points, fail to capture critical structural context in highly complex scenes? Method records trajectories by extracting only top-k essential points from 2D attention maps to reduce storage, but this assumes localized high-attention points contain sufficient information without needing global context.

## Limitations
- Paper lacks precise architectural specifications for connector module (convolutional channel sizes, number of layers) and exact method for attention map extraction and trajectory windowing, critical for faithful reproduction
- While experimental results show improvements over several baselines, direct comparison to state-of-the-art methods is limited as some recent approaches are not included in evaluation
- Efficiency gains (14% less memory, 14% less time vs. EATA) are demonstrated on specific benchmarks but not systematically evaluated across diverse model sizes or datasets

## Confidence
- **High confidence** in core conceptual contributions: adaptive weight adjustment based on validation feedback and trajectory-based knowledge preservation are novel and well-motivated
- **Medium confidence** in empirical results: reported improvements are significant, but lack of architectural details and ablation studies for key hyperparameters introduces uncertainty about reproducibility
- **Low confidence** in scalability claims: efficiency gains are demonstrated on specific benchmarks but not systematically evaluated across diverse model sizes or datasets

## Next Checks
1. Reproduce baseline and ablated models: Train naive MTL student and Adap-KD-only models on Cityscapes to verify reported metrics (mIoU ~0.8149, Rel Err ~36.8 → ~24)
2. Vary trajectory hyperparameters: Test trajectory length t ∈ {3, 5, 10, 20} and top-k values to identify stability points and optimal configurations
3. Cross-dataset robustness: Evaluate JointDistill on third dataset (e.g., KITTI) to assess generalization beyond Cityscapes and NYU-v2