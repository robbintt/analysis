---
ver: rpa2
title: Direct Motion Models for Assessing Generated Videos
arxiv_id: '2505.00209'
source_url: https://arxiv.org/abs/2505.00209
tags:
- motion
- videos
- video
- generated
- trajan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating the motion quality
  of generated videos, which is not well captured by existing metrics like FVD. The
  authors propose TRAJAN, a novel method based on auto-encoding point tracks to obtain
  motion features.
---

# Direct Motion Models for Assessing Generated Videos

## Quick Facts
- arXiv ID: 2505.00209
- Source URL: https://arxiv.org/abs/2505.00209
- Reference count: 40
- Key outcome: TRAJAN, a novel motion quality metric using auto-encoding point tracks, shows better sensitivity to temporal distortions and predicts human evaluations of video consistency more accurately than existing metrics like VideoMAE and I3D.

## Executive Summary
The paper addresses the challenge of evaluating motion quality in generated videos, which existing metrics like FVD poorly capture. TRAJAN proposes using point tracks auto-encoding to obtain motion features that are disentangled from semantic content. By focusing on point tracks rather than pixel reconstruction or action recognition, TRAJAN demonstrates markedly improved sensitivity to temporal distortions and better prediction of human ratings for temporal consistency and realism. The method can evaluate video distributions, pairs, or individual videos while providing interpretable spatiotemporal localization of generative inconsistencies.

## Method Summary
TRAJAN extracts point tracks from videos using BootsTAPIR, obtaining trajectories of individual points across frames. These tracks are encoded using a Perceiver-style architecture that aggregates the unordered set of track tokens into fixed-size representations through cross-attention. The model is trained as an autoencoder to reconstruct tracks from query points, with the reconstruction error (measured by Average Jaccard) serving as the motion quality score. The key innovation is the disentanglement of motion from semantic content, achieved by focusing on point trajectories rather than pixel-level details or action labels.

## Key Results
- TRAJAN shows superior sensitivity to temporal distortions in synthetic data compared to VideoMAE, I3D, and motion histograms
- The metric better predicts human evaluations of temporal consistency and realism in generated videos
- TRAJAN can localize generative video inconsistencies spatiotemporally, providing interpretability for failure modes

## Why This Works (Mechanism)

### Mechanism 1: Motion-Semantics Disentanglement via Point Tracking
TRAJAN uses BootsTAPIR to extract point trajectories that naturally encode temporal coherence without reconstructing pixel-level details. This isolates motion quality from appearance/semantic content, allowing the model to detect unrealistic motion patterns (like morphing fingers) that deviate from learned distributions of real-world motion.

### Mechanism 2: Perceiver-Based Set Encoding for Permutation Invariance
The variable-sized, orderless point track sets are compressed into fixed-size representations using a Perceiver-style cross-attention architecture. This approach ensures permutation invariance (track ordering doesn't matter) while preserving dense motion field information through self-attention with occlusion-aware masking.

### Mechanism 3: Reconstruction Error as Realism Proxy
The autoencoder learns priors about plausible motion from real videos during training. At inference, unrealistic motion patterns are harder to reconstruct, yielding higher reconstruction error (lower Average Jaccard). This "familiarity" assumption means the model generalizes to real motion but struggles with implausible motion.

## Foundational Learning

- **Point Tracking (BootsTAPIR/TAP-Vid)**: Essential for TRAJAN's input representation; understanding what point tracks capture and their limitations. Quick check: Given a video with a person walking behind a pole, what happens to tracks on the person during occlusion?

- **Perceiver / Set-Based Architectures**: Core to the encoder design; understanding why standard transformers fail for unordered track sets. Quick check: Why can't we simply concatenate all tracks and feed to a standard transformer?

- **Average Jaccard Metric**: Primary evaluation metric for reconstruction quality; combines position accuracy and occlusion handling. Quick check: How does Average Jaccard differ from pure L2 distance for evaluating point predictions?

## Architecture Onboarding

**Component map:**
Video → BootsTAPIR → Point Tracks S = {(x_t,j, y_t,j, o_t,j)} → Track Encoder (self-attention per track + readout token) → Perceiver Aggregator (cross-attention to 128 latent tokens) → Projection → φ_S (128 × 64) → Fréchet Distance / Decoder + Query Point → Reconstructed Track → Average Jaccard

**Critical path:**
1. Track extraction quality (BootsTAPIR failures propagate)
2. Occlusion masking in attention (determines invariance)
3. Query point decoder training (L1 loss weight 5000 vs. occlusion loss 1e-8 heavily prioritizes position)

**Design tradeoffs:**
- 150-frame clips vs. 24 in prior work: better long-range motion but higher compute
- Huber loss weight 5000 vs. occlusion 1e-8: prioritizes motion over occlusion prediction
- Upsampling operator with temporal window: improves localization but adds complexity

**Failure signatures:**
- High reconstruction score but physically implausible motion (e.g., Figure 6: glass collapsing smoothly)
- Low inter-rater agreement (σ≈0.5) suggests human ground truth is noisy
- Videos with semantic implausibility but smooth motion may score incorrectly

**First 3 experiments:**
1. **Validation run:** Extract tracks from 10 real + 10 generated videos, visualize reconstruction error heatmaps per Figure 9 to verify spatiotemporal localization works
2. **Ablation:** Replace Perceiver with simple mean-pooling of track embeddings to measure aggregation importance
3. **Corruption sensitivity:** Apply elastic transformations from §5.1 at different levels; confirm temporal/spatial ratio matches Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
How can evaluation models be developed to assess physical plausibility and "how something *should* move," rather than just reconstructing observed tracks? The paper notes that future work must develop models that "make more accurate judgements not just about how something does move, but how it should move if it was obeying physical principles." This is unresolved because TRAJAN assigns high scores to smooth but impossible motions (e.g., a glass collapsing into a table) because the tracks are easily reconstructed, failing to capture semantic or physical violations.

### Open Question 2
How can human evaluation protocols be redesigned to reduce the high inter-rater variability observed when assessing video consistency? The paper notes that "individual people seem to care about different aspects of videos," suggesting "future work is needed to determine how to elicit better judgements from human raters." This is unresolved because the study found low consistency across human participants (Inter-rater $\sigma \approx 0.5$), complicating the use of human judgment as a stable ground truth for training or validating models.

### Open Question 3
Does the decoupling of semantic content from motion features inherently limit the ability of metrics to predict human "realism" ratings? The paper notes in the Discussion (Figure 13) that humans rate videos with plausible motion but weird semantics (e.g., dogs playing poker) as unrealistic, whereas TRAJAN rates them highly. This is unresolved because TRAJAN explicitly focuses on motion independent of appearance/semantics, but human realism judgments are inextricably linked to semantic plausibility.

## Limitations
- TRAJAN's reliance on point tracks may miss semantic violations that humans consider unrealistic, such as physically impossible object deformations
- The human evaluation protocol shows low inter-rater reliability (σ≈0.5), suggesting human ground truth may be noisy and limiting evaluation precision
- The exact training dataset composition is not fully specified, making exact reproduction challenging

## Confidence

**High**: TRAJAN's mechanism of disentangling motion from semantics via point tracks; reconstruction error correlating with human ratings for temporal consistency.

**Medium**: Superior sensitivity to temporal distortions vs. VideoMAE/I3D (based on synthetic test data); interpretability via spatiotemporal error localization.

**Low**: Generalization to all types of generative failures (e.g., semantic plausibility with implausible motion).

## Next Checks

1. **Dataset fidelity**: Attempt to reconstruct the exact training set using the listed filtering criteria; if impossible, retrain on a proxy set and compare performance deltas.

2. **Architectural ablation**: Replace the Perceiver encoder with mean-pooling of track embeddings to quantify the contribution of permutation-invariant aggregation.

3. **Failure mode analysis**: Generate videos with semantically plausible but physically implausible motion (e.g., floating objects) and test TRAJAN's sensitivity.