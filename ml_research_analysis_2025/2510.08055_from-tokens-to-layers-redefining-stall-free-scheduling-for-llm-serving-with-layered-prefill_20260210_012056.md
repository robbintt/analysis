---
ver: rpa2
title: 'From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with
  Layered Prefill'
arxiv_id: '2510.08055'
source_url: https://arxiv.org/abs/2510.08055
tags:
- prefill
- layered
- chunked
- arxiv
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces layered prefill, a new scheduling strategy
  for LLM inference that shifts the scheduling axis from tokens to transformer layers.
  By partitioning the model vertically into contiguous layer groups and interleaving
  prefill and decode across groups, layered prefill eliminates the redundant expert
  weight reloads inherent in token-based chunked prefill scheduling.
---

# From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill

## Quick Facts
- **arXiv ID:** 2510.08055
- **Source URL:** https://arxiv.org/abs/2510.08055
- **Reference count:** 16
- **Primary result:** Layered prefill reduces time-to-first-token by up to 70% and end-to-end latency by 41% for MoE models while preserving stall-free decoding.

## Executive Summary
Layered prefill is a new scheduling strategy for LLM inference that shifts the scheduling axis from tokens to transformer layers. By partitioning the model vertically into contiguous layer groups and interleaving prefill and decode across groups, layered prefill eliminates the redundant expert weight reloads inherent in token-based chunked prefill scheduling. This design preserves stall-free decoding while reducing off-chip bandwidth demand, lowering time-to-first-token (TTFT) by up to 70%, end-to-end latency by 41%, and per-token energy consumption by up to 22% on long-context workloads with MoE models. Evaluations show that layered prefill consistently improves the TTFT–TBT Pareto frontier and reduces expert-load traffic by up to 39%, demonstrating its effectiveness for high-efficiency, energy-aware LLM serving.

## Method Summary
The method partitions the decoder stack into G contiguous layer groups based on input prompt length (G = max(1, ceil(L/512))). In each iteration, exactly one group performs both prefill and decode while all others perform decode-only. Prefill advances by one group per iteration, completing after G iterations while decode proceeds continuously. The scheduler implements this one-group-per-iteration rule using custom CUDA kernels, torch.compile, and CUDA Graphs on top of vLLM with FlashAttention-3 and PagedAttention. The approach is evaluated on Qwen3-30B-A3B (128 experts, top-k=8) and GPT-OSS-20B (32 experts, top-k=4) using ShareGPT and arXiv datasets with bfloat16 precision.

## Key Results
- Reduces time-to-first-token by up to 70% compared to chunked prefill scheduling
- Achieves 41% end-to-end latency reduction while maintaining stall-free decoding
- Lowers per-token energy consumption by up to 22% through reduced off-chip memory traffic
- Reduces expert-load traffic by up to 39% on long-context workloads

## Why This Works (Mechanism)

### Mechanism 1: Layer-Group Scheduling Axis Shift
Partitioning the model vertically along the layer dimension instead of horizontally along the token dimension eliminates redundant expert weight loads inherent to chunked prefill. The scheduler divides the decoder stack into G contiguous layer groups. In each iteration, exactly one group performs both prefill and decode; all others perform decode-only. Prefill advances by one group per iteration, completing after G iterations while decode proceeds continuously. This design guarantees that each input prompt traverses the prefill path of each layer exactly once as opposed to chunked prefill, which forces every layer to reprocess the prompt once per chunk.

### Mechanism 2: Sparsity Preservation Under TBT Constraints
Layered prefill preserves MoE sparsity benefits by avoiding the "sparsity erosion" problem where chunked hybrid batches activate most experts but fail to saturate compute per expert. Chunked prefill with small chunks creates large hybrid batches that activate 86%+ of experts, but each expert processes only ~8 tokens—far below the ridge point. Layered prefill processes full prompts through each layer once, enabling larger effective batches per expert when combined with optional chunking.

### Mechanism 3: Memory Bandwidth Reduction Translating to Energy Savings
Reducing redundant off-chip memory transfers directly lowers per-token energy consumption by cutting the dominant energy component (DRAM/HBM access). Energy is modeled as static + compute + memory + communication. Memory traffic dominates. By eliminating repeated expert weight loads, layered prefill reduces GB transferred per request, directly lowering mJ/token.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing and sparsity**
  - Why needed here: Layered prefill's benefit is specific to MoE models where expert weight loading dominates. Understanding top-k routing and the experts-to-top-k ratio is essential to quantify potential gains.
  - Quick check question: For a 128-expert model with top-8 routing and batch size 64, what fraction of experts are activated on average? (Answer: ~69% per Table 1)

- **Concept: Prefill vs. decode stages and their bottleneck characteristics**
  - Why needed here: Prefill is compute-bound with full-sequence parallelism; decode is memory-bound with single-token iterations. The scheduling conflict arises from these asymmetric profiles.
  - Quick check question: Why does batching help decode more than prefill? (Answer: Decode has poor weight reuse per token; batching amortizes weight loads across more tokens)

- **Concept: Ridge point and compute/memory-bound regime classification**
  - Why needed here: The paper argues that MoE experts operate below the ridge point in typical serving, making them memory-bound. This justifies why reducing memory traffic improves both latency and energy.
  - Quick check question: If an accelerator has peak 300 TFLOPS and 2 TB/s memory bandwidth, what is the ridge point? (Answer: 150 Op/Byte; batch sizes yielding higher arithmetic intensity are compute-bound)

## Architecture Onboarding

- **Component map:** Request arrives → Scheduler partitions layers into G groups → KV cache manager tracks partial prefill state → Layer group 1 runs prefill+decode, others decode-only → Iteration advances → First token emitted after G iterations → Decode continues stall-free

- **Critical path:** 1. Request arrives with prompt length L → compute G = max(1, ceil(L/512)). 2. Iteration 1: Group 1 runs prefill+decode; Groups 2..G run decode-only. 3. Iteration i: Group i runs prefill+decode; all others decode-only. 4. After G iterations: Prefill complete, first token emitted, decode continues stall-free.

- **Design tradeoffs:** Number of groups (G): Larger G reduces per-iteration prefill work (lower TBT risk) but increases total iterations to first token. Combination with chunking: Layered + chunked prefill enables very long prompts with compute-bound MoE. Group granularity: Uneven layer counts per group may cause load imbalance.

- **Failure signatures:** TBT violations: If per-iteration prefill work exceeds decode slack, TBT spikes. TTFT inflation: If G is too large or system is overloaded, queuing delay dominates. Energy regression: If compute-bound regime is reached, further traffic reduction yields minimal energy gain.

- **First 3 experiments:** 1. Baseline comparison: Run chunked prefill (512-token chunks) vs. layered prefill on ShareGPT and arXiv datasets with Qwen. 2. Expert load traffic profiling: Instrument MoE kernels to count expert weight load bytes per request. 3. Energy-per-token measurement: Use NVML to measure GPU energy during steady-state serving.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How does layered prefill interact with pipeline parallelism in multi-GPU distributed environments?
**Basis in paper:** The authors state in the conclusion that "Future work can extend this approach to complex multi-GPU environments."
**Why unresolved:** The evaluation was limited to two GPUs using tensor parallelism; the interplay between the "one-group-per-iteration" rule and pipeline parallelism stages is undefined.
**What evidence would resolve it:** Benchmarks on larger clusters (e.g., 4+ GPUs) comparing layered prefill performance under pipeline parallelism versus tensor parallelism.

### Open Question 2
**Question:** Can dynamic, runtime-adaptive layer grouping strategies outperform the static heuristic (G = ceil(L/512)) used in the implementation?
**Basis in paper:** The conclusion suggests the need to "explore adaptive layer grouping strategies, particularly for models where the layer count is not an even multiple of the group count."
**Why unresolved:** The current implementation uses a static calculation based solely on input length, which may not be optimal for variable system load or hardware utilization.
**What evidence would resolve it:** A study comparing the current static grouping against a feedback-driven scheduler that adjusts group sizes based on real-time memory bandwidth and compute utilization.

### Open Question 3
**Question:** Does layered prefill yield efficiency gains for dense (non-MoE) Transformer models, or is the benefit strictly limited to MoE architectures?
**Basis in paper:** The paper frames its motivation almost entirely around "sparsity erosion" and redundant MoE expert weight reloads; it does not evaluate whether the scheduling overhead is justified for dense models.
**Why unresolved:** Dense models lack expert-specific reloading overheads, so it is unclear if the layer-based scheduling provides memory or latency benefits over standard chunked prefill for these architectures.
**What evidence would resolve it:** End-to-end latency and energy measurements on dense models (e.g., Llama 3) contrasting chunked prefill against layered prefill.

## Limitations

- **Model Architecture Assumptions:** Layered prefill assumes transformer layers can be cleanly partitioned into contiguous groups without cross-group dependencies. Any deviation (e.g., early exit, layer skipping) would require significant adaptation.
- **SLO Configuration Sensitivity:** The claimed benefits are demonstrated under specific SLO settings (1.3 req/s target, 20s SLO) and may not generalize to all scenarios or non-MoE dense models.
- **Integration Complexity:** While claimed compatible with existing systems, the actual integration requires modifications to scheduler hooks, KV cache management across layer groups, and expert weight loading coordination.

## Confidence

**High Confidence:** The core mechanism of layer-group scheduling eliminating redundant expert weight loads is well-founded. The theoretical analysis correctly identifies that chunked prefill forces each layer to reprocess the prompt once per chunk, while layered prefill processes each layer exactly once.

**Medium Confidence:** The TTFT improvement claims (up to 70%) are well-supported for long-context workloads but may not generalize to all scenarios. The benefits scale with prompt length and MoE configuration.

**Low Confidence:** The claim that layered prefill is "compatible with any existing LLM serving system" without performance degradation is not fully validated. The benefits are specific to MoE models and may not translate to dense models.

## Next Checks

1. **Cross-Architecture Generalization Test:** Implement layered prefill on a non-standard MoE architecture with heterogeneous layer structures or early exit capabilities. Measure whether the scheduling mechanism requires adaptation and quantify any performance degradation compared to standard transformer architectures.

2. **SLO Pareto Frontier Analysis:** Systematically vary SLO targets (e.g., 0.5-5.0 req/s, 10-60s response times) and prompt length distributions to map the full TTFT-TBT Pareto frontier. Identify scenarios where layered prefill provides minimal benefit or potential trade-offs between TTFT and TBT.

3. **Dense Model Comparative Study:** Implement layered prefill on a high-quality dense transformer model (e.g., Llama 3, Mixtral) and compare performance against the MoE implementations. Quantify whether the scheduling mechanism provides any benefits for dense models or whether the advantages are exclusively MoE-specific.