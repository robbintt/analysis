---
ver: rpa2
title: Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish
arxiv_id: '2504.14630'
source_url: https://arxiv.org/abs/2504.14630
tags:
- kurdish
- summarization
- text
- language
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of automatic text summarization (ATS)
  systems for Sorani Kurdish, a low-resource language. The authors developed a dataset
  of 231 scientific papers from four departments across two universities in the Kurdistan
  Region of Iraq, averaging 26 pages per document.
---

# Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish

## Quick Facts
- arXiv ID: 2504.14630
- Source URL: https://arxiv.org/abs/2504.14630
- Reference count: 4
- Best ROUGE-1 score: 19.58% (Experiment 2, without conclusions)

## Executive Summary
This study addresses the lack of automatic text summarization systems for Sorani Kurdish, a low-resource language. The authors developed a dataset of 231 scientific papers from four departments across two universities in the Kurdistan Region of Iraq, averaging 26 pages per document. Using Sentence Weighting and TF-IDF algorithms, they conducted two experiments—one including and one excluding conclusions—to evaluate summarization performance. Results were assessed using ROUGE metrics and manual expert evaluation. The best ROUGE-1 score achieved was 19.58% in the second experiment. The research provides valuable resources for Kurdish NLP and demonstrates the feasibility of ATS for low-resource languages, with potential for future improvements through expanded datasets and abstractive methods.

## Method Summary
The researchers collected 231 Sorani Kurdish research papers from four academic departments, converted them from PDF to text, and performed extensive preprocessing including cleansing, sentence segmentation using the Punkt algorithm, tokenization, normalization, standardization, numeral unification, stemming, and stopword removal. They implemented a two-stage extractive summarization approach: first applying a Sentence Weighting algorithm to assign importance scores (0-1) to each sentence and removing the bottom 50%, then using TF-IDF to rank the remaining sentences. Two experiments were conducted—one with conclusions included and one without—to assess the impact of document structure on summarization quality. The system generated summaries limited to approximately 182 words, matching average abstract length, and evaluated results using ROUGE metrics and manual expert assessment.

## Key Results
- Best ROUGE-1 F-score: 19.58% (Experiment 2, without conclusions, Social Science department)
- ROUGE-2 scores remained consistently low (2-6% range) across all experiments
- Cross-department variance: Social Science achieved 19.58% while Kurdish Language dropped to 11.69%
- Manual evaluation by 6 experts using 5-point scale confirmed ROUGE findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining sentence weighting with TF-IDF enables extractive summarization for Sorani Kurdish scientific documents without requiring large neural models.
- Mechanism: The system first assigns a normalized weight (0-1) to each sentence based on positional and frequency features. The bottom 50% of sentences by weight are discarded. TF-IDF then scores remaining sentences by term importance, and top-ranked sentences are selected for the summary.
- Core assumption: Key information in Kurdish scientific texts follows discoverable frequency and distribution patterns similar to higher-resource languages.
- Evidence anchors:
  - [abstract] "Using Sentence Weighting and Term Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were conducted"
  - [section] "Each sentence in the document was assigned a weight between 0 and 1, indicating its relative importance within the text... we eliminated the bottom 50% of sentences with the lowest scores"
  - [corpus] Weak corpus evidence; related papers focus on NER, speech-to-text, and MT for Sorani Kurdish, not summarization-specific validation of TF-IDF effectiveness.
- Break condition: If Kurdish scientific terminology doesn't follow expected frequency distributions (e.g., rare but critical technical terms), the algorithm will systematically undervalue important content.

### Mechanism 2
- Claim: Domain-specific stopwords improve summarization precision by filtering discipline-specific non-informative terms.
- Mechanism: Experts identify and validate stopwords for each academic department (Political Sciences: 22, Kurdish Language: 18, Sociology: 30, Social Sciences: 6). These are removed during preprocessing, reducing noise in subsequent TF-IDF calculations.
- Core assumption: Each academic domain in Kurdish contains predictable non-informative vocabulary that differs across disciplines.
- Evidence anchors:
  - [section] "domain-specific stop words were compiled for each department through expert consultation and online research, then validated by experts and saved in JSON files"
  - [section] Table 1 shows stopword counts varying from 6 to 30 across departments
  - [corpus] No corpus papers address stopword strategies for Kurdish; this appears to be a novel contribution.
- Break condition: If stopword lists are incomplete (retaining noise) or overinclusive (removing domain-critical terms), summarization quality degrades asymmetrically across departments.

### Mechanism 3
- Claim: Excluding conclusion sections before summarization improves ROUGE scores by reducing redundant content that doesn't match abstract references.
- Mechanism: Two experiments were run with identical parameters except for conclusion inclusion. Experiment 2 (without conclusions) achieved 19.58% ROUGE-1 F-score versus Experiment 1's 14.62% best result.
- Core assumption: Conclusions in Kurdish scientific papers recapitulate body content in ways that, when extracted, create mismatched overlap with abstracts used as reference summaries.
- Evidence anchors:
  - [abstract] "two experiments were conducted, differing in whether the conclusions were included"
  - [section] Table 8 shows best results: 14.62% (with conclusion) vs 19.58% (without conclusion), both in Social Science department
  - [corpus] Corpus papers don't examine document structure effects on summarization for low-resource languages.
- Break condition: If conclusions in other Kurdish document types contain unique information absent from the body, exclusion would harm rather than help summarization quality.

## Foundational Learning

- Concept: Extractive vs. Abstractive Summarization
  - Why needed here: This paper implements extractive summarization (selecting existing sentences). Understanding this explains why ROUGE scores are modest and why the authors suggest abstractive methods as future work.
  - Quick check question: Why would an extractive summary struggle to match a human-written abstract that paraphrases rather than quotes source text?

- Concept: ROUGE Metrics (ROUGE-1, ROUGE-2, ROUGE-L)
  - Why needed here: Results are reported across three ROUGE variants. The paper consistently shows ROUGE-1 > ROUGE-L > ROUGE-2, indicating unigram overlap exists but phrase-level and structural matching are weaker.
  - Quick check question: Why does ROUGE-2 (bigram overlap) score lowest across all experiments, and what does this suggest about the gap between extracted sentences and reference abstracts?

- Concept: TF-IDF Weighting
  - Why needed here: The core ranking mechanism relies on term frequency-inverse document frequency. Understanding how IDF penalizes common terms helps explain why domain-specific vocabulary matters.
  - Quick check question: If you doubled the dataset size with papers from the same departments, how would IDF values change for department-specific technical terms?

## Architecture Onboarding

- Component map:
  - Data Ingestion: PDF → Text (Foxit PDF Reader + custom Python scripts for RTL/encoding fixes)
  - Preprocessing: Cleansing → Segmentation (Punkt) → Tokenization → Normalization → Standardization → Numeral unification → Stemming → Stopword removal
  - Feature Extraction: Numerical embeddings for semantic/contextual representation
  - Scoring: Sentence Weighting (0-1 scale) → 50% pruning → TF-IDF ranking
  - Output: Top sentences sorted by TF-IDF, truncated to ~182 words (average abstract length)
  - Evaluation: ROUGE-1/2/L + 6-expert manual evaluation

- Critical path: PDF quality → OCR accuracy → Preprocessing integrity → Stopword completeness → Sentence weight calculation → TF-IDF scoring → Summary length control. (OCR errors propagate through all stages; authors report ~2 months for manual verification.)

- Design tradeoffs:
  - Extractive vs. abstractive: Extractive chosen for feasibility with limited data; sacrifices fluency
  - Fixed summary length (~182 words): Based on average abstract length; may over-compress longer documents
  - Conclusion exclusion: Improves ROUGE but removes potentially unique synthesis content
  - Small dataset (231 docs): Limits applicability of neural/transformer approaches

- Failure signatures:
  - ROUGE-2 consistently low (2-6%): Phrase-level patterns don't match reference abstracts
  - Cross-department variance: Social Science achieves 19.58% while Kurdish Language drops to 11.69% (Experiment 2)
  - Encoding/RTL artifacts in output: Requires manual post-processing scripts
  - Long training time (20-22 hours on 46-core server): Despite non-neural methods, suggesting inefficiency

- First 3 experiments:
  1. Baseline reproduction: Implement the full pipeline on provided train/val/test splits to reproduce the 19.58% ROUGE-1 best result. Log per-department variance to verify implementation correctness.
  2. Stopword ablation: Run with only general Kurdish stopwords (remove domain-specific lists) and measure ROUGE degradation. This quantifies the contribution of domain customization.
  3. Conclusion-inclusion validation: On a held-out 10% subset, run both experimental conditions (with/without conclusions) to verify the ~5 percentage point improvement before committing to one approach for production.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can abstractive summarization models produce higher ROUGE scores and human acceptance ratings than the current extractive TF-IDF baseline for Sorani Kurdish research documents?
- Basis in paper: [explicit] The authors state in the conclusion that "transitioning from extractive to abstractive summarization could produce more natural, human-like summaries."
- Why unresolved: The current study strictly utilized extractive algorithms (Sentence Weighting and TF-IDF) and did not test neural or abstractive approaches.
- What evidence would resolve it: Comparative evaluation of a sequence-to-sequence or transformer-based model against the current baseline using the same dataset.

### Open Question 2
- Question: Does expanding the dataset size beyond 231 documents significantly improve the accuracy and vocabulary coverage of the summarization model?
- Basis in paper: [explicit] The authors note that "Expanding the dataset... would further improve performance" and acknowledge the current dataset is "relatively small."
- Why unresolved: The study was limited to a specific set of papers from two universities, potentially restricting the model's exposure to diverse linguistic structures.
- What evidence would resolve it: Performance benchmarks generated by retraining the model on a scaled-up corpus (e.g., 1,000+ documents) compared to the current results.

### Open Question 3
- Question: To what extent can the developed ATS model generalize to other Kurdish dialects, such as Kurmanji, or to non-academic text types?
- Basis in paper: [explicit] The conclusion suggests "future work could involve training the model on various text types beyond research papers and extending it to other Kurdish dialects."
- Why unresolved: The model was trained exclusively on Sorani Kurdish scientific papers, which utilize specific formal vocabulary and structure.
- What evidence would resolve it: Cross-dialect and cross-domain testing results without retraining (zero-shot) or with fine-tuning on datasets like KurdSum.

## Limitations

- Sentence Weighting algorithm lacks mathematical specification, making exact replication impossible
- Dataset of 231 research papers with abstracts is not publicly available
- Domain-specific stopword lists (76 terms across 4 departments) are not provided
- Results highly sensitive to PDF-to-text conversion quality, requiring manual verification

## Confidence

- **High**: Basic pipeline feasibility (preprocessing → TF-IDF → ROUGE evaluation) for Sorani Kurdish
- **Medium**: Domain-specific stopword strategy improves summarization precision
- **Medium**: Excluding conclusions improves ROUGE scores by reducing redundant content
- **Low**: The specific 19.58% ROUGE-1 F-score is directly reproducible without access to original dataset and implementation details

## Next Checks

1. Implement the full pipeline on any available Kurdish text corpus and measure ROUGE scores to verify the basic approach works outside the original dataset
2. Run ablation tests comparing general vs. domain-specific stopwords to quantify their contribution to performance gains
3. Test conclusion-exclusion on a held-out subset to confirm the ~5 percentage point ROUGE improvement before committing to this approach for production use