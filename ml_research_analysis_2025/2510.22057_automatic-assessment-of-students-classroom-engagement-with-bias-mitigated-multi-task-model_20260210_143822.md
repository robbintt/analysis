---
ver: rpa2
title: Automatic Assessment of Students' Classroom Engagement with Bias Mitigated
  Multi-task Model
arxiv_id: '2510.22057'
source_url: https://arxiv.org/abs/2510.22057
tags:
- engagement
- dataset
- bias
- predictions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gender bias in automated student
  engagement assessment models, specifically targeting the DAiSEE dataset where females
  were annotated as more engaged than males. The authors propose a novel training
  method using attribute-orthogonal regularization (AOR) combined with a split-model
  classifier architecture.
---

# Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model

## Quick Facts
- arXiv ID: 2510.22057
- Source URL: https://arxiv.org/abs/2510.22057
- Reference count: 25
- Addresses gender bias in automated engagement assessment models

## Executive Summary
This paper addresses gender bias in automated student engagement assessment models, specifically targeting the DAiSEE dataset where females were annotated as more engaged than males. The authors propose a novel training method using attribute-orthogonal regularization (AOR) combined with a split-model classifier architecture. Their approach discourages models from leveraging gender-related features by penalizing correlation between engagement and gender classifier branches. They also employ transfer learning, training the gender classifier on an external dataset before freezing its layers. The results show a significant reduction in prediction disparity between genders, with the Pearson correlation coefficient improving from 0.897 (unmitigated) to 0.999 (mitigated).

## Method Summary
The method employs a split-model architecture where Xception feature extractor shares information between gender and engagement classifiers, which then branch separately. The key innovation is attribute-orthogonal regularization (AOR) that penalizes correlation between the weight vectors of the two classifier branches. The gender classifier is first trained on an external dataset (OUI) to establish a robust reference signal, then frozen during engagement training. The total loss combines classification loss with AOR term weighted by hyperparameter λ. Images are preprocessed to 299×244 pixels and scaled to [-1, 1].

## Key Results
- Pearson correlation coefficient between gender prediction distributions improved from 0.897 (unmitigated) to 0.999 (mitigated)
- Accuracy decreased modestly from 54.9% to 48% on validation set
- F1-score for class 3 (most engaged) dropped from 0.6985 to 0.5528 for female samples
- AOR successfully reduced model's reliance on gender-related features for engagement predictions

## Why This Works (Mechanism)

### Mechanism 1: Attribute-Orthogonal Regularization (AOR)
Penalizing correlation between engagement and gender classifier weights reduces the model's reliance on gender-related features for engagement predictions. The AOR term (Lortho) computes normalized correlation between weight vectors of the two classifier branches immediately after the shared feature extractor. Adding this penalty to loss function pushes weight vectors toward orthogonality, forcing engagement classifier to learn representations independent from gender classification.

### Mechanism 2: External Transfer Learning for Robust Gender Detection
Pre-training gender classifier on larger, demographically diverse external dataset creates more reliable reference signal for AOR than training on biased DAiSEE dataset alone. OUI dataset provides 38k+ samples with gender labels across diverse environments. Training gender classifier on OUI first, then freezing weights before engagement training, establishes stable, high-quality gender representation that serves as orthogonalization target.

### Mechanism 3: Split-Model Architecture for Controlled Feature Sharing
Bifurcating classifier branches after shared feature extractor balances shared low-level representation learning with task-specific specialization. Xception backbone extracts general visual features (edges, textures, facial regions) that both classifier branches receive but develop independently. AOR operates at junction point, ensuring engagement branch cannot simply copy gender branch's learned representations.

## Foundational Learning

- **Spurious Correlation**: Understanding that models exploit correlations regardless of causality is essential. Quick check: If a model predicts engagement from hair length, is this spurious or legitimate? How would you determine this?

- **Transfer Learning with Frozen Layers**: The method relies on freezing pre-trained Xception layers and pre-trained gender classifier. Quick check: Why freeze gender classifier layers before training engagement branch? What would happen if you fine-tuned both simultaneously?

- **Fairness Metrics (Distribution Parity vs. Accuracy)**: The paper trades accuracy (54.9% → 48%) for fairness (PCC 0.897 → 0.999). Understanding that these metrics can conflict is critical. Quick check: If accuracy drops after debiasing, is the model worse? What additional validation would you need to determine this?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Xception backbone (frozen) -> Gender classifier branch (frozen) -> Engagement classifier branch (trainable) -> AOR loss component

- **Critical path**: 1) Train gender classifier on OUI dataset → achieve 80%+ accuracy; 2) Load pre-trained Xception backbone, freeze all convolutional layers; 3) Attach frozen gender classifier branch and new engagement classifier branch; 4) Train engagement branch with loss = L_cls1 + λLortho; 5) Monitor both accuracy and PCC between gender-specific prediction distributions

- **Design tradeoffs**: λ hyperparameter - higher values enforce stronger debiasing but may excessively harm accuracy; Branch point depth - earlier split → more independent learning but less feature sharing; Frozen vs. fine-tuned backbone - freezing reduces overfitting but may limit adaptability

- **Failure signatures**: Accuracy collapses without bias reduction (λ too high or gender classifier poorly trained); Bias remains despite AOR (gender classifier accuracy too low, or λ too low); Class 0/1 predictions remain zero (severe label imbalance requires additional handling); F1-score drops for specific gender-class combinations (expected if ground truth labels themselves are biased)

- **First 3 experiments**: 1) Baseline replication - train Xception + dense layers on DAiSEE without AOR; confirm PCC ~0.897 and accuracy ~55%; 2) λ sweep - train with λ ∈ {0.01, 0.1, 1.0, 10.0}; plot accuracy vs. PCC tradeoff curve; 3) Uniform subset validation - evaluate both models on 168-sample uniform distribution subset; confirm AOR model shows reduced gender disparity when label imbalance is controlled

## Open Questions the Paper Calls Out

1. **Generalization to out-of-distribution samples**: Further work is needed to validate how both models generalize for out-of-distribution samples to determine if accuracy drop is true performance loss or correction of dataset's inherent bias. Performance metrics on external, balanced dataset would resolve this.

2. **Optimal λ hyperparameter value**: Additional research is needed to model AOR hyperparameter λ across varying values from zero to arbitrary values. Sensitivity analysis plotting validation accuracy and Pearson correlation coefficient across λ values would resolve this.

3. **Human validation of label skew**: A validation study would be helpful to determine whether bias in label generation is product of cultural bias or if ground truths are accurate. Controlled study with human annotators labeling balanced subsample would establish verified baseline.

4. **Fine-tuning convolutional layers**: The effect of fine-tuning CNN layers for either or both classifiers could yield significant improvements. Ablation study comparing frozen-layer approach against model where shared layers are updated during engagement training would resolve this.

## Limitations

- Critical hyperparameters including λ and exact classifier architecture were not specified, limiting reproducibility
- Gender classifier's external training on OUI dataset assumes sufficient domain transferability to DAiSEE without validation
- Fairness improvement measured through distribution parity rather than outcome fairness
- 6% class 0/1 label imbalance may introduce confounding factors not addressed by AOR alone

## Confidence

- High confidence: AOR mechanism for reducing weight correlation between classifier branches is technically sound and mathematically coherent
- Medium confidence: External transfer learning approach for robust gender detection is reasonable but unverified for cross-dataset generalization
- Medium confidence: Observed tradeoff between accuracy and fairness metrics is expected given spurious correlation problem, though specific operating point lacks justification

## Next Checks

1. **Lambda Sensitivity Analysis**: Systematically vary λ across multiple orders of magnitude to quantify accuracy-fairness tradeoff curve and identify optimal operating points

2. **Domain Transfer Validation**: Evaluate frozen OUI-trained gender classifier on DAiSEE validation set to confirm claimed 80-85% accuracy and assess domain shift effects

3. **Uniform Distribution Evaluation**: Test both unmitigated and mitigated models on 168-sample uniform gender distribution subset to verify AOR improvements persist when label imbalance is controlled