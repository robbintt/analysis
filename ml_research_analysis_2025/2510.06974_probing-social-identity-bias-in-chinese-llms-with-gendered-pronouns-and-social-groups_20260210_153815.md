---
ver: rpa2
title: Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social
  Groups
arxiv_id: '2510.06974'
source_url: https://arxiv.org/abs/2510.06974
tags:
- chinese
- outgroup
- social
- they
- ingroup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines social identity biases in Chinese large language
  models (LLMs) by evaluating ten representative models across two experimental settings.
  Using Mandarin-specific prompts with gendered pronouns and 240 Chinese social groups,
  the research found systematic ingroup-positive and outgroup-negative tendencies
  across all models.
---

# Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups

## Quick Facts
- arXiv ID: 2510.06974
- Source URL: https://arxiv.org/abs/2510.06974
- Reference count: 40
- Ten Chinese LLMs systematically show ingroup-positive and outgroup-negative tendencies across two experimental settings.

## Executive Summary
This study examines social identity biases in Chinese large language models (LLMs) by evaluating ten representative models across two experimental settings. Using Mandarin-specific prompts with gendered pronouns and 240 Chinese social groups, the research found systematic ingroup-positive and outgroup-negative tendencies across all models. Instruction-tuned models showed more balanced behavior than pretrained ones, though both exhibited significant biases. Notably, female outgroups triggered stronger negativity than male outgroups, particularly in pretrained models. Extending beyond controlled experiments, analysis of 4,079 real user-chatbot conversations from WildChat revealed that these biases persist and intensify in naturalistic dialogue, with assistant responses showing stronger ingroup favoritism and outgroup hostility than user inputs. The study provides the first systematic examination of social identity framing in Chinese LLMs, demonstrating that such biases documented in English generalize cross-linguistically and are amplified in real-world interactions.

## Method Summary
The study employs two experimental approaches: (1) controlled generation experiments using 10 Chinese LLMs (5 pretrained, 5 instruction-tuned) with prompts containing ingroup ("We"/我们) vs outgroup ("They"/他们/她们) framing, plus 240 Chinese social groups; (2) naturalistic analysis of 4,079 real user-chatbot conversations from WildChat-1M. For controlled experiments, 2,000 continuations per prompt are generated with max_new_tokens=100, keeping only the first sentence after Chinese punctuation. Generated sentences are filtered for length and n-gram overlap, then classified by Alibaba Aliyun sentiment classifier. Logistic regression models estimate odds ratios for ingroup solidarity (positive sentiment) and outgroup hostility (negative sentiment). The naturalistic analysis extracts sentences containing social group markers and applies mixed-effects regression to account for model-level variation.

## Key Results
- All Chinese LLMs show systematic ingroup-positive and outgroup-negative tendencies, with odds ratios exceeding 1 across models.
- Instruction-tuned models exhibit more balanced behavior than pretrained models, though both show significant biases.
- Female outgroups trigger stronger negativity than male outgroups, particularly in pretrained models (odds ratios 1.1-1.3).
- In naturalistic WildChat conversations, assistant responses show stronger ingroup favoritism and outgroup hostility than user inputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit ingroup solidarity and outgroup hostility when prompted with minimal group-identity markers ("We" vs. "They" framing).
- Mechanism: The model's learned associations from training data encode statistical regularities where ingroup language correlates with positive contexts and outgroup language with negative or threatening contexts, producing asymmetric sentiment outputs.
- Core assumption: Sentiment classification accurately captures the affective dimension of social identity bias.
- Evidence anchors:
  - [abstract] "systematic ingroup-positive and outgroup-negative tendencies across all models"
  - [section 4.1] "odds ratios exceed 1 across all models" for both solidarity and hostility
  - [corpus] Limited; related work shows LLMs reproduce documented societal stereotypes but direct corpus-to-output tracing is not established here.
- Break condition: If sentiment classifiers misclassify neutral/factual statements as biased, observed effects may be measurement artifacts.

### Mechanism 2
- Claim: Instruction tuning partially mitigates but does not eliminate social identity bias.
- Mechanism: Alignment procedures (e.g., RLHF, safety fine-tuning) add preference signals that dampen extreme outgroup hostility, but underlying representations remain biased; mitigation is superficial rather than structural.
- Core assumption: Observed behavioral differences between base and instruction-tuned variants stem from alignment, not architectural differences.
- Evidence anchors:
  - [abstract] "Instruction-tuned models showed more balanced behavior than pretrained ones, though both exhibited significant biases"
  - [section 4.1.1] "pretrained models in particular amplify outgroup hostility more strongly than ingroup solidarity"
  - [corpus] Prior work (Deshpande et al., 2023; Ganguli et al., 2022) shows alignment reduces but does not remove stereotype expression; consistent but not causal.
- Break condition: If instruction-tuned models in the sample are also larger or trained on different base corpora, differences may be confounded.

### Mechanism 3
- Claim: Female outgroups elicit stronger negativity than male outgroups, particularly in pretrained models.
- Mechanism: Gendered asymmetries in training data (e.g., more negative or stereotyped contexts surrounding feminine pronouns) propagate through generation, with instruction tuning providing only partial suppression.
- Core assumption: The Chinese third-person plural gender distinction (他们 vs. 她们) is consistently interpreted by models as gender markers.
- Evidence anchors:
  - [abstract] "female outgroups triggered stronger negativity than male outgroups, particularly in pretrained models"
  - [section 4.2.2] "all pretrained models yield odds ratios above 1 (in the range [1.1,1.3])" for female vs. male negativity
  - [corpus] Weak; no direct corpus analysis of gendered pronoun context is provided in this study.
- Break condition: If models do not robustly distinguish gendered pronouns (e.g., tokenization or training noise), observed asymmetries may be unstable.

## Foundational Learning

- Concept: Social Identity Theory
  - Why needed here: The paper's framing ("We" vs. "They") is grounded in Tajfel & Turner's theory; without this, the experimental design appears arbitrary.
  - Quick check question: Explain why minimal group paradigms (arbitrary ingroup/outgroup assignment) produce bias even without explicit conflict.

- Concept: Odds Ratio Interpretation
  - Why needed here: All results are reported via logistic regression odds ratios; misinterpreting these leads to over- or under-estimating effect sizes.
  - Quick check question: An odds ratio of 1.4 for ingroup solidarity means what percentage increase in the odds of positive sentiment?

- Concept: Sentiment Classification Limitations
  - Why needed here: The study's conclusions hinge on sentiment labels as proxies for bias; understanding classifier error modes is critical for assessing robustness.
  - Quick check question: List two scenarios where a sentiment classifier might mislabel a neutral statement about a social group.

## Architecture Onboarding

- Component map:
  Prompt templates (ingroup/outgroup, gendered variants) → LLM generation (base or instruction-tuned) → Sentiment classifier (Aliyun) → Logistic regression (odds ratio estimation)
  Study 2 adds: WildChat corpus → keyword extraction → sentence-level sentiment → mixed-effects regression

- Critical path:
  1. Define prompt templates (Table 1, Table 3)
  2. Generate completions with controlled parameters (max_new_tokens=100, retain first sentence)
  3. Filter by length and n-gram overlap (survival rate 60–80%)
  4. Classify sentiment
  5. Fit regression models

- Design tradeoffs:
  - Single sentiment classifier vs. ensemble: The study uses only Aliyun; introduces method-specific bias but ensures consistency.
  - Controlled prompts vs. naturalistic dialogue: Controlled enables causal attribution; naturalistic validates ecological validity but introduces confounds.
  - Mixed-effects vs. fixed-effects: Mixed-effects used in Study 2 due to limited variation across models in WildChat; may understate model-level heterogeneity.

- Failure signatures:
  - Low survival rate after filtering (e.g., Qwen3-8B instruction-tuned at 15–23%) indicates refusal or off-template generation; results may not generalize.
  - Sparse gendered pronoun instances in naturalistic data (65 female outgroup vs. 1,505 male) prevent robust gender comparisons in Study 2.
  - Instruction-tuned models refusing minimal prompts requires neutral-context prepend; this changes the generation distribution.

- First 3 experiments:
  1. Replicate the core finding on one base and one instruction-tuned model: generate 500 sentences each for "We are" and "They are," classify sentiment, compute odds ratios.
  2. Ablate the neutral-context prepend for instruction-tuned models: compare refusal rates and sentiment distributions with and without prepend.
  3. Extend gender analysis to a balanced synthetic dataset: generate equal numbers of male and female outgroup prompts, test whether asymmetry persists across multiple sentiment classifiers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the amplified ingroup favoritism and outgroup hostility observed in real-world interactions persist when analyzing conversations with native Chinese LLMs, as opposed to the Western-based models (GPT-3.5/4) used in the WildChat dataset?
- Basis in paper: [explicit] The authors state in the Limitations section that relying on "Chinese-language dialogues generated by Western-based models... limited the ecological validity of our findings for native deployment contexts," and suggest in Future Work "collecting conversational data directly from Chinese LLMs."
- Why unresolved: The study's naturalistic analysis (Study 2) relied exclusively on WildChat, which contains logs from OpenAI models, leaving the behavior of native Chinese models in deployed settings unexplored.
- What evidence would resolve it: A replication of Study 2 using conversation logs extracted specifically from native Chinese LLMs (e.g., ERNIE, DeepSeek, Qwen-Plus) in deployment.

### Open Question 2
- Question: To what extent are the observed bias patterns, particularly the anomalous negative odds ratios for "Ethnicity," artifacts of the specific sentiment analysis tool used (Aliyun) rather than the model's internal representations?
- Basis in paper: [explicit] The Limitations section notes that "tools for Chinese remain limited; for consistency, we relied on a single state-of-the-art classifier (Aliyun), which may introduce method-specific biases."
- Why unresolved: The reliance on a single classifier introduces the risk that the sentiment labels—and thus the regression results regarding bias—are skewed by the classifier's own training data or architecture.
- What evidence would resolve it: A comparative evaluation using multiple diverse Chinese sentiment analysis tools or human annotation to validate the direction and magnitude of the bias effects.

### Open Question 3
- Question: Does safety alignment specifically suppress outgroup hostility toward "Ethnicity" categories more effectively than for "Gender" or "Level of Education" categories?
- Basis in paper: [inferred] The Results section notes that "Ethnicity" was the only category with odds ratios significantly smaller than 1 (indicating less negativity), leading the authors to "hypothesize that these effects reflect safety alignment mechanisms."
- Why unresolved: The paper identifies the anomaly but does not test the specific mechanism (safety alignment) versus other potential explanations (e.g., training data distribution).
- What evidence would resolve it: An ablation study comparing base and aligned versions of the same model specifically on "Ethnicity" prompts, or an analysis of refusal rates for these specific categories.

### Open Question 4
- Question: How do social identity biases manifest in multi-turn, open-ended conversations compared to the single-sentence or single-turn evaluations utilized in this study?
- Basis in paper: [explicit] The Future Work section states the need to "explore more interactive and diverse evaluation settings" and "move beyond prompt-based textual evaluations."
- Why unresolved: The current methodology relies on sentence-level sentiment analysis of single prompts or extracted sentences, which fails to capture how bias might escalate or be negotiated over the course of a longer dialogue.
- What evidence would resolve it: A longitudinal analysis of bias dynamics in multi-turn dialogue datasets, measuring sentiment trajectory rather than single-snapshot sentiment.

## Limitations
- The study relies on a single Chinese sentiment classifier (Aliyun), which may introduce method-specific biases and measurement uncertainty.
- Gendered pronoun analysis in naturalistic data is severely limited by class imbalance (65 female vs 1,505 male outgroup instances in WildChat).
- The filtering procedure removes 20-40% of generated sentences, potentially introducing selection bias correlated with social group characteristics.
- The study does not examine intersectional identities or other dimensions of social categorization beyond gender and ingroup/outgroup framing.

## Confidence

- **High confidence**: The existence of systematic ingroup-positive and outgroup-negative tendencies across all tested Chinese LLMs, and the observation that instruction-tuned models show reduced but not eliminated bias compared to base models. These findings are supported by large sample sizes (500+ completions per prompt type), consistent odds ratios exceeding 1 across multiple models, and replication in naturalistic dialogue data.

- **Medium confidence**: The specific magnitude of effects (e.g., odds ratios of 1.1-1.3 for gendered outgroup negativity) and the relative strength of female vs male outgroup negativity. These estimates depend on the sentiment classifier's accuracy and the assumption that first-sentence extraction captures the intended sentiment.

- **Low confidence**: The claim that female outgroups elicit stronger negativity than male outgroups in naturalistic dialogue, due to the extreme class imbalance in WildChat data (n=65 vs n=1,505). The study itself acknowledges this limitation and excludes gendered analysis from Study 2.

## Next Checks

1. **Sentiment classifier validation**: Apply three different sentiment classifiers (Aliyun, another Chinese model, and an English cross-lingual model with Chinese capability) to a random 10% sample of generated sentences. Compare agreement rates and conduct human annotation on a subset to establish ground truth and quantify measurement uncertainty.

2. **Intersectional identity analysis**: Extend the experimental design to test compound social identities (e.g., combining gender with occupational or regional groups). Generate prompts like "We are female doctors from Shanghai" vs "They are male farmers from rural areas" to examine whether bias effects compound or interact across identity dimensions.

3. **Temporal stability and scaling analysis**: Repeat the core experiments with the same models after 3-6 months to assess stability of bias patterns. Additionally, test whether larger model variants (e.g., Qwen2.5-72B vs Qwen2.5-7B) show systematically different bias profiles, examining whether scale provides consistent mitigation or amplification of social identity bias.