---
ver: rpa2
title: "Usando LLMs para Programar Jogos de Tabuleiro e Varia\xE7\xF5es"
arxiv_id: '2511.05114'
source_url: https://arxiv.org/abs/2511.05114
tags:
- jogos
- para
- llms
- implementac
- odigo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of three large language models
  (Claude 3.7 Sonnet, DeepSeekV3, and ChatGPT-4o) to generate Python code for board
  games and their variations. The research tests these models on six classic board
  games (Tic-Tac-Toe, Peg Solitaire, Reversi, Nine Men's Morris, Checkers, and Chess)
  by requesting implementations both with and without the Boardwalk API, including
  two rule variations for each game.
---

# Usando LLMs para Programar Jogos de Tabuleiro e Variações

## Quick Facts
- arXiv ID: 2511.05114
- Source URL: https://arxiv.org/abs/2511.05114
- Authors: Álvaro Guglielmin Becker; Lana Bertoldo Rossato; Anderson Rocha Tavares
- Reference count: 0
- Primary result: Evaluates three LLMs' ability to generate Python code for 6 board games and their variations, testing both independent and Boardwalk API implementations.

## Executive Summary
This study investigates whether large language models can generate functional Python code for classic board games and their rule variations. The research tests three models (Claude 3.7 Sonnet, DeepSeekV3, and ChatGPT-4o) on six games by requesting implementations both with and without the Boardwalk API, including two rule variations for each game. The methodology employs standardized prompts and manual playtesting to identify errors in syntax, API usage, piece movement, victory conditions, and game mechanics. The study aims to determine how well models can leverage their existing knowledge of game rules to generate accurate code, particularly when modifications are introduced.

## Method Summary
The study uses standardized prompts to request Python implementations of six classic board games (Tic-Tac-Toe, Peg Solitaire, Reversi, Nine Men's Morris, Checkers, and Chess) and two variations each. Three LLMs are tested via the Poe platform, generating code both independently and using the Boardwalk API with only documentation access. Manual playtesting evaluates each implementation across seven error categories. The total of 108 tests (3 models × 6 games × 2 implementations × 3 test types) aims to assess pre-training knowledge activation, compositional reasoning for rule variations, and API constraint effects on code generation quality.

## Key Results
- Three LLMs tested on six classic board games with two variations each
- Both independent and Boardwalk API implementations evaluated
- Seven error categories tracked: syntax, API usage, piece movement, victory conditions, game effects, board formatting, and turn order
- 108 total tests conducted using standardized prompts and manual playtesting
- Primary success metric: zero errors across all evaluation categories

## Why This Works (Mechanism)

### Mechanism 1: Pre-Training Knowledge Activation
- Claim: LLMs can generate functional game code by retrieving and structuring knowledge from training data rather than learning rules from scratch.
- Mechanism: The prompt design invokes only game names (e.g., "Tic-Tac-Toe") without full rule descriptions, requiring models to access internal representations of game mechanics and translate them into executable code structures.
- Core assumption: The target games appear frequently enough in training corpora that models have reliable internal representations of their rules.
- Evidence anchors:
  - [abstract] "given their capacity to efficiently generate code from simple contextual information"
  - [Section 1] "devido à massiva quantidade de dados usada nos seus treinamentos, terão familiaridade com as regras e implementações existentes dos jogos de tabuleiro mais populares"
  - [corpus] Boardwalk paper (Becker et al. 2025) demonstrates prior success with rule-description prompting, suggesting transferability
- Break condition: If models generate syntactically correct code with systematically wrong rules (hallucinated mechanics), the pre-training knowledge activation hypothesis weakens.

### Mechanism 2: Rule Variation Synthesis via Compositional Reasoning
- Claim: LLMs can apply novel rule modifications to known games by composing learned rule representations with natural language modification instructions.
- Mechanism: Models must (1) retrieve base game rules, (2) parse modification descriptions, (3) identify which code components require changes, and (4) integrate modifications without breaking existing logic.
- Core assumption: Models have sufficient reasoning capability to identify dependencies between game rules (e.g., changing board size affects win condition checking).
- Evidence anchors:
  - [Section 1] "verificar a capacidade de raciocínio em alto nível dos modelos sobre aspectos dos jogos"
  - [Section 3] Figure 1 shows prompt structure with `$CHANGE` placeholder for rule modifications
  - [corpus] GAVEL and Ludii-related work report high error rates with complex game grammars, suggesting compositional rule synthesis remains challenging
- Break condition: If variation implementations fail at significantly higher rates than base games due to cascading logic errors, compositional reasoning may exceed model capabilities.

### Mechanism 3: API Constraint-Guided Code Generation
- Claim: Providing API documentation scaffolds code generation toward standardized, testable outputs while potentially reducing creative errors.
- Mechanism: The Boardwalk API constrains the solution space by pre-defining game component abstractions (board, pieces, players), reducing the need for models to design architectures from scratch.
- Core assumption: Models can correctly interpret API documentation and map abstract game concepts to API method calls.
- Evidence anchors:
  - [Section 1] "permite uma relativa padronização do formato do código e fácil integração com agentes de IA jogadores"
  - [Section 3] "será fornecida apenas a documentação da API, e não seu código fonte... isso permitiria verificar se o uso da API atrapalha na geração de código"
  - [corpus] Related work on Ludii GDL shows grammar complexity increases error rates; Boardwalk may reduce this by using general-purpose Python
- Break condition: If API-based implementations show higher error rates in API usage categories than independent implementations show in architecture errors, the API constraint hypothesis fails.

## Foundational Learning

- Concept: **Game State Formalization**
  - Why needed here: To evaluate generated code, you must understand how board position, player turns, legal moves, and win conditions map to data structures and control flow.
  - Quick check question: Can you sketch a Python class that represents Tic-Tac-Toe state with `is_valid_move()`, `make_move()`, and `check_winner()` methods?

- Concept: **Prompt Engineering for Code Generation**
  - Why needed here: The study uses standardized prompts with template variables; understanding prompt structure is essential for replicating or extending the methodology.
  - Quick check question: Given the prompt template in Figure 1, how would you construct a prompt for Chess with a modified starting position?

- Concept: **Manual Playtesting Methodology**
  - Why needed here: The evaluation depends on systematic human testing; understanding error categories (syntax, API usage, piece movement, victory conditions, etc.) is critical for reproducible assessment.
  - Quick check question: If a Checkers implementation allows kings to move backward but correctly implements captures, which error category does this fall under?

## Architecture Onboarding

- Component map:
  - Poe platform -> LLMs (Claude 3.7 Sonnet, DeepSeekV3, ChatGPT-4o) -> Prompt templates -> Code generation -> Manual playtesting -> Error classification

- Critical path:
  1. Define prompt template for test category
  2. Submit prompt via Poe platform
  3. Extract generated Python code
  4. Execute code and conduct manual playtest
  5. Record binary error presence across 7 categories
  6. Classify as "perfect" (no errors), "functional with errors," or "non-executable"

- Design tradeoffs:
  - **Manual vs. automated testing**: Manual playtesting captures nuanced rule violations but limits scalability (108 tests require significant human effort)
  - **API documentation vs. source code**: Documentation-only prevents memorization but may increase API usage errors
  - **Binary error classification**: Simplifies analysis but loses severity information

- Failure signatures:
  - **Syntax errors**: Code fails to execute at all
  - **API errors**: Import failures, wrong method signatures, type mismatches
  - **Logic errors**: Game plays but rules are wrong (most subtle category)
  - **Cascading failures**: Variation modifications break unrelated game logic

- First 3 experiments:
  1. Replicate baseline: Run original-game prompts for all 6 games without API to establish expected success rate for pre-training knowledge retrieval
  2. Isolate variation difficulty: Compare error rates between equipment changes (simpler, structural) and rule changes (semantic, potentially complex)
  3. Measure API impact: For a single game (e.g., Tic-Tac-Toe), compare error distributions between API and non-API implementations to identify whether API reduces or shifts error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs successfully implement completely novel board games that fall outside their pre-training data?
- Basis in paper: [explicit] Section 4 lists the "implementation of completely new games, outside the LLMs' prior knowledge" as a topic for future work.
- Why unresolved: The current study only tests six classic games and their variations, relying on the models' existing familiarity with the rules.
- What evidence would resolve it: Functional Python code generated for unique games described solely by natural language rules provided in the prompt.

### Open Question 2
- Question: How can the playtesting process for LLM-generated game logic be effectively automated?
- Basis in paper: [explicit] Section 4 identifies the "automated execution of playtests" as an open aspect to be addressed in future research.
- Why unresolved: The proposed methodology relies on manual playtesting to identify errors in syntax, victory conditions, and mechanics.
- What evidence would resolve it: A system capable of autonomously detecting rule violations and logic errors without human intervention.

### Open Question 3
- Question: Does the integration of a specific framework (Boardwalk API) hinder or help the accuracy of LLM code generation?
- Basis in paper: [explicit] Section 3 states the goal to "verify if the use of the API hinders code generation" despite offering standardization.
- Why unresolved: While the paper proposes comparing independent implementations versus API implementations, the results of this specific trade-off are part of the expected outcome.
- What evidence would resolve it: Comparative error rates between the "with API" and "without API" test groups across the three models.

### Open Question 4
- Question: Are LLMs capable of correctly reconciling pre-trained knowledge of standard game rules with prompted rule variations?
- Basis in paper: [explicit] Section 1 highlights the need to test "high-level reasoning" by "reconciling what they know about the original rules with the requested variations."
- Why unresolved: It is unclear if models can override strong memorized patterns of standard rules (e.g., standard Chess) when implementing modifications.
- What evidence would resolve it: Low error rates in the "rule variation" category, specifically in game mechanics and victory conditions.

## Limitations

- The Boardwalk API documentation is referenced but not provided, making API-based implementation validation impossible without access to the external library
- Specific rule variations for games beyond Tic-Tac-Toe are not enumerated, limiting reproducibility
- Manual playtesting methodology lacks detail on test case design and evaluator training consistency

## Confidence

- **High confidence**: Pre-training knowledge activation for base games - supported by extensive board game presence in training data and standardized prompt structure
- **Medium confidence**: API constraint benefits - theoretical framework exists but Boardwalk API access is required for validation
- **Medium confidence**: Rule variation synthesis - compositional reasoning capability is demonstrated in related work but error rates for complex modifications remain uncertain

## Next Checks

1. Obtain and test Boardwalk API documentation to enable complete API-based implementation evaluation
2. Define standardized test cases for each game's core mechanics to ensure consistent manual playtesting across evaluators
3. Replicate the study using an alternative game development framework (e.g., Pygame) to compare API constraint effects against general-purpose libraries