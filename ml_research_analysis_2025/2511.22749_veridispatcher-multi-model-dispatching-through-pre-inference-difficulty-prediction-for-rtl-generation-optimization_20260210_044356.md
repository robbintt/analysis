---
ver: rpa2
title: 'VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction
  for RTL Generation Optimization'
arxiv_id: '2511.22749'
source_url: https://arxiv.org/abs/2511.22749
tags:
- uni00000014
- uni00000015
- uni00000013
- difficulty
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeriDispatcher is a framework that improves multi-LLM RTL generation
  by routing each task to the most suitable model using pre-inference difficulty prediction.
  It trains lightweight classifiers per model on semantic embeddings of task descriptions,
  predicting whether each LLM can handle the task.
---

# VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization

## Quick Facts
- **arXiv ID:** 2511.22749
- **Source URL:** https://arxiv.org/abs/2511.22749
- **Reference count:** 37
- **Primary result:** Improves RTL generation accuracy up to 18.18% on RTLLM and 5.77% on VerilogEval while reducing commercial API calls by up to 40% through intelligent model selection

## Executive Summary
VeriDispatcher introduces a novel framework for multi-LLM dispatching in RTL generation that routes each task to the most suitable model based on pre-inference difficulty prediction. By training lightweight classifiers per model on semantic embeddings of task descriptions, VeriDispatcher predicts whether each LLM can successfully handle the task before generation occurs. The framework achieves significant accuracy improvements over single-model baselines while reducing reliance on expensive commercial APIs, demonstrating that intelligent model selection outperforms both single-model approaches and random selection for diverse hardware design tasks.

## Method Summary
VeriDispatcher employs a two-stage approach: first generating difficulty labels through extensive simulation of LLM outputs, then training per-model classifiers to predict success likelihood from semantic embeddings. The framework generates 10 prompt variants per task through reverse engineering, runs 10 generations per variant per model, and scores outputs using a combined difficulty metric (syntax, structural similarity via Dolos, and functional correctness). Classifiers (MLPs, XGBoost, LightGBM) are trained on these labels using either internal model embeddings (average pooling) or external embeddings (text-embedding-3-large, Qwen3-Embedding-8B). During inference, the system queries all model-specific classifiers and routes tasks to the Top-k predicted successful models.

## Key Results
- Achieves up to 18.18% accuracy improvement on RTLLM and 5.77% on VerilogEval compared to single-model baselines
- Reduces commercial API calls by up to 40% while maintaining or exceeding baseline performance
- Demonstrates superior performance to random selection and single-model approaches across diverse RTL generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RTL task difficulty is model-dependent; a task that is "hard" for one LLM may be "easy" for another based on architectural and training differences.
- **Mechanism:** VeriDispatcher trains a separate lightweight classifier (e.g., MLP) for *each* LLM in the pool. These classifiers map semantic embeddings of task descriptions to a binary "easy/hard" label derived from that specific model's empirical success rate, thereby learning individual capability boundaries.
- **Core assumption:** The semantic embedding of a task description contains sufficient information to predict the model's likelihood of generating functionally correct code *before* generation occurs.
- **Evidence anchors:**
  - [Abstract]: "...trains a compact classifier over semantic embeddings... predicting whether each LLM can handle the task."
  - [Section 1]: "...task difficulty is inherently model specific."
  - [Figure 1]: Shows distinct t-SNE clustering of "easy" vs "hard" instances for different LLMs.
- **Break condition:** If the correlation between the semantic embedding density and the model's functional success rate is weak, the classifier provides no better than random routing.

### Mechanism 2
- **Claim:** Pre-inference difficulty prediction enables cost-efficient routing that maintains or exceeds single-model accuracy.
- **Mechanism:** Instead of broadcasting a task to all models, the system queries the per-model predictors. It routes the task only to the Top-k models predicted to succeed (dispatching strategy). This filters out models likely to fail, saving API calls and reducing noise in the final output selection.
- **Core assumption:** The "filtered difficulty score" (based on syntax, structure, and function) used to train the classifiers generalizes to unseen tasks.
- **Evidence anchors:**
  - [Section 3.3]: "...queries all model-specific classifiers... to obtain difficulty predictions."
  - [Table 1]: Shows dispatching strategies achieving comparable or better Pass@k than GPT-5 while using fewer commercial calls.
- **Break condition:** If the overhead of running the embedding extraction and classification outweighs the cost savings from skipped LLM calls, or if the classifier has high false-negative rates (missing capable models), the system becomes inefficient or inaccurate.

### Mechanism 3
- **Claim:** External embedding models or specific internal pooling strategies (average pooling) provide the most robust semantic proxy for difficulty prediction.
- **Mechanism:** For open-source models, the framework extracts hidden states (using Average Pooling > Decay/Last-Token). For commercial models where internals are inaccessible, it uses external embeddings (OpenAI/Qwen) as proxies. These embeddings serve as the feature space for the classifiers.
- **Core assumption:** External embedding models capture semantic nuances relevant to RTL logic sufficiently well to act as a "universal" proxy for models with hidden internals.
- **Evidence anchors:**
  - [Section 5.1]: "Average pooling consistently outperforms last-token... External embedding encoders provide consistent difficulty signals."
- **Break condition:** If the external embedding model fails to capture domain-specific RTL features that the target commercial model is sensitive to, the proxy classifier will fail to predict difficulty accurately.

## Foundational Learning

- **Concept: RTL (Register Transfer Level) Correctness**
  - **Why needed here:** The paper defines difficulty not just by syntax, but by functional correctness and structural similarity. You must understand that valid Verilog syntax does not guarantee a working circuit.
  - **Quick check question:** Can a Verilog module pass syntax checking but fail simulation? (Yes).

- **Concept: Semantic Embeddings & Hidden States**
  - **Why needed here:** This is the input signal for the prediction mechanism. You need to understand that LLMs compress meaning into high-dimensional vectors *before* generating output tokens.
  - **Quick check question:** What is the difference between "last token embedding" and "average pooling" in the context of a prompt? (Last token captures final context; average pooling captures holistic prompt semantics).

- **Concept: Pass@k Metric**
  - **Why needed here:** The results measure "Pass@k" (probability of success if the best of *k* samples is chosen). The mechanism aims to improve this metric efficiently.
  - **Quick check question:** If a model has Pass@1 = 0.5 and Pass@10 = 0.9, what does that imply about the model's consistency? (The model is capable but inconsistent/stochastic).

## Architecture Onboarding

- **Component map:**
  - Embedder: (Open-source internals OR OpenAI/Qwen API) -> Converts Task Description -> Feature Vector
  - Difficulty Oracle: (Per-model MLPs) -> Input: Feature Vector -> Output: Probability of Success (Easy/Hard)
  - Dispatcher: Logic -> Takes probabilities -> Selects Top-k models -> Invokes LLM Generation
  - Verifier: (Post-generation) -> Syntax/Functional checks -> Used only for *training data generation*, not inference routing

- **Critical path:** The **Training Data Generation**. The system relies on "reverse engineering" prompts and running expensive simulations (using tools like Icarus Verilog/Yosys) to create the "Ground Truth" difficulty labels. Without high-quality labels, the classifiers are useless.

- **Design tradeoffs:**
  - **Internal vs. External Embeddings:** Internal embeddings require open-weight models but capture model-specific "self-awareness." External embeddings are universal but may lack nuance for specific model quirks.
  - **Accuracy vs. Cost:** Increasing "Top-k" (dispatching to more models) improves accuracy (Pass@k) but linearly increases cost/API usage.

- **Failure signatures:**
  - **Classifier Collapse:** Low F1-macro scores (Fig 3/4) indicating the model cannot distinguish easy from hard tasks.
  - **Semantic Bottleneck:** On semantically sparse datasets (like VerilogEval), classifiers struggle more than on richer datasets (RTLLM).
  - **Misrouting:** The dispatcher consistently selects a weak model over a strong one for specific task types (e.g., protocol-heavy tasks).

- **First 3 experiments:**
  1. **Reproducibility Check:** Train a single MLP classifier on the provided RTLLM embeddings using the paper's hyperparameters (hidden size 512/256, lr 1e-4) to verify reported F1-scores.
  2. **Ablation on Embeddings:** Swap the "Average Pooling" embedding for "Last Token" embedding on a specific open-source model (e.g., OriGen) and measure the drop in classifier accuracy.
  3. **Stress Test:** Evaluate the dispatcher on a small set of "adversarial" tasks (e.g., highly concise prompts with complex logic) to see if the external embeddings fail to predict difficulty.

## Open Questions the Paper Calls Out
- **Question:** How would a hierarchical dispatch strategy (routing from cheap to premium models) affect cost-efficiency versus quality trade-offs compared to direct single-hop dispatch?
  - **Basis in paper:** [explicit] Conclusion states: "Future work includes hierarchical dispatch from cheap to premium models..."
  - **Why unresolved:** VeriDispatcher currently dispatches directly to predicted-best models without cascading through progressively more capable/expensive models, leaving potential cost savings unexplored.

- **Question:** Can the difficulty-aware routing framework generalize to other EDA tasks such as testbench generation and assertion synthesis with comparable gains?
  - **Basis in paper:** [explicit] Same conclusion statement explicitly lists "extending to other EDA tasks, e.g., testbench generation, assertion synthesis" as future work.
  - **Why unresolved:** Difficulty classifiers were trained on RTL generation benchmarks; testbench and assertion tasks may have different complexity signals, embedding distributions, and model capability boundaries.

- **Question:** Would training dedicated difficulty-aware embeddings improve prediction accuracy over repurposed general-purpose semantic embeddings?
  - **Basis in paper:** [explicit] Conclusion lists "developing difficulty-aware embeddings" as future work.
  - **Why unresolved:** Current approach uses external embeddings or internal hidden states not explicitly optimized to encode task difficulty for RTL generation.

## Limitations
- The framework's effectiveness relies on computationally expensive ground truth generation through extensive simulations (17,160 runs for VerilogEval alone)
- Performance may degrade on semantically sparse datasets where distinguishing easy from hard tasks is inherently difficult
- The approach assumes sufficient diversity in task descriptions to train robust classifiers, but the reverse engineering method's coverage of real-world RTL design scenarios is not fully validated

## Confidence
- **Core multi-LLM dispatching mechanism:** Medium - demonstrates improved accuracy and reduced commercial API calls, but performance heavily depends on quality of reverse-engineered task variants and reliability of difficulty scoring function

## Next Checks
1. **Cross-Architecture Transferability**: Test VeriDispatcher's classifiers trained on one LLM (e.g., OriGen) on completely different architectures (e.g., GPT-4) to validate whether the semantic embeddings capture universal RTL difficulty signals rather than model-specific quirks.

2. **Domain Generalization**: Evaluate the system on RTL tasks from different hardware design domains (networking, signal processing, cryptographic circuits) to assess whether the learned difficulty predictors generalize beyond the training benchmarks.

3. **Dynamic Threshold Analysis**: Systematically vary the difficulty score threshold (currently fixed at 0.5) and analyze how this affects classifier performance and dispatching efficiency, potentially revealing optimal threshold settings for different LLM architectures.