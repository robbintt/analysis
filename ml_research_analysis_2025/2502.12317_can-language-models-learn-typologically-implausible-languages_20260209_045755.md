---
ver: rpa2
title: Can Language Models Learn Typologically Implausible Languages?
arxiv_id: '2502.12317'
source_url: https://arxiv.org/abs/2502.12317
tags:
- language
- languages
- counterfactual
- word
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models (LMs) can learn
  typologically implausible languages by creating counterfactual versions of English
  and Japanese that violate Greenbergian word order correlations. Using a top-down
  approach, the authors systematically swapped grammatical elements (e.g., verb-object
  order) in natural corpora to generate plausible and implausible variants.
---

# Can Language Models Learn Typologically Implausible Languages?

## Quick Facts
- arXiv ID: 2502.12317
- Source URL: https://arxiv.org/abs/2502.12317
- Reference count: 40
- Primary result: LMs learn typologically implausible languages more slowly and with reduced accuracy compared to natural languages, particularly for highly unnatural swaps like <Verb, Object>.

## Executive Summary
This paper investigates whether language models (LMs) can learn typologically implausible languages by creating counterfactual versions of English and Japanese that violate Greenbergian word order correlations. Using a top-down approach, the authors systematically swapped grammatical elements (e.g., verb-object order) in natural corpora to generate plausible and implausible variants. They evaluated LMs (GPT-2 and LTG-BERT) on these counterfactual languages using perplexity, minimal pair preferences, and syntactic benchmarks (BLiMP/JBLiMP). Results show that LMs often learn counterfactual languages more slowly and with reduced accuracy compared to natural languages, particularly for highly unnatural swaps like <Verb, Object>. While counterfactual languages remain learnable, these findings suggest that LMs exhibit learning biases aligned with typological patterns, supporting the idea that such biases may arise from domain-general mechanisms rather than language-specific ones.

## Method Summary
The authors created counterfactual corpora by swapping grammatical elements in Wiki-40B English and Japanese using dependency parses from Stanza. They generated five counterfactual variants per language based on Greenbergian correlation pairs: <V, O>, <Adp, NP>, <Cop, Pred>, <Aux, V>, and <Noun, Genitive>. GPT-2 small and LTG-BERT were trained from scratch for 12 epochs with 3 random seeds each. Models were evaluated on held-out perplexity, minimal pair preferences on Wiki-40B sentences, and macro-averaged accuracy on downsampled BLiMP/JBLiMP benchmarks (5 examples per circuit). The study compared learning trajectories across natural (ORIGINAL), baseline (BASELINE), and counterfactual languages to assess typological plausibility effects.

## Key Results
- LMs learn counterfactual languages more slowly and with reduced accuracy compared to natural languages
- Performance drops are most pronounced for highly unnatural swaps like <Verb, Object> (20-30% lower accuracy)
- Counterfactual languages remain learnable despite substantial performance degradation
- Learning biases align with typological patterns across both English and Japanese

## Why This Works (Mechanism)
The paper demonstrates that language models exhibit learning biases that align with typological patterns observed across natural languages. When exposed to counterfactual languages that violate these patterns, models struggle more than with natural or baseline variants. This suggests that the models' learning dynamics are influenced by statistical regularities in natural language corpora rather than arbitrary syntactic rules.

## Foundational Learning
- **Typological universals**: Greenbergian word order correlations describe consistent patterns across languages. Why needed: Forms the theoretical foundation for identifying which counterfactual languages are implausible. Quick check: Verify understanding of head-initial vs head-final languages and the five correlation pairs tested.
- **Dependency parsing**: UD formalism used to identify grammatical relationships for swapping. Why needed: Enables systematic transformation of natural corpora into counterfactual variants. Quick check: Confirm understanding of dependency relations like nsubj, obj, and how they map to correlation pairs.
- **Perplexity**: Measures how well a language model predicts a test corpus. Why needed: Primary metric for comparing model performance across natural and counterfactual languages. Quick check: Understand that lower perplexity indicates better model fit to the data.

## Architecture Onboarding

**Component Map**
Wiki-40B -> Stanza Parser -> Counterfactual Generator -> GPT-2/LTG-BERT -> Evaluation (PPL, Minimal Pairs, BLiMP)

**Critical Path**
Counterfactual corpus generation -> Model training -> Perplexity evaluation -> Minimal pair preference testing -> BLiMP syntactic accuracy assessment

**Design Tradeoffs**
Top-down vs bottom-up counterfactual generation: The paper chose a top-down approach using dependency parsing for precision, accepting parser error rates (~80% precision/recall for most pairs) rather than potentially noisier bottom-up methods. This prioritizes grammatical validity over complete coverage.

**Failure Signatures**
Parser errors cascade into invalid counterfactual sentences, particularly problematic for <Cop, Pred> pairs (59.1/54.2 precision/recall). Japanese tokenization inconsistencies arise from Stanza's UD parser word-delimitation issues requiring long-unit-word parser selection.

**First 3 Experiments**
1. Generate a small counterfactual corpus (100 sentences) for one correlation pair and manually verify grammatical validity
2. Train GPT-2 small on a 1M word subset of counterfactual language for 3 epochs, check perplexity trajectory
3. Run minimal pair preference test on 100 held-out sentences to verify preference for original over counterfactual

## Open Questions the Paper Calls Out

**Open Question 1**: To what extent are the observed learning difficulties caused by noise introduced during counterfactual corpus generation (e.g., parser errors) versus inherent linguistic implausibility? The authors acknowledge they "cannot entirely rule out confounds due to errors introduced in the creation of counterfactual corpora" and call for future work to "explore alternative methods to reduce noise... or to control for the amount of noise."

**Open Question 2**: Do these harmonic learning biases persist across a wider variety of languages, particularly those with inconsistent word orders (e.g., German) or those outside the SVO/SOV dichotomy? The authors state their "conclusions are based only on two languages" and explicitly identify the need to "replicate these results with more SVO and SOV languages, and also on languages with inconsistent VO ordering, such as German."

**Open Question 3**: Can the observed typological patterns be better explained by communicative pressures (e.g., dependency length minimization) rather than domain-general learning biases? The discussion notes that "communicative pressures are another mechanism that might explain these phenomena" and identifies "extending our methods to test this mechanism" as a "promising avenue for future work."

## Limitations
- Counterfactual corpus generation relies on imperfect parsers, introducing noise that may confound results
- Study limited to two languages (English and Japanese), both with relatively consistent word orders
- Optimizer hyperparameters not fully specified, affecting reproducibility
- Computational requirements and hardware specifications not stated

## Confidence

**High confidence**: The core finding that LMs learn counterfactual languages more slowly and with reduced accuracy compared to natural languages is well-supported by multiple evaluation metrics (perplexity, minimal pairs, BLiMP/JBLiMP) across two language pairs and two model architectures.

**Medium confidence**: The claim that LMs exhibit learning biases aligned with typological patterns is supported but depends on the counterfactual corpus generation quality, which varies with parser accuracy. The interpretation that these biases arise from domain-general mechanisms rather than language-specific ones is plausible but not directly tested.

**Low confidence**: The claim that counterfactual languages remain "learnable" is somewhat misleading given the substantial performance drops observed, particularly for <Verb, Object> swaps where accuracy can be 20-30% lower than natural languages.

## Next Checks
1. Re-run the minimal pair preference task on held-out Wiki-40B sentences with additional evaluation checkpoints (epochs 6, 9, 12) to verify the learning trajectory patterns across all five correlation pairs.
2. Conduct ablation studies varying the random seed for BLiMP/JBLiMP downsampling (5 examples per circuit) to quantify variance in syntactic benchmark results and ensure findings are robust to sampling.
3. Implement diagnostic parsing validation on counterfactual corpora, checking the percentage of valid sentences per correlation pair against the reported precision/recall metrics (targeting 80%+ for most pairs, with special attention to <Cop, Pred> at 59.1/54.2).