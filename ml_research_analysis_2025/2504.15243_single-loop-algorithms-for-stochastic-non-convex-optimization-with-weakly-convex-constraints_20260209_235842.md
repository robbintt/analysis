---
ver: rpa2
title: Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex
  Constraints
arxiv_id: '2504.15243'
source_url: https://arxiv.org/abs/2504.15243
tags:
- constraint
- penalty
- learning
- optimization
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses stochastic non-convex optimization with weakly-convex
  constraints, a problem prevalent in machine learning applications. Existing methods
  often suffer from slow convergence rates or rely on double-loop designs.
---

# Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints

## Quick Facts
- arXiv ID: 2504.15243
- Source URL: https://arxiv.org/abs/2504.15243
- Authors: Ming Yang; Gang Li; Quanqi Hu; Qihang Lin; Tianbao Yang
- Reference count: 40
- Primary result: Achieves O(ϵ⁻⁶) complexity for finding approximate KKT solutions to stochastic non-convex optimization with weakly-convex constraints using a single-loop hinge-penalty method

## Executive Summary
This paper addresses stochastic non-convex optimization with weakly-convex constraints, a problem prevalent in machine learning applications. Existing methods often suffer from slow convergence rates or rely on double-loop designs. The authors propose a single-loop penalty-based stochastic algorithm using a hinge-based penalty function, which allows for a constant penalty parameter and achieves a state-of-the-art complexity of O(ϵ⁻⁶) for finding an approximate Karush-Kuhn-Tucker (KKT) solution. The method is extended to finite-sum coupled compositional objectives, common in AI applications, establishing improved complexity over existing approaches. Experiments on fair learning with ROC fairness constraints and continual learning with non-forgetting constraints demonstrate the effectiveness of the proposed algorithms compared to existing methods.

## Method Summary
The paper proposes a single-loop stochastic algorithm for weakly-convex constrained optimization using a hinge-based penalty function. The method employs MSVR (Multi-block-Single-Probe Variance Reduction) to efficiently handle stochastic constraints by maintaining running estimates of constraint values while sampling only a subset per iteration. The algorithm decouples constraint value tracking from parameter updates, allowing for constant batch sizes even with many constraints. The approach is extended to finite-sum coupled compositional objectives using similar variance reduction techniques. Key hyperparameters include the penalty parameter β, MSVR parameters γ₂ and γ′₂, and step size η, with theoretical guarantees under a regularity condition on the constraint subgradients.

## Key Results
- Achieves O(ϵ⁻⁶) complexity for finding approximate KKT solutions, improving over existing single-loop methods
- Constant penalty parameter β enables stable optimization landscape and prevents conditioning issues seen in squared-penalty methods
- Effective handling of stochastic constraints with MSVR allows sampling only a subset of constraints per iteration
- Experiments demonstrate improved performance on fair learning with ROC fairness constraints and continual learning with non-forgetting constraints compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
The non-smooth hinge penalty preserves the problem's weak convexity while allowing a constant penalty parameter β, preventing the conditioning issues seen in squared-penalty methods. Standard squared-hinge penalties require the penalty coefficient β to scale as O(1/ϵ) to force feasibility, creating a large weak-convexity modulus (O(1/ϵ)) that breaks convergence guarantees. The proposed hinge penalty [hₖ(x)]₊ is a composition of a convex function with a weakly convex function, preserving weak convexity and allowing β to remain constant (O(1)), stabilizing the optimization landscape.

### Mechanism 2
The convergence to a feasible solution is guaranteed by a "Full-Rank-at-Violating-Points" (FRVP) condition, which ensures the subgradients of the constraints provide a non-zero push towards the feasible region. The algorithm converges to a stationary point of the penalty function Φ(x). To ensure this stationary point is actually feasible (i.e., hₖ(x) ≤ 0), the paper relies on Eq. (3): for any violating point x, the subgradients of the active constraints must not cancel out to zero (distance from 0 is ≥ δ). This geometric condition ensures that if the solution were infeasible, the penalty term would generate a non-zero gradient to push it back, contradicting stationarity.

### Mechanism 3
Efficient handling of stochastic constraints is achieved by decoupling the tracking of constraint values from the parameter update via the MSVR (Multi-block-Single-Probe Variance Reduction) estimator. The subgradient of the penalty requires [hₖ(x)]₊, which is non-smooth. Simply using a mini-batch estimator for hₖ(x) is too noisy for the hinge function. MSVR maintains a running estimate u₂,ₖ for each constraint value. Crucially, it allows sampling only a subset of constraints B_c per iteration (constant batch size) rather than computing all m constraints, which is essential for problems with many constraints (e.g., ROC fairness with many thresholds).

## Foundational Learning

- **Concept: Weak Convexity & Moreau Envelopes**
  - Why needed here: The paper's theoretical backbone is built on "weakly convex" functions (a broader class than convex, but nicer than general non-convex). Since the objective is non-smooth, standard gradients don't always exist. The Moreau envelope acts as a smooth approximation of the non-smooth function, allowing the use of gradient-descent-style analysis.
  - Quick check question: Can you explain why minimizing the Moreau envelope Φ_θ(x) is equivalent to finding a stationary point of the original non-smooth Φ(x)?

- **Concept: KKT (Karush-Kuhn-Tucker) Conditions**
  - Why needed here: In constrained optimization, "solving" means finding a KKT point, not just a gradient-zero point. You need to verify primal feasibility (hₖ ≤ 0), stationarity (gradient balance), and complementary slackness (λₖhₖ = 0). The paper defines a "nearly ϵ-KKT" point to account for stochastic errors.
  - Quick check question: In the "nearly ϵ-KKT" definition (3.3), why is there a distinction between the solution x and the point x̄ where conditions actually hold?

- **Concept: Variance Reduction (VR) in Stochastic Optimization**
  - Why needed here: Standard SGD suffers from noise. VR techniques (like SPIDER, SARAH, or the MSVR used here) maintain a state to reduce variance over time. This paper uses MSVR because it handles "multi-block" structures (estimating m different constraint values) efficiently.
  - Quick check question: How does MSVR differ from simple momentum when estimating the constraint value hₖ(x)? (Hint: look at the recursive update term involving x_t and x_{t-1}).

## Architecture Onboarding

- **Component map:**
  Parameter Server (x_t) -> Constraint Estimator Bank (u₂,ₖ) -> Stochastic Oracles -> Penalty Gradient Computer

- **Critical path:**
  1. Sample a subset of constraint indices B_c ⊂ {1...m}
  2. For sampled constraints, retrieve data samples to compute hₖ(x_t) and update MSVR estimates u₂,ₖ
  3. Compute subgradient of the hinge penalty using the stored estimates u₂,ₖ (not the current batch average)
  4. Aggregate with objective gradient G₁ and step x

- **Design tradeoffs:**
  - Hinge vs. Squared Hinge: Hinge allows constant β (faster convergence theoretically) but introduces non-smoothness (requires subgradient methods). Squared hinge is smoother but theoretically requires β → ∞ (unstable)
  - Batch Size (B_c) vs. Accuracy: Smaller B_c reduces per-iteration cost but increases the variance of the MSVR estimator update, potentially requiring more total iterations T

- **Failure signatures:**
  - Stalling at Infeasibility: If β is set too low (below the theoretical threshold (ϵ+L_F)/δ), the optimizer may converge to a point where the gradient is zero but constraints are still violated
  - Estimator Drift: If learning rate η is too high relative to MSVR parameter γ₂, the constraint value estimates u₂,ₖ will lag too far behind the true hₖ(x_t), leading to unstable updates

- **First 3 experiments:**
  1. Synthetic Verification: Construct a 2D non-convex problem with weakly convex constraints (e.g., minimize |x| subject to |x-c| ≤ r). Verify that the algorithm converges to the KKT point visually
  2. Ablation on Penalty β: Run the Fair Learning task (Adult dataset) varying β. Plot "Constraint Violation vs. Epoch" and "AUC vs. Epoch". Expect to see a sharp threshold where constraints are satisfied (validating Theorem 4.2)
  3. Efficiency Comparison: Compare against the double-loop baseline (ICPPAC) on the CheXpert dataset. Measure wall-clock time to reach a feasible solution with target AUC, highlighting the overhead reduction of the single-loop design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the complexity for weakly-convex inequality constraints be improved to match the rates of smooth equality-constrained optimization?
- Basis in paper: The conclusion identifies the current O(ϵ⁻⁶) rate as a limitation compared to better rates in smooth equality-constrained settings
- Why unresolved: The non-smoothness and structure of inequality constraints introduce difficulties that the current single-loop penalty method cannot overcome to achieve faster rates
- What evidence would resolve it: A single-loop algorithm for this setting with a proven complexity lower than O(ϵ⁻⁶)

### Open Question 2
- Question: Can primal-dual techniques be effectively extended to stochastic, fully non-convex constrained optimization?
- Basis in paper: The conclusion lists this as an interesting future direction to potentially achieve better convergence guarantees than penalty-based methods
- Why unresolved: Existing smoothed proximal primal-dual methods are largely restricted to convex or linearly constrained settings
- What evidence would resolve it: A primal-dual algorithm convergence analysis for stochastic weakly-convex constraints matching or beating the proposed complexity

### Open Question 3
- Question: Can the regularity assumption (Eq. 3) be formally relaxed to hold only on a subset V_c rather than the entire infeasible set V?
- Basis in paper: Section 4 notes that requiring the condition only on the subset V_c = {x : c ≥ max_k h_k(x) > 0} is strictly weaker and potentially feasible
- Why unresolved: The authors utilize the global assumption for simplicity, noting that the relaxed version requires careful step-size selection to keep iterates within V_c
- What evidence would resolve it: A convergence proof demonstrating that initializing within the feasible region and using specific step sizes ensures the relaxed condition suffices

## Limitations

- The FRVP condition's practical testability remains unclear; real-world constraints may not satisfy this regularity in degenerate cases
- Extension to finite-sum compositional objectives relies on idealized variance reduction assumptions that may degrade with large m
- Theoretical β thresholds require problem-specific parameters (δ, L_F) that may be difficult to estimate in practice

## Confidence

- **High confidence**: O(ϵ⁻⁶) complexity bound for single-loop hinge penalty method under stated assumptions
- **Medium confidence**: FRVP condition sufficiency for feasibility (condition is necessary but not always easy to verify)
- **Low confidence**: Extension to compositional objectives (Sec. 5) due to dependence on idealized MSVR assumptions

## Next Checks

1. **FRVP violation test**: Construct a degenerate constraint system (e.g., perfectly correlated constraints) and verify whether the algorithm can get stuck at an infeasible stationary point
2. **MSVR parameter sensitivity**: Systematically vary γ₂ and η to identify regimes where estimator drift causes constraint violation spikes
3. **Scalability benchmark**: Test the algorithm on a synthetic problem with m=1000 constraints to verify the constant |B_c| scaling advantage in practice