---
ver: rpa2
title: 'FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive
  Programming'
arxiv_id: '2507.13337'
source_url: https://arxiv.org/abs/2507.13337
tags:
- graph
- problems
- vertices
- vertex
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FormulaOne, a benchmark designed to evaluate
  the depth of algorithmic reasoning capabilities of AI models. Unlike standard competitive
  programming benchmarks, FormulaOne focuses on real-life research problems at the
  intersection of graph theory, logic, and algorithms, using Monadic Second-Order
  (MSO) logic on graphs as its foundation.
---

# FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming

## Quick Facts
- **arXiv ID:** 2507.13337
- **Source URL:** https://arxiv.org/abs/2507.13337
- **Reference count:** 8
- **Primary Result:** FormulaOne benchmark achieves <1% success rate for o3 on hard problems, highlighting gap between current AI and deep algorithmic reasoning

## Executive Summary
FormulaOne is a benchmark designed to evaluate the depth of algorithmic reasoning capabilities of AI models, focusing on real-life research problems at the intersection of graph theory, logic, and algorithms. Unlike standard competitive programming benchmarks, FormulaOne uses Monadic Second-Order (MSO) logic on graphs as its foundation, with 120 challenging dynamic programming problems and 100 warmup problems generated from a principled, semi-mechanistic framework. The benchmark is deeply connected to cutting-edge theoretical computer science, with many problems related to central conjectures like the Strong Exponential Time Hypothesis (SETH). Despite being well within the training distribution, state-of-the-art models like OpenAI's o3 achieve less than 1% success rate on the hard problems, even with multiple attempts and explanatory fewshot examples.

## Method Summary
FormulaOne employs a semi-mechanistic framework based on Monadic Second-Order (MSO) logic on graphs to generate challenging algorithmic problems. The benchmark includes 220 total problems: 120 hard problems and 100 warmup problems, all focusing on dynamic programming. Problems are designed to be deeply connected to theoretical computer science frontiers, with connections to central conjectures like SETH. The benchmark supports automatic problem generation at scale and includes a comprehensive evaluation framework with automatic test-case generation and grading. The methodology emphasizes real-life research problems rather than typical competitive programming challenges, aiming to measure reasoning depth beyond pattern matching.

## Key Results
- State-of-the-art models like o3 achieve less than 1% success rate on FormulaOne's hard problems
- Despite being within training distribution, models struggle with the depth of reasoning required
- Benchmark supports automatic problem generation, making it ideal for building RL environments
- Highlights significant gap between current AI capabilities and the reasoning required for frontier research problems

## Why This Works (Mechanism)
FormulaOne works by grounding algorithmic reasoning problems in Monadic Second-Order logic on graphs, which provides a principled mathematical foundation for generating problems that are both theoretically meaningful and practically challenging. The semi-mechanistic generation framework ensures problems are not just difficult but also connected to real research frontiers in theoretical computer science, particularly through relationships to central conjectures like SETH. By focusing on dynamic programming problems that require deep structural understanding rather than surface-level pattern matching, the benchmark effectively discriminates between genuine algorithmic reasoning and superficial problem-solving approaches.

## Foundational Learning

**MSO Logic on Graphs**: Understanding monadic second-order logic over graph structures - why needed: forms the mathematical foundation for problem generation; quick check: can formalize graph properties using MSO syntax

**Strong Exponential Time Hypothesis (SETH)**: Knowledge of this central conjecture in theoretical computer science - why needed: many problems are connected to SETH's implications; quick check: understand the relationship between SETH and algorithmic lower bounds

**Dynamic Programming Principles**: Deep understanding of dynamic programming paradigms and their theoretical foundations - why needed: all hard problems use dynamic programming; quick check: can identify optimal substructure and overlapping subproblems in graph contexts

**Problem Generation Semi-mechanisms**: Understanding how problems are algorithmically generated from logical specifications - why needed: ensures benchmark problems are principled rather than arbitrary; quick check: can trace problem generation from MSO specifications to executable instances

## Architecture Onboarding

**Component Map**: Problem Generation Engine -> Problem Repository -> Evaluation Framework -> Model Interface -> Grading System -> Results Dashboard

**Critical Path**: The evaluation pipeline follows: (1) Problem selection from repository, (2) Model attempt with test cases, (3) Automatic grading against ground truth, (4) Performance aggregation and analysis

**Design Tradeoffs**: Heavy focus on dynamic programming provides depth but may limit breadth; MSO-based generation ensures theoretical grounding but may reduce problem diversity; automatic generation enables scalability but requires careful validation of problem quality

**Failure Signatures**: Models fail primarily on problems requiring novel algorithmic insights rather than pattern matching; common failure modes include incorrect identification of problem structure, suboptimal DP state design, and inability to generalize from warmup problems to hard variants

**First Experiments**:
1. Run warmup problems to establish baseline performance and identify easy vs. hard problem characteristics
2. Test sensitivity to attempt limits by varying the number of allowed attempts per problem
3. Compare performance across different model families (reasoning vs. non-reasoning models) to establish relative difficulty

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Evaluation with only 5 attempts per problem may underestimate model capabilities; sensitivity to attempt limits is not fully explored
- Heavy focus on dynamic programming problems (120 out of 220 total) may limit generalizability to other algorithmic domains
- Claim that problems represent "real-life research problems" needs more concrete validation from theoretical computer science experts

## Confidence

**High confidence**: Technical framework and problem generation methodology are well-documented and principled, providing a solid foundation for the benchmark
**Medium confidence**: Empirical evaluation results are meaningful but require careful interpretation given limited attempts and lack of sensitivity analysis
**Low confidence**: Generalizability of results beyond the specific subset of dynamic programming problems tested, and the connection to actual research challenges needs stronger validation

## Next Checks

1. Conduct sensitivity analysis by varying the number of attempts per problem (e.g., 5, 10, 20) to establish the relationship between attempt limits and success rates
2. Expand evaluation to include problems from other algorithmic paradigms beyond dynamic programming to assess benchmark breadth
3. Perform expert validation by having theoretical computer scientists evaluate whether the generated problems truly represent frontier research challenges in graph theory and algorithms