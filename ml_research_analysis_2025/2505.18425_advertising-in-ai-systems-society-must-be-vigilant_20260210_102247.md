---
ver: rpa2
title: 'Advertising in AI systems: Society must be vigilant'
arxiv_id: '2505.18425'
source_url: https://arxiv.org/abs/2505.18425
tags:
- content
- advertising
- systems
- commercial
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper envisions the future of advertising within AI systems,
  predicting that commercial incentives will increasingly shape content served by
  generative AI, much like they have for web search and social media. The authors
  identify two main types of sponsored content: static advertisements (unchanging
  media) and generative advertisements (dynamic, personalized AI outputs influenced
  by commercial factors).'
---

# Advertising in AI systems: Society must be vigilant

## Quick Facts
- arXiv ID: 2505.18425
- Source URL: https://arxiv.org/abs/2505.18425
- Reference count: 21
- Primary result: Commercial incentives will increasingly shape AI-generated content, requiring new design principles and bias mitigation strategies.

## Executive Summary
This paper examines the emerging integration of advertising within generative AI systems, predicting that commercial content will become as prevalent in AI outputs as it is in search and social media today. The authors propose a framework for incorporating "generative advertisements" - sponsored content woven directly into AI responses - while maintaining four core design principles: faithfulness to sponsor content, utility preservation, user privacy with opt-out options, and provenance tracking. They identify critical challenges around commercial bias detection, training contamination risks, and regulatory compliance for dynamic, personalized ad content. The paper concludes with open questions requiring coordinated research efforts to ensure responsible commercialization of AI technologies.

## Method Summary
The proposed approach extends retrieval-augmented generation (RAG) by treating ad retrieval as a parallel tool call alongside content search. Sponsored content is injected into the context window with explicit tags (e.g., `<ad> zad </ad>`) and integrated into the final response while maintaining provenance mapping φ that tracks which output tokens originated from sponsored sources. The system design emphasizes concurrent ad auction mechanics, privacy-preserving opt-out mechanisms, and verification through cached KV state analysis. Training involves supervised fine-tuning on curated examples of natural ad integration and reinforcement learning using implicit user feedback as rewards, though the exact constitution of training data and ad auction logic remain unspecified.

## Key Results
- Two primary ad formats identified: static advertisements (unchanging media) and generative advertisements (dynamic, personalized AI outputs)
- Four core design principles established: faithfulness, utility, privacy, and provenance
- Multi-sampling approach proposed for isolating commercial bias from organic content
- Provenance tracking φ enables compliance verification but requires further validation for stochastic outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-call patterns in LLMs can serve advertisements concurrently with content retrieval while preserving provenance.
- Mechanism: The system treats ad retrieval as a parallel tool call alongside search/RAG. Sponsored content is injected into the context with explicit tags (e.g., `<ad> zad </ad>`), allowing the model to integrate it into responses while maintaining traceability for compliance and disclosure.
- Core assumption: Models can synthesize sponsored and organic content coherently without degrading utility. Assumption: Users will accept tagged disclosures without abandoning the platform.
- Evidence anchors:
  - [Section 3.3]: "tool calls occur whenever external information or additional computation is needed, and acquiring advertisements is treated as one such tool call that runs concurrently upon receipt of the user's query."
  - [Section 3.2]: Defines provenance mapping φ: yi → (0,1)^{z1,...,zn} tracking which sources contributed to each output token.
  - [corpus]: Weak direct evidence—neighbors focus on ad summarization and auction mechanics (NGA, GPR papers), not tool-call ad serving specifically.
- Break condition: If ad insertion degrades response utility below threshold α (per the utility constraint), or if provenance tracking fails under stochastic outputs.

### Mechanism 2
- Claim: Multi-sampling across varied serving contexts can isolate commercial bias from organic content.
- Mechanism: Generate k outputs under different parameters (cookies, geolocation, synthetic personas). Commercial content varies with targeting; stable segments across samples represent unbiased core content. Aggregate to filter sponsor-specific elements.
- Core assumption: Ads vary across contexts; organic content remains stable. Assumption: Multiple LLM calls are affordable.
- Evidence anchors:
  - [Section 4.2]: "changing platforms or other settings may lead to differing profiles of the same user, leading to shifts in the distribution of targeted advertisements."
  - [Section 4.2]: References statistical tests for identifying distributional differences in LLM outputs (Maini et al., 2024; Modarressi et al., 2025).
  - [corpus]: No direct corpus validation for this specific debiasing approach.
- Break condition: If ads do not vary across contexts, or if commercial influence persists even in "ad-free" model weights from training contamination.

### Mechanism 3
- Claim: Commercial content in human feedback loops risks embedding sustained bias into model weights.
- Mechanism: If ad-influenced outputs enter RLHF/alignment datasets without proper labeling, models may "forget" content is sponsored and reproduce commercial preferences even without active ad injection.
- Core assumption: Human feedback significantly shapes model behavior (well-supported in literature). Assumption: Sponsored content labels are stripped or ignored during training.
- Evidence anchors:
  - [Section 2.2]: "If commercial content is naively included as part of these alignment datasets, AI models may inadvertently 'forget' that these content are sponsored."
  - [Section 5]: "Determining how to incorporate sponsored outputs into alignment pipelines without systematically biasing the model remains an unsolved problem."
  - [corpus]: No direct corpus evidence on longitudinal alignment degradation from ads.
- Break condition: If training pipelines explicitly exclude or properly label sponsored outputs.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The proposed architecture extends RAG to include sponsored retrieval as a parallel information source.
  - Quick check question: Can you explain how RAG differs from relying solely on model weights for knowledge?

- Concept: RLHF / Preference Alignment
  - Why needed here: Understanding how human feedback shapes model behavior is essential to grasping why ad-contaminated training data poses long-term risks.
  - Quick check question: What role does reward modeling play in aligning LLMs to human preferences?

- Concept: Native Advertising
  - Why needed here: "Generative advertisements" are a form of native advertising—content integrated into the primary media format rather than displayed separately.
  - Quick check question: How does native advertising differ from display advertising in terms of user perception and regulatory treatment?

## Architecture Onboarding

- Component map:
  User Prompt (x) → Concurrent Tool Calls → [Content Tool (zw/o ad) | Ad Tool + Auction (zad)] → Optional Chain-of-Thought → Ad Insertion (tagged context) → Final Generation (yw/ad) → Post-generation Analysis (ad-report)

- Critical path:
  1. User query received
  2. Parallel retrieval: organic content + ad auction results
  3. Ad content tagged and injected into context
  4. Model generates response integrating both sources
  5. Provenance logged for compliance; optional self-verification via cached KV state

- Design tradeoffs:
  - Transparency vs. UX: Prominent disclosures erode trust if covert; excessive disclosures disrupt interaction flow.
  - Personalization vs. privacy: Opt-out users receive generic ads; this may reduce platform revenue but ensures compliance.
  - Training inclusion vs. bias: Including ad-influenced outputs in alignment data may improve ad integration but risks long-term bias.

- Failure signatures:
  - Utility drops below threshold α after ad insertion (measurable via relevance/accuracy metrics).
  - Provenance mapping φ fails to attribute sponsored tokens (compliance risk).
  - Debiasing produces incoherent outputs (indicates ad-free model assumption violated).
  - Users cannot effectively opt out of targeting despite ou=OUT selection.

- First 3 experiments:
  1. Implement parallel tool-call prototype with mock ad sources; measure latency overhead and utility degradation vs. ad-free baseline.
  2. Test multi-sampling debiasing on synthetic ad-influenced outputs; quantify precision/recall of commercial content removal.
  3. Audit provenance tracking φ on a held-out set of ad-tagged generations; verify token-level source attribution accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation metrics can effectively quantify commercial bias in LLM outputs?
- Basis in paper: [explicit] The authors state in Section 5, "It is not yet clear what metrics best reflect commercial bias in LLM outputs," noting that current practices focus on social bias or factuality rather than promotional influence.
- Why unresolved: Commercial bias differs from social bias as it is not innate to weights but shifts dynamically based on sponsor requirements, making standard debiasing metrics insufficient.
- Evidence to resolve it: A standardized benchmark dataset and scoring methodology capable of detecting subtle promotional language across diverse generative tasks.

### Open Question 2
- Question: How can sponsored content be included in alignment training data without systematically biasing the model's core knowledge?
- Basis in paper: [explicit] Section 5 highlights that "determining how to incorporate sponsored outputs into alignment pipelines without systematically biasing the model remains an unsolved problem."
- Why unresolved: Models trained on ad-augmented responses via RLHF may fail to distinguish between factual knowledge and paid placements, leading to persistent commercial biases or hallucinations.
- Evidence to resolve it: A training protocol that successfully isolates or "surgically" removes commercial influence from the model's reasoning process while retaining the ability to serve ads at inference time.

### Open Question 3
- Question: How can regulators verify compliance with disclosure standards when generative AI outputs are stochastic and highly personalized?
- Basis in paper: [explicit] Section 5 asks, "It is not yet clear how regulators will verify compliance when model outputs differ across users or sessions."
- Why unresolved: Unlike static ads, generative ads are woven dynamically into text, making it difficult to reproduce exact outputs for audit purposes or apply traditional disclosure verification.
- Evidence to resolve it: Development of reliable logging mechanisms, reproducibility techniques, or automated auditing tools specifically designed for dynamic, AI-generated content.

## Limitations

- Proposed architectures for commercial bias detection and mitigation remain largely conceptual with limited empirical validation.
- Long-term alignment risks from commercial content in training data lack empirical evidence from real deployment scenarios.
- Multi-sampling debiasing approach assumes ads vary across contexts while organic content remains stable - this assumption requires testing.

## Confidence

- **High confidence**: Commercial incentives will shape generative AI content delivery (supported by historical precedent in search/social media).
- **Medium confidence**: Tool-call architecture for concurrent ad retrieval can work technically, but utility preservation and user acceptance remain unproven.
- **Low confidence**: Long-term alignment risks from commercial content in training data are theoretically sound but lack empirical evidence.

## Next Checks

1. Implement and measure the parallel tool-call prototype with mock ad sources to quantify actual latency overhead and utility degradation compared to ad-free baselines across 100+ diverse user queries.
2. Conduct a multi-sampling debiasing experiment using synthetic ad-influenced outputs to measure precision/recall of commercial content removal and assess whether stable content segments truly represent unbiased core information.
3. Audit provenance tracking φ on a held-out set of ad-tagged generations to verify token-level source attribution accuracy under varying generation parameters and measure tracking overhead.