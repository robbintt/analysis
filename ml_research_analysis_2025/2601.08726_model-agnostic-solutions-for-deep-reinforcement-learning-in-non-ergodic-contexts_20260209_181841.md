---
ver: rpa2
title: Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts
arxiv_id: '2601.08726'
source_url: https://arxiv.org/abs/2601.08726
tags:
- agent
- policy
- value
- expected
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of optimizing reinforcement learning
  agents in non-ergodic environments, where expected-value optimization fails to capture
  long-term growth dynamics. The authors extend prior work by examining deep RL implementations,
  showing that standard DQN and actor-critic architectures still rely on expected
  values and thus produce suboptimal policies in multiplicative, path-dependent settings.
---

# Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts

## Quick Facts
- arXiv ID: 2601.08726
- Source URL: https://arxiv.org/abs/2601.08726
- Reference count: 33
- Key outcome: Temporal repetition in DRL enables agents to learn time-average growth rates in non-ergodic environments, outperforming expected-value optimization

## Executive Summary
This work addresses a fundamental limitation in deep reinforcement learning: standard RL algorithms optimize for expected values, which diverges from time-average growth in non-ergodic (multiplicative, path-dependent) environments. The authors demonstrate that even sophisticated DQN and actor-critic architectures fail to capture optimal policies in such settings. They propose a simple solution—repeating decision steps during training—which allows neural networks to implicitly learn growth-rate-optimal behavior without modifying the objective function. Experiments on toy models and portfolio allocation problems show that agents trained with temporal repetition align with theoretically optimal Kelly criterion policies, while traditional single-step training does not.

## Method Summary
The authors propose temporal repetition during training as a model-agnostic solution for non-ergodic RL. Instead of modifying rewards or objective functions, they repeat each action M times per episode, allowing agents to experience compounding dynamics. This exposes neural network function approximators to typical outcome distributions rather than ensemble averages. The approach is implemented with standard DQN and actor-critic architectures, requiring only modification of the training loop to repeat actions before computing updates. Experiments use simple network architectures (1 hidden layer) with varying repetition counts M to demonstrate convergence toward time-average optimal policies.

## Key Results
- DQN agents with M=1 reproduce expected-value behavior, while M≥5 shifts indifference points toward time-growth predictions
- Actor-critic agents trained with temporal repetition learn portfolio fractions that match Kelly criterion optimal policies
- The approach works without modifying reward structures or objective functions
- Increasing model complexity alone (tabular → DQN → actor-critic) does not resolve ergodicity-breaking limitations

## Why This Works (Mechanism)

### Mechanism 1: Expected Value-Ensemble Divergence in Non-Ergodic Dynamics
- Claim: Standard DRL produces suboptimal policies in non-ergodic environments because the Bellman equation's expected value formulation diverges from time-average growth.
- Mechanism: In non-ergodic (multiplicative, path-dependent) dynamics, the ensemble average E[R] ≠ time-average growth rate. The Bellman equation V(s) = E[r_t+1 + γV(s_t+1)|s_t=s] implicitly assumes ergodicity, causing agents to optimize for the wrong objective.
- Core assumption: The environment exhibits multiplicative, path-dependent dynamics where trajectory outcomes dominate over ensemble statistics.
- Evidence anchors:
  - [abstract]: "the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies"
  - [section 2]: "the multiplicative dynamic is non-ergodic and therefore well described by the time-average growth rate, but not by its expected value"
  - [corpus]: Weak direct corpus support; related papers address ensemble strategies but not ergodicity-breaking in RL
- Break condition: If environment dynamics are ergodic (additive rewards, path-independent), expected value optimization is appropriate and this mechanism does not apply.

### Mechanism 2: Temporal Repetition Exposes Compounding Dynamics
- Claim: Repeating decision steps during training allows neural network function approximators to learn time-average growth rates without modifying the objective function.
- Mechanism: By repeating actions M times per episode, the agent experiences multiple trajectories from the same policy decision. This exposes the network to compounding effects and typical outcome distributions, allowing Qθ(s,·) or πθ(f|s) to implicitly encode growth-rate-optimal behavior.
- Core assumption: Sufficient repetitions (M) allow sampling from the typical outcome distribution that dominates long-term behavior.
- Evidence anchors:
  - [abstract]: "explicitly incorporating temporal information by repeating decision steps during training, allowing agents to learn time-average growth rates"
  - [section 4.1, Figure 3b]: Indifference point p0 shifts from expected-value prediction pE toward time-growth prediction pT as M increases
  - [corpus]: No direct validation; ensemble trading papers use different approaches
- Break condition: If M is too small, or if multiplicative process variance is extreme, convergence to typical outcomes may not occur within practical episode lengths.

### Mechanism 3: Architectural Complexity Insufficiency
- Claim: Increasing model complexity (tabular RL → DQN/actor-critic) does not resolve ergodicity-breaking; temporal structure must be explicitly introduced.
- Mechanism: Neural networks can capture complex dependencies, but when the learning signal is defined via expected values (Bellman backup), the network learns a sophisticated expected-value estimator rather than a growth-rate-optimal policy.
- Core assumption: The gradient signal from standard loss functions (SmoothL1 on Bellman target) reinforces ensemble-average optimization.
- Evidence anchors:
  - [section 1]: "neural networks, as function approximators, are designed to capture complex dependencies in the data, our findings show that this capability does not resolve the underlying limitation"
  - [section 4.1]: DQN with M=1 reproduces expected-value behavior despite neural network approximation
  - [corpus]: Ensemble DQN work focuses on forgetting, not ergodicity
- Break condition: If the objective or reward signal is pre-transformed (e.g., logarithmic utility), architectural complexity becomes sufficient.

## Foundational Learning

- Concept: **Ergodicity vs Non-Ergodicity**
  - Why needed here: Determines whether ensemble averages equal time averages, which dictates if standard RL is appropriate.
  - Quick check question: For your environment, would 100 parallel agents' average outcome equal one agent's 100-step trajectory average?

- Concept: **Time-Average Growth Rate vs Expected Value**
  - Why needed here: Multiplicative dynamics require growth-rate optimization, not expected-value optimization.
  - Quick check question: Does your reward compound multiplicatively (percentage growth) or additively (fixed increments)?

- Concept: **Kelly Criterion**
  - Why needed here: Provides the theoretical optimal policy for multiplicative settings; used as ground truth for validation.
  - Quick check question: Can you derive the optimal bet fraction f* for your problem using the Kelly formula?

## Architecture Onboarding

- Component map:
  - State encoder: Current wealth W_t as input scalar
  - Value network Qθ(s,·): DQN (1 hidden layer, 16 units), outputs Q-values for |A| actions
  - Policy network πθ(f|s): Actor for continuous action f ∈ [0,1]
  - Critic Vψ(s): Value estimate for advantage calculation
  - Replay buffer D: Stores transitions (s, a, r, s')
  - Training loop: Modified to repeat actions M times per episode before update

- Critical path:
  1. Initialize wealth W_0, probability p
  2. For M repetitions: select action → apply multiplicative reward → update wealth
  3. Store trajectory; compute Bellman targets from accumulated rewards
  4. Backpropagate loss, update weights
  Key modification: M > 1 enables growth-rate learning

- Design tradeoffs:
  - **Repetitions M**: Higher M → better growth-rate alignment but slower training
  - **Episode collection**: Full-episode buffer before updates (not step-by-step)
  - **Discrete vs continuous**: DQN for discrete choice, Actor-Critic for portfolio fraction
  - Assumption: Single hidden layer suffices for toy problems

- Failure signatures:
  - **Indifference point stuck at pE**: Agent optimizing expected values (M too small)
  - **All-or-nothing portfolio (f ∈ {0,1})**: Expected-value optimization
  - **High policy variance**: Insufficient episodes or M
  - **Divergence in full policy learning**: Joint optimization across p unstable (Appendix A.3)

- First 3 experiments:
  1. Train DQN with M=1 on toy model; verify indifference point ≈ pE (expected-value prediction)
  2. Sweep M ∈ {1, 2, 5, 10, 20}; plot shift toward pT (time-growth prediction)
  3. Train actor-critic on portfolio; compare learned f*(p) to Kelly optimal; measure MSE to theoretical policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the temporal repetition methodology be extended to standard risk-sensitive reinforcement learning tasks without altering the objective function?
- Basis in paper: [explicit] The conclusion states, "A future consideration of our methodology can extend to the field of risk-sensitive RL as well."
- Why unresolved: The current experiments are limited to growth-rate optimization (Kelly criterion) and do not evaluate standard risk metrics like variance or drawdown.
- What evidence would resolve it: Application of the repetition method to established risk-sensitive benchmarks (e.g., financial hedging) showing convergence to risk-optimal policies.

### Open Question 2
- Question: How does the stability and sample efficiency of this method scale to high-dimensional control tasks or complex state representations?
- Basis in paper: [inferred] Appendix A.3 notes that global policy learning is "considerably more difficult to stabilise" and requires "larger number of training episodes" even in the simplified portfolio task.
- Why unresolved: While the method works for toy models, the instability observed when learning full policies suggests potential limitations in more complex, high-dimensional environments.
- What evidence would resolve it: Successful convergence analysis in complex environments (e.g., robotics simulators) without requiring exponential increases in training samples.

### Open Question 3
- Question: Is the temporal repetition approach effective for non-ergodic dynamics that are additive rather than strictly multiplicative?
- Basis in paper: [inferred] The paper focuses exclusively on "multiplicative dynamics" and "compounding processes" (Page 2, Page 9), leaving other forms of ergodicity breaking untested.
- Why unresolved: The proposed solution relies on the agent observing compounding effects over time; it is unclear if additive processes provide a sufficient learning signal for this method.
- What evidence would resolve it: Empirical results from non-ergodic additive environments (e.g., random walks with drift) demonstrating that the agent learns the time-optimal policy.

## Limitations

- The critical role of M repetitions is demonstrated empirically but lacks theoretical grounding for convergence rates or optimal M selection across problem classes
- High learning rate (α=0.8) in DQN may be a typo; standard rates are 10-100× smaller, potentially affecting reproducibility
- Actor-critic joint optimization across all p values shows instability, suggesting the method may not scale to richer state spaces without additional constraints

## Confidence

- **High**: Non-ergodic environments require time-average growth optimization rather than expected-value optimization (supported by multiplicative dynamics analysis)
- **Medium**: Temporal repetition (M > 1) effectively shifts policies toward time-growth optimality (strong empirical evidence but limited theoretical analysis)
- **Medium**: Standard DRL architectures fail to capture non-ergodic dynamics even with increased model complexity (demonstrated in toy problems but untested in complex domains)

## Next Checks

1. **Learning rate verification**: Re-run DQN experiments with α=0.001 and α=0.01 to determine if reported results require the unusually high rate of 0.8
2. **Theoretical convergence analysis**: Derive or bound the number of repetitions M required for convergence to the typical outcome distribution as a function of multiplicative process variance
3. **Scalability test**: Apply the temporal repetition method to a higher-dimensional non-ergodic problem (e.g., multi-asset portfolio with correlated returns) to assess whether the approach extends beyond simple toy models