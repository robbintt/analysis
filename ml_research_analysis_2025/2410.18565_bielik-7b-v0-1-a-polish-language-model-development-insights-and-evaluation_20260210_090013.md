---
ver: rpa2
title: 'Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation'
arxiv_id: '2410.18565'
source_url: https://arxiv.org/abs/2410.18565
tags:
- language
- polish
- training
- tokens
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bielik 7B v0.1 is a 7-billion-parameter Polish language model developed
  through pretraining on curated Polish corpora and instruction tuning. It introduces
  innovative techniques including Weighted Instruction Cross-Entropy Loss for balancing
  different instruction types and Adaptive Learning Rate for dynamic learning rate
  adjustment.
---

# Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation

## Quick Facts
- arXiv ID: 2410.18565
- Source URL: https://arxiv.org/abs/2410.18565
- Reference count: 40
- Bielik-7B-Instruct-v0.1 achieves 86.00 RAG Reader score vs Mistral-7B-Instruct-v0.1 at 73.68

## Executive Summary
Bielik 7B v0.1 is a 7-billion-parameter Polish language model developed through continued pretraining from Mistral-7B-v0.1 on 36 billion tokens (22B Polish, 14B English) followed by supervised fine-tuning on 2.3 million instructions. The model introduces two novel training techniques: Weighted Instruction Cross-Entropy Loss for handling mixed-quality instruction data and Adaptive Learning Rate for dynamic learning rate adjustment. Bielik demonstrates significant improvements on Polish-specific benchmarks, particularly excelling in RAG tasks (86.00 vs 73.68) and Polish MT-Bench (6.15/10 in Reasoning, 7.83/10 in Role-playing). Multiple quantized versions are available, with imatrix-quantized models showing consistently better performance across various quantization levels.

## Method Summary
The model was developed through continued pretraining from Mistral-7B-v0.1 on 36 billion tokens (22B Polish + 14B English) using AdamW optimizer with cosine learning rate decay, followed by supervised fine-tuning on 2.3 million instructions with novel Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate techniques. The training pipeline used 256 NVIDIA GH200 GPUs, achieving ~9,200 tokens/GPU/sec during pretraining. Post-training incorporated masked token loss (user instructions excluded from loss calculation) and quality-weighted instruction pairs (high/medium/low quality with weights 1.0/0.7/0.5). The model retains Mistral's original 32K tokenizer despite suboptimal Polish efficiency to avoid vocabulary merge ambiguity issues.

## Key Results
- RAG Reader task: 86.00 vs Mistral-7B-Instruct-v0.1 at 73.68 (9 percentage point improvement)
- Polish MT-Bench: 6.15/10 in Reasoning category, 7.83/10 in Role-playing category
- Outperforms Trurl-7B-v0.1-Instruct on Open PL LLM Leaderboard with 29.38 average vs 27.93
- Multiple quantized versions available (Q2_K through Q8_0), with imatrix-quantized models showing consistent performance advantages

## Why This Works (Mechanism)

### Mechanism 1
Weighted Instruction Cross-Entropy Loss enables training on mixed-quality data without degrading benchmark performance by assigning quality weights (1.0/0.7/0.5) to instruction-response pairs, reducing gradient contribution from lower-quality samples while preserving their diversity signal. This modifies standard cross-entropy: l(oi, yi) = −wi · Σc yi,c · logpi,c. The technique assumes quality labels correlate with training utility while lower-quality data still contains useful structural patterns.

### Mechanism 2
Adaptive Learning Rate stabilizes training when instruction lengths vary significantly by scaling base learning rate by √(T/BS) where T = actual token count per batch, BS = baseline batch size. This normalizes effective gradient magnitude per instruction. The technique assumes fluctuations in token count cause meaningful variance in loss, making learning rate scaling more effective than padding/masking alone.

### Mechanism 3
Continued pre-training on curated Polish corpora with English mixing mitigates catastrophic forgetting while improving Polish fluency. Initializing from Mistral-7B-v0.1 and training on 22B Polish tokens + 14B English tokens (36B total) retains original capabilities while shifting distribution toward Polish. The approach assumes retaining original tokenizer is acceptable tradeoff vs vocabulary expansion complexity.

## Foundational Learning

- **Catastrophic forgetting in continual learning**: Why needed - The paper explicitly includes English data (14B/36B tokens) to prevent loss of original Mistral capabilities. Quick check - What would happen if you trained only on Polish data without English mixing?

- **Weighted loss functions for imbalanced/heterogeneous data**: Why needed - The Weighted Instruction Cross-Entropy Loss directly applies this concept. Quick check - How does multiplying loss by 0.5 vs 1.0 change the gradient update for that sample?

- **Tokenizer efficiency metrics (CpT, TpW)**: Why needed - Section 2.2 evaluates tokenizers using characters-per-token and tokens-per-word. Quick check - Why might a higher TpW (tokens per word) hurt model performance?

## Architecture Onboarding

- **Component map**:
Mistral-7B-v0.1 (frozen weights, original tokenizer) -> Continued Pre-training (36B tokens: 22B PL + 14B EN) -> Supervised Fine-Tuning (2.3M instructions, 700M tokens) -> Quantization (GGUF/imatrix, Q2_K to Q8_0)

- **Critical path**:
1. Data quality filtering: XGBoost classifier on 266 stylometric features → 90% threshold → 18M documents selected
2. Tokenizer decision: Attempted Mistral+APT3 merge → ambiguity issues → reverted to original Mistral tokenizer
3. Training infrastructure: ALLaMo framework on 256 GH200 GPUs, ~9,200 tokens/GPU/sec

- **Design tradeoffs**:
- **Tokenizer efficiency vs. stability**: Kept suboptimal 32K vocab to avoid BPE merge conflicts; accepted ~35% higher TpW for Polish vs. English
- **Data quality vs. volume**: Included low-quality instructions with 0.5 weight rather than discarding entirely
- **Specialization vs. generality**: Base model optimized for RAG tasks (88.39 Reader score) at cost of general benchmark average (29.38 vs. Mistral's 30.67)

- **Failure signatures**:
- Tokenizer merge attempts cause "incorrect token combinations" due to vocabulary overlap ambiguity
- Without English mixing, Polish-only training risks losing broader capabilities
- Aggressive quantization (Q2_K) without imatrix calibration shows 16.4% RMS ∆p degradation

- **First 3 experiments**:
1. **Validate data quality classifier**: Train XGBoost on annotated subset, verify 90% threshold correlates with manual quality assessment on held-out 1,000 documents
2. **Ablate weighted loss**: Compare uniform weighting vs. 1.0/0.7/0.5 scheme on held-out Polish benchmarks; expect larger gap on reasoning tasks
3. **Quantization stress test**: Compare Q4_K_M with and without imatrix calibration on Polish generation tasks (not just perplexity); verify calibration dataset covers Polish linguistic patterns

## Open Questions the Paper Calls Out

### Open Question 1
How can the tokenizer be effectively expanded to include Polish-specific tokens without introducing the ambiguity issues observed during BPE merging? The current Mistral tokenizer is suboptimal for Polish (higher tokens per word), but the technical solution to expand the vocabulary without generation errors remains unidentified. Evidence would require a successful fine-tuning run or pre-training phase utilizing a merged or novel Polish-optimized tokenizer that demonstrates lower Tokens per Word (TpW) and generates text without the previously observed "incorrect token combinations."

### Open Question 2
What is the actual performance delta on the Open PL LLM Leaderboard when evaluating the instruction-tuned model using its intended prompt templates? The reported scores (e.g., for Bielik-7B-Instruct-v0.1) may underestimate the model's true capabilities because the evaluation treated the instruct model as a base model, potentially confusing it with raw inputs. Evidence would require a comparative benchmark evaluation showing the score difference between "raw" (template-free) evaluation and "templated" evaluation for the Bielik-7B-Instruct-v0.1 model.

### Open Question 3
To what extent do the Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate contribute to model performance compared to standard SFT methods? The paper introduces these as "innovative techniques" but compares Bielik only against external baselines rather than ablations isolating these specific training objectives. Evidence would require an ablation study training two versions of the model—one with the proposed weighted loss/adaptive LR and one with standard hyperparameters—on the same dataset to isolate the impact of the training algorithms.

## Limitations

- **Tokenizer efficiency constraints**: Retention of Mistral's 32K tokenizer results in suboptimal Polish efficiency (higher TpW, lower CpT), imposing ceiling effects on generation quality that cannot be overcome through training improvements alone.

- **Evaluation scope constraints**: Performance comparisons focus primarily on Polish benchmarks and RAG tasks, with limited analysis of general-purpose capabilities. The 9 percentage point RAG Reader improvement comes at cost of reduced general benchmark performance (29.38 vs Mistral's 30.67).

- **Data composition opacity**: While the paper details corpus size and quality filtering, specific topic diversity metrics, exact XGBoost classifier hyperparameters, and the complete instruction dataset remain unavailable. The 90% quality threshold and 266 StyloMetrix features used for classification are not fully specified.

## Confidence

**High confidence**: The technical implementation of weighted instruction loss and adaptive learning rate follows established principles with clear mathematical formulations. The core mechanism—assigning sample weights to handle mixed-quality data and scaling learning rates by batch composition—is theoretically sound and consistent with the cited literature.

**Medium confidence**: The reported performance improvements, particularly the 9 percentage point RAG Reader gain and Polish MT-Bench scores, appear robust within the Polish-specific evaluation framework. However, the generalization to broader task sets shows mixed results, and the evaluation methodology for instruction quality remains somewhat opaque.

**Low confidence**: Claims about catastrophic forgetting mitigation through English mixing are supported by observed retention of broader capabilities but lack ablation studies demonstrating what would happen with Polish-only training. The assertion that imatrix quantization consistently outperforms standard methods across all levels needs verification with diverse Polish generation tasks beyond perplexity.

## Next Checks

1. **Ablation study on English mixing**: Train an otherwise identical model using only Polish data (36B tokens, no English) to empirically validate catastrophic forgetting claims. Compare retention of general capabilities and analyze generation quality degradation across language tasks.

2. **Polish-specific quantization evaluation**: Move beyond perplexity-based quantization assessment to test imatrix-calibrated models (Q4_K_M through Q8_0) on Polish language generation quality, including grammaticality, coherence, and task completion on Polish-specific prompts not present in the calibration dataset.

3. **Quality classifier validation**: Conduct blind human evaluation on 1,000 documents from the quality-filtered corpus to verify that the 90% XGBoost probability threshold correlates with actual content quality. Test whether the weighted loss scheme (1.0/0.7/0.5) improves downstream task performance compared to uniform weighting on this validated subset.