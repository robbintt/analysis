---
ver: rpa2
title: 'On the usability of generative AI: Human generative AI'
arxiv_id: '2502.17714'
source_url: https://arxiv.org/abs/2502.17714
tags:
- user
- interac-on
- systems
- system
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies usability challenges in generative AI systems,
  particularly issues with user control, unpredictability, and difficulties in fine-tuning
  outputs through traditional prompt-based interactions. The authors propose a human-centered
  AI interface design that incorporates both voice and prompt-based interaction modes,
  with additional features like voice assistance, a control panel for output parameters
  (style, tone, length), and prompt-writing assistance using established frameworks.
---

# On the usability of generative AI: Human generative AI

## Quick Facts
- arXiv ID: 2502.17714
- Source URL: https://arxiv.org/abs/2502.17714
- Reference count: 0
- Primary result: Human-centered AI interface with voice/prompt modes and parameter controls showed 80% failure rate on complex tasks despite improved learnability

## Executive Summary
This paper presents a human-centered generative AI interface design that combines voice and prompt-based interactions with explicit parameter controls and prompt-writing assistance frameworks. The interface aims to address usability challenges in current GenAI systems by providing users with more control over outputs through style, tone, and length adjustments while maintaining familiar conversational AI elements. Usability testing with 5 participants revealed that while users quickly adapted to basic tasks (completion times dropped from 52 to 30.2 seconds), complex navigation tasks had an 80% failure rate, indicating significant discoverability issues with novel features.

## Method Summary
The study employed a Figma-based prototype tested with 5 participants (ages <18 to >55; 4 with prior gen-AI experience, 1 novice) using a 3-phase usability test (10min introduction, 15min task execution, 5min feedback). Four tasks were designed to evaluate interface navigation: changing response style, adjusting voice playback speed, regenerating responses with parameter changes, and searching for chat history deletion. Task completion times and success scores (1-3 scale) were recorded, along with qualitative Think Aloud protocol data.

## Key Results
- Task completion times decreased from 52 to 30.2 seconds across tasks, indicating learnability
- 80% failure rate on Task 4 (searching for chat history deletion) revealed significant discoverability issues
- Interface familiarity with existing AI systems helped with basic navigation but didn't transfer to novel feature discovery
- Users often confused the control panel with settings menu when adjusting parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interface familiarity with existing conversational AI systems accelerates initial task learning, but does not transfer to novel feature discovery.
- Mechanism: Users leverage recognition memory from prior chatbot/GenAI experience to locate common elements (chat history, new conversation, input box). This reduces cognitive load for basic navigation but creates false confidence when features are placed in non-standard locations.
- Core assumption: Users have prior exposure to at least one generative AI or chatbot interface.
- Evidence anchors:
  - [abstract] "maintains familiar elements from existing systems"
  - [section 5] "The similarity between the presented system and familiar generative AI systems or chatbots helped users recognize icons and functions easily. However, some features placed on the main screen to simplify the user experience caused confusion, as many users resorted to the settings to adjust parameters."
- Break condition: When users have no prior GenAI experience, familiarity benefits diminish—only 1 of 5 test participants was a complete novice, limiting generalizability.

### Mechanism 2
- Claim: Optional parameter control panels (style, tone, length, language) restore user agency without mandating expertise, partially addressing the intent-based interaction's "black box" problem.
- Mechanism: Shifting implicit prompt engineering into explicit GUI controls lets users modify outputs via recognition (selecting from menus) rather than recall (writing effective prompts). The control remains optional, so novices aren't forced to engage before they're ready.
- Core assumption: Users can map their intent to abstract categories like "tone" and "style" as defined by the system.
- Evidence anchors:
  - [abstract] "a control panel for output parameters (style, tone, length)"
  - [section 3.2.2] "The use of the control panel is absolutely optional and the interaction can be successful even without the user controlling the parameters. The presence of this possibility helps, at the same time, to maintain its sense of mastery"
- Break condition: If parameter categories don't align with user mental models (e.g., "empathetic" vs. "persuasive" tone distinctions), users may still struggle to articulate intent.

### Mechanism 3
- Claim: Scaffolding prompt writing with structured frameworks (COSTAR, CARE, Chain of Thought, Few-Shot) reduces the cognitive gap between user intent and prompt formulation.
- Mechanism: Instead of requiring users to internalize prompt engineering, the system externalizes framework steps into guided interaction. Users follow prompts step-by-step, reducing the "tip of the iceberg" problem where implicit knowledge isn't articulated.
- Core assumption: Frameworks chosen (COSTAR, CARE, CoT, Few-Shot) generalize across the task types users actually perform.
- Evidence anchors:
  - [abstract] "prompt-writing assistance using established frameworks"
  - [section 3.2.2] "Once the AI system has selected the prompting method for which to receive assistance, it will provide a brief guide for the user directly in the chat, explaining the operation of the method as a whole and then guiding him/her through the various steps required."
- Break condition: When tasks are highly novel or creative, rigid frameworks may constrain rather than assist—this was not tested in the study's limited task set.

## Foundational Learning

- Concept: **Intent-based vs. command-based interaction paradigms**
  - Why needed here: The paper's core argument is that GenAI shifted interaction from "specify how" (command-based, iterative refinement) to "specify what" (intent-based). Understanding this distinction is prerequisite to evaluating why users struggle with prompts.
  - Quick check question: Can you explain why adding more parameters to a prompt doesn't guarantee better outputs in an intent-based system?

- Concept: **Human-Machine Teaming and Hybrid Intelligence**
  - Why needed here: The interface design explicitly targets "collaboration" rather than pure automation, drawing on HCAI principles. Without this framing, features like voice assistance and memory management appear as mere conveniences rather than trust-building mechanisms.
  - Quick check question: What's the difference between "Computer in the Loop" and "Human in the Loop," and which does Hybrid Intelligence supersede?

- Concept: **Prompt engineering frameworks (COSTAR, CARE, Chain of Thought, Few-Shot)**
  - Why needed here: The control panel offers these as assistance options. Engineers need to understand what each framework optimizes for to assess whether they're appropriate for their user population.
  - Quick check question: Which framework would you recommend for a user who needs consistent output formatting across 50 similar requests?

## Architecture Onboarding

- Component map: Main conversation screen -> Chat display + input box + floating sidebar (history, projects, prompt library) -> Voice interaction module -> Microphone icon -> speech-to-text + voice playback + control panel -> Prompt control panel -> Side panel with basic output controls (style: 8 options, tone: 8 options, length, language) -> Memory management -> Conversational memory store + per-chat memory toggle -> Help/information overlay -> Searchable help triggered by question mark icon -> Project folders + prompt library -> Persistent storage for organizing conversations and sharing prompts

- Critical path:
  1. User authentication -> first-time onboarding (comprehensive guide presentation)
  2. New conversation -> warning message displayed -> memory activation choice
  3. User input (voice or text) -> parameter panel (optional) -> generation -> status indicators
  4. Post-response actions (regenerate, modify parameters, share, delete, read aloud)

- Design tradeoffs:
  - **Familiarity vs. discoverability**: Keeping standard chat interface aids recognition but buries novel features (80% failure rate on Task 4—finding history deletion search)
  - **Control panel complexity vs. simplicity**: 8 tone options + 8 style options + frameworks provide flexibility but may overwhelm; optional by default mitigates this
  - **Voice assistance intervention**: Auto-activation could frustrate expert users; the paper makes it user-controllable but doesn't test this feature

- Failure signatures:
  - **Task 4 failure pattern (80%)**: Users default to "Settings" when they can't find features; suggests help/search isn't salient enough
  - **Declining but still significant task times**: 52s -> 30.2s improvement indicates learnability but also initial friction
  - **Parameter confusion**: Users didn't naturally discover the control panel; some went to settings instead

- First 3 experiments:
  1. **A/B test control panel discoverability**: Compare current icon placement vs. contextual tooltip prompts when users struggle (measured by parameter usage rates and task completion times)
  2. **Task complexity gradient test**: Replicate Task 4 with varying information architectures (prominent search bar vs. help menu vs. guided tutorial) to isolate what specifically caused the 80% failure rate
  3. **Framework effectiveness study**: Track which prompt assistance frameworks users select, and correlate with output satisfaction ratings (the paper provides no data on whether COSTAR vs. CARE vs. CoT produces better outcomes for specific task types)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interface designs better support complex navigational tasks (e.g., finding specific settings) to reduce the high failure rates observed in control-heavy interfaces?
- Basis in paper: [explicit] The authors report an "80% failure rate on task 4" where users failed to search for how to delete chat history, highlighting a specific bottleneck in the proposed design.
- Why unresolved: The study only identified the failure point but did not test alternative navigational paradigms to resolve the lack of intuitiveness in finding specific functions.
- What evidence would resolve it: Results from a comparative study testing alternative layouts for search and help functions, showing a success rate significantly higher than 20% for similar tasks.

### Open Question 2
- Question: To what extent does separating "control panels" from standard "settings" confuse users when attempting to modify output parameters?
- Basis in paper: [explicit] The authors note that "many users resorted to the settings to adjust parameters," bypassing the dedicated control panel intended to simplify the experience.
- Why unresolved: It is unclear if the redundancy of control locations (control panel vs. settings) hinders or helps the user experience, as the test showed user confusion despite the intent to improve mastery.
- What evidence would resolve it: A/B testing a unified interface versus the proposed split interface, measuring the time taken and error rates for parameter adjustment tasks.

### Open Question 3
- Question: Does the integration of structured prompt-writing assistance (e.g., COSTAR, CARE frameworks) effectively lower the cognitive load for non-expert users compared to standard free-form prompting?
- Basis in paper: [inferred] While the paper proposes a "prompt-writing assistance" feature to address the difficulty of "translating intentions into prompts," the usability test focused on system navigation (Tasks 1-4) rather than measuring the quality or ease of the actual content generation process.
- Why unresolved: The study evaluated the interface's navigability but did not validate whether the specific prompt assistance features improved the user's ability to express intent or the quality of the AI output.
- What evidence would resolve it: A study measuring task load (e.g., NASA-TLX) and output satisfaction scores between users utilizing the assistance frameworks versus those using manual prompting.

### Open Question 4
- Question: How does the proposed human-centered interface impact the learnability and error rates of novice users compared to the experienced demographic tested?
- Basis in paper: [inferred] The study tested only 5 participants, 4 of whom were aged 18-24 and had prior experience with generative AI, leaving the efficacy of the design for the "non-expert users" mentioned in the introduction unproven.
- Why unresolved: The results (e.g., decreasing task times) may reflect the prior experience of the sample rather than the intuitiveness of the interface itself for a truly novice population.
- What evidence would resolve it: A longitudinal study with participants who have never used generative AI, analyzing their learning curves and failure rates on the same set of tasks.

## Limitations
- Sample size of 5 participants severely limits generalizability and statistical significance
- Only 1 complete novice participant provides insufficient data on novice user experience
- No validation of whether parameter controls actually improve output quality compared to traditional prompting
- Prompt assistance frameworks were not evaluated for their specific effectiveness

## Confidence

**Low** - The usability testing sample size (n=5) severely limits generalizability. The study reports aggregate task completion times and qualitative feedback but lacks statistical significance testing or analysis of individual differences. The single complete novice participant (out of 5) provides insufficient data on how users without prior GenAI experience navigate the interface.

**Medium** - The paper doesn't validate whether the parameter control panel actually improves output quality or user satisfaction compared to traditional prompting. While the mechanism is theoretically sound (recognition vs. recall), no empirical evidence demonstrates that users achieve better results using style/tone controls versus well-crafted prompts.

**Medium** - The prompt assistance frameworks (COSTAR, CARE, Chain of Thought, Few-Shot) are presented as features but not evaluated for their specific effectiveness. The study doesn't track which frameworks users select, correlate framework choice with task success, or measure whether framework guidance actually improves novice users' prompt engineering capabilities.

## Next Checks

1. **A/B Test Control Panel Discoverability**: Replicate the study with two interface variants - one with the current control panel placement and one with contextual tooltip prompts triggered when users struggle with parameter adjustments. Measure parameter usage rates, task completion times, and navigation patterns to determine if the 80% Task 4 failure rate stems from discoverability issues.

2. **Framework Effectiveness Study**: Track user selections among the four prompt assistance frameworks across different task types. Correlate framework choice with output satisfaction ratings and task completion success scores to determine which frameworks work best for specific use cases (creative writing vs. factual queries vs. formatting tasks).

3. **Mixed-Experience Cohort Test**: Recruit a larger sample (n=20-30) with balanced representation of users with zero, minimal, and extensive GenAI experience. Compare task completion times, failure rates, and qualitative feedback across experience levels to determine whether the interface's familiarity advantage actually benefits novice users or primarily serves those with existing GenAI exposure.