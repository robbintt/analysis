---
ver: rpa2
title: Learning Stabilizing Policies via an Unstable Subspace Representation
arxiv_id: '2505.01348'
source_url: https://arxiv.org/abs/2505.01348
tags:
- unstable
- subspace
- system
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-phase approach to learn stabilizing controllers
  for linear systems by focusing on the unstable subspace rather than the full state
  space. It first estimates the left unstable subspace using data from the adjoint
  system, then applies discounted LQR with policy gradient updates on this reduced
  space.
---

# Learning Stabilizing Policies via an Unstable Subspace Representation

## Quick Facts
- arXiv ID: 2505.01348
- Source URL: https://arxiv.org/abs/2505.01348
- Reference count: 40
- Primary result: Sample complexity reduced from $\tilde{O}(d_X^2 d_U)$ to $\tilde{O}(\ell^2 d_U)$ when $\ell \ll d_X$

## Executive Summary
This paper proposes a two-phase method to learn stabilizing controllers for linear systems by focusing on the unstable subspace rather than the full state space. The approach first estimates the left unstable subspace using data from the adjoint system, then applies discounted LQR with policy gradient updates on this reduced space. Theoretical guarantees are provided for both subspace estimation and controller stabilization, including finite-sample bounds and non-asymptotic convergence rates. The method is particularly effective when the number of unstable modes is much smaller than the state dimension, offering significant sample complexity improvements over standard approaches.

## Method Summary
The method operates in two phases: Phase 1 estimates the left unstable subspace $\hat{\Phi} \in \mathbb{R}^{d_X \times \ell}$ by collecting data from the adjoint system $x_{t+1} = A^\top x_t$ and performing SVD to extract the top $\ell$ singular vectors. Phase 2 applies discounted LQR policy gradient optimization on the reduced $z_t = \hat{\Phi}^\top x_t$ space, iteratively increasing the discount factor $\gamma$ from an initial value below 1 until convergence to a stabilizing controller. The final controller is lifted back to full state space as $K = \theta \hat{\Phi}^\top$. The approach decouples the hard problem of system identification from controller optimization, allowing sample-efficient stabilization when $\ell \ll d_X$.

## Key Results
- Theoretical sample complexity reduced from $\tilde{O}(d_X^2 d_U)$ to $\tilde{O}(\ell^2 d_U)$ for systems with small $\ell$
- Finite-sample bounds for both subspace estimation error and controller stabilization
- Numerical experiments on cartpole and inverted pendulum systems demonstrate the claimed sample complexity reduction
- Guarantees hold for non-diagonalizable systems using Jordan decompositions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Estimating the left unstable subspace via the adjoint system separates unstable dynamics from stable ones without inducing a coupling bias that would otherwise inflate sample complexity.
- **Mechanism**: The method collects data from the adjoint system $x_{t+1} = A^\top x_t$. Since the eigenvectors of $A^\top$ (left eigenvectors of $A$) span the left unstable subspace, SVD on this data matrix recovers the basis $\Phi$. Unlike right-subspace approaches, this parameterization yields a block-triangular closed-loop structure, ensuring that stabilizing the reduced unstable dynamics ($A_u$) is sufficient to stabilize the full system.
- **Core assumption**: The number of unstable modes $\ell$ is small relative to state dimension $d_X$, and the least stable stable mode $|\lambda_{\ell+1}|$ is strictly less than 1.
- **Evidence anchors**:
  - [abstract] "estimates the left unstable subspace using data from the adjoint system"
  - [section 2.2] "operating on the left unstable subspace allows for controlling the closed-loop spectral radius... contrasts with prior work... which recovers a basis of the right unstable subspace... [which] incurs a bias"
- **Break condition**: If the gap between unstable and stable modes vanishes ($|\lambda_\ell| \to 1$), the singular value separation in the data matrix fails, making subspace recovery inconsistent (Appendix G, Lemma G.1).

### Mechanism 2
- **Claim**: A two-phase approach decouples the hard problem of system identification from the optimization of the controller, allowing sample-efficient stabilization.
- **Mechanism**: Instead of identifying the full $A$ matrix (scaling with $d_X^2$), Phase 1 identifies only the $\ell$-dimensional subspace basis $\hat{\Phi}$ (scaling with $\ell, d_X$). Phase 2 then restricts the policy gradient search to a low-dimensional controller $\theta \in \mathbb{R}^{d_U \times \ell}$. This reduces the effective parameter space from $d_U d_X$ to $d_U \ell$.
- **Core assumption**: The subspace estimation error $d(\hat{\Phi}, \Phi)$ is sufficiently small such that the gradient error bounds (Lemma 4.1) hold during policy optimization.
- **Evidence anchors**:
  - [abstract] "reduces sample complexity from $\tilde{O}(d_X^2 d_U)$ to $\tilde{O}(\ell^2 d_U)$"
  - [section 5] "Algorithm 1 returns a stabilizing controller... within $S_c = \text{log}(\rho(A))\tilde{O}(\ell^2 d_U)$ trajectories"
- **Break condition**: If the unstable subspace is estimated poorly, the projected gradient $\hat{\nabla}J(\theta, \hat{\Phi})$ deviates excessively from the true gradient, preventing convergence.

### Mechanism 3
- **Claim**: A discount factor annealing scheme on the reduced subspace allows a trivial initial policy to converge to a globally stabilizing controller without requiring prior system knowledge.
- **Mechanism**: The algorithm initializes with a small discount factor $\gamma_0 < 1/\bar{\lambda}_1^2$ where the zero policy is stabilizing for the "damped" system. It iteratively solves the discounted LQR via PG while increasing $\gamma$ towards 1. Because the optimization occurs on the low-dimensional unstable subspace, the annealing process requires fewer samples to track the optimal policy compared to full-state methods.
- **Core assumption**: An upper bound $\bar{\lambda}_1$ on the system's spectral radius is known to initialize $\gamma_0$.
- **Evidence anchors**:
  - [section 4.2] "initializes the discount factor with $\gamma_0 < 1/\bar{\lambda}_1^2$... ensures that the initial controller $\theta_0 \equiv 0$ stabilizes"
- **Break condition**: If the step size $\eta$ or update rate $\xi$ is too aggressive, the controller may exit the region of attraction ("stabilizing set") for the current discount factor before convergence.

## Foundational Learning

- **Concept: Adjoint Systems and Controllability**
  - **Why needed here**: The method relies on sampling the adjoint system $x_{t+1} = A^\top x_t$ to recover the *left* unstable subspace.
  - **Quick check question**: How does the spectrum of the adjoint system $A^\top$ relate to the original system $A$, and how does this affect the "stability" of the adjoint trajectories?

- **Concept: Spectral Radius and Jordan Forms**
  - **Why needed here**: The theoretical guarantees accommodate non-diagonalizable systems by leveraging Jordan decompositions to bound subspace distance errors.
  - **Quick check question**: If a matrix is non-diagonalizable, does the power iteration $A^t$ still allow separating unstable modes from stable ones, and how does the Jordan block structure affect the convergence rate?

- **Concept: Zeroth-Order Optimization**
  - **Why needed here**: Policy gradients are estimated using finite differences (two-point estimation) on trajectory rollouts because the model is unknown.
  - **Quick check question**: Why does the smoothing radius $r$ and the number of rollouts $n_s$ need to scale specifically to bound the gradient estimation variance in LQR?

## Architecture Onboarding

- **Component map**: Adjoint Sampler -> Subspace Estimator -> Reduced PG Loop -> Discount Scheduler -> Lifter
- **Critical path**: The accuracy of the Subspace Estimator ($\hat{\Phi}$). If the singular value gap ($\sigma_\ell - \hat{\sigma}_{\ell+1}$) is small, the subspace distance $d(\hat{\Phi}, \Phi)$ remains high, causing the subsequent PG updates to optimize a distorted objective.
- **Design tradeoffs**:
  - **Phase 1 vs Phase 2 Data**: Spending more samples $T$ in Phase 1 reduces subspace error, allowing fewer PG iterations in Phase 2. Under-sampling in Phase 1 may cause Phase 2 to fail entirely.
  - **Dimensionality**: The method is efficient only if $\ell \ll d_X$. If the system is fully unstable ($\ell \approx d_X$), the complexity reverts to standard PG methods plus the overhead of subspace estimation.
- **Failure signatures**:
  - **Subspace Collapse**: SVD yields rank $< \ell$ (not enough separation between stable/unstable dynamics in data).
  - **Gradient Explosion**: $\|\nabla J\|$ grows during PG updates (indicates $d(\hat{\Phi}, \Phi)$ is too large or $\gamma$ increased too fast).
  - **Non-convergence**: Discount factor $\gamma$ stalls or oscillates below 1.0.
- **First 3 experiments**:
  1. **Synthetic Validation**: Generate a random $d_X=30$ system with $\ell=2$ unstable modes. Plot subspace distance $d(\hat{\Phi}, \Phi)$ vs. sample count $T$ to verify the $\mathcal{O}(\text{log}(d_X)/\text{log}|\lambda_\ell|)$ scaling.
  2. **Ablation on "Left" vs "Right"**: Implement the algorithm using the *right* unstable subspace (via direct system simulation) and compare the sample complexity against the *left* subspace method on a non-symmetric $A$ matrix to observe the coupling bias mentioned in Remark 2.1.
  3. **Cartpole Stabilization**: Deploy Algorithm 1 on a linearized cartpole ($d_X=4$, $\ell=2$) to verify that the closed-loop spectral radius $\rho(A+BK)$ drops below 1.0 as $\gamma \to 1$.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the learning to stabilize (LTS) framework be extended to handle multiple dynamical systems that share "similar" unstable subspaces?
  - Basis in paper: [explicit] Page 15, Section 7: "Future work includes studying the LTS problem for multiple systems with 'similar' unstable subspaces..."
  - Why unresolved: The current theoretical analysis is restricted to a single system and does not quantify the benefits of representation transfer across different but related dynamical environments.
  - What evidence would resolve it: Derivation of finite-sample bounds showing sample complexity reduction when sharing the estimated representation $\hat{\Phi}$ across a set of related systems.

- **Open Question 2**: Can the unstable subspace representation be learned and updated online as new data arrives, rather than in a distinct offline phase?
  - Basis in paper: [explicit] Page 15, Section 7: "Future work includes... learning the representation online where we continuously update the learned unstable subspace as more data becomes available."
  - Why unresolved: The current "two-phase" approach separates estimation and control; online updating introduces non-stationarity in the policy gradient landscape that current convergence proofs do not address.
  - What evidence would resolve it: A unified algorithm and convergence proof demonstrating that simultaneous updates to the subspace estimate and controller maintain stability guarantees.

- **Open Question 3**: How robust is the algorithm to misspecification of the initialization parameter $\bar{\lambda}_1$ (the upper bound on the system's largest eigenvalue)?
  - Basis in paper: [inferred] Page 11, Section 4.2: "We assume access to an upper bound on the largest eigenvalue... This assumption is necessary to initialize the discount factor."
  - Why unresolved: The algorithm initializes $\gamma_0$ based on $\bar{\lambda}_1$ to ensure the zero controller is stabilizing. If this bound is inaccurate, the initial stability assumption fails, and the paper does not analyze recovery from such an initialization error.
  - What evidence would resolve it: A sensitivity analysis or modified algorithm that adapts $\gamma_0$ dynamically without prior knowledge of $\bar{\lambda}_1$.

## Limitations

- The effectiveness critically depends on a spectral gap between unstable and stable modes; if $|\lambda_\ell| \to 1$, the subspace estimation error bounds become vacuous and the method may fail.
- The theoretical sample complexity gains rely on $\ell \ll d_X$; for systems with many unstable modes, the complexity reverts to standard PG methods plus subspace estimation overhead.
- The assumption that an upper bound $\bar{\lambda}_1$ on the spectral radius is known to initialize $\gamma_0$ is practical but limits full model-free operation.

## Confidence

- **High Confidence**: The core mechanism of using the adjoint system to recover the left unstable subspace is well-founded and theoretically justified.
- **Medium Confidence**: The two-phase approach effectively reduces sample complexity for systems with a clear spectral gap and small $\ell$.
- **Low Confidence**: The discount annealing scheme's practical implementation is sensitive to hyperparameters ($\xi$, $\alpha_j$) and may require careful tuning to avoid instability during training.

## Next Checks

1. **Synthetic Spectral Gap Test**: Generate a $d_X=30$ system with $\ell=2$ unstable modes where $|\lambda_\ell| = 0.99$ (small gap). Compare the subspace estimation error and controller convergence to a system with $|\lambda_\ell| = 0.9$ (large gap) to quantify the impact of spectral separation.

2. **Ablation on Subspace Choice**: Implement the algorithm using the right unstable subspace (via direct system simulation) and compare sample complexity against the left subspace method on a non-symmetric $A$ matrix to observe the coupling bias mentioned in Remark 2.1.

3. **Discount Factor Sensitivity**: Run Algorithm 1 on Cartpole with varying $\xi$ (0.1 to 0.9) and $\alpha_j$ schedules. Plot $\gamma_j$ progression and closed-loop spectral radius $\rho(A+BK)$ to identify parameter ranges that cause annealing failure or non-convergence.