---
ver: rpa2
title: Detecting Religious Language in Climate Discourse
arxiv_id: '2510.23395'
source_url: https://arxiv.org/abs/2510.23395
tags:
- religious
- language
- llms
- llama
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the detection of religious language in
  climate discourse using both rule-based and machine learning methods. The rule-based
  approach employs a hierarchical tree of religious terms from ecotheology literature,
  while LLMs (GPT-4o mini and Llama 3.3 70B) operate in a zero-shot setting.
---

# Detecting Religious Language in Climate Discourse

## Quick Facts
- arXiv ID: 2510.23395
- Source URL: https://arxiv.org/abs/2510.23395
- Reference count: 9
- Primary result: Rule-based method consistently labels more sentences as religious than LLMs when detecting religious language in climate discourse

## Executive Summary
This study investigates the detection of religious language in climate discourse using both rule-based and machine learning methods. The rule-based approach employs a hierarchical tree of religious terms from ecotheology literature, while LLMs (GPT-4o mini and Llama 3.3 70B) operate in a zero-shot setting. Analyzing over 880,000 sentences from nine environmental NGOs, the rule-based method consistently labeled more sentences as religious than LLMs. The study reveals methodological challenges in computational detection of religious language, particularly regarding implicit forms and the tension between vocabulary-based versus contextual definitions. Results show LLMs are sensitive to prompt design and often miss implicit religious expressions, reflecting broader cultural-linguistic patterns about what constitutes religion.

## Method Summary
The study combines a rule-based approach using a hierarchical tree of religious terms from ecotheology literature with zero-shot LLM classification. The rule-based method matches inflected forms of religious vocabulary organized by tradition (Christianity, Islam, Judaism, Hinduism, Buddhism, indigenous cosmovisions, nature spiritualities) with a "general" root for cross-traditional terms. LLMs process individual sentences with engineered prompts specifying that descriptive references should not count as religious language, while genuine religious ideas or meanings should. The analysis covers 880,000+ sentences from nine environmental NGOs, comparing agreement rates and analyzing disagreement cases to understand boundary detection between religious and non-religious language.

## Key Results
- Rule-based method consistently labeled more sentences as religious than LLMs (vocabulary-based vs. contextual reasoning)
- LLMs show inconsistent classification of identical sentences across batch runs (GPT: 33.8% inconsistency on "Mother Earth", Llama: 7/12 instances on "Mother Earth's heart beating")
- LLM results are highly sensitive to prompt design and often miss implicit religious expressions lacking explicit terminology

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary-Based Detection via Hierarchical Tree
- Claim: A manually curated hierarchical tree of religious terms detects religious language more consistently than contextual LLM classification.
- Mechanism: Domain experts derived religious concepts from ecotheology handbooks (Jenkins et al., 2016; Conradie & Koster 2019), organizing them by tradition with a "general" root for cross-traditional terms. The tree matches word forms including plurals and inflections.
- Core assumption: Religious language can be identified through the presence of specific vocabulary items, regardless of contextual usage.
- Evidence anchors: [abstract] "rule-based method consistently labeled more sentences as religious than LLMs"; [section 2.2.1] Describes tree construction from ecotheology literature with manual refinement based on false positive analysis.
- Break condition: The tree cannot detect implicit religious language lacking explicit religious vocabulary (e.g., biblical allusions without keywords), and produces false positives when religious terms appear in purely descriptive contexts (e.g., "Christian organizations").

### Mechanism 2: Contextual Reasoning via Zero-Shot LLM Classification
- Claim: LLMs can classify religious language through prompt-engineered instructions that leverage learned semantic associations, but exhibit inconsistent application of classification criteria.
- Mechanism: GPT-4o mini and Llama 3.3 70B process individual sentences with a prompt specifying that descriptive references should not count as religious language, while genuine religious ideas or meanings should. Models generate JSON outputs with binary classifications, confidence scores, and free-text argumentation.
- Core assumption: LLMs' training on diverse corpora provides sufficient cultural-linguistic knowledge to recognize religious language patterns without task-specific fine-tuning.
- Evidence anchors: [section 2.2.2] Details prompt engineering process, including initial failures to recognize "sacred earth" and "Mother Earth" as religious; [section 3.3] Shows both models use similar argumentation types but apply them inconsistently.
- Break condition: LLMs produce inconsistent classifications for identical sentences (e.g., the "Mother Earth's heart beating" sentence labeled religious in 2/12 instances by GPT, 7/12 by Llama), and Llama generates 26.6% incorrectly formatted responses in disagreement cases vs. 3.3% for GPT.

### Mechanism 3: Comparative Disagreement Analysis for Boundary Detection
- Claim: Analyzing where detection methods disagree reveals the implicit/explicit religion boundary and exposes models' embedded cultural assumptions about what constitutes religion.
- Mechanism: Three-way comparison (tree vs. GPT vs. Llama) identifies cases where vocabulary-based and contextual approaches diverge. Close reading of disagreement cases, particularly around terms like "sacrifice," "Mother Earth," and "sacred earth," illuminates how different detection logics operationalize religious language.
- Core assumption: Disagreement cases are more informative than agreement cases for understanding conceptual boundaries.
- Evidence anchors: [section 3.2-3.4] Systematic analysis of disagreements reveals LLMs often treat religious terms as descriptive, miss implicit references, and exhibit bias toward institutional/Western religious concepts; [section 4.3] Argues that zero-shot LLM behavior reflects broader cultural-linguistic patterns that privilege explicit over implicit religion.
- Break condition: This mechanism is qualitative and exploratory; findings cannot be generalized beyond the analyzed subset without further validation.

## Foundational Learning

- Concept: Zero-shot classification
  - Why needed here: The study relies on LLMs performing religious language detection without task-specific training data, making understanding zero-shot capabilities and limitations essential.
  - Quick check question: Can you explain why zero-shot performance might differ from fine-tuned performance on ambiguous classification tasks?

- Concept: Implicit vs. explicit religion (Bailey's framework)
  - Why needed here: The paper explicitly engages with debates about whether religious language requires explicit theological terminology or can emerge through implicit structures of meaning, commitment, and transcendence.
  - Quick check question: How would Bailey's concept of "implicit religion" classify environmental activism that uses apocalyptic rhetoric without referencing divine agents?

- Concept: Prompt engineering for classification tasks
  - Why needed here: The study's LLM results depend heavily on prompt design, including explicit instructions to exclude descriptive references and inclusion of example sentences.
  - Quick check question: What happens to LLM classification behavior when you add negative examples (what NOT to classify) to a prompt?

## Architecture Onboarding

- Component map: Data layer (NGO sentences) -> Rule-based detector (hierarchical tree) -> LLM classifiers (GPT-4o mini, Llama 3.3 70B) -> Analysis layer (agreement statistics, disagreement categorization, close reading)
- Critical path: 1. Data collection → cleaning → sentence tokenization; 2. Tree-based labeling (vocabulary matching with parent path tracking); 3. LLM batch inference with JSON response format; 4. Agreement calculation and disagreement subset extraction; 5. Qualitative analysis of argumentation patterns
- Design tradeoffs: Sentence-level (vs. document-level) analysis enables granular detection but loses broader discourse context; Zero-shot (vs. fine-tuned) approach enables rapid experimentation but increases sensitivity to prompt wording; Binary classification (vs. multi-category) simplifies analysis but collapses nuance in implicit cases
- Failure signatures: Llama produces 26.6% malformed JSON outputs on short/ambiguous sentences (vs. 3.3% for GPT); Both LLMs classify identical sentences inconsistently across batch runs; GPT systematically under-detects indigenous/spiritual terminology ("Mother Earth" 33.8%, "sacred earth" 11.5%); Tree produces false positives on descriptive religious references (e.g., "Christian organizations")
- First 3 experiments: 1. Replicate on a held-out subset with inter-annotator agreement measurement to establish human baseline for comparison; 2. Fine-tune a smaller model (e.g., DeBERTa) on human-labeled religious language to measure gap between zero-shot and supervised approaches; 3. Test prompt variations explicitly including indigenous spirituality examples to assess whether GPT's under-detection of "Mother Earth"/"sacred earth" can be corrected through prompt design alone

## Open Questions the Paper Calls Out

- How do LLMs compare in detecting language associated with spirituality or animism versus language tied to institutionalized "world religions"? [explicit] The authors state: "Another direction would be to examine more thoroughly how LLMs respond to language associated with spirituality or animism in contrast to language tied to so-called 'world religions', to assess whether particular types of religious or spiritual language are more easily recognized." Why unresolved: This study found that terms like "Mother Earth" and "sacred earth" were inconsistently labeled, suggesting LLMs may privilege explicit, institutional religious markers over implicit or indigenous spiritual expressions.

- To what extent does prompt design influence LLM classifications of religious language, and can prompt engineering reduce inconsistencies? [explicit] The authors note: "different phrasings could lead to substantially different outputs" and that "LLMs are sensitive to prompt design." Why unresolved: The study used one engineered prompt after iterative refinement, but did not systematically test prompt variations; duplicate sentences received inconsistent labels even with identical prompts.

- Would providing broader document-level context improve LLM detection of implicit religious language beyond single-sentence analysis? [inferred] The authors acknowledge that "the broader text or the context in which the sentence was originally uttered cannot be considered" and that "LLMs can only take limited context into account, as they are only fed one sentence at a time." Why unresolved: LLMs occasionally indicated needing more context in their argumentation, but the study design precluded testing whether multi-sentence or document-level context would improve accuracy.

## Limitations
- Hierarchical tree term coverage and false-positive filtering remain underspecified; only an excerpt is shown and the full list of removed terms is not documented
- LLM classification shows high sensitivity to prompt wording and produces inconsistent labels for identical sentences, with GPT systematically under-detecting indigenous/spiritual terminology
- The analysis is exploratory and qualitative; findings about disagreement patterns cannot be generalized beyond the analyzed subset without further validation

## Confidence
- **High confidence**: Rule-based tree detects more religious sentences than LLMs due to vocabulary-based matching vs. contextual reasoning; LLMs miss implicit religious language lacking explicit terminology
- **Medium confidence**: LLM results are highly sensitive to prompt design and embedded cultural-linguistic assumptions about what constitutes religion; both models show inconsistent application of classification criteria
- **Low confidence**: Disagreement analysis reveals implicit/explicit religion boundary; findings about broader cultural-linguistic patterns cannot be generalized without replication

## Next Checks
1. Replicate the study on a held-out subset with inter-annotator agreement measurement to establish a human baseline for comparison
2. Fine-tune a smaller model (e.g., DeBERTa) on human-labeled religious language to quantify the gap between zero-shot and supervised approaches
3. Test prompt variations explicitly including indigenous spirituality examples to assess whether GPT's under-detection of "Mother Earth"/"sacred earth" can be corrected through prompt engineering alone