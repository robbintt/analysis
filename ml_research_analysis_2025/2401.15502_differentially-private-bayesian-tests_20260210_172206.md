---
ver: rpa2
title: Differentially private Bayesian tests
arxiv_id: '2401.15502'
source_url: https://arxiv.org/abs/2401.15502
tags:
- bayes
- private
- test
- factor
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel differentially private Bayesian hypothesis
  testing framework that maintains the interpretability of Bayesian inference while
  ensuring privacy guarantees. The method extends Johnson's Bayes factors based on
  test statistics to a privacy-preserving setting by using a principled data-generative
  mechanism with carefully specified priors.
---

# Differentially private Bayesian tests

## Quick Facts
- arXiv ID: 2401.15502
- Source URL: https://arxiv.org/abs/2401.15502
- Authors: Abhisek Chakraborty; Saptati Datta
- Reference count: 9
- This paper introduces a novel differentially private Bayesian hypothesis testing framework that maintains the interpretability of Bayesian inference while ensuring privacy guarantees.

## Executive Summary
This paper presents a framework for differentially private Bayesian hypothesis testing that preserves the interpretability of Bayesian inference while ensuring ε-differential privacy. The method extends Johnson's Bayes factors based on test statistics to a privacy-preserving setting using a principled data-generative mechanism with carefully specified priors. The approach employs a subsample-and-aggregate method combined with Laplace mechanism noise addition, achieving privacy guarantees while maintaining reasonable statistical power. Experiments demonstrate that the method performs well across various sample sizes and effect magnitudes, with power approaching non-private methods as sample size increases.

## Method Summary
The framework implements differentially private Bayesian hypothesis testing by first partitioning the data into M_n subsamples, computing partition-specific Bayes factors for common test statistics (z, t, χ², F), then aggregating these via average log-Bayes factors with Laplace noise addition. The method uses a mixture prior on the non-centrality parameter (δ₀ and normal-moment prior J(τ²)) and employs a subsample-and-aggregate approach to achieve ε-differential privacy. Hyperparameter tuning is performed through Monte Carlo simulation to maximize statistical power while maintaining Type I error control at α=0.05. The authors prove asymptotic consistency of the privatized Bayes factors under the true model.

## Key Results
- The privatized Bayes factors maintain asymptotic consistency under the true model while achieving ε-differential privacy
- Type I error is controlled at α=0.05 across different privacy budgets (ε∈{1, 1.5, 2}) and sample sizes
- Statistical power approaches that of non-private methods as sample size increases, with reasonable power retention even at modest sample sizes
- The framework provides a principled alternative to existing differentially private frequentist tests

## Why This Works (Mechanism)
The method achieves differential privacy by combining the subsample-and-aggregate approach with Laplace mechanism noise addition. By partitioning data into M_n subsamples and computing partition-specific Bayes factors, the sensitivity of the log-Bayes factor is bounded by 2a_n/M_n, where a_n = k·n^β with 0<β<1. The Laplace mechanism then adds calibrated noise to achieve ε-differential privacy. The carefully specified mixture priors (δ₀ and J(τ²)) ensure that the Bayes factors have bounded sensitivity while maintaining statistical power. The subsampling approach inherently limits the influence of any single individual on the final test statistic.

## Foundational Learning
- **Differential Privacy (ε-DP)**: A framework ensuring that the output distribution of an algorithm doesn't change significantly when any single individual's data is added or removed. Why needed: Provides the formal privacy guarantee that individual contributions cannot be easily inferred from the output.
- **Bayes Factors**: Ratio of marginal likelihoods under competing hypotheses, providing a measure of evidence for one hypothesis over another. Why needed: Enables interpretable Bayesian hypothesis testing while extending Johnson's test-statistic-based approach to a private setting.
- **Subsample-and-Aggregate**: A technique that partitions data, applies analysis to each subsample, then aggregates results to limit sensitivity. Why needed: Reduces the influence of any single data point on the final statistic, enabling privacy preservation.
- **Laplace Mechanism**: Adds noise drawn from Laplace distribution to achieve differential privacy, with noise scale calibrated to the sensitivity of the query. Why needed: Provides the formal mechanism to achieve ε-DP by adding calibrated noise to the aggregated statistic.
- **Mixture Priors**: Combination of point mass at null value and continuous distribution (J(τ²)) for non-centrality parameter. Why needed: Enables bounded sensitivity while maintaining statistical power through the use of non-local priors.
- **Hypergeometric 2F1 Function**: Special function appearing in the Bayes factor formula for certain test statistics. Why needed: Required for exact computation of Bayes factors based on test statistics.

## Architecture Onboarding
- **Component Map**: Data -> Partition into M_n subsamples -> Compute partition-specific BFs -> Aggregate via average log BF -> Add Laplace noise -> Decision via cutoff
- **Critical Path**: The privacy guarantee depends critically on correct implementation of the Laplace mechanism with scale 2a_n/(ε M_n) and proper calibration of hyperparameters through Algorithm 1
- **Design Tradeoffs**: Higher M_n reduces sensitivity but increases computational cost; larger a_n improves privacy but may reduce power; mixture prior specification balances sensitivity bounds with statistical performance
- **Failure Signatures**: Type I error inflation indicates incorrect Laplace scale or a_n/ω_n specification; low power suggests hyperparameter mis-tuning or prior misalignment; numerical instability in 2F1 indicates computational issues
- **First Experiments**: 1) Implement partition-specific BF calculation for t-test and verify against Lemma 2.1 bounds; 2) Test privatized average log BF implementation with known Laplace noise scale; 3) Run Algorithm 1 to compute size-α cutoff and verify empirical size equals α under H₀

## Open Questions the Paper Calls Out
- Can finite-sample theoretical guarantees (e.g., Type I/II error bounds, power rates) be established for the proposed private Bayesian tests under fixed n? Current theory relies on asymptotic consistency and vanishing-sensitivity regimes; finite-sample error control remains unquantified.
- Can data-driven, privacy-preserving hyperparameter selection for M_n be developed without exceeding the privacy budget? Currently, M_n is chosen via pre-specified grids that don't consume privacy budget; a data-dependent selection would require additional privacy accounting.
- How robust is the method when the standardized effect size π★ is misspecified or unknown? The procedure requires specifying π★ a priori to set prior modes and cutoffs; real analyses often lack reliable effect-size knowledge.

## Limitations
- The framework relies on asymptotic consistency proofs but lacks finite-sample guarantees for privatized Bayes factors
- Hyperparameter tuning requires extensive Monte Carlo simulation under H₀, which may be computationally prohibitive for large-scale applications
- The method assumes test statistics follow their theoretical distributions exactly, which may not hold in finite samples or with complex data structures

## Confidence
- **High confidence**: The theoretical framework for achieving ε-differential privacy through subsample-and-aggregate and Laplace mechanism is sound and well-established
- **Medium confidence**: The consistency proofs and Type I error control claims are valid asymptotically but their finite-sample behavior requires further validation
- **Medium confidence**: The empirical results showing reasonable power retention are based on simulations that follow the paper's methodology closely, though exact hyperparameter choices were not fully specified

## Next Checks
1. Implement Algorithm 1 to compute size-α cutoffs and verify empirical Type I error matches α=0.05 across different ε and n values
2. Test the privatized BF implementation on real-world datasets (e.g., DAIC-WOZ) to assess practical performance beyond simulations
3. Evaluate the computational cost of hyperparameter tuning for various sample sizes to assess scalability for larger datasets