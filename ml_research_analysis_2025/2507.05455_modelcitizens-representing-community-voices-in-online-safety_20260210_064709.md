---
ver: rpa2
title: 'ModelCitizens: Representing Community Voices in Online Safety'
arxiv_id: '2507.05455'
source_url: https://arxiv.org/abs/2507.05455
tags:
- toxicity
- context
- citizens
- annotators
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MODEL CITIZENS, a dataset of 6.8K social
  media posts and 40K toxicity annotations across eight identity groups. It captures
  community perspectives on toxicity by collecting annotations from both ingroup and
  outgroup annotators, and augments posts with LLM-generated conversational contexts
  to model real-world social media discourse.
---

# ModelCitizens: Representing Community Voices in Online Safety

## Quick Facts
- arXiv ID: 2507.05455
- Source URL: https://arxiv.org/abs/2507.05455
- Reference count: 34
- 6.8K social media posts with 40K toxicity annotations across 8 identity groups; ingroup annotators outperform aggregated labels by 5.5% accuracy

## Executive Summary
MODEL CITIZENS introduces a dataset of 6.8K social media posts annotated by both ingroup and outgroup annotators across eight identity groups, revealing significant disagreements in toxicity perceptions. The dataset includes LLM-generated conversational contexts that shift toxicity judgments for posts across all groups. To address performance gaps, the authors fine-tune LLaMA-3.1-8B and Gemma-3-12B on ingroup-annotated data, creating models that outperform GPT-4o-mini by 5.5% on in-distribution evaluations while showing promising generalization to out-of-distribution datasets.

## Method Summary
The dataset was created by sampling posts from Toxigen, generating conversational contexts using GPT-4o with human validation (86% plausibility), and collecting annotations from ingroup and outgroup annotators on Prolific. Models were fine-tuned using LLaMA-Factory with specific hyperparameters (LR 1e-5, 10 epochs) on the formatted data using a structured prompt template. Evaluation was conducted on a held-out test set and out-of-distribution datasets using accuracy and F1 metrics.

## Key Results
- Existing toxicity detection models achieve only 63.6% accuracy on ingroup-labeled MODELCITIZENS test data
- Outgroup annotators label content as more toxic than ingroup annotators, with amplified harm rates reaching 17.5% for Black content
- LLAMACITIZEN-8B and GEMMACITIZEN-12B models outperform GPT-4o-mini by 5.5% on in-distribution evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ingroup annotators provide more reliable toxicity signals than outgroup annotators or aggregated labels
- Mechanism: Individuals who self-identify with the target group possess lived experience that informs contextual judgments about reclaimed language, intragroup humor, and genuine harm—distinctions lost when annotations are aggregated or sourced from non-members.
- Core assumption: Ingroup annotator labels better reflect the actual harm experienced by target communities (unproven causal claim; assumes self-identification correlates with accurate harm perception).
- Evidence anchors:
  - [abstract] "Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language."
  - [section 5.3, Table 7] Models trained on ingroup labels outperformed those trained on outgroup (72.3%) and aggregated labels (74.9%), with ingroup achieving 75.2% accuracy.
  - [corpus] Related work (Goyal et al., 2022; Sap et al., 2022) documents similar annotator identity effects, but causal mechanisms remain hypothesized.
- Break condition: If ingroup annotators exhibit systematic biases (e.g., desensitization to certain harm types), ingroup-grounded models may underdetect genuinely harmful content.

### Mechanism 2
- Claim: Conversational context shifts toxicity judgments by providing discourse framing
- Mechanism: Preceding and following comments establish whether a statement functions as hostile reinforcement, neutral discussion, or counter-speech, changing the perceived intent and harm severity.
- Core assumption: LLM-generated contexts adequately simulate real social media discourse (86% human validation rate suggests plausibility but not equivalence).
- Evidence anchors:
  - [abstract] "State-of-the-art toxicity detection tools...underperform on MODELCITIZENS, with further degradation on context-augmented posts."
  - [section 4.2, Figure 2] Adding context changed toxicity labels for posts across all groups—increasing toxic labels for Muslim, Black, and Women content; decreasing for others.
  - [corpus] Prior work (Pavlopoulos et al., 2020; Yu et al., 2022) found context improves hate speech detection; this paper finds it makes detection harder for current systems.
- Break condition: If generated contexts contain artifacts or biases not present in real discourse, model improvements may not transfer to production.

### Mechanism 3
- Claim: Community-grounded finetuning improves detection alignment with target populations
- Mechanism: Supervised finetuning on ingroup-annotated examples teaches models community-specific toxicity boundaries, reducing both false positives (over-censorship) and false negatives (missed harm).
- Core assumption: Training distribution matches deployment distribution; identity groups in MODELCITIZENS adequately represent broader community perspectives.
- Evidence anchors:
  - [abstract] "LLAMACITIZEN-8B and GEMMACITIZEN-12B...outperform GPT-o4-mini by 5.5% on in-distribution evaluations."
  - [section 5.2, Table 5] LLAMACITIZEN-8B generalizes to out-of-distribution datasets (Toxigen: 74.2 F1, HateModerate: 76.0 F1, Counter-Context: 53.8 F1).
  - [corpus] Corpus neighbors show toxic content moderation remains vulnerable to adversarial attacks and low-resource settings; community-grounded training may not address these.
- Break condition: If deployed on populations or identity groups not represented in training data, performance gains may not hold.

## Foundational Learning

### Concept: Annotation subjectivity and perspectivist approaches
- Why needed here: Toxicity is not objective; collapsing labels erases legitimate disagreement. Understanding this prevents expecting a single "correct" answer.
- Quick check question: Can you explain why aggregating annotations might harm marginalized communities specifically?

### Concept: Contextual grounding in content moderation
- Why needed here: The same statement can be harmful or benign depending on conversational framing. Systems must process context, not isolated utterances.
- Quick check question: How might "All Asians love kung fu movies" be interpreted differently in a stereotype-reinforcing thread vs. a media representation discussion?

### Concept: Ingroup vs. outgroup annotation frameworks
- Why needed here: Recruiting annotators based on identity alignment requires understanding selection criteria, screening, and ethical considerations.
- Quick check question: What are two potential failure modes if you only use outgroup annotators for toxicity labeling?

## Architecture Onboarding

### Component map
Toxigen sample → GPT-4o context generation → human validation (86% pass) → Prolific recruitment (ingroup/outgroup screening) → 6 annotations per post → label aggregation → LLaMA-3.1-8B/Gemma-3-12B finetuning (LLaMAFactory, 10 epochs, lr=1e-5) → evaluation on test set + OOD benchmarks

### Critical path
Annotation quality and annotator-group alignment are the bottleneck. Human validation of synthetic contexts (not model architecture) determines data quality.

### Design tradeoffs
- 8 identity groups (limited by Prolific pool representativeness) vs. broader coverage
- Synthetic context (scalable, 86% plausible) vs. real discourse (authentic but scarce)
- Binary threshold (score > 3) vs. preserving continuous ratings for uncertainty modeling

### Failure signatures
- High amplified harm rate (outgroup over-labeling) suggests models trained on aggregated data may over-censor
- 63.6% baseline accuracy on ingroup labels indicates existing systems misaligned with community standards
- Performance degradation on context-augmented posts (59.6%) signals context-processing failures

### First 3 experiments
1. Establish baseline: Evaluate existing moderation APIs on MODELCITIZENS-test with both ingroup and outgroup labels to quantify current system-community alignment gap.
2. Ablate annotation source: Finetune identical architectures on ingroup-only, outgroup-only, and aggregated labels; compare test performance to isolate annotation-source effects.
3. Test context sensitivity: Evaluate all models on paired samples (with/without context) to measure per-group context impact and identify which identity groups show largest context-dependent label shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can community-informed annotation frameworks like ModelCitizens be extended to additional identity groups beyond the current eight, and how does annotation quality scale to less-represented communities?
- Basis in paper: [explicit] "Future work may extend our annotation framework to include additional identity groups" and "Future work can explore extending LLAMACITIZEN-8B to include a broader range of identity groups and social media contexts."
- Why unresolved: Toxigen includes 13 identity groups but ModelCitizens only covers 8 groups well-represented on Prolific. Many vulnerable communities face online hate but lack annotator pool representation.
- What evidence would resolve it: Successful data collection and model training across additional groups (e.g., people with disabilities, Indigenous communities) with comparable inter-annotator agreement and model performance.

### Open Question 2
- Question: How does toxicity detection performance differ when using real-world conversational contexts versus LLM-generated synthetic contexts?
- Basis in paper: [explicit] "We use LLMs to generate context for our examples due to the limitations of human annotation, which in turn affects the quality and realism of the generated contexts. Future work should consider leveraging real-world examples from online platforms or framing context generation as a human annotation task."
- Why unresolved: 14% of generated contexts failed quality validation. Synthetic contexts may not fully capture real-world discourse patterns.
- What evidence would resolve it: Comparative study evaluating models trained on synthetic vs. platform-sourced contexts on held-out real-world toxicity detection tasks.

### Open Question 3
- Question: How do intersectional identities (e.g., Black LGBTQ+ individuals) influence toxicity perceptions, and can aggregation methods capture these nuanced perspectives?
- Basis in paper: [inferred] "Such aggregation also overlooks the role of intersectionality in shaping individual experiences." The paper collects demographic data but treats identity groups as discrete categories rather than examining overlapping identities.
- Why unresolved: Current annotation framework treats ingroup membership as binary based on single identity dimensions, potentially missing compounded harms experienced at identity intersections.
- What evidence would resolve it: Analysis of annotator subsets with multiple marginalized identities showing systematically different toxicity ratings compared to single-dimension ingroup aggregations.

## Limitations

- Annotator representativeness relies on self-reported identity without validation of community membership or harm perception accuracy
- 14% of LLM-generated contexts failed quality validation, potentially introducing systematic biases in context framing
- Generalization claims limited to English-language toxicity datasets; cross-cultural applicability remains unproven

## Confidence

- **High Confidence**: The empirical finding that existing toxicity detection models underperform on this dataset, particularly with context augmentation, is well-supported by the experimental results.
- **Medium Confidence**: The claim that ingroup annotators provide more reliable toxicity signals is supported by performance differences but relies on untested assumptions about annotator representativeness and harm perception accuracy.
- **Medium Confidence**: The effectiveness of community-grounded fine-tuning (5.5% improvement over GPT-4o-mini) is demonstrated, but the practical significance depends on real-world deployment outcomes not measured here.

## Next Checks

1. **Annotator Validation Study**: Conduct a follow-up study to validate that self-identified ingroup annotators actually share community norms and harm perceptions, using community leader interviews or ethnographic validation.
2. **Context Artifact Analysis**: Systematically analyze the 14% of generated contexts deemed implausible to identify systematic biases (e.g., overrepresentation of certain harms) and measure their impact on model training.
3. **Deployment Simulation**: Test the fine-tuned models on a realistic simulation of content moderation workflows, measuring false positive/negative rates across different identity groups in a controlled environment before production deployment.