---
ver: rpa2
title: Feature-Selective Representation Misdirection for Machine Unlearning
arxiv_id: '2512.16297'
source_url: https://arxiv.org/abs/2512.16297
tags:
- unlearning
- srmu
- representation
- misdirection
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning for large
  language models, specifically focusing on scenarios where harmful and benign knowledge
  are highly entangled. The authors propose a novel framework called Selective Representation
  Misdirection for Unlearning (SRMU) that performs feature-selective and directionally
  controlled perturbations on model representations.
---

# Feature-Selective Representation Misdirection for Machine Unlearning

## Quick Facts
- arXiv ID: 2512.16297
- Source URL: https://arxiv.org/abs/2512.16297
- Reference count: 26
- Primary result: Reduces WMDP accuracy from 54.7% to 27.2% while maintaining 57.1% MMLU accuracy under low entanglement

## Executive Summary
This paper tackles the challenging problem of machine unlearning in large language models when harmful and benign knowledge are highly entangled. The authors propose a novel framework called Selective Representation Misdirection for Unlearning (SRMU) that performs feature-selective and directionally controlled perturbations on model representations. SRMU achieves state-of-the-art unlearning performance with minimal utility loss, outperforming existing baselines even in high-entanglement scenarios where other perturbation-based approaches fail.

## Method Summary
The SRMU framework addresses machine unlearning by performing feature-selective and directionally controlled perturbations on model representations. It constructs a Dynamic Importance Map to identify feature dimensions associated with target knowledge and employs a Directional Misdirection Vector to enable controlled semantic displacement. The approach works by selectively targeting and redirecting specific features in the model's representation space, allowing for precise removal of harmful knowledge while preserving benign information. This method is particularly effective in scenarios where harmful and benign knowledge are highly entangled, a scenario that has proven challenging for previous unlearning approaches.

## Key Results
- Achieves state-of-the-art unlearning performance with minimal utility loss
- Reduces WMDP accuracy from 54.7% to 27.2% while maintaining 57.1% MMLU accuracy under low entanglement
- Further reduces WMDP accuracy to 37.7% under high entanglement scenarios
- Outperforms existing baselines and remains effective even under 20-30% overlap where other perturbation-based approaches fail

## Why This Works (Mechanism)
The framework works by performing targeted perturbations on model representations through two key components: the Dynamic Importance Map identifies which feature dimensions are most associated with the target knowledge to be unlearned, while the Directional Misdirection Vector enables controlled semantic displacement. By selectively targeting these specific features rather than applying broad perturbations, SRMU can effectively remove harmful knowledge while minimizing impact on benign information. The directional control ensures that the semantic displacement moves the model's representations away from the harmful knowledge in a controlled manner, rather than simply degrading performance across the board.

## Foundational Learning
- Representation space manipulation: Understanding how models encode knowledge in their internal representations is crucial for targeted unlearning
  - Why needed: Provides the theoretical foundation for feature-selective perturbation
  - Quick check: Verify that the model's representations can be meaningfully partitioned into harmful and benign subspaces
- Gradient-based attribution: Used to construct the Dynamic Importance Map
  - Why needed: Enables identification of feature dimensions most associated with target knowledge
  - Quick check: Confirm that gradient attributions consistently identify relevant features across different model architectures
- Directional vector control: The mechanism for controlled semantic displacement
  - Why needed: Allows precise manipulation of representations without indiscriminate degradation
  - Quick check: Validate that the misdirection vectors achieve intended semantic displacement without introducing artifacts

## Architecture Onboarding

Component Map: Input -> Dynamic Importance Map -> Directional Misdirection Vector -> Perturbed Representations -> Unlearned Model

Critical Path: The core workflow involves constructing the Dynamic Importance Map through gradient-based attribution, generating Directional Misdirection Vectors based on this map, and applying these perturbations to the model's representations during inference or fine-tuning.

Design Tradeoffs: The framework balances between precise knowledge removal (requiring detailed feature identification) and computational efficiency (avoiding exhaustive feature space analysis). The use of directional vectors provides controlled manipulation but assumes linear separability of knowledge in representation space.

Failure Signatures: The method may struggle when harmful and benign knowledge are completely inseparable in representation space, leading to either incomplete unlearning or excessive utility loss. Performance degradation can also occur when the Dynamic Importance Map fails to accurately identify relevant features due to optimization hyperparameter sensitivity.

First Experiments:
1. Validate Dynamic Importance Map construction on simple entangled knowledge pairs to ensure accurate feature identification
2. Test Directional Misdirection Vector effectiveness on isolated representation perturbations before full model integration
3. Evaluate unlearning performance on low-entanglement scenarios as a baseline before testing high-entanglement cases

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes feature representations can be cleanly disentangled even under high entanglement scenarios
- Dynamic Importance Map construction relies on gradient-based attribution, which can be sensitive to optimization hyperparameters
- Directional Misdirection Vector approach assumes linear separability of harmful knowledge in representation space
- Evaluation focuses primarily on targeted unlearning performance without extensive analysis of potential side effects on related but benign knowledge

## Confidence
- High confidence in core unlearning claims due to quantitative improvements over baselines and controlled experimental setup
- Medium confidence in method's generalizability to real-world scenarios as evaluation is constrained to synthetic entanglement benchmarks
- Low confidence in theoretical foundations of feature-selective perturbation under extreme entanglement as behavior in truly inseparable knowledge scenarios remains unproven

## Next Checks
1. Test SRMU on knowledge entanglement scenarios with >50% overlap to establish failure boundaries and characterize performance degradation
2. Conduct ablation studies removing the Dynamic Importance Map to quantify its contribution versus baseline representation perturbation
3. Evaluate downstream task performance on knowledge domains adjacent to the unlearned content to measure collateral damage to related capabilities