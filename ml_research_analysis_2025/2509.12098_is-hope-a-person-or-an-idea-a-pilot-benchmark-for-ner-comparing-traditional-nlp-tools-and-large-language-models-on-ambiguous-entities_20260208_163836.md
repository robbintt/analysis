---
ver: rpa2
title: 'Is ''Hope'' a person or an idea? A pilot benchmark for NER: comparing traditional
  NLP tools and large language models on ambiguous entities'
arxiv_id: '2509.12098'
source_url: https://arxiv.org/abs/2509.12098
tags:
- organization
- person
- location
- date
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This pilot study evaluates six NER systems\u2014three traditional\
  \ NLP libraries (NLTK, spaCy, Stanza) and three LLMs (Gemini, DeepSeek, Qwen)\u2014\
  on a manually annotated dataset of 119 tokens across five entity types. The results\
  \ show that LLMs generally outperform traditional tools in recognizing context-sensitive\
  \ entities like person names, with Gemini achieving the highest average F1-score\
  \ of 0.824."
---

# Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities

## Quick Facts
- arXiv ID: 2509.12098
- Source URL: https://arxiv.org/abs/2509.12098
- Authors: Payam Latifi
- Reference count: 10
- Primary result: LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names

## Executive Summary
This pilot study evaluates six NER systems—three traditional NLP libraries (NLTK, spaCy, Stanza) and three LLMs (Gemini, DeepSeek, Qwen)—on a manually annotated dataset of 119 tokens across five entity types. The results show that LLMs generally outperform traditional tools in recognizing context-sensitive entities like person names, with Gemini achieving the highest average F1-score of 0.824. Traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE. The findings suggest that while LLMs offer improved contextual understanding, traditional tools remain competitive in specific tasks, informing model selection based on the nature of the entity recognition task.

## Method Summary
The study compares six NER systems on a 119-token manually annotated dataset with five entity types. Three traditional tools (NLTK, spaCy, Stanza) use pre-trained pipelines for English, while three LLMs (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) are evaluated using single-shot prompting with temperature-tuned parameters. Token-level F1-scores are computed with macro-averaging across entity types. The evaluation uses NLTK's word_tokenize as reference tokenization, with LLM outputs manually aligned to this tokenization scheme.

## Key Results
- LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names
- Gemini achieves the highest overall performance with F1 = 0.824 and perfect PERSON recognition (F1 = 0.960)
- Traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform traditional tools on context-sensitive entity disambiguation when pre-training provides pragmatic signals about role words vs. proper nouns
- Mechanism: Transformer attention across the full input sequence enables inference about whether "Hope" functions as a person name or abstract concept based on surrounding syntactic and semantic cues
- Core assumption: Pre-training corpora contain sufficient examples of similar syntactic patterns to enable generalization to unseen ambiguous cases
- Evidence anchors: Gemini achieves highest overall performance with perfect PERSON recognition (F1 = 0.960) for ambiguous names like Justice and Hope

### Mechanism 2
- Claim: Traditional NLP tools maintain advantages on structured entity types (LOCATION, DATE, ORGANIZATION) through explicit gazetteers, rule-based pattern matching, and consistent boundary detection
- Mechanism: Libraries like Stanza combine neural sequence labeling with dictionary lookups and regex patterns for temporal expressions, yielding deterministic behavior on well-defined spans
- Core assumption: Target entities conform to recognizable surface patterns or appear in pre-built gazetteers
- Evidence anchors: Stanza excels at recognizing PERSON (F1 = 0.870) and matches spaCy's performance on LOCATION (F1 = 0.857)

### Mechanism 3
- Claim: Single-shot prompting with low temperature produces variable LLM outputs that may conflate sampling noise with genuine model capability differences
- Mechanism: Even at temperature 0.1–0.2, autoregressive sampling retains stochasticity; single-run evaluation cannot distinguish intrinsic model quality from random variation in token selection
- Core assumption: Observed performance gaps reflect model architecture/training rather than sampling luck
- Evidence anchors: The study notes that temperatures were set to lowest values offered by each API, and observed performance differences could partly reflect sampling variation

## Foundational Learning

- Concept: **Token-level vs. span-level F1 evaluation**
  - Why needed here: Paper uses token-level alignment (119 tokens), which penalizes partial boundary matches differently than span-level CoNLL-style evaluation
  - Quick check question: If a model tags "Center for Civic Leadership" as "Center" (O), "for Civic" (ORG), "Leadership" (O), what happens to F1 under token-level vs. span-level scoring?

- Concept: **Macro-averaging across imbalanced entity types**
  - Why needed here: Dataset has 22 ORGANIZATION tokens vs. 6 TIME tokens; macro-averaging gives equal weight to each entity type
  - Quick check question: Why might macro-F1 be lower than micro-F1 when a model fails completely on TIME (6 tokens) but succeeds on ORGANIZATION (22 tokens)?

- Concept: **Single-shot vs. few-shot prompting tradeoffs**
  - Why needed here: Paper deliberately uses single-shot to simulate "out-of-the-box" LLM behavior
  - Quick check question: What additional cost and reproducibility tradeoffs would few-shot prompting introduce?

## Architecture Onboarding

- Component map: Raw text → NLTK tokenization (119 tokens) → 6 parallel NER systems → Manual token-level mapping → Token-level precision/recall/F1 → Macro-average aggregation
- Critical path: Tokenization consistency → Prompt standardization → F1 computation with macro-averaging
- Design tradeoffs: Single-run LLM evaluation (low cost, low statistical validity) vs. multi-run with CI intervals; Token-level scoring (simpler alignment) vs. span-level (more realistic for downstream tasks)
- Failure signatures: LLM outputs non-JSON or verbose explanations → parsing failure; Multi-token entity split across boundary → token-level F1 drops; Traditional tool misses temporal expressions entirely → indicates rule coverage gap
- First 3 experiments: 1) Replicate with 5 runs per LLM at constant temperature to quantify sampling variance; 2) Add span-level CoNLL evaluation alongside token-level; 3) Expand dataset to 500+ tokens with balanced entity distribution

## Open Questions the Paper Calls Out

1. Do the performance advantages of LLMs on ambiguous entities persist when evaluated on larger, less controlled datasets across diverse domains?
2. To what extent are the reported performance differences between LLMs attributable to intrinsic model capability versus random sampling variation?
3. How does the relative ranking of traditional tools versus LLMs shift when using span-level exact match metrics instead of token-level alignment?
4. How does the lack of inter-annotator agreement (IAA) affect the validity of the gold standard for ambiguous entities like "Hope" or "Justice"?

## Limitations

- The dataset contains only 119 tokens across five entity types, limiting statistical power
- Single-shot prompting cannot distinguish between intrinsic model capability and sampling variation
- Token-level evaluation methodology may underestimate performance on multi-token entities
- Study lacks temporal reproducibility validation and statistical confidence intervals

## Confidence

- **High Confidence**: Traditional NLP tools maintaining consistent performance on structured entity types; Gemini's superior PERSON recognition performance
- **Medium Confidence**: General claim that LLMs outperform traditional tools on context-sensitive disambiguation
- **Low Confidence**: Specific numerical F1 scores and exact ranking of all six systems

## Next Checks

1. Run each LLM model 10 times at constant temperature (0.1) and report mean ± standard deviation for each entity type's F1 score
2. Compute both token-level and span-level (CoNLL-style) F1 scores for all systems to measure boundary error penalty
3. Expand the evaluation dataset to 500+ tokens with balanced entity distribution to test whether Gemini's PERSON advantage persists when ambiguity density decreases