---
ver: rpa2
title: The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single
  Direction
arxiv_id: '2503.23084'
source_url: https://arxiv.org/abs/2503.23084
tags:
- reasoning
- language
- arxiv
- features
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mechanistic analysis of the reasoning-memorization
  interplay in large language models (LLMs). The authors identify Linear Reasoning
  Features (LiReFs) - a set of linear directions in the model's residual stream that
  govern the balance between reasoning and memory recall.
---

# The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction

## Quick Facts
- arXiv ID: 2503.23084
- Source URL: https://arxiv.org/abs/2503.23084
- Authors: Yihuai Hong; Dian Zhou; Meng Cao; Lei Yu; Zhijing Jin
- Reference count: 39
- Key outcome: A single linear direction (LiReF) in residual streams mediates reasoning-memorization balance, with intervention improving reasoning task performance by 0.8-15.5%.

## Executive Summary
This paper presents a mechanistic analysis of reasoning and memorization in large language models, identifying Linear Reasoning Features (LiReFs) - single linear directions in residual streams that govern the balance between reasoning and memory recall. Through difference-in-means analysis contrasting reasoning-intensive versus memory-intensive questions, the authors extract directions that linearly separate these two modes. Causal interventions demonstrate that steering models along these directions during inference can improve reasoning performance by shifting models into "thinking mode," with gains observed across four LLMs and six datasets.

## Method Summary
The method involves extracting Linear Reasoning Features (LiReFs) by computing the difference-in-means between residual stream activations for reasoning-intensive versus memory-intensive questions at specific layers. The intervention applies α-scaled LiReFs to the residual stream during inference, with positive α for reasoning tasks and negative α for memory tasks. Performance is evaluated across multiple datasets including GSM8K, MGSM, PopQA, and MMLU-Pro, measuring accuracy improvements from intervention.

## Key Results
- LiReFs extracted from 7-9B models show cross-domain and cross-lingual generalization (Spearman correlation 0.75-0.84)
- Intervention on reasoning tasks improves performance by 0.8-15.5% across models and datasets
- Base and instruction-tuned models show nearly identical LiReF patterns, suggesting development during pretraining
- Coding tasks requiring both reasoning and memory cluster near the LiReF boundary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning and memorization modes are mediated by linear directions in the residual stream
- Mechanism: LiReFs are extracted via difference-in-means: compute mean residual stream activations for reasoning-intensive vs memory-intensive questions, take the difference vector. This direction linearly separates the two modes in activation space, primarily in middle layers.
- Core assumption: The reasoning-memorization spectrum maps to a linear subspace in activation space (Linear Representation Hypothesis).
- Evidence anchors:
  - [abstract] "identify Linear Reasoning Features (LiReFs) - a set of linear directions in the model's residual stream that govern the balance between reasoning and memory recall"
  - [section 3.1] Equation 2 shows difference-in-means extraction; Equation 3 shows intervention via h'(x) = h(x) + α * r
  - [corpus] Related work on refusal directions (Arditi et al.) suggests similar single-direction mechanisms exist for other behaviors, though "There Is More to Refusal" notes this may be incomplete for complex behaviors.
- Break condition: If reasoning requires distributed, non-linear representations across many subspaces, single-direction interventions will show diminishing returns on complex multi-step reasoning.

### Mechanism 2
- Claim: Intervention on LiReFs causally shifts model behavior by activating relevant problem-solving capabilities
- Mechanism: Adding α*r(l) to residual stream activations shifts representations toward the "reasoning mode" centroid. For reasoning tasks, positive α improves performance; for memory tasks, negative α helps. This works because downstream attention and MLP layers receive modified inputs that preferentially activate reasoning vs recall circuits.
- Core assumption: Intervention at the last token of the user turn (pre-generation) is sufficient to influence the entire generation trajectory.
- Evidence anchors:
  - [abstract] "intervening on LiReFs during inference time can improve model performance on reasoning tasks by shifting the model into a 'thinking mode'"
  - [section 4.1] Table 1 shows gains of 0.8%-15.5% across models and datasets
  - [section 4.3] Figure 6 shows GSM-Symbolic performance responds more strongly to LiReF suppression than GSM-8K, suggesting genuine reasoning generalization vs memorization
  - [corpus] Corpus evidence on intervention effectiveness is limited; no direct replications found.
- Break condition: If α scaling causes unintended activation of other feature directions (superposition interference), performance will degrade at higher α values.

### Mechanism 3
- Claim: LiReFs emerge during pretraining, not instruction-tuning, and generalize across domains/languages
- Mechanism: The same LiReF direction separates reasoning from memory tasks regardless of domain (math, physics, logic) or language (English, Chinese). Base and instruct models show highly consistent layerwise cosine similarity profiles (3/4 model families), suggesting the feature develops during pretraining.
- Core assumption: The reasoning-memorization distinction is task-intrinsic rather than format-dependent.
- Evidence anchors:
  - [section 3.3] Figure 3 shows base and instruct models have nearly identical cosine similarity profiles; Figure 5 shows coding tasks (requiring both) cluster near the boundary
  - [section 3.4] Spearman correlation of 0.840 (LLaMA3) and 0.752 (Mistral) between LiReF projection and GPT-4o reasoning scores
  - [corpus] "Refusal Direction is Universal Across Safety-Aligned Languages" provides parallel evidence for cross-linguistic generalization of activation-space directions.
- Break condition: If different reasoning types (mathematical, logical, commonsense) require orthogonal subspaces, a single LiReF will show weak correlation with some domains.

## Foundational Learning
- Concept: Residual Stream and Layerwise Activations
  - Why needed here: LiReFs are extracted from h(l)(xT), the residual stream at layer l for the last token. Understanding how attention and MLP outputs accumulate is essential for intervention.
  - Quick check question: At which layer does the paper find strongest reasoning-memory separation, and why might middle layers be optimal?
- Concept: Difference-in-Means for Feature Extraction
  - Why needed here: The core method for extracting LiReFs. Requires understanding of contrastive activation analysis.
  - Quick check question: What happens to LiReF quality if D_Reasoning and D_Memory contain mislabeled examples?
- Concept: Causal Intervention via Activation Steering
  - Why needed here: The paper's intervention method (Equation 3) requires choosing α and layer. Understanding steering coefficients is critical for replication.
  - Quick check question: Why does the paper use negative α for memory tasks rather than simply ablating the reasoning direction?

## Architecture Onboarding
- Component map:
  - Data Pipeline: GPT-4o labels questions 0-1 on reasoning-memory spectrum → D_Reasoning (score > 0.5, GSM8K, MGSM) vs D_Memory (score ≤ 0.5, PopQA, C-Eval-H)
  - Extraction Module: Forward pass on D_Reasoning and D_Memory → collect h(l)(xT) for all layers → compute difference-in-means vector r(l)
  - Intervention Module: At inference, add α*r(l) to residual stream at target layer for all tokens
  - Evaluation: Compare accuracy before/after intervention on held-out test sets

- Critical path:
  1. Curate balanced D_Reasoning and D_Memory datasets (quality of contrast determines LiReF quality)
  2. Extract r(l) at middle layers (layers 10-20 typically show best separation for 7-9B models)
  3. Tune α on validation set (paper uses 0.05 intervals)
  4. Apply intervention at inference

- Design tradeoffs:
  - Layer selection: Early layers encode syntax; late layers encode task-specific outputs. Middle layers show strongest reasoning-memory separation but may miss task-specific reasoning circuits.
  - α magnitude: Larger α gives stronger intervention but risks activation-space interference with other features.
  - Token selection: Paper intervenes on all tokens; intervening only on the last token may reduce interference but weaken effect.

- Failure signatures:
  - Low validation improvement: D_Reasoning and D_Memory may not be well-separated → re-examine labeling
  - Test degradation at high α: Superposition interference → reduce α or use ablation (Equation 4) instead of addition
  - Cross-domain failure: LiReF may be domain-specific → extract separate LiReFs per domain

- First 3 experiments:
  1. Reproduce Figure 3 on your model: Plot layerwise cosine similarity for D_Reasoning vs D_Memory to identify optimal intervention layer.
  2. Intervention sweep on validation set: Test α ∈ {-0.2, -0.15, ..., 0.2} on GSM8K (reasoning) and PopQA (memory) to find best α for each task type.
  3. Generalization test: Extract LiReF from MMLU-Pro, apply to MGSM (multilingual math) to verify cross-domain/language transfer claimed in the paper.

## Open Questions the Paper Calls Out
- Open Question 1: Can Linear Reasoning Features (LiReFs) effectively enhance model performance on deliberate, multi-step reasoning tasks utilizing prompt engineering techniques like Chain-of-Thought (CoT) or Tree-of-Thought (ToT)?
  - Basis in paper: [explicit] The authors state in the Limitations section that "it remains unclear whether LiReFs can be utilized to enhance model's ability of performing deliberate reasoning via various prompt engineering techniques such as chain-of-thought... and tree-of-thought."
  - Why unresolved: The current study focuses exclusively on short-answer reasoning benchmarks rather than long-form, self-reflective reasoning processes.
  - What evidence would resolve it: Applying LiReF intervention vectors during the generation of CoT reasoning traces and measuring the resulting accuracy on complex logical benchmarks.

- Open Question 2: Are "perturbational memorization" (failure to generalize to input variations) and "counterfactual memorization" (reliance on specific training examples) mechanistically equivalent, and are they both mediated by LiReFs?
  - Basis in paper: [explicit] The paper notes, "Future work should investigate Whether perturbational and counterfactual memorization are mechanistically equivalent and, therefore, can be both mediated by LiReFs."
  - Why unresolved: This work defines memorization solely as poor generalization (perturbation), leaving the mechanism for counterfactual dependency unexplored.
  - What evidence would resolve it: Experiments correlating LiReF activation levels with counterfactual memorization scores derived from training data ablation studies.

- Open Question 3: Do LiReFs persist and function similarly in much larger frontier models (e.g., 70B+ parameters), or does the reasoning-memorization interplay change with increased scale and inference-time compute?
  - Basis in paper: [explicit] The authors limit their scope to "relatively small LLMs" and acknowledge that "scaling up both model size and inference-time computation" significantly impacts reasoning.
  - Why unresolved: The study is restricted to models in the 7B–9B parameter range (LLaMA3-8B, Gemma2-9B, etc.).
  - What evidence would resolve it: Replicating the extraction of difference-in-means vectors and the resulting intervention performance gains on significantly larger model architectures.

## Limitations
- The paper doesn't test whether non-linear features might capture additional variance or provide better performance gains than linear LiReFs
- Intervention effects may represent memorization of reasoning patterns rather than genuine reasoning capability enhancement
- The study doesn't explore potential negative side effects of intervention on other model capabilities or generation quality

## Confidence
**High Confidence**: The extraction methodology (difference-in-means) and basic intervention framework are well-established techniques with clear mathematical foundations. The cross-linguistic generalization findings (LiReFs work across English and Chinese) are supported by direct experimental evidence.

**Medium Confidence**: The claim that LiReFs mediate reasoning ability across diverse domains has experimental support but relies on correlation analysis rather than direct causal proof across all domains. The mechanism explanation (linear directions shift representations between modes) is plausible but simplified.

**Low Confidence**: The assertion that a single direction can fully capture the reasoning-memorization spectrum for complex reasoning tasks, and that intervention effects represent genuine reasoning capability enhancement rather than pattern memorization, requires further validation.

## Next Checks
1. **Non-linear Feature Analysis**: Extract non-linear reasoning features using methods like autoencoders or sparse autoencoders, then compare their performance and interpretability to LiReFs. Test whether combining linear and non-linear features yields multiplicative gains beyond the 15.5% maximum reported.

2. **Multi-step Reasoning Test**: Design experiments using datasets requiring chained reasoning (e.g., StrategyQA, MuSiQue) to test whether LiReF interventions maintain effectiveness on complex reasoning paths versus single-step problems. Monitor for degradation patterns that would indicate superposition interference.

3. **Ablation Stability Study**: Systematically vary the size and composition of D_Reasoning and D_Memory (e.g., 50% vs 90% reasoning questions, different reasoning types) to establish how stable LiReFs are to dataset composition. This would validate whether the single-direction mediation claim holds under realistic data variations.