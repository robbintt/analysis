---
ver: rpa2
title: 'CoKV: Optimizing KV Cache Allocation via Cooperative Game'
arxiv_id: '2502.17501'
source_url: https://arxiv.org/abs/2502.17501
tags:
- cokv
- cache
- heads
- value
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models require substantial memory for key-value
  cache storage, limiting their efficiency in long-text processing. To address this,
  we propose CoKV, a novel method that evaluates attention head importance through
  cooperative game theory, specifically using the Sliced Shapley value to assess collaborative
  contributions rather than independent importance.
---

# CoKV: Optimizing KV Cache Allocation via Cooperative Game

## Quick Facts
- arXiv ID: 2502.17501
- Source URL: https://arxiv.org/abs/2502.17501
- Authors: Qiheng Sun, Hongwei Zhang, Haocheng Xia, Jiayao Zhang, Jinfei Liu, Kui Ren
- Reference count: 29
- Primary result: Achieves 97.29% of full KV cache performance using only 1.6% of cache memory (128 tokens per group average)

## Executive Summary
Large language models face significant memory and latency bottlenecks due to the KV cache growing linearly with sequence length. CoKV addresses this by treating attention head importance as a cooperative game, using Sliced Shapley Value (SSV) to evaluate collaborative contributions rather than independent importance. The method dynamically allocates cache budgets based on these importance scores, achieving state-of-the-art performance on LongBench while substantially reducing memory usage and decoding latency.

## Method Summary
CoKV employs a two-phase approach: (1) offline profiling computes SSV scores for each attention head via Algorithm 1, sampling M iterations across coalition sizes {32, 64, 96, 128} to measure marginal contributions to model accuracy; (2) online inference allocates cache budgets per head using min-max normalized SSV scores and evicts tokens using a local window-based attention pooling mechanism. For GQA models like Llama-3, the method treats each group of 4 heads sharing KV cache as a single player in the cooperative game.

## Key Results
- Achieves 97.29% of full KV cache performance using only 1.6% of cache memory (128 tokens per group average)
- Outperforms full KV cache when retaining 512 tokens on average
- Reduces decoding latency by 50% and peak memory usage by 64% compared to full cache

## Why This Works (Mechanism)
CoKV leverages cooperative game theory to model how attention heads work together rather than independently. By using Sliced Shapley Value, it captures the complementary contributions of head coalitions to overall model performance, allowing for more intelligent cache allocation that preserves critical collaborative patterns while pruning redundant or less important heads.

## Foundational Learning
- **Concept: Key-Value (KV) Cache in Transformers** - Why needed here: The entire paper is about optimizing this cache. You must understand that during autoregressive decoding, the model recomputes attention over all past tokens. The KV cache stores the Keys and Values for past tokens to avoid redundant computation, but its size grows linearly with sequence length, creating a memory and latency bottleneck. Quick check: Why does the KV cache grow with sequence length, and what is the primary resource bottleneck it creates?
- **Concept: Shapley Value and Complementary Contribution** - Why needed here: This is the core theoretical tool used to value attention heads. The Shapley value fairly distributes a total gain (model performance) among players (heads) based on their marginal contribution to all possible coalitions. Understanding that computing it exactly is #P-hard and that the paper uses an approximation (SSV) based on complementary contribution is key. Quick check: What is the primary computational bottleneck the "Sliced Shapley Value" approximation aims to solve compared to the exact Shapley value?
- **Concept: Multi-Head and Grouped-Query Attention (GQA)** - Why needed here: CoKV allocates budget at the head (or GQA group) level. You need to know that a transformer layer has multiple attention heads that operate in parallel. In GQA, multiple query heads share the same Key and Value, so the KV cache is shared, and CoKV treats this shared cache unit as a single "player" in its cooperative game. Quick check: In the context of CoKV applied to a GQA model like Llama-3, what is the "player" in the cooperative game?

## Architecture Onboarding
- **Component map:** Offline Profiling -> Coalition Sampler (creates random subsets of heads) -> Utility Evaluator (measures model accuracy for each subset) -> SSV Calculator (averages complementary contributions). Online Inference -> Budget Allocator (uses precomputed SSV scores to set cache size c_i per head) -> Token Evictor (retains top tokens based on attention scores from local window).
- **Critical path:** During inference, for each new token, compute attention scores using the query of the new token and keys in the local window. Pool these scores to get an importance score for all past keys. Evict lower-scoring keys until the cache for that head is at its allocated size c_i. This eviction must happen before the final attention output is computed for that layer.
- **Design tradeoffs:** Primary tradeoff is precomputation cost vs. inference quality. More thorough profiling yields more accurate SSV scores but increases setup time. Hyperparameter α provides direct tradeoff: higher α aggressively prunes heads, saving memory but risking loss of useful heads. System trades off generality for specificity; optimized for particular task type.
- **Failure signatures:** 1. Task Drift: Performance drops sharply if inference-time task differs significantly from profiling task. 2. Needle Loss: Model fails to answer specific factual questions embedded in long documents, indicating crucial fact KV pair was pruned. 3. Fragmented Reasoning: For complex reasoning tasks, model's chain-of-thought becomes incoherent, suggesting critical intermediate reasoning steps were evicted.
- **First 3 experiments:** 1. Budget Sweep: On fixed dataset, run CoKV with different global cache budgets (64, 128, 256, 512 tokens per head) and compare average accuracy against baselines. 2. Ablation on α: Profile single task and run inference sweeping α (1, 5, 10, 20) to observe tradeoff between pruned heads and accuracy. 3. Needle-in-a-Haystack Test: Embed specific fact at various context positions (10% to 100%) and verify CoKV preserves "needle" KV pair with high probability compared to Full KV and baseline eviction.

## Open Questions the Paper Calls Out
1. Can the precomputation overhead of Sliced Shapley Values be reduced to match or beat baseline methods without sacrificing accuracy?
2. Can CoKV be adapted to avoid separate precomputation for every specific task type?
3. How does CoKV perform when combined with KV cache quantization techniques?
4. Does the correlation of complementary contributions across coalition sizes hold for Mixture-of-Experts (MoE) or very large (70B+) models?

## Limitations
- The SSV approximation introduces uncertainty about how well it captures true head importance compared to exact Shapley values
- Local window-based eviction mechanism (window size = 8) may not capture long-range dependencies effectively
- Offline profiling phase requires substantial computational resources that scale poorly with model size

## Confidence
- **High Confidence**: Empirical results showing 50% decoding latency reduction and 64% peak memory reduction while maintaining 97.29% of full cache performance are well-documented and reproducible
- **Medium Confidence**: Theoretical framing using cooperative game theory is sound, but practical implementation details are underspecified
- **Low Confidence**: Claims about behavior on extremely long sequences (>32k tokens) or robustness to task drift are not empirically validated

## Next Checks
1. Ablation on local window size: Systematically vary the local window parameter (currently fixed at 8) and measure its impact on performance across different task types, particularly focusing on tasks requiring long-range dependencies
2. Cross-task generalization test: Profile CoKV on one task type (e.g., single-document QA) and evaluate performance when applied to a different task type (e.g., code generation) to quantify task drift effects
3. Scalability analysis: Implement CoKV on a larger model (e.g., Llama-3-70B) and measure how the offline profiling time scales with model size and number of attention heads