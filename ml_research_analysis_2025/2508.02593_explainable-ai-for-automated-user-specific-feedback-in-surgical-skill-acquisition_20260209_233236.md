---
ver: rpa2
title: Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition
arxiv_id: '2508.02593'
source_url: https://arxiv.org/abs/2508.02593
tags:
- feedback
- surgical
- skill
- expert
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of providing personalized, objective
  feedback in surgical skill acquisition, where traditional expert feedback is limited
  by faculty availability and subjective variability. The authors developed an explainable
  AI (XAI)-based feedback system that analyzes video recordings of suturing tasks
  to extract interpretable surgical skill proxies, such as hand orientation and thumb-index
  finger distance, and compares trainee performance against expert benchmarks.
---

# Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition

## Quick Facts
- arXiv ID: 2508.02593
- Source URL: https://arxiv.org/abs/2508.02593
- Reference count: 17
- Primary result: XAI-based feedback system for surgical skill acquisition shows trends toward improved performance and higher perceived usefulness but lacks statistical significance in small-scale trial.

## Executive Summary
This study addresses the challenge of providing personalized, objective feedback in surgical skill acquisition, where traditional expert feedback is limited by faculty availability and subjective variability. The authors developed an explainable AI (XAI)-based feedback system that analyzes video recordings of suturing tasks to extract interpretable surgical skill proxies, such as hand orientation and thumb-index finger distance, and compares trainee performance against expert benchmarks. In a randomized controlled trial with 12 medical students, XAI-generated feedback was compared to traditional video-based coaching. While no statistically significant differences were found between feedback types in reducing performance gaps or improving practice adjustments, the XAI group showed trends toward more expert-like behavior and reported higher perceived feedback usefulness. The study suggests that XAI has potential for surgical education but requires further refinement and larger sample sizes to demonstrate clear advantages over traditional methods.

## Method Summary
The method uses a pipeline of computer vision models to analyze surgical videos: YOLOX-S detects hands and tools, pose estimation refines hand keypoints, and MSTCN++ classifies video into six gesture phases. From these classifications, two interpretable proxies are calculated per gesture: hand orientation and thumb-index finger distance. Expert performance data establishes benchmarks for each proxy-gesture combination. For feedback generation, the system computes deviations from expert standards, ranks the top three largest gaps, and presents targeted video demonstrations alongside explicit textual explanations. A randomized controlled trial with 12 medical students compared this XAI feedback to traditional video-based coaching, measuring performance gaps, cognitive load, confidence, and perceived usefulness.

## Key Results
- No statistically significant differences between XAI and control groups in reducing performance gaps or improving practice adjustments
- XAI group showed non-significant trends toward more expert-like behavior in proxy values post-feedback
- XAI group reported significantly higher perceived feedback usefulness compared to control group

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI can provide actionable feedback by decomposing surgical performance into quantifiable, interpretable proxies and comparing them to expert benchmarks.
- Mechanism: The system extracts primitive action proxies (hand orientation, thumb-index distance) from classified gesture phases, computes deviations from expert reference values, ranks the top-3 largest gaps, and presents targeted video demonstrations with explicit textual explanations of the deviation.
- Core assumption: That expert-averaged proxy values represent optimal technique, and that reducing deviations produces meaningful skill improvement.
- Evidence anchors:
  - [abstract]: "providing user-specific feedback by comparing trainee performance to expert benchmarks and highlighting deviations from optimal execution through understandable proxies for actionable guidance"
  - [section 2.2]: "For each participant, we first calculated the absolute difference between the average proxy value for each gesture and the corresponding expert standard... We then selected the top three differences with the largest deviations"
  - [corpus]: Paper 16275 addresses real-time detection with interpretable outputs for instructional feedback; Paper 74889 generates natural-language surgical feedback from structured representations
- Break condition: If proxy measurements do not correlate with actual skill outcomes, or if experts show high variance, the benchmark fails.

### Mechanism 2
- Claim: Temporal gesture segmentation enables phase-specific, interpretable feedback rather than aggregate scoring.
- Mechanism: YOLOX-S detects hands/tools, pose estimation refines keypoints, and MSTCN++ classifies video into six gesture phases. Proxies are computed per-phase, allowing feedback tied to specific actions (e.g., hand orientation during "Lay The Knot").
- Core assumption: Surgical skill can be decomposed into discrete, measurable primitive actions with meaningful phase boundaries.
- Evidence anchors:
  - [abstract]: "extract interpretable surgical skill proxies from primitive actions"
  - [section 2.2]: "classifies gestures into six categories: 'No Gesture,' 'Needle Passing (G1),' 'Pull The Suture (G2),' 'Instrumental Tie (G3),' 'Lay The Knot (G4),' and 'Cut The Suture (G5)'"
  - [corpus]: Paper 81904 examines OSATS-based skill assessment; Paper 24121 analyzes structured task decomposition in robotic surgery training
- Break condition: If gesture boundaries are ambiguous or skill spans categories, decomposition loses information.

### Mechanism 3
- Claim: Explained contrastive feedback—pairing trainee video with expert demonstrations and explicit deviation text—helps learners identify and correct specific performance gaps.
- Mechanism: The UI displays the trainee's video alongside curated expert clips for top deviation pairs, with text following a template: "During [gesture], your [hand] [proxy name] had [relative position] average values than experts."
- Core assumption: Learners can translate abstract proxy descriptions into motor adjustments.
- Evidence anchors:
  - [abstract]: "highlighting deviations from optimal execution through understandable proxies for actionable guidance"
  - [section 2.2]: "The explainable feedback intervention included explicit guidance next to each expert video, following this template"
  - [corpus]: Paper 74889 emphasizes linguistically grounded feedback; corpus evidence for mechanism efficacy is weak given null statistical findings in this study
- Break condition: If trainees cannot physically interpret verbal descriptions, feedback is not actionable.

## Foundational Learning

- Concept: **Deliberate Practice Theory**
  - Why needed here: The system is designed to provide targeted, feedback-driven practice without continuous expert presence.
  - Quick check question: How does deliberate practice differ from simple repetition, and what role does specific feedback play?

- Concept: **Explainable AI (XAI) Principles**
  - Why needed here: The system must produce interpretable outputs (proxies) rather than black-box scores to support learning.
  - Quick check question: What makes an output "explainable," and why might accuracy alone be insufficient for training?

- Concept: **Cognitive Load Theory**
  - Why needed here: The study measures cognitive load before/after; effective feedback must inform without overwhelming.
  - Quick check question: How could providing too much detailed feedback simultaneously harm learning?

## Architecture Onboarding

- Component map: RGB video -> YOLOX-S (hand/tool detection) -> pose estimation (hand keypoints) -> MSTCN++ (gesture classification) -> proxy calculation (HO, DF) -> deviation ranking -> expert clip retrieval -> display
- Critical path: Video capture → detection → pose → gesture classification → proxy calculation → deviation ranking → retrieval → display. Early errors cascade downstream.
- Design tradeoffs:
  - 2D vs 3D: RGB-only reduces hardware complexity but may reduce spatial accuracy (acknowledged limitations).
  - Granularity vs cognitive load: Limiting to top-3 deviations manages information load but may omit relevant feedback.
  - Pre-trained vs fine-tuned: Out-of-the-box models enable rapid deployment but may lack robustness to environment variations.
- Failure signatures:
  - Misclassified gestures produce incorrect proxy associations.
  - High expert variance in P_ref undermines benchmark reliability.
  - No measurable change in proxies post-feedback suggests non-actionable explanations.
  - Participants unable to translate verbal descriptions into motor adjustments indicates vocabulary misalignment.
- First 3 experiments:
  1. Validate proxy-expert correlation: Measure proxy distributions across experts; confirm proxies discriminate known skill levels before using as training targets.
  2. Ablate feedback components: Compare video-only, text-only, and video+text conditions to isolate drivers of observed trends.
  3. Temporal resolution analysis: Test per-frame vs averaged proxy analysis to determine whether within-gesture dynamics provide additional signal beyond means.

## Open Questions the Paper Calls Out

- **Can XAI feedback significantly accelerate surgical skill acquisition?**
  - Question: Can explainable AI feedback significantly accelerate the acquisition of surgical skills compared to traditional video-based coaching?
  - Basis in paper: [explicit] The authors explicitly state, "A key question remains: Can XAI effectively accelerate the acquisition of surgical skills while maintaining trainee engagement and educational value?"
  - Why unresolved: The study found only non-significant trends toward improvement in the XAI group due to high variance and a small sample size (N=12).
  - What evidence would resolve it: A sufficiently powered randomized controlled trial demonstrating statistically significant differences in skill gap reduction between XAI and control groups.

- **Does XAI feedback effectiveness vary by trainee expertise level?**
  - Question: Does the effectiveness of XAI feedback depend on the trainee's prior level of surgical expertise?
  - Basis in paper: [inferred] The authors note that participants had "little surgical exposure" and that "translating these insights into improved practice remains challenging for novices," suggesting advanced feedback may suit intermediates better.
  - Why unresolved: The study was limited to medical students, so it is unclear if more experienced trainees would benefit more from interpretable, granular feedback.
  - What evidence would resolve it: A stratified user study comparing XAI feedback efficacy across novice, intermediate, and expert cohorts.

- **Can 3D and temporal data improve proxy reliability?**
  - Question: Does incorporating temporal dynamics and depth information improve the reliability of automated surgical skill proxies?
  - Basis in paper: [inferred] The authors acknowledge that using averaged proxy values ignores "temporal variation within each gesture" and that using "pixel coordinates without depth information" may introduce errors.
  - Why unresolved: The current system relied on 2D frontal-view data and static averaging, which limits the spatial accuracy and temporal richness of the feedback.
  - What evidence would resolve it: A comparative validation study of skill assessment pipelines using spatiotemporal 3D data versus the current 2D averaged approach.

## Limitations
- Limited statistical power with only 12 participants prevents reliable detection of meaningful differences between feedback methods.
- Single-task specificity restricts generalizability beyond interrupted suturing procedures.
- Expert benchmark reliability is uncertain due to unquantified inter-expert variability in surgical technique.

## Confidence
- XAI feedback trends toward expert-like behavior: Medium confidence
- XAI feedback perceived as more useful: Medium confidence
- XAI can provide actionable, interpretable feedback: High confidence

## Next Checks
1. Quantify inter-expert variability in proxy values across all gesture-phase combinations to establish benchmark reliability.
2. Conduct component ablation study comparing video-only, text-only, and combined video+text feedback conditions.
3. Evaluate whether within-gesture temporal dynamics provide additional discriminative power beyond mean proxy values.