---
ver: rpa2
title: 'MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing
  Dialogues'
arxiv_id: '2509.11860'
source_url: https://arxiv.org/abs/2509.11860
tags:
- memory
- moom
- dialogue
- information
- dialogues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes MOOM, a dual-branch memory plugin designed
  to address the challenge of maintaining coherent ultra-long dialogues in role-playing
  scenarios. The framework extracts and organizes memory by modeling two core story
  elements: plot development and character portrayal.'
---

# MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues

## Quick Facts
- arXiv ID: 2509.11860
- Source URL: https://arxiv.org/abs/2509.11860
- Reference count: 40
- Primary result: MOOM outperforms state-of-the-art memory extraction methods in ultra-long role-playing dialogues

## Executive Summary
MOOM addresses the challenge of maintaining coherent memory in ultra-long role-playing dialogues by introducing a dual-branch memory management system. The framework extracts and organizes memory by modeling plot development and character portrayal separately, while implementing a forgetting mechanism to prevent uncontrolled memory growth. The authors construct ZH-4O, a Chinese ultra-long dialogue dataset with 600 turns per dialogue on average, to evaluate their approach. Experimental results demonstrate improved memory extraction accuracy, reduced LLM invocation frequency, and controllable memory capacity compared to existing methods.

## Method Summary
MOOM implements a dual-branch memory plugin that processes ultra-long role-playing dialogues through two parallel memory extraction streams. The plot development branch summarizes conflicts across multiple time scales, while the character portrayal branch constructs and updates user personas. A competition-inhibition based forgetting mechanism prevents memory expansion beyond practical limits. The system was evaluated on ZH-4O, a manually annotated Chinese dataset containing ultra-long dialogues averaging 600 turns each.

## Key Results
- Achieves higher memory extraction accuracy than state-of-the-art methods
- Requires fewer large language model invocations during operation
- Maintains controllable memory capacity through effective forgetting mechanisms
- Outperforms baseline approaches in handling ultra-long dialogue scenarios

## Why This Works (Mechanism)
The dual-branch architecture allows MOOM to separately model the two core elements of narrative coherence in role-playing: plot progression and character consistency. By extracting plot conflicts across different temporal scales, the system maintains awareness of both immediate and long-term narrative threads. The persona construction branch ensures character behavior remains consistent over extended interactions. The competition-inhibition forgetting mechanism draws from cognitive science principles to selectively retain relevant information while discarding less important memories, preventing memory bloat while preserving narrative continuity.

## Foundational Learning
- Competition-inhibition memory theory: Explains how memories compete for retention while inhibiting less relevant information; needed for effective memory pruning in ultra-long dialogues; quick check: validate that forgetting mechanism preserves plot-critical information while discarding redundant details
- Dual-branch memory architecture: Separates plot and character memory processing to handle different narrative elements independently; needed because plot and character information have different retention patterns; quick check: verify that plot branch maintains temporal coherence while character branch preserves personality consistency
- Multi-scale temporal summarization: Extracts narrative elements at different time resolutions to maintain both short-term and long-term coherence; needed to prevent loss of context in ultra-long dialogues; quick check: confirm that summaries capture both immediate conflicts and overarching story arcs
- Manual memory annotation: Involves human experts labeling memory-relevant information in dialogues; needed to create high-quality training and evaluation datasets; quick check: assess inter-rater agreement and annotation consistency

## Architecture Onboarding

Component Map: Dialogue Input -> Dual-Branch Memory Extractor -> Forgetting Mechanism -> Memory Store -> LLM Output

Critical Path: Dialogue Input → Plot Branch → Character Branch → Competition-Inhibition Forgetting → Memory Store → LLM Integration

Design Tradeoffs: The dual-branch approach sacrifices some cross-branch information sharing for specialized processing efficiency, while the forgetting mechanism trades perfect recall for practical memory constraints.

Failure Signatures: Memory capacity explosion without forgetting mechanism, loss of character consistency over time, plot incoherence due to poor temporal summarization, degradation in extraction accuracy as dialogue length increases.

First Experiments:
1. Measure memory extraction accuracy on ZH-4O dataset with varying dialogue lengths
2. Compare LLM invocation frequency between MOOM and baseline methods
3. Evaluate memory capacity growth over time with and without forgetting mechanism

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research.

## Limitations
- ZH-4O dataset construction lacks detailed methodology for annotation consistency and inter-rater reliability
- Competition-inhibition memory theory application not empirically validated against alternative forgetting mechanisms
- Evaluation focuses on memory extraction accuracy without thorough examination of downstream dialogue coherence impact
- Claims of outperforming state-of-the-art lack external validation across different dialogue domains

## Confidence
- Memory extraction accuracy improvements: High
- Reduced LLM invocation frequency: Medium
- Controllable memory capacity maintenance: High
- Generalizability to non-Chinese dialogues: Low

## Next Checks
1. Conduct cross-linguistic validation using an English or multilingual ultra-long dialogue dataset to assess MOOM's language independence
2. Perform ablation studies to isolate the contribution of the competition-inhibition forgetting mechanism versus alternative memory pruning approaches
3. Implement a user study measuring subjective dialogue coherence and engagement when using MOOM versus baseline memory systems in real-world role-playing scenarios