---
ver: rpa2
title: 'SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection'
arxiv_id: '2506.17288'
source_url: https://arxiv.org/abs/2506.17288
tags:
- retrieval
- slimrag
- entity
- index
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SlimRAG introduces a graph-free, entity-aware retrieval framework
  for knowledge-intensive NLP tasks, addressing the inefficiency of graph-based RAG
  systems that rely on costly entity linking and relation extraction while often returning
  irrelevant context. Instead of constructing explicit knowledge graphs, SlimRAG builds
  a lightweight entity-to-chunk inverted index and uses entity-guided semantic search
  to retrieve contextually relevant chunks.
---

# SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection

## Quick Facts
- arXiv ID: 2506.17288
- Source URL: https://arxiv.org/abs/2506.17288
- Authors: Jiale Zhang; Jiaxiang Chen; Zhucong Li; Jie Ding; Kui Zhao; Zenglin Xu; Xin Pang; Yinghui Xu
- Reference count: 18
- Primary result: Entity-aware retrieval without graphs outperforms graph-based RAG on multi-hop QA while using 70% less index storage

## Executive Summary
SlimRAG introduces a graph-free, entity-aware retrieval framework for knowledge-intensive NLP tasks, addressing the inefficiency of graph-based RAG systems that rely on costly entity linking and relation extraction while often returning irrelevant context. Instead of constructing explicit knowledge graphs, SlimRAG builds a lightweight entity-to-chunk inverted index and uses entity-guided semantic search to retrieve contextually relevant chunks. This approach decouples similarity-based indexing from relevance-driven retrieval, avoiding the pitfalls of treating semantic similarity as semantic relevance. Evaluated on multi-hop QA benchmarks, SlimRAG outperforms strong flat and graph-based baselines in accuracy (e.g., 57.41% vs. 55.32%) while achieving dramatically higher index compactness, measured by Relative Index Token Utilization (RITU), of 16.31 compared to 56+ for competitors. It also reduces indexing time and scales efficiently with corpus size. SlimRAG demonstrates that lightweight, entity-centric retrieval can deliver superior performance without structural overhead.

## Method Summary
SlimRAG operates in two phases: indexing and retrieval. During indexing, text chunks are segmented from the corpus, entities are extracted using gpt-4o-mini with coreference resolution, and an entity-to-chunk inverted map (M_EC) is built mapping each entity to the chunks containing it. Embeddings are stored using text-embedding-3-small. During retrieval, queries are decomposed into sub-queries via LLM, entities are extracted, and semantic search finds top-K matching corpus entities. Chunks are retrieved via M_EC lookup and scored by multiplying query-chunk similarity with entity hit count. Top-K chunks are selected, reordered by document position, and merged within a token limit to form the final context.

## Key Results
- Accuracy: 57.41% on HotpotQA vs. 55.32% for graph-based RAPTOR and 35.65% for LightRAG Hybrid
- Index compactness: RITU of 16.31 (70% less storage than competitors with RITU 56+)
- Indexing efficiency: Faster indexing time compared to graph-based baselines
- Scalability: Maintains performance with corpus size growth while reducing structural overhead

## Why This Works (Mechanism)

### Mechanism 1: Entity-to-Chunk Inverted Index Replaces Graph Structure
A lightweight inverted index mapping entities to chunk IDs can achieve comparable or better retrieval accuracy than graph-based approaches while reducing structural overhead. During indexing, extract entities from each chunk and build M_EC (entity → {chunk_ids}). At retrieval, look up chunks via entity matching rather than traversing graph edges. This eliminates edge construction, relation extraction, and subgraph refinement. Core assumption: Entity-level signals are sufficient for retrieval without explicit inter-entity relationships.

### Mechanism 2: Decoupling Similarity (Indexing) from Relevance (Retrieval)
Separating semantic similarity used for organization from semantic relevance used for retrieval improves precision by avoiding retrieval of similar-but-irrelevant content. Indexing uses embedding similarity to group entities efficiently. Retrieval scores chunks by BOTH query-chunk similarity AND entity overlap count—prioritizing task-specific relevance over global proximity. Core assumption: High cosine similarity does not guarantee query-specific relevance (e.g., "Waiter" and "Waitress" are similar but latter is irrelevant for "What is the gender of this waiter?").

### Mechanism 3: Dual-Factor Chunk Scoring
Multiplying semantic similarity by entity hit count yields more relevant chunk rankings than either factor alone. For each candidate chunk C_k, compute `score_k = φ_q × count_k` where φ_q is query-chunk similarity and count_k is matched entity count for that chunk. Core assumption: Chunks with both high semantic alignment AND entity coverage are most informative.

## Foundational Learning

- Concept: Semantic Similarity vs. Semantic Relevance
  - Why needed here: The paper's core claim depends on distinguishing these. Similarity = embedding proximity (query-independent). Relevance = alignment with specific information need (query-sensitive).
  - Quick check question: Given query "Who founded Microsoft?", would "Apple" have high similarity or high relevance? Explain the difference.

- Concept: Inverted Index
  - Why needed here: SlimRAG replaces graph traversal with a lookup table. Understanding M_EC: entity → {chunk_ids} is essential for implementing the retrieval phase.
  - Quick check question: If entity "Microsoft" appears in chunks {1, 5, 23}, what does M_EC("Microsoft") return? How is this used during query processing?

- Concept: Coreference Resolution
  - Why needed here: Ablation shows removing coreference resolution drops accuracy from 57.41% to 40.98%—a 16+ point decline. Canonicalizing mentions (e.g., "he" → "Bill Gates") is critical for index quality.
  - Quick check question: In "Gates founded Microsoft. He later stepped down," what should "He" resolve to, and why does this affect retrieval?

## Architecture Onboarding

- Component map:
  Indexing Phase: Chunk Segmentation → Entity Extraction (with coreference) → Build E_I (entity set) + M_EC (entity→chunks map)
  Retrieval Phase: Query → Decompose into sub-queries → Extract query entities E_Q → Semantic search for top-K matching corpus entities → Lookup chunks via M_EC → Score (similarity × entity_count) → Rank & assemble context

- Critical path:
  Indexing: `ExtractEntities()` with coreference is the bottleneck—quality here determines everything downstream.
  Retrieval: Entity matching quality (`RetrieveTopK`) directly affects which chunks enter the candidate pool.

- Design tradeoffs:
  - Graph vs. Table: Gains scalability and simplicity; loses explicit relational reasoning
  - LLM-based extraction: gpt-4o-mini balances cost/quality; cheaper models may miss entities
  - Hyperparameters: K=5 (entity expansion), H=10 (output sentences)—tuned on HotpotQA, may need domain adjustment

- Failure signatures:
  - High accuracy, low recall: Overly aggressive entity filtering
  - High RITU, low accuracy: Noisy entity extraction inflating irrelevant chunks
  - Poor performance on single-hop queries: Over-decomposition fragmenting simple queries

- First 3 experiments:
  1. Reproduce HotpotQA baseline: Index Wikipedia corpus, verify RITU ~16 and accuracy ~57% to confirm implementation correctness.
  2. Ablate coreference resolution: Run on 500-doc subset with/without coreference; expect ~16% accuracy drop per paper's ablation.
  3. Domain stress test: Apply to your corpus (e.g., technical docs); measure RITU and manually inspect top-10 chunks for relevance on 20 sample queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SlimRAG generalize to domains beyond multi-hop QA, such as scientific summarization, dialogue systems, or legal reasoning tasks?
- Basis in paper: Conclusion states: "suggest promising extensions to broader domains, larger corpora, and adaptive query decomposition strategies."
- Why unresolved: All experiments were conducted solely on HotpotQA; no evaluation on other knowledge-intensive NLP tasks mentioned.
- What evidence would resolve it: Evaluation results on diverse benchmarks like Musique, QMSum, or domain-specific corpora with comparable metrics.

### Open Question 2
- Question: Can adaptive query decomposition strategies outperform the current LLM-based decomposition approach?
- Basis in paper: Conclusion mentions "adaptive query decomposition strategies" as a promising extension; current method uses fixed LLM prompting.
- Why unresolved: Query decomposition uses a single approach without comparing alternative strategies or adaptive methods that adjust to query complexity.
- What evidence would resolve it: Ablation comparing static vs. complexity-aware decomposition, measuring accuracy gains and computational overhead.

### Open Question 3
- Question: How robust is SlimRAG when entity extraction quality degrades due to smaller or less capable language models?
- Basis in paper: Entity extraction relies on GPT-4o-mini; ablation shows 16+ point accuracy drop when coreference resolution fails.
- Why unresolved: No experiments test performance with weaker or open-source entity extractors; dependency on high-quality extraction is critical.
- What evidence would resolve it: Experiments with varying extraction model sizes/capabilities, reporting accuracy degradation curves.

## Limitations

- Domain specificity: Performance validated only on HotpotQA Wikipedia corpus; generalization to specialized domains untested
- Entity extraction dependency: Critical reliance on high-quality entity extraction with coreference resolution; accuracy drops 16+ points when ablated
- Implicit reasoning: Replaces explicit graph-based reasoning chains with entity co-occurrence; may struggle with complex relational queries
- No learned scoring: Uses handcrafted dual-factor scoring without exploring learned or attention-based alternatives

## Confidence

**High Confidence**: The empirical results showing SlimRAG's accuracy advantage over flat and graph-based baselines on HotpotQA (57.41% vs. 55.32% for RAPTOR) are well-documented and reproducible given the described methodology.

**Medium Confidence**: The scalability claims (RITU=16.31 vs. 56+ for competitors) appear robust for the tested corpus size but may not generalize to extremely large or highly specialized knowledge bases.

**Low Confidence**: The paper's assertion that graph structures are unnecessary for multi-hop reasoning relies on implicit entity relationships rather than explicit reasoning chains; the mechanism for handling complex reasoning without graph traversal is underspecified.

## Next Checks

1. **Domain Transfer Test**: Apply SlimRAG to a non-Wikipedia corpus (e.g., scientific literature or legal documents) and measure accuracy and RITU. This validates whether entity-centric retrieval generalizes beyond the training domain and whether domain-specific entity types affect performance.

2. **Entity Extraction Stress Test**: Systematically degrade entity extraction quality (e.g., use smaller models, introduce coreference errors) and measure the resulting accuracy drop. This quantifies the fragility of the inverted index approach and identifies acceptable quality thresholds.

3. **Graph vs. Index Efficiency Comparison**: Implement a minimal graph-based RAG system on the same corpus and measure memory usage, indexing time, and retrieval latency alongside SlimRAG's metrics. This provides empirical validation of the claimed efficiency gains and reveals hidden costs of the table-based approach.