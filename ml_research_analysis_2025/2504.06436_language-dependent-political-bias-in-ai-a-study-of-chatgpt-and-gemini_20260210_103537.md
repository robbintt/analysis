---
ver: rpa2
title: 'Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini'
arxiv_id: '2504.06436'
source_url: https://arxiv.org/abs/2504.06436
tags:
- political
- language
- chatgpt
- gemini
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated political bias in AI models, specifically
  ChatGPT and Gemini, using a political compass test in 14 languages. Results revealed
  both models exhibit left-wing and liberal biases, with Gemini showing stronger bias
  than ChatGPT.
---

# Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini

## Quick Facts
- arXiv ID: 2504.06436
- Source URL: https://arxiv.org/abs/2504.06436
- Reference count: 0
- Both ChatGPT and Gemini exhibit left-wing and liberal biases, with Gemini showing stronger bias

## Executive Summary
This study investigated political bias in AI language models by testing ChatGPT 3.5 and Gemini 1.0 Pro across 14 languages using a standardized political compass test. Results revealed that both models consistently exhibit left-wing and liberal tendencies, with Gemini showing a more pronounced bias than ChatGPT. The political leanings varied significantly across languages, with high-resource languages (particularly Romance languages) clustering together and showing more consistent patterns, while low-resource languages like Persian and Turkish displayed greater volatility. The findings highlight the need for greater transparency in AI model development and raise concerns about the cultural and political neutrality of widely used language technologies.

## Method Summary
The study employed a standardized 62-item Political Compass test, translated into 14 languages, to measure political bias in two AI models. ChatGPT was queried once per question (assumed deterministic), while Gemini was queried seven times per question with the mode selected as the final answer. Responses were mapped to political coordinates (X: Economic Left/Right, Y: Social Authoritarian/Liberal) and analyzed using Euclidean distances from the neutral center point (0,0). Statistical analysis included t-tests for significance and clustering analysis to identify language groups based on political tendencies.

## Key Results
- Both models exhibit left-wing and liberal biases across all tested languages
- Gemini shows stronger political bias than ChatGPT in most languages
- Persian and Turkish display the highest bias variance due to data scarcity and structural differences
- Romance languages cluster together, indicating similar political leanings
- Statistical analysis confirms significant deviation from political neutrality (p<0.05)

## Why This Works (Mechanism)

### Mechanism 1
Political bias propagates from Western-centric training data dominance in high-resource languages. Models ingest corpora where liberal/left-wing perspectives are statistically over-represented, causing consistent but biased outputs when prompted in high-resource languages like Romance languages.

### Mechanism 2
Linguistic structural differences and data scarcity cause higher volatility in political bias for low-resource languages. Languages with distinct grammatical structures or limited training data map less precisely to the model's latent "neutral" space, resulting in inconsistent political positions.

### Mechanism 3
Fine-tuning and alignment processes prioritize "safety" heuristics that correlate with specific political leanings. Safety filters designed to avoid hate speech or misinformation often penalize conservative/authoritarian-leaning outputs more frequently than liberal ones.

## Foundational Learning

**Concept: Political Compass Coordinate System**
- Why needed: The study quantifies bias using (X, Y) coordinates; understanding this 2D grid is required to interpret Euclidean distances
- Quick check: If a model scores (-5, -5), is it Right-Wing Authoritarian or Left-Wing Libertarian? (Answer: Left-Wing Libertarian)

**Concept: High-Resource vs. Low-Resource Languages**
- Why needed: Bias variance is explicitly linked to training data volume; distinguishing abundance (Romance) from scarcity (Persian/Turkish) is essential for diagnosis
- Quick check: Why would a model exhibit more unpredictable political bias in a language it has seen very little of during training?

**Concept: Stochastic vs. Deterministic Decoding**
- Why needed: The methodology treated ChatGPT (deterministic) and Gemini (stochastic) differently; Gemini required repeated queries to find a stable answer
- Quick check: Why is asking a stochastic model a political question once insufficient for determining its "true" bias?

## Architecture Onboarding

**Component map:** Web interface -> ChatGPT (GPT-3.5 Dense Transformer) vs. Gemini 1.0 Pro (MoE) -> Political Compass test responses -> (X, Y) coordinate calculation -> Euclidean distance analysis

**Critical path:** 1. Prompt injection (Translate compass questions) -> 2. Response extraction (Map text answers to Agree/Disagree) -> 3. Coordinate calculation (Compute X/Y scores) -> 4. Variance analysis (Cluster languages by bias profile)

**Design tradeoffs:**
- Test Validity vs. Coverage: Restricted to languages with officially adapted Political Compass tests
- Static vs. Dynamic Data: ChatGPT (static training) vs. Gemini (real-time access) creates confounding variable

**Failure signatures:**
- Outlier Drift: High Euclidean distances (e.g., Persian in ChatGPT) indicate "hallucination" of political stance
- Refusal Triggers: Safety filters blocking sensitive questions creates data gaps

**First 3 experiments:**
1. Cross-Model Consistency Check: Run Political Compass test in controlled environment (temperature 0) for both models in English and Farsi
2. Data Volume Correlation: Plot Euclidean Distance from Neutrality against estimated training data size for all 14 languages
3. Safety Filter Stress Test: Prompt both models with Authoritarian vs. Liberal phrased questions in Turkish vs. Spanish to test refusal rates

## Open Questions the Paper Calls Out

**Open Question 1:** Do paid or advanced versions of LLMs exhibit different political biases compared to their free counterparts? (The study used only free versions, identified as a limitation)

**Open Question 2:** Do LLMs drift toward political neutrality as they undergo iterative updates? (The paper recommends longitudinal studies to track this)

**Open Question 3:** Is political bias variance in low-resource languages primarily driven by data scarcity or by grammatical/structural features? (The paper attributes it to both but doesn't isolate variables)

## Limitations

- Scoring algorithm mapping responses to (X, Y) coordinates is proprietary and opaque
- Comparison between static (ChatGPT) and dynamic (Gemini) models introduces confounding variables
- Study doesn't control for cultural or contextual nuances in political interpretation across languages

## Confidence

**High Confidence:** Both models exhibit left-wing and liberal biases (supported by consistent statistical patterns and clustering analysis)

**Medium Confidence:** Gemini shows stronger bias than ChatGPT (quantitative metrics support this, but architectural differences complicate attribution)

**Low Confidence:** Linguistic structural differences and data scarcity cause higher volatility in low-resource languages (plausible but lacks direct empirical validation)

## Next Checks

1. Run identical Political Compass test using controlled settings (temperature 0) for both models in English and Farsi to verify if "Farsi Outlier" persists
2. Quantify relationship between training data availability and bias variance by plotting Euclidean distance against estimated training data size
3. Systematically test whether safety filters differentially affect responses based on political leaning across multiple languages