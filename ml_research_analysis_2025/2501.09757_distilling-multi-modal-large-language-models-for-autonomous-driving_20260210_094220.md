---
ver: rpa2
title: Distilling Multi-modal Large Language Models for Autonomous Driving
arxiv_id: '2501.09757'
source_url: https://arxiv.org/abs/2501.09757
tags:
- dima
- latexit
- planning
- scene
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DiMA, a framework that distills knowledge
  from multi-modal large language models (MLLMs) into vision-based planners for autonomous
  driving. DiMA addresses the challenge of leveraging MLLM's world knowledge while
  maintaining the efficiency of vision-based planners.
---

# Distilling Multi-modal Large Language Models for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2501.09757
- **Source URL:** https://arxiv.org/abs/2501.09757
- **Reference count:** 40
- **Primary result:** DiMA achieves 37% reduction in L2 trajectory error and 80% reduction in collision rate by distilling MLLM knowledge into vision-based planners

## Executive Summary
This paper introduces DiMA, a framework that distills knowledge from multi-modal large language models (MLLMs) into vision-based planners for autonomous driving. The key insight is using the vision planner's scene encoder as a trainable tokenizer to provide structured inputs to the MLLM, enabling efficient planning without compromising robustness. DiMA employs joint training with surrogate tasks (scene editing, future prediction, masked reconstruction) to align the vision planner and MLLM objectives. At inference, the MLLM can be discarded while maintaining performance through KL-divergence distillation. Experiments show state-of-the-art performance on nuScenes planning benchmark with significant improvements in both trajectory accuracy and safety.

## Method Summary
DiMA employs a two-stage training strategy: first pre-training a vision-based planner (VAD or UniAD) for 60 epochs to establish a robust scene encoder, then jointly training with an MLLM (LLaVA-v1.5-7B with LoRA) for 30 epochs. The scene encoder transforms multi-view images into BEAM tokens (Bird's-eye-view, Ego, Agent, Map) that serve as structured inputs to the MLLM. The system includes three branches: a vision planner for efficient inference, an MLLM for knowledge distillation and VQA, and surrogate task heads for masked reconstruction, future prediction, and scene editing. Training uses AdamW optimizer with a 2Ã—10^-4 learning rate, and distillation is achieved through KL-divergence between hidden features of the vision planner and MLLM.

## Key Results
- 37% reduction in L2 trajectory error for the vision-based planner
- 80% reduction in collision rate compared to baseline planners
- 44% trajectory error reduction in long-tail scenarios (e.g., 3-point turns)
- State-of-the-art performance on nuScenes planning benchmark
- Distilled vision planner matches or exceeds performance of running full MLLM at inference

## Why This Works (Mechanism)

### Mechanism 1: Structured Tokenization as a Trainable Interface
Using the vision-based planner's scene encoder as a trainable tokenizer for the LLM improves planning robustness over using frozen, unstructured image encoders. The scene encoder explicitly models scene components into "BEAM" tokens, which are semantically grounded rather than generic image patches. Making this tokenizer trainable during LLM joint-optimization forces visual features to align with the LLM's reasoning capabilities.

### Mechanism 2: Causal Enrichment via Surrogate Tasks
Training on "surrogate tasks" like scene editing synthetically adds or removes agents and forces the model to predict resulting ego-behavior. This acts as counterfactual reasoning exercise, encouraging the model to learn causal relationships rather than mere correlation, leading to better generalization in long-tail scenarios.

### Mechanism 3: Inference-Efficient Knowledge Distillation
The vision planner inherits the "world knowledge" of the LLM via KL-divergence distillation, allowing the LLM to be discarded at inference time without losing performance. During training, the penultimate layer features of the lightweight Planning Transformer are aligned with the hidden features of the heavy LLM, forcing the efficient planner to mimic the internal "logic" of the LLM.

## Foundational Learning

**Bird's-Eye-View (BEV) Representations**: DiMA relies on transforming multi-view camera images into a unified top-down view to create BEAM tokens. Quick check: Can you explain why a point-wise BEV feature is generally better for planning than a feature map from a single front-facing camera?

**Query-Based Transformers (Q-Former/DETR)**: The architecture uses specific "queries" to extract information from BEV features. Quick check: How does a "learnable query" know what to look for in an image feature map (e.g., how does the "Agent Query" find a car)?

**Knowledge Distillation (Teacher-Student)**: The core contribution is distilling the LLM (Teacher) into the Vision Planner (Student). Quick check: In KL-divergence distillation, why do we often compare the soft probabilities (logits) rather than just the hard class predictions?

## Architecture Onboarding

**Component map:** Multi-view Images + Text Prompts -> Visual Encoder (ResNet) -> BEV Projection -> Scene Encoder (shared) -> BEAM Tokens -> Planning Transformer -> Trajectory Head (Branch A) AND Q-Formers -> LLM (LLaVA) -> VQA Head / Distillation Head (Branch B) AND Surrogate Task Heads (Masked Reconstruction, Future Prediction, Scene Editing)

**Critical path:** The Scene Encoder is the bottleneck. It must serve two masters: the direct planning head (requiring geometric precision) and the LLM (requiring semantic richness). Debugging typically starts here.

**Design tradeoffs:** The paper uses a trainable tokenizer versus a frozen tokenizer for better alignment. Using a 7B parameter model versus smaller (1B) may affect the "world knowledge" needed for distillation.

**Failure signatures:** High collision rate but low L2 error indicates the model is driving smoothly but ignoring obstacles - check the Scene Editing loss. Mode Collapse in VQA with generic text answers suggests the Q-Former may not be passing enough visual context - check the BEAM token injection.

**First 3 experiments:** 1) Train only the vision planner (VAD Baseline) without LLM or distillation to establish baseline L2 and collision rate. 2) Freeze the Scene Encoder and train the LLM to compare against trainable tokenizer setup. 3) Disable the "Scene Editing" task specifically and evaluate on the "Targeted" validation split (turning scenarios) to see if collision rates spike.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored: closed-loop autonomous driving performance where planner actions dynamically influence the environment, the extent to which hallucinations or reasoning errors from the MLLM are distilled into the final planner, and whether the knowledge generalizes to unseen geographic locations or sensor configurations.

## Limitations
- The paper relies entirely on open-loop nuScenes planning benchmark, which doesn't account for error accumulation or dynamic agent reactions in real-world deployment
- The rate at which negative semantic biases or false premises from the LLM are distilled into the final planner is not quantified
- All experiments are confined to the nuScenes dataset (Boston/Singapore), leaving cross-domain transfer unverified

## Confidence

**High Confidence (Mechanism 1 - Structured Tokenization)**: The conceptual advantage of structured BEAM tokens over unstructured image embeddings is well-supported by the architecture description and ablation studies.

**Medium Confidence (Mechanism 2 - Surrogate Tasks)**: While the paper provides ablation evidence showing surrogate tasks improve performance, the specific contribution of the scene editing task versus other surrogate tasks is not isolated.

**Medium Confidence (Mechanism 3 - Distillation)**: The KL-divergence distillation approach is a standard technique with established theoretical foundations, but the paper doesn't analyze what aspects of the LLM's reasoning are actually captured in the distilled features.

**Low Confidence (Quantitative Claims)**: Without access to the exact evaluation protocol, loss weights, and implementation details, reproducing the specific 37% L2 reduction and 80% collision rate reduction would be challenging.

## Next Checks

1. **Tokenizer Ablation Reproducibility**: Freeze the scene encoder and train the LLM end-to-end on nuScenes. Compare the resulting planner performance against the trainable tokenizer setup to verify the 37% improvement claim under controlled conditions.

2. **Surrogate Task Isolation**: Disable the scene editing task specifically while keeping other components unchanged. Evaluate on the "Targeted" validation split (turning scenarios) to measure the precise impact on collision rates and verify the causal reasoning benefits.

3. **Distillation Fidelity Analysis**: Compare the internal representations of the distilled vision planner versus the full LLM system on a held-out validation set. Measure feature correlation in the ego-token space to quantify how much of the LLM's reasoning is actually captured in the distilled model.