---
ver: rpa2
title: 'ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking'
arxiv_id: '2509.25814'
source_url: https://arxiv.org/abs/2509.25814
tags:
- community
- topic
- graph
- global
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReTAG, a Retrieval-Enhanced, Topic-Augmented
  Graph framework for global sensemaking that improves response quality and reduces
  inference time compared to the baseline. ReTAG constructs topic-specific subgraphs
  and retrieves relevant community summaries to generate answers.
---

# ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking

## Quick Facts
- arXiv ID: 2509.25814
- Source URL: https://arxiv.org/abs/2509.25814
- Reference count: 40
- Primary result: Achieves winning rates exceeding 50% and reduces inference time by up to 90.3% via topic-specific subgraphs and retrieval-augmented summary selection.

## Executive Summary
This paper introduces ReTAG, a Retrieval-Enhanced, Topic-Augmented Graph framework for global sensemaking that improves response quality and reduces inference time compared to the baseline. ReTAG constructs topic-specific subgraphs and retrieves relevant community summaries to generate answers. Experiments show that ReTAG achieves winning rates exceeding 50% across both Podcast and News Articles datasets, while reducing inference time by up to 90.3% (News Articles) and 65.4% (Podcast) at higher community levels. Topic augmentation improves content relevance and reduces computational complexity, while retrieval augmentation further enhances efficiency by selecting the most pertinent summaries.

## Method Summary
ReTAG builds upon GraphRAG by introducing topic augmentation and retrieval augmentation. The method constructs topic-specific subgraphs by mining topics from the corpus, then extracts only topic-relevant entities and relations. Hierarchical Leiden clustering partitions each subgraph into communities, which are summarized with context compression. During inference, queries are classified into topics, expanded with keywords, and used to retrieve the most relevant community summaries via BM25. Only these summaries are passed to the LLM for answer generation, dramatically reducing inference cost while maintaining quality.

## Key Results
- Winning rates exceeding 50% across Podcast and News Articles datasets
- Inference time reductions of up to 90.3% (News Articles) and 65.4% (Podcast) at higher community levels
- Topic augmentation reduces graph size by 30.88% fewer nodes and 35.86% fewer edges on average

## Why This Works (Mechanism)

### Mechanism 1: Topic-Augmented Subgraph Pruning
Constraining the entity-relation graph to topic-relevant content reduces irrelevant context and improves both relevance and efficiency. Topics are mined via a global sensemaking query over the corpus. For each topic t, only entities and relations clearly relevant to t are extracted into G_c^(t), yielding smaller graphs (30.88% fewer nodes, 35.86% fewer edges on average). Smaller graphs produce fewer community summaries, reducing LLM prompts during inference. Core assumption: The LLM-based topic classifier correctly maps queries to topics, and topic-specific extraction does not lose cross-topic dependencies needed for the query.

### Mechanism 2: Hierarchical Community Summarization with Context Compression
Hierarchical clustering with context-aware summarization preserves global structure while fitting within LLM context windows. Hierarchical Leiden clustering partitions G_c into L levels of communities. Leaf-level communities summarize all node/edge contexts; non-leaf communities aggregate child contexts, iteratively replacing longest contexts with their summaries to fit token limits. Shuffling and chunking summaries during answer generation prevents information aggregation bias. Core assumption: Edge prominence (sum of endpoint degrees) is a valid proxy for context importance when truncating.

### Mechanism 3: Retrieval-Augmented Summary Selection with Keyword Expansion
Retrieving only the top-p most relevant community summaries via keyword-expanded queries maintains answer quality while dramatically reducing inference cost. Given query q and dataset description, an LLM generates expansion keywords. BM25 retrieves top-p summaries S(c_l) from the relevant topic's community set. Only these summaries proceed to answer generation. Retrieval recall increases with p and benefits from keyword expansion. Core assumption: BM25 similarity on summaries + keyword expansion approximates relevance to the global sensemaking query; missing summaries do not critically degrade global coherence.

## Foundational Learning

- **GraphRAG / Graph-based Retrieval-Augmented Generation**: ReTAG builds directly on Edge et al. (2025) GraphRAG, which constructs contextualized entity-relation graphs and hierarchical community summaries. Without this baseline, the topic and retrieval augmentations lack context.
  - Quick check: Can you explain how GraphRAG differs from standard RAG in handling corpus-wide queries?

- **Hierarchical Community Detection (Leiden Algorithm)**: The hierarchical Leiden algorithm partitions the graph into multi-level communities, enabling variable-granularity summarization. Understanding levels and their tradeoffs is critical for selecting l.
  - Quick check: What does increasing the community level l do to summary granularity and inference cost?

- **BM25 Retrieval and Keyword Expansion**: Retrieval augmentation uses BM25 with LLM-generated keyword expansion. Understanding how keyword expansion affects recall and why BM25 is chosen over dense retrieval informs optimization decisions.
  - Quick check: Why might keyword expansion help BM25 for global sensemaking queries?

## Architecture Onboarding

- **Component map**: Graph Construction -> Topic Mining -> Topic-Specific Graphs -> Hierarchical Clustering -> Community Summarization -> Inference (Topic Classification -> Keyword Expansion -> BM25 Retrieval -> Sub-answer Generation -> Final Answer Aggregation)
- **Critical path**: Graph construction and topic mining are prerequisites for all downstream tasks. Errors in entity extraction or topic misclassification propagate through summarization and retrieval.
- **Design tradeoffs**:
  - Indexing cost vs inference speed: Topic augmentation increases preprocessing (~5-10x longer indexing), but reduces inference time by up to 90.3%.
  - Granularity vs coverage: Lower l yields fewer, broader summaries (faster but may miss details); higher l yields more, narrower summaries (slower but more detailed).
  - Retrieval cutoff p: Higher p improves recall but increases inference time; optimal p depends on dataset and query scope.
- **Failure signatures**:
  - Topic misclassification: Query routed to wrong topic graph; answer lacks relevant information. Fallback to general summaries mitigates but loses efficiency gains.
  - Retrieval miss: Relevant communities not in top-p; answer incomplete. Keyword expansion helps but is not guaranteed.
  - Context overflow: Even after compression, community context exceeds W; truncation may drop critical edges/nodes.
  - Fragmented global view: Overly specific topics or too-small p prevent synthesis across corpus.
- **First 3 experiments**:
  1. Baseline reproduction: Implement GraphRAG (Edge et al.) on Podcast/News datasets; measure inference time and answer quality at each level l.
  2. Topic augmentation ablation: Add topic mining and topic-specific graphs; compare node/edge counts, summary counts, and winning rates vs baseline.
  3. Retrieval augmentation ablation: Add keyword expansion + BM25 retrieval (varying p); measure recall@p, inference time, and winning rates vs topic-only model.

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational overhead of indexing topic-augmented subgraphs be reduced to make the framework viable for dynamic, rapidly evolving corpora? The paper explicitly identifies the "non-trivial preprocessing overhead" and the resource-intensive nature of using high-performing LLMs for indexing as a significant drawback.

### Open Question 2
How does the restriction to a single target topic impact response quality for complex queries that bridge multiple distinct semantic themes? Section 3.2.2 states the framework classifies a query into a single target topic t or defaults to general summaries, assuming a query's intent can be siloed into one pre-mined topic cluster.

### Open Question 3
Would replacing the lexical BM25 retrieval with dense semantic retrieval improve the accuracy of selecting relevant community summaries? Section 3.3 relies on BM25 for efficiency, compensating for its lexical limitations with a keyword expansion step rather than exploring semantic vector similarity.

## Limitations
- Computational overhead of indexing topic-augmented subgraphs is non-trivial and resource-intensive
- Topic classifier reliability is not evaluated; misclassification could degrade both relevance and efficiency
- Cross-topic dependency handling is not validated; critical information may be lost for multi-topic queries

## Confidence
- **High Confidence**: ReTAG achieves significant inference time reductions (up to 90.3% on News Articles) and winning rates exceeding 50% across datasets.
- **Medium Confidence**: Hierarchical community summarization with context compression preserves global structure while fitting within LLM context windows.
- **Low Confidence**: The claim that topic augmentation improves content relevance is primarily supported by inference efficiency gains and ablation studies.

## Next Checks
1. **Topic Classifier Evaluation**: Implement and evaluate the LLM-based topic classifier on a held-out test set. Measure classification accuracy and analyze error patterns to quantify the risk of misclassification.
2. **Cross-Topic Query Analysis**: Design and execute experiments with queries explicitly spanning multiple topics. Compare ReTAG's performance with and without topic augmentation to assess information loss.
3. **Prominence Heuristic Validation**: Conduct ablation studies varying the context truncation method. Compare ReTAG's performance using prominence-based truncation against alternative importance metrics (e.g., frequency-based, LLM-scored).