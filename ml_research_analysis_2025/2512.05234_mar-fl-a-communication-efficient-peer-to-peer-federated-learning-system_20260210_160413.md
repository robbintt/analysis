---
ver: rpa2
title: 'MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System'
arxiv_id: '2512.05234'
source_url: https://arxiv.org/abs/2512.05234
tags:
- uni00000013
- uni00000008
- uni00000003
- uni00000014
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MAR-FL, a peer-to-peer federated learning\
  \ system that reduces communication overhead by using iterative group-based aggregation.\
  \ Instead of all-to-all peer communication, MAR-FL groups peers into smaller clusters\
  \ that exchange model updates in rounds, achieving O(N log N) communication complexity\
  \ versus O(N\xB2) for baselines."
---

# MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System

## Quick Facts
- arXiv ID: 2512.05234
- Source URL: https://arxiv.org/abs/2512.05234
- Reference count: 27
- This paper introduces MAR-FL, a peer-to-peer federated learning system that reduces communication overhead by using iterative group-based aggregation.

## Executive Summary
MAR-FL introduces a peer-to-peer federated learning system that dramatically reduces communication overhead through iterative group-based aggregation. Instead of requiring all-to-all peer communication, MAR-FL organizes peers into small clusters that exchange model updates in rounds, achieving O(N log N) communication complexity versus O(N²) for baselines. Experiments on MNIST and 20NG datasets demonstrate that MAR-FL matches the model accuracy of FedAvg, RDFL, and AR-FL while using up to 10× less communication. The system also supports optional knowledge distillation for faster convergence and integrates differential privacy without degrading performance, making it practical for distributed learning in bandwidth-limited wireless networks.

## Method Summary
MAR-FL is a peer-to-peer federated learning system that replaces traditional all-to-all communication with iterative group-based averaging. Peers organize into small groups of size M and exchange model updates over G rounds, with group membership dynamically regenerated each round through deterministic key updates. The system uses Momentum-SGD with local training, exchanges both model weights and momentum vectors within groups, and coordinates through a Kademlia DHT for peer discovery. Optional MKD (Moshpit Knowledge Distillation) accelerates convergence by exchanging logits instead of weights in early iterations. The architecture is robust to peer churn since failures only affect specific groups rather than the entire network, and it can integrate differential privacy with adaptive clipping.

## Key Results
- Achieves O(N log N) communication complexity versus O(N²) for baselines like RDFL and AR-FL
- Matches model accuracy of FedAvg, RDFL, and AR-FL while using up to 10× less communication
- Maintains robustness to peer churn with minimal accuracy degradation under 20% dropout rates
- Reduces communication volume by over 2× when using MKD for faster convergence

## Why This Works (Mechanism)

### Mechanism 1: Iterative Group Mixing for Log-Scale Communication
- **Claim:** MAR-FL reduces peer-to-peer communication complexity from $O(N^2)$ to $O(N \log N)$ by replacing all-to-all broadcast with iterative group-based averaging.
- **Mechanism:** Peers organize into small groups of size M per round. They average models locally within the group, then deterministically regenerate group keys to shuffle membership and repeat the averaging process over G rounds.
- **Core assumption:** The deterministic key-update strategy successfully prevents peers from re-matching in subsequent rounds within the same iteration, ensuring rapid information mixing.
- **Evidence anchors:** [Abstract] "MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines."
- **Break condition:** If the group key distribution fails (e.g., due to DHT latency) and peers repeatedly form clusters with the same partners, information mixing stalls and convergence degrades.

### Mechanism 2: Churn Resilience via Localized Failure Isolation
- **Claim:** The system maintains robustness to peer dropouts because failures affect only the specific small group containing the dropped peer, rather than stalling a global ring or central server.
- **Mechanism:** Unlike Ring All-Reduce (RDFL) where a single failure breaks the chain, MAR-FL averages asynchronously within small groups.
- **Core assumption:** The remaining peers in a degraded group can successfully complete the averaging step and re-sync in future rounds without timing out.
- **Evidence anchors:** [Section 2.2] "Peer dropouts only affect a single group (i.e., a very restricted number of peers)."
- **Break condition:** If churn rates exceed a threshold where groups consistently fall below minimum size (e.g., <2 peers), the averaging logic fails to produce valid updates.

### Mechanism 3: Convergence Acceleration via Moshpit Knowledge Distillation (MKD)
- **Claim:** Optional Knowledge Distillation (KD) reduces the total communication volume required to reach target accuracy by smoothing the optimization landscape early in training.
- **Mechanism:** In early iterations, peers act as "teachers" for each other. Instead of averaging raw weights, they exchange logits (predictions) on local data.
- **Core assumption:** Peers hold sufficient local data to generate meaningful logits for distillation, even in non-IID settings (handled by top-ℓ teacher selection).
- **Evidence anchors:** [Figure 2] "With MKD, the communication load... is further reduced, as we require over 2x less communication to reach 50% accuracy."
- **Break condition:** If data heterogeneity is extreme and the teacher selection ratio ρℓ is too high, students may distill noisy or misleading "dark knowledge" from incompatible teachers.

## Foundational Learning

- **Concept: Distributed Hash Tables (DHT)**
  - **Why needed here:** MAR-FL uses a Kademlia DHT for peer discovery and group coordination.
  - **Quick check question:** Does the DHT transfer model weights or just peer presence metadata?

- **Concept: All-Reduce Algorithms (Ring vs. Tree)**
  - **Why needed here:** The paper positions itself against Ring All-Reduce (RDFL).
  - **Quick check question:** Why does a Ring topology fail if one node disconnects, but a Group-based topology survives?

- **Concept: Momentum SGD**
  - **Why needed here:** The system averages not just model weights (θ) but also momentum vectors (m).
  - **Quick check question:** What two vectors does a peer exchange during the aggregation phase?

## Architecture Onboarding

- **Component map:** Peer Process -> DHT Node -> MAR Aggregator -> MKD Module (Optional)
- **Critical path:** 1. Peer computes local update → 2. Peer publishes key to DHT → 3. Peer discovers group members via DHT → 4. Peer exchanges model states with group → 5. Peer updates group key for next round
- **Design tradeoffs:**
  - Exact vs. Approximate Averaging: Configuring group size M and rounds d to perfectly fit N (e.g., 125=5³) yields exact averaging. Mismatched parameters yield faster but approximate convergence.
  - Memory vs. Communication: Transferring momentum (m) alongside weights (θ) doubles communication payload but is necessary for convergence stability.
- **Failure signatures:**
  - DHT Timeout: Peer cannot find a group; stalls indefinitely.
  - Stale Peers: Peer publishes key but crashes before exchange; group forms with "ghost" member, causing timeout or incomplete average.
  - Key Collision: Peers fail to shuffle keys correctly, re-grouping with previous partners, causing mixing stagnation.
- **First 3 experiments:**
  1. Baseline Scaling: Run MAR-FL with 16, 64, and 125 peers on MNIST. Verify communication cost grows roughly N log N rather than N².
  2. Churn Injection: Run a 64-peer simulation and force a 20% dropout rate during the aggregation phase. Confirm the system continues to converge (accuracy curve should be noisy but trending up).
  3. MKD Ablation: Run 20NG with and without MKD enabled. Plot communication volume vs. accuracy to verify the 2× efficiency gain claimed in Figure 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the interaction between group-based aggregation and momentum updates affect differential privacy guarantees and noise calibration in MAR-FL?
- **Basis in paper:** [explicit] The limitations section states: "We offer a starting point for using DP with MAR-FL but analyzing the impact of group-based aggregation in combination with momentum on DP dynamics remains open."
- **Why unresolved:** The paper adapts DP-FedAvg with adaptive clipping to the decentralized setting but does not analyze how the iterative group averaging interacts with momentum terms during noise propagation