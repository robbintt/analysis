---
ver: rpa2
title: Iterative refinement, not training objective, makes HuBERT behave differently
  from wav2vec 2.0
arxiv_id: '2508.08110'
source_url: https://arxiv.org/abs/2508.08110
tags:
- hubert
- training
- wav2vec
- speech
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of training objectives and iterative
  refinement on the linguistic information encoded in self-supervised speech representations.
  Comparing HuBERT and wav2vec 2.0 models, the research finds that iterative refinement,
  rather than the choice of training objective (contrastive vs.
---

# Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0

## Quick Facts
- arXiv ID: 2508.08110
- Source URL: https://arxiv.org/abs/2508.08110
- Authors: Robin Huo; Ewan Dunbar
- Reference count: 0
- Key outcome: Iterative refinement, not training objective, drives differences in linguistic encoding between HuBERT and wav2vec 2.0 models

## Executive Summary
This study investigates whether the choice of training objective (contrastive vs. predictive) or iterative refinement of pseudo-labels explains differences in linguistic information encoding between HuBERT and wav2vec 2.0 models. Through systematic ablation experiments comparing HuBERT with a contrastive variant (cHuBERT), the research demonstrates that iterative refinement—rather than the training objective—is responsible for improved phoneme and word encoding in final layers while decreasing speaker identity correlation. The findings suggest that the frequency of pseudo-label updates is a critical architectural parameter for designing self-supervised speech models that capture linguistic structure.

## Method Summary
The paper compares HuBERT and wav2vec 2.0 models through iterative refinement experiments on LibriSpeech-960h. Models are trained for 250k steps per iteration with pseudo-labels generated by k-means clustering on either MFCCs (iteration 1) or intermediate layer representations (iterations 2+). A contrastive variant (cHuBERT) replaces HuBERT's predictive loss with wav2vec 2.0's contrastive loss. Canonical Correlation Analysis (CCA) measures layerwise correlation to phoneme identity, word identity, and speaker identity. The key comparison is between models trained with different objectives but identical iterative refinement schedules.

## Key Results
- Iterative refinement increases layerwise CCA correlation to phonemes and words while decreasing correlation to speaker identity
- Training objective (contrastive vs. predictive) does not explain differences in linguistic encoding patterns
- First-iteration models show decreasing linguistic correlation in final layers, while second-iteration models maintain or increase it
- Speaker information in final layers degrades with iteration, suggesting a trade-off with linguistic abstraction

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement of pseudo-labels—not training objective—improves linguistic abstraction in final layers. Each iteration uses clustered representations from the previous iteration as pseudo-labels, allowing intermediate layers to operationalize abstractions aligned with these targets. This progressively amplifies linguistically-relevant structure while attenuating irrelevant signal. The effect holds even when controlling for total training time, suggesting that pseudo-label update frequency is the critical factor.

### Mechanism 2
Slower pseudo-label update rates may reduce "moving target" interference during abstraction learning. Wav2vec 2.0 updates pseudo-labels at every parameter update (online quantization), potentially creating unstable targets that hinder stable abstraction learning. HuBERT freezes pseudo-labels for entire training passes, allowing the model to learn stable abstractions before targets refresh. This suggests that joint learning of pseudo-labels and representations creates optimization instability.

### Mechanism 3
Increased linguistic encoding across iterations trades off against speaker information in final layers. MFCC-based first-iteration pseudo-labels already de-emphasize speaker information (lower-order coefficients retained), and single-speaker training sequences prevent cross-speaker attention. Iterative refinement amplifies this bias, progressively backgrounding speaker identity to favor linguistic targets. This trade-off suggests speaker information is not useful for the masked prediction task given the training data structure.

## Foundational Learning

- **Masked prediction with pseudo-labels**: Both HuBERT and wav2vec 2.0 learn by predicting pseudo-labels for masked frames rather than raw audio. Quick check: Can you explain why pseudo-labels are needed rather than predicting raw audio directly?

- **k-means clustering for discrete unit discovery**: HuBERT generates pseudo-labels by clustering acoustic features (MFCCs) or learned representations. Quick check: What happens to downstream performance if k (number of clusters) is set too low or too high?

- **Canonical Correlation Analysis (CCA)**: The paper measures representation quality via projection-weighted CCA between hidden states and linguistic/speaker categories. Quick check: Why is CCA preferred over simple accuracy for probing learned representations?

## Architecture Onboarding

- Component map: Audio waveform -> 7-layer Conv encoder -> 12-layer Transformer -> Projection -> Masked frame prediction -> Loss against pseudo-labels -> [Iteration boundary] -> k-means on layer-K -> New pseudo-labels

- Critical path:
  1. Pre-train iteration 1 with MFCC-clustered pseudo-labels (k=100)
  2. Extract representations from layer 6, cluster with k=500
  3. Re-initialize model, pre-train iteration 2 with new pseudo-labels
  4. Repeat for iteration 3 (layer 9, k=500)

- Design tradeoffs:
  - **Cold start vs. warm start**: Re-initializing each iteration may allow better abstraction (unburdened by stale targets) but discards learned weights
  - **Cluster source layer**: Earlier layers retain more acoustic detail; later layers more abstract. Paper uses layer 6→9 progression
  - **Update frequency spectrum**: wav2vec 2.0 (every update) ←→ HuBERT (every 250k updates). Optimal point unknown

- Failure signatures:
  - Linguistic CCA dropping in final layers → likely first-iteration model or online quantization
  - High speaker CCA in final layers → insufficient iteration or speaker-agnostic pseudo-labels needed
  - No improvement across iterations → check pseudo-label quality (cluster purity, linguistic alignment)

- First 3 experiments:
  1. **Ablation**: Train cHuBERT (contrastive loss) for 2 iterations vs. HuBERT for 2 iterations → should show similar CCA patterns, confirming objective doesn't matter
  2. **Duration control**: Train wav2vec 2.0 for 500k updates (double time) → should NOT match iteration-2 HuBERT, confirming effect isn't cumulative training
  3. **Iteration scaling**: Train iteration 3 HuBERT → verify word CCA continues increasing, speaker CCA continues decreasing

## Open Questions the Paper Calls Out

### Open Question 1
What is the mechanistic explanation for why iterative refinement improves linguistic encoding while suppressing speaker information? The paper establishes correlation between refinement and linguistic abstraction but lacks causal analysis of the learning dynamics. What evidence would resolve it: A causal analysis of the learning dynamics, such as tracing how specific phonetic features survive clustering iterations while speaker features attenuate, or layer-wise ablation studies during the refinement process.

### Open Question 2
How does the specific frequency of pseudo-label updates affect the trade-off between linguistic abstraction and speaker information retention? The study compares only two extremes: wav2vec 2.0 (updating every step) and HuBERT (updating after 250k steps). What evidence would resolve it: Experiments training models with intermediate label update frequencies (e.g., every 10k, 50k steps) to plot the relationship between update rate and CCA scores for phonemes versus speakers.

### Open Question 3
Is the "cold start" (weight re-initialization) necessary for the benefits of iterative refinement, or can similar results be achieved through continued training with updated labels? The experimental design confounds the update of pseudo-labels with the re-initialization of model weights. What evidence would resolve it: An ablation study comparing standard iterative refinement against a setup where pseudo-labels are updated but model weights are carried over (warm start) to isolate the effect of re-initialization.

## Limitations

- The ablation experiment comparing cHuBERT and HuBERT is theoretically strong but not empirically validated in the paper itself, leaving a gap between theoretical claim and experimental evidence
- Computational intensity of iteration-3 experiments creates uncertainty about whether observed trends would continue or plateau
- The mechanism linking pseudo-label update frequency to learning stability is hypothesized but not directly tested with intermediate update frequencies

## Confidence

- **High confidence**: The empirical observation that iterative refinement increases phoneme/word CCA while decreasing speaker CCA in final layers
- **Medium confidence**: The claim that training objective (contrastive vs. predictive) does not explain CCA differences
- **Medium confidence**: The mechanism that slower pseudo-label updates improve abstraction learning

## Next Checks

1. **Direct ablation**: Train cHuBERT for two iterations and compare layer-wise CCA patterns to HuBERT-2 to provide direct evidence that the training objective is not the causal factor
2. **Update frequency sweep**: Train wav2vec 2.0 variants with different pseudo-label update intervals (e.g., every 50k, 100k, 250k steps) to identify the optimal frequency and test whether slow updates specifically improve final-layer linguistic encoding
3. **Speaker-aware training**: Modify the training regime to include cross-speaker attention or multi-speaker sequences to test whether the speaker-linguistic trade-off is fundamental or an artifact of the single-speaker training setup