---
ver: rpa2
title: 'MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust
  Multimodal Affective Computing'
arxiv_id: '2602.00811'
source_url: https://arxiv.org/abs/2602.00811
tags:
- missing
- multimodal
- modality
- conference
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MissMAC-Bench, a benchmark designed to evaluate
  the "missing modality issue" in multimodal affective computing (MAC), where models
  struggle when one or more input modalities (text, audio, vision) are absent. The
  benchmark addresses limitations in prior work by establishing two principles: no
  missing modality priors during training, and using a single model for both complete
  and incomplete inputs.'
---

# MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing

## Quick Facts
- **arXiv ID:** 2602.00811
- **Source URL:** https://arxiv.org/abs/2602.00811
- **Reference count:** 40
- **Primary result:** Benchmark reveals larger language models amplify modality imbalance rather than improving robustness to missing modalities in multimodal affective computing.

## Executive Summary
This paper introduces MissMAC-Bench, a comprehensive benchmark designed to evaluate models' robustness when one or more input modalities (text, audio, vision) are absent in multimodal affective computing tasks. The benchmark addresses critical limitations in prior work by establishing two guiding principles: no missing modality priors during training, and using a single unified model for both complete and incomplete inputs. Through extensive experiments across two subtasks (multimodal sentiment analysis and emotion recognition), four datasets, and three language models, the authors demonstrate that larger language models do not consistently improve robustness to missing modalities and can worsen performance gaps when text is absent. The benchmark provides standardized protocols for fixed and random missing scenarios at dataset and instance levels, revealing that adaptation-based and generation-based methods perform best under random missing, while alignment-based methods show superior balance in fixed missing conditions.

## Method Summary
The benchmark evaluates missing modality robustness in multimodal affective computing using four datasets (CMU-MOSI, CMU-MOSEI, IEMOCAP, MELD) with three modalities (text, audio, vision). Text is encoded using BERT-base, sBERT-base, or DeBERTaV3-large, while acoustic and visual features are extracted using ImageBind. The evaluation follows two core principles: (1) no missing-modality simulation during training to avoid prior bias, and (2) a single unified model capable of handling both complete and incomplete modality scenarios. The benchmark defines fixed missing protocols (removing specific modalities) and random missing protocols at dataset-level (Missing Rate: MR=0.1-0.7) and instance-level (Missing Probability: MP=0.1-1.0). Performance is measured using task-specific metrics (Acc7, F1, MAE, Corr for sentiment; Acc, weighted F1 for emotion) plus C&R dimensions (Competence and Resilience).

## Key Results
- Larger language models (e.g., DeBERTa) do not consistently improve robustness to missing modalities and can worsen modality imbalance when text is absent
- Adaptation-based and generation-based methods perform best under random missing scenarios, while alignment-based methods show superior balance in fixed missing conditions
- The benchmark reveals significant performance gaps between language-present and language-absent conditions, particularly with larger language models
- A single unified evaluation protocol without missing priors during training provides more realistic assessment of real-world robustness

## Why This Works (Mechanism)

### Mechanism 1: Modality Imbalance Amplification
Scaling language model size increases the model's reliance on textual features (dominant modality). When text is present, this yields gains; when text is missing, the undertrained acoustic/visual pathways cannot compensate, leading to larger performance disparities between language-present and language-absent conditions. This assumes multimodal affective computing tasks rely heavily on textual semantic content, and cross-modal synergy is not sufficiently learned when the text encoder capacity dominates.

### Mechanism 2: Unified Evaluation Without Missing Priors
By not exposing the model to any specific missing rate or mask distribution during training, the model cannot overfit to particular patterns. At inference, a single set of pretrained weights must handle both complete and incomplete inputs, forcing the model to rely on learned cross-modal synergies rather than dataset-specific priors. This assumes real-world missing patterns are unknown and variable; prior exposure to specific patterns creates undesirable bias and reduces robustness.

### Mechanism 3: Method-Type Suitability by Missing Protocol
Adaptation-based methods (e.g., prompt tuning, MoE) dynamically adjust model parameters/weights to handle varying missing patterns, providing flexibility under randomness. Generation-based methods (e.g., VAEs, diffusion models) synthesize missing modalities, directly filling information gaps, which aids under severe random missing. Alignment-based methods (e.g., CCA, contrastive learning) transfer shared/invariant features across modalities, which helps when specific modalities are consistently missing (fixed patterns), but they struggle with severe random scenarios due to static fusion representations.

## Foundational Learning

- **Concept: Multimodal Affective Computing (MAC)**
  - **Why needed here:** MissMAC-Bench targets MAC tasks (sentiment analysis, emotion recognition) where text, audio, and video modalities are fused to predict affective states.
  - **Quick check:** Can you name at least two modalities used in MAC and one regression and one classification subtask?

- **Concept: Missing Modality Issue**
  - **Why needed here:** The central problem the benchmark addresses; models degrade when modalities are absent due to sensor failure, occlusion, etc.
  - **Quick check:** What is the difference between intra-modal (noisy/partial) and inter-modal (entire modality missing) robustness, and which does MissMAC focus on?

- **Concept: Fixed vs. Random Missing Protocols**
  - **Why needed here:** The benchmark defines these protocols to simulate different real-world scenarios; understanding them is crucial for interpreting results.
  - **Quick check:** In the random missing protocol, what is the difference between dataset-level (Missing Rate, MR) and instance-level (Missing Probability, MP) evaluation?

## Architecture Onboarding

- **Component map:** Raw multimodal inputs (text, audio, vision) → Feature extractors (BERT/ImageBind) → Fusion module f(·) → Missing handling layer → Prediction head → C&R evaluation

- **Critical path:**
  1. Preprocess multimodal data using standardized feature extractors (BERT/sBERT/DeBERTa for text, ImageBind for audio/visual)
  2. Initialize or load a pretrained MAC model with a fusion module
  3. Train the model on complete data only, adhering to the no missing prior principle
  4. Evaluate using the single trained model across complete, fixed missing (text-only, audio+visual), and random missing patterns (MR=0.7, MP=1.0)
  5. Compute C&R metrics: Competence (mean performance) and Resilience (std deviation of performance)

- **Design tradeoffs:**
  - Alignment-based vs. Generation-based vs. Adaptation-based: Alignment offers better balance in fixed missing but struggles in severe random missing. Generation provides strong imputation but has high computational cost. Adaptation offers flexibility for random missing but may overfit to dominant modalities (e.g., text).
  - Language Model Choice: Larger models (DeBERTa) boost complete-modality performance but exacerbate imbalance; smaller models (BERT) may yield more robust, though lower, overall performance.
  - Evaluation Protocol Complexity: Instance-level evaluation (MP) is more realistic but computationally intensive due to per-sample missing patterns; dataset-level (MR) is simpler but less fine-grained.

- **Failure signatures:**
  - Modality Collapse: Model heavily relies on one modality (usually text); performance collapses when that modality is missing (indicated by large performance gaps in fixed missing results).
  - Prior Overfitting: If missing patterns are leaked during training, the model shows high performance only on matching inference patterns and poor generalization otherwise.
  - High Resilience Variance: Large standard deviation across missing scenarios indicates unstable robustness; models may excel in some conditions but fail catastrophically in others.

- **First 3 experiments:**
  1. Baseline Reproduction: Implement a simple multimodal fusion model (e.g., concatenation + MLP) and evaluate it under complete, fixed missing (text-only, audio+visual), and random missing (MR=0.7, MP=1.0) protocols using BERT-base features.
  2. Method Family Comparison: Select one representative model from each of the four method families (e.g., ALMT for data augmentation, MissModal for alignment, IMDer for generation, LNLN for adaptation). Evaluate them using DeBERTaV3 features across all protocols to validate the paper's claims about method-type suitability.
  3. Language Model Scaling Analysis: Take the best-performing method from experiment 2 and re-evaluate it using BERT-base, sBERT-base, and DeBERTaV3-large features. Compare complete-modality performance vs. performance under fixed and random missing to empirically verify the modality imbalance amplification mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can large language model (LLM) architectures be adapted to mitigate the modality imbalance that worsens as model size increases?
- **Basis in paper:** Appendix D.1 states that "research on addressing missing modality issue in the era of large language models still requires further investigation," noting that larger models like DeBERTa increase reliance on text.
- **Why unresolved:** The experiments show that while larger LMs improve complete-data performance, they exacerbate the performance gap between text-present and text-absent scenarios (Section 6.3).
- **What evidence would resolve it:** A training methodology or architecture that allows a large LM (e.g., DeBERTaV3) to maintain high performance on the {a, v} (text-missing) fixed protocol comparable to smaller models.

### Open Question 2
- **Question:** Is it possible to unify alignment-based and adaptation-based approaches to achieve superior performance across both fixed and random missing protocols?
- **Basis in paper:** The results in Section 6.3 and 6.4 show a trade-off: alignment methods excel in fixed missing scenarios, while adaptation methods perform best in random scenarios.
- **Why unresolved:** The benchmark reveals that current methods specialize in specific types of missing uncertainty; no single method dominates the C&R (Competence & Resilience) dimensions across all evaluation levels.
- **What evidence would resolve it:** A novel model that statistically outperforms the current leaders (e.g., MissModal for fixed, LNLN for random) on the aggregate C&R dimension visualization (Figure 5).

### Open Question 3
- **Question:** To what extent do current "inter-modal" robust methods fail when subjected to "intra-modal" missing scenarios (e.g., temporal occlusion or noise within a modality)?
- **Basis in paper:** Section 1 explicitly distinguishes between intra-modal and inter-modal missing but limits the benchmark to the latter, stating, "in this paper, we focus on scenarios with whole modality missing."
- **Why unresolved:** The paper defines intra-modal robustness as a distinct difficulty level (Figure 2) but leaves the evaluation of methods against partial data corruption/noise as an unexplored area within this specific benchmark framework.
- **What evidence would resolve it:** An extension of the MissMAC-Bench protocols to include partial feature masking, testing if top-performing methods (like IMDer or LNLN) retain their Competence scores.

## Limitations

- The benchmark's evaluation relies heavily on pretrained feature extractors (ImageBind for audio/visual, various BERT variants for text), introducing potential variability based on feature quality.
- The claim that larger language models worsen modality imbalance assumes text dominance is inherent to MAC tasks rather than a consequence of training methodology or dataset characteristics.
- The absence of ablation studies on fusion architectures limits understanding of whether modality imbalance is primarily driven by encoder size versus fusion strategy.

## Confidence

- **High Confidence:** The benchmark design principles (no missing priors, unified model evaluation) are well-founded and address documented limitations in prior work. The distinction between fixed and random missing protocols provides valuable experimental granularity.
- **Medium Confidence:** The mechanism explaining why larger language models increase modality imbalance is plausible but requires empirical validation through controlled ablation studies. The observed method-type suitability patterns (adaptation/generation vs. alignment) are supported by results but may depend on specific implementation choices.
- **Low Confidence:** Claims about cross-modal synergy learning being insufficient in current MAC models lack direct evidence. The benchmark does not address potential interactions between missing modality patterns and specific fusion architectures.

## Next Checks

1. **Controlled Language Model Scaling:** Implement the same MAC model architecture using identical fusion mechanisms but systematically scale only the language encoder (BERT-base → sBERT-base → DeBERTa-large). Measure modality-specific performance gaps to isolate the effect of encoder size from other architectural factors.

2. **Cross-Architecture Generalization:** Replicate key experiments using different fusion architectures (attention-based, graph-based, concatenation-based) to determine whether modality imbalance amplification is consistent across architectural families or specific to certain designs.

3. **Domain Transfer Validation:** Evaluate benchmark models on out-of-domain multimodal datasets to assess whether the observed robustness patterns generalize beyond the four datasets used in the benchmark, addressing potential dataset-specific biases.