---
ver: rpa2
title: 'DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval
  and Learned Reranking'
arxiv_id: '2601.15518'
source_url: https://arxiv.org/abs/2601.15518
tags:
- retrieval
- dense
- reranking
- queries
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage retrieval system for the TREC Tip-of-the-Tongue
  (ToT) task that combines multiple retrieval methods with learned and LLM-based reranking.
  The approach uses hybrid retrieval merging LLM, sparse (BM25), and dense (BGE-M3)
  methods in the first stage, along with topic-aware multi-index dense retrieval that
  partitions Wikipedia into 24 topical domains.
---

# DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking

## Quick Facts
- arXiv ID: 2601.15518
- Source URL: https://arxiv.org/abs/2601.15518
- Reference count: 28
- Primary result: Best system achieves Recall@1000 of 0.66 and NDCG@1000 of 0.41 on test set using hybrid retrieval with Gemini-2.5-flash reranking

## Executive Summary
This paper presents a two-stage retrieval system for the TREC Tip-of-the-Tongue (ToT) task that combines multiple retrieval methods with learned and LLM-based reranking. The approach uses hybrid retrieval merging LLM, sparse (BM25), and dense (BGE-M3) methods in the first stage, along with topic-aware multi-index dense retrieval that partitions Wikipedia into 24 topical domains. In the second stage, both a trained LambdaMART reranker and LLM-based reranking are evaluated. To support training, 5,000 synthetic ToT queries are generated using LLMs. The best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating that fusion retrieval significantly improves performance over individual methods.

## Method Summary
The system employs a two-stage pipeline for ToT retrieval. Stage 1 uses hybrid retrieval that merges LLM-based, sparse (BM25), and dense (BGE-M3) methods through round-robin interleaving. An alternative topic-aware approach partitions Wikipedia into 24 topical domains using a classifier and searches within each domain separately. Stage 2 applies reranking using either a trained LambdaMART model with features including dense/sparse scores, pageviews, PageRank, and query length, or LLM-based listwise reranking with Gemini-2.5-flash using a sliding window approach. The LambdaMART model is trained on 5,000 synthetic ToT queries generated by LLMs to augment the training dataset, with sampling strategy including 1 golden document, 5 pseudo-relevant from top-10, and 10 irrelevant from ranks 10-100 per query.

## Key Results
- Hybrid retrieval combining LLM, sparse, and dense methods achieves 0.8302 Recall@1000 on Dev3, outperforming individual methods
- LLM-based reranking with Gemini-2.5-flash achieves best NDCG@1000 of 0.41 on test set
- Topic-aware multi-index retrieval provides computational efficiency but reduces recall@1000 from 0.5498 to 0.5096
- LambdaMART reranker improves Recall@100 on Test from 0.60 to 0.66 for sparse retrieval but significantly drops Reciprocal Rank

## Why This Works (Mechanism)

### Mechanism 1: Complementary Signal Fusion via Round-Robin Merging
Combining sparse, dense, and LLM-based retrieval methods using round-robin merging improves recall by capturing different aspects of relevance. Sparse retrieval captures exact keyword matches, dense retrieval captures semantic similarity, and LLM retrieval leverages parametric knowledge to identify entities directly. By interleaving results, the system mitigates vocabulary mismatch of sparse methods and hallucination risk of LLMs, ensuring diverse candidate coverage.

### Mechanism 2: LLM-Based Listwise Reranking for Vague Context
LLM-based listwise reranking yields strongest performance by directly comparing verbose query context against document content. Unlike pointwise scorers, listwise rerankers view a sliding window of documents simultaneously, allowing the model to disambiguate false memories and complex multi-hop reasoning in ToT queries by comparing relative relevance of candidate documents directly.

### Mechanism 3: Synthetic Data Augmentation for Learning to Rank
Training LambdaMART reranker on 5,000 synthetic ToT queries improves Recall@100, though it may compromise precision on test sets. Synthetic queries generated by LLMs simulate imperfect memory structure of real ToT queries, providing necessary volume of labeled data to train gradient-boosted trees to prioritize relevant features over noise.

## Foundational Learning

- **Concept: Hybrid/Score-based Fusion (RRF vs. Round-Robin)**
  - Why needed here: The authors use specific "round-robin" interleaving rather than weighted score summation or Reciprocal Rank Fusion. Understanding why simple interleaving works helps diagnose fusion bottlenecks.
  - Quick check question: How does round-robin merging prevent one high-scoring but noisy retriever from dominating top results compared to weighted sum fusion?

- **Concept: Listwise vs. Pointwise Reranking**
  - Why needed here: The architecture compares LambdaMART (pointwise) against LLM Listwise reranking. The performance gap highlights importance of context comparison.
  - Quick check question: Why might a listwise reranker (looking at 20 docs at once) perform better on "vague" queries than a pointwise reranker scoring docs one by one?

- **Concept: Network-Theoretic Centrality (PageRank)**
  - Why needed here: The authors construct a graph (PageRank, In-Degree) as features. Understanding why popularity is weak signal for ToT items is critical for future graph-based attempts.
  - Quick check question: Why might "popularity" (PageRank) be weak signal for a "tip-of-the-tongue" item if user is trying to recall specific, potentially obscure entity?

## Architecture Onboarding

- **Component map:** LLM Retriever -> Dense Retriever (BGE-M3) -> Sparse Retriever (BM25) -> Round-Robin Fusion -> Reranker (LambdaMART/LLM)
- **Critical path:** The LLM Reranker (Gemini-2.5-flash) is identified as best performer. Engineering focus should be on context window management (sliding window size 20, stride 10) for the LLM reranker.
- **Design tradeoffs:**
  - Topic-Aware Routing: Partitioning Wikipedia into 24 indexes reduced search space but lowered Recall@1000 (0.5096 vs 0.5498 baseline). Do not use for maximum recall; use for latency-constrained applications.
  - LambdaMART: Offers computational efficiency but suffers from poor generalization (RR drop). Use only as high-recall filter or fallback if LLM reranking too expensive.
- **Failure signatures:**
  - LambdaMART Generalization: Model improves Recall@100 but drastically hurts Reciprocal Rank. Manifests as "Relevant doc found at rank 400 instead of 1."
  - Topic Routing Misclassification: If query is ambiguous, classifier might route to wrong topic index, guaranteeing retrieval failure (0 recall).
- **First 3 experiments:**
  1. Fusion Ablation: Compare Round-Robin vs. Reciprocal Rank Fusion (RRF) on Hybrid Retrieval set to validate paper's choice of simple interleaving.
  2. Reranking Context Window Sweep: Test LLM Reranking with "Title Only" vs. "First Paragraph" context to quantify trade-off between inference cost and NDCG lift.
  3. LambdaMART Feature Ablation: Retrain LambdaMART removing PageRank/Graph features (which paper notes had low importance) to verify if model simplifies without performance loss.

## Open Questions the Paper Calls Out

### Open Question 1
Can retrieval expansion via breadth-first search (BFS) on the composite Wikipedia graph successfully improve recall for tip-of-the-tongue queries? The authors constructed the composite graph but utilized it only for centrality features (PageRank) in reranking. They identified that direct popularity bias was ineffective but have not yet tested if structural proximity (BFS) can aid retrieval. An ablation study showing that expanding initial candidate set with graph neighbors increases Recall@1000 without introducing excessive noise would resolve this.

### Open Question 2
Does applying LLM-based reranking to the topic-aware multi-index retrieval pipeline fully recover the recall lost by partitioning the corpus? The topic-aware approach reduced recall@1000 (0.5096 vs 0.5498) but the paper hypothesizes that "the addition of a reranker could significantly boost these results while still maintaining the computational advantages." The top_model_dense run submitted to TREC represented first-stage retrieval only. End-to-end performance metrics of the top_model_dense system when followed by the Gemini-2.5-flash reranker, compared against full-corpus hybrid baseline, would resolve this.

### Open Question 3
Why does the trained LambdaMART model exhibit a severe drop in Reciprocal Rank (RR) on the test set despite improving recall, and how can this generalization failure be mitigated? The paper identifies the symptom (relevant documents are surfaced but displaced from top ranks) but does not determine if the cause is overfitting to synthetic query features, distribution shift in NIST topics, or the pseudo-relevance labeling strategy. An analysis of feature importance shifts between dev and test sets, or experiments using different sampling strategies for negative/pseudo-relevant documents in training data, would resolve this.

## Limitations
- Synthetic query generation process relies on manual prompt engineering that may not generalize across different LLM backends
- Round-robin fusion strategy lacks comparison against more sophisticated weighted fusion methods
- LambdaMART's recall gains come at significant precision cost on test sets, suggesting limited real-world transferability
- Topic-aware routing approach reduces recall by 7.8% despite computational benefits

## Confidence

- **Hybrid Retrieval Fusion (High):** 6.6% recall improvement over individual methods is well-supported by ablation results and aligns with established RAG fusion literature.
- **LLM Listwise Reranking (High):** 68% Reciprocal Rank improvement and best NDCG@1000 scores are directly demonstrated with clear mechanism explanation.
- **Synthetic Data Augmentation (Medium):** While synthetic queries show high correlation with originals, recall gains come at significant precision cost on test sets, suggesting limited real-world transferability.
- **Topic-Aware Routing (Low):** Despite computational benefits, 7.8% recall drop makes this approach unsuitable for primary retrieval task, though potentially valuable for efficiency-constrained applications.

## Next Checks

1. **Fusion Method Comparison:** Re-run hybrid retrieval with Reciprocal Rank Fusion (RRF) against round-robin to quantify cost-benefit of simplicity versus weighted score fusion.
2. **Reranker Context Sensitivity:** Systematically vary document context provided to Gemini-2.5-flash (title-only vs first paragraph vs full text) to determine optimal trade-off between inference cost and ranking quality.
3. **Synthetic Data Generalization:** Evaluate LambdaMART trained on synthetic queries versus human-annotated queries (if available) on same test set to measure transfer gap and identify specific synthetic query characteristics causing overfitting.