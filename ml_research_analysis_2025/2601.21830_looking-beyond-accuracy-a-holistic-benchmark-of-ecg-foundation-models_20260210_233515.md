---
ver: rpa2
title: 'Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models'
arxiv_id: '2601.21830'
source_url: https://arxiv.org/abs/2601.21830
tags:
- embedding
- performance
- dataset
- datasets
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for comprehensive benchmarking of
  ECG Foundation Models (FMs) beyond standard downstream performance evaluation. It
  proposes a holistic methodology combining performance metrics (F1 score) with representation-level
  analysis using SHAP and UMAP techniques to assess feature importance, embedding
  space geometry, and cross-dataset generalizability.
---

# Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models

## Quick Facts
- arXiv ID: 2601.21830
- Source URL: https://arxiv.org/abs/2601.21830
- Reference count: 15
- Key outcome: Proposes a holistic benchmarking framework combining performance metrics with representation-level analysis (SHAP, UMAP) to assess ECG Foundation Models' generalization and feature informativeness

## Executive Summary
This paper addresses the need for comprehensive benchmarking of ECG Foundation Models (FMs) beyond standard downstream performance evaluation. It proposes a holistic methodology combining performance metrics (F1 score) with representation-level analysis using SHAP and UMAP techniques to assess feature importance, embedding space geometry, and cross-dataset generalizability. The study benchmarks four ECG-expert FMs (ECG-FM, ECGFounder, HuBERT-ECG variants, and ECG-JEPA) across multiple geographically diverse datasets and data scarcity settings. Key results show that ECG-FM and ECGFounder consistently achieve higher classification performance and better cross-dataset feature overlap, indicating stronger generalization. Representation analysis reveals that these models produce embeddings with higher label-level separability and lower dataset-level bias, suggesting more clinically aligned features.

## Method Summary
The methodology evaluates ECG FMs through two complementary lenses: performance-based evaluation using 15-fold cross-validated F1 scores from linear probes, and representation-level analysis using SHAP feature importance rankings and UMAP visualization of embedding geometry. Four datasets (PTB-XL, CODE-15%, Georgia, CHN) are evaluated across two tasks (Conduction Disturbance, Atrial Fibrillation) with data scarcity settings (XS to L). Five linear classifiers (XGBoost, Decision Tree, Random Forest, Logistic Regression, MLP) are used to probe frozen embeddings extracted from four FMs (ECG-FM, ECGFounder, HuBERT-ECG variants, ECG-JEPA). The representation analysis measures cross-dataset feature overlap (top-50 SHAP-ranked features), label-level separability (kNN@10, ARI), and dataset-level separability to assess clinical alignment and generalization.

## Key Results
- ECG-FM and ECGFounder achieve higher classification performance (F1 scores) and better cross-dataset feature overlap (58.7-71.7% shared features) compared to HuBERT-ECG (19.8-47.9%) and ECG-JEPA
- ECG-FM and ECGFounder produce embeddings with higher label-level ARI (0.23-0.70) and lower dataset-level ARI, indicating better clinical feature discrimination and less dataset-specific bias
- HuBERT-ECG and ECG-JEPA retain more dataset-specific information (high dataset-level ARI of 0.42-1.00) with poor label separation (ARI of 0.00-0.07), suggesting less clinically aligned representations
- Performance trends persist across data scarcity settings, with ECG-FM and ECGFounder showing stronger generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representational structure analysis exposes generalization capabilities that performance metrics alone cannot reveal
- **Mechanism:** The framework evaluates embeddings through two complementary lenses: label-level separability via kNN and ARI metrics measures how well embeddings discriminate clinical classes, while dataset-level separability measures whether embeddings encode domain-specific artifacts. Models that minimize dataset-level while maximizing label-level separability produce more clinically aligned, transferable representations
- **Core assumption:** Frozen embeddings from pretraining reflect the FM's intrinsic representational quality; finetuning would confound this assessment
- **Evidence anchors:** [abstract] "introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques"; [Section 3.4] "performance shows what embeddings achieve, whereas representation analysis reveals how they are built"

### Mechanism 2
- **Claim:** Cross-dataset feature overlap predicts downstream generalization in data-scarce settings
- **Mechanism:** Extract top-50 SHAP-ranked features from the highest-performing dataset, then measure overlap when the same classifier ranks features on other datasets. Higher overlap indicates the FM encodes class-informative rather than dataset-specific patterns
- **Core assumption:** SHAP importance rankings on linear probes approximate what the FM itself attends to; this proxy remains valid under dataset shift
- **Evidence anchors:** [Section 3.4] "observing a greater overlap of influential features across the considered datasets suggests stronger generalization capabilities"; [Figure 1] ECG-FM and ECGFounder show 58.7% and 71.7% shared features respectively vs. 19.8-47.9% for HuBERT-ECG variants

### Mechanism 3
- **Claim:** Pretraining strategy determines whether embeddings encode clinically meaningful structure or dataset artifacts
- **Mechanism:** Contrastive + masking objectives (ECG-FM) and multi-label classification (ECGFounder) preserve fine-grained signal information, yielding high label-level ARI (0.23-0.70). Discretized embedding spaces via k-means clustering (HuBERT-ECG) or prediction in embedding space alone (ECG-JEPA) may discard subtle discriminative details, leading to high dataset-level ARI (0.42-1.00) and poor label separation
- **Core assumption:** Architectural backbone (CNN, Transformer, hybrid) is less determinative than pretraining objective for embedding quality
- **Evidence anchors:** [Section 3.2] ECG-FM uses "contrastive learning and masking strategies"; HuBERT-ECG uses "self-supervised label induction via k-means clustering"; [Table 4] ECG-JEPA achieves dataset-level ARI of 0.98-1.00 but label-level ARI of only 0.00-0.07

## Foundational Learning

- **Concept:** Foundation Models as frozen embedding extractors
  - **Why needed here:** The entire benchmark assumes embeddings are evaluated "zero-shot" without finetuning; misunderstanding this conflates probe quality with FM quality
  - **Quick check question:** If you finetune the encoder weights during probing, what does the resulting SHAP analysis actually measure?

- **Concept:** Self-supervised pretraining objectives (contrastive, masking, JEPA)
  - **Why needed here:** The paper attributes performance differences to pretraining strategy rather than architecture size; understanding these objectives is prerequisite to interpreting results
  - **Quick check question:** Why might predicting masked tokens in embedding space (JEPA) discard more low-level signal detail than predicting in raw input space?

- **Concept:** Dimensionality reduction for qualitative inspection (UMAP) vs. quantitative cluster metrics (kNN, ARI)
  - **Why needed here:** UMAP is explicitly described as hyperparameter-sensitive and used only for visualization; quantitative metrics drive conclusions
  - **Quick check question:** If UMAP shows clean cluster separation but kNN@10 agreement is low, which signal should you trust and why?

## Architecture Onboarding

- **Component map:** Preprocessing layer -> Frozen FM encoders -> Linear probing with 5 classifiers -> Evaluation layer (performance + representation metrics)
- **Critical path:** Preprocessing → embedding extraction → best-classifier selection per FM → SHAP feature ranking → cross-dataset feature overlap → UMAP + quantitative geometry metrics
- **Design tradeoffs:** Last-Token vs. Max pooling (impact not reported); 50 top features (heuristic cutoff); Dataset subset sizes (enables data-scarcity analysis but reduces statistical power)
- **Failure signatures:** High dataset-level ARI with low label-level ARI → FM encodes acquisition artifacts, not clinical signal; Low cross-dataset feature overlap despite acceptable F1 → potential overfitting to training distribution
- **First 3 experiments:** 1) Reproduce F1 scores on C15 dataset to validate pipeline correctness; 2) Compute cross-dataset top-50 feature overlap for ECG-FM across all four datasets; 3) Generate UMAP visualization for ECG-JEPA embeddings to verify near-perfect dataset clustering

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do ECG Foundation Models perform on rigorously curated, out-of-distribution (OOD) datasets that are guaranteed to be excluded from their pretraining data?
- **Basis in paper:** [explicit] The authors note in the Conclusion that avoiding overlap between pretraining and benchmarking datasets was infeasible due to a lack of available OOD sources
- **Why unresolved:** Current benchmarks may report inflated generalization metrics if the test data was seen during the FM's pretraining phase, masking potential overfitting
- **What evidence would resolve it:** Evaluation results from a newly released, geographically distinct ECG dataset confirmed to be absent from all pretraining corpora

### Open Question 2
- **Question:** Do the observed trends regarding embedding geometry and feature overlap generalize to cardiac pathologies beyond Conduction Disturbance (CD) and Atrial Fibrillation (AF)?
- **Basis in paper:** [explicit] The authors explicitly state an intention to "investigate the behaviour of FMs across a broader range of cardiac pathologies" in the Conclusion
- **Why unresolved:** The study was restricted to two classes; it remains unclear if the superior separability of models like ECGFounder holds for other conditions
- **What evidence would resolve it:** Application of the proposed benchmarking methodology to a wider set of diagnostic labels to confirm the consistency of the ranking between FMs

### Open Question 3
- **Question:** To what extent is the embedding quality of ECG Foundation Models determined by model scale (parameter count) versus pretraining strategy?
- **Basis in paper:** [explicit] The Results section highlights that HuBERT-ECG performance did not improve with increased model size, leading the authors to conclude that "it is not always true that large-scale or highly parametrized FMs lead to more informative embeddings"
- **Why unresolved:** This finding contradicts standard scaling laws observed in other modalities, leaving the optimal balance of architecture size and training objective for ECGs undefined
- **What evidence would resolve it:** A controlled ablation study isolating model size as a variable while comparing distinct pretraining objectives on fixed data

### Open Question 4
- **Question:** Does the strong correlation between dataset-level separability and downstream failure persist when Foundation Models are subjected to full fine-tuning rather than linear probing?
- **Basis in paper:** [inferred] The methodology relies on "frozen embedding extractors" (linear probing) to assess raw generalization, but it is inferred that fine-tuning might allow models like ECG-JEPA to overcome their high dataset-bias
- **Why unresolved:** The study proves that dataset-level bias exists in the static embeddings, but does not test if this bias is reversible via gradient updates
- **What evidence would resolve it:** A comparison of the proposed representation metrics against performance metrics before and after end-to-end fine-tuning on the target datasets

## Limitations
- The framework assumes frozen embeddings fully capture FM representational quality without controlling for dataset overlap or encoder capacity
- SHAP-based cross-dataset feature overlap metric depends heavily on the choice of linear probe and feature ranking cutoff (top-50), which are heuristic and not validated for stability
- UMAP visualizations are acknowledged as hyperparameter-sensitive and secondary to quantitative metrics
- Preprocessing pipelines for each FM are not standardized, introducing potential confounding in embedding quality

## Confidence
- **High confidence**: Classification performance trends (F1 scores) showing ECG-FM and ECGFounder outperform HuBERT-ECG and ECG-JEPA across datasets
- **Medium confidence**: Representation-level metrics (SHAP overlap, kNN/ARI) supporting better generalization of ECG-FM and ECGFounder
- **Low confidence**: Attribution of differences solely to pretraining objectives without controlling for dataset exposure or architectural capacity

## Next Checks
1. **Feature overlap sensitivity**: Vary the SHAP feature cutoff (top-25, top-50, top-100) and assess stability of cross-dataset overlap scores across all FMs
2. **Dataset contamination test**: Identify and quantify dataset overlap between pretraining corpora and evaluation sets; re-run benchmark excluding overlapping samples
3. **Architecture ablation**: Compare embeddings from same encoder architecture trained with different objectives (e.g., contrastive vs. masked prediction) to isolate pretraining effect from architectural bias