---
ver: rpa2
title: Analysis of Zero Day Attack Detection Using MLP and XAI
arxiv_id: '2501.16638'
source_url: https://arxiv.org/abs/2501.16638
tags:
- dataset
- truncated
- classes
- weighted
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of machine learning and
  deep learning approaches, specifically multilayer perceptrons (MLPs), for detecting
  zero-day attacks using the KDD99 dataset. The study addresses the challenges of
  class imbalance and bias in intrusion detection systems (IDS) by synthesizing the
  dataset into fewer classes and employing weighted and truncated models.
---

# Analysis of Zero Day Attack Detection Using MLP and XAI

## Quick Facts
- arXiv ID: 2501.16638
- Source URL: https://arxiv.org/abs/2501.16638
- Reference count: 0
- Primary result: Truncated MLP achieves 99.62% accuracy; weighted truncated MLP achieves 97.26% accuracy with improved minority class recall.

## Executive Summary
This paper evaluates multilayer perceptron (MLP) approaches for zero-day attack detection on the KDD99 dataset, addressing class imbalance through label aggregation and weighted loss functions. The authors synthesize 23 attack classes into 4 parent categories and train four MLP variants: base, weighted, truncated, and weighted truncated. Results show that while the truncated model achieves the highest accuracy (99.62%), the weighted truncated model significantly improves recall on underrepresented classes (Unauthorized Access) from 0% to 33.68%, demonstrating better class representation despite lower overall accuracy. SHAP analysis reveals distinct feature importance patterns between weighted and unweighted models, validating the interpretability benefits of class weighting approaches.

## Method Summary
The study uses the KDD99 dataset (4.9M samples, 41 features, 23 original classes) aggregated into 4 categories to address extreme class imbalance. Four MLP variants are trained: base (62,871 parameters), weighted base, truncated (13,892 parameters), and weighted truncated. Models are trained for 20 epochs with class weights applied to weighted variants. Performance is evaluated using accuracy, precision, recall, F1-score, and confusion matrices. SHAP kernel explainer is applied to 50 random test samples for interpretability analysis, comparing feature importance between weighted and unweighted models.

## Key Results
- Truncated MLP achieves highest accuracy of 99.62% with 13,892 parameters (vs 62,871 for base)
- Weighted truncated MLP achieves 97.26% accuracy with recall improving from 0% to 33.68% on Unauthorized Access class
- Class aggregation from 23 to 4 categories enables base MLP to achieve non-zero F1 scores on all classes
- SHAP analysis reveals weighted models attend to different features than unweighted models, supporting interpretability claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating fine-grained attack labels into broader categories reduces extreme class imbalance and improves pattern recognition across minority attack types.
- Mechanism: The 23 original attack classes in KDD99 (ranging from 2 to 2.8 million samples) are grouped into 4 parent categories—DoS, Normal, Probe, and Unauthorized Access. This consolidation mitigates the dominance of the Smurf class (60% of data) and enables the MLP to learn decision boundaries that include underrepresented attack patterns.
- Core assumption: Attacks within the same parent category share sufficiently similar feature signatures that aggregation does not destroy discriminative information needed for detection.
- Evidence anchors:
  - [abstract] "synthesizing the dataset to have fewer classes for multi-class classification"
  - [section 3.2] "To reduce the class imbalance for pattern recognition and supervised machine learning algorithms, the 23 classes were further grouped into four main parent categories... This organization helps reduce the huge discrepancy between the classes."
  - [corpus] Related paper "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data" confirms that handling imbalance is critical for zero-day detection performance.
- Break condition: If the aggregated categories combine attacks with fundamentally different network behavior signatures (e.g., merging reconnaissance with exploitation), the model may lose granularity and produce false positives on benign traffic.

### Mechanism 2
- Claim: Applying class weights to the loss function shifts model attention toward minority classes, improving recall on underrepresented attack types at the cost of overall accuracy.
- Mechanism: Weighted loss assigns higher penalty for misclassifying samples from underrepresented classes (e.g., Unauthorized Access with 1,178 samples vs DoS with 3,883,370). The optimizer compensates by learning features that distinguish minority classes, rather than minimizing loss primarily through majority class accuracy.
- Core assumption: The weight assignment correctly compensates for frequency imbalance without causing overcorrection that degrades majority class performance below acceptable thresholds.
- Evidence anchors:
  - [abstract] "weighted truncated MLP model, despite lower accuracy (97.26%), demonstrates better class representation and improved unweighted recall score"
  - [section 4] "the weighted model at least produced results in all of the classes and increased the recall from 0 to 0.33" for Unauthorized Access; "weighing the model can help to make it less biased towards underrepresented classes in IDS"
  - [corpus] "A Novel Contrastive Loss for Zero-Day Network Intrusion Detection" addresses similar class imbalance issues, suggesting this is a recognized approach in the field.
- Break condition: If weights are set too aggressively, the model may overfit to noise in minority class samples, producing high recall but unacceptable precision (false positive spikes).

### Mechanism 3
- Claim: Truncating MLP architecture (fewer layers/neurons) reduces overfitting on imbalanced data while improving training efficiency, without proportionally degrading detection performance.
- Mechanism: The truncated models reduce parameters from 62,871 to 13,892 by decreasing hidden layer neurons and removing layers. Smaller capacity limits the model's ability to memorize majority class patterns, acting as implicit regularization that can improve generalization on structured tabular data like KDD99.
- Core assumption: The original base MLP was overparameterized relative to the complexity of patterns in KDD99 features, so reducing capacity does not underfit the data.
- Evidence anchors:
  - [abstract] "truncated MLP model achieves the highest accuracy of 99.62%"
  - [section 3.4] "The main advantage of the truncated models lies in their efficiency. The truncated models have 13892 parameters, whereas the base models have 62871."
  - [section 4] "the truncated model improved the validation accuracy by 0.4% and the weighted counterpart increased it by 3.2%"
  - [corpus] No direct corpus papers validate truncated MLPs for zero-day detection; this appears to be a methodological contribution specific to this work.
- Break condition: If the dataset had higher feature complexity or more classes, truncation could underfit and fail to capture necessary decision boundaries.

## Foundational Learning

- Concept: Class Imbalance and Metric Selection
  - Why needed here: The paper critiques prior work achieving 99.3% accuracy on KDD99 while ignoring that the model may simply predict the majority class. Without understanding why accuracy is misleading on imbalanced data, you cannot interpret Table II and III correctly.
  - Quick check question: If 60% of your data is class A and your model predicts only class A, what is your accuracy? What is your F1-score for class B?

- Concept: Weighted Cross-Entropy Loss
  - Why needed here: The weighted models use class-specific weights to rebalance gradient updates. Understanding how loss weighting affects optimization is essential for tuning weights or adapting this approach to new datasets.
  - Quick check question: When you multiply the loss for class B by a weight of 10x, what happens to the gradient magnitude for misclassified class B samples during backpropagation?

- Concept: SHAP (Shapley Additive Explanations) for Feature Attribution
  - Why needed here: The paper uses SHAP to demonstrate that weighted and unweighted models attend to different features. Understanding SHAP basics is required to interpret Figures 4-5 and assess whether the model is learning meaningful attack signatures.
  - Quick check question: If a feature has a SHAP value of +0.3 for a given prediction, what does that mean about the feature's contribution to that specific output?

## Architecture Onboarding

- Component map: KDD99 dataset (4.9M samples, 41 features) -> Class aggregation (23→4 classes) -> Stratified 67/33 train-test split -> Four MLP variants -> 20 epochs training -> SHAP explainer (50 samples) -> Performance evaluation

- Critical path:
  1. Class aggregation is non-optional—the base model on 23 classes achieved near-zero F1 on most classes (Table II)
  2. Choose architecture based on deployment constraints: truncated for efficiency, base for maximum capacity
  3. Choose weighting based on operational priority: unweighted for maximum accuracy (99.62%), weighted for minority class coverage (recall improves from 0 to 0.33 on Unauthorized Access)
  4. Always evaluate using confusion matrix and per-class metrics—never rely solely on aggregate accuracy

- Design tradeoffs:
  - Truncated + Unweighted: 99.62% accuracy, but 0% recall on Unauthorized Access (misses rare attacks)
  - Truncated + Weighted: 97.26% accuracy, but 33.68% recall on Unauthorized Access (catches some rare attacks)
  - Parameter efficiency: Truncated models train faster and are more deployable, with minimal accuracy loss
  - SHAP overhead: Adds inference-time cost but enables bias auditing

- Failure signatures:
  - Near-zero F1 scores on multiple classes (Table II, Base Model) → model is predicting only majority classes
  - Large accuracy gap between weighted and unweighted variants (>2%) → significant imbalance bias present
  - SHAP importance concentrated on features only relevant to one class → model has not learned generalized patterns

- First 3 experiments:
  1. **Baseline replication**: Train a base MLP on the original 23-class KDD99 labels without aggregation. Compare per-class F1 scores against the truncated 4-class model to quantify the aggregation benefit.
  2. **Weight sensitivity sweep**: Train weighted truncated MLPs with three weighting schemes—inverse frequency, square-root inverse frequency, and manually tuned. Plot accuracy vs. minority-class recall to identify the operating point that matches your security requirements.
  3. **SHAP stability check**: Train SHAP explainers on 50, 100, and 200 random test samples. Compare feature importance rankings to verify that explanations are stable and not artifacts of small sample size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an ensemble method combining truncated and weighted truncated MLPs effectively balance the trade-off between the high accuracy of truncated models and the reduced bias of weighted models?
- Basis in paper: [explicit] The conclusion states, "The two truncated and truncated weighted models have subtle differences that would have the potential for an improved system if combined."
- Why unresolved: The paper evaluates the models independently but does not architect or test a unified ensemble model.
- What evidence would resolve it: Performance metrics of a hybrid model demonstrating high accuracy while maintaining high recall for minority classes.

### Open Question 2
- Question: How do advanced ensemble techniques, such as stacking with gradient boosting or blending with adaptive weights, compare to the proposed MLP approaches?
- Basis in paper: [explicit] The authors suggest future research should "examine the potential of ensemble techniques like stacking with gradient boosting models or blending with adaptive weights."
- Why unresolved: The study was limited to four specific variations of Multilayer Perceptrons.
- What evidence would resolve it: A comparative analysis showing detection rates and computational efficiency of these advanced ensembles against the baseline MLPs.

### Open Question 3
- Question: Can hierarchical models with specialized layers for different attack types improve the detection of underrepresented classes like "Unauthorized Access"?
- Basis in paper: [explicit] The paper posits that "Investigating hierarchical models with specialized layers for different attack types could also be fruitful."
- Why unresolved: The current MLP architecture uses a flattened, fully connected approach rather than a specialized hierarchical structure.
- What evidence would resolve it: Improved recall scores for minority classes in a hierarchical model without significant loss in overall accuracy.

### Open Question 4
- Question: Do the classification improvements observed using weighted and truncated models on KDD99 generalize to modern network intrusion datasets?
- Basis in paper: [inferred] The paper relies exclusively on the KDD99 dataset, which was created in 1999, potentially limiting applicability to modern zero-day threats.
- Why unresolved: Methodological limitations restricted testing to a single, legacy dataset.
- What evidence would resolve it: Successful replication of the weighting and truncation methodology on contemporary datasets like ToN-IoT or CIC-IDS-2017.

## Limitations

- **Unknown Architecture Details**: The paper does not specify exact MLP layer configurations, activation functions, or regularization techniques, creating uncertainty in reproducing precise performance metrics.
- **SHAP Interpretation Scale**: SHAP explanations are based on only 50 samples, which may not capture full model behavior or stability across the test set.
- **No External Validation**: Results are only validated on KDD99; generalizability to other datasets or real-world zero-day attack scenarios is untested.

## Confidence

- **High Confidence**: The core finding that class imbalance in KDD99 severely biases MLPs toward majority classes, and that weighted models improve minority class recall, is well-supported by quantitative results (e.g., recall rising from 0 to 0.33 for Unauthorized Access).
- **Medium Confidence**: The truncation mechanism’s benefit (13,892 vs. 62,871 parameters) is demonstrated, but the exact architectural choices and their necessity remain unclear without replication details.
- **Low Confidence**: The SHAP-based interpretability claims (e.g., weighted vs. unweighted models attending to different features) are exploratory; the small sample size (50) and lack of statistical validation weaken confidence.

## Next Checks

1. **Architecture Replication**: Reconstruct the MLP variants using the parameter counts provided and test if truncated models consistently achieve similar accuracy gains on KDD99.
2. **Weight Sensitivity Analysis**: Systematically vary class weights (e.g., inverse frequency vs. square-root inverse) and measure the trade-off between overall accuracy and minority class recall to identify optimal weighting schemes.
3. **SHAP Stability Test**: Increase the SHAP sample size to 100–200 and compare feature importance rankings to assess whether the reported interpretability findings are stable or artifacts of small sample size.