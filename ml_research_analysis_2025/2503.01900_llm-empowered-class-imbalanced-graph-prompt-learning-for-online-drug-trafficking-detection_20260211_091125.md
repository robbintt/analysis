---
ver: rpa2
title: LLM-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking
  Detection
arxiv_id: '2503.01900'
source_url: https://arxiv.org/abs/2503.01900
tags:
- drug
- graph
- node
- trafficking
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles illicit drug trafficking detection on social\
  \ media, which is challenging due to severe class imbalance (drug traffickers are\
  \ rare) and limited labeled data. The proposed LLM-HetGDT framework addresses this\
  \ by: (1) pre-training a heterogeneous graph neural network (HGNN) on unlabeled\
  \ data via contrastive learning to capture node and structural patterns, (2) using\
  \ a large language model (LLM) to generate high-quality synthetic minority (drug\
  \ trafficker) user nodes and their edges, and (3) fine-tuning soft prompts\u2014\
  including node, structure, and class prototype prompts\u2014on the augmented graph\
  \ for effective downstream detection."
---

# LLM-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking Detection

## Quick Facts
- arXiv ID: 2503.01900
- Source URL: https://arxiv.org/abs/2503.01900
- Reference count: 40
- Primary result: Achieves 70.52%, 71.36%, and 74.06% macro-F1 on 10%, 20%, and 40% training splits respectively on Twitter-HetDrug dataset

## Executive Summary
This paper tackles illicit drug trafficking detection on social media, which is challenging due to severe class imbalance (drug traffickers are rare) and limited labeled data. The proposed LLM-HetGDT framework addresses this by: (1) pre-training a heterogeneous graph neural network (HGNN) on unlabeled data via contrastive learning to capture node and structural patterns, (2) using a large language model (LLM) to generate high-quality synthetic minority (drug trafficker) user nodes and their edges, and (3) fine-tuning soft prompts—including node, structure, and class prototype prompts—on the augmented graph for effective downstream detection. Experiments on a newly collected Twitter dataset (Twitter-HetDrug) show that LLM-HetGDT outperforms 21 state-of-the-art baselines, achieving macro-F1 scores of 70.52%, 71.36%, and 74.06% on 10%, 20%, and 40% training splits, respectively. Ablation and deployment studies confirm the effectiveness and practical applicability of the approach.

## Method Summary
The LLM-HetGDT framework addresses class imbalance in drug trafficking detection through a three-stage pipeline. First, it pre-trains a heterogeneous graph neural network (HGNN) on unlabeled social media data using cross-view contrastive learning to capture structural patterns. Second, it employs a large language model (Llama3.1 70B) to generate synthetic user profiles for the minority drug trafficking class, creating realistic user descriptions and connecting them to appropriate neighbors in the graph. Third, it fine-tunes soft prompts—including node-specific feature tokens, structure-aware meta-path aggregations, and class prototype tokens—on the augmented graph while keeping the HGNN encoder frozen. The framework reformulates classification as similarity matching between node embeddings and class prototypes, optimizing with a combination of cross-entropy and orthogonal regularization losses.

## Key Results
- Achieves 70.52% macro-F1 on 10% training split, outperforming 21 baselines including HetGNN (36.13%), HGT (57.87%), and GraphBT (67.82%)
- Synthetic node generation improves macro-F1 by 2.16-4.52% compared to no augmentation across different training ratios
- Prompt tuning consistently outperforms full fine-tuning and achieves 6.17-7.67% improvement over vanilla HGNN
- Model scales effectively with more training data, achieving 74.06% macro-F1 on 40% split

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic minority nodes improve detection of drug trafficking participants under class imbalance.
- Mechanism: The LLM generates synthetic user profiles from text-level descriptions (username, ID, profile) that preserve drug-related semantic markers while introducing controlled variability. These synthetic nodes are then connected to neighbors of the original nodes via LLM-prompted edge selection. This text-level augmentation avoids the information loss inherent in feature-level interpolation methods like SMOTE.
- Core assumption: The LLM can capture and reproduce the semantic patterns that distinguish drug trafficking participants from benign users.
- Evidence anchors:
  - [abstract] "employ LLM to augment the HG by generating high-quality synthetic user nodes in minority classes"
  - [section 3.2.1] "we feed the user text content ti into LLM to obtain a synthetic node v′i with corresponding user text content t′i"
  - [corpus] Weak direct evidence; related work on wildlife trafficking detection (FMR=0.459) uses LLMs for identification but not graph augmentation.
- Break condition: If synthetic nodes introduce distribution shift that conflicts with real minority class patterns, performance degrades. Ablation removing LLM augmentation would reveal this.

### Mechanism 2
- Claim: Contrastive pre-training on unlabeled heterogeneous graphs captures transferable structural patterns for downstream drug trafficking detection.
- Mechanism: The HGNN is trained via cross-view contrastive learning (meta-path view vs. network schema view) to maximize agreement between different views of the same node. This forces the encoder to learn structural and semantic features without labels, which are then reused when fine-tuning prompts on the labeled subset.
- Core assumption: The structural patterns learned during contrastive pre-training are relevant to the drug trafficking detection task despite the objective gap.
- Evidence anchors:
  - [abstract] "pre-train HGNN over a contrastive pretext task to capture the inherent node and structure information over the unlabeled drug trafficking heterogeneous graph"
  - [section 3.1] "we leverage the temperature-scaled contrastive loss as the objective function for the pretext task"
  - [corpus] No directly comparable corpus papers on contrastive HGNN pre-training for illicit activity detection.
- Break condition: If pre-training captures task-irrelevant patterns, the prompt tuning stage may fail to bridge the objective gap. Pre-training ablation would expose this dependency.

### Mechanism 3
- Claim: Class-specific prompt tokens and prototype-based classification align the downstream task with the contrastive pre-training objective.
- Mechanism: Three prompt types are designed: (1) node prompts with class-specific feature tokens that augment node attributes, (2) structure prompts that aggregate meta-path neighborhood information, and (3) drug trafficking prompts (class prototypes) that reformulate classification as similarity matching between node embeddings and prototype tokens. This aligns downstream classification with the contrastive pre-training paradigm.
- Core assumption: Class-specific prompt tokens can capture minority class characteristics without being dominated by majority class samples.
- Evidence anchors:
  - [section 3.2.2] "the key insight behind the class-specific feature tokens is to separate the prompt for each class to alleviate the influence of class imbalance"
  - [section 3.2.3] "the key idea behind the task prompt is shifting the node classification to pairwise similarity calculation between the node embeddings and prototype tokens"
  - [corpus] GraphSB (FMR=0.470) addresses structural balance for imbalanced node classification but uses different mechanisms.
- Break condition: If prompts are dominated by majority class gradients during tuning, minority class performance suffers. Prompt component ablation (Figure 3) shows degradation when drug trafficking prompt is removed.

## Foundational Learning

- Concept: **Heterogeneous Graphs and Meta-paths**
  - Why needed here: The framework models users, tweets, and keywords as different node types with multiple relation types. Meta-paths (e.g., User-Tweet-User) define composite relationships for message passing.
  - Quick check question: Can you explain why a User-Tweet-Keyword-Tweet-User meta-path might capture different information than direct User-User edges?

- Concept: **Contrastive Learning on Graphs**
  - Why needed here: Pre-training uses cross-view contrastive learning between meta-path and network schema views. Understanding how contrastive losses pull positive pairs together and push negative pairs apart is essential.
  - Quick check question: Why does the contrastive loss use temperature scaling, and what happens if temperature is too high or too low?

- Concept: **Prompt Tuning vs. Fine-tuning**
  - Why needed here: The framework freezes the pre-trained HGNN and only tunes prompt tokens. This differs from full fine-tuning and requires understanding how prompts modify input representations.
  - Quick check question: Why might prompt tuning be preferable to fine-tuning when labeled data is scarce?

## Architecture Onboarding

- Component map: SentenceBERT (text encoding) -> HGNN Encoder (frozen after pre-training) -> LLM Augmenter (synthetic nodes + edges) -> Prompt Modules (node, structure, drug trafficking prompts) -> Classification Head (similarity matching)

- Critical path:
  1. Pre-training: Original graph G → contrastive learning → frozen HGNN weights
  2. Augmentation: Labeled minority nodes → LLM → synthetic nodes + edges → augmented graph G′
  3. Prompt Tuning: G′ + prompts → frozen HGNN → embeddings → similarity matching → classification loss

- Design tradeoffs:
  - **LLM choice**: Paper uses Llama3.1 70B (open-source). Stronger closed-source models (GPT-4o, DeepSeek-R1) may improve synthetic node quality but increase cost and latency
  - **Text encoder**: SentenceBERT vs. alternatives (paper acknowledges this limitation in Section 6)
  - **Prompt granularity**: Class-specific prompts add parameters but prevent majority dominance; simpler prompts may underperform on minority classes

- Failure signatures:
  - Synthetic nodes are disconnected or poorly integrated → edge generation prompts need refinement
  - Minority class F1 remains low despite augmentation → increase synthetic node count or improve prompt diversity
  - Pre-training embeddings lack discriminative power → meta-path selection or contrastive loss hyperparameters need adjustment

- First 3 experiments:
  1. **Ablation by prompt component**: Remove each prompt type (node, structure, drug trafficking) individually to measure contribution. Expected: drug trafficking prompt removal causes largest drop (Figure 3)
  2. **Synthetic node quality analysis**: Manually inspect generated profiles for semantic coherence and drug-related markers. Compare with real minority nodes to assess distribution alignment
  3. **Edge generation validation**: Compare LLM-selected edges against heuristics (e.g., connect to all neighbors, connect based on feature similarity) on a held-out subset

## Open Questions the Paper Calls Out

- **Question:** Does replacing the SentenceBert encoder or Llama3.1 70B backbone with more powerful, closed-source alternatives (e.g., GPT-4o or DeepSeek-R1) yield significant performance improvements in synthetic node generation?
- **Basis in paper:** [explicit] Section 6 (Limitations) states: "It remains uncertain whether more powerful text encoders or LLMs can further improve the performance of our proposed method to some extent."
- **Why unresolved:** The authors intentionally utilized open-source models (SentenceBert and Llama3.1 70B) and explicitly did not test against closed-source alternatives acknowledged to have higher capabilities.
- **What evidence would resolve it:** A comparative ablation study substituting the current LLM backbone with GPT-4o or similar models, measuring the change in Macro-F1 scores on the Twitter-HetDrug dataset.

- **Question:** Can the framework's graph construction and prompt tuning methodology transfer effectively to platforms with distinct social topologies or privacy constraints, such as Telegram or Instagram?
- **Basis in paper:** [inferred] The Introduction identifies multiple platforms (Twitter/X, Facebook, Instagram) as intermediaries for trafficking, but the Experiments (Section 4.1) and the "New Data" contribution are restricted exclusively to the Twitter-HetDrug dataset.
- **Why unresolved:** The model relies on specific heterogeneous relations (e.g., User-follow-User, Tweet-include-Keyword) and prompt templates optimized for Twitter's public data structure, leaving cross-platform efficacy unverified.
- **What evidence would resolve it:** Benchmarking LLM-HetGDT on an illicit trafficking dataset from a different platform (e.g., Instagram) to evaluate if the meta-path definitions and prompts require re-engineering.

- **Question:** To what extent does the LLM-generated synthetic data introduce semantic drift or hallucinated patterns that negatively bias the classifier against real-world minority classes?
- **Basis in paper:** [explicit] Section 7 (Ethical Considerations) notes the "potential for the generated content to include biased or harmful information" and the reliance on prompt design to mitigate this.
- **Why unresolved:** While the framework ensures structural connectivity via the LLM, there is no quantitative analysis measuring the semantic fidelity or "realism" of the synthetic user profiles compared to the ground truth minority samples.
- **What evidence would resolve it:** A semantic analysis (e.g., using human evaluators or distinct embedding distances) comparing the distribution of synthetic nodes against held-out real drug trafficking profiles to measure hallucination rates.

## Limitations

- **Dataset availability**: The Twitter-HetDrug dataset is not publicly accessible, and the provided GitHub repository link is broken, making exact reproduction impossible without reconstruction.
- **Implementation details**: Critical architectural specifics (exact HGNN encoder implementation, Set Transformer attention mechanisms, full LLM prompt templates) are either missing or only partially specified in the paper.
- **LLM dependency**: Performance is heavily dependent on the LLM's ability to generate realistic synthetic minority nodes. The paper uses Llama3.1 70B, but no ablation against other LLMs or automatic quality metrics for synthetic nodes is provided.
- **Generalizability**: The approach is evaluated on a single dataset with specific characteristics (28,839 users, ~12% minority class, Twitter-specific text patterns). Performance on other platforms or drug trafficking detection contexts remains unverified.

## Confidence

- **High confidence**: The framework's core design (pre-training → augmentation → prompt tuning) is well-specified and the ablation studies provide strong evidence for the contribution of each component.
- **Medium confidence**: The quantitative results are convincing within the tested dataset, but the lack of dataset access and implementation details prevents full validation of the claims.
- **Low confidence**: Claims about LLM-generated synthetic node quality and the generalizability of the approach to other illicit activity detection contexts are not empirically supported beyond the Twitter-HetDrug dataset.

## Next Checks

1. **Synthetic node quality audit**: Conduct a blind evaluation where human annotators assess the realism and drug-trafficking relevance of LLM-generated synthetic user profiles against real profiles. Compute agreement rates and identify failure patterns.
2. **Dataset reconstruction and reproducibility**: Attempt to recreate a comparable heterogeneous graph from alternative social media sources (e.g., Reddit, Telegram) with manual annotation. Compare performance metrics against the reported Twitter-HetDrug results.
3. **Cross-platform generalization**: Apply the pre-trained LLM-HetGDT model (if available) or re-train it on the new dataset. Measure performance degradation/gain to assess how well the approach transfers to different social media platforms and user behaviors.