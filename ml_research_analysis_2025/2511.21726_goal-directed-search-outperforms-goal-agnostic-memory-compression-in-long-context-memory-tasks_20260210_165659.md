---
ver: rpa2
title: Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context
  Memory Tasks
arxiv_id: '2511.21726'
source_url: https://arxiv.org/abs/2511.21726
tags:
- memory
- search
- answer
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents SUMER, a reinforcement learning agent with verifiable
  reward (RLVR) that learns to search uncompressed conversational memory to answer
  long-context questions, outperforming traditional goal-agnostic memory compression
  methods on the LoCoMo benchmark. SUMER uses an LLM agent trained via group relative
  policy optimization (GRPO) to interact with an external memory database through
  semantic and keyword search tools, optimizing for final answer accuracy.
---

# Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks

## Quick Facts
- arXiv ID: 2511.21726
- Source URL: https://arxiv.org/abs/2511.21726
- Reference count: 36
- Primary result: SUMER with Qwen2.5-7B-Instruct achieves 48.65 F1 on LoCoMo, 43% gain over prior best

## Executive Summary
This paper presents SUMER, a reinforcement learning agent with verifiable reward (RLVR) that learns to search uncompressed conversational memory to answer long-context questions, outperforming traditional goal-agnostic memory compression methods on the LoCoMo benchmark. SUMER uses an LLM agent trained via group relative policy optimization (GRPO) to interact with an external memory database through semantic and keyword search tools, optimizing for final answer accuracy. The results demonstrate that goal-directed search over raw data can outperform biased compression algorithms in current long-context memory tasks.

## Method Summary
SUMER trains a Qwen-2.5-7B-Instruct agent to search uncompressed conversational memory using semantic and keyword search tools. The agent learns via GRPO with terminal-only rewards from an LLM judge (gpt-oss-120b). Memory is stored as individual messages embedded with Qwen3-Embedding-0.6B, with temporal context padding (±2 messages) for retrieved results. Training uses 8 rollouts per question, with token-level loss masking to train only on agent-generated tokens. The system achieves state-of-the-art performance on LoCoMo's long-term conversational QA.

## Key Results
- SUMER achieves 48.65 F1, 43.44 BLEU-1, and 66.79% LLM-judge correctness on LoCoMo
- 43% gain over prior best and 37.57% relative improvement over pre-RL baseline
- Ablation studies show semantic search reduces turns by 66% and temporal context doubles efficiency
- Training shows steady reward increase from 0 to 0.8 over 400 steps

## Why This Works (Mechanism)

### Mechanism 1: Goal-Directed Search Avoids Information Loss from Compression
Raw messages are stored without transformation; the agent learns which queries retrieve relevant evidence after seeing the question, rather than committing to a compression schema before knowing what will be asked. Information relevance cannot be reliably predicted at storage time; queries are sufficiently diverse that any fixed compression schema will discard some query-critical details.

### Mechanism 2: GRPO with Multi-Turn Masking Enables Stable Policy Learning
Group-relative policy optimization trains effective search behavior using only terminal verifiable rewards, without a critic. For each question, sample G=8 rollouts; compute standardized advantage across the group. Mask out prompts and tool-response tokens from loss so gradients flow only through agent-generated tokens.

### Mechanism 3: Temporal Context Enrichment Improves Search Efficiency
Padding retrieved memories with ±2 neighboring messages reduces required search turns by providing local conversational coherence. When a message matches the query, the system concatenates 2 preceding and 2 following messages, giving the agent context without additional tool calls.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Core RL algorithm that eliminates need for value-function critic by normalizing rewards within rollout groups. Quick check: Why does normalizing rewards within a group of rollouts stabilize training compared to using raw rewards?

- **Token-level loss masking in multi-turn RL**: Ensures agent learns to generate tool calls and reasoning, not to predict tool responses. Quick check: Which tokens should have mask m_i,t = 0 vs. m_i,t = 1 in SUMER's training?

- **Verifiable rewards (RLVR)**: Enables fully automated training without human annotation. Quick check: What properties must a task have for rewards to be "verifiable" by a program?

## Architecture Onboarding

- **Component map**: Memory Bank (Langmem with Qwen3-Embedding-0.6B, 1024-dim vectors) → Search Tools (semantic via cosine similarity, keyword via exact match) → Agent (Qwen-2.5-7B-Instruct, max 20 turns, max 5 parallel calls) → GRPO Trainer (G=8, lr=1e-6, clip ε_high=0.28) → Reward (LLM-judge binary × F1 score)

- **Critical path**: Embedding quality determines semantic search recall → Search tool behavior determines evidence coverage → Agent policy determines query formulation and stopping behavior → Judge model determines reward signal quality

- **Design tradeoffs**: Semantic vs. keyword: semantic is more efficient (fewer turns), keyword provides precision on entity-specific queries. Context enrichment: +2/-2 padding triples turn efficiency but assumes local coherence. Turn limit (20): bounds compute; may truncate optimal search on complex multi-hop questions. No KL regularization: freer optimization but risks policy degradation.

- **Failure signatures**: Validation J plateaus early → check reward signal quality or embedding drift. Turn counts spike (>15 avg) → check if search tools are returning empty/irrelevant results. F1 high but J low → agent generates verbose answers; adjust reward weighting toward judge.

- **First 3 experiments**: 1) Reproduce SUMER-Base: Run pre-RL agent with tools on validation set; establish baseline turn counts and accuracy. 2) Ablate semantic search: Disable semantic tool, run GRPO training; expect ~2x turn increase and ~5-point J drop. 3) Cross-conversation generalization: Train on conv-48, evaluate on all 9 held-out conversations; verify results match reported 48.65 F1 / 66.79 J.

## Open Questions the Paper Calls Out

### Open Question 1
Does goal-directed search maintain its advantage over compression when conversation histories vastly exceed the model's context window? Current experiments use conversations averaging ~17k tokens against a 32k context window, leaving the extreme long-horizon regime untested.

### Open Question 2
Under what conditions does compression become beneficial or even necessary for long-context memory? The paper only demonstrates search superiority in one regime; no characterization of the boundary where compression's bias becomes advantageous.

### Open Question 3
Can agents trained with RLVR learn to extract schemas and abstractions from repeated experiences for genuine generalization? LoCoMo only tests information retrieval, not whether the agent can learn reusable structures from distributed evidence.

### Open Question 4
Does training on a single conversation limit the learned search policy's generalizability? The policy may overfit to the interaction patterns of two specific speakers rather than learning general search strategies.

## Limitations
- The absolute performance remains modest (48.65 F1, 66.79% correctness) despite representing a 43% relative gain
- The study only examines one specific RL algorithm (GRPO) and one embedding model (Qwen3-Embedding-0.6B)
- The comparison doesn't explore hybrid approaches that might combine selective compression with goal-directed search

## Confidence
- **High confidence**: The core empirical claim that SUMER achieves higher performance than prior compression-based methods on LoCoMo
- **Medium confidence**: The mechanism explanation that goal-directed search avoids information loss because compression is lossy and query-agnostic
- **Low confidence**: The generalizability of these findings to other long-context memory tasks beyond conversational QA

## Next Checks
1. **Cross-dataset generalization test**: Evaluate SUMER on a different long-context memory benchmark (e.g., QuALITY, NarrativeQA, or a custom multi-session dialogue dataset) to verify whether the search advantage extends beyond LoCoMo.

2. **Hybrid compression-search analysis**: Implement a hybrid system that compresses low-relevance portions of memory while maintaining uncompressed "hot zones" around recent activity. Compare this against pure search and pure compression to identify optimal tradeoff points.

3. **Latency and resource consumption study**: Measure the wall-clock time and computational resources required for SUMER's search approach versus compression methods across varying memory sizes (10K, 100K, 1M tokens) to quantify the practical efficiency tradeoffs.