---
ver: rpa2
title: Human-Centric Evaluation for Foundation Models
arxiv_id: '2506.01793'
source_url: https://arxiv.org/abs/2506.01793
tags:
- evaluation
- subjective
- performance
- arxiv
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the limitations of objective benchmarks in
  evaluating foundation models, which fail to capture authentic human experiences
  and subjective perceptions. The authors propose a Human-Centric Evaluation (HCE)
  framework that assesses models across three dimensions: problem-solving ability,
  information quality, and interaction experience through human-rated experiments.'
---

# Human-Centric Evaluation for Foundation Models

## Quick Facts
- arXiv ID: 2506.01793
- Source URL: https://arxiv.org/abs/2506.01793
- Authors: Yijin Guo; Kaiyuan Ji; Xiaorong Zhu; Junying Wang; Farong Wen; Chunyi Li; Zicheng Zhang; Guangtao Zhai
- Reference count: 26
- Key outcome: Human-Centric Evaluation framework reveals distinct model strengths across problem-solving, information quality, and interaction experience dimensions

## Executive Summary
This study addresses the limitations of objective benchmarks in evaluating foundation models, which fail to capture authentic human experiences and subjective perceptions. The authors propose a Human-Centric Evaluation (HCE) framework that assesses models across three dimensions: problem-solving ability, information quality, and interaction experience through human-rated experiments. Testing Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5 across 540+ participant-driven evaluations revealed distinct model strengths and adaptability patterns. Grok 3 achieved the highest overall score (4.30/5), followed by Deepseek R1 and Gemini 2.5 (4.08-4.09/5), while OpenAI o3 mini performed comparatively poorly (3.91/5). The framework and resulting dataset offer a novel approach for standardized, automated subjective evaluation, advancing LLM development for research and practical applications.

## Method Summary
The study evaluates four foundation models using a Human-Centric Evaluation framework where university student evaluators interact freely with models for 20 minutes to complete open-ended tasks spanning eight disciplines and two problem types. Evaluators then rate nine subjective dimensions on a 5-point scale covering problem-solving ability (accuracy, comprehensiveness, efficiency), information quality (reliability, depth), and interaction experience (relevance, adaptability, naturalness, timeliness). Final scores are arithmetic means of human ratings, with 5 evaluators per task-language combination providing 540+ total evaluations.

## Key Results
- Grok 3 achieved the highest overall score (4.30/5), followed by Deepseek R1 and Gemini 2.5 (4.08-4.09/5), while OpenAI o3 mini performed comparatively poorly (3.91/5)
- Cross-disciplinary testing revealed distinct model specialization patterns, with Grok 3 ranking first in 7/8 disciplines
- Law showed the largest performance gap among models while medicine showed the smallest variation
- DeepSeek R1 demonstrated a 0.13-point advantage in Chinese over English evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional subjective evaluation captures human experience dimensions that objective benchmarks miss.
- Mechanism: The HCE framework decomposes subjective experience into nine measurable sub-dimensions across three categories (problem-solving ability, information quality, interaction experience), each rated on a 5-point scale by human evaluators who complete open-ended tasks with models.
- Core assumption: Human raters can reliably distinguish between dimensions like "feedback adaptability" and "expression naturalness" during a single 20-minute interaction session.
- Evidence anchors:
  - [abstract] "focusing on three core dimensions: problem-solving ability, information quality, and interaction experience"
  - [section 3.2] Defines nine sub-dimensions with specific criteria (e.g., "Feedback adaptability indicates the model's ability to adjust the focus and presentation of its responses based on user feedback")
  - [corpus] Limited direct corpus support for this specific decomposition; neighboring papers focus on domain-specific benchmarks rather than subjective framework design
- Break condition: If inter-rater reliability is low across dimensions, or if evaluators conflate dimensions (e.g., rating "naturalness" based on "timeliness"), the framework's discriminative validity fails.

### Mechanism 2
- Claim: Cross-disciplinary task design reveals model specialization patterns that single-domain benchmarks obscure.
- Mechanism: By spanning eight disciplines (CS, law, finance, medicine, sociology, environmental engineering, biology, education) and two problem types (literature synthesis, innovation-driven), the framework detects domain-specific strengths—for example, Grok 3's 4.41 in law vs. Gemini 2.5's 4.28 in finance.
- Core assumption: University student evaluators possess sufficient domain expertise in their selected discipline to accurately assess model output quality.
- Evidence anchors:
  - [section 4.2] "law showing the largest performance gap among models and medicine the smallest"
  - [table 2b] Shows Grok 3 ranking first in 7/8 disciplines, with domain-specific variation across models
  - [corpus] Neighboring papers (e.g., ophthalmology benchmarking, o3-mini vs DeepSeek-R1 safety comparisons) confirm domain-specific performance variation is a consistent finding across evaluation methodologies
- Break condition: If evaluator expertise is insufficient, domain scores reflect evaluator knowledge gaps rather than model capability differences.

### Mechanism 3
- Claim: Time-limited free-form interaction surface model behaviors that static benchmarks cannot detect.
- Mechanism: Evaluators engage in 20-minute unstructured dialogue with models to complete open-ended research tasks, then retrospectively rate nine dimensions. This design captures emergent behaviors like Grok 3's use of "rhetorical questions and encouraging statements" that improved emotional engagement.
- Core assumption: A 20-minute session provides sufficient interaction density to evaluate all nine dimensions meaningfully.
- Evidence anchors:
  - [section 3.4] "This evaluation method employs a time-limited free-form Q&A format... Final scores are determined by comprehensively assessing solution completeness, logical consistency, response efficiency, and language understanding accuracy during interactions"
  - [section 4.2] "evaluators commonly reported suboptimal interaction experiences [with DeepThink/DeepSearch features], particularly in research scenarios requiring rapid reasoning"
  - [corpus] Weak corpus connection; neighboring papers do not examine time-limited interaction protocols
- Break condition: If evaluators cannot form stable judgments across all nine dimensions in 20 minutes, scores become noisy or collapse onto a single "overall impression" factor.

## Foundational Learning

- **Concept: Objective vs. Subjective Evaluation Paradigms**
  - Why needed here: The paper explicitly positions HCE as a corrective to objective benchmarks that "fail to reflect authentic human experiences." Understanding this distinction is prerequisite to interpreting why the framework designs tasks as open-ended rather than Q&A format.
  - Quick check question: Can you articulate one dimension that an accuracy-based benchmark (like MMLU) cannot measure, but HCE can?

- **Concept: Multi-dimensional Construct Validity**
  - Why needed here: The framework assumes nine sub-dimensions are distinct constructs. Without understanding construct validity, practitioners may treat the framework as a single "satisfaction" score, losing diagnostic granularity.
  - Quick check question: If "Assistance Efficiency" and "Response Timeliness" scores always correlate at r>0.9 across models, what does this suggest about the dimension decomposition?

- **Concept: Human-in-the-Loop Evaluation Design**
  - Why needed here: The methodology depends on human evaluators exercising "subjective initiative" in task selection and rating. Understanding human factors (expertise, bias, fatigue) is essential for interpreting result reliability.
  - Quick check question: What selection bias might arise from recruiting only university students as evaluators for a framework intended to generalize to "practical applications"?

## Architecture Onboarding

- **Component map:**
  Task Pool: 8 disciplines × 2 problem types (Literature Synthesis, Innovation-Driven) → 16 task categories
  Evaluator Interface: 20-minute free-form chat session with single model
  Rating Instrument: 9-item questionnaire (5-point Likert scale), organized into 3 dimension categories
  Aggregation Layer: Arithmetic mean per dimension, per discipline, per model
  Dataset Output: 540+ evaluation records with task metadata, discipline, language, and dimensional scores

- **Critical path:**
  1. Evaluator selects task based on major/interest → 2. Engages in 20-min free interaction with assigned model → 3. Completes 9-item rating questionnaire → 4. Scores aggregated into dimension/discipline leaderboards. Failure at step 2 (e.g., model API timeout) or step 3 (incomplete ratings) invalidates the evaluation record.

- **Design tradeoffs:**
  - Sample size vs. depth: 5 evaluators per task-language combination provides limited statistical power, but deeper per-evaluator engagement (20-minute sessions) yields richer qualitative signal than large-N shallow comparisons.
  - Standardization vs. ecological validity: Fixed 20-minute limit improves comparability but may truncate tasks requiring longer exploration; free-form interaction improves realism but reduces reproducibility.
  - Language balance: Bilingual design (Chinese/English) doubles coverage but introduces potential cross-linguistic performance confounds (e.g., DeepSeek R1's 0.13-point Chinese advantage).

- **Failure signatures:**
  - Flat dimension profiles: If a model scores ~4.0 across all 9 dimensions, evaluators may be exhibiting centrality bias rather than discriminating.
  - High variance within task-model cells: If 5 evaluators for the same model-task combination produce scores ranging 2–5, the rating criteria may be ambiguous.
  - Language-specific collapse: If all models score systematically higher in one language, task translation quality—not model capability—may be the driver.

- **First 3 experiments:**
  1. Inter-rater reliability check: Compute intra-class correlation (ICC) for each dimension across evaluators rating the same model-task pair. If ICC < 0.5, refine dimension definitions or rater training.
  2. Dimension distinctiveness test: Correlate all 9 dimension scores across the full dataset. If >3 dimension pairs show r>0.85, consider merging or redefining constructs.
  3. Language confound isolation: For a single model (e.g., DeepSeek R1), run paired evaluations where the same evaluator rates both language versions of equivalent tasks. Compare within-evaluator score differences to between-evaluator variance.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework's reliance on university student evaluators raises questions about generalizability to professional users
- 20-minute interaction constraint may truncate complex tasks and miss long-term interaction dynamics
- Cross-linguistic performance differences suggest task translation quality may confound model capability assessment
- Framework assumes stable construct validity across nine dimensions requiring further validation

## Confidence

- **High Confidence**: Model ranking differences (Grok 3 > DeepSeek R1/Gemini 2.5 > OpenAI o3 mini) are consistent across disciplines and dimensions, supported by multiple evaluators per task.
- **Medium Confidence**: Domain-specific performance patterns (e.g., Grok 3 excelling in law) are meaningful but may reflect evaluator expertise rather than pure model capability.
- **Low Confidence**: The nine-dimensional decomposition captures truly distinct constructs—inter-rater reliability and discriminant validity require empirical verification.

## Next Checks

1. Conduct inter-rater reliability analysis (ICC) for each dimension across identical model-task pairs to confirm rating consistency.
2. Test discriminant validity by correlating all nine dimensions across the dataset; investigate any dimension pairs with r > 0.85.
3. Implement paired within-evaluator comparisons across both language versions of equivalent tasks to isolate translation confounds from model performance differences.