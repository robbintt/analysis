---
ver: rpa2
title: Proactive User Information Acquisition via Chats on User-Favored Topics
arxiv_id: '2504.07698'
source_url: https://arxiv.org/abs/2504.07698
tags:
- topic
- utterance
- user
- question
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the PIVOT task, where a system must proactively
  acquire user answers to predefined questions during a chat on a user-favored topic
  without making the user feel abrupt. Recent large language models show poor performance
  on this task, with success rates below 20%.
---

# Proactive User Information Acquisition via Chats on User-Favored Topics

## Quick Facts
- arXiv ID: 2504.07698
- Source URL: https://arxiv.org/abs/2504.07698
- Reference count: 40
- LLMs show poor performance on proactive information acquisition with success rates below 20%

## Executive Summary
This study introduces the PIVOT task, where a system must proactively acquire user answers to predefined questions during a chat on a user-favored topic without making the user feel abrupt. Recent large language models show poor performance on this task, with success rates below 20%. To address this, the authors construct a dataset of 650 PIVOT chats and analyze it to uncover effective strategies for non-abrupt information acquisition. They find that explicitly associating the topic with the question, using a single cushion utterance, and including explanations of relevance help avoid abruptness. A simple belief-desire-intention (BDI) model-based system is developed that incorporates these insights and achieves a 40% success rate—significantly outperforming baseline systems with only prompt-based instructions. The findings highlight both the challenge of PIVOT and the effectiveness of data-driven strategies in achieving natural, proactive user information acquisition.

## Method Summary
The study constructs a PIVOT dataset of 650 chats where systems must acquire user answers to Yes-No questions while chatting about user-favored topics. The dataset includes 50 topics from Wizard of Wikipedia and persona sets from ConvAI2. The strategy-based BDI system generates four candidate types—key utterances using seven relationship types (SUB-THEME, PLACE, MEANS, CO-OCCUR, CAUSE, PREREQUISITE, DOER), cushion utterances, vanilla candidates, and safe candidates. An abruptness evaluator LLM fine-tuned on GPT-4o filters candidates, selecting the highest-priority non-abrupt option. The system tracks information acquisition state to avoid over-pursuit, achieving 40% success rate versus 12% for baseline LLMs.

## Key Results
- Baseline LLMs achieve only 12% success rate on PIVOT task
- Strategy-based BDI system achieves 40% success rate, outperforming baselines
- Explicit relationship association reduces Type 2 abruptness (unnatural associations) from 26% to 11%
- Fine-tuning abruptness evaluator improves F1 from 40.1 to 88.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly establishing semantic relationships between the TOPIC and QUESTION reduces perceived abruptness during information acquisition.
- Mechanism: The system identifies one of seven relationship types connecting the chat topic to the target question, then generates utterances that first explain this relationship before introducing the question content.
- Core assumption: Users perceive topic transitions as natural when a plausible causal, temporal, or thematic connection is explicitly articulated, even if the question is semantically unrelated to the topic.
- Evidence anchors: [section 6.1] LLM relationship identification using seven types; [section 6.3] 38% reduction in abrupt key utterances with relationship explanations; [corpus] Proactive dialogue systems benefit from topic transition strategies.

### Mechanism 2
- Claim: Maintaining explicit state awareness of information acquisition prevents the system from continuing to pursue already-acquired information, which is a major source of abruptness.
- Mechanism: After each user turn, an evaluator LLM predicts whether the user's answer to the QUESTION can be inferred. When prediction confidence reaches threshold, the system removes information-acquisition instructions from subsequent response generation prompts.
- Core assumption: LLMs can accurately infer user answers from dialogue history with sufficient reliability to gate state transitions; the 88% agreement rate with human evaluators suggests this is conditionally true.
- Evidence anchors: [section 4.3.2] 88.0% agreement between LLM and human evaluators on answer prediction; [section 5.1] State-based prompt switching prevents over-pursuit; [corpus] Proactive dialogue research notes LLMs struggle with planning.

### Mechanism 3
- Claim: Generating multiple response candidates across a priority hierarchy and filtering via a fine-tuned abruptness evaluator yields higher-quality outputs than single-pass generation.
- Mechanism: The system generates four candidate types and selects the highest-priority candidate that passes the abruptness threshold as judged by a fine-tuned evaluator LLM.
- Core assumption: A fine-tuned LLM can approximate human abruptness judgments with sufficient accuracy to serve as a filter; the F1 improvement from 40.1 to 88.5 supports this conditionally.
- Evidence anchors: [section 4.3.2] F1 score improved from 40.1 to 88.5 after fine-tuning; [section 7.1.2] Candidate selection prioritizes key utterances; [corpus] BDI-based dialogue agents demonstrate intention selection improves goal-directed dialogue.

## Foundational Learning

- **Concept: Belief-Desire-Intention (BDI) Model**
  - Why needed here: The strategy-based system explicitly models Beliefs (information acquisition state), Desires (goal to acquire user information), and Intentions (selected response candidate). Understanding BDI helps engineers grasp why the system separates state tracking from response generation rather than end-to-end generation.
  - Quick check question: Can you explain how the system's "belief" (whether user information has been acquired) influences which "intention" (response candidate type) is selected?

- **Concept: Abruptness vs. Naturalness in Dialogue**
  - Why needed here: The paper's core constraint is acquiring information "without making the user feel abrupt." Engineers must understand that abruptness is a subjective, context-dependent judgment—not a syntactic property—and requires human or human-approximating evaluation.
  - Quick check question: Given a chat about "motorcycles" and a question "Do you like cooking?", would the utterance "Speaking of motorcycles, do you cook?" be abrupt? What about "Long rides make me hungry—do you enjoy cooking after trips?" Why?

- **Concept: Proactive vs. Reactive Dialogue Systems**
  - Why needed here: PIVOT requires the system to pursue its own goal (information acquisition) while the user pursues theirs (chatting about their topic). This differs from typical task-oriented dialogue where user and system goals align.
  - Quick check question: How does the PIVOT task differ from a standard FAQ chatbot? What additional capabilities does the system need?

## Architecture Onboarding

- **Component map:**
  State Tracker -> Candidate Generators -> Abruptness Evaluator -> Response Selector -> BDI Controller

- **Critical path:**
  1. Receive user utterance
  2. Update belief: Run state tracker to predict if QUESTION answer is inferable
  3. If acquisition complete → disable information-pursuit in prompt
  4. Generate all candidate types in parallel
  5. Score each candidate with abruptness evaluator
  6. Select highest-priority candidate scoring ≥3 (non-abrupt)
  7. Output selected response

- **Design tradeoffs:**
  - **Latency vs. quality**: Generating 4+ candidates and evaluating each increases response time but improves selection quality; consider parallelizing candidate generation
  - **Safety vs. task success**: Safe candidate prioritizes topic coherence over information acquisition; over-reliance reduces success rate (observed: strategy-based achieves 50% ACQ vs. 92% for prompt-based, but 82% N-ABR vs. 22%)
  - **Evaluator accuracy vs. training data size**: Fine-tuning improved F1 from 40.1 to 88.5 with ~500 training instances; marginal improvement from additional data was limited (89.8 F1 with 650 chats), suggesting diminishing returns

- **Failure signatures:**
  - **Type 1 abruptness**: Sudden question introduction without context → likely missing relationship-grounded candidate or evaluator failure
  - **Type 4 abruptness**: Continued pursuit after acquisition → state tracker false negative
  - **Low acquisition rate**: System overly conservative, selecting safe candidates → evaluator threshold too strict or relationship identification failing
  - **Commonality overuse**: System relies on weak "COMMONALITY" relationship → relationship type selector defaulting to fallback

- **First 3 experiments:**
  1. **Baseline replication**: Run standard GPT-4o with task-instruction-only prompt on 50 TOPIC/QUESTION pairs; measure ACQ, N-ABR, SUC to confirm ~12% success rate
  2. **Ablation study**: Remove one candidate type at a time (key, cushion, vanilla, safe) from the strategy-based system; measure impact on success rate to identify which components contribute most
  3. **Relationship type analysis**: On failed chats, manually categorize which relationship types were attempted vs. successful; identify whether certain types (e.g., COMMONALITY, DOER) correlate with higher abruptness rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced reasoning techniques, specifically chain-of-thought (CoT) prompting, enhance LLM performance in the PIVOT task beyond the current strategy-based system?
- Basis in paper: [explicit] The Conclusion states future work includes "examining the effectiveness of these techniques [CoT] in this task."
- Why unresolved: The current study utilized a BDI-model-based system and standard prompting. The potential for CoT to improve the complex planning required to bridge unrelated topics remains untested.
- What evidence would resolve it: An experiment evaluating LLMs using CoT prompting on the PIVOT dataset, showing a success rate higher than the 40% achieved by the strategy-based system.

### Open Question 2
- Question: To what extent can sophisticated data-driven methods, such as fine-tuning, leverage the constructed dataset to improve the success rate of information acquisition?
- Basis in paper: [explicit] The authors note that incorporating "sophisticated data-driven methods that leverage our dataset as training data could further refine the system’s performance."
- Why unresolved: The current best system relies on a modular architecture with prompting and heuristics rather than training an end-to-end model on the 650 collected chats.
- What evidence would resolve it: A comparative evaluation showing that a model fine-tuned on the PIVOT dataset outperforms the current strategy-based baseline.

### Open Question 3
- Question: How does the level of intimacy or the specific relationship between the user and the system influence the success of information acquisition?
- Basis in paper: [explicit] The Limitations section states, "In actual information acquisition, there is a possibility that different behavior will be shown depending on the intimacy with the chat partner."
- Why unresolved: The current experimental design assumes a generic relationship. Strategies effective for a neutral bot might fail or seem intrusive if the bot is framed as a close companion.
- What evidence would resolve it: A user study where the system's persona implies different levels of intimacy, analyzing how the threshold for "abruptness" and the success rate of information acquisition shift accordingly.

## Limitations

- The PIVOT dataset is relatively small (650 chats) with potential domain dependence on the seven relationship types
- State inference mechanism reliability (88% agreement) may not generalize to different domains or question types
- Optimal balance between information acquisition success and naturalness remains unclear (40% success vs. 92% for prompt-based systems)

## Confidence

- **High confidence**: Baseline LLMs struggle with proactive information acquisition (12% success rate); characterization of abruptness types and relationship-based strategy effectiveness are robust
- **Medium confidence**: State inference mechanism reliability (88% agreement) is supported but generalizability remains untested; BDI model's contribution could benefit from additional ablation studies
- **Low confidence**: Optimal balance between success rate and naturalness remains unclear, as strategy-based achieves 40% success versus 92% for prompt-based but with substantially better naturalness (82% vs. 22%)

## Next Checks

1. Test the strategy-based system on a held-out set of 100 new TOPIC/QUESTION pairs with different relationship structures to assess generalizability beyond the original dataset
2. Conduct a human evaluation comparing the strategy-based system against a fine-tuned end-to-end model to determine whether explicit BDI reasoning provides advantages over learned patterns
3. Perform an ablation study removing the relationship type identification component to quantify its specific contribution to reducing Type 2 abruptness (unnatural associations)