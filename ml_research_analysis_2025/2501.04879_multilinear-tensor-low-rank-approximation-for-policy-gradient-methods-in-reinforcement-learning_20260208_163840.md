---
ver: rpa2
title: Multilinear Tensor Low-Rank Approximation for Policy-Gradient Methods in Reinforcement
  Learning
arxiv_id: '2501.04879'
source_url: https://arxiv.org/abs/2501.04879
tags:
- policy
- tensor
- low-rank
- learning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces multilinear tensor low-rank policy models
  for policy-based reinforcement learning (RL). The core idea involves collecting
  policy parameters into a tensor and enforcing low rank via the PARAFAC decomposition,
  which allows for a parsimonious multi-linear representation of the policy.
---

# Multilinear Tensor Low-Rank Approximation for Policy-Gradient Methods in Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.04879
- Source URL: https://arxiv.org/abs/2501.04879
- Reference count: 40
- Primary result: Tensor low-rank policies achieve similar returns to neural networks while requiring fewer parameters and converging faster.

## Executive Summary
This paper introduces multilinear tensor low-rank policy models for policy-based reinforcement learning, offering an alternative to neural network parameterizations. The approach collects policy parameters into a tensor and enforces low rank via PARAFAC decomposition, creating a parsimonious multi-linear representation that exploits redundancies in state-action mappings. The method addresses neural network limitations like convergence issues and parameter inefficiency, providing four algorithms (TLRPG, TLRAC, TRTLRPO, PTLRPO) that converge under mild conditions. Empirical results demonstrate that tensor low-rank policies achieve comparable returns to neural network methods while using significantly fewer parameters and converging faster, particularly in classical control problems and wireless communications.

## Method Summary
The method replaces neural network policy parameterizations with PARAFAC low-rank tensor factors, collecting policy parameters into a D-dimensional tensor and constraining it to rank K via the PARAFAC decomposition Θ = Σₖ₌₁ᴷ x₁ₖ ⊚ ... ⊚ x_Dₖ. This reduces parameters from exponential (∏_d N_d) to linear (K·Σ_d N_d) in the number of dimensions while acting as an implicit regularizer. Four tensor low-rank algorithms are proposed: tensor low-rank policy gradient (TLRPG), tensor low-rank actor-critic (TLRAC), trust-region tensor low-rank policy optimization (TRTLRPO), and proximal tensor low-rank policy optimization (PTLRPO). These algorithms integrate into standard policy gradient frameworks using custom policy score gradients that factorize into distribution-dependent and model-dependent terms, enabling efficient computation. The approach requires discretizing continuous states to map them to tensor indices.

## Key Results
- Tensor low-rank policies achieve median returns of 500-600 on Pendulum with only 1000-2000 parameters, compared to 5000-10000 parameters for neural networks.
- Empirical validation shows NN-learned policies exhibit low-rank structure, with parameter efficiency dropping sharply at low ranks.
- Tensor low-rank methods demonstrate notably smaller variance in returns compared to neural network actor-critic approaches.
- The four proposed algorithms (TLRPG, TLRAC, TRTLRPO, PTLRPO) converge under mild conditions and are easy to tune via the rank hyperparameter.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing neural network policy parameterizations with PARAFAC low-rank tensor factors reduces parameter count while preserving expressivity.
- Mechanism: Policy parameters are collected into a D-dimensional tensor Θ ∈ ℝ^{N₁×...×N_D} and constrained to rank K via PARAFAC decomposition Θ = Σₖ₌₁ᴷ x₁ₖ ⊚ ... ⊚ x_Dₖ. This reduces parameters from ∏_d N_d (exponential in D) to K·Σ_d N_d (linear in D), while the low-rank constraint acts as an implicit regularizer that exploits structure in state-action mappings.
- Core assumption: The underlying policy mapping admits a low-dimensional multilinear representation (the true parameter tensor is approximately low-rank).
- Evidence anchors:
  - [abstract]: "collecting policy parameters into a tensor and enforcing low rank via the PARAFAC decomposition, which allows for a parsimonious multi-linear representation"
  - [Section III.A, Eq. 8-10]: Shows parameter reduction formula and low-rank policy optimization formulation
  - [Section IV, Fig. 1]: Empirical validation showing NN-learned policies exhibit low-rank structure (NFE drops sharply at low ranks)
- Break condition: If the environment requires highly non-linear state-action mappings that cannot be factorized multilinearly (true rank >> achievable K), the approximation error will degrade returns.

### Mechanism 2
- Claim: Policy score gradients factorize into distribution-dependent and model-dependent terms, enabling efficient computation.
- Mechanism: The chain rule decomposition ∂log πΘ(a|s)/∂[Θ_d]_{i,k} = ∂log πΘ(a|s)/∂[Θ]_{i_s} · ∂[Θ]_{i_s}/∂[Θ_d]_{i,k} shows that only K·D entries are non-zero per gradient. The first term depends on the probability distribution (Gaussian/softmax), while the second is the product of factors from all other modes.
- Core assumption: The state indexing i_s can be determined at each time step, and the indicator function properly localizes gradients.
- Evidence anchors:
  - [Section III.A, Eq. 12]: Derives the factorization with indicator function
  - [Section III.A, Eq. 13-14]: Shows concrete forms for Gaussian and softmax policies
  - [corpus]: Limited direct corpus support for this specific gradient factorization claim
- Break condition: If gradient sparsity is lost (e.g., continuous states without proper discretization), computational benefits diminish.

### Mechanism 3
- Claim: Tensor low-rank policies integrate into trust-region methods (TRPO/PPO) because the Fisher Information Matrix decomposes via policy scores.
- Mechanism: Both TRPO's constrained optimization and PPO's clipping mechanism require computing ∇Θ log πΘ. Since the FIM can be expressed as Ĥ = E[∇Θ log πΘ · ∇Θ log πΘᵀ], and TRPO/PPO updates boil down to policy score computation, tensor low-rank models plug directly into these frameworks with modified gradient formulas.
- Core assumption: The trust-region constraint (KL-divergence) remains meaningful in the factor space; projection onto low-rank manifolds preserves monotonic improvement properties.
- Evidence anchors:
  - [Section III.C, Eq. 22-23]: Shows stochastic approximations of gradient and Hessian in terms of policy scores
  - [Section III.C, Eq. 25-27]: Derives PPO gradient with indicator for clipping regions
  - [corpus]: Paper 45708 discusses GAE for distributional policy gradients, suggesting broader compatibility of score-based methods
- Break condition: If the low-rank projection violates trust-region assumptions (e.g., large KL divergence after projection), convergence guarantees may not hold.

## Foundational Learning

- Concept: **PARAFAC/CANDECOMP Tensor Decomposition**
  - Why needed here: Core representation that factorizes tensors into sum of rank-1 outer products; understanding this is essential to grasp how parameter compression works.
  - Quick check question: Given a 3-mode tensor of size 100×100×100 with rank 5, how many parameters does its PARAFAC representation require versus the full tensor?

- Concept: **Policy Gradient Theorem and Score Function**
  - Why needed here: All four algorithms (TLRPG, TLRAC, TRTLRPO, PTLRPO) rely on computing ∇θ log π_θ(a|s); understanding why this gradient enables learning is foundational.
  - Quick check question: Why does the policy gradient theorem express the gradient as an expectation over trajectories rather than direct differentiation?

- Concept: **Trust Region Methods (KL-Divergence, Fisher Information)**
  - Why needed here: TRTLRPO and PTLRPO require understanding why constraining policy updates via KL-divergence or clipping improves stability.
  - Quick check question: What is the relationship between the Fisher Information Matrix and the Hessian of the KL-divergence constraint?

## Architecture Onboarding

- Component map:
  - State discretizer: Maps continuous/discrete states S to tensor indices i_s = [i_1, ..., i_D]
  - Factor tensors {Θ_d}: D matrices of size N_d × K storing the PARAFAC factors
  - Policy sampler: Uses factors to compute distribution parameters (µ_s or z_s) via Eq. 8, then samples actions
  - Score computer: Implements Eq. 13 (Gaussian) or Eq. 14 (softmax) for gradient computation
  - Critic (for AC/TR/PPO): Separate tensor low-rank value function with factors ω = {V_d}
  - Optimizer: Standard SGD for PG/AC; conjugate gradient for TRPO; clipped SGD for PPO

- Critical path:
  1. Initialize factors {Θ_d} (small random values or structured initialization)
  2. For each iteration: sample trajectories using current policy
  3. Compute policy scores using factorized gradients (Eq. 13/14)
  4. Update factors via gradient ascent (with baseline, trust-region, or clipping as appropriate)
  5. If using AC/TR/PPO: update critic factors ω via Eq. 19

- Design tradeoffs:
  - **Rank K vs. expressivity**: Lower K → fewer parameters, faster convergence, but potential underfitting. Paper shows K=5-20 works for tested environments.
  - **Discretization resolution N_d**: Finer grids → better approximation but larger factors. Paper uses N_d=100 per dimension.
  - **Gaussian vs. softmax**: Gaussian for continuous actions (simpler scores), softmax for discrete (more complex due to fiber derivatives)
  - **Assumption**: State space discretization required; continuous states must be binned, introducing quantization error.

- Failure signatures:
  - Returns plateau below NN baseline → rank K too low, increase K
  - High variance in returns → critic underfitting, increase K' for value function
  - Gradient instability → check factor initialization; factors growing unbounded may need projection onto set O
  - Slow convergence despite low params → discretization too coarse, increase N_d

- First 3 experiments:
  1. **Sanity check on Pendulum**: Implement TLRPG with K=5, N_d=50, compare return curve against paper's Fig. 2. Verify ~1000-2000 params achieve return ~500-600.
  2. **Rank sensitivity analysis**: On Pendulum/MountainCar, sweep K ∈ {2, 5, 10, 20, 50} and plot final return vs. parameter count. Identify the "knee" where returns saturate.
  3. **Discretization ablation**: Fix K=10, vary N_d ∈ {20, 50, 100, 200} to measure impact of state resolution on convergence speed and final return.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the tensor low-rank framework be adapted for high-dimensional continuous state spaces (e.g., raw pixel inputs) without relying on explicit discretization?
- Basis in paper: [inferred] Section III.A explicitly states, "The state space is assumed to be discrete," and Section IV.A describes discretizing continuous environments to form the parameter tensor.
- Why unresolved: The reliance on pre-discretization limits the method's scalability to unstructured, high-dimensional data (like vision) common in modern RL, where grid-based tensors become intractable.
- What evidence would resolve it: An extension of the TLRPG/PTLRPO algorithms using functional tensor representations (e.g., Tensor Trains) applied successfully to vision-based control benchmarks.

### Open Question 2
- Question: Can the rank K of the policy tensor be learned adaptively or determined automatically during training rather than requiring exhaustive search?
- Basis in paper: [inferred] Section III.A identifies rank K as the "primary hyperparameter," and Section IV.A notes that K was "tuned... via exhaustive search."
- Why unresolved: Exhaustive search is computationally expensive and assumes the optimal complexity (rank) is known, which may not hold for dynamic or unknown environments.
- What evidence would resolve it: A modified algorithm incorporating rank regularization or incremental rank updates that converges to an efficient rank without manual tuning.

### Open Question 3
- Question: Does the multilinear structure of tensor policies provide theoretical advantages in reducing gradient estimator variance compared to neural networks?
- Basis in paper: [inferred] Section IV.A empirically observes that "the variance in return achieved by TLRAC is notably smaller" than NN-AC, but Theorem 1 only guarantees convergence to a stationary point.
- Why unresolved: While the paper demonstrates lower variance empirically, it does not provide a theoretical bound explaining how the low-rank multilinear constraint reduces gradient variance relative to nonlinear neural networks.
- What evidence would resolve it: A formal derivation of the variance bound for the proposed tensor policy gradient estimator compared to standard NN-based estimators.

## Limitations
- The approach relies on state discretization for continuous control tasks, introducing quantization error that may not scale to high-dimensional state spaces.
- Theoretical convergence guarantees for trust-region variants (TRTLRPO, PTLRPO) when applied to low-rank manifolds are not rigorously proven.
- Empirical validation is limited to relatively simple environments (Pendulum, MountainCar, a 6-state wireless setup), raising questions about scalability to complex domains.

## Confidence
- **High confidence**: Core PARAFAC decomposition mechanics and parameter reduction claims (supported by explicit equations and clear tensor algebra).
- **Medium confidence**: Empirical performance claims (returns and parameter efficiency) - the methodology is sound but the results are from a limited set of environments and may not generalize.
- **Low confidence**: Convergence proofs for trust-region methods on low-rank manifolds and the assumption that low-rank policies always admit a multilinear representation.

## Next Checks
1. **Robustness to discretization**: On Pendulum, systematically vary the state space discretization resolution (N_d = 20, 50, 100, 200) while keeping rank K fixed, and measure the impact on final return and convergence speed.
2. **Scalability test**: Implement TLRPG on a more complex environment (e.g., LunarLanderContinuous) to assess whether the parameter efficiency advantage holds as state-action dimensionality increases.
3. **Theoretical gap**: Rigorously prove or disprove that the KL-divergence constraint in TRPO remains a valid trust region when the policy parameters are projected onto the low-rank PARAFAC manifold.