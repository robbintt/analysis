---
ver: rpa2
title: Incremental Object Keypoint Learning
arxiv_id: '2503.20248'
source_url: https://arxiv.org/abs/2503.20248
tags:
- keypoints
- keypoint
- learning
- ka-net
- main
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Incremental Object Keypoint Learning (IKL),
  a new paradigm for continually learning new keypoints without retaining old training
  data. To address the label non-co-occurrence issue where old and new keypoints are
  not jointly labeled, the authors propose KAMP, a two-stage method that first trains
  an auxiliary KA-Net to associate old and new keypoints based on spatial and anatomical
  relations, then jointly leverages the old model and KA-Net to consolidate knowledge
  and learn new keypoints simultaneously using a keypoint-oriented spatial distillation
  loss.
---

# Incremental Object Keypoint Learning

## Quick Facts
- arXiv ID: 2503.20248
- Source URL: https://arxiv.org/abs/2503.20248
- Reference count: 40
- This paper introduces Incremental Object Keypoint Learning (IKL), a new paradigm for continually learning new keypoints without retaining old training data. To address the label non-co-occurrence issue where old and new keypoints are not jointly labeled, the authors propose KAMP, a two-stage method that first trains an auxiliary KA-Net to associate old and new keypoints based on spatial and anatomical relations, then jointly leverages the old model and KA-Net to consolidate knowledge and learn new keypoints simultaneously using a keypoint-oriented spatial distillation loss. Experiments on four keypoint datasets (Cephalometric, Chest, MPII, ATRW) demonstrate that KAMP significantly outperforms existing exemplar-free incremental learning methods, achieving the highest accuracy and positive transfer for both old and new keypoints while maintaining labeling efficiency, even in low-shot scenarios.

## Executive Summary
This paper addresses the challenge of incremental learning for object keypoint detection, where new keypoints must be learned on new data without access to old training data (privacy/memory constraints). The key innovation is KAMP, a two-stage method that overcomes the "label non-co-occurrence" problem through an auxiliary KA-Net that learns anatomical associations between old and new keypoints, combined with spatial distillation that preserves keypoint-specific spatial knowledge. The method achieves positive transfer (improving old keypoint accuracy while learning new ones) on multiple datasets including medical imaging and human pose estimation, significantly outperforming traditional incremental learning approaches that suffer from catastrophic forgetting.

## Method Summary
KAMP is a two-stage method for incremental object keypoint learning. In Stage-I, an auxiliary KA-Net is trained to predict a selected old keypoint given two related new keypoints, learning anatomical constraints from pseudo-labels of the frozen old model. In Stage-II, the new model is trained jointly on all keypoints using ground-truth loss for new keypoints and two distillation losses: ℓ_KSD from the old model (spatial-wise softmax over H,W dimensions) and ℓ_KA from KA-Net (only for the selected old keypoint). The method uses HRNet backbone and operates in exemplar-free mode, forbidding old data storage. The total training consists of 20 epochs for KA-Net plus 80 epochs for Stage-II training.

## Key Results
- KAMP achieves 79.93% AAA4 on Split MPII, significantly outperforming LWF (75.75%) and other exemplar-free methods
- KAMP demonstrates positive average transfer (AT ≈ +1.80) on Split MPII, while other methods show negative transfer
- KAMP maintains high performance in low-shot scenarios (50 training images) with 77.97% AAA4 on Split MPII
- The method successfully handles diverse datasets including medical imaging (Cephalometric, Chest) and human pose (MPII, ATRW)

## Why This Works (Mechanism)

### Mechanism 1: Triangulation-Based Knowledge Association
- **Claim:** An auxiliary network can learn anatomical constraints between old and new keypoints even without co-labeled data, by conditioning old keypoint predictions on spatially adjacent new keypoints.
- **Mechanism:** The KA-Net is trained to predict a selected old keypoint K^o_j given two related new keypoints K^n_1, K^n_2. This captures the conditional probability P(K^o_j | K^n_1, K^n_2) based on triangulation constraints (Eq. 1-2). Ground-truth heatmaps of new keypoints are element-wise multiplied with visual features from the frozen feature extractor, then processed through conv layers to predict the old keypoint.
- **Core assumption:** Spatially adjacent keypoints share local anatomical constraints that can be approximated by a learnable function, even when using pseudo-labels from the old model (which may contain errors).
- **Evidence anchors:**
  - [Section 3.2.1]: "KA-Net learns to predict the selected old keypoint given the related new keypoints to acquire their intrinsic anatomical relevance."
  - [Section 3.2.1]: Training uses pseudo-labels: "we use the pseudo-labels predicted by the old model m^{t-1} to supervise KA-Net"
  - [Corpus]: Weak direct evidence—neighbor papers focus on keypoint detection but not incremental learning; triangulation constraints are common in keypoint estimation literature but not explicitly validated for IKL.

### Mechanism 2: Keypoint-Oriented Spatial Distillation
- **Claim:** Applying softmax over spatial dimensions (H, W) rather than across keypoints (channels) preserves keypoint-specific spatial knowledge more effectively for regression tasks.
- **Mechanism:** Traditional IL methods like LWF use channel-wise softmax across classes, which normalizes across all old keypoint heatmaps. For keypoint estimation, this fails to regularize each keypoint's spatial distribution individually. KAMP applies spatial softmax over height and width dimensions (Eq. 5), encouraging the new model to match the old model's spatial predictions per-keypoint: s^d_{sp}(·) over d ∈ {H, W}, then averaged.
- **Core assumption:** The spatial distribution of each keypoint's heatmap contains independent, critical knowledge that should be preserved separately rather than normalized across keypoints.
- **Evidence anchors:**
  - [Section 3.2.2]: "To better preserve keypoint-specific knowledge, we adopt a spatial softmax operation over height (H) and width (W) dimensions"
  - [Table 2]: Ablation shows KAMP with only ℓ_KSD achieves 76.93% AAA4 vs. LWF's 75.75% on Split MPII—a 1.18% improvement from spatial distillation alone.

### Mechanism 3: Dual-Teacher Knowledge Consolidation
- **Claim:** Combining two distillation sources with distinct functions—old model for general anti-forgetting and KA-Net for relationship-aware improvement—enables positive transfer beyond mere forgetting prevention.
- **Mechanism:** Stage-II training uses three losses (Eq. 4): (1) ℓ_GT for new keypoints with ground truth, (2) ℓ_KA from KA-Net to improve selected old keypoints via anatomical priors, (3) ℓ_KSD from old model to preserve all old keypoints. KA-Net provides different knowledge than the old model because it conditions on new keypoints unavailable during original training (Table 5: KA-Net achieves 67.36% vs. old model's 64.39% on selected old keypoints in Step-1 Split MPII).
- **Core assumption:** The anatomical prior captured by KA-Net (conditioned on new keypoints) provides complementary and potentially superior supervision for related old keypoints compared to the old model alone.
- **Evidence anchors:**
  - [Section 3.2.2]: "ℓ_KA is only applied to the selected old keypoint for knowledge transfer instead of mitigating the forgetting, we further consolidate the knowledge of all old keypoints by the loss ℓ_KSD"
  - [Table 2]: Full KAMP achieves 79.93% AAA4, while ℓ_KSD alone achieves 76.93% and ℓ_KA alone catastrophically fails at 54.46% (Table 8 in supplement).

## Foundational Learning

- **Concept: Heatmap-based keypoint regression**
  - **Why needed here:** IKL operates on 2D Gaussian heatmaps (not coordinate regression). Understanding that each keypoint produces a spatial probability distribution is essential for grasping the spatial distillation mechanism.
  - **Quick check question:** Can you explain why applying softmax across channels (keypoints) vs. spatial dimensions (H, W) would affect how knowledge is preserved?

- **Concept: Knowledge distillation in incremental learning**
  - **Why needed here:** The core technique for preventing catastrophic forgetting is distilling predictions from the old model. LWF-style distillation is the baseline KAMP improves upon.
  - **Quick check question:** What is the difference between feature distillation and output/logit distillation, and why does this paper use output-level distillation?

- **Concept: Exemplar-free incremental learning**
  - **Why needed here:** IKL explicitly forbids storing old data (privacy, memory constraints). This constraint motivates the two-stage design and pseudo-label usage.
  - **Quick check question:** Why can't this method simply replay old images with pseudo-labels for all keypoints, and what problem does KA-Net solve that pseudo-labeling doesn't?

## Architecture Onboarding

- **Component map:**
  - Feature extractor f_t -> Estimation heads {G_i} -> Output heatmaps
  - KA-Net: new keypoints heatmaps -> frozen features -> old keypoint prediction
  - Old model m^{t-1}: frozen teacher for spatial distillation

- **Critical path:**
  1. Start incremental step t with new data D^t_train labeled only on new keypoints
  2. **Stage-I (20 epochs):** Train KA-Net using pseudo-labels from old model m^{t-1} for selected old keypoints
  3. **Stage-II (80 epochs):** Train new model m^t with combined loss: ℓ_GT (new keypoints) + α(ℓ_KSD + ℓ_KA)
  4. Freeze m^t, repeat for next step

- **Design tradeoffs:**
  - **Hyperparameter α:** Balance between new keypoint acquisition vs. old keypoint preservation. Paper uses α ∈ {1e2, 1e4} (Table 11: α=100 optimal for MPII). Too high → rigid model, poor new keypoint learning; too low → catastrophic forgetting.
  - **KA-Net task selection:** Only one auxiliary task created per step (selecting one old keypoint and two nearest new keypoints from anatomy diagram). Paper shows this is sufficient (Table 2), though multiple tasks could be used at higher cost.
  - **Backbone choice:** HRNet used for all experiments, but RSN also tested (81.45% vs. 79.93% on Split MPII), confirming architecture-agnostic design.

- **Failure signatures:**
  - **High forgetting (negative AT):** α too low, or spatial distillation not working. Check if ℓ_KSD is being computed correctly over spatial dimensions.
  - **Poor new keypoint accuracy:** α too high, or KA-Net dominating training. Verify that ℓ_GT is active and ground-truth labels are correct.
  - **KA-Net provides no improvement over old model:** Task selection may be incorrect (new keypoints not spatially related to old). Verify using anatomy diagram.
  - **Training instability:** Check learning rates (2e-3 for medical datasets, 1e-2 for MPII) and ensure frozen components (old model, old heads) are not being updated.

- **First 3 experiments:**
  1. **Reproduce LWF baseline on Split MPII:** Implement standard channel-wise softmax distillation. Expected: ~75.75% AAA4, negative AT. This establishes the baseline KAMP improves upon.
  2. **Ablate spatial distillation alone:** Implement ℓ_KSD with spatial softmax (no KA-Net). Expected: ~76.93% AAA4 on Split MPII. Verifies spatial distillation mechanism independently.
  3. **Full KAMP with single auxiliary task:** Select one old keypoint (e.g., "upper neck") and two nearby new keypoints (e.g., "right shoulder", "left shoulder") using the human skeleton diagram. Train KA-Net for 20 epochs, then Stage-II for 80 epochs. Expected: ~79.93% AAA4, positive AT ~1.80. Verify positive transfer by checking per-keypoint improvements.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the IKL paradigm be successfully extended to handle novel object categories by applying it to keypoint estimators pretrained with Category-Agnostic Pose Estimation (CAPE)?
  - **Basis:** [explicit] The authors state in the Conclusion: "In the future, we will explore extending the IKL paradigm to novel object categories by applying IKL to keypoint estimators pretrained with CAPE."
  - **Why unresolved:** The current study validates IKL only for adding new keypoints to a known category (e.g., adding keypoints to a human model).
  - **Evidence:** Experiments applying KAMP to a CAPE-pretrained backbone and evaluating performance on a sequence of unseen object categories.

- **Open Question 2:** Can uncertainty estimation be leveraged to filter noisy pseudo-labels during the Knowledge Association stage to improve robustness?
  - **Basis:** [explicit] Supplementary Section D proposes: "In the future, we can further explore leveraging the technique of uncertainty estimation to filter out those uncertain predictions from the old model."
  - **Why unresolved:** KAMP currently uses raw pseudo-labels from the old model to supervise the KA-Net, which can propagate errors if the old model is uncertain.
  - **Evidence:** An ablation study where KA-Net is trained only on high-confidence pseudo-labels (determined by an uncertainty metric) compared to the baseline using all pseudo-labels.

- **Open Question 3:** How can the method be adapted to maintain performance when new keypoints are anatomically distant from old keypoints (e.g., learning lower body keypoints after upper body)?
  - **Basis:** [inferred] Supplementary Section F.8 analyzes a setup where old and new keypoints are physically disconnected, showing that current spatial adjacency assumptions limit performance in this "extrapolation" scenario.
  - **Why unresolved:** KAMP relies on spatial adjacency to construct the auxiliary task; this inductive bias fails when relevant old and new keypoints are not locally adjacent.
  - **Evidence:** A modification to the auxiliary task creation (e.g., using semantic graph distance instead of spatial distance) tested on the "upper-body to lower-body" split protocol.

## Limitations

- The method's effectiveness depends on the assumption that spatially adjacent keypoints share learnable anatomical constraints, which may not hold for all keypoint configurations
- The KA-Net component requires at least two spatially related new keypoints per step (though single new keypoint cases can be handled with modifications)
- Performance may degrade when pseudo-labels from the old model contain significant errors, particularly in early incremental steps
- The optimal hyperparameter α is dataset-dependent and requires manual selection rather than automatic tuning

## Confidence

- **High Confidence:** The spatial distillation mechanism (ℓ_KSD) and its superiority over channel-wise distillation are well-supported by ablation studies (Table 2: 76.93% vs. 75.75% AAA4). The dual-teacher architecture with KA-Net and old model providing complementary knowledge is empirically validated.
- **Medium Confidence:** The generalizability of KAMP across different backbone architectures is supported by testing on both HRNet and RSN, though only two architectures were evaluated. The method's performance in extremely low-shot scenarios (≤50 training images) is demonstrated but not extensively explored.
- **Low Confidence:** The theoretical justification for why spatial softmax better preserves keypoint-specific knowledge compared to channel-wise normalization lacks rigorous mathematical proof. The selection of which old keypoint to associate with which new keypoints is currently manual and may not be optimal.

## Next Checks

1. **Ablation on Single New Keypoint:** Implement the modified KA-Net variant for single new keypoint scenarios (per Section E.1) and evaluate whether the performance degradation is minimal compared to multi-keypoint cases.

2. **Pseudo-label Quality Analysis:** Systematically evaluate the impact of pseudo-label accuracy on KAMP performance by injecting varying levels of noise into the old model predictions and measuring the degradation in AAA and AT metrics.

3. **Cross-dataset Transferability:** Test KAMP on a fourth dataset with fundamentally different keypoint structures (e.g., animal keypoints or vehicle keypoints) to assess whether the spatial distillation and KA-Net mechanisms generalize beyond human-centric keypoints.