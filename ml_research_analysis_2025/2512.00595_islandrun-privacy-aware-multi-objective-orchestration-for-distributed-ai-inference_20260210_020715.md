---
ver: rpa2
title: 'IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI
  Inference'
arxiv_id: '2512.00595'
source_url: https://arxiv.org/abs/2512.00595
tags:
- privacy
- cloud
- trust
- routing
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IslandRun addresses the multi-objective orchestration problem for
  AI inference across heterogeneous computing resources (personal devices, edge servers,
  public cloud) by treating resources as autonomous "islands" with varying trust,
  privacy, cost, and capacity profiles. The system introduces a novel agent-based
  architecture where specialized components (MIST for privacy, TIDE for resources,
  WAVES for routing, LIGHTHOUSE for coordination) decompose the optimization problem
  into modular, fault-tolerant units.
---

# IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference

## Quick Facts
- arXiv ID: 2512.00595
- Source URL: https://arxiv.org/abs/2512.00595
- Reference count: 38
- Primary result: Agent-based architecture for privacy-preserving AI inference routing across heterogeneous computing resources

## Executive Summary
IslandRun addresses the multi-objective orchestration problem for AI inference across heterogeneous computing resources (personal devices, edge servers, public cloud) by treating resources as autonomous "islands" with varying trust, privacy, cost, and capacity profiles. The system introduces a novel agent-based architecture where specialized components (MIST for privacy, TIDE for resources, WAVES for routing, LIGHTHOUSE for coordination) decompose the optimization problem into modular, fault-tolerant units. Each agent exposes standardized scoring functions that enable compositional reasoning about multi-dimensional routing decisions.

The core routing algorithm employs a constraint-based approach that ensures privacy preservation (Pj ≥ sr) while optimizing weighted combinations of cost, latency, and trust scores. Chat context privacy is maintained through reversible typed placeholder sanitization when crossing trust boundaries. The system demonstrates theoretical properties including zero privacy violations by design, cost optimization through local compute utilization, and graceful degradation under resource pressure via tiered routing priorities.

## Method Summary
IslandRun implements privacy-aware multi-objective routing for AI inference across heterogeneous islands using an agent-based architecture. The system takes inference requests with sensitivity scores and routes them through WAVES, which filters candidates using hard privacy constraints (Pj ≥ sr), then optimizes weighted combinations of cost, latency, and trust. MIST provides sensitivity detection via regex patterns and local classifiers, TIDE monitors resource capacity, and LIGHTHOUSE coordinates island topology. Cross-trust context migration uses reversible typed placeholders ([PERSON X], [LOCATION A]) to preserve semantic relationships while preventing PII exposure.

## Key Results
- Constraint-based privacy routing ensures zero violations by design through monotonic filtering
- Typed placeholder sanitization preserves conversational context semantics while preventing PII exposure
- Agent-based score aggregation via scalarization enables modular multi-objective optimization

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Based Privacy Routing
- Claim: Routing decisions preserve privacy through a monotonic constraint filter that eliminates non-compliant islands before optimization.
- Mechanism: WAVES filters candidate islands using Pj ≥ sr, where Pj is the island's privacy score and sr is the request's detected sensitivity. Only islands passing this filter enter cost-latency optimization (Algorithm 1, lines 4-9). This fail-closed approach rejects requests rather than degrading to lower-trust islands under resource pressure.
- Core assumption: MIST correctly detects data sensitivity sr, and island privacy scores Pj accurately reflect actual data handling practices.
- Evidence anchors:
  - [abstract] "ensures privacy preservation (Pj ≥ sr) while optimizing weighted combinations of cost, latency, and trust scores"
  - [Section III.A, Definition 3] "An island ij is eligible for request r if and only if its privacy score satisfies the request's sensitivity threshold: Pj ≥ sr"
  - [corpus] Weak direct evidence; related work (PWC-MoE, ELSA) addresses privacy in federated/split learning but not routing-time constraint enforcement.
- Break condition: If MIST sensitivity detection has false negatives (misses sensitive data), privacy constraint becomes ineffective regardless of routing logic.

### Mechanism 2: Typed Placeholder Sanitization for Context Continuity
- Claim: Reversible anonymization preserves conversational context semantics while preventing PII exposure to lower-trust islands.
- Mechanism: When routing from high-trust to low-trust island (P1 > P2), MIST applies transformation τ(hr) replacing PII with typed placeholders ([PERSON X], [LOCATION A]). A bidirectional mapping ϕ enables de-anonymization on response return (Section VII.B). Unlike redaction, placeholders maintain entity relationships, allowing remote LLMs to reason about structure.
- Core assumption: Placeholder types are coarse enough to prevent re-identification via statistical analysis, and adversaries cannot infer original values from co-occurrence patterns.
- Evidence anchors:
  - [abstract] "Chat context privacy is maintained through reversible typed placeholder sanitization when crossing trust boundaries"
  - [Section VII.B] "typed placeholders enable the cloud LLM to understand entity relationships (e.g., '[PERSON 1] visited [LOCATION A]') and generate contextually accurate responses"
  - [corpus] FedOnco-Bench addresses federated privacy but not placeholder-based context migration; corpus lacks direct validation of this approach.
- Break condition: Sophisticated inference attacks on placeholder patterns could de-anonymize data across sessions; formal differential privacy guarantees are explicitly noted as unproven (Section XII).

### Mechanism 3: Agent-Based Score Aggregation via Scalarization
- Claim: Multi-objective optimization decomposes into modular scoring functions that WAVES aggregates through weighted sum scalarization.
- Mechanism: Each agent exposes Score(r, ij) → [0,1]. WAVES computes composite score S(r, ij) = w1·Cj + w2·Lj + w3·(1-Pj) (Equation 1), selecting argmin S among constraint-satisfying islands. Agent isolation enables independent failure handling (MIST crash → assume sr=1.0).
- Core assumption: User preferences are accurately captured by linear scalarization weights; non-linear preferences (e.g., "privacy violations unacceptable at any cost") may be poorly approximated.
- Evidence anchors:
  - [Section IV.C] "Each agent exposes a standardized interface... Lower is better (0 = optimal, 1 = worst)"
  - [Section VI.C] "The greedy solution is Pareto-optimal if weights w1, w2, w3 accurately reflect user preferences"
  - [corpus] Toward Carbon-Aware Container Orchestration uses federated learning for prediction but not multi-objective agent decomposition; approach remains unvalidated externally.
- Break condition: Conflicting agent scores under resource exhaustion may cause thrashing; hysteresis (Section IX.C) mitigates but doesn't eliminate oscillation risk.

## Foundational Learning

- Concept: Pareto optimality and scalarization in multi-objective optimization
  - Why needed here: Understanding why weighted sums approximate preferences and when they fail (non-convex frontiers).
  - Quick check question: Given islands A(latency=100ms, cost=$0) and B(latency=50ms, cost=$0.01), which dominates if weights are w_latency=0.7, w_cost=0.3?

- Concept: K-anonymity and re-identification risk
  - Why needed here: Evaluating whether typed placeholders provide meaningful privacy against linkage attacks.
  - Quick check question: If placeholder [PERSON_1] appears in 3 queries discussing "diabetes treatment in Chicago," what re-identification vectors exist?

- Concept: Fail-closed vs fail-open system design
  - Why needed here: IslandRun explicitly chooses fail-closed (reject requests) over fail-open (degrade privacy); understanding this tradeoff is critical for threat modeling.
  - Quick check question: When TIDE crashes, what is the conservative fallback and why?

## Architecture Onboarding

- Component map:
  - Decision Layer: WAVES (routing orchestration), MIST (privacy scoring), TIDE (resource monitoring)
  - Coordination Layer: LIGHTHOUSE (mesh topology, heartbeat, island registry)
  - Execution Layer: SHORE (local islands, Tier 1-2), HORIZON (cloud islands, Tier 3)
  - Data Flow: Client → WAVES → [MIST scoring + TIDE capacity check] → Constraint filter → Island selection → Sanitization (if needed) → Execution

- Critical path: MIST sensitivity detection → WAVES constraint filtering → TIDE capacity validation → Island selection → Cross-tier sanitization. Latency budget: routing overhead claimed <10ms for n<10 islands, m≈50 regex patterns (Section VI.B).

- Design tradeoffs:
  - Privacy vs accuracy: Local 7B models produce lower-quality outputs than cloud 70B+ models; system prioritizes privacy by design.
  - Cost vs latency: Zero-cost local compute preferred but bounded; cloud provides unbounded capacity at financial and privacy cost.
  - Scalarization vs constraint-based: Equation 1 uses weighted sums; alternative hard-constraint approach described but not implemented (Section VI.C).

- Failure signatures:
  - MIST crash: All requests treated as sr=1.0 (maximum sensitivity), routing restricted to personal islands only.
  - TIDE crash: Rlocal(t) assumed 0, triggering immediate cloud fallback.
  - LIGHTHOUSE failure: Cached island list used; new islands undiscovered.
  - Privacy violation: Only possible via MIST false negatives (missed sensitive data) or incorrect trust score assignment during island registration.

- First 3 experiments:
  1. **Sensitivity detection accuracy**: Inject synthetic prompts with known PII (medical codes, SSNs, addresses); measure MIST recall/precision vs regex-only baseline.
  2. **Routing behavior under resource pressure**: Simulate TIDE reporting declining capacity (100%→20%); verify tiered routing triggers secondary/burstable cloud fallback while preserving primary local execution.
  3. **Placeholder re-identification resistance**: Generate 1000 queries with typed placeholders; attempt statistical linkage using entity co-occurrence and frequency analysis to quantify inference risk.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the actual privacy preservation rates, cost savings, and latency trade-offs of IslandRun under real-world workloads?
- Basis in paper: [explicit] "This paper presents a theoretical framework without empirical evaluation on real workloads. Future work (IEEE conference submission) will include controlled experiments measuring privacy preservation, cost savings, and latency trade-offs across diverse deployment scenarios."
- Why unresolved: The paper provides theoretical analysis and expected properties but lacks benchmark datasets and production deployments to validate quantitative claims.
- What evidence would resolve it: Controlled experiments with representative workloads (healthcare queries, legal document processing, code completion) measuring actual privacy violation rates, dollar-cost reductions, and latency distributions across multi-tier deployments.

### Open Question 2
- Question: Can typed placeholder sanitization provide formal differential privacy guarantees against statistical inference attacks?
- Basis in paper: [explicit] "While typed placeholders prevent direct data leakage, sophisticated adversaries may infer original values through statistical analysis (e.g., co-occurrence patterns, demographic correlation). Providing formal differential privacy guarantees for placeholder mappings remains an open challenge."
- Why unresolved: Current placeholder design uses coarse-grained types and session-randomized identifiers, but lacks formal analysis against correlation attacks across multiple user sessions or demographic inference.
- What evidence would resolve it: Formal proof or empirical demonstration that placeholder mappings satisfy ε-differential privacy, or identification of attack vectors that break k-anonymity guarantees under realistic adversarial models.

### Open Question 3
- Question: How can hardware-based attestation replace user-declared trust scores to provide cryptographic proof of island integrity?
- Basis in paper: [explicit] "Current trust scores rely on manual island registration. Future work will integrate hardware-based attestation (Intel SGX, AMD SEV, ARM TrustZone) to provide cryptographic proof of island integrity."
- Why unresolved: Trust scores are manually assigned during island registration, but incorrect assessments (e.g., assigning T=1.0 to a compromised server) undermine the entire security model.
- What evidence would resolve it: Working implementation integrating remote attestation protocols (measured boot chains, kernel integrity verification) with dynamic trust scoring that reflects runtime isolation guarantees rather than user-declared values.

## Limitations

- Privacy constraint enforcement depends entirely on MIST sensitivity detection accuracy
- Typed placeholder sanitization lacks formal privacy guarantees against statistical inference
- Multi-objective optimization uses weighted scalarization that may poorly represent user preferences

## Confidence

- High confidence: Agent-based architecture decomposition, constraint-based privacy routing mechanism, LIGHTHOUSE coordination layer design
- Medium confidence: Tiered island abstraction, hysteresis-based oscillation prevention, typed placeholder system feasibility
- Low confidence: MIST sensitivity detection accuracy, typed placeholder privacy guarantees, weighted scalarization preference representation

## Next Checks

1. **Sensitivity Detection Validation:** Create benchmark dataset of 10,000 prompts with ground-truth PII annotations (medical codes, SSNs, addresses, financial data). Measure MIST regex+classifier recall/precision against this dataset and compare to regex-only baseline.

2. **Privacy vs Context Tradeoff Analysis:** For 1,000 chat conversations with typed placeholders, conduct user studies measuring perceived context preservation quality while quantifying potential re-identification vectors through statistical linkage attacks on placeholder patterns.

3. **Routing Under Resource Pressure:** Simulate progressive resource exhaustion (100%→20% capacity across islands) while generating 10,000 inference requests. Verify tiered routing triggers appropriate cloud fallbacks, measure cost/latency impact, and confirm privacy constraints remain enforced throughout degradation.