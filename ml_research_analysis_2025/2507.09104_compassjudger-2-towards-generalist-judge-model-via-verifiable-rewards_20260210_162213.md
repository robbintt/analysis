---
ver: rpa2
title: 'CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards'
arxiv_id: '2507.09104'
source_url: https://arxiv.org/abs/2507.09104
tags:
- judge
- data
- loss
- arxiv
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CompassJudger-2 introduces a generalist judge model with task-driven,
  multi-domain data curation and verifiable rewards to overcome specialization and
  robustness limitations in existing judge models. The method employs rejection sampling
  with critical thinking chain-of-thought generation, and margin policy gradient loss
  to optimize judgment accuracy.
---

# CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards

## Quick Facts
- **arXiv ID:** 2507.09104
- **Source URL:** https://arxiv.org/abs/2507.09104
- **Reference count:** 40
- **Primary result:** 7B generalist judge model achieves 72.11% average score on multiple judge/reward benchmarks, competitive with much larger models

## Executive Summary
CompassJudger-2 introduces a generalist judge model that overcomes specialization and robustness limitations in existing judge models through task-driven, multi-domain data curation and verifiable rewards. The method employs rejection sampling with critical thinking chain-of-thought generation and margin policy gradient loss to optimize judgment accuracy. Empirically, CompassJudger-2 achieves state-of-the-art performance on multiple judge and reward benchmarks, with the 7B model reaching 72.11% average score—competitive with much larger models like DeepSeek-V3 and Qwen3-235B-A22B—and demonstrates strong generalization across objective and subjective tasks. The work also proposes JudgerBenchV2, a standardized benchmark using a Mix-of-Judgers as ground truth and novel metrics for both sample-level accuracy and rank consistency, advancing evaluation standards for judge models.

## Method Summary
CompassJudger-2 trains a generalist judge model through a multi-stage pipeline: (1) Data curation combining public judge/reward datasets with synthetic pairs generated by Qwen2.5-72B-Instruct, (2) Rejection sampling that generates 8 reasoning chains per prompt and filters to retain only those matching ground truth labels, (3) Training with a hybrid loss combining standard cross-entropy on non-prefix tokens with margin policy gradient loss at the prediction position (γ=10, top-10 logits). The model is initialized from Qwen2.5-Instruct (7B/32B), trained for 1 epoch with LR=6e-5 and batch size 512. The approach explicitly optimizes for verifiable rewards and critical thinking alignment while maintaining broad domain coverage.

## Key Results
- 7B CompassJudger-2 achieves 72.11% average score on multiple judge/reward benchmarks
- Outperforms much larger models (DeepSeek-V3, Qwen3-235B-A22B) on several individual benchmarks
- Strong generalization across objective tasks (knowledge-based) and subjective tasks (chat, style evaluation)
- Demonstrates state-of-the-art performance on both sample-level accuracy and rank consistency metrics

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Driven Domain Adaptation
The data pipeline reconstructs outdated judgments from older models using a stronger generator (Qwen2.5-72B) and synthesizes new "contrast pairs" for knowledge and chat domains. This forces the model to learn evaluation heuristics that transfer across different response styles and query types, rather than overfitting to a single prompt distribution. Core assumption: The generator model is significantly more capable than the target judge model, ensuring high-quality supervision.

### Mechanism 2: Verifiable Reward Optimization via Margin Loss
Instead of just predicting the next token (SFT), the model maximizes the probability of the correct judgment token (A or B) while enforcing a margin γ against the incorrect token. This explicitly optimizes the decision boundary for the classification task ("Which response is better?"). Core assumption: The ground truth label in the datasets is reliable and consistently verifiable (binary reward of 1 or 0).

### Mechanism 3: Rejection Sampling for Reasoning Alignment
The system generates M=8 candidate responses for a prompt and discards any path that leads to the wrong final judgment, training only on trajectories where the "reasoning" actually supports the "correct" label. Core assumption: The ability to generate a correct rationale is correlated with the ability to judge accurately; forcing alignment reduces hallucination in critiques.

## Foundational Learning

### Concept: Rejection Sampling Fine-Tuning (RFT)
- **Why needed here:** This is the core data filtration step. You must understand that they generate multiple solutions and keep only the correct ones to form the training set.
- **Quick check question:** If the generator model only achieves 50% accuracy on a hard task, how does rejection sampling affect the training data volume?

### Concept: Policy Gradient (RL) vs. Supervised Fine-Tuning (SFT)
- **Why needed here:** The paper proposes a hybrid loss. You need to distinguish between learning to mimic text (SFT) vs. learning to maximize a reward signal (RL/Policy Gradient).
- **Quick check question:** Why does the paper claim Policy Gradient is better for "exploration" than standard SFT with teacher forcing?

### Concept: Mix-of-Judgers (MoJ) Ground Truth
- **Why needed here:** To evaluate the judge, they need a ground truth. Understanding that they use a consensus of large models rather than human labels is crucial for interpreting the benchmark results.
- **Quick check question:** What are the potential failure modes of using model consensus instead of human annotation for ground truth?

## Architecture Onboarding

### Component map:
Qwen2.5-72B-Instruct (Synthesizer) -> Critical Thinking Prompts -> Rejection Sampler (Filter) -> Qwen2.5-7B/32B (Base) -> CompassJudger-2 (Trained) -> Hybrid Loss (SFT + Margin PG)

### Critical path:
1. **Data Curation:** Identify outdated/wrong judgments in public sets; regenerate with Qwen-72B
2. **RFT Generation:** Generate 8 reasoning chains per prompt; discard those with wrong final answer
3. **Training:** Apply Margin Loss to the final prediction token to sharpen the decision boundary

### Design tradeoffs:
- **Inference Cost vs. Quality:** Rejection sampling requires generating 8x the training data, significantly increasing pre-training compute costs
- **Generalization vs. Style Bias:** The model is trained to be "style-sensitive"; this may bias it against concise but correct answers

### Failure signatures:
- **Hallucinated Rationale:** The model produces a correct judgment but with logically flawed reasoning (CoT) because it was trained only on outcome-based rewards
- **Length Bias:** The style-training might cause the model to irrationally prefer longer responses

### First 3 experiments:
1. **Loss Ablation:** Compare standard SFT vs. DPO Loss vs. Margin Loss on the JudgerBenchV2 validation set
2. **RFT Scaling:** Train with rejection sampling rates of M=1, 4, 8 to measure impact of reasoning diversity on judgment accuracy
3. **Critique Utility:** Use the model to critique a policy model (e.g., Llama3-8B) and measure if the policy model improves after refinement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the CompassJudger-2 framework be effectively extended to multi-modal and interactive evaluation scenarios?
- **Basis in paper:** [explicit] The conclusion states that "extending this work to multi-modal and interactive evaluation scenarios will further enhance its applicability and impact."
- **Why unresolved:** The current study focuses exclusively on text-based large language models, leaving the efficacy of the data curation and policy gradient methods on image or audio inputs unexplored.

### Open Question 2
- **Question:** How can the high inference costs associated with rejection sampling be reduced without compromising judgment accuracy?
- **Basis in paper:** [explicit] Section 7 (Limitations) notes that "Rejection sampling incurs relatively higher inference costs."
- **Why unresolved:** The method requires generating multiple candidate responses (M samples) to approximate the policy gradient, which is computationally expensive compared to standard single-pass inference.

### Open Question 3
- **Question:** What mechanisms can effectively detect or mitigate hallucinations in the LLM-based data synthesis process?
- **Basis in paper:** [explicit] Section 7 (Limitations) states that "hallucinations produced by the LLM when synthesizing data may pose potential risks."
- **Why unresolved:** While the paper verifies final labels against ground truth, the Chain-of-Thought rationales generated by Qwen2.5-72B could contain plausible but factually incorrect reasoning that evades current filters.

## Limitations

- **Data Quality Dependency:** The effectiveness of rejection sampling depends heavily on the quality of the generator model (Qwen2.5-72B), and the paper doesn't provide quantitative analysis of how many candidates survive rejection sampling
- **Evaluation Benchmark Reliability:** Using Mix-of-Judgers as ground truth introduces potential failure modes including amplification of systematic biases and lack of human verification
- **Style Bias Concerns:** The "style-sensitive" training approach may introduce biases that favor certain response characteristics over others

## Confidence

| Claim | Confidence |
|-------|------------|
| Rejection sampling effectiveness | Medium |
| MoJ ground truth reliability | Low |
| Generalization claims | Medium |

## Next Checks

1. **Ground Truth Verification:** Conduct a small-scale human evaluation study (200-300 samples) comparing MoJ consensus judgments against human expert ratings across different domains to quantify the reliability of the evaluation benchmark itself.

2. **Distribution Shift Analysis:** Test CompassJudger-2 on out-of-distribution prompts that systematically differ from the training data (e.g., different cultural contexts, specialized technical domains, or adversarial prompt constructions) to assess true generalization capabilities.

3. **Rejection Sampling Efficiency:** Vary the rejection sampling parameter M (1, 4, 8, 16) and measure the trade-off between judgment accuracy and training data efficiency to determine whether the computational overhead is justified by performance gains.