---
ver: rpa2
title: Fixed Aggregation Features Can Rival GNNs
arxiv_id: '2601.19449'
source_url: https://arxiv.org/abs/2601.19449
tags:
- features
- aggregation
- graph
- gnns
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fixed Aggregation Features (FAFs) are a training-free approach
  that transforms graph learning tasks into tabular problems by applying fixed aggregation
  functions over neighborhoods at multiple hops and concatenating the results into
  a tabular feature matrix. This enables the use of well-established tabular methods,
  offering strong interpretability and flexibility to deploy diverse classifiers.
---

# Fixed Aggregation Features Can Rival GNNs

## Quick Facts
- arXiv ID: 2601.19449
- Source URL: https://arxiv.org/abs/2601.19449
- Reference count: 40
- Primary result: FAFs rival or outperform GNNs on 12/14 benchmarks using only mean aggregation

## Executive Summary
Fixed Aggregation Features (FAFs) transform graph learning into tabular problems by applying fixed aggregation functions over neighborhoods at multiple hops and concatenating the results into a feature matrix. This enables standard tabular methods to achieve strong performance on graph benchmarks, often matching or exceeding state-of-the-art GNNs. FAFs offer excellent interpretability and flexibility to deploy diverse classifiers while avoiding GNN-specific optimization challenges.

The approach shows that simple fixed aggregators like mean, sum, and max often suffice for most graph benchmarks, with diminishing information gains at higher hops. Theoretical analysis connects these findings to Kolmogorov-Arnold representations, showing lossless neighborhood aggregations exist but may be impractical due to numerical instability. FAFs provide both a strong baseline for graph learning and a diagnostic tool for understanding which datasets truly benefit from learned message passing.

## Method Summary
FAFs compute fixed aggregations over node neighborhoods at multiple hops using reducers like mean, sum, max, min, and std. For each node v and reducer r, initialize h^(0,r)_v = x_v and iterate h^(k,r)_v = r({h^(k-1,r)_u : u ∈ N(v)}) for k hops. All hop representations are concatenated with the original features to form z_v, which becomes input to a standard MLP classifier. The aggregation is computed once without gradients, decoupling representation from learning. FAFs are evaluated on 14 graph benchmarks using MLP classifiers with hyperparameter sweeps over dropout, learning rate, normalization, hidden size, and depth.

## Key Results
- FAFs rival or outperform state-of-the-art GNNs on 12 out of 14 benchmarks
- Mean aggregation alone often matches more complex FAF4 configurations
- FAFs significantly outperform GNNs on Coauthor-CS, Amazon-Ratings, and Questions datasets
- Only Roman Empire and Minesweeper datasets consistently require deep GNNs over FAFs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixed aggregation can preserve sufficient neighborhood information for downstream classification on many benchmarks.
- Mechanism: For 1-hop neighborhoods with approximately orthogonal features, sum and mean aggregation act as injective multiset functions—they preserve complete information about feature counts. The downstream MLP learns to decode this compressed representation into class predictions.
- Core assumption: Task-relevant signal is concentrated in low-hop neighborhood distributions rather than requiring learned, feature-dependent neighbor weighting.
- Evidence anchors: [abstract] "often using only mean aggregation"; [section 4, Theorem 4.1] "the function h(X) = Σx∈X x defined on multisets X⊆X of bounded size is injective"
- Break condition: Features are highly non-orthogonal, or the task requires distinguishing which specific neighbors contributed which values (not just aggregate counts).

### Mechanism 2
- Claim: Concatenating multi-hop aggregations prevents information loss during repeated aggregation rounds.
- Mechanism: At hop k≥2, aggregated features are no longer orthogonal, so subsequent aggregation loses distributional information. By concatenating h^(0), h^(1), ..., h^(k) rather than using only the final hop, the classifier retains access to the relatively lossless 0-hop and 1-hop representations.
- Core assumption: The MLP has sufficient capacity to weight different hop representations and extract task-relevant patterns from concatenated features.
- Evidence anchors: [section 4, Information loss for k-hops] "from k≥2, not all information about the distribution of features across neighbors is preserved"; [Table 10, Appendix D] Using only last hop (H_K) underperforms concatenating all hops
- Break condition: The task requires complex inter-hop interactions that cannot be captured by independent per-hop features.

### Mechanism 3
- Claim: Tabular optimization decouples representation from learning, improving trainability relative to end-to-end GNNs.
- Mechanism: FAFs fix the aggregation (no gradients through message passing), eliminating GNN-specific optimization issues: early overfitting during message-passing training, gradient vanishing through deep layers, and attention trainability failures. The MLP solves a well-conditioned tabular classification problem.
- Core assumption: Simple reducers (mean, sum, max, min, std) provide sufficient inductive bias that standard tabular optimization can succeed.
- Evidence anchors: [section 3.1, Optimization] "GNNs usually exhibit early overfitting, where training accuracy converges almost immediately while validation and test accuracy plateau"; [section 5, Table 9] MLP classifier outperforms single linear layer on FAF features
- Break condition: The task requires learned, adaptive aggregation (e.g., hop-specific weights, feature-dependent attention) that fixed reducers cannot express.

## Foundational Learning

- Concept: **Permutation-invariant aggregation over multisets**
  - Why needed here: FAFs treat node neighborhoods as multisets. Understanding what each aggregator preserves (sum: counts; mean: normalized counts; max/min: existence) is essential for reducer selection.
  - Quick check question: Given neighbor features [1,0], [1,0], [0,1], what does sum return? What does max return?

- Concept: **Information preservation vs. learnability gap**
  - Why needed here: The Kolmogorov-Arnold construction (Theorem 4.2) proves lossless fixed aggregation exists, but Table 11 shows it fails empirically due to numerical instability. Simple lossy aggregators train better.
  - Quick check question: Why doesn't the theoretically lossless KA function Φ achieve strong empirical results?

- Concept: **Feature orthogonality and multiset representation**
  - Why needed here: Theorem 4.1's injectivity for 1-hop aggregation requires orthogonal features. Real-world features may violate this, causing information loss even at hop 1.
  - Quick check question: If two feature vectors are nearly identical, can sum aggregation distinguish the multisets {[x₁, x₁, x₂]} from {[x₁, x₃, x₃]} where x₂ ≈ x₃?

## Architecture Onboarding

- Component map: Preprocessing stage (compute h^(k,r) for all nodes, reducers, hops) -> Concatenation (build z_v features) -> Classifier (MLP training)
- Critical path:
  1. Set K (max hops): Start with 2–4; increase only if validation improves (most datasets peak at k=2 per Table 7)
  2. Select reducers R: Begin with {mean}; add std or {mean,sum,max,min} (FAF4) if underperforming
  3. Precompute FAF features once (no gradients during aggregation)
  4. Train MLP with hyperparameter sweep: LR (1e-3 to 1e-2), dropout (0.2–0.7), normalization type, hidden size
- Design tradeoffs:
  - More reducers increase dimensionality and memory but provide complementary information
  - Higher K captures longer-range signal but shows diminishing returns and can cause overfitting
  - Mean alone often matches FAF4; use single-reducer when dimensionality is a concern
- Failure signatures:
  - Large train-test gap → overfitting; reduce MLP depth/width, increase dropout
  - FAF lags GNN by >5% → check if task needs deep message passing (Roman-Empire, Minesweeper patterns) or learned aggregation
  - KA aggregation fails to train → expected; Table 11 shows numerical instability
  - Single-reducer outperforms FAF4 → optimization difficulty from high input dimension
- First 3 experiments:
  1. **Hop ablation**: Run FAF4 with K=1, 2, 4. Identify where validation accuracy plateaus
  2. **Reducer ablation**: Compare FAFmean, FAFmean+std, FAF4 on same hyperparameter grid. Determine minimal sufficient reducer set
  3. **Classifier ablation**: Compare 1-layer linear vs. 2-layer MLP vs. 3-layer MLP on FAF4 features. Confirm nonlinearity is beneficial (Table 9)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What characteristics define a graph benchmark that strictly requires learned, adaptive neighborhood aggregations?
- Basis in paper: [explicit] The authors call for developing "new benchmarks that genuinely require long-range dependencies" and "benefit from learning diverse neighborhood aggregations."
- Why unresolved: FAFs rivaled SOTA GNNs on 12/14 benchmarks using only fixed mean/sum aggregation, suggesting current benchmarks fail to necessitate complex learned message passing.
- Evidence: Creation of datasets where standard GNNs consistently and significantly outperform well-tuned FAF baselines.

### Open Question 2
- Question: How can we design fixed aggregation functions that are both theoretically injective (lossless) and numerically stable?
- Basis in paper: [explicit] The paper identifies an "open challenge" to design embeddings that extract relevant information without the numerical brittleness of the Kolmogorov-Arnold representation.
- Why unresolved: Theoretical lossless aggregations exist but are unstable, while practical aggregators (mean, max) are stable but discard information.
- Evidence: A novel fixed aggregator that achieves higher performance on complex datasets like Roman-Empire without suffering from the instability of the theoretical construction.

### Open Question 3
- Question: Does the underperformance of FAFs on specific datasets stem from inherent information loss at high hops or the lack of trainable residual connections?
- Basis in paper: [inferred] The authors note FAFs trail on Minesweeper and Roman-Empire, hypothesizing these tasks require "hop-specific aggregations" similar to the linear residuals used in deep GNNs.
- Why unresolved: It is unclear if the failure is due to the fixed nature of the aggregation or the specific depth/residual architecture of the competing GNNs.
- Evidence: FAF variants incorporating learned hop-weights or residual-style feature connections matching GNN performance on these specific tasks.

## Limitations
- The theoretical injectivity results assume approximately orthogonal features, which may not hold in real-world datasets with correlated features.
- The Kolmogorov-Arnold construction proves lossless aggregation exists but fails empirically due to numerical instability, with no exploration of approximate solutions.
- FAFs consistently underperform GNNs by 5-10% on datasets requiring deep message passing, but the paper doesn't investigate whether learned aggregation or specific architectural features drive GNN success.

## Confidence

- **High Confidence**: FAFs rival GNNs on 12/14 benchmarks using simple aggregators like mean, as empirically demonstrated across multiple datasets with consistent results.
- **Medium Confidence**: The mechanism that concatenation prevents information loss is theoretically sound but lacks direct empirical validation beyond comparing H_K vs. concatenated features in one appendix table.
- **Medium Confidence**: The claim that tabular optimization improves trainability over end-to-end GNNs is supported by training curves but doesn't rule out optimizer or architecture differences as confounding factors.

## Next Checks

1. **Feature correlation ablation**: Measure pairwise feature correlations in benchmark datasets and correlate with FAF performance gaps. This would quantify how orthogonality assumptions affect real-world results.

2. **KA approximation study**: Implement regularized or approximate Kolmogorov-Arnold functions and test whether numerical stability can be achieved while maintaining lossless information preservation.

3. **Learned aggregation comparison**: Add a single trainable weight per hop or per reducer to FAFs and measure performance impact, testing whether fixed aggregation is truly sufficient or if minimal learning improves results.