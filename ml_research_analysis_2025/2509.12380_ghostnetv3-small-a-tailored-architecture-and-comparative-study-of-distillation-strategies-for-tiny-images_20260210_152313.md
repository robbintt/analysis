---
ver: rpa2
title: 'GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation
  Strategies for Tiny Images'
arxiv_id: '2509.12380'
source_url: https://arxiv.org/abs/2509.12380
tags:
- teacher
- distillation
- knowledge
- student
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores model compression techniques for deploying
  efficient neural networks on resource-constrained edge devices, focusing on GhostNetV3
  and its variants. The authors propose GhostNetV3-Small, an adapted architecture
  optimized for low-resolution images like CIFAR-10, and conduct a comparative study
  of knowledge distillation strategies including standard KD, teacher assistant, and
  ensemble teacher approaches.
---

# GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images

## Quick Facts
- arXiv ID: 2509.12380
- Source URL: https://arxiv.org/abs/2509.12380
- Authors: Florian Zager; Hamza A. A. Gardi
- Reference count: 32
- Primary result: GhostNetV3-Small (2.8x width) achieves 93.94% CIFAR-10 accuracy, outperforming baseline GhostNetV3-D (91.23%) while all distillation strategies reduced performance

## Executive Summary
This paper addresses the challenge of deploying efficient neural networks on edge devices for low-resolution image classification tasks. The authors propose GhostNetV3-Small, an architecture specifically optimized for 32×32 pixel inputs like CIFAR-10, and conduct a comparative study of knowledge distillation strategies. Surprisingly, while the architectural adaptation significantly improves performance, all tested distillation approaches (standard KD, teacher assistant, ensemble teacher) led to accuracy degradation ranging from -0.13% to -1.78% compared to baseline training.

## Method Summary
The study focuses on CIFAR-10 classification using GhostNetV3 variants, with training conducted using SGD with Nesterov momentum (0.9) and weight decay (0.01) for 200 epochs. The authors implement GhostNetV3-Small with width multipliers from 1.0x to 3.4x and compare against the default GhostNetV3-D architecture. Knowledge distillation experiments use various teacher models including ResNet-50, VGG-13, and ensemble approaches with standard KL-divergence-based loss functions. All experiments use consistent data augmentation (random crop with 4-pixel padding, horizontal flip) and are implemented in PyTorch with code available at https://github.com/FlorianREGAZ/PML.

## Key Results
- GhostNetV3-Small (2.8x width) achieves 93.94% accuracy, significantly outperforming default GhostNetV3-D (91.23%) on CIFAR-10
- All knowledge distillation strategies resulted in reduced accuracy compared to baseline training, with performance degradations of -0.13% to -1.78%
- Width scaling shows peak performance at 2.8x configuration (2.4M parameters), with slight accuracy decline beyond this point
- Baseline training with standard SGD consistently outperformed all KD variants across different student architectures

## Why This Works (Mechanism)

### Mechanism 1: Architectural Adaptation for Low-Resolution Inputs
Tailoring a network's spatial hierarchy for small images (32×32 px) yields higher gains than simply retraining a standard large-image architecture. Standard architectures optimized for 224×224 images likely employ downsampling schedules or receptive fields that destroy information on 32×32 inputs. By reducing complexity and modifying the architecture (GhostNetV3-Small), the model preserves spatial resolution and feature relevance appropriate for the input scale.

### Mechanism 2: Negative Transfer via Resolution Mismatch
Knowledge distillation degrades student performance when the teacher relies on visual features inaccessible at the student's input resolution. Teachers like EfficientNetV2 or ResNet-50 (trained on high-res data) encode "dark knowledge" (class similarities) based on high-frequency details. When the student operates on 32×32 images, it cannot replicate these features. The distillation loss forces the student to mimic outputs based on absent features, acting as regularization noise rather than helpful supervision.

### Mechanism 3: Width Scaling Saturation
Increasing network width improves performance on small images only up to a specific capacity threshold, after which overfitting occurs. Widening the network (increasing channel count) increases capacity. For the limited complexity of CIFAR-10, a width multiplier of 2.8x (2.4M params) provides sufficient representational power. Beyond this (e.g., 3.4x), the parameter count grows, but the dataset does not provide enough unique variance to justify the capacity, leading to slight generalization degradation.

## Foundational Learning

- Concept: **Knowledge Distillation (KD) Loss (L_KD)**
  - Why needed here: To understand why standard distillation failed. The formula (Eq 1) combines Cross-Entropy (hard labels) and KL-Divergence (soft teacher logits). The paper's results suggest the KL term was detrimental.
  - Quick check question: What happens to the gradient signal if the teacher's "soft" logits are derived from features the student cannot see?

- Concept: **Input Resolution vs. Receptive Field**
  - Why needed here: The core failure of the default GhostNetV3-D and the distillation teachers is the mismatch between their design (224px) and the task (32px).
  - Quick check question: If a model's first stride reduces a 32px image to 16px, how much spatial information is lost compared to reducing a 224px image to 112px?

- Concept: **Width Multiplier (Scaling Factor)**
  - Why needed here: The paper optimizes performance by tuning the "width" of GN-S (1.0x to 3.4x). Understanding this is crucial for replicating the "Small" variant.
  - Quick check question: Does increasing the width multiplier from 1.0x to 2.8x increase the depth of the network, the number of channels, or the input resolution?

## Architecture Onboarding

- Component map:
  Input: 32×32 RGB image (CIFAR-10) -> Backbone: GhostNetV3-Small (GN-S) -> Training Objective: Standard SGD + Cross-Entropy -> Optimal Config: GN-S (2.8x width, ~2.4M parameters)

- Critical path:
  1. Select GhostNetV3-Small architecture (not Default)
  2. Set Width Multiplier to 2.8x (verified peak performance)
  3. Train with Standard SGD (Momentum 0.9, Weight Decay 0.01)
  4. Avoid Knowledge Distillation (KD) unless teacher resolution matches student

- Design tradeoffs:
  - Accuracy vs. Size: Moving from 2.8x (2.4M params) to 3.4x (3.4M params) decreases accuracy (93.94% -> 93.61%) while increasing model size
  - Training Complexity vs. Performance: Using KD (Teacher/Ensemble) increases training complexity but results in worse accuracy (-0.13% to -1.78%) compared to baseline

- Failure signatures:
  - KD Degradation: If using EfficientNetV2 or ResNet-50 as teachers, expect a drop in Top-1 accuracy (e.g., -1.78% delta)
  - Over-capacity: If using GN-S 3.4x, expect slightly lower accuracy than 2.8x despite higher parameter count
  - Resolution Mismatch: Using default GN-D (designed for ImageNet) results in 91.23% accuracy, significantly lower than the adapted GN-S (93.94%)

- First 3 experiments:
  1. Baseline Validation: Train GhostNetV3-Small (2.8x) on CIFAR-10 with standard augmentation (random crop, horizontal flip). Target: >93.9% Top-1 accuracy
  2. Width Sweep: Train GN-S variants (1.0x, 1.3x, 1.6x, 1.9x) to confirm the performance curve peaks at 2.8x before dropping at 3.4x
  3. Distillation Negative Test: Attempt KD using ResNet-50 (Teacher) -> GN-S (1.0x Student). Verify the negative delta (performance drop) to confirm the paper's finding that distillation is detrimental in this specific low-res setup

## Open Questions the Paper Calls Out

### Open Question 1
Do the architectural benefits of GhostNetV3-Small and the observed negative effects of knowledge distillation generalize to datasets beyond CIFAR-10? The entire experimental scope was limited to a single low-resolution dataset (32x32 pixels), leaving the model's behavior on higher resolutions or different data distributions unknown. Replicating the GhostNetV3-Small training and distillation experiments on datasets like ImageNet or CIFAR-100 and observing if distillation remains detrimental would resolve this.

### Open Question 2
Can advanced distillation techniques like AMTML-KD or DGKD prevent the performance degradation observed with standard distillation methods on tiny images? The study only tested standard KD, Teacher Assistant, and Ensemble Teacher approaches, all of which failed to improve upon the baseline. Applying adaptive or densely guided distillation algorithms to the GhostNetV3-Small architecture and achieving a positive delta over the non-distilled baseline would resolve this.

### Open Question 3
Do transformer-based teacher models offer better compatibility for distilling knowledge into compact CNNs on low-resolution tasks compared to CNN teachers? The paper explicitly suggests future work should "explore a wider variety of teacher models, including those based on transformer architectures." The current study used CNN-based teachers (ResNet, VGG, EfficientNet), which may have contributed to the representational alignment issues hypothesized as a cause for negative results. A comparative study measuring student performance when distilling from Vision Transformers (ViT) versus standard CNNs on the CIFAR-10 dataset would resolve this.

## Limitations
- The paper does not fully specify the architectural modifications in GhostNetV3-Small beyond stating "reduced complexity"
- Exact knowledge distillation hyperparameters (α, T) are not specified in the methodology
- The study is limited to a single low-resolution dataset (CIFAR-10), limiting generalizability claims

## Confidence

- High confidence in the core finding that architectural adaptation (GN-S) outperforms the default GhostNetV3-D on CIFAR-10, given the clear performance gap (93.94% vs 91.23%) and supporting evidence from resolution-aware feature preservation literature
- Medium confidence in the negative distillation results, as the claim that KD degrades performance is supported but the specific mechanisms (resolution mismatch vs. teacher quality) require further validation
- Low confidence in the width scaling saturation claim (Mechanism 3) due to weak or missing corpus evidence specifically for GhostNet variants

## Next Checks

1. **Architectural fidelity check**: Implement and verify GhostNetV3-Small's architectural modifications through ablation studies on key design choices (e.g., stage reductions, stride patterns) to confirm the reported performance gains

2. **KD hyperparameter sensitivity**: Systematically vary α (CE/KL trade-off) and temperature T to identify conditions under which KD might succeed or fail, testing the resolution mismatch hypothesis

3. **Domain alignment test**: Train teachers specifically on CIFAR-10 (rather than ImageNet) and re-run KD experiments to determine if negative transfer is truly due to resolution mismatch versus other factors