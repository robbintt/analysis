---
ver: rpa2
title: 'PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded
  Evaluation'
arxiv_id: '2512.05930'
source_url: https://arxiv.org/abs/2512.05930
tags:
- reasoning
- problem
- figure
- numerical
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRiSM is a multimodal benchmark for evaluating scientific reasoning
  in vision-language models. It uses a dynamic agent-based pipeline to generate 24,750
  university-level physics and math problems with textual, visual, and symbolic components,
  including executable Python code for verification.
---

# PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation

## Quick Facts
- arXiv ID: 2512.05930
- Source URL: https://arxiv.org/abs/2512.05930
- Authors: Shima Imani; Seungwhan Moon; Adel Ahmadyan; Lu Zhang; Kirmani Ahmed; Babak Damavandi
- Reference count: 31
- Primary result: PRiSM is a multimodal benchmark for evaluating scientific reasoning in vision-language models

## Executive Summary
PRiSM is a multimodal benchmark designed to evaluate scientific reasoning in vision-language models (VLMs) through university-level physics and mathematics problems. The benchmark uses a dynamic agent-based pipeline to generate 24,750 problems with textual, visual, and symbolic components, including executable Python code for verification. Five tasks assess different aspects of reasoning: robustness to input variations, visual perturbation resilience, error correction, programmatic solution synthesis, and ambiguity handling. Across tasks, models like Gemini-2.5 Pro and o4-mini-high achieve up to 80% accuracy, but scores drop significantly under variation and ambiguity, revealing key gaps in scientific reasoning capabilities.

## Method Summary
PRiSM uses the PrismAgent pipeline to process undergraduate physics and math PDFs into structured problems with textual prompts, generated figures, step-by-step reasoning, and executable Python code. The pipeline employs LLaMA-3.2-90B-Vision-Instruct for structured extraction and generation, SymPy for symbolic mathematics, and Pint for unit validation. Each problem generates N=5 perturbed variations to test robustness. The benchmark evaluates VLMs across five tasks measuring robustness, visual perturbation, reasoning correction, programmatic synthesis, and ambiguity handling using metrics including Overall Accuracy, TRUE Score (≥90% across variations), Volatility Rate, and Total Failure Rate (TFR).

## Key Results
- Gemini-2.5 Pro and o4-mini-high achieve up to 80% accuracy on standard problems, but scores drop under variation and ambiguity
- Task IV (programmatic synthesis) shows 75%+ failure rates across all models, indicating code generation is a distinct challenge from answer prediction
- Models rarely request clarification under uncertainty (3-4% deferral rate), instead making unjustified assumptions
- TRUE Score and Volatility metrics reveal that high overall accuracy often masks unreliable reasoning across variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic parameterization enables controlled probing of model robustness across semantically equivalent variations.
- Mechanism: PrismAgent generates problems with symbolic placeholders, then substitutes concrete values to create multiple variations (N=5 per task). This isolates whether failures stem from reasoning gaps vs. surface-level sensitivity.
- Core assumption: Models with genuine reasoning capability should maintain consistent accuracy across paraphrased text and varied numerical inputs.
- Evidence anchors: [abstract] "The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing"; [section 4] "For each problem instance, we generate N=5 perturbed versions aligned with the task-specific variation"

### Mechanism 2
- Claim: Executable Python code as ground truth enables automated verification of symbolic and dimensional correctness that static answer keys cannot provide.
- Mechanism: Each problem instance includes SymPy-based symbolic computation and Pint-based unit validation. The synthesized code is executed with concrete inputs to verify outputs against expected results, with iterative correction on failure.
- Core assumption: Scientific reasoning errors manifest either as dimensional inconsistencies or incorrect symbolic derivations that executable code will catch.
- Evidence anchors: [section 3.3] "Pint enforces unit consistency throughout computations, while SymPy supports symbolic manipulation, algebraic simplification, and equation solving"

### Mechanism 3
- Claim: Structured error taxonomies in Task III reveal whether models can detect and correct specific failure modes in reasoning chains.
- Mechanism: Four error types are synthetically injected (conceptual, careless, encoding, knowledge gaps). Models must identify errors and provide corrected solutions rather than simply accepting flawed premises.
- Core assumption: Models that implicitly correct without explicit error identification lack diagnostic reasoning transparency needed for educational or high-stakes applications.
- Evidence anchors: [section 4] "We present models with partially incorrect solutions, which are synthetically generated... to simulate common student reasoning mistakes"

## Foundational Learning

- Concept: **VLM Robustness Metrics (TRUE Score, Volatility, TFR)**
  - Why needed here: Standard accuracy obscures whether models solve consistently or get lucky on specific phrasings. TRUE score (≥90% accuracy across variations) measures reliable understanding; Volatility captures 40-60% consistency (unreliable); TFR identifies problems where no variation succeeds.
  - Quick check question: A model achieves 70% overall accuracy but 5% TRUE score on Task I. What does this indicate about its generalization?

- Concept: **Symbolic vs. Numerical Reasoning Decoupling**
  - Why needed here: Task IV requires generating code with symbolic inputs that generalizes across concrete substitutions. This separates symbolic understanding from numerical calculation ability.
  - Quick check question: Why does Task IV show 75%+ failure rates when Task I shows 50-80% accuracy on the same problems?

- Concept: **Clarification Deferral Under Uncertainty**
  - Why needed here: Task V reveals models rarely request clarification (3-4% deferral rate) when inputs are masked, instead making unjustified assumptions. This is critical for reliable deployment in scientific contexts.
  - Quick check question: A model outputs "for simplicity, assume x = y" when x is undefined. What failure mode does this represent, and why is it problematic?

## Architecture Onboarding

- Component map: OCR Extraction Layer -> PrismAgent -> Code Synthesis Module -> Validation Pipeline -> Figure Generation -> Instance Generator

- Critical path: OCR → PrismAgent extraction → Parameterization → Code synthesis + validation → Figure generation → Substitution → Human review (all examples manually filtered)

- Design tradeoffs:
  - Synthetic data enables controlled variation but may not capture edge cases in authentic problems
  - LLM-as-judge for Task V introduces potential bias; paper uses manual review of 100 samples to validate
  - Python code requirement restricts evaluation to models capable of code synthesis, excluding some VLMs

- Failure signatures:
  - **Modality conflict**: Correct algebraic reasoning overridden by misleading visual cues (Appendix A)
  - **Ambiguity-induced errors**: Misreading handwritten digits or decimal points without cross-modal verification
  - **Multi-step collapse**: Collapsing chained operations (e.g., exponentials + unit conversions) into single steps causes errors
  - **Numerical precision**: Correct symbolic expressions but wrong significant digits or magnitude errors

- First 3 experiments:
  1. **Baseline Task I-II comparison**: Run target model on 100 problems with 5 variations each; compute TRUE score and Volatility to establish robustness baseline. Compare visual perturbation (Task II) vs. textual variation (Task I) sensitivity.
  2. **Code synthesis error taxonomy**: Execute Task IV on 50 problems; classify failures into syntactic (missing imports, parameter errors) vs. conceptual (wrong formulas, incorrect symbolic derivations) to identify intervention targets.
  3. **Ambiguity behavior audit**: Run Task V on 50 masked problems; manually code responses as (a) deferral/clarification request, (b) symbolic placeholder use, or (c) unjustified assumption. Identify which visual or contextual cues trigger assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can enabling models to self-correct by providing execution tools improve their programmatic synthesis performance on Task IV?
- Basis in paper: [explicit] "For future work, we intend to investigate these metrics after enabling the model to self-correct by providing them with the execution tool to run their code."
- Why unresolved: Current Task IV evaluation shows high Total Failure Rates (75-78%) across all models due to syntactic and conceptual errors, but models were not given access to execution feedback for iterative correction.
- What evidence would resolve it: Re-run Task IV with an interactive code execution environment, measuring whether syntax errors decrease and whether conceptual corrections occur after runtime feedback.

### Open Question 2
- Question: What mechanisms can reliably induce models to request clarification rather than make unjustified assumptions when faced with ambiguous inputs?
- Basis in paper: [explicit] "Our findings underscore the need for explicit deferral or clarification capabilities in VLMs" and models "prioritize providing an answer even when such an answer is not logically supportable."
- Why unresolved: Despite explicit instructions to defer, models requested clarification only 3-4% of the time in Task V, instead defaulting to unsupported assumptions or partial symbolic simplifications.
- What evidence would resolve it: Develop and test intervention methods (e.g., uncertainty-aware training, explicit deferral tokens) that measurably increase clarification rates on ambiguous PRiSM problems.

### Open Question 3
- Question: How can vision-language models be improved to resist overriding correct symbolic reasoning with misleading visual cues?
- Basis in paper: [explicit] "Despite correctly solving problems through algebraic or symbolic reasoning, the model sometimes overrides these solutions due to ambiguity or noise present in the visual modality."
- Why unresolved: The modality conflict failure mode is documented (Figure 4), but no architectural or training interventions are proposed to ensure symbolic reasoning takes precedence over potentially misleading perceptual inputs.
- What evidence would resolve it: Design models with explicit verification mechanisms that reconcile perceptual and symbolic reasoning, then measure reduction in modality conflict errors on controlled PRiSM examples.

## Limitations
- Synthetic data generation may not capture all authentic reasoning complexity despite manual review requirements
- LLM-as-judge approach for Task V introduces potential bias in ambiguity evaluation
- Python code requirement limits evaluation to models capable of code synthesis, excluding some VLMs

## Confidence
- **High Confidence**: The core mechanism of using executable Python code for automated ground truth verification is well-supported and technically sound
- **Medium Confidence**: The robustness metrics (TRUE Score, Volatility Rate) provide meaningful differentiation between models, but their interpretation depends on the assumption that N=5 variations per task is sufficient
- **Low Confidence**: The specific claim that PRiSM uniquely addresses "reasoning gaps" in VLMs is difficult to verify given limited prior work on reasoning correction evaluation

## Next Checks
1. **Variation Sensitivity Analysis**: Test whether performance changes when increasing N from 5 to 10 variations per task. Significant shifts would indicate the current sample size may not capture true robustness patterns.

2. **Human Expert Validation**: Have domain experts evaluate a random sample of 100 generated problems across all tasks to verify that synthetic problems maintain authentic scientific reasoning complexity and that automated ground truth matches expert solutions.

3. **Cross-Domain Transfer**: Apply PRiSM evaluation to VLMs on problems from different STEM domains (chemistry, biology) not used in benchmark creation to test whether the identified reasoning gaps generalize beyond physics and mathematics.