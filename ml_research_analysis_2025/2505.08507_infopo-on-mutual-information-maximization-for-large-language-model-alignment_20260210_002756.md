---
ver: rpa2
title: 'InfoPO: On Mutual Information Maximization for Large Language Model Alignment'
arxiv_id: '2505.08507'
source_url: https://arxiv.org/abs/2505.08507
tags:
- infopo
- chosen
- arxiv
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) with human preferences using preference data. The core method, InfoPO, is
  a novel preference optimization algorithm based on mutual information maximization
  that eliminates the reliance on the Bradley-Terry (BT) model assumption, which is
  prone to overfitting and suboptimal performance, particularly on reasoning-heavy
  tasks.
---

# InfoPO: On Mutual Information Maximization for Large Language Model Alignment

## Quick Facts
- arXiv ID: 2505.08507
- Source URL: https://arxiv.org/abs/2505.08507
- Reference count: 23
- One-line primary result: InfoPO consistently outperforms DPO on reasoning and generation tasks by using NWJ mutual information estimation to prevent chosen response likelihood from decreasing during training.

## Executive Summary
This paper introduces InfoPO, a novel preference optimization algorithm that addresses the key limitation of DPO where chosen response likelihood tends to decrease during training. InfoPO reframes preference optimization as mutual information maximization and employs the NWJ estimator instead of InfoNCE, which changes the bias-variance tradeoff in gradient estimation. The method stabilizes training dynamics by using a fixed reference policy in gradient weights and implicitly minimizes reverse KL divergence to the chosen response distribution, promoting mode-seeking behavior that concentrates probability on high-quality outputs.

Extensive experiments demonstrate InfoPO's effectiveness across multiple benchmarks including the HuggingFace Open LLM Leaderboard, AlpacaEval 2, and coding tasks. The method achieves consistent improvements over DPO, particularly on reasoning-heavy tasks like MATH and GSM8K, with relative gains exceeding 12% on Mistral-7B for math tasks. InfoPO also shows stronger performance in summarization and dialogue tasks, achieving win rates of at least 60% against chosen responses.

## Method Summary
InfoPO is a preference optimization algorithm that replaces DPO's InfoNCE-based mutual information estimation with the NWJ estimator. The loss function is `L = -log πθ(yw|x) + πθ(yl|x)/πref(yl|x)`, where the second term uses a fixed reference policy πref (typically the SFT checkpoint) to prevent gradient explosion on rejected responses. The method trains for one epoch with batch size 128 using Adam optimizer and cosine learning rate schedule with 10% warmup. Length normalization is applied to log-probabilities to mitigate length bias. The algorithm implicitly minimizes reverse KL divergence between the learned policy and the chosen response distribution.

## Key Results
- InfoPO achieves consistent improvements over DPO across reasoning and generation tasks
- Relative gains exceeding 12% on Mistral-7B for Math tasks (MATH, GSM8K)
- Win rate of at least 60% against chosen responses in summarization and dialogue tasks
- Maintains stable chosen response likelihood during training, unlike DPO which shows decline
- Effective across multiple models (Mistral-7B, Llama3-8B) and benchmarks (Open LLM Leaderboard, AlpacaEval 2)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Weight Stabilization via Fixed Reference Policy
Replacing the dynamic model probability in the gradient weight with the fixed reference probability prevents runaway gradient magnitudes on rejected responses. In DPO, the gradient weight for rejected responses is `1/πθ(yl|x)`, which grows unboundedly as the model suppresses rejected responses. InfoPO uses `1/πref(yl|x)` instead—a fixed quantity that bounds the gradient norm. This causes "conservative" updates on rejected responses while maintaining upward pressure on chosen response likelihood.

### Mechanism 2: NWJ Estimator Substitution for Conditional Mutual Information
Using the NWJ mutual information estimator instead of InfoNCE changes the bias-variance tradeoff and avoids the Bradley-Terry sigmoid formulation. The paper proves DPO's objective is equivalent to maximizing conditional mutual information `I(Y; C|X)` using the InfoNCE lower bound (contrastive). InfoPO substitutes the NWJ lower bound `E[f(y,c)] - E[exp(f(y,c))] + 1`, which yields a different loss: `-log πθ(yw|x) + πθ(yl|x)/πref(yl|x)`. NWJ has lower bias but higher variance than InfoNCE; this tradeoff empirically improves reasoning-heavy tasks.

### Mechanism 3: Reverse KL Divergence Minimization to Chosen Distribution
InfoPO implicitly minimizes `DKL(πθ || πchosen)`, promoting mode-seeking behavior that concentrates probability on high-reward responses. Theorem 4.1 proves InfoPO's objective is equivalent to minimizing reverse KL divergence between the learned policy and the unknown chosen response distribution. Reverse KL is mode-seeking (puts mass where `πchosen` is high) versus forward KL which is mode-covering. This aligns with alignment goals: generate focused high-quality outputs rather than cover all possibilities.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) and the Bradley-Terry model**
  - Why needed here: InfoPO is positioned as a drop-in alternative to DPO that avoids the BT assumption. Understanding DPO's implicit reward formulation and why BT can cause overfitting is essential to grasp InfoPO's motivation.
  - Quick check question: Can you derive why DPO's loss contains a sigmoid of log-likelihood ratios, and what happens when the model confidently rejects a response?

- **Concept: Mutual Information (MI) estimation methods (InfoNCE vs. NWJ)**
  - Why needed here: The core theoretical contribution reframes DPO as MI maximization and substitutes estimators. Understanding the bias-variance properties of different MI bounds is necessary to interpret why NWJ might work better.
  - Quick check question: Given samples from joint `P(Y,C)` and product of marginals `P(Y)P(C)`, how would you construct an InfoNCE loss versus an NWJ loss?

- **Concept: Reverse vs. Forward KL Divergence behavior**
  - Why needed here: Theorem 4.1 connects InfoPO to reverse KL minimization. The distinction between mode-seeking (reverse) and mode-covering (forward) KL directly impacts what kinds of outputs the aligned model produces.
  - Quick check question: If `πchosen` is a mixture of two Gaussians, will minimizing `DKL(q || πchosen)` or `DKL(πchosen || q)` tend to put mass on both modes? Which does InfoPO use?

## Architecture Onboarding

- **Component map**: Input (preference pairs) -> Loss function (InfoPO loss) -> Optimizer (Adam) -> Output (aligned policy πθ)
- **Critical path**: Start from SFT model as both πref and πθ initialization → Compute log-likelihoods of chosen and rejected responses under current πθ → Compute reference probabilities πref(yl|x) (fixed, no gradients) → Form loss as negative chosen log-likelihood plus rejected likelihood ratio → Backpropagate and update πθ only (reference remains frozen) → Monitor chosen/rejected likelihood trajectories to verify training dynamics
- **Design tradeoffs**: β parameter search [0.5, 1.0, 2.0]; length normalization applied (as in SimPO); learning rate search [3e-7, 5e-7, 6e-7, 1e-6]; batch size 128; single epoch training
- **Failure signatures**: Chosen likelihood still decreases (check πref implementation); training instability/loss spikes (NWJ variance may require gradient clipping); poor performance on instruction-following (over-regularization, try lowering β); minimal improvement over DPO (verify length normalization, check rejected response similarity)
- **First 3 experiments**: 1) Likelihood dynamics reproduction: Train InfoPO and DPO on UltraFeedback subset, plot chosen/rejected likelihood curves over steps. 2) Ablation on β values: Run InfoPO with β ∈ {0.5, 1.0, 2.0} on GSM8K and AlpacaEval 2. 3) Off-policy vs. on-policy comparison: Generate preference pairs using SFT model versus using provided dataset pairs, evaluate on MATH and GSM8K.

## Open Questions the Paper Calls Out

- Can InfoPO be further improved by employing alternative mutual information estimators beyond the NWJ estimator?
- What theoretical principles determine which mutual information estimation technique is most effective for aligning large language models?
- Does the high variance associated with the NWJ estimator introduce training instability in InfoPO compared to low-variance methods like DPO?

## Limitations

- The paper does not provide statistical significance testing for reported improvements
- Ablation studies on key hyperparameters like β are incomplete
- No direct experimental validation of the bias-variance tradeoff between InfoNCE and NWJ estimators
- The theoretical benefit of reverse KL divergence is not experimentally isolated

## Confidence

- **Theoretical framework (DPO ↔ MI, InfoPO ↔ NWJ)**: High confidence
- **Empirical performance gains over DPO**: Medium-High confidence
- **NWJ estimator bias-variance tradeoff justification**: Medium confidence (theoretical but not directly validated)
- **Reverse KL as optimal for alignment**: Medium confidence (theoretical, not directly tested)

## Next Checks

1. **Likelihood dynamics validation**: Reproduce Figure 2 by training both DPO and InfoPO on a subset of UltraFeedback and plotting chosen/rejected likelihood trajectories. Confirm InfoPO stabilizes chosen likelihood while DPO causes decline.

2. **β hyperparameter ablation**: Systematically vary β ∈ {0.5, 1.0, 2.0} on reasoning tasks (MATH, GSM8K) and instruction-following (AlpacaEval 2). Identify the optimal β and verify the tradeoff between reasoning gains and instruction-following retention.

3. **On-policy vs. off-policy comparison**: Generate preference pairs using the SFT model (on-policy) and compare against the provided UltraFeedback pairs (off-policy). Evaluate both settings on reasoning benchmarks to quantify the impact of response regeneration.