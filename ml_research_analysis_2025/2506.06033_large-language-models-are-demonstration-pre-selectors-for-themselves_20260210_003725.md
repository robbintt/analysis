---
ver: rpa2
title: Large Language Models are Demonstration Pre-Selectors for Themselves
arxiv_id: '2506.06033'
source_url: https://arxiv.org/abs/2506.06033
tags:
- feeder
- dtrain
- performance
- data
- dfeeder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEEDER, a pre-selection framework that identifies
  a representative subset of demonstrations for in-context learning (ICL) and fine-tuning
  of large language models (LLMs). By introducing "sufficiency" and "necessity" metrics,
  FEEDER evaluates whether plugging in or unplugging specific demonstrations affects
  the LLM's ability to generate correct outputs.
---

# Large Language Models are Demonstration Pre-Selectors for Themselves

## Quick Facts
- arXiv ID: 2506.06033
- Source URL: https://arxiv.org/abs/2506.06033
- Reference count: 40
- Key outcome: Reduces training data size by >20% while maintaining performance through sufficiency-necessity demonstration pre-selection

## Executive Summary
This paper introduces FEEDER, a framework that identifies representative demonstration subsets for in-context learning and fine-tuning of large language models. By introducing "sufficiency" and "necessity" metrics, FEEDER evaluates whether plugging in or unplugging specific demonstrations affects the LLM's ability to generate correct outputs. A tree-based approximation algorithm efficiently identifies these representative examples, reducing training data size by over 20% while maintaining performance. Experiments with LLMs ranging from 300M to 8B parameters show that FEEDER improves both ICL and fine-tuning efficiency, seamlessly integrating with various downstream demonstration selection strategies.

## Method Summary
FEEDER uses a tree-based approximation algorithm that leverages LLM one-shot inference to assess sufficiency relationships between training examples. The framework iteratively checks whether one demonstration enables correct answers to another, merging or pruning redundant examples while maintaining representative coverage. For fine-tuning integration, FEEDER employs bi-level optimization where the pre-selected subset is refined as the LLM's capabilities evolve during training. The method reduces computational complexity from O(2^N) to O(R·K·log|DTRAIN|²) while preserving performance through sufficiency guarantees.

## Key Results
- Reduces training data size by over 20% while maintaining or improving performance
- Works effectively across LLMs ranging from 300M to 8B parameters
- One-shot inference (K=1) is sufficient for reliable sufficiency assessment
- Improves both ICL demonstration selection and fine-tuning efficiency through bi-level optimization

## Why This Works (Mechanism)

### Mechanism 1: Sufficiency-Necessity Evaluation Framework
- Claim: FEEDER identifies representative demonstrations by evaluating whether plugging in or removing specific examples affects the LLM's ability to generate correct outputs, rather than relying on generic similarity or diversity metrics.
- Mechanism: The framework introduces two complementary metrics: (1) Sufficiency checks if adding demonstration (xn, yn) enables the LLM to correctly answer input xm; (2) Necessity checks if removing (xn, yn) causes the LLM to produce incorrect output for xm. This creates a model-specific assessment of demonstration utility.
- Core assumption: The sufficiency relationship is transitive across sets (if A is sufficient for B and B for C, then A is sufficient for C).
- Evidence anchors:
  - [abstract] "By introducing 'sufficiency' and 'necessity' metrics, FEEDER evaluates whether plugging in or unplugging specific demonstrations affects the LLM's ability to generate correct outputs."
  - [Section 4] "The sufficiency metric is introduced to assess whether plugging in one data point is adequate for the LLM to produce the correct answer to another data point."
  - [corpus] Related work (MarginSel, demonstration preference learning) focuses on similarity/uncertainty metrics; no corpus papers use sufficiency-necessity evaluation.
- Break condition: If sufficiency is not transitive for certain demonstration types, the tree-based approximation may not preserve representativeness.

### Mechanism 2: Tree-Based Approximation with LLM-Guided Pruning
- Claim: The tree-based algorithm reduces computational complexity from O(2^N) to O(R·K·log|DTRAIN|²) while maintaining sufficiency guarantees through iterative pairwise evaluation.
- Mechanism: The algorithm initializes each training sample as a leaf node, then iteratively checks sufficiency relationships between node pairs. If node Wi is sufficient for Wj, it replaces the pair with Wi (the smaller set). This bottom-up tree construction leverages the LLM's one-shot inference to identify redundant demonstrations without exhaustive subset enumeration.
- Core assumption: One-shot inference by the LLM is sufficient to reliably assess sufficiency between demonstration pairs.
- Evidence anchors:
  - [Section 4] "We develop a tree-based approximation algorithm that leverages the capability of the given LLM to assess whether each subset is sufficient and necessary for representing others."
  - [Section 5.4] "Setting K = 1 already produces excellent performance. This indicates that a one-shot inference by LLM is sufficient to assess the sufficiency between each pair of samples."
  - [corpus] No corpus papers implement tree-based demonstration pruning; related methods use clustering or retrieval-based approaches.
- Break condition: If LLM one-shot inference is unreliable for certain task types (e.g., complex reasoning), the approximation may incorrectly prune necessary demonstrations.

### Mechanism 3: Bi-Level Optimization for Fine-Tuning Integration
- Claim: Pre-selected demonstrations enhance fine-tuning efficiency through iterative refinement where the LLM tunes on the selected subset and the subset re-selects based on the tuned LLM's capabilities.
- Mechanism: The outer loop updates DFEEDER using the frozen LLM to identify representative examples. The inner loop fine-tunes the LLM parameters on the fixed DFEEDER subset. This alternating optimization allows the data selection to adapt to the LLM's evolving knowledge state.
- Core assumption: Fine-tuning on a high-quality subset provides comparable or better performance than fine-tuning on the full dataset.
- Evidence anchors:
  - [Algorithm 1] "Update eDFEEDER by using our approximation algorithm with frozen LLM ΨLLM. Tune LLM ΨLLM by using Eq. (3) as our loss function on fixed eDFEEDER."
  - [Section 5.2] "FEEDER can improve the LLM fine-tuning performance within our bi-level framework."
  - [corpus] Corpus papers focus on ICL demonstration selection; none address bi-level optimization for fine-tuning integration.
- Break condition: If the subset becomes too small during iterative refinement, it may lose task-critical edge cases, degrading fine-tuning performance.

## Foundational Learning

- Concept: In-Context Learning (ICL) with Demonstration Selection
  - Why needed here: FEEDER operates as a pre-selector before ICL demonstration selection, requiring understanding of how demonstrations affect LLM few-shot performance.
  - Quick check question: Can you explain how demonstration selection affects ICL performance and why pre-selection might improve efficiency?

- Concept: Bi-Level Optimization
  - Why needed here: The fine-tuning integration uses alternating optimization between data selection (outer level) and model training (inner level).
  - Quick check question: Can you describe a scenario where optimizing data selection and model parameters alternately would be beneficial?

- Concept: Core-Set Selection and Data Pruning
  - Why needed here: FEEDER identifies representative subsets analogous to core-set selection, but uses LLM-specific sufficiency/necessity instead of gradient or uncertainty-based metrics.
  - Quick check question: How does LLM-guided selection differ from traditional gradient-based core-set methods?

## Architecture Onboarding

- Component map:
  Pre-selection Module -> Demonstration Selector Interface -> Bi-Level Optimizer -> Evaluation Harness

- Critical path:
  1. Initialize DFEEDER = DTRAIN
  2. Run tree-based sufficiency checks (K=1, R=1 default)
  3. For ICL: Pass DFEEDER to demonstration selector → select n-shot examples
  4. For fine-tuning: Train LLM on DFEEDER → re-select DFEEDER with tuned LLM → repeat

- Design tradeoffs:
  - K (tree depth) vs. R (rounds): Higher K gives more robust filtering but higher cost; more R is cheaper but risks over-pruning
  - Subset size vs. performance: Paper shows ~50% reduction maintains performance, but overly aggressive pruning degrades accuracy
  - LLM size for pre-selection: Smaller LLMs can pre-select for larger LLMs with some performance tradeoff

- Failure signatures:
  - If DFEEDER size drops below 20% of DTRAIN, check if sufficiency checks are too aggressive
  - If ICL performance degrades with DFEEDER, verify demonstration selector compatibility
  - If fine-tuning diverges, ensure bi-level optimization alternates correctly between frozen/unfrozen states

- First 3 experiments:
  1. **Baseline comparison**: Run FEEDER + Similarity on SST-2 with GPT-2 (0.8B), compare accuracy and DFEEDER size vs. DTRAIN + Similarity. Target: comparable accuracy with >20% data reduction.
  2. **Hyperparameter sensitivity**: Vary K∈{1,2,3} and R∈{1,2,4} on COLA dataset with GPT-neo. Plot accuracy vs. DFEEDER size to identify Pareto frontier.
  3. **Bi-level validation**: Fine-tune GPT-neo on DFEEDER (SUBJ dataset), re-select DFEEDER with tuned model, evaluate ICL performance. Compare to fine-tuning on full DTRAIN.

## Open Questions the Paper Calls Out

- **Computational scalability for large models**: The paper notes that FEEDER becomes "exceedingly time-consuming" for models larger than 8B parameters, suggesting the need to explore efficient implementations for 70B+ parameter models.

- **Cross-model generalization**: While the paper uses smaller LLMs to pre-select demonstrations for larger ones, it raises questions about how well demonstrations selected by one model family transfer to another model family with different architectures.

- **Bias amplification**: The Impact Statement warns that the sufficiency-necessity framework may amplify existing biases present in the LLM's weights, as the selection depends on the model's internal capabilities and limitations.

## Limitations

- The sufficiency-necessity framework's effectiveness depends on the transitivity assumption, which may not hold for complex reasoning tasks where demonstrations encode specific problem-solving strategies.

- The computational efficiency claims rely on one-shot inference being sufficient for sufficiency assessment, which may not generalize to smaller LLMs (below 300M parameters) or tasks requiring more complex context integration.

- The method has only been validated on classification and basic reasoning tasks (300M-8B parameters), with limited testing on multi-step reasoning or open-ended generation tasks.

## Confidence

- **High Confidence**: The basic framework of sufficiency-necessity evaluation for demonstration selection is sound and well-supported by empirical results across multiple datasets and model sizes (300M-8B parameters).
- **Medium Confidence**: The tree-based approximation algorithm achieves the claimed computational efficiency improvements, though real-world performance may vary based on LLM inference costs and task complexity.
- **Medium Confidence**: Bi-level optimization for fine-tuning integration provides benefits, but the magnitude may depend on task characteristics and the quality of the pre-selected subset.

## Next Checks

1. **Transitivity Validation**: Systematically test whether the sufficiency relationship holds transitively across demonstration pairs in complex reasoning tasks (e.g., multi-step math problems) where demonstrations may encode specific solution strategies.

2. **Edge Case Coverage Analysis**: After applying FEEDER to a dataset, analyze the frequency of rare but critical edge cases in the pre-selected subset versus the original training data to quantify potential loss of task-critical examples.

3. **Small Model Scalability**: Evaluate FEEDER's effectiveness on LLMs below 300M parameters to determine if the sufficiency assessment quality degrades significantly and identify the minimum model size threshold for reliable pre-selection.