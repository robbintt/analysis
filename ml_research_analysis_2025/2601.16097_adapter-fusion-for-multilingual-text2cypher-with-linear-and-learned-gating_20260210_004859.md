---
ver: rpa2
title: Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating
arxiv_id: '2601.16097'
source_url: https://arxiv.org/abs/2601.16097
tags:
- language
- multilingual
- arxiv
- fusion
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores adapter fusion for multilingual Text2Cypher,
  incrementally supporting new languages without full retraining. Language-specific
  LoRA adapters for English, Spanish, and Turkish are combined via uniform linear
  merging or learned fusion MLP with dynamic gating.
---

# Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating

## Quick Facts
- arXiv ID: 2601.16097
- Source URL: https://arxiv.org/abs/2601.16097
- Reference count: 11
- Primary result: Fusion MLP recovers ~75% of joint multilingual fine-tuning gains using only 20% of training data

## Executive Summary
This work proposes adapter fusion for multilingual Text2Cypher, enabling incremental language support without full model retraining. Language-specific LoRA adapters for English, Spanish, and Turkish are combined using either uniform linear merging or a learned fusion MLP with dynamic gating. The fusion MLP achieves 0.79 average ROUGE-L across all three languages, outperforming linear merging (0.75) and recovering about 75% of the accuracy gains from joint multilingual fine-tuning while requiring only a subset of training data. The approach allows adding new languages by freezing existing adapters and retraining only the lightweight MLP, reducing training data requirements by 54% compared to joint fine-tuning.

## Method Summary
The method trains separate LoRA adapters per language (English, Spanish, Turkish) using low-rank decomposition (rank r=8, α=16) on a multilingual Text2Cypher dataset (~12K samples per language). These adapters are then combined using two approaches: (1) uniform linear merging with equal weights, and (2) a fusion MLP that learns dynamic routing weights via adapter previews. The fusion MLP takes pooled base embeddings and logits from each adapter's output on the last 200 tokens of the input (where language-identifying questions typically appear), computes softmax weights, and combines the adapter outputs. For incremental language addition, existing adapters remain frozen while only the MLP is retrained on a small subset of data (2,500 instances per language).

## Key Results
- Fusion MLP achieves 0.79 average ROUGE-L across EN/ES/TR, outperforming linear merging (0.75)
- Recovers ~75% of joint multilingual fine-tuning accuracy gains using only 20% of training data
- Adding a fourth language requires 54% less training data (22K vs 48K samples) compared to joint fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Language-Specialized Low-Rank Adaptation
Training separate LoRA adapters per language allows each adapter to specialize in both the Text2Cypher task and language-specific patterns without cross-language interference. LoRA decomposes weight updates into low-rank matrices (W0 + BA), where each adapter learns language-conditioned modifications to the frozen base model. The rank constraint forces adapters to capture only the most task-relevant features per language.

### Mechanism 2: Dynamic Routing via Adapter Previews
The fusion MLP learns to route inputs to appropriate adapters by analyzing adapter preview logits from the final 200 tokens, which contain language-identifying question text. For each input, the MLP receives pooled base embeddings and preview logits from each adapter's output on the last 200 tokens. A softmax produces weights for combining adapter outputs, with the preview capturing language signals since questions appear at prompt end.

### Mechanism 3: Frozen-Adapter Modular Fusion
Freezing trained adapters and training only the lightweight fusion MLP enables incremental language addition without catastrophic forgetting or expensive full retraining. When adding a new language, existing adapters remain frozen while only the MLP is retrained on a small subset (2,500 instances per language) to learn updated routing weights.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: The entire approach depends on training separate adapters per language
  - Quick check: Given rank r=8 and hidden dimension d=4096, what is the parameter reduction ratio vs. full fine-tuning of a single weight matrix?

- **Concept: Task Arithmetic / Adapter Merging**
  - Why needed: Linear merging serves as the baseline
  - Quick check: If adapters A1, A2, A3 are merged with weights [0.5, 0.3, 0.2], what happens to task knowledge that only exists in A3?

- **Concept: Softmax Gating / Mixture of Experts**
  - Why needed: The fusion MLP produces softmax weights over adapters
  - Quick check: If softmax outputs [0.4, 0.35, 0.25] for an English input, is the routing confident or uncertain? What threshold indicates failure?

## Architecture Onboarding

- **Component map:** Input (question + schema) → Base Model (frozen Llama-3.1-8B) → [Parallel] LoRA-EN, LoRA-ES, LoRA-TR (each produces logits) → Adapter Preview Extractor (last 200 tokens per adapter) → Fusion MLP (takes pooled embeddings + previews) → Softmax → weights [w1, w2, w3] → Weighted Logit Combination: logits_fused = Σ wi(x) · logits_i → Output Cypher query

- **Critical path:** The fusion MLP training loop—specifically how preview logits are extracted and pooled—determines routing quality. Errors in preview extraction (wrong token indices, incorrect pooling) cause cascading routing failures.

- **Design tradeoffs:**
  - Static vs. dynamic merging: Linear merge is simpler but 4 ROUGE-L points worse (0.75 vs 0.79 avg)
  - Preview window size: 200 tokens is heuristic; longer windows increase compute, shorter may miss language cues
  - Training data for MLP: 20% (7,500 samples) works, but paper doesn't ablate minimum viable subset

- **Failure signatures:**
  - Uniform routing: If MLP outputs ~[0.33, 0.33, 0.33] for all inputs, gating isn't learning. Check preview extraction and MLP convergence.
  - Language confusion: Spanish queries routed to English adapter → ROUGE-L drops for Spanish. Inspect routing weights.
  - Catastrophic forgetting on MLP retrain: Adding language 4 degrades performance on languages 1-3 → reduce MLP learning rate or increase replay data.

- **First 3 experiments:**
  1. Single-adapter baseline: Test each LoRA-EN/ES/TR on its target language only. Expected: high performance on target, low on others.
  2. Linear merge ablation: Merge with weights [1/3, 1/3, 1/3] without MLP. Compare to 0.75 avg ROUGE-L result.
  3. Routing weight visualization: Log MLP outputs on test set. Confirms language-appropriate routing. If weights don't correlate with input language, debug preview extraction.

## Open Questions the Paper Calls Out

- How does inference latency scale with the number of language adapters in fusion MLP, and what is the production deployment overhead? The paper reports no slowdown but does not provide quantitative latency measurements across different adapter counts.

- Can adaptive preview window mechanisms outperform the fixed 200-token heuristic for language identification and routing? The 200-token window was chosen as a simple heuristic without comparison to learned or variable-length approaches.

- What mechanisms could close the remaining 25% performance gap between fusion MLP and joint multilingual fine-tuning? The paper reports fusion MLP recovers "around 75% of the accuracy gains" without analyzing what knowledge is captured in joint training but missed by adapter fusion.

- Does the fusion MLP approach generalize to typologically diverse languages and different graph database domains? Only three languages (EN, ES, TR) and one benchmark dataset were evaluated.

## Limitations
- Fusion MLP architecture and training hyperparameters remain underspecified
- Adapter preview-based routing mechanism relies on heuristic 200-token windows without validation
- Performance evaluation uses ROUGE-L as primary metric, which measures string overlap rather than semantic equivalence

## Confidence

- **High Confidence:** The adapter specialization mechanism (LoRA decomposition per language) - follows well-established low-rank adaptation principles
- **Medium Confidence:** The incremental language addition benefits - quantified 54% training data reduction but operational overhead not explored
- **Low Confidence:** The adapter preview-based routing mechanism - appears novel without direct corpus validation

## Next Checks

1. **Routing Weight Analysis:** Log fusion MLP softmax outputs across the entire test set and compute correlation between highest-weight adapter and input language. Expected: >90% of samples should show correct adapter receiving dominant weight (>0.75).

2. **Adapter Capacity Ablation:** Train LoRA adapters with different ranks (r=4, r=8, r=16) and measure impact on both single-language performance and fusion MLP performance.

3. **Preview Window Sensitivity:** Vary the adapter preview extraction window (100, 200, 300 tokens) and measure routing accuracy and overall performance.