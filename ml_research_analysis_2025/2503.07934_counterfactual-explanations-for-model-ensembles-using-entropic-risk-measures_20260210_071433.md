---
ver: rpa2
title: Counterfactual Explanations for Model Ensembles Using Entropic Risk Measures
arxiv_id: '2503.07934'
source_url: https://arxiv.org/abs/2503.07934
tags:
- counterfactual
- risk
- ensemble
- counterfactuals
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating reliable counterfactual
  explanations for ensemble models, where individual models may provide different
  counterfactuals. The authors propose a novel approach using entropic risk measures
  to quantify the reliability of counterfactuals across multiple models.
---

# Counterfactual Explanations for Model Ensembles Using Entropic Risk Measures

## Quick Facts
- arXiv ID: 2503.07934
- Source URL: https://arxiv.org/abs/2503.07934
- Authors: Erfaun Noorani; Pasan Dissanayake; Faisal Hamman; Sanghamitra Dutta
- Reference count: 40
- This paper proposes using entropic risk measures to generate counterfactual explanations for model ensembles, providing a smooth trade-off between cost and validity.

## Executive Summary
This paper addresses the challenge of generating reliable counterfactual explanations for ensemble models, where individual models may provide different counterfactuals. The authors propose a novel approach using entropic risk measures to quantify the reliability of counterfactuals across multiple models. Their method allows for tuning the trade-off between cost (effort) and validity across ensemble models. They establish a connection between their risk-based approach and worst-case methods, showing that the latter is a limiting case of their approach.

## Method Summary
The method trains an ensemble of MLPs on bootstrapped subsets of data and generates counterfactuals using gradient descent to minimize entropic risk. The approach involves computing an empirical entropic risk measure that captures the reliability of counterfactuals across the ensemble, then using constrained optimization to find counterfactuals that satisfy both a cost objective and a validity constraint. The risk-aversion parameter θ provides a smooth control over the trade-off between cost and validity.

## Key Results
- Entropic risk measure provides a smooth trade-off between cost and validity across ensemble models
- As risk parameter θ increases, the method approaches worst-case validity (valid for all models)
- Experiments on HELOC, German Credit, and Adult Income datasets show effective management of cost-validity trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropic risk measure generalizes from average-case to worst-case validity over a model ensemble through a single tunable parameter.
- Mechanism: The measure computes (1/θ) * log(E[e^(θ*Loss)]) over the ensemble's predicted distribution. The risk-aversion parameter θ acts as an exponential weighting factor. As θ increases, the calculation is increasingly dominated by models with the highest loss (worst predictions). The paper proves that as θ approaches infinity, the measure converges to the maximum loss (worst-case) model.
- Core assumption: The outputs of ensemble models at a candidate point can be treated as samples from an underlying probability distribution.
- Evidence anchors:
  - [abstract] "They show that as this risk parameter increases, their method approaches the worst-case approach (valid for all models)."
  - [section] Section 3.2, Theorem 1 provides the formal proof of convergence from the entropic-risk formulation to the worst-case min-max formulation.
  - [corpus] The corpus contains no directly comparable use of entropic risk for this task; related work (e.g., Hamman et al.) uses probabilistic guarantees and stability measures.
- Break condition: The mechanism fails if there is no region where the acceptance regions of a sufficient fraction of models overlap, making a valid counterfactual impossible.

### Mechanism 2
- Claim: A constrained optimization problem using the empirical entropic risk can generate a single counterfactual that balances cost and ensemble validity.
- Mechanism: The optimization (Problem P4) minimizes the cost (distance from original instance) subject to an upper bound (τ) on the empirical entropic risk. Because the risk measure is differentiable, gradient descent can be used to iteratively update an initial candidate counterfactual until it moves into a region where the risk constraint is satisfied, implying validity across a substantial portion of the ensemble.
- Core assumption: The ensemble models are differentiable, allowing for gradient-based optimization.
- Evidence anchors:
  - [abstract] "The approach is validated through a two-step gradient descent algorithm..."
  - [section] Section 3.3 and Algorithm 1 detail the optimization procedure (P4) and the gradient descent steps: x′ ← x′ − η∇x′ρent
θ (x′).
  - [corpus] Corpus evidence is weak for this exact optimization formulation; no neighbor papers use this specific entropic risk constraint.
- Break condition: The gradient descent may fail to converge or get stuck in poor local minima if the learning rate (η) or iteration count is insufficient.

### Mechanism 3
- Claim: Increasing the risk-aversion parameter θ produces a smooth trade-off, yielding counterfactuals with higher ensemble validity but higher cost (distance from original input).
- Mechanism: A higher θ forces the optimization to prioritize satisfying the most restrictive models in the ensemble. This requires moving the counterfactual further from the original instance to find a region of overlapping acceptance, thereby increasing cost (ℓ1-distance) and validity (fraction of accepting models).
- Core assumption: The ensemble exhibits a degree of agreement, such that a shared acceptance region exists but may be further from the original instance than any single model's region.
- Evidence anchors:
  - [abstract] "...their method effectively trades off cost and validity, with the risk parameter providing a smooth control over this trade-off."
  - [section] Tables 2, 3, and 4 demonstrate empirically that for a fixed threshold τ, both cost and validity increase as θ increases.
  - [corpus] Neighbors like "CID" and "On the Definition and Detection of Cherry-Picking" discuss trade-offs in counterfactual generation but do not address this ensemble-validity mechanism.
- Break condition: The smooth trade-off can break down if the ensemble contains highly discordant models, leading to erratic jumps in cost without corresponding validity gains.

## Foundational Learning

- **Concept:** **Convex Risk Measures**
  - Why needed here: The paper's proposed entropic risk measure is a convex risk measure. Understanding this class of functions explains its desirable properties (e.g., monotonicity, translation invariance) and why it is suitable for optimization.
  - Quick check question: Does a convex risk measure penalize the combined risk of two models more or less than the average of their individual risks?

- **Concept:** **Worst-Case vs. Expected-Case Optimization**
  - Why needed here: The central contribution is framing counterfactual generation for ensembles as a problem between optimizing for the average model and optimizing for the single worst-case model. The entropic risk parameter θ bridges these two extremes.
  - Quick check question: If you set θ to a very high value, are you optimizing for the average model's acceptance or the most restrictive model's acceptance?

- **Concept:** **Pareto Optimality and Multi-Objective Trade-offs**
  - Why needed here: Generating counterfactuals involves conflicting objectives: minimizing cost (distance) and maximizing validity (robustness). The paper's method produces solutions on the Pareto front, and understanding this concept is key to interpreting the trade-off curves.
  - Quick check question: If one counterfactual has lower cost and higher validity than another, is the second counterfactual on the Pareto front?

## Architecture Onboarding

- **Component map:** Dataset → Ensemble Module (20 MLPs) → Risk Calculator (Entropic Risk) → Counterfactual Optimizer (Gradient Descent) → Valid Counterfactual
- **Critical path:** The differentiability of the ensemble models is the critical dependency. If models are non-differentiable (e.g., random forests), the gradient ∇ρ_ent cannot be computed, and the algorithm fails.
- **Design tradeoffs:**
  - **θ (Risk Aversion):** A high θ yields high-validity, high-cost counterfactuals and is computationally slower due to gradient calculation on large exponents. A low θ is faster but produces less robust counterfactuals.
  - **τ (Risk Threshold):** A low τ imposes a stricter validity constraint, pushing counterfactuals further from the original instance.
  - **Ensemble Size (N):** A larger N improves the statistical robustness of the risk estimate but increases the computational cost per gradient step.
- **Failure signatures:**
  - **Non-Convergence:** The algorithm runs for max_iter and exits with "Error (Invalid counterfactual)." This suggests τ is too strict, η is too small, or the ensemble has no shared acceptance region.
  - **High Cost/Validity Plateau:** Counterfactual cost increases dramatically with θ, but validity does not improve, indicating highly discordant ensemble models.
  - **Gradient Instability:** The algorithm produces NaN values, likely from numerical overflow in the exp(θ * loss) calculation when θ and loss are both large.
- **First 3 experiments:**
  1. **Hyperparameter Sweep for Trade-off Curve:** For a fixed dataset and ensemble, run the algorithm across a grid of θ and τ values. Plot the resulting (cost, validity) pairs to visualize the Pareto front (e.g., recreate Figure 2).
  2. **Ablation on Ensemble Size:** Fix θ and τ. Run the experiment with ensembles of varying sizes (N=5, 10, 20). Measure the change in average validity and wall-clock time per counterfactual.
  3. **Baseline Comparison:** Compare the entropic risk method (at θ=1.0, τ=0.5) against a naive baseline that simply uses the counterfactual from a single, randomly selected model from the ensemble. Report cost and validity for both.

## Open Questions the Paper Calls Out
None

## Limitations
- The entropic risk measure formulation depends critically on treating ensemble model outputs as samples from a probability distribution, an assumption not fully validated in the paper.
- The convergence proof for θ→∞ relies on specific properties of the loss function and ensemble behavior that may not hold in practice.
- The empirical risk calculation (1/θ)log(1/N Σ exp(θ*loss)) is computationally unstable for large θ values, potentially causing numerical overflow.

## Confidence

- **High Confidence**: The existence of a smooth trade-off between cost and validity controlled by θ parameter (supported by experimental Tables 2-4 showing monotonic relationships)
- **Medium Confidence**: The worst-case limit (θ→∞) formally converges to the max-loss model (supported by Theorem 1 proof)
- **Medium Confidence**: The gradient descent optimization successfully finds valid counterfactuals (supported by reported success rates but not detailed convergence analysis)

## Next Checks

1. **Numerical Stability Test**: Implement the entropic risk calculation with different θ values (0.1, 1, 10, 100) and measure computation time and numerical stability (check for NaN/overflow values)
2. **Ensemble Agreement Analysis**: For each dataset, measure the overlap fraction of acceptance regions across all ensemble models before counterfactual generation to quantify the "shared region" assumption
3. **Gradient Descent Convergence**: Run the optimization with different learning rates (η=0.01, 0.1, 0.5) and maximum iterations (500, 1000, 2000) on a subset of instances, measuring convergence rates and final cost/validity pairs