---
ver: rpa2
title: 'In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning
  via Self-Feedback'
arxiv_id: '2511.09865'
source_url: https://arxiv.org/abs/2511.09865
tags:
- reasoning
- intro
- arxiv
- training
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces InTRO (In-Token Rationality Optimization),\
  \ a novel training framework that enables accurate and concise reasoning in Large\
  \ Language Models by combining token-level exploration with self-generated feedback.\
  \ Instead of optimizing an intractable objective over all valid reasoning paths,\
  \ InTRO aligns the model\u2019s generative policy with its answer-conditioned posterior\
  \ using KL divergence minimization."
---

# In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback

## Quick Facts
- arXiv ID: 2511.09865
- Source URL: https://arxiv.org/abs/2511.09865
- Reference count: 40
- This paper introduces InTRO (In-Token Rationality Optimization), a novel training framework that enables accurate and concise reasoning in Large Language Models by combining token-level exploration with self-generated feedback.

## Executive Summary
This paper introduces InTRO (In-Token Rationality Optimization), a novel training framework that enables accurate and concise reasoning in Large Language Models by combining token-level exploration with self-generated feedback. Instead of optimizing an intractable objective over all valid reasoning paths, InTRO aligns the model's generative policy with its answer-conditioned posterior using KL divergence minimization. This is implemented by computing token-wise correction factors that reflect information discrepancy, enabling self-evaluation and fine-grained exploration within a single forward pass. Across six math reasoning benchmarks, InTRO consistently outperforms strong baselines, achieving up to 20% relative accuracy improvement over base models while producing notably more concise rationales. It also demonstrates robust generalization to out-of-domain reasoning tasks.

## Method Summary
InTRO addresses the challenge of optimizing LLMs for accurate reasoning by aligning the generative policy with an answer-conditioned posterior through KL divergence minimization. The method computes token-wise correction factors as importance weights between the forward policy and answer-conditioned policy, providing dense self-generated feedback without external supervision. During training, the model generates multiple reasoning paths, filters to those yielding correct answers, and updates parameters using weighted gradients based on these correction factors. The approach is built on OpenRLHF and trained on MATH datasets (levels 3-5, ~9.2k samples) using base models from Qwen2.5 and Qwen3 families.

## Key Results
- Achieves up to 20% relative accuracy improvement over base models on math reasoning benchmarks
- Produces notably more concise rationales while maintaining accuracy
- Demonstrates robust generalization to out-of-domain reasoning tasks including LiveCodeBench and GPQA
- Outperforms strong baselines like GRPO across multiple difficulty levels

## Why This Works (Mechanism)

### Mechanism 1: Gradient Equivalence via KL Alignment
The paper proves that minimizing KL divergence between the generative policy and the answer-conditioned posterior is gradient-equivalent to maximizing the marginal likelihood of correct answers—making an intractable objective tractable. This transforms a sum over exponentially many reasoning paths into an alignment problem between two distributions the model already computes. The core assumption is that the answer y is a deterministic function of the reasoning path z (no multiple valid answers for identical reasoning).

### Mechanism 2: Token-Level Correction Factors as Dense Self-Feedback
Computing importance weights at each token position provides fine-grained credit assignment, reinforcing tokens that align with the answer-conditioned posterior. At position t, correction factor w_t = π_θ(z_t|x⊕y, z_{<t}) / π_θ(z_t|x, z_{<t}). Tokens with w_t > 1 receive positive gradient weight; tokens with w_t < 1 are suppressed. This replaces sparse sequence-level rewards with dense per-token signals.

### Mechanism 3: Answer-Conditioned Approximation of True Posterior
Prompting the model with both question and answer (x⊕y) elicits reasoning that approximates the true posterior, enabling teacher-free training. Modern LLMs can interpret in-context instructions; conditioning on x⊕y is equivalent to asking "justify this known solution." The estimated posterior is used as the teacher signal without requiring external verifiers or human annotation.

## Foundational Learning

- **KL Divergence (Forward vs Reverse)**
  - Why needed here: The paper uses forward KL (mode-covering) to encourage broad exploration over valid reasoning paths, avoiding mode collapse.
  - Quick check: Can you explain why forward KL penalizes placing mass where q(x)=0 but p(x)>0, leading to mode-covering behavior?

- **Importance Sampling**
  - Why needed here: The token-level correction factors are importance weights that enable estimating expectations under the posterior using samples from the policy.
  - Quick check: Given samples from p(x), how would you estimate E_q[f(x)] using importance sampling?

- **Credit Assignment in RL**
  - Why needed here: InTRO's core innovation is replacing sparse sequence-level rewards with dense token-level signals, directly addressing the credit assignment problem.
  - Quick check: Why does a reward arriving only at the end of a long chain-of-thought make it harder to attribute credit to early reasoning steps?

## Architecture Onboarding

- **Component map:** Base LLM policy π_θ -> Forward pass module -> Answer-conditioned pass module -> Correction factor calculator -> Response filter -> Gradient aggregator

- **Critical path:**
  1. Generate G rationales per query via current policy
  2. Filter to rationales with correct answers
  3. For each valid rationale, at each position t:
     - Sample n token candidates (including ground truth)
     - Compute forward and answer-conditioned probabilities
     - Compute correction factor w_t
  4. Aggregate gradients: Σ w_t · ∇ log π_θ(z_t | x, z_{<t})
  5. Update θ; clip w_t ∈ [0, 200] for stability

- **Design tradeoffs:**
  - n (tokens sampled per step): Higher n → better exploration, more compute. Paper finds n=5–10 optimal; n>20 degrades.
  - G (sequences per query): More coverage but linear compute increase.
  - Correction factor clipping: Prevents unstable gradients but may limit expressiveness.
  - Assumption: Estimated posterior quality depends on base model capability.

- **Failure signatures:**
  - Exploding correction factors → check clipping threshold
  - Estimated posterior diverges from true posterior → validate KL on held-out set
  - Mode collapse (low entropy) → monitor policy entropy; may need entropy bonus
  - No improvement on easier tasks → answer-conditioning may not help where model is already confident

- **First 3 experiments:**
  1. Gradient equivalence sanity check: On a small synthetic task, compare gradients from marginal likelihood (brute-force enumeration) vs KL-divergence formulation to validate Proposition 2.1.
  2. Ablation on clipping thresholds: Sweep clipping bounds [50, 100, 200, 500] and measure stability vs accuracy tradeoff.
  3. Posterior approximation quality: Estimate true posterior via importance sampling (N=1000 paths) and measure KL divergence from estimated posterior π_θ(z|x⊕y) across different model scales and task difficulties.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the divergence between the estimated posterior $\pi_\theta(z|x \oplus y)$ and the true posterior $\pi_\theta(z|x, y)$ fundamentally limit the theoretical guarantees of gradient equivalence?
- Basis in paper: From Section 2.4 and Page 7, which notes the KL divergence between the two distributions is approximately 2.3.
- Why unresolved: While Proposition 2.1 proves equivalence under ideal conditions, the paper relies on a heuristic approximation for the "teacher" distribution without analyzing the bias introduced by this specific distribution gap.
- What evidence would resolve it: A theoretical analysis of the error bounds introduced by the approximation or empirical measurements of gradient bias compared to an oracle posterior estimator.

### Open Question 2
- Question: Can InTRO be effectively adapted for open-ended reasoning tasks where a definitive ground truth answer $y$ is unavailable for conditioning?
- Basis in paper: From Section 2.4, which requires conditioning the teacher policy on the concatenated answer $x \oplus y$.
- Why unresolved: The method explicitly relies on the existence of a correct solution $y$ to guide the token-level correction factors, restricting its application to verifiable domains like mathematics.
- What evidence would resolve it: Demonstrating a variant of InTRO that conditions on soft rewards or ranking scores (rather than exact answers) and successfully improves performance on creative writing or dialogue tasks.

### Open Question 3
- Question: What causes the performance degradation when the number of sampled tokens ($n$) exceeds a specific threshold (e.g., 40 in Table 3)?
- Basis in paper: From Table 3, which shows performance improves up to $n=20$ but then degrades at $n=40$.
- Why unresolved: The paper suggests larger $n$ enables denser exploration, but does not explain why "too much" exploration harms the final accuracy or if this is an artifact of gradient variance.
- What evidence would resolve it: An analysis of gradient variance and policy entropy dynamics as $n$ increases, specifically checking for over-fitting to noise in the correction factors.

## Limitations

- The theoretical foundation rests on the assumption that answers are deterministic functions of reasoning paths, which may not hold for open-ended reasoning tasks.
- The method shows performance degradation on simpler tasks where models are already confident, suggesting answer-conditioning isn't universally beneficial.
- The paper doesn't establish whether accuracy gains stem from improved reasoning quality or simply more verbose rationales that pass verification.

## Confidence

- **High confidence**: The mathematical formulation of the KL alignment objective and the gradient equivalence proof are internally consistent and well-specified.
- **Medium confidence**: The empirical results showing accuracy improvements and conciseness gains across multiple benchmarks appear robust, though the effect sizes vary significantly by task difficulty.
- **Low confidence**: The theoretical claims about why token-level correction factors work as self-feedback are weakly supported by the corpus evidence, which mentions self-feedback concepts but doesn't validate this specific mechanism.

## Next Checks

1. **Gradient equivalence empirical validation**: Implement a small-scale synthetic reasoning task where you can enumerate all valid reasoning paths. Compute gradients using both the marginal likelihood formulation and the KL-divergence formulation, then verify they produce identical results within numerical precision.

2. **Posterior approximation quality measurement**: For a held-out validation set, estimate the true posterior π_θ(z|x,y) using importance sampling with N=1000 reasoning paths. Measure the KL divergence to the estimated posterior π_θ(z|x⊕y) across different base model scales (1.5B, 3B, 7B) and task difficulties to quantify when the approximation breaks down.

3. **Ablation on correction factor clipping**: Systematically vary the clipping threshold [50, 100, 200, 500] while monitoring both training stability (gradient norms, correction factor distributions) and final task performance. This will reveal whether the clipping is merely stabilizing training or actively limiting the method's potential.