---
ver: rpa2
title: 'Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?'
arxiv_id: '2510.06410'
source_url: https://arxiv.org/abs/2510.06410
tags:
- reasoning
- arxiv
- llms
- recoverability
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether standard solo-reasoning LLMs can
  effectively collaborate with off-distribution reasoning trajectories, introducing
  the twin tests of Recoverability (resisting misleading steers) and Guidability (leveraging
  helpful guidance). Experiments with 15 open-weight LLMs (1.5B-32B) reveal that stronger
  solo-reasoners are often more fragile under distraction, with recoverability dropping
  to 74.9% on problems they originally solved perfectly, and all models fail to effectively
  leverage guiding steps from collaborators (under 9.2% success rate).
---

# Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?

## Quick Facts
- arXiv ID: 2510.06410
- Source URL: https://arxiv.org/abs/2510.06410
- Authors: Aochong Oliver Li; Tanya Goyal
- Reference count: 24
- Key outcome: Stronger benchmark-performing LLMs show higher fragility under reasoning trajectory disruption, with recoverability dropping to 74.9% on problems they originally solved perfectly, and all models fail to leverage guiding steps from collaborators (under 9.2% success rate).

## Executive Summary
This paper introduces a novel framework for evaluating whether LLMs can collaborate on reasoning trajectories through twin tests of Recoverability and Guidability. The key finding is that standard solo-reasoning LLMs struggle when presented with off-distribution reasoning paths - they either fail to recover from distracting steers or ignore helpful guidance from stronger models. Surprisingly, models with higher benchmark performance often exhibit greater fragility when their reasoning trajectories are perturbed. The study reveals that behavioral patterns from teacher models transfer to students during distillation, and that RL fine-tuning after SFT saturation can improve both recoverability and guidability.

## Method Summary
The paper evaluates 15 open-weight LLMs (1.5B-32B) on mathematical reasoning tasks using twin tests. Recoverability measures whether a model can correctly continue reasoning after a distracting steer (reasoning from a different question) is injected at controlled positions (0-80%) into its own correct trajectory. Guidability tests whether a model can solve previously unsolvable problems when given partial correct reasoning from a stronger model. Both metrics use Pass@1 accuracy measured over 8 samples per question. The experimental pipeline involves filtering questions by solo performance, constructing steers by sampling from different questions or stronger models, and evaluating continuation quality with Math-Verify and LLM judge verification.

## Key Results
- All models show recoverability dropping to 74.9% on problems they originally solved perfectly, with the steepest decline at 0% insertion position
- Guidability remains below 9.2% across all models, even when correct answers are present in guiding steers
- Stronger benchmark performers exhibit higher fragility under trajectory disruption
- Teacher model recoverability behaviors transfer to distilled students even when distillation uses only correct trajectories
- RL fine-tuning after SFT saturation improves both recoverability (15.3%-28.9% gains) and guidability

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Anchoring at Trajectory Onset
- **Claim**: Models depend critically on early trajectory context as an anchor for coherent reasoning continuation; disruption at 0% causes maximal degradation.
- **Mechanism**: The initial reasoning segment establishes a contextual frame that conditions subsequent token predictions. When this is replaced by off-distribution content, the model lacks its familiar starting state.
- **Core assumption**: The anchoring effect is positional and contextual rather than informational.
- **Evidence anchors**: Counterintuitive finding that 'stronger' LLMs on benchmarks are often more fragile under distraction; distraction at 0% leads to largest degradation while average recoverability exceeds 83.5% after small tweaks.
- **Break condition**: If early context is preserved but model still fails to recover, anchoring is not the primary driver.

### Mechanism 2: Behavioral Distillation Transfer Beyond Correctness
- **Claim**: Teacher models transmit recoverability patterns to students even when distillation uses only correct trajectories.
- **Mechanism**: Distillation on correct traces still exposes students to teacher's reasoning style and failure modes encoded in trajectory structure, which students internalize.
- **Core assumption**: Recoverability is a property of reasoning style, not just outcome correctness.
- **Evidence anchors**: Suboptimal recoverability behaviors of teacher models transfer to distilled students; AM-Distill models show significantly lower recoverability than QwQ- and Qwen3-Distill counterparts despite similar benchmark scores.
- **Break condition**: If students trained from different teachers on identical data show no recoverability differences, mechanism is dataset-driven.

### Mechanism 3: RL-Induced Trajectory Diversity Improves Recovery
- **Claim**: RL fine-tuning after SFT saturation improves recoverability by rewarding successful recoveries from noisy trajectories.
- **Mechanism**: Outcome-based RL exposes model to diverse trajectories including partial failures and reinforces paths leading to correct answers despite perturbations.
- **Core assumption**: Reward signals can shape reasoning dynamics beyond what demonstrations provide.
- **Evidence anchors**: RL fine-tuning improves both recoverability and guidability after SFT saturation; RL training bridges recoverability gap between different teacher distillations.
- **Break condition**: If RL without explicit recovery-reward shaping shows no improvement, mechanism requires targeted reward design.

## Foundational Learning

- **Concept: Distribution Shift in Autoregressive Models**
  - **Why needed here**: Off-trajectory reasoning inherently inputs sequences outside training distribution; understanding why models fail on OOD inputs explains recoverability fragility.
  - **Quick check question**: Can you explain why a model trained only on correct trajectories might fail when conditioned on a partial correct trace followed by an incorrect step?

- **Concept: Knowledge Distillation vs. Behavioral Cloning**
  - **Why needed here**: The paper shows teacher behaviors transfer; distinguishing what is distilled is critical for interpreting results and designing teachers.
  - **Quick check question**: If a teacher model has a known failure mode, would you expect a student distilled only from its correct outputs to inherit that failure mode? Why or why not?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here**: RLVR is central to improvement mechanism; understanding how reward signals shape reasoning trajectories is necessary to replicate and extend findings.
  - **Quick check question**: How does outcome-based RL differ from SFT in terms of diversity of trajectories the model is exposed to during training?

## Architecture Onboarding

- **Component map**: Twin test framework -> Recoverability (distraction injection at position m, trajectory from different question) and Guidability (partial guide from stronger model, no original trajectory) -> Evaluation pipeline (8 completions per question, Pass@1, Math-Verify + LLM judge) -> Training variants (SFT from teacher trajectories, RL from SFT checkpoint, data filtering strategies)

- **Critical path**: Select questions where model has 100% solo solve rate (recoverability) or ≤1/8 solve rate (guidability) -> Inject steer at controlled position and length -> Measure if model completes correctly despite perturbation -> For intervention: apply RL after SFT saturation or select teachers with high recoverability

- **Design tradeoffs**:
  - SFT vs. RL: SFT is sample-efficient but may bake in teacher weaknesses; RL improves robustness but requires more compute and careful reward design
  - Data quality vs. quantity: Aggressive filtering yields high benchmark scores but high recoverability variance; larger mixed-quality data yields more stable off-trajectory behavior
  - Teacher selection: High benchmark score ≠ high recoverability; must evaluate teachers on twin tests before distillation

- **Failure signatures**:
  - Recoverability drops sharply at 0% insertion → missing question-restatement anchor
  - Guidability remains low even when correct answer is in steer → model ignores or rejects off-distribution context
  - Checkpoint variance in recoverability with similar benchmark scores → overfitting to benchmark-style trajectories

- **First 3 experiments**:
  1. Reproduce recoverability degradation curve: For target model, inject distractions at 0%, 20%, 40%, 60%, 80% positions on 200 questions it solves perfectly. Confirm 0% is worst; test if preserving first paragraph recovers performance.
  2. Teacher transfer ablation: Distill two student models from teachers with high vs. low recoverability (e.g., QwQ-32B vs. AM-Thinking-32B) on same dataset. Compare their recoverability at matched benchmark performance.
  3. RL recovery gain test: Take checkpoint with saturated SFT performance, apply GRPO with verifiable rewards on reasoning dataset, and measure recoverability before/after RL at fixed intervals. Confirm improvement and note when gains plateau.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic changes does RL induce that improve recoverability, beyond simply exposing models to noisy trajectories?
- Basis in paper: The paper demonstrates RL improves recoverability but only offers hypothesis about outcome-based RL exposing models to noisy trajectories and explicitly rewarding successful recoveries, without mechanistic analysis of what RL actually changes in model's representations or reasoning patterns.
- Why unresolved: This is a novel contribution without investigation into whether changes are in representations, attention patterns, or uncertainty calibration.
- What evidence would resolve it: Ablation studies isolating trajectory noise exposure from reward shaping, layer-wise analysis of attention patterns during recovery, or probing experiments measuring uncertainty calibration changes post-RL.

### Open Question 2
- Question: Can off-trajectory reasoning capabilities generalize beyond mathematical reasoning to domains like code generation or scientific reasoning?
- Basis in paper: All experiments use only five math benchmarks; no other domains tested despite motivation mentioning coding and safety applications.
- Why unresolved: Framework and findings are specific to verifiable mathematical reasoning; whether recoverability and guidability behaviors transfer to code debugging, scientific hypothesis generation, or multi-modal reasoning remains unknown.
- What evidence would resolve it: Replicating twin tests on code benchmarks (HumanEval, MBPP) and comparing whether same fragility patterns emerge, particularly whether stronger benchmark performers remain more fragile under distraction.

### Open Question 3
- Question: Why do LLMs actively reject correct reasoning traces and pivot to incorrect paths even when answer derivation is already present in guiding trajectory?
- Basis in paper: Table 2 shows 18.6% of guiding steers already contain correct answers, yet models fail to leverage them; the paper notes models "can often fail to recognize such correct reasoning, reject the given answer and pivot to an incorrect path."
- Why unresolved: This counterintuitive behavior suggests distribution shift issues or over-reliance on self-generated reasoning patterns, but paper does not investigate whether this is due to attention patterns, token-level unfamiliarity, or learned dismissal of external guidance.
- What evidence would resolve it: Attention visualization showing whether models attend to provided answers; experiments varying linguistic style of guiding traces to match model's own output patterns; or intervention studies forcing attention to key answer tokens.

## Limitations

- The recoverability degradation curve may be partially dataset-specific rather than a universal property of autoregressive models
- The behavioral distillation mechanism lacks direct experimental validation beyond AM-Distill comparison
- While RL shows improvement in recoverability, exact reward structure and its impact on reasoning dynamics are not fully characterized
- The 0% position degradation might reflect context-window effects rather than pure trajectory anchoring

## Confidence

- **High confidence**: Experimental methodology is sound and reproducible; recoverability degradation at trajectory onset is consistently observed across multiple models; RL improves both recoverability and guidability metrics
- **Medium confidence**: Behavioral distillation transfer mechanism is plausible but lacks direct ablation studies; RL improvement mechanism is supported by checkpoint comparisons but could benefit from reward ablation studies
- **Low confidence**: Exact mechanism behind 0% position fragility (whether anchoring or context-window effects) is not definitively established; generalizability of findings across different reasoning domains remains uncertain

## Next Checks

1. **Anchoring vs. Context-Window Test**: Run recoverability experiment with models that preserve only first 100 tokens (or first paragraph) of original trajectory before injecting steer. If performance recovers significantly compared to 0% insertion, this confirms anchoring as primary mechanism. If not, context-window effects or other factors may dominate.

2. **Behavioral Distillation Ablation**: Create two student models distilled from same teacher on identical data, but systematically vary trajectory style by injecting synthetic steers during distillation. Compare their recoverability on held-out questions. This directly tests whether recoverability is learned behavior or emergent property of distillation process.

3. **Reward Structure Impact Study**: Train two RL models from same SFT checkpoint, one with current verifiable-reward (GRPO) setup and another with explicit "recovery bonus" reward for correct answers despite steer injection. Compare their recoverability gains and final performance. This clarifies whether standard outcome-based RL is sufficient or targeted recovery rewards are necessary.