---
ver: rpa2
title: 'Grounded Gesture Generation: Language, Motion, and Space'
arxiv_id: '2507.04522'
source_url: https://arxiv.org/abs/2507.04522
tags:
- motion
- gesture
- dataset
- generation
- referential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal dataset and framework for grounded
  gesture generation, combining synthetic pointing gestures with real conversational
  data from VR environments. The dataset, over 7.7 hours in length, is standardized
  in the HumanML3D format and includes synchronized motion, speech, and 3D scene information.
---

# Grounded Gesture Generation: Language, Motion, and Space

## Quick Facts
- arXiv ID: 2507.04522
- Source URL: https://arxiv.org/abs/2507.04522
- Reference count: 38
- Over 7.7 hours of multimodal data; synthetic pointing gestures and VR conversations; fine-tuned OmniControl achieves lowest wrist joint errors.

## Executive Summary
This paper introduces a multimodal dataset and framework for grounded gesture generation, combining synthetic pointing gestures with real conversational data from VR environments. The dataset, over 7.7 hours in length, is standardized in the HumanML3D format and includes synchronized motion, speech, and 3D scene information. The authors fine-tune OmniControl, a diffusion-based motion generation model, on this data to produce spatially controllable pointing gestures. Results show that fine-tuning consistently improves motion quality and control accuracy, with the synthetic-only model achieving the lowest errors for wrist joints. This work lays a foundation for research in situated gesture generation and grounded multimodal interaction, addressing the gap in spatially grounded, context-aware gesture synthesis.

## Method Summary
The authors fine-tune OmniControl, a diffusion-based motion generation model, on a new dataset combining synthetic pointing gestures and real VR conversational data, all formatted in HumanML3D. Spatial grounding is enforced via a composite loss (L_total = L_denoise + λ_spatial * L_spatial + λ_realism * L_realism), where L_spatial optimizes joint positions to match spatial targets. Synthetic gestures are generated via VLM-prompted utterances and aligned to motions using the Hungarian algorithm. The model is evaluated on three training splits (synthetic only, real+synthetic, all) using FID for motion naturalness and Control L2 for joint accuracy.

## Key Results
- Fine-tuning consistently improves motion quality and control accuracy over the base OmniControl model.
- The synthetic-only fine-tuned model achieves the lowest wrist joint errors (Control L2 0.058–0.060).
- All fine-tuned models show reduced FID scores (0.65–0.82), indicating improved motion naturalness.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning a diffusion-based motion model on task-specific, spatially grounded data improves joint-level control accuracy while maintaining motion naturalness.
- **Mechanism:** OmniControl uses a composite loss during denoising: `L_total = L_denoise + λ_spatial * L_spatial + λ_realism * L_realism`. The spatial loss `L_spatial = ‖x^j_t − g^j_t‖²` forces controlled joint positions toward target coordinates at each diffusion timestep, while the realism term preserves natural trajectories. Fine-tuning adapts the pretrained HumanML3D weights to the distribution of pointing gestures.
- **Core assumption:** Position-based joint constraints can capture the communicative intent of deictic gestures.
- **Evidence anchors:**
  - [abstract] "Results show that fine-tuning consistently improves motion quality and control accuracy, with the synthetic-only model achieving the lowest errors for wrist joints."
  - [Section 4.3, Table 5] ST-only fine-tuned model achieves FID 0.65–0.82 and Control L2 0.058–0.060 for wrists vs. base model's 3.82–4.60 and 0.114–0.116.
  - [corpus] Related corpus papers (3DGesPolicy, SARGes) address gesture semantics but do not evaluate diffusion fine-tuning for spatial control—limited direct comparison.
- **Break condition:** If spatial loss optimizes absolute joint positions but not the *direction* from elbow-to-wrist toward the referent, pointing accuracy degrades (acknowledged in Section 5).

### Mechanism 2
- **Claim:** Synthetic pointing gesture data, generated via VLM-prompted utterances and aligned with motion using the Hungarian algorithm, can complement scarce real grounded gesture data.
- **Mechanism:** A VLM (GroundingGPT) receives rendered scene views and generates exophoric demonstrative utterances. TTS synthesizes speech with voice cloning. WhisperX extracts word-level timestamps, and the Hungarian algorithm matches utterances to motions based on demonstrative location and duration.
- **Core assumption:** VLM-generated referential expressions approximate the distribution and timing of natural demonstrative utterances in situated dialogue.
- **Evidence anchors:**
  - [Section 3.2] Full pipeline description; 1,406 audio samples matched to 1,135 motions; WER<0.3 filtering applied.
  - [Section 3.5, Table 4] Synthetic (ST) split: 1,135 clips, mean 4.85s; used alone yields lowest wrist error.
  - [corpus] No corpus papers evaluate synthetic gesture augmentation for grounding—weak external validation.
- **Break condition:** If VLM utterances diverge from natural referential patterns or temporal alignment fails to capture gesture-speech synchrony, model learns misaligned associations.

### Mechanism 3
- **Claim:** Modular decoupling of perception (reference resolution) and generation (motion synthesis) improves scalability and interpretability for embodied agents.
- **Mechanism:** A perception module (VLM or 3D scene graph) extracts spatial cues (e.g., object centroids, bounding boxes) and projects them to world coordinates. The motion generator conditions on these cues via OmniControl's spatial guidance, producing gestures grounded to detected locations.
- **Core assumption:** Intermediate spatial representations (centroids, boxes) are sufficient for gesture grounding; end-to-end learning is not required.
- **Evidence anchors:**
  - [Section 4] "Modular systems, where perception and generation are decoupled, offer better scalability and interpretability."
  - [Figure 6] Framework diagram showing perception → spatial cue → motion generation pipeline.
  - [corpus] TeSMo, HUMANISE, and related work adopt modular, scene-aware architectures, supporting the design pattern.
- **Break condition:** If perception errors (incorrect object localization, missed references) cascade uncorrected into motion generation, grounding accuracy collapses.

## Foundational Learning

- **Concept: HumanML3D motion representation**
  - **Why needed here:** All dataset integration and OmniControl training depend on the 263-dimensional pose vector (root velocities, joint positions, rotations, contacts) sampled at 20 fps.
  - **Quick check question:** Can you explain why pelvic translation is encoded as frame-to-frame displacement while other joints are pelvis-relative?

- **Concept: Diffusion models for motion synthesis**
  - **Why needed here:** OmniControl is a diffusion model; understanding denoising timesteps, guidance, and loss composition is essential for fine-tuning and debugging.
  - **Quick check question:** What does the spatial guidance term do at each denoising timestep?

- **Concept: Deictic gesture and exophoric reference**
  - **Why needed here:** The task is referential grounding—"this/that" pointing toward objects—distinct from beat or iconic co-speech gestures.
  - **Quick check question:** How does an exophoric demonstrative differ from an endophoric one, and why does that matter for spatial cue extraction?

## Architecture Onboarding

- **Component map:**
  Scene/VLM -> Spatial Cue (object centroid/box) -> OmniControl + fine-tuned weights <- Audio/Text -> Text encoder -> Conditioned motion output (SMPL-X)

- **Critical path:**
  1. Convert raw BVH/marker data -> SMPL-X -> HumanML3D format (Section 3.1).
  2. Generate synthetic utterances and align with pointing motions (Section 3.2).
  3. Fine-tune OmniControl on ST / REF+ST / ALL splits with spatial joint constraints.
  4. Evaluate using FID (naturalness) and Control L2 (accuracy) vs. base model.

- **Design tradeoffs:**
  - ST-only: Best wrist precision but limited conversational diversity.
  - REF+ST: Better pelvis/full-body coordination; slight wrist accuracy drop.
  - ALL: Maximum diversity; modest precision tradeoff.
  - Position vs. directional loss: Current L_spatial optimizes joint position; pointing tasks may require elbow->wrist->target directional alignment (Section 5).

- **Failure signatures:**
  - Base model produces punching motion instead of pointing (sample24.mp4).
  - Fine-tuned model overfits synthetic distribution; unnatural motion on conversational references.
  - Spatial cues extracted from incorrect objects; gesture points to wrong location.
  - Long or complex referential utterances (>20s) filtered out; coverage gaps.

- **First 3 experiments:**
  1. **Baseline replication:** Run OmniControl base vs. fine-tuned on ST split; measure FID and Control L2 for pelvis, left/right wrist. Confirm reported improvements.
  2. **Directional loss ablation:** Replace position-based L_spatial with directional alignment loss (elbow->wrist unit vector vs. wrist->referent vector). Evaluate pointing accuracy on held-out test set.
  3. **Perception robustness test:** Inject noise into spatial cue inputs (centroid jitter, wrong object selection). Measure gesture grounding error sensitivity; identify cascading failure thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can spatial loss functions be redesigned to enforce directional alignment (e.g., elbow-to-wrist vector) rather than absolute joint position matching to better capture the communicative intent of pointing?
- **Basis in paper:** [explicit] The authors note that current evaluation does not reflect desired behavior because "the goal is not to match absolute joint positions but to produce accurate pointing directions." They suggest "redefining the spatial loss in OmniControl to enforce directional alignment... rather than relying solely on positional targets" (Page 8).
- **Why unresolved:** The current Control L2 loss optimizes for Euclidean distance to a target point, which can result in unnatural motions (like "punching") to satisfy the constraint, rather than semantically correct pointing gestures.
- **What evidence would resolve it:** A study comparing current positional losses against a directional vector loss, showing improved angular accuracy to the referent object and higher human ratings for gesture naturalness.

### Open Question 2
- **Question:** What is the optimal spatial semantic representation (e.g., bounding boxes, object centroids, or 3D scene graphs) required for gesture generation to balance spatial grounding with semantic nuance?
- **Basis in paper:** [explicit] Page 6 asks, "does the gesture module require the same spatial granularity as humanoid locomotion, or a more semantically nuanced representation?" Page 9 further proposes investigating "the effect of different spatial representations, such as bounding boxes, keypoints, or object centroids, on gesture quality."
- **Why unresolved:** The paper utilizes specific representations (bounding boxes/centroids) but does not ablate or compare them against denser or sparser spatial inputs to determine which best facilitates grounded generation.
- **What evidence would resolve it:** Ablation experiments training the model with different spatial input modalities and measuring the trade-off between grounding accuracy (FID/L2) and computational efficiency or semantic correctness.

### Open Question 3
- **Question:** Does pretraining on "socially relevant" motion subsets or integrating with multimodal conversational datasets (e.g., BEAT2) improve grounded gesture generation compared to pretraining on general-purpose motion data like AMASS?
- **Basis in paper:** [explicit] The Discussion states, "pretraining on general motion corpora like AMASS may be suboptimal for communicative behaviors. Subsampling AMASS to retain only socially relevant sequences, or integrating our data with multimodal datasets like BEAT2, could support more effective representation learning" (Page 8).
- **Why unresolved:** The current work fine-tunes a model pretrained on general motion (HumanML3D/AMASS), leaving the specific benefit of "communicative" pretraining data as an untested hypothesis.
- **What evidence would resolve it:** Comparative results showing that a model pretrained exclusively on social/communicative motion data outperforms a generally pretrained model on grounded gesture metrics.

## Limitations
- The synthetic pointing data may not fully capture the variety of conversational gestures in natural dialogue.
- The spatial loss optimizes absolute joint positions, not directional alignment, potentially producing unnatural "punching" motions.
- Lack of end-to-end perception-generation evaluation limits understanding of how perception errors cascade into gesture errors.
- Detailed hyperparameter settings and conversion pipeline specifics are not provided, hindering exact replication.

## Confidence
- High confidence: The fine-tuning consistently improves joint-level control accuracy (Control L2 error) and motion naturalness (FID) on held-out test sets.
- Medium confidence: Synthetic data can meaningfully supplement scarce real grounded gesture data for spatial gesture synthesis.
- Medium confidence: Modular decoupling of perception and generation improves scalability and interpretability, though robustness to perception errors is not fully quantified.

## Next Checks
1. Replicate the fine-tuning results by running OmniControl on the ST split and comparing reported vs. observed FID and Control L2 values for pelvis and wrist joints.
2. Replace the position-based spatial loss with a directional alignment loss (elbow→wrist to wrist→target vector) and measure improvements in pointing accuracy.
3. Systematically degrade the spatial cue inputs (e.g., via object localization noise or wrong object selection) and quantify how gesture grounding accuracy degrades, identifying failure thresholds.