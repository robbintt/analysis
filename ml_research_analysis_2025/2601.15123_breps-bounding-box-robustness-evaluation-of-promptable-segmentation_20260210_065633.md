---
ver: rpa2
title: 'BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation'
arxiv_id: '2601.15123'
source_url: https://arxiv.org/abs/2601.15123
tags:
- segmentation
- image
- tight
- bbox
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BREPS, a white-box adversarial attack method
  to evaluate the robustness of promptable segmentation models (e.g., SAM) to natural
  bounding box variations. The authors conducted a large-scale user study with 2,500
  annotators, collecting thousands of real bounding boxes across desktop and mobile
  devices.
---

# BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation

## Quick Facts
- arXiv ID: 2601.15123
- Source URL: https://arxiv.org/abs/2601.15123
- Authors: Andrey Moskalenko; Danil Kuznetsov; Irina Dudko; Anastasiia Iasakova; Nikita Boldyrev; Denis Shepelev; Andrei Spiridonov; Andrey Kuznetsov; Vlad Shakhuro
- Reference count: 40
- Primary result: Introduces BREPS, a white-box adversarial attack method to evaluate robustness of promptable segmentation models (e.g., SAM) to natural bounding box variations, revealing an average 30% IoU performance gap under realistic prompt variations.

## Executive Summary
This paper addresses a critical gap in evaluating promptable segmentation models like SAM: their sensitivity to natural bounding box variations. While these models are trained on synthetic "tight" boxes with small jitter, real human annotations exhibit systematic biases and higher variance across devices and users. The authors conducted a large-scale user study (2,500 annotators, 25,000 boxes) and found significant performance variability for the same model and instance. To efficiently evaluate robustness without exhaustive search, they developed BREPS - a white-box adversarial attack that optimizes bounding box coordinates to maximize or minimize segmentation error while constrained by a statistical "realism" prior derived from human data. Large-scale evaluation across 10 datasets revealed an average 30% IoU gap under realistic prompt variations, highlighting overfitting to synthetic prompts.

## Method Summary
BREPS reformulates robustness evaluation as a white-box adversarial attack in bounding box prompt space. The method computes gradients of a segmentation loss (Dice) with respect to bbox coordinates through the frozen promptable segmentation model. To prevent degenerate solutions, a realism regularizer based on a Gamma distribution fitted to human bbox CIoU-Loss is added to the loss. The attack initializes from tight bounding boxes and uses Adam optimization for 50 steps, with learning rate scaled by image resolution. This approach efficiently discovers worst-case (IoU-Min@BBox) and best-case (IoU-Max@BBox) segmentation performance under realistic prompt variations, providing a new robustness metric (IoU-∆@BBox = Max - Min).

## Key Results
- Average 30% IoU performance gap under realistic prompt variations across 10 datasets
- Significant variability in segmentation quality across users for the same model and instance
- Tight boxes overestimate real-world performance, revealing overfitting to synthetic prompts
- BREPS successfully identifies realistic adversarial bounding boxes that break segmentation models
- Method validated across general and medical imaging datasets with multiple SAM-family models

## Why This Works (Mechanism)

### Mechanism 1: Sim-to-Real Gap via Prompt Overfitting
State-of-the-art promptable segmentation models are highly sensitive to natural bounding box variations because they overfit to the synthetic "tight" bounding boxes used in standard training pipelines. During training, bounding boxes are typically generated by adding small jitter to the tight ground-truth box, creating a manifold of prompts that the model learns robustly. Real human annotations, however, exhibit systematic biases (e.g., mobile vs. desktop offsets, loose fits) and higher variance. When inference prompts fall outside the training manifold, the model's performance drops abruptly. This sensitivity reveals that models have not learned to handle the full distribution of human-like prompts.

### Mechanism 2: White-Box Gradient-Based Prompt Optimization
Robustness evaluation can be efficiently automated by reformulating it as a white-box adversarial attack on the prompt coordinates, rather than an exhaustive search. The promptable segmentation model is treated as a differentiable function f(Image, Prompt) → Mask. By freezing model weights and computing gradients of a loss function (e.g., Dice) with respect to the bounding box coordinates (x₁, y₁, x₂, y₂), an optimizer (Adam) can iteratively shift the box to minimize (attack) or maximize (idealize) the segmentation quality. This approach leverages the differentiability of promptable models to efficiently explore the prompt space.

### Mechanism 3: Realism-Constrained Adversarial Search
Naive gradient optimization produces degenerate or "unrealistic" adversarial prompts (e.g., collapsing to a point); constraining the search with a statistical prior on human box "quality" ensures discovered failures are actually plausible. The paper fits a Gamma distribution to the "Complete IoU" (CIoU) loss between user-drawn boxes and tight boxes. The negative log-likelihood of this distribution is added as a regularization term to the loss function. This penalizes the optimizer for generating boxes that deviate too far from the statistical norm of human error, preventing trivial solutions while still finding meaningful vulnerabilities.

## Foundational Learning

- **Concept: Complete IoU (CIoU)**
  - **Why needed here:** Standard IoU is zero for non-overlapping boxes, offering no gradient signal. CIoU adds penalties for center distance and aspect ratio consistency, making it a robust metric for measuring "realism" in the BREPS regularizer.
  - **Quick check question:** Why would standard IoU fail as a loss function for optimizing bounding box coordinates in an attack setting?

- **Concept: Promptable Segmentation (SAM Paradigm)**
  - **Why needed here:** The paper evaluates the fundamental robustness of the "segment anything" paradigm where prompts (boxes) define the task. Understanding that prompts are dense spatial conditioners is key to grasping why 1-pixel shifts matter.
  - **Quick check question:** How does the "tight bbox" heuristic used in current training pipelines differ from the "real-user" distribution found in this study?

- **Concept: White-Box Adversarial Attacks**
  - **Why needed here:** BREPS is fundamentally an adversarial attack in prompt space. Understanding how to compute gradients w.r.t. inputs (rather than weights) is necessary to implement the evaluation pipeline.
  - **Quick check question:** In the BREPS pipeline, what part of the system is "frozen" and what part is "optimized"?

## Architecture Onboarding

- **Component map:**
  1. Data Collection: Crowdsourced user study (2,500 users) → Real bbox distribution
  2. Statistical Analysis: Compute CIoU-Loss for user bboxes → Fit Gamma PDF (k=1.789, θ=0.121)
  3. Optimization Loop (BREPS):
     - Inputs: Image, Initial (Tight) BBox, Ground Truth Mask
     - Forward Pass: Pass (Image, Current BBox) through frozen model → Predicted Mask
     - Loss Calculation: L = DiceLoss - λ·log(GammaPDF(CIoU))
     - Backward Pass: Compute gradients ∇bbox L
     - Update: Adam optimizer updates bbox coordinates

- **Critical path:**
  The fidelity of the realism regularizer is the critical bottleneck. If the Gamma distribution does not accurately model human error (e.g., if mobile vs. desktop behaviors diverge too much and you average them), the "adversarial" boxes found may not represent realistic failure modes.

- **Design tradeoffs:**
  - Lambda (λ=0.1): Balances attack strength vs. realism. Higher λ enforces human-like boxes but might miss the true worst-case IoU; lower λ finds severe failures that users would never trigger.
  - Learning Rate Scaling: Gradients operate in pixel coordinates. The paper scales LR relative to 1024×1024 resolution to ensure consistent step sizes across model architectures.

- **Failure signatures:**
  - Degenerate Boxes: BBox collapsing to a line or point → Regularizer weight is too low or Gamma PDF is mis-specified
  - No Convergence: IoU bouncing wildly → Learning rate is too high for the specific image resolution
  - Trivial Attack: Found box is identical to the tight box → Loss function or gradient flow is broken; check if Dice loss is differentiable

- **First 3 experiments:**
  1. Sensitivity Replication: Select 5 images. For each, generate IoU heatmaps by shifting a tight box by 1 pixel in all directions to visualize the "steepness" of the model's response surface.
  2. Regularizer Validation: Implement the CIoU loss and the Gamma PDF. Generate random boxes and plot their "realism" probability vs. their IoU to verify the distribution fit.
  3. Min/Max Attack: Run the BREPS optimizer on a single instance to find the IoU-Min@BBox and IoU-Max@BBox. Visualize the resulting bounding boxes to ensure they look "human-like" (realistic) while producing vastly different masks.

## Open Questions the Paper Calls Out
- Can fine-tuning promptable segmentation models with realistic noise distributions (e.g., the fitted Gamma distribution) mitigate the observed 30% IoU performance gaps?
- Would employing device-specific regularizers (desktop vs. mobile) significantly alter the BREPS attack results compared to the unified regularizer?
- How does shifting the bounding box center (currently fixed at the object center in the exhaustive search) alter the sensitivity landscape compared to width/height variations?

## Limitations
- The user study combined desktop and mobile distributions without analyzing mode separation, potentially masking critical interaction-specific biases
- The evaluation is limited to SAM-family models; generalization to non-promptable architectures is untested
- The specific λ=0.1 value for the realism regularizer appears heuristic without systematic ablation

## Confidence
- **High confidence:** The mechanism of prompt overfitting to synthetic tight boxes (supported by user study showing 30% IoU variance). The white-box gradient formulation and differentiability claims (confirmed by implementation details). The statistical fit of Gamma distribution to CIoU-Loss.
- **Medium confidence:** The realism regularizer effectively prevents degenerate attacks (validated via KS test, but could be sensitive to λ calibration). The aggregate 30% IoU gap represents a general problem (assumes consistent behavior across datasets and models).
- **Low confidence:** The specific λ=0.1 value is optimal (appears heuristic without ablation). The claim that this represents "overfitting" rather than inherent model instability (could be architecture limitation). The assumption that tight bboxes overestimate real-world performance (correlation not causation established).

## Next Checks
1. **Distribution Separation Analysis:** Re-analyze the user study data to compute CIoU-Loss distributions separately for desktop vs. mobile annotations. Test whether BREPS performance differs significantly when trained on each subset's Gamma parameters.
2. **Model Retraining Experiment:** Fine-tune a SAM model using the real-user bbox distribution (instead of tight+jitter) and re-run BREPS attacks. Quantify the reduction in the 30% IoU gap to directly test the overfitting hypothesis.
3. **Cross-Architecture Generalization:** Apply BREPS to a non-promptable segmentation model (e.g., Mask R-CNN with manual box input) to determine if the sensitivity to prompt variation is a universal segmentation problem or specific to the SAM paradigm.