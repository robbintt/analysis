---
ver: rpa2
title: Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised
  Adaptation with Regularization
arxiv_id: '2511.14238'
source_url: https://arxiv.org/abs/2511.14238
tags:
- depth
- adaptation
- absrel
- performance
- self-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting monocular depth
  estimation foundation models to unseen and corrupted data domains. The authors propose
  WeSTAR, a parameter-efficient framework that combines self-training with semantic-aware
  hierarchical normalization and weakly-supervised adaptation using pairwise ordinal
  depth annotations.
---

# Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization

## Quick Facts
- arXiv ID: 2511.14238
- Source URL: https://arxiv.org/abs/2511.14238
- Authors: Yan Huang; Yongyi Su; Xin Lin; Le Zhang; Xun Xu
- Reference count: 5
- Primary result: WeSTAR improves generalization of depth estimation models to unseen and corrupted domains using self-training with semantic-aware hierarchical normalization and weakly-supervised ordinal annotations

## Executive Summary
This paper addresses the challenge of adapting monocular depth estimation foundation models to unseen and corrupted data domains. The authors propose WeSTAR, a parameter-efficient framework that combines self-training with semantic-aware hierarchical normalization and weakly-supervised adaptation using pairwise ordinal depth annotations. The method employs low-rank adapters (LoRA) with weight regularization to prevent overfitting and catastrophic forgetting during adaptation. Experiments on diverse benchmarks including NYU, KITTI, Sintel, DIODE, and corrupted variants show that WeSTAR consistently improves generalization and achieves state-of-the-art performance, particularly under challenging conditions.

## Method Summary
WeSTAR adapts a pre-trained Depth Anything V2 model to target domains using a teacher-student self-training framework with EMA updates. The method injects LoRA adapters (rank=8) into the attention layers for parameter-efficient adaptation. Three loss components are combined: self-training loss with semantic-aware hierarchical normalization (SA-HDN) using SAM2 instance masks, weakly-supervised margin ranking loss on pairwise ordinal annotations, and weight regularization to prevent overfitting. The framework operates in a source-free setting with unlabeled target images and optional sparse ordinal annotations.

## Key Results
- On Sintel-C (corrupted synthetic dataset), WeSTAR achieves δ1 of 71.8 and AbsRel of 24.1
- Significantly outperforms baselines: iBOT* (δ1: 62.7, AbsRel: 29.0) and SGRL (δ1: 66.5, AbsRel: 29.9)
- Consistent improvements across diverse datasets including NYU, KITTI, Sintel, DIODE, and corrupted variants
- Demonstrates robust adaptation across both realistic and corrupted datasets

## Why This Works (Mechanism)

### Mechanism 1
Semantic-aware hierarchical normalization provides more stable multi-scale supervision than content-agnostic grid-based approaches during self-training. SA-HDN defines normalization contexts using semantic instance masks from SAM2, creating per-instance statistics (median, MAD) alongside global statistics. This prevents fragmentation of coherent objects that occurs with fixed grids, enabling geometric consistency at semantically meaningful scales. Core assumption: Instance segmentation boundaries align with depth discontinuities; SAM2 generalizes sufficiently to unseen domains without retraining.

### Mechanism 2
Pairwise ordinal annotations provide sparse but strong constraints that correct topological errors and reduce confirmation bias from pseudo-labels. A small set of relative depth labels (k pairs per image) introduces supervision independent of model predictions. The margin ranking loss enforces transitive ordinal relations, providing globally coherent depth structure that dense self-training alone cannot achieve due to accumulated pseudo-label errors. Core assumption: Ordinal annotations capture meaningful geometric relationships; annotation cost remains low enough for practical deployment.

### Mechanism 3
LoRA with weight regularization anchors adaptation near pre-trained weights, preventing catastrophic forgetting while enabling domain-specific adjustments. Low-rank matrices (U, V with rank r=8) constrain updates to a subspace, reducing overfitting risk. The regularization term (L_reg) penalizes deviation from initial weights, requiring stronger evidence from target data to justify updates. This filters noisy pseudo-labels and sparse ordinal constraints. Core assumption: Pre-trained knowledge is valuable and should be preserved; target domain shift can be captured in low-rank updates.

## Foundational Learning

- **Concept: Mean Teacher / Self-Training Framework**
  - Why needed here: Core mechanism for unsupervised adaptation; teacher generates pseudo-labels via EMA of student weights, providing stable targets
  - Quick check question: Can you explain why EMA provides more stable targets than using the student's current predictions directly?

- **Concept: Scale-Shift Ambiguity in Monocular Depth**
  - Why needed here: Depth from single images is inherently ambiguous up to scale and shift; normalization is required to make pseudo-labels and predictions comparable
  - Quick check question: Why does median/MAD normalization provide more robust statistics than mean/std for depth?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables parameter-efficient fine-tuning of large vision transformers without full weight updates
  - Quick check question: What is the relationship between LoRA rank (r) and the expressivity vs. overfitting tradeoff?

## Architecture Onboarding

- **Component map:**
  Input Image -> SAM2 -> Instance Masks
  Input Image -> ViT Encoder + LoRA -> DPT Head -> Depth Map
  Teacher (EMA) -> Pseudo-labels

- **Critical path:**
  1. Generate weak/strong augmentations (T_w, T_s)
  2. Teacher produces pseudo-labels on weak augmentation
  3. SAM2 generates instance masks for SA-HDN contexts
  4. Normalize pseudo-labels and predictions using per-instance and global statistics
  5. Compute L_st (self-training), L_weak (ordinal), L_reg (regularization)
  6. Update only LoRA parameters via weighted sum

- **Design tradeoffs:**
  - Higher LoRA rank: More expressivity but higher overfitting risk
  - More annotation pairs: Better supervision but higher labeling cost
  - Stronger regularization: More stability but potentially underfitting target domain

- **Failure signatures:**
  - Training loss decreasing but validation degrading: Confirmation bias; reduce ST weight or increase regularization
  - Depth predictions become scale-inconsistent: Check SA-HDN contexts; segmentation may be failing
  - Early training instability: EMA decay factor may be too low; increase α (default 0.996)

- **First 3 experiments:**
  1. Baseline self-training without SA-HDN or regularization on NYU-C to isolate each component's contribution
  2. Ablation on LoRA rank (r=4, 8, 16) on Sintel to find optimal expressivity-regularization balance
  3. Vary annotation pairs per image (k=1, 3, 5, 10) on NuScenes to measure weak supervision efficiency curve

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit limitations remain regarding the sensitivity to segmentation quality, minimum annotation requirements, and automatic loss weighting.

## Limitations
- SA-HDN relies on SAM2's generalization to unseen domains without retraining, but no validation of SAM2's segmentation quality on corrupted or out-of-distribution datasets is provided
- Weak supervision mechanism assumes pairwise ordinal annotations can be reliably sampled, but the protocol for generating these annotations from ground truth is not detailed
- LoRA regularization strength is fixed across datasets without justification for the chosen λ_reg=1.0

## Confidence
- **High confidence:** Self-training framework with EMA teacher works as described; performance improvements on standard benchmarks are reproducible
- **Medium confidence:** SA-HDN provides consistent benefits across datasets; the normalization strategy's advantages depend on SAM2's domain generalization which is not verified
- **Low confidence:** Weak supervision with pairwise annotations provides substantial gains beyond self-training alone; the reported improvements may depend heavily on annotation quality and sampling strategy

## Next Checks
1. Conduct ablation study on SAM2's segmentation quality across corrupted datasets (NYU-C, Sintel-C) to verify SA-HDN's assumptions hold under severe corruption
2. Test LoRA rank sensitivity (r=4, 8, 16) on a held-out validation set to confirm the chosen rank=8 provides optimal generalization-regularization tradeoff
3. Implement systematic evaluation of annotation sampling strategies (random vs. structured vs. uncertainty-based) to quantify weak supervision efficiency and confirm the claimed benefits are robust to sampling choices