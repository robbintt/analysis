---
ver: rpa2
title: Cross-functional transferability in universal machine learning interatomic
  potentials
arxiv_id: '2504.05565'
source_url: https://arxiv.org/abs/2504.05565
tags:
- energy
- materials
- learning
- energies
- atomref
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of transferring universal machine
  learning interatomic potentials (uMLIPs) from lower-fidelity (GGA/GGA+U) to higher-fidelity
  (r2SCAN) density functional theory datasets. The main issue is the significant energy
  scale shifts and poor correlations between functionals, which hinder effective transfer
  learning.
---

# Cross-functional transferability in universal machine learning interatomic potentials

## Quick Facts
- arXiv ID: 2504.05565
- Source URL: https://arxiv.org/abs/2504.05565
- Reference count: 0
- Key outcome: Transfer learning from GGA/GGA+U to r2SCAN DFT improves energy MAE from 27 to 17 meV/atom when using proper energy referencing

## Executive Summary
This work addresses the challenge of transferring universal machine learning interatomic potentials (uMLIPs) from lower-fidelity (GGA/GGA+U) to higher-fidelity (r2SCAN) density functional theory datasets. The main issue is the significant energy scale shifts and poor correlations between functionals, which hinder effective transfer learning. By analyzing the MP-r2SCAN dataset of 238,247 structures, the authors demonstrate that proper energy referencing—specifically refitting atomic reference energies—is critical for successful transfer. They show that without energy referencing, transfer learning yields similar or worse performance compared to training from scratch. With proper referencing, energy MAE improves from 27 to 17 meV/atom, and force MAE from 45 to 38 meV/Å. The scaling law analysis reveals that transfer learning remains data-efficient even with sub-million structure datasets, achieving performance gains equivalent to 10-fold more high-fidelity data.

## Method Summary
The paper presents a transfer learning approach for uMLIPs that addresses the challenge of energy scale shifts between different DFT functionals. The method involves fitting atomic reference energies (AtomRef) to the target functional using linear regression on composition-weighted elemental energies, then replacing the source AtomRef with the fitted target AtomRef in a pre-trained model. The GNN weights are then fine-tuned on the high-fidelity dataset with the AtomRef layer frozen. This approach is compared against alternatives including trainable AtomRef and training from scratch. The method is validated on the MP-r2SCAN dataset, demonstrating significant improvements in energy and force prediction accuracy when proper energy referencing is employed.

## Key Results
- Transfer learning with proper energy referencing improves energy MAE from 27 to 17 meV/atom and force MAE from 45 to 38 meV/Å
- Transfer learning with 1K high-fidelity points outperforms training from scratch with >10K points (10-fold data efficiency)
- Superior performance persists even with the full MP-r2SCAN dataset of 0.24 million structures
- Cross-functional transfer learning enables high-fidelity energy and force predictions without expensive high-fidelity calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Refitting atomic reference energies (AtomRef) enables effective transfer learning across DFT functionals with large energy scale shifts.
- Mechanism: The total energy prediction decomposes into E_total = c_elem · E_elem + E_GNN. By refitting E_elem (AtomRef) to the target functional before fine-tuning the GNN, the model shifts its energy scale to match the new labels, improving dataset correlation from ρ=0.0917 to ρ=0.9250. This allows the GNN to learn functional-specific physics rather than compensating for reference energy misalignment.
- Core assumption: The remaining energy difference after AtomRef adjustment represents learnable physical differences between functionals rather than noise.
- Evidence anchors:
  - [abstract] "We show that significant energy scale shifts and poor correlations between GGA and r²SCAN pose challenges to cross-functional data transferability in uMLIPs."
  - [Page 5] "After replacing the AtomRef, a stronger correlation between GGA/GGA+U and r²SCAN total energies can be achieved. Pearson's correlation coefficient ρ improves from 0.0917... to 0.9250."
  - [corpus] Weak direct support; corpus papers address uMLIP evaluation and architectures but not cross-functional transfer mechanisms.
- Break condition: If the functional differences involve fundamental changes in bonding physics (not just energy referencing), correlation after AtomRef adjustment may remain weak, limiting transfer benefits.

### Mechanism 2
- Claim: Large uncorrected energy shifts cause gradient instability during fine-tuning, degrading model performance through negative transfer.
- Mechanism: Without AtomRef refitting, initial prediction errors reach eV/atom scale, producing large loss values and gradients >10x larger than normal. These excessive gradients push model weights to suboptimal regions early in training, where they can become trapped. Method 3 (frozen AtomRef) and Method 2 (trainable AtomRef) both underperformed training from scratch on forces and stresses.
- Core assumption: Gradient magnitude correlates with training instability and final model quality.
- Evidence anchors:
  - [Page 6-7] "Method 3 without refitting AtomRef exhibits gradient magnitudes at least one order larger than those of Method 4 with refitting."
  - [Page 7] "Methods 2 and 3 exhibit similar MAEs since they both begin with GGA/GGA+U AtomRef, and the large energy shifts... cause poor correlation and excessive weight adjustments during early fine-tuning."
  - [corpus] No direct corpus evidence on gradient-based transfer failure modes.
- Break condition: If using very low learning rates or gradient clipping, the instability may be partially mitigated, though the fundamental correlation problem remains.

### Mechanism 3
- Claim: Transfer learning with proper energy referencing provides >10x data efficiency that persists even at sub-million dataset scales.
- Mechanism: Pre-training on large low-fidelity data (MPtrj: 1.58M structures) provides chemical knowledge that transfers when energy scales are properly aligned. The scaling law analysis shows transfer learning with 1K high-fidelity points outperforms training from scratch with >10K points. The crossover point where transfer benefits saturate occurs at ~317K-720K training points.
- Core assumption: Low-fidelity pre-training captures transferable representations of atomic environments that generalize across functionals.
- Evidence anchors:
  - [Page 8] "TL with merely 1K high-fidelity data points can outperform training from scratch on a high-fidelity dataset with more than 10K data points, marking more than 10-fold data efficiency."
  - [Page 8] "Superior performance of Transfer over Scratch does not saturate even given the full-sized MP-r²SCAN dataset of 0.24 million structures."
  - [corpus] Neighbor paper "Data-efficient construction of high-fidelity graph deep learning interatomic potentials" supports data efficiency claims but in different context.
- Break condition: If source and target datasets have fundamentally different chemical coverage (e.g., organic molecules vs. oxides), pre-training benefits may not transfer regardless of energy referencing.

## Foundational Learning

- Concept: **Atomic Reference Energies (AtomRef)**
  - Why needed here: Understanding that DFT total energies are "gauge dependent" with arbitrary vacuum references is essential. Different functionals produce different absolute energies for the same structure (0-70 eV/atom shifts), but physical quantities like formation energies involve energy differences where references cancel.
  - Quick check question: Can you explain why two DFT functionals might give formation energies within 50 meV/atom of each other but total energies differing by 50 eV/atom?

- Concept: **Transfer Learning vs. Multi-fidelity Learning**
  - Why needed here: The paper distinguishes transfer learning (pre-train then fine-tune) from multi-fidelity approaches (using low-fidelity as input features or learning residuals). Understanding these distinctions clarifies why AtomRef alignment is specific to the transfer learning approach.
  - Quick check question: In transfer learning, what happens if source and target labels have correlation ρ < 0.5?

- Concept: **Convex Hull and Decomposition Energy**
  - Why needed here: Decomposition energy prediction is the paper's most stringent benchmark, requiring accurate energy differences between a compound and its competing phases. Small systematic errors can flip stability predictions.
  - Quick check question: Why is decomposition energy prediction more challenging than single-structure energy prediction for MLIPs?

## Architecture Onboarding

- Component map:
  Input: Structure → Composition vector c_elem (1×94) → AtomRef: c_elem · E_elem ← Linear regression fit on target functional → + → GNN (CHGNet architecture) ← Pre-trained weights, then fine-tuned → Output: E_total, forces, stresses, magmoms

- Critical path:
  1. Fit AtomRef on target functional: `E_elem = (A^T A)^{-1} A^T E_total` using all target structures
  2. Replace source AtomRef with target AtomRef in pre-trained model (freeze this layer)
  3. Fine-tune GNN weights with target data using standard loss (energy:force:stress:magmom = 3:1:0.1:1)
  4. Validate on held-out structures before deploying

- Design tradeoffs:
  - **Frozen vs. trainable AtomRef during fine-tuning**: Paper shows frozen target AtomRef (Method 4) outperforms trainable (Method 2). Trainable AtomRef can drift to compensate for GNN errors, harming generalization.
  - **Transfer learning vs. training from scratch**: Transfer wins at all tested scales (<240K structures) but requires compatible pre-trained model. From scratch simpler but needs more data.
  - **Energy vs. formation energy training**: Formation energy automatically handles referencing but requires elemental reference calculations. Total energy with AtomRef is equivalent and more flexible.

- Failure signatures:
  - **Negative transfer**: Fine-tuned model worse than random initialization on forces/stresses → AtomRef likely misaligned or functional correlation too weak
  - **Ionic relaxation failures**: Atoms displaced >6Å from neighbors → PES destabilized by large gradients from energy shift
  - **Decomposition energy MAE > formation energy MAE**: Model not capturing error cancellation needed for phase stability

- First 3 experiments:
  1. **Correlation check**: Before any fine-tuning, compute correlation between source functional predictions (with source AtomRef subtracted) and target functional labels (with target AtomRef subtracted). If ρ < 0.8, transfer learning benefits will be limited.
  2. **Gradient magnitude monitoring**: During first epoch, log gradient norms for early convolution layers. If >10x typical values (compare to training from scratch), AtomRef misalignment is likely the cause.
  3. **Ablation on AtomRef strategy**: Compare Methods 2, 3, 4 on a small validation subset (1K structures) before committing to full fine-tuning. The paper's ranking (4 > 1 > 3 ≈ 2) should replicate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the atom reference energy (AtomRef) refitting strategy generalize effectively to uMLIP architectures that lack an explicit, separable linear composition model?
- Basis in paper: [inferred] The study validates the method solely on CHGNet, which uses an explicit composition model ($E_{total} = c_{elem} \cdot E_{elem} + E_{GNNs}$). The authors state the conclusion "should hold more generally," but this is not demonstrated for end-to-end models.
- Why unresolved: Other architectures (e.g., MACE, NequIP) may encode elemental reference energies implicitly within deep graph layers, making the proposed linear refitting pre-processing step inapplicable or insufficient for resolving energy scale shifts.
- What evidence would resolve it: A comparative ablation study applying the AtomRef refitting method to message-passing neural networks (MPNNs) that predict energy directly without a final linear composition layer.

### Open Question 2
- Question: What benchmark frameworks are required to evaluate uMLIPs on high-fidelity quantum mechanical methods beyond meta-GGAs, such as coupled cluster (CCSD)?
- Basis in paper: [explicit] The authors explicitly "advocate for more comprehensive benchmarking frameworks that go beyond GGA/GGA+U" and highlight the "need to establish benchmark tests tailored to these computationally demanding quantum mechanical methods... such as... coupled cluster methods."
- Why unresolved: Current benchmarks like Matbench Discovery are limited to GGA levels. The scarcity of high-fidelity CCSD data for solids makes it difficult to assess whether transfer learning gains persist at higher levels of theory.
- What evidence would resolve it: The release of a standardized benchmark dataset for solids containing CCSD or multi-reference energies, along with a comparison of uMLIP transfer learning performance on this data.

### Open Question 3
- Question: How can diverse uMLIP datasets with conflicting "compatibility adjustments" (mixing schemes) be integrated without introducing the data noise that hinders model accuracy?
- Basis in paper: [explicit] The authors note that "coarse-grained, non-universal adjustments" in current datasets cause potential energy jumps and noise. They suggest that "a well-founded strategy to integrate diverse datasets... will provide a promising avenue" but leave the specific integration method undefined.
- Why unresolved: The paper demonstrates that transfer learning works well between distinct functionals (GGA to r2SCAN), but does not solve the challenge of simultaneously training on multiple low-fidelity datasets that utilize incompatible mixing schemes.
- What evidence would resolve it: A methodology that successfully trains a single uMLIP on a combined corpus (e.g., Materials Project, OQMD, AFLOW) while minimizing the energy jumps currently caused by their respective correction schemes.

## Limitations

- The AtomRef refitting mechanism is validated only for CHGNet architecture and may not generalize to other uMLIP architectures
- The study focuses on a single functional pair (GGA/GGA+U to r2SCAN) and may not represent all cross-functional transfer scenarios
- The dataset size (0.24M structures) may limit applicability to domains requiring larger training sets

## Confidence

- **High confidence**: The energy scale shifts (0-70 eV/atom) between functionals are well-established; the AtomRef refitting mechanism for aligning energy scales is technically sound and empirically validated.
- **Medium confidence**: The scaling law analysis showing >10x data efficiency is convincing but based on a single functional pair and specific dataset size range. Generalization to other contexts requires further validation.
- **Medium confidence**: The claim that transfer learning benefits persist even at sub-million scales is supported by the data but may not hold for all functional pairs or chemical systems.

## Next Checks

1. **Cross-functional generalization test**: Apply the same transfer learning protocol to GGA → SCAN or SCAN → r2SCAN pairs to verify the AtomRef refitting mechanism works across different functional transitions.
2. **Chemical system expansion**: Test transfer learning performance on organic molecular systems or alloy datasets to evaluate chemical transferability beyond the inorganic compounds studied.
3. **Gradient stability analysis**: Systematically measure gradient magnitudes during early fine-tuning epochs across different AtomRef strategies to confirm the paper's hypothesis about gradient-induced weight instability causing negative transfer.