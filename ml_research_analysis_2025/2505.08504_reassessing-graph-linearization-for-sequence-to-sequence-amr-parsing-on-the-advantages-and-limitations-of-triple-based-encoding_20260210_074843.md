---
ver: rpa2
title: 'Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the
  Advantages and Limitations of Triple-Based Encoding'
arxiv_id: '2505.08504'
source_url: https://arxiv.org/abs/2505.08504
tags:
- penman
- triple
- graph
- encoding
- roles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates a triple-based linearization method for training
  sequence-to-sequence AMR parsers, comparing it with the traditional Penman encoding.
  While triples keep parent-child nodes adjacent and eliminate inverse roles, the
  results show Penman encoding performs better overall, achieving SMATCH scores of
  77.0 (The Little Prince) and 80.9 (AMR 3.0) versus the best triple model at 76.5
  and 80.0.
---

# Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding

## Quick Facts
- **arXiv ID:** 2505.08504
- **Source URL:** https://arxiv.org/abs/2505.08504
- **Reference count:** 6
- **Primary result:** Penman encoding outperforms triple-based encoding in AMR parsing, with SMATCH scores of 77.0 vs 76.5 on The Little Prince

## Executive Summary
This paper evaluates a triple-based linearization method for training sequence-to-sequence AMR parsers, comparing it with the traditional Penman encoding. While triples keep parent-child nodes adjacent and eliminate inverse roles, the results show Penman encoding performs better overall, achieving SMATCH scores of 77.0 (The Little Prince) and 80.9 (AMR 3.0) versus the best triple model at 76.5 and 80.0. Removing variables consistently hurt performance, suggesting important information loss. The study found no advantage of triple linearization for deeper or longer graphs, contrary to expectations. Penman's concise and explicit representation of nested graph structures proved more effective. Multi-task learning combining both formats did not outperform single-task Penman training. The marginal performance gap between top models indicates potential for direct graph prediction without tree-based representations.

## Method Summary
The study compares two linearization approaches for AMR parsing: Penman encoding and triple-based encoding. The triple-based method represents AMR graphs as sets of subject-predicate-object triples, keeping parent-child nodes adjacent and removing inverse roles. The researchers evaluate these approaches using a T5-based sequence-to-sequence model on two AMR datasets (The Little Prince and AMR 3.0). They conduct controlled experiments including variable removal, depth/length analysis, and multi-task learning to assess the relative strengths and weaknesses of each encoding method.

## Key Results
- Penman encoding achieved higher SMATCH scores than triple-based encoding (77.0 vs 76.5 on The Little Prince, 80.9 vs 80.0 on AMR 3.0)
- Removing variables consistently degraded performance across all models and datasets
- Triple linearization provided no advantage for parsing deeper or longer graphs, contrary to theoretical expectations
- Multi-task learning combining both formats did not outperform single-task Penman training

## Why This Works (Mechanism)
The mechanism behind Penman's superior performance appears to stem from its more explicit and concise representation of nested graph structures. The study suggests that the triple-based encoding, despite keeping parent-child nodes adjacent, may introduce ambiguity or require more complex decoding patterns. The consistent performance drop when removing variables indicates that variable information, while theoretically redundant, provides important structural cues for the parser.

## Foundational Learning
- **AMR graph representation:** Abstract Meaning Representation graphs encode semantic relationships; understanding different linearization formats is crucial for sequence-to-sequence parsing
- **Penman notation:** Linearized AMR format using parentheses and variables; needed for its widespread adoption and explicit nesting representation
- **Triple-based linearization:** Converting graphs to subject-predicate-object triples; quick check: can you convert a simple AMR to both formats?
- **SMATCH metric:** Evaluation metric for AMR parsing that measures graph similarity; why needed: provides standardized comparison across approaches
- **Variable handling in AMR:** How variables are used to represent concepts and maintain coreference; quick check: what happens when you remove variables from an AMR?
- **Sequence-to-sequence parsing:** Using neural models to convert text to graph representations; understanding this helps grasp the study's approach

## Architecture Onboarding

**Component Map:** Text Input -> T5 Encoder -> Sequence-to-Sequence Decoder -> AMR Graph Output

**Critical Path:** The critical path involves converting linearized AMR representations (either Penman or triples) into target sequences that the T5 model learns to predict. The decoder must reconstruct the graph structure from the linear sequence.

**Design Tradeoffs:** The main tradeoff is between explicit structural representation (Penman) versus adjacency preservation (triples). Penman provides more explicit nesting but longer sequences; triples are more compact but may lose structural clarity.

**Failure Signatures:** Performance degradation when removing variables suggests the model relies on variable information for structural understanding. The lack of advantage for deeper graphs indicates the triple linearization may not effectively capture hierarchical relationships.

**First Experiments:**
1. Train T5 on both Penman and triple formats separately to establish baseline performance
2. Remove variables from both formats to test their importance
3. Evaluate model performance across different AMR depths and lengths to test depth/length advantages

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap between Penman and triple encoding is relatively small (1-2 SMATCH points), suggesting limited practical significance
- Experiments only evaluate one specific transformer-based parser architecture, limiting generalizability
- Variable removal experiments are confounded because variables serve multiple functions in AMR

## Confidence
- Penman encoding performs better than triple-based encoding: Medium confidence (small performance margins, single architecture tested)
- Removing variables consistently hurts performance: High confidence (consistent pattern across experiments)
- Triple linearization provides no advantage for deeper/longer graphs: Low confidence (contradicts theory, limited exploration of linearization variants)

## Next Checks
1. Test multiple sequence-to-sequence architectures (RNN, T5, BART variants) to establish whether Penman advantage generalizes beyond T5
2. Systematically vary triple linearization strategies (different node ordering, variable handling) to identify if specific configurations improve performance
3. Evaluate on out-of-domain AMR datasets to test whether observed advantages persist across different AMR domains and genres