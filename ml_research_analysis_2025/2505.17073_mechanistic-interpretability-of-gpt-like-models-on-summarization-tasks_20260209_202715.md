---
ver: rpa2
title: Mechanistic Interpretability of GPT-like Models on Summarization Tasks
arxiv_id: '2505.17073'
source_url: https://arxiv.org/abs/2505.17073
tags:
- attention
- summarization
- lora
- activation
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mechanistic interpretability framework for
  analyzing how GPT-like models adapt to summarization tasks. The research quantifies
  internal transformations during fine-tuning by examining attention patterns and
  activation distributions, revealing how model components adapt to perform information
  selection and compression.
---

# Mechanistic Interpretability of GPT-like Models on Summarization Tasks

## Quick Facts
- arXiv ID: 2505.17073
- Source URL: https://arxiv.org/abs/2505.17073
- Authors: Anurag Mishra
- Reference count: 5
- Key outcome: Targeted LoRA adaptation of identified "summarization circuits" in middle layers achieved ROUGE-1 0.182 vs 0.148 for standard LoRA

## Executive Summary
This paper presents a mechanistic interpretability framework for analyzing how GPT-like models adapt to summarization tasks. The research quantifies internal transformations during fine-tuning by examining attention patterns and activation distributions, revealing how model components adapt to perform information selection and compression. The analysis identified a "summarization circuit" primarily located in middle layers (2, 3, and 5), with 62% of attention heads showing decreased entropy post-fine-tuning, indicating focused information selection. Targeted LoRA adaptation of these identified circuits achieved significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs.

## Method Summary
The study fine-tuned GPT-2 (124M) on CNN/DailyMail summarization dataset using AdamW (weight decay 0.01, LR 3e-5, batch 32, 10 epochs). Attention weights and activations were extracted via forward hooks for pre-trained vs. fine-tuned comparison. KL divergence and entropy metrics were computed per head to identify high-change layers (2, 3, 5). Targeted LoRA (rank=8) was applied to query/value projections in identified layers only, with LR 5e-5, batch 16, converging in ~6 epochs. ROUGE-1/2/L scores were used for evaluation.

## Key Results
- 62% of attention heads showed decreased entropy post-fine-tuning, indicating focused information selection
- Middle layers (2, 3, 5) exhibited highest KL divergence (layer 2: 0.024) and activation changes (~12% of neurons)
- Targeted LoRA on identified circuits achieved ROUGE-1 0.182 vs 0.148 for standard LoRA with 75% fewer trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning for summarization causes attention heads to shift from diffuse to focused attention patterns, enabling selective information extraction.
- **Mechanism:** The entropy of attention distributions decreases as models learn to identify salient content. Lower entropy = more concentrated attention on relevant tokens.
- **Core assumption:** Attention entropy reduction correlates with improved information selection for summarization (causal direction not proven through ablation).
- **Evidence anchors:**
  - [abstract] "62% of attention heads showing decreased entropy, indicating a shift toward focused information selection"
  - [section 3.3] "Head 11 in Layer 4 shows strongest focus (-0.47), with 62% of heads showing decreased entropy overall"
  - [corpus] Weak direct support; related work on attention interpretability (Vig 2019, Clark et al. 2019) cited but no corpus papers specifically validate entropy-to-summarization causality.

### Mechanism 2
- **Claim:** Summarization capability localizes to a specific "circuit" in middle layers (2, 3, 5), which undergo the most dramatic transformations during fine-tuning.
- **Mechanism:** KL divergence analysis between pre-trained and fine-tuned models shows layers 2, 3, and 5 exhibit highest attention distribution changes. Neuron-level analysis reveals ~12% of neurons in these layers show substantial activation changes.
- **Core assumption:** High KL divergence and activation change magnitude indicate causal importance for the task (correlation ≠ causation without intervention studies).
- **Evidence anchors:**
  - [abstract] "middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes"
  - [section 3.4] "neuron 304 demonstrated a dramatic reversal in activation polarity... approximately 12% of neurons exhibited substantial activation changes"
  - [corpus] Related work on transformer circuits (Elhage et al. 2021, 2022) supports modular circuit hypothesis but does not validate summarization-specific circuits.

### Mechanism 3
- **Claim:** Targeted parameter-efficient fine-tuning on identified circuits achieves better performance with fewer resources than uniform adaptation.
- **Mechanism:** By restricting LoRA adaptation to query and value projections in layers 2, 3, and 5 only, the model maintains core language capabilities while adapting summarization-specific computations.
- **Core assumption:** Circuit identification via interpretability analysis generalizes; the layers identified as important for summarization on CNN/DailyMail transfer to held-out test data.
- **Evidence anchors:**
  - [section 3.4] "Convergence in 6 epochs compared to 10 for standard LoRA, representing a 40% reduction... 75% reduction in trainable parameters"
  - [table 2] LoRA-targeted achieves ROUGE-1 0.182 vs standard LoRA 0.148
  - [corpus] No corpus papers validate targeted LoRA based on interpretability analysis; this appears novel.

## Foundational Learning

- **Concept: KL Divergence for Distribution Comparison**
  - **Why needed here:** The paper uses KL divergence to quantify how much attention distributions shift between pre-trained and fine-tuned models.
  - **Quick check question:** If KL(P||Q) = 0.85 for attention head (6,8), what does this tell you about the relationship between pre-trained and fine-tuned attention patterns for that head?

- **Concept: Attention Entropy**
  - **Why needed here:** Entropy measures how concentrated vs. diffuse attention is. The paper's central finding—that 62% of heads show decreased entropy—requires understanding what entropy means in the context of attention weights.
  - **Quick check question:** An attention head with entropy H(A) = -0.47 (negative entropy difference) post-fine-tuning: is it more focused or more diffuse than before?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper's intervention strategy relies on LoRA. Understanding that LoRA adds trainable low-rank matrices to existing weights is necessary to interpret why targeted LoRA reduces parameters while maintaining performance.
  - **Quick check question:** If LoRA has rank=8 applied to a 768-dimensional query projection, how many trainable parameters does this add per layer?

## Architecture Onboarding

- **Component map:** Input → Tokenizer (GPT-2, 1024 token limit) → Embedding → Layers 1-12 (each with multi-head attention + FFN) → Output projection
- **Critical path:** Extract attention weights via forward hooks during inference (pre-trained and fine-tuned) → Compute KL divergence and entropy metrics per head → Identify high-change layers (2, 3, 5) → Apply LoRA (rank=8) to Q/V projections in identified layers only → Train with LR=5e-5, batch=16, early stopping patience=6
- **Design tradeoffs:**
  - Full fine-tuning vs. LoRA vs. Targeted LoRA: Full fine-tuning achieves best ROUGE but requires most compute. Targeted LoRA achieves near-full performance with 75% fewer parameters.
  - Layer coverage: Targeting only layers 2, 3, 5 is efficient but may miss cross-layer interactions.
  - Analysis depth: Attention-level analysis performed; deep circuit-level causal tracing limited by compute.
- **Failure signatures:**
  - If targeted LoRA underperforms standard LoRA: circuit identification incorrect, or task requires more distributed processing
  - If entropy doesn't decrease: model may not be learning focused selection; check learning rate and data quality
  - If validation loss diverges from training loss: overfitting to circuit layers; consider expanding target layers
- **First 3 experiments:**
  1. Reproduce entropy analysis: Fine-tune GPT-2 on CNN/DailyMail, extract attention entropy per head, verify 62% show decrease.
  2. Ablate circuit layers: Compare targeted LoRA on (2,3,5) vs. random layer selection (e.g., 1,7,10).
  3. Domain transfer test: Apply same analysis to a different summarization dataset (e.g., scientific papers).

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the identified summarization circuits (layers 2, 3, and 5) transfer to larger model architectures like LLaMA-2/3, or do circuit locations and characteristics differ fundamentally at scale?
  - Basis: Future work section states intent to "extend this analysis to larger models including LLaMA architectures" and "port the pipeline to LLaMA-2/3."
  - Why unresolved: All findings derive from GPT-2 (124M parameters); the authors acknowledge that "circuits in larger models may exhibit different characteristics or more complex interactions."

- **Open Question 2:** How do rotary position embeddings (RoPE) reshape the identified summarization circuits, and can targeted LoRA still effectively capture circuit geometry under RoPE-induced attention patterns?
  - Basis: Authors explicitly state they will "investigate how rotary position embeddings (RoPE) reshape these circuits and whether low-rank adapters still capture the RoPE-induced geometry."
  - Why unresolved: GPT-2 uses absolute positional embeddings; RoPE fundamentally alters attention computation, potentially reorganizing where and how summarization-specialized circuits form.

- **Open Question 3:** Do the identified summarization mechanisms (62% entropy decrease, neuron 304 polarity reversal) generalize to non-news domains such as scientific or legal document summarization?
  - Basis: Limitations section notes the mechanisms "may not generalize to other domains (e.g., scientific papers, legal documents) where summarization might involve different semantic operations."
  - Why unresolved: CNN/DailyMail contains news articles with consistent structure; different domains may require distinct information selection and compression strategies not captured by the identified circuits.

## Limitations

- The mechanistic claims about the "summarization circuit" are primarily correlational rather than causal, lacking direct validation through ablation studies
- Findings are based on a single architecture (GPT-2) and dataset (CNN/DailyMail), raising questions about generalizability to other domains or larger models
- Neuron-level observations (e.g., neuron 304 polarity reversal) are speculative without systematic analysis of whether such changes are unique to summarization or generic fine-tuning artifacts

## Confidence

- **High confidence**: The entropy reduction in attention heads (62% showing decreased entropy) and the targeted LoRA performance improvements (ROUGE-1: 0.182 vs 0.148) are empirically validated with clear metrics and reproducible methodology
- **Medium confidence**: The localization of summarization capability to middle layers (2, 3, 5) based on KL divergence analysis is well-supported by the data but lacks causal validation through intervention studies
- **Low confidence**: Claims about neuron-level mechanisms (e.g., neuron 304 polarity reversal) are speculative without systematic analysis of whether such changes are unique to summarization or generic fine-tuning artifacts

## Next Checks

1. **Causal circuit validation**: Perform targeted ablation of identified attention heads (6,8) and (10,5) in layers 2, 3, 5 to test whether removing them degrades summarization performance specifically, versus random head ablation

2. **Cross-domain transferability**: Apply the same interpretability framework to a different summarization domain (e.g., scientific paper summarization) to determine whether the circuit localization remains consistent or shifts based on domain characteristics

3. **Scale sensitivity analysis**: Test whether the identified middle-layer circuit pattern persists in larger GPT variants (GPT-2 Medium/Large) to evaluate if circuit localization is architecture-dependent or scales with model size