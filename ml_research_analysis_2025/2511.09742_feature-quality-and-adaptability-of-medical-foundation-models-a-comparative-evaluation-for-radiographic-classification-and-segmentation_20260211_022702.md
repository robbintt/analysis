---
ver: rpa2
title: 'Feature Quality and Adaptability of Medical Foundation Models: A Comparative
  Evaluation for Radiographic Classification and Segmentation'
arxiv_id: '2511.09742'
source_url: https://arxiv.org/abs/2511.09742
tags:
- performance
- segmentation
- linear
- classification
- pneumothorax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates vision encoders from eight medical and general-domain
  foundation models (FMs) for chest X-ray classification and segmentation tasks. We
  assess feature quality through linear probing and adaptability through fine-tuning
  on pneumothorax and cardiomegaly classification, and pneumothorax and cardiac segmentation.
---

# Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation

## Quick Facts
- arXiv ID: 2511.09742
- Source URL: https://arxiv.org/abs/2511.09742
- Reference count: 0
- Domain-specific pretraining provides superior initial feature quality compared to general-domain models for chest X-ray classification tasks.

## Executive Summary
This study systematically evaluates vision encoders from eight foundation models (FMs) for chest X-ray classification and segmentation tasks. We assess feature quality through linear probing and adaptability through fine-tuning on pneumothorax and cardiomegaly classification, and pneumothorax and cardiac segmentation. Domain-specific pretraining provides a significant advantage, with medical FMs consistently outperforming general models in linear probing, demonstrating superior initial feature quality. However, feature utility is highly task-dependent: pre-trained embeddings excel at global classification and segmenting salient anatomy (e.g., heart), but perform poorly on complex, subtle pathologies (e.g., pneumothorax) without significant fine-tuning. Importantly, expensive text-image alignment is not a prerequisite for strong performance; image-only (RAD-DINO) and label-supervised (Ark+) FMs are among the top performers. Notably, a supervised, end-to-end baseline (SegFormer) remains highly competitive, matching or exceeding the best FMs on segmentation tasks.

## Method Summary
The study evaluates eight foundation models (6 medical: Ark+, BiomedCLIP, CheXagent, MedImageInsights, MedSigLIP, RAD-DINO; 2 general: DINOv2, SigLIP2) on chest X-ray classification and segmentation tasks using SIIM-ACR pneumothorax dataset (10,675 CXRs) and a private cardiomegaly dataset (5,000 CXRs). The evaluation protocol includes linear probing (frozen encoder, train linear head only) and fine-tuning (differential learning rates: encoder 1e-7, head 1e-4) with 5-fold patient-level cross-validation. Classification uses global embeddings (CLS token) while segmentation uses patch embeddings with 1×1 conv and bilinear upsampling. Hierarchical models use 16×16 effective patch size. Statistical comparisons use Friedman test with Nemenyi post-hoc analysis.

## Key Results
- Medical FMs achieved 0.877–0.964 AUROC vs. 0.870–0.871 for general models on pneumothorax classification, demonstrating superior initial feature quality through linear probing.
- Cardiac segmentation: 0.890–0.942 Dice with linear probing; Pneumothorax segmentation: 0.219–0.433 Dice, showing pretrained features struggle with subtle pathologies.
- Subgroup analysis revealed models rely on confounding shortcuts (e.g., chest tubes), achieving 0.933–0.967 sensitivity with tubes but ~0.50 without, which fail for precise segmentation.

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific pretraining on medical images provides superior initial feature quality compared to general-domain models, as measured by linear probing performance. Medical FMs encode task-appropriate visual features during pretraining because the training distribution matches the target domain (chest radiographs), reducing the representational gap between pretrained features and downstream tasks. If target pathology is not represented in pretraining data, or if pretraining creates representation collapse from weak text supervision, domain pretraining advantage may not hold.

### Mechanism 2
Pretrained features work well for global classification and salient anatomy segmentation but fail for subtle, complex pathology localization without significant fine-tuning. Global embeddings capture holistic image semantics sufficient for binary decisions, but lack fine-grained spatial discrimination needed to delineate pathologies with indistinct boundaries (e.g., pneumothorax pleural lines). If pathology has clear, high-contrast boundaries, pretrained features may transfer better to segmentation without fine-tuning.

### Mechanism 3
Models learn confounding shortcuts (e.g., chest tubes) for classification that do not transfer to segmentation, revealing feature representations are correlational rather than pathologically grounded. Classification models exploit highly visible proxy features statistically associated with disease labels; segmentation requires precise boundary delineation that correlational shortcuts cannot provide. If training data is balanced to remove correlation between shortcut features and labels, models may learn more robust features.

## Foundational Learning

- **Concept: Linear Probing vs. Fine-Tuning**
  - **Why needed here:** The paper uses linear probing (frozen encoder, train only prediction head) to measure inherent feature quality, and fine-tuning (update encoder with small learning rate) to measure adaptability. This distinction determines whether you can use off-the-shelf embeddings or need full model adaptation.
  - **Quick check question:** If your downstream task has limited labels, should you start with linear probing or fine-tuning? (Answer: Start with linear probing—it reveals whether pretrained features are already task-appropriate.)

- **Concept: Global vs. Patch Embeddings**
  - **Why needed here:** Classification uses global embeddings (CLS token or pooled patch embeddings) capturing holistic image semantics; segmentation requires patch embeddings preserving spatial information for pixel-level predictions.
  - **Quick check question:** Which embedding type would you use for a lesion detection task requiring bounding boxes? (Answer: Patch embeddings—they retain spatial localization needed for dense prediction tasks.)

- **Concept: Effective Patch Size in Hierarchical Models**
  - **Why needed here:** Hierarchical models (Ark+, MedImageInsights) generate multi-scale features. The paper found 16×16 effective patch size consistently outperformed 32×32 for segmentation, demonstrating finer spatial resolution matters.
  - **Quick check question:** You're adapting MedImageInsights for pneumothorax segmentation. Which feature layer should you extract? (Answer: Layer corresponding to 16×16 effective patch size for higher spatial fidelity.)

## Architecture Onboarding

- **Component map:** Vision Encoder -> Extracts features (global + patch embeddings); freeze for probing, update for fine-tuning -> Prediction Head -> Linear layer (classification) or 1×1 conv + upsampling (segmentation); always trainable

- **Critical path:**
  1. Select FM based on pretraining domain (medical > general) and architecture (hierarchical for segmentation)
  2. Extract appropriate embeddings (global for classification, patch for segmentation)
  3. Run linear probing first to assess feature quality
  4. If performance insufficient, fine-tune with differential learning rates
  5. For segmentation, ensure effective patch size ≤16×16

- **Design tradeoffs:**
  - Text-guided vs. Image-only pretraining: Text-image alignment is NOT required—RAD-DINO (image-only) and Ark+ (label-supervised) match or exceed VLMs
  - Medical vs. General domain: Medical FMs have higher initial quality; general models (DINOv2, SigLIP2) require more fine-tuning
  - Pretrained FM vs. Supervised baseline (SegFormer): SegFormer is highly competitive for segmentation; FMs provide more value for classification

- **Failure signatures:**
  - Linear probing performance near random (AUROC ~0.5) suggests domain mismatch or feature collapse
  - Large gap between classification and segmentation performance indicates shortcut learning
  - Performance collapse after fine-tuning (e.g., MedImageInsights on cardiomegaly: 0.945→0.937) suggests overfitting—reduce encoder learning rate

- **First 3 experiments:**
  1. **Linear probe your task:** Extract global embeddings from 2–3 candidate FMs (include at least one medical domain model like RAD-DINO or Ark+), train linear classifier. Result tells you if features are already task-appropriate.
  2. **Stratified subgroup analysis:** Evaluate on challenging cases (e.g., small pneumothorax) with/without known confounders (e.g., chest tubes). Result reveals shortcut reliance.
  3. **Patch size ablation (for segmentation):** If using hierarchical models, compare segmentation performance at 16×16 vs. 32×32 effective patch sizes. Result confirms optimal spatial resolution for your task.

## Open Questions the Paper Calls Out
- How do medical foundation model encoder performance rankings generalize across imaging modalities beyond chest radiography (e.g., CT, MRI, ultrasound) and across pathologies with different visual characteristics?
- Can pre-training objectives be designed to explicitly encode fine-grained spatial features necessary for localizing subtle pathologies, rather than relying on post-hoc fine-tuning?
- What mitigation strategies can effectively decouple shortcut learning (e.g., chest tube detection) from true pathology recognition without degrading classification performance?

## Limitations
- Private cardiomegaly dataset prevents full replication and limits generalizability to non-public institutional data.
- Poor documentation of model-specific preprocessing (bit depth, normalization) creates ambiguity in feature extraction pipeline.
- Medical FMs significantly outperform general models in linear probing, but this advantage diminishes or disappears when target pathology is not represented in pretraining data.

## Confidence

- **High Confidence:** Domain-specific pretraining provides superior feature quality (linear probing results consistent across classification tasks).
- **Medium Confidence:** Pretrained features excel at global classification but fail on complex segmentation without fine-tuning (supported by segmentation performance gap).
- **Medium Confidence:** Models learn confounding shortcuts (chest tube presence) that don't transfer to segmentation (subgroup analysis shows dramatic performance drop when confounding removed).

## Next Checks
1. Test linear probing performance on rare or underrepresented pathologies to validate whether medical domain advantage persists when pretraining data coverage is limited.
2. Conduct ablation study removing chest tubes from pneumothorax classification training data to verify shortcut learning mechanism.
3. Compare feature quality of text-image aligned vs. image-only models on segmentation tasks with varying boundary clarity (clear vs. ambiguous pathology boundaries).