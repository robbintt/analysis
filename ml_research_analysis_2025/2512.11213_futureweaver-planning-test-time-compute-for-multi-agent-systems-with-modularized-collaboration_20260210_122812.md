---
ver: rpa2
title: 'FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized
  Collaboration'
arxiv_id: '2512.11213'
source_url: https://arxiv.org/abs/2512.11213
tags:
- budget
- arxiv
- collaboration
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FUTUREWEAVER, a framework for optimizing
  test-time compute allocation in multi-agent systems under fixed budget constraints.
  The key innovation is the introduction of modularized collaboration, which encapsulates
  reusable multi-agent workflows as callable functions, and a dual-level planning
  architecture that balances short-horizon action selection with long-horizon speculation.
---

# FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration

## Quick Facts
- arXiv ID: 2512.11213
- Source URL: https://arxiv.org/abs/2512.11213
- Authors: Dongwon Jung; Peng Shi; Yi Zhang
- Reference count: 40
- Primary result: Introduces modularized collaboration and dual-level planning for budget-constrained multi-agent orchestration, achieving higher task success rates than baselines on GAIA and BrowseComp-Plus benchmarks.

## Executive Summary
FutureWeaver addresses the challenge of test-time compute allocation in multi-agent systems under fixed budget constraints. The framework introduces modularized collaboration, encapsulating reusable multi-agent workflows as callable functions, and a dual-level planning architecture that balances short-horizon action selection with long-horizon speculation. Through self-play reflection on validation tasks, the system automatically derives collaboration modules and cost estimates, enabling principled orchestration that outperforms standard orchestrator-worker approaches.

## Method Summary
FutureWeaver operates through a two-stage process: offline self-play reflection and online dual-level planning. During self-play, the system executes multiple trajectories on validation tasks, logging agent invocations, outputs, and costs to estimate d_cost(a) for each agent/module. Successful trajectories are analyzed to extract recurring interaction patterns, which are abstracted into collaboration modules. At inference, the orchestrator samples candidate actions conditioned on task state and budget, computing short-term gain via self-consistency scoring and long-term feasibility via speculative trajectories. The combined utility f(α) = g(α) + h(α) selects actions that are both locally promising and globally viable, maximizing task success within the fixed budget.

## Key Results
- Consistently outperforms baseline orchestrator-worker approaches across diverse budget settings on GAIA and BrowseComp-Plus benchmarks
- Achieves higher task success rates through principled compute allocation rather than sequential one-agent-at-a-time execution
- Demonstrates more effective budget utilization by balancing immediate action quality against long-term feasibility through dual-level planning

## Why This Works (Mechanism)

### Mechanism 1
Encapsulating multi-agent workflows as callable collaboration modules enables principled compute allocation rather than sequential one-agent-at-a-time execution. Each module m = (S, κ) specifies a subset of agents S and a coordination strategy κ, expanding the orchestrator's action space from individual agents to include collaboration patterns as first-class primitives. This transforms multi-agent coordination into a function-calling problem where the orchestrator selects which module to invoke based on task state and remaining budget.

### Mechanism 2
Dual-level planning balances immediate action quality against budget feasibility through A*-style f-score computation. Short-term gain g(α) uses self-consistency scoring across K candidate actions sampled from the policy, while long-term gain h(α) generates speculative trajectories—abstract module-level rollouts without execution—to compute the fraction of budget-feasible continuations. Combined utility f(α) = g(α) + h(α) selects actions that are locally promising and globally viable.

### Mechanism 3
Self-play reflection unifies cost estimation and module discovery without manual engineering. By executing multiple trajectories on validation tasks, the system logs agents invoked, outputs, and costs to estimate average invocation costs d_cost(a). Successful trajectories are analyzed by an LLM to identify recurring interaction sequences, which are abstracted into new collaboration modules added to the action space.

## Foundational Learning

- **A* Search with f = g + h**: Dual-level planning directly maps g to short-term gain and h to long-term feasibility; understanding this is essential for interpreting action selection. *Quick check: Can you explain why combining immediate reward with estimated future cost improves over greedy selection?*

- **Self-Consistency Decoding (Wang et al., 2022)**: Short-term planning uses self-consistency to score candidate actions; understanding majority-voting over samples clarifies g(α) computation. *Quick check: How does counting action frequency across K samples serve as a proxy for action quality?*

- **Speculative Decoding (Leviathan et al., 2023)**: Long-term planning is explicitly inspired by speculative decoding—generating lightweight rollouts to estimate future costs without execution. *Quick check: What is the tradeoff between speculation accuracy and speculation cost?*

## Architecture Onboarding

- **Component map**: Self-Play Reflection Module -> Collaboration Modules (M) + Cost Estimates (d_cost) -> Orchestrator Agent (π) -> Short-Term Planner (g(α)) -> Long-Term Planner (h(α)) -> Action Selection (f(α) = g(α) + h(α)) -> Worker Agents (A)

- **Critical path**: 1) Self-play reflection (offline) → produces collaboration modules M and cost estimates d_cost 2) At inference: Sample K candidates → compute g(α) → generate speculative trajectories → compute h(α) → select argmax f(α) → execute and update state/budget → repeat

- **Design tradeoffs**: More candidates K improves short-term signal quality but increases latency; more speculative trajectories per candidate improves h(α) reliability but adds overhead; more self-play rounds yields richer module library but increases setup cost; coarser module granularity reduces planning complexity but may miss task-specific optimizations

- **Failure signatures**: Accuracy plateaus despite increasing budget: cost estimates may be inaccurate; recalibrate d_cost via additional self-play. Budget consistently underutilized: long-term planner over-conserves; consider relaxing feasibility threshold. Modules never invoked: self-play may have induced task-specific patterns that don't generalize; re-run reflection on broader validation set. Single module dominates: action space collapse; verify candidate sampling diversity

- **First 3 experiments**: 1) Ablate long-term planning: Set h(α) = 0 and compare accuracy/budget utilization on GAIA. Expect degraded performance at higher budgets. 2) Vary candidate count K: Test K ∈ {3, 5, 10} and measure accuracy vs. latency tradeoff. 3) Transfer modules across benchmarks: Train modules on GAIA validation, test on BrowseComp-Plus without retraining self-play. Measures module generality assumption.

## Open Questions the Paper Calls Out

### Open Question 1
Can collaboration modules learned from self-play reflection generalize to significantly different task distributions beyond the validation set they were derived from? The paper uses only the first 30 questions in the benchmark as a validation set for self-play reflection, raising concerns about module transferability to out-of-distribution tasks or entirely new domains. Cross-benchmark evaluation where modules learned on one benchmark are applied zero-shot to another would resolve this.

### Open Question 2
How sensitive is dual-level planning performance to errors in cost estimation for speculative trajectories? The paper acknowledges assuming costs are "transferable across tasks" by averaging over subtasks, relaxing the dependency between cost and specific subtask content. If cost estimates systematically underestimate or overestimate true execution costs, long-term planning feasibility scores could misguide action selection, but this sensitivity is not analyzed. Ablation studies injecting controlled noise into cost estimates would resolve this.

### Open Question 3
Can collaboration modules be extended to support dynamic online refinement during deployment rather than requiring offline self-play? Self-play reflection is described as an "iterative process" conducted before deployment, with no mechanism mentioned for adapting modules at test time. Real-world deployments may encounter novel interaction patterns not seen in validation trajectories, limiting pre-learned module effectiveness. Implementing an online module discovery variant that updates modules during inference would resolve this.

## Limitations
- The paper's claims rest heavily on the transferability of cost estimates and collaboration modules from validation tasks to test tasks, an assumption not directly validated
- Exact prompt formulations, candidate sampling parameters (K), and speculative trajectory generation details are unspecified, making exact reproduction challenging
- The paper does not report ablations on the self-consistency component or module discovery quality, leaving gaps in understanding the contribution of each mechanism

## Confidence

- **High confidence**: The core mechanism of dual-level planning (combining short-term self-consistency with long-term feasibility scoring) is well-defined and the experimental results on GAIA and BrowseComp-Plus are directly reported
- **Medium confidence**: The self-play reflection procedure for deriving modules and costs is described but not validated for cross-task transfer; the benefit of modularized collaboration over baseline orchestrator-worker is shown empirically but not deeply explained
- **Low confidence**: The paper does not provide ablation on the number of self-play rounds, candidate counts, or speculative trajectory counts, making it unclear how sensitive performance is to these hyperparameters

## Next Checks

1. **Ablate long-term planning**: Set h(α) = 0 and compare accuracy/budget utilization on GAIA. Expect degraded performance at higher budgets per Table 2 results.

2. **Vary candidate count K**: Test K ∈ {3, 5, 10} and measure accuracy vs. latency tradeoff. Paper uses unspecified K; this establishes sensitivity.

3. **Transfer modules across benchmarks**: Train modules on GAIA validation, test on BrowseComp-Plus without retraining self-play. Measures module generality assumption.