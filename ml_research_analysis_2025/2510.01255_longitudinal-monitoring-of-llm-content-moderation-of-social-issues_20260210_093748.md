---
ver: rpa2
title: Longitudinal Monitoring of LLM Content Moderation of Social Issues
arxiv_id: '2510.01255'
source_url: https://arxiv.org/abs/2510.01255
tags:
- content
- moderation
- refusal
- social
- openai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Longitudinal Monitoring of LLM Content Moderation of Social Issues

## Quick Facts
- **arXiv ID**: 2510.01255
- **Source URL**: https://arxiv.org/abs/2510.01255
- **Reference count**: 40
- **Primary result**: Longitudinal audit framework detects unannounced LLM content moderation policy changes.

## Executive Summary
This paper introduces AI Watchman, a longitudinal auditing framework that monitors large language models' content moderation behaviors over time. The system uses a "repeat after me" prompt with Wikipedia text on social issues to elicit moderation responses from OpenAI's GPT-4.1, GPT-5, DeepSeek, and the Moderation Endpoint. By tracking refusal rates across 421 topics over time, the authors demonstrate that policy changes can be detected even when not publicly announced, providing evidence that real-world events correlate with shifts in moderation behavior.

## Method Summary
The researchers built a monitoring system that queries LLMs with Wikipedia text on social issues using the prompt "repeat after me:" followed by the article content. They track refusal rates over time using phrase matching and error code detection to classify responses. The framework monitors multiple models (OpenAI GPT-4.1, GPT-5, DeepSeek in English and Chinese) on a bi-weekly or weekly basis, creating longitudinal datasets that reveal changes in content moderation policies. The system processes 421 topics across 52 categories, using 3,121 Wikipedia pages as test content.

## Key Results
- AI Watchman successfully detected a spike in refusals for Israel-related content from March to May 2024, correlating with real-world events.
- GPT-5 showed a marked increase in refusals for abortion-related content during May 2024.
- The system identified cross-lingual moderation differences, with DeepSeek showing varying refusal rates between English and Chinese queries on the same topics.

## Why This Works (Mechanism)

### Mechanism 1: Repeat Prompt Elicitation
The "repeat after me" prompt with encyclopedic content triggers content moderation filters, making refusal behaviors observable and measurable. The model processes the content of the text to be repeated, and if it touches upon topics that the model's underlying safety system flags, it will generate a refusal instead of complying.

### Mechanism 2: Longitudinal Change Detection via Consistent Probes
Repeatedly querying LLMs with a stable dataset over time allows for the detection of shifts in refusal rates that may correspond to unannounced changes in company policy or external pressures. By maintaining constant input sets and observation frequency, statistically significant changes in output can be attributed to changes in the system's behavior.

### Mechanism 3: Refusal Detection via Phrase & Code Matching
A combination of matching known refusal phrases in natural language responses and trapping structured API error codes provides an automated method for binary classification (refusal vs. compliance) at scale. This approach bypasses the need for complex semantic classifiers.

## Foundational Learning

- **Concept: Content Moderation & Refusal in LLMs**
  - Why needed here: This is the core phenomenon the paper studies. Understanding that moderation includes "refusal" (declining to engage) is essential.
  - Quick check question: How does the paper define "refusal" in the context of an LLM? Is it only a direct "I cannot answer," or can it include other behaviors?

- **Concept: Algorithm Auditing (Black-box & Longitudinal)**
  - Why needed here: AI Watchman is an auditing tool. Grasping the difference between a one-time audit and longitudinal monitoring is key to understanding the work's value.
  - Quick check question: What is the primary advantage of a longitudinal audit over a one-time snapshot, as argued in the paper?

- **Concept: The Role of Prompt Engineering in Elicitation**
  - Why needed here: The choice of the "repeat after me" prompt is a critical design decision. Understanding why this specific prompt is used to elicit moderation behaviors is important.
  - Quick check question: Why is Wikipedia text used as the payload for the repeat prompt, rather than generating text about social issues?

## Architecture Onboarding

- **Component map**: Social Issues Dataset (421 topics -> 52 categories -> 3121 Wikipedia pages) -> Prompt Generator -> Monitoring Pipeline (cron job) -> LLM/ME Query Clients -> Refusal Classifier -> Data Aggregator -> Visualization Frontend

- **Critical path**: 
  1. Define dataset (topics, Wikipedia pages)
  2. For each monitoring run: Generate prompts, query APIs, collect responses, classify as "refusal" or "compliance", aggregate results by topic, category, date, and model
  3. Update public website with new data points

- **Design tradeoffs**:
  - Wikipedia for Neutrality vs. Length: Using Wikipedia ensures content neutrality but leads to length-based refusals, requiring a retry mechanism with truncated text
  - Phrase-based Detection vs. Semantic Classification: Fast and interpretable but may miss novel or subtle refusals
  - Public Website vs. Raw Data API: Maximizes transparency but may limit programmatic access

- **Failure signatures**:
  - Inconsistent Refusals (Stochasticity): Same prompt refused only part of the time
  - Non-Explicit Refusal (Deceptive Compliance): Model claims to comply but summarizes or redacts
  - Catastrophic Policy Drift: Sudden, massive change in refusal rates across many categories

- **First 3 experiments**:
  1. Reproduce Core Result: Pick one topic (e.g., "Abortion"), use the same Wikipedia text, and query GPT-4.1 and GPT-5 via API
  2. Test Non-Explicit Refusal: Manually inspect responses marked as "compliant" to check for summaries or redactions
  3. Pilot a New Language: Add Spanish to the pipeline for a subset of topics and compare refusal rates to English/Chinese results

## Open Questions the Paper Calls Out

### Open Question 1
How can auditing methodologies be adapted to reliably detect and classify "non-explicit refusals" (e.g., content redaction or subtle substitution) that mimic compliance? Current detection relies largely on keyword matching for explicit refusal phrases; subtle content modification is difficult to distinguish from valid generation without deep semantic analysis.

### Open Question 2
To what extent does content moderation behavior vary across languages other than English and Chinese, particularly regarding culturally sensitive social issues? The conclusion explicitly calls for future work to "expand multilingual analyses," noting that the current study was limited to two languages yet still observed significant differences based on cultural context.

### Open Question 3
How can auditors definitively distinguish between transient model nondeterminism and intentional, unannounced policy updates when refusal rates fluctuate? The authors acknowledge that the high monetary cost prevents running queries multiple times to establish error bounds, making it difficult to determine if spikes in refusals are statistical noise or definitive policy changes.

## Limitations

- The detection method relies on static refusal phrase lists, which may miss nuanced or evolving moderation behaviors such as "safe completions" or content redaction.
- Cross-lingual moderation differences are observed but not deeply explained.
- The causal link between real-world events and observed refusal rate changes remains inferential rather than definitively proven.

## Confidence

- **High confidence**: The longitudinal monitoring framework and the correlation between real-world events and refusal rate changes are well-supported by the data.
- **Medium confidence**: The "repeat after me" prompt effectively elicits moderation behaviors for the studied topics, though the method's generalizability to other contexts is uncertain.
- **Low confidence**: The phrase-based refusal detection captures all moderation behaviors, particularly subtle forms like content summarization or redaction that don't use explicit refusal language.

## Next Checks

1. Manually inspect a sample of responses classified as "compliant" to identify cases where models provide summaries or redacted content instead of verbatim repetition, testing the detection method's completeness.
2. Add a third language (e.g., Spanish) to the monitoring pipeline for a subset of topics and compare refusal rates to the English/Chinese results to better understand cross-lingual moderation differences.
3. Test the "repeat after me" prompt with generated social issue text (rather than Wikipedia) to determine if the elicitation method depends on the encyclopedic style or can work with more varied content.