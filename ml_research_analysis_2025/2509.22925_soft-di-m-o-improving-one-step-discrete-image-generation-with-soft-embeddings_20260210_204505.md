---
ver: rpa2
title: 'Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings'
arxiv_id: '2509.22925'
source_url: https://arxiv.org/abs/2509.22925
tags:
- uni00000013
- arxiv
- preprint
- teacher
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The work addresses two key limitations of one-step discrete image\
  \ generators distilled from masked diffusion models: modeling bias inherited from\
  \ the teacher and inability to apply post-training refinements due to discrete outputs.\
  \ It introduces soft embeddings\u2014continuous, differentiable token embeddings\
  \ computed as expectations under the generator\u2019s output distribution\u2014\
  to enable end-to-end training and compatibility with existing backbones."
---

# Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings

## Quick Facts
- **arXiv ID:** 2509.22925
- **Source URL:** https://arxiv.org/abs/2509.22925
- **Reference count:** 40
- **One-line primary result:** Achieves 1.56 FID on ImageNet-256 with GAN refinement, setting state-of-the-art for one-step discrete image generation

## Executive Summary
Soft-Di[M]O addresses fundamental limitations in one-step discrete image generators distilled from masked diffusion models (MDMs): inherited teacher bias and inability to apply post-training refinements due to discrete outputs. The key innovation is "soft embeddings"—continuous, differentiable token embeddings computed as expectations under the generator's output distribution—which enable end-to-end training and compatibility with existing MDM backbones. By integrating soft embeddings into the Di[M]O framework, the method supports GAN-based refinement, differentiable reward fine-tuning, and test-time embedding optimization. Empirically, Soft-Di[M]O achieves state-of-the-art one-step performance across multiple benchmarks and tasks.

## Method Summary
The method replaces discrete token sampling with soft embeddings—probability-weighted averages of token embeddings computed from generator logits. This continuous relaxation enables gradient flow for adversarial training and reward optimization while maintaining compatibility with frozen MDM backbones. The training combines Di[M]O distillation loss with GAN loss (for class-to-image) or reward loss (for text-to-image), using mask augmentation on soft embeddings. The generator outputs logits, which are converted to soft embeddings via the teacher's embedding matrix, allowing downstream discriminators or reward models to provide gradients back to the generator.

## Key Results
- Achieves **1.56 FID** on ImageNet-256 with GAN refinement (MaskBit teacher)
- Improves **GenEval** and **HPS** scores on text-to-image with reward tuning (MaskGen-L teacher)
- Demonstrates **test-time embedding optimization (TTEO)** for further quality gains
- Shows robust performance across different teacher models (MaskGit, MaskBit, Meissonic, MaskGen-L)

## Why This Works (Mechanism)

### Mechanism 1: Continuous Relaxation of Discrete Tokens (Soft Embeddings)
Soft embeddings replace discrete token samples with a probability-weighted expectation of embeddings, restoring gradient flow without significantly altering the representation. The method computes $\tilde{e} = \sum p_j E_j$ as a deterministic, differentiable function of logits, assuming the generator's output distribution is often concentrated (low entropy).

### Mechanism 2: Adversarial Correction of Teacher Bias
The soft embeddings enable adversarial training where a discriminator (built on frozen teacher backbone) distinguishes between generated soft sequences and real token embeddings. This guides the student to produce embeddings that lie on the manifold of real data, potentially surpassing teacher performance.

### Mechanism 3: Differentiable Alignment via Reward Optimization
Soft embeddings make the image decoding pipeline differentiable, allowing direct optimization of human preference rewards (CLIP, HPS). The generator logits → soft embedding → tokenizer decoder → image → reward model path becomes fully differentiable for fine-tuning.

## Foundational Learning

- **Masked Diffusion Models (MDMs)**: The "Teacher" is an MDM that generates images by iteratively predicting masked tokens, which is slow and requires distillation. *Why needed:* Understanding the teacher's multi-step generation process is crucial for grasping the distillation challenge. *Quick check:* How does an MDM generate an image differently than a standard continuous diffusion model?

- **Knowledge Distillation (Teacher-Student)**: The paper builds upon Di[M]O, a distillation method that compresses a multi-step teacher into a one-step student. *Why needed:* Understanding the base Di[M]O framework is essential for comprehending how soft embeddings enhance it. *Quick check:* In standard distillation, what is the student trained to mimic?

- **The Gumbel-Softmax Trick / Straight-Through Estimators**: The paper contrasts "Soft Embeddings" with these older methods for handling discrete variables. *Why needed:* Understanding the limitations of existing methods (high variance or bias) explains why a new approach was necessary. *Quick check:* Why can't we simply backpropagate through a standard categorical sample?

## Architecture Onboarding

- **Component map**: Input $x_{init}$ → Student Generator ($G_\theta$) → Logits $z_\theta$ → Soft Embedding Layer → Downstream Heads (Distillation/Discriminator/Reward)

- **Critical path**: The implementation of `Emb(z_theta)` must use the exact embedding weights from the Tokenizer or Teacher backbone. The gradient must flow: `Loss -> Soft Embedding -> Logits`

- **Design tradeoffs**: Temperature (default 1.0 works best; higher temps increase approximation error), Loss Balancing (GAN-only leads to mode collapse, Reward-only leads to hacking)

- **Failure signatures**: Mode Collapse (different prompts yield identical images), Reward Hacking (over-saturated, unnatural images with high reward scores)

- **First 3 experiments**:
  1. Gradient Sanity Check: Verify gradients exist and are non-zero for input logits
  2. Ablation: Soft vs. Hard - Compare image quality using argmax vs. soft embedding decoding
  3. GAN Stability Test: Train with only Di[M]O loss vs. Di[M]O + GAN and plot FID over time

## Open Questions the Paper Calls Out

- **Open Question 1**: Can soft embeddings combined with continuous tokenizers overcome the reconstruction FID bottleneck currently limiting discrete tokenizers?
- **Open Question 2**: Can soft embeddings enable SiD-style (Score Identity Distillation) methods for MDMs by allowing gradients to backpropagate through both teacher and auxiliary models?
- **Open Question 3**: Can soft embeddings extend to multi-modal discrete diffusion models (e.g., Show-o, Fudoki) for unified understanding and generation?
- **Open Question 4**: Can soft embeddings overcome the near-perfect output distribution precision requirement that makes DLLM (Diffusion Large Language Model) distillation significantly more challenging than visual MDM distillation?

## Limitations

- **Representation Fidelity under High Entropy**: The soft embedding approximation relies on concentrated generator distributions and becomes a "blurry" average when the distribution is flat, potentially degrading quality for rare tokens
- **Over-regularization Risk**: Keeping Di[M]O loss as regularization may prevent full exploitation of adversarial and reward optimization paths, remaining too constrained by teacher errors
- **Teacher Bias Amplification**: The method shows only marginal improvement (+0.04 FID) for strong teachers like MaskGen-L, suggesting it may not robustly overcome all forms of teacher bias

## Confidence

**High Confidence** (well-supported by evidence):
- Soft embeddings enable differentiable training by providing continuous relaxation of discrete tokens
- The representation fidelity claim is supported by MSE measurements
- The Di[M]O loss as regularization prevents reward hacking during fine-tuning

**Medium Confidence** (supported but with caveats):
- Adversarial training effectively corrects teacher bias (evidence shows improvement but with diminishing returns and potential instability)
- Reward optimization improves human preference metrics (evidence shows gains but also potential for hacking without sufficient regularization)

**Low Confidence** (inferred but not directly tested):
- Soft embeddings generalize well to high-entropy distributions (not systematically tested)
- The soft embedding approximation performs equivalently to direct token sampling in all scenarios (only tested on concentrated distributions)

## Next Checks

1. **Entropy Sensitivity Analysis**: Systematically measure representation fidelity (MSE and perceptual quality) of soft embeddings across a range of output entropies (temperature sweep from 0.1 to 10.0) and identify failure thresholds.

2. **Teacher Bias Amplification Study**: Implement a version that completely removes Di[M]O regularization and compare performance across multiple teacher strengths to measure whether the method amplifies teacher errors.

3. **Multi-round GAN Refinement Experiment**: Test whether multiple rounds of GAN training are needed to fully correct teacher bias by implementing iterative refinement and measuring improvements at each stage.