---
ver: rpa2
title: Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling
  with an Open-Source Large Language Model
arxiv_id: '2502.01691'
source_url: https://arxiv.org/abs/2502.01691
tags:
- uncertainty
- weights
- radiology
- agent
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an agent-based uncertainty-aware approach
  to improve LLM reliability in extracting structured data from Hebrew radiology reports
  of Crohn's disease patients. Using Llama 3.1 with Bayesian Prompt Ensembles and
  six semantically equivalent prompts, an Agent-Based Decision Model aggregated outputs
  into five confidence levels.
---

# Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model

## Quick Facts
- arXiv ID: 2502.01691
- Source URL: https://arxiv.org/abs/2502.01691
- Reference count: 0
- Primary result: Agent-based uncertainty estimation improves F1 from 0.3967 to 0.4787 when filtering high-uncertainty cases in Hebrew radiology reports

## Executive Summary
This study introduces an agent-based uncertainty-aware approach to improve LLM reliability in extracting structured data from Hebrew radiology reports of Crohn's disease patients. Using Llama 3.1 with Bayesian Prompt Ensembles and six semantically equivalent prompts, an Agent-Based Decision Model aggregated outputs into five confidence levels. Compared to entropy-based models, the agent-based approach achieved the best balance of performance (F1=0.3967, recall=0.6437, Cohen's Kappa=0.3006) and calibrated uncertainty estimates. Filtering high-uncertainty cases (≥0.5) further improved results (F1=0.4787, Kappa=0.4258), demonstrating clear separation between correct and incorrect predictions. This method enhances LLM interpretability and trustworthiness in high-stakes medical applications.

## Method Summary
The method uses Llama 3-8b-instruct as a backbone LLM with six semantically equivalent prompts (Bayesian Prompt Ensembles) to generate extraction predictions for structured findings from Hebrew radiology reports. An Agent-Based Decision Model using Llama 3-70B aggregates these outputs into five confidence levels (Definitely Yes/No, Likely Yes/No, Uncertain). Three entropy-based aggregation methods (uniform weights, linear-optimized weights, MLP-learned weights) serve as baselines. The approach estimates uncertainty through prompt-output variance and uses filtering (≥0.5 threshold) to improve performance on retained cases.

## Key Results
- Agent-based model achieved F1=0.3967, recall=0.6437, and Kappa=0.3006 on the full test set
- Filtering high-uncertainty cases (≥0.5) improved F1 to 0.4787 and Kappa to 0.4258
- Agent-based approach provided the most well-calibrated uncertainty estimates with clear separation between correct and incorrect predictions
- Compared to entropy-based models, the agent-based method offered superior balance of performance and uncertainty calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically equivalent prompt ensembles expose model uncertainty through output variance
- Mechanism: Six prompts with identical intent but different phrasing are fed to the LLM independently. Variation in answers across prompts serves as an uncertainty signal—when prompts disagree, the model is implicitly less certain about the correct extraction.
- Core assumption: Prompt-output inconsistency correlates with underlying prediction uncertainty; models that answer consistently across paraphrased prompts are more likely correct.
- Evidence anchors:
  - [abstract] "Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty"
  - [section] "We employed the Bayesian Prompt Ensembles (BayesPE) method to estimate uncertainty in LLM-generated predictions by leveraging an ensemble of semantically equivalent prompts"
  - [corpus] Related work on uncertainty-adjusted label extraction (arXiv:2510.05664) similarly uses LLM-based uncertainty for label quality, supporting the general premise but not the specific BayesPE mechanism.
- Break condition: If prompts are too similar syntactically, variance may underestimate uncertainty; if too different semantically, variance may conflate prompt quality with model uncertainty.

### Mechanism 2
- Claim: Agent-based aggregation provides better-calibrated uncertainty than entropy-based weighting by evaluating response coherence
- Mechanism: A larger LLM agent (Llama 3-70B) receives all prompt outputs (answers + explanations) and synthesizes them into one of five discrete confidence levels (Definitely Yes/No, Likely Yes/No, Uncertain), explicitly reasoning about consistency and explanation quality.
- Core assumption: A stronger model can act as a reliable meta-judge of weaker model outputs, and discrete confidence categories provide actionable calibration.
- Evidence anchors:
  - [abstract] "An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty"
  - [section] "The agent evaluates response consistency, assesses the clarity and coherence of explanations, and identifies indications of uncertainty"
  - [corpus] Limited direct corpus evidence for this specific agent-based calibration approach; related agent-based evaluation work (arXiv:2508.02808) focuses on report evaluation, not uncertainty aggregation.
- Break condition: If the agent model has systematic biases (e.g., over-trusting verbose explanations), calibration degrades; agent must outperform backbone on this meta-task.

### Mechanism 3
- Claim: Filtering high-uncertainty cases improves downstream metrics by separating correct from incorrect predictions
- Mechanism: After uncertainty estimation, cases with uncertainty ≥0.5 are flagged for exclusion or human review. This removes ambiguous or conflicting predictions, improving aggregate F1 and Kappa on retained cases.
- Core assumption: Uncertainty estimates are sufficiently calibrated that thresholding preferentially removes incorrect predictions.
- Evidence anchors:
  - [abstract] "Filtering high-uncertainty cases (≥0.5) further improved results (F1=0.4787, Kappa=0.4258)"
  - [section] "Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates"
  - [corpus] Consistent with uncertainty-filtering approaches in related medical LLM work, though corpus evidence is observational rather than mechanistic.
- Break condition: If threshold is too aggressive, too many cases are excluded; if too permissive, noise remains. Class imbalance may skew optimal threshold per label.

## Foundational Learning

- Concept: **Bayesian Prompt Ensembles**
  - Why needed here: Core technique for uncertainty estimation without model retraining; requires understanding how prompt variance proxies for confidence.
  - Quick check question: Can you explain why six semantically equivalent prompts would produce different outputs, and what that implies about model certainty?

- Concept: **Uncertainty Calibration**
  - Why needed here: Raw uncertainty scores must distinguish correct from incorrect predictions; calibration determines filtering effectiveness.
  - Quick check question: If a model outputs uncertainty=0.4 for both correct and incorrect predictions, is it well-calibrated? What would the histogram show?

- Concept: **Agent-as-Meta-Judge Pattern**
  - Why needed here: The agent model must aggregate and evaluate weaker model outputs; understanding this delegation pattern is critical for architecture design.
  - Quick check question: What capabilities must the agent model have that the backbone model lacks for this aggregation task?

## Architecture Onboarding

- Component map:
  - Backbone LLM (Llama 3-8b-instruct) -> 6 parallel prompt calls -> JSON-formatted answers + explanations -> Aggregation layer -> Final decision + uncertainty score -> Filtering gate (if uncertainty ≥ threshold)

- Critical path:
  1. Radiology report → 6 parallel prompt calls to backbone LLM
  2. Each call returns JSON with Answer + Explanation
  3. Aggregation layer consumes all 6 outputs → final decision + uncertainty score
  4. If uncertainty ≥ threshold, case flagged for exclusion/review

- Design tradeoffs:
  - Agent-based vs. entropy-based: Agent gives better calibration and recall; entropy-based (MLP) gives higher precision and accuracy
  - Filtering aggressiveness: Higher exclusion improves metrics on retained data but reduces coverage (33% excluded in best case)
  - Prompt count: 6 chosen for efficiency; more prompts give marginal gains per paper

- Failure signatures:
  - Low variance across prompts but high error rate → prompts insufficiently diverse or model systematically wrong
  - Agent over-confident on incorrect predictions → agent model not calibrated for this task
  - High exclusion rate (>40%) with modest metric gains → uncertainty estimates poorly calibrated

- First 3 experiments:
  1. Replicate single-prompt baseline vs. 6-prompt ensemble on a small held-out set to validate aggregation benefit
  2. Implement uniform-weight entropy aggregation as a minimal baseline before adding agent complexity
  3. Plot uncertainty histograms for correct vs. incorrect predictions to verify calibration before deploying filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the agent-based uncertainty-aware approach maintain its performance advantages when applied to other medical document types (e.g., pathology reports, electronic medical records) and clinical domains beyond Crohn's disease radiology?
- Basis in paper: [explicit] "First, while the focus was on radiology reports, extending these methods to LLM-based analysis of other medical reports, such as pathology reports or electronic medical records, requires additional validation."
- Why unresolved: The current study only validated the approach on Hebrew radiology reports for Crohn's disease patients. Different document types have distinct linguistic patterns, structures, and clinical terminologies that may affect the agent's ability to synthesize prompt outputs effectively.
- What evidence would resolve it: Comparative evaluation of the agent-based method on pathology reports and EMRs across multiple clinical domains, demonstrating consistent improvements in F1, Kappa, and uncertainty calibration.

### Open Question 2
- Question: How can ensemble-based uncertainty estimation be optimized for highly imbalanced datasets where pathological findings occur rarely?
- Basis in paper: [explicit] "Second, optimizing ensemble techniques for highly imbalanced datasets remains a challenge. Future work should explore tailored approaches to address these limitations."
- Why unresolved: The study excluded organ–finding combinations with ≤15 positive cases, and the MLP model showed lower recall (0.3977) despite high precision, indicating difficulty detecting minority classes. The threshold-based filtering may disproportionately eliminate rare positive cases.
- What evidence would resolve it: Development and validation of class-specific uncertainty thresholds or weighted loss functions that improve recall for rare findings while maintaining calibrated uncertainty estimates.

### Open Question 3
- Question: Does the computational overhead of using a larger agent model (Llama 3-70B) to aggregate outputs from a smaller backbone model (Llama 3-8B) justify the performance gains compared to simpler aggregation methods?
- Basis in paper: [inferred] The paper uses Llama 3-70B as the agent model while the backbone LLM is Llama 3-8B-instruct. While performance improvements are demonstrated, no analysis is provided regarding computational cost, inference time, or resource requirements.
- Why unresolved: The agent model is approximately 10× larger than the backbone model. In clinical deployment scenarios, computational resources and inference latency are critical constraints that could limit practical adoption.
- What evidence would resolve it: Comparative analysis of inference time, GPU memory requirements, and cost-per-prediction across all aggregation methods (uniform, linear, MLP, agent), with evaluation of performance-per-computational-unit metrics.

## Limitations

- Data accessibility: Hebrew radiology reports and in-house HSMP-BERT model not publicly available, limiting reproducibility
- Narrow clinical scope: Limited to Crohn's disease and specific organ-finding pairs, reducing generalizability to broader radiology reporting
- Resource intensity: Agent-based approach requires larger LLM (70B parameters) for aggregation, increasing computational costs
- Potential prompt sensitivity: Performance relies heavily on prompt engineering quality and semantic equivalence, which may not transfer across languages or medical contexts

## Confidence

- **High confidence** in the mechanism showing agent-based aggregation improves calibration over entropy-based methods, supported by clear metric improvements (F1: 0.3967→0.4787, Kappa: 0.3006→0.4258)
- **Medium confidence** in the generalizability claim, as results are demonstrated only on Hebrew Crohn's disease reports with specific organ-finding pairs
- **Low confidence** in the independence of the agent-based approach from prompt quality, given the critical role of semantically equivalent prompts in the BayesPE method

## Next Checks

1. Replicate the single-prompt baseline vs. 6-prompt ensemble comparison on a small held-out set to verify aggregation benefits before adding agent complexity
2. Implement uniform-weight entropy aggregation as a minimal baseline, then incrementally add linear optimization and MLP methods to isolate performance contributions
3. Conduct cross-linguistic validation by testing the agent-based approach on English radiology reports to assess language dependency and prompt sensitivity