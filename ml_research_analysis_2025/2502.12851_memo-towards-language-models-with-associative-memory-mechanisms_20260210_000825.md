---
ver: rpa2
title: 'MeMo: Towards Language Models with Associative Memory Mechanisms'
arxiv_id: '2502.12851'
source_url: https://arxiv.org/abs/2502.12851
tags:
- vectors
- sequences
- memo
- language
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MeMo, a novel architecture for language modeling
  that explicitly memorizes sequences of tokens using layered associative memories.
  Unlike traditional transformer-based models that learn to capture sequential dependencies
  through training, MeMo directly stores associations between input sequences and
  output tokens using correlation matrix memories (CMMs) combined with multivariate
  Gaussian vectors and Johnson-Lindenstrauss transforms.
---

# MeMo: Towards Language Models with Associative Memory Mechanisms

## Quick Facts
- arXiv ID: 2502.12851
- Source URL: https://arxiv.org/abs/2502.12851
- Reference count: 15
- Key outcome: Novel architecture using layered associative memories to directly memorize token sequences with >96% accuracy for 250,000 sequences

## Executive Summary
MeMo introduces a language modeling architecture that prioritizes explicit memorization over learned generalization. Unlike transformer models that implicitly capture sequential dependencies through training, MeMo uses correlation matrix memories (CMMs) to directly store associations between input and output token sequences. The architecture employs multivariate Gaussian vectors and Johnson-Lindenstrauss transforms to create sparse, separable representations that enable transparent editing and forgetting of stored knowledge.

The paper demonstrates that MeMo can achieve high memorization accuracy for large numbers of sequences while maintaining editability through its explicit storage mechanism. With three layers and appropriate dimensionality (4096-8192), the model successfully handles decoy patterns and achieves over 96% accuracy on memorization tasks. This approach offers potential advantages for controlled knowledge storage and retrieval in language models while reducing data requirements compared to training-intensive methods.

## Method Summary
MeMo replaces traditional learned attention mechanisms with explicit associative memory storage. The architecture uses correlation matrix memories to store associations between input and output sequences, where each token is represented as a multivariate Gaussian vector transformed by Johnson-Lindenstrauss to ensure sparsity. The model supports both single-layer direct memorization and multi-layer configurations for handling longer sequences through hierarchical storage. The explicit nature of the memory allows for controlled forgetting by subtracting associations, making the system transparent and editable compared to black-box transformer approaches.

## Key Results
- Single-layer MeMo stores sequences linearly proportional to its parameter count
- Three-layer MeMo with d=4096-8192 achieves >96% memorization accuracy for up to 250,000 sequences
- Model successfully handles decoy patterns that challenge simpler architectures
- Memorization accuracy scales with dimensionality and number of layers

## Why This Works (Mechanism)
The core mechanism relies on correlation matrix memories (CMMs) that explicitly store associations between input and output sequences. Each token is encoded as a multivariate Gaussian vector, transformed via Johnson-Lindenstrauss to create sparse representations that minimize interference between stored patterns. The correlation matrices capture the statistical relationships between input-output pairs, enabling direct retrieval through vector operations. This explicit storage contrasts with transformer attention mechanisms by making all associations transparent and directly modifiable.

## Foundational Learning
- Correlation Matrix Memories (CMMs): Associative storage systems that map input vectors to output vectors through matrix operations
  - Why needed: Enables explicit memorization rather than learned inference
  - Quick check: Can you describe how a CMM stores and retrieves associations?
- Johnson-Lindenstrauss Transform: Dimensionality reduction technique that preserves distances between points
  - Why needed: Creates sparse vector representations that minimize pattern interference
  - Quick check: What mathematical property ensures the transform maintains vector separability?
- Multivariate Gaussian Vectors: Random vectors with normally distributed components
  - Why needed: Provides probabilistic encoding that supports sparse representation after transformation
  - Quick check: How does the covariance structure affect the resulting sparse vectors?
- Tokenization: Conversion of text into discrete tokens (words, subwords, or characters)
  - Why needed: Enables discrete encoding of language into vector representations
  - Quick check: What tokenization strategy would work best for MeMo's memory architecture?

## Architecture Onboarding

Component Map:
Tokenizer -> Gaussian Vector Generator -> JL Transform -> CMM Layer -> Output Decoder

Critical Path:
Tokenization → Gaussian Encoding → JL Transformation → Memory Storage/Retrieval → Output Decoding

Design Tradeoffs:
- Memory vs. Generalization: Explicit memorization trades learned generalization for transparent control
- Dimensionality vs. Capacity: Higher dimensions increase storage capacity but require more parameters
- Single-layer vs. Multi-layer: Single layers simpler but limited; multiple layers handle longer sequences but add complexity

Failure Signatures:
- Low accuracy indicates insufficient dimensionality or interference between stored patterns
- Forgetting failures suggest incorrect subtraction of associations or overlap with retained memories
- Generalization issues point to limitations in partial sequence matching capabilities

First Experiments:
1. Test single-layer memorization with small sequence sets (10-100) to verify basic functionality
2. Evaluate forgetting mechanism by storing sequences, removing some, and measuring retention of others
3. Test multi-layer configuration on progressively longer sequences to establish capacity limits

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can MeMo effectively embed and leverage explicit knowledge representations such as knowledge graphs, linguistic ontologies, and transformation rules in practical applications?
- Basis in paper: The authors state: "With MeMo, we could control how linguistic knowledge is used to generalize examples, we could embed transformation rules, and we could represent knowledge graphs and linguistic ontologies."
- Why unresolved: The paper demonstrates memorization but does not experiment with structured knowledge integration or linguistic rule embedding.
- What evidence would resolve it: Experiments showing MeMo storing and querying knowledge graph triples, applying morphological rules, or using ontological hierarchies to improve retrieval accuracy.

### Open Question 2
- Question: How does MeMo compare to transformer-based LLMs on standard language modeling benchmarks (e.g., perplexity on WikiText, question answering, summarization)?
- Basis in paper: The authors acknowledge: "it has not been possible to experiment with the model using the current evaluation suites" due to incompatibility with the HuggingFace ecosystem.
- Why unresolved: All experiments use randomly generated sequences rather than real-world language tasks, precluding direct comparison with existing models.
- What evidence would resolve it: Benchmarking MeMo against transformers on standard datasets using perplexity, F1 scores, or task-specific metrics.

### Open Question 3
- Question: Does the forgetting mechanism enable precise selective erasure without degrading the retrieval accuracy of non-targeted sequences?
- Basis in paper: The paper describes the forgetting equation but provides no experimental validation of its effectiveness or side effects on stored memories.
- Why unresolved: Forgetting is presented as a key advantage for editability and privacy, yet no experiments test whether subtracting associations corrupts neighboring stored patterns.
- What evidence would resolve it: Experiments measuring accuracy on retained sequences after targeted forgetting operations, with analysis of interference effects.

### Open Question 4
- Question: How does MeMo's limited generalization capability (partial sequence matching) compare to the compositional generalization of transformer-based models?
- Basis in paper: The paper claims generalization through partial matches but only demonstrates trivial cases (e.g., 0.75 weight for shared tokens), leaving compositional and syntactic generalization unexplored.
- Why unresolved: Real language requires systematic generalization to novel combinations, which random vector summation may not capture.
- What evidence would resolve it: Evaluation on compositional generalization benchmarks (e.g., SCAN, COGS) comparing MeMo's rule-like behavior against neural baselines.

## Limitations
- Cannot evaluate on standard language modeling benchmarks due to HuggingFace ecosystem incompatibility
- Experimental validation limited to controlled random sequence memorization tasks
- Generalization capabilities remain unexplored beyond trivial partial sequence matching
- Forgetting mechanism effectiveness not experimentally validated

## Confidence
- High: Architecture details and mathematical foundations are clearly explained with detailed experimental results
- Medium: Claims about editability and forgetting are theoretically sound but lack experimental validation
- Medium: Potential applications to knowledge graphs and linguistic rules are proposed but not tested

## Next Checks
1. Evaluate MeMo's performance on real-world language modeling benchmarks (WikiText, GLUE, SuperGLUE) to compare against transformer baselines
2. Test forgetting mechanism effectiveness by storing multiple sequences, performing selective erasure, and measuring retention accuracy of remaining patterns
3. Investigate MeMo's capability on compositional generalization tasks (SCAN, COGS) to assess its rule-like behavior compared to neural approaches