---
ver: rpa2
title: Contextual Embedding-based Clustering to Identify Topics for Healthcare Service
  Improvement
arxiv_id: '2504.14068'
source_url: https://arxiv.org/abs/2504.14068
tags:
- topic
- feedback
- topics
- patient
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addressed the challenge of identifying complaint topics
  from short-text patient feedback using unsupervised methods. A keyword-based filtering
  approach was developed to isolate complaint-related comments from 439 survey responses.
---

# Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement

## Quick Facts
- arXiv ID: 2504.14068
- Source URL: https://arxiv.org/abs/2504.14068
- Reference count: 24
- Identified complaint topics from short-text patient feedback using embedding-based clustering with kBERT achieving highest coherence

## Executive Summary
This study addressed the challenge of identifying complaint topics from short-text patient feedback using unsupervised methods. A keyword-based filtering approach was developed to isolate complaint-related comments from 439 survey responses. Four topic modeling approaches—LDA, GSDMM, BERTopic, and a novel kBERT—were compared. kBERT combined BERT embeddings with k-means clustering and outperformed others with a coherence score of 0.53 and perfect topic separation (IRBOavg = 1.00). GSDMM also performed well on short texts, while BERTopic excelled on larger datasets but struggled with smaller ones. The results demonstrate that embedding-based models like kBERT are effective for extracting meaningful insights from sparse, domain-specific healthcare feedback.

## Method Summary
The study employed a keyword-based filtering approach using a domain-specific lexicon of 202 "hot words" to isolate complaint-related feedback from patient surveys. The filtered comments underwent preprocessing with stopword removal and morphological expansion via the `word-forms` library. Four topic modeling approaches were compared: LDA, GSDMM, BERTopic, and kBERT. The kBERT model used frozen bert-base-uncased embeddings with k-means clustering, avoiding dimensionality reduction to preserve linguistic features. Performance was evaluated using coherence scores (Cv) and topic diversity (IRBOavg) across datasets of varying sizes, from 439 patient responses to public datasets like Clickbait-title and 20Newsgroup subsets.

## Key Results
- kBERT achieved the highest coherence score of 0.53 for 3 topics on Patient Feedback, demonstrating effectiveness with short and sparse data
- kBERT achieved perfect topic separation with IRBOavg = 1.00 on Patient Feedback
- BERTopic showed limited performance on smaller datasets (Cv = 0.46) but excelled on larger datasets (Cv = 0.49 on Clickbait at 20 topics)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BERT embeddings preserve semantic context better than bag-of-words representations for short, sparse healthcare feedback.
- **Mechanism:** The paper uses frozen BERT embeddings (bert-base-uncased) as pure feature extractors, preserving high-dimensional contextual representations. Unlike LDA's word-frequency approach, BERT captures polysemy and domain-specific nuances. The authors explicitly avoid dimensionality reduction, arguing it removes key linguistic features in low-resource settings.
- **Core assumption:** Pre-trained BERT representations transfer sufficiently to healthcare domain without fine-tuning.
- **Evidence anchors:**
  - [abstract]: "kBERT combined BERT embeddings with k-means clustering and outperformed others with a coherence score of 0.53"
  - [section IV]: "kBERT achieved the highest coherence score of 0.53 for 3 topics on Patient Feedback, demonstrating its effectiveness in short and sparse data"
  - [corpus]: Neighbor paper "IOCC" confirms short text representations have limited expressiveness, making embedding quality critical—though weak direct evidence for this specific mechanism.
- **Break condition:** If texts contain heavy medical jargon or abbreviations not well-represented in BERT's pre-training corpus, semantic quality degrades.

### Mechanism 2
- **Claim:** Domain-specific keyword filtering with morphological expansion improves complaint detection sensitivity.
- **Mechanism:** A curated lexicon of 202 "hot words" across four categories (full, shortened, prefix, jargon) filters feedback. The `word_forms` library generates all POS variants (noun, verb, adjective, adverb), followed by lemmatization to capture morphological variants (e.g., "argue" → "argued," "arguing").
- **Core assumption:** Complaint-related feedback contains detectable lexical markers that general sentiment tools miss.
- **Evidence anchors:**
  - [abstract]: "A keyword-based filter was used to isolate complaint-related feedback using a domain-specific lexicon"
  - [section III.B]: "generating and matching keywords across different POS after lemmatization... propelled the match count to a peak of 102 instances"
  - [corpus]: No direct corpus evidence for this specific keyword-expansion mechanism.
- **Break condition:** If complaints are expressed through subtle sarcasm, implied dissatisfaction, or domain terms absent from the lexicon, filtering fails.

### Mechanism 3
- **Claim:** k-means clustering on frozen embeddings works better than HDBSCAN-based approaches for small, short-text datasets.
- **Mechanism:** BERTopic uses UMAP + HDBSCAN, which struggles when data density is insufficient for automatic cluster detection. kBERT uses k-means with explicit cluster count specification, giving control over topic granularity. The paper reports BERTopic generated only 3 topics on patient feedback (Cv = 0.46) versus kBERT's configurable approach achieving Cv = 0.53.
- **Core assumption:** Pre-specifying cluster counts compensates for insufficient data density in small corpora.
- **Evidence anchors:**
  - [section IV]: "BERTopic... showed limited performance on smaller ones (Cv = 0.46). kBERT achieved the highest coherence score of 0.53"
  - [Table III]: BERTopic produced N/A for most small dataset configurations while kBERT remained stable
  - [corpus]: Neighbor paper "LLMs Enable Bag-of-Texts Representations" suggests embedding-based clustering quality depends heavily on corpus size—partial support.
- **Break condition:** If k (cluster count) is misspecified relative to true thematic diversity, topics become over-merged or fragmented.

## Foundational Learning

- **Concept: Topic coherence metrics (Cv, NPMI)**
  - **Why needed here:** The paper optimizes for Cv scores; understanding what Cv measures (semantic relatedness of top topic words via NPMI) is essential for interpreting results.
  - **Quick check question:** If Cv = 0.53 versus 0.35, what does that numerically indicate about word co-occurrence patterns in top topic words?

- **Concept: Rank-Biased Overlap (RBO) for topic diversity**
  - **Why needed here:** IRBOavg measures topic separation; perfect 1.00 score means zero overlap in top words across topics.
  - **Quick check question:** Why might high coherence but low diversity indicate a problematic model?

- **Concept: Frozen vs. fine-tuned embeddings**
  - **Why needed here:** The paper explicitly freezes BERT to prevent overfitting on 439 samples; this design choice affects generalization.
  - **Quick check question:** What is the tradeoff between freezing embeddings (no overfitting risk) versus fine-tuning (better domain adaptation)?

## Architecture Onboarding

- **Component map:** Preprocessing (Tokenization → Stopword removal) → Keyword Filter (Morphological expansion → POS-aware lemmatization → Lexicon matching) → Embedding Layer (bert-base-uncased frozen) → Clustering (k-means with configurable k) → Interpretability (Centroid-nearest comments → Top frequent words → Topic labels)

- **Critical path:** Keyword filtering quality → Embedding semantic coverage → k selection → Coherence/diversity evaluation. The filtering step (102/439 complaints identified) determines downstream topic quality.

- **Design tradeoffs:**
  - High-dimensional embeddings (no UMAP) preserve information but may include noise
  - Fixed k requires manual tuning versus BERTopic's automatic detection
  - Uncased model reduces variability but may lose emphasis signals (e.g., "UNACCEPTABLE")

- **Failure signatures:**
  - BERTopic generating ≤3 topics on small datasets (under-clustering)
  - LDA Cv dropping below 0.35 on short texts (sparse word co-occurrence)
  - Low IRBO (<0.90) indicating topic word overlap/semantic redundancy

- **First 3 experiments:**
  1. **Baseline reproduction:** Run all four models (LDA, GSDMM, BERTopic, kBERT) on the 439-sample dataset with k=3, compute Cv and IRBOavg to verify paper's reported scores (kBERT: Cv=0.53, IRBO=1.00).
  2. **Ablation on keyword filtering:** Compare complaint detection rates: (a) raw keywords only, (b) +lemmatization, (c) +POS expansion. Target: replicate 89→92→102 match progression.
  3. **Scale sensitivity test:** Run kBERT on Clickbait-title subsets (500 vs. 10,000 samples) to characterize performance degradation curve for small short-text scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can BERTopic's clustering mechanism be adapted to prevent failure (topic collapse) in datasets with fewer than 500 documents?
- Basis in paper: [explicit] Table III and the results section note that BERTopic failed to generate valid topics (N/A) or generated only 2 topics on the 500-sample subsets, struggling with the density assumptions of HDBSCAN on small data.
- Why unresolved: The paper identifies the limitation of BERTopic on small datasets but does not investigate parameter tuning or alternative clustering algorithms within the BERTopic pipeline to mitigate this.
- What evidence would resolve it: A comparative study on the 500-sample subsets replacing BERTopic's default HDBSCAN with k-means or agglomerative clustering to see if valid topics emerge.

### Open Question 2
- Question: Does the performance advantage of kBERT over GSDMM persist as the dataset size scales beyond the "small data" regime (N > 1000)?
- Basis in paper: [inferred] The results in Table II (large datasets) show GSDMM outperforming kBERT on the Clickbait dataset (0.49 vs 0.41 at 20 topics), whereas Table III (small datasets) shows the opposite. This suggests kBERT's advantage is specific to low-data regimes.
- Why unresolved: The study concludes kBERT is superior based on the primary small dataset (439 samples) but provides evidence that GSDMM may be superior for larger short-text corpora.
- What evidence would resolve it: A benchmarking study plotting coherence scores for both models across a gradient of dataset sizes (e.g., 100, 500, 1000, 5000 samples) using the same short-text source.

### Open Question 3
- Question: Does the preservation of high-dimensional embeddings in kBERT specifically improve the detection of domain-specific jargon compared to dimensionality-reduced approaches?
- Basis in paper: [inferred] The authors state in Section III.D that they avoided dimensionality reduction (used in BERTopic) to preserve linguistic features for domain-specific language, but they did not isolate this variable to prove it helps "jargon" specifically.
- Why unresolved: While kBERT performed better overall, it is unclear if this was due to the clustering algorithm (k-means) or the preservation of the full embedding space.
- What evidence would resolve it: An ablation study running kBERT with and without UMAP dimensionality reduction, specifically measuring the frequency of domain-specific "hot words" appearing in the top topic representations.

## Limitations
- Private 439-sample patient feedback dataset prevents independent validation of reported performance metrics
- Domain-specific keyword lexicon incompletely specified—only 15 of 202 keywords provided
- Frozen BERT approach may miss domain-specific semantic nuances that fine-tuning could capture

## Confidence
- **High confidence:** The comparative performance ranking of kBERT over LDA, GSDMM, and BERTopic on the tested datasets is internally consistent and methodologically sound
- **Medium confidence:** The claim that frozen BERT embeddings capture sufficient semantic context for healthcare complaints is plausible but lacks direct validation against fine-tuned alternatives
- **Low confidence:** The specific effectiveness of the keyword filtering approach (102/439 complaints identified) cannot be independently verified due to incomplete lexicon specification

## Next Checks
1. **Keyword filtering reproducibility:** Using the public Clickbait-title dataset as a surrogate, implement the full keyword expansion pipeline (POS forms + lemmatization) and measure complaint detection sensitivity. Target: achieve at least 90% of the 102-match count reported for patient feedback.
2. **Scale sensitivity characterization:** Systematically test kBERT performance on datasets ranging from 100 to 10,000 samples to quantify the claimed superiority over BERTopic's automatic clustering on small datasets. Track coherence scores and topic counts across scales.
3. **Domain transfer validation:** Fine-tune bert-base-uncased on a small medical text corpus (e.g., MIMIC-III discharge summaries) and compare coherence scores against the frozen model on the 439-sample dataset to assess the tradeoff between overfitting prevention and domain adaptation.