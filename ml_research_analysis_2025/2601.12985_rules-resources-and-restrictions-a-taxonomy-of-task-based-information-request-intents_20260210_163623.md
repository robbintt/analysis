---
ver: rpa2
title: 'Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request
  Intents'
arxiv_id: '2601.12985'
source_url: https://arxiv.org/abs/2601.12985
tags:
- information
- action
- task
- taxonomy
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a taxonomy of task-based information request
  intents derived from grounded-theory-based expert interviews with airport information
  clerks. The taxonomy bridges traditional query-focused classifications and emerging
  AI-driven task-oriented search by capturing task-contextualized user intents.
---

# Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents

## Quick Facts
- arXiv ID: 2601.12985
- Source URL: https://arxiv.org/abs/2601.12985
- Reference count: 40
- Primary result: Introduces a 4-layer taxonomy of task-based information request intents (20 L1 categories, 86 sub-categories) with high inter-rater reliability (κ = 0.84), bridging query-focused classifications and task-oriented search.

## Executive Summary
This study presents a taxonomy of task-based information request intents derived from expert interviews with airport information clerks. The taxonomy addresses the gap between traditional query-focused intent classifications and emerging task-oriented search by capturing task-contextualized user intents. It includes 20 level-1 categories and 86 sub-categories, validated through grounded theory methodology with substantial inter-rater agreement. The findings demonstrate that user intent understanding benefits from incorporating task context, enabling more effective support in complex information-seeking scenarios.

## Method Summary
The taxonomy was developed using grounded theory methodology from 8 expert interviews with Munich Airport information clerks (120+ years combined experience, ~621 minutes total), yielding 720 identified information request statements. The process involved open, axial, and selective coding with constant comparisons and memoing. GPT-4o was used to challenge categories post-hoc while retaining human cognitive lead. Validation was performed by an outside assessor on 147 randomly selected questions (20% of dataset), achieving Cohen's κ = 0.84 (95% CI [0.78, 0.91]).

## Key Results
- Taxonomy structure: 4-layer hierarchy with 20 level-1 categories and 86 sub-categories
- High inter-rater reliability: Cohen's κ = 0.84 (95% CI [0.78, 0.91]) on validation set
- Context-dependent classification: Identical queries ("Where is...?") map to different intents based on user task stage and location
- Declarative statement handling: Successfully categorized "ill-formed queries" as valid information requests

## Why This Works (Mechanism)

### Mechanism 1: Context-Driven Intent Disambiguation
- Claim: Identical query formulations may map to distinct task-based intents depending on user's current task stage and location
- Mechanism: Expected answer types are derived from intersection of query and task context (planning vs. execution)
- Core assumption: Users optimize for cognitive efficiency; request only detail necessary for immediate task step
- Evidence anchors: Section 4.8 notes "Where is..." questions during planning require less detail than during execution; abstract mentions "task-contextualized user intents"
- Break condition: Fails without reliable context signals (user location or task phase)

### Mechanism 2: Functional Granularity via Sub-Category Depth
- Claim: High-level categories insufficient; complex task support requires granular sub-intents
- Mechanism: Decomposes user goals into 86 functional sub-categories (e.g., "necessities" vs. "permissibilities")
- Core assumption: Complex tasks have distinct procedural constraints users cannot easily articulate
- Evidence anchors: Section 4.2 distinguishes "necessities" (must do) from "permissibilities" (allowed to do); abstract notes 86 sub-categories bridge gap to AI-driven search
- Break condition: Classification collapses sub-categories into top-level buckets

### Mechanism 3: Handling Ill-Formed Queries via Goal Inference
- Claim: Users frequently issue declarative statements rather than interrogative questions when seeking task support
- Mechanism: Accounts for "Goal-Attaining Instructions" triggered by statements like "I want to take my dog"
- Core assumption: In high-stakes environments, users verbalize goal state rather than information gap
- Evidence anchors: Section 4.8 identifies "ill-formed queries" (declarative statements) as typical pattern for "How-to" requests; corpus neighbors support multi-step reasoning
- Break condition: System relies on keyword matching or question marks as triggers

## Foundational Learning

### Concept: Grounded Theory Methodology
- Why needed: Taxonomy derived inductively from 720 specific request statements, not hypothesized a priori
- Quick check: Can you distinguish between taxonomy built from log clustering vs. qualitative coding of expert interviews?

### Concept: Task-Based Information Retrieval (IR)
- Why needed: Argues against "isolated information needs" (traditional IR) in favor of "work tasks" (Task-Based IR)
- Quick check: Does user want list of restaurants (Resource), or recommendation for where to eat (Personal Recommendation)?

### Concept: Inter-Rater Reliability (Cohen's κ)
- Why needed: Score of 0.84 validates categories are distinct and interpretable by humans, prerequisite for training automated classifiers
- Quick check: Why is high agreement (κ=0.84) critical before using taxonomy as training data for LLM?

## Architecture Onboarding

### Component map
Taxonomy Core (20 L1, 86 sub-categories) -> Context Layer (Task Stage, User Constraints) -> Intent Parser (maps input to taxonomy nodes) -> Response Generator (selects strategy based on Intent Type)

### Critical path
1. Input (Text/Speech) -> 2. Context Extraction (Location/Task Phase) -> 3. Taxonomy Classification (L1 -> L4) -> 4. Actionable Output

### Design tradeoffs
- Abstraction vs. Utility: Uses abstract labels (e.g., "Scheduled Event Completion Time" vs. "Flight Arrival") to aid generalization; tradeoff: models may struggle without few-shot examples
- Context Dependence: High accuracy requires user location/state; tradeoff: privacy invasion vs. intent accuracy

### Failure signatures
- Context Blindness: Treating "Where is the gate?" as factoid query when user is rushing (needs navigation/warnings)
- Declarative Blindness: Ignoring "I missed my flight" because lacks question mark, failing to trigger "Problem-Handling Approaches"
- Over-generalization: Merging "Best Practices" and "Recommendations" into one class, forcing subjective advice where objective data required

### First 3 experiments
1. Validation Check: Test classifier on "Ill-formed Query" set (Section 4.8) to verify maps declarative statements to "How-to Instructions" rather than rejecting them
2. Context Ablation: Measure classification accuracy for "Where is..." queries with and without location context to quantify "Context-Driven" mechanism
3. Granularity Test: Evaluate if system using L1 categories performs worse on task-completion rates than one using L2-L4 for complex workflows

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the taxonomy be systematically extended to distinct domains, such as cooking or public welfare, without fundamental structural changes?
- Basis: Section 5.3 states future work should extend scope to additional domains by categorizing and integrating request intents
- Why unresolved: Only validated through "thought experiment" mapping, not empirical data collection in other domains
- What evidence would resolve: Empirical studies applying taxonomy to datasets from other fields to assess category coverage and inter-rater agreement

### Open Question 2
- Question: Does incorporating this taxonomy into AI-driven search systems measurably improve task-support capabilities compared to standard query-focused classifications?
- Basis: Section 5.4 suggests taxonomy "may... inform the design of AI-driven conversational search systems" and "enhance their task-supportive capabilities"
- Why unresolved: Paper focuses on taxonomy derivation and validation, not implementation or performance within algorithmic system
- What evidence would resolve: User study or simulation measuring task completion success and user satisfaction in systems using task-based taxonomy versus traditional intent models

### Open Question 3
- Question: To what extent does the taxonomy cover intent categories found in independent, unassisted online search behaviors?
- Basis: Section 5.5 notes limitation that taxonomy "only contains request intents that were expressed in interactions with information desk workers"
- Why unresolved: Expert interviews capture mediated information needs, whereas unassisted search logs may reveal different intents
- What evidence would resolve: Comparative analysis mapping taxonomy against large-scale query logs to identify gaps or intents unique to independent search

## Limitations
- Generalizability beyond airport information desks remains uncertain due to potential domain-specific bias
- Sample size of 720 statements from 8 expert interviews may not capture full diversity of task-based information needs across domains
- Practical utility depends on accurate context detection, which was not validated in this study

## Confidence

### Major Claims and Confidence Labels
- **High Confidence:** Taxonomy structure and inter-rater reliability (κ=0.84) are well-supported by methodology and validation process
- **Medium Confidence:** Task context significantly improves intent classification is supported by expert analysis but requires empirical testing
- **Medium Confidence:** Ill-formed queries require special handling is grounded in data but needs validation with actual user queries

## Next Checks

1. **Cross-Domain Transfer Test:** Apply taxonomy to information requests from different high-complexity environment (e.g., hospital information desks) and measure classification accuracy with and without task context

2. **User Query Validation:** Collect actual user queries from airport information channels and evaluate whether taxonomy correctly classifies declarative statements as "How-to" requests versus rejecting them as malformed

3. **Context Ablation Study:** Implement classifier using taxonomy and measure classification accuracy for ambiguous queries ("Where is...?") with and without user location/task phase information to quantify context mechanism's contribution