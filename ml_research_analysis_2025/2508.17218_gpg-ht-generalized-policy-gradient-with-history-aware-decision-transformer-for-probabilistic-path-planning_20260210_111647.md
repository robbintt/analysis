---
ver: rpa2
title: 'GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer
  for Probabilistic Path Planning'
arxiv_id: '2508.17218'
source_url: https://arxiv.org/abs/2508.17218
tags:
- transformer
- path
- time
- decision
- travel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a path planning solution for stochastic transportation
  networks, where vehicle travel times are uncertain and correlated. To address the
  limitations of existing approaches that rely on Markovian assumptions and ignore
  historical routing information, the authors introduce a history-aware decision Transformer
  integrated with a generalized policy gradient (GPG) framework.
---

# GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning

## Quick Facts
- arXiv ID: 2508.17218
- Source URL: https://arxiv.org/abs/2508.17218
- Reference count: 22
- Primary result: History-aware Transformer with GPG improves on-time arrival probability in stochastic networks

## Executive Summary
This work proposes a path planning solution for stochastic transportation networks with correlated, uncertain travel times. The authors introduce a history-aware Decision Transformer integrated with a generalized policy gradient (GPG) framework to address limitations of Markovian assumptions in existing routing approaches. By explicitly incorporating past trajectory information and leveraging Transformer attention mechanisms, the method models long-term temporal dependencies in routing decisions. Experiments on Sioux Falls and Anaheim networks demonstrate consistent improvements in on-time arrival probability compared to baseline models, with mean SOTA probabilities reaching up to 0.68 in SFN and 0.80 in AN under looser time budgets.

## Method Summary
The method uses a Transformer-based encoder-decoder architecture that takes six inputs: traversed edges, current node, destination, travel times, remaining budget, and positional encoding. The encoder employs cross-attention to fuse edge embeddings with travel-time embeddings, then stacks Transformer blocks to capture long-range temporal dependencies. The decoder uses cross-attention to couple historical network state with current decision constraints (remaining budget, destination) to produce action distributions. Training uses a Generalized Policy Gradient formulation that optimizes on-time arrival probability through Monte Carlo sampling, avoiding explicit distribution models. The approach handles correlated travel times by incorporating full trajectory history rather than relying on Markovian assumptions.

## Key Results
- Mean SOTA probability reaches 0.68 in Sioux Falls Network and 0.80 in Anaheim Network under looser time budgets
- Consistent improvements across all budget levels compared to baseline models (FMA, DP, DOT)
- Ablation study confirms effectiveness of both Transformer architecture and GPG optimization
- Performance degrades significantly when history module is removed, validating history-awareness

## Why This Works (Mechanism)

### Mechanism 1: History-Aware Attention for Non-Markovian Dependencies
Incorporating historical trajectory information via Transformer attention improves routing decisions in stochastic networks where future travel-time distributions depend on past delays. The encoder uses cross-attention to fuse edge embeddings with travel-time embeddings, then stacks Transformer blocks to capture long-range temporal dependencies. This allows the policy to condition decisions on the entire trajectory history rather than just the current state.

### Mechanism 2: Monte Carlo Gradient Estimation for Probability Maximization
The Generalized Policy Gradient formulation enables direct optimization of on-time arrival probability through Monte Carlo sampling, avoiding the need for explicit distribution models. Theorem 1 derives that the gradient can be approximated by sampling trajectories and weighting each by the indicator function 1{G(ξj)≤T}. This binary weighting naturally emphasizes successful trajectories.

### Mechanism 3: Cross-Attention State-Decision Coupling
The decoder's cross-attention mechanism effectively couples historical network state with current decision constraints (remaining budget, destination). The decoder attends over encoder output using query representations that combine current node, destination, remaining budget, and positional encoding. This produces context-aware action distributions.

## Foundational Learning

- Concept: **Policy Gradient Theorem**
  - Why needed here: The entire GPG framework builds on policy gradient theory. Understanding ∇θ log πθ(a|s) as the core gradient estimator is essential before tackling the non-Markovian extension.
  - Quick check question: Can you explain why policy gradient methods optimize expected return by taking gradients of log-probabilities weighted by rewards?

- Concept: **Transformer Self-Attention and Cross-Attention**
  - Why needed here: The encoder uses self-attention over trajectory history; the decoder uses cross-attention to query the encoder. You need to distinguish softmax(QK^T/√d)V for self-attention from the encoder-decoder variant.
  - Quick check question: In Eq. 1, identify which term acts as query, key, and value in the cross-attention fusion of edges and travel times.

- Concept: **Stochastic On-Time Arrival (SOTA) Problem**
  - Why needed here: The objective function J(πθ) = P[G(ξj)≤T] differs fundamentally from expected-cost minimization. This probability maximization is what necessitates the indicator-weighted gradient.
  - Quick check question: Why does the Bellman equation (used in DP baselines) struggle with correlated link travel times?

## Architecture Onboarding

- Component map:
  Inputs (6 types) -> Embedding layer -> Encoder (4 Transformer blocks with cross-attention) -> S_enc -> Decoder (4 Transformer blocks with cross-attention) -> D_dec -> Output head (FFN + softmax) -> Action probability

- Critical path:
  1. Trajectory history encoding (encoder quality determines temporal dependency capture)
  2. Cross-attention state-decision coupling (decoder determines action relevance)
  3. Monte Carlo gradient estimation (sample efficiency determines convergence)

- Design tradeoffs:
  - **Embedding dimension (256) vs. compute**: Larger dimensions capture richer interactions but slow inference; authors report 0.5s per decision at 256d
  - **Transformer layers (4) vs. gradient depth**: Deeper encoders capture longer dependencies but may overfit on limited training trajectories (50K SFN, 100K AN)
  - **Monte Carlo samples M vs. variance**: More samples reduce gradient variance but increase training time; paper doesn't specify M

- Failure signatures:
  - **Budget too tight (T=0.95 on complex OD pairs)**: SOTA probability drops to ~0.29-0.42; gradient variance spikes as most trajectories fail
  - **Missing history module (ablation)**: Performance degrades across all budgets, confirming history-awareness is not optional
  - **Vanilla PG instead of GPG**: Higher variance gradients cause unstable training (ablation confirms performance drop)

- First 3 experiments:
  1. **Sanity check on single OD pair**: Train on OD 2-15 (SFN) with T=1.00. Verify SOTA probability exceeds 0.49 (baseline FMA) within 10K iterations. If not, check embedding dimensions and learning rate.
  2. **Ablation of history module**: Remove cross-attention embeddings (set X'i = Xi without Ri fusion). Confirm performance drops below full model by comparing Table III rows. This validates the encoder's temporal fusion role.
  3. **Scale test to Anaheim**: Train on AN with identical hyperparameters. If SOTA probability at T=1.05 is below 0.77 (baseline DOT), investigate whether 4 layers are insufficient for 416-node topology.

## Open Questions the Paper Calls Out
- The paper states future work will focus on scaling the framework to more complex and larger-scale network topologies, suggesting uncertainty about performance on city-scale road networks with thousands of nodes.

## Limitations
- Performance on city-scale networks with thousands of nodes remains untested, raising questions about scalability of the Transformer architecture
- The Gaussian travel time assumption may not generalize to real-world traffic with heavy-tailed delays or multimodal distributions
- Ablation study only tests history and GPG modules separately rather than their interaction, leaving uncertainty about which component drives improvements

## Confidence

- Transformer attention for history encoding: Medium
- GPG optimization improving stability: Medium
- Overall SOTA probability improvements: High

## Next Checks
1. Run ablation combining both history removal AND GPG removal to test interaction effects
2. Implement a recurrent alternative (e.g., LSTM policy) and compare history encoding capabilities
3. Test performance degradation as correlation strength approaches zero to validate temporal dependency claims