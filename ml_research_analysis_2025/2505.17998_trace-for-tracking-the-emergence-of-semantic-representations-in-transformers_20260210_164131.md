---
ver: rpa2
title: TRACE for Tracking the Emergence of Semantic Representations in Transformers
arxiv_id: '2505.17998'
source_url: https://arxiv.org/abs/2505.17998
tags:
- training
- abstraction
- curvature
- semantic
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRACE, a diagnostic framework that tracks
  how transformers transition from memorisation to abstraction during training. It
  combines geometric (Hessian curvature and intrinsic dimensionality), linguistic
  (probe-based category alignment), and information-theoretic signals to detect phase
  transitions.
---

# TRACE for Tracking the Emergence of Semantic Representations in Transformers

## Quick Facts
- arXiv ID: 2505.17998
- Source URL: https://arxiv.org/abs/2505.17998
- Reference count: 40
- Primary result: Framework detects consistent phase transitions from memorization to abstraction in transformers through coordinated geometric, linguistic, and information-theoretic signals

## Executive Summary
TRACE introduces a diagnostic framework that tracks how transformers transition from memorization to abstraction during training by combining geometric (Hessian curvature and intrinsic dimensionality), linguistic (probe-based category alignment), and information-theoretic signals. The authors develop ABSynth, a synthetic data generator grounded in frame semantics, to create controlled corpora with transparent syntactic and semantic structure. Experiments show that abstraction emerges through a consistent phase transition marked by coordinated shifts in curvature flattening, intrinsic dimensionality stabilization, and improved probe accuracy. These patterns persist across model scales and architectural variants, revealing predictable, scalable patterns in abstraction emergence that offer a principled approach to understanding representation learning in transformers.

## Method Summary
TRACE combines three parallel monitoring streams: spectral curvature computed via Lanczos approximation on loss Hessian, intrinsic dimensionality estimated using TWO-NN on hidden states across all layers, and linguistic probing via linear classifiers trained on frozen checkpoints. The framework uses ABSynth, a synthetic corpus generated from frame-semantic structures with ground-truth annotations for semantic roles and POS tags. Transformers are trained with dense checkpointing every 500 steps, and the phase transition is detected as the intersection point between curvature collapse and ID stabilization, validated by probe accuracy improvements. The method includes architectural ablations and probes trained on randomly initialized checkpoints to verify that linguistic alignment emerges during training rather than from architectural priors.

## Key Results
- Phase transitions from memorization to abstraction occur at consistent intersection points between curvature collapse and intrinsic dimensionality stabilization
- Larger models transition earlier (step 5000) and more sharply than smaller models (step 30000+), with FFNs contributing to optimization stability
- Linguistic probes show evolving category alignment coinciding with geometric transitions, while output accuracy stabilizes earlier, indicating a two-phase development process
- Mutual information analysis proved too noisy to serve as a reliable signal for detecting abstraction emergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase transitions from memorization to abstraction can be detected through coordinated geometric signals
- Mechanism: The framework tracks when Hessian curvature collapse (C(H) = Tr(H)/√r_eff) intersects with intrinsic dimensionality stabilization—this intersection point marks when models reorganize representations from surface patterns to abstract structures
- Core assumption: Abstraction emergence produces consistent geometric signatures in the loss landscape that correlate with linguistic capability gains
- Evidence anchors:
  - [abstract] "phase transitions align with clear intersections between curvature collapse and dimension stabilisation"
  - [section 4.1] "this intersection is marked by a consistent intersection between the Hessian curvature score (blue) and ID trajectories (red), which we interpret as a phase shift"
  - [corpus] Weak external validation; related work on phase transitions exists (Attention to Order, Crosscoding Through Time) but lacks direct replication of TRACE's multi-signal approach
- Break condition: If curvature-ID intersection occurs without corresponding probe accuracy improvements, the geometric signal may indicate optimization artifacts rather than genuine abstraction

### Mechanism 2
- Claim: Frame-semantic synthetic data enables precise tracking of when linguistic categories emerge in representations
- Mechanism: ABSynth generates sentences from abstract event frames with predefined semantic roles (AGENT, PATIENT, THEME), creating ground-truth annotations that allow direct measurement of when internal representations align with linguistic abstractions versus surface token patterns
- Core assumption: Abstraction patterns observed in controlled synthetic settings transfer to natural language learning dynamics
- Evidence anchors:
  - [abstract] "ABSynth, a synthetic data generator grounded in frame semantics, to produce controlled corpora with transparent syntactic and semantic structure"
  - [section 3.5] "Each example includes ground-truth semantic roles and syntactic categories (POS tags) derived directly from the underlying frame structure"
  - [corpus] No direct corpus evidence for frame-semantic grounding specifically; synthetic benchmarks (SCAN, PCFG datasets) focus on algorithmic tasks rather than semantic structure
- Break condition: If models achieve high probe accuracy through statistical shortcuts rather than genuine role-based generalization (e.g., memorizing token co-occurrences), annotations may not reflect abstraction

### Mechanism 3
- Claim: Architectural components affect optimization stability and timing but not the fundamental abstraction trajectory
- Mechanism: FFNs act as distributed lookup structures that smooth representational development, while attention capacity influences phase transition timing in smaller models—yet the core curvature-ID intersection pattern persists across ablations
- Core assumption: The abstraction emergence pattern is an inherent property of gradient-based optimization on sequential data rather than architecture-dependent
- Evidence anchors:
  - [abstract] "components like feedforward networks affecting optimisation stability rather than fundamentally altering trajectories"
  - [section 4.2] "All variants preserve the fundamental pattern of an initial curvature peak followed by a decline, concurrent with rising ID that eventually stabilises"
  - [corpus] Weak; limited external validation of architectural resilience claims across transformer variants
- Break condition: If removing architectural components prevents phase transitions entirely or fundamentally changes the curvature-ID relationship, the claimed universality would not hold

## Foundational Learning

- **Hessian-Based Curvature Analysis**
  - Why needed here: The framework's primary geometric signal relies on computing spectral properties of the loss landscape (trace, effective rank) to detect representational reorganization
  - Quick check question: Can you explain why flat minima (low curvature) are associated with better generalization in neural networks?

- **Intrinsic Dimensionality Estimation (TWO-NN)**
  - Why needed here: The framework uses ID to track when representations compress from high-dimensional memorization to low-dimensional abstraction manifolds
  - Quick check question: Given a batch of activation vectors, how would you compute the intrinsic dimension using nearest-neighbor distance ratios?

- **Representation Probing Methodology**
  - Why needed here: TRACE relies on linear probes trained on frozen checkpoints to measure when linguistic categories become decodable from internal representations
  - Quick check question: Why might probe accuracy increase even if the model hasn't learned genuine linguistic abstractions?

## Architecture Onboarding

- **Component map:** ABSynth generator -> Transformer training with dense checkpointing -> Three parallel monitoring streams (Lanczos Hessian curvature, TWO-NN intrinsic dimensionality, frozen checkpoint probing) -> Phase transition detection at curvature-ID intersection

- **Critical path:** The phase transition detection depends on synchronized checkpointing—all three signal streams must be extracted at identical training steps to identify coordinated shifts. The curvature-ID intersection point is the primary trigger signal; probe accuracy serves as validation.

- **Design tradeoffs:** Mutual information estimation was tested but abandoned due to high variance (Appendix C). Synthetic data (ABSynth) provides annotation precision but limits ecological validity compared to natural language. Model scale affects transition timing and stability—larger models transition earlier and more sharply.

- **Failure signatures:** If curvature spikes without ID stabilization, suspect optimization instability rather than abstraction. If probe accuracy plateaus below expected levels, check annotation quality or probe capacity. If phase transitions appear at inconsistent steps across random seeds, the signal may be too noisy for reliable detection.

- **First 3 experiments:**
  1. Replicate the curvature-ID intersection detection on a small transformer (1-layer, 64-dim) trained on ABSynth-25K, logging all three signal streams every 500 steps
  2. Train probes on randomly initialized model checkpoints to verify that linguistic alignment emerges during training rather than from architectural priors (per Appendix B.9)
  3. Run the FFN ablation (remove feedforward blocks) and compare transition timing and stability against the baseline to validate the optimization stability claim

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Do the geometric phase transitions identified via TRACE persist in large-scale pretraining on noisy, ambiguous natural language data?
  - Basis in paper: [explicit] The Discussion section states, "Extending TRACE to large-scale pretraining could reveal whether similar phase transitions emerge in noisier, real-world settings."
  - Why unresolved: The current experiments use ABSynth, a synthetic corpus which, according to the Limitations section, "does not fully capture the ambiguity and richness of NL."
  - What evidence would resolve it: Applying curvature and intrinsic dimensionality diagnostics to standard LLM pretraining runs (e.g., on The Pile or C4) and observing consistent intersection patterns.

- **Open Question 2**
  - Question: Does the flattening of the loss landscape curvature causally drive the emergence of linguistic abstraction, or is it merely a correlated epiphenomenon?
  - Basis in paper: [explicit] The Limitations section notes the framework "does not establish causal relationships or quantify the relative contribution of each factor."
  - Why unresolved: TRACE observes coordinated shifts (correlation), but the paper acknowledges it does not determine if geometric reorganisation forces abstraction or vice versa.
  - What evidence would resolve it: Interventionist experiments where curvature is artificially constrained or manipulated to observe the specific effect on probe accuracy and generalisation.

- **Open Question 3**
  - Question: How do the layer-wise geometric transitions detected by TRACE align with the formation of specific mechanistic circuits, such as induction heads?
  - Basis in paper: [explicit] The Discussion suggests that "Integrating TRACE with mechanistic interpretability tools could help localise where and how abstraction-related circuits emerge."
  - Why unresolved: The paper notes probes provide a static view and "may not reflect the dynamic computational mechanisms" used at inference, leaving the link between geometric shifts and circuits unexplored.
  - What evidence would resolve it: Temporal mapping of curvature/ID shifts alongside causal tracing or logit attribution to verify if geometric phases coincide with circuit formation.

## Limitations

- The synthetic ABSynth corpus may not fully capture the complexity and noise of natural language, limiting ecological validity
- Mutual information-based measures proved too noisy to be reliable for detecting abstraction emergence
- The framework observes correlation between geometric signals and abstraction but does not establish causal relationships
- Architectural claims about FFN and attention components affecting optimization stability lack extensive external validation across transformer variants

## Confidence

- **High Confidence**: The core geometric signal (curvature-ID intersection) and its correlation with probe accuracy improvements is well-supported by controlled experiments with consistent patterns across model scales
- **Medium Confidence**: The claim that architectural components affect optimization stability but preserve fundamental abstraction trajectories is plausible but needs broader validation
- **Medium Confidence**: The ABSynth synthetic data successfully enables controlled experiments with clean annotations, though ecological validity for natural language remains uncertain
- **Low Confidence**: Claims about the universality of phase transition patterns across all transformer architectures and natural language settings require more extensive empirical validation

## Next Checks

1. **Cross-domain validation**: Apply TRACE to transformer models trained on natural language corpora (e.g., Wikitext, OpenWebText) to verify whether the curvature-ID intersection pattern persists without synthetic frame-semantic structure

2. **Alternative abstraction measures**: Implement complementary abstraction detection methods (e.g., probing for compositional generalization, syntactic depth, or semantic similarity tasks) to test whether TRACE's geometric signals predict performance on independent abstraction benchmarks

3. **Architectural robustness**: Test TRACE's signal consistency across diverse transformer variants (GPT-2, BERT, OPT) and training regimes (masked LM, autoregressive, prefix LM) to validate the claimed architecture-agnostic nature of phase transition detection