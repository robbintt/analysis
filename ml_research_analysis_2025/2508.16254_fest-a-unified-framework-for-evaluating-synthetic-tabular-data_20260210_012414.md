---
ver: rpa2
title: 'FEST: A Unified Framework for Evaluating Synthetic Tabular Data'
arxiv_id: '2508.16254'
source_url: https://arxiv.org/abs/2508.16254
tags:
- data
- synthetic
- dataset
- privacy
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEST, a unified framework for evaluating
  synthetic tabular data that integrates diverse privacy metrics (attack-based and
  distance-based) with statistical similarity and machine learning utility assessments.
  The framework addresses the gap in comprehensive evaluation of synthetic data generation
  by balancing privacy preservation with data utility.
---

# FEST: A Unified Framework for Evaluating Synthetic Tabular Data

## Quick Facts
- arXiv ID: 2508.16254
- Source URL: https://arxiv.org/abs/2508.16254
- Reference count: 1
- Key outcome: FEST provides comprehensive evaluation of synthetic tabular data across privacy, statistical similarity, and machine learning utility metrics

## Executive Summary
This paper introduces FEST, a unified framework for evaluating synthetic tabular data that integrates diverse privacy metrics (attack-based and distance-based) with statistical similarity and machine learning utility assessments. The framework addresses the gap in comprehensive evaluation of synthetic data generation by balancing privacy preservation with data utility. FEST was implemented as an open-source Python library and validated on three real-world datasets (Diabetes, Cardio, and Insurance) using six synthetic data generation models including CTGAN, Gaussian Mixture Models, Gaussian Copula, CopulaGAN, TVAE, and a Random baseline. The evaluation demonstrated FEST's effectiveness in analyzing privacy-utility trade-offs, revealing that most models showed strong privacy preservation with low DiSCO and repU values, while maintaining high statistical similarity and ML utility scores.

## Method Summary
FEST implements a comprehensive evaluation pipeline that assesses synthetic tabular data across three dimensions: privacy preservation, statistical similarity, and machine learning utility. The framework computes attack-based privacy metrics (Singling Out, Linkability, Attribute Inference) using the Anonymeter framework, distance-based privacy metrics (DiSCO, repU, NNDR, DCR, NNAA), statistical similarity metrics (Wasserstein, KS-test, correlation coefficients, mutual information, Jensen-Shannon), and ML utility through train-on-synthetic/test-on-real experiments. The framework allows configurable quasi-identifier selection and attack parameters, making it adaptable to different domain requirements. Implementation was validated on three datasets with six synthetic generation models, demonstrating the framework's ability to reveal privacy-utility trade-offs across different generation paradigms.

## Key Results
- Most synthetic data generation models (CTGAN, Gaussian Copula, CopulaGAN, TVAE) demonstrated strong privacy preservation with low DiSCO and repU values below 0.2 across all three datasets
- Statistical similarity metrics showed high correlation coefficients (0.90-0.99) and low Wasserstein distances, indicating synthetic data closely matched original distributions
- ML utility assessments revealed that models like CTGAN and TVAE maintained high classification accuracy when trained on synthetic data and tested on real data, comparable to models trained on real data
- The Random baseline served as an effective upper bound for privacy risk (DiSCO=100, repU=100) and lower bound for utility, validating the framework's sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Privacy-Utility Trade-off Quantification
The framework computes Singling Out, Linkability, and Attribute Inference risks (attack-based) together with DiSCO, repU, NNDR, DCR, and NNAA (distance-based), then combines these with statistical similarity and ML utility scores to reveal trade-offs between privacy preservation and data usefulness. No single metric adequately captures privacy risk; comprehensive evaluation requires both adversarial and distributional perspectives.

### Mechanism 2: Standardized Cross-Model Comparison Pipeline
FEST applies identical metric computations to outputs from diverse generators (CTGAN, Gaussian Mixture, Gaussian Copula, CopulaGAN, TVAE, Random), enabling direct comparison of privacy-utility profiles. Different generation paradigms (GAN-based, statistical, autoencoder) produce distinct privacy-utility characteristics that standardized metrics can reveal.

### Mechanism 3: Configurable Quasi-Identifier and Attack Parameterization
Users specify which attributes constitute quasi-identifiers and targets for DiSCO/repU calculations, and configure attack parameters (sample sizes, neighbor counts) for singling out, linkability, and inference assessments. Privacy risk is context-dependent; appropriate quasi-identifiers vary by dataset and threat model.

## Foundational Learning

- **Concept: Tabular Generative Models (GANs, Copulas, VAEs)**
  - Why needed here: FEST evaluates outputs from fundamentally different generation paradigms; understanding their mechanisms helps interpret why certain models exhibit specific privacy-utility profiles
  - Quick check question: Why might a Gaussian Copula preserve correlation structure differently than CTGAN, and how would this affect NNDR values?

- **Concept: GDPR-Aligned Privacy Attacks (Singling Out, Linkability, Inference)**
  - Why needed here: FEST implements attack-based metrics derived from GDPR guidelines; understanding these attack types is essential for interpreting risk scores and confidence intervals
  - Quick check question: If linkability risk is low but inference risk is high, what does this indicate about the synthetic data's privacy posture?

- **Concept: Distributional Distance Metrics (Wasserstein, KS, Jensen-Shannon)**
  - Why needed here: Statistical similarity metrics quantify how closely synthetic data matches original data distributions; understanding their properties aids in selecting appropriate metrics for specific data types
  - Quick check question: When would Wasserstein distance be preferred over KS test for comparing synthetic and original tabular data?

## Architecture Onboarding

- **Component map:**
  Privacy Assessment Module -> Statistical Similarity Module -> ML Utility Module -> Visualization Module
  Configuration Layer provides input to all modules

- **Critical path:**
  1. Load original and synthetic datasets (pandas DataFrames)
  2. Define quasi-identifiers and target attributes based on domain knowledge
  3. Execute privacy assessment (attack-based metrics require control dataset per Anonymeter methodology)
  4. Execute statistical similarity metrics (use Sinkhorn for large datasets >10K rows)
  5. Execute ML utility assessment (train classifier on synthetic, evaluate on held-out real data)
  6. Generate comparison visualizations and aggregate scores

- **Design tradeoffs:**
  - Comprehensiveness vs. runtime: Full metric suite on large datasets (Cardio: 70K records) requires sampling (Section 5.3.1 used 1,000 samples) and Sinkhorn approximation
  - Attack sensitivity vs. computational cost: Higher attack counts and neighbor values improve confidence intervals but increase runtime
  - Metric interpretability vs. precision: Aggregate scores (means across columns) simplify comparison but may obscure column-specific issues

- **Failure signatures:**
  - DiSCO/repU consistently returning 0.00 across all models: Indicates inappropriate quasi-identifier selection for dataset structure
  - Wide confidence intervals in attack metrics (e.g., Inference risk CI spanning 0.0 to 0.68 in Table 2): Suggests insufficient sample size or attack counts
  - NNAA near 0.0: Indicates synthetic data nearly identical to original (overfitting privacy risk)
  - NNAA near 1.0: Indicates synthetic data easily distinguishable (low utility)
  - Wasserstein computation timeout on large datasets: Requires switching to Sinkhorn approximation

- **First 3 experiments:**
  1. Baseline calibration: Run FEST on Random sampling model to establish upper bounds for privacy risk and perfect utility scores; verify metrics return expected values (DiSCO=100, repU=100, all similarity=1.0)
  2. Single-dataset multi-model comparison: Generate synthetic data from all six models for Diabetes dataset; compare privacy-utility profiles to identify which generation paradigm best balances trade-offs for small tabular data
  3. Quasi-identifier sensitivity analysis: For Cardio dataset, systematically vary quasi-identifier sets (minimal vs. comprehensive) and observe impact on DiSCO/repU values to understand metric sensitivity to configuration choices

## Open Questions the Paper Calls Out
None

## Limitations
- Framework effectiveness depends critically on appropriate quasi-identifier selection, which may be challenging for small datasets or those with limited attribute combinations
- Evaluation covers three real-world datasets but may not generalize to all tabular data domains
- Some attack-based metrics showed high variance (wide confidence intervals), and the framework's performance on very small or highly sparse datasets remains unvalidated

## Confidence
- **High Confidence**: The framework's architectural design, implementation details, and basic metric computations are well-established
- **Medium Confidence**: Cross-model comparisons show consistent patterns, but results are dataset-dependent
- **Low Confidence**: Some attack-based metrics showed high variance (wide confidence intervals), and the framework's performance on very small or highly sparse datasets remains unvalidated

## Next Checks
1. Parameter Sensitivity Analysis: Systematically vary quasi-identifier selections and attack parameters (sample sizes, neighbor counts) across all three datasets to quantify metric stability and identify breaking points
2. Domain Generalization Test: Apply FEST to datasets from new domains (e.g., financial transactions, healthcare imaging metadata) to validate framework robustness beyond the original three datasets
3. Emerging Model Evaluation: Test FEST on synthetic data generated by diffusion-based methods to assess whether additional privacy metrics are needed for these newer generation paradigms