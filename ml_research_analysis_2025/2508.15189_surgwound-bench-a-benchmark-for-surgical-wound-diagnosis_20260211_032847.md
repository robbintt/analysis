---
ver: rpa2
title: 'SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis'
arxiv_id: '2508.15189'
source_url: https://arxiv.org/abs/2508.15189
tags:
- wound
- surgical
- risk
- infection
- woundqwen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurgWound-Bench introduces the first open-source dataset and benchmark
  for surgical wound diagnosis, addressing data scarcity and enabling reproducible
  AI research in this clinical domain. SurgWound comprises 697 images annotated by
  three surgeons with eight fine-grained clinical attributes.
---

# SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis

## Quick Facts
- **arXiv ID:** 2508.15189
- **Source URL:** https://arxiv.org/abs/2508.15189
- **Reference count:** 40
- **Primary result:** Introduces the first open-source dataset and benchmark for surgical wound diagnosis with a three-stage AI framework achieving 83.21% infection risk classification accuracy and 0.917 BERTScore in report generation.

## Executive Summary
SurgWound-Bench addresses the critical challenge of surgical wound diagnosis by introducing the first open-source dataset and benchmark for this clinical domain. The SurgWound dataset contains 697 images annotated by three surgeons with eight fine-grained clinical attributes. The proposed three-stage WoundQwen framework leverages specialized Multimodal Large Language Models (MLLMs) to predict wound characteristics, diagnose infection risk and urgency, and generate clinical reports. Experiments demonstrate that WoundQwen outperforms state-of-the-art models, achieving 83.21% accuracy in infection risk classification and 0.917 BERTScore in report generation, advancing AI-assisted surgical wound diagnosis for personalized wound care and timely interventions.

## Method Summary
The framework employs a three-stage cascaded architecture based on Qwen2.5-VL-7B initialized with HuatuoGPT-Vision-7B weights. Stage 1 uses five independent LoRA fine-tuned MLLMs to predict specific wound characteristics (healing status, closure method, exudate type, erythema, edema). Stage 2 employs two MLLMs that take the image plus Stage 1 predictions as context to diagnose infection risk and urgency level. Stage 3 uses a MLLM that integrates all prior predictions to generate comprehensive clinical reports. The system is trained using LoRA with standard hyperparameters (LR 1e-4, cosine scheduler, 10% warmup, batch size 8) on the 480-image training split.

## Key Results
- WoundQwen achieves 83.21% accuracy in infection risk classification, outperforming state-of-the-art models
- Report generation achieves 0.917 BERTScore, demonstrating high semantic alignment with clinical narratives
- Stage 1 attribute prediction models show strong performance (Exudate Type F1 0.884) despite severe class imbalance
- The three-stage cascade architecture enables detailed intermediate predictions that improve final diagnostic accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing structured intermediate predictions as explicit text context to downstream diagnostic models improves infection risk accuracy compared to direct inference from raw images.
- **Mechanism:** The framework decouples visual perception from clinical reasoning by feeding outputs of specialized models as text prompts into diagnostic models, allowing them to focus on symptom-diagnosis relationships rather than raw pixel interpretation.
- **Core assumption:** Stage 1 model accuracy is sufficient that their textual outputs serve as reliable cues for downstream reasoning.
- **Evidence anchors:** Abstract states predictions serve as additional knowledge inputs; Section 4.2 explains this enables final classifiers to leverage detailed intermediate feature predictions.
- **Break condition:** If Stage 1 error rates are significant (e.g., misclassifying "Healed" as "Not Healed"), noise propagates to Stage 2, degrading performance below baseline models.

### Mechanism 2
- **Claim:** Training independent, specialized MLLMs for specific wound attributes effectively handles class imbalance and visual complexity better than a single multi-label classifier.
- **Mechanism:** By splitting the task into five separate models, each optimizes for its specific distribution, preventing dominant classes in one attribute from overwhelming learning of rare classes in another.
- **Core assumption:** Visual features for identifying one attribute are sufficiently distinct and do not strictly require simultaneous inference of another attribute.
- **Evidence anchors:** Abstract mentions five independent MLLMs; Table 1 shows severe class imbalance (Home Care 87.5% vs Emergency Care 2.6%); Table 2 shows high F1 scores for specific attributes.
- **Break condition:** If strong visual correlations exist between attributes that a single shared encoder would capture better, independent models might fail to learn these cross-features.

### Mechanism 3
- **Claim:** Grounding report generation in structured predictions of previous stages reduces hallucination and increases clinical semantic alignment.
- **Mechanism:** By forcing the Stage 3 model to take explicit structured data as input context, the generation task shifts from open-ended description to summarization and expansion of known facts.
- **Core assumption:** The structured 8-attribute schema captures the majority of clinically relevant information.
- **Evidence anchors:** Abstract states the report model integrates diagnostic results from previous stages; Section 4.3 notes this enriches understanding beyond raw images.
- **Break condition:** If structured inputs contain errors, the report will confidently hallucinate a narrative consistent with those errors.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The authors fine-tune 7B-parameter models on a relatively small dataset (480 training images). Full fine-tuning would be computationally expensive and prone to overfitting. LoRA allows efficient adaptation.
  - **Quick check question:** In Stage 1 implementation, which specific modules (attention vs. MLP) are targeted by LoRA adapters, and which parts of the model are frozen?

- **Concept: Cascaded/Curriculum Learning**
  - **Why needed here:** The system architecture relies on a pipeline where easy tasks (characteristics) inform hard tasks (diagnosis). Understanding how information flows and potentially degrades across stages is critical.
  - **Quick check question:** If Stage 1 model predicts "Uncertain" for a feature, how should the Stage 2 model theoretically handle this input to avoid discarding the sample?

- **Concept: Multimodal Context Injection**
  - **Why needed here:** Unlike standard VQA where the model answers based on the image alone, Stage 2 and 3 models receive composite input: [Image] + [Text Prompt] + [Structured Predictions].
  - **Quick check question:** How does the formatting of Stage 1 predictions (e.g., natural language sentence vs. JSON dictionary) likely impact the Stage 2 LLM's ability to reason over them?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B (initialized with HuatuoGPT-Vision-7B) -> 5 parallel Stage 1 models (Healing, Closure, Exudate, Erythema, Edema) -> 2 Stage 2 models (Risk, Urgency) -> 1 Stage 3 model (Report)

- **Critical path:** 1. Image passed to 5 Stage-1 models 2. Stage 1 text outputs + Location metadata concatenated 3. Combined string appended to prompt for Stage 2 models 4. Image passed again to Stage 2 alongside text prompt 5. Stage 3 takes all prior text context + image to generate final report

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Three-stage approach with 5+ parallel models offers higher accuracy but introduces significant inference latency compared to single-shot classifier
  - **Modularity vs. Error Propagation:** System is highly modular (easy to update "Exudate" model independently) but creates "siloed" perception where diagnostic model relies entirely on quality of perception models

- **Failure signatures:**
  - **"Uncertain" Loops:** If Stage 1 models output "Uncertain" frequently, Stage 2 receives degraded context, potentially defaulting to "Home Care" regardless of visual urgency
  - **Text/Image Mismatch:** If Stage 2 ignores image and relies solely on text context, it might miss visual nuances that weren't picked up by Stage 1

- **First 3 experiments:**
  1. **Stage 1 Isolation Test:** Run 5 Stage-1 models on test set individually. Verify "Exudate Type" and "Closure Method" models aren't just predicting majority class (check Precision/Recall, not just Accuracy)
  2. **Context Ablation (Stage 2):** Run WoundQwen_risk with Stage 1 inputs zeroed out or replaced with "Unknown" to quantify performance drop. Compare against "GT Cues" result in Table 5 to bound system's potential
  3. **Semantic Consistency Check:** Generate reports for 10 random test images and manually verify every statement in report matches Stage 1/2 predictions (ensuring Stage 3 is actually using context and not hallucinating)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the error propagation between stage-1 characteristic prediction models and stage-2 diagnostic outcome models be minimized?
- **Basis in paper:** [explicit] The ablation study results (Table 5) explicitly note that "a gap remains between the stage-1 predicted features and the ground-truth features," causing performance degradation when using predicted cues compared to ground-truth cues.
- **Why unresolved:** The current three-stage architecture uses sequential feed-forward approach where stage 2 depends on noisy stage 1 outputs without any feedback loop or uncertainty-aware mechanism.
- **What evidence would resolve it:** Implementing a joint training objective or confidence-weighted fusion mechanism that closes performance gap between using predicted cues versus ground-truth cues in Stage 2.

### Open Question 2
- **Question:** To what extent does reliance on social media images affect model's generalizability to standardized clinical wound imaging environments?
- **Basis in paper:** [inferred] Authors constructed dataset using images from Instagram and Reddit to navigate privacy issues, whereas real-world clinical deployment would likely involve standardized patient-submitted photos from telehealth portals.
- **Why unresolved:** Social media images may have selection biases (e.g., unique lighting, filters, or wound types shared for social engagement) that differ from visual characteristics of routine clinical data, creating potential domain shift.
- **What evidence would resolve it:** External validation of WoundQwen framework on private, clinically sourced dataset of surgical wound images to measure domain gap.

### Open Question 3
- **Question:** Do improvements in automated NLP metrics (BERTScore, ROUGE) for report generation correlate with clinical factuality and reduced hallucination rates?
- **Basis in paper:** [inferred] While authors demonstrate WoundQwen achieves superior BERTScore (0.917) and ROUGE scores, they acknowledge general MLLMs tend to hallucinate yet do not report human evaluation of clinical safety or factual consistency for generated reports.
- **Why unresolved:** Semantic similarity metrics do not capture medical errors; a report can achieve high BERTScore while generating clinically dangerous instructions or incorrect infection risk assessments.
- **What evidence would resolve it:** Human clinician evaluation of generated reports specifically scoring for "hallucination rate" and "clinical safety" to validate correlation with reported automated metrics.

## Limitations
- The performance of the entire pipeline is fundamentally constrained by accuracy of Stage 1 perception models, particularly for rare classes like purulent exudate (2.8% prevalence)
- The system does not address how to handle cases where Stage 1 models output "Uncertain" or low-confidence predictions, which could propagate errors through the cascade
- Evaluation focuses primarily on technical metrics without extensive clinical validation from medical professionals to verify practical utility and safety of generated reports

## Confidence
- **High Confidence:** Dataset construction methodology and basic architecture design are well-documented and reproducible
- **Medium Confidence:** Reported performance metrics are credible given experimental setup, though impact of error propagation through cascade is not fully characterized
- **Low Confidence:** Clinical impact and real-world safety remain uncertain without physician review of generated reports and validation in clinical settings

## Next Checks
1. **Error Propagation Analysis:** Run full pipeline with Stage 1 predictions systematically corrupted (e.g., random noise, systematic bias toward majority classes) to quantify how error rates in early stages affect final diagnosis accuracy
2. **Clinical Expert Review:** Have three independent surgeons review 50 randomly selected generated reports against ground truth annotations to assess semantic alignment and identify potential clinical risks or hallucinations
3. **Baseline Comparison:** Implement and evaluate a single end-to-end model (same backbone, full fine-tuning) on same dataset to determine if three-stage cascade actually outperforms direct inference, controlling for model capacity and training data