---
ver: rpa2
title: Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward
  Offline RL
arxiv_id: '2506.20904'
source_url: https://arxiv.org/abs/2506.20904
tags:
- bpsa
- span
- have
- ntot
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first sample complexity bound for average-reward
  offline reinforcement learning (RL) that depends only on the target policy's bias
  span, a single-policy complexity measure. Previous work required additional uniform-policy
  complexity measures like the uniform mixing time, leading to vacuous bounds.
---

# Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL

## Quick Facts
- arXiv ID: 2506.20904
- Source URL: https://arxiv.org/abs/2506.20904
- Authors: Matthew Zurek; Guy Zamir; Yudong Chen
- Reference count: 40
- One-line primary result: First optimal sample complexity bound for average-reward offline RL depending only on target policy's bias span, without requiring uniform policy coverage or prior parameter knowledge.

## Executive Summary
This paper develops the first sample complexity bound for average-reward offline reinforcement learning (RL) that depends only on the target policy's bias span, a single-policy complexity measure. Previous work required additional uniform-policy complexity measures like the uniform mixing time, leading to vacuous bounds. The authors introduce a pessimistic value iteration algorithm enhanced by a novel quantile clipping technique, which enables the use of a sharper empirical-span-based penalty function. The algorithm achieves an optimal rate of O(√(S·bias span/m)) where S is the number of states and m is the effective dataset size, without requiring prior knowledge of any parameters. Notably, the paper shows that learning under these conditions requires data beyond the stationary distribution of the target policy, distinguishing single-policy complexity measures from previously examined cases.

## Method Summary
The algorithm is a pessimistic discounted value iteration with quantile clipping. It sets the discount factor γ = 1 - 1/n_tot where n_tot is the total dataset size, and uses a novel penalty function based on the empirical variance and clipped value span. The key innovation is the quantile clipping operator that clips the top β-percentile of next-state values before computing the variance term in the penalty. This ensures the penalty scales with the bias span rather than exploding as γ → 1. The algorithm runs for K ≈ n_tot steps and outputs a greedy policy with respect to the final Q-values.

## Key Results
- Achieves optimal sample complexity O(√(S·bias span/m)) for average-reward offline RL
- First algorithm requiring only single-policy complexity measures (bias span) rather than uniform policy measures
- Introduces novel quantile clipping technique enabling span-based penalties without parameter knowledge
- Proves learning requires data beyond stationary distribution (transient coverage)
- Provides matching lower bounds confirming optimality of the result

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantile clipping enables a penalty function that scales with the *bias span* of the target policy rather than the potentially infinite *value magnitude* inherent in average-reward settings.
- **Mechanism:** Standard pessimistic value iteration uses Bernstein-style penalties that typically scale with the upper bound of the value function (≈ 1/(1-γ)). As the discount factor γ → 1 (approaching the average-reward limit), this penalty explodes, rendering learning vacuous. The authors introduce a quantile clipping operator T_β that clips the top β-percentile of next-state values before computing the variance term in the penalty. This ensures the penalty function satisfies a *constant shift property*, meaning it only penalizes the relative difference between states (span) rather than the absolute cumulative reward.
- **Core assumption:** The target policy has a finite bias span ||h^π^||_{span}.
- **Evidence anchors:**
  - [abstract] "introduce... a novel quantile clipping technique, which enables the use of a sharper empirical-span-based penalty function."
  - [section 3.1] "The ||·||_{span}-based second term in our penalty function... is essential for this constant-shift property."
- **Break condition:** If the target policy's bias span is extremely large (approaching the effective horizon), or if the clipping parameter β is set too aggressively relative to the dataset size, the estimate may become biased or the penalty insufficient to enforce pessimism.

### Mechanism 2
- **Claim:** The algorithm achieves near-optimal sample complexity by implicitly recovering from "off-support" errors through a transient coverage requirement, rather than assuming uniform policy coverage.
- **Mechanism:** The paper proves (Theorem 3.3) that learning in average-reward RL requires data not just from the stationary distribution of the target policy μ^π^star, but also from *transient* state-action pairs. These are states where μ^π^star(s) = 0 (the optimal policy doesn't visit them in steady state), but which are necessary to learn how to navigate back to the recurrent class if the agent deviates stochastically. The algorithm requires n(s, π^star(s)) ≈ T_hit^2 samples from these transient pairs.
- **Core assumption:** The MDP is weakly communicating, and the dataset contains sufficient "transient" samples to cover these recovery paths.
- **Evidence anchors:**
  - [abstract] "learning under these conditions requires data beyond the stationary distribution of the target policy."
  - [section 3.3] Theorem 3.3 "implies that the transient state dataset coverage requirement... is nearly unimprovable."
- **Break condition:** If the dataset strictly covers only the stationary distribution μ^π^star and excludes transient states, the algorithm may fail to learn a policy with vanishing sub-optimality, as it cannot learn the recovery dynamics.

### Mechanism 3
- **Claim:** A discounted reduction approach (using a very large discount factor) suffices to solve the average-reward problem without prior knowledge of system parameters.
- **Mechanism:** The algorithm sets the discount factor γ = 1 - 1/n_tot, where n_tot is the total dataset size. This effectively sets the "effective horizon" to the size of the dataset. Unlike previous work which required knowing the bias span or mixing time to set γ, this reduction allows the algorithm to be parameter-free while the span-regularization (Mechanism 1) handles the scaling issues usually associated with large γ.
- **Core assumption:** The total dataset size n_tot is large enough to define an effective horizon that captures the necessary policy dynamics.
- **Evidence anchors:**
  - [section 3.2] "Let γ = 1 - 1/n_tot... Our algorithm also does not require any prior parameter knowledge for its implementation."
  - [corpus] Related work ("Finite-Time Bounds for Average-Reward Fitted Q-Iteration") notes that existing approaches often rely on "restrictive assumptions... or linearity," whereas this mechanism reduces reliance on specific structural assumptions.
- **Break condition:** If n_tot is very small, the effective horizon 1/(1-γ) becomes short, potentially failing to capture the long-term average reward properties or under-penalizing long-horizon risks.

## Foundational Learning

- **Concept:** **Average-Reward MDPs & Bias Span**
  - **Why needed here:** Unlike discounted RL where value is bounded by 1/(1-γ), average-reward values can be unbounded or arbitrarily shifted. The *bias span* (difference between best and worst starting states) is the critical complexity measure, not the horizon.
  - **Quick check question:** Can you explain why a standard "max-value" penalty fails in average-reward RL but a "span-based" penalty succeeds?

- **Concept:** **Pessimistic Value Iteration (Pessimism Principle)**
  - **Why needed here:** In offline RL, you cannot explore. Pessimism works by under-estimating the value of actions where data is sparse (high uncertainty). This paper innovates by *how* that uncertainty is calculated (using span).
  - **Quick check question:** If an action has high uncertainty, should a pessimistic algorithm increase or decrease its estimated value relative to the empirical mean?

- **Concept:** **Weakly Communicating MDPs**
  - **Why needed here:** This is the most general MDP structure where an optimal policy exists. It allows for states that are transient (visited briefly or never in steady state), which is central to the paper's "transient coverage" requirement.
  - **Quick check question:** Does the algorithm require data from every state, or only states visited in the stationary distribution of the target policy? (Answer: It requires transient states too).

## Architecture Onboarding

- **Component map:** Input Dataset D, Rewards r -> Empirical Model P -> Quantile Clipping Module T_β -> Penalty Calculator b(s,a,V) -> Pessimistic Operator T_pe -> Value Iteration Loop -> Output Greedy Policy π
- **Critical path:** The clipping mechanism (Component 3) is the novel critical path. If the clipping logic (Eq. 4) is implemented incorrectly (e.g., clipping the wrong tail or using the wrong quantile parameter), the constant-shift property is lost, and the algorithm may diverge as γ → 1.
- **Design tradeoffs:**
  - **γ Selection:** Setting γ = 1 - 1/n_tot is parameter-free but computationally expensive (long horizon). Shorter horizons (smaller γ) run faster but lose theoretical guarantees for average reward.
  - **Clipping Threshold β:** If β (Eq. 5) is too large, you clip too much signal (high bias). If too small, the penalty explodes (high variance/over-pessimism). The paper sets β ∝ log(n_tot)/n(s,a).
- **Failure signatures:**
  - **"Vacuous Bound" Failure:** If you implement standard Bernstein penalties instead of the clipped version, the performance will degrade rapidly as the dataset size increases (because the penalty scales with 1/(1-γ) ≈ n_tot).
  - **"Missing Recovery" Failure:** If the dataset contains *only* the target policy's stationary distribution (no transient state coverage), the algorithm may output a policy that performs well initially but has a non-vanishing suboptimality gap (ρ^star - ρ^π^ > 0).
- **First 3 experiments:**
  1. **Transient Coverage Ablation:** Construct a "Hard Instance" MDP (like Figure 1) where the optimal policy has a transient state. Run the algorithm on a dataset with and without samples from that transient state. Verify that performance collapses without transient coverage (validating Theorem 3.3).
  2. **Scaling Law Check:** Run the algorithm on a family of MDPs with increasing bias span ||h^π^star||_{span}. Plot the resulting suboptimality gap against √(S · span / m). The curve should be linear if the bound is tight (validating Theorem 3.2).
  3. **Baseline Comparison (Horizon):** Compare against a standard pessimistic VI (e.g., from Li et al. 2023) with fixed γ vs. this algorithm's dynamic γ. Show that the baseline's performance plateaus or degrades as n_tot grows, while this algorithm continues to improve.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the bias-span-dependent sample complexity guarantees be extended to function approximation settings (e.g., linear MDPs) to remove the dependence on the number of states S?
  - **Basis in paper:** [explicit] The conclusion states: "...a main limitation of our work is its focus on the tabular setting, hence an important direction is to extend these improvements to function approximation setups to avoid dependence on S in the results."
  - **Why unresolved:** The current analysis relies on empirical Bernstein inequalities and covering numbers over the state space, which typically scale with S or fail to generalize to infinite-dimensional feature spaces without significant modification.
  - **What evidence would resolve it:** An algorithm for linear MDPs with a sample complexity bound depending on the feature dimension d and the target policy span, rather than the total state count S.

- **Open Question 2:** What structural assumptions or side information could be provided to enable learning a near-optimal policy without requiring data coverage of states that are transient under the target policy?
  - **Basis in paper:** [explicit] The conclusion notes: "...interesting future direction is to explore additional assumptions or information that could be provided to the algorithm to circumvent this requirement [of transient coverage]."
  - **Why unresolved:** Theorem 3.3 proves that under standard offline RL assumptions with only stationary-distribution coverage, learning is impossible because the algorithm cannot predict recovery trajectories from transient states back to the recurrent class.
  - **What evidence would resolve it:** Identification of a specific structural property (e.g., known connectivity or smoothness constraints on transitions) under which Theorem 3.3 fails and stationary-distribution coverage suffices.

- **Open Question 3:** Can the optimal statistical rates obtained in this paper be achieved with a computational complexity that is sublinear in the total dataset size n_tot?
  - **Basis in paper:** [inferred] The paper sets the discount factor γ = 1 - 1/n_tot, which requires the pessimistic value iteration to run for K ≈ n_tot steps. The authors acknowledge in Section 3.2 that running for this horizon is "suboptimal from a computational perspective."
  - **Why unresolved:** The discounted reduction approach links the approximation quality to the effective horizon, forcing a linear dependence on dataset size for iteration count. Simultaneously achieving optimal statistical efficiency (requiring large horizons) and computational efficiency remains unaddressed.
  - **What evidence would resolve it:** An algorithm (potentially based on linear programming or variance-reduced VI) that achieves the O(√(S/m)) error rate with a running time of O(poly(S, A, log n_tot)).

## Limitations
- The analysis assumes exact empirical models and stationary distribution estimates, without addressing function approximation challenges in large state spaces.
- The transient coverage requirement may be impractical in real-world settings where certain states are genuinely unreachable from the data collection policy.
- The clipping parameter β requires careful tuning - while the paper provides theoretical guidance, practical performance may be sensitive to this choice.

## Confidence
- **High Confidence**: The theoretical framework for average-reward MDPs and the necessity of transient coverage (Theorem 3.3) are well-established within the RL literature. The span-based penalty mechanism (Mechanism 1) follows logically from the constant-shift property proof.
- **Medium Confidence**: The sample complexity bound O(√(S·bias span/m)) appears tight based on the matching lower bound, but empirical validation on real-world benchmarks is needed to confirm practical performance.
- **Medium Confidence**: The parameter-free approach using γ = 1 - 1/n_tot is theoretically sound, but may face computational challenges in practice due to the large effective horizon.

## Next Checks
1. **Transient Coverage Experiment**: Construct a synthetic MDP with a known transient state that's critical for optimal performance. Generate datasets with and without this transient state coverage and measure the performance gap to validate Theorem 3.3 empirically.

2. **Clipping Sensitivity Analysis**: Systematically vary the clipping parameter β across different dataset sizes and MDP structures. Plot performance against theoretical predictions to identify practical regimes where the clipping mechanism breaks down.

3. **Scalability Test**: Implement the algorithm with neural network function approximation on a continuous control benchmark (e.g., D4RL datasets). Compare against existing average-reward offline RL methods to assess real-world applicability beyond tabular settings.