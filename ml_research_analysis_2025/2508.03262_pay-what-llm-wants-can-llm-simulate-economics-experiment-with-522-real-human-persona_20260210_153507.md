---
ver: rpa2
title: 'Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human
  Persona?'
arxiv_id: '2508.03262'
source_url: https://arxiv.org/abs/2508.03262
tags:
- value
- llms
- human
- persona
- format
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether Large Language Models (LLMs) can simulate
  economic decision-making by predicting willingness-to-pay in Pay-What-You-Want experiments
  using real persona data from 522 Korean participants. The researchers compared three
  state-of-the-art multimodal LLMs under two interaction conditions and tested various
  persona formats and prompting methods.
---

# Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?

## Quick Facts
- arXiv ID: 2508.03262
- Source URL: https://arxiv.org/abs/2508.03262
- Reference count: 19
- Primary result: LLMs achieved <5% individual-level accuracy predicting willingness-to-pay, with structured survey formats outperforming narrative storytelling

## Executive Summary
This study evaluates whether Large Language Models (LLMs) can simulate human economic decision-making by predicting willingness-to-pay in Pay-What-You-Want experiments. Using persona data from 522 Korean participants, researchers tested three multimodal LLMs across various persona formats and prompting methods. Results show LLMs captured group-level tendencies but failed at individual-level accuracy (under 5%), with structured survey formats outperforming narrative storytelling. Current prompting techniques offered no significant advantage over simple methods, suggesting fundamental architectural limitations rather than prompt engineering issues.

## Method Summary
The researchers used 522 Korean participants' persona data (65 survey items including demographics, cultural attitudes, knowledge scores, education history, and social norms) to predict willingness-to-pay in PWYW scenarios for art exhibitions and music performances. Three multimodal LLMs (GPT-4o, Llama-3.2-90B-Vision-Instruct, Qwen2.5-VL-72B-Instruct) were tested via API with temperature=0 under two interaction conditions (Sequential vs Human-guided) and two persona formats (Survey vs Storytelling). Four prompting methods were evaluated: Base, Chain-of-Thought, RAG, and Few-shot with 3 examples. Individual-level accuracy measured exact match rates, while group-level metrics (CSA, SSA, Jaccard index from Heckman regression) assessed variable importance agreement.

## Key Results
- Individual-level accuracy remained under 5% across all models and conditions
- Structured Survey format consistently outperformed Storytelling format (GPT-4o: 4.02% vs 0.19% in Base condition)
- Human-guided condition improved accuracy over Sequential by providing actual human responses as context
- Advanced prompting methods (CoT, RAG, Few-shot) showed no significant advantage over simple Base prompting
- Models exhibited strong clustering in price predictions compared to diverse human responses

## Why This Works (Mechanism)

### Mechanism 1: Human-Guided Context as Implicit Few-Shot
Providing actual human responses in conversation history improves LLM prediction accuracy by constraining the reasoning space. When human responses are injected into the conversation log, LLMs use these as implicit examples that ground subsequent predictions, reducing error accumulation from self-generated response chains. Evidence shows Human-guided Condition achieved 33.74% accuracy for Qwen on Q2.1 versus only 9.78% in Sequential condition.

### Mechanism 2: Structured Format Superiority via Reduced Attention Dispersion
Structured survey formats outperform narrative storytelling because they reduce attentional dispersion and maintain clearer decision-relevant signal paths. Survey format presents persona attributes in discrete, labeled fields that map directly to decision variables, while storytelling embeds the same information in continuous narrative that may cause the model to attend to irrelevant narrative elements while deprioritizing decision-critical attributes.

### Mechanism 3: Response Clustering from Lack of Behavioral Heterogeneity
LLMs produce clustered price predictions because they lack the individual-level behavioral heterogeneity that characterizes real human decision-making. Even with identical persona information, LLMs converge toward similar price points due to shared training distributions, whereas humans exhibit idiosyncratic variation from unobserved factors not captured in persona data.

## Foundational Learning

- Concept: Pay-What-You-Want (PWYW) Economic Paradigm
  - Why needed here: PWYW requires integrating willingness-to-pay, income constraints, fairness perceptions, and social norms—making it a demanding test of behavioral simulation beyond simple classification
  - Quick check question: Can you explain why PWYW is more cognitively demanding than a binary purchase decision?

- Concept: Heckman Two-Step Selection Model
  - Why needed here: The paper uses this to correct for sample selection bias when modeling willingness-to-pay; understanding it is necessary to interpret the group-level evaluation metrics (CSA, SSA, Jaccard)
  - Quick check question: Why does the paper calculate an inverse Mills ratio in Step 1 before running the outcome regression in Step 2?

- Concept: Persona Injection Dimensions
  - Why needed here: The paper systematically varies persona format (Survey vs. Storytelling) and prompting method (Base, CoT, RAG, Few-shot); understanding these axes is prerequisite to interpreting why advanced methods showed no advantage
  - Quick check question: What two orthogonal axes define the experimental space in RQ2?

## Architecture Onboarding

- Component map:
  Persona Encoder -> Prompt Constructor -> Interaction Controller -> Evaluation Layer

- Critical path:
  1. Encode persona (Survey or Storytelling format)
  2. Inject persona as system prompt
  3. Present experimental questions (Q1 binary, Q2 binary, Q2.1 numeric WTP, Q3 multi-select reason)
  4. Either accumulate LLM responses (Sequential) or replace with human responses (Human-guided)
  5. Compare predictions against ground truth at both individual and group levels

- Design tradeoffs:
  - Survey vs. Storytelling: Survey format yields higher accuracy but may not capture emergent persona coherence; Storytelling may better represent holistic identity but introduces attention dispersion
  - Human-guided vs. Sequential: Human-guided improves accuracy but requires access to actual human responses—invalid for forward prediction scenarios
  - Prompting sophistication vs. simplicity: Paper finds no significant gain from CoT/RAG/Few-shot over naive prompting—suggests investment in prompt engineering has diminishing returns for behavioral simulation

- Failure signatures:
  - Overall accuracy <5% despite perfect information parity: Indicates fundamental architectural limitation
  - Low Jaccard indices (0.17-0.67): LLMs identify different important variables than humans
  - Response clustering: Price predictions concentrate at specific price points vs. human distributional diversity
  - CoT performance degradation: GPT-4o overall accuracy dropped from 4.02% (Base) to 2.49% (CoT)

- First 3 experiments:
  1. Replicate the Survey Format × Base × Human-guided baseline with a single model (GPT-4o recommended) on a 50-persona subset to validate pipeline integrity
  2. Ablate the Human-guided advantage by progressively corrupting injected human responses (random substitution, noise injection) to measure robustness
  3. Test response diversity interventions: prompt explicitly for heterogeneity, sample with temperature >0, or ensemble multiple personas—compare resulting distributions against human variance using KL divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific persona components (e.g., demographics vs. attitudes) most significantly influence the LLM's prediction accuracy?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that they "did not conduct detailed analysis of which specific persona components among the 65 collected items most significantly influence prediction performance."
- Why unresolved: The study treated the persona as a holistic input and did not perform ablation studies to isolate the predictive power of specific survey categories.
- What evidence would resolve it: A feature importance analysis or ablation study showing how accuracy degrades or improves when specific persona categories (cultural knowledge vs. income) are removed.

### Open Question 2
- Question: Can advanced technical modeling approaches, such as ensemble methods or fine-tuning, overcome the low individual-level accuracy observed with prompting?
- Basis in paper: [explicit] The authors list as a limitation that they "did not explore advanced technical modeling approaches such as ensemble methods or fine-tuning that could potentially improve accuracy."
- Why unresolved: The study focused strictly on inference-time strategies (prompting formats like CoT and RAG) using off-the-shelf models.
- What evidence would resolve it: Experiments applying fine-tuning (e.g., LoRA) or ensemble voting on the PWYW dataset to determine if model adaptation yields significant gains over zero-shot or few-shot prompting.

### Open Question 3
- Question: What architectural or training paradigm changes are required to induce response diversity in LLMs to match human variance?
- Basis in paper: [inferred] The paper concludes that current models exhibit "strong clustering in price predictions" compared to humans, highlighting "fundamental limitations in current LLM architectures" rather than prompting issues.
- Why unresolved: Current models tend to converge on specific price points (low variance), failing to capture the high diversity of human willingness-to-pay even when group-level tendencies are accurate.
- What evidence would resolve it: Testing models trained with specific diversity-promoting objectives or different sampling strategies to see if the variance of generated prices can match the human distribution.

## Limitations

- The evaluation metrics measure group-level variable importance rather than individual prediction fidelity, limiting conclusions about behavioral realism
- All models used temperature=0, which may artificially suppress response diversity even if models could generate it
- The persona-to-response pipeline assumes linear transferability from structured survey items to coherent economic decisions

## Confidence

- **High**: Group-level metric findings (LLMs cluster responses, identify different significant variables than humans)
- **Medium**: Format superiority (Survey vs Storytelling) and prompting method ineffectiveness (Base outperforms CoT/RAG/Few-shot)
- **Low**: Individual-level accuracy numbers (<5%) as definitive proof of fundamental limitations, given evaluation granularity constraints

## Next Checks

1. Replicate individual-level accuracy calculations with relaxed matching criteria (within ±10% of actual price) to test whether strict exact-match metrics are overly punitive
2. Test temperature variation (0.3, 0.7) with the same models to assess whether response diversity can be induced without architectural changes
3. Implement cross-validation where persona subsets are held out to measure prediction generalization vs. memorization effects