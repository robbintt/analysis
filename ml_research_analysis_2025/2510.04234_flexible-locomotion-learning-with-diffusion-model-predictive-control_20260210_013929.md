---
ver: rpa2
title: Flexible Locomotion Learning with Diffusion Model Predictive Control
arxiv_id: '2510.04234'
source_url: https://arxiv.org/abs/2510.04234
tags:
- diffusion
- planning
- reward
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion-MPC, a novel framework that leverages
  diffusion models as generative priors for model predictive control in legged locomotion.
  The method addresses the challenge of achieving flexible and adaptable behavior
  synthesis at test time, overcoming the limitations of fixed RL policies and classical
  MPC's reliance on accurate dynamics models.
---

# Flexible Locomotion Learning with Diffusion Model Predictive Control

## Quick Facts
- arXiv ID: 2510.04234
- Source URL: https://arxiv.org/abs/2510.04234
- Reference count: 40
- Primary result: Diffusion-MPC framework for flexible quadruped locomotion with test-time adaptation to new tasks and constraints

## Executive Summary
This paper introduces Diffusion-MPC, a novel framework that leverages diffusion models as generative priors for model predictive control in legged locomotion. The method addresses the challenge of achieving flexible and adaptable behavior synthesis at test time, overcoming the limitations of fixed RL policies and classical MPC's reliance on accurate dynamics models. Diffusion-MPC jointly predicts future states and actions, incorporating reward-based planning and constraint projection at each reverse step to synthesize trajectories that satisfy task objectives while remaining within physical limits.

## Method Summary
Diffusion-MPC operates in two stages: first, it pretrains a diffusion model (DDPM) on 4,000 offline trajectories from a PPO demonstrator in IsaacGym; second, it finetunes the model interactively using reward-weighted regression that collects rollouts, filters by returns, and updates the denoiser. At inference, the planner samples multiple candidate trajectories, denoises them with reward gradients and constraint projections, and executes the best action asynchronously. The framework is validated on a real-world Unitree Go2 quadruped, demonstrating strong locomotion performance and flexible adaptation to new tasks.

## Key Results
- Real-world experiments show up to 20% energy savings and successful deployment on challenging terrains including soft uneven grass and grassy slopes
- Demonstrated flexible adaptation to new tasks such as base height variation, joint limit restriction, energy saving, joint acceleration/velocity regularization, and balancing under external disturbances
- Achieved 98.8% success rate at 1.0 m/s speed after interactive finetuning, compared to 18.4% without finetuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly predicting states and actions allows the model to serve as an implicit dynamics prior.
- **Mechanism:** Unlike action-only policies, Diffusion-MPC learns a distribution over sequences containing both states and actions. By denoising correlated state-action pairs, the model learns valid transitions without an explicit analytical dynamics model.
- **Core assumption:** The offline dataset sufficiently covers the state-action space such that valid trajectories lie within high-density regions of the learned distribution.
- **Evidence anchors:** [Page 3, Section IV.A] states the "joint state–action formulation provides a unified representation... naturally respects system dynamics, eliminating the need for an explicit dynamics model."

### Mechanism 2
- **Claim:** Test-time flexibility is achieved by guiding the sampling process with reward gradients and hard constraint projections.
- **Mechanism:** Instead of retraining weights, the method optimizes the trajectory during the reverse diffusion steps. It uses reward gradients to pull samples toward high-reward behaviors and projection operators to enforce physical limits.
- **Core assumption:** The reward landscape is locally smooth enough for gradient-based guidance to improve the trajectory without destroying the structure of the diffusion prior.
- **Evidence anchors:** [Page 3, Section IV.A] details the update rules: $\tau^{k-1} \leftarrow \tau^{k-1} + \lambda \Sigma_k \nabla R$ and the projection $\Pi_C$.

### Mechanism 3
- **Claim:** Interactive training with reward-weighted regression adapts the planner beyond the limits of static demonstrations.
- **Mechanism:** The system rolls out the current planner, collects trajectories, and reweights them based on realized returns. It then updates the denoiser using a weighted loss, amplifying the probability of high-return behaviors.
- **Core assumption:** The initial policy is sufficiently robust to collect some successful trajectories (non-zero return) to serve as positive examples for the reweighting step.
- **Evidence anchors:** [Page 3, Section IV.C] describes the "interactive training algorithm... filter and reweight the collected trajectories by their realized returns."

## Foundational Learning

### Concept: Denoising Diffusion Probabilistic Models (DDPM)
- **Why needed here:** The core engine of this controller is a diffusion model. You must understand that it generates data (trajectories) by reversing a gradual noising process, and that this reverse process can be conditioned or guided.
- **Quick check question:** Can you explain why adding noise to a trajectory and learning to predict that noise allows a model to generate valid motion sequences?

### Concept: Model Predictive Control (MPC)
- **Why needed here:** Diffusion-MPC is a hybrid approach. Understanding traditional MPC helps you see why this method uses a receding horizon (planning $H$ steps ahead but executing only the first action) and why handling constraints is critical.
- **Quick check question:** Why does MPC typically re-solve the optimization problem at every time step rather than computing a full trajectory once?

### Concept: Reward-Weighted Regression
- **Why needed here:** This is the mechanism for the "Interactive Training" phase. It differs from standard gradient descent; it uses the outcome of rollouts to weight the loss function.
- **Quick check question:** How does weighting the loss function by the return of a trajectory differ from simply maximizing the return directly via policy gradients?

## Architecture Onboarding

### Component map:
Observation -> Diffusion U-Net -> Guidance Engine ($\nabla R$, $\Pi_C$) -> Candidate Ranking -> Action Execution

### Critical path:
1. Observation (state $s_t$) is fixed as the start of the noisy trajectory
2. Sampling (Reverse Diffusion): $K$ steps of denoising, interleaved with guidance updates
3. Selection: Generate $N$ candidates, rank by reward, pick best
4. Execution: Send first action to the robot; trigger replan $D$ steps before the buffer empties

### Design tradeoffs:
- **DDPM vs. DDIM:** DDPM offers higher quality but is slower; DDIM is faster but may degrade motion quality (noted in Table VI)
- **Horizon ($H$) vs. Replan Margin ($D$):** A larger $D$ ensures real-time safety (latency buffer) but reduces responsiveness to immediate feedback
- **Candidate Count ($N$):** More candidates improve robustness (Table II) but cost linear compute time

### Failure signatures:
- **"Freezing" or Stuttering:** Likely caused by aggressive caching without refreshing, preventing the planner from adapting to new commands (Table VI)
- **Joint Limit Violation:** The projection step $\Pi_C$ may be misconfigured or the constraint set $C$ is incorrectly defined
- **Drift/Falling:** If the planner frequency drops below real-time requirements (e.g., < 50Hz), the open-loop action buffer runs dry

### First 3 experiments:
1. **Overfitting Test:** Train on a tiny dataset of flat-ground walking. Verify the model can denoise a trajectory perfectly but fails when you apply a test-time constraint (e.g., "lower height") without guidance
2. **Guidance Ablation:** Isolate the sampling loop. Generate trajectories with Reward Planning ON / Constraints OFF and vice versa. Verify that Reward changes the style (e.g., energy saving) while Constraints enforce hard limits (e.g., joint angles)
3. **Latency Stress Test:** Deploy on the robot with $N=1$ candidate vs. $N=10$. Measure the control frequency. If frequency drops below control rate (50Hz), adjust the "Replan Margin" ($D$) or switch to DDIM

## Open Questions the Paper Calls Out
- **Can the constraint projection mechanism be extended to handle nonconvex or time-varying constraints without destabilizing the planning process?** [explicit] The conclusion states that the present study considers only simple constraint classes and that "extending the approach to richer, task-dependent (and potentially nonconvex or time-varying) constraints is a natural next step."
- **Is it possible to train a competitive Diffusion-MPC planner from scratch without relying on pre-collected offline demonstration data?** [explicit] The authors identify "develop[ing] interactive finetuning methods that can train competitive planners without relying on offline data" as a key direction to remove the need for expert demonstrations.
- **How does the integration of high-dimensional sensory inputs (e.g., LiDAR, vision) affect the diffusion model's ability to perform real-time planning?** [explicit] The conclusion lists "extending the framework to integrate diverse inputs—such as LiDAR, visual perception, and natural language" as a future direction.

## Limitations
- The approach depends heavily on the offline dataset covering sufficient diversity for the diffusion prior to generalize, with no explicit validation of coverage outside the original training distribution
- The constraint projection mechanism is described but lacks implementation details for complex non-convex sets
- Sim-to-real transfer success relies on interactive finetuning, but the sample efficiency and generalization of this adaptation phase are not quantified across diverse terrains

## Confidence
- **High confidence:** The core mechanism of using diffusion models as generative priors for trajectory optimization, the two-stage training procedure, and the general structure of incorporating reward gradients and constraints during reverse sampling
- **Medium confidence:** The effectiveness of interactive training for sim-to-real transfer, given that results show strong performance but the number of required interaction iterations and sample efficiency are not detailed
- **Medium confidence:** The specific implementation details of the constraint projection operator and the exact hyperparameter schedules for gradient guidance during sampling

## Next Checks
1. **Dataset Coverage Validation:** Quantify the state-action space coverage of the offline dataset by measuring the percentage of test-time states (from challenging terrains like slopes and grass) that fall within high-density regions of the learned diffusion model
2. **Constraint Projection Stress Test:** Implement a controlled test where the planner must satisfy increasingly complex constraint combinations (e.g., joint limits + velocity bounds + height constraints) and measure success rates and sampling stability
3. **Interactive Training Sample Efficiency:** Measure the number of interaction iterations and total samples required for the planner to achieve stable locomotion on novel terrains (e.g., grass) starting from the pre-trained model, reporting the learning curve of success rate vs. interaction steps