---
ver: rpa2
title: On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization
arxiv_id: '2510.11792'
source_url: https://arxiv.org/abs/2510.11792
tags:
- final
- additive
- iter
- evals
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether bilateral uncertainty (BU) in additive
  Bayesian optimization (BO) should be accounted for in Thompson Sampling. While most
  acquisition functions in additive BO discard BU due to computational intractability,
  the author develops an efficient method to sample respecting BU by exploiting conditional
  independence among additive subfunctions.
---

# On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization

## Quick Facts
- arXiv ID: 2510.11792
- Source URL: https://arxiv.org/abs/2510.11792
- Authors: Nathan Wycoff
- Reference count: 40
- Primary result: Bilateral uncertainty can be safely ignored in additive BO without meaningful performance loss

## Executive Summary
This paper addresses whether bilateral uncertainty should be accounted for in Thompson Sampling within additive Bayesian optimization. The author develops an efficient method to sample respecting bilateral uncertainty by exploiting conditional independence among additive subfunctions. This enables direct empirical comparison between exact Thompson Sampling and its additive approximation across 15 benchmark functions. The results show that exact Thompson Sampling slightly outperforms its additive approximation in sequential settings, but the difference is not practically significant, supporting the theoretical understanding that bilateral uncertainty can be safely ignored in additive BO.

## Method Summary
The paper proposes a method to efficiently sample from bilateral uncertainty in additive Bayesian optimization by exploiting conditional independence among additive subfunctions. This allows for exact Thompson Sampling that respects bilateral uncertainty, which can then be compared against the standard additive approximation. The approach involves decomposing the objective function into additive components and using the conditional independence structure to sample from the joint posterior distribution efficiently. The method is evaluated across 15 benchmark functions using both sequential and batched acquisition modes.

## Key Results
- Exact Thompson Sampling (ts) slightly outperforms additive approximation (ats) in sequential settings
- Performance differences between ts and ats are negligible in batched settings
- Results support theoretical understanding that bilateral uncertainty can be safely ignored in additive BO
- Empirical validation conducted across synthetic, real-world, and cosmological problems

## Why This Works (Mechanism)
The method works by exploiting the conditional independence structure inherent in additive Bayesian optimization. By decomposing the objective function into additive components, the joint posterior distribution can be sampled efficiently while respecting bilateral uncertainty. This is achieved through careful manipulation of the conditional independence relationships, allowing for exact Thompson Sampling without the computational intractability that typically forces approximations to discard bilateral uncertainty.

## Foundational Learning
- **Additive Bayesian Optimization**: Decomposes objective functions into additive components for scalability
  - Why needed: Enables efficient optimization in high-dimensional spaces
  - Quick check: Verify decomposition maintains accuracy of original function
- **Bilateral Uncertainty**: Captures joint uncertainty between input dimensions
  - Why needed: Provides more accurate uncertainty quantification for exploration
  - Quick check: Compare with unilateral uncertainty approaches
- **Thompson Sampling**: Bayesian method for balancing exploration and exploitation
  - Why needed: Provides principled approach to sequential decision making
  - Quick check: Validate convergence properties on test functions
- **Conditional Independence**: Statistical property enabling efficient computation
  - Why needed: Allows decomposition of complex joint distributions
  - Quick check: Verify conditional independence assumptions hold
- **Kernel Functions**: Define similarity structure in Gaussian processes
  - Why needed: Determine smoothness and correlation properties
  - Quick check: Assess sensitivity to different kernel choices
- **Batch Acquisition**: Parallel evaluation of multiple points
  - Why needed: Improves computational efficiency in practice
  - Quick check: Compare sequential vs. batch performance

## Architecture Onboarding

Component Map: Input -> Additive Decomposition -> Conditional Independence Analysis -> Thompson Sampling Sampling -> Evaluation

Critical Path: The core workflow involves decomposing the objective function, analyzing conditional independence structure, sampling from the posterior respecting bilateral uncertainty, and evaluating the selected points. The most critical component is the efficient sampling method that exploits conditional independence to make exact Thompson Sampling computationally tractable.

Design Tradeoffs: The main tradeoff is between computational efficiency and accuracy of uncertainty quantification. Traditional additive BO sacrifices bilateral uncertainty for tractability, while the proposed method maintains exact bilateral uncertainty sampling at the cost of more complex computations. The conditional independence structure provides the key to achieving both goals simultaneously.

Failure Signatures: Potential failures include breakdown of conditional independence assumptions in highly correlated problems, computational explosion in very high dimensions despite the conditional independence structure, and poor performance when the additive decomposition poorly approximates the true objective function.

First Experiments:
1. Validate conditional independence structure on synthetic test functions
2. Compare exact Thompson Sampling vs additive approximation on simple 2D functions
3. Benchmark computational time vs standard additive BO approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation lacks systematic sensitivity analysis of additive structure approximation quality
- Computational overhead in high-dimensional or highly correlated settings not explored
- Limited investigation of kernel choice impact on observed results
- Does not explore applicability to other acquisition functions beyond Thompson Sampling

## Confidence

High: The claim that bilateral uncertainty can be safely ignored in additive BO without meaningful performance loss is well-supported by the empirical results.

Medium: The assertion that the proposed method enables efficient sampling of bilateral uncertainty is validated, but the computational cost analysis is limited.

Medium: The comparison between exact Thompson Sampling and its additive approximation is robust, but the generalizability to other acquisition functions or more complex problem structures remains uncertain.

## Next Checks

1. Conduct a sensitivity analysis to evaluate how kernel choice and additive structure approximations affect the performance gap between exact and approximate Thompson Sampling.

2. Extend the experimental validation to high-dimensional or highly correlated settings to assess the scalability and robustness of the proposed sampling method.

3. Investigate the computational overhead of bilateral uncertainty sampling in large-scale or resource-constrained applications to determine practical feasibility.