---
ver: rpa2
title: LLM-based Semantic Augmentation for Harmful Content Detection
arxiv_id: '2504.15548'
source_url: https://arxiv.org/abs/2504.15548
tags:
- text
- llms
- data
- meme
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-based semantic augmentation for harmful
  content detection, using LLMs to clean noisy text and generate context-rich explanations
  for memes, toxic comments, and hateful memes. The method enhances training sets
  without substantially increasing data volume by adding LLM-generated explanations
  alongside original text.
---

# LLM-based Semantic Augmentation for Harmful Content Detection

## Quick Facts
- arXiv ID: 2504.15548
- Source URL: https://arxiv.org/abs/2504.15548
- Authors: Elyas Meguellati; Assaad Zeghina; Shazia Sadiq; Gianluca Demartini
- Reference count: 24
- Key outcome: LLM-generated explanations enhance training sets without substantially increasing data volume, achieving 62.2% hierarchical F1 on SemEval 2024 persuasive memes.

## Executive Summary
This paper introduces LLM-based semantic augmentation for harmful content detection, using LLMs to clean noisy text and generate context-rich explanations for memes, toxic comments, and hateful memes. The method enhances training sets without substantially increasing data volume by adding LLM-generated explanations alongside original text. Evaluation on SemEval 2024 persuasive memes shows the approach achieves 62.2% hierarchical F1, outperforming zero-shot LLMs (54.2%) and traditional supervised models. The method also generalizes well to toxic comment detection (F1 25.3, AUC 91.8) and hateful meme detection (F1 38.0, AUC 63.0).

## Method Summary
The approach uses LLMs to augment harmful content datasets by generating cleaned captions for images, context-rich explanations for text, and trigger words for offensive content. For persuasive meme detection, the method concatenates original meme text with cleaned image captions and LLM-generated explanations. For toxic and hateful content detection, it generates explicit trigger words alongside brief explanations to preserve offensive language that standard LLM safeguards would censor. The augmented data trains downstream classifiers including BART decoders for hierarchical classification and DistilBERT or CLIP models for other tasks.

## Key Results
- LLM-based semantic augmentation achieves 62.2% hierarchical F1 on SemEval 2024 persuasive memes
- Outperforms zero-shot LLMs (54.2%) and traditional supervised models on multi-label classification
- Trigger-based augmentation achieves F1 25.3, AUC 91.8 for toxic comments and F1 38.0, AUC 63.0 for hateful memes

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated explanations enrich feature representations by surfacing latent rhetorical and psychological cues that raw inputs may obscure. A sequence-to-sequence model consumes concatenated text plus explanation as a richer training signal.

### Mechanism 2
Trigger-based augmentation preserves explicit offensive language that LLM safeguards would otherwise censor, allowing classifiers to learn from profanity and hate keywords that standard explanations would sanitize.

### Mechanism 3
LLM-based cleaning of noisy image captions improves downstream classification by removing grammatical errors and redundancies while preserving task-relevant visual context.

## Foundational Learning

- **Concept: Zero-shot LLM classification vs. supervised fine-tuning trade-offs**
  - Why needed: The paper shows zero-shot LLMs underperform (54.2% vs. 62.2%) on complex multi-label tasks
  - Quick check: Given a multi-label social media classification task, would you expect a zero-shot LLM to outperform a smaller fine-tuned model with semantic augmentation?

- **Concept: Concatenative augmentation vs. synthetic data generation**
  - Why needed: The method enriches inputs by appending explanations rather than generating new synthetic samples
  - Quick check: If downstream model capacity is fixed, when would appending explanations hurt rather than help?

- **Concept: Content moderation safeguards in LLMs and adversarial prompting**
  - Why needed: Trigger extraction requires carefully designed prompts to bypass refusal behaviors
  - Quick check: Design a prompt that requests offensive keyword extraction while minimizing refusal.

## Architecture Onboarding

- **Component map:** Captioning (BLIP/GIT) -> LLM cleaner (LLaMA 3.1/GPT-4o) -> LLM explainer/trigger generator -> Concatenated input -> Classifier (BART/DistilBERT/CLIP)
- **Critical path:** 1. Run captioning on meme images -> 2. Clean captions via LLM -> 3. Generate explanations/triggers -> 4. Concatenate text + caption + explanation/triggers -> 5. Train downstream classifier -> 6. Evaluate on held-out test
- **Design tradeoffs:** GPT-4o yields strongest results but higher cost; LLaMA 3.1 is cheaper but more variance; aggressive cleaning may discard useful captions
- **Failure signatures:** High variance across explanation runs suggests prompt instability; performance drop in explanation-only mode indicates over-reliance on LLM output
- **First 3 experiments:** 1. Replicate T+C+E vs. T+C ablation on SemEval persuasive memes; 2. Test trigger-based vs. explanation-based augmentation on Jigsaw toxic comments; 3. Swap LLaMA 3.1 for GPT-4o in caption cleaning

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM-based semantic augmentation compare to LLM-based synthetic data generation in terms of cost-efficiency and classification accuracy for specialized domains? The study focused exclusively on enriching existing samples rather than expanding dataset size with synthetic samples.

### Open Question 2
To what extent do different fusion strategies between original text and LLM-generated explanations impact downstream classifier performance? The paper utilized straightforward concatenation without optimizing integration methods.

### Open Question 3
Can LLM-generated semantic augmentations be effectively integrated with rich visual features to improve performance beyond caption-based text substitution? The current method simplifies multimodal inputs into text, potentially losing visual nuances.

## Limitations

- Relative contribution of different augmentation components (cleaned captions, explanations, triggers) to overall performance gains is not rigorously isolated
- Method's generalizability to languages other than English and different harmful content domains remains untested
- Statistical significance testing is limited to specific comparisons rather than comprehensive validation

## Confidence

- **High confidence**: LLM-generated explanations outperform zero-shot LLMs on complex multi-label classification tasks
- **Medium confidence**: Superiority of trigger-based augmentation over explanation-only approaches for toxic/hateful content detection
- **Medium confidence**: Effectiveness of LLM-based caption cleaning, though evidence is limited to specific models

## Next Checks

1. Conduct systematic ablation studies to quantify individual and combined contributions of cleaned captions, explanations, and triggers to downstream model performance

2. Test the method's robustness across different LLM providers (Claude, Gemini, open-source models) to assess whether performance gains depend on specific model capabilities

3. Evaluate performance on multilingual datasets and different harmful content types (misinformation, cyberbullying) to assess domain and language generalization