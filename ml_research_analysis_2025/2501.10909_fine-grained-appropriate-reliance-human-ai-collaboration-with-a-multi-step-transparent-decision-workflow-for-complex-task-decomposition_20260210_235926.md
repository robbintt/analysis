---
ver: rpa2
title: 'Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step
  Transparent Decision Workflow for Complex Task Decomposition'
arxiv_id: '2501.10909'
source_url: https://arxiv.org/abs/2501.10909
tags:
- reliance
- decision
- workflow
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates human-AI collaboration in complex fact-checking
  tasks using a Multi-Step Transparent (MST) workflow that breaks tasks into sub-tasks
  with visible intermediate steps. A user study (N=233) compared one-step and MST
  workflows, finding that while MST did not improve overall team performance or appropriate
  reliance, it helped users develop critical thinking and reduce over-reliance when
  AI advice was misleading.
---

# Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition

## Quick Facts
- arXiv ID: 2501.10909
- Source URL: https://arxiv.org/abs/2501.10909
- Reference count: 40
- Multi-step transparent workflows do not improve overall human-AI team performance in complex fact-checking tasks

## Executive Summary
This paper investigates human-AI collaboration in complex fact-checking tasks using a Multi-Step Transparent (MST) workflow that breaks tasks into sub-tasks with visible intermediate steps. A user study (N=233) compared one-step and MST workflows, finding that while MST did not improve overall team performance or appropriate reliance, it helped users develop critical thinking and reduce over-reliance when AI advice was misleading. Fine-grained analysis showed that users who made more accurate intermediate decisions achieved better performance and appropriate reliance. However, an intervention requiring users to annotate document usefulness increased cognitive load without improving outcomes. The study highlights that no single workflow is optimal for all contexts and that designing effective human-AI collaboration requires balancing transparency benefits against potential cognitive burdens.

## Method Summary
The study used a between-subjects user study (N=233) with 4 conditions: Control (one-step), MST-GT (+global transparency), MSTworkflow (+multi-step workflow), MSTworkflow+ (+document usefulness annotation). Participants verified composite claims by checking sub-facts using retrieved Wikipedia documents, then made final decisions with AI advice. The AI system used GPT-3.5 for task decomposition, BM25 for document retrieval, and flan-t5-xl for sub-fact verification. Metrics included Team Performance, RAIR/RSR (appropriate reliance), Agreement/Switch Fraction, AR-Intermediate (intermediate step accuracy), AR-Evidence (document usefulness agreement), NASA-TLX cognitive load, and TiA trust questionnaire. Statistical analysis used Kruskal-Wallis H-test, post-hoc Mann-Whitney, ANCOVA, and Spearman correlation with Bonferroni-corrected α=0.017.

## Key Results
- MST workflows did not improve overall team performance or appropriate reliance compared to one-step workflows
- MSTworkflow+ condition reduced over-reliance when AI advice was misleading
- Intermediate step accuracy strongly correlated with final performance and appropriate reliance
- Document usefulness annotation intervention increased cognitive load without improving outcomes

## Why This Works (Mechanism)
The MST workflow aims to improve human-AI collaboration by providing transparency through intermediate decision steps. By breaking complex tasks into sub-tasks and showing both AI and human intermediate decisions, users can develop critical thinking skills and better calibrate their reliance on AI advice. The mechanism relies on users learning from intermediate feedback and using transparency to detect when AI advice may be misleading.

## Foundational Learning
- **Task decomposition**: Breaking composite claims into sub-facts (why needed: enables granular verification; quick check: verify decomposition accuracy)
- **Appropriate reliance**: Balancing trust in AI advice with independent judgment (why needed: prevents over/under-reliance; quick check: measure RAIR/RSR scores)
- **Transparency workflows**: Showing intermediate AI decisions and supporting evidence (why needed: enables critical thinking; quick check: compare MST vs one-step conditions)
- **Cognitive load measurement**: Using NASA-TLX to assess mental burden (why needed: identify trade-offs between transparency and usability; quick check: monitor frustration/effect scores)
- **Fine-grained performance metrics**: Measuring intermediate decision accuracy (why needed: identify performance drivers; quick check: correlate AR-Intermediate with final outcomes)

## Architecture Onboarding
- **Component map**: GPT-3.5 decomposition -> BM25 document retrieval -> flan-t5-xl verification -> user decision workflow -> final aggregation
- **Critical path**: User verifies sub-facts → makes intermediate decisions → receives AI advice → makes final decision
- **Design tradeoffs**: Transparency vs cognitive load, automation vs human control, granularity vs complexity
- **Failure signatures**: High cognitive load in annotation conditions, under-reliance in MST conditions, poor intermediate accuracy predicting final failure
- **First experiments**: 1) Compare MST vs one-step baseline performance, 2) Test correlation between intermediate and final accuracy, 3) Measure cognitive load impact of annotation intervention

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- Crowdsourced participants may not represent expert fact-checkers who could better leverage transparency features
- Document usefulness annotation task design may not represent all forms of human input in AI workflows
- Observational data cannot establish causation between MST workflow and improved critical thinking

## Confidence
- **High confidence**: MST workflows do not improve overall team performance or appropriate reliance
- **Medium confidence**: MST helps reduce over-reliance when AI advice is misleading
- **Medium confidence**: Intermediate step accuracy predicts better final outcomes

## Next Checks
1. Conduct replication study with expert participants to test generalization beyond crowdsourced workers
2. Test alternative document usefulness annotation methods with reduced cognitive load
3. Perform mediation analysis to determine how MST affects over-reliance through intermediate decision-making