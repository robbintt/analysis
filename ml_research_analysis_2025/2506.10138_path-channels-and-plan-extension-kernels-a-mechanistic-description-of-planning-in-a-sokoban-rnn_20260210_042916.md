---
ver: rpa2
title: 'Path Channels and Plan Extension Kernels: a Mechanistic Description of Planning
  in a Sokoban RNN'
arxiv_id: '2506.10138'
source_url: https://arxiv.org/abs/2506.10138
tags:
- channels
- channel
- agent
- plan
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reverse-engineers a recurrent neural network trained
  to play Sokoban, identifying how it stores and extends plans for solving puzzles.
  The authors find that certain hidden state channels, called "path channels," directly
  represent the likelihood of moving in specific directions at future timesteps.
---

# Path Channels and Plan Extension Kernels: a Mechanistic Description of Planning in a Sokoban RNN

## Quick Facts
- arXiv ID: 2506.10138
- Source URL: https://arxiv.org/abs/2506.10138
- Reference count: 40
- Key outcome: Reverse-engineers a Sokoban RNN to identify path channels and plan extension kernels as mechanisms for storing and extending plans.

## Executive Summary
This paper reverse-engineers a recurrent neural network trained to play Sokoban, identifying how it stores and extends plans for solving puzzles. The authors find that certain hidden state channels, called "path channels," directly represent the likelihood of moving in specific directions at future timesteps. They show that the network initializes these channels near boxes and targets, then extends them bidirectionally using learned kernels that act like a transition model. The plan extension kernels can propagate both positive and negative activations, enabling the network to prune unpromising paths (backtracking). A winner-takes-all mechanism selects between competing plans. The authors verify their findings through ablation studies and demonstrate that the planning algorithm is consistent across multiple training runs.

## Method Summary
The authors reverse-engineer a DRC(3,3) architecture trained on Sokoban using IMPALA V-trace. The network consists of a 2-layer convolutional encoder feeding into 3 ConvLSTM layers, each ticked 3 times per environment step, followed by an MLP head for policy and value prediction. The training used 2×10^9 environment steps on the Boxoban dataset with 900k unfiltered train levels. The authors analyze the trained model's hidden state representations, identify specific channels that store future action plans ("path channels"), and visualize the convolutional kernels to understand how plans are extended bidirectionally from boxes and targets.

## Key Results
- Identified 96-channel ConvLSTM hidden state where specific channels store directional plans for future moves
- Demonstrated bidirectional plan extension kernels that propagate activations from boxes forward and targets backward
- Showed negative activation propagation enables backtracking by pruning unpromising paths
- Ablation studies confirm path channels are causally important for planning (57.6% performance drop when zeroed)
- Winner-takes-all mechanism selects between competing plans

## Why This Works (Mechanism)

### Mechanism 1: Path Channel Spatial Encoding
The network represents future action plans as spatially localized activations in specific hidden state channels ("path channels") rather than distributed representations. The ConvLSTM hidden state maintains 96 channels, where a subset activates at grid location (x,y) if a move in that direction is planned for that location. This effectively stores a vector field of intended movements. The MLP head reads these activations to predict the immediate next action.

### Mechanism 2: Kernel-Based Bidirectional Search
Plan construction occurs via convolutional kernels that propagate activations spatially, implementing a form of bidirectional search. "Plan extension kernels" shift activations to adjacent grid squares. Positive activations extend forward from the box location and backward from the target location. The collision of these activation fronts forms a complete plan.

### Mechanism 3: Backtracking via Negative Activation Propagation
The network prunes invalid paths by propagating negative activations from obstacles back to the start, suppressing bad plans. Obstacles generate negative activations in path channels. The plan extension kernels, which work bidirectionally, propagate this negative signal backward along the potential path, canceling out the positive "plan" activation and effectively deleting the plan segment leading to a dead end.

## Foundational Learning

- **ConvLSTM Architecture**: Why needed: The paper relies on ConvLSTM's spatial-temporal properties to explain plan storage and propagation. Quick check: How does a ConvLSTM differ from a standard LSTM in processing 2D grid data?

- **Model-Free Reinforcement Learning**: Why needed: The paper emphasizes that planning mechanisms emerged implicitly from model-free training rather than being hard-coded. Quick check: Why is the emergence of a "transition model" surprising in a model-free RL agent?

- **Bidirectional Search**: Why needed: To understand plan extension kernels, one must recognize the algorithmic pattern of searching from start and goal simultaneously. Quick check: What is the computational advantage of searching from both start and goal simultaneously?

## Architecture Onboarding

- **Component map**: Input (RGB Observation) -> Encoder (2-layer Conv) -> Core (3 ConvLSTM layers with 3 ticks) -> Representation (Path Channels + Entity Channels) -> Output (MLP Head to Policy/Value)

- **Critical path**: 1) Init: Encoder detects Box/Target → Activates Path Channels adjacent to objects. 2) Extension: Plan Extension Kernels shift activations until paths collide. 3) Selection: Winner-Takes-All mechanism suppresses weaker competing paths. 4) Execution: MLP reads the immediate next move from "Next Action" channels.

- **Design tradeoffs**: The network learns a robust planning algorithm but fails to generalize to large grids ($40\times40$) without weight steering because plan activations decay over distance. The WTA mechanism stabilizes plans but introduces bias (e.g., preferring paths that connect first rather than shortest paths).

- **Failure signatures**: Plan Decay (activations fade on large levels, leading to inaction). Deadlock (WTA mechanism suppresses valid alternative paths if primary path is slightly stronger but ultimately blocked).

- **First 3 experiments**: 1) Ablate Path Channels: Zero-out identified path channels (e.g., L0H13) and measure drop in solve rate to verify causal importance. 2) Visualize Kernels: Plot weights of "Plan Extension Kernels" to confirm they represent directional shifts. 3) Causal Intervention for Backtracking: Identify square with high negative activation, force it positive, and observe if agent erroneously plans through the wall.

## Open Questions the Paper Calls Out

- **High-Level Switching Logic**: What mechanisms govern the agent's high-level switching logic between multiple boxes and the selection of specific heuristics? The current explanation cannot account for observed behaviors where the agent switches between boxes or chooses plans based on "seemingly irrelevant" features.

- **Cross-Architecture Generalization**: Do other neural network architectures (e.g., Transformers, ResNets) learn isomorphic planning representations to the DRC(3,3)? This study is limited to a specific ConvLSTM architecture; it is untested whether the "path channel" and "plan extension kernel" solution is a universal convergent solution for Sokoban.

- **Internal Value Function**: Why does the network's learned internal value function fail to capture the per-step penalty, contrary to the training reward? The learned value function does not capture plan length/step count, despite the training reward having a -0.1 per-step term.

## Limitations
- Analysis focuses on final trained model, making it difficult to determine whether mechanisms are explicitly learned or emerge as artifacts of optimization.
- Weight steering is needed for generalization to larger grids, suggesting the learned planning algorithm has inherent limitations in scalability.
- Causal validation of backtracking mechanisms relies on intervention experiments that may not capture all edge cases in complex Sokoban puzzles.

## Confidence
- **High Confidence**: Identification of path channels as storing future action plans (57.6% performance drop when zeroed, strong spatial correlation with future moves).
- **Medium Confidence**: Bidirectional search mechanism (kernel visualizations support interpretation but don't definitively prove active bidirectional search).
- **Medium Confidence**: Backtracking mechanism (supported by intervention experiments but not fully explored across edge cases).

## Next Checks
1. **Cross-Architecture Generalization**: Train a different architecture (e.g., Transformer-based or standard LSTM) on Sokoban and test whether similar path channel mechanisms emerge.

2. **Intermediate Training Analysis**: Analyze model checkpoints during training to determine when and how the path channel representation and plan extension kernels emerge.

3. **Adversarial Plan Disruption**: Systematically introduce perturbations to path channel activations at different distances from the agent and measure the resulting behavior.