---
ver: rpa2
title: A Multi-Task Evaluation of LLMs' Processing of Academic Text Input
arxiv_id: '2508.11779'
source_url: https://arxiv.org/abs/2508.11779
tags:
- text
- llms
- evaluation
- content
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates how well large language models (LLMs) can\
  \ process academic texts in the context of peer review. It constructs a four-task\
  \ workflow\u2014content reproduction, comparison, scoring, and reflection\u2014\
  to assess LLMs\u2019 reliability, scalability, discrimination, and insightfulness."
---

# A Multi-Task Evaluation of LLMs' Processing of Academic Text Input

## Quick Facts
- arXiv ID: 2508.11779
- Source URL: https://arxiv.org/abs/2508.11779
- Reference count: 40
- Primary result: LLMs show limited utility for unsupervised academic peer review, with acceptable performance only in basic summarization tasks.

## Executive Summary
This study evaluates how well large language models (LLMs) can process academic texts in the context of peer review. It constructs a four-task workflow—content reproduction, comparison, scoring, and reflection—to assess LLMs' reliability, scalability, discrimination, and insightfulness. Using 246 IS articles and Google's Gemini, the study finds that while LLMs can produce acceptable summaries and keywords, their pairwise text comparisons are error-prone (22–41% error rate), their scoring is biased and poorly discriminating, and their qualitative critiques lack depth and insight. These limitations are robust across prompt variations and consistently reflected in both automated and human evaluations. The findings suggest that LLMs have limited utility for unsupervised academic peer review, with acceptable performance only in basic summarization tasks.

## Method Summary
The study evaluates Google's Gemini Pro on four academic text processing tasks using 246 Information Systems articles. Task 1 (Oracle) generates keywords and abstracts; Task 2 (Arbiter) performs pairwise comparisons; Task 3 (Arbiter) assigns scores (1-10); and Task 4 (Collaborator) provides 3-point critiques. The methodology uses zero-shot prompting with specific templates, runs 5 iterations per prompt, and evaluates results using both automated metrics (BLEU, ROUGE, Copeland scores) and human evaluation by 87 graduate students. Inputs are restricted to Introduction and Conclusion sections to manage token limits.

## Key Results
- LLMs produce acceptable summaries and keywords but struggle with pairwise text comparisons (22–41% error rate)
- Scoring performance shows significant shrinkage bias, with outputs clustering narrowly and failing to discriminate quality differences
- Qualitative critiques lack depth and insight, often summarizing rather than providing substantive feedback
- Task performance degrades systematically as intellectual demand increases from reproduction to reflection

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition Reveals Capability Asymmetry
The paper structures evaluation by cognitive load: reproduction (oracle) < comparison (judgmental arbiter) < scoring (knowledgeable arbiter) < reflection (collaborator). As the task shifts from information compression to generative critique, the model's lack of scientific "understanding" creates a performance gap.

### Mechanism 2: Scalability Loss via Stochastic Error Propagation
The study models ranking unscalability through Copeland scores. Because LLMs exhibit stochasticity (comparing A vs. B can yield different results than B vs. A), the error rate accumulates. As sample size grows, the probability of recovering the true ranking approaches zero.

### Mechanism 3: Score Discrimination Failure via Shrinkage Bias
LLMs compress score distributions, defaulting to a narrow band (e.g., 6.0–9.6). Because the output range is narrower than the ground truth, small differences in actual quality are mapped to identical LLM scores, resulting in "skewed discrimination."

## Foundational Learning

- **Concept: Prompt Robustness Taxonomy**
  - Why needed here: The study validates that LLM failures are not merely prompt engineering issues. Understanding the four dimensions (Semantic, Richness, Abundance, Specificity) is required to distinguish between "bad prompts" and "model limitations."
  - Quick check question: If changing "quality" to "scientific value" in a prompt alters the ranking significantly, which robustness dimension (R1-R4) is being tested?

- **Concept: Metric-Based Evaluation (IE vs. EE)**
  - Why needed here: To objectively measure "reliability" and "insightfulness" without relying solely on costly human evaluation. The paper contrasts internal metrics (linguistic complexity) with external metrics (similarity to ground truth).
  - Quick check question: Why is BLEU sufficient for Task 1 (Reproduction) but problematic for Task 4 (Reflection) when comparing short critiques to long articles?

- **Concept: LLM as "Intelligent Layman"**
  - Why needed here: It frames the architectural constraint. The paper suggests LLMs possess broad but shallow knowledge ("knowing-all-by-a-little"), making them suitable for cross-disciplinary filtering but insufficient for expert depth.
  - Quick check question: How does the "intelligent layman" persona explain the high performance in keyword generation but low performance in novelty detection?

## Architecture Onboarding

- **Component map:** Ingest (246 IS articles) -> Process (Google Gemini Pro) -> Tasks (Reproduction, Comparison, Scoring, Reflection) -> Evaluate (Metrics + Human Arbiters)
- **Critical path:** The "scalability check" is the most fragile component. While Task 1 works reliably, the architecture breaks down if Task 2 is scaled up.
- **Design tradeoffs:**
  - **Metric vs. Human:** The study uses metrics (BLEU/ROUGE) for scale but validates with humans. Note: Metrics often failed to capture "insightfulness" in Task 4, showing high similarity despite low human scores.
  - **Input Length:** Restricting input to Introduction/Conclusion avoids token limits but risks missing methodological details critical for scoring.
- **Failure signatures:**
  - **The "Middle-Ground" Trap:** If score distributions cluster tightly around 8.0–8.5 (high mean, low std), the model is failing to discriminate (Task 3 failure).
  - **Order Bias:** If Compare(A, B) ≠ ¬Compare(B, A), the ranking engine is unstable (Task 2 failure).
- **First 3 experiments:**
  1. **Baseline Stability Test:** Run 5 iterations of Task 2 (Pairwise Comparison) on the same pair of documents. If the preference flips >20% of the time, the model cannot be used for ranking.
  2. **Discrimination Calibration:** Input distinct quality texts (e.g., a published paper vs. a random draft) into Task 3. Check if the score separation Δŵ meets the theoretical threshold derived in Eq. (13).
  3. **Insight vs. Summary Confusion:** Run Task 4 (Reflection) and compare the critique's ROUGE score against the input text. If similarity is high, the model is likely summarizing (hallucinating "validity") rather than critiquing.

## Open Questions the Paper Calls Out

### Open Question 1
Does LLM performance in peer review tasks degrade or improve in scientific fields with highly specialized nomenclature compared to the interdisciplinary Information Systems texts used in this study? The authors note that "the LLM's performance may differ across disciplines with more specialized content" and suggest that future work should adopt texts from diverse disciplines to replicate the analysis.

### Open Question 2
To what extent does providing the full text of an article (rather than just the introduction and conclusion) improve the accuracy of content scoring and the depth of content reflection? The authors identify the "incomplete input of scientific articles into the LLM due to its input length limit" as a major limitation, suggesting future work could use models with longer context windows or multi-round dialogues.

### Open Question 3
Can advanced prompting strategies, such as chain-of-thought reasoning or few-shot examples, mitigate the high error rates (22–41%) and poor scalability observed in pairwise content comparison? The authors state that the LLM output may be enhanced through techniques like "chain-of-thought... fine-tuning... or model merging," which were not utilized in the current experiments.

## Limitations

- Study focuses on a single LLM (Google Gemini) and narrow domain (Information Systems), limiting generalizability across models and disciplines
- Use of Introduction/Conclusion segments may omit critical methodological details necessary for accurate scoring
- Ground-truth rankings and scores are derived from IS citations and AI-interpreted reviewer comments rather than direct expert consensus, introducing potential bias
- Human evaluation sample (87 students) is relatively small and may not represent professional peer reviewers' perspectives

## Confidence

- **Task Decomposition Validity (High):** The four-task framework is well-grounded in peer review literature and consistently supported by experimental results across multiple metrics
- **Scalability Theorem (Medium):** The mathematical proof of ranking unscalability is sound, but relies on the assumption that LLM errors are random rather than systematic, which wasn't explicitly tested
- **Discrimination Failure (Medium):** The shrinkage bias is clearly demonstrated, but the threshold condition (Δw ≥ 0.9/(H_h - H_l)) is derived theoretically and could benefit from empirical validation across more diverse quality ranges

## Next Checks

1. **Cross-Model Validation:** Replicate the four tasks using at least two other LLMs (e.g., GPT-4, Claude) to determine if Gemini's limitations are model-specific or architectural
2. **Full-Text Input Test:** Run Task 3 (Scoring) with complete articles rather than just Introduction/Conclusion to assess whether methodological details improve discrimination
3. **Expert Human Benchmark:** Conduct a small-scale validation with professional peer reviewers comparing their pairwise preferences and scores against LLM outputs to validate the ground-truth quality assessment methodology