---
ver: rpa2
title: Structured Uncertainty guided Clarification for LLM Agents
arxiv_id: '2511.08798'
source_url: https://arxiv.org/abs/2511.08798
tags:
- tool
- user
- page
- question
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ambiguous user instructions
  in large language model (LLM) agents that invoke external tools and APIs. The authors
  introduce a principled formulation of structured uncertainty over tool-call parameters,
  modeling joint tool-argument clarification as a partially observable Markov decision
  process (POMDP) with an Expected Value of Perfect Information (EVPI) objective for
  optimal question selection.
---

# Structured Uncertainty guided Clarification for LLM Agents

## Quick Facts
- **arXiv ID**: 2511.08798
- **Source URL**: https://arxiv.org/abs/2511.08798
- **Reference count**: 40
- **Primary result**: Structured uncertainty framework increases coverage by 7-39% while reducing clarification questions by 1.5-2.7× compared to baselines

## Executive Summary
This paper addresses ambiguous user instructions in LLM agents that invoke external tools by introducing structured uncertainty over tool-call parameters. The authors model joint tool-argument clarification as a partially observable Markov decision process (POMDP) with Expected Value of Perfect Information (EVPI) for optimal question selection. Their SAGE-Agent approach incorporates aspect-based cost modeling to prevent redundancy and demonstrates superior efficiency compared to strong prompting and uncertainty-based baselines. The work also shows that structured uncertainty provides effective training signals for reinforcement learning, significantly improving when-to-call accuracy through uncertainty-weighted GRPO training.

## Method Summary
The method maintains a belief state over structured candidate tool calls, representing parameter uncertainty as domain constraints rather than unstructured text. For each candidate tool call, the system computes parameter certainty based on whether values are specified (certainty = 1), unspecified with finite domain (|D_{i,j}(t)|^{-1}), or infinite (ε = 10^{-4}). Questions are selected by maximizing EVPI minus redundancy cost, where EVPI quantifies expected improvement in decision quality from perfect information. The system terminates clarification when the information gain minus cost falls below a threshold proportional to the best candidate's certainty. For RL fine-tuning, the method uses certainty-weighted rewards that up-weight confident tool calls and penalize low-certainty calls.

## Key Results
- **Coverage improvement**: 7-39% increase on ambiguous tasks compared to baselines
- **Question reduction**: 1.5-2.7× fewer clarification questions while maintaining task execution quality
- **RL performance boost**: When2Call accuracy improves from 36.5% to 65.2% (3B model) and 36.7% to 62.9% (7B model) through uncertainty-weighted GRPO training
- **Benchmark contribution**: Introduction of ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with LLM-based user simulation

## Why This Works (Mechanism)

### Mechanism 1: Structured Belief State Over Parameter Domains
Representing uncertainty over tool parameters as structured domain constraints enables more precise clarification than token-level uncertainty estimation. The system maintains a belief state B(t) = {(c_i, π_i(t))} over candidate tool calls, where parameter certainty is computed as p(θ_{i,j}|T_i, obs_t) = 1 if specified, |D_{i,j}(t)|^{-1} if unspecified with finite domain, and ε if infinite. Belief updates propagate domain constraints from user responses: D_{i,j}(t+1) = D_{i,j}(t) ∩ ExtractConstraints(r, θ_{i,j}, T_i). The approach assumes parameters are conditionally independent given tool selection and that domain constraints can be reliably extracted from natural language responses.

### Mechanism 2: EVPI-Guided Question Selection with Cost Penalty
Selecting questions by maximizing Expected Value of Perfect Information minus redundancy cost reduces questions by 1.5-2.7× while maintaining coverage. For each candidate question q, compute EVPI(q,B(t)) = E_r[max_c π_i(t|q,r)] - max_c π_i(t). Score questions as Score(q,t) = EVPI(q) - λ·Σ_{a∈A(q)} n_a(t), where n_a(t) counts how often aspect a has been queried. Select q* = argmax_q Score(q,t). Terminate when max_q[EVPI(q) - Cost(q)] < α·max_c π_c(t). The approach assumes EVPI approximates actual value of information in tool-calling contexts and that linear cost model adequately captures redundancy penalty.

### Mechanism 3: Certainty-Weighted Reward for RL Fine-Tuning
Using structured uncertainty as a reward signal improves "when to call" accuracy more than model scaling alone. Define Cert(a_t) = max_c π_c(t) for tool calls, 1 - max_c π_c(t) for questions, 1 otherwise. Reward becomes R_category(a_t) = Cert(a_t) · r_base(a_t), up-weighting confident tool calls and penalizing low-certainty calls while rewarding clarification only under high uncertainty. The approach assumes the belief state π_c(t) accurately reflects epistemic uncertainty and that certainty-weighted rewards transfer to improved decision boundaries during GRPO training.

## Foundational Learning

**Concept: Partially Observable Markov Decision Processes (POMDPs)**
- Why needed here: Core theoretical framework for sequential decision-making under uncertainty; SAGE models clarification as a POMDP where the true user intent is hidden state
- Quick check question: Can you explain why a POMDP is more appropriate than a standard MDP for clarification dialogue?

**Concept: Value of Information (EVPI)**
- Why needed here: Provides principled criterion for which questions are worth asking; EVPI quantifies expected improvement in decision quality from perfect information
- Quick check question: What does it mean when EVPI(q) = 0 for all candidate questions?

**Concept: Bayesian Belief Updating with Domain Constraints**
- Why needed here: Mechanism for refining uncertainty as new information arrives; responses are interpreted as constraints on parameter domains rather than free-form text
- Quick check question: How would you handle a user response that constrains multiple parameters simultaneously?

## Architecture Onboarding

**Component map**:
Reason Module (R) -> Uncertainty Quantifier -> Question Generator -> EVPI Scorer -> Belief Updater -> Termination Checker

**Critical path**:
1. User query → Candidate generation with uncertainty markers
2. If max_c π_c(t) ≥ τ_exec → Execute immediately
3. Else → Generate questions → Score with EVPI - Cost → Select best
4. If Score(q*) < α·max_c π_c(t) → Execute best candidate despite uncertainty
5. Else → Ask question → Update beliefs → Loop

**Design tradeoffs**:
- **λ (redundancy weight)**: [0.2, 0.5] recommended. Lower = aggressive questioning (more thorough but higher user burden); higher = conservative (lower burden but risk under-clarification)
- **α (termination threshold)**: [0.1, 0.3] recommended. Lower = early termination (faster but less certain); higher = late termination (more questions)
- **τ_exec (execution threshold)**: Controls when confidence is sufficient to skip clarification entirely
- Assumption: Paper uses ε = 10^{-4} for infinite domains; this affects belief calibration

**Failure signatures**:
1. **Infinite question loops**: Check if λ is too low or domain extraction f_update() fails to constrain domains
2. **Premature execution with wrong parameters**: Check if τ_exec is too low or α is too aggressive
3. **Redundant questions on same aspect**: Verify n_a(t) counter is incrementing correctly
4. **Belief state collapse to zero**: Check for domain intersections that produce empty sets (invalid user responses)

**First 3 experiments**:
1. **Ablate λ on validation subset**: Run λ ∈ {0, 0.2, 0.5, 1.0} on 70 samples per split (matching Figure 5) to characterize question-coverage tradeoff for your domain
2. **Domain extraction validation**: Manually inspect f_update() outputs on 50 user responses to verify constraint extraction accuracy before full deployment
3. **Baseline comparison on single domain**: Replicate Table 3 results on one ClarifyBench domain (e.g., Travel) to validate implementation before multi-domain testing

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can learned contextual priors over tool usage distributions outperform the uniform prior assumption in SAGE-Agent, particularly for users with consistent domain-specific behavior?
- Basis in paper: [explicit] Footnote 1 states: "Future work could incorporate learned tool usage patterns or contextual priors, but uniform assumptions provide a principled, unbiased baseline."
- Why unresolved: The uniform prior assumption ignores that real users have habitual tool preferences and contextual patterns that could improve disambiguation efficiency.
- What evidence would resolve it: Experiments comparing uniform vs. learned priors on user datasets with longitudinal interaction histories, measuring both disambiguation accuracy and question reduction.

**Open Question 2**
- Question: Does relaxing the naive independence assumption across parameters (via dependency-aware belief modeling) yield measurable improvements in EVPI-guided question selection for tools with strongly coupled arguments?
- Basis in paper: [explicit] Section 3.2: "we make a naive independence assumption across parameters for tractability."
- Why unresolved: Many APIs have conditional parameter dependencies (e.g., valid values for parameter B depend on parameter A), which the current factorized belief model cannot capture.
- What evidence would resolve it: Ablation study comparing factorized vs. joint belief representations on a held-out set of dependency-rich tool schemas, with measurement of coverage and question efficiency.

**Open Question 3**
- Question: Does SAGE-Agent's reduced questioning translate to improved subjective user experience and task completion rates in studies with human users?
- Basis in paper: [inferred] ClarifyBench uses LLM-based user simulation rather than real human participants; Section 10 mentions deployment considerations but lacks human-subject evaluation.
- Why unresolved: Question reduction may not correlate with user satisfaction if eliminated questions were perceived as helpful rather than redundant.
- What evidence would resolve it: Human-user study measuring task success rate, subjective satisfaction scores (e.g., SUS), and perceived cognitive load comparing SAGE-Agent to baselines.

## Limitations
- **Belief independence assumption**: Naive Bayes assumption across parameters may fail for tools with conditional dependencies between arguments
- **Synthetic benchmark**: Performance gains demonstrated on LLM-simulated users may not generalize to real human behavior patterns
- **Domain constraint extraction**: Reliability depends on accurate natural language to parameter domain constraint mapping, which may fail on ambiguous responses

## Confidence
- **High Confidence**: Structured uncertainty formulation as POMDP, EVPI-based question selection mechanism, certainty-weighted GRPO training framework
- **Medium Confidence**: Quantitative performance improvements on ClarifyBench benchmarks (methodologically sound but limited by synthetic nature)
- **Low Confidence**: Transferability to real-world deployment scenarios with uncooperative users or complex tool schemas

## Next Checks
1. **Belief State Stress Test**: Deploy SAGE-Agent on ClarifyBench with artificially corrupted user responses and measure belief state stability metrics including percentage of π_c(t) that collapse to zero
2. **Domain Constraint Extraction Validation**: Implement and test the f_update(θ_{i,j}, r) function on 200 manually annotated user responses across all tool domains, measuring precision/recall
3. **Cross-Domain Generalization Study**: Train SAGE-Agent on 3 ClarifyBench domains and test on the remaining 2 held-out domains, measuring coverage rate drop and question count increase