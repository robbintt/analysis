---
ver: rpa2
title: Investigating Intersectional Bias in Large Language Models using Confidence
  Disparities in Coreference Resolution
arxiv_id: '2508.07111'
source_url: https://arxiv.org/abs/2508.07111
tags:
- coreference
- confidence
- gender
- bias
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WinoIdentity, a new benchmark for evaluating
  intersectional bias in large language models through uncertainty-aware coreference
  resolution. The authors augment the WinoBias dataset with 25 demographic markers
  across 10 attributes (including age, nationality, race, gender identity, and sexual
  orientation) to create 245,700 prompts evaluating 50 intersectional bias patterns.
---

# Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution

## Quick Facts
- arXiv ID: 2508.07111
- Source URL: https://arxiv.org/abs/2508.07111
- Authors: Falaah Arif Khan; Nivedha Sivakumar; Yinong Oliver Wang; Katherine Metcalf; Cezanne Camacho; Barry-John Theobald; Luca Zappella; Nicholas Apostoloff
- Reference count: 35
- Key outcome: WinoIdentity benchmark reveals LLM confidence disparities up to 40% for marginalized identities, with doubly-disadvantaged groups showing highest uncertainty in anti-stereotypical contexts

## Executive Summary
This paper introduces WinoIdentity, a new benchmark for evaluating intersectional bias in LLMs through uncertainty-aware coreference resolution. By augmenting WinoBias with 25 demographic markers across 10 attributes, the authors create 245,700 prompts evaluating 50 intersectional bias patterns. They propose Coreference Confidence Disparity as a group fairness metric measuring confidence differences across demographic subgroups. Evaluating five LLMs, they find confidence disparities as high as 40% for body type, sexual orientation, and socio-economic status, with doubly-disadvantaged identities showing highest uncertainty in anti-stereotypical contexts. Surprisingly, even hegemonic markers like "White" and "cisgender" show decreased confidence, suggesting LLMs rely on memorization rather than reasoning.

## Method Summary
The paper evaluates intersectional bias by constructing 245,700 prompts from WinoBias base sentences (1,575 unique) augmented with 25 demographic markers across 10 attributes using three strategies: referent augmentation (R-Aug), non-referent augmentation (NR-Aug), and contrastive augmentation (C-Aug). For each prompt, they compute Coreference Confidence (CC) as the probability difference between referent and non-referent occupations. The Coreference Confidence Disparity metric measures maximum subgroup disparity across intersectional identities. Five LLMs are evaluated using greedy decoding, with log probabilities extracted for both occupations and aggregated by subgroup to identify bias patterns.

## Key Results
- Confidence disparities reach 40% for body type, sexual orientation, and socio-economic status markers
- Doubly-disadvantaged identities (e.g., transgender women) show highest uncertainty in anti-stereotypical contexts
- Even hegemonic identities like "White" and "cisgender" experience confidence decreases, indicating memorization over reasoning
- Two independent failures compound: invalidity (poor reasoning) and value misalignment (unfair bias)

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Based Bias Detection
- Claim: Coreference confidence disparities reveal representational harms that accuracy-based metrics often miss
- Mechanism: Systematic differences in model confidence across demographic groups constitute procedural unfairness
- Evidence: Confidence decreases under referent augmentation for marginalized groups; accuracy-based metrics can misinterpret bias as improvement
- Break condition: If uncertainty were randomly distributed across groups rather than systematically patterned by identity

### Mechanism 2: Intersectional Amplification via Anti-Stereotypical Contexts
- Claim: Bias is most pronounced for "doubly-disadvantaged" identities in anti-stereotypical contexts
- Mechanism: When demographic markers combine, model confidence drops compound rather than add linearly
- Evidence: Transgender fem + mechanic shows -0.24 confidence versus -0.065 for fem + mechanic
- Break condition: If debiasing interventions successfully disentangled occupational associations from demographic priors

### Mechanism 3: Memorization Over Reasoning as a Validity Failure
- Claim: LLMs rely on memorized statistical associations rather than genuine syntactic/semantic reasoning
- Mechanism: Demographic markers in these sentences provide no logically relevant information, so any confidence change indicates over-reliance on surface patterns
- Evidence: Coreference confidence decreases even for hegemonic markers like "White" or "cisgender"
- Break condition: If models were trained with robustness to lexical perturbations

## Foundational Learning

- **Concept: Coreference Resolution**
  - Why needed here: The entire benchmark evaluates whether LLMs can correctly determine which noun phrase a pronoun refers to
  - Quick check question: Given "The librarian helped the CEO because she was overwhelmed," does syntactic structure alone determine the referent of "she"?

- **Concept: Intersectionality (Crenshaw, 1989)**
  - Why needed here: The paper's central thesis is that bias at the intersection of multiple identities differs from single-axis bias
  - Quick check question: Why might bias measurements that evaluate gender and race separately fail to capture the experience of someone at their intersection?

- **Concept: Uncertainty Quantification in LLMs**
  - Why needed here: The Coreference Confidence metric uses next-token probability differences as a proxy for model certainty
  - Quick check question: If P(referent) = 0.51 and P(non-referent) = 0.49, what does a Coreference Confidence of 0.02 suggest about the model's internal state?

## Architecture Onboarding

- **Component map:** Dataset Construction Pipeline -> Evaluation Engine -> Metric Computation
- **Critical path:** Prompt construction → log-probability extraction → CC computation → subgroup aggregation
- **Design tradeoffs:** Binary gender limits generalizability; US-centric markers may not transfer culturally; combinatorial explosion creates evaluation cost
- **Failure signatures:** "Fair" models that are actually uniformly underconfident; accuracy increases while confidence decreases under non-referent augmentation
- **First 3 experiments:**
  1. Reproduce baseline disparities on 2 models using gender identity + socio-economic status attributes only
  2. Ablate augmentation type on Type-2 sentences to verify NR-Aug produces smaller confidence changes than R-Aug
  3. Pilot non-demographic augmentation to confirm these affect pronouns more equally than demographic markers

## Open Questions the Paper Calls Out

- What thresholds of coreference confidence disparity constitute unacceptable unfairness in practice, and how should stakeholders define these context-dependent bounds?
- Can mitigation strategies be developed that improve both value alignment and validity simultaneously, rather than trading one for the other?
- Can subsampling techniques over demographic markers or questions make intersectional bias evaluation computationally tractable while preserving statistical guarantees for all groups?
- Do these intersectional bias findings generalize beyond the US context to other cultural and sociolinguistic settings?

## Limitations

- US-centric demographic markers and categorizations may not transfer to other cultural contexts
- Binary gender pronouns (fem/masc) limit generalizability to non-binary identities
- Computational cost of 245,700 prompts creates evaluation barriers
- Current mitigation approaches trade fairness for validity or vice versa

## Confidence

- High: Coreference Confidence Disparity effectively captures intersectional bias through uncertainty measurement
- Medium: Findings generalize to other cultural contexts and identity categories
- Medium: Proposed mitigation strategies will balance fairness and reasoning capability

## Next Checks

1. Verify prompt construction pipeline produces correct referent/non-referent augmentation across all 25 demographic markers
2. Validate log probability extraction for multi-token occupations by comparing against manual tokenization
3. Test whether non-demographic augmentations (e.g., "confused," "relaxed") produce smaller confidence disparities than demographic markers