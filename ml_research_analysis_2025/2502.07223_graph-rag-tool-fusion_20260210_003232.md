---
ver: rpa2
title: Graph RAG-Tool Fusion
arxiv_id: '2502.07223'
source_url: https://arxiv.org/abs/2502.07223
tags:
- name
- tool
- tools
- type
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Graph RAG-Tool Fusion introduces a novel approach that combines
  vector-based retrieval with graph traversal to capture structured tool dependencies
  in large-scale tool selection. Unlike traditional RAG methods that miss semantic
  relationships between interdependent tools, this method first retrieves top-k relevant
  tools via vector search, then traverses a predefined knowledge graph to retrieve
  all direct and indirect dependencies.
---

# Graph RAG-Tool Fusion

## Quick Facts
- arXiv ID: 2502.07223
- Source URL: https://arxiv.org/abs/2502.07223
- Authors: Elias Lumer; Pradeep Honaganahalli Basavaraju; Myles Mason; James A. Burke; Vamse Kumar Subbiah
- Reference count: 29
- Primary result: 71.7% mAP@10 improvement on ToolLinkOS benchmark over na誰ve RAG

## Executive Summary
Graph RAG-Tool Fusion introduces a novel approach that combines vector-based retrieval with graph traversal to capture structured tool dependencies in large-scale tool selection. Unlike traditional RAG methods that miss semantic relationships between interdependent tools, this method first retrieves top-k relevant tools via vector search, then traverses a predefined knowledge graph to retrieve all direct and indirect dependencies. We introduce ToolLinkOS, a new benchmark of 573 fictional tools spanning 15 industries, each with an average of 6.3 dependencies. Experiments show Graph RAG-Tool Fusion achieves absolute improvements of 71.7% and 22.1% over na誰ve RAG on ToolLinkOS and ToolSandbox benchmarks respectively (mAP@10), demonstrating significant gains in retrieving tools with complex dependency structures.

## Method Summary
The method employs a two-stage retrieval process: (1) vector search retrieves top-k candidate tools using Azure OpenAI embeddings and HNSW indexing, optionally followed by LLM reranking to reduce truncation errors, and (2) depth-first graph traversal in Neo4j collects all direct and indirect dependencies for each retrieved tool. Tools and dependencies are stored in a knowledge graph with four relationship types (Tool directly depends on, Tool indirectly depends on, Parameter directly depends on, Parameter indirectly depends on). The final output concatenates unique tools in order (primary tool followed by its dependencies, then next primary tool) and truncates to the final top-K limit.

## Key Results
- 71.7% absolute improvement in mAP@10 on ToolLinkOS benchmark (573 fictional tools, avg 6.3 dependencies)
- 22.1% absolute improvement in mAP@10 on ToolSandbox benchmark (33 real API tools)
- Reranking reduces truncation errors by 52% and improves accuracy by 7% on ToolLinkOS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining vector-based similarity search with graph traversal captures tools that are semantically related to a query as well as their functional dependencies, which often lack semantic overlap with the user's intent.
- **Mechanism:** A two-stage retrieval process where the first pass uses vector search to find "entry point" tools based on the query, and the second pass performs a depth-first search (DFS) on a knowledge graph to retrieve all connected dependency nodes (tools required to execute the entry point tool).
- **Core assumption:** Tool dependencies (e.g., `get_wifi_status` required by `get_stock_price`) are structurally related but semantically distant from the user query (e.g., "check Apple stock"), making them unrecoverable by vector similarity alone.
- **Evidence anchors:**
  - [abstract] "...traditional RAG-based tool retrieval fails to capture structured dependencies... limiting the retrieval accuracy..."
  - [section 6.2] "...tool dependencies are often semantically unrelated to the main tool, na誰ve RAG struggles to retrieve all relevant dependencies."
  - [corpus] Related work "Tool Graph Retriever" (arXiv:2508.05152) supports the efficacy of dependency graph-based retrieval over semantic-only methods.
- **Break condition:** If the knowledge graph schema is incomplete (missing edges) or if tools have zero dependencies, the mechanism degrades to standard RAG performance.

### Mechanism 2
- **Claim:** A predefined schema distinguishing between "regular" tools and "core" (utility) tools with specific edge types (e.g., "parameter directly depends on") allows for precise, logical dependency resolution.
- **Mechanism:** Tools are indexed as nodes with specific relationship edges. During retrieval, the system traverses these edges to resolve parameters. For example, resolving a "stock ticker" parameter by traversing a "parameter directly depends on" edge to a "get stock ticker" tool.
- **Core assumption:** Dependencies can be statically pre-defined and categorized (direct vs. indirect) accurately within the graph index before inference.
- **Evidence anchors:**
  - [section 3.1.3] Defines four relationship types (e.g., "Tool directly depends on", "Parameter directly depends on") to model logic.
  - [figure 2] Visualizes the schema where `Get Current Stock Price` connects to `Get Stock Ticker` via a parameter dependency.
  - [corpus] Corpus evidence is supportive but generic; "Agent-as-a-Graph" (arXiv:2511.18194) similarly utilizes graph structures for agent retrieval but doesn't validate this specific schema.
- **Break condition:** If the dependency logic is dynamic (conditional on specific user input values rather than parameter existence), the static schema may retrieve unnecessary tools or fail to retrieve context-specific ones.

### Mechanism 3
- **Claim:** Reranking the initial top-k vector search results minimizes "truncation errors" where relevant tools are pushed out of the final context window.
- **Mechanism:** An LLM-based reranker re-orders the initial $k$ retrieved tools before graph traversal. This ensures that the most semantically relevant tool is processed first (along with its dependency tree), maximizing the probability that it fits within the final Top-K limit.
- **Core assumption:** The primary source of retrieval error (beyond vector failure) is the truncation of the final tool list, and ordering relevance mitigates this.
- **Evidence anchors:**
  - [section 7.1] "reranking reduced truncation errors by 52% and improved accuracy by 7% (ToolLinkOS, mAP@10)."
  - [figure 3] Shows "Retrieved in top-k but not Top 1 (Truncated)" as a major error category for non-reranked runs.
  - [corpus] No specific corpus papers were provided that analyze reranking in the specific context of graph-augmented tool retrieval error analysis.
- **Break condition:** If the initial vector search fails to retrieve the correct tool within the top-k window ($k=3$ in the paper), reranking cannot recover the lost recall.

## Foundational Learning

- **Concept:** **Graph Traversal vs. Vector Search**
  - **Why needed here:** You must understand that vector search finds "what looks similar" while graph traversal finds "what is functionally connected." The paper relies on the latter to find prerequisites that don't look like the user's query.
  - **Quick check question:** If a user asks for "weather in London," would a vector search likely retrieve a `get_current_location` tool? Why or why not?

- **Concept:** **Dependency Injection / Tool Chaining**
  - **Why needed here:** The core value proposition is that LLM agents often need to chain tools (A needs B which needs C). Understanding this hierarchy is essential for building the Knowledge Graph schema.
  - **Quick check question:** In the paper's schema, what is the difference between "Tool directly depends on" and "Parameter directly depends on"?

- **Concept:** **Mean Average Precision (mAP)**
  - **Why needed here:** The paper claims improvements based on mAP@10/20/30. You need to know that this metric penalizes the system if relevant tools appear low in the list or are missing entirely.
  - **Quick check question:** Why does the paper emphasize mAP over simple Recall for evaluating tool retrieval?

## Architecture Onboarding

- **Component map:** Query -> Vector Search (k=3) -> Rerank (optional) -> Graph Traversal (DFS) -> De-duplication/Ordering -> Final Top-K selection
- **Critical path:** Query -> Vector Search ($k$) -> Rerank -> Graph Traversal (DFS) -> De-duplication/Ordering -> Final Top-K selection
- **Design tradeoffs:**
  - **Static vs. Dynamic Graph:** The paper uses a manual/predefined schema (static), which is accurate but maintenance-heavy. A dynamic approach would be flexible but prone to hallucination.
  - **k vs. K limits:** A small $k$ (initial search) limits compute but risks missing the main tool (8.1% error rate in paper). A large final $K$ increases context token cost.
- **Failure signatures:**
  - **High Truncation Error:** The final list cuts off dependencies of the 2nd or 3rd retrieved tool. *Fix:* Increase $K$ or improve reranking.
  - **Missing Prerequisites:** The agent tries to call a tool but lacks a parameter. *Fix:* Verify "Parameter directly depends on" edges in the KG.
  - **Hallucinated Dependencies:** The graph traversal brings in irrelevant tools. *Fix:* Check the schema for overly broad "indirect" dependencies.
- **First 3 experiments:**
  1. **Reproduction:** Replicate the "Na誰ve RAG vs. Graph RAG-Tool Fusion" comparison on the ToolLinkOS subset to validate the 71.7% improvement claim.
  2. **Reranker Ablation:** Run the pipeline with and without the LLM reranker to quantify the specific reduction in truncation errors on your specific tool set.
  3. **Traversal Depth Analysis:** Test how varying the DFS depth limit ($d_{limit}$) affects mAP to determine if deep dependencies are actually necessary or just noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the tool knowledge graph construction process be automated using LLMs?
- **Basis in paper:** [explicit] Section 9 states that the current method requires a manual process to create the tool knowledge graph, but suggests "future work can build on this for automatic LLM-created tool knowledge graphs."
- **Why unresolved:** Automating the extraction of structured dependencies (edges) from unstructured API documentation without manual labeling remains a complex schema-mapping challenge.
- **What evidence would resolve it:** An automated pipeline that generates knowledge graphs from raw API documentation with high fidelity compared to human-annotated ground truth.

### Open Question 2
- **Question:** How should the retrieval system prioritize dependency types (direct vs. indirect) to mitigate truncation errors?
- **Basis in paper:** [explicit] Section 9 notes the system "does not prioritize certain relationships" and suggests future work should prioritize direct dependencies over indirect ones if sub-graphs grow large.
- **Why unresolved:** The current Depth-First Search (DFS) treats all edges similarly; as the number of dependencies scales, retrieved lists may hit the `top-K` limit (truncation) before fetching critical tools.
- **What evidence would resolve it:** A weighted graph traversal algorithm that ranks direct dependencies higher, demonstrating higher mAP scores in high-density dependency scenarios.

### Open Question 3
- **Question:** Does Graph RAG-Tool Fusion maintain its performance advantage when applied to functional, real-world APIs?
- **Basis in paper:** [inferred] Section 4.1 explicitly states that the primary dataset, ToolLinkOS, comprises "fictional tools" that are "not functional," unlike ToolSandbox which has real API responses.
- **Why unresolved:** Fictional tools may have cleaner semantic descriptions and more deterministic dependency structures than noisy, real-world APIs, potentially inflating retrieval scores.
- **What evidence would resolve it:** Benchmarking the method on a large-scale dataset of live, executable APIs where tool descriptions are heterogeneous and noisy.

## Limitations
- Evaluation relies on synthetic benchmarks with artificial tool dependencies, raising questions about external validity
- Static knowledge graph requires manual maintenance and doesn't handle dynamic tool updates
- Fixed depth-first traversal could become computationally expensive for tools with deep dependency chains
- Claims about real-world applicability and maintenance costs are largely speculative

## Confidence
- **High Confidence**: The 71.7% improvement on ToolLinkOS (mAP@10) is well-supported by the experimental methodology and controlled comparisons
- **Medium Confidence**: The 22.1% improvement on ToolSandbox is credible but less robust due to the smaller dataset size (33 tools vs 573)
- **Low Confidence**: Claims about real-world applicability and maintenance costs are largely speculative

## Next Checks
1. **External Validation**: Test Graph RAG-Tool Fusion on a real-world tool repository (e.g., popular Python libraries or API collections) to verify the synthetic benchmark results translate to practical scenarios
2. **Dynamic Dependency Analysis**: Implement a conditional dependency system where edges are resolved based on runtime parameter values, then compare performance against the static schema to quantify the trade-off between accuracy and maintenance overhead
3. **Scalability Assessment**: Measure end-to-end latency and computational cost as graph depth increases beyond the tested limits, identifying the practical depth threshold where the approach becomes infeasible for real-time applications