---
ver: rpa2
title: 'From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory
  Prediction Approaches for Automated Driving'
arxiv_id: '2507.05254'
source_url: https://arxiv.org/abs/2507.05254
tags:
- prediction
- joint
- actor
- predictions
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic comparison of different approaches
  for joint motion prediction in autonomous driving. The authors extend a marginal
  prediction baseline (SIMPL) using three strategies: recombining marginal modes,
  training with scene-level losses, and framing the task as a generative CVAE problem.'
---

# From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving

## Quick Facts
- arXiv ID: 2507.05254
- Source URL: https://arxiv.org/abs/2507.05254
- Reference count: 0
- Primary result: CVAE generative models achieve best positional accuracy (minSFDE: 2.014) while deterministic models with Transformer decoders excel at collision avoidance (actorCR: 0.0085)

## Executive Summary
This paper presents a systematic comparison of three strategies for extending marginal trajectory prediction to joint multi-agent prediction in autonomous driving. The authors evaluate marginal mode recombination, scene-level loss training, and CVAE generative formulations against the SIMPL baseline on the Argoverse 2 dataset. Key findings show that generative CVAE models produce more diverse and realistic multi-modal predictions with superior positional accuracy, while deterministic models with Transformer decoders achieve better collision consistency. The study reveals that architectural modifications are necessary beyond loss changes, with inference times ranging from 16-32ms across approaches.

## Method Summary
The study extends SIMPL's marginal prediction baseline using three strategies: (1) recombining marginal modes via beam search to create joint predictions, (2) training with scene-level losses that aggregate per-agent regression across all agents in a mode, and (3) framing the task as a generative CVAE problem where latent scene variables condition a deterministic decoder. All models use a shared Symmetric Fusion Transformer encoder for actor and map tokens, with different decoder architectures: Multi-MLP for strategy 2, Anchor Transformer for inter-mode communication, and separate prior/posterior/decoder networks for CVAE. Training uses batch size 16, Adam optimizer, and varies epochs between 50-80 with latent dimension 32.

## Key Results
- CVAE models achieve best positional accuracy (minSFDE: 2.014, minSADE: 0.844) but have higher collision rates (actorCR: 0.0153)
- Deterministic Anchor Transformer models excel at collision avoidance (actorCR: 0.0085) with faster inference (19.5ms)
- Marginal recombination baseline shows competitive accuracy but suffers from combinatorial explosion (31.6ms inference)
- Performance is highly sensitive to β regularization in CVAE (0.05 vs. 0.5 shows significant trade-offs)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based decoders with inter-mode attention improve collision consistency in joint predictions.
- Mechanism: Learnable anchor tokens are refined through Transformer-decoder layers using Multi-Head Self Attention to model dependencies between predicted modes, followed by cross-attention with actor tokens.
- Core assumption: Inter-mode communication during decoding enables better coordination across predicted futures than independent mode decoders.
- Evidence anchors:
  - [section IV-B]: "introducing a set of learnable anchors... refined for each agent through multiple Transformer-decoder layers... Each Transformer-decoder layer consists of a Multi-Head Self Attention (MHSA) mechanism to model mode dependencies"
  - [section V-A]: "this model achieves the lowest actorCR among all models, demonstrating the benefit of message propagation between modes"
  - [corpus]: JAM paper (arXiv:2507.17152) also uses classification-aware marginal proposals before joint refinement

### Mechanism 2
- Claim: CVAE generative formulations produce more diverse and realistic multi-modal predictions than deterministic WTA-trained models.
- Mechanism: Latent scene variables capture the full distribution of joint futures. During training, B is sampled from posterior q(B|Y,X,M); during inference, from prior p(B|X,M).
- Core assumption: The latent space can encode meaningful scene-level behavioral variations that factor across agents.
- Evidence anchors:
  - [section V-B]: "The consistent plausibility and diversity of the predicted modes arise from the model's training structure... trained to capture the full distribution of future outcomes"
  - [section V-B]: "deterministic model struggles to capture the possible multi-modality... These issues stem from the WTA loss used during training, which optimizes only the best mode"
  - [corpus]: Mode collapse risk in joint prediction is noted in arXiv:2506.23164

### Mechanism 3
- Claim: Scene-level loss alone is insufficient; architectural modifications are necessary for effective joint prediction.
- Mechanism: Scene-level loss aggregates per-agent regression across all agents in a mode and applies WTA at scene level. Without decoder capacity increases or inter-mode reasoning, the model cannot jointly reason about agent interactions.
- Core assumption: Joint prediction requires both explicit scene-level supervision and architectural capacity to model cross-agent dependencies.
- Evidence anchors:
  - [section V-A]: "applying a scene-level loss to explicitly encourage joint consistency in the predicted modes only reduced the collision rate slightly, while all other metrics deteriorated"
  - [section IV-B]: "when using a single decoder-MLP... it would be forced to update its weights to improve the performance on all predictions, resulting in some form of weight-averaging"

## Foundational Learning

- Concept: **Conditional Variational Autoencoder (CVAE)**
  - Why needed here: The generative approach hinges on understanding how encoder-decoder architectures with latent variables enable sampling-based multi-modal prediction.
  - Quick check question: Can you explain why the prior network is used at inference time but the posterior is used during training?

- Concept: **Winner-Takes-All (WTA) Loss**
  - Why needed here: The paper's critique of deterministic models centers on WTA leaving K-1 modes unpenalized.
  - Quick check question: In a 6-mode predictor, how many modes receive gradient updates under standard WTA?

- Concept: **Symmetric Fusion Transformer (SFT)**
  - Why needed here: All evaluated models build on SIMPL's SFT layers for encoding actor and map tokens.
  - Quick check question: How does SFT's relative positional encoding differ from absolute positional encodings in standard Transformers?

## Architecture Onboarding

- Component map: Input X, M → Actor Encoder + Map Encoder → SFT Layers → Three decoder branches → Output Ŷ, α
- Critical path: For CVAE models, the prior network must produce meaningful latent distributions. Check that μ_prior and σ²_prior have reasonable magnitudes.
- Design tradeoffs:
  - Deterministic (Anchor Transformer): Best collision rate (actorCR: 0.0085), faster inference (19.5ms), but implausible non-ground-truth modes common
  - Generative (CVAE): Best positional accuracy (minSFDE: 2.014), realistic diverse modes, but sensitive to β tuning
  - Marginal recombination: No retraining needed, but slowest inference (31.6ms) due to O(K^Na) combinations
- Failure signatures:
  - High actorCR with good minSFDE → likely sampling unrealistic latent combinations
  - Low diversity across modes → β too large, causing posterior collapse toward prior
  - Modes with vehicles "driving backwards" → deterministic model with WTA not penalizing implausible non-best modes
- First 3 experiments:
  1. Reproduce marginal recombination baseline on Argoverse 2 validation set
  2. Ablate β ∈ {0.01, 0.05, 0.1, 0.5} for CVAE model
  3. Compare Multi-MLP vs. Anchor Transformer decoders with fixed training epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would replacing actor-specific latent tokens with a shared latent scene variable in the CVAE architecture improve joint prediction consistency?
- Basis in paper: [explicit] Authors state in conclusion: "Potential future work includes... exploring the adaptation of the CVAE-based approach to employ a shared latent scene variable instead of grouped actor-specific latent tokens."
- Why unresolved: The current CVAE uses actor-anchored latent tokens that may not fully capture inter-agent dependencies at the latent level.
- What evidence would resolve it: Ablation comparing actor-specific vs. shared latent variables on joint consistency metrics and positional accuracy.

### Open Question 2
- Question: Can the complementary strengths of deterministic and generative approaches be combined to achieve both high positional accuracy and low collision rates?
- Basis in paper: [inferred] Results show CVAE achieves best positional accuracy while Transformer decoders excel at collision avoidance.
- Why unresolved: The paper evaluates approaches independently; no hybrid architecture was tested.
- What evidence would resolve it: Experiments with ensemble or hierarchical models combining generative sampling with deterministic refinement stages.

### Open Question 3
- Question: What is the optimal β regularization strategy that balances diversity, realism, and positional accuracy across different traffic densities?
- Basis in paper: [inferred] Performance is highly sensitive to β (0.05 vs. 0.5), with smaller β improving diversity but increasing collision rates.
- Why unresolved: Only two discrete β values were tested; the trade-off curve and its interaction with scene complexity remain unexplored.
- What evidence would resolve it: Systematic β ablation across different traffic densities with analysis of mode diversity metrics.

## Limitations
- The study is limited to the Argoverse 2 dataset with specific model capacities
- CVAE sensitivity to β regularization suggests hyperparameter tuning is critical for real-world deployment
- The deterministic Anchor Transformer model's collision avoidance strength may not generalize across different interaction densities
- Marginal recombination baseline suffers from combinatorial explosion making it impractical for scenes with many agents

## Confidence

- **High Confidence**: CVAE models achieving best positional accuracy (minSFDE: 2.014), deterministic models excelling at collision avoidance (actorCR: 0.0085)
- **Medium Confidence**: Architectural modifications being necessary beyond loss changes, inference time scaling predictions
- **Low Confidence**: Generalization of collision rate improvements across datasets, real-world impact of mode diversity beyond Argoverse 2

## Next Checks

1. **Generalization Test**: Evaluate the three best-performing models on a separate dataset like nuScenes or Waymo Open Motion Dataset to assess cross-dataset performance consistency

2. **Latent Space Analysis**: For CVAE models, visualize the latent space B distributions across different traffic scenarios to verify the learned scene variables capture meaningful behavioral variations

3. **Real-time Feasibility**: Implement the inference-time agent scaling model (γa = 0.684 ms/agent) on actual hardware and measure whether the predicted latency bounds hold under concurrent processing loads