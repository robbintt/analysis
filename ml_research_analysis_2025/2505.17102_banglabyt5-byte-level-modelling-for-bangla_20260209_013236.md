---
ver: rpa2
title: 'BanglaByT5: Byte-Level Modelling for Bangla'
arxiv_id: '2505.17102'
source_url: https://arxiv.org/abs/2505.17102
tags:
- banglabyt5
- bangla
- language
- linguistics
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BanglaByT5, the first byte-level encoder-decoder
  model for Bangla. Built on a small variant of Google's ByT5 architecture and trained
  on a 14GB corpus, it outperforms similar-sized models on both classification and
  generative tasks.
---

# BanglaByT5: Byte-Level Modelling for Bangla

## Quick Facts
- **arXiv ID:** 2505.17102
- **Source URL:** https://arxiv.org/abs/2505.17102
- **Reference count:** 18
- **Primary result:** Byte-level encoder-decoder outperforms subword-tokenized models on Bangla classification and generative tasks, including zero-shot generation quality comparable to 5x larger models.

## Executive Summary
This paper introduces BanglaByT5, the first byte-level encoder-decoder model for Bangla, built on a small ByT5 architecture and trained on a 14GB monolingual corpus. By operating directly on UTF-8 bytes rather than subword tokens, the model avoids morphological fragmentation inherent in BPE/SentencePiece tokenizers for Bangla. The byte-level span corruption denoising objective forces the model to learn compositional representations without language-specific segmentation. In evaluations, BanglaByT5 matches or exceeds similar-sized baselines on multiple tasks and achieves zero-shot generation quality comparable to models 5x larger, while also surpassing larger multilingual models on classification benchmarks.

## Method Summary
BanglaByT5 is a 300M parameter encoder-decoder model based on ByT5-small architecture (12 layers, 6 attention heads, 1472 hidden dim, 3584 FFN) with gated-GELU activations. It uses a custom byte-level tokenizer with 384 vocabulary (256 base bytes + 100 special + 28 reserved) trained on 14GB of curated Bangla data (Vācaspati literary + IndicCorp news corpora, 947M words). Pre-training employs byte-level span corruption denoising with 3M steps (batch 16, grad accum 2, lr 3e-5, warmup 500, cosine decay). Downstream fine-tuning covers classification (sentiment, NER) and generative tasks (MT, paraphrasing, GEC) using encoder-decoder format with task-specific prompts.

## Key Results
- Outperforms similar-sized models on both classification and generative tasks in supervised fine-tuning
- Achieves zero-shot generation quality comparable to GPT2-XL (5x larger) on curated prompts
- Surpasses models twice its size on multiple benchmarks while matching or exceeding larger multilingual models on classification
- Demonstrates intermediate fertility score (7.96) compared to alternatives, suggesting optimal token granularity for Bangla

## Why This Works (Mechanism)

### Mechanism 1: Tokenization-Free Representation Eliminates Morphological Fragmentation
Operating directly on raw bytes avoids inconsistent word fragmentation that BPE/SentencePiece introduce in morphologically rich languages like Bangla. UTF-8 bytes provide a fixed, language-agnostic vocabulary, and the model learns to compose morphemes implicitly through attention rather than relying on predefined subword boundaries that may split affixes from roots.

### Mechanism 2: Span Corruption at Byte Level Forces Composition Learning
Pre-training with byte-level span corruption compels the model to reconstruct continuous byte sequences, building robust internal representations without language-specific tokenization. Random byte spans are masked and replaced with sentinel tokens; the decoder must regenerate original bytes, requiring learning of character-to-morpheme-to-word composition hierarchies.

### Mechanism 3: Curated Monolingual Corpus Concentrates Signal-to-Noise for Low-Resource Setting
Combining high-quality literary and newspaper sources yields cleaner training signal than web-scraped multilingual corpora. Homogeneous monolingual data reduces cross-language interference and vocabulary dilution, while quality-controlled sources minimize noise that would otherwise require larger model capacity to filter.

## Foundational Learning

- **Concept: Byte-level modeling vs. subword tokenization**
  - Why needed here: BanglaByT5's core innovation is abandoning BPE/SentencePiece. Understanding why subword methods fragment morphologically rich languages is prerequisite to grasping the design rationale.
  - Quick check question: Given the Bengali word-formation pattern, would BPE likely split a common inflected verb into meaningful subwords or arbitrary fragments?

- **Concept: Encoder-decoder architectures with span corruption**
  - Why needed here: The model follows T5's text-to-text framework. Without understanding encoder-decoder separation and denoising objectives, the pre-training mechanism will be opaque.
  - Quick check question: How does span corruption differ from masked language modeling (MLM) in BERT, and why does it suit generation tasks better?

- **Concept: Fertility score and tokenization efficiency**
  - Why needed here: Section 3.3 reports fertility as a key diagnostic. Interpreting this metric is essential for evaluating tokenizer quality across architectures.
  - Quick check question: A fertility score of 7.96 means approximately how many tokens per word, and why is neither very high nor very low desirable?

## Architecture Onboarding

- **Component map:** Raw text → UTF-8 bytes → tokenizer encoding (max 512 bytes) → encoder → decoder with cross-attention → reconstructed spans → loss computation
- **Critical path:** 1) Raw text → UTF-8 bytes → tokenizer encoding (max 512 bytes) 2) Encoder processes full corrupted sequence 3) Decoder generates reconstructed spans autoregressively 4) Loss computed only on target spans (not encoder input)
- **Design tradeoffs:** Sequence length vs. context: Bytes inflate sequences ~4x vs. subwords for Bangla; 512-byte window ≈ 5 sentences limits long-document tasks. Model size vs. data: 300M parameters chosen to match 14GB corpus; larger variant would risk overfitting without more data. Monolingual focus vs. cross-lingual transfer: No multilingual pre-training means stronger Bangla performance but zero cross-lingual capability.
- **Failure signatures:** Excessive repetition or truncated outputs: May indicate context window exhaustion or decoding hyperparameters misconfigured. Poor morphological agreement in generation: Suggests insufficient pre-training steps or corpus lacks morphological diversity. High latency on CPU: Byte sequences are longer; batch size tuning critical.
- **First 3 experiments:** 1) Reproduce zero-shot generation evaluation: Run 2000 curated prompts through BanglaByT5 and baselines, score with LLaMA-3.1-8B/Mistral-7B using Fluency/Coherence/Relevance/Creativity rubrics. 2) Ablate tokenizer fertility: Train variant with standard Google-byt5-small tokenizer on same corpus, measure delta on MT and GEC tasks. 3) Stress-test context limits: Evaluate on tasks requiring >5 sentences of context to characterize 512-byte window bottleneck.

## Open Questions the Paper Calls Out

- **Scaling to larger variants:** Can performance gains be effectively scaled to larger model variants (e.g., Base or Large) given the scarcity of high-quality Bangla training data? The authors were unable to pre-train larger variants due to data constraints, leaving scaling laws untested.

- **Context window bottleneck:** Does the byte-level "fertility" score (7.96) create an effective context window bottleneck compared to subword models? The paper doesn't evaluate tasks requiring long-range dependencies that might exceed the compressed byte-level context window.

- **Evaluation bias:** To what extent does the "LLM-as-a-Judge" methodology using LLaMA-3.1 and Mistral introduce evaluation bias against the specific linguistic nuances of Bangla? The alignment between these LLM judges and human perception of Bangla generation quality remains unverified.

## Limitations

- Small 14GB training corpus relative to typical pre-training scales may not represent colloquial or technical Bangla domains
- Unspecified span corruption hyperparameters prevent exact reproduction
- 512-byte context window constrains performance on longer-form tasks requiring >5 sentences
- No multilingual capability demonstrated; benefits don't extend to cross-lingual scenarios
- LLM-as-judge evaluation introduces subjectivity and potential bias depending on judge model

## Confidence

- **High confidence:** Architecture specification, training pipeline, and downstream datasets are clearly described and reproducible. Zero-shot generation claims are well-documented and benchmarked.
- **Medium confidence:** Superiority of byte-level modeling is supported by fertility comparisons but lacks ablation against other segmentation approaches. Claims about curated corpus quality are reasonable but not empirically validated.
- **Low confidence:** Exact span corruption hyperparameters are missing, preventing exact replication. LLM-as-judge evaluation introduces variability; reported scores may shift with different judge models.

## Next Checks

1. **Ablation study on tokenizer fertility:** Train two variants—one with BanglaByT5's byte-level tokenizer (fertility ~7.96) and one with Google-byt5-small's tokenizer (fertility ~15.02)—on identical 14GB corpus. Fine-tune both on MT and GEC tasks and measure performance delta.

2. **Cross-judge reliability test for zero-shot generation:** Rerun 2000 curated prompts through BanglaByT5 and baselines. Score outputs with two different LLM judges (e.g., LLaMA-3.1-8B and Mistral-7B) using the same rubric. Compare score distributions to assess robustness.

3. **Context window stress test:** Design benchmark of Bangla tasks requiring >5 sentences of context (e.g., multi-sentence summarization). Evaluate BanglaByT5's performance against subword-tokenized baseline as context requirements increase. Measure point where 512-byte window limits performance.