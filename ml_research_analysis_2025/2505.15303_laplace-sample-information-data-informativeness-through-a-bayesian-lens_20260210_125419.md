---
ver: rpa2
title: 'Laplace Sample Information: Data Informativeness Through a Bayesian Lens'
arxiv_id: '2505.15303'
source_url: https://arxiv.org/abs/2505.15303
tags:
- samples
- information
- sample
- high
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Laplace Sample Information (LSI), a novel
  measure of per-sample informativeness in deep learning that leverages Bayesian approximations
  and KL divergence to assess the unique information contribution of individual training
  samples to neural network parameters. LSI overcomes computational limitations of
  existing methods by using a Laplace approximation to construct a quasi-Bayesian
  learner post-training, making it applicable across diverse model architectures and
  learning settings.
---

# Laplace Sample Information: Data Informativeness Through a Bayesian Lens

## Quick Facts
- arXiv ID: 2505.15303
- Source URL: https://arxiv.org/abs/2505.15303
- Reference count: 40
- One-line primary result: Laplace Sample Information (LSI) provides a scalable, theoretically grounded measure of per-sample informativeness using Bayesian approximations that transfers from small probe models to large architectures.

## Executive Summary
This paper introduces Laplace Sample Information (LSI), a novel measure of per-sample informativeness in deep learning that leverages Bayesian approximations and KL divergence to assess the unique information contribution of individual training samples to neural network parameters. LSI overcomes computational limitations of existing methods by using a Laplace approximation to construct a quasi-Bayesian learner post-training, making it applicable across diverse model architectures and learning settings. Experiments on image and text datasets demonstrate LSI's effectiveness in ordering samples by typicality, detecting mislabeled data, measuring class-wise informativeness, and assessing dataset difficulty.

## Method Summary
The method applies a Laplace approximation (second-order Taylor expansion) around optimized weights to construct a Gaussian posterior distribution for deterministic models. LSI is defined as the KL divergence between parameter distributions trained on full vs. leave-one-out datasets, measuring each sample's unique contribution. To address computational bottlenecks, the paper demonstrates that LSI computed on small probe models (trained on frozen feature embeddings) transfers effectively to larger architectures, enabling efficient computation with up to three orders of magnitude speedup. The approach uses diagonal Hessian approximation for scalability while maintaining strong correlation with full Hessian results.

## Key Results
- LSI computed via small probe models transfers well to larger architectures, enabling efficient computation with up to three orders of magnitude speedup
- LSI strongly correlates with point-wise sliced mutual information and outperforms influence-based approaches in detecting out-of-distribution samples
- LSI effectively detects mislabeled data, measures class-wise informativeness, and assesses dataset difficulty across multiple domains
- Gradient clipping reduces per-sample informativeness variance, demonstrating connections to differential privacy

## Why This Works (Mechanism)

### Mechanism 1: Post-Hoc Bayesian Distribution Construction
If the loss landscape of a trained neural network is approximately convex near the local minimum, one can approximate the posterior distribution of parameters as Gaussian, enabling information-theoretic measures on deterministic models. The method applies Laplace approximation around optimized weights, modeling parameters as N(θ̂, Σ) where covariance is inverse Hessian of loss. This converts point estimates into distributions based on local curvature.

### Mechanism 2: KL Divergence as Unique Sample Contribution
If a training sample is highly informative, removing it should cause measurable shift in parameter distribution, quantified by KL divergence. LSI is defined as KL divergence between parameter distributions trained on full dataset vs. dataset with sample removed. High LSI indicates significant distribution shift, implying unique information content.

### Mechanism 3: Probe-Based Transferability
If a small probe model trained on same data as larger model maintains consistent relative ordering of sample informativeness, efficient computation becomes possible. Instead of retraining entire architecture for every leave-one-out instance, a lightweight probe is trained. The LSI ranking calculated on probe correlates strongly with ranking on full model.

## Foundational Learning

- **Laplace Approximation**: Why needed: Converts deterministic weights into probability distribution for information-theoretic analysis. Quick check: How does loss landscape curvature relate to posterior variance in Laplace approximation?
- **Kullback-Leibler (KL) Divergence**: Why needed: LSI is explicitly defined as KL divergence between posterior distributions. Quick check: Why is KL asymmetric, and why measure KL(p(θ|D) || p(θ|D_{-i})) rather than reverse?
- **Leave-One-Out (LOO) Analysis**: Why needed: Theoretical grounding depends on algorithmic stability - how much output changes when single data point is removed. Quick check: What is computational bottleneck of naive LOO, and how does paper approximate it?

## Architecture Onboarding

- **Component map**: Frozen ResNet-18 feature extractor -> Probe model (Linear + MLP) -> Hessian Approximator -> LSI Calculator
- **Critical path**: 1) Run all data through frozen backbone, 2) Train probe on full feature set, 3) Compute diagonal Hessian at optimum, 4) Iterate through samples retraining probe on LOO sets, 5) Compute LSI using distribution differences
- **Design tradeoffs**: Diagonal vs. Full Hessian (efficiency vs. parameter correlation), Probe vs. Full Model (speed vs. fidelity)
- **Failure signatures**: Non-convergence (ill-conditioned Hessian), Data leakage (backbone contamination), Numerical instability (flat landscapes)
- **First 3 experiments**: 1) Validate diagonal vs. exact Hessian approximation correlation, 2) Detect mislabeled samples with corrupted labels, 3) Transfer LSI ordering from probe to full ResNet

## Open Questions the Paper Calls Out

### Open Question 1
Can LSI computed via efficient Hessian approximations (e.g., K-FAC) scale effectively to LLM-scale workflows, and does probe-based ordering transfer to billion-parameter models? Basis: Paper anticipates transfer to LLM-scale workflows but doesn't evaluate. Evidence needed: Experiments computing LSI on LLM training runs using efficient approximations with correlation analysis.

### Open Question 2
Why does adding Gaussian noise in DP-SGD have no observable effect on LSI distribution while gradient clipping reduces both magnitude and variance? Basis: DP experiments show asymmetric effect without theoretical explanation. Evidence needed: Theoretical analysis connecting noise mechanisms to KL divergence plus controlled experiments.

### Open Question 3
What conditions cause diagonal Hessian approximation to break down in LSI computation, and what is theoretical relationship between approximation quality and sample ordering accuracy? Basis: Paper shows strong correlation but doesn't characterize failure modes. Evidence needed: Systematic experiments on diverse architectures with comparison to full Hessian.

## Limitations
- Laplace approximation assumes local convexity that may not hold for highly non-convex deep networks
- Diagonal Hessian approximation ignores parameter correlations, potentially underestimating uncertainty
- Probe-to-full-model transferability relies on feature extractor capturing sufficient task-relevant geometry

## Confidence

- **High Confidence**: LSI's mathematical formulation via KL divergence and Laplace approximation; empirical validation on mislabeling detection and class-wise informativeness
- **Medium Confidence**: Probe-to-full-model transferability; connection to differential privacy and gradient clipping effects
- **Low Confidence**: Generalizability to extremely large-scale datasets and models; behavior in highly non-convex optimization regimes

## Next Checks

1. Test LSI computation on a singular model (e.g., ReLU network with low-rank parameterization) to verify effective dimension assumption
2. Validate probe transferability across different backbone architectures (ViT vs ResNet) on same dataset
3. Quantify LSI sensitivity to hyperparameter choices (learning rate, weight decay) in probe training stage