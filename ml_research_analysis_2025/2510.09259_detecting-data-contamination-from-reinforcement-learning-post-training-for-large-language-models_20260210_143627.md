---
ver: rpa2
title: Detecting Data Contamination from Reinforcement Learning Post-training for
  Large Language Models
arxiv_id: '2510.09259'
source_url: https://arxiv.org/abs/2510.09259
tags:
- contamination
- arxiv
- data
- self-critique
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of detecting data contamination\
  \ in the Reinforcement Learning (RL) post-training phase of Large Language Models\
  \ (LLMs), a critical vulnerability that existing methods designed for pre-training\
  \ and Supervised Fine-Tuning fail to handle due to RL's reward-driven, likelihood-decoupled\
  \ objective. The core method, Self-Critique, detects RL-induced policy collapse\
  \ by actively probing the model\u2019s reasoning path dependencies."
---

# Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models

## Quick Facts
- arXiv ID: 2510.09259
- Source URL: https://arxiv.org/abs/2510.09259
- Reference count: 39
- Primary result: Self-Critique method achieves up to 30% AUC improvement over baselines for detecting RL-phase data contamination in LLMs

## Executive Summary
This paper addresses the critical challenge of detecting data contamination in the Reinforcement Learning (RL) post-training phase of Large Language Models (LLMs). Existing methods designed for pre-training and Supervised Fine-Tuning fail because RL's reward-driven, likelihood-decoupled objective fundamentally changes the optimization landscape. The authors introduce Self-Critique, a novel detection method that probes for policy collapse by comparing entropy sequences between initial responses and self-critique responses. Experiments on the newly constructed RL-MIA benchmark demonstrate significant improvements over baseline methods, with Self-Critique achieving up to 30% AUC improvement.

## Method Summary
Self-Critique detects RL-induced contamination by actively probing the model's reasoning path dependencies. The method generates two responses for each test sample: an initial response using greedy decoding, and a self-critique response where the model is instructed to provide an alternative solution while conditioning on the initial response. Token-level entropy sequences are computed for both responses, and a penalized cosine similarity is calculated. High similarity indicates contamination—the model cannot deviate from its memorized reasoning path despite being instructed to change. The approach leverages the observation that RL optimization causes policy collapse on contaminated samples, creating detectable path dependence patterns.

## Key Results
- Self-Critique achieves up to 30% AUC improvement over baseline detectors on RL-MIA benchmark
- Performance improves by up to 55% in dual-contamination scenarios when pretraining contamination is reduced
- The method is robust across different RL algorithms (PPO, GRPO, DAPO) and model scales (0.5B to 7B parameters)
- Likelihood-based baselines (PPL, Min-K%, etc.) perform near chance level (~0.5 AUC) on RL-phase contamination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RL post-training induces policy collapse on contaminated samples, creating detectable path dependence that persists even when the model is instructed to change its reasoning.
- **Mechanism:** RL optimizes for reward rather than likelihood, causing the policy to converge to narrow, highly-rewarded reasoning trajectories. When contaminated samples receive consistent reward signals, entropy sequences collapse into sparse patterns. The self-critique probe exposes this rigidity: contaminated samples exhibit high entropy similarity between original and critique responses because the model cannot deviate from its memorized path.
- **Core assumption:** Policy collapse manifests more strongly on rewarded (contaminated) samples than clean samples, even though both may show sparse entropy patterns.
- **Evidence anchors:**
  - [abstract] "Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction."
  - [Section 3.3] "Empirical observations show that RL tends to push entropy sequences into sparse patterns, where many tokens are nearly deterministic. Crucially, this collapse is stronger for contaminated samples that were explicitly rewarded during RL training."
  - [corpus] Weak/no direct corpus support for RL-specific entropy collapse patterns; related work (e.g., "The Impact of Post-training on Data Contamination") studies contamination broadly but not entropy-based detection.

### Mechanism 2
- **Claim:** Likelihood-based detection signals (perplexity, token probability) become unreliable for RL-phase contamination because RL's objective function decouples from token-level likelihood maximization.
- **Mechanism:** Pre-training and SFT use MLE objectives that directly maximize probability of training sequences, creating strong memorization signals. RL (especially RLVR) optimizes expected reward from generated outputs via sparse external signals (e.g., correct/incorrect answer). This decoupling means contaminated samples from RL training do not necessarily show unusually low perplexity or high token probability.
- **Core assumption:** The policy gradient updates from reward-driven optimization do not create the same probability-density footprints as likelihood-based training.
- **Evidence anchors:**
  - [abstract] "RL's reward-driven, likelihood-decoupled objective" makes existing methods "designed for pre-training and Supervised Fine-Tuning...fail to handle."
  - [Section 3.2] Equations 1-3 formalize the distinction: pre-training/SFT maximize log-likelihood; RL maximizes expected reward function f(R(o_i), π_θ).
  - [corpus] Survey work ("A Survey on Data Contamination for Large Language Models") confirms contamination detection is largely studied for pre-training/SFT stages, supporting the research gap claim.

### Mechanism 3
- **Claim:** Self-critique probing (conditioning on the initial response when requesting an alternative) is necessary to create comparable entropy trajectories that reveal path dependence.
- **Mechanism:** Simply instructing "use an unconventional approach" without the initial response anchor produces unconstrained, heterogeneous trajectories for both members and non-members. By conditioning the critique request on the specific initial response, the probe fixes a baseline reasoning path and measures deviation from it. Contaminated samples struggle to deviate, producing similar entropy sequences; clean samples can diverge, producing dissimilar entropy sequences.
- **Core assumption:** The instruction to provide an alternative solution is sufficient to expose memorization; models with genuine reasoning capability can genuinely take alternative paths.
- **Evidence anchors:**
  - [Section 4.1] Step 2: "q′ = q ⊕ I_critique(r_1)" where the critique prompt explicitly includes the initial response.
  - [Appendix C.1] Ablation shows removing the initial response anchor causes AUC to collapse from ~0.70 to 0.06-0.28 across datasets—"performance collapses to near random guess."
  - [corpus] No corpus papers specifically test self-critique as a probing mechanism for contamination.

## Foundational Learning

- **Concept:** Token-level entropy in language models (H_t = -Σ p_θ(v|x_{<t}) log p_θ(v|x_{<t}))
  - **Why needed here:** The entire Self-Critique method operates on entropy sequences; understanding how entropy reflects model uncertainty and policy sharpness is essential.
  - **Quick check question:** Given a vocabulary of 50K tokens, if a model assigns probability 0.99 to one token, how would the entropy compare to a distribution where top-5 tokens each have ~0.2 probability?

- **Concept:** Reinforcement Learning with Verifiable Rewards (RLVR)
  - **Why needed here:** RLVR is the specific post-training paradigm studied; understanding reward-based policy optimization vs. likelihood-based training explains why existing detectors fail.
  - **Quick check question:** In RLVR for math problems, what signal typically drives the policy update—token-by-token correctness or final-answer correctness?

- **Concept:** Membership Inference Attacks (MIA) for LLMs
  - **Why needed here:** Contamination detection is framed as an MIA problem; understanding the black-box setting and AUC evaluation metric is prerequisite.
  - **Quick check question:** Why does AUC=0.5 indicate random guessing, and what does AUC=0.70 practically mean for a detector's ranking ability?

## Architecture Onboarding

- **Component map:** Input problem q → Chat template T(q) → Initial response r₁ (greedy) → Entropy sequence E₁ → Self-critique prompt (q + I_critique(r₁)) → Alternative response r₂ → Entropy sequence E₂ → Penalized cosine similarity → Contamination score

- **Critical path:**
  1. Construct RL-MIA benchmark: Inject known subset into RL training corpus (e.g., 15 AIME items into OpenR1-Math-46K)
  2. Train model with RL algorithm (PPO/GRPO/DAPO) on contaminated corpus
  3. For each test problem, run Self-Critique: generate r₁, compute E₁, generate r₂, compute E₂
  4. Compute similarity score; evaluate AUC against ground-truth membership labels

- **Design tradeoffs:**
  - Greedy vs. temperature sampling: Greedy for both responses yields best AUC (Figure 5); temperature introduces noise that obscures entropy collapse signal
  - Top-K entropy approximation: K=3 achieves near-identical AUC to K=50 (Table 4) due to long-tailed token distributions; enables API-only deployment
  - Length penalty in similarity: Multiplicative penalty (min/max) prevents gaming via response length manipulation

- **Failure signatures:**
  - High variance across random seeds on synthetic datasets (Qwen2.5-7B-Math on K&K/SAT noted as unstable)
  - Likelihood-based baselines near 0.5 AUC confirms RL-phase detection is fundamentally different from pre-training detection
  - Dual-contamination scenarios (pre-training + RL) require filtering by pretraining contamination proxy first (Figure 3)

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run PPL, Min-K%, Min-K%++, Recall, CDD on RL-MIA benchmark with Qwen2.5-7B-Instruct; verify Self-Critique achieves 0.70 AUC vs. ~0.50 baseline average
  2. **Ablate probing mechanism:** Compare Self-Critique (with initial response) vs. "unconventional reasoning" prompt (without initial response); confirm ~0.4+ AUC gap
  3. **Test Top-K approximation:** Run entropy computation with K∈{3,5,10,20,50} on AIME25 and K&K; verify variance < 4×10⁻⁵ in AUC

## Open Questions the Paper Calls Out

- **Question:** How does the "policy collapse" signature and Self-Critique detection performance vary in domains with high solution diversity, such as code generation, compared to the mathematical and logical reasoning tasks tested?
  - **Basis in paper:** [explicit] The authors explicitly state in Appendix G (Limitations) that code generation involves "a wider diversity of valid solutions" where "the signature of policy collapse might manifest differently," warranting domain-specific adaptations.
  - **Why unresolved:** The current RL-MIA benchmark and experiments are restricted to math (AIME) and synthetic logic (K&K, SAT), leaving the method's efficacy on syntactically strict but semantically diverse tasks like code unverified.
  - **What evidence would resolve it:** Extending the RL-MIA benchmark to code generation datasets (e.g., HumanEval, MBPP) and evaluating Self-Critique's AUC performance against baseline detectors in that specific domain.

- **Question:** Do the dynamics of RL-induced policy collapse and the associated entropy patterns remain consistent enough for detection in frontier-scale models (exceeding hundreds of billions of parameters)?
  - **Basis in paper:** [explicit] Appendix G notes that while the study covers 0.5B to 7B parameter models, "the dynamics of policy collapse and memorization at such scales [frontier models] are not yet fully understood."
  - **Why unresolved:** The memorization and generalization dynamics of massive models often differ qualitatively from smaller ones, and the sensitivity of the entropy-based similarity metric may behave differently in high-dimensional embedding spaces of frontier models.
  - **What evidence would resolve it:** Applying the Self-Critique method to large-scale open-source models (e.g., Llama 3 70B+) or proprietary APIs (if accessible) to verify if the entropy similarity signal remains distinct from noise.

- **Question:** Can entropy-based probing (Self-Critique) be unified with likelihood-based metrics to create a single detector robust to simultaneous contamination in both pre-training and RL post-training phases?
  - **Basis in paper:** [inferred] Section 5.4 demonstrates that Self-Critique performance improves by up to 55% when pretraining contamination is reduced (filtered), suggesting the method struggles to isolate RL signals when strong pretraining signals (likelihood) are present.
  - **Why unresolved:** The paper currently treats pre-training and RL detection as separate problems, yet real-world models often suffer from dual-stage contamination; a method is needed to disentangle these overlapping signals without requiring manual subset filtering.
  - **What evidence would resolve it:** A study proposing a composite score or multi-head detector that jointly analyzes perplexity (pre-training signal) and entropy similarity (RL signal) and validating it on datasets known to be contaminated in both stages.

## Limitations

- The RL-MIA benchmark construction relies on synthetic contamination injection, which may not capture real-world contamination dynamics
- The method shows high variance across random seeds on synthetic datasets (Qwen2.5-7B-Math on K&K/SAT noted as unstable)
- Performance on code generation tasks with high solution diversity remains unverified and may require domain-specific adaptations

## Confidence

- **High Confidence:** The empirical observation that likelihood-based baselines (PPL, Min-K%, etc.) perform near chance level (~0.5 AUC) on RL-phase contamination data is well-supported and directly validates the paper's core premise about RL's likelihood-decoupled objective.
- **Medium Confidence:** The mechanism explaining why entropy similarity detects contamination (policy collapse creating path dependence) is theoretically sound but lacks direct experimental validation that this collapse is specifically stronger on contaminated samples versus clean samples with similar reward structures.
- **Medium Confidence:** The claim that Self-Critique achieves "up to 30% AUC improvement" requires careful interpretation—this is relative to baselines that are essentially random, making the absolute improvement more modest.

## Next Checks

1. **Ecological Validity Test:** Apply Self-Critique to real-world contaminated models where ground truth contamination is known through watermarking or logging, rather than relying solely on synthetic RL-MIA benchmark injections.

2. **Mechanism Isolation Experiment:** Design a controlled experiment where clean samples are exposed to strong reward signals during RL training without memorization, to determine whether entropy similarity detects contamination specifically or reward-induced policy rigidity more generally.

3. **Cross-Architecture Generalization:** Test Self-Critique across different model families (not just Qwen2.5 and DeepSeek-Math) and architectures to verify the entropy similarity signal is not model-specific but reflects a fundamental property of RL-induced policy collapse.