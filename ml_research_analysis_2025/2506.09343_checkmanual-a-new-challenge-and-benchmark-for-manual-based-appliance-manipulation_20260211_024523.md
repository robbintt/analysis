---
ver: rpa2
title: 'CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation'
arxiv_id: '2506.09343'
source_url: https://arxiv.org/abs/2506.09343
tags:
- manipulation
- appliance
- manual
- part
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CheckManual, the first benchmark for manual-based
  appliance manipulation, to address the gap in robot manipulation of appliances using
  manuals. Existing manipulation models lack the ability to read multi-page manuals
  and comprehend their content for complex tasks.
---

# CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation

## Quick Facts
- **arXiv ID:** 2506.09343
- **Source URL:** https://arxiv.org/abs/2506.09343
- **Reference count:** 39
- **Primary result:** First benchmark for manual-based appliance manipulation with 1107 manuals for 369 appliances across 11 categories, evaluating robot ability to read manuals and execute complex manipulation tasks.

## Executive Summary
This paper introduces CheckManual, the first benchmark addressing the challenge of robot manipulation of household appliances using user manuals. The authors identify a critical gap in current manipulation systems: the inability to read multi-page manuals and translate their content into actionable manipulation plans for complex appliances. To address this, they create a dataset of 1107 appliance manuals covering 369 appliances across 11 categories, generated through a large model-assisted human-revised pipeline from CAD models. The benchmark proposes three challenge tracks: aligned planning, CAD-assisted manipulation, and pure manual-based manipulation. The authors introduce ManualPlan, a novel planning model that processes manuals, generates task plans, and aligns manual parts to physical appliances using visual prompting techniques.

## Method Summary
The authors propose a multi-stage pipeline for manual-based appliance manipulation. First, they generate synthetic appliance manuals using a large model-assisted human-revised pipeline: Multimodal Large Language Models annotate CAD parts with functional names and states, Large Language Models generate manipulation tasks and compile them into PDF manuals via LaTeX code, with human verification correcting semantic errors. The ManualPlan model then processes these manuals through three modules: Manual Resolution (OCR + MLLM for PDF parsing), Manipulation Planning (LLM-based step planning), and Part Alignment (SoM visual prompting with Grounding-DINO + SAM). The benchmark evaluates performance across three tracks: aligned planning (planning-only), CAD-assisted manipulation (using kinematic primitives), and pure manual-based manipulation (using learned policies like VoxPoser), with success metrics for alignment, planning, and task execution.

## Key Results
- ManualPlan can generate partial correct task plans based on multi-page manuals, demonstrating feasibility of manual-based manipulation
- Part alignment between manual annotations and CAD/observation remains the primary bottleneck, limiting overall performance
- Performance significantly degrades in pure manual-based manipulation (Track 3) compared to CAD-assisted methods (Track 2), highlighting the difficulty of the task
- Error accumulation in long-horizon tasks causes cascading failures when early steps fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large model-assisted, human-revised pipelines can generate scalable, simulation-ready appliance manuals that bridge the gap between NLP document datasets and robotic manipulation tasks.
- **Mechanism:** The system uses Multimodal Large Language Models (MLLMs) to annotate CAD model parts with functional names (e.g., "temperature knob") and states. It then uses Large Language Models (LLMs) to generate manipulation tasks and compiles these into PDF manuals via LATEX code. Human verification corrects "anti-commonsense" errors (e.g., 500Â°C oven settings) before compilation.
- **Core assumption:** MLLMs can correctly infer part functionality primarily from visual appearance and CAD geometry, and human review can efficiently catch semantic or logical errors in the generated annotations.
- **Evidence anchors:** [Section 3.1] "We design a large model-assisted human-revised manual creation pipeline... to generate diverse appliance manuals... based on CAD appliance models." [Section 3.1.2] "MLLM sometimes fails to identify tiny visual elements... LLM may create an anti-commonsense design... Therefore, we check... through our designed human verification website." [corpus] Related work like "RealAppliance" emphasizes the necessity of high-fidelity, controllable assets aligned with manuals, supporting the need for this data generation approach.
- **Break condition:** If the MLLM hallucinates functional labels that pass human review, or if the CAD models lack the articulation joints required for the generated tasks, the resulting manual will be misaligned with the physical simulation.

### Mechanism 2
- **Claim:** Decomposing the manual-based manipulation problem into distinct modules (Resolution, Planning, Alignment) allows large models to handle long-horizon tasks that pure end-to-end models cannot.
- **Mechanism:** The "ManualPlan" model first resolves the PDF into text/visuals using OCR and MLLM. It then generates a step-by-step plan in dictionary format (e.g., `{'Part Name': 'Action'}`). Finally, it aligns these part names to visual masks using a Set-of-Mark (SoM) prompting strategy.
- **Core assumption:** The semantic reasoning capabilities of LLMs are sufficient to translate often ambiguous or non-linear manual instructions into a strictly ordered, linear executable plan.
- **Evidence anchors:** [Abstract] "The authors also introduce ManualPlan, a manual-based manipulation planning model... [which] can generate partial correct task plans based on multi-page manuals." [Section 4.1] "ManualPlan introduces a novel framework... It processes manual content, generates detailed plans, and aligns parts in the real appliance with those referenced in the manual." [corpus] Corpus evidence for modular planning vs. end-to-end in this specific domain is weak; however, the paper posits this modularity as a baseline necessity due to the complexity of multi-page manuals.
- **Break condition:** The mechanism fails if the manual contains instructions that rely heavily on implicit common sense not captured in the text, or if the OCR fails to read diagrams essential to the plan.

### Mechanism 3
- **Claim:** Visual prompting via Set-of-Mark (SoM) enables MLLMs to bridge the "semantic gap" between abstract 2D manual diagrams and 3D physical appliance parts.
- **Mechanism:** The system uses Grounding-DINO and SAM to segment all movable parts in the visual observation and overlays numeric masks. The MLLM is then prompted to match these masked IDs to the function names extracted from the manual (e.g., matching mask #1 to the "Power Button").
- **Core assumption:** The segmentation models can successfully distinguish between adjacent, visually similar parts (e.g., a row of buttons), and the MLLM possesses sufficient visual grounding to map a 2D manual sketch to a 3D segmented view.
- **Evidence anchors:** [Section 4.1] "We incorporate a visual prompting mechanism known as Set-of-Mark (SoM) [29] to align parts in the field of view and parts depicted in the manual." [Section 5.2] "Even the most advanced Multi-Modal Large Language Models (MLLMs) still face significant challenges in aligning manual part annotations... which greatly limits the overall performance."
- **Break condition:** If the appliance contains parts that are visually occluded or physically identical (e.g., a grid of identical buttons with no text labels), the SoM alignment will produce ambiguous mappings, leading to execution failure.

## Foundational Learning

- **Concept: Document Image Analysis (OCR & Layout)**
  - **Why needed here:** The Manual Resolution Module must parse PDFs into structured text and figures. Standard OCR often fails on complex manual layouts (tables, multi-column text, exploded diagrams).
  - **Quick check question:** Can your system distinguish between a labeled diagram and a decorative icon in the manual PDF?

- **Concept: 6-DoF Pose Estimation & CAD Alignment**
  - **Why needed here:** In "Track 2" (CAD-based), the robot must estimate the appliance's pose to overlay the CAD model. This allows it to access joint axes and kinematic data not visible in the RGB-D image.
  - **Quick check question:** How does your pose estimator handle symmetrical appliances (like a plain cube-shaped microwave) where orientation is ambiguous?

- **Concept: Set-of-Mark (SoM) Visual Prompting**
  - **Why needed here:** This is the core alignment technique. You must understand how to superimpose segmentation masks with numeric IDs onto images to "prompt" the MLLM for grounding.
  - **Quick check question:** If the MLLM outputs "Press button 3," how do you verify that the visual segmentation ID 3 corresponds to the correct physical button on the hardware?

## Architecture Onboarding

- **Component map:** Input (Manual PDF + Task + RGB-D [+ CAD]) -> Manual Resolution (OCR + MLLM) -> Planner (LLM) -> Aligner (GroundingDINO + SAM + SoM) -> Executor (Primitives/VoxPoser)
- **Critical path:** The **Part Alignment Module** is the bottleneck. As noted in the paper (Section 5.2), even with a correct high-level plan, alignment failures (mapping the wrong part name to a visual mask) cause cascading execution errors.
- **Design tradeoffs:**
  * **Track 2 (CAD-assisted) vs. Track 3 (Pure Manual):** Track 2 offers higher precision via kinematic primitives but requires exact CAD models and robust pose estimation. Track 3 generalizes to unknown appliances but relies on the noisy reasoning of open-vocabulary manipulation models like VoxPoser.
  * **Generation vs. Collection:** The authors generate synthetic manuals to ensure copyright safety and data volume, trading off the "messiness" of real-world manuals for structured, clean data.
- **Failure signatures:**
  * **Accumulated Error:** A failure in Step 1 (e.g., opening the wrong door) makes Step 2 impossible, crashing the long-horizon task.
  * **Semantic Hallucination:** The LLM proposes a step that is semantically logical but physically impossible (e.g., "Rotate button A" when Button A is fixed).
  * **Visual Ambiguity:** SoM fails to distinguish parts, resulting in the robot turning the "Timer" knob when it meant to turn the "Temperature" knob.
- **First 3 experiments:**
  1. **Module Isolation Test:** Run the Manual Resolution Module on a validation set of generated manuals. Calculate the extraction accuracy of part names and task steps to ensure the planner receives correct context.
  2. **Alignment Stress Test:** Place two visually similar appliances (e.g., two different microwaves) in the simulator. Run the Aligner to see if it maps "Door" correctly to the specific appliance's door or confuses it with the other object in the scene.
  3. **Track 2 Baseline:** Execute the "ManualPlan + Primitive Actions" pipeline on the simplest appliances (e.g., a box with one button) to verify that the CAD-to-Reality alignment works before scaling to multi-part appliances.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-modal models improve the visual grounding required to align manual part annotations with corresponding parts in appliance CAD models or observations?
- **Basis in paper:** [explicit] The authors state that even advanced MLLMs "face significant challenges in aligning manual part annotations with the corresponding CAD models," which limits the performance of the proposed ManualPlan model.
- **Why unresolved:** The baseline fails to consistently map function names (e.g., "Temperature Knob") found in the manual to specific part IDs in the visual observation, creating a bottleneck for planning.
- **What evidence would resolve it:** A model that achieves significantly higher "Part Alignment Success Rate" on Track 1 without human intervention.

### Open Question 2
- **Question:** How can manual-based manipulation systems mitigate error accumulation to prevent total failure in long-horizon tasks?
- **Basis in paper:** [explicit] The analysis of failure cases in Track 2 and Track 3 identifies "the accumulation of errors in part identification, task planning, and multi-step execution" as the primary cause of failure.
- **Why unresolved:** The current sequential pipeline propagates early mistakes (e.g., misidentifying a part) through the rest of the task, causing the robot to fail subsequent steps.
- **What evidence would resolve it:** A method that demonstrates robustness to intermediate step failures, perhaps via re-planning or error correction, maintaining a high task completion rate over long horizons.

### Open Question 3
- **Question:** Can a purely manual-based manipulation policy (Track 3) achieve performance comparable to CAD-assisted methods (Track 2) without relying on explicit geometric priors?
- **Basis in paper:** [inferred] The paper highlights that Track 3 (Pure Manual-based) is the setting aligned with real-world applications. However, the baseline results show a massive performance drop compared to Track 2 (e.g., ~2-4% success vs ~4-11%), leaving the generalization problem largely unsolved.
- **Why unresolved:** The "Pure Manual" setting removes the safety net of CAD model geometry, requiring the robot to infer interaction physics solely from manual diagrams and visual input, a task where current open-vocabulary models struggle.
- **What evidence would resolve it:** A system that closes the performance gap between Track 2 and Track 3, successfully manipulating appliances using only RGB-D input and a PDF manual.

## Limitations
- **Synthetic data quality:** Generated manuals lack the ambiguity and noise of real-world user manuals, potentially limiting real-world applicability
- **Alignment module brittleness:** Set-of-Mark visual prompting remains unreliable when dealing with visually similar parts or complex appliance geometries
- **Generalization gap:** The ManualPlan model trained on generated manuals for CAD appliances may not generalize to real-world appliances with different visual appearances or control schemes

## Confidence

- **High Confidence:** The dataset generation methodology and benchmark track definitions are well-specified and reproducible. The evaluation metrics are clear and appropriate for measuring progress on this novel task.
- **Medium Confidence:** The ManualPlan model architecture is clearly described, but the performance claims are limited by the challenging nature of the alignment problem rather than model capability.
- **Low Confidence:** The extent to which this approach will scale to real-world scenarios remains uncertain, given the reliance on synthetic data and the known brittleness of current visual grounding systems.

## Next Checks

1. **Cross-Domain Transfer Test:** Evaluate ManualPlan on a small set of real user manuals (e.g., from manufacturer websites) for appliances similar to those in the generated dataset. Measure degradation in planning accuracy compared to synthetic manuals.

2. **Part Alignment Robustness Analysis:** Systematically vary visual occlusion, lighting conditions, and camera angles in the simulation to quantify how SoM alignment performance degrades. Identify specific failure modes (e.g., similar-looking buttons, partial occlusions).

3. **Error Propagation Study:** Analyze the correlation between alignment errors at different task steps and overall task completion rates. Determine if certain types of alignment mistakes (e.g., confusing adjacent buttons vs. distant parts) have disproportionate impact on success.