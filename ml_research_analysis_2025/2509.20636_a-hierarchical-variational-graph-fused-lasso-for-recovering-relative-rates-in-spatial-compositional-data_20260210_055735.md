---
ver: rpa2
title: A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates
  in Spatial Compositional Data
arxiv_id: '2509.20636'
source_url: https://arxiv.org/abs/2509.20636
tags:
- variational
- data
- spatial
- relative
- rates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of recovering relative molecular
  rates in spatial biological imaging data, where competitive sampling processes convolve
  signals from co-localized molecules. The authors develop a scalable Bayesian framework
  using a hierarchical variational graph fused lasso prior to model spatial compositional
  data.
---

# A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates in Spatial Compositional Data

## Quick Facts
- **arXiv ID:** 2509.20636
- **Source URL:** https://arxiv.org/abs/2509.20636
- **Reference count:** 40
- **Primary result:** Hierarchical variational graph fused lasso recovers relative molecular rates from compositional spatial imaging data with RMSE 1-2 orders of magnitude better than point estimates and 90% credible interval coverage of 0.86

## Executive Summary
This paper addresses the fundamental challenge of recovering true spatial distributions of molecules from compositional imaging data where competitive sampling processes convolve signals from co-localized molecules. The authors develop a scalable Bayesian framework using a hierarchical variational graph fused lasso prior to model spatial compositional data. The method incorporates a heavy-tailed graphical lasso prior and a novel hierarchical variational family, enabling efficient inference via automatic differentiation variational inference. Applied to mouse kidney tissue IMS data, the approach outperforms state-of-the-practice point estimate methodologies, better recovers anatomical structure, removes artifacts, and detects active regions missed by standard analysis approaches.

## Method Summary
The method models observed compositional data through a censored multinomial likelihood where observed counts follow a multinomial distribution and censored counts are modeled via the negative multinomial CDF. A graph-fused gamma lasso prior is imposed on log-rates, penalizing differences between adjacent pixels using edge-specific Laplace shrinkage with gamma-distributed local penalties. The hierarchical variational family parameterizes per-vertex variances as functions of edge-specific shrinkage parameters, creating spatially-dependent marginal posteriors while preserving conditional independence for gradient computation. Inference is performed using ADVI with reparameterized gradients, with ELBO computed via Monte Carlo sampling of the negative multinomial CDF.

## Key Results
- RMSE improvements of 1-2 orders of magnitude across all metabolites compared to Total Ion Count (TIC) point estimates
- 90% credible interval coverage of 0.86 compared to 0.13-0.27 for benchmark mean-field variational inference models
- Better recovery of anatomical structure and removal of artifacts in real kidney tissue IMS data
- Detection of active regions missed by standard TIC analysis approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Imposing sparse priors on adjacent-pixel rate differences enables recovery of true relative rates from compositionally convolved observations that are otherwise underdetermined.
- **Mechanism:** The graph-fused gamma lasso prior penalizes |log(θ_i,d) − log(θ_j,d)| for adjacent pixels using edge-specific Laplace shrinkage with gamma-distributed local penalties. This reduces the effective degrees of freedom to approximately the number of spatial change points, making the system identifiable when change points are fewer than M − M/D.
- **Core assumption:** Molecular abundances exhibit piecewise-constant spatial patterns aligned with tissue substructure.
- **Evidence anchors:** [abstract] "leverages natural sparsity in spatial signal patterns to recover relative rates"; [Section 3.3] "so long as the number of change points is sufficiently small...we reduce the parameter space...restoring identifiability"; [corpus] Spacing Test for Fused Lasso and related work address uncertainty quantification in fused lasso, but corpus lacks direct validation of identifiability-through-sparsity claims for compositional settings.
- **Break condition:** When spatial patterns are smooth or have long length-scales rather than piecewise-constant; or when change points exceed the identifiability threshold.

### Mechanism 2
- **Claim:** Sparse structured variational inference achieves well-calibrated posterior coverage (0.85 at 90% CI) while maintaining computational tractability comparable to mean-field VI.
- **Mechanism:** The variational family parameterizes per-vertex variances γ_d = (H⁺)ᵀ(ν_d⁻¹) as functions of edge-specific shrinkage parameters. This creates spatially-dependent marginal posteriors while preserving conditional independence for gradient computation. Mean-field distributions over ν and λ couple the vertex distributions implicitly.
- **Core assumption:** Posterior dependencies can be adequately captured through shared hierarchical variance parameters rather than full covariance structure.
- **Evidence anchors:** [abstract] "superior posterior coverage than mean-field variational inference techniques"; [Section 4.1] "by parameterizing a distribution across logθ_d dependent on a vertex-transformed distribution over ν_d, we create a set of spatially-dependent marginal variational distributions"; [Section 5.2] Table 2 shows HV-GFGL achieves 0.85 overall coverage vs. 0.13 for MF-GFGL; [corpus] Variational nearest neighbor Gaussian processes (Wu et al.) address related spatial VI challenges; corpus supports that structured VI often outperforms mean-field in dependent settings.
- **Break condition:** When posteriors require covariance structures not expressible through variance-only coupling; when ELBO local optima cause severe shrinkage miscalibration.

### Mechanism 3
- **Claim:** The censored multinomial likelihood correctly handles left-censored data from limits of detection without biasing relative rate estimates.
- **Mechanism:** Observed counts follow a multinomial; censored counts are modeled via the negative multinomial CDF evaluated at detection limits, conditioned on observed totals. Monte Carlo sampling estimates the intractable CDF.
- **Core assumption:** Censoring is at the limit-of-detection threshold and is missing-not-at-random in a way the negative multinomial correctly captures.
- **Evidence anchors:** [Section 3.2] "we model the observed counts marginally as a multinomial and the censored counts as a negative multinomial, conditional on the total observed counts"; [Section 6.1] "138 metabolites had less than 15% left-censoring while 67 exhibited greater than 85% left-censoring"; [corpus] Corpus lacks comparative methods for censored compositional spatial data; external validation limited.
- **Break condition:** When censoring mechanisms differ from the assumed threshold model; when Monte Carlo CDF estimates are too noisy (few samples) or cause memory/compute bottlenecks.

## Foundational Learning

- **Concept: Compositional data and identifiability**
  - Why needed here: The core problem is that competitive sampling produces within-pixel compositions, but scientists need within-molecule spatial rates. Without assumptions, these are non-identifiable.
  - Quick check question: Given two pixels with compositions (0.1, 0.4, 0.5) and (0.15, 0.6, 0.25), can you determine whether molecule C decreased or A and B increased?

- **Concept: Global-local shrinkage priors (gamma-lasso / Horseshoe)**
  - Why needed here: The model uses hierarchical shrinkage to force most adjacent differences toward zero while allowing large deviations at true change points.
  - Quick check question: Why does a heavy-tailed prior outperform standard Laplace for detecting sparse change points in heterogeneous spatial patterns?

- **Concept: Variational inference and the ELBO**
  - Why needed here: Inference uses ADVI with reparameterized gradients; understanding the ELBO decomposition is essential for debugging convergence and coverage issues.
  - Quick check question: What goes wrong with mean-field VI when approximating posteriors over spatially dependent latent variables?

## Architecture Onboarding

- **Component map:** Graph G (spatial pixels) -> H (edge-adjacency matrix) -> log-rates θ_d -> softmax -> multinomial likelihood; ν_d (edge shrinkage) -> γ_d (vertex variance) -> θ_d variational distribution

- **Critical path:**
  1. Construct spatial graph G from pixel grid (define edges E)
  2. Initialize variational parameters at data scale: b*_r,d = E(x_d), λ*_1d = E(x_d), τ_d = Var(x_d), μ*_i,d = local proportions
  3. For each ELBO iteration: sample ν, compute γ, sample logθ, compute softmax probabilities, evaluate likelihood and priors, backpropagate
  4. Monitor convergence and posterior coverage

- **Design tradeoffs:**
  - Structured vs. mean-field VI: ~25% slower per iteration (0.01s vs. 0.008s) but dramatically better coverage (0.85 vs. 0.13)
  - Negative multinomial CDF samples: More samples improve accuracy but increase memory; paper uses 10–100 depending on censoring level
  - Gradient samples: Paper uses only 2 samples; fewer speeds iteration but increases gradient variance

- **Failure signatures:**
  - Over-shrinkage: All rates collapse to uniform (initialization at wrong scale; check b*_r,d)
  - Under-coverage: Credible intervals too narrow (mean-field used accidentally; check variational family structure)
  - Non-convergence: ELBO oscillates or diverges (check initialization matches data scale per Section 4.3)
  - Memory overflow: High censoring with many CDF samples (reduce Monte Carlo samples)

- **First 3 experiments:**
  1. Reproduce simulation study (Section 5) with the three model variants (HV-GFGL, MF-GFGL, MF-GFL) on the 7-metabolite simulated data; verify RMSE and coverage tables match.
  2. Ablate the censored likelihood component by running on simulated data with no censoring; confirm RMSE doesn't degrade and inference is faster.
  3. Test initialization sensitivity: run HV-GFGL with default initialization vs. random initialization on a single metabolite; quantify convergence speed and final ELBO difference.

## Open Questions the Paper Calls Out
- **Question 1:** What are the theoretical requirements for identifiability and the finite-sample convergence rates of the proposed relative rate recovery method?
  - Basis in paper: [explicit] The authors state in the Discussion that the approach "would benefit from strong theoretical guarantees to precisely delineate the requirements for identifiability and, ideally, finite-sample rates."
  - Why unresolved: The paper currently relies on empirical validation through simulations (showing 0.85 coverage) and heuristic arguments regarding degrees of freedom, lacking formal mathematical proofs of convergence or identifiability bounds.
  - What evidence would resolve it: Formal proofs establishing the necessary conditions for unique recovery of $\tilde{\theta}$ and error bounds on the variational approximation relative to the true posterior as sample size increases.

- **Question 2:** Can the model be adapted for smooth or long length-scale spatial patterns without compromising the identifiability of the relative rates?
  - Basis in paper: [explicit] Section 7.2 notes the method is "less suited to spatial patterns that are either smoother or at longer length-scales" and asks whether "adapting the prior to have a smoother penalty could prove fruitful... though whether the relative rates would still be identifiable is unclear."
  - Why unresolved: The current model leverages sparsity (piecewise constant changes) to reduce the degrees of freedom in the underdetermined compositional system; removing this constraint via smoother priors might reintroduce identifiability failures.
  - What evidence would resolve it: Derivation of a smoother penalty variant (e.g., fused smoothness) and empirical validation showing that true rates can still be recovered in simulation scenarios with gradual spatial gradients.

- **Question 3:** How does the accuracy of the sparse structured variational posterior compare to ground-truth MCMC sampling?
  - Basis in paper: [explicit] The authors list "a comparative analysis of posterior approximation to MCMC methods" as a specific area for future work.
  - Why unresolved: The method is benchmarked against point estimates (TIC) and mean-field VI, but the fidelity of the variational approximation to the true Bayesian posterior remains unquantified against a gold-standard sampler.
  - What evidence would resolve it: A comparative study on small datasets where MCMC is computationally tractable, analyzing the KL-divergence or distributional differences between the HV-GFGL posterior and the MCMC posterior.

## Limitations
- Identifiability claims for compositional data deconvolution lack formal mathematical proof
- Computational complexity of censored likelihood may become prohibitive for larger datasets
- Claims about recovering "missed active regions" in real data are qualitative without quantification
- Comparison to TIC estimates using SSIM is somewhat subjective without ground truth

## Confidence
- **High confidence:** RMSE improvements on both simulated and real data are substantial and reproducible; structured VI consistently outperforms mean-field VI in coverage; initialization scheme and optimization procedure appear sound
- **Medium confidence:** Identifiability-through-sparsity argument is plausible but not rigorously proven; negative multinomial CDF estimation accuracy depends on Monte Carlo sample count
- **Low confidence:** Qualitative claims about detecting missed regions; SSIM comparison to TIC lacks ground truth validation

## Next Checks
1. **Ablation study on sparsity:** Run the method on simulated data with varying numbers of change points (below, at, and above the identifiability threshold) to empirically test when the method breaks down
2. **Sensitivity to censoring level:** Systematically vary the proportion of left-censored data (0%, 50%, 90%) in simulations and measure impact on RMSE and coverage
3. **Ground truth validation on real data:** If possible, apply the method to a controlled experiment where some metabolites have known spatial patterns (e.g., synthetic spike-in standards in biological tissue), or use cross-validation by masking portions of the data and measuring recovery accuracy