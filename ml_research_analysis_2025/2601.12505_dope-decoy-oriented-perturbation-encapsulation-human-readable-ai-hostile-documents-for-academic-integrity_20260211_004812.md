---
ver: rpa2
title: 'DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile
  Documents for Academic Integrity'
arxiv_id: '2601.12505'
source_url: https://arxiv.org/abs/2601.12505
tags:
- question
- dope
- text
- detection
- exam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOPE, a document-layer defense that embeds
  semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies
  in MLLMs. By instrumenting exams at authoring time, DOPE provides prevention (stopping
  automated solving) and detection (flagging blind AI reliance) without relying on
  one-shot classifiers.
---

# DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity

## Quick Facts
- **arXiv ID**: 2601.12505
- **Source URL**: https://arxiv.org/abs/2601.12505
- **Reference count**: 40
- **Primary result**: Achieves 91.4% detection rate at 8.7% false-positive rate and 96.3% prevention rate on AI-assisted exam cheating

## Executive Summary
This paper introduces DOPE, a document-layer defense that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLMs. By instrumenting exams at authoring time, DOPE provides prevention (stopping automated solving) and detection (flagging blind AI reliance) without relying on one-shot classifiers. The system uses FEWSORT-Q to generate question-level semantic decoys and FEWSORT-D to embed them into watermarked documents. Evaluated on Integrity-Bench, a benchmark of 1,826 exams, DOPE achieves a 91.4% detection rate at 8.7% false-positive rate and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. The approach maintains human readability while creating AI-hostile documents, with human evaluation confirming high usability and low perceptibility.

## Method Summary
DOPE operates through a three-stage pipeline: FEWSORT-Q generates semantic decoys by rewriting questions to make non-gold options uniquely correct while preserving plausibility; FEWSORT-D embeds these decoys using document-layer perturbations like invisible character replacement (ICW), font remapping, and dual-layer overlays; the Verifier then uses an LLM-as-Judge to score student responses against decoy signatures, flagging submissions with high alignment. The system exploits the render-parse gap where human renderers display original content while MLLM parsers extract underlying decoy streams, creating documents that appear normal to humans but contain hidden semantic traps for AI systems.

## Key Results
- Achieves 91.4% detection rate at 8.7% false-positive rate for flagging AI-assisted responses
- Prevents successful completion or induces decoy-aligned failures in 96.3% of automated solving attempts
- Maintains human readability with only 51.3% detection rate in blinded user studies
- Outperforms conventional adversarial training by up to 36.8% detection rate and reduces false positives by up to 47.4%

## Why This Works (Mechanism)

### Mechanism 1: Render-Parse Gap Exploitation
Embedding semantic decoys via document-layer perturbations creates a render-parse gap where humans see original content while MLLMs parse altered semantic content. PDF/HTML documents contain multiple structural layers (hidden text spans, Unicode mappings, image overlays). DoPE injects decoy content into these layers using ICW (zero-opacity text), font remapping (ToUnicode/CMap edits), and dual-layer overlays. Human renderers display the visible layer; MLLM parsers extract the underlying decoy stream. Core assumption: MLLM document pipelines do not perfectly reconcile visual rendering with parsed text content. Break condition: If MLLMs switch to vision-only document processing (screenshots), the parse gap collapses and detection drops to ~2.3%.

### Mechanism 2: Semantic Decoy Generation
FEWSORT-Q generates semantically coherent decoy questions that shift correct answers to targeted distractors while preserving plausibility. An LLM agent receives the original question, gold answer, and options, then produces a substitute stem that makes a non-gold option uniquely correct (MCQ), flips truth value (T/F), or shifts focus with presence/absence markers (long-form). The decoy is semantically valid but incompatible with the original gold answer. Core assumption: LLMs can reliably generate substitution questions with predictable answer shifts under the SRT (Situation-Role-Task) prompting framework. Break condition: If decoy questions are semantically incoherent or fail to shift answers, the Verifier's signature matching will not detect AI reliance.

### Mechanism 3: Signature-Based Detection
Blind AI assistance produces responses that align with decoy signatures at significantly higher rates than independent or critically-edited human work. The Verifier computes a weighted match score between student responses and decoy signatures using an LLM-as-Judge. Submissions with Ctotal ≥ τ are flagged. Independent reasoning rarely hits multiple decoy signatures; uncritical MLLM output copying does. Core assumption: Students who blindly copy MLLM outputs do not systematically edit responses in ways that remove decoy-aligned content. Break condition: Paraphrasing AI outputs or partial parsing (visible text only) reduces detection; false positives occur when students coincidentally choose targeted distractors (~62% of FPs).

## Foundational Learning

- **Concept: Render-Parse Gap in Document Processing**
  - Why needed here: DoPE's core exploit depends on understanding that PDF/HTML documents have separate rendering and parsing paths.
  - Quick check question: When a PDF contains white text on white background, does a typical MLLM file parser see it?

- **Concept: Semantic Adversarial Perturbations**
  - Why needed here: FEWSORT-Q doesn't add random noise—it generates coherent alternative questions with controlled answer shifts.
  - Quick check question: Why is replacing "economic impact" with "social impact" in a question stem a better decoy than adding invisible random characters?

- **Concept: LLM-as-Judge Verification**
  - Why needed here: Detection relies on an LLM scoring response-to-signature alignment; understanding prompt design for judges is critical.
  - Quick check question: What failure mode might occur if the judge LLM is less capable than the attacker's MLLM?

## Architecture Onboarding

- **Component map**: FEWSORT-Q -> FEWSORT-D -> Verifier
- **Critical path**: Clean exam → FEWSORT-Q generates decoys → FEWSORT-D embeds into PDF/HTML → Student receives D′ → (if AI used) MLLM parses decoy content → Student submits response → Verifier scores alignment
- **Design tradeoffs**: ICW alone: simpler but lower DR (74.3%) / higher FPR (14.2%); Full ensemble: highest DR (91.4%) / lowest FPR (8.7%) but more complex document generation; Font remap only: strong DR (86.2%) but may break accessibility tools
- **Failure signatures**: Screenshot attack: DR drops to 2.3%; Query splitting: DR drops to 84.7%; Manual transcription: bypasses all defenses (costs ~18 min/exam); False positives: 62% from coincidental distractor selection, 24% from vocabulary overlap
- **First 3 experiments**:
  1. Replicate ICW vs. font remap vs. dual-layer ablation on a subset of Integrity-Bench to verify Table 4 results on your target MLLM versions
  2. Test screenshot bypass: convert watermarked PDFs to images, re-submit to vision models, measure detection degradation
  3. Calibrate threshold τ on held-out data: sweep from 0.4–0.8 and plot DR vs. FPR to validate the τ=0.6 operating point for your student population

## Open Questions the Paper Calls Out

- **Question**: How can document-layer defenses be adapted to maintain effectiveness when students upload exam screenshots directly to vision-capable MLLMs?
  - **Basis in paper**: The authors state in Limitations: "when students submit full-page screenshots to vision-capable MLLMs, the render–parse gap largely disappears and document-layer signals weaken substantially."
  - **Why unresolved**: Screenshot queries bypass the structural perturbations entirely by capturing the rendered output rather than the underlying document structure.
  - **What evidence would resolve it**: Development and evaluation of visual-domain perturbations (e.g., subtle image watermarks) that survive screenshot capture and remain detectable in MLLM outputs.

- **Question**: What mechanisms can sustain document-layer defense effectiveness as MLLM document parsing and OCR pipelines improve over time?
  - **Basis in paper**: The authors note that "as document parsing and OCR pipelines improve, specific perturbation mechanisms may need to be updated, even though the underlying render–parse gap remains a structural feature."
  - **Why unresolved**: Current defenses exploit specific parsing behaviors that may change as vendors improve their document processing.
  - **What evidence would resolve it**: Longitudinal evaluation tracking detection/prevention rates across model versions, plus identification of perturbation mechanisms that target fundamental rather than implementation-specific parsing behaviors.

- **Question**: Why do Claude-family models show substantially lower detection rates (~60%) on True/False questions compared to near-perfect rates on MCQ and long-form items?
  - **Basis in paper**: Table 1 shows Claude models achieving ~60% DR on True/False versus ~100% on long-form and ~99% on MCQ for DOPE-v1, but the paper notes only that "binary questions offer fewer semantic handles for decoy signatures" without explaining the model-family asymmetry.
  - **Why unresolved**: The architectural or parsing differences causing this gap are not investigated.
  - **What evidence would resolve it**: Ablation studies isolating whether the gap stems from different parsing behavior, text extraction, or semantic reasoning patterns across model families.

- **Question**: How do detection and prevention rates vary when DOPE is deployed on institution-specific exam formats beyond benchmark-derived and OCW-style assessments?
  - **Basis in paper**: The authors acknowledge: "our empirical evaluation is based on benchmark-derived and OCW-style exams; while these are diverse, behaviour on institution-specific formats and policies may differ, and should be validated locally before high-stakes deployment."
  - **Why unresolved**: Real institutional exams may have different layouts, question densities, or format constraints that affect perturbation efficacy.
  - **What evidence would resolve it**: Multi-institution field trials comparing DOPE performance across diverse real-world exam formats, disciplines, and LMS platforms.

## Limitations

- Effectiveness drops significantly when students upload screenshots directly to vision-capable MLLMs, defeating the render-parse gap exploit
- Cannot detect manually transcribed AI responses, which bypass all document-layer protections at the cost of ~18 minutes per exam
- False positive rate of 8.7% stems primarily from students coincidentally selecting targeted distractors (62%) or vocabulary overlap (24%)

## Confidence

**High confidence**: The dual goals (prevention and detection) are well-defined and measurable. The document-layer perturbation mechanism (ICW, font remapping, dual-layer) is technically sound and reproducible with the provided code snippets. The 96.3% prevention rate on screenshot and query-splitting attacks is directly measurable.

**Medium confidence**: The 91.4% detection rate at 8.7% FPR assumes consistent MLLM parsing behavior and the specific judge LLM calibration. The claim that MLLMs "cannot reason around" decoy constraints is plausible but not definitively proven across all model variants.

**Low confidence**: Claims about human perceptibility (51.3% detection rate in user study) and pedagogical efficacy (students critically editing AI responses) rely on limited user studies and hypothetical scenarios not fully validated.

## Next Checks

1. **Ablation on your target MLLMs**: Test ICW vs. font remap vs. dual-layer variants on your specific MLLM versions (gpt-4o, claude-3.5-sonnet) to verify the relative effectiveness matches Table 4.

2. **Screenshot attack validation**: Convert watermarked PDFs to images and re-submit to vision models to measure actual detection degradation from the claimed 2.3% DR.

3. **False positive calibration**: Sweep threshold τ from 0.4-0.8 on a held-out validation set from your institution to find the optimal DR/FPR tradeoff for your student population, accounting for subject-specific vocabulary overlap.