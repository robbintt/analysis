---
ver: rpa2
title: 'HAR-DoReMi: Optimizing Data Mixture for Self-Supervised Human Activity Recognition
  Across Heterogeneous IMU Datasets'
arxiv_id: '2503.13542'
source_url: https://arxiv.org/abs/2503.13542
tags:
- data
- domain
- algorithm
- datasets
- mahony
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of limited model generalization
  in cross-dataset human activity recognition (HAR), where variations in sensor orientation
  and data heterogeneity severely hinder model performance when transferred to unseen
  datasets. To address this, the authors propose HAR-DoReMi, a novel pre-training
  framework that optimizes data mixture ratios for heterogeneous IMU datasets and
  integrates the Mahony fusion algorithm for sensor orientation alignment.
---

# HAR-DoReMi: Optimizing Data Mixture for Self-Supervised Human Activity Recognition Across Heterogeneous IMU Datasets

## Quick Facts
- arXiv ID: 2503.13542
- Source URL: https://arxiv.org/abs/2503.13542
- Reference count: 40
- Outperforms CrossHAR by 6.51% accuracy on average while using only 30-50% of the data

## Executive Summary
This paper addresses the challenge of domain generalization in cross-dataset human activity recognition (HAR), where variations in sensor orientation and data heterogeneity severely limit model transferability. The authors propose HAR-DoReMi, a novel pre-training framework that optimizes data mixture ratios for heterogeneous IMU datasets while aligning sensor orientations across domains. The key innovation is replacing the discrete sequence prediction task from the original DoReMi framework with a masked reconstruction task using MSE loss, better suited to the continuous nature of IMU data. The method demonstrates significant improvements in both accuracy and data efficiency across four public HAR datasets.

## Method Summary
HAR-DoReMi employs a four-step pipeline to optimize data mixture for cross-dataset HAR. First, it applies the Mahony fusion algorithm to align sensor data from different datasets to a unified global coordinate system, addressing orientation heterogeneity. Second, a reference model is trained using masked reconstruction with MSE loss on the merged dataset with initial domain weights proportional to dataset size. Third, a proxy model using Group Distributionally Robust Optimization (Group DRO) iteratively updates domain weights to minimize excess loss relative to the reference model. Finally, the target model is trained using the optimized data mixture ratios derived from the proxy model's domain weights, achieving superior generalization performance with reduced data requirements.

## Key Results
- Achieves 6.51% average accuracy improvement over CrossHAR baseline
- Demonstrates 30-50% data efficiency while maintaining performance
- Outperforms state-of-the-art methods on cross-dataset HAR tasks
- Shows consistent improvements across four public IMU datasets (HHAR, Motion, Shoaib, UCI)

## Why This Works (Mechanism)
The framework addresses two key challenges in cross-dataset HAR: sensor orientation heterogeneity and domain distribution mismatch. By using the Mahony filter to align all sensor data to a common coordinate frame, it eliminates the need for computationally expensive spatial transformer networks. The masked reconstruction task with MSE loss is better suited for continuous IMU data compared to discrete sequence prediction. The Group DRO optimization ensures the model learns from the most informative data mixture across domains, preventing overfitting to any single dataset's distribution.

## Foundational Learning
- **Mahony filter for sensor fusion**: Combines accelerometer and gyroscope data to estimate device orientation. Needed to transform heterogeneous sensor data to a common coordinate frame. Quick check: Verify that aligned waveforms from different datasets show consistent patterns for the same activities.
- **Group Distributionally Robust Optimization**: Optimizes model performance across multiple domains by minimizing worst-case excess loss. Needed to find optimal data mixture ratios. Quick check: Monitor domain weight convergence during proxy model training.
- **Masked reconstruction with MSE loss**: Reconstructs masked portions of continuous sensor signals. Needed to handle the continuous, multi-channel nature of IMU data. Quick check: Ensure reconstruction loss decreases during reference model training.
- **Transformer-based encoder architecture**: Processes sequential sensor data through self-attention mechanisms. Needed to capture temporal dependencies in IMU signals. Quick check: Verify attention weights show meaningful temporal patterns.

## Architecture Onboarding

**Component Map**: Raw IMU data -> Mahony Alignment -> Reference Model (Masked Reconstruction) -> Proxy Model (Group DRO) -> Optimized Weights -> Target Model (CrossHAR)

**Critical Path**: The critical path for performance improvement is the Mahony alignment followed by the Group DRO optimization loop. Without proper orientation alignment, the model cannot learn meaningful cross-dataset patterns. Without optimal data mixture weights, the model cannot generalize effectively to unseen domains.

**Design Tradeoffs**: The method trades computational complexity in the pre-processing phase (Mahony alignment) for improved generalization. It also requires multiple training stages (reference, proxy, target) but achieves better performance with less data. The use of MSE loss instead of cross-entropy simplifies the reconstruction task but may lose some discrete pattern information.

**Failure Signatures**: Poor performance typically manifests as: 1) Failure to align sensor data properly, resulting in inconsistent features across domains; 2) Domain weight collapse during Group DRO optimization, causing the model to ignore certain datasets; 3) Inadequate reconstruction performance, indicating the model cannot learn meaningful representations from the IMU data.

**First Experiments**: 1) Implement Mahony filter and visually verify alignment of waveforms from different datasets for the same activities; 2) Run the Group DRO optimization loop and plot domain weight trajectories to ensure convergence; 3) Train the target model with optimized weights and compare performance against baseline models on held-out test data.

## Open Questions the Paper Calls Out
None

## Limitations
- Precise Mahony filter parameters (proportional/integral gains) are not specified, which could affect reproducibility
- Reliance on pre-existing baseline models without full transparency into their hyperparameter configurations
- Assumption that downsampling all datasets to 20Hz preserves sufficient temporal information

## Confidence

| Claim | Confidence |
|-------|------------|
| 6.51% accuracy improvement over CrossHAR | High |
| 30-50% data efficiency with maintained performance | High |
| Mahony filter effectively aligns cross-dataset sensor data | Medium |
| Group DRO optimization finds optimal data mixture ratios | Medium |
| Masked reconstruction with MSE loss is optimal for IMU data | Medium |

## Next Checks

1. **Implement and validate the Mahony filter**: Recreate the sensor fusion algorithm with multiple gain configurations and visually verify that aligned waveforms from different datasets show consistent patterns for the same activities.

2. **Reproduce the proxy model weight optimization**: Run the Group DRO optimization loop and plot the domain weight trajectories over the 1000 steps to confirm they stabilize rather than collapse or oscillate.

3. **Test data efficiency claims**: Train the final target model using only 30% and 50% of the total training data (as per optimized ratios) and verify that performance degradation is minimal compared to using the full dataset.