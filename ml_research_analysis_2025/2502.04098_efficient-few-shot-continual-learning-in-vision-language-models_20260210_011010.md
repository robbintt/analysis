---
ver: rpa2
title: Efficient Few-Shot Continual Learning in Vision-Language Models
arxiv_id: '2502.04098'
source_url: https://arxiv.org/abs/2502.04098
tags:
- lorsu
- learning
- continual
- dataset
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRSU, a parameter-efficient fine-tuning
  method designed for few-shot continual learning in vision-language models. The approach
  selectively updates specific parameters in the image encoder, particularly attention
  heads and MLP layers, based on gradient importance scores.
---

# Efficient Few-Shot Continual Learning in Vision-Language Models

## Quick Facts
- arXiv ID: 2502.04098
- Source URL: https://arxiv.org/abs/2502.04098
- Authors: Aristeidis Panos; Rahaf Aljundi; Daniel Olmeda Reino; Richard E. Turner
- Reference count: 40
- One-line result: LoRSU achieves up to 25× computational savings and consistent performance gains across ten VQA datasets in three few-shot continual learning settings

## Executive Summary
This paper introduces LoRSU, a parameter-efficient fine-tuning method designed for few-shot continual learning in vision-language models. The approach selectively updates specific parameters in the image encoder, particularly attention heads and MLP layers, based on gradient importance scores. This structured adaptation improves task-specific performance while preserving general knowledge. Experimental results show LoRSU achieves up to 25× computational savings compared to full model updates and demonstrates consistent performance gains across ten VQA datasets in three few-shot continual learning settings, with minimal forgetting of previously learned knowledge.

## Method Summary
LoRSU targets the image encoder (CLIP-L-14) within LLaVA-v1.5, using gradient-based importance scores to select which attention heads and MLP parameters to update via LoRA adapters. The method computes importance scores $s_i = \sum_{m,l}[(G_q^{(i)}[m,l])^2 + (G_k^{(i)}[m,l])^2 + (G_v^{(i)}[m,l])^2]$ for each attention head, then updates only the highest-scoring heads via LoRA. For the first MLP layer (fc1), it applies a binary mask selecting the top proportion of parameters with largest squared gradient values. The approach operates with rank-64 LoRA adapters, updating only 2 attention heads per transformer block and 10% of fc1 parameters, achieving 25× computational savings compared to full model updates.

## Key Results
- LoRSU achieves up to 25× computational savings compared to full model updates
- Consistent performance gains across ten VQA datasets in three few-shot continual learning settings
- Minimal forgetting of previously learned knowledge (near-zero CC scores)
- Outperforms baselines including F-FT, F-EWC, LoRA, AdaLoRA, and SPU across multiple metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively updating only the top-k attention heads based on gradient importance scores reduces interference with general knowledge while improving task performance.
- **Mechanism:** LoRSU computes importance scores $s_i = \sum_{m,l}[(G_q^{(i)}[m,l])^2 + (G_k^{(i)}[m,l])^2 + (G_v^{(i)}[m,l])^2]$ for each head, then updates only the highest-scoring heads via LoRA adapters. This targets parameters with largest potential impact on task loss change.
- **Core assumption:** Parameters with high gradient magnitude are most critical for the current task and updating them maximizes task improvement while minimizing total parameter perturbation.
- **Evidence anchors:**
  - [abstract]: "selectively updating specific parameters in the image encoder, particularly attention heads and MLP layers, based on gradient importance scores"
  - [section 3]: Equations (3-4) define importance scores and LoRA parametrization; Table 4 shows LoRSU outperforms both random head selection (LoRSU-Rand) and updating all heads (LoRSU-AAH)
  - [corpus]: Related work on gradient-based parameter selection is limited in provided corpus; SPU (Zhang et al., 2024) uses similar structured sparsity for fc1 only
- **Break condition:** If gradient magnitudes don't correlate with parameter importance for a given task distribution, head selection becomes arbitrary; ablation shows random selection degrades TI by 1.6-2.3% (Table 4).

### Mechanism 2
- **Claim:** Structured sparse updates to the first MLP linear layer (fc1) preserve general knowledge while enabling task adaptation.
- **Mechanism:** A binary mask $M_{fc1}$ selects only the top proportion of parameters with largest squared gradient values. Only these positions receive gradient updates via $\hat{\nabla}_{W_{fc1}}L_t = M_{fc1} \odot \nabla_{W_{fc1}}L_t$.
- **Core assumption:** Knowledge in transformer blocks is localized; fc1 parameters contain task-relevant information that can be updated independently of other layers.
- **Evidence anchors:**
  - [section 3]: "we selectively update a subset of parameters from the first linear layer in the MLP block of each transformer layer, as proposed in (Zhang et al., 2024)"
  - [section 2]: SPU baseline comparison shows structured sparsity helps; LoRSU improves over SPU (Table 1: TI 6.8 vs 4.5 on ESAT CL-5)
  - [corpus]: Corpus mentions knowledge neuron theory (Dai et al., 2021) as inspiration but doesn't provide direct validation
- **Break condition:** If task knowledge is distributed across many fc1 parameters rather than localized, sparsity=10% may miss critical weights; paper doesn't ablate sparsity levels for fc1.

### Mechanism 3
- **Claim:** Updating the image encoder rather than the LLM is more efficient and less prone to catastrophic forgetting in VLMs.
- **Mechanism:** The image encoder (CLIP-L-14, ~400M params) is significantly smaller than the LLM (Vicuna-7B). LoRSU operates only on vision encoder parameters while keeping LLM frozen, reducing computational cost by 25×.
- **Core assumption:** Visual domain shifts, not LLM reasoning deficits, are the primary failure mode for VLMs on new domains.
- **Evidence anchors:**
  - [section 1, Figure 1]: LLaVA fails on TSI real images but succeeds on DALL-E generated images for same action ("cooking on a stove"), suggesting visual encoder—not LLM—is the bottleneck
  - [section 4.4, Table 3]: LoRA-F (full model) achieves high TI but shows large negative CC (e.g., -10.6 on VSR); LoRSU maintains near-zero CC
  - [corpus]: Related papers on CL for VLMs focus on LLM or projection layers; corpus doesn't directly compare encoder vs. LLM update strategies
- **Break condition:** If task requires new linguistic knowledge rather than visual adaptation, freezing the LLM prevents learning; LoRA-L baseline shows this (Table 3: negative TI on most datasets).

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: LoRSU's primary goal is adapting to new tasks while preserving previously learned knowledge; understanding how parameter updates interfere with prior knowledge is essential.
  - Quick check question: Can you explain why updating all parameters on a new task might degrade performance on earlier tasks?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: LoRSU builds on LoRA to parametrize weight updates as $W' = W + AB$ where A and B are low-rank matrices; understanding this decomposition is critical for grasping the efficiency gains.
  - Quick check question: How does decomposing a weight update into two low-rank matrices reduce the number of trainable parameters?

- **Concept: Vision-Language Model Architecture**
  - Why needed here: LoRSU specifically targets the image encoder component within a VLM architecture that includes CLIP encoder, MLP projector, and LLM; knowing which component handles what is essential.
  - Quick check question: In a VLM like LLaVA, which component is responsible for extracting visual features from images?

## Architecture Onboarding

- **Component map:**
  LLaVA VLM: CLIP-L-14 (Image Encoder [LoRSU - fc1 MLP - Attn heads via LoRA]) -> MLP Projector (frozen) -> Vicuna-7B (LLM frozen)

- **Critical path:**
  1. Forward pass through frozen CLIP encoder to compute embeddings
  2. Compute task loss (CLIP loss or perplexity loss)
  3. Backward pass to obtain gradients for all encoder parameters
  4. Calculate importance scores per attention head (Eq. 3)
  5. Select top-k heads and apply LoRA adapters
  6. Generate sparse mask for fc1 based on gradient magnitude
  7. Update only selected parameters via masked gradients

- **Design tradeoffs:**
  - **CLIP loss vs. Perplexity loss:** CLIP loss is 25× faster (encoder-only forward/backward) but perplexity loss (LoRSU-Ppl) can achieve higher TI on VQA tasks (Table 3: +10% on VSR)
  - **Number of attention heads (k):** Paper uses k=2; increasing to 16 provides marginal TI gain but slightly worse CC (Table 24)
  - **LoRA rank (r):** r=64 provides best balance; higher ranks don't improve performance (Table 23)

- **Failure signatures:**
  - Negative CC with large magnitude: Model is overfitting to target task, forgetting general knowledge (reduce epochs or sparsity)
  - Low TI improvement: Gradient-based head selection may be failing; verify gradient computation is correct
  - High variance across runs: Random seed sensitivity; check if k=2 is too aggressive for task complexity

- **First 3 experiments:**
  1. **Sanity check on frozen components:** Verify LLM and MLP projector remain frozen during training by logging their gradient norms (should be zero)
  2. **Ablation on number of attention heads:** Run k∈{0,1,2,4,8,16} on a single dataset (e.g., GTS CL-50) to reproduce Table 24 and confirm k=2 is optimal for your compute budget
  3. **Compare CLIP vs. perplexity loss:** Run both LoRSU and LoRSU-Ppl on the same dataset split to quantify the TI vs. efficiency tradeoff for your specific use case

## Open Questions the Paper Calls Out
None

## Limitations
- Parameter selection validity: The fundamental assumption that gradient magnitude correlates with task importance remains theoretically underexplored
- Generalization to non-VQA tasks: All evaluated datasets are visual question answering formats
- Baseline comparison completeness: Doesn't compare against more recent VL-specific continual learning methods

## Confidence
- **High confidence:** The efficiency claims (25× computational savings) and the core experimental results showing consistent TI improvements across multiple datasets and settings
- **Medium confidence:** The mechanism claims about why gradient-based parameter selection works better than alternatives
- **Medium confidence:** The catastrophic forgetting mitigation claims

## Next Checks
1. **Ablation on sparsity levels:** Systematically vary the fc1 sparsity parameter (currently fixed at 10%) across tasks to determine optimal trade-offs between adaptation capacity and knowledge preservation
2. **Cross-task generalization test:** Evaluate LoRSU on a non-VQA VL task (e.g., image captioning or visual grounding) to assess whether the approach generalizes beyond the demonstrated domain
3. **Long-term forgetting analysis:** Extend continual learning sessions beyond 5 tasks to evaluate whether the minimal forgetting observed persists under extended task sequences