---
ver: rpa2
title: Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio
  Generation
arxiv_id: '2510.24103'
source_url: https://arxiv.org/abs/2510.24103
tags:
- audio
- mgaudio
- training
- alignment
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MGAudio, a flow-based framework for video-to-audio
  generation that introduces model-guided dual-role alignment. The framework uses
  a flow-based Transformer model and a dual-role audio-visual encoder, where the encoder
  serves both as a conditioning module and as a feature aligner.
---

# Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2510.24103
- Source URL: https://arxiv.org/abs/2510.24103
- Reference count: 40
- Primary result: Achieves FAD=0.40 on VGGSound, outperforming existing methods

## Executive Summary
This paper introduces MGAudio, a flow-based video-to-audio generation framework that employs model-guided dual-role alignment. The method combines a flow-based Transformer architecture with a dual-role audio-visual encoder that serves both as conditioning and feature aligner. A novel model-guided training objective (AMG) enhances cross-modal coherence and audio realism. MGAudio achieves state-of-the-art performance on VGGSound, reducing FAD to 0.40 and demonstrating strong generalization to UnAV-100.

## Method Summary
MGAudio is a flow-based denoising transformer framework for video-to-audio generation. It uses a dual-role audio-visual encoder (CAVP) that provides both video conditioning and alignment supervision. The core innovation is the Audio Model-Guidance (AMG) training objective, which self-distills the guidance signal during training rather than computing it at inference. The framework employs flow-matching for stable generation and combines AMG training with classifier-free guidance at inference for optimal fidelity. Training uses 1.1M steps with batch size 64, achieving strong performance on VGGSound.

## Key Results
- Achieves FAD=0.40 on VGGSound, significantly outperforming previous methods
- Demonstrates strong generalization on UnAV-100 benchmark
- Shows data efficiency benefits: AMG training outperforms 100% data baseline with only 10% training data
- CAVP encoder provides superior alignment compared to DINOv2 and CLAP alternatives

## Why This Works (Mechanism)

### Mechanism 1: Audio Model-Guidance (AMG) Training Objective
AMG provides more efficient training signal than classifier-free guidance by directly optimizing the CFG trajectory during training. The training target is modified to u' = u + w·sg(u_θ(x_t, v, t)) - u_θ(x_t, ∅, t), incorporating the conditional-unconditional difference as direct supervision. This allows the model to learn what CFG would produce without needing separate branches. Evidence shows MGAudio outperforms CFG-trained FRIEREN across all CFG scales.

### Mechanism 2: Dual-Role CAVP Feature Alignment
The CAVP audio encoder is used for intermediate representation alignment during training, regularizing the denoising process toward semantically meaningful audio features. Clean audio passes through CAVP audio encoder to produce G_0, while noisy intermediate features H_t are aligned to G_0 via cosine similarity. This creates auxiliary supervision that guides the transformer to learn representations consistent with CAVP's pretrained audio-visual correspondence. CAVP outperforms DINOv2 and matches CLAP in alignment accuracy.

### Mechanism 3: Flow-Based Transport with Joint CFG+MG Inference
Combining AMG training with CFG at inference achieves better fidelity than either alone in the audio domain. The flow-matching formulation provides continuous transport directions u_t = x_0 - ε, which are more stable than diffusion's score matching. Audio's temporal structure benefits from both implicit guidance (via AMG training) and explicit guidance (via CFG inference). Evidence shows AMG-trained weights + CFG at inference achieves highest quality.

## Foundational Learning

- **Flow Matching (Continuous Normalizing Flows):** Core generative paradigm replacing diffusion; requires understanding transport ODEs x_t = (1-t)x_0 + tε and velocity prediction u_t = x_0 - ε. Quick check: Can you explain why flow matching predicts velocity u_t = x_0 - ε instead of noise ε?
- **Classifier-Free Guidance (CFG):** Baseline approach that AMG improves upon; must understand conditional vs unconditional branches and guidance scale tradeoffs. Quick check: How does CFG modify the prediction at inference, and what happens when CFG scale = 1.0?
- **Contrastive Audio-Visual Pretraining (CAVP):** Provides the dual-role encoder; understanding how contrastive learning aligns audio-visual embeddings is critical for the alignment mechanism. Quick check: Why would a contrastively trained encoder be better for alignment than a reconstruction-trained autoencoder?

## Architecture Onboarding

- **Component map:** Silent Video → CAVP Video Encoder → Global Feature v → AdaLN → Flow Transformer → VAE Decoder → Vocoder → Audio; Clean Audio → CAVP Audio Encoder → G_0 → Cosine Similarity → L_align; Noisy Latent → Flow Transformer → H_t → MLP → Cosine Similarity → L_align
- **Critical path:** 1) Video frame extraction → CAVP encoding → temporal aggregation → global conditioning vector; 2) Audio mel-spectrogram → VAE encoding → noise injection → patchify → transformer tokens; 3) Transformer forward with AdaLN conditioning → velocity prediction → alignment with G_0 (training only); 4) AMG target computation: u' = u + w·sg(u_cond) - u_uncond (training only); 5) VAE decode → vocoder → waveform
- **Design tradeoffs:** CAVP best for FAD; CLAP comparable but requires additional model; DINOv2 fails semantically; global aggregation better for distributional fidelity; AMG-only sufficient for robustness; AMG+CFG optimal for SOTA fidelity; batch size 64 achieves FAD=0.51; 16 degrades to FAD=1.14
- **Failure signatures:** High FAD with low KL: model mode-collapsed; good FAD but poor alignment accuracy: conditioning not integrating properly; DINOv2 produces semantic errors; CFG=1.0 with CFG-trained model causes severe quality drop; overtraining >1.7M steps shows minimal gains
- **First 3 experiments:** 1) Reproduce ablation Table 4: Train SiT baseline with CFG, then MGAudio with AMG, then add dual-alignment; 2) Encoder comparison: Swap CAVP for CLAP and DINOv2 in alignment branch; 3) Data efficiency check: Train on 10% VGGSound subset

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the MGAudio framework be adapted to effectively synthesize linguistically complex audio, such as dialogue or singing, which currently suffers from a lack of structural and phonetic awareness? (Section 4.4 limitations)
- **Open Question 2:** Can acceleration techniques like Consistency Models or Physics-Informed Distillation be integrated to mitigate the inference speed bottleneck without degrading audio fidelity? (Section 4.4 proposes as future work)
- **Open Question 3:** Does replacing the MDSGen-style global feature aggregation with a fine-grained temporal conditioning mechanism yield better synchronization scores without compromising distributional fidelity? (Appendix J discusses as promising direction)

## Limitations
- Reduced effectiveness for generating human vocalizations or linguistically complex audio due to lack of structural and phonetic awareness
- Inference speed bottleneck due to VAE and iterative sampling requirements
- Current global feature aggregation may compromise temporal precision despite strong distributional fidelity

## Confidence

- **High confidence:** Dual-role alignment improves FAD (1.19→1.14 with alignment loss); CFG+MG combination achieves SOTA results (0.40 FAD); data efficiency benefits of AMG training
- **Medium confidence:** CAVP encoder superiority over DINOv2 and CLAP; generalizability to UnAV-100; superiority of flow-matching over diffusion for V2A
- **Low confidence:** AMG's theoretical advantage over CFG in audio domain; optimal guidance scale of 1.45; exact contribution of each alignment component

## Next Checks
1. **Ablation stability test:** Train MGAudio with guidance scales w ∈ {1.0, 1.45, 2.0} and λ ∈ {0.3, 0.5, 0.75} to identify failure boundaries and verify the reported optimal values aren't locally optimal.
2. **Cross-dataset generalization:** Evaluate MGAudio on AudioSet or Kinetics-Sounds to test whether CAVP alignment generalizes beyond VGGSound's distribution.
3. **Human perceptual validation:** Conduct listening tests comparing MGAudio vs CFG-trained FRIEREN at equivalent CFG scales to verify the quantitative FAD improvements correspond to subjective audio quality differences.