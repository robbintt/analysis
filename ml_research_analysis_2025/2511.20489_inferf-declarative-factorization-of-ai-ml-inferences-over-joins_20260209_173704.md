---
ver: rpa2
title: 'InferF: Declarative Factorization of AI/ML Inferences over Joins'
arxiv_id: '2511.20489'
source_url: https://arxiv.org/abs/2511.20489
tags:
- join
- factorization
- node
- inference
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InferF, a declarative system that optimizes
  AI/ML inference over multi-way joins by factorizing computations and pushing them
  selectively to qualified nodes in the join tree. The core idea is to decompose inference
  workflows into sub-computations that can be executed on normalized datasets, reducing
  redundant calculations and join costs.
---

# InferF: Declarative Factorization of AI/ML Inferences over Joins

## Quick Facts
- arXiv ID: 2511.20489
- Source URL: https://arxiv.org/abs/2511.20489
- Reference count: 40
- Primary result: Declarative system for optimizing AI/ML inference over multi-way joins, achieving up to 11.3× speedup over baselines

## Executive Summary
InferF introduces a declarative system that optimizes AI/ML inference over multi-way joins by factorizing computations and pushing them selectively to qualified nodes in the join tree. The core innovation is decomposing inference workflows into sub-computations that can be executed on normalized datasets before joining, reducing redundant calculations and join costs. Evaluated on real-world datasets, InferF achieves significant speedups over existing in-database ML systems while maintaining production-level performance.

## Method Summary
InferF implements a cost-driven optimization framework for AI/ML inference over joins. It uses two optimization algorithms—a genetic algorithm and a greedy benefit-first optimizer—to explore the exponential search space of factorization plans. The system converts inference logic into an Expression Graph (DAG of ML/Relational ops), analyzes factorizable subgraphs based on data dependencies, and applies selective push-down of computations. Implemented on Velox, it supports various models including FFNN, decision trees, XGBoost, and product quantization, using cost models that balance CPU and I/O costs.

## Key Results
- Up to 11.3× speedup over Velox baselines for inference over joins
- Up to 18.7× speedup over other in-database ML systems
- Greedy optimizer achieves consistent performance with lower overhead (8-21× faster than genetic)
- Cost model effectively balances computation and I/O costs across diverse query patterns

## Why This Works (Mechanism)

### Mechanism 1: Redundancy Elimination via Pre-Join Computation
Decomposing inference logic into sub-computations that depend only on specific source tables significantly reduces total arithmetic operations if those source tables are smaller than the join output. The system partitions inference computations (e.g., $W^T x$) into independent sub-vectors corresponding to different joined relations ($W_i^T x_i$). These sub-computations are executed on the base tables before the join multiplies cardinality.

### Mechanism 2: Cost-Based Selective Push-down
Pushing factorized computations to the lowest possible node is not always optimal; pushing to intermediate nodes can yield better performance. InferF uses a cost model that balances CPU cost against I/O cost. If an intermediate join node has lower cardinality than a base table, pushing computation there reduces the number of invocations of that sub-computation.

### Mechanism 3: Aggressive Aggregation (Group Push-down)
Aggressively aggregating intermediate inference results at join nodes, rather than propagating raw sub-results, reduces I/O bandwidth. When multiple sub-computations are pushed to a node, they are aggregated immediately (e.g., `matAdd`). This reduces the tuple width passed up the tree, lowering the I/O cost for subsequent joins.

## Foundational Learning

**Relational Algebra & Query Optimization**: Understanding join trees, cardinality estimation, and predicate push-down is essential to grasp how InferF extends standard DB optimization to ML inference. *Quick check*: If Table A (100 rows) joins with Table B (1,000 rows) to produce 10,000 rows, where is the "redundancy" in an inference computation context?

**Associative Property in Linear Algebra**: The core factorization relies on splitting operations like $(W_A x_A + W_B x_B)$; understanding why this is mathematically valid is key to the IR design. *Quick check*: Why can $W^T x$ be computed as $W_1^T x_1 + W_2^T x_2$ if $x = [x_1, x_2]$?

**Sobol Sensitivity Analysis**: The Greedy optimizer uses Sobol indices to weight cost factors (cardinality ratio, etc.) for its decision-making heuristic. *Quick check*: What does a high Sobol sensitivity index for "Cardinality Ratio" imply about the push-down decision logic?

## Architecture Onboarding

**Component map**: SQL/UDFs -> Parser/Lowering -> Analyzer -> Optimizer -> Executor. The Parser converts inference logic into an Expression Graph (DAG of ML/Relational ops). The Analyzer (Algorithm 3) identifies factorizable subgraphs. The Optimizer selects push-down plan using either Genetic or Greedy algorithms. The Executor runs on Velox engine.

**Critical path**: The Factorization Analysis (Algorithm 3). If the system fails to correctly identify which sub-expressions depend on which tables, the entire optimization is invalid. This determines the "Degree of Freedom" for the optimizer.

**Design tradeoffs**: Genetic vs. Greedy: Genetic explores the exponential space more thoroughly but has high latency overhead ($8\times$ to $21\times$ higher than Greedy). Greedy is fast and consistent but relies on learned heuristics. Decoupling: The system optimizes join order first, then factorization, reducing search space complexity but theoretically risking missing the global optimum.

**Failure signatures**: "Negative Speedup" if the optimizer chooses a node where output dimension is large, causing I/O cost to outweigh CPU savings. Timeout if Genetic optimizer runs too many generations for complex queries. Accuracy loss if incorrect implementation of aggregation logic causes floating point errors.

**First 3 experiments**: 
1. **Sanity Check (Single Table)**: Run an inference query over a single table. Verify InferF does not apply factorization (or does so with zero overhead).
2. **Star Schema Stress Test**: Create a 5-way join where the central "fact" table is large and "dimension" tables are small. Compare "Full Factorization" vs. InferF. Verify if InferF correctly leaves heavy computation on the fact table or intermediate nodes.
3. **Break the Cost Model**: Manually inject a query where intermediate join cardinality is misestimated. Observe if Greedy optimizer picks a suboptimal node compared to the Genetic optimizer.

## Open Questions the Paper Calls Out

**Open Question 1**: Can factorization strategies be co-designed with the model training process to explicitly account for and preserve cross-tuple dependencies in relational data? The paper identifies this as a future direction, noting that current models assume i.i.d. datasets, violating relational dependencies.

**Open Question 2**: Under what specific conditions (e.g., data skew, query shapes) does the decoupling of join ordering from factorization planning fail to find a globally optimal plan compared to a joint optimization strategy? While the paper argues decoupling is reasonable to reduce search space, it relies on empirical observation rather than formal proof.

**Open Question 3**: How can the "analyzable expression" requirement be relaxed to support factorization of truly opaque, black-box UDFs that cannot be statically lowered into the InferF IR? The framework currently excludes models that cannot be decomposed into analyzable IR components.

## Limitations
- Cost estimation for ML operators remains underspecified with no details on training methodology or feature selection
- Genetic optimizer's stochastic nature introduces performance variance across runs
- Sobol sensitivity analysis methodology for greedy optimizer lacks implementation details
- Requires inference workflows to be represented in specific analyzable Intermediate Representation

## Confidence
- **High confidence** in the core factorization mechanism and its mathematical validity
- **Medium confidence** in the comparative performance claims and benchmark setup
- **Low confidence** in the generalizability of the cost model across different ML frameworks and hardware configurations

## Next Checks
1. Implement a simplified version of Algorithm 3 to identify factorizable subgraphs and verify correctness on basic FFNN queries with synthetic data
2. Measure Sobol sensitivity indices for the greedy optimizer's cost factors on a representative query workload to validate the feature weighting
3. Conduct ablation studies comparing "All Push-down," "Full Factorization," and InferF variants on a 5-table star schema with controlled cardinalities to isolate the impact of selective push-down decisions