---
ver: rpa2
title: An Epidemiological Knowledge Graph extracted from the World Health Organization's
  Disease Outbreak News
arxiv_id: '2509.02258'
source_url: https://arxiv.org/abs/2509.02258
tags:
- data
- disease
- outbreak
- information
- extracted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a daily-updated knowledge graph (eKG) of epidemiological\
  \ data extracted from WHO Disease Outbreak News using an ensemble of large language\
  \ models (LLMs). The ensemble approach\u2014combining Mistral-7B-OpenOrca, Zephyr-7B-Beta,\
  \ and Meta-Llama-3-70B-Instruct\u2014achieves superior performance in extracting\
  \ disease names, countries, dates, cases, and deaths compared to individual models\
  \ and commercial alternatives."
---

# An Epidemiological Knowledge Graph extracted from the World Health Organization's Disease Outbreak News

## Quick Facts
- **arXiv ID:** 2509.02258
- **Source URL:** https://arxiv.org/abs/2509.02258
- **Reference count:** 40
- **Primary result:** Ensemble of three open-source LLMs extracts epidemiological entities from WHO DONs with F1 up to 0.962 for country and 0.851 for disease, enabling a FAIR, daily-updated knowledge graph.

## Executive Summary
This study develops a daily-updated epidemiological knowledge graph (eKG) extracted from WHO Disease Outbreak News (DONs) using an ensemble of large language models (LLMs). The ensemble—combining Mistral-7B-OpenOrca, Zephyr-7B-Beta, and Meta-Llama-3-70B-Instruct—achieves superior performance in extracting disease names, countries, dates, cases, and deaths compared to individual models and commercial alternatives. Evaluated against a gold-standard subset of the Incident Database, the ensemble attains an F1 score of 0.851 for disease extraction, 0.962 for country extraction, 0.869 for case counts, and 0.658 for dates. The resulting eKG contains approximately 2.9K outbreak events with ~26K triples, offering a FAIR, interoperable resource for public health research and surveillance.

## Method Summary
The method employs an ensemble of three open-source LLMs to extract structured epidemiological data from WHO DONs. Each model runs a prompt-engineered extraction to produce JSON objects for disease, country, date, cases, and deaths. Outputs are aggregated via majority voting, with textual fields normalized using Sentence-BERT semantic similarity clustering (threshold 0.8) to resolve synonyms. Long reports (>8K tokens) are first summarized using a dedicated prompt. The final JSON per report is mapped to an ontology (IDO, GeoNames) and emitted as RDF triples, then served via SPARQL endpoint and linked data interfaces.

## Key Results
- Ensemble achieves F1 of 0.851 for disease extraction, outperforming individual models and commercial GPT alternatives.
- Country extraction reaches F1 of 0.962; case count extraction F1 of 0.869; date extraction weakest at F1 0.658.
- Knowledge graph contains ~2.9K outbreak events and ~26K triples, accessible via SPARQL and linked data browsers.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An ensemble of three open-source LLMs with majority voting achieves higher F1 scores for epidemiological information extraction than any single model in the ensemble.
- Mechanism: Each LLM independently extracts entities from the same DON report; majority voting selects the most agreed-upon output per field, canceling isolated model errors while preserving consensus predictions.
- Core assumption: Model errors are partially uncorrelated across the three LLMs so that voting reduces variance more than it introduces bias.
- Evidence anchors:
  - [abstract] "the ensemble approach—combining Mistral-7B-OpenOrca, Zephyr-7B-Beta, and Meta-Llama-3-70B-Instruct—achieves superior performance in extracting disease names, countries, dates, cases, and deaths compared to individual models and commercial alternatives."
  - [section: Methods - Ensemble] "Ensemble methods combining the predictions from multiple models are able to improve the overall performance... by pooling together the strengths and mitigating the weaknesses of various models."
- Break condition: If model errors become correlated (e.g., shared tokenizer biases), majority voting may not improve and could amplify systematic errors.

### Mechanism 2
- Claim: Semantic similarity clustering with SBERT (threshold 0.8) enables meaningful majority voting over textual fields like disease names and country names.
- Mechanism: Before voting, textual outputs are clustered by syntactic similarity, WordNet synset overlap, and SBERT semantic similarity; items within a cluster are treated as equivalent, allowing majority voting across semantically equivalent but lexically distinct terms.
- Core assumption: A cosine similarity threshold of 0.8 accurately separates same-concept pairs from different-concept pairs in the epidemiological domain.
- Evidence anchors:
  - [section: Methods - Ensemble] "We set an experimental threshold of 0.8 for the semantic similarity... two terms having a semantic similarity larger than this threshold were considered to represent the same concept."
  - [section: Methods - Ensemble] Uses all-mpnet-base-v2 for countries and BioBERT for disease names.
- Break condition: If threshold is too low, distinct concepts may incorrectly merge; if too high, synonyms may fail to cluster, fragmenting votes and reducing ensemble benefit.

### Mechanism 3
- Claim: Daily ETL from WHO DONs to RDF knowledge graph enables FAIR, queryable epidemiological surveillance via SPARQL endpoints and linked data interfaces.
- Mechanism: Pipeline fetches new DON reports daily, cleans/normalizes text, runs LLM ensemble extraction, maps entities to ontology classes (IDO, GeoNames), emits RDF triples, and exposes via SPARQL and browsers.
- Core assumption: WHO DONs continue publishing in a consistent HTML/text structure; prompt-based extraction remains stable across LLM versions.
- Evidence anchors:
  - [abstract] "The extracted information is made available in a daily-updated dataset and a knowledge graph... offering a FAIR, interoperable resource for public health research and surveillance."
  - [section: Methods - FAIR Publishing] Details RDF/OWL, IDO/GeoNames mappings, and SPARQL endpoint.
- Break condition: If WHO changes DON format significantly or LLM prompt adherence degrades, extraction quality may drop; date extraction (F1=0.658) is the weakest link and may limit temporal queries.

## Foundational Learning

- **Concept:** Transformer attention and LLM prompting for IE
  - Why needed here: The ensemble relies on prompt-engineered extraction from generative LLMs; understanding attention, context windows (8K tokens), and in-context behavior helps debug failures and iterate prompts.
  - Quick check question: Can you explain why an 8K-token context window might require chunking+summarization for long DON reports?

- **Concept:** Knowledge graphs and RDF triples (subject-predicate-object)
  - Why needed here: The eKG is structured as RDF triples with ontology alignments (IDO, GeoNames); you need to query via SPARQL and understand class/property semantics.
  - Quick check question: What is the difference between an RDF triple and a property graph edge, and how does SPARQL query each?

- **Concept:** Ensemble voting and semantic similarity for NER normalization
  - Why needed here: The core mechanism is majority voting across LLM outputs after normalizing textual variants via SBERT; you must tune thresholds and understand precision/recall trade-offs.
  - Quick check question: If three models output "MERS", "MERS-CoV", and "Middle East respiratory syndrome", how would you determine if they should count as the same vote?

## Architecture Onboarding

- **Component map:** Data ingestion (daily ETL) -> IE layer (3 LLMs + chunking+summarization) -> Normalization layer (syntactic/WordNet/SBERT clustering) -> Ensemble layer (majority voting) -> KG construction (ontology mapping) -> Serving layer (SPARQL endpoint, Virtuoso, LodView, LodLive).

- **Critical path:** Prompt → LLM extraction → semantic clustering → majority vote → ontology mapping → RDF triple emission → SPARQL query. Any failure in clustering or voting propagates directly to KG quality.

- **Design tradeoffs:**
  - Open-source LLMs vs. commercial GPT models: avoids usage limits/cost but requires on-prem GPU resources.
  - Fixed SBERT threshold (0.8) vs. adaptive: simpler but may not generalize across disease/country vocabularies.
  - Daily batch ETL vs. streaming: simpler, but introduces latency for very recent outbreaks.

- **Failure signatures:**
  - Low F1 on dates (0.658) → many events lack valid date_extracted; queries filtering by time may miss records.
  - Missing country/disease in output → record excluded (2384 entries after removing missing).
  - Semantic clustering errors → spurious majority votes or fractured consensus.

- **First 3 experiments:**
  1. Reproduce ensemble voting on a held-out 50-report sample; compute per-field P/R/F1 vs. gold annotations; isolate which field degrades most.
  2. Ablate SBERT threshold (try 0.7, 0.85, 0.9) on a synonym-annotated subset; measure clustering precision/recall and downstream voting accuracy.
  3. Query SPARQL endpoint for a known outbreak (e.g., MERS-CoV in Saudi Arabia 2013–2024); compare case counts vs. WHO official time series to validate reconstruction quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ensemble's extraction performance for mortality figures (deaths) be reliably validated given the absence of this label in the benchmark dataset?
- Basis in paper: [explicit] The authors state, "It was not possible to evaluate the task of extraction of the number of deaths as this information was not annotated in the IDB."
- Why unresolved: While case extraction was validated (F1 0.869), the performance for death extraction relies on the assumption of similarity to case extraction, lacking direct empirical evidence.
- What evidence would resolve it: Construction of a new gold-standard dataset annotated with mortality counts to calculate Precision, Recall, and F1 scores specifically for the "deaths" field.

### Open Question 2
- Question: To what extent does the preliminary summarization of long reports (>8K tokens) impact the recall of critical epidemiological entities?
- Basis in paper: [inferred] The methodology describes summarizing reports that exceed the context window using a specific prompt before extraction, potentially losing fine-grained details.
- Why unresolved: Summarization models may omit specific numerical figures or location details considered less salient, which the subsequent extraction step cannot recover.
- What evidence would resolve it: A comparative study evaluating extraction accuracy on long documents processed via summarization versus those processed using larger context windows (e.g., 128k tokens) without summarization.

### Open Question 3
- Question: How does the use of a fixed 0.8 semantic similarity threshold for majority voting affect the disambiguation of distinct but closely related disease strains?
- Basis in paper: [inferred] The paper notes an experimental threshold of 0.8 is used to cluster synonyms (e.g., MERS vs. MERS-CoV) for the voting mechanism.
- Why unresolved: A rigid threshold might incorrectly merge distinct outbreak events (false positives) or fail to group valid synonyms for rare diseases (false negatives).
- What evidence would resolve it: Sensitivity analysis testing the impact of varying similarity thresholds on the precision and recall of disease name clustering.

## Limitations
- Semantic clustering threshold (0.8) is set empirically without domain-specific validation, risking over-merging or under-merging entities.
- Daily ETL pipeline may degrade if WHO DON format changes; no long-term stability testing reported.
- Date extraction F1 (0.658) is weak, potentially limiting temporal queries and analyses.

## Confidence
- **High confidence:** Ensemble majority voting improves over individual models (supported by direct F1 comparison in evaluation).
- **Medium confidence:** Semantic similarity clustering with SBERT threshold 0.8 appropriately normalizes entity names (no direct validation corpus provided).
- **Low confidence:** Daily ETL pipeline will remain robust to future WHO DON format changes (no long-term stability testing reported).

## Next Checks
1. **Temporal robustness test:** Re-run the extraction pipeline on a recent 30-day window of WHO DONs and compare output quality (F1 per field) against the original IDB evaluation set.
2. **Threshold sensitivity analysis:** Systematically vary the SBERT similarity threshold (0.7, 0.8, 0.9) on a held-out set of synonym-annotated disease names and measure impact on downstream voting accuracy.
3. **Real-world query validation:** Use the SPARQL endpoint to retrieve all records for a major ongoing outbreak (e.g., dengue in the Americas), and cross-validate case counts and dates against WHO official situation reports.