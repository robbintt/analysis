---
ver: rpa2
title: 'ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants'
arxiv_id: '2508.03936'
source_url: https://arxiv.org/abs/2508.03936
tags:
- code
- reasoning
- input
- figure
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASTRA is an automated red-teaming system that discovers vulnerabilities
  in AI coding assistants by performing spatial and temporal exploration guided by
  domain-specific knowledge graphs. It systematically probes input spaces and reasoning
  processes to uncover realistic, domain-relevant safety flaws.
---

# ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants

## Quick Facts
- **arXiv ID:** 2508.03936
- **Source URL:** https://arxiv.org/abs/2508.03936
- **Reference count:** 40
- **Primary result:** Automated system discovers 11-66% more AI coding assistant vulnerabilities than baselines through spatial-temporal exploration guided by domain knowledge graphs

## Executive Summary
ASTRA is an automated red-teaming system that systematically discovers safety vulnerabilities in AI coding assistants by exploring both input spaces (spatial) and reasoning processes (temporal). The system uses domain-specific knowledge graphs to structure exploration and identifies vulnerabilities that existing techniques miss. Across secure code generation and software security guidance domains, ASTRA found significantly more vulnerabilities and demonstrated that temporal exploration alone can increase attack success rates by 6-39%.

## Method Summary
ASTRA employs a three-stage pipeline: (1) Offline domain modeling where experts define abstraction dimensions and LLM interrogation generates knowledge graph hierarchies, (2) Online spatial exploration using probabilistic sampling over the knowledge graph to identify boundary cases, and temporal exploration that analyzes reasoning traces to find and exploit flawed logic, and (3) Alignment training using discovered vulnerabilities to improve model safety. The system uses a composite online judge trained via SFT+RL to evaluate responses, with rewards for format compliance, verdict accuracy, and reasoning consistency.

## Key Results
- Found 11-66% more vulnerabilities than existing techniques across two domains
- Temporal exploration increased attack success rates by 6-39% depending on target model
- Spatial exploration outperformed bandit baselines in efficiency
- Alignment training with discovered vulnerabilities improved effectiveness by 17%
- Deliberative alignment showed 50-60% defense success rates with better robustness

## Why This Works (Mechanism)

### Mechanism 1: Abstraction-Guided Probabilistic Sampling Over Input Dimensions
Structuring the input space as hierarchical knowledge graphs across orthogonal dimensions enables efficient identification of boundary cases where models disagree on safety. The system decomposes each domain into 6-8 dimensions with manually constructed upper abstraction hierarchies and LLM-generated leaf nodes. A Beta distribution over each node's success/failure counters guides path selection from root to leaf, propagating outcomes upward to generalize findings across abstract classes.

### Mechanism 2: Temporal Exploration via Reasoning-Trace Discrepancy Detection
When models correctly refuse unsafe requests but do so through flawed reasoning chains, targeted paraphrasing that "fixes" the incorrectly identified issue while preserving malicious intent exposes latent alignment failures. For each boundary case, the system constructs decision diagrams encoding valid reasoning paths from high-capacity models. During online testing, if the target model refuses a prompt, ASTRA extracts its chain-of-thought and identifies three discrepancy types: missing steps, wrong steps, and additional steps.

### Mechanism 3: Compositional and Factual Abstraction for Realistic Threat Modeling
Decomposing malicious behaviors into sequences of benign-appearing primitive operations (compositional abstraction) and embedding them in temporally-delayed factual references (factual instantiation) bypasses alignment that relies on recognizing overt malicious patterns. Rather than "write ransomware," prompts request independent steps (list files, encrypt, delete originals, display message) that collectively encode malicious semantics. The "factual lag" in training data means obscure or newly-emerged dangerous references are not recognized as unsafe.

## Foundational Learning

- **Concept: Gibbs Sampling and Bayesian Posterior Updates**
  - Why needed here: ASTRA's spatial exploration adapts Gibbs sampling to traverse high-dimensional prompt spaces. Understanding how MCMC methods balance exploration vs. exploitation via posterior updates is essential for grasping why the algorithm outperforms bandit baselines.
  - Quick check question: Given a binary outcome (safe/unsafe) at a leaf node, how would you update the Beta distribution parameters α and β for that node and propagate the update to its parent abstraction class?

- **Concept: Chain-of-Thought Reasoning Extraction and Verification**
  - Why needed here: Temporal exploration relies on extracting and comparing reasoning traces against decision diagrams. You need to understand how LLMs generate CoT, what makes a reasoning step "valid," and how to algorithmically compare trace structures.
  - Quick check question: If a model refuses a prompt with reasoning "This request is unsafe because it involves encryption, which can be used maliciously," but the prompt actually involves file deletion, what discrepancy type is this and what paraphrasing strategy applies?

- **Concept: Static Analysis Concepts (Source-Sink-Path)**
  - Why needed here: The online judge for secure code generation is trained to mimic static analyzers by identifying tainted data flows. Understanding source (untrusted input origin), sink (dangerous operations), and sanitization checks along paths is critical for interpreting judge outputs and training data construction.
  - Quick check question: In the file upload example, the check `if '/' in filename` is a sanitization attempt. Why is it insufficient for preventing unrestricted file upload, and what additional check would a complete sanitizer require?

## Architecture Onboarding

- **Component map:**
  Stage 1 (Offline): Domain Expert → Dimension Selection → LLM Interrogation Agent → Abstraction Hierarchies → Oracle Ensemble (CodeGuru + Blue-Team Models) → Probabilistic KG with Boundary Cases
  
  Stage 2 (Online): KG Sampler → Target Model → Online Judge (8B Reasoning Model) → Posterior Update → [Spatial Loop]
  Target Model Refusal → CoT Extraction → Decision Diagram Matcher → Discrepancy Classifier → Paraphraser → [Temporal Loop]
  
  Stage 3 (Alignment): Violation-Inducing Cases + Safe Responses → Balanced Dataset Construction → SFT + RL Training

- **Critical path:** The online judge accuracy (F1=32 on DA samples) is the bottleneck. If the judge misclassifies responses, posterior updates corrupt the KG, and temporal exploration wastes queries on already-safe prompts. Prioritize judge improvement before scaling query budgets.

- **Design tradeoffs:**
  - Manual vs. automated hierarchy construction: Authors found LLM-generated upper hierarchies either too granular or too coarse. Manual construction for upper levels trades scalability for alignment with domain semantics.
  - Oracle cost vs. coverage: Using Claude 3.7 + CodeGuru as oracle is expensive but necessary for boundary-case identification. The lightweight online judge trades some accuracy (especially on DA fixes) for query efficiency.
  - Spatial vs. temporal budget allocation: Temporal exploration requires models to expose reasoning, which some hardened systems don't do. Spatial is more universally applicable but may miss reasoning-based vulnerabilities.

- **Failure signatures:**
  - ASR plateaus early in spatial exploration despite increasing queries → Abstraction dimensions don't capture failure modes; add/revise dimensions.
  - Temporal exploration shows <10% improvement on all targets → Models either refuse all follow-ups (check refusal logs) or produce sound reasoning (inspect CoT quality).
  - Judge shows high precision but low recall on DA samples → Model is overfitting to explicit vulnerabilities and missing subtle semantic fixes; augment training with adversarially similar safe/unsafe pairs.
  - Alignment training degrades utility metrics (HumanEval pass@1 drops >5%) → Utility samples underrepresented in training corpus; increase utility sample ratio in balanced dataset.

- **First 3 experiments:**
  1. Validate spatial exploration efficiency: Run ASTRA on a held-out code model with query budgets of 50, 100, 200, 500. Compare cumulative ASR against (a) uniform random sampling from the KG and (b) bandit baseline. Plot ASR vs. queries to verify claimed efficiency gains.
  2. Isolate temporal exploration contribution: For 5 boundary-case prompts where the target model refuses, manually classify discrepancy types in its CoT. Apply corresponding paraphrasing strategies and measure attack success rate. Compare against unparaphrased prompts to quantify per-discrepancy-type effectiveness.
  3. Characterize judge failure modes: Sample 50 false negatives from the online judge on DA-protected outputs. Manually analyze whether failures stem from (a) subtle semantic fixes, (b) novel vulnerability patterns outside training distribution, or (c) reasoning trace quality issues. Use findings to prioritize judge training data augmentation.

## Open Questions the Paper Calls Out
None

## Limitations
- Oracle ensemble dependence creates cost and availability constraints that aren't quantified for sensitivity
- Online judge accuracy bottleneck (F1=32 on DA samples) creates critical failure points with unmeasured false negative rates
- Manual abstraction hierarchy construction trades scalability for alignment quality without reporting human effort requirements
- Temporal exploration effectiveness varies significantly (6-39% ASR improvement) based on target model reasoning trace accessibility

## Confidence

- **High Confidence:** Abstraction-guided probabilistic sampling mechanism is well-supported by quantitative results (11-66% improvement) and mathematical framework
- **Medium Confidence:** Temporal exploration mechanism shows measurable effectiveness (6-39% ASR improvement) but depends on model characteristics
- **Medium Confidence:** Compositional and factual abstraction claims are supported by ablation studies but lack broader validation
- **Low Confidence:** Alignment training effectiveness claims (17% improvement) based on single experiment without component-level ablation

## Next Checks

1. **Judge Performance Characterization:** Sample 100 false negatives from the online judge on DA-protected samples and manually categorize failure modes. Measure false negative rate to identify whether judge improvements or training data augmentation would be more effective.

2. **Oracle Sensitivity Analysis:** Run the spatial exploration pipeline using only a single oracle (CodeGuru) and compare ASR results against the ensemble approach. Quantify how oracle availability and accuracy impact vulnerability discovery rates.

3. **Temporal Exploration Conditions:** For 20 boundary-case prompts where the target model refuses, manually classify whether each refusal has accessible CoT, whether reasoning is sound, and whether the model accepts follow-up prompts. Correlate these characteristics with observed temporal exploration effectiveness.