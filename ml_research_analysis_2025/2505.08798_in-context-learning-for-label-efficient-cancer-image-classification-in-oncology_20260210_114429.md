---
ver: rpa2
title: In-Context Learning for Label-Efficient Cancer Image Classification in Oncology
arxiv_id: '2505.08798'
source_url: https://arxiv.org/abs/2505.08798
tags:
- paligemma
- gpt-4o
- align
- shot
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores in-context learning (ICL) as a label-efficient
  approach for cancer image classification in oncology. By using few-shot prompting
  with vision-language models (VLMs) like GPT-4o, Paligemma, CLIP, and ALIGN, the
  authors evaluate performance across three oncology datasets (MHIST, PatchCamelyon,
  HAM10000) without model retraining.
---

# In-Context Learning for Label-Efficient Cancer Image Classification in Oncology

## Quick Facts
- arXiv ID: 2505.08798
- Source URL: https://arxiv.org/abs/2505.08798
- Reference count: 37
- Primary result: GPT-4o achieved F1 scores of 0.81 (binary) and 0.60 (multi-class) using in-context learning without model retraining

## Executive Summary
This study evaluates in-context learning (ICL) as a label-efficient approach for cancer image classification using vision-language models (VLMs). The authors demonstrate that VLMs like GPT-4o, Paligemma, CLIP, and ALIGN can perform cancer classification tasks through few-shot prompting without requiring model retraining. The approach shows particular promise for rare cancers and resource-constrained clinical environments where traditional fine-tuning is infeasible due to limited labeled data.

## Method Summary
The study employs in-context learning with four VLMs: GPT-4o, Paligemma, CLIP, and ALIGN. Researchers use few-shot prompting by providing examples of correctly classified images alongside query images. Performance is evaluated across three oncology datasets: MHIST, PatchCamelyon, and HAM10000, testing both binary and multi-class classification scenarios. The methodology avoids model retraining while leveraging the VLMs' pre-trained knowledge to classify cancer images.

## Key Results
- GPT-4o achieved F1 scores of 0.81 for binary classification and 0.60 for multi-class classification
- All VLMs showed consistent performance improvements with few-shot examples
- Open-source models (Paligemma, CLIP) demonstrated competitive performance with GPT-4o
- Notable performance gains observed in underrepresented classes

## Why This Works (Mechanism)
In-context learning leverages the pre-trained knowledge of vision-language models to adapt to new classification tasks without parameter updates. By providing a small set of labeled examples within the prompt, the model can generalize from these demonstrations to classify unseen cancer images. This approach capitalizes on the VLMs' broad visual understanding and language reasoning capabilities developed during pretraining.

## Foundational Learning
- **Vision-Language Models**: Neural networks trained on both images and text, enabling cross-modal reasoning. Needed for bridging visual patterns with clinical terminology. Quick check: Verify model architecture includes both vision and language encoders.
- **Few-Shot Learning**: Learning from limited examples without explicit training. Needed for clinical scenarios with scarce labeled data. Quick check: Count examples per class in prompts.
- **In-Context Learning**: Prompting strategy where model learns from examples within the prompt. Needed to avoid expensive retraining. Quick check: Verify examples are formatted correctly in prompts.
- **F1 Score**: Harmonic mean of precision and recall. Needed to balance false positives and false negatives in medical diagnosis. Quick check: Calculate both precision and recall separately.
- **Classification Thresholding**: Decision boundary setting for binary/multi-class classification. Needed to optimize for clinical sensitivity/specificity requirements. Quick check: Plot ROC curves for different thresholds.

## Architecture Onboarding

**Component Map:**
Vision Encoder -> Text Encoder -> Cross-Attention Layer -> Classification Head

**Critical Path:**
Input image -> Vision encoder embeddings -> Cross-attention with text embeddings -> Classification logits -> Final prediction

**Design Tradeoffs:**
- GPT-4o: Higher accuracy but higher inference costs and computational requirements
- Open-source models: Lower costs and better accessibility but potentially reduced accuracy
- Few-shot examples: More examples improve accuracy but increase prompt token costs and complexity

**Failure Signatures:**
- Poor performance on underrepresented classes with insufficient few-shot examples
- Sensitivity to prompt formatting and example selection quality
- Decreased accuracy with image quality variations not present in training data

**First 3 Experiments:**
1. Vary number of few-shot examples (1-5 per class) to identify optimal prompt length
2. Test model performance across different image quality levels and artifacts
3. Compare classification accuracy for rare cancer subtypes versus common types

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to standardized datasets that may not reflect clinical imaging complexity
- No assessment of computational latency or token costs for practical deployment
- Performance on truly rare cancer subtypes with minimal training examples remains unverified

## Confidence

**High confidence**: In-context learning achieves reasonable classification performance without model retraining across multiple VLMs and datasets.

**Medium confidence**: Comparative performance between VLMs is demonstrated but may not capture all clinical scenarios and image quality variations.

**Medium confidence**: Benefits for rare cancers and data-limited settings are theoretically sound but require validation with truly scarce data scenarios.

## Next Checks
1. Validate approach on diverse, multi-institutional clinical datasets with varying image quality and acquisition protocols
2. Evaluate performance specifically on rare cancer subtypes with minimal training examples (fewer than 10-20 cases)
3. Measure inference latency, computational requirements, and per-case costs across clinical workflow scenarios