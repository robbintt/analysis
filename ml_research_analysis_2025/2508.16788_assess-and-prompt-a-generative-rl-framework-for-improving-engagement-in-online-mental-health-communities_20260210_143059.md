---
ver: rpa2
title: 'Assess and Prompt: A Generative RL Framework for Improving Engagement in Online
  Mental Health Communities'
arxiv_id: '2508.16788'
source_url: https://arxiv.org/abs/2508.16788
tags:
- support
- event
- effect
- more
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning framework to improve
  engagement in online mental health communities by prompting users to better articulate
  their posts. The authors propose REDDME, a dataset of 4,760 mental health posts
  annotated for event, effect, and requirement attributes, along with a hierarchical
  taxonomy CUETAXO for controlled question generation.
---

# Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities

## Quick Facts
- arXiv ID: 2508.16788
- Source URL: https://arxiv.org/abs/2508.16788
- Reference count: 40
- This paper introduces a reinforcement learning framework to improve engagement in online mental health communities by prompting users to better articulate their posts, achieving consistent improvements across all metrics over strong baselines.

## Executive Summary
This paper presents MH-COPILOT, a reinforcement learning framework that generates targeted questions to elicit missing support attributes from mental health posts, thereby improving engagement in online mental health communities. The system identifies gaps in event, effect, and requirement descriptions and prompts users with controlled questions to provide more complete information. Evaluated on the REDDME dataset with 4,760 annotated Reddit posts, the approach demonstrates consistent improvements across multiple metrics when using Gemma-2 with DPO fine-tuning, showing +27.49% ROUGE-L, +17.80% BLEU-4, +2.81% BERTScore, and +17.54% METEOR gains over strong baselines.

## Method Summary
The framework consists of four sequential modules: CSpan for extracting event/effect/requirement spans using RoBERTa-large with BIO tagging, an Intensity Classifier (RoBERTa) to predict attribute presence levels (0-2) using ordinal and MSE loss, a Question Generator (Gemma-2 with QLoRA) trained via supervised fine-tuning on CUETAXO taxonomy-based prompts, and a Verifier reward model with four components assessing context, empathy, coherence, and structure. DPO fine-tuning uses verifier scores to generate preference pairs, optimizing question quality. The system processes mental health posts to identify missing attributes and generates targeted questions to elicit more complete information.

## Key Results
- ROUGE-L score improved by +27.49% over SFT baselines
- BLEU-4 score improved by +17.80% over SFT baselines
- BERTScore improved by +2.81% over SFT baselines
- METEOR score improved by +17.54% over SFT baselines
- Human evaluation confirmed higher quality in contextual relevance, empathy, and coherence

## Why This Works (Mechanism)
The framework works by systematically identifying gaps in mental health posts through span extraction and intensity classification, then generating targeted questions using a taxonomy-guided approach. The CUETAXO hierarchy provides structured control over question generation, ensuring questions are appropriate to the missing information level. The verifier reward model with multiple components (context, empathy, coherence, structure) provides nuanced feedback for DPO fine-tuning, improving question quality beyond simple text similarity metrics.

## Foundational Learning
- **BIO tagging for span extraction**: Why needed - to identify boundaries of event, effect, and requirement spans in informal Reddit text. Quick check - verify BIO tag consistency across training samples.
- **Ordinal classification with MSE loss**: Why needed - to capture intensity levels (absent, moderate, present) as ordered categories rather than independent classes. Quick check - compare ordinal vs standard cross-entropy performance on validation set.
- **QLoRA for efficient fine-tuning**: Why needed - to enable Gemma-2 fine-tuning on limited GPU memory while maintaining performance. Quick check - verify memory usage stays within 1x A100 (80GB) + 1x RTX A6000 (50GB) constraints.
- **DPO with verifier rewards**: Why needed - to optimize question quality based on multiple criteria rather than just text similarity. Quick check - compare human evaluation scores before/after DPO.
- **CUETAXO taxonomy for controlled generation**: Why needed - to ensure generated questions are appropriate to the missing information level and attribute type. Quick check - validate template mapping accuracy for all 5 levels × 3 attributes.
- **Preference pair generation from verifier scores**: Why needed - to create synthetic training data for DPO without requiring human annotations. Quick check - manually inspect sample preference pairs for quality.

## Architecture Onboarding

**Component Map**: Reddit post → CSpan → Intensity Classifier → Question Generator → Verifier → DPO-fined Gemma-2

**Critical Path**: The pipeline processes posts sequentially through CSpan (span extraction) → Intensity Classifier (attribute detection) → Question Generator (controlled question generation) → Verifier (quality assessment) → DPO optimization.

**Design Tradeoffs**: Uses Gemma-2 (smaller model) with QLoRA instead of larger models to reduce computational cost while maintaining quality. Employs synthetic preference pairs from verifier rather than human annotations to scale DPO training. Implements hierarchical taxonomy for controlled generation rather than free-form prompting to ensure question relevance.

**Failure Signatures**: 
- Span extraction errors (52.52% F1) lead to incorrect attribute identification
- Intensity misclassification causes inappropriate question generation (effect questions when effect is already present)
- Verifier reward aggregation may not properly balance competing criteria
- DPO may degrade fluency despite small metric gains

**First Experiments**:
1. Train CSpan on REDDME with RoBERTa-large and evaluate BIO F1 score
2. Train Intensity Classifier with ordinal loss and compare to MSE variant
3. SFT Gemma-2 with CUETAXO prompts and evaluate on held-out validation set

## Open Questions the Paper Calls Out
- Does prompting users with taxonomy-guided questions actually increase response rates in live OMHC deployments?
- Can the framework generalize beyond Reddit to other OMHC platforms with different community norms?
- How does error propagation from upstream modules (CSpan, intensity classifier) affect downstream question quality?
- What are the optimal reward model components and weights for balancing empathy, relevance, and structural adherence?

## Limitations
- Evaluation relies on synthetic preference pairs rather than real user feedback, raising concerns about distributional shift
- Human evaluation sample size (75 posts) is modest for assessing nuanced qualities like empathy
- Framework assumes three attributes (event, effect, requirement) capture sufficient missing information
- No deployment study or longitudinal analysis was conducted to validate clinical utility

## Confidence
- **High confidence**: Baseline performance gains (ROUGE-L +27.49%, BLEU-4 +17.80%) are well-supported by methodology and dataset structure
- **Medium confidence**: DPO improvement claims depend on verifier reliability and preference pair quality, which are not fully specified
- **Low confidence**: Claims about clinical utility and real-world engagement improvements cannot be validated without deployment study

## Next Checks
1. **Verifier ablation study**: Evaluate MH-COPILOT performance with and without the verifier reward model to quantify DPO contribution and assess reward hacking risks
2. **Preference pair quality audit**: Sample and manually review synthetic preference pairs used for DPO training to ensure they reflect genuine quality differences
3. **Clinical expert review**: Have mental health professionals assess generated questions for clinical appropriateness, potential harm, and therapeutic alignment