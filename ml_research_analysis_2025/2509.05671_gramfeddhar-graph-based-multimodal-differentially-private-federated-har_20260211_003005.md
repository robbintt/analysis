---
ver: rpa2
title: 'GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR'
arxiv_id: '2509.05671'
source_url: https://arxiv.org/abs/2509.05671
tags:
- federated
- privacy
- learning
- centralized
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraMFedDHAR introduces a graph-based multimodal federated learning
  framework for privacy-preserving human activity recognition. It models each sensor
  stream as a modality-specific graph, processes them through residual GCNs, and fuses
  the embeddings using attention-based weighting.
---

# GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR

## Quick Facts
- arXiv ID: 2509.05671
- Source URL: https://arxiv.org/abs/2509.05671
- Authors: Labani Halder; Tanmay Sen; Sarbani Palit
- Reference count: 23
- One-line primary result: Graph-based multimodal framework with federated DP outperforms FFN baselines by 7-13% under strict privacy constraints on MEx dataset.

## Executive Summary
GraMFedDHAR introduces a graph-based multimodal federated learning framework for privacy-preserving human activity recognition. It models each sensor stream as a modality-specific graph, processes them through residual GCNs, and fuses the embeddings using attention-based weighting. Differential privacy is integrated into the federated aggregation to ensure user-level privacy. Experiments on the MEx dataset show that the MultiModalGCN outperforms the baseline MultiModalFFN by up to 2% in non-DP settings and by 7-13% under DP constraints, demonstrating superior resilience to privacy-induced noise. Federated learning consistently outperforms centralized learning under strict privacy budgets, and multi-modal combinations further enhance performance.

## Method Summary
The method processes multimodal sensor data (accelerometers, depth camera, pressure mat) from the MEx dataset through modality-specific GCN encoders with residual connections. Each modality is preprocessed differently: DCT for accelerometers, autoencoders for vision-based sensors, then embedded into graph nodes with distance-based edges. The framework applies federated averaging with differential privacy (Gaussian noise, gradient clipping) at the client level. Attention-based fusion combines modality embeddings before classification. The system is evaluated across varying privacy budgets (ε = 0.5 to 2.0) and compared against centralized baselines and FFN alternatives.

## Key Results
- MultiModalGCN outperforms MultiModalFFN by 7-13% accuracy under strict DP constraints (ε=0.5)
- Federated learning shows significantly better privacy-utility trade-off than centralized training under tight privacy budgets
- Attention-based fusion successfully compensates for weak modalities (e.g., pressure mat alone at 0.37 Acc)
- Increasing client participation in federated setting reduces DP noise impact through statistical averaging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GCNs retain higher accuracy than FFNs under DP noise injection
- **Mechanism**: GCN message-passing aggregates features from neighboring nodes, acting as a low-pass filter that partially cancels uncorrelated Gaussian DP noise through averaging
- **Core assumption**: Graph topology correctly links relevant data points so averaging neighbors is semantically meaningful
- **Evidence anchors**: Abstract states GNNs prove more resilient to DP degradation; section 6.3.1 shows GCN drops ~18% vs FFN drops >29% under strong DP

### Mechanism 2
- **Claim**: Attention-based fusion outperforms simple concatenation by suppressing uninformative or noisy modalities
- **Mechanism**: Fusion layer assigns learnable scalar weights to modality embeddings, allowing network to downweight low-quality sensor streams
- **Core assumption**: At least one modality provides strong signal for any given activity class
- **Evidence anchors**: Abstract mentions attention-based weighting; section 6.3.7 ablation shows 'pm' modality performs poorly alone yet contributes when fused

### Mechanism 3
- **Claim**: FedAvg with high client participation mitigates utility loss from DP noise better than centralized training
- **Mechanism**: Averaging updates from m clients reduces effective noise variance from σ² to σ²/m through independent noise cancellation
- **Core assumption**: Client data distributions are diverse yet aligned enough to allow convergence
- **Evidence anchors**: Section 6.3.1 states federated learning is more resilient to privacy constraints; section 6.3.6 shows increasing client participation boosts accuracy

## Foundational Learning

- **Concept: Residual Graph Convolutional Networks (Res-GCN)**
  - **Why needed here**: Prevents "over-smoothing" where node features become indistinguishable after multiple graph layers
  - **Quick check question**: If you remove residual connections, does model performance drop on 'act' modality due to feature homogenization?

- **Concept: Differential Privacy - Gaussian Mechanism (DP-SGD)**
  - **Why needed here**: Core privacy engine involving gradient clipping and Gaussian noise addition
  - **Quick check question**: As you tighten privacy (decrease ε towards 0), does 'pm' modality accuracy collapse faster than 'dc' due to lower baseline signal-to-noise ratio?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here**: Framework relies on local training and global aggregation
  - **Quick check question**: If clients have non-IID data, would global model drift, and how does local epoch count (20) affect this drift?

## Architecture Onboarding

- **Component map**: Input Sensors -> Preprocessing (DCT/Autoencoder) -> Graph Constructor (Nodes=Windows, Edges=Distance-based) -> Modality-specific GCN Encoder -> Attention-based Fusion -> Classification
- **Critical path**: Graph Construction and Preprocessing are most brittle; poor topology or compressed latent space causes GCN failure regardless of training loop
- **Design tradeoffs**: GCN offers +7-13% accuracy under DP but adds graph construction overhead; Federated learning is more robust to strict DP (ε=0.5) while centralized learning collapses
- **Failure signatures**: Accuracy plateau (~50%) indicates privacy budget too tight; high variance across rounds suggests low client participation; specific class confusion indicates modality/graph failure
- **First 3 experiments**:
  1. Run with fully connected graph vs distance-based graph to verify structural inductive bias helps
  2. Remove 'pm' modality entirely to test if attention mechanism recovers performance
  3. Vary ε (0.1, 0.5, 1.0, 2.0) specifically on centralized version to confirm centralized learning fails faster than FL under strong noise

## Open Questions the Paper Calls Out

- **Question**: How does the graph-based multimodal framework scale to larger, more diverse HAR datasets with significantly more users and activity classes than MEx dataset?
  - **Basis in paper**: Authors state framework "will evolve with larger dataset," acknowledging current work is limited to MEx dataset
  - **Why unresolved**: Current experiments use 30 participants and 7 activities; unknown if convergence guarantees and DP resilience hold with higher complexity or non-IID distributions
  - **What evidence would resolve it**: Evaluation on large-scale HAR benchmarks (e.g., UCI-HAR or PAMAP2) with thousands of users

- **Question**: Can adaptive privacy budgeting strategies be integrated into federated aggregation to improve utility-privacy trade-off compared to fixed noise scales?
  - **Basis in paper**: Conclusion lists "adaptive privacy" as future direction, implying fixed ε and noise parameters may be sub-optimal
  - **Why unresolved**: Paper evaluates fixed privacy budgets (ε = 0.5, 1.5, 2.0) without exploring dynamic mechanisms
  - **What evidence would resolve it**: Modified training with dynamically adjusted σ based on training loss or gradient norms showing improved accuracy for same final privacy cost

- **Question**: Does standard RDP accounting underestimate actual privacy loss given temporal correlations in continuous sensor streams?
  - **Basis in paper**: Introduction states "Traditional Differential Privacy... struggles to account for temporal correlations" yet method uses standard RDP without temporal correction
  - **Why unresolved**: Privacy analysis assumes samples are independent or treats windowing as bound, potentially ignoring correlation between overlapping 60% sliding windows
  - **What evidence would resolve it**: Theoretical analysis or empirical attack simulation demonstrating privacy leakage from 5-second sliding windows with 2-second strides

## Limitations
- Graph construction methodology not fully specified (exact distance metric and threshold values unknown)
- Attention fusion mechanism details incomplete (exact equations and normalization not provided)
- Autoencoder training details for dc/pm modalities unspecified (epochs, lr, optimization, weight sharing)

## Confidence
- **High Confidence**: GCNs outperform FFNs under DP noise is well-supported by 7-13% accuracy differential in section 6.3.1
- **Medium Confidence**: Federated learning advantage over centralized training under strict privacy budgets is empirically demonstrated
- **Medium Confidence**: Attention fusion mechanism's ability to suppress noisy modalities is supported by 'pm' modality ablation study

## Next Checks
1. Run model with fully connected graphs versus proposed distance-based topology to verify structural inductive bias provides claimed benefits
2. Systematically vary privacy budget (ε = 0.1, 0.5, 1.0, 2.0) in centralized version to confirm centralized learning fails faster than FL under strong noise
3. Remove pressure mat ('pm') modality entirely to measure if remaining modalities recover performance through attention mechanism