---
ver: rpa2
title: Reasoning Capabilities and Invariability of Large Language Models
arxiv_id: '2505.00776'
source_url: https://arxiv.org/abs/2505.00776
tags:
- reasoning
- llms
- language
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark dataset designed to evaluate
  the logical reasoning capabilities of Large Language Models (LLMs) using a domain-free
  context with geometric figures, avoiding reliance on world knowledge. The dataset
  contains 432 binary questions testing shallow logical reasoning (existential restrictions,
  negations, quantification) across four variants to assess invariability under linguistic
  changes.
---

# Reasoning Capabilities and Invariability of Large Language Models

## Quick Facts
- arXiv ID: 2505.00776
- Source URL: https://arxiv.org/abs/2505.00776
- Reference count: 40
- Primary result: Most LLMs score near random chance (~51%) on domain-free geometric reasoning tasks, with only a few exceeding 70% accuracy

## Executive Summary
This paper introduces a novel benchmark dataset to evaluate the logical reasoning capabilities of Large Language Models using geometric figures as domain-free context. The dataset contains 432 binary questions testing shallow logical reasoning operations including existential restrictions, negations, and quantification. The study evaluates 24 LLMs ranging from 1.8B to 70B parameters, finding that most models perform near random chance levels, with only a few models exceeding 70% accuracy. The research demonstrates that model performance remains stable across different linguistic variants, suggesting limited impact of prompt engineering on reasoning capabilities.

## Method Summary
The authors created a benchmark dataset using geometric figures to test logical reasoning capabilities while avoiding reliance on world knowledge. The dataset contains 432 binary questions across three types of logical reasoning operations: existential restrictions, negations, and quantification. Four linguistic variants of each question were created to test model invariability under linguistic changes. Twenty-four LLMs ranging from 1.8B to 70B parameters were evaluated using zero-shot prompting, with additional testing of chain-of-thought prompting strategies. Performance was measured against a baseline of random guessing (51% accuracy for binary questions).

## Key Results
- Most LLMs (24 tested) scored around 51% accuracy, indistinguishable from random chance
- Only a few models exceeded 70% accuracy on the geometric reasoning tasks
- Chain-of-thought prompting improved performance when rationale was given after the answer
- Model performance remained stable across four linguistic variants, showing limited impact of prompt engineering

## Why This Works (Mechanism)
Assumption: The geometric figures provide domain-free context that isolates logical reasoning from world knowledge dependencies. The binary question format creates clear ground truth for evaluating reasoning accuracy. The multiple linguistic variants help identify whether models truly understand logical relationships or are exploiting linguistic patterns.

## Foundational Learning
Unknown: The paper does not explicitly discuss what foundational learning principles explain the observed reasoning limitations. The near-random performance suggests models may not be learning genuine logical reasoning capabilities from their training data, but rather pattern matching strategies that fail on domain-free tasks.

## Architecture Onboarding
**Component Map**
Geometric Figures -> Logical Reasoning Operations -> Binary Question Generation -> Model Input -> Model Output -> Performance Evaluation

**Critical Path**
Geometric Figure Description -> Logical Question Formulation -> Model Response Generation -> Accuracy Assessment

**Design Tradeoffs**
Domain-free context (geometric figures) vs. real-world applicability, binary questions vs. nuanced reasoning assessment, zero-shot prompting vs. few-shot learning potential

**Failure Signatures**
Performance near random chance (51%), limited improvement across linguistic variants, inconsistent response to chain-of-thought prompting

**First Experiments**
1. Test models on expanded logical reasoning structures beyond current three types
2. Evaluate few-shot prompting performance with minimal guidance examples
3. Assess robustness to broader linguistic variations including structural changes

## Open Questions the Paper Calls Out
Unknown: The source material does not explicitly identify open questions. However, implicit questions include: Why do some models perform better than others despite similar architectures? What training data characteristics enable better geometric reasoning? How can reasoning capabilities be improved beyond current limitations?

## Limitations
- The domain-free geometric reasoning benchmark may not fully capture real-world reasoning capabilities
- Binary question format with simple logical structures might not adequately challenge sophisticated reasoning
- Only four linguistic variants tested may be insufficient for comprehensive invariance evaluation
- Zero-shot prompting methodology may underestimate model capabilities compared to few-shot approaches

## Confidence
**High confidence**: Experimental methodology using controlled geometric figures is sound for isolating reasoning from world knowledge
**Medium confidence**: Chain-of-thought findings require further validation due to specific prompt formulations
**Low confidence**: Generalizability to broader reasoning tasks beyond tested logical operations remains uncertain

## Next Checks
1. Expand benchmark to include more complex logical structures and reasoning patterns
2. Test model performance with few-shot prompting examples in addition to zero-shot
3. Evaluate robustness across broader range of linguistic variations including different question structures