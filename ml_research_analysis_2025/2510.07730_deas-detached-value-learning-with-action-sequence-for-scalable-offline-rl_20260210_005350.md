---
ver: rpa2
title: 'DEAS: DEtached value learning with Action Sequence for Scalable Offline RL'
arxiv_id: '2510.07730'
source_url: https://arxiv.org/abs/2510.07730
tags:
- learning
- value
- action
- tasks
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEAS is an offline RL framework that learns value functions using
  action sequences through the options framework, enabling horizon reduction without
  explicit goal conditioning. The method addresses value overestimation in offline
  RL by employing detached value learning that decouples critic training from the
  actor, steering estimates toward high-return in-distribution actions.
---

# DEAS: DEtached value learning with Action Sequence for Scalable Offline RL

## Quick Facts
- **arXiv ID**: 2510.07730
- **Source URL**: https://arxiv.org/abs/2510.07730
- **Reference count**: 31
- **Primary result**: DEAS achieves up to 88% success rate on OGBench tasks, outperforming previous methods

## Executive Summary
DEAS introduces a novel offline reinforcement learning framework that learns value functions using action sequences through the options framework, enabling horizon reduction without explicit goal conditioning. The method addresses the critical challenge of value overestimation in offline RL by employing detached value learning that decouples critic training from the actor, steering estimates toward high-return in-distribution actions. The framework demonstrates significant improvements across 30 complex OGBench tasks and scales effectively to large Vision-Language-Action models, showing state-of-the-art performance in both simulated and real-world robotic manipulation tasks.

## Method Summary
DEAS leverages the options framework to learn value functions over action sequences, effectively reducing the effective horizon of the RL problem. The core innovation is detached value learning, where the critic is trained independently from the actor, allowing it to provide more conservative and accurate value estimates. This approach addresses the common overestimation bias in offline RL by focusing on in-distribution actions that are likely to yield high returns. The method combines this with distributional RL objectives and employs dual discount factors to balance short-term and long-term rewards, creating a robust learning framework that can handle complex, long-horizon tasks without explicit goal conditioning.

## Key Results
- Achieves 88% success rate on OGBench tasks versus 69% for previous best methods
- Improves RoboCasa Kitchen VLA success from 12% to 25%
- Enhances real-world manipulation tasks from 64% to 78% success rate

## Why This Works (Mechanism)
DEAS addresses the fundamental challenge in offline RL of learning accurate value functions from fixed datasets without access to online exploration. By using action sequences through the options framework, the method effectively compresses the temporal credit assignment problem, making long-horizon tasks more tractable. The detached value learning component is particularly crucial - by decoupling critic training from actor optimization, the value estimates become more conservative and focused on in-distribution actions, reducing the overestimation bias that plagues many offline RL algorithms. This architectural choice allows the critic to serve as a more reliable guide for policy learning, especially in the critical regions of the state-action space where data is sparse.

## Foundational Learning
- **Options framework**: Hierarchical RL approach using temporally extended actions; needed for horizon reduction in long tasks; check by verifying sub-policies learn coherent behaviors
- **Distributional RL**: Learning full return distributions rather than expectations; needed for more robust value estimates; check by examining return distribution shapes
- **Offline RL fundamentals**: Learning from fixed datasets without exploration; needed as the core problem setting; check by ensuring no online interaction occurs
- **Value overestimation**: Common failure mode in offline RL where estimates exceed true values; needed context for understanding detached learning motivation; check by comparing estimated vs. actual returns
- **Actor-critic architecture**: Separate policy (actor) and value (critic) networks; needed as baseline for understanding decoupled training; check by verifying separate loss functions

## Architecture Onboarding

**Component Map**: State/Input -> Feature Extractor -> Options Encoder -> Action Sequence Predictor -> Value Estimator -> Detached Critic

**Critical Path**: The detached critic training loop is critical - it must maintain independence from actor updates while still providing useful gradient signals for policy improvement.

**Design Tradeoffs**: Decoupling critic and actor enables more conservative value estimates but may slow policy improvement; action sequence length affects horizon reduction effectiveness vs. computational cost; distributional objectives add robustness but increase training complexity.

**Failure Signatures**: Policy collapse (overly conservative behavior), value overestimation despite detached training, or failure to generalize action sequences beyond training distribution.

**First Experiments**: 1) Validate horizon reduction by comparing performance with/without options on short tasks, 2) Test detached vs. joint critic training on a simple gridworld, 3) Examine value distribution shapes to verify distributional RL objectives are functioning correctly.

## Open Questions the Paper Calls Out
None

## Limitations
- Results heavily dependent on authors' specific codebase and experimental setup
- Theoretical justification for detached value learning could be more rigorous
- Performance claims rely on single experimental runs without extensive variance analysis

## Confidence
- **High**: Empirical results on OGBench tasks with extensive baseline comparisons
- **Medium**: VLA scalability claims, sensitive to implementation details
- **Low**: Theoretical contributions regarding detached value learning mechanism

## Next Checks
1. Independent implementation and verification of DEAS on a subset of OGBench tasks to confirm baseline comparisons
2. Analysis of training stability and variance across multiple random seeds for the VLA experiments
3. Investigation of the method's behavior on out-of-distribution action sequences beyond those seen during training