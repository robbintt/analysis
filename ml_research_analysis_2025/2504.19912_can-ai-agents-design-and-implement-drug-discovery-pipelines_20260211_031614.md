---
ver: rpa2
title: Can AI Agents Design and Implement Drug Discovery Pipelines?
arxiv_id: '2504.19912'
source_url: https://arxiv.org/abs/2504.19912
tags:
- agent
- research
- agents
- deep
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DO Challenge, a novel benchmark designed
  to evaluate the decision-making abilities of autonomous AI agents in a drug discovery
  task resembling virtual screening. The benchmark challenges systems to independently
  develop, implement, and execute strategies for identifying promising molecular structures
  from a dataset of one million conformations, while navigating chemical space, selecting
  models, and managing limited resources.
---

# Can AI Agents Design and Implement Drug Discovery Pipelines?

## Quick Facts
- arXiv ID: 2504.19912
- Source URL: https://arxiv.org/abs/2504.19912
- Authors: Khachik Smbatyan, Tsolak Ghukasyan, Tigran Aghajanyan, Hovhannes Dabaghyan, Sergey Adamyan, Aram Bughdaryan, Vahagn Altunyan, Gagik Navasardyan, Aram Davtyan, Anush Hakobyan, Aram Gharibyan, Arman Fahradyan, Artur Hakobyan, Hasmik Mnatsakanyan, Narek Ginoyan, Garik Petrosyan
- Reference count: 40
- Primary result: Deep Thought multi-agent system achieved 33.5% overlap score on DO Challenge, outperforming most human teams but falling short of expert-designed solutions (77.8%)

## Executive Summary
This paper introduces DO Challenge, a benchmark for evaluating autonomous AI agents in drug discovery tasks that require developing, implementing, and executing virtual screening strategies. The benchmark challenges systems to identify promising molecular structures from one million conformations while managing limited labeling budgets and multiple submission opportunities. Results from the 2025 challenge and evaluations of the Deep Thought multi-agent system demonstrate that AI agents can achieve competitive performance in complex drug discovery workflows, though they still lag behind expert human-designed solutions and exhibit significant instability.

## Method Summary
The DO Challenge benchmark evaluates AI agents on identifying top 1000 molecular structures from 1 million conformations using a 100K label budget and 3 submission opportunities. The Deep Thought multi-agent system uses specialized roles including Software Engineer, Reviewer, and optional ML Engineer agents coordinated through structured sessions. Agents develop strategies involving active learning for budget management, position-sensitive feature engineering for 3D molecular data, and multi-submission planning under partial observability. The benchmark uses synthetic DO Scores derived from docking simulations against 4 proteins.

## Key Results
- Deep Thought achieved 33.5% overlap score, outperforming most human teams but falling short of expert solutions (77.8%)
- Best-performing LLMs in primary roles: Claude 3.7 Sonnet, Gemini 2.5 Pro, and o3; auxiliary roles: GPT-4o and Gemini 2.0 Flash
- System exhibited high instability with scores ranging from 9.0% to 32.3% across runs
- Key success factors: active learning (correlation 0.53), position non-invariance (correlation 0.56), strategic submission planning (correlation 0.37)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Specialization with Hierarchical Coordination
Decomposing drug discovery workflows into specialized agent roles with managerial coordination improves solution quality over monolithic approaches. The Deep Thought system uses a Software Engineer agent (primary code development), Reviewer agent (code inspection), and ML Engineer agent (optional, for ML-specific guidance). Managerial agents coordinate task decomposition and integrate interim results across planning meetings, exploration sessions, coding sessions, and review sessions. Complex scientific tasks benefit from role separation similar to human software teams, where specialized review prevents specific error classes.

### Mechanism 2: Active Learning-Based Budget Management Under Resource Constraints
Strategic label querying through iterative model-guided selection outperforms random sampling for identifying top candidates. Rather than exhaust the 100,000 label budget randomly, successful solutions use clustering (K-Means on fingerprints), train preliminary models on initial batches, then iteratively query labels for high-predicted-score structures. This concentrates labels in promising regions of chemical space. The target property correlates with learnable molecular features; model predictions on unlabeled data provide actionable signal for prioritization.

### Mechanism 3: Position-Sensitive Feature Engineering for 3D Molecular Data
Explicitly encoding position-dependent features (raw coordinates, distance-based descriptors) rather than rotation/translation-invariant representations improves prediction when the target score depends on absolute spatial configuration. The DO Score is explicitly position-sensitive—translation and rotation alter scores. Solutions must use features that preserve this sensitivity (raw coordinates, position-dependent descriptors) rather than standard equivariant/invariant GNN approaches that normalize away this signal. The target property's dependence on absolute position is genuine, and models can learn position-specific patterns from limited labeled examples.

## Foundational Learning

- Concept: Active Learning Budget Allocation
  - Why needed here: The benchmark constrains label queries to 10% of data; naive random sampling achieves only ~10% overlap. Understanding uncertainty sampling, diversity sampling, and exploitation-exploration tradeoffs is essential for competitive performance.
  - Quick check question: Given 100,000 labels for 1M molecules, would you query all at once or iteratively? What signal determines which molecules to label next?

- Concept: Position-Sensitive vs. Invariant Representations
  - Why needed here: Standard molecular ML assumes rotational/translational invariance is desirable. This benchmark explicitly violates that assumption—agents must recognize when to break convention and encode absolute positions.
  - Quick check question: If a molecule's "score" changes when you rotate it 90°, what features would capture this? How do standard GNNs handle rotations, and why might that fail here?

- Concept: Multi-Submission Strategy Under Partial Observability
  - Why needed here: Three submissions with overlap feedback provide implicit signal. Strategic solutions combine confirmed hits from prior submissions with model predictions, rather than treating each submission independently.
  - Quick check question: After submission 1 returns 63 confirmed hits (6.3%), how should you allocate the 2,937 remaining slots in submission 2? What information does the overlap score provide?

## Architecture Onboarding

- Component map:
  - Scientist agent (optional) generates action plan
  - Software Engineer group (Planning → Exploration → Coding → Review → Execute)
  - Simple Agents: Individual LLM-based entities with Generation Behavior Settings
  - Agent Groups: Collaborative teams with managerial coordination
  - Tools Layer: Research tools (web search, summarization), Coding tools (file management, version control), Task management tools (planning, decomposition), Editor/Execution tools (command execution, package installation)
  - Observation Manager: Context window compression for long-running sessions
  - Provider Abstraction Layer: Unified interface across Anthropic, OpenAI, Gemini APIs

- Critical path:
  1. Task definition → Scientist agent (optional) generates action plan
  2. Software Engineer group receives plan, conducts planning meeting
  3. Exploration session: agents investigate codebase/data
  4. Coding session: Software Engineer writes/modifies code iteratively
  5. Review session: Reviewer agent inspects for errors, completeness
  6. Installation agent resolves dependencies
  7. Evaluation agent executes in isolated Conda environment
  8. Feedback loop: errors returned to Software Engineer
  9. Submission via DOChallengeClient

- Design tradeoffs:
  - More agents vs. coordination overhead: Adding ML Engineer + Reviewer improved performance vs. Software Engineer alone, but "including all three agents reduced scores again"—three-way interaction introduces conflicts
  - Advanced vs. simpler models: Agents chose simpler boosting models (LightGBM) over deep learning due to "lack of proper hyperparameter tuning" time; simpler models are more robust to limited tuning
  - Research group inclusion: Advanced models (Claude 3.7, Gemini 2.5 Pro) never invoked Research agents; weaker models did but showed no performance gain—suggests domain knowledge encoded in frontier models reduces external research need

- Failure signatures:
  - Endless optimization loops: "Claude 3.5 Sonnet... continually increase complexity rather than executing"—agent refines past submission threshold
  - Tool abandonment: "Gemini 2.0 Flash... after exceeding approximately 20,000-50,000 tokens... disregard provided tools, opting instead to generate arbitrary code"
  - Budget exhaustion without validation: "Agents failed to perform adequate validation... before fully consuming the labeling budget"—no held-out evaluation set
  - Submission amnesia: "No evidence of planning, coordination, or iterative refinement across attempts"—each submission treated independently
  - Feature-type mismatch: "Correctly acknowledge the importance... but then mistakenly suggest using equivariant or invariant methods"

- First 3 experiments:
  1. Baseline validation: Run the provided random baselines (Baseline 1: random sampling with strategic submission; Baseline 2: fingerprint similarity extension) to establish 10-11% floor. Verify your client integration works and timing is acceptable for 100K label queries.
  2. Ablation on position sensitivity: Train identical LightGBM models with (a) standard RDKit fingerprints only, (b) fingerprints + position-dependent features (raw coordinate min/max, centroid distance). Compare scores to quantify position-sensitivity contribution (expect ~15-25% relative improvement based on Table 1 correlations).
  3. Active learning loop test: Implement single-agent active learning: random sample 2K → train → predict on remaining → query top 10K → retrain → repeat. Compare final score against one-shot 100K random query. This validates the core budget optimization mechanism before adding multi-agent complexity.

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms can reduce the high instability (variance from 9.0% to 32.3% across runs) in multi-agent system performance while maintaining competitive results? The paper identifies instability but does not isolate causes or propose mitigation strategies beyond noting failure modes. What evidence would resolve it: Controlled ablation studies varying random seeds, temperature settings, and agent coordination protocols with repeated runs.

### Open Question 2
How can AI agents be trained or prompted to strategically utilize multiple submission opportunities rather than treating submissions as isolated events? The Scientist agent partially addressed this but reduced overall performance; the paper provides no systematic solution. What evidence would resolve it: Benchmark runs with agents explicitly evaluated on submission-strategy metrics, comparing naive vs. strategic approaches.

### Open Question 3
To what extent would DO Challenge benchmark findings generalize to drug discovery tasks with real experimental validation rather than synthetic DO Scores? The DO Score is derived from docking simulations against 4 proteins; the paper acknowledges this represents "only a narrow and simplified segment" of real-world discovery. What evidence would resolve it: A parallel benchmark using wet-lab-confirmed activity data, comparing the same agent systems' performance.

### Open Question 4
What architectural or prompting improvements could prevent agents from incorrectly selecting rotation-invariant features when task descriptions explicitly require position-sensitive representations? Section 4.2.5 notes agents "failed to properly address this positional sensitivity" despite explicit instructions in the task description. What evidence would resolve it: Ablation testing different prompt phrasings and feature-extraction modules with explicit position-awareness checks.

## Limitations

- The evaluation framework depends critically on an opaque scoring backend that is not fully open-sourced, limiting reproducibility and third-party validation
- The multi-agent system's performance gains over human teams are qualified—it outperformed most participants but still fell short of expert-designed solutions (77.8% vs. 33.5%)
- The instability observed across runs and agent configurations indicates sensitivity to model choice, prompting, and coordination mechanisms that aren't fully characterized

## Confidence

- **High Confidence**: The DO Challenge benchmark structure, dataset composition, and baseline results are well-documented and reproducible via provided code
- **Medium Confidence**: The comparative performance claims between Deep Thought and human teams are credible given the controlled tournament setup, though exact conditions and human team compositions aren't fully transparent
- **Low Confidence**: The attribution of performance differences to specific agent behaviors (e.g., Claude 3.7's collaboration failures) relies on post-hoc analysis of individual runs without systematic ablation across all model combinations

## Next Checks

1. **Backend Transparency**: Request complete open-sourcing of the DO Score calculation pipeline (Balto docking API integration and DEKOIS2 classifiers) to enable independent verification of benchmark results
2. **Stability Analysis**: Run Deep Thought with fixed random seeds across 10+ trials using the top 3 agent configurations to quantify variance and identify systematic failure modes
3. **Human Benchmark Parity**: Replicate the human team strategies (active learning with LightGBM, position-sensitive features) using the same compute constraints to establish whether the gap to expert solutions is algorithmic or implementation-based