---
ver: rpa2
title: Safety Representations for Safer Policy Learning
arxiv_id: '2502.20341'
source_url: https://arxiv.org/abs/2502.20341
tags:
- safety
- learning
- agent
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of RL agents becoming overly conservative
  due to early failures in safety-critical environments, leading to suboptimal policies.
  The authors propose Safety Representations for Policy Learning (SRPL), a framework
  that augments RL agents' state representations with state-conditioned safety information
  learned from the agent's experiences.
---

# Safety Representations for Safer Policy Learning

## Quick Facts
- arXiv ID: 2502.20341
- Source URL: https://arxiv.org/abs/2502.20341
- Authors: Kaustubh Mani; Vincent Mai; Charlie Gauthier; Annie Chen; Samer Nashed; Liam Paull
- Reference count: 20
- One-line primary result: SRPL significantly improves sample efficiency and reduces constraint violations compared to baseline algorithms across multiple safety-critical environments.

## Executive Summary
This paper addresses the problem of RL agents becoming overly conservative due to early failures in safety-critical environments, leading to suboptimal policies. The authors propose Safety Representations for Policy Learning (SRPL), a framework that augments RL agents' state representations with state-conditioned safety information learned from the agent's experiences. The safety representation models a distribution over proximity to unsafe states, capturing the immediate and future likelihood of failure. SRPL can be integrated with any RL algorithm and is shown to significantly improve sample efficiency and reduce constraint violations during training compared to baseline algorithms like CPO, CRPO, CVPO, and CSC across multiple environments including AdroitHandPen, SafeMetaDrive, and Safety Gym. Additionally, SRPL demonstrates effective transfer of safety representations across tasks and improves the risk-reward tradeoff for RL agents.

## Method Summary
SRPL augments RL agents' state representations with learned safety information by training a Steps-to-Cost (S2C) neural network to predict a discrete probability distribution over proximity to unsafe states. The S2C model is trained on an off-policy experience buffer containing labeled states with their true distance to constraint violations. During training, the S2C output distribution is concatenated to the original state vector and fed to the policy and value networks. This approach mitigates primacy bias by learning safety representations from diverse experience rather than being overly influenced by early failures. The framework is compatible with any RL algorithm and can transfer learned safety representations across tasks.

## Key Results
- SRPL significantly improves sample efficiency and reduces constraint violations compared to baseline algorithms (CPO, CRPO, CVPO, CSC) across multiple environments
- The method demonstrates effective transfer of safety representations across tasks, improving sample efficiency on target tasks
- SRPL improves the risk-reward tradeoff for RL agents by preventing over-conservatism while maintaining safety constraints

## Why This Works (Mechanism)

### Mechanism 1: State-Augmented Safety via Learned Steps-to-Cost (S2C) Model
Augmenting the agent's state with an explicit, learned probability distribution over proximity to unsafe states mitigates conservatism and improves sample efficiency. An S2C neural network is trained to predict a discrete distribution over the number of steps to a constraint violation, providing the policy with a separate, explicit safety signal.

### Mechanism 2: Mitigation of Primacy Bias via Off-Policy Safety Learning
Learning safety representations from a diverse, off-policy experience buffer counteracts the tendency to overfit to early, penalizing failures. By training the S2C model on data from various policies, the safety representation is more generalized and prevents the policy from getting trapped in a risk-averse local optimum.

### Mechanism 3: Cross-Task Transfer of State-Centric Safety Representations
Safety representations learned in a policy-agnostic manner can be transferred to new tasks, accelerating learning. The S2C model learns state-centric features from the agent's entire experience, making them less dependent on any single policy and transferable to new tasks.

## Foundational Learning

**Concept: Constrained Markov Decision Process (CMDP)**
- Why needed here: This is the foundational problem formulation. Understanding the separate cost function `C` and constraint threshold `β` is essential to grasp what the S2C model is learning to predict.
- Quick check question: In a CMDP, what is the agent's primary objective, and how does it differ from a standard MDP?

**Concept: Primacy Bias in Reinforcement Learning**
- Why needed here: This is the core problem SRPL addresses. Comprehending how early negative experiences can disproportionately shape policy is crucial for appreciating the solution.
- Quick check question: Why might resetting an RL agent's network to encourage exploration be problematic in a safety-critical context?

**Concept: Distributional vs. Scalar Safety Critics**
- Why needed here: A key design choice in SRPL is modeling safety as a distribution (Steps-to-Cost) rather than a single scalar value (expected cost).
- Quick check question: What are two advantages of modeling safety as a distribution over steps-to-cost compared to a single scalar safety score?

## Architecture Onboarding

**Component map:**
1. Base RL Algorithm (e.g., CPO, CVPO) interacts with the environment
2. SRPL Wrapper manages the S2C model and experience buffers
3. S2C Model takes a state `s` and outputs a categorical distribution over steps-to-cost
4. Data Labeler labels each state in a trajectory with its true steps-to-cost value after episode ends
5. Augmented State concatenates original state `s` with S2C output `S_ν(s)` for the Base RL Algorithm's policy

**Critical path:**
1. Roll out a trajectory with the current policy
2. Label states in the trajectory with steps-to-cost values
3. Add labeled data to the S2C replay buffer `D_S2C`
4. Train the S2C model on a batch from `D_S2C` by minimizing NLL loss
5. Get the augmented state `s' = {s, S_ν(s)}` for the next policy update
6. Update the Base RL policy using its standard algorithm (e.g., CPO)

**Design tradeoffs:**
- S2C Update Frequency: For on-policy algorithms, update S2C more frequently. For off-policy, update less frequently to stabilize training (similar to target networks)
- Replay Buffer Size: Must be large enough to contain diverse policies but not so large that it stores irrelevant, old data
- Safety Horizon (`Hs`) vs. Model Complexity: A longer horizon provides more information but increases output dimensionality. Bins are used to reduce this

**Failure signatures:**
- Conservative Policy: Agent fails to explore risky but high-reward states. Potential causes: S2C model is not training properly, or reward scaling is too weak compared to safety signal
- High Violations: Agent violates constraints frequently. Potential causes: Safety horizon is too short, or S2C model is underfitting and not providing accurate safety signal
- Training Instability: S2C model's loss diverges. Potential cause: Learning rate is too high, or S2C update frequency is too high for off-policy algorithm

**First 3 experiments:**
1. Ablation on Horizon: Train on `Safety-Gym-PointGoal1` with `Hs` values of [20, 40, 80] and measure performance and safety
2. Transfer Learning Test: Train S2C model on `Safety-Gym-PointButton1`, freeze it, and use as prior for training policy on `Safety-Gym-PointGoal1`
3. Risk-Reward Tradeoff: Vary initial Lagrange multiplier for both baseline algorithm (e.g., CPO) and SRPL version, plot final performance vs. total cost

## Open Questions the Paper Calls Out

**Open Question 1:** How can the S2C model be adapted to accurately capture long-horizon causal safety mechanisms where the distance to unsafe states exceeds the fixed safety horizon `Hs`?

**Open Question 2:** Can the SRPL framework be extended to model continuous degrees of safety or harm severity rather than binary failure indicators?

**Open Question 3:** How does the update frequency of the S2C model relative to the policy update impact the stability of off-policy algorithms?

## Limitations
- The method has difficulty capturing long-horizon causal mechanisms where safety risks manifest far in the future beyond the fixed safety horizon
- The approach does not consider degrees of safety or harm severity, only binary failure indicators
- Implementation details like exact replay buffer sizes and binning logic are not fully specified, requiring assumptions for reproduction

## Confidence

**High confidence:** The core mechanism of augmenting state representations with learned safety distributions is well-supported by ablation studies and comparisons against baselines across multiple environments.

**Medium confidence:** Claims about cross-task transfer of safety representations are supported by a single transfer experiment, but broader generalization remains to be tested.

**Medium confidence:** The computational efficiency claims are based on comparison with baselines but don't provide absolute training time metrics or complexity analysis.

## Next Checks

1. **Robustness to Safety Horizon:** Systematically vary the safety horizon `Hs` across multiple tasks to identify optimal ranges and test the method's sensitivity to this critical hyperparameter.

2. **Cross-Domain Transfer:** Extend transfer learning experiments to tasks from different domains (e.g., from navigation to manipulation) to test the generality of learned safety representations.

3. **Alternative Distributional Models:** Replace the discrete S2C model with continuous distributional safety critics (e.g., quantile regression) to evaluate whether alternative safety representation approaches provide similar or improved performance.