---
ver: rpa2
title: Identifiability of Deep Polynomial Neural Networks
arxiv_id: '2506.17093'
source_url: https://arxiv.org/abs/2506.17093
tags:
- identifiability
- hpnn
- layer
- proposition
- polynomial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the identifiability of deep Polynomial Neural
  Networks (PNNs) using monomial activation functions. The key insight is that the
  identifiability of a deep PNN can be localized to its 2-layer subnetworks: a deep
  PNN is finitely identifiable if and only if every 2-layer subnetwork is finitely
  identifiable for some subset of their inputs.'
---

# Identifiability of Deep Polynomial Neural Networks

## Quick Facts
- arXiv ID: 2506.17093
- Source URL: https://arxiv.org/abs/2506.17093
- Authors: Konstantin Usevich; Ricardo Borsoi; Clara Dérand; Marianne Clausel
- Reference count: 40
- Primary result: Deep PNN identifiability localizes to 2-layer subnetworks

## Executive Summary
This paper establishes a fundamental theoretical result about the identifiability of deep Polynomial Neural Networks (PNNs) with monomial activation functions. The key insight is that the identifiability of a deep PNN can be determined by examining only its 2-layer subnetworks, rather than requiring analysis of the entire network architecture. This localization property connects shallow and deep network identifiability in a surprising and powerful way. The authors prove that a deep PNN is finitely identifiable if and only if every 2-layer subnetwork is finitely identifiable for some subset of their inputs. As applications, they show that pyramidal networks are generically identifiable, while encoder-decoder networks are identifiable when decoder widths do not grow too rapidly relative to activation degrees.

## Method Summary
The authors leverage a connection between PNNs and partially symmetric tensor decompositions, allowing them to apply Kruskal-type uniqueness theorems to establish identifiability conditions. They introduce a homogenization procedure to handle bias terms in PNNs by converting them to homogeneous PNNs (hPNNs) without bias. The core theoretical framework involves analyzing Kruskal ranks of weight matrices and deriving conditions under which the polynomial representations generated by PNNs are unique up to permutation and scaling equivalence. The analysis provides explicit conditions on network architecture (layer widths and activation degrees) that guarantee identifiability.

## Key Results
- Deep PNN identifiability localizes to 2-layer subnetworks: a deep PNN is finitely identifiable if and only if every 2-layer subnetwork is finitely identifiable
- Pyramidal networks (where layer widths are non-increasing) are generically identifiable
- Encoder-decoder networks are identifiable when decoder widths do not grow too rapidly compared to activation degrees
- The homogenization procedure enables handling of bias terms while preserving identifiability properties

## Why This Works (Mechanism)
The mechanism relies on connecting PNNs to partially symmetric tensor CP decompositions, where the polynomial outputs can be represented as tensors with specific symmetry properties. By applying Kruskal's uniqueness theorem to these tensor representations, the authors establish conditions for finite identifiability. The localization theorem works because the polynomial structure of PNNs creates a hierarchical decomposition where each layer's contribution can be analyzed independently in terms of tensor rank and Kruskal conditions.

## Foundational Learning

**Tensor Decomposition Basics**: Why needed - forms the mathematical foundation for analyzing PNN outputs. Quick check - verify understanding of CP decomposition and Kruskal rank.

**Kruskal Rank**: Why needed - provides the key condition for uniqueness in tensor decompositions. Quick check - compute krank for random matrices and verify Definition 32.

**Partially Symmetric Tensors**: Why needed - characterizes the special structure of PNN outputs. Quick check - understand how monomial activations create partial symmetries.

**Polynomial Vector Spaces**: Why needed - describes the output space of PNNs. Quick check - verify that PNN outputs lie in appropriate polynomial vector spaces.

**Network Homogenization**: Why needed - enables handling of bias terms. Quick check - implement homogenization procedure and verify equivalence with original PNN.

## Architecture Onboarding

**Component Map**: Input → W1 → ρr1 → W2 → ρr2 → ... → WL → Output, where each ρri is monomial activation and Wi are weight matrices.

**Critical Path**: The 2-layer subnetworks (Wi, Wi+1, ρri) are the critical components determining overall network identifiability.

**Design Tradeoffs**: Higher activation degrees improve identifiability but increase computational complexity; wider layers provide more expressiveness but may violate Kruskal conditions.

**Failure Signatures**: Non-identifiable networks occur when weight matrices have insufficient Kruskal rank or when activation degrees are too low relative to layer widths.

**First Experiments**:
1. Test Kruskal rank conditions for random 2-layer networks with varying widths and degrees
2. Verify localization theorem by checking identifiability of deep networks through their 2-layer subnetworks
3. Implement homogenization procedure and compare identifiability properties with original PNN

## Open Questions the Paper Calls Out
None

## Limitations
- Kruskal rank conditions require sufficient linear independence in weight matrices, which may not hold in practice
- Genericity conditions for identifiable parameters are stated but not quantified in terms of specific distributions
- Homogenization procedure adds computational complexity that is not fully characterized
- Inherits computational intractability of general tensor decomposition problems

## Confidence
- Theorem 11 (localization): High - follows established Kruskal-based uniqueness arguments
- Corollary 16 (pyramidal networks): Medium - depends on generic Kruskal conditions being satisfied
- Corollary 19 (bottleneck networks): Medium - requires careful verification of degree-width constraints
- Homogenization procedure: Medium - theoretical validity established but practical implications unclear

## Next Checks
1. Verify Kruskal rank conditions numerically for randomly generated 2-layer PNNs across different width-degree configurations
2. Implement the homogenization procedure and measure computational overhead compared to standard PNN forward passes
3. Test identifiability recovery using actual tensor decomposition algorithms on synthetic data from identifiable PNN architectures