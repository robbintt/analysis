---
ver: rpa2
title: Towards a General Framework for Predicting and Explaining the Hardness of Graph-based
  Combinatorial Optimization Problems using Machine Learning and Association Rule
  Mining
arxiv_id: '2512.20915'
source_url: https://arxiv.org/abs/2512.20915
tags:
- hardness
- features
- problem
- algorithms
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GCO-HPIF, a general machine-learning-based framework
  for predicting and explaining the hardness of graph-based combinatorial optimization
  problems. The framework uses problem-agnostic graph features and association rule
  mining to provide explainable predictions.
---

# Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining

## Quick Facts
- **arXiv ID:** 2512.20915
- **Source URL:** https://arxiv.org/abs/2512.20915
- **Reference count:** 40
- **Primary result:** Achieved weighted F1-score of 0.9921, minority-class F1-score of 0.878, and ROC-AUC score of 0.9083 using only three graph features.

## Executive Summary
This paper introduces GCO-HPIF, a general framework for predicting and explaining the hardness of graph-based combinatorial optimization problems using machine learning and association rule mining. The framework extracts problem-agnostic spectral graph features, applies supervised learning to predict instance hardness, and uses FP-Growth to generate human-interpretable association rules. Evaluated on the Maximum Clique Problem across 3287 instances, the framework achieves excellent classification performance while providing explainable rules with high support. The approach demonstrates potential for transfer to other graph-based COPs, though this requires further validation.

## Method Summary
The framework extracts 23 graph features (primarily spectral) from input instances, then applies an 80/20 stratified split with Min-Max scaling. Binary hardness labels are assigned based on whether three algorithms (MOMC, EGN, HGS) fail to find maximum cliques verified against Gurobi/CliSAT. The classification pipeline uses SVC, XGBoost, and Logistic Regression with 5-fold cross-validation, selecting models by weighted F1-score. For explainability, features are discretized into percentile bins and FP-Growth mines association rules meeting minimum support thresholds. Separate regression models predict computation times for each solver, with Random Forest and XGBoost optimized for RMSE.

## Key Results
- Achieved weighted F1-score of 0.9921, minority-class F1-score of 0.878, and ROC-AUC score of 0.9083 using only three graph features (node count, smallest eigenvalue, second smallest eigenvalue).
- Best association rule achieved support of 0.8829 for hard instances with overall accuracy of 87.64%.
- Regression models for computation time prediction achieved percentage RMSE of 5.12 and R2 value of 0.991 for GNN-based algorithms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Problem-agnostic spectral graph features can predict instance hardness with high accuracy for graph-based COPs.
- Mechanism: Eigenvalues of the adjacency matrix (specifically smallest and second smallest) combined with node count encode structural properties that correlate with algorithmic difficulty. The SVC model maps these features to binary hardness labels via learned decision boundaries in 3D feature space.
- Core assumption: Graph spectral properties capture sufficient information about combinatorial structure to distinguish easy from hard instances, regardless of problem-specific constraints.
- Evidence anchors:
  - [abstract] "achieving a weighted F1-score of 0.9921... using only three graph features"
  - [Section 8.1] "the SVC model with 3 features, specifically the second smallest and smallest eigenvalues of the adjacency matrix, and the number of nodes"
  - [corpus] Limited direct corroboration; ARM-Explainer (arXiv:2511.22866) uses ARM for MCP explanation but focuses on node-level features rather than graph-level spectral features.
- Break condition: If spectral features fail to generalize to path-based problems (TSP, LRP) where edge structure dominates, the mechanism would require edge-aware features.

### Mechanism 2
- Claim: Association rule mining on feature quantiles produces human-interpretable hardness explanations with high support.
- Mechanism: FP-Growth algorithm discretizes continuous features into percentile bins (P0-P25, P75-P100) and mines rules of the form "IF feature A in range X AND feature B in range Y THEN Hard" that satisfy minimum support thresholds.
- Core assumption: Hard instances cluster in specific regions of the feature space, and these clusters can be described by axis-aligned hyperrectangles (conjunctions of feature range conditions).
- Evidence anchors:
  - [abstract] "best association rule found... had a support of 0.8829 for hard instances and an overall accuracy of 87.64%"
  - [Section 6.2] "FP-Growth utilizes a compact data structure known as the FP-Tree... bypassing generating candidates"
  - [Section 8.2] Expression 6: rules combining node count > P75 with eigenvalue features achieve 87.64% accuracy
  - [corpus] ARM-Explainer (arXiv:2511.22866) directly extends ARM-based explainability to MCP, providing convergent evidence for this approach.
- Break condition: If rules achieve high support but low lift (as observed, lift=1 due to class imbalance focus), discriminative power may be insufficient for balanced datasets.

### Mechanism 3
- Claim: Computation time prediction via regression works better for GNN-based algorithms than exact B&B solvers.
- Mechanism: Random Forest regression learns mapping from graph features to log-scale computation times. GNN algorithms exhibit more predictable (near-linear) scaling with problem size, while B&B algorithms show exponential scaling with high variance.
- Core assumption: Variance in computation time is primarily explained by graph structure rather than algorithm-internal stochasticity or implementation details.
- Evidence anchors:
  - [Section 8.3] "predictive models performed well... for GNN-based solvers HGS and EGN, yet underperformed with exact B&B algorithms"
  - [Section 8.3] "exact algorithms tend to scale exponentially... complicating accurate prediction"
  - [corpus] No direct corroboration found in neighbor papers for regression-based runtime prediction.
- Break condition: For B&B algorithms, prediction requires either log-transformed targets or additional features capturing search space structure.

## Foundational Learning

- **Concept: Spectral Graph Theory**
  - Why needed here: Three of the top predictive features are eigenvalues of the adjacency matrix. Understanding what spectral properties encode (connectivity, clustering, expansion) is essential for interpreting why these features predict hardness.
  - Quick check question: Given a graph with low algebraic connectivity (second smallest Laplacian eigenvalue), would you expect it to be easier or harder to partition, and why?

- **Concept: Association Rule Mining Metrics (Support, Confidence, Lift)**
  - Why needed here: The paper uses FP-Growth to generate rules; evaluating rule quality requires understanding tradeoffs between support (coverage), confidence (precision), and lift (interestingness vs. random).
  - Quick check question: If a rule has support=0.9 and confidence=0.5 on a dataset where the consequent appears in 50% of instances, what is the lift value, and is this rule useful?

- **Concept: Class Imbalance in Classification**
  - Why needed here: Only 3.3% of instances are "hard" (111 of 3287). Standard accuracy is misleading; weighted F1 and minority-class F1 are the reported metrics.
  - Quick check question: A classifier achieves 96.7% accuracy by predicting "not hard" for all instances. What metric would reveal this failure mode?

## Architecture Onboarding

- **Component map:**
  - Graph instances -> Feature extraction (NetworkX) -> Hardness labeling (5 algorithms) -> MinMax scaling -> Classification pipeline (SVC/XGB/LR) -> Association rule mining (FP-Growth) -> Regression pipeline (RF/XGB)

- **Critical path:**
  1. Feature computation (bottleneck for large graphs due to O(|V|³) spectral operations)
  2. Hardness labeling (requires running 3 algorithms per instance)
  3. FP-Growth (scalability degrades with >25 features due to combinatorial explosion)

- **Design tradeoffs:**
  - **Feature count vs. explainability**: Using 3 features instead of 30 improves rule interpretability with marginal F1 loss (0.878 vs. ~0.89 minority F1)
  - **Binary vs. continuous hardness**: Binary labels enable classification + ARM explanation; continuous time prediction requires regression with reduced interpretability
  - **Algorithm-specific vs. unified models**: Current design trains separate regression models per algorithm; a unified model would require algorithm identity as a feature

- **Failure signatures:**
  - High accuracy but lift=1 for rules -> class imbalance masking poor discrimination
  - Good F1 but poor regression R² for B&B algorithms -> exponential scaling not captured by polynomial-time features
  - Rules that don't transfer across datasets -> overfitting to specific graph distributions

- **First 3 experiments:**
  1. **Baseline replication**: Reproduce the 3-feature SVC result on the MCP dataset (3287 graphs) using NetworkX for feature extraction. Verify weighted F1 ≥ 0.99 and minority F1 ≥ 0.87.
  2. **Feature ablation**: Train models with individual features removed to confirm that all three (node count, smallest eigenvalue, second smallest eigenvalue) are necessary. Check if any single feature achieves >0.85 ROC-AUC.
  3. **Rule validation on held-out data**: Apply the best ARM rule (Expression 6) to the test set (658 instances) and compute accuracy separately for hard vs. non-hard instances to verify the reported 87.64% overall accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the GCO-HPIF framework maintain high prediction accuracy and meaningful association rules when applied to edge-dependent combinatorial optimization problems like the Traveling Salesperson Problem (TSP) or Location Routing Problem (LRP)?
- Basis in paper: [explicit] Section 9 states the framework requires evaluation on "tasks involving edge information and path-based objectives like TSP and LRP" to assess generalizability beyond the Maximum Clique Problem.
- Why unresolved: The current study utilized node-based spectral features which may not capture the constraints of path-based problems.
- What evidence would resolve it: Application of the framework to TSP/LRP datasets using edge-weighted features, followed by an analysis of F1-scores and rule support.

### Open Question 2
- Question: Do the association rules derived for predicting hardness on small graphs (under 388 nodes) remain valid for graph instances containing thousands of nodes?
- Basis in paper: [explicit] The conclusion notes that graphs with thousands of nodes exist and asks whether "the same association rules still hold good for these larger graphs."
- Why unresolved: The correlation between spectral features and hardness may be sensitive to graph scale, and the current dataset was limited to smaller instances.
- What evidence would resolve it: Running the pipeline on large-scale benchmark graphs and comparing the FP-Growth rule antecedents to those found in the current study.

### Open Question 3
- Question: How does the framework's predictive performance change when using a continuous hardness metric (e.g., combining solution quality and computation time) instead of a binary classification?
- Basis in paper: [explicit] Section 9 suggests applying "different criteria for measuring hardness," such as a metric combining clique size with computation time.
- Why unresolved: The binary metric used treats near-optimal solutions as failures and does not account for variability in exact solver runtimes.
- What evidence would resolve it: Retraining the regression models using a composite hardness score and analyzing the resulting feature importances.

## Limitations

- **Solver reproducibility**: Critical solver configurations (timeouts, random seeds) are not specified, creating potential reproducibility gaps for the "Hard" instance labels.
- **Cross-problem generalizability**: All results are validated only on the Maximum Clique Problem; transfer to path-based problems may require edge-aware features beyond spectral properties.
- **Class imbalance handling**: ARM rules achieve high support (0.8829) but lift=1, indicating they capture the class imbalance rather than discriminative structure, limiting rule utility on balanced datasets.

## Confidence

- **High confidence**: Spectral features (eigenvalues + node count) predict MCP hardness with excellent accuracy (weighted F1=0.9921). The classification mechanism is well-supported by experimental results.
- **Medium confidence**: Association rule mining produces interpretable explanations with good support (87.64% accuracy). However, the lift=1 metric suggests rules may not generalize beyond the imbalanced dataset.
- **Low confidence**: Regression performance for exact B&B solvers is not well-explained. The exponential scaling behavior appears to exceed the explanatory power of polynomial-time graph features.

## Next Checks

1. **Reproduce the 3-feature SVC result**: Train and evaluate the model on the MCP dataset to verify the claimed weighted F1≥0.99 and minority F1≥0.87. Document solver configurations used.

2. **Test transferability to TSP**: Apply the same three features (node count, smallest eigenvalue, second smallest eigenvalue) to predict TSP instance hardness using Gurobi as ground truth. Compare performance to the MCP results.

3. **Evaluate rule lift on balanced subsets**: Create balanced subsets of the dataset (50% hard, 50% not hard) and recompute lift for the best ARM rule. Assess whether the rule maintains discriminative power beyond the original imbalanced distribution.