---
ver: rpa2
title: 'AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment
  framework'
arxiv_id: '2509.10104'
source_url: https://arxiv.org/abs/2509.10104
tags:
- harm
- severity
- harms
- risk
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of assessing and prioritizing
  AI-related harms in a stakeholder-centric manner, especially when precise numerical
  severity data is unavailable. It introduces AI Harmonics, a framework that leverages
  ordinal severity rankings derived from incident annotations to quantify harm concentration
  using a novel metric called AIH.
---

# AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework

## Quick Facts
- arXiv ID: 2509.10104
- Source URL: https://arxiv.org/abs/2509.10104
- Reference count: 40
- Introduces AI Harmonics framework to prioritize AI harms by stakeholder severity concentration using ordinal data

## Executive Summary
This paper addresses the challenge of assessing and prioritizing AI-related harms in a stakeholder-centric manner when precise numerical severity data is unavailable. The authors introduce AI Harmonics, a framework that leverages ordinal severity rankings derived from incident annotations to quantify harm concentration using a novel metric called AIH. This metric extends the Gini coefficient to ordinal data, capturing how unequally harms are distributed across stakeholders. Tested on the AIAAIC dataset, the framework identifies political and physical harms as the most concentrated and thus highest-priority categories. The work bridges the gap between high-level fairness principles and actionable, data-driven harm prioritization, offering a flexible tool for policymakers and practitioners.

## Method Summary
The AI Harmonics framework processes incident datasets with (harm_category, stakeholder, severity) annotations to compute harm concentration scores. For each harm category, it calculates stakeholder frequency distributions, constructs a derivative Lorenz curve from ordinal severity rankings, and integrates this curve to produce the AIH metric. The framework was validated using the AIAAIC dataset with 816 annotations across 9 stakeholder groups and 9 harm categories, comparing AIH results against Criticality Index as a benchmark.

## Key Results
- Political & Economic harms showed highest concentration (AIH ≈ 0.85), while Financial & Business harms were least concentrated (AIH ≈ 0.51)
- Framework successfully identified concentrated harm categories without requiring precise numerical severity data
- Sensitivity analyses confirmed AIH metric robustness under various severity orderings and data perturbations
- Stakeholder frequency distributions revealed Human Rights harms concentrate on vulnerable groups (40 incidents) vs. investors (0 incidents)

## Why This Works (Mechanism)

### Mechanism 1: Ordinal Severity Aggregation via Derivative Lorenz Curves
Converting ordinal stakeholder severity rankings into derivative Lorenz curves enables harm concentration measurement without requiring precise numerical intervals. The AIH metric computes ∫₀¹ ℓᵢ(x)dx where ℓᵢ(x) is constructed from cumulative stakeholder frequencies against normalized severity ranks, preserving ordinal ordering while avoiding unjustified numerical distance assumptions.

### Mechanism 2: Stakeholder Frequency Distribution Captures Harm Burden Patterns
Aggregating incident-level stakeholder annotations into frequency distributions per harm category reveals which harms concentrate on specific vulnerability tiers. Categories where high-severity stakeholders have high frequency yield high AIH scores, capturing the distributional structure of harm burden.

### Mechanism 3: Ranking Stability Under Severity Permutation
Harm category rankings remain stable when severity orderings are perturbed because concentration patterns are driven by distributional structure rather than precise ordinal assignments. Spearman correlations between original and perturbed AIH rankings remain ≥0.97, indicating harm concentration is an inherent property of the frequency distribution.

## Foundational Learning

- **Concept: Ordinal vs. Cardinal Measurement**
  - **Why needed here:** The entire AIH metric hinges on distinguishing rank-based from value-based severity. Without this, you might incorrectly interpret AIH as a probability or expect numerical intervals to matter.
  - **Quick check question:** If severity levels are {Negligible, Moderate, Severe, Catastrophic}, does AIH change if you encode them as {1,2,3,4} vs. {1,10,100,1000}?

- **Concept: Gini Coefficient and Lorenz Curves**
  - **Why needed here:** AIH is a modification of Gini for ordinal data. Understanding how Gini measures inequality via area-under-curve is essential to interpreting AIH scores and derivative Lorenz curves.
  - **Quick check question:** What does a Gini of 0.85 indicate about income distribution? How does this analogy transfer to harm concentration?

- **Concept: Sensitivity Analysis for Ordinal Data**
  - **Why needed here:** The paper's robustness claims depend on understanding how rank perturbations and annotation removal affect metrics. You need to distinguish statistical significance from ranking stability.
  - **Quick check question:** If removing 50% of annotations changes AIH values but preserves category rankings, is the framework robust?

## Architecture Onboarding

- **Component map:** Data Ingestion Layer -> Stakeholder Annotation -> Severity Ordering -> Frequency Distribution -> AIH Metric Engine -> Sensitivity Analyzer -> Prioritization Output
- **Critical path:** Data preparation → Stakeholder annotation → Severity ordering → Frequency distribution → AIH calculation → Sensitivity validation → Ranking output. The severity ordering step is the most assumption-heavy and user-configurable.
- **Design tradeoffs:**
  - **Ordinal vs. numerical:** Choosing ordinal sacrifices granularity for robustness; if numerical severities are reliable, standard Gini provides finer resolution
  - **Stakeholder granularity:** More stakeholder groups increase discriminative power but complicate severity ordering consensus
  - **Dataset requirements:** Framework works without stakeholder data (using incident-level ratings) but loses stakeholder-centric interpretation
- **Failure signatures:**
  - Annotator bias: If certain stakeholder groups are systematically over/under-annotated, frequency distributions will misrepresent concentration
  - Severity ordering reversal: If domain experts disagree fundamentally on which stakeholders are more severely impacted, AIH rankings may become domain-specific
  - Sparse categories: Categories with few annotations (<20) may show unstable AIH scores under perturbation
- **First 3 experiments:**
  1. **Baseline replication:** Run AIH on the provided AIAAIC dataset with the paper's severity ordering; verify Political & Economic has highest AIH (~0.85) and Financial & Business lowest (~0.51)
  2. **Permutation robustness test:** Apply 5 random severity order permutations; confirm Spearman correlation ≥0.97 between original and perturbed rankings
  3. **Cross-dataset validation:** Apply framework to MIT AI Incident Tracker data (using ratings as severity); compare category rankings to AIAAIC results to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is it possible to generalize ordinal risk assessment methodologies to any AI risk assessment context, and which specific metrics are ideal for diverse data structures?
- **Basis in paper:** Section 2 explicitly asks, "...is it really possible to generalize this methodology to any AI risk assessment? If so, how and which metrics are ideal to be used in this case?"
- **Why unresolved:** While the paper proposes the AIH metric as a solution, the authors note that most existing work remains tied to narrow domains. Empirical validation across the full spectrum of AI risk domains (beyond the AIAAIC dataset) is not fully demonstrated.

### Open Question 2
- **Question:** How can a standardized, objective ordinal severity ranking of stakeholders be established to minimize subjectivity in the AIH calculation?
- **Basis in paper:** Table 3 explicitly states the severity values used are "for illustrative purposes only" and "arbitrary," serving only to encode an order. The paper notes that users "may specify this ordering directly," relying on judgment rather than a universal standard.
- **Why unresolved:** The AIH score is sensitive to the "worst-case" and "best-case" severity orderings (Section 6.3.1), yet the paper provides no mechanism for defining the "correct" or "true" ordering of stakeholder severity, leaving it as a customizable input.

### Open Question 3
- **Question:** How can the AI Harmonics framework be effectively adapted to datasets that rely on automated (e.g., LLM-based) annotations rather than expert human curation?
- **Basis in paper:** Section 7 notes that datasets like the OECD AIM rely on automated extraction which "often results in stakeholder information being inconsistently captured," and mentions that severity must be assigned manually or via LLMs for such datasets.
- **Why unresolved:** The framework relies on accurate stakeholder-frequency pairs. The authors acknowledge that automated annotation introduces noise and misclassification, but the experiments primarily utilize the expert-annotated AIAAIC dataset, leaving the robustness against automated-extraction noise less explored.

## Limitations
- **Data completeness assumption**: Framework assumes comprehensive stakeholder annotation coverage, but real-world databases may underrepresent marginalized groups
- **Severity ordering subjectivity**: Stakeholder severity rankings rely on expert consensus that may vary across cultural contexts and regulatory frameworks
- **Generalizability constraints**: AIAAIC dataset's specific annotation schema may not transfer directly to other incident databases without recalibration

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| AIH metric mechanics (Lorenz curve construction, integration method) and its ordinal-to-concentration transformation are mathematically sound and well-specified. | High |
| Empirical findings from AIAAIC dataset are robust under the tested perturbations but may not generalize to datasets with different annotation patterns or stakeholder definitions. | Medium |
| The mechanism linking frequency distributions to actual harm concentration assumes annotation frequency correlates with real-world prevalence, which requires external validation. | Medium |

## Next Checks
1. **Cross-dataset replication**: Apply AI Harmonics to MIT AI Risk Repository data and compare category rankings to AIAAIC results. Focus on whether high-AIH categories remain consistent across databases with different annotation schemas.
2. **Annotation bias audit**: Systematically remove annotations for specific stakeholder groups (e.g., vulnerable populations) and measure impact on AIH scores and rankings. This tests whether frequency distributions accurately reflect harm concentration or annotation coverage.
3. **Ordinal ordering sensitivity**: Conduct expert elicitation across multiple jurisdictions to establish alternative severity orderings. Compare resulting AIH rankings to test whether the framework produces domain-specific or universally robust prioritization.