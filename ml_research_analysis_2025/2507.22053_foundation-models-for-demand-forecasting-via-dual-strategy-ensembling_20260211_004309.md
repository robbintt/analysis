---
ver: rpa2
title: Foundation Models for Demand Forecasting via Dual-Strategy Ensembling
arxiv_id: '2507.22053'
source_url: https://arxiv.org/abs/2507.22053
tags:
- forecasting
- ensemble
- across
- foundation
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of accurate demand forecasting
  in complex supply chain environments, where hierarchical data structures and domain
  shifts hinder performance. The authors propose a unified ensemble framework that
  enhances foundation models by integrating two complementary strategies: Hierarchical
  Ensemble (HE), which partitions data by semantic levels (e.g., store, category)
  to capture localized patterns, and Architectural Ensemble (AE), which combines diverse
  model backbones to reduce bias and improve stability.'
---

# Foundation Models for Demand Forecasting via Dual-Strategy Ensembling

## Quick Facts
- arXiv ID: 2507.22053
- Source URL: https://arxiv.org/abs/2507.22053
- Authors: Wei Yang; Defu Cao; Yan Liu
- Reference count: 40
- Primary result: Dual-strategy ensemble framework (Hierarchical + Architectural) improves demand forecasting accuracy on M5 benchmark and three external datasets

## Executive Summary
This paper addresses demand forecasting challenges in complex supply chain environments where hierarchical data structures and domain shifts hinder performance. The authors propose a unified ensemble framework that enhances foundation models by integrating two complementary strategies: Hierarchical Ensemble (HE), which partitions data by semantic levels (e.g., store, category) to capture localized patterns, and Architectural Ensemble (AE), which combines diverse model backbones to reduce bias and improve stability. Extensive experiments on the M5 benchmark and three external sales datasets show consistent improvements in forecasting accuracy across both in-domain and zero-shot settings.

## Method Summary
The method employs a dual-strategy ensemble framework combining Hierarchical Ensemble (HE) and Architectural Ensemble (AE). HE partitions training data by semantic levels (store, store+category, store+department) to capture localized patterns through specialized models. AE integrates diverse backbones including LightGBM, DeepAR, PatchTST, TEMPO, and Chronos via weighted fusion to mitigate model-specific biases. The framework uses equal normalized weights for aggregation and operates in two modes: fine-tuning foundation models on M5 partitions or zero-shot transfer to external datasets.

## Key Results
- WRMSSE on M5 benchmark improves from 0.5116 (best single model) to 0.4989 (ensemble)
- Zero-shot transfer to external datasets shows MSE/MAE reductions across all three validation sets
- Hierarchical Ensemble specializes at different levels: Store-Dept excels at Levels 1-5, Store-Cate at Levels 10-12
- Architectural Ensemble combines contrasting strengths of tree-based (LightGBM) and sequence models (PatchTST)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Partitioning for Localized Specialization
Partitioning training data by semantic levels (e.g., store, category) allows models to capture distinct local patterns that global models often smooth over, reducing error at granular hierarchy levels. By training independent models for specific groups, the system reduces variance caused by heterogeneous demand signals found in global datasets.

### Mechanism 2: Inductive Bias Complementarity via Architectural Fusion
Aggregating predictions from structurally diverse backbones (e.g., tree-based vs. transformer-based) mitigates model-specific biases and stabilizes forecasts under distribution shifts. Tree-based models excel at sparse tabular feature interactions while sequence models capture temporal dependencies; their error patterns are often uncorrelated, reducing overall variance when fused.

### Mechanism 3: Structural Regularization of Foundation Models
Ensemble strategies act as a regularizer for pre-trained foundation models, anchoring their broad generalization capabilities to specific structural and local patterns. Foundation models can struggle with specific domain shifts; HE anchors them to localized patterns while AE provides complementary views, smoothing out residual overfitting from pre-training objectives.

## Foundational Learning

- **Concept: Hierarchical Time Series Forecasting**
  - Why needed here: The paper's core method (HE) relies on partitioning data by hierarchy levels (e.g., Store, Dept). You must understand that retail data is nested (State -> Store -> Category -> Item) and that optimizing for one level might degrade performance at another.
  - Quick check question: Can you explain why a model minimizing error at the "Total Sales" level might fail to capture the volatility of "Item-Level" sales in a specific store?

- **Concept: Foundation Models for Time Series (e.g., Chronos, TEMPO)**
  - Why needed here: The paper attempts to enhance these specific models. You need to distinguish them from standard models by understanding they are pre-trained on massive, diverse corpora for zero-shot generalization.
  - Quick check question: How does a pre-trained foundation model approach a new forecasting dataset differently than a model trained from scratch on that specific dataset?

- **Concept: Bias-Variance Tradeoff in Ensembling**
  - Why needed here: The AE strategy explicitly aims to reduce "model-specific variance" and "mitigate bias." Understanding how averaging uncorrelated errors reduces variance is essential to grasp why the method works.
  - Quick check question: If two models in the ensemble make the exact same mistakes on the test set, will ensembling them improve performance? Why or why not?

## Architecture Onboarding

- **Component map:** Data Partitioner -> Backbone Zoo -> Trainers -> Aggregation Layer
- **Critical path:**
  1. Ingest: Load M5 or external sales data
  2. Partition: Apply Hierarchical Ensemble logic to create training subsets
  3. Train/Infer: Run Backbone models on these subsets (fine-tune foundation models on M5 partitions; zero-shot on external)
  4. Fuse: Aggregate predictions first by hierarchy, then by architecture (or vice versa)
  5. Evaluate: Calculate WRMSSE (for M5) or MSE/MAE (for others)

- **Design tradeoffs:**
  - Equal Weights vs. Learned Weights: Simplicity/robustness vs. potential accuracy gains from dynamic weighting
  - Computational Cost: HE requires training N models per hierarchy level. Tradeoff: Better accuracy vs. linear increase in training infrastructure
  - Foundation Flexibility: Fine-tuning foundation models (M5) vs. Zero-shot (External). Tradeoff: Performance vs. data availability/labeling effort

- **Failure signatures:**
  - High WRMSSE at Lower Levels (10-12): Indicates HE strategy failing to specialize; global patterns dominating
  - Zero-Shot Collapse: If external dataset errors (MSE) are excessively high for foundation models, pre-training domain gap too wide
  - Correlated Errors: If AE provides no improvement over best single model, chosen backbones share similar architectural limitations

- **First 3 experiments:**
  1. Baseline vs. HE: Run single PatchTST on full M5 dataset vs. PatchTST trained on specific semantic levels (Store, Dept). Verify if WRMSSE decreases
  2. AE Complementarity: Train LightGBM and PatchTST separately. Compare their error correlations on validation set. Combine them to see if WRMSSE drops below better of two
  3. Zero-Shot Robustness: Take pre-trained foundation model (e.g., Chronos) and apply to one external dataset (Sales1) with and without HE strategy. Observe if MSE/MAE improves

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Retrieval-Augmented Generation (RAG) mechanisms effectively improve the adaptability of the ensemble framework by incorporating external signals such as promotions and macroeconomic trends?
- **Open Question 2:** Do domain-specialized foundation models pre-trained specifically on supply chain data outperform general-purpose time series foundation models when integrated into this ensemble framework?
- **Open Question 3:** Does implementing context-aware or dynamic weighting schemes improve performance over the fixed combination schemes currently used in the ensemble?

## Limitations

- Model hyperparameter sensitivity is not analyzed; ensemble gains could be hyperparameter-dependent
- Zero-shot generalization claims need validation on truly out-of-domain data beyond M5-pretrained foundation models
- Scalability concerns not addressed; HE requires training N models per semantic level with unclear practical deployment feasibility

## Confidence

- **High Confidence:** The hierarchical partitioning mechanism (HE) is well-justified and empirically supported by Level-specific performance gains
- **Medium Confidence:** The architectural complementarity claim (AE) is supported by error reduction but lacks error correlation analysis between backbones
- **Low Confidence:** The foundation model regularization claim lacks direct evidence showing foundation models benefit more than traditional models

## Next Checks

1. **Error Correlation Analysis:** Compute pairwise correlation coefficients between backbone residual errors on validation sets to verify uncorrelated residuals assumption
2. **Out-of-Domain Transfer:** Apply ensemble to non-retail time series dataset (e.g., electricity demand or weather) to test true zero-shot generalization
3. **Hyperparameter Ablation:** Systematically vary key hyperparameters to determine if WRMSSE improvements persist across configurations or are tied to specific settings