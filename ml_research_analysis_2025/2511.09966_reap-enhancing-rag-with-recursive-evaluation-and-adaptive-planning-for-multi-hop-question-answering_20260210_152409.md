---
ver: rpa2
title: 'REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop
  Question Answering'
arxiv_id: '2511.09966'
source_url: https://arxiv.org/abs/2511.09966
tags:
- reasoning
- arxiv
- reap
- question
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces REAP, a dual-module framework for multi-hop
  question answering that integrates recursive evaluation with adaptive planning.
  By maintaining structured sub-tasks and facts, REAP uses a globally-aware Sub-task
  Planner to guide reasoning trajectories and a Fact Extractor to gather reliable
  evidence.
---

# REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering

## Quick Facts
- **arXiv ID:** 2511.09966
- **Source URL:** https://arxiv.org/abs/2511.09966
- **Reference count:** 34
- **Primary result:** REAP achieves 68.0% F1 on HotpotQA, outperforming Standard RAG (48.6%) and other methods, with strong generalization on out-of-domain datasets.

## Executive Summary
REAP is a dual-module framework designed to improve multi-hop question answering within RAG systems by integrating recursive evaluation and adaptive planning. It maintains structured sub-tasks and facts, uses a globally-aware Sub-task Planner to guide reasoning trajectories, and a Fact Extractor to gather reliable evidence. This modular design enables dynamic optimization, robust error recovery, and interpretability. Experimental results show significant performance gains over baseline methods and strong generalization across multiple benchmarks.

## Method Summary
REAP introduces a dual-module framework that combines recursive evaluation with adaptive planning for multi-hop question answering. The system maintains structured sub-tasks and facts, using a globally-aware Sub-task Planner to guide reasoning trajectories and a Fact Extractor to gather reliable evidence. This modular approach allows for dynamic optimization and robust error recovery, enabling the system to adaptively plan and execute multi-hop reasoning steps while maintaining interpretability through structured task decomposition.

## Key Results
- Achieves 68.0% F1 score on HotpotQA, significantly outperforming Standard RAG (48.6%) and other baseline methods.
- Demonstrates strong generalization with consistent performance improvements on out-of-domain datasets.
- Ablation studies confirm the necessity of both the Sub-task Planner and Fact Extractor modules for optimal performance.

## Why This Works (Mechanism)
REAP's effectiveness stems from its recursive evaluation mechanism, which continuously assesses intermediate reasoning steps and enables dynamic correction of errors. The adaptive planning component uses global context awareness to optimize reasoning trajectories, avoiding dead-ends and redundant paths. By maintaining structured sub-tasks and facts, the system can recover from errors mid-process and ensure each reasoning step builds toward the final answer. The modular design separates planning from evidence extraction, allowing each component to specialize and optimize its respective function while maintaining end-to-end coordination.

## Foundational Learning

**Recursive Evaluation**
- *Why needed:* Enables continuous assessment and correction of intermediate reasoning steps in multi-hop questions.
- *Quick check:* Verify the system can identify and recover from errors in intermediate reasoning steps.

**Adaptive Planning**
- *Why needed:* Allows dynamic optimization of reasoning trajectories based on global context and available evidence.
- *Quick check:* Test whether the planner can adjust strategies when initial reasoning paths prove ineffective.

**Structured Sub-task Decomposition**
- *Why needed:* Breaks complex questions into manageable components while maintaining traceability and interpretability.
- *Quick check:* Confirm that decomposed sub-tasks can be individually verified and contribute meaningfully to final answers.

**Evidence Extraction**
- *Why needed:* Ensures reliable fact gathering from retrieved documents to support each reasoning step.
- *Quick check:* Validate that extracted facts are both relevant and sufficient to support the current reasoning step.

## Architecture Onboarding

**Component Map**
REAP -> Sub-task Planner -> Fact Extractor -> Evidence Retrieval -> Answer Generation

**Critical Path**
Question -> Sub-task Decomposition -> Planning Phase -> Evidence Retrieval -> Fact Extraction -> Recursive Evaluation -> Answer Generation

**Design Tradeoffs**
The modular separation of planning and extraction enables specialization but introduces coordination overhead. Recursive evaluation improves accuracy but adds computational cost. Structured sub-tasks enhance interpretability but may limit flexibility in handling highly unstructured queries.

**Failure Signatures**
- Sub-task Planner gets stuck in loops or dead-ends
- Fact Extractor fails to find sufficient evidence for critical reasoning steps
- Recursive evaluation fails to correct initial planning errors
- Coordination breakdown between modules leads to incomplete or inconsistent reasoning

**First 3 Experiments to Run**
1. Ablation test: Remove Sub-task Planner and measure performance degradation
2. Error injection: Introduce retrieval errors and observe recovery capability
3. Scalability test: Measure performance on increasingly complex multi-hop questions

## Open Questions the Paper Calls Out

None

## Limitations
- Results primarily validated on HotpotQA and select out-of-domain datasets, with limited exploration of real-world, open-domain scenarios
- Recursive evaluation mechanism may introduce latency or computational overhead in production settings
- Interpretability claims are not fully explored qualitativelyâ€”limited analysis of how users can meaningfully intervene or debug reasoning

## Confidence

**High:** Core technical claims of improved accuracy and error recovery are well-supported by empirical results and ablation studies.

**Medium:** Generalizability and practical deployment readiness are reasonably supported but primarily confined to curated benchmarks.

**Medium:** Interpretability benefit is plausible but lacks deep qualitative analysis of practical usability.

## Next Checks
1. **Large-scale user studies** to assess practical interpretability and usability of REAP's structured sub-task representations in real-world settings.
2. **Efficiency benchmarking** under production-like loads and variable retrieval quality to quantify trade-offs of recursive evaluation.
3. **Transfer experiments** on diverse, open-domain, and dynamic knowledge sources (e.g., news or social media) to evaluate robustness beyond curated datasets.