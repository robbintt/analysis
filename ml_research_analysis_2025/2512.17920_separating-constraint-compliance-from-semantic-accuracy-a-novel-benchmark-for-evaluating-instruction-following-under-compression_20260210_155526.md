---
ver: rpa2
title: 'Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark
  for Evaluating Instruction-Following Under Compression'
arxiv_id: '2512.17920'
source_url: https://arxiv.org/abs/2512.17920
tags:
- constraint
- compression
- compliance
- words
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel benchmark to evaluate how prompt
  compression affects instruction-following in large language models (LLMs). The Compression-Decay
  Comprehension Test (CDCT) independently measures constraint compliance (e.g., word
  count adherence) and semantic accuracy across varying compression levels.
---

# Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression

## Quick Facts
- arXiv ID: 2512.17920
- Source URL: https://arxiv.org/abs/2512.17920
- Reference count: 1
- Key outcome: Introduces CDCT benchmark revealing U-shaped constraint violations at medium compression (27 words), orthogonal CC/SA dimensions, and 598% CC improvement via RLHF ablation

## Executive Summary
This paper introduces the Compression-Decay Comprehension Test (CDCT) to evaluate how prompt compression affects instruction-following in LLMs. The benchmark independently measures constraint compliance (e.g., word count adherence) and semantic accuracy across five compression levels. Key findings include a universal U-shaped pattern in constraint violations peaking at medium compression (~27 words), statistically orthogonal compliance and accuracy dimensions, and dramatic improvements in constraint compliance (598% average) when helpfulness signals are removed from system prompts. The study reveals that medium-length prompts pose the highest risk for instruction-following failures and offers actionable guidelines for improving deployed systems.

## Method Summary
CDCT evaluates 9 LLMs across 8 scientific concepts at 5 compression levels (c=0.0 to 1.0, yielding ~2-135 words). Each prompt includes an explicit 35-word constraint for responses. Constraint compliance (CC) and semantic accuracy (SA) are scored independently by a three-judge LLM panel (Claude Opus 4.1-2, GPT-5.1, DeepSeek-v3.1) using 0-10 rubrics. Full context prompts (c=1.0) are compressed via GPT-5.1 for shorter versions. RLHF ablation removes helpfulness-related phrases from system prompts. Inter-rater reliability is measured using Fleiss' kappa (CC: κ=0.90, SA: κ=0.25).

## Key Results
- Constraint compliance follows a U-shaped curve, peaking at medium compression (c=0.5, ~27 words) with 40-60% higher violations
- CC and SA are statistically orthogonal (r=0.12), indicating independent dimensions of instruction-following
- RLHF ablation improves constraint compliance by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance
- Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96) in constraint compliance

## Why This Works (Mechanism)

### Mechanism 1: Constraint Salience Gradient
Constraint compliance follows a U-shaped curve because constraint salience is non-monotonic across compression levels—high at extremes, low in the middle. At extreme compression (c=0.0, ~2 words), prompts implicitly signal brevity. At full context (c=1.0, ~135 words), constraints are explicitly repeated. At medium compression (c=0.5, ~27 words), constraints are "buried" among semantic content, creating task-frame ambiguity where models default to helpfulness behaviors.

### Mechanism 2: RLHF Helpfulness-Compliance Tension
RLHF training for "helpfulness" actively suppresses constraint-following at medium compression lengths. RLHF optimizes for comprehensive, detailed responses. When prompts contain partial context (medium compression), helpfulness priors dominate over constraint adherence. Removing helpfulness signals allows constraint-following circuits to activate.

### Mechanism 3: Reasoning Architecture Advantage
Reasoning-optimized models (O3, GPT-5, O4-Mini) maintain constraint compliance better because their architecture supports explicit step-wise constraint checking before output generation. Reasoning models generate intermediate planning states where constraints can be verified against planned output length, enabling correction before final response. Efficient models skip this verification step.

## Foundational Learning

- **RLHF (Reinforcement Learning from Human Feedback)**: The paper's central mechanism—that helpfulness training suppresses constraint compliance—requires understanding what RLHF optimizes for and how it shapes behavioral priors. Quick check: Can you explain why an RLHF-trained model might produce longer, more detailed answers than requested?

- **Inter-rater Reliability (Fleiss' Kappa)**: The paper claims CC is "objectively measurable" based on κ=0.90. Understanding why this matters for distinguishing real phenomena from measurement artifacts is critical. Quick check: Why is Fleiss' κ=0.90 for CC vs. κ=0.25 for SA significant for the paper's claims?

- **Task-Frame Ambiguity**: The theoretical model hinges on models entering different behavioral modes based on perceived task type. This concept explains why medium-length prompts fail uniquely. Quick check: At c=0.5, what two competing signals does the model receive, and why doesn't it just follow the explicit constraint?

## Architecture Onboarding

Component map: [Prompt Compression Level] → [Constraint Salience Calculation] → [Behavioral Mode Selection] → [Output Generation]

Critical path: Prompt length → constraint salience → behavioral mode → output. The failure point is at behavioral mode selection when salience is low and helpfulness priors dominate.

Design tradeoffs:
- Reasoning models: Higher CC (+27.5%) but slower inference and higher cost
- Explicit constraint repetition: Improves CC at c=0.5 but increases prompt token overhead
- RLHF ablation: Dramatically improves CC but may reduce user satisfaction on open-ended queries

Failure signatures:
- Medium-length prompts (~20-35 words) with single constraint mentions → 40-60% higher violation rates
- Concepts with strong helpfulness associations (e.g., "explain impressionism") → worse CC at c=0.5
- Efficient models on formal science concepts → SA maintained but CC degraded

First 3 experiments:
1. Baseline mapping: Run your target model on CDCT-style prompts across c∈{0.0, 0.25, 0.5, 0.75, 1.0} to identify your specific "danger zone" word count range.
2. RLHF ablation test: Compare constraint compliance with standard system prompts vs. prompts explicitly removing "be helpful and comprehensive" language at your identified danger zone length.
3. Constraint emphasis manipulation: At c=0.5, test whether "EXACTLY 35 WORDS. DO NOT EXCEED 35 WORDS." improves CC over "35 words"—validates salience hypothesis for your deployment context.

## Open Questions the Paper Calls Out

### Open Question 1
Do attention patterns on constraint tokens (e.g., "35 words") show minimal weights at medium compression (c=0.5) compared to full context (c=1.0), as predicted by the constraint salience hypothesis? No attention analysis was conducted; the paper validates the hypothesis behaviorally via RLHF ablation but not mechanistically.

### Open Question 2
Does the U-curve pattern generalize to non-length constraints such as JSON formatting requirements, toxicity avoidance, or stylistic constraints? CDCT only tests word-count constraints; other constraint types remain unexamined.

### Open Question 3
Do algorithmic compression methods (e.g., LLMLingua) produce the same U-curve in constraint compliance observed with model-generated compression? All experiments used GPT-5.1 for compression; extractive or algorithmic approaches may fragment constraint language differently.

### Open Question 4
Does explicit constraint emphasis (bold, repetition, capitalization) at medium compression improve constraint compliance by 30–40% without affecting semantic accuracy? The paper tests RLHF ablation but not formatting manipulations.

## Limitations
- Compression achieved through model-generated rewriting rather than algorithmic schemes like LLMLingua
- Only tests word-count constraints; other constraint types remain unexamined
- External validity untested—specific prompt construction choices may drive U-shaped pattern

## Confidence

- **High confidence**: The CC/SA orthogonality (r=0.12) and RLHF ablation magnitude (598% CC improvement) are robust findings supported by multiple models and statistical significance.
- **Medium confidence**: The U-shaped compression effect generalizes beyond the specific concepts and word counts tested.
- **Low confidence**: The architectural reasoning advantage derives from mechanisms rather than observed scaling relationships.

## Next Checks

1. **Cross-dataset replication**: Apply CDCT methodology to non-scientific concepts (e.g., legal, creative writing) to test whether U-shaped compression effects persist across domains.

2. **Ablation boundary testing**: Systematically vary which "helpfulness" phrases are removed (e.g., "comprehensive" vs. "detailed") to map the suppression mechanism's specificity.

3. **Attention pattern analysis**: Compare self-attention distributions at c=0.5 between models showing high vs. low CC to identify whether constraint signals are genuinely "buried" or attention patterns differ systematically.