---
ver: rpa2
title: 'VoxRAG: A Step Toward Transcription-Free RAG Systems in Spoken Question Answering'
arxiv_id: '2505.17326'
source_url: https://arxiv.org/abs/2505.17326
tags:
- retrieval
- answer
- audio
- segments
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VoxRAG, a speech-to-speech retrieval-augmented
  generation system that bypasses transcription to retrieve semantically relevant
  audio segments directly from spoken queries. The system employs silence-aware segmentation,
  speaker diarization, CLAP audio embeddings, and FAISS retrieval using L2-normalized
  cosine similarity.
---

# VoxRAG: A Step Toward Transcription-Free RAG Systems in Spoken Question Answering

## Quick Facts
- **arXiv ID**: 2505.17326
- **Source URL**: https://arxiv.org/abs/2505.17326
- **Reference count**: 9
- **Key outcome**: Transcription-free speech-to-speech retrieval achieves Recall@10 of 0.60 for somewhat relevant segments but only 0.34 for very relevant segments, with answer precision scores of 0.46 on a 0-2 scale

## Executive Summary
VoxRAG introduces a transcription-free retrieval-augmented generation system that retrieves semantically relevant audio segments directly from spoken queries without converting speech to text during retrieval. The system uses CLAP audio embeddings, silence-aware segmentation with speaker diarization, and FAISS retrieval to enable speech-to-speech RAG. Tested on 50 spoken queries from podcast content, the system shows strong topical alignment but struggles with factual precision, highlighting both the feasibility and limitations of transcription-free audio retrieval.

## Method Summary
VoxRAG processes spoken queries through silence-aware segmentation using Silero VAD and NeMo ClusteringDiarizer, creating speaker-labeled segments under 90 seconds. CLAP audio embeddings are L2-normalized and indexed in FAISS FlatIP for cosine similarity retrieval. The top-10 segments are retrieved and combined with adjacent context, then transcribed using Faster-Whisper for LLM input. GPT-4o generates answers from the segment transcripts. The system processes spoken queries to spoken answers without transcription during retrieval, only using Whisper for answer generation.

## Key Results
- Recall@10 for very relevant segments: 0.34
- Recall@10 for somewhat relevant segments: 0.60
- nDCG@10 for somewhat relevant segments: 0.27
- Answer quality scores (0-2 scale): relevance 0.84, accuracy 0.58, completeness 0.56, precision 0.46
- End-to-end latency: approximately 3 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLAP embeddings enable semantic-level retrieval from audio without word-level transcription
- Mechanism: CLAP maps audio into a joint audio-language embedding space where acoustically diverse but semantically similar content clusters together, allowing cosine similarity to identify topically aligned segments even with informal speech, background noise, or laughter
- Core assumption: The pre-trained CLAP model generalizes sufficiently from its training distribution to podcast-style informal audio
- Evidence anchors: [abstract] "VoxRAG employs... CLAP audio embeddings... to retrieve semantically relevant segments directly from spoken queries"; [section 2.1] "This allows semantic-level retrieval even in the absence of exact word overlap, making it more robust for podcast audio that includes informal speech, background noise, or laughter"
- Break condition: When queries require fine-grained factual precision rather than topical alignment

### Mechanism 2
- Claim: Silence-aware segmentation combined with speaker diarization produces retrievable chunks that respect both acoustic and discourse boundaries
- Mechanism: Silero VAD identifies speech vs. silence regions; NeMo's ClusteringDiarizer assigns speaker labels; these are merged to create speaker-coherent segments under 90 seconds, avoiding cross-speaker contamination within retrieval units
- Core assumption: Speaker turns and silence boundaries approximate semantic topic boundaries in conversational podcast content
- Evidence anchors: [abstract] "VoxRAG employs silence-aware segmentation, speaker diarization..."; [section 2.1] "VAD identifies valid speech spans, which are then merged with speaker labels to define segments"
- Break condition: When semantic coherence spans multiple speaker turns or when overlapping speech causes segmentation errors

### Mechanism 3
- Claim: L2-normalized CLAP embeddings with FAUSS inner-product search provide computationally tractable retrieval at interactive latencies
- Mechanism: L2 normalization converts dot product to cosine similarity; FAUSS FlatIP index enables brute-force similarity search over normalized vectors without approximation
- Core assumption: Cosine similarity in CLAP space meaningfully ranks semantic relevance for spoken queries
- Evidence anchors: [section 2.2] "The top ten segments are retrieved by cosine similarity"; [appendix C] "This normalization step ensures that inner product similarity in the FAUSS FlatIP index is equivalent to cosine similarity"
- Break condition: Adding a cross-encoder reranker decreased performance (Recall@10 from 0.34 to 0.26 for very relevant), suggesting reranking on transcript text introduces ASR noise that degrades audio-native retrieval signals

## Foundational Learning

- Concept: **Contrastive Language-Audio Pretraining (CLAP)**
  - Why needed here: CLAP is the core embedding mechanism; understanding its joint audio-language space is essential for diagnosing retrieval failures
  - Quick check question: Can you explain why CLAP might retrieve "somewhat relevant" but not "very relevant" segments for the same query?

- Concept: **Speaker Diarization**
  - Why needed here: Diarization determines chunk boundaries; errors here propagate to retrieval units
  - Quick check question: What happens to retrieval quality if diarization splits a coherent answer across two segments?

- Concept: **Recall@K and nDCG@K**
  - Why needed here: These are the primary evaluation metrics; understanding them is necessary to interpret results and design improvements
  - Quick check question: Why does Recall@10 of 0.60 for "somewhat relevant" but only 0.34 for "very relevant" indicate a precision problem rather than a recall problem?

## Architecture Onboarding

- Component map: Spoken query → CLAP embedding → FAISS cosine search → top-10 segment transcripts → GPT-4o prompt → text answer
- Critical path: Spoken query → CLAP embedding → FAISS cosine search → top-10 segments + adjacent context → GPT-4o prompt → text answer. Transcription occurs only after retrieval, preserving audio-native retrieval signals
- Design tradeoffs:
  - **Audio-only retrieval vs. hybrid**: Avoids ASR error propagation during retrieval but sacrifices lexical precision; cross-encoder reranking on transcripts degraded performance
  - **Chunk size (<90s)**: Balances context preservation against retrieval granularity; longer chunks provide more context but dilute relevance signals
  - **Transcription for generation only**: Maintains audio-native retrieval but reintroduces ASR noise at generation stage—a partial compromise acknowledged by authors
- Failure signatures:
  - **High relevance, low precision**: Answers are "about the right thing" but lack factual specificity (relevance 0.84, precision 0.46)
  - **Topic-specific anomalies**: Queries containing "shower" achieved perfect scores 40% of the time vs. 20% overall—suggests CLAP embedding quality varies by concept domain
  - **Reranker degradation**: Cross-encoder reranking reduced Recall@10 (0.34→0.26), indicating transcript-based reranking injects noise from ASR errors
- First 3 experiments:
  1. **Baseline comparison**: Implement text-based RAG with Whisper transcripts + BM25 or dense text retrieval to quantify the transcription-free tradeoff
  2. **Chunk size ablation**: Test segment length thresholds (30s, 60s, 90s, 120s) to find optimal granularity for podcast content
  3. **Embedding model swap**: Compare CLAP against wav2vec 2.0 or SpeechDPR embeddings to determine if alternative audio encoders improve precision without sacrificing transcription-free operation

## Open Questions the Paper Calls Out

- Can audio-native generation methods replace the current reliance on transcripts for answer synthesis? (explicitly calls for future work to "explore audio-native generation methods")
- How can audio embeddings be optimized to distinguish between topical alignment and precise factual retrieval? (identifies "gap between topical relevance and factual precision" and cites need for "improved embedding fine-tuning")
- What is the performance tradeoff when comparing direct audio retrieval against transcript-based or hybrid retrieval baselines? (lists "absence of transcript-based or hybrid retrieval baselines" as key limitation)

## Limitations

- Limited evaluation on 50 queries from single native English speaker raises generalization concerns
- Transcription used for answer generation reintroduces ASR noise, undermining fully transcription-free goal
- Absence of transcript-based or hybrid retrieval baselines prevents quantifying the transcription-free tradeoff

## Confidence

**High Confidence** - The architectural design combining CLAP embeddings, silence-aware segmentation, and FAISS retrieval is technically sound and internally consistent

**Medium Confidence** - The core claim that transcription-free retrieval is "feasible" is supported by experimental results, but practical utility is limited by low precision scores

**Low Confidence** - Claims about CLAP's robustness to informal speech and background noise are based on mechanism description rather than systematic ablation studies

## Next Checks

1. **Multi-speaker evaluation** - Test system with 100+ queries from speakers with diverse accents, non-native English speakers, and different age groups to assess generalization claims

2. **Direct comparison baseline** - Implement text-based RAG system using Whisper transcripts and standard dense retrieval, then compare both retrieval quality and answer accuracy on identical query sets

3. **Embedding ablation study** - Systematically compare CLAP against alternative audio embeddings (wav2vec 2.0, SpeechDPR) while holding all other components constant