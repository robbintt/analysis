---
ver: rpa2
title: Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?
arxiv_id: '2512.17394'
source_url: https://arxiv.org/abs/2512.17394
tags:
- reasoning
- cultural
- visual
- task
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CulturalToM-VQA, a benchmark of 5,095 visual
  Theory of Mind questions across six culturally diverse ToM task types and four complexity
  levels. A VLM-assisted human-in-the-loop pipeline is used to construct the dataset,
  combining expert image curation with GPT-4.1 for structured question generation
  and validation.
---

# Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?

## Quick Facts
- arXiv ID: 2512.17394
- Source URL: https://arxiv.org/abs/2512.17394
- Reference count: 40
- Primary result: Introduces CulturalToM-VQA benchmark revealing VLMs rely on parametric social priors rather than visual grounding for Theory of Mind reasoning

## Executive Summary
This paper introduces CulturalToM-VQA, a benchmark of 5,095 visual Theory of Mind questions across six culturally diverse task types and four complexity levels. Using a VLM-assisted human-in-the-loop pipeline, the authors construct the dataset and evaluate 10 VLMs (2023-2025), finding that while recent models achieve >93% accuracy, they often rely on parametric social priors rather than visual grounding. The study reveals significant challenges in false belief reasoning and cross-cultural generalization, with SOTA models exhibiting social desirability bias by systematically favoring positive answer choices without visual evidence.

## Method Summary
The CulturalToM-VQA benchmark is constructed through a human-in-the-loop pipeline using GPT-4.1 for scene description and question generation. The dataset comprises 394 images from FindingEmo, CulturalVQA, and CVQA sources, yielding 5,095 multiple-choice questions across six ToM tasks and four complexity levels. The evaluation involves 10 VLMs using three prompting strategies (zero-shot, CoT, and compositional CoT) with visual ablation conditions to distinguish between visual grounding and parametric priors.

## Key Results
- Frontier VLMs achieve >93% accuracy but maintain high performance even without visual input, suggesting reliance on parametric social priors
- False belief reasoning remains challenging, with accuracy ranging from 19-83% across models
- Cross-cultural generalization shows 20-30% performance gaps between regions, with systematic drops in non-Western contexts
- Social desirability bias is evident, with models favoring semantically positive answers 20%+ more often than chance when visual evidence is absent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frontier VLMs achieve high ToM accuracy partly by leveraging parametric social priors rather than visual grounding.
- Mechanism: Models use learned social knowledge from pretraining to infer plausible answers without processing visual input, creating an "illusion of empathy."
- Core assumption: Social reasoning patterns in training data transfer to ToM tasks without requiring visual perception.
- Evidence anchors:
  - [abstract] "Ablation studies reveal that SOTA models often rely on parametric social priors... frequently defaulting to positive answer choices without visual input."
  - [section] Table 3 shows Qwen2.5-VL maintains 94.6% accuracy without images vs 95.7% with images—a minimal 1.1% gap.
  - [corpus] Related work on ToM in embodied agents (MindPower, arXiv:2511.23055) similarly finds models struggle with perspective-taking without explicit architectural support.
- Break condition: Tasks requiring strict visual evidence (e.g., identifying specific facial expressions contradicted by cultural context) should degrade blind performance significantly.

### Mechanism 2
- Claim: Safety alignment induces social desirability bias, causing models to prefer semantically positive answers.
- Mechanism: Alignment procedures teach models to avoid harmful outputs, which generalizes to preferring harmonious/positive interpretations even when incorrect.
- Core assumption: Alignment training creates a heuristic linking "positive sentiment" to "correct/safe" responses.
- Evidence anchors:
  - [abstract] "SOTA models show social desirability bias—systematically favoring semantically positive answer choices over negative ones without visual evidence."
  - [section] Table 9: In conflict cases where correct answer isn't most positive, Qwen2.5-VL selects the most positive distractor 55.6% of time vs 33.3% random baseline.
  - [corpus] Related work on prosocial beliefs steering LLM behavior (arXiv:2505.24255) shows similar alignment effects on social reasoning tasks.
- Break condition: Tasks with neutral or negative correct answers in low-ambiguity visual contexts should show reduced positivity drift.

### Mechanism 3
- Claim: Compositional CoT scaffolding benefits older VLMs but yields minimal gains for frontier models near their zero-shot ceiling.
- Mechanism: Explicit scene graph construction forces structured visual reasoning, helping models with weaker inherent grounding; frontier models already internalize this structure.
- Core assumption: Reasoning gains from explicit scaffolding diminish as models acquire stronger implicit reasoning capabilities through scale/alignment.
- Evidence anchors:
  - [abstract] "While compositional CoT benefits older models, newer ones show little gain."
  - [section] Table 2: LLaVA-Next improves from 71.3% to 76.5% with Comp-CoT (+5.2%), while Qwen2.5-VL stays within 1.5% variance across strategies.
  - [corpus] Evidence limited; neighboring papers don't directly address CoT effectiveness variance across model generations.
- Break condition: Novel task types outside training distribution should restore CoT benefits even for frontier models.

## Foundational Learning

- **Theory of Mind (ToM) Task Taxonomy**
  - Why needed here: The benchmark spans six distinct cognitive abilities (MSA, FBR, NLC, SNV, PC, MAR) with different complexity profiles—understanding this taxonomy is essential for interpreting failure patterns.
  - Quick check question: Can you explain why False Belief Reasoning (FBR) requires suppressing visible reality to infer an agent's mistaken belief?

- **Q-Matrix for Cognitive Diagnosis**
  - Why needed here: The paper formalizes task-complexity mapping using a binary Q-matrix from cognitive diagnosis models, encoding which tasks are cognitively appropriate at each complexity level.
  - Quick check question: Given the Q-matrix in Table 1, why is Multi-agent Reasoning (MAR) only valid at Complexity Level 4?

- **Visual Ablation Methodology**
  - Why needed here: The paper's central diagnostic relies on comparing performance with vs. without images to distinguish visual grounding from parametric priors.
  - Quick check question: If a model achieves 90% accuracy both with and without visual input, what does this suggest about its reasoning mechanism?

## Architecture Onboarding

- **Component map:**
  - Image sources -> Human expert curation -> GPT-4.1 scene description with ToM cues -> Structured question generation -> Human validation -> CulturalToM-VQA benchmark

- **Critical path:**
  1. Filter images for social/mental state interpretability and cultural specificity (4 criteria in Section 2.2)
  2. Generate structured descriptions with Emotional/ToM/Cultural cues via conditioned prompting
  3. Map each valid (task, complexity) pair to one MCQ with near-miss distractors
  4. Validate via automated parsing + expert review (κ=0.86 on cultural appropriateness)

- **Design tradeoffs:**
  - **Dataset scale vs. inter-question dependence**: 5,095 questions from 394 images (~13/image) enables controlled variation but risks scene-archetype exploitation
  - **GPT-4.1 annotation efficiency vs. bias propagation**: Scalable but introduces teacher model's cultural knowledge boundaries
  - **English-only evaluation vs. cross-lingual coverage**: Enables focused cultural-content testing but excludes language-specific ToM reasoning

- **Failure signatures:**
  - High accuracy gap between visual and blind conditions → weak visual grounding
  - Elevated "excess positivity drift" on incorrect predictions → social desirability bias
  - Sharp performance drops on False Belief Reasoning (19–83% range) → struggle with belief-reality dissociation
  - Regional variance (20–30% gaps) → uneven cultural representation in training

- **First 3 experiments:**
  1. **Visual ablation baseline**: Run zero-shot evaluation with and without images on Qwen family to quantify parametric prior reliance (replicate Table 3 methodology).
  2. **Positivity bias probe**: Implement the sentiment-based classifier and measure Excess Positivity Drift across model generations to validate alignment-induced bias hypothesis.
  3. **Cross-cultural variance analysis**: Stratify CVQA split results by region to identify systematic gaps (replicate Figure 4 analysis on additional models).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can vision-language model architectures be modified to enforce strict visual grounding for social reasoning, preventing models from relying on parametric social priors?
- Basis in paper: [explicit] The authors conclude that "achieving robust, visually grounded understanding remains an open challenge," noting that frontier models maintain high accuracy even when visual inputs are removed.
- Why unresolved: Ablation studies show SOTA models achieve high scores via "visual bypass," utilizing pre-trained knowledge rather than the provided image.
- What evidence would resolve it: Demonstration of a model where performance drops significantly (e.g., >40%) when visual inputs are ablated, or where attention mechanisms provably track visual social cues.

### Open Question 2
- Question: Can safety alignment techniques be refined to reduce "social desirability bias" without compromising helpfulness, allowing models to predict negative or socially undesirable mental states accurately?
- Basis in paper: [explicit] The paper highlights an "illusion of empathy" where models systematically favor semantically positive answer choices (social desirability bias) due to alignment, potentially masking a lack of true understanding.
- Why unresolved: Current models select "positive" distractors 20%+ more often than chance when visual grounding is absent.
- What evidence would resolve it: A model that maintains high accuracy on negative-valence ToM questions (e.g., detecting deception or anger) without exhibiting excess positivity drift.

### Open Question 3
- Question: What specific data curation strategies are required to close the 20–30% performance gap in Theory of Mind reasoning between Western and non-Western cultural contexts?
- Basis in paper: [inferred] While the authors explicitly note "high regional variance," the underlying cause—whether due to training data distribution or architectural limitations—remains unaddressed.
- Why unresolved: Models perform well on Western-centric tasks but show "drastic drops" in regions like Bulgaria and Russia, and existing benchmarks are noted as Western-centric.
- What evidence would resolve it: Evaluation of models fine-tuned on geographically balanced social interaction datasets showing reduced variance across the regions tested in CulturalToM-VQA.

## Limitations
- The study relies on GPT-4.1 for annotation, which may introduce cultural knowledge biases from the teacher model
- English-only evaluation limits generalizability to multilingual ToM reasoning
- Visual ablation methodology may underestimate multimodal pretraining capabilities

## Confidence
- High confidence: The parametric social priors mechanism (Mechanism 1) is strongly supported by the minimal accuracy gap between visual and blind conditions in Table 3
- Medium confidence: The social desirability bias mechanism (Mechanism 2) is well-documented through sentiment analysis, though alternative explanations for positivity drift exist
- Low confidence: The compositional CoT benefits claim (Mechanism 3) has limited corpus support and the evidence shows mixed patterns across model generations

## Next Checks
1. **Visual grounding validation**: Replicate the blind ablation experiment with additional VLMs (e.g., Gemini, GPT-4V) to test whether parametric priors consistently explain high accuracy without visual input

2. **Cross-cultural generalization stress test**: Create adversarial examples where cultural context contradicts learned social priors to determine whether models can override parametric knowledge with visual evidence

3. **Alignment bias quantification**: Implement counterfactual prompting that explicitly rewards negative or neutral responses in low-ambiguity scenarios to measure the strength of social desirability bias