---
ver: rpa2
title: 'RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment'
arxiv_id: '2506.21037'
source_url: https://arxiv.org/abs/2506.21037
tags:
- selection
- training
- data
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a reinforcement learning-driven data selection\
  \ method to address the redundancy issue in large-scale datasets. The method introduces\
  \ the concept of \u03F5-sample cover to quantify sample redundancy based on inter-sample\
  \ relationships and reformulates data selection as an RL process."
---

# RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment

## Quick Facts
- arXiv ID: 2506.21037
- Source URL: https://arxiv.org/abs/2506.21037
- Reference count: 40
- Primary result: RL-based data selection method that outperforms state-of-the-art baselines while maintaining competitive training efficiency

## Executive Summary
This paper proposes RL-Selector, a reinforcement learning-driven approach to data selection that addresses redundancy in large-scale datasets. The method introduces the concept of ε-sample cover to quantify sample redundancy based on inter-sample relationships and reformulates data selection as an RL process. A lightweight A2C-based RL agent optimizes sample-wise scores by leveraging ε-sample cover as reward signals. Extensive experiments on benchmark datasets demonstrate that the method consistently outperforms state-of-the-art baselines in performance, cross-architecture generalization, and generalization to challenging scenarios, while maintaining competitive training efficiency. Models trained with the selected datasets show enhanced generalization performance compared to those trained on full datasets.

## Method Summary
RL-Selector combines a target classification model (e.g., ResNet-18) with a lightweight A2C agent to perform data selection. The target model extracts features from the penultimate layer, which serve as state input to the RL agent. The agent outputs continuous selection scores for each sample, which are optimized using a reward signal composed of two components: a ratio alignment term (r₁) that encourages the selection ratio to match a target, and a coverage-based term (r₂) that rewards pruning samples that are ε-covered by near neighbors. The ε-sample cover concept quantifies redundancy by measuring feature distance within classes. After training, selection scores are binarized (threshold 0.5) to create the final dataset subset. The method demonstrates strong cross-architecture generalization, with subsets selected using a lightweight proxy architecture effectively training larger, more complex target architectures.

## Key Results
- RL-Selector consistently outperforms state-of-the-art data selection baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1k
- Selected datasets exhibit superior cross-architecture generalization, benefiting training on architectures different from the proxy model
- Models trained on RL-Selector subsets achieve enhanced generalization performance compared to those trained on full datasets
- The method maintains competitive training efficiency while achieving these performance gains

## Why This Works (Mechanism)

### Mechanism 1: Geometric Redundancy Pruning via ε-Sample Cover
The method reduces dataset redundancy by pruning samples that are "covered" by near neighbors in the feature space. The ε-sample cover quantifies if sample xᵢ is ε-covered by xⱼ (feature distance ≤ ε), hypothesizing they yield similar gradients. By calculating the degree of ε-coverage and rewarding pruning of highly covered samples via r₂, the RL agent prioritizes keeping geometrically diverse samples. The core assumption is that the geometric distribution of samples in the feature space accurately reflects information density and redundancy relevant to training. This mechanism fails if the feature extractor doesn't capture semantic similarity or if noisy outliers (which are "uncovered" by definition) are erroneously kept.

### Mechanism 2: Dynamic RL Formulation for Group Effect
Formulating selection as an RL process allows the system to capture training dynamics and the "group effect" better than static scoring methods. Unlike static methods that score once with a pre-trained model, this method uses a lightweight A2C agent that optimizes selection scores based on an evolving reward signal derived from the changing feature distribution during training. This theoretically accounts for how sample utility changes as the model trains. The core assumption is that the reward signal (balance of selection ratio and coverage) is a valid proxy for long-term utility for generalization. This mechanism breaks if reward shaping is misaligned with generalization, causing the agent to converge to a suboptimal policy.

### Mechanism 3: Proxy Architecture Transferability
A dataset selected using features from a lightweight proxy architecture generalizes effectively to larger, more complex target architectures. The method uses a "target model" (e.g., ResNet-18) solely to estimate feature distances, capturing the intrinsic data structure (the "moderate coreset"). The paper argues this structure is robust enough to benefit training on unrelated architectures like ViT or Swin-Transformer. The core assumption is that redundancy defined by the proxy model's geometry is transferable to other architectures' feature spaces. This mechanism fails if the proxy model's feature space is fundamentally misaligned with the target model (e.g., CNN texture bias vs. Transformer shape bias).

## Foundational Learning

- **Concept: The "Group Effect" in Data Selection**
  - Why needed here: The paper explicitly positions itself against prior "importance score-based methods" that evaluate samples individually. Understanding that combining high-score samples doesn't always yield the best subset is crucial.
  - Quick check question: Why does selecting the top 10% of "hardest" samples often perform worse than selecting a diverse moderate subset?

- **Concept: Actor-Critic (A2C) Architecture**
  - Why needed here: The core optimization engine is a lightweight A2C network. You must understand the roles of the Actor (policy for selection scores) and Critic (value estimation) to debug the training loop.
  - Quick check question: In this context, what represents the "Action" and what represents the "Environment" in the MDP formulation?

- **Concept: Feature Space Geometry (Coresets)**
  - Why needed here: The theoretical grounding relies on ε-sample cover. You need to understand how distances in the feature space (Euclidean distance of feature maps) relate to sample redundancy.
  - Quick check question: If two images of different classes have very similar feature vectors, would the ε-cover mechanism correctly handle them? (Note: The method computes distances within the same class k per Eq. 6, preventing cross-class merging).

## Architecture Onboarding

- **Component map:** Target Model (f_θ) -> Distance Calculator -> RL Agent (A2C) -> Reward Module
- **Critical path:** Forward pass of Target Model → get feature maps → Compute intra-class distances → get E_c(x) (degree of cover) → RL Agent acts on state → gets scores → Calculate Reward → Update Actor/Critic → After training, binarize scores (threshold 0.5) to select final dataset
- **Design tradeoffs:** Proxy Model Size (smaller proxy speeds up selection but may lose semantic nuance), A2C Simplicity (3 linear layers limit capacity but reduce overhead), Transferability (fine-tuning a pre-trained policy is faster but assumes similar training dynamics)
- **Failure signatures:** Selection Ratio Drift (if r₁ is weighted incorrectly, agent might select 100% or 0% of data), Outlier Retention (uncovered samples are rewarded, so noisy outliers might be aggressively retained, degrading accuracy), Instability (A2C can be unstable; if target model learns too fast, feature space shifts too quickly for agent to track)
- **First 3 experiments:**
  1. Overfit Test: Run RL-Selector on a synthetic dataset with known redundancy (e.g., duplicate images). Verify that the "degree of cover" metric correctly identifies and prunes duplicates.
  2. Ablation on Reward: Isolate r₁ (random selection at target ratio) vs r₂ (coverage-only). Confirm that the combination is necessary for performance gains over random baselines.
  3. Architecture Transfer Check: Select data using ResNet-18, then train a ResNet-50 on that subset. Compare against selecting data with ResNet-50 to quantify the "proxy gap."

## Open Questions the Paper Calls Out

- **Question:** How can the RL-Selector framework be adapted to maintain robustness when applied to datasets where label noise or corrupted samples are dominant, rather than assuming limited noise?
- **Question:** Can the proposed ε-sample cover reward signal effectively generalize to dense prediction tasks like semantic segmentation or object detection?
- **Question:** How can the sample-wise scoring mechanism be modified to scale efficiently to datasets significantly larger than ImageNet-1k (e.g., hundreds of millions of samples)?

## Limitations

- The method's reliance on feature space geometry from a proxy architecture introduces transferability risks that are empirically demonstrated but not formally bounded
- The A2C implementation uses minimal network capacity (3 linear layers), which may limit performance on more complex datasets despite being chosen for efficiency
- Optimizing sample-wise scores may restrict applicability to extremely large-scale datasets where O(N²) distance calculations become computationally prohibitive

## Confidence

- **High confidence**: The RL formulation as an MDP and the basic training procedure (target model + A2C agent with standard optimization)
- **Medium confidence**: The cross-architecture generalization results, as they show consistent improvements but depend on the proxy model's feature space alignment
- **Medium confidence**: The efficiency claims, as training time improvements are shown relative to baselines but absolute training costs for the selection phase are not fully detailed

## Next Checks

1. **Robustness to proxy model choice**: Systematically vary the proxy architecture (ResNet-18, ResNet-50, EfficientNet) and measure the variance in selected subset quality to quantify transferability limits
2. **Outlier sensitivity analysis**: Construct synthetic datasets with injected outliers and measure whether the ε-cover mechanism incorrectly retains them due to their "uncovered" status
3. **Ablation on reward components**: Isolate r₁ and r₂ contributions by training with only one reward component to determine if their combination is synergistic or if one dominates performance