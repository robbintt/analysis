---
ver: rpa2
title: No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with
  Gaussian Processes
arxiv_id: '2510.20725'
source_url: https://arxiv.org/abs/2510.20725
tags:
- regret
- learning
- value
- bounds
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes sublinear regret bounds for Thompson Sampling
  in finite-horizon Markov Decision Processes with Gaussian Process priors over rewards
  and transitions. The authors introduce novel confidence bounds for compositional
  functions of GPs and a multi-output elliptical potential lemma that captures correlations
  across outputs.
---

# No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes

## Quick Facts
- **arXiv ID**: 2510.20725
- **Source URL**: https://arxiv.org/abs/2510.20725
- **Reference count**: 40
- **One-line primary result**: Sublinear regret bounds for Thompson Sampling in finite-horizon MDPs with Gaussian Process priors over rewards and transitions

## Executive Summary
This paper establishes sublinear regret bounds for Thompson Sampling in finite-horizon Markov Decision Processes with Gaussian Process priors over rewards and transitions. The authors introduce novel confidence bounds for compositional functions of GPs and a multi-output elliptical potential lemma that captures correlations across outputs. The theoretical analysis provides a regret bound of $\tilde{O}(\sqrt{KH\Gamma(KH)})$ where $\Gamma(\cdot)$ captures GP model complexity, demonstrating how kernel smoothness affects learning efficiency. Experiments validate the theoretical predictions across synthetic MDPs and sparse navigation tasks, showing sublinear regret growth consistent with the analysis.

## Method Summary
The paper presents RL-GPS, a Thompson Sampling algorithm for episodic RL in continuous MDPs with GP priors. The method maintains a multi-output Gaussian Process model for both rewards and transitions, sampling a model realization at each episode and computing a greedy policy via finite-horizon value iteration. The algorithm updates the GP posterior only at episode boundaries (delayed feedback), using sparse LMC (Linear Model of Coregionalization) with 100 inducing points. The theoretical analysis leverages compositional confidence bounds and a multi-output elliptical potential lemma to establish sublinear regret guarantees.

## Key Results
- Establishes sublinear regret bounds of $\tilde{O}(\sqrt{KH\Gamma(KH)})$ for Thompson Sampling in finite-horizon MDPs
- Introduces novel confidence bounds for compositional functions of GPs through Taylor expansion analysis
- Develops a multi-output elliptical potential lemma that captures correlations across reward and transition functions
- Demonstrates through experiments that kernel smoothness significantly impacts regret accumulation, with Matérn kernels outperforming RBF in less smooth environments

## Why This Works (Mechanism)

### Mechanism 1: Compositional Confidence Propagation
The paper suggests that confidence bounds can be constructed for value functions that are recursive compositions of Gaussian Processes (GPs), despite the composition not being a GP itself. The analysis linearizes the value function $V(f(z))$ around the GP posterior mean $\mu(z)$ using a Taylor expansion. By bounding the gradient $\nabla V$ and Hessian $\nabla^2 V$, the error of the composition is bounded by the GP's posterior standard deviation $\sigma(z)$ and a scaling factor $\beta$. This converts a complex recursive analysis into a tractable sum of uncertainty terms.

### Mechanism 2: Multi-Output Elliptical Potential Lemma
Regret can be bounded by the Maximum Information Gain (MIG) of the joint model rather than scaling linearly with state dimension $d_S$. The work introduces a multi-output Elliptical Potential Lemma that uses the norm of the posterior covariance vector $\|\sigma\|^2$ and bounds the cumulative sum by the log-determinant of the joint kernel matrix. This exploits correlations between state dimensions and rewards captured by the multi-output GP.

### Mechanism 3: Delayed Feedback Adaptation
Sublinear regret is maintained even when model updates are batched at the end of episodes rather than performed step-by-step. The algorithm updates the GP posterior only after $H$ steps. The analysis uses a "delayed-update lemma" to relate the "stale" posterior variance used during the episode to the "fresh" variance that would exist with sequential updates.

## Foundational Learning

- **Concept: Thompson Sampling (Posterior Sampling)**
  - **Why needed here:** This is the core exploration strategy. Unlike UCB (optimism), TS samples a specific environment model from the posterior and acts optimally w.r.t that sample.
  - **Quick check question:** Can you explain why sampling a model realization might be computationally cheaper or more stable than constructing confidence sets for every state-action pair in continuous spaces?

- **Concept: Gaussian Process (GP) Priors & Kernels**
  - **Why needed here:** The theoretical bounds depend entirely on the GP's ability to model uncertainty via kernels. The regret scaling $\tilde{O}(\sqrt{KH\Gamma(KH)})$ is a direct function of the kernel information gain $\Gamma$.
  - **Quick check question:** How does the Matérn kernel smoothness parameter $\nu$ affect the theoretical regret bound and what does this imply for "rough" environments?

- **Concept: Bellman Recursion & Value Iteration**
  - **Why needed here:** The "compositional" challenge arises because RL value functions are built recursively ($Q = r + V_{next}$). Understanding this recursion is necessary to see why standard GP confidence bounds don't immediately apply.
  - **Quick check question:** Why does the recursive nature of $Q$-functions prevent us from treating the optimal value function as a simple Gaussian Process?

## Architecture Onboarding

- **Component map:** Buffer -> Multi-Output GP -> Sampler -> Planner
- **Critical path:** 
  1. Sample: Draw $\hat{f}_k \sim p(f | D_{1:k-1})$
  2. Plan: Compute $Q^*_{h,k}$ via backward induction on $\hat{f}_k$
  3. Act: Execute $\pi_k$ for $H$ steps
  4. Update: Update GP posterior with new transition data

- **Design tradeoffs:**
  - **RBF vs. Matérn Kernels:** RBF assumes high smoothness (lower regret $\tilde{O}(\sqrt{T})$ if correct), while Matérn ($\nu < \infty$) handles rougher dynamics at the cost of higher regret $\tilde{O}(T^{\frac{\nu+d}{2\nu+d}})$.
  - **Batch vs. Sequential Update:** Batching (episodic update) is computationally efficient but limits adaptivity within an episode.
  - **LMC Structure:** Using the Linear Model of Coregionalization (LMC) allows scaling to higher dimensions but assumes a specific correlation structure.

- **Failure signatures:**
  - **Linear Regret:** If cumulative regret grows linearly, the GP kernel is likely misspecified.
  - **Posterior Collapse:** If uncertainties ($\sigma$) decay too fast without converging to the true model, the exploration stops prematurely.

- **First 3 experiments:**
  1. **Sanity Check (GP-Sampled MDP):** Validate that regret is indeed sublinear in a controlled environment where the GP prior is well-specified.
  2. **Kernel Ablation (Navigation):** Compare RBF vs. Matérn $\nu=1.5$ vs. $\nu=2.5$ in a sparse navigation task to verify the trade-off between smoothness assumptions and regret accumulation.
  3. **Correlation Study (LMC vs Independent):** Test if explicitly modeling correlations between reward and transition functions (LMC) reduces regret compared to independent GPs, particularly in the "maze" environment.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the regret analysis for Thompson sampling with GP models be extended to infinite-horizon MDPs (discounted or average-reward settings)? The paper focuses on finite-horizon episodic MDPs; extending the analysis to infinite-horizon settings remains a promising direction for future research.

- **Open Question 2:** How does kernel misspecification affect the regret bounds of RL-GPS when the true environment dynamics do not match the assumed GP kernel smoothness? The experiments show that in sparse, less smooth environments, the RBF kernel accumulates regret more rapidly than the Matérn kernels due to model misspecification, but no theoretical characterization is provided.

- **Open Question 3:** Can the smoothness assumptions on value functions (bounded gradients and Hessians) be relaxed while maintaining sublinear regret guarantees? The paper acknowledges these assumptions exclude settings with discontinuous or non-smooth dynamics but claims they are milder than those commonly imposed.

## Limitations
- The analysis assumes twice-differentiable value functions, excluding environments with discontinuous or non-smooth dynamics
- The delayed-update mechanism may become a bottleneck in non-stationary environments or with extremely long horizons
- The theoretical guarantees require well-specified GP priors, and the impact of kernel misspecification on regret bounds is not quantified

## Confidence
- **High Confidence**: Regret bound structure and main theoretical results (Section 4)
- **Medium Confidence**: Experimental validation across different kernels and tasks
- **Low Confidence**: Claims about computational efficiency and scalability to higher dimensions

## Next Checks
1. **Kernel Robustness Test**: Evaluate regret performance when using RBF kernel in environments known to have non-smooth dynamics to verify the kernel smoothness requirement.
2. **Correlation Structure Ablation**: Compare LMC multi-output GP against independent GPs in tasks where reward and transition are truly independent to measure the actual benefit of modeling correlations.
3. **Non-Stationary Extension**: Modify the algorithm to handle slowly varying environments and test if the regret bounds degrade gracefully or if the delayed-update mechanism becomes a bottleneck.