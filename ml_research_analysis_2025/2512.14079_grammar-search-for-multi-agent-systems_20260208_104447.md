---
ver: rpa2
title: Grammar Search for Multi-Agent Systems
arxiv_id: '2512.14079'
source_url: https://arxiv.org/abs/2512.14079
tags:
- search
- mases
- grammar
- answer
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Grammar Search, a structured framework for
  discovering multi-agent systems (MAS) using a context-free grammar to compose modular
  components. By constraining the MAS space with composable terminals, the method
  guarantees syntactic correctness and eliminates invalid candidates during search.
---

# Grammar Search for Multi-Agent Systems

## Quick Facts
- arXiv ID: 2512.14079
- Source URL: https://arxiv.org/abs/2512.14079
- Reference count: 40
- Primary result: Grammar Search achieves 81.4% accuracy on MATH500 and 67.2% on MATH level 5, outperforming leading baselines (ADAS, AFlow) and manual MAS designs in 4 of 5 benchmarks.

## Executive Summary
This paper introduces Grammar Search, a structured framework for discovering multi-agent systems (MAS) using a context-free grammar to compose modular components. By constraining the MAS space with composable terminals, the method guarantees syntactic correctness and eliminates invalid candidates during search. Evaluation on MATH, AIME, MMLU-Pro, and GPQA shows Grammar Search achieves 81.4% accuracy on MATH500 and 67.2% on MATH level 5, outperforming leading baselines (ADAS, AFlow) and manual MAS designs in 4 of 5 benchmarks. The approach also reduces search cost by ~12%, generates valid MASes 100% of the time, and produces simpler, more interpretable code with less growth over iterations.

## Method Summary
Grammar Search defines a context-free grammar with production rules for assembling multi-agent systems from pre-defined components. The method uses forced sampling to ensure all components are evaluated, tracks component usage to maintain balanced coverage, and assembles MAS by stitching together verified code fragments rather than generating from scratch. The search operates at the corpus level, finding one good MAS for a dataset using a fixed validation set. The framework uses gpt-4o-mini as the backbone LLM and includes an LLM-based answer equivalence checker for validation.

## Key Results
- Grammar Search achieves 81.4% accuracy on MATH500 and 67.2% on MATH level 5
- Outperforms ADAS (77.5%) and AFlow baselines on MATH benchmarks
- Generates valid MAS code 100% of the time with ~12% lower search cost than ADAS
- Produces simpler, more interpretable code (4,391.5 chars vs 8,611.84 for ADAS)

## Why This Works (Mechanism)

### Mechanism 1: Syntax-Guarded Composability
- Claim: If the MAS search space is constrained by a context-free grammar (CFG), the system generates syntactically valid candidates 100% of the time, eliminating wasted compute on invalid code.
- Mechanism: The grammar defines production rules where terminals are typed components (e.g., SISO, MIMO). By only allowing derivations that satisfy input/output type constraints (e.g., a Multi-Input component must follow a Multi-Output component), the search cannot assemble structurally unsound pipelines.
- Core assumption: The space of valid MAS architectures can be adequately approximated by the specific grammar rules defined, and restricting "free-form" code does not prune optimal solutions.
- Evidence anchors:
  - [abstract] "constraining the MAS space with composable terminals... guarantees syntactic correctness and eliminates invalid candidates."
  - [section 4] "Sampling MASes from this grammar guarantees syntactic correctness and enables a structured search."
  - [corpus] "Alpha Discovery via Grammar-Guided Learning" notes that ignoring syntactic constraints leads to "exhaustive search over unstructured... spaces," supporting the efficiency of grammar-based constraints.
- Break condition: If the grammar is too restrictive and excludes novel node types or connection patterns required for a specific domain (e.g., recursive memory access), performance will cap or degrade compared to free-form methods.

### Mechanism 2: Coverage-Driven Forced Sampling
- Claim: If pure random sampling is used, components deep in the grammar tree may be under-represented; forcing balanced coverage improves discovery reliability.
- Mechanism: The algorithm tracks component sampling counts. It clusters components by frequency and force-samples sequences containing low-frequency components to ensure all "building blocks" are evaluated on the validation set before converging.
- Core assumption: Under-tested components possess utility that random sampling is likely to miss given finite search budgets.
- Evidence anchors:
  - [section 5] "To ensure that all components are fairly represented, we adopt a forced sampling strategy."
  - [table 4] Shows consistent accuracy gains (e.g., MATH 81.6 vs 80.8) using forced sampling over random sampling.
  - [corpus] Evidence for forced sampling specifically in MAS is weak; however, general search theory (referenced in Section 5 regarding Thompson Sampling/MCTS) supports exploration strategies over pure exploitation.
- Break condition: If the component library is large and mostly irrelevant, forcing coverage of low-utility components wastes evaluation budget on poor candidates, reducing overall efficiency.

### Mechanism 3: Modular Code Assembly
- Claim: Replacing LLM-based code generation with the assembly of pre-verified modular fragments significantly reduces search cost and code complexity.
- Mechanism: Instead of generating raw Python code (which requires LLM "optimizer" calls), the system stitches together stored code fragments corresponding to grammar terminals. This bypasses syntax errors and "code bloat" often seen in generative approaches.
- Core assumption: The performance gain from "creative" LLM-generated code structures is outweighed by the cost and error rate of generating them.
- Evidence anchors:
  - [abstract] "reduces search cost by ~12%, generates valid MASes 100% of the time, and produces simpler, more interpretable code."
  - [table 3] "Grammar Search incurs zero cost for MAS generation... Avg. code length 4,391.5 vs ADAS 8,611.84."
  - [corpus] "Neural-Guided Equation Discovery" supports the efficacy of modular systems (MGMT), though does not directly contrast with generative code.
- Break condition: If a task requires highly idiosyncratic logic not decomposable into the existing component set, the modular approach will fail to discover the necessary custom logic.

## Foundational Learning

- **Context-Free Grammars (CFGs):**
  - Why needed here: The core of the paper is defining a CFG to restrict the search space. You must understand "production rules" (how non-terminals expand) and "terminals" (the actual components) to read Figure 2.
  - Quick check question: Given the rule `<MI> → <MISO>`, can a `StepByStepReasoner` (which is Single-Input) be placed directly after the Start node if the derivation requires `<MI>`?

- **Data Flow Types (SISO vs. MIMO):**
  - Why needed here: The grammar is built on the input/output cardinality of components (Single/Multi). Understanding this type system is required to know why `MajorityVoter` (MISO) must follow a multi-output component like `StepByStepReasoner(cnt=5)`.
  - Quick check question: If `Component A` outputs a list of 5 answers and `Component B` requires a single string input, can they be directly composed under this architecture?

- **Corpus-Level vs. Query-Level Search:**
  - Why needed here: The paper explicitly optimizes for "corpus-level" search (finding one good MAS for a dataset). This distinction is vital for understanding why they use a fixed validation set rather than adapting the system per prompt.
  - Quick check question: Does the "Best MAS" found in this paper change depending on the specific input question provided by the user at test time?

## Architecture Onboarding

- **Component map:**
  - Grammar Definition: Python dictionaries/lists representing production rules (Fig 2)
  - Search Driver: `grammar_mas_search` function (Listing 1) implementing forced sampling
  - Evaluator: Executes generated MAS code on the validation set (e.g., 160 MATH problems)
  - Component Library: Pre-written Python functions (`StepByStepReasoner`, `DebateIteration`) implementing terminals

- **Critical path:**
  1. Define terminals (e.g., `StepByStepReasoner`) with correct `Info` namedtuples
  2. Implement the `expand_grammar` function to sample valid sequences
  3. Run the forced sampling loop to rank sequences by validation accuracy

- **Design tradeoffs:**
  - **Expressivity vs. Reliability:** You trade the infinite flexibility of LLM-generated code for the guarantee of syntactically valid, runnable code
  - **Simplicity vs. Optimality:** The resulting MASes are shorter (~4k chars) and cheaper, but may theoretically miss complex "hacky" solutions found by generative baselines

- **Failure signatures:**
  - **Stagnant Accuracy:** If the best validation accuracy plateaus early, the grammar likely lacks the necessary components (terminals) for the task
  - **Runtime Errors:** If a generated MAS fails to run, check the `forward` pass in the component library—specifically, ensure the `prev_answer` handling logic matches the input type (list vs. string)

- **First 3 experiments:**
  1. **Grammar Sanity Check:** Implement the grammar in Figure 2 and sample 10 random sequences; verify they are valid Python code using the provided `LLMAgentBase`
  2. **Component Ablation:** Run the search on MATH-500 with `DebateIteration` removed from the grammar to quantify its contribution (Table 5 suggests it is a key component)
  3. **Efficiency Benchmark:** Compare the API cost ($) and time of the Grammar Search (30 iterations) against a single run of ADAS (20 iterations) on the same validation set to replicate the ~12% savings claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of the forced sampling strategy degrade relative to sophisticated algorithms (e.g., Monte Carlo Tree Search) as the grammar size expands significantly?
- Basis in paper: [explicit] The authors note that while random search sufficed for the current grammar, "expanding the grammar with more components will require more sophisticated search algorithms."
- Why unresolved: The current component set is small enough that MCTS and Thompson sampling provided no performance gains over forced sampling.
- What evidence would resolve it: Benchmarking forced sampling against MCTS on an extended grammar containing an order of magnitude more terminals.

### Open Question 2
- Question: Can an automated method for discovering or suggesting new grammar components outperform the manual decomposition of base MASes used in this study?
- Basis in paper: [explicit] The authors acknowledge the method is "constrained by the grammar we developed" and that "best MASes... depend on the initial components."
- Why unresolved: The current work relies exclusively on manual decomposition of four existing MAS architectures (CoT, CoT-SC, Self-Refine, Debate) to populate the grammar.
- What evidence would resolve it: An experiment where an LLM proposes new component types during the search phase, compared against the static, manually defined set.

### Open Question 3
- Question: Does the rigidity of the context-free grammar impose a performance ceiling on tasks that require novel control flow or logic not expressible by the predefined input-output components?
- Basis in paper: [inferred] The authors admit the approach "lacks the free-form expressivity of LLM-based code generation," trading it for validity.
- Why unresolved: It is unclear if the high validity rate and lower cost come at the expense of missing complex, "lucky" solutions that free-form code generation (like ADAS) might occasionally stumble upon.
- What evidence would resolve it: Evaluation on a domain requiring complex, non-linear tool use or recursion, comparing the diversity of solution logic between grammar-based and free-form methods.

## Limitations

- **Grammar Expressivity Gap**: The CFG ensures syntactic validity but may exclude highly creative but valid MAS architectures that don't fit the predefined production rules.
- **Component Coverage Assumption**: Forced sampling assumes all components have non-zero utility, wasting budget if the component library contains many irrelevant modules.
- **Equivalence Checking Reliability**: The use of gpt-5 for answer validation is a black box; minor formatting differences could lead to false negatives, artificially deflating accuracy.

## Confidence

- **High Confidence**: Claims about syntactic validity (100% valid MAS generation) and cost reduction (~12%) are directly measurable from the reported data and code structure.
- **Medium Confidence**: Claims about outperformance versus baselines (e.g., MATH 81.4% vs ADAS 77.5%) are credible given the controlled evaluation, but depend on the exact equivalence-checking protocol which isn't fully specified.
- **Low Confidence**: Claims about the necessity of specific components (e.g., DebateIteration being "key") are based on ablation but don't account for potential synergies or alternative architectures that could compensate.

## Next Checks

1. **Grammar Ablation Study**: Systematically remove individual production rules (not just components) to measure impact on expressivity and performance across all benchmarks.
2. **Free-Form vs. Grammar Comparison**: Implement a minimal LLM-based code generator (without CFG constraints) and compare both validity rates and final accuracies on a held-out dataset.
3. **Equivalence Checker Audit**: Manually inspect 50 randomly selected LLM answers and their gpt-5 evaluations to quantify false negative rates and assess if the validation accuracy is overstated.