---
ver: rpa2
title: On the Paradoxical Interference between Instruction-Following and Task Solving
arxiv_id: '2601.22047'
source_url: https://arxiv.org/abs/2601.22047
tags:
- constraints
- constraint
- instruction
- task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SUSTAINSCORE, a metric to quantify how instruction-following\
  \ constraints interfere with core task-solving performance in LLMs. The key idea\
  \ is to measure task performance drops when self-evident constraints\u2014derived\
  \ from a model\u2019s own successful outputs\u2014are added to instructions."
---

# On the Paradoxical Interference between Instruction-Following and Task Solving

## Quick Facts
- **arXiv ID**: 2601.22047
- **Source URL**: https://arxiv.org/abs/2601.22047
- **Reference count**: 18
- **Primary result**: Introduces SUSTAINSCORE metric quantifying performance drop when self-evident constraints are added to LLM instructions

## Executive Summary
This paper reveals a fundamental tension in LLM capabilities: the act of following explicit instructions interferes with solving the underlying task. The authors introduce SUSTAINSCORE, a novel metric that measures how much performance degrades when self-evident constraints (derived from a model's own successful outputs) are added to instructions. Experiments across mathematics, multi-hop QA, and code generation show significant degradation, with even advanced models like Claude-Sonnet-4.5 retaining only ~85% performance under constraints. The study finds that code generation tasks are most affected due to structural fragility, and that failed cases tend to allocate significantly more attention to constraints. Analysis suggests RL-based post-training may enhance robustness, while SFT on long CoT data increases susceptibility to degradation.

## Method Summary
The authors measure task performance drops when self-evident constraints—derived from a model's own successful outputs—are added to instructions. They first run models on task benchmarks (GSM8K, HotpotQA, HumanEval) to identify successful cases, then use an LLM (Qwen3-32B) to extract constraints from those successful outputs via back-translation. These constraints are categorized as hard (length, keyword) or soft (method, style, structure) and appended to original instructions. The SUSTAINSCORE metric is calculated as the ratio of successful constrained runs to total successful unconstrained runs. Attention analysis is performed on failed cases to investigate mechanisms, and different post-training paradigms (RL vs SFT-LongCoT) are compared for robustness.

## Key Results
- Advanced models like Claude-Sonnet-4.5 retain only ~85% performance under self-evident constraints
- Code generation tasks show the most severe degradation (SUSTAINSCORE < 50% for some models)
- Failed generations exhibit significantly higher constraint attention scores than successful ones
- RL-based models show better robustness than SFT-LongCoT models
- Performance drops are domain-dependent, with structural tasks most affected

## Why This Works (Mechanism)

### Mechanism 1: Attention Displacement Hypothesis
Excessive allocation of attention to explicit constraint tokens may disrupt the model's reasoning pathway. The paper observes that failed generations exhibit higher constraint attention scores compared to successful ones, particularly in later decoding steps, suggesting competition for limited attention capacity.

### Mechanism 2: Alignment Training Susceptibility
The specific post-training paradigm (RL vs. SFT) significantly alters robustness to constraints. RL-based models maintain higher SUSTAINSCOREs than SFT-LongCoT models, implying RL better integrates constraints into the policy while SFT may cause "reasoning rigidity."

### Mechanism 3: Structural Fragility in Code
Code generation suffers most severe performance degradation because constraints interfere with global logic and execution flow. Unlike natural language, code requires valid syntax and logic, so constraints can break the global dependency graph.

## Foundational Learning

- **Concept: Self-Evident Constraints**
  - Why needed: To ensure performance drops are caused by constraint presence rather than task difficulty
  - Quick check: If a model fails with a constraint it naturally followed before, is the constraint "self-evident"?

- **Concept: Constraint Back-Translation**
  - Why needed: Automated methodology to generate test cases by reversing the process from successful outputs
  - Quick check: Why use a separate LLM to generate constraints rather than copying the original prompt?

- **Concept: Reasoning Error vs. Output Specification Error**
  - Why needed: Distinguishing failure modes is crucial for debugging
  - Quick check: A model correctly calculates but omits a required keyword. Is this a Reasoning Error or Output Specification Error?

## Architecture Onboarding

- **Component map:** Input Sampler -> Constraint Generator (Back-Translation) -> Evaluator
- **Critical path:** The Constraint Back-Translation step. If generated constraint is logically impossible or redundant, the metric loses validity.
- **Design tradeoffs:** Uses LLM to automate soft constraint extraction vs. rule-based scripts for hard constraints, trading scalability for potential hallucination.
- **Failure signatures:**
  1. Reasoning Displacement: Model enters "meta-commentary" loop about constraints and loses mathematical thread
  2. Syntax Breakage (Code): Code includes required keyword but introduces syntax error preventing execution
- **First 3 experiments:**
  1. Baseline Verification: Run GSM8K without constraints to establish successful cases
  2. Constraint Injection: Apply single "self-evident" constraint to successful set and measure drop
  3. Attention Analysis: Visualize attention map of failed generation to verify constraint attention clustering

## Open Questions the Paper Calls Out

1. Does the paradoxical interference effect persist across non-reasoning tasks such as creative writing, summarization, or agentic planning?
2. To what extent do linguistic features influence the interference effect in languages other than English?
3. What specific mechanisms allow Reinforcement Learning (RL) to enhance robustness while Supervised Fine-Tuning (SFT) on long Chain-of-Thought (CoT) data increases susceptibility?

## Limitations
- Constraint generation reliability introduces potential hallucination or mismatch
- Attention score interpretation assumes attention is zero-sum resource
- Generalization across domains may be limited by execution-based evaluation bias

## Confidence

**High Confidence**: SUSTAINSCORE metric construction and basic observation that instruction-following constraints cause performance degradation

**Medium Confidence**: Mechanistic claims about attention displacement and differential robustness between RL vs SFT-LongCoT

**Low Confidence**: Specific interpretation that code generation's structural fragility uniquely causes severe degradation

## Next Checks

1. Manually validate 100 randomly sampled constraint-output pairs across all three domains to measure actual accuracy of back-translation process

2. For failed cases, ablate constraint attention component and re-run generation to test causal relationship

3. Replicate analysis using domain where both reasoning and execution can be precisely measured to compare degradation patterns