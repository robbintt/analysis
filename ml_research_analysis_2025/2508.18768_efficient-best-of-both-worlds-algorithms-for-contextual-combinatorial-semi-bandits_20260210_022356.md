---
ver: rpa2
title: Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits
arxiv_id: '2508.18768'
source_url: https://arxiv.org/abs/2508.18768
tags:
- regret
- algorithm
- lemma
- combinatorial
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first best-of-both-worlds algorithm for\
  \ contextual combinatorial semi-bandits that achieves O(\u221AT) regret in the adversarial\
  \ regime and O(ln T) regret in the corrupted stochastic regime. The core method\
  \ builds on Follow-the-Regularized-Leader (FTRL) with a Shannon entropy regularizer,\
  \ combined with an efficient projection subroutine."
---

# Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits
## Quick Facts
- arXiv ID: 2508.18768
- Source URL: https://arxiv.org/abs/2508.18768
- Authors: Mengmeng Li; Philipp Schneider; Jelisaveta Aleksić; Daniel Kuhn
- Reference count: 40
- Presents the first best-of-both-worlds algorithm for contextual combinatorial semi-bandits with O(√T) adversarial regret and O(ln T) stochastic regret

## Executive Summary
This paper introduces the first best-of-both-worlds algorithm for contextual combinatorial semi-bandits that simultaneously achieves optimal regret bounds in both adversarial and corrupted stochastic regimes. The algorithm combines Follow-the-Regularized-Leader (FTRL) with Shannon entropy regularization and an efficient projection subroutine based on Karush-Kuhn-Tucker (KKT) conditions. By transforming the high-dimensional K-dimensional convex projection into a single-variable root-finding problem, the method dramatically accelerates per-round computation, making it suitable for large-scale, real-time applications.

## Method Summary
The algorithm builds on Follow-the-Regularized-Leader (FTRL) with Shannon entropy regularization, enhanced by an efficient projection subroutine. The key innovation is using Karush-Kuhn-Tucker (KKT) conditions to transform the K-dimensional convex projection into a single-variable root-finding problem. This optimization dramatically reduces computational complexity per round while maintaining theoretical regret guarantees. The approach achieves O(√m ln(K/m)T) regret in the adversarial regime and O(κ²m^(3/2) ln((K-m)T)/∆_min) regret in the stochastic regime, where κ = √(Kd ln T + K(ln T)²/λ_min(Σ)).

## Key Results
- First algorithm achieving best-of-both-worlds regret bounds for contextual combinatorial semi-bandits
- Dramatically accelerates per-round computation through KKT-based projection optimization
- Empirical evaluations show substantial speed-ups while maintaining attractive regret bounds
- Regret bounds: O(√m ln(K/m)T) adversarial, O(κ²m^(3/2) ln((K-m)T)/∆_min) stochastic

## Why This Works (Mechanism)
The algorithm leverages FTRL with Shannon entropy regularization to balance exploration and exploitation while maintaining stability against adversarial perturbations. The KKT-based projection transforms a computationally expensive high-dimensional optimization into an efficient single-variable root-finding problem. This reduction exploits the structure of combinatorial constraints to enable fast updates while preserving the theoretical properties needed for both adversarial and stochastic regret guarantees. The entropy regularization provides the flexibility to adapt to both regimes without prior knowledge of the environment.

## Foundational Learning
- Follow-the-Regularized-Leader (FTRL): Needed for achieving optimal regret bounds in both regimes; Quick check: Verify that the cumulative loss minimization with entropy regularization satisfies the required regret bounds
- Karush-Kuhn-Tucker (KKT) conditions: Essential for transforming high-dimensional projection into efficient root-finding; Quick check: Confirm that KKT conditions correctly characterize the optimal solution for the projection problem
- Shannon entropy regularization: Provides the mathematical structure enabling best-of-both-worlds performance; Quick check: Ensure entropy regularization maintains the required smoothness and boundedness properties
- Combinatorial semi-bandit feedback: Required for understanding the partial information setting; Quick check: Verify that the algorithm correctly handles the combinatorial action space and semi-bandit feedback structure
- Corruption models: Critical for analyzing performance in non-stochastic regimes; Quick check: Confirm that the algorithm's guarantees hold under the specified corruption assumptions

## Architecture Onboarding
- Component map: FTRL update -> KKT projection -> Action selection -> Feedback processing
- Critical path: Each round consists of (1) loss estimation, (2) FTRL update with entropy regularization, (3) KKT-based projection, (4) action selection, (5) feedback processing
- Design tradeoffs: Computational efficiency (via KKT reduction) vs. generality of constraint handling; Theoretical guarantees vs. practical implementation complexity
- Failure signatures: Numerical instability in KKT root-finding, constraint violations in projection, poor exploration-exploitation balance
- First experiments: (1) Verify regret bounds on synthetic combinatorial problems with known optimal solutions, (2) Benchmark runtime against standard FTRL implementations, (3) Test robustness under varying corruption levels in semi-synthetic environments

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the main contribution.

## Limitations
- Theoretical tightness of bounds under specific corruption models remains uncertain, as analysis focuses on general regimes without intermediate case details
- KKT-based projection efficiency may not extend to all combinatorial constraints beyond those explicitly covered
- Empirical validation is limited to few benchmark problems, leaving scalability questions in highly dynamic or non-stationary environments

## Confidence
- High confidence in core algorithmic framework and theoretical regret guarantees, given rigorous mathematical formulation and alignment with established FTRL methods
- Medium confidence in practical efficiency claims, as demonstrated speedup lacks extensive benchmarking across diverse problem sizes or real-world datasets
- Low confidence in robustness to edge cases or extreme adversarial conditions, due to limited stress-testing in evaluation

## Next Checks
1. Conduct extensive experiments comparing runtime and regret on large-scale combinatorial problems with thousands of arms and actions
2. Test the algorithm under varying corruption levels and non-stationary reward distributions to assess adaptability
3. Analyze the sensitivity of the KKT-based projection subroutine to numerical precision and constraint violations in high-dimensional settings