---
ver: rpa2
title: 'Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect
  Translations'
arxiv_id: '2510.09994'
source_url: https://arxiv.org/abs/2510.09994
tags:
- translation
- participants
- spanish
- proficiency
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human study (n=452) conducted in a public
  museum to understand how fluency and adequacy errors in machine translation (MT)
  affect bilingual and non-bilingual users' reliance on MT during casual use. The
  study found that non-bilingual users often over-rely on MT not because they assume
  outputs are correct, but due to a lack of evaluation strategies and alternatives.
---

# Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations

## Quick Facts
- arXiv ID: 2510.09994
- Source URL: https://arxiv.org/abs/2510.09994
- Reference count: 26
- Key outcome: Non-bilingual users over-rely on MT due to lack of evaluation strategies, not high confidence in correctness; experiencing error impact in low-stakes settings prompts trust recalibration

## Executive Summary
This paper presents a large-scale human study (n=452) conducted in a public museum to understand how fluency and adequacy errors in machine translation affect users' reliance on MT during casual use. The study found that non-bilingual users often over-rely on MT not because they assume outputs are correct, but due to a lack of evaluation strategies and alternatives. Experiencing the impact of errors in low-stakes settings prompted users to reassess future reliance on MT. The results highlight the need for MT evaluation and NLP explanation techniques to promote MT literacy among users.

## Method Summary
The study used a mixed 2x3 experimental design with 452 participants completing a 4-trial navigation task in a museum setting. Participants selected images matching Spanish descriptions provided via English MT outputs. Translations were generated using LLAMA-3 8B, ChatGPT, and GPT-4, then perturbed to create controlled fluency and adequacy errors. Within-subject manipulation varied MT correctness (correct/incorrect), while between-subject manipulation varied error type (fluency without impact, adequacy without impact, adequacy with impact). Analysis employed Cumulative Link Mixed Models (CLMM) for ordinal data and Generalized Linear Mixed Models (GLMM) for binary outcomes, with fixed effects for error type and Spanish proficiency, and random effects for participant and stimulus IDs.

## Key Results
- Non-bilingual users over-rely on MT due to lack of evaluation strategies, not because they assume outputs are correct
- Fluency errors were undetected by non-bilingual users but caught by bilingual users
- Experiencing the impact of errors in low-stakes settings prompted users to reassess future reliance on MT

## Why This Works (Mechanism)

### Mechanism 1: The Default Reliance Trap
- **Claim:** Non-bilingual users rely on imperfect MT outputs not due to high confidence in correctness, but because they lack alternative strategies for verification
- **Mechanism:** When a user cannot verify source text and has no established heuristic for evaluating output, the path of least resistance is to accept the system's output as the "default truth"
- **Evidence anchors:** Non-bilingual users over-rely "due to a lack of evaluation strategies and alternatives" (abstract); "because they do not know what else to do" (section 5)
- **Break condition:** Users provided with accessible verification methods (e.g., back-translation) reduce reliance regardless of language proficiency

### Mechanism 2: Impact-Driven Trust Calibration
- **Claim:** Experiencing the direct consequence of an error in a low-stakes environment prompts users to recalibrate their future willingness to use the system more effectively than simply observing the error
- **Mechanism:** Affective feedback from a "wrong decision" (losing points or realizing a mistake) serves as a stronger signal for trust adjustment than simply observing a linguistic anomaly
- **Evidence anchors:** "Experiencing the impact of errors in low-stakes settings prompted users to reassess future reliance" (abstract); willingness to reuse was significantly lower when participants encountered impactful errors (section 4.2)
- **Break condition:** If the user attributes the error to the specific context rather than the system capability, calibration fails to generalize

### Mechanism 3: Proficiency-Gated Error Detection
- **Claim:** Source language proficiency gates the ability to detect semantic errors (adequacy), and counter-intuitively, low proficiency creates a blindness to target-side structural errors (fluency) that monolinguals would normally catch
- **Mechanism:** Users with "some" proficiency attempt to cross-reference source and target, but lacking full command, they struggle to resolve conflicts. Users with "no" proficiency focus entirely on the target text but seemingly lower their critical standards
- **Evidence anchors:** "Participants with No Spanish Proficiency were not able to perceive any MT errors, even the fluency errors" (section 4.1); high proficiency participants successfully detected errors regardless of type
- **Break condition:** If the fluency error renders the target text semantically impossible rather than just ungrammatical, detection by non-bilinguals may succeed

## Foundational Learning

- **Concept:** **Fluency vs. Adequacy**
  - **Why needed here:** The study manipulates these two dimensions to distinct effects (Fluency affects perception/trust; Adequacy affects decision accuracy)
  - **Quick check question:** *If a translation reads smoothly in English but says "buy fire" instead of "buy flowers," is this a failure of fluency or adequacy?* (Answer: Adequacy)

- **Concept:** **Strategy Vacuum**
  - **Why needed here:** The paper identifies the lack of evaluation strategies as the primary driver of over-reliance
  - **Quick check question:** *What is the primary alternative strategy for a monolingual user evaluating a translation?* (Answer: None definitive, often back-translation or intuition)

- **Concept:** **Low-Stakes Calibration**
  - **Why needed here:** The study suggests that "safe failure" is a feature, not a bug, for training user intuition
  - **Quick check question:** *Why might a system designer intentionally allow users to fail on easy tasks?* (Answer: To trigger trust recalibration mechanisms before high-stakes use)

## Architecture Onboarding

- **Component map:** Stimuli Generator -> Decision Interface -> Feedback Module -> User Model
- **Critical path:** User Authentication -> Proficiency Assessment -> Task Assignment -> User Decision -> **Feedback Delivery (The Critical Intervention)** -> Re-evaluation of Willingness to Reuse
- **Design tradeoffs:**
  - Control vs. Realism: Synthetic errors ensure specific mechanism testing but may lack nuance of real LLM hallucinations
  - Literacy vs. Friction: Forcing users to adopt "evaluation strategies" improves accuracy but increases task time and cognitive load
  - Error Hiding vs. Signaling: Hiding errors improves perceived usability; signaling errors improves literacy and calibration
- **Failure signatures:**
  - Blind Compliance: Non-bilingual users consistently accept outputs despite fluency errors
  - Trust Collapse: Willingness to reuse drops to zero after a single high-impact error
  - Strategy Gaming: Users find ways to guess correct images without reading text
- **First 3 experiments:**
  1. A/B Test Feedback Modalities: Compare "Correct/Incorrect" feedback against "Explanatory Feedback" to measure retention of MT literacy
  2. Proficiency Boundary Test: Run the study with users of "intermediate" proficiency to identify the specific threshold where detection of adequacy errors improves
  3. Longitudinal Retention: Re-survey participants 1 week post-study to see if "reassessment of reliance" persists

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does question-answering based feedback improve decision-making accuracy for non-bilingual users compared to intuitive strategies?
- **Basis in paper:** The authors state that while question-answering helps users decide if a translation is safe to share, "it remains unclear whether such feedback also supports users in making more accurate inferences when answering content-specific questions."
- **Why unresolved:** The study identified that non-bilingual users lack evaluation strategies but did not test the efficacy of interventions like QA-based cognitive forcing functions
- **What evidence would resolve it:** A comparative user study measuring decision accuracy between control and QA-enhanced groups

### Open Question 2
- **Question:** Does exposure to MT errors in low-stakes simulations effectively calibrate user trust for future high-stakes scenarios?
- **Basis in paper:** The conclusion highlights the need for "understanding how trust formation in casual settings common to everyday users impact their behaviors in high-stakes use cases."
- **Why unresolved:** The current study used a fictional, low-stakes museum task; it could not measure if this "literacy" transfers to critical real-world contexts like healthcare or legal situations
- **What evidence would resolve it:** A longitudinal study tracking participants' reliance behaviors in real-world, high-stakes environments after undergoing similar low-stakes error-exposure training

### Open Question 3
- **Question:** How do user reliance strategies and trust calibration evolve during extended interactions with MT systems?
- **Basis in paper:** The limitations section notes that the "short interaction duration limits the ability to examine trust development over time"
- **Why unresolved:** The experiment consisted of only four discrete trials, providing a snapshot of immediate reaction to errors rather than the long-term dynamics of trust repair or habituation
- **What evidence would resolve it:** A longitudinal diary study or extended laboratory session involving continuous MT usage over days or weeks

## Limitations

- **Synthetic stimuli:** The study used controlled errors (misspellings, deletions, round-trip translation) that may not reflect real-world LLM hallucinations
- **Short interaction duration:** The four-trial design limits examination of trust development over time and long-term retention of MT literacy gains
- **Context-specific calibration:** Users' recalibration of trust may not transfer from museum navigation tasks to other domains (medical, legal, etc.)

## Confidence

- **High Confidence:** Non-bilingual users lack systematic evaluation strategies; fluency errors are undetected by non-bilinguals; error impact drives trust recalibration
- **Medium Confidence:** The proficiency threshold for detecting adequacy errors; the relative contribution of lack of alternatives vs. evaluation knowledge to over-reliance
- **Low Confidence:** Whether museum task calibration generalizes to high-stakes domains; long-term retention of MT literacy gains

## Next Checks

1. **Cross-Domain Replication:** Test whether error-impact calibration in museum tasks transfers to medical or legal MT use cases with different consequence structures
2. **Longitudinal Follow-up:** Re-survey participants 1-4 weeks post-study to measure persistence of altered MT reliance patterns
3. **Strategy Intervention Test:** Compare three conditions (control, evaluation strategy training, verification tool provision) to isolate which component reduces over-reliance most effectively