---
ver: rpa2
title: 'IG-Pruning: Input-Guided Block Pruning for Large Language Models'
arxiv_id: '2511.02213'
source_url: https://arxiv.org/abs/2511.02213
tags:
- pruning
- mask
- sparsity
- language
- ig-pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IG-Pruning, a novel input-aware block-wise
  pruning method that dynamically selects layer masks at inference time. The method
  discovers diverse mask candidates through semantic clustering and L0 optimization,
  then implements efficient dynamic pruning without additional training.
---

# IG-Pruning: Input-Guided Block Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2511.02213
- Source URL: https://arxiv.org/abs/2511.02213
- Reference count: 23
- Primary result: Dynamic input-aware block pruning that preserves 87.18% of Llama-3-8B performance at 25% sparsity, outperforming static methods by 10.86 percentage points.

## Executive Summary
This paper introduces IG-Pruning, a novel method for efficient large language model inference through dynamic block-level pruning. Unlike static depth pruning that uses a single mask for all inputs, IG-Pruning routes each input to a specialized pruning mask based on semantic similarity clustering. The method discovers diverse mask candidates through K-means clustering of input embeddings and trains separate masks using L0 regularization, while keeping model weights frozen. This approach consistently outperforms state-of-the-art static depth pruning methods across different sparsity levels and model architectures, demonstrating significant improvements in preserving model performance during aggressive compression.

## Method Summary
IG-Pruning operates in two stages: first, it clusters calibration data into semantic groups using an external encoder, then trains specialized block masks for each cluster using Hard Concrete distribution and L0 regularization. During inference, inputs are routed to the nearest cluster centroid and the corresponding mask is applied. The method treats Attention and FFN blocks as independent pruning units, computing KV projections for Attention even when skipped to maintain cache integrity. Only mask parameters are trained while model weights remain frozen, enabling rapid adaptation with minimal computational overhead.

## Key Results
- Llama-3-8B at 25% sparsity: preserves 87.18% of dense model performance, outperforming best baseline by 10.86 percentage points
- Qwen-3-8B at 13.9% sparsity: maintains 96.01% of dense model performance compared to 90.37% for best baseline
- Dynamic routing consistently outperforms static pruning methods across all tested sparsity levels and model architectures
- Block-level pruning (separating Attention and FFN) provides better efficiency than layer-level pruning

## Why This Works (Mechanism)

### Mechanism 1: Input-Conditioned Specialization
The method addresses the "one-size-fits-all" limitation of static pruning by aligning computational paths with input complexity. It encodes calibration data into embeddings and clusters them using K-means, then trains distinct binary masks for each cluster. At inference, a router maps the input to the nearest cluster centroid to retrieve the specific mask. This works because semantic similarity in the embedding space correlates with optimal layer execution paths for the LLM.

### Mechanism 2: Hard Concrete Relaxation for Differentiable Sparsity
The method models layer masks with a hard concrete distribution, allowing gradient-based optimization of discrete pruning decisions without modifying model weights. The distribution outputs continuous values in [0, 1] during training (soft masks) and samples {0, 1} during inference. A Lagrangian term enforces target sparsity while minimizing the language modeling loss, enabling learned masking over static heuristics.

### Mechanism 3: Asymmetric Block Granularity
By treating Attention and FFN blocks as independent pruning units, the method preserves performance better than pruning entire transformer layers. It skips FFNs entirely but computes Key/Value projections for Attention even when "skipped" to maintain KV cache integrity, bypassing only the compute-heavy scaled dot-product. This asymmetric handling recognizes that redundancy is not uniformly distributed across residual blocks.

## Foundational Learning

### Concept: Hard Concrete Distribution / L0 Regularization
- **Why needed:** This mathematical engine allows learning which layers to drop without intractable discrete search
- **Quick check:** How does the "stretching" interval [l, r] (where l < 0, r > 1) allow the model to push mask probabilities to exact 0 or 1?

### Concept: Residual Connections and Depth Pruning
- **Why needed:** The paper relies on the observation that adjacent layers in LLMs often have similar embeddings due to residuals, making depth pruning viable
- **Quick check:** Why does pruning a layer in a residual network effectively reduce to an identity function or scaling, rather than breaking the gradient flow entirely?

### Concept: KV Cache Management in Autoregression
- **Why needed:** Critical for understanding the asymmetric handling; you cannot simply "skip" an attention layer in a decoder-only model without accounting for the rolling cache required for future tokens
- **Quick check:** Why must the Key and Value projections be computed even when the attention block is "skipped" during inference?

## Architecture Onboarding

- **Component map:** all-MiniLM-L6-v2 (frozen) -> K-Means clustering -> Mask Pool (N masks) -> Router (Euclidean distance) -> LLM Executor (Llama/Qwen with conditional execution)
- **Critical path:** The routing decision (distance calculation) happens before the first LLM layer. If this routing latency > time saved by skipping blocks, the method fails.
- **Design tradeoffs:**
  - Cluster Count (N): Higher N (e.g., 16) increases mask specialization but requires managing more mask parameters and routing compute
  - Calibration Data: Method is sensitive to the domain of calibration data (fineweb-edu vs wikitext2)
- **Failure signatures:**
  - Routing Collapse: If the encoder fails to distinguish inputs, all inputs route to a single mask
  - Factual Degradation: Aggressive pruning may remove "fact storage" layers, increasing hallucination
  - Hardware Inefficiency: Conditional logic for mixed block skipping may negate latency gains
- **First 3 experiments:**
  1. Cluster Ablation: Run inference with N=1 (static equivalent) vs N=16 on diverse dataset to isolate dynamic routing gains
  2. Granularity Check: Compare "Block-level" vs "Layer-level" pruning at 25% sparsity to verify asymmetric efficiency
  3. Routing Overhead Profiling: Measure wall-clock time for routing step vs actual LLM inference time

## Open Questions the Paper Calls Out
- Does block pruning degrade the factual reliability of LLMs, and can this be mitigated? The authors note they "do not investigate the impact of block pruning on model factuality," highlighting that removing blocks risks eliminating critical components for factual recall.
- How does IG-Pruning perform on domain-specific tasks where semantic similarity may not correlate with computational needs? The authors acknowledge that generalization to "domain-specific applications [is] less thoroughly validated" and that performance is sensitive to calibration data domain.
- To what extent does the choice of the external sentence encoder impact the optimality of the dynamic routing? The method relies on an external encoder (all-MiniLM-L6-v2) to cluster inputs, and the authors state that performance "heavily depends on clustering quality."

## Limitations
- Performance is highly sensitive to the domain of calibration data, potentially limiting generalization beyond educational reasoning tasks
- The asymmetric block pruning assumes hardware can efficiently execute conditional logic for partial attention computations
- The method's sensitivity to hyperparameters like Lagrangian penalty weights and learning rates remains unclear

## Confidence

**High Confidence:**
- Input-aware dynamic pruning consistently outperforms static pruning methods
- Asymmetric block-level pruning provides efficiency gains
- Method successfully maintains model weights frozen while optimizing only mask parameters

**Medium Confidence:**
- Claims of "10.86 percentage points" improvement need verification with statistical testing
- Routing overhead claims lack detailed profiling data
- Performance gains may be specific to chosen evaluation datasets

**Low Confidence:**
- Claims about "rapid adaptation with minimal computational overhead" are not validated across diverse architectures
- Method's robustness to domain shifts in calibration data is largely untested
- Hardware efficiency claims are theoretical rather than empirically validated

## Next Checks

1. **Domain Transfer Validation:** Train IG-Pruning using calibration data from one domain (e.g., educational data) and evaluate on a completely different domain (e.g., medical or legal texts) to reveal whether semantic clustering and mask specialization generalize beyond the training distribution.

2. **Hardware-Realistic Profiling:** Implement asymmetric block pruning on actual GPU/TPU hardware and measure real latency improvements, including routing decision time and conditional execution overhead, to verify claimed efficiency gains translate to wall-clock time improvements.

3. **Statistical Significance Testing:** Conduct proper statistical tests (e.g., paired t-tests) comparing IG-Pruning against baseline methods across multiple random seeds and dataset splits to establish whether the reported percentage improvements are statistically significant.