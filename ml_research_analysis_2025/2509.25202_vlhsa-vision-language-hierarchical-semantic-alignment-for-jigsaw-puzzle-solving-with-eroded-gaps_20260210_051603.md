---
ver: rpa2
title: 'VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving
  with Eroded Gaps'
arxiv_id: '2509.25202'
source_url: https://arxiv.org/abs/2509.25202
tags:
- visual
- global
- puzzle
- semantic
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving jigsaw puzzles with
  eroded gaps, where traditional visual-only methods struggle due to insufficient
  edge and visual coherence information. The proposed VLHSA framework introduces hierarchical
  vision-language semantic alignment, combining Vision Mamba and BLIP for dual visual
  encoding with CLIP-based text encoding.
---

# VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps

## Quick Facts
- arXiv ID: 2509.25202
- Source URL: https://arxiv.org/abs/2509.25202
- Reference count: 16
- Key outcome: Achieves 66.9% piece accuracy and 19.0% perfect accuracy on 5×5 eroded jigsaw puzzles, outperforming vision-only methods by 14.2 percentage points

## Executive Summary
This paper addresses the challenge of solving jigsaw puzzles with eroded gaps, where traditional visual-only methods struggle due to insufficient edge and visual coherence information. The proposed VLHSA framework introduces hierarchical vision-language semantic alignment, combining Vision Mamba and BLIP for dual visual encoding with CLIP-based text encoding. The core innovation lies in the Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns visual patches with textual descriptions across token, region, and global semantic levels. VLHSA achieves state-of-the-art performance on both JPwLEG-3 and JPwLEG-5 datasets, with a 14.2 percentage point improvement in piece accuracy (66.9% vs 52.7%) over the previous best vision-only approach.

## Method Summary
VLHSA employs a dual-encoder architecture with Vision Mamba and BLIP for complementary visual feature extraction, paired with CLIP for text encoding. The framework uses hierarchical semantic alignment at three levels: token (individual patch-word), region (spatial window–phrase), and global (image–caption). Visual patches are aligned with textual descriptions through multi-head cross-attention, with learned softmax weights combining the three alignment levels. A residual adapter bridges the dual visual encoders, and predictions are made via an MLP head outputting N×N logits solved using the Hungarian algorithm. The model is trained with cross-entropy loss on assignments plus alignment losses at each level, achieving efficient training (1.68h on RTX 3090) with 27.1M parameters.

## Key Results
- Achieves 66.9% piece accuracy on JPwLEG-5 (5×5 puzzles), outperforming the previous best vision-only method (52.7%) by 14.2 percentage points
- Reaches 19.0% perfect accuracy on 5×5 puzzles, demonstrating strong reconstruction capability under erosion
- Shows consistent improvements across both JPwLEG-3 and JPwLEG-5 datasets, with ablation confirming the importance of hierarchical alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical semantic alignment at multiple granularities improves puzzle reconstruction when visual edge cues are degraded.
- Mechanism: The VLHSA module establishes correspondences between visual patches and textual descriptions at three levels—token (individual patch-word), region (spatial window–phrase), and global (image–caption)—then fuses them with learnable weights via softmax normalization.
- Core assumption: Captions provide disambiguating constraints for visually similar fragments; global semantic context is more informative than local token alignment when gaps erode edge continuity.
- Evidence anchors:
  - [abstract] "Our approach centers on the Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns visual patches with textual descriptions through multi-level semantic matching from local tokens to global context."
  - [section] Ablation Table 7 shows global-only alignment yields 17.50% Perfect Accuracy; all three levels together reach 19.00% Perfect and 66.90% Piece Accuracy.
  - [corpus] Related work (PuzzleBench, Agentic Jigsaw Interaction Learning) confirms puzzle solving stresses perception and reasoning, but does not directly validate hierarchical alignment in gap puzzles.
- Break condition: If captions are absent, noisy, or semantically uninformative (e.g., generic descriptions), the cross-modal alignment provides weak or misleading constraints, reducing gains.

### Mechanism 2
- Claim: Combining Vision Mamba's long-range spatial modeling with BLIP's vision-language aligned features improves piece placement under erosion.
- Mechanism: Vision Mamba encodes sequential spatial dependencies across patches; BLIP adds cross-modally aligned semantics. Features are projected to a shared dimension and integrated via residual connection with a learned adapter, preserving spatial structure while enriching semantics.
- Core assumption: Eroded gaps disrupt local cues but long-range spatial relations and semantic grounding remain informative; both are necessary and complementary.
- Evidence anchors:
  - [section] "A dual-encoder architecture that leverages the complementary strengths of Vision Mamba and BLIP vision encoder is employed."
  - [section] Ablation Table 6 shows Vision Mamba alone yields 3.55% Perfect; adding BLIP improves to 5.95%; adding CLIP further improves to 8.85%.
  - [corpus] No direct corpus validation for Mamba+BLIP synergy in puzzle solving; evidence is paper-internal.
- Break condition: If patch size or positional structure is inconsistent with Mamba's sequence assumptions, long-range dependency modeling may weaken; if BLIP pretraining domain diverges significantly from artwork imagery, alignment quality may drop.

### Mechanism 3
- Claim: Adaptive gating at the global level better integrates image–caption semantics than static fusion.
- Mechanism: A gating network computes element-wise weights from concatenated global visual and expanded textual features; the gated sum modulates how much textual context influences the fused representation (Equations 12–13).
- Core assumption: The relative importance of textual guidance varies across images and regions; a learned gate can adjust fusion dynamically.
- Evidence anchors:
  - [section] "Global alignment employs a gating mechanism to fuse visual and textual information adaptively."
  - [section] Ablation indicates global alignment contributes the largest single gain among the three levels.
  - [corpus] HiMo-CLIP explores hierarchy in vision-language alignment, but does not validate gating specifically for puzzle solving.
- Break condition: If textual features are low-quality or misaligned (e.g., caption describes absent elements), gating may suppress or inappropriately incorporate text, reducing alignment utility.

## Foundational Learning

- Concept: Cross-attention and multimodal alignment (CLIP/BLIP style)
  - Why needed here: VLHSA uses multi-head cross-attention to align patches with tokens and regions with phrases; understanding query/key/value roles and contrastive alignment is essential.
  - Quick check question: Given a set of patch features and token embeddings, write the attention equation and explain what the attention matrix represents.

- Concept: State space models for sequences (Mamba basics)
  - Why needed here: Vision Mamba encodes patches with selective state space mechanisms for long-range dependencies; interpreting its selectivity and scanning behavior helps debug spatial modeling.
  - Quick check question: In one sentence, how does Mamba's selective state space differ from standard attention for long sequences?

- Concept: Bipartite matching and the Hungarian algorithm
  - Why needed here: Puzzle solving requires a bijective piece-to-position mapping; the Hungarian algorithm converts model logits into valid assignments during training and inference.
  - Quick check question: Why can't argmax per row suffice for assignment, and how does the Hungarian algorithm enforce a permutation?

## Architecture Onboarding

- Component map: Scrambled puzzle patches → Vision Mamba (24 layers, 256-dim) + BLIP vision encoder → CLIP text encoder (global + token embeddings) → VLHSA alignment module (token, region, global levels) → Learned softmax fusion → MLP prediction head → Hungarian algorithm → Final assignment

- Critical path:
  1. Verify patch extraction matches dataset sizes (96×96; 3×3 or 5×5 grids).
  2. Run each encoder separately and confirm output shapes: Vision Mamba (N×d_v), BLIP (N×d_b), CLIP global (d_t) and tokens (L×d_t).
  3. Step through one VLHSA level to confirm attention shapes and gating dimensions.

- Design tradeoffs:
  - Three-level alignment improves performance but adds parameters and loss terms; global level dominates gains, so initial experiments might prioritize it.
  - Dual encoders increase representational richness but require projection and careful initialization to avoid conflicting gradients.
  - Manual caption editing improves quality but does not scale; consider robustness tests with raw BLIP captions.

- Failure signatures:
  - Many puzzles with 6+ errors (55.15% in Table 2) often involve highly similar pieces; this signals ambiguity limits rather than alignment failure.
  - Large performance drop from 3×3 to 5×5 (Table 9) indicates scaling challenges; check whether patch features remain discriminative.
  - If global-only outperforms full hierarchy, token/region losses may be misweighted or captions may lack fine-grained detail.

- First 3 experiments:
  1. Reproduce the global-only ablation on JPwLEG-5 to validate the dominant contribution and establish a stable baseline.
  2. Compare raw BLIP captions vs. manually edited captions to quantify sensitivity to caption quality.
  3. Visualize attention maps for token- and region-level alignment on failure cases to assess whether captions attend to correct patches or collapse to generic terms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VLHSA perform when textual descriptions are generated fully automatically without manual editing, and what is the relationship between caption quality and reconstruction accuracy?
- Basis in paper: [explicit] The authors note captions use "approximately 10 words each, with manual editing to address model-generated errors and improve clarity" (Page 2), indicating manual intervention that may not scale.
- Why unresolved: The paper does not ablate caption quality or compare fully automatic vs. manually-edited captions, leaving unclear how robust the method is to caption noise.
- What evidence would resolve it: Experiments comparing reconstruction accuracy using raw BLIP-generated captions vs. manually refined captions, plus controlled caption degradation studies.

### Open Question 2
- Question: Can the hierarchical semantic alignment approach be extended to handle puzzles with pieces that are nearly visually and semantically identical?
- Basis in paper: [explicit] The authors state: "Our hierarchical semantic alignment cannot fully resolve cases where pieces look nearly identical. When fragments share similar visual and semantic properties, neither visual nor textual cues provide enough discriminative power" (Page 10).
- Why unresolved: The fundamental ambiguity problem remains unsolved even with multimodal guidance; no mechanism is proposed to distinguish truly indistinguishable pieces.
- What evidence would resolve it: Analysis of failure cases with quantified ambiguity metrics, or introduction of additional modalities (e.g., depth, material properties) and evaluation of their discriminative contribution.

### Open Question 3
- Question: How does VLHSA scale to larger puzzle configurations beyond 5×5 grids, and does the computational efficiency advantage persist at scale?
- Basis in paper: [inferred] The paper evaluates only 3×3 and 5×5 puzzles (Page 5), and while noting "lower training time" (Table 8), the quadratic assignment complexity may become prohibitive for larger N.
- Why unresolved: No experiments on 7×7, 10×10, or larger puzzles are presented, and the Hungarian algorithm has O(N³) complexity that may not scale efficiently.
- What evidence would resolve it: Experiments on larger puzzle sizes (7×7, 10×10, 15×15) with runtime and accuracy measurements, comparing against baseline scalability.

## Limitations
- The model struggles with highly similar puzzle pieces, particularly in artwork, where visual and semantic ambiguity limits discriminative power
- Performance drops significantly from 3×3 to 5×5 puzzles (55.15% to 73.5% errors), indicating fundamental scaling challenges
- Reliance on manually edited captions raises scalability concerns for real-world deployment where caption quality cannot be guaranteed

## Confidence
- **High confidence**: Hierarchical alignment improves performance over vision-only baselines, with ablation showing consistent gains across all three alignment levels
- **Medium confidence**: Vision Mamba + BLIP + CLIP provides complementary strengths, though lacks external corpus validation for this specific combination in puzzle solving
- **Low confidence**: The necessity of three-level hierarchy is not definitively established; paper does not test all pairwise combinations or establish statistical significance

## Next Checks
1. Reproduce global-only baseline on JPwLEG-5 to verify the 17.50% Perfect Accuracy baseline and confirm global alignment dominance
2. Evaluate model performance using raw BLIP-generated captions versus manually refined captions on the same test set
3. Generate and analyze attention weight visualizations for token- and region-level alignments on failure cases to assess attention quality