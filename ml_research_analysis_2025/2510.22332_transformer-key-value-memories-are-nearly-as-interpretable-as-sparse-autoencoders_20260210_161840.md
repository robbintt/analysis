---
ver: rpa2
title: Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders
arxiv_id: '2510.22332'
source_url: https://arxiv.org/abs/2510.22332
tags:
- feature
- features
- ff-kv
- layer
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares the interpretability of feature vectors in feed-forward
  (FF) layers, viewed as key-value memories (FF-KV), with those discovered by sparse
  autoencoders (SAEs). Using the SAEBENCH benchmark, the researchers evaluated both
  methods across multiple metrics, including feature alive rate, explained variance,
  absorption score, sparse probing, auto-interpretation, and disentanglement.
---

# Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders

## Quick Facts
- arXiv ID: 2510.22332
- Source URL: https://arxiv.org/abs/2510.22332
- Authors: Mengyu Ye; Jun Suzuki; Tatsuro Inaba; Tatsuki Kuribayashi
- Reference count: 40
- Key outcome: FF-KV features achieve similar interpretability to SAEs across most metrics, with FF-KVs often performing as well or better in areas like absorption and feature isolation.

## Executive Summary
This paper challenges the necessity of proxy-based interpretability methods by demonstrating that feature vectors from feed-forward (FF) layers—when viewed as key-value memories—achieve nearly equivalent interpretability to sparse autoencoders (SAEs). Using the SAEBENCH benchmark, the researchers evaluate both methods across multiple metrics including feature alive rate, explained variance, absorption score, sparse probing, auto-interpretation, and disentanglement. The results show that FF-KV features often perform as well or better than SAEs, particularly in absorption and feature isolation. Human evaluation further confirms comparable interpretability, with FF-KV features being easily identifiable due to distinct activation patterns. The analysis reveals that many features discovered by proxy models like Transcoders do not align with FF-KV features, suggesting potential feature hallucination. Overall, the findings indicate that FF-KV features serve as a strong interpretability baseline, challenging the necessity of proxy-based methods like SAEs.

## Method Summary
The study reframes feed-forward layers as key-value memories, treating the input/down-projection matrix as keys and the output/up-projection matrix as values. Feature vectors are extracted from the columns of the output matrix. These FF-KV features are then evaluated using the SAEBENCH interpretability benchmark and compared against SAEs and Transcoders. The evaluation includes vanilla FF-KV, TopK FF-KV (with sparse activation masking), and Normalized FF-KV (with L2-normalized weights). The SAEBENCH metrics assess various aspects of interpretability including feature activation patterns, concept absorption, feature disentanglement, and causal manipulability. The analysis spans multiple model families including Gemma, Llama, GPT-2, and Pythia across different scales.

## Key Results
- FF-KV features achieve similar or better interpretability scores than SAEs across most SAEBENCH metrics
- TopK FF-KV variants with enforced sparsity show inconsistent benefits across different metrics
- Feature alignment analysis reveals many Transcoder features do not align with FF-KV features (cosine similarity < 0.3)
- Human evaluation confirms FF-KV features are easily identifiable due to distinct activation patterns
- Both FF-KV and SAE methods show poor performance on RAVEL causality scores, suggesting fundamental limitations in model steering

## Why This Works (Mechanism)
The interpretability of FF-KV features stems from their direct extraction from the model's learned weights without introducing proxy approximations. Unlike SAEs which decompose activations through additional training, FF-KV features are inherently faithful to the model's computation. The key-value memory framing captures how FF layers store and retrieve patterns, with the output matrix directly encoding the model's learned transformations. This direct approach avoids the potential hallucination errors introduced by proxy methods while maintaining comparable or superior interpretability across multiple evaluation metrics.

## Foundational Learning
- **Concept: Feed-Forward Layers as Key-Value Memories**
  - Why needed here: The core methodology relies on reframing the FF layer not just as a projection, but as an associative memory where weights store retrievable patterns.
  - Quick check question: Can you write the FF forward pass (`x_out = f(WK*x + bK) * WV + bV`) and identify which matrix represents the "keys" and which represents the "values"?

- **Concept: Sparse Autoencoders (SAEs) for Decomposition**
  - Why needed here: This is the prevailing proxy-based method the paper challenges. Understanding its goal—decomposing activations into sparse, interpretable latents—is essential context.
  - Quick check question: Explain the role of the sparsity penalty in SAE training and why a "feature" is considered the decoder vector for an active latent dimension.

- **Concept: Mechanistic Interpretability Metrics (SAEBENCH)**
  - Why needed here: The paper's claims are anchored in these quantitative metrics. Without understanding what they measure (e.g., absorption, RAVEL), the results are not interpretable.
  - Quick check question: For the "Absorption" metric, describe a scenario where a feature for "starts with 'A'" would absorb part of a concept and what that implies about feature quality.

## Architecture Onboarding
- **Component map:**
  - Residual stream activations -> FF-KV wrapper -> Feature vectors (columns of WV) + Activation patterns (post-non-linearity)
  - SAE/Transcoder baseline modules -> Proxy features for comparison

- **Critical path:**
  1. Extract the FF layer's weight matrices (WV) from a frozen model checkpoint
  2. Run a text corpus through the model and capture internal FF activations (keys)
  3. Treat each neuron's activation pattern over the corpus as a feature's "activations" and the corresponding column of WV as the "feature vector"
  4. Use these activations and vectors directly in the SAEBENCH evaluation suite

- **Design tradeoffs:**
  - **Faithfulness vs. Interpretability:** FF-KV features are perfectly faithful (they are the model), but the paper questions if they are as interpretable as SAE features
  - **Proxy vs. Direct:** SAEs introduce an approximation and potential hallucination error but are designed to disentangle polysemantic neurons. FF-KV avoids this approximation but may inherit model polysemanticity
  - **Sparsity:** The TopK FF-KV variant introduces sparsity to mimic SAEs, but the paper finds the benefits are inconsistent across metrics

- **Failure signatures:**
  - Low Explained Variance (SAEs/Transcoders): Indicates the proxy is a poor reconstruction of the original FF output
  - High Absorption Score: Indicates a concept is split across many features, suggesting redundancy
  - Low RAVEL Causality Score: Indicates difficulty in using features for causal intervention (model steering), a shared weakness of both FF-KV and SAE methods found in the paper
  - Low Feature Alignment (Corpus/Section 6): A low cosine similarity between a proxy's feature vectors and the original WV vectors suggests the proxy may be hallucinating features

- **First 3 experiments:**
  1. Baseline Quantitative Comparison: Run the SAEBENCH suite (all 8 metrics) on a target model's vanilla FF-KV features and its publicly available SAE features. Confirm if the scores fall into a "similar range" as reported in Table 1
  2. Feature Alignment Analysis: Using a pre-trained Transcoder, calculate the Max-Cosine Score (MCS) between its feature vectors and the original FF's WV vectors. Quantify the percentage of "unaligned" features (MCS < 0.3) and manually inspect a sample for semantic equivalence or hallucination
  3. Ablation of Sparsity (TopK FF-KV): Implement the TopK FF-KV variant and sweep the k parameter (e.g., k=1, 5, 10, 50). Plot the effect on key metrics like Absorption and Sparse Probing to test the paper's finding that higher sparsity does not uniformly improve interpretability

## Open Questions the Paper Calls Out
- **Open Question 1:** Do the unaligned features discovered by proxy modules (like Transcoders) represent valid decompositions of polysemantic FF neurons, or are they "hallucinated" artifacts not actually used by the original model?
  - Basis in paper: The authors state in the Conclusion that the "interpretation of this difference (faithfulness of the learned features) remains unclear; future work should elaborate on this point"
  - Why unresolved: The paper identifies a large portion of unaligned features (e.g., 66% in Transcoders) but stops short of definitively distinguishing between successful fine-grained decomposition and feature hallucination
  - What evidence would resolve it: Causal intervention studies where specific unaligned proxy features are ablated or activated to see if they induce predictable changes in the model's output logic

- **Open Question 2:** How does the comparative interpretability gap between FF-KVs and SAEs change when scaling the SAE feature dimension (width) significantly beyond the FF hidden dimension?
  - Basis in paper: The Limitations section notes that the "feature dimension of SAEs and Transcoders we used was fixed" and explicitly suggests that "more diverse configurations should be examined"
  - Why unresolved: While the paper compares specific widths (e.g., 16k), it does not test if the theoretical benefits of SAEs only emerge at much higher expansion factors (overcomplete representations)
  - What evidence would resolve it: Evaluating the SAEBENCH metrics of SAEs trained with varying widths (e.g., 32k, 131k) against the static FF-KV baseline to see if SAEs eventually outperform the baseline

- **Open Question 3:** Why is the theoretical advantage of SAEs in handling superposition not reflected in empirical interpretability improvements over vanilla FF-KV features?
  - Basis in paper: The Introduction and Conclusion highlight that "the theoretical advantage of SAEs is not observed empirically," and the paper notes that FF activations are already naturally sparse
  - Why unresolved: The study confirms the empirical equivalence but does not provide a mechanistic explanation for why the disentanglement provided by SAEs fails to translate into superior interpretability scores
  - What evidence would resolve it: A detailed analysis of the geometric properties of FF activation space versus SAE latent space to quantify the actual reduction in superposition achieved by the proxy models

## Limitations
- The study relies on existing SAEBENCH metrics, which may not fully capture whether features are truly "interpretable" in a human sense
- Both FF-KV and SAE methods perform poorly on RAVEL causality scores, suggesting fundamental limitations in steering models using learned features
- The analysis focuses on feed-forward layers, leaving open questions about whether these findings generalize to other architectural components like attention heads

## Confidence
- **FF-KV Features Match SAE Interpretability:** High - The quantitative results across multiple SAEBENCH metrics consistently show FF-KV features achieving similar or better performance than SAEs
- **Transcoders Hallucinate Features:** Medium - The feature alignment analysis shows many Transcoder features do not align with FF-KV features, but the paper acknowledges this could result from either hallucination or genuine disentanglement
- **SAE Proxy Adds Little Value for Interpretability:** Medium - While FF-KV performs comparably, the paper doesn't fully address potential benefits of SAEs in other domains like representation compression or robustness to adversarial examples

## Next Checks
1. **Cross-Architecture Validation:** Replicate the FF-KV vs SAE comparison on attention head weights and embeddings to determine if the interpretability advantage extends beyond feed-forward layers

2. **Human Evaluation Replication:** Conduct blinded human evaluations where annotators rate the interpretability of randomly sampled features from FF-KV, SAE, and Transcoder methods to validate the GPT-4o auto-interpretation results

3. **Dynamic Feature Analysis:** Track how feature interpretability metrics change during model training to identify whether FF-KV features emerge naturally or require post-hoc analysis to become interpretable