---
ver: rpa2
title: Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge
  Distillation
arxiv_id: '2602.01956'
source_url: https://arxiv.org/abs/2602.01956
tags:
- uncertainty
- draft
- target
- estimation
- epistemic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel framework for efficient epistemic
  uncertainty estimation in large language models by leveraging small draft models
  used in speculative decoding. The method decomposes uncertainty into a variance
  proxy (Jensen-Shannon divergence among draft models) and a bias proxy (KL divergence
  between draft mixture and target), eliminating the need for expensive ensemble passes.
---

# Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation

## Quick Facts
- arXiv ID: 2602.01956
- Source URL: https://arxiv.org/abs/2602.01956
- Authors: Seonghyeon Park; Jewon Yeom; Jaewon Sok; Jeongjae Park; Heejun Kim; Taesup Kim
- Reference count: 18
- Primary result: Introduces framework estimating epistemic uncertainty using draft models from speculative decoding, achieving 37.67% lower RMSE than baselines

## Executive Summary
This paper proposes an efficient method for estimating epistemic uncertainty in large language models by repurposing draft models from speculative decoding. The approach decomposes uncertainty into a variance proxy (Jensen-Shannon divergence among draft models) and a bias proxy (KL divergence between draft mixture and target), eliminating the need for expensive ensemble passes. The method introduces Online Stochastic Distillation (OSD) and Data-Diverse Drafts (DDD) training to ensure effective approximation. Experiments on GSM8K demonstrate significant improvements in uncertainty estimation accuracy and hallucination detection performance while reducing inference costs by up to 42% compared to traditional perturbation-based approaches.

## Method Summary
The method leverages small draft models used in speculative decoding to efficiently estimate epistemic uncertainty. It decomposes uncertainty into two components: variance (JSD among drafts) and bias (KL between draft mixture and target). Draft models are trained on disjoint data partitions (DDD) to maintain diversity, while a proxy model is trained via Online Stochastic Distillation (OSD) to approximate the target's Bayesian Model Average. This allows token-level uncertainty estimation without expensive ensemble passes. For hallucination detection, the estimated uncertainty is aggregated to the sequence level and used as a feature in a logistic regression classifier.

## Key Results
- 37.67% reduction in RMSE compared to baselines for uncertainty estimation on GSM8K
- Matches TokUR's hallucination detection performance (AUROC) while reducing FLOPs by up to 42%
- 2×3 DDD configuration provides optimal trade-off between performance and training complexity
- 1B draft models achieve competitive performance with 42% FLOPs reduction versus 3B drafts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Epistemic Uncertainty can be approximated by combining disagreement among draft models (variance) and their divergence from the target (bias).
- **Mechanism:** Theoretically grounded in Bias-Variance Decomposition (Theorem 3.4), approximates EU as sum of JSD among drafts and KL between draft mixture and target.
- **Core assumption:** Expectation over target posterior can be approximated by expectation over discrete set of draft models (Proxy Posterior Assumption 3.3).
- **Evidence anchors:** Abstract states EU approximation via JSD and KL; Theorem 3.4 proves decomposition; Credal Ensemble Distillation supports efficient ensemble proxies.
- **Break condition:** If drafts fail to sample diverse modes (mode collapse), variance proxy vanishes, leading to under-estimation of uncertainty.

### Mechanism 2
- **Claim:** Online Stochastic Distillation enables single proxy model to approximate Bayesian Model Average of target.
- **Mechanism:** Minimizing expected Forward KL against stochastically perturbed teacher samples theoretically converges to arithmetic mean of teacher's distributional modes.
- **Core assumption:** Low-rank Gaussian noise injection into target model creates valid samples approximating posterior distribution.
- **Evidence anchors:** Section 3.4 defines OSD and states minimizing expected Forward KL yields BMA; Section 4.1 describes noise injection.
- **Break condition:** If noise injection is insufficient or Forward KL is swapped for Reverse KL, proxy fails to capture full multi-modal uncertainty.

### Mechanism 3
- **Claim:** Data-Diverse Drafts are required to maintain diversity necessary for variance proxy to function.
- **Mechanism:** Training drafts on disjoint data partitions forces them to learn distinct inductive biases and cover different target distribution modes.
- **Core assumption:** Disjoint partitions of target-generated data contain sufficient signal to represent distinct modes of target's behavior.
- **Evidence anchors:** Section 4.3 defines DDD as training on disjoint partitions; Section 6 states DDD prevents draft collapse.
- **Break condition:** If training data is homogeneous or partitions not sufficiently distinct, drafts may collapse to single mode, nullifying variance term.

## Foundational Learning

- **Concept:** Epistemic vs. Aleatoric Uncertainty
  - **Why needed here:** Paper specifically targets epistemic uncertainty (model ignorance/parameters) to detect hallucinations, distinguishing from aleatoric uncertainty (data noise).
  - **Quick check question:** Does the method measure noise in training data or model's lack of knowledge?

- **Concept:** Speculative Decoding (Draft/Verify)
  - **Why needed here:** Method repurposes draft models used for inference acceleration as "ensemble" for uncertainty estimation.
  - **Quick check question:** Why is using draft model cheaper than using full ensemble of target models?

- **Concept:** Forward vs. Reverse KL Divergence
  - **Why needed here:** Paper argues Reverse KL (MiniLLM/GKD) causes mode collapse, while Forward KL (OSD) promotes mode covering essential for uncertainty.
  - **Quick check question:** Which divergence encourages student to cover all possibilities of teacher, even unlikely ones?

## Architecture Onboarding

- **Component map:** Target Model -> Draft Family (q_k) -> Proxy Model (p_mix)
- **Critical path:** 
  1. Target generates multi-response data
  2. Partition data into disjoint sets (DDD strategy)
  3. Train drafts via standard distillation on respective partitions
  4. Train proxy via OSD (stochastic teacher, Forward KL)
  5. Compute Variance (JSD of drafts) + Bias (KL of p_mix vs Draft Mix) for EU
- **Design tradeoffs:**
  - Draft Size vs. Cost: 1B drafts offer ~42% FLOPs reduction with slight AUROC drop; 3B drafts offer higher fidelity
  - Partitioning vs. Complexity: 2×3 config offers best trade-off; 3×3 offers marginal gains at higher training cost
- **Failure signatures:**
  - JSD → 0: Indicates draft collapse; check DDD implementation
  - High RMSE: Bias term too high; check OSD convergence or noise injection
  - High Inference Cost: Verify draft count not excessive; ensure p_mix is single pass
- **First 3 experiments:**
  1. Compare RMSE of p_mix vs raw target ensemble to verify stable bias estimation
  2. Compare DDD vs IDD/GKD to confirm data partitioning drives variance
  3. Run 8B → 1B configuration to verify performance under high capacity gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DDD strategy faithfully approximate target's true Bayesian posterior, or introduce artificial variance leading to over-estimation of epistemic uncertainty?
- Basis: From Assumption 3.3 and Section 5.3, noting DDD prevents variance collapse but forcing distinct heuristics
- Why unresolved: Paper doesn't verify if disjointly trained draft modes correspond to target's actual posterior beliefs or represent distinct hallucinated approximations
- What evidence would resolve it: Compare draft ensemble's distribution against ground-truth posterior approximation or analyze error types where DDD over-estimates uncertainty

### Open Question 2
- Question: How does framework perform on tasks with high aleatoric uncertainty (creative writing) vs deterministic tasks (GSM8K) used in study?
- Basis: From Section 4.2 noting experiments conducted exclusively on GSM8K with deterministic ground truths
- Why unresolved: Method equates draft disagreement with epistemic uncertainty, but in open-ended tasks high disagreement might reflect valid stylistic variance rather than model ignorance
- What evidence would resolve it: Evaluate correlation between estimated epistemic uncertainty and factual accuracy on datasets with high semantic diversity

### Open Question 3
- Question: What is limit of capacity gap between target and draft models before draft ensemble fails to capture target's reasoning modes?
- Basis: From Table 3 and Section 5.3 noting degradation when switching from 3B to 1B drafts for 8B target
- Why unresolved: Paper demonstrates feasibility for 8B targets but acknowledges performance dip with smaller drafts; unclear if trend is linear or breaking point exists
- What evidence would resolve it: Experiments scaling target to 70B+ while varying draft sizes to identify minimum draft capacity required

## Limitations
- Relies critically on draft diversity, which fails with standard on-policy distillation requiring significant additional engineering overhead
- Decomposition theorem assumes proxy posterior can adequately represent target posterior, may not hold with small K relative to posterior complexity
- Requires logistic regression calibration on held-out data for hallucination detection, adding deployment complexity

## Confidence
- **High Confidence**: Empirical demonstration that draft diversity is essential and DDD successfully maintains it (ablation studies show DDD outperforms IDD/GKD)
- **Medium Confidence**: Theoretical decomposition of EU into variance and bias proxies (Theorem 3.4 provides justification but practical approximation quality depends on uncharacterized factors)
- **Low Confidence**: Generalizability beyond GSM8K and specific 8B→3B/1B architecture pairing (no evidence for different tasks or model scale combinations)

## Next Checks
1. **Diversity Sensitivity Analysis**: Systematically vary number of draft models and data partitions to determine minimum diversity threshold required for reliable uncertainty estimation
2. **Cross-Domain Generalization**: Evaluate method on non-mathematical tasks and with different model pairs to assess robustness across domains and scale gaps
3. **Real-time Deployment Feasibility**: Measure inference latency and memory overhead of complete pipeline including logistic regression calibration, compare against baseline methods without calibration requirements