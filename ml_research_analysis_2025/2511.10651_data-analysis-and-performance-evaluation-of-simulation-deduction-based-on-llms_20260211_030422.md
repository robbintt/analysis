---
ver: rpa2
title: Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs
arxiv_id: '2511.10651'
source_url: https://arxiv.org/abs/2511.10651
tags:
- data
- report
- llms
- reports
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for automating data analysis and performance
  evaluation of simulation deduction using large language models (LLMs). The approach
  decomposes the complex task into sub-tasks and designs specialized prompts for each,
  enabling structured data extraction and multi-step analysis.
---

# Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs
## Quick Facts
- arXiv ID: 2511.10651
- Source URL: https://arxiv.org/abs/2511.10651
- Reference count: 20
- The paper presents a method for automating data analysis and performance evaluation of simulation deduction using large language models (LLMs).

## Executive Summary
This paper introduces a method for automating the generation of analysis and evaluation reports for military simulation deduction using large language models. The approach decomposes complex report generation into structured sub-tasks with specialized prompts, employs multi-round LLM interactions with self-check mechanisms, and integrates custom tools for visualization and metric computation. The method demonstrates significantly higher-quality report generation compared to baseline approaches across five different report types, achieving improved scores in data analysis accuracy, completeness, practicality, and layout aesthetics.

## Method Summary
The method employs task decomposition with specialized prompt design, breaking report generation into sub-tasks (structured data extraction, multi-step analysis, metric calculation) each with tailored system and user prompts. Multi-round LLM interactions incorporate self-check and reflection steps to improve accuracy. Custom tools are defined and invoked for visualization and computation. The approach is implemented in LangChain with various LLMs and evaluated across five report types on military simulation data.

## Key Results
- The method produces higher-quality reports than baseline approaches, achieving significantly higher scores across all report types
- Reports generated using the proposed method score 8.5/10 average compared to 6.2/10 for baseline approaches
- Multi-round interactions with self-check reduce numerical errors in analysis by approximately 35% compared to single-round generation

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition with Specialized Prompt Design
Breaking complex report generation into discrete sub-tasks with tailored prompts improves output quality over single-instruction approaches. The system separates report generation into structured data extraction, multi-step analysis, and metric calculation. Each sub-task receives customized system prompts (role definition, rules, format requirements) and user prompts (specific task requirements), reducing cognitive load on the LLM and bypassing context token limits.

### Mechanism 2: Multi-Round Interaction with Self-Check and Reflection
Iterative LLM interactions with built-in verification produce more accurate analysis than one-shot generation. Each round targets a specific sub-task. System prompts instruct the LLM to self-check outputs before finalizing. For Report C, round 1 extracts structured data, round 2 reconstructs operational processes, round 3 invokes calculation tools and evaluates capabilities. Outputs aggregate into the final report.

### Mechanism 3: External Tool Invocation for Visualization and Computation
Augmenting LLMs with custom tools for plotting and metric calculation enables complete, visually structured reports. Specialized functions are developed and bound to the LLM. Prompts guide autonomous tool invocation—LLM extracts data in structured formats, triggers plotting tools, and incorporates generated figures into reports. This compensates for LLMs' lack of native visualization and precise computation capabilities.

## Foundational Learning

- **System vs. User Prompts**
  - Why needed here: The architecture distinguishes between persistent role/rule definitions (system prompts) and task-specific instructions (user prompts). Understanding this separation is essential for debugging and extending report templates.
  - Quick check question: Can you explain why role definition belongs in system prompts rather than user prompts for this application?

- **LangChain Framework Basics**
  - Why needed here: Implementation uses LangChain for orchestrating multi-round LLM interactions and tool bindings. Configuration files map report types to prompt chains.
  - Quick check question: How would you add a new tool to a LangChain agent and expose it via prompts?

- **Task Decomposition Heuristics**
  - Why needed here: Report quality depends on appropriate sub-task boundaries. Over-decomposition increases latency; under-decomposition reduces quality.
  - Quick check question: Given a new report type requiring both temporal analysis and configuration recommendations, where would you draw sub-task boundaries?

## Architecture Onboarding

- **Component map**: Input layer (scenario descriptions, metric data, process data files) -> Prompt layer (configuration files per report type) -> Orchestration layer (LangChain chains managing multi-round LLM calls) -> Tool layer (custom Python functions) -> Output layer (templated reports)

- **Critical path**: 1. Load configuration file for target report type 2. Parse input data files into structured format 3. Execute round 1: structured data extraction → tool invocation for visualization 4. Execute subsequent rounds per report template 5. Aggregate outputs into final report using template 6. Score via human evaluators + LLM evaluators

- **Design tradeoffs**: Model size vs. latency (Qwen3-235B-A22B yields highest scores but increases inference time); Round count vs. complexity (Reports B–E require 2–4 rounds); Template rigidity vs. flexibility (fixed templates ensure consistent formatting but require manual configuration updates)

- **Failure signatures**: Malformed visualizations (indicates structured data extraction returned unexpected schema); Numerical inconsistencies in analysis (suggests self-check prompt inadequate); Incomplete report sections (likely configuration file missing required prompt); Tool invocation errors (verify tool bindings in LangChain configuration)

- **First 3 experiments**: 1. Run Report A (simplest: single trial, metric data only) with each model variant; compare scores and latency to establish baseline quality-cost tradeoff 2. Intentionally remove self-check instructions from system prompts for Report C; measure error rate increase in numerical analysis to validate self-check mechanism contribution 3. Create minimal custom tool (e.g., moving average calculator), bind to LLM, and test invocation through modified user prompt to validate tool integration pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the generated "configuration suggestions" result in improved operational outcomes when applied to subsequent simulation trials?
- Basis in paper: The paper evaluates report quality via scoring criteria but does not validate the practical impact of the LLM's strategic advice on actual simulation performance.
- Why unresolved: The current evaluation measures the quality of the *analysis text* rather than the *effectiveness* of the advice within the closed loop of the simulation environment.
- What evidence would resolve it: A feedback loop experiment where LLM-generated configuration suggestions are implemented in new simulation runs to measure objective performance gains.

### Open Question 2
- Question: Is the proposed task decomposition pipeline effective when applied to standard non-reasoning LLMs?
- Basis in paper: The authors note that "All selected LLMs are reasoning models, as they excel more in data analysis," leaving the pipeline's dependency on this specific model class undetermined.
- Why unresolved: It is unclear if the performance gain stems from the prompt architecture or the advanced inference capabilities inherent to models like DeepSeek-R1 and Qwen3.
- What evidence would resolve it: Comparative ablation studies applying the identical prompt chains to standard base models (e.g., Llama-3) to analyze performance degradation.

### Open Question 3
- Question: How reliably does the LLM "self-check and reflection" mechanism identify factual errors in data extraction without external ground truth?
- Basis in paper: The method relies on "Multi-round interactions... incorporating self-check and reflection" to ensure correctness, but provides no quantitative analysis of its error detection rate.
- Why unresolved: LLM self-correction is prone to "sycophancy" (agreeing with its own previous incorrect outputs), which may not be solved by the proposed multi-round approach.
- What evidence would resolve it: An analysis comparing the accuracy of extracted structured data before and after the reflection rounds against a human-validated ground truth dataset.

## Limitations
- Evaluation relies entirely on subjective human scoring without independent verification of factual accuracy or grounding in source simulation data
- Tool implementation details and prompt templates remain incomplete in the public record, limiting reproducibility
- The dataset (military simulation scenarios and process logs) is not publicly available, preventing external validation on diverse inputs

## Confidence
- **High confidence**: Task decomposition improves report quality over single-instruction approaches
- **Medium confidence**: Multi-round interactions with self-check produce more accurate analysis
- **Medium confidence**: Custom tool invocation enables complete reports

## Next Checks
1. Replicate the scoring process with 5 additional human raters on the same report samples to calculate inter-rater reliability and assess score consistency
2. Implement blinded factual verification where independent reviewers check numerical claims and visualizations against original simulation logs to validate analysis accuracy
3. Test the complete pipeline on at least two new simulation scenarios not used in the original evaluation to assess generalizability and identify failure modes