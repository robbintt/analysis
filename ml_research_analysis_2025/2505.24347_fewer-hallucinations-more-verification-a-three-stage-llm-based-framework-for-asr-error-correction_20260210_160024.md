---
ver: rpa2
title: 'Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework
  for ASR Error Correction'
arxiv_id: '2505.24347'
source_url: https://arxiv.org/abs/2505.24347
tags:
- correction
- error
- speech
- recognition
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a reliable LLM-based ASR error correction
  framework (RLLM-CF) that avoids fine-tuning and external information while addressing
  hallucination issues. The core method uses a three-stage process: error pre-detection
  to filter correct sentences, chain-of-thought subtask decomposition (localization,
  pronunciation, candidate generation, selection) with iterative correction, and reasoning
  process verification to ensure output reliability.'
---

# Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction

## Quick Facts
- arXiv ID: 2505.24347
- Source URL: https://arxiv.org/abs/2505.24347
- Reference count: 35
- Primary result: 21%, 11%, 9%, and 11.4% relative reductions in CER/WER on AISHELL-1, AISHELL-2, and LibriSpeech datasets

## Executive Summary
This paper presents RLLM-CF, a three-stage LLM-based framework for ASR error correction that avoids fine-tuning and external information while addressing hallucination issues. The core method uses error pre-detection to filter correct sentences, chain-of-thought subtask decomposition with iterative correction, and reasoning process verification to ensure output reliability. Evaluated on Chinese and English datasets, the GPT-4o-enhanced framework demonstrates strong performance in reducing ASR errors while maintaining text fidelity.

## Method Summary
The RLLM-CF framework implements a three-stage pipeline for LLM-based ASR error correction. Stage 1 performs error pre-detection to avoid modifying correct text. Stage 2 uses iterative chain-of-thought subtask decomposition (localization, pronunciation, candidate generation, selection) with confidence assessment. Stage 3 verifies output format and reasoning completeness before accepting corrections. The framework uses GPT-4o or DeepSeek-V2 with temperature=0.2, top_p=0.8, and groups sentences to reduce token consumption.

## Key Results
- 21% relative CER reduction on AISHELL-1 Chinese dataset
- 11% relative WER reduction on AISHELL-2 Chinese dataset
- 9% relative WER reduction on LibriSpeech test-clean English dataset
- 11.4% relative WER reduction on LibriSpeech test-other English dataset

## Why This Works (Mechanism)

### Mechanism 1: Error Pre-Detection
A binary error detection gate significantly reduces modification of correct text by preventing the LLM from attempting unnecessary corrections. The framework first prompts the LLM to classify input sentences as containing errors or not, bypassing the correction pipeline when no errors are detected. This works because the LLM's false positive rate for detecting errors is lower than its tendency to introduce errors when generating corrections without such a gate.

### Mechanism 2: Chain-of-Thought Subtask Decomposition
Structuring the correction task into explicit subtasks (localization, pronunciation, candidate generation, selection) improves reasoning reliability and constrains the LLM's output space. Instead of a single "correct this" prompt, the model is guided through a step-by-step process: first locate the defective phrase, then analyze its pronunciation, generate phonetically similar candidates, and finally select the best one based on context.

### Mechanism 3: Iterative Correction with Verification
A multi-pass loop with confidence assessment and final verification filters out low-quality generations and ensures output adherence to task constraints. The framework attempts correction up to a max steps limit, with the LLM self-assessing confidence in generated corrections. Only when both format and reasoning checks are satisfied is the corrected output accepted; otherwise, the original text is retained.

## Foundational Learning

- **Concept: LLM Hallucination Types**
  - Why needed here: The framework is explicitly designed to mitigate hallucinations. Distinguishing between faithful hallucinations (e.g., instruction violations, grammar corrections) and factual hallucinations (content errors) is crucial for diagnosing failure modes.
  - Quick check question: Based on Table I, what type of faithful hallucination occurs when the LLM refuses to process the input?

- **Concept: ASR Error Metrics (CER/WER)**
  - Why needed here: The paper's success is measured by relative reductions in Character Error Rate (CER) and Word Error Rate (WER). Understanding that these metrics are a function of substitutions, deletions, and insertions is key to interpreting the results.
  - Quick check question: According to Table II, did the framework's correction increase or decrease deletion errors compared to the baseline ASR?

- **Concept: Prompt Engineering (CoT & Few-Shot)**
  - Why needed here: The framework's core capability is elicited via carefully designed prompts. The CoT strategy and few-shot examples are fundamental techniques for guiding LLM behavior.
  - Quick check question: What are the four specific subtasks defined in the Chain-of-Thought prompt?

## Architecture Onboarding

- **Component map:** Error Pre-Detection Module -> Iterative CoT Corrector -> Answer Verifier -> Orchestrator
- **Critical path:**
  1. ASR sentence enters. Orchestrator calls Stage 1 (Error Pre-Detection).
  2. If D(y) == 0, Orchestrator returns original. If D(y) == 1, Orchestrator calls Stage 2 (Iterative CoT Corrector).
  3. Stage 2 runs CoT prompt. If confidence is low and steps < max, it retries. If confident, it returns candidate y* and reasoning R.
  4. Orchestrator sends y* and R to Stage 3 (Answer Verifier).
  5. Stage 3 checks rules. If pass, Orchestrator outputs y*. If fail, it outputs original y.

- **Design tradeoffs:**
  - Latency vs. Reliability: Three-stage process with iterative loops introduces significant latency compared to single LLM call
  - Token Cost vs. Performance: CoT prompts and iterative retries consume more tokens, though grouping reduces cost
  - Conservatism vs. Correction Rate: System biased towards preserving original text if verification fails, reducing false positives but potentially missing valid corrections

- **Failure signatures:**
  - Format Errors: LLM generates valid correction but fails to wrap in [] or add [changed] flag
  - Instruction Drift: LLM ignores subtask decomposition and tries to rewrite entire sentence
  - Stuck Iteration: Model repeatedly generates low-confidence outputs until max steps reached

- **First 3 experiments:**
  1. Baseline Hallucination Quantification: Run simple "correct this" prompt on AISHELL-1 test set, manually categorize introduced errors
  2. Ablation of Pre-Detection: Run full framework bypassing Stage 1, measure resulting CER and change in insertion errors
  3. Verification Strictness Test: Vary Stage 3 criteria (format only vs format and reasoning), plot CER reduction vs rejection rate tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can the RLLM-CF framework achieve comparable performance using smaller, open-source LLMs instead of proprietary models like GPT-4o? The experiments are limited to GPT-4o and DeepSeek-V2; the framework's dependence on high-capability models is not analyzed.

### Open Question 2
What is the optimal trade-off between token consumption and correction quality when grouping multiple sentences for batch inference? Section V-C states grouping slightly degrades performance, but this degradation is not quantified.

### Open Question 3
How does the maximum iteration count in Stage 2 affect final correction accuracy, and what is the optimal stopping criterion? Algorithm 1 defines max steps but the paper does not systematically vary or analyze this hyperparameter.

### Open Question 4
Can the reasoning process verification module be strengthened to further reduce the remaining hallucination-induced deletion and insertion errors? Section IV-E states a slight increase in deletion and insertion errors is observed, primarily due to hallucinations.

## Limitations
- Performance heavily depends on specific prompt wording and few-shot examples not fully specified in paper
- Error pre-detection assumes LLM's false positive rate is low, not empirically validated across different ASR models
- Iterative correction effectiveness relies on LLM's self-assessment being well-calibrated, may not generalize to other LLMs or domains
- Answer verification may be overly conservative, potentially rejecting valid corrections that don't perfectly match expected format

## Confidence

- **High confidence:** Framework reduces CER/WER on AISHELL-1, AISHELL-2, and LibriSpeech datasets compared to baseline ASR models
- **Medium confidence:** Three-stage architecture (error pre-detection, CoT subtasks, verification) is primary driver of performance improvements
- **Low confidence:** Framework's effectiveness in preventing hallucinations is overstated; verification stage may reject many valid corrections

## Next Checks

1. **Prompt sensitivity analysis:** Systematically vary prompt wording and few-shot examples to quantify impact on CER/WER and hallucination rates
2. **Cross-LLM validation:** Test framework with different LLMs (e.g., Claude, LLaMA) to determine if performance gains are specific to GPT-4o or generalizable
3. **Real-time streaming evaluation:** Implement framework in streaming ASR context and measure trade-off between latency and accuracy