---
ver: rpa2
title: Fast and Low-Cost Genomic Foundation Models via Outlier Removal
arxiv_id: '2505.00598'
source_url: https://arxiv.org/abs/2505.00598
tags:
- germ
- performance
- dnabert-2
- quantization
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scarce computational resources
  in genomic modeling by introducing GERM, a genomic foundation model with strong
  compression performance and fast adaptability. GERM improves upon models like DNABERT-2
  by eliminating outliers that hinder low-rank adaptation and post-training quantization,
  enhancing both efficiency and robustness.
---

# Fast and Low-Cost Genomic Foundation Models via Outlier Removal

## Quick Facts
- arXiv ID: 2505.00598
- Source URL: https://arxiv.org/abs/2505.00598
- Reference count: 40
- Key outcome: GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over baseline DNABERT-2 while reducing average kurtosis by 92.14% and maximum infinity norm by 82.77%.

## Executive Summary
This paper addresses the challenge of scarce computational resources in genomic modeling by introducing GERM, a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. The core method replaces the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, GERM-T employs small-step continual learning within the outlier-free framework to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model, while reducing average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings.

## Method Summary
GERM modifies the attention mechanism in genomic foundation models by replacing standard softmax with Softmax1, which adds a constant to the denominator to prevent outlier formation. This change is applied during both pre-training (GERM) and fine-tuning (GERM-T) phases. The method uses ALiBi positional encoding and is evaluated on 27 downstream genomic datasets. GERM-T offers a cost-effective alternative by applying small-step continual learning to existing checkpoints rather than full pre-training. The approach targets improved low-rank adaptation and post-training quantization while maintaining performance.

## Key Results
- GERM improves fine-tuning performance by 37.98% over baseline DNABERT-2
- GERM achieves 64.34% better quantization performance than baseline
- Outlier metrics show GERM reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%

## Why This Works (Mechanism)

### Mechanism 1
Replacing standard softmax with Softmax1 mitigates high-magnitude activation outliers during transformer pre-training and fine-tuning. Standard softmax normalizes attention scores to sum to 1, forcing low-information tokens to receive disproportionate attention weights and creating "vertical" attention patterns (outliers). Softmax1 adds a constant 1 to the denominator, allowing the attention mechanism to scale down the entire output when query-key interactions are weak, effectively allowing the layer to "do nothing" rather than amplifying noise. This assumes outliers stem largely from softmax forcing assignments to uninformative tokens rather than solely from data complexity.

### Mechanism 2
Reducing the kurtosis (tailedness) of activation distributions directly improves low-bit post-training quantization accuracy. Quantization maps continuous floating-point values to a discrete grid, and high kurtosis implies "fat-tailed" distributions with extreme values (outliers). These outliers force the quantization grid to cover a massive range, reducing precision for the majority of values in the dense center. By reducing outliers, GERM allows W4A4 and W6A6 quantization to use tighter grids, preserving information in the core distribution. This assumes outliers are noise or redundancy rather than rare but critical biological features.

### Mechanism 3
Outlier-free activation landscapes enable more stable and accurate Low-Rank Adaptation (LoRA). LoRA fine-tunes models by injecting low-rank matrices to approximate weight changes, but prior works suggest LoRA can worsen outliers. When the base model has high activation magnitude variance (outliers), the low-rank adapter struggles to correct the output distribution, leading to performance drops. An outlier-free architecture ensures the adapter learns task-specific features rather than compensating for pre-training instabilities. This assumes the performance gap in standard LoRA is due to interference from outliers rather than insufficient rank capacity.

## Foundational Learning

- **Concept: Modern Hopfield Networks & Associative Memory**
  - Why needed here: GERM replaces the attention layer with an "outlier-free Hopfield layer." Understanding this requires knowing that attention can be viewed as a differentiable lookup in an associative memory store, where Softmax1 controls the retrieval sharpness (beta) and sparsity.
  - Quick check question: How does the "energy landscape" of a Hopfield network differ from standard attention when trying to retrieve a pattern (token) that doesn't perfectly match the query?

- **Concept: Activation Kurtosis and Quantization Error**
  - Why needed here: The paper uses "average kurtosis" as a proxy for quantizability. One must understand that kurtosis measures "tailedness"â€”high kurtosis means frequent, extreme deviations from the mean.
  - Quick check question: Why does a high-kurtosis distribution make 4-bit integer quantization specifically prone to overflow or underflow errors compared to a Gaussian distribution?

- **Concept: ALiBi (Attention with Linear Biases)**
  - Why needed here: GERM uses ALiBi for positional encoding instead of learned embeddings. This is crucial for genomic sequences which can vary drastically in length.
  - Quick check question: Why does ALiBi allow for extrapolation to sequence lengths longer than those seen during training, and why is this preferred for DNA sequences over sinusoidal embeddings?

## Architecture Onboarding

- **Component map:** Tokenizer (SentencePiece BPE, vocab size 4096) -> Embedding (Token Embeddings + ALiBi) -> Encoder Stack (Standard Transformer layers with Softmax1 attention) -> Output
- **Critical path:** The Attention Mechanism Implementation is critical. Standard: Output = Dropout(Softmax(Scores)) @ V. GERM: Output = Dropout(Softmax1(Scores)) @ V. Constraint: You must implement Softmax1(x) = exp(x) / (1 + sum(exp(x))). This denominator shift allows the attention to output a near-zero vector (sparse update) when no query matches keys well.
- **Design tradeoffs:**
  - GERM (Scratch) vs. GERM-T (Continual): GERM requires full pre-training (~4 days on A100s) but yields the best quantization (W4A4). GERM-T is cheap (fine-tuning existing models) but struggles at extreme compression (W4A4) compared to the scratch version.
  - Vocabulary Size: The authors settled on 4096. Smaller vocabularies increase sequence length (compute cost), larger ones fragment biological motifs.
- **Failure signatures:**
  1. Instability in Continual Learning (GERM-T): If the learning rate is too high during the "small-step" phase, the model may unlearn biological priors before stabilizing the outliers.
  2. Inference Divergence: If deploying on hardware without bf16 support (e.g., older CPUs), QLoRA might be slower than LoRA despite the model optimizations due to type emulation overhead.
  3. Loss of Long-Range Context: While ALiBi helps, if the outlier removal is too aggressive (functionally setting attention to 0), weak long-range DNA interactions might be dropped.
- **First 3 experiments:**
  1. Outlier Visualization: Train a vanilla BERT vs. GERM on a small DNA subset (e.g., Mouse TF prediction). Plot the infinity norm (||x||_inf) of the FFN output. Confirm GERM's "spikes" are significantly lower.
  2. Quantization Stress Test: Take a pre-trained DNABERT-2 checkpoint. Apply OmniQuant (W4A4). Record accuracy drop. Then, apply GERM-T (continual learning for 40k steps) and re-quantize. Verify the "64.34% improvement" metric.
  3. LoRA Convergence Speed: Fine-tune GERM vs. DNABERT-2 on a resource-constrained device (e.g., RTX 2080 Ti). Measure time-to-target-MCC. The paper claims 5 minutes for GERM; verify this wall-clock speedup.

## Open Questions the Paper Calls Out

- Can an efficient strategy be developed to remove outliers without retraining from scratch while preventing the significant performance drops GERM-T exhibits during 4-bit quantization? The authors state, "In future work, we aim to develop strategies that efficiently remove outliers without necessitating retraining from scratch," noting GERM-T fails at W4A4.

- Can a quantization-aware training (QAT) approach effectively manage the approximation errors and performance trade-offs in the GERM-T continual learning strategy? Appendix E mentions, "In future work, we explore... a more robust quantization-aware training (QAT) approach to better manage these trade-offs."

- Does the Softmax1 outlier-free layer generalize effectively to non-transformer genomic architectures, such as State Space Models (Mamba) or convolution-based models (HyenaDNA)? Section 2 states that models like HyenaDNA and Caduceus "require further research" and were excluded because they are not transformer-based.

## Limitations

- Training Data Dependence: The method's robustness to out-of-distribution sequences (e.g., viral genomes, metagenomic samples) or longer contexts (>512 tokens) is untested, as validation only covers 27 curated datasets from the GUE benchmark.

- Quantitative Uncertainty in Relative Gains: The reported "37.98% improvement" and "64.34% improvement" are relative to DNABERT-2 without absolute baseline values, making it difficult to assess practical significance.

- GERM-T vs. GERM Trade-off Ambiguity: The paper positions GERM-T as a cheaper alternative but shows it struggling with W4A4 quantization compared to GERM, without quantifying the cost-performance trade-off curve.

## Confidence

- **High Confidence:** The core mechanism (replacing softmax with Softmax1) and its effect on kurtosis reduction. The math is verifiable, and the outlier metric improvements (92.14% kurtosis reduction, 82.77% infinity norm reduction) are directly measurable from activation distributions.

- **Medium Confidence:** The downstream task performance improvements (MCC gains). While the method is sound, the GUE benchmark's construction and exact pre-training data are not fully specified, introducing potential reproducibility gaps.

- **Low Confidence:** The claim that GERM-T's "small-step continual learning" is universally applicable. The paper does not detail the learning rate schedule, optimizer, or convergence criteria, making it difficult to reproduce or adapt to other genomic foundation models.

## Next Checks

1. **Out-of-Distribution Robustness Test:** Evaluate GERM on a held-out dataset of viral or synthetic long-read sequences (>1024 tokens) not present in the GUE benchmark. Measure both MCC and outlier metrics to confirm the method's generalization beyond curated datasets.

2. **Ablation on Softmax1 Variants:** Implement and compare Softmax1 against other outlier mitigation strategies (e.g., SmoothQuant's scaling factor, RotateKV's rotation matrix) on the same DNABERT-2 backbone. This isolates whether the denominator shift in Softmax1 is the primary driver of gains or if it's synergistic with other architectural choices.

3. **Extreme Quantization at W2A2:** Push GERM and GERM-T to 2-bit weight and 2-bit activation quantization (W2A2). Measure the point at which MCC drops below a usability threshold (e.g., MCC < 0.3) to establish the practical limits of the outlier-removal approach for ultra-low-resource deployment.