---
ver: rpa2
title: Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits
arxiv_id: '2510.10730'
source_url: https://arxiv.org/abs/2510.10730
tags:
- ensemble
- regret
- sampling
- neural
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper extends ensemble sampling to nonlinear contextual bandit
  problems, focusing on generalized linear bandits (GLM) and neural contextual bandits.
  The authors propose two algorithms: GLM-ES for generalized linear bandits and Neural-ES
  for neural contextual bandits.'
---

# Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits

## Quick Facts
- arXiv ID: 2510.10730
- Source URL: https://arxiv.org/abs/2510.10730
- Reference count: 9
- This paper extends ensemble sampling to nonlinear contextual bandit problems with provable regret bounds

## Executive Summary
This paper proposes ensemble sampling algorithms for nonlinear contextual bandits, specifically generalized linear bandits (GLM) and neural contextual bandits. The algorithms maintain an ensemble of models trained on randomly perturbed data, achieving theoretical guarantees with high-probability regret bounds that match state-of-the-art randomized exploration methods. The work introduces anytime versions using the doubling trick, eliminating the need for fixed-time horizon assumptions while maintaining competitive empirical performance.

## Method Summary
The paper introduces two ensemble sampling algorithms: GLM-ES for generalized linear bandits and Neural-ES for neural contextual bandits. Both methods maintain an ensemble of m models trained via maximum likelihood estimation on rewards perturbed by independent Gaussian noise. In each round, all m models are updated with the new data point but each receives a distinct random perturbation. A model is sampled uniformly from the ensemble to select actions. Theoretical analysis provides high-probability regret bounds, and anytime versions are developed using the doubling trick. Empirical evaluation demonstrates competitive cumulative regret compared to baselines while maintaining computational efficiency.

## Key Results
- GLM-ES achieves regret bound O(d^{3/2}√T + d^{9/2}) for generalized linear bandits
- Neural-ES achieves regret bound O(√(edT)) for neural contextual bandits where ed is the effective dimension
- Anytime versions maintain asymptotic rates up to a constant factor without requiring fixed horizon knowledge
- Empirical results show competitive performance with state-of-the-art algorithms while being computationally efficient

## Why This Works (Mechanism)

### Mechanism 1: Diversity via Perturbed Gradients
Maintaining an ensemble of models trained on rewards perturbed by independent Gaussian noise may approximate posterior sampling, inducing exploration in high-uncertainty regions without calculating confidence bounds. In each round, all m models are updated with the new data point but each receives a distinct random perturbation. Where data is sparse, the gradients from perturbations dominate, causing models to diverge. Where data is dense, the likelihood overwhelms noise, causing consensus. Sampling a model uniformly effectively samples from an approximation of the posterior. The ensemble size m scales logarithmically with horizon T (specifically m = Ω(K log T)) and perturbations are i.i.d. Gaussian. If m is too small or perturbation variance σR is misscaled, the optimism probability drops below the constant required for sub-linear regret.

### Mechanism 2: Linearization of Neural Rewards (NTK)
Neural-ES achieves tractable regret bounds by approximating the deep network as a linear model under the Neural Tangent Kernel (NTK) regime. If the network is sufficiently wide, the change in weights during training is small. The network function can be approximated by its first-order Taylor expansion around initialization: f(X; θ) ≈ ⟨g(X; θ₀), θ⟩. This maps the neural problem to a linear contextual bandit problem in the gradient feature space. The network width N must be large and the learning rate η must be small. If the network is too narrow or trained aggressively, the linear approximation fails and the concentration bounds likely do not hold.

### Mechanism 3: Anytime Regret via Geometric Restarting
Removing the dependence on known horizon T is achieved by restarting the algorithm at geometric intervals (T_i = ⌊T₀b^i⌋), preserving asymptotic regret rates up to a constant factor. Instead of tuning hyperparameters for a fixed T, the algorithm runs in epochs. At the start of each epoch, hyperparameters are reset based on the epoch length. This prevents the horizon dependence failure mode where an algorithm tuned for T=1M explores too cautiously if the true horizon is T=10k. The base algorithm's regret must scale as O(T^γ(log T)^δ) with γ < 1. If the ratio b is set too close to 1, the overhead of frequent restarts dominates; if b is too large, the algorithm suffers from stale hyperparameters for too long.

## Foundational Learning

- **Concept:** Generalized Linear Models (GLM) & Self-Concordance
  - **Why needed here:** GLM-ES requires bounding the error of the link function derivative. Self-concordance provides the mathematical tool to control Hessian variations, which is critical for proving the concentration of the perturbed estimator θ_t^j.
  - **Quick check question:** Can you explain why a link function with unbounded derivatives would break the regret bound in Theorem 5.5?

- **Concept:** Neural Tangent Kernel (NTK)
  - **Why needed here:** Neural-ES relies on the NTK to bound the difference between the neural network output and its linear approximation ⟨g(X; θ₀), θ⟩. Without this, the theoretical analysis would be intractable.
  - **Quick check question:** As network width N → ∞, what does the NTK matrix H converge to, and how does this affect the "effective dimension" d̃?

- **Concept:** Doubling Trick (Geometric Restarts)
  - **Why needed here:** To make the algorithm "anytime." You need to understand how to partition time into epochs where the algorithm behaves as if it knows the horizon.
  - **Quick check question:** Why does the regret bound only degrade by a constant factor (approx 3.3x) when using the doubling trick, rather than accumulating unbounded error?

## Architecture Onboarding

- **Component map:** Ensemble Manager -> Perturbation Sampler -> Action Selector -> Model Trainer
- **Critical path:** 
  1. Initialization: Run Warm-up (G-optimal design for GLM, round-robin for Neural) to build initial covariance
  2. Inference: Sample one model -> Predict rewards for all arms -> Pull arm
  3. Update: Observe reward -> Generate m perturbations -> Update m models (can be parallelized)
- **Design tradeoffs:**
  - Memory vs. Compute: Unlike Perturbed-History Exploration (PHE), which resamples all history perturbations every round (O(t) compute), Ensemble-ES stores m models (O(md) memory). This trades memory for O(1) per-step compute.
  - Warm-up Cost: GLM-ES requires O(d^{9/2}) warm-up rounds to guarantee optimism. This is theoretically improved from O(d^9) but still expensive in high dimensions.
- **Failure signatures:**
  - Linear Regret: If ensemble size m is too small, the algorithm fails to maintain optimism, leading to linear regret (converging to suboptimal arms).
  - Gradient Explosion (Neural): If learning rate η is too high relative to width N, the NTK approximation breaks, causing unstable updates.
- **First 3 experiments:**
  1. Sanity Check (Linear/Logistic): Replicate Figure 1(a) and 1(b). Verify that Ensemble-ES tracks LinTS/GLM-TSL but with stable compute time. Check if regret drops below baselines at T=10k.
  2. Ablation on Ensemble Size m: Run GLM-ES with m ∈ {2, 5, 10, 25}. Confirm the "threshold" behavior where regret transitions from linear to sub-linear as m crosses the theoretical lower bound.
  3. Anytime Stress Test: Compare Fixed-ES vs. Anytime-ES on a horizon T much larger than the fixed-tuning parameter. Verify that Anytime-ES continues to explore effectively while Fixed-ES stagnates.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the warm-up complexity for GLM-ES be reduced further?
  - **Basis in paper:** The paper states in Remark 5.6 that the number of warm-up rounds is optimized from d^9 to d^{9/2}, but this remains a dominant polynomial term.
  - **Why unresolved:** The analysis in Appendix B.5 relies on a specific condition for the "sufficient condition of optimism" to have a solution, forcing the warm-up duration τ to scale as d^{9/2}. It is unproven whether this is a fundamental requirement or an artifact of the specific perturbation/growth analysis.
  - **What evidence would resolve it:** A proof of a lower bound requiring this warm-up duration, or a modified algorithm/analysis achieving a warm-up linear in d.

- **Open Question 2:** Can ensemble sampling regret bounds be established for nonlinear bandits with infinite action sets?
  - **Basis in paper:** Section 3 explicitly restricts the problem setting to "finite with K arms," while Section 2 (Related Work) notes that recent theoretical advances (Janz et al., 2024a) have addressed infinite arms for linear bandits.
  - **Why unresolved:** The theoretical proofs for the nonlinear setting (particularly Lemma B.4 regarding equivalent permutations and the counting of sequences) likely rely on the finiteness of the action space to bound failure probabilities.
  - **What evidence would resolve it:** A regret bound dependent on the covering number or dimension of the context space rather than the cardinality of the arm set.

- **Open Question 3:** Is it possible to design an anytime ensemble sampling algorithm that avoids the constant factor loss and hard resets of the doubling trick?
  - **Basis in paper:** Section 6 extends the algorithms to anytime versions using the doubling trick, which involves fully restarting the algorithm at specific intervals and incurs a constant factor loss (approx. 3.3) in the regret bound.
  - **Why unresolved:** The ensemble size and perturbation variance scale with the time horizon T. The doubling trick is a "black box" solution; constructing an algorithm that adapts these parameters smoothly online without discarding history remains an open challenge.
  - **What evidence would resolve it:** An algorithm design that dynamically adjusts hyperparameters (like ensemble size m) without resetting the model states, achieving the original asymptotic bound without the multiplicative constant.

## Limitations

- Theoretical analysis depends critically on ensemble size scaling as m = Ω(K log T) for optimism, but experiments use a fixed m=10 without sensitivity analysis.
- Warm-up phase for GLM-ES requires O(d^{9/2}) rounds, which becomes prohibitive for high-dimensional problems.
- Neural tangent kernel approximation in Neural-ES assumes very wide networks, yet experiments use width 20, creating a potential gap between theory and practice.
- Hyperparameter tuning (perturbation scale σR, regularization λ) is not extensively explored.

## Confidence

- **High confidence:** The ensemble sampling mechanism (perturbed rewards → model diversity → exploration) is well-established in the literature and the algorithmic framework is clearly specified. The doubling trick for anytime regret follows standard techniques with proper theoretical justification.
- **Medium confidence:** The GLM regret bounds leverage self-concordance properties appropriately, but the gap between O(d^{3/2}√T + d^{9/2}) and the claimed O(d^{3/2}√T) is not fully explained. The neural network analysis relies on NTK approximations that may not hold for the narrow networks used in experiments.
- **Low confidence:** The empirical evaluation shows competitive performance but lacks ablation studies on key parameters like ensemble size m and perturbation scale σR. The comparison with state-of-the-art baselines could be more comprehensive.

## Next Checks

1. **Ensemble size sensitivity:** Run GLM-ES with varying m ∈ {2, 5, 10, 25} to identify the threshold where regret transitions from linear to sub-linear behavior, validating the theoretical lower bound requirement.

2. **Neural width scalability:** Increase neural network width from 20 to 100 and 500 to test whether performance improvements align with NTK theory predictions as width increases.

3. **Anytime vs fixed-horizon comparison:** Design an experiment where the true horizon significantly differs from the fixed-tuning parameter (e.g., fixed-tuned for T=100k, test on T=10k) to demonstrate Anytime-ES's robustness advantage.