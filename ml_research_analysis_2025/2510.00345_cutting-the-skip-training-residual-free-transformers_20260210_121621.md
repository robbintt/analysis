---
ver: rpa2
title: 'Cutting the Skip: Training Residual-Free Transformers'
arxiv_id: '2510.00345'
source_url: https://arxiv.org/abs/2510.00345
tags:
- skipless
- skip
- training
- initialization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training transformers without
  skip connections, which are essential for stability but disrupt hierarchical feature
  learning. The authors analyze the Jacobian of a skipless transformer and show that
  skip connections improve conditioning.
---

# Cutting the Skip: Training Residual-Free Transformers

## Quick Facts
- arXiv ID: 2510.00345
- Source URL: https://arxiv.org/abs/2510.00345
- Reference count: 22
- The paper demonstrates stable training of skipless transformers using principled weight initialization that recovers conditioning benefits without architectural changes

## Executive Summary
Transformers typically rely on skip connections for stable training, but these connections disrupt hierarchical feature learning by adding features across scales. This paper analyzes the Jacobian of skipless transformers and shows that skip connections improve conditioning. The authors propose a principled initialization scheme that combines scaled orthonormal weights for the value-weight product with diagonal-dominant initialization for the query-key product, enabling stable training of skipless vision transformers. This approach allows first-of-its-kind studies of truly deep skipless ViTs while maintaining competitive performance with standard residual models.

## Method Summary
The core innovation involves a principled initialization scheme for skipless transformers that recovers the conditioning benefits of skip connections without architectural changes. The method combines two initialization strategies: scaled orthonormal initialization for the WV·WO term to ensure stable forward propagation, and diagonal-dominant initialization for the WQ·WK⊤ term to stabilize gradient flow. This dual approach addresses the conditioning issues identified in the Jacobian analysis, enabling stable training of transformers without skip connections. The initialization is designed to be plug-and-play, requiring no architectural modifications while maintaining the benefits of skipless design for hierarchical feature learning.

## Key Results
- Skipless ViTs with the proposed initialization converge as fast as standard residual models and match their accuracy (80.8% on ImageNet-1k)
- In self-supervised learning, skipless models outperform skip-based counterparts on dense prediction tasks and object discovery
- PCA visualization confirms clearer hierarchical representations in skipless models compared to skip-based architectures

## Why This Works (Mechanism)
Skip connections improve conditioning by adding a scaled identity matrix to the Jacobian, preventing ill-conditioning that would otherwise cause gradient explosion or vanishing. The proposed initialization scheme mimics this effect through careful weight initialization: scaled orthonormal weights for WV·WO stabilize forward propagation by controlling the spectral norm, while diagonal-dominant initialization for WQ·WK⊤ ensures stable gradients by maintaining appropriate conditioning in the attention mechanism. This approach preserves the benefits of skip connections for training stability while allowing skipless architectures to learn true hierarchical representations without cross-scale interference.

## Foundational Learning
- **Jacobian conditioning**: Measures how sensitive a network's output is to input perturbations; critical because poor conditioning causes exploding/vanishing gradients
  - Quick check: Compute condition number of Jacobian matrix during training
- **Orthogonal initialization**: Ensures weights preserve norm through layers; needed to prevent signal amplification or decay
  - Quick check: Verify singular values of weight matrices remain close to 1
- **Diagonal dominance**: Property where diagonal elements dominate row sums; ensures stable matrix inversion and conditioning
  - Quick check: Measure ratio of diagonal to off-diagonal elements in weight matrices
- **Hierarchical feature learning**: Progressive abstraction of features across layers; skip connections interfere by mixing scales
  - Quick check: Analyze feature maps at different depths for increasing abstraction
- **Attention mechanism conditioning**: The query-key product must be well-conditioned for stable gradient flow
  - Quick check: Monitor gradient norms through attention layers during training
- **Residual learning**: Skip connections enable training very deep networks but at cost of cross-scale interference
  - Quick check: Compare feature representations with/without skip connections at same depth

## Architecture Onboarding

**Component Map**: Input -> Patch Embedding -> Skipless Transformer Blocks (Multi-Head Attention + Feed-Forward) -> Classification Head

**Critical Path**: Patch embedding → Multi-head attention (with WQ, WK, WV, WO) → Feed-forward network → Classification head

**Design Tradeoffs**: Skipless architecture enables cleaner hierarchical feature learning but requires careful initialization for stable training; residual models train easily but mix feature scales across layers

**Failure Signatures**: Training instability (gradient explosion/vanishing), poor convergence, or degraded performance compared to residual baselines indicate improper initialization or conditioning issues

**First Experiments**: 1) Train skipless ViT on CIFAR-10 with proposed initialization to verify basic stability, 2) Compare feature representations at different depths with/without skips to confirm hierarchical learning, 3) Evaluate training curves and final accuracy on ImageNet-1k to benchmark against residual models

## Open Questions the Paper Calls Out
The paper identifies several open questions: whether the initialization scheme generalizes to other transformer architectures beyond ViTs, how the approach scales to larger models and datasets, and whether the conditioning analysis holds for different training regimes and objectives. The authors also note that the theoretical analysis relies on simplifying assumptions about weight distributions that may not hold in practice, particularly for later layers or alternative initialization schemes.

## Limitations
- Analysis focuses primarily on ViTs and may not generalize to other transformer architectures or modalities
- Conditioning analysis relies on simplifying assumptions about weight distributions that may not hold in practice
- Comparative evaluations are limited to specific benchmarks (ImageNet-1k, COCO, PASCAL) and training regimes
- No exploration of scaling behavior to larger models or datasets

## Confidence
High: Initialization scheme effectively enables stable training of skipless ViTs on standard supervised learning tasks, matching baseline accuracy
Medium: Self-supervised learning improvements shown but on narrower set of tasks and need further validation
Low: Generality to other transformer architectures, modalities, and extreme scaling scenarios remains unproven

## Next Checks
1. Test the initialization scheme on transformer architectures outside computer vision, such as language models or multimodal transformers, to assess cross-domain applicability
2. Evaluate scaling behavior by applying the method to larger ViT variants (e.g., ViT-Huge) and measuring performance degradation or training stability compared to residual models
3. Conduct ablation studies removing either the WV·WO initialization or the WQ·WK⊤ initialization independently to quantify their individual contributions to training stability and final performance