---
ver: rpa2
title: Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling
  under Uncertainty
arxiv_id: '2512.04918'
source_url: https://arxiv.org/abs/2512.04918
tags:
- policy
- overtime
- marl
- scheduling
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles intraday operating room (OR) scheduling under
  uncertainty, balancing elective throughput, urgent and emergency demand, delays,
  sequence-dependent setups, and overtime. A multi-agent reinforcement learning (MARL)
  framework is proposed where each OR is an agent trained via Proximal Policy Optimization
  (PPO) with centralized training and decentralized execution.
---

# Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty

## Quick Facts
- arXiv ID: 2512.04918
- Source URL: https://arxiv.org/abs/2512.04918
- Reference count: 7
- Primary result: MARL framework with PPO achieves better efficiency and reduced waiting times than six rule-based heuristics in realistic OR simulations

## Executive Summary
This study develops a multi-agent reinforcement learning framework to tackle the complex problem of intraday operating room (OR) scheduling under uncertainty. The approach balances competing objectives including elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. Using a centralized training and decentralized execution architecture, the framework trains OR agents via Proximal Policy Optimization to construct conflict-free joint schedules through a within-epoch sequential assignment protocol. When tested in realistic simulations with six ORs, eight surgery types, and random urgent/emergency arrivals, the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, achieving better overall efficiency, reduced waiting times, and maintained throughput while revealing interpretable scheduling behaviors.

## Method Summary
The method employs a cooperative Markov game formulation where each OR is an agent trained via Proximal Policy Optimization with centralized training and decentralized execution. A within-epoch sequential assignment protocol allows agents to act in order, observing state changes from earlier agents to construct conflict-free joint schedules without expanding the action space exponentially. The framework uses a unified scalar reward function incorporating type-specific quadratic delay penalties and a terminal overtime penalty to capture throughput, timeliness, and staff workload. The environment simulates realistic OR operations with stochastic arrivals (urgent cases via Poisson process, emergencies via Bernoulli events) and Gamma-distributed surgery durations, while maintaining sequence-dependent setup times between different surgery types.

## Key Results
- MARL policy outperforms six rule-based heuristics across seven performance metrics
- Achieves 40% improvement over the best rule-based heuristic (Rule H) in overall efficiency
- Reduces average patient waiting time while maintaining high throughput across evaluation subsets
- Reveals interpretable behaviors including batching similar cases, prioritizing emergencies, and balancing load across ORs

## Why This Works (Mechanism)

### Mechanism 1: Sequential Conflict Resolution via Within-Epoch Assignment
The sequential assignment protocol reduces combinatorial complexity by having OR agents act in sequence rather than selecting joint actions simultaneously. Later agents observe state changes induced by earlier agents, ensuring conflict-free schedules without requiring an intractable joint action dictionary. This approach relies on the system dynamics satisfying "weak coupling" assumptions where marginal gains are largely independent of simultaneous actions by other agents.

### Mechanism 2: Unified Scalar Reward for Multi-Objective Trade-offs
A single scalar reward function enables the RL agent to learn dynamic trade-offs between throughput, patient waiting times, and staff overtime. The reward combines linear utility for throughput with quadratic penalties for delays (capturing increasing marginal disutility over time) and terminal penalties for overtime. This unified formulation allows the agent to balance typically conflicting objectives through a single optimization process.

### Mechanism 3: Centralized Training with Decentralized Execution (CTDE)
CTDE facilitates learning of cooperative behaviors like load balancing across ORs by leveraging global information during training while maintaining scalability for real-time execution. The global critic evaluates actions based on full system state while the shared actor learns to act based on local observations augmented with sequential information, enabling implicit coordination without explicit communication overhead.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) & Markov Games**
  - **Why needed here:** The scheduling problem is formulated as a finite-horizon Markov game requiring understanding of state transitions and Bellman equations for value estimation
  - **Quick check question:** Can you identify the "State" and "Action" variables in Section 2, and explain why surgery duration uncertainty requires a stochastic policy?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** PPO is the specific algorithm used to train the policy, preventing drastic policy changes during updates for stability in complex environments
  - **Quick check question:** What role does the "clipped surrogate objective" play in preventing the policy from collapsing during training?

- **Concept: Queueing Theory & Scheduling**
  - **Why needed here:** The problem involves stochastic arrivals and service times, requiring understanding of the trade-off between idle time and waiting time for reward function tuning
  - **Quick check question:** Why does the reward function use a *quadratic* penalty for waiting time rather than a linear one?

## Architecture Onboarding

- **Component map:** Environment -> Agent Wrapper -> Coordinator -> Learner (PPO)
- **Critical path:** 1) Pre-scheduling: Run MIP to generate elective reference times 2) Rollout: Environment steps; Coordinator cycles through ORs sequentially; Agents sample actions; Env returns reward 3) Update: Store trajectories in buffer; PPO updates networks using Generalized Advantage Estimation
- **Design tradeoffs:** Sequential vs. Joint Action (trading optimality for tractability); Homogeneity vs. Heterogeneity (paper assumes homogeneous ORs for simplicity)
- **Failure signatures:** Deadlock/Starvation (policy avoids overtime so aggressively it idles while patients queue); Queue Instability (quadratic penalties insufficient to counter throughput utility); Cycling (policy repeatedly switches between surgery types due to setup time mismanagement)
- **First 3 experiments:** 1) Overfit Sanity Check: Train on deterministic instance to verify perfect MIP match 2) Ablation on Reward: Compare linear vs. quadratic delay penalties on waiting time distributions 3) Regret Validation: Compare MARL performance against Ex post MIP Oracle to measure suboptimality gap

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes homogeneous ORs without explicit modeling of OR-specific capabilities or constraints
- Omits explicit staffing constraints and resource limitations beyond OR availability
- Missing critical numerical parameters including network architecture details and setup time matrices

## Confidence

| Claim | Confidence Level |
|---|---|
| Core MARL framework (CTDE with PPO, sequential assignment) is well-defined | High |
| General reward structure captures multi-objective trade-offs | Medium |
| Specific numerical performance claims are verifiable | Low |

## Next Checks

1. **Resolve Ambiguity:** Implement and test both interpretations of emergency arrival probability (5% per slot vs. 0.40 per day) to determine which yields stated performance
2. **Parameter Sensitivity:** Conduct ablation study on missing setup time matrix values to quantify impact on policy performance and identify most sensitive surgery type transitions
3. **Hyperparameter Sweep:** Perform systematic sweep of missing PPO hyperparameters (clip ratio, learning rate, GAE lambda) to identify stable training configuration and establish baseline for final model