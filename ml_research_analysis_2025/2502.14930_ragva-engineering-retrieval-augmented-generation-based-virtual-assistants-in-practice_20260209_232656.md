---
ver: rpa2
title: 'RAGVA: Engineering Retrieval Augmented Generation-based Virtual Assistants
  in Practice'
arxiv_id: '2502.14930'
source_url: https://arxiv.org/abs/2502.14930
tags:
- customer
- arxiv
- systems
- engineering
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a step-by-step guide for engineering a Retrieval-Augmented
  Generation (RAG)-based virtual assistant (RAGVA), drawing from real-world experience
  at Transurban. The authors identified eight key challenges in developing RAG-based
  applications, including multi-modal data engineering, adaptive security guardrails,
  LLM version management, balancing response relevance and conciseness, automated
  testing, evaluation metrics, human feedback incorporation, and Responsible AI.
---

# RAGVA: Engineering Retrieval Augmented Generation-based Virtual Assistants in Practice

## Quick Facts
- arXiv ID: 2502.14930
- Source URL: https://arxiv.org/abs/2502.14930
- Reference count: 40
- Primary result: Eight key challenges in RAG-based virtual assistant development identified from Transurban case study, with proposed research directions

## Executive Summary
This paper presents a step-by-step guide for engineering Retrieval-Augmented Generation (RAG)-based virtual assistants (RAGVA), drawing from real-world experience at Transurban. The authors identify eight key challenges in developing RAG-based applications, including multi-modal data engineering, adaptive security guardrails, LLM version management, balancing response relevance and conciseness, automated testing, evaluation metrics, human feedback incorporation, and Responsible AI. For each challenge, they propose concrete research directions. The study contributes foundational understanding of RAG-based conversational applications and outlines emerging AI software engineering challenges through practitioner insights.

## Method Summary
The study employed a focus group methodology with nine practitioners from Transurban, Australia, to identify challenges and research directions for RAG-based virtual assistants. The approach involved three stages: (1) Data Ingestion—collecting documents, applying data engineering, chunking, embedding, and storing in vector databases; (2) Response Generation—filtering inputs through guardrails, embedding queries, retrieving relevant chunks, augmenting prompts, generating responses via LLM, and applying output guardrails; (3) Evaluation—creating test cases, running automated evaluation with tools like RAGAS or DeepEval, incorporating human feedback, and refining the system.

## Key Results
- Eight key challenges identified: multi-modal data engineering, adaptive security guardrails, LLM version management, balancing response relevance and conciseness, automated testing, evaluation metrics, human feedback incorporation, and Responsible AI
- RAGVA framework proposed with three-stage pipeline for data ingestion, response generation, and evaluation
- Focus group with nine Transurban practitioners provided insights into practical implementation issues
- Emerging AI software engineering challenges outlined for RAG-based conversational applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves response accuracy by grounding LLM outputs in domain-specific documents rather than relying solely on pre-trained knowledge.
- Mechanism: User queries are embedded into vectors, semantically matched against a pre-indexed knowledge base, and relevant document chunks are injected into the LLM prompt as context before generation.
- Core assumption: The embedding model captures semantic similarity between queries and document chunks well enough that retrieved context is genuinely relevant.
- Evidence anchors:
  - [abstract] "These systems excel at combining retrieval mechanisms with generative capabilities, resulting in more accurate, contextually relevant responses that enhance user experience."
  - [section 3.2] "In Step 2d, the embeddings are utilized in the prompt augmentation process, where additional context is retrieved from the vector store... These retrieved chunks are then combined with the user query to form an enriched prompt."
  - [corpus] Weak direct validation—neighbor papers focus on RAG applications but do not provide comparative benchmarks proving RAG outperforms non-RAG approaches for virtual assistants.
- Break condition: Retrieved chunks are irrelevant to the query (poor semantic matching), causing the LLM to generate responses based on wrong or noisy context.

### Mechanism 2
- Claim: Document chunking with metadata tagging enables efficient retrieval of self-contained, contextually meaningful units rather than arbitrary text segments.
- Mechanism: Raw documents are structurally segmented (e.g., by FAQ sections, headings), enriched with metadata tags (Content type, Keywords), then embedded and indexed—allowing retrieval to return coherent, domain-relevant chunks.
- Core assumption: Chunk boundaries align with semantic units that can answer user queries independently without requiring surrounding context.
- Evidence anchors:
  - [section 3.1] "Each chunk represents a self-contained and retrievable unit... Metadata tags were added to the extracted content such as Content: Billing Support, Type: FAQ and Keywords: Toll Payment, Invoice, Deadlines."
  - [section 7.1] "Document chunking strategy can affect the performance of RAG systems, and should be considered carefully based on several criteria such as document type, content density, and the intended use case."
  - [corpus] Not explicitly validated in corpus; chunking strategies are discussed but optimal approaches remain an open research question.
- Break condition: Chunks are too large (retrieving noise), too small (missing context), or split mid-concept, degrading response quality.

### Mechanism 3
- Claim: Guardrail rules applied pre- and post-generation mitigate security risks and content policy violations without requiring LLM retraining.
- Mechanism: User inputs pass through rule-based filters to sanitize prompts (masking PII, blocking injection attacks), and LLM outputs pass through classifiers (e.g., LlamaGuard, NeMo Guardrails) to detect unsafe content before display.
- Core assumption: Rule-based and classifier-based guardrails can sufficiently cover the space of malicious inputs and harmful outputs without excessive false positives.
- Evidence anchors:
  - [section 3.2] "In Step 2b, this input is immediately passed through a set of guardrail rules designed to filter and sanitize the prompt... Sensitive information such as personal details and billing details are also masked and anonymized."
  - [section 5, Challenge 2] "Prior works have proposed security guardrails to mitigate security exposure such as prompt injection attacks and sensitive data breaches... they typically rely on rule-based systems with pre-defined guardrails... often lack adaptiveness and flexibility."
  - [corpus] Neighbor paper on medical RAG mentions similar guardrail needs but provides no comparative effectiveness data.
- Break condition: Novel attack patterns bypass static guardrails, or guardrails are overly aggressive and block legitimate customer queries.

## Foundational Learning

- Concept: **Vector embeddings and semantic similarity**
  - Why needed here: Understanding how text becomes numerical representations that enable "meaning-based" retrieval rather than keyword matching is essential for debugging poor retrievals.
  - Quick check question: Can you explain why two sentences with no shared words might have high cosine similarity in an embedding space?

- Concept: **Prompt augmentation / context injection**
  - Why needed here: The core RAG pattern relies on constructing prompts that combine user queries with retrieved context—understanding token limits and context window management is critical.
  - Quick check question: What happens to response quality if you inject 10 retrieved chunks that exceed the LLM's context window?

- Concept: **LLM non-determinism and evaluation challenges**
  - Why needed here: RAG systems inherit LLM unpredictability; traditional software testing approaches (exact match oracles) do not apply.
  - Quick check question: Why can't you use a simple string equality check to validate RAG responses, and what alternatives exist?

## Architecture Onboarding

- Component map:
  - Data Ingestion Pipeline: Document collection → Data engineering (structuring, metadata) → Chunking → Embedding model → Vector store
  - Response Generation Pipeline: User input → Input guardrails → Query embedding → Semantic search → Prompt augmentation → LLM → Output guardrails → Response
  - Evaluation Pipeline: Test cases → Automated evaluation tools (DeepEval, RAGAS) → Metrics report → Human feedback → System refinement

- Critical path:
  1. Start with document quality—garbage in, garbage out applies strongly here
  2. Chunking strategy determines retrieval granularity
  3. Embedding model selection gates semantic matching quality
  4. Prompt construction (how context + query are combined) directly affects LLM behavior
  5. Evaluation metrics determine what "good" means for your domain

- Design tradeoffs:
  - Chunk size: Smaller = more precise retrieval but may lose context; larger = more context but more noise
  - Temperature setting: Low = more deterministic but potentially terse; high = more varied but risk of verbosity or hallucination (paper notes limited impact from temperature alone)
  - Guardrail strictness: Tighter = safer but may block valid queries; looser = more flexible but higher risk
  - Evaluation depth: Automated metrics are fast but may miss nuanced failures; human evaluation is thorough but doesn't scale

- Failure signatures:
  - Retrieval failure: Retrieved chunks don't contain information needed to answer the query (check embedding quality, chunk boundaries)
  - Hallucination: LLM generates facts not grounded in retrieved context (check prompt formatting, reduce temperature, improve context relevance)
  - Verbosity/terseness imbalance: Responses too long or too short (hyperparameter tuning has limited effect—explore prompt instructions, top-k/top-p sampling)
  - Guardrail bypass: Malicious inputs reach the LLM or harmful outputs reach users (update rules, add classifier-based guardrails)

- First 3 experiments:
  1. Chunk size sweep: Test fixed chunk sizes (128, 256, 512 tokens) against a held-out set of 20-50 domain questions; measure retrieval precision@k and response quality ratings.
  2. Embedding model comparison: Benchmark 2-3 embedding models (e.g., OpenAI embeddings vs. domain-fine-tuned BERT) on semantic similarity tasks using your actual document corpus; measure MRR and latency.
  3. End-to-end evaluation baseline: Establish baseline metrics (faithfulness, answer relevancy, hallucination rate) using an automated tool like RAGAS or DeepEval on a curated test set before making any system changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we automatically generate test inputs and test oracles for RAG-based systems at scale?
- Basis in paper: [explicit] RQ5.1 in Table 3 asks, "How can we automatically generate test inputs and test oracle (including a benchmark dataset) for RAG-based systems at scale?"
- Why unresolved: Ground truth oracles are difficult to establish due to the non-deterministic nature of LLMs and the high manual effort required for domain-specific scenarios.
- What evidence would resolve it: An automated pipeline that generates high-coverage test cases with reliable expected outcomes verified against domain-specific requirements.

### Open Question 2
- Question: How can we efficiently identify the optimal combination of hyperparameters to balance relevancy and conciseness in RAG-generated responses?
- Basis in paper: [explicit] RQ4.1 in Table 3 asks, "How can we efficiently identify the optimal combination of hyperparameters to balance relevancy and conciseness in RAG-generated responses?"
- Why unresolved: The authors note that temperature adjustments alone have limited impact, and the effects of other parameters (e.g., top-k sampling) on this balance remain unexplored.
- What evidence would resolve it: A systematic methodology or tool that configures hyperparameters to produce responses satisfying user-defined thresholds for both relevancy and conciseness.

### Open Question 3
- Question: How can we develop adaptive guardrail frameworks that dynamically learn and adjust to new security threats in RAG systems?
- Basis in paper: [explicit] RQ2.1 in Table 3 asks, "How can we develop adaptive guardrail frameworks that dynamically learn and adjust to new security threats in RAG systems?"
- Why unresolved: Current guardrails typically rely on static, rule-based systems that lack the flexibility to handle evolving adversarial attacks or diverse scenarios.
- What evidence would resolve it: A framework capable of successfully detecting and mitigating novel prompt injection attacks in real-time without requiring manual rule updates.

## Limitations

- The framework is based on a single organizational case study at Transurban, limiting external validity across different domains and contexts
- Critical implementation details remain unspecified, including exact embedding models, chunk sizes, vector database choices, prompt templates, and guardrail rule definitions
- Guardrail effectiveness remains unproven, with the authors noting that static rule-based systems lack adaptiveness against novel attack patterns

## Confidence

- High confidence: The eight identified challenges represent real, widely-observed problems in RAG system engineering based on practitioner experience
- Medium confidence: The proposed research directions are reasonable extensions of current literature, though unproven in practice
- Low confidence: Specific technical solutions and their effectiveness (embedding models, chunk sizes, guardrail implementations) remain unspecified

## Next Checks

1. **Chunking strategy validation**: Implement a controlled experiment testing multiple chunk sizes (128, 256, 512 tokens) on a representative document corpus, measuring retrieval precision@k and response quality ratings across 20-50 domain-specific questions.

2. **Guardrail effectiveness testing**: Design a test suite containing both benign and adversarial inputs to systematically evaluate the false positive and false negative rates of rule-based guardrails versus classifier-based approaches like LlamaGuard.

3. **End-to-end evaluation baseline**: Establish baseline metrics (faithfulness, answer relevancy, hallucination rate) using RAGAS or DeepEval on a curated test set before implementing any system improvements, then track changes as modifications are made.