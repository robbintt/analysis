---
ver: rpa2
title: 'THOR: A Versatile Foundation Model for Earth Observation Climate and Society
  Applications'
arxiv_id: '2601.16011'
source_url: https://arxiv.org/abs/2601.16011
tags:
- thor
- patch
- data
- size
- land
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: THOR introduces a "compute-adaptive" foundation model for Earth
  observation that solves the dual challenges of input heterogeneity and deployment
  rigidity. It is the first architecture to unify Copernicus Sentinel-1, -2, and -3
  data across 10 m to 1000 m resolutions, processing heterogeneous sensor inputs in
  a single model.
---

# THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications

## Quick Facts
- **arXiv ID:** 2601.16011
- **Source URL:** https://arxiv.org/abs/2601.16011
- **Reference count:** 40
- **Primary result:** First architecture to unify Copernicus Sentinel-1, -2, and -3 data across 10 m to 1000 m resolutions with compute-adaptive inference

## Executive Summary
THOR introduces a groundbreaking foundation model for Earth observation that addresses two critical challenges in the field: input heterogeneity across multiple satellite sensors and deployment rigidity in computational requirements. The model unifies multi-resolution Sentinel data (10m-1000m) in a single architecture while enabling dynamic trade-offs between computational cost and feature resolution through randomized patch and input image size strategies during pre-training. This innovation allows a single set of pre-trained weights to be deployed at inference with any patch size, producing denser token sequences for fine-grained tasks or reducing compute for global-scale analysis. Pre-trained on a new 22TB multi-sensor dataset (THOR Pretrain), the model demonstrates state-of-the-art performance across downstream benchmarks and excels particularly in data-limited regimes.

## Method Summary
THOR employs a compute-adaptive architecture that leverages randomized patch sampling and resolution-flexible training strategies. During pre-training, the model randomly varies both patch sizes and input image dimensions, forcing it to learn features that are robust across different resolutions and computational constraints. This approach enables the model to generate adaptive feature maps during inference without requiring retraining for different deployment scenarios. The architecture is pre-trained on THOR Pretrain, a large-scale multi-sensor dataset comprising Sentinel-1, -2, and -3 data totaling 22TB. The model's ability to process heterogeneous sensor inputs in a unified framework represents a significant advancement over traditional approaches that require separate models or extensive fine-tuning for different sensor combinations and resolutions.

## Key Results
- Achieved state-of-the-art performance on downstream benchmarks, particularly excelling in data-limited regimes like PANGAEA 10% split
- Demonstrated superior data-efficiency with a single set of pre-trained weights deployable across varying patch sizes without retraining
- Successfully unified Copernicus Sentinel-1, -2, and -3 data across 10 m to 1000 m resolutions in a single architecture

## Why This Works (Mechanism)
THOR's effectiveness stems from its randomized training strategy that exposes the model to diverse patch sizes and resolutions during pre-training. This forces the network to develop scale-invariant features that generalize across different computational requirements and sensor configurations. The compute-adaptive inference mechanism allows the model to dynamically adjust token density based on deployment needs, enabling fine-grained analysis when resources permit or efficient global-scale processing when constrained. The unified multi-sensor architecture eliminates the need for separate models for different sensor types, reducing deployment complexity while maintaining high performance across all input modalities.

## Foundational Learning
- **Multi-sensor data fusion:** Combines radar (Sentinel-1), optical (Sentinel-2), and altimetry (Sentinel-3) data; needed for comprehensive Earth observation; quick check: verify each sensor's spectral/spatial characteristics are preserved
- **Scale-invariant feature learning:** Features robust across 10m-1000m resolution range; needed for heterogeneous sensor inputs; quick check: test feature consistency across resolution changes
- **Compute-adaptive inference:** Dynamic token generation based on patch size; needed for flexible deployment scenarios; quick check: measure accuracy vs. computational cost trade-offs
- **Foundation model pre-training:** Large-scale self-supervised learning on 22TB dataset; needed for strong downstream generalization; quick check: evaluate performance on out-of-distribution data
- **Randomized patch sampling:** Training with varying patch sizes; needed for resolution adaptability; quick check: assess performance degradation outside training range
- **Multi-task optimization:** Joint learning across diverse downstream applications; needed for versatile real-world deployment; quick check: benchmark across task diversity

## Architecture Onboarding

**Component Map:** Input sensors (Sentinel-1/-2/-3) -> Patch sampling module -> Vision Transformer backbone -> Multi-head attention -> Adaptive feature generation -> Downstream task heads

**Critical Path:** Multi-sensor input → Randomized patch sampling → Vision Transformer processing → Attention-based feature fusion → Adaptive token generation → Task-specific output

**Design Tradeoffs:** The randomized patch strategy enables flexibility but may sacrifice some specialization for specific resolutions; unified architecture reduces model count but increases complexity; large pre-training corpus improves generalization but requires substantial computational resources

**Failure Signatures:** Performance degradation on sensor types not well-represented in pre-training; reduced accuracy when patch sizes fall outside the 224-896 pixel training range; potential loss of fine-grained detail when aggressively reducing compute

**Three First Experiments:** 1) Test inference with varying patch sizes (224, 448, 896 pixels) on a single downstream task to verify adaptive behavior; 2) Evaluate cross-sensor generalization by testing Sentinel-2 trained model on Sentinel-1 data; 3) Measure compute-accuracy trade-offs by systematically varying patch sizes on semantic segmentation benchmark

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on sensor configurations outside the pre-training distribution remains untested
- Scaling behavior beyond the reported 224-896 pixel range is unexplored
- Temporal generalization across different acquisition periods not explicitly validated

## Confidence

**High confidence:** Multi-sensor unification capability, compute-adaptive inference mechanism, state-of-the-art benchmark performance on most tasks

**Medium confidence:** Claims of being first to achieve multi-resolution unification, data-efficiency advantages in few-shot regimes

**Low confidence:** Generalization to unseen sensor configurations beyond pre-training distribution, long-term stability of adaptive inference across diverse deployment scenarios

## Next Checks
1. Evaluate THOR's performance on temporally disjoint datasets to assess temporal generalization beyond spatial generalization
2. Test the compute-adaptive inference mechanism with patch sizes outside the 224-896 pixel training range to characterize breaking points
3. Conduct ablation studies comparing THOR against other multi-sensor approaches using identical training data and evaluation protocols to isolate architectural contributions from dataset effects