---
ver: rpa2
title: 'Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment
  Analysis'
arxiv_id: '2506.04574'
source_url: https://arxiv.org/abs/2506.04574
tags:
- reasoning
- positive
- financial
- prompting
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates how different large language models (LLMs)\
  \ and prompting strategies perform on zero-shot financial sentiment analysis using\
  \ the Financial PhraseBank dataset. The research compares reasoning-based and non-reasoning\
  \ models\u2014including GPT-4o, GPT-4.1, o3-mini, FinBERT-Prosus, and FinBERT-Tone\u2014\
  under four prompting paradigms mapped to dual-process cognition (System 1, System\
  \ 2, and intermediate modes)."
---

# Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis

## Quick Facts
- arXiv ID: 2506.04574
- Source URL: https://arxiv.org/abs/2506.04574
- Authors: Dimitris Vamvourellis; Dhagash Mehta
- Reference count: 40
- Primary result: No-CoT prompting outperforms reasoning-based approaches for financial sentiment analysis, with reasoning-optimized models showing systematic biases.

## Executive Summary
This study challenges the assumption that more reasoning improves LLM performance by showing that direct label prediction without chain-of-thought (No-CoT) achieves higher alignment with human sentiment annotations than reasoning-based approaches. The research evaluates five models (GPT-4o, GPT-4.1, o3-mini, FinBERT-Prosus, FinBERT-Tone) under four prompting paradigms mapped to dual-process cognition. Results show that reasoning-optimized models like o3-mini exhibit systematic positivity biases, while extended reasoning chains often degrade performance, particularly in low-ambiguity cases. The findings suggest that System 1-style intuition aligns more closely with human judgment than deliberative reasoning for subjective financial sentiment tasks.

## Method Summary
The study evaluates zero-shot financial sentiment classification on the Financial PhraseBank dataset using five models under four prompting paradigms: No-CoT (direct prediction), CoT-Short, CoT-Long (reasoning followed by label), and LIRA (label followed by reasoning). Models are assessed using macro F1 score with class-balanced evaluation due to label imbalance (59.4% neutral, 28.1% positive, 12.4% negative). The analysis stratifies performance by inter-annotator agreement levels and Flesch-Kincaid readability scores, while also tracking completion token counts to examine the relationship between reasoning verbosity and accuracy.

## Key Results
- No-CoT prompting with GPT-4o achieved the highest human alignment for financial sentiment classification.
- Reasoning-optimized models like o3-mini showed systematic positivity biases, misclassifying 22-26% of neutral cases as positive.
- Extended reasoning chains (CoT-Long) degraded performance, with token count negatively correlated with macro-F1.
- LIRA prompting outperformed forward-chained CoT by reversing the reasoning order.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct label prediction without intermediate reasoning produces higher alignment with human sentiment annotations than chain-of-thought approaches in subjective classification tasks.
- **Mechanism:** No-CoT prompting allows models to leverage learned pattern recognition capabilities without introducing potentially spurious reasoning traces. Extended reasoning chains can amplify model biases and encourage extrapolation beyond textual evidence, while direct prediction constrains outputs to surface-level patterns that more closely mirror human heuristic judgment.
- **Core assumption:** Human annotators apply fast, intuitive (System 1-like) judgments when labeling financial sentiment.
- **Evidence anchors:** Abstract states GPT-4o without CoT is most accurate and human-aligned; section 5 shows token count negatively correlated with macro-F1; related work on "overthinking" in reasoning models supports performance degradation.

### Mechanism 2
- **Claim:** The autoregressive ordering of reasoning tokens relative to label tokens causally affects prediction outcomes, with label-first generation (LIRA) outperforming reasoning-first generation (CoT).
- **Mechanism:** In autoregressive models, later tokens are conditioned on earlier tokens. When reasoning precedes labels (CoT), the label is conditioned on potentially verbose or extrapolated reasoning traces. When labels precede reasoning (LIRA), the model commits to a decision first, then generates post hoc justification.
- **Core assumption:** The relative positioning of reasoning and decision tokens influences model behavior due to the autoregressive nature of LLMs.
- **Evidence anchors:** Section 4.2 describes LIRA as prompting label then explanation; section 5 shows LIRA outperformed both CoT-Short and CoT-Long; related prompting work explores reasoning order effects in different contexts.

### Mechanism 3
- **Claim:** Reasoning-optimized model architectures (e.g., o3-mini) exhibit systematic biases that persist across prompting strategies, causing consistent misalignment with human sentiment judgments.
- **Mechanism:** Models trained with reinforcement learning for reasoning capability develop inductive biases toward extended deliberation, which may not align with subjective human judgment patterns. These models may also inherit training distribution biases that manifest regardless of prompt structure.
- **Core assumption:** Reasoning optimization creates architectural tendencies that persist even when prompted to avoid explicit reasoning.
- **Evidence anchors:** Section 5 shows o3-mini yields lowest performance overall despite being optimized for internal reasoning; section 6.1 documents 22-26% neutral-to-positive misclassification rates; related work shows reasoning optimization benefits logic-heavy tasks but corpus evidence for negative effects in subjective tasks remains sparse.

## Foundational Learning

- **Concept: Dual-Process Theory (System 1 vs. System 2 cognition)**
  - Why needed here: The paper's analytical framework maps prompting strategies to cognitive modes; understanding this mapping is essential for interpreting results.
  - Quick check question: Which prompting strategy would you select for a task where human experts report relying on "gut feel" rather than explicit analysis?

- **Concept: Autoregressive Language Modeling and Token Conditioning**
  - Why needed here: The LIRA vs. CoT comparison hinges on understanding how earlier tokens condition later tokens in autoregressive generation.
  - Quick check question: If a model generates reasoning before a label, what implicit assumption does this make about the relationship between reasoning and the final decision?

- **Concept: Zero-Shot Evaluation and Distribution Shift**
  - Why needed here: The study deliberately uses zero-shot evaluation to isolate intrinsic reasoning tendencies; understanding this paradigm clarifies why FinBERT-Prosus serves only as a benchmark reference despite its high scores.
  - Quick check question: Why might a model fine-tuned on the test dataset show inflated performance compared to zero-shot LLMs, and how does this affect interpretation?

## Architecture Onboarding

- **Component map:** Financial PhraseBank sentences → Prompt templates (No-CoT, CoT-Short, CoT-Long, LIRA) → Model inference (GPT-4o, GPT-4.1, o3-mini, FinBERT-Prosus, FinBERT-Tone) → Token count logging → Label extraction → Macro F1 computation → Stratified analysis

- **Critical path:** Dataset preparation → Prompt construction → Zero-shot inference → Completion token logging → Label extraction → Macro F1 computation → Stratified analysis (by agreement level, readability, token quantiles)

- **Design tradeoffs:**
  - No-CoT maximizes human alignment but provides no explainability
  - LIRA balances alignment with post hoc explanations but may produce confabulated justifications
  - CoT variants provide pre-decision reasoning but degrade accuracy
  - Reasoning-optimized models (o3-mini) produce verbose outputs with lower alignment
  - Fine-tuned models (FinBERT) offer highest in-domain accuracy but poor out-of-domain generalization

- **Failure signatures:**
  - Positivity bias: Over-prediction of positive sentiment, especially in reasoning-optimized models (o3-mini misclassifies 22–26% of neutral cases as positive)
  - Overthinking: Performance degradation correlated with completion token count (Q5 < Q1 across all model-method combinations)
  - Prompt-induced variability: Same model producing different predictions across prompting strategies for identical inputs
  - Compositional reasoning failure: FinBERT models fail on negated positive constructions

- **First 3 experiments:**
  1. Replicate the No-CoT vs. CoT-Short comparison on a held-out financial sentiment dataset to validate the overthinking effect generalizes beyond Financial PhraseBank.
  2. Test LIRA vs. forward CoT on a task where human annotators explicitly document their reasoning process to determine whether post hoc rationalization better matches human explanations.
  3. Isolate the positivity bias in o3-mini by evaluating on synthetic financial sentences with controlled sentiment polarity to quantify the bias magnitude independent of dataset characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed performance degradation from reasoning extend to other subjective NLP tasks beyond financial sentiment analysis, such as emotion detection, opinion mining, or aesthetic judgment?
- **Basis in paper:** The authors state their results challenge the default assumption that more reasoning always leads to better LLM decisions and question whether similar patterns hold for tasks grounded in perception and intuition.
- **Why unresolved:** The study only examines financial sentiment on one dataset; generalizability to other subjective domains with different annotation protocols remains unknown.
- **What evidence would resolve it:** Replication across multiple subjective classification tasks showing consistent No-CoT superiority, or identification of task characteristics that predict when reasoning helps versus harms.

### Open Question 2
- **Question:** What causes the systematic positive-class bias observed in o3-mini, and is this bias present in other reasoning-optimized models?
- **Basis in paper:** The authors note that further investigation is needed to determine whether this systematic bias stems from training data composition, inductive biases introduced during pretraining, or specific architectural choices.
- **Why unresolved:** The paper documents o3-mini's misclassification rates but cannot isolate the cause from API-only access.
- **What evidence would resolve it:** Controlled experiments comparing reasoning-optimized models with transparent training data, or ablation studies isolating reasoning training procedures.

### Open Question 3
- **Question:** How does LLM-generated financial sentiment correlate with actual market outcomes compared to human-annotated sentiment labels?
- **Basis in paper:** The authors acknowledge that Financial PhraseBank labels reflect human-perceived sentiment, not actual financial outcomes, and that such an application would require temporally linked sentence-market pairs and backtesting.
- **Why unresolved:** Human annotators may systematically diverge from market reactions; a model less aligned with humans might paradoxically predict market movements better.
- **What evidence would resolve it:** Construction of a dataset with both human sentiment annotations and contemporaneous stock price movements, enabling comparison of human-aligned versus market-aligned model predictions.

## Limitations

- Zero-shot evaluation design cannot fully disentangle prompt effects from model architecture differences.
- Findings may not generalize to domains where reasoning is objectively necessary rather than subjectively interpretative.
- Absence of human-in-the-loop validation means we cannot definitively confirm that No-CoT predictions better reflect actual human judgment processes.

## Confidence

**High Confidence:** The empirical observation that No-CoT prompting outperforms reasoning-based approaches on this dataset is well-supported by the data and robust across agreement levels and readability categories.

**Medium Confidence:** The mechanism explaining why reasoning degrades performance (overthinking, confabulation, bias amplification) is plausible but not definitively proven. Alternative explanations include prompt-induced variability or dataset-specific characteristics.

**Low Confidence:** The broader claim that "System 1 intuition" universally aligns better with human judgment than "System 2 reasoning" for subjective classification tasks requires validation across multiple domains before generalization.

## Next Checks

1. **Cross-Domain Replication:** Test No-CoT vs. CoT performance on sentiment analysis datasets from non-financial domains (e.g., product reviews, social media) to determine if the overthinking effect is domain-specific or generalizes to subjective classification tasks.

2. **Human Judgment Validation:** Conduct a controlled experiment where human annotators classify financial sentences using both intuitive (time-constrained) and deliberative (think-aloud) approaches, then compare model alignment with each human judgment type to validate the System 1/System 2 mapping.

3. **Bias Isolation Experiment:** Evaluate o3-mini on synthetically generated financial sentences with controlled sentiment polarity and linguistic complexity to quantify positivity bias independently of dataset characteristics and determine whether this bias persists across prompting strategies.