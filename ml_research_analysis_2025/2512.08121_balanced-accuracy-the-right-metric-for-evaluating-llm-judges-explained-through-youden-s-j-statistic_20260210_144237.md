---
ver: rpa2
title: 'Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained
  through Youden''s J statistic'
arxiv_id: '2512.08121'
source_url: https://arxiv.org/abs/2512.08121
tags:
- judge
- accuracy
- balanced
- prevalence
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies the challenge of evaluating LLM judges for
  the downstream task of comparing model prevalence estimates. Common metrics like
  Accuracy, F1, and Precision are shown to be prevalence-dependent and label-asymmetric,
  potentially distorting judge selection and leading to unreliable model comparisons.
---

# Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic

## Quick Facts
- arXiv ID: 2512.08121
- Source URL: https://arxiv.org/abs/2512.08121
- Reference count: 15
- Primary result: Balanced Accuracy (Youden's J) is theoretically optimal for selecting LLM judges to compare model prevalence estimates because it is prevalence-independent and label-symmetric.

## Executive Summary
This paper identifies a critical challenge in evaluating LLM judges: traditional metrics like Accuracy, F1, and Precision are prevalence-dependent and label-asymmetric, potentially distorting judge selection and leading to unreliable model comparisons. The authors argue that Youden's J statistic and its equivalent transformation, Balanced Accuracy, are theoretically aligned with the goal of judge selection because they are independent of class prevalence and treat both classes symmetrically. Empirical studies on simulated and real-world datasets demonstrate that using Balanced Accuracy leads to selecting better judges, resulting in more accurate model rankings and reduced downstream error compared to traditional metrics.

## Method Summary
The paper employs Monte Carlo simulations to compare judge selection performance across different metrics. Each scenario samples 5 models with prevalence uniformly distributed between 0.01-0.5, and 3 judges with true positive rates (TPR) and false positive rates (FPR) uniformly distributed between 0 and 1. Judges are evaluated on a golden set (typically imbalanced) and applied to benchmark responses to estimate model prevalence. The primary metric is Balanced Accuracy = (TPR + TNR)/2, which equals Youden's J = TPR - FPR. Selection success is measured by whether the chosen judge matches the ground-truth rank-optimal judge, and downstream quality is measured by pairwise model-ranking accuracy.

## Key Results
- Balanced Accuracy outperforms Accuracy, F1, and Macro-F1 in selecting judges that preserve true prevalence differences between models.
- The advantage of Balanced Accuracy increases as the target behavior becomes rarer and class imbalance grows.
- Using Balanced Accuracy reduces downstream error in model rankings by 7-10 percentage points compared to Accuracy in rare-event detection scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Linear Filtering of Prevalence Differences
- Claim: A judge's ability to preserve true prevalence differences between models is directly proportional to Youden's J (TPR - FPR).
- Mechanism: The judge acts as a linear filter—measured prevalence y = TPR·x + FPR·(1−x), so the difference between two models scales by (TPR−FPR) = J. Higher J yields stronger, more reliable signals when comparing models.
- Core assumption: Judge error rates (TPR, FPR) remain stable across the assistant models being compared; model-specific biases are absent.
- Evidence anchors: [section 4.1]: "∆y = (TPR − FPR)·∆x... A judge with a higher J more faithfully preserves the magnitude of true prevalence differences and produces stronger, more reliable signals when comparing models."
- Break condition: If a judge exhibits model-specific bias (e.g., self-preference or family-preference), the linear-filter assumption fails regardless of metric choice.

### Mechanism 2: Prevalence Independence via Conditional Metrics
- Claim: Sensitivity and Specificity (and thus Balanced Accuracy) remain stable across datasets with different class ratios, enabling consistent judge selection.
- Mechanism: These metrics condition on true class membership (P(predicted=1|true=1)) rather than predictions, decoupling them from class distribution. Precision, by contrast, conditions on predicted positives and collapses when positives are rare.
- Core assumption: Conditional error rates are stable; class imbalance does not affect the judge's intrinsic classification behavior.
- Evidence anchors: [abstract]: "Balanced Accuracy... is independent of class prevalence"; [section 3.1]: "Sensitivity and Specificity measure conditional accuracy within each class and therefore remain stable across datasets with different class ratios."
- Break condition: If the golden set is so small that conditional rates have high variance, prevalence independence is theoretical but practically noisy.

### Mechanism 3: Label Symmetry Prevents Arbitrary Bias
- Claim: Balanced Accuracy treats both classes symmetrically, preventing arbitrary positive-class designation from distorting judge selection.
- Mechanism: Unlike Precision/Recall which privilege the positive class, Balanced Accuracy = (Recall_pos + Recall_neg)/2 weights both classes equally. Swapping labels swaps the two component recalls but leaves the average unchanged.
- Core assumption: Both classes are equally important for downstream prevalence estimation—neither false positives nor false negatives are asymmetrically costly.
- Evidence anchors: [section 3]: Lists "Label symmetry" as core criterion; [section 3.1]: "Precision... turns into Negative Predictive Value (NPV)" when labels flip—"[t]is asymmetry creates inconsistencies across datasets."
- Break condition: If downstream costs are asymmetric (e.g., safety teams prioritize high recall on violations), BA alone does not capture this priority.

## Foundational Learning

- Concept: **ROC Analysis and Youden's J**
  - Why needed here: Understanding the geometric interpretation—J is the vertical distance from an ROC point to the diagonal chance line—clarifies why it measures separability.
  - Quick check question: If a judge has TPR=0.8 and FPR=0.1, what is its Youden's J? (Answer: 0.7)

- Concept: **Prevalence Dependence vs. Independence**
  - Why needed here: Distinguishing why Precision/F1 fail in imbalanced settings while Sensitivity/Specificity do not is central to the paper's argument.
  - Quick check question: A classifier has Precision=0.9 on a dataset with 50% positives. Will Precision stay the same if positives drop to 1%? Why or why not?

- Concept: **Confusion Matrix Decomposition (TP, FP, TN, FN → TPR, TNR)**
  - Why needed here: Computing Balanced Accuracy requires extracting class-conditional rates from raw counts.
  - Quick check question: Given TP=63, FP=133, TN=784, FN=20, compute Sensitivity and Specificity. (Answers: 0.76, 0.855)

## Architecture Onboarding

- Component map:
  - Golden Set -> Judge -> Prevalence Estimate -> Model Ranking
  - (Prompt, Response, Gold_Label) tuples feed into judge evaluation
  - Binary classifier (LLM, fine-tuned model, or human) produces violation/safe predictions
  - Mean of binary labels estimates behavior prevalence across models

- Critical path:
  1. Collect golden set (aim for balance; accept imbalance given proper metric)
  2. Compute Balanced Accuracy = (TPR + TNR)/2 for each candidate judge
  3. Select highest-scoring judge
  4. Apply selected judge to benchmark responses
  5. Compare prevalence estimates across assistant models; validate rankings

- Design tradeoffs:
  - Golden set size: Diminishing returns after 1000-2000 labels; metric choice matters more than adding more labels
  - Prevalence regime: Balanced Accuracy's advantage over Accuracy/F1 grows as the target behavior becomes rarer
  - Multi-class extension: BA weights all classes equally; for long-tailed distributions, class reweighting may be needed

- Failure signatures:
  - Model-specific bias: Self-preference or family-preference effects distort prevalence estimates regardless of metric (orthogonal issue)
  - Volatile rare-class performance: In multi-class settings, sparse/noisy rare-class annotations can disproportionately influence BA
  - Misaligned operational needs: BA does not encode asymmetric costs; high-stakes domains require complementary inspection of raw TPR/FPR

- First 3 experiments:
  1. **Baseline metric comparison**: On your golden set, compute BA, Accuracy, F1, and Macro-F1 for all candidate judges; verify they produce different rankings and inspect divergence cases
  2. **Prevalence sensitivity test**: Subsample the golden set to simulate different class ratios (e.g., 5%, 20%, 50% prevalence); confirm BA remains stable while F1 and Accuracy fluctuate
  3. **Downstream ranking validation**: Apply multiple judges to a held-out benchmark; compare their model-ranking consistency against a ground-truth ranking (if available) or inter-judge agreement to validate simulation findings (BA success rate 75.2% vs. 67.5% for Accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model-specific judge bias (e.g., self-preference or family-preference effects) be detected and mitigated when judges evaluate outputs from related models?
- Basis in paper: [explicit] "If a judge exhibits model-specific bias... then prevalence estimates may be distorted regardless of the metric used. Detecting and mitigating this form of judge bias is an essential but orthogonal challenge that must accompany any metric-based judge selection."
- Why unresolved: The paper focuses on metric selection assuming stable TPR/FPR across models, but real-world judges may have systematic biases favoring certain model families.
- What evidence would resolve it: A methodology for quantifying model-specific bias across judge families, plus mitigation techniques validated through improved cross-model prevalence estimation accuracy.

### Open Question 2
- Question: What is the optimal class weighting scheme for Balanced Accuracy in multi-class settings with long-tailed label distributions?
- Basis in paper: [explicit] "In multi-class settings, Balanced Accuracy weights each class equally, which may be inappropriate for tasks with long-tailed label distributions. Rare classes with sparse or noisy annotations can disproportionately influence the metric."
- Why unresolved: Equal weighting treats all classes symmetrically, but rare classes with noisy labels may introduce unwanted variance in the metric.
- What evidence would resolve it: Empirical comparison of alternative weighting schemes (e.g., inverse frequency, variance-based) on multi-class judge selection tasks with long-tailed distributions.

### Open Question 3
- Question: How does judge performance on Balanced Accuracy relate to the effectiveness of statistical debiasing approaches that require high correlation between judge and ground truth scores?
- Basis in paper: [explicit] "Statistical debiasing approaches require high correlation between judge and ground truth scores to provide meaningful sample efficiency gains."
- Why unresolved: Balanced Accuracy optimizes for prevalence difference preservation, but debiasing methods may require different judge properties (score correlation) that are not guaranteed by high Balanced Accuracy alone.
- What evidence would resolve it: Correlation analysis between Balanced Accuracy scores and debiasing sample efficiency across multiple judges and datasets.

## Limitations

- Confidence in generalizability is medium because empirical validation relies heavily on synthetic data rather than diverse real-world golden sets.
- Model-specific biases (self-preference, family-preference) are acknowledged as orthogonal issues that can distort prevalence estimates regardless of metric choice.
- The paper does not address asymmetric costs where false positives and false negatives have different consequences.

## Confidence

- **High Confidence**: Prevalence independence of Sensitivity/Specificity; equivalence of Balanced Accuracy and Youden's J; theoretical alignment of BA with judge selection for prevalence estimation.
- **Medium Confidence**: Simulation results showing BA outperforms Accuracy/F1 in judge selection; downstream ranking accuracy improvements.
- **Low Confidence**: Generalizability to all LLM judge scenarios; effectiveness when golden sets are extremely small or noisy; handling of multi-class prevalence estimation.

## Next Checks

1. **Cross-Dataset Validation**: Apply BA-based judge selection to 3-5 independently collected golden sets with varying prevalence rates (5%, 20%, 50%) and compare judge rankings against Accuracy/F1-based selections.
2. **Model-Specific Bias Analysis**: Test whether BA selection remains robust when judges exhibit self-preference or family-preference biases, and quantify the magnitude of downstream distortion.
3. **Golden Set Size Sensitivity**: Systematically vary golden set sizes (25, 100, 500, 2000) and measure how BA's advantage over Accuracy/F1 changes, particularly in rare-event detection scenarios.