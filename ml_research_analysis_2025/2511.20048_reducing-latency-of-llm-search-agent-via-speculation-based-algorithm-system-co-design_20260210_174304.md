---
ver: rpa2
title: Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System
  Co-Design
arxiv_id: '2511.20048'
source_url: https://arxiv.org/abs/2511.20048
tags:
- speculative
- speculation
- agent
- inference
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the high latency of LLM-based search agents
  caused by strict serialization between reasoning and tool execution steps. It introduces
  SPAgent, a co-designed algorithm-system framework that uses two-phase adaptive speculation
  to reduce LLM inference time and overlap action execution.
---

# Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design

## Quick Facts
- arXiv ID: 2511.20048
- Source URL: https://arxiv.org/abs/2511.20048
- Authors: Zixiao Huang; Wen Zeng; Tianyu Fu; Tengxuan Liu; Yizhou Sun; Ke Hong; Xinhao Yang; Chengchun Liu; Yan Li; Quanlu Zhang; Guohao Dai; Zhenhua Zhu; Yu Wang
- Reference count: 40
- Primary result: Up to 1.65× end-to-end speedup while maintaining or improving accuracy

## Executive Summary
This paper addresses the high latency of LLM-based search agents caused by strict serialization between reasoning and tool execution steps. The authors introduce SPAgent, a co-designed algorithm-system framework that uses two-phase adaptive speculation to reduce LLM inference time and overlap action execution. The method predicts actions directly in early steps and uses verified speculation later, regulated by a load-aware scheduler. Across models, benchmarks, and serving conditions, SPAgent achieves significant latency reductions while maintaining or improving accuracy, demonstrating that speculation can significantly accelerate search agents without degrading performance.

## Method Summary
SPAgent introduces a two-phase adaptive speculation mechanism for LLM search agents. In the aggressive phase, early steps generate actions without verification to save latency. The system transitions to a verified phase when a plausibility score falls below a threshold, where reasoning and speculation run in parallel with verification. A two-level scheduler regulates speculative requests based on engine load and prioritizes short speculative jobs over long main requests. The framework reduces end-to-end latency by overlapping tool execution with inference while maintaining accuracy through selective verification.

## Key Results
- Achieves up to 1.65× end-to-end speedup compared to baseline
- Maintains or improves accuracy while reducing latency
- Demonstrates effectiveness across multiple LLM families (GPT-4o, Gemini-1.5-Pro, Claude-3-Sonnet)
- Validated on diverse benchmarks including 2WikiMultiHopQA, HotpotQA, and FreshQA

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Adaptive Speculation
The system operates in an Aggressive Speculation Phase for early steps, where it samples actions directly via π^s_θ without generating reasoning tokens (Thoughts). It transitions to a Verified Speculation Phase when a plausibility score falls below a threshold β. In the verified phase, the system runs reasoning and speculation in parallel, using the reasoning to verify the speculative action. Early errors from unverified speculation are low-cost and recoverable, and incorrect speculative actions often still provide useful auxiliary evidence rather than corrupting the context. Preliminary results show 73.4% match rate between speculative and reasoned actions.

### Mechanism 2: Load-Aware Intra-Speculation Scheduling
A scheduler selects a subset S ⊆ R of running requests to speculate on, maximizing an objective function balancing expected latency reduction T_{r,a} against decode T_{o,d} and prefill T_{o,p} overheads. It prioritizes requests from earlier steps (higher hit probability) and newer arrivals (longer overlap window), stopping when marginal benefit becomes non-positive. The scheduler prevents speculative requests from overwhelming the system under high concurrency by dynamically selecting which requests should generate speculative actions.

### Mechanism 3: Short-Job-First (SJF) Inter-Request Scheduling
The scheduler enforces a speculation-first policy, prioritizing short speculative requests (<10 tokens) over long main agent requests (hundreds of tokens). Since speculative requests are extremely short, prioritizing them yields disproportionate benefit by ensuring they begin early and enlarge the overlap window. This ensures the action result is available in the Action Buffer when the main path needs it without significantly holding up the main request.

## Foundational Learning

- **Concept: The ReAct (Reason-Act) Loop**
  - Why needed here: The paper defines the problem as the *serialization* of this specific loop. You must understand that standard agents do Reason → Act → Observe sequentially.
  - Quick check question: In a standard ReAct loop, can the agent query a search engine while it is generating the "Thought" token? (Answer: No)

- **Concept: Speculative Decoding vs. Speculative Execution**
  - Why needed here: This paper is not standard speculative decoding (guessing tokens). It is speculative *execution* of *actions*. Distinguishing these is vital to understand why an "Action Server" is needed.
  - Quick check question: In speculative *decoding*, if the draft is wrong, we reject tokens. In SPAgent's Aggressive Phase, do we reject the action if it was wrong? (Answer: Generally no, we accept the observation unless it fails the plausibility check to transition phases)

- **Concept: Overhead vs. Overlap**
  - Why needed here: The core system tradeoff. Speculation adds work (overhead). It only helps if that work hides other work (overlap). If the system is busy, overhead wins, and latency increases.
  - Quick check question: Why does the paper disable speculation at high request rates if the scheduler isn't active? (Answer: Because the extra inference requests saturate the GPU, increasing queue times for everyone)

## Architecture Onboarding

- **Component map:** Agent Loop -> Speculative Worker -> Action Server -> Action Buffer -> Agent Loop; Scheduler sits in front of Inference Engine
- **Critical path:** Request arrives → Scheduler checks load → Launch Speculative Worker (Aggressive or Verified) → Spec Worker generates Action → Action Server executes → Result returned to Agent Loop (skipping main inference in Aggressive phase) or Main Agent generates Thought + Action while Spec Worker generates Action (Verified phase)
- **Design tradeoffs:** Threshold β (high = safer but slower, low = faster but riskier); Sample count k (high = higher hit rate but higher GPU cost, paper finds k=3 optimal); SJF scheduling (reduces latency for speculators but slightly increases waiting time for long non-speculative requests)
- **Failure signatures:** Accuracy Drop + Fast Latency (Aggressive phase threshold β is too low); Increased Latency (High Concurrency) (Scheduler disabled or misconfigured); Low Action Buffer Hit Rate (Model not deterministic enough or speculative requests finishing after main requests)
- **First 3 experiments:** Phase Ablation (Aggressive only vs. Verified only); Scheduler Stress Test (2 rps, toggle Scheduler ON/OFF); Threshold Sweep (vary β 1 to 5 on 2WikiMultiHopQA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Aggressive Speculation phase perform in agent domains with irreversible actions, such as code execution or database modifications?
- Basis in paper: The authors explicitly note that for search agents, early-step mistakes are "low-risk" and "recoverable," implying this safety assumption may not hold for all agent types.
- Why unresolved: The design selectively omits verification to save latency, relying on the ability to tolerate incorrect tool outputs, which is not possible in stateful or destructive environments.
- What evidence would resolve it: Evaluation on benchmarks requiring irreversible actions (e.g., SWE-bench) to measure the rate of critical errors introduced by unverified speculation.

### Open Question 2
- Question: What specific training or architectural factors cause the observed divergence in speculation behavior between the Qwen and Gemma model families?
- Basis in paper: The paper states that "Qwen models are more likely to speculate aggressively... whereas Gemma models evaluate more conservatively," but offers no causal explanation.
- Why unresolved: It is unclear if this behavior stems from inherent uncertainty representation in the models or differences in training data distributions.
- What evidence would resolve it: A comparative analysis of internal confidence scores or uncertainty thresholds across different model architectures during the phase transition scoring step.

### Open Question 3
- Question: How robust is the scheduling mechanism when tool execution latency is significantly lower (e.g., sub-100ms) than the profiled Wikipedia API?
- Basis in paper: The evaluation relies on a search tool with ~1.5s latency, and the speedup is derived from overlapping this specific duration with inference.
- Why unresolved: If tool execution is nearly instant, the overhead of managing speculative requests might outweigh the minimal parallelism gains.
- What evidence would resolve it: Stress testing the framework with synthetic tools of varying latencies to identify the lower bound of execution time where speculation becomes detrimental.

## Limitations
- Generalizability concerns: Effectiveness may not transfer to domains requiring precise reasoning from first step (e.g., mathematical problem-solving, code generation)
- Cost-benefit trade-offs: 20-30% additional GPU inference load from speculative requests, with cost model potentially becoming stale under volatile traffic
- Action Server bottleneck: No empirical data on scalability with thousands of concurrent requests or distributed deployments

## Confidence
- High Confidence (9/10): Two-phase adaptive speculation mechanism and latency benefits, validated by 73.4% action match rate
- Medium Confidence (7/10): Load-aware scheduler's mathematical formulation and effectiveness, though limited validation under dynamic load conditions
- Low Confidence (5/10): Inter-request SJF scheduling's isolated impact, lacking controlled experiments separating its contribution

## Next Checks
1. **Accuracy Recovery Analysis**: Design experiment where aggressive phase makes early errors, measure how often verified phase successfully corrects these errors versus when they compound into final task failure
2. **Cross-Domain Generalization Test**: Apply SPAgent to non-search domains like mathematical reasoning (GSM8K) or code generation (HumanEval) where early-step accuracy is critical
3. **Production-Scale Load Simulation**: Implement Action Server component and run stress tests with 10,000+ concurrent requests, measuring memory overhead, CPU utilization, and hit rate decay as Action Buffer grows