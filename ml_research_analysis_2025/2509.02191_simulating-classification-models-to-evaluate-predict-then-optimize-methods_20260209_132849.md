---
ver: rpa2
title: Simulating classification models to evaluate Predict-Then-Optimize methods
arxiv_id: '2509.02191'
source_url: https://arxiv.org/abs/2509.02191
tags:
- actual
- optimization
- predictions
- predictive
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Predict-Then-Optimize methods by simulating
  classification model predictions without training real models. Building on existing
  work for binary classifiers, the authors introduce a new algorithm for multiclass
  classification using normalized error modeling.
---

# Simulating classification models to evaluate Predict-Then-Optimize methods

## Quick Facts
- **arXiv ID**: 2509.02191
- **Source URL**: https://arxiv.org/abs/2509.02191
- **Reference count**: 14
- **Primary result**: Simulated classification models accurately approximate real model behavior for Predict-Then-Optimize evaluation, with mean TPR/FPR differences ≤0.011 (binary) and ≤0.006 (multiclass)

## Executive Summary
This paper introduces a methodology for evaluating Predict-Then-Optimize methods by simulating classification model predictions without training actual models. Building on prior work for binary classifiers, the authors develop a new algorithm for multiclass classification using normalized error modeling. The approach is validated on a single-machine scheduling problem where simulated performance profiles accurately capture the relationship between predictive accuracy and optimization quality. When applied to real ML models (logistic regression, random forest, XGBoost), the simulation correctly identifies XGBoost as achieving the best results, demonstrating the framework's practical utility for determining minimum predictive performance requirements.

## Method Summary
The paper proposes two simulation algorithms to generate synthetic classification predictions at controlled true positive rate (TPR) and false positive rate (FPR) levels. Algorithm 1 (binary) and Algorithm 2 (multiclass) create confusion matrices matching target TPR/FPR vectors without training real models. The framework is tested on a single-machine scheduling problem minimizing total weighted completion time, where jobs are classified into types with different weights. For each problem instance, the methodology simulates predictions at various TPR/FPR combinations, solves the scheduling problem using the WSPT rule, and computes the relative gap to the true optimal solution. The approach is validated by comparing simulated TPR/FPR against ground truth from all possible confusion matrices and by positioning real ML models within the simulated performance landscape.

## Key Results
- Simulation algorithms achieve high accuracy: mean TPR/FPR differences are ≤0.011 for binary and ≤0.006 for multiclass settings across 100 repetitions
- Solution quality improves with higher TPR and lower FPR, but sensitivity depends on instance characteristics and weight variability
- XGBoost consistently achieves the best results among real models (LR, RF, XGB), correctly situated within the simulated performance profiles
- The methodology provides a practical tool for determining minimum predictive performance requirements in PTO methods

## Why This Works (Mechanism)
The simulation methodology works by decoupling predictive modeling from optimization evaluation. Instead of training multiple real classifiers to explore the predictive accuracy space, the algorithms directly generate predictions that achieve specific TPR/FPR targets. This is computationally efficient because it avoids the cost of training numerous models while maintaining statistical validity through controlled confusion matrix generation. The multiclass extension normalizes FPR redistribution across classes, preserving the overall error structure while achieving target rates. By evaluating optimization quality across this simulated accuracy landscape, the approach reveals how predictive errors propagate to decision quality without the confounding effects of specific model architectures.

## Foundational Learning
- **Predict-Then-Optimize (PTO)**: Framework where ML predictions are used as inputs to downstream optimization problems. Needed to understand the context; check by identifying how predicted job types are used in scheduling.
- **True Positive Rate (TPR) and False Positive Rate (FPR)**: Metrics measuring classification accuracy. Needed for simulation control; check by verifying TPR/FPR calculations from confusion matrices.
- **WSPT rule**: Weighted Shortest Processing Time heuristic for scheduling. Needed as the optimization method; check by confirming WSPT correctly orders jobs by weight/processing time ratio.
- **Confusion matrix validation**: Process of verifying simulated predictions match target TPR/FPR. Needed to assess simulation accuracy; check by comparing simulated vs. actual rates across all valid matrices.
- **Relative gap to optimal**: Performance metric measuring solution quality degradation. Needed to quantify optimization impact; check by computing (actual-optimal)/optimal for each solution.

## Architecture Onboarding
**Component map**: Synthetic data generation -> Simulation algorithms (Alg1/Alg2) -> WSPT optimization -> Gap calculation -> Performance profiling

**Critical path**: The simulation algorithms are the core innovation. Algorithm 1 generates binary predictions matching target TPR/FPR by randomly assigning correct/incorrect labels according to class-specific rates. Algorithm 2 extends this to multiclass by first assigning correct predictions, then redistributing FPR across incorrect classes proportionally.

**Design tradeoffs**: The methodology trades computational efficiency (avoiding multiple model trainings) for approximation accuracy. Using normalized FPR redistribution in multiclass is simple but may introduce noise compared to more sophisticated approaches. The approach assumes uniform class priors, which may not reflect real-world imbalances.

**Failure signatures**: High variance in simulated TPR/FPR across runs indicates insufficient repetitions or numerical instability in the randomization process. Invalid TPR/FPR combinations that produce no valid confusion matrix suggest the targets are outside feasible ROC space.

**First experiments**: 1) Validate Algorithm 1 by generating all 121 binary confusion matrices and computing mean TPR/FPR differences over 100 runs. 2) Implement Algorithm 2 for T=3 classes and validate against 24,750 possible matrices. 3) Simulate predictions at grid points in ROC space and verify the resulting gap contours match expected patterns.

## Open Questions the Paper Calls Out
- How can the simulation methodology be extended to optimization problems driven by continuous uncertainty (regression) rather than classification?
- Can alternative simulation algorithms improve the approximation accuracy and reduce the variability observed in the multiclass setting?
- How does class imbalance affect the validity of the simulation and the relationship between predictive accuracy and decision quality?
- Can the simulation framework effectively inform when to use Decision-Focused Learning (DFL) versus standard Predict-Then-Optimize?

## Limitations
- The methodology is currently limited to classification-based PTO problems and requires extension for continuous uncertainty settings
- High variability in multiclass simulation accuracy suggests the normalized FPR approach may introduce noise
- Experiments assume balanced class priors, leaving the impact of class imbalance unexplored
- The framework focuses on standard PTO without addressing when Decision-Focused Learning might be preferable

## Confidence
- **High confidence**: The general simulation methodology and its accuracy (mean differences in TPR/FPR ≤0.011 for binary, ≤0.006 for multiclass) are well-supported by the computational study
- **Medium confidence**: The claim that prediction error structure significantly impacts optimization quality is supported but would benefit from more extensive validation across different problem types
- **Medium confidence**: The assertion that the methodology can determine minimum predictive performance requirements is theoretically sound but relies on the specific scheduling problem studied

## Next Checks
1. Replicate the simulation accuracy validation by generating all valid confusion matrices for T=2 and T=3 classes, comparing simulated vs. actual TPR/FPR over multiple runs to verify the reported mean differences
2. Implement the full computational study with a fixed random seed to assess variability in the results and confirm the observed trends in solution quality as a function of TPR/FPR rates
3. Test the methodology on a different optimization problem (e.g., knapsack or shortest path) to evaluate the generalizability of the simulation approach beyond the single-machine scheduling context