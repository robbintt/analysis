---
ver: rpa2
title: 'Lifelong Evolution: Collaborative Learning between Large and Small Language
  Models for Continuous Emergent Fake News Detection'
arxiv_id: '2506.04739'
source_url: https://arxiv.org/abs/2506.04739
tags:
- news
- learning
- data
- knowledge
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection

## Quick Facts
- arXiv ID: 2506.04739
- Source URL: https://arxiv.org/abs/2506.04739
- Authors: Ziyi Zhou; Xiaoming Zhang; Litian Zhang; Yibo Zhang; Zhenyu Guan; Chaozhuo Li; Philip S. Yu
- Reference count: 40
- Primary result: None specified

## Executive Summary
This paper introduces C²EFND, a framework for continuous fake news detection that enables both large and small language models to learn from sequential emergent events without forgetting prior knowledge. The approach combines MoE-based lifelong knowledge editing for LLMs, replay-based continual learning with distillation for SLMs, and multi-round collaborative filtering between both models. Evaluated on Pheme and Twitter16 datasets with 5 events each, the method shows improved accuracy on both target and all-events scenarios compared to baseline approaches.

## Method Summary
C²EFND processes sequential fake news events by first extracting knowledge entities from news content using a LLM agent and Wikipedia API. A two-stage active learning approach then identifies diverse samples for initial human annotation. The LLM undergoes lifelong knowledge editing through an expanding MoE architecture, adding event-specific FFN experts while preserving existing knowledge. The SLM (RoBERTa-based) is trained with replay from a memory bank and knowledge distillation from the previous model. Finally, a multi-round collaborative learning loop uses both models' predictions to generate pseudo-labels, with high-confidence agreed samples forming a clean pool for semi-supervised training and disagreed samples entering a noisy pool for additional annotation.

## Key Results
- Accuracy improves from 80.2% (Round 1) to 88.1% (Round 3) through multi-round collaborative learning
- SLM maintains performance on earlier events while adapting to new events through replay + distillation
- The framework achieves better all-events accuracy compared to baselines that do not employ continual learning mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-Experts (MoE) based knowledge editing enables LLMs to incrementally learn emergent news events without catastrophic forgetting of prior events.
- Mechanism: For each new event ei, a dedicated FFN expert (FFNi) is added and trained on labeled data while all previous experts remain frozen. A router network computes proportional scores to select top-k experts during inference, allowing the model to leverage event-specific knowledge modules.
- Core assumption: Knowledge in transformer FFN layers is sufficiently localized that editing specific projection matrices (Wproj) can incorporate new factual knowledge without disrupting existing knowledge representations.
- Evidence anchors:
  - [abstract] "lifelong knowledge editing module based on a Mixture-of-Experts architecture to incrementally update LLMs"
  - [section IV-B] "we incorporate multiple FFN layers within the transformer to learn knowledge specific to each event respectively... keeping other FFN layers frozen, including the original FFNmain module"
  - [corpus] Related work on LoRAMoE (arXiv:2401.04088) shows MoE-style plugins can alleviate knowledge forgetting, providing indirect support for this architectural choice.
- Break condition: If events share overlapping knowledge that maps to the same FFN regions, sequential editing may still cause interference; the paper does not validate this boundary condition.

### Mechanism 2
- Claim: Replay-based continual learning with knowledge distillation preserves SLM detection capability on past events while adapting to new events.
- Mechanism: The SLM is trained with three loss components: (1) classification loss on current event data, (2) replay loss on samples drawn from a fixed-capacity memory bank, and (3) distillation loss (KL divergence) between the current and previous model's output distributions. This dual regularization prevents catastrophic forgetting.
- Core assumption: A bounded memory bank of representative past samples (cluster centroids via k-means) is sufficient to approximate the full historical data distribution for replay purposes.
- Evidence anchors:
  - [abstract] "replay-based continue learning method to ensure SLMs retain prior knowledge without retraining entirely"
  - [section IV-C] "Lslm = Lcls + λ1·Lreplay + λ2·Ldil" with replay drawing from Mi-1 and distillation from previous model Si-1
  - [corpus] Limited direct validation; related work Shi & Wang (NeurIPS 2024) provides theoretical grounding for memory-based domain incremental learning.
- Break condition: If memory capacity is severely constrained relative to event diversity, clustering may fail to preserve decision boundaries; ablation study (Table IX) shows performance degrades without distillation loss but does not test extreme memory constraints.

### Mechanism 3
- Claim: Multi-round collaborative filtering between LLM and SLM progressively improves pseudo-label quality and model agreement on unlabeled emergent data.
- Mechanism: Both models predict on unlabeled data; samples with consistent predictions and high SLM confidence (≥ω threshold) enter the "clean pool" for semi-supervised training. Disagreed/low-confidence samples enter the "noisy pool" and are prioritized for human annotation via uncertainty-based active learning. This repeats for N rounds.
- Core assumption: Agreement between LLM (with edited knowledge) and SLM (with specialized classification) indicates higher label reliability than either model alone.
- Evidence anchors:
  - [abstract] "multi-round collaborative learning framework" leveraging "LLMs' generalization power and SLMs' classification expertise"
  - [section IV-D] "Dclean = {(xi, yi) | ŷ1 = ŷ2 and p(ŷ2) ≥ ω}" with Table X showing accuracy improves from 80.2% (Round 1) to 88.1% (Round 3)
  - [corpus] Similar collaborative patterns in ARG (AAAI 2024) show LLM rationale guidance helps SLMs, but C²EFND uniquely extends to bidirectional iterative refinement.
- Break condition: After ~3 rounds, Table X shows diminishing returns and slight performance drops (Round 4: 86.7%), suggesting potential overfitting to pseudo-labels or accumulation of systematic errors.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper's core contribution is preventing both LLM and SLM from losing detection capability on past events when learning new ones.
  - Quick check question: Can you explain why training sequentially on new event data without regularization would cause performance collapse on earlier events?

- Concept: **Mixture-of-Experts Routing**
  - Why needed here: Understanding how the router network computes softmax scores and selects top-k experts is essential for debugging the lifelong knowledge editing module.
  - Quick check question: How does the router decide which FFN expert(s) to activate for a given news input, and what happens if multiple events share similar semantic features?

- Concept: **Active Learning Sampling Strategies**
  - Why needed here: The two-stage annotation pipeline (diversity-based → uncertainty-based) is critical for minimizing labeling costs while maximizing information gain.
  - Quick check question: Why sample high-distance (diverse) examples first, then high-uncertainty examples second—what does each stage contribute?

## Architecture Onboarding

- Component map:
  Input News → [Knowledge Extraction (LLM agent + Wikipedia API)]
           → [Two-Stage Active Learning] → Annotated Subset
           → [LLM Lifelong KE Module: Router → FFN_experts (frozen/expanding)]
           → [LLM generates Rationales via CoT]
           → [SLM Module: RoBERTa encoders → Cross-Attention → Classifier]
           → [Multi-Round Collaborative Loop: LLM+SLM predictions → Data Selection → Clean/Noisy pools]

- Critical path:
  1. First-stage active learning (diversity sampling) determines which samples get human labels
  2. Knowledge editing on the new FFN expert must complete before the LLM can generate reliable rationales
  3. SLM training requires both labeled data AND LLM-generated rationales/knowledge
  4. Multi-round loop cannot start until both models have been updated on initial labeled data

- Design tradeoffs:
  - Memory bank size (Mmax): Larger preserves more past knowledge but increases replay computation; paper uses 400 as threshold
  - Confidence threshold (ω): Lower accepts more pseudo-labels (faster training) but introduces noise; paper uses 0.85
  - Number of rounds (N): More rounds extend training time with diminishing returns; paper shows 3 rounds optimal
  - Expert selection (k1): More experts per inference increases computation; paper uses top-2

- Failure signatures:
  - SLM accuracy drops sharply on earlier events → distillation loss (λ2) or replay coefficient (λ1) too low
  - LLM provides outdated/wrong judgments → knowledge edit may have failed; check if FFNi was actually trained on current event
  - Clean pool grows slowly across rounds → ω threshold too restrictive or models fundamentally disagree (possible architecture/initialization issue)
  - Performance degrades after Round 3 → overfitting to pseudo-labels; reduce N or increase ω

- First 3 experiments:
  1. **Ablation on memory bank size**: Test Mmax ∈ {100, 200, 400, 800} on all-events performance to validate the 400 threshold and identify forgetting boundaries.
  2. **Knowledge edit layer sensitivity**: Vary which FFN layer is edited (layers 20, 26, 30 in Llama3-8B) to confirm layer 26 is optimal for factual knowledge storage.
  3. **Round convergence analysis**: Run 5-7 rounds with per-round accuracy tracking on a held-out validation set to confirm the Round 3 peak is consistent across events (not just Twitter E1).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be effectively adapted for multimodal fake news detection scenarios involving images and videos?
- **Basis in paper:** [explicit] The conclusion explicitly states, "Future research will extend the framework’s applicability to more complex multimodal scenarios."
- **Why unresolved:** The current study limits its scope to textual data and extracted knowledge, lacking mechanisms to process visual content which is intrinsic to modern misinformation.
- **What evidence would resolve it:** An extension of the C²EFND framework incorporating visual encoders and cross-modal fusion mechanisms, evaluated on benchmark multimodal datasets.

### Open Question 2
- **Question:** What specific optimizations are required to reduce the latency and computational cost of the multi-round collaborative learning process for real-time applications?
- **Basis in paper:** [explicit] The conclusion identifies the need to "further optimize computational efficiency" as a primary direction for future work.
- **Why unresolved:** The methodology involves iterative loops of active learning, knowledge editing, and collaborative filtering (up to N rounds), which is computationally intensive and may hinder deployment in breaking-news situations.
- **What evidence would resolve it:** A complexity analysis and latency benchmarks of the proposed iterative loops compared to one-pass baselines, potentially accompanied by a proposed acceleration method.

### Open Question 3
- **Question:** How does the framework scale in terms of parameter count and routing stability when the number of sequential events grows significantly larger than the five events tested?
- **Basis in paper:** [inferred] Section IV-B describes adding a new Mixture-of-Experts (MoE) FFN layer (FFNi) specifically for each new event ei. While effective for the evaluated datasets (5 events), this design implies linear parameter growth and increased router complexity over long timeframes.
- **Why unresolved:** The paper does not analyze the memory footprint or routing performance when the number of events (n) scales to hundreds or thousands, which is typical for long-term social media monitoring.
- **What evidence would resolve it:** Experiments on datasets with a significantly higher number of sequential events, analyzing the correlation between the number of experts, model performance, and memory usage.

## Limitations

- The paper assumes knowledge in FFN layers is sufficiently localized for MoE-based editing, but this is not empirically validated.
- Results are presented on two datasets with five events each, which may not generalize to scenarios with significantly more events or different event distributions.
- The router network's architecture and training procedure are underspecified, leaving critical design choices unclear.

## Confidence

- **High Confidence**: The SLM architecture (RoBERTa + cross-attention + classifier) and basic replay + distillation training procedure are standard and well-documented.
- **Medium Confidence**: The multi-round collaborative learning framework and its convergence behavior are demonstrated through empirical results, though the exact prompt templates remain incomplete.
- **Low Confidence**: The MoE-based lifelong knowledge editing mechanism for the LLM has critical underspecifications in router design, expert selection criteria, and knowledge localization assumptions.

## Next Checks

1. **Router Network Validation**: Implement and test the router architecture with varying k1 values (1-4) on a held-out validation set to verify the top-2 expert selection is optimal and that the router can effectively route to event-specific experts.

2. **Knowledge Localization Analysis**: Perform ablation studies on different FFN layers (20, 26, 30) to confirm layer 26 is indeed optimal for knowledge editing, and conduct error analysis on edited vs. non-edited events to validate the knowledge localization assumption.

3. **Memory Bank Capacity Stress Test**: Systematically vary Mmax from 100 to 800 samples and measure catastrophic forgetting on earlier events to identify the true capacity threshold where performance begins degrading, rather than relying on the fixed 400 threshold.