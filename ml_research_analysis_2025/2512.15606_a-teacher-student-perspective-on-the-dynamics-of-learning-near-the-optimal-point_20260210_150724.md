---
ver: rpa2
title: A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal
  Point
arxiv_id: '2512.15606'
source_url: https://arxiv.org/abs/2512.15606
tags:
- hessian
- network
- matrix
- distribution
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines how the Hessian eigenspectrum near the optimal
  point determines long-term learning performance in neural networks. By analyzing
  teacher-student setups with matching weights, the authors derive Hessian components
  for linear, polynomial, and error function networks under Gaussian input assumptions.
---

# A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point

## Quick Facts
- **arXiv ID:** 2512.15606
- **Source URL:** https://arxiv.org/abs/2512.15606
- **Reference count:** 40
- **Primary result:** The Hessian eigenspectrum near the optimal point determines long-term learning performance in teacher-student neural network setups.

## Executive Summary
This paper analyzes how the Hessian matrix's eigenspectrum near the optimal point governs learning dynamics in teacher-student neural network setups. By deriving explicit forms of the Hessian for linear, polynomial, and error function networks under Gaussian input assumptions, the authors show that the smallest eigenvalues determine the asymptotic decay rate of the loss. The work provides a quantitative framework linking spectral properties of the Hessian to parameter efficiency and convergence behavior, particularly in overparameterized regimes.

## Method Summary
The authors study two-layer teacher-student networks where a student network learns from a fixed teacher with identical architecture. They derive analytical expressions for the Hessian matrix at the optimal point (where student weights match teacher weights) for different activation functions. The analysis leverages Gaussian input assumptions and random matrix theory, particularly Marchenko-Pastur distributions, to characterize the spectral properties. For linear networks, they prove the spectrum follows a convolution of scaled chi-squared and Marchenko-Pastur distributions. For polynomial networks, they show the Hessian rank equals an effective number of parameters. Error function networks are analyzed numerically.

## Key Results
- Small Hessian eigenvalues govern the asymptotic decay of the loss near the optimal point
- Linear network Hessian spectrum follows a convolution of scaled chi-squared and Marchenko-Pastur distributions
- Polynomial networks have Hessian rank equal to effective number of parameters, while error functions maintain full rank
- The spectral properties directly link to parameter efficiency and learning dynamics in overparameterized regimes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a student network is initialized near the global optimum in a teacher-student setup, the asymptotic decay of the generalization error is governed by the smallest eigenvalues of the Hessian matrix.
- **Mechanism:** Near the optimum, the loss landscape is approximated by a quadratic form defined by the Hessian ($H$). Gradient flow dynamics lead to parameter evolution $\delta\theta(t) \approx \delta\theta(0)e^{-Ht}$, meaning the slowest decaying modes (corresponding to the smallest non-zero eigenvalues) dominate the long-time error behavior.
- **Core assumption:** The initialization is sufficiently close to the optimum such that higher-order terms in the Taylor expansion of the loss are negligible.
- **Evidence anchors:**
  - [abstract] "showing that the smaller eigenvalues of the Hessian determine long-time learning performance."
  - [Section 2.1] Derives the loss evolution $\langle L(t) \rangle \approx \frac{\sigma_0^2}{2} \int_{0}^{+\infty} e^{-2\lambda t} \lambda \rho(\lambda) d\lambda$.
  - [corpus] Consistent with "Gradient Flow Equations for Deep Linear Neural Networks" [73683] regarding dynamics near minima.
- **Break condition:** If the initialization is far from the optimum, the quadratic approximation fails, and the functional Hessian (non-zero at non-optimal points) invalidates this specific spectral dynamic.

### Mechanism 2
- **Claim:** In linear networks with Gaussian inputs and weights, the Hessian spectrum at the optimum asymptotically follows a convolution of a scaled chi-squared distribution and a scaled Marchenko-Pastur distribution.
- **Mechanism:** The Hessian separates into diagonal blocks. One block depends on the norm of the output weights (inducing a $\chi^2$ distribution), and the other depends on the structure of the input-to-hidden weights (inducing a Marchenko-Pastur distribution). The total spectrum is the sum of these independent random variables.
- **Core assumption:** Input data follows a standard normal distribution, and network weights are sampled from a Gaussian with variance $1/N$.
- **Evidence anchors:**
  - [abstract] "For linear networks, we analytically establish that... the spectrum asymptotically follows a convolution..."
  - [Section 3.1] Proves that eigenvalues are sums of eigenvalues from diagonal sub-matrices $A$ and $B$.
  - [corpus] [4035] supports the tractability of geometry in polynomial/shallow networks, though specific spectral distributions here are unique to this paper.
- **Break condition:** If the input distribution deviates significantly from Gaussian (e.g., heavy-tailed or sparse), the Marchenko-Pastur result may not hold.

### Mechanism 3
- **Claim:** For polynomial activation networks, the rank of the Hessian at the optimum equals the effective number of parameters, whereas for generic non-linear activations (e.g., error function), the Hessian remains full rank.
- **Mechanism:** Polynomial networks effectively map inputs to a finite set of monomials, creating redundancy where multiple weight configurations yield the same function (zero Hessian curvature in those directions). Generic smooth activations (erf) do not exhibit this finite polynomial redundancy, maintaining curvature in all parameter directions.
- **Core assumption:** The "effective number of parameters" is defined by the degrees of freedom in the function space (symmetric tensors for polynomials) rather than raw weight count.
- **Evidence anchors:**
  - [Section 4.1.1] "the rank of the Hessian matrix can be seen as an effective number of parameters."
  - [Section 5.1] "For the error function network, we empirically find that... the Hessian matrix is always full rank."
  - [corpus] Corpus evidence for the specific Hessian rank of error function networks is weak; related works focus mostly on ReLU or linear cases.
- **Break condition:** If the hidden layer size $N_h$ is smaller than the input dimension $N_i$ in polynomial networks, the upper bound on effective parameters is not saturated.

## Foundational Learning

- **Concept:** **Hessian Matrix & Eigenspectrum**
  - **Why needed here:** The paper centralizes the Hessian (matrix of second derivatives) as the primary object determining learning speed and parameter efficiency. You must understand that eigenvalues represent curvature (flat vs. sharp directions).
  - **Quick check question:** If a Hessian has many zero eigenvalues near the optimum, what does that imply about the "effective" complexity of the model?

- **Concept:** **Teacher-Student Setup**
  - **Why needed here:** This is the experimental framework. A "student" network learns the output of a fixed "teacher" network. This guarantees a known global optimum (matching weights) and isolates the analysis from data noise.
  - **Quick check question:** In this setup, why is the "functional Hessian" (dependence on residual error) zero at the optimal point?

- **Concept:** **Random Matrix Theory (Marchenko-Pastur)**
  - **Why needed here:** To interpret the spectral analysis of the linear network, one must recognize that random weight matrices produce specific, predictable eigenvalue densities (the Marchenko-Pastur law).
  - **Quick check question:** Does the Marchenko-Pastur distribution appear here because of the input data distribution or the distribution of the network weights?

## Architecture Onboarding

- **Component map:** Input vectors $x \in \mathbb{R}^{N_i}$ -> Two-layer network ($W_1$: Input → Hidden, $W_2$: Hidden → Output) with activation $g(\cdot)$ -> Output compared to teacher network output

- **Critical path:**
  1. Select activation $g(x)$ (e.g., linear).
  2. Compute analytical Hessian components using Equations (8-10) or (16-18).
  3. Diagonalize the resulting Hessian matrix at the point $\theta = \theta^*$.
  4. Compare the resulting eigenvalue histogram against theoretical predictions (e.g., convolution of $\chi^2$ and MP).

- **Design tradeoffs:**
  - **Analytical Tractability vs. Realism:** Linear/Polynomial networks allow for exact spectral formulas but lack the "bulk near zero" behavior seen in real deep networks. Error functions are more realistic but require numerical integration.
  - **Rank vs. Dimensions:** For polynomial networks, increasing hidden width $N_h$ eventually yields diminishing returns on Hessian rank (effective parameters) because the function space is capped by the polynomial degree.

- **Failure signatures:**
  - **Numerical Instability:** When computing the Hessian for large $N$, standard float precision might incorrectly identify near-zero eigenvalues as exactly zero (or vice versa).
  - **Non-Gaussian Inputs:** The derived formulas (Eqs 8-10) explicitly assume Gaussian inputs; feeding MNIST or similar real data will break the theoretical spectral match.

- **First 3 experiments:**
  1. **Linear Spectrum Validation:** Implement the linear network Hessian (Eq 8-10) with $N_i=10, N_h=20$. Plot the eigenvalue histogram and overlay the scaled chi-squared + Marchenko-Pastur convolution to verify the fit.
  2. **Rank Saturation Check:** For a quadratic activation network, vary hidden layer size $N_h$ while keeping $N_i$ fixed. Plot the Hessian rank vs. $N_h$ to observe the saturation at the effective parameter count derived in Eq (14).
  3. **Asymptotic Decay Rate:** Initialize a linear student with small noise $\delta \theta$ around the teacher. Run gradient descent and plot $\log(Loss)$ vs. time. Verify the slope matches $-2\lambda_{min}$ as predicted in Eq (7) and Figure 3.

## Open Questions the Paper Calls Out
None

## Limitations
- The spectral analysis relies heavily on Gaussian input assumptions, which may not hold in real-world scenarios
- The quadratic approximation near the optimum may break down for initialization far from the optimal point
- The error function network analysis lacks strong empirical validation for the full-rank claim

## Confidence
- **High confidence:** The core claim that small eigenvalues govern asymptotic decay (Mechanism 1) is well-established theoretically
- **Medium confidence:** The spectral convolution result for linear networks (Mechanism 2) relies critically on Gaussian input assumptions
- **Low confidence:** The claim about error function networks maintaining full rank lacks strong empirical backing from the corpus (Mechanism 3)

## Next Checks
1. Test the spectral predictions with non-Gaussian input distributions to assess robustness of the Marchenko-Pastur component
2. Conduct systematic experiments varying hidden layer size in polynomial networks to empirically verify the saturation of effective parameters
3. Implement gradient descent experiments with controlled initialization distances from the optimum to quantify when the quadratic approximation breaks down