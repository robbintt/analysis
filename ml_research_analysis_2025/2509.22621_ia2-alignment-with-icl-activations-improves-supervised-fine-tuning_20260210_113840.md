---
ver: rpa2
title: 'IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning'
arxiv_id: '2509.22621'
source_url: https://arxiv.org/abs/2509.22621
tags:
- training
- arxiv
- performance
- only
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates why supervised fine-tuning (SFT) often underperforms
  in-context learning (ICL) in low-data regimes. It shows that SFT and ICL produce
  distinct activation patterns, revealing different functional mechanisms of adaptation.
---

# IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2509.22621
- Source URL: https://arxiv.org/abs/2509.22621
- Reference count: 40
- Key outcome: IA2→SFT significantly improves accuracy and calibration compared to SFT alone across 12 benchmarks and two model families in low-data regimes

## Executive Summary
This paper investigates why supervised fine-tuning (SFT) often underperforms in-context learning (ICL) in low-data regimes, revealing that SFT and ICL produce distinct activation patterns that reflect different functional mechanisms of adaptation. To bridge this gap, the authors introduce ICL Activation Alignment (IA2), a self-distillation method that primes SFT models to replicate ICL's internal activation patterns before standard SFT. Across extensive experiments with two model families on 12 benchmarks, IA2→SFT consistently improves accuracy and calibration compared to SFT alone, with gains attributed to a unique training signal absent from standard SFT.

## Method Summary
IA2 is a two-stage self-distillation approach designed to align SFT models with ICL's internal representations. First, the method uses ICL-generated activations as soft targets to train the model via KL divergence loss, creating an "ICL-aligned" model. Second, standard SFT is performed on this primed model. The key insight is that SFT and ICL represent fundamentally different adaptation mechanisms, and IA2 serves as a bridge by forcing the model to adopt ICL-like activation patterns before applying supervised training. This approach is particularly effective in low-data regimes where SFT struggles to capture the generalizable patterns that ICL naturally exploits.

## Key Results
- IA2→SFT consistently outperforms SFT alone across 12 benchmarks and two model families
- The method improves both accuracy and calibration compared to standard SFT
- IA2 provides a unique training signal absent from standard SFT, as demonstrated by ablation studies
- Benefits extend to parameter-efficient fine-tuning methods like LoRA

## Why This Works (Mechanism)
The paper demonstrates that SFT and ICL produce fundamentally different activation patterns, with ICL exhibiting more diverse and generalizable representations. IA2 works by aligning SFT's activation patterns with ICL's before applying supervised training, effectively combining the generalization benefits of ICL with the task-specific optimization of SFT. This alignment creates a unique training signal that helps the model learn more robust and transferable representations.

## Foundational Learning
- **Activation pattern analysis**: Understanding how different training methods (SFT vs ICL) produce distinct internal representations is crucial for identifying the divergence between these approaches. Quick check: Compare activation distances using L2 and cosine metrics across layers.
- **Self-distillation techniques**: IA2 leverages self-distillation to transfer activation patterns from one training regime to another, a fundamental concept in knowledge transfer. Quick check: Verify that KL divergence loss effectively transfers ICL patterns to the student model.
- **Low-data regime adaptation**: The study focuses on scenarios where data is scarce, highlighting the importance of efficient learning strategies. Quick check: Test performance across varying data sizes to identify the effectiveness threshold.

## Architecture Onboarding
- **Component map**: ICL model -> Activation extraction -> IA2 alignment -> SFT model -> Fine-tuned model
- **Critical path**: ICL inference → Activation capture → KL distillation → SFT training → Evaluation
- **Design tradeoffs**: The method requires two training stages (ICL alignment + SFT) which increases computational cost but provides better performance in low-data settings. The tradeoff favors accuracy over efficiency in resource-constrained scenarios.
- **Failure signatures**: If activation alignment fails, the model will show no improvement over standard SFT. Poor alignment manifests as similar activation distances between SFT and IA2→SFT models.
- **3 first experiments**:
  1. Verify that SFT and ICL produce significantly different activation patterns using L2/cosine distance metrics
  2. Test IA2 alignment effectiveness by comparing pre- and post-alignment activation distances
  3. Run ablation study removing IA2 alignment to confirm it provides unique training signal

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The fundamental reasons why ICL and SFT produce divergent activation patterns remain empirically observed rather than mechanistically understood
- The study focuses primarily on low-data regimes, leaving questions about IA2's effectiveness in high-resource settings unexplored
- Analysis relies heavily on aggregate activation distance metrics without deeper investigation into specific neurons or attention heads driving differences

## Confidence
- **High Confidence**: Empirical results showing IA2→SFT outperforming SFT alone are robust across 12 benchmarks and two model families
- **Medium Confidence**: The claim that IA2 provides a "unique training signal" absent from standard SFT is supported by ablation studies but could benefit from more granular analysis
- **Medium Confidence**: Extension to parameter-efficient fine-tuning methods shows promise but is limited to a single PEFT method (LoRA)

## Next Checks
1. Conduct neuron-level analysis to identify specific activation patterns that distinguish ICL from SFT, moving beyond aggregate L2/cosine distances
2. Test IA2's effectiveness in high-data regimes to determine whether benefits persist or diminish as training data increases
3. Evaluate IA2 across multiple parameter-efficient fine-tuning methods (e.g., prefix tuning, full adapters) to establish broader applicability