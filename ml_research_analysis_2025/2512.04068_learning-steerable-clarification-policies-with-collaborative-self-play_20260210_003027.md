---
ver: rpa2
title: Learning Steerable Clarification Policies with Collaborative Self-play
arxiv_id: '2512.04068'
source_url: https://arxiv.org/abs/2512.04068
tags:
- answer
- clarification
- question
- reward
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of managing uncertainty in AI
  assistants when faced with ambiguous or underspecified queries. The authors propose
  a steerable policy framework that enables AI assistants to adapt their response
  strategy based on contextual factors like user preferences or interaction modality.
---

# Learning Steerable Clarification Policies with Collaborative Self-play

## Quick Facts
- arXiv ID: 2512.04068
- Source URL: https://arxiv.org/abs/2512.04068
- Authors: Jonathan Berant; Maximillian Chen; Adam Fisch; Reza Aghajani; Fantine Huot; Mirella Lapata; Jacob Eisenstein
- Reference count: 40
- Primary result: Framework enables steerable clarification policies that adapt response strategies based on cost coefficients

## Executive Summary
This paper addresses the challenge of managing uncertainty in AI assistants when faced with ambiguous or underspecified queries. The authors propose a steerable policy framework that enables AI assistants to adapt their response strategy based on contextual factors like user preferences or interaction modality. They train a single model that can dynamically choose between guessing the user intent, enumerating multiple possible interpretations, or asking clarification questions, conditioned on input cost coefficients that represent the relative costs of these strategies. The model is trained using Reinforced Self-Training (ReST) with collaborative self-play between a user simulator and an AI assistant. Experiments on AmbigQA and Pacific benchmarks show that the trained model significantly outperforms prompted baselines, achieving higher accuracy while reducing the number of clarification questions and length of answers. Notably, the model generalizes to cost coefficients not seen during training and demonstrates effective steerability, adjusting its behavior predictably based on the provided cost coefficients.

## Method Summary
The authors develop a framework for learning steerable clarification policies that can adapt their response strategy based on contextual factors. The approach trains a single model to choose between three response strategies - guessing intent, enumerating interpretations, or asking clarification questions - based on input cost coefficients. The model is trained using Reinforced Self-Training (ReST) with collaborative self-play between a user simulator and AI assistant. During training, the user simulator provides feedback on the AI assistant's responses, allowing the model to learn optimal strategies for different cost configurations. The key innovation is the ability to steer the model's behavior through cost coefficients without requiring retraining, enabling dynamic adaptation to different user preferences and interaction modalities.

## Key Results
- Trained model significantly outperforms prompted baselines on AmbigQA and Pacific benchmarks
- Achieves higher accuracy while reducing the number of clarification questions and length of answers
- Demonstrates generalization to cost coefficients not seen during training
- Shows effective steerability, adjusting behavior predictably based on provided cost coefficients

## Why This Works (Mechanism)
The framework works by training a unified model that can dynamically choose between different response strategies based on contextual cost coefficients. During reinforcement learning with collaborative self-play, the model learns to associate different cost configurations with optimal response strategies through interaction with a user simulator. The cost coefficients effectively encode preferences about response length, clarification frequency, and precision, allowing the model to adapt its behavior without retraining. This approach captures the trade-offs between different strategies (guessing vs. asking vs. enumerating) and learns to balance them based on the relative costs specified by the coefficients.

## Foundational Learning
**Reinforcement Learning**: Why needed - to train the model to make sequential decisions about response strategies based on feedback. Quick check - verify that the model learns to maximize reward through interaction with the user simulator.

**Collaborative Self-play**: Why needed - to create synthetic training data where the AI assistant learns from interactions with a simulated user. Quick check - ensure the user simulator generates diverse and realistic responses that challenge the AI assistant.

**Cost-based Steering**: Why needed - to enable dynamic adaptation of response strategies without retraining. Quick check - verify that changing cost coefficients produces predictable changes in model behavior.

**Multi-task Learning**: Why needed - to train a single model that can handle multiple response strategies. Quick check - confirm the model can effectively switch between guessing, enumerating, and asking strategies.

## Architecture Onboarding

**Component Map**: User Query -> Cost Coefficients -> Steerable Policy Model -> Response Strategy -> User Simulator Feedback -> Policy Update

**Critical Path**: The critical path flows from the user query through the cost coefficients to the steerable policy model, which determines the response strategy. The user simulator then provides feedback that drives policy updates through reinforcement learning.

**Design Tradeoffs**: The framework trades model complexity for flexibility - a single model handles multiple strategies rather than training separate models for each. This increases inference time but enables steerability. The collaborative self-play approach trades computational cost during training for the ability to learn without extensive human-labeled data.

**Failure Signatures**: 
- Model becomes overly conservative, asking too many clarification questions
- Model consistently guesses wrong when cost coefficients favor guessing
- Model fails to adapt behavior when cost coefficients change
- User simulator generates unrealistic scenarios that don't transfer to real users

**First 3 Experiments**:
1. Test model response to ambiguous queries with varying cost coefficients to verify steerability
2. Compare model performance against prompted baselines on AmbigQA benchmark
3. Evaluate generalization by testing on cost coefficients not seen during training

## Open Questions the Paper Calls Out
None

## Limitations
- Model architecture and training dependencies may be sensitive to user simulator quality and reward shaping
- Evaluation focuses on controlled benchmarks, with uncertain performance in complex real-world applications
- Cost coefficient interpretability and calibration for practical use remains unclear

## Confidence

| Claim | Confidence |
|-------|------------|
| Model behavior remains predictable across diverse scenarios | Medium |
| Framework generalizes beyond controlled benchmarks | Medium |
| Cost coefficients can be effectively calibrated in practice | Low |

## Next Checks
1. Conduct ablation studies to isolate the contribution of collaborative self-play versus other training components to the model's performance
2. Test the model's generalization on more diverse, real-world datasets with varying levels of ambiguity and user interaction patterns
3. Evaluate the interpretability and usability of cost coefficients by conducting user studies to assess how well users can specify and understand these parameters