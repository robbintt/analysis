---
ver: rpa2
title: 'OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language
  Models'
arxiv_id: '2402.13524'
source_url: https://arxiv.org/abs/2402.13524
tags:
- language
- arxiv
- llms
- multilingual
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OMGEval, the first open-source multilingual
  generative evaluation benchmark for large language models (LLMs). OMGEval addresses
  the need for comprehensive evaluation of LLMs in diverse languages and cultural
  contexts, as most existing benchmarks focus primarily on English.
---

# OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2402.13524
- Source URL: https://arxiv.org/abs/2402.13524
- Authors: Yang Liu; Meng Xu; Shuo Wang; Liner Yang; Haoyu Wang; Zhenghao Liu; Cunliang Kong; Yun Chen; Yang Liu; Maosong Sun; Erhong Yang
- Reference count: 11
- Primary result: First open-source multilingual generative evaluation benchmark with 804 questions × 5 languages (Chinese, Russian, French, Spanish, Arabic), using GPT-4 adjudication correlated with human evaluation

## Executive Summary
OMGEval addresses the critical gap in multilingual LLM evaluation by providing the first open-source benchmark that goes beyond English-centric testing. The benchmark consists of 804 open-ended questions per language, with rigorous cultural localization to capture culturally-bound knowledge rather than simple translation. Human annotators verify all questions, and GPT-4 serves as an adjudicator to automatically score model outputs with strong correlation to human judgment. Experiments show GPT-4 significantly outperforms other models (55.52% win rate), but open-source multilingual models demonstrate substantial room for improvement, particularly in handling cultural nuances. The benchmark provides both full-language and localization subsets to distinguish general language proficiency from cultural competence.

## Method Summary
OMGEval constructs a multilingual generative evaluation benchmark by first translating 805 English questions from AlpacaEval into five target languages using GPT-4, then manually localizing culturally-specific elements (names, places, festivals, foods) with equivalent target-culture items. Each question undergoes verification by two linguistics MA holders plus one reviewer per language. The benchmark employs GPT-4 as an adjudicator to automatically score model outputs through pairwise comparison against a GPT-3.5-turbo baseline, computing win rates and standard errors. The methodology includes separate evaluation on localization subsets to isolate cultural competence from general language proficiency, with human evaluation validation showing strong correlation (F1 0.79-0.91) for Chinese and Spanish.

## Key Results
- GPT-4 achieves 55.52% win rate against GPT-3.5-turbo across all languages
- Open-source models show significant performance gaps, especially on localization subsets
- Guanaco-13b drops from 14.8% (full Chinese) to 13.7% (localized Chinese)
- Human-GPT-4 agreement: Chinese F1=0.88, Spanish F1=0.91 on validation samples
- Localization subset sizes vary by cultural distance: Chinese 28.7%, Arabic 26.4%, French 24.5%

## Why This Works (Mechanism)

### Mechanism 1: Cultural Localization Over Translation
Direct translation of English benchmarks fails to capture culturally-bound knowledge; localization by replacing culture-specific entities with target-culture equivalents exposes model gaps that translation masks. Annotators identify culturally-laden elements and substitute them with functionally equivalent items (e.g., Thanksgiving/turkey → Dragon Boat Festival/zongzi for Chinese). Questions without cultural elements receive direct translation only. This requires distinct knowledge graphs per linguistic community, not merely language fluency.

### Mechanism 2: LLM-as-Adjudicator Correlation with Human Judgment
GPT-4's pairwise preference judgments correlate sufficiently with human annotators to serve as a scalable proxy for multilingual generative evaluation. GPT-4 receives a prompt with baseline output and candidate model output, then selects which response better satisfies quality criteria. Win rate is computed across all questions. This assumes GPT-4's multilingual competence exceeds the models being evaluated, enabling reliable discrimination.

### Mechanism 3: Localization Subset as Cultural Sensitivity Probe
The localization subset provides a higher-resolution signal for cultural competence than the full translated dataset. Separate evaluation on localization subset isolates performance on culturally-specific knowledge from general language proficiency, as localized questions require target-culture knowledge not present in source. Localization subset difficulty is driven by cultural knowledge gaps, not translation artifacts.

## Foundational Learning

- **Generative vs. Discriminative Evaluation**: Generative evaluation captures different capabilities (fluency, coherence, cultural appropriateness) than discriminative tasks (MMLU-style exact match). Quick check: If a model achieves 90% accuracy on translated MMLU but 30% win rate on OMGEval, what does this suggest about the evaluation modalities?

- **Cultural Localization vs. Translation**: Localization requires understanding what makes two entities functionally equivalent across cultures (e.g., "Renaissance period" → "明朝" requires mapping historical significance, not just word-for-word translation). Quick check: Translate "Who is Larry Page?" to Chinese, then localize it. What entity substitution preserves the question's intent for a Chinese speaker?

- **LLM-as-Judge Reliability Constraints**: GPT-4 adjudication enables scalability but introduces dependency on the judge's own multilingual/cultural competence and potential systematic biases. Quick check: If GPT-4 systematically prefers verbose responses, how would this affect win-rate comparisons between a concise model and a verbose model?

## Architecture Onboarding

- **Component map**: AlpacaEval (805 English prompts) → GPT-4 Translation → Manual Localization → Manual Verification → OMGEval Dataset → Model outputs → GPT-4 Adjudicator vs. GPT-3.5-Turbo baseline → Win Rate + Standard Error

- **Critical path**: Localization (Step 2) determines benchmark quality. Poor localization produces noisy evaluation. Table 1's examples show the intended standard: "Hawaii" → "三亚" (both vacation destinations), not "泰山" (mountain with different cultural function).

- **Design tradeoffs**:
  - GPT-4 adjudicator vs. human evaluation: Scalability vs. validation breadth. Human correlation verified only for 100 samples × 2 languages
  - GPT-3.5-Turbo baseline vs. stronger baseline: Chosen for availability after text-davinci-003 deprecation, but may compress win-rate distribution if baseline is too strong
  - 5 languages vs. broader coverage: Pragmatic constraint; method extensible but requires native-speaking annotators per language

- **Failure signatures**:
  - Cultural hallucination: Model generates fluent but factually incorrect cultural knowledge
  - Cross-lingual inconsistency: High English win rate with low localized-subset win rate indicates cultural knowledge gap
  - Judge-model correlation breakdown: If GPT-4's preferences diverge from human judgment on specific languages, win rates may not reflect human-perceived quality

- **First 3 experiments**:
  1. Baseline sanity check: Run GPT-3.5-Turbo against itself. Expected win rate: 50% ± noise. Deviation indicates prompt or adjudicator instability
  2. Localization delta analysis: Compare full-set vs. localization-subset win rates for your model across languages. Large drops (>5%) indicate cultural knowledge as primary weakness
  3. Adjudicator robustness test: For a 50-sample subset, run both GPT-4 adjudication and human evaluation. Compute agreement. If F1 < 0.7, adjudicator may be unreliable

## Open Questions the Paper Calls Out

### Open Question 1
Can the OMGEval construction pipeline be effectively extended to low-resource languages while maintaining quality of localization and evaluation? The current methodology relies on GPT-4 for preliminary translation and human annotators with master's degrees in linguistics, resources which may not be available for "low-resourced" or "under-represented" languages. Constructing an OMGEval subset for a low-resource language and comparing inter-annotator agreement and GPT-4 adjudication consistency against current high-resource baselines would address this.

### Open Question 2
How does the uneven distribution of capability types affect the reliability of the "Win Rate" as a holistic measure of model performance? It is unclear if a model's high win rate is driven primarily by proficiency in over-represented knowledge categories, potentially masking deficiencies in safety or conversational nuance. Re-balancing the dataset to uniform capability distribution and re-evaluating the models to see if the ranking changes significantly would address this.

### Open Question 3
Does GPT-4 exhibit specific evaluation biases when adjudicating localized content for non-Latin languages (e.g., Arabic, Russian) compared to Latin-based languages? The paper validates the GPT-4 adjudicator against human judgment only for Chinese and Spanish, leaving the validity of the adjudicator for Russian, French, and Arabic assumed rather than empirically proven. Performing human vs. GPT-4 correlation analysis specifically on the Arabic and Russian localization subsets would address this.

## Limitations
- Reliance on GPT-4 as both translation initiator and adjudicator creates potential circular validation
- Human evaluation validation limited to only two languages (Chinese and Spanish)
- Localization methodology lacks quantitative measures of cultural equivalence quality
- Current benchmark covers only five high-resource languages, limiting global applicability

## Confidence

- **High confidence**: The benchmark construction methodology and data collection process are well-documented and reproducible
- **Medium confidence**: The human-GPT-4 correlation results (F1 0.79-0.91) are promising but limited in scope
- **Medium confidence**: The claim that cultural localization reveals different model capabilities than direct translation, though mechanism is sound

## Next Checks

1. **Adjudicator bias validation**: Run human evaluation on 100 samples × 3 additional languages (Arabic, Russian, French) to verify GPT-4 adjudication reliability across all benchmark languages

2. **Localization quality audit**: Select 50 localized questions per language and have independent annotators assess whether cultural substitutions preserve functional equivalence using standardized rubrics

3. **Cross-modal consistency test**: Evaluate the same models on OMGEval localization subset versus direct translation of English cultural benchmarks to quantify the additional signal provided by localization