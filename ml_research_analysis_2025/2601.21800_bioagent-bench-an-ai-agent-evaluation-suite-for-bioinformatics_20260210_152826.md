---
ver: rpa2
title: 'BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics'
arxiv_id: '2601.21800'
source_url: https://arxiv.org/abs/2601.21800
tags:
- agent
- data
- evaluation
- bioinformatics
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioAgent Bench is a benchmark dataset and evaluation suite for
  assessing AI agents on realistic bioinformatics workflows. It includes 10 tasks
  (e.g., RNA-seq, variant calling, metagenomics) with prompts specifying concrete
  outputs for automated grading, plus stress tests under controlled perturbations.
---

# BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics

## Quick Facts
- arXiv ID: 2601.21800
- Source URL: https://arxiv.org/abs/2601.21800
- Reference count: 35
- Primary result: Evaluates AI agents on 10 bioinformatics pipelines, showing frontier models achieve high completion rates (>90%) without custom scaffolding while open-weight models lag in agentic competence.

## Executive Summary
BioAgent Bench introduces a benchmark dataset and evaluation suite for assessing AI agents on realistic bioinformatics workflows. The suite includes 10 tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts specifying concrete outputs for automated grading, plus stress tests under controlled perturbations. The study evaluates frontier closed-source models (e.g., Claude, GPT) and open-weight models across agent harnesses, using an LLM grader to score pipeline progress and result validity. Results show frontier agents achieve high pipeline completion rates without elaborate scaffolding, while open-weight models exhibit weaker agentic capabilities. Perturbation tests reveal failure modes in step-level reasoning under corrupted inputs, decoy files, and prompt bloat.

## Method Summary
The benchmark provides 10 bioinformatics tasks with natural-language prompts, input data files, and optional reference data. Agents run in harnesses (Claude Code, Codex CLI, OpenCode) with a system prompt enabling tool-calling and file I/O. A GPT-5.1 grader scores steps completed, final result reached, results match, and F1-score (for GIAB). Evaluation is performed across frontier and open-weight models with resource caps (<4 hours, ≤48GB RAM). Robustness is tested via perturbation modules injecting corrupted inputs, decoys, and prompt bloat. The study reports completion rates, planning quality correlations, and failure mode analysis under perturbations.

## Key Results
- Frontier agents complete multi-step bioinformatics pipelines without custom scaffolding, achieving >90% completion rates
- Planning quality correlates with completion (Pearson r=0.61), but success requires baseline domain knowledge
- Open-weight models show lower completion due to weaker agentic capabilities, not purely knowledge deficits
- Perturbation tests reveal shallow file-selection heuristics that bypass biological grounding
- Pipeline completion alone does not guarantee robustness under corrupted inputs and decoy conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frontier models achieve high completion because planning quality and agentic error-recovery compound across execution steps
- Mechanism: Stronger planning generates coherent multi-step tool sequences. When errors occur, models recover rather than terminate. This creates a gap between plan quality (r=0.61 with completion) and final artifact production.
- Core assumption: Planning scores proxy for ability to maintain coherent internal state across 5-10 tool invocations
- Evidence anchors: [abstract] frontier agents complete pipelines without elaborate scaffolding; [section 6] planning quality correlates with performance (r=0.61); success requires baseline domain knowledge
- Break condition: If planning scores become uncorrelated with completion (r<0.3), or error-recovery loops dominate token budget

### Mechanism 2
- Claim: Perturbation tests expose shallow file-selection heuristics that bypass biological grounding
- Mechanism: Under decoy/corruption conditions, agents rely on surface cues (filename suffixes, default databases) rather than validating input integrity. This reveals that high-level pipeline construction masks brittle step-level reasoning.
- Core assumption: Biological reasoning requires context-sensitive validation that current agents lack
- Evidence anchors: [abstract] robustness tests reveal failure modes under perturbations; [section 6.2] agent globbed all inputs matching .genomic.fna suffix, including decoy organism; [corpus] similar shallow-reasoning failures in SEC-bench
- Break condition: If agents validate inputs against task context (e.g., check organism consistency before alignment), perturbation failures drop sharply

### Mechanism 3
- Claim: Open-weight models' lower completion stems from weaker agentic competence, not purely knowledge deficits
- Mechanism: Open models produce lower-quality explicit plans AND exhibit more error-correction loops or premature termination. The gap persists even when task-specific knowledge is provided via prompts.
- Core assumption: Agentic competence (tool-use reliability, state tracking, error recovery) is partially independent from domain knowledge
- Evidence anchors: [section 6] open-weight models receive lower planning scores; shortcomings in agentic capabilities remain bottleneck; [table 4] GLM-4.7 achieves 82.5% in Codex CLI but drops to 75% in Claude Code
- Break condition: If providing gold-standard tool-calling sequences closes performance gap, mechanism shifts to knowledge-dominant

## Foundational Learning

- Concept: Bioinformatics pipeline stages (QC → alignment → quantification → downstream analysis)
  - Why needed here: Agents must sequence tools correctly; misordering produces invalid artifacts that graders flag
  - Quick check question: Can you name the canonical order for RNA-seq differential expression analysis?

- Concept: Input vs reference data in genomic workflows
  - Why needed here: Decoy tests exploit confusion between sample-specific inputs and shared references
  - Quick check question: For viral metagenomics, is a bacterial taxonomy database appropriate as reference data?

- Concept: Perturbation testing as proxy for robustness
  - Why needed here: Vanilla completion rates overestimate reliability; corrupted inputs and decoys reveal hidden failure modes
  - Quick check question: What are the three perturbation types used, and which caused largest completion drop?

## Architecture Onboarding

- Component map: Prompt + input data → agent tool calls → intermediate artifacts → final CSV/TSV → grader evaluation
- Critical path: Evaluation harness executes tasks in sandbox, captures transcripts; agent harness wraps model with tool-calling, file I/O; LLM grader scores pipeline progress and result validity
- Design tradeoffs:
  - LLM grading enables flexible rubrics but introduces subjectivity and run-to-run variance
  - Resource caps (<4 hours, ≤48GB RAM) improve reproducibility but exclude large-genome workflows
  - Prompt bloat tests distraction robustness but may conflate context-length issues with reasoning failures
- Failure signatures:
  - Repeated error-correction loops without progress (frontier models)
  - Premature termination before final artifact (open-weight models)
  - Glob-based file selection ignoring biological context (both, under decoys)
  - Proceeding despite detected corruption (7/10 tasks continued after flagging)
- First 3 experiments:
  1. Run vanilla vs decoy vs corrupted on single task (e.g., giab) with 3 trials each; compare transcripts for file-selection heuristics
  2. Swap harness (Codex CLI → Claude Code) on top 2 open-weight models to isolate harness effects from model effects
  3. Add explicit input-validation step to system prompt; re-run perturbation suite to measure failure-mode reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the performance gap between open-weight and closed-source models primarily driven by lack of bioinformatics domain knowledge or deficiency in multi-turn agentic capabilities?
- Basis in paper: [explicit] Discussion hypothesizes deficit is not purely "bioinformatics knowledge," but combination of weaker end-to-end agentic competence and reduced ability to form/maintain high-quality execution plans
- Why unresolved: Paper observes correlation between plan ratings and completion (r=0.61) but notes relationship is "not deterministic," leaving specific bottleneck unidentified
- What evidence would resolve it: Ablation study where open-weight models are provided with gold-standard high-level plans to isolate execution ability from planning/knowledge retrieval

### Open Question 2
- Question: How does agent performance degrade when tasks require discovering and justifying external references currently provided as inputs?
- Basis in paper: [explicit] Conclusion states intent to "add tasks that require sourcing and justifying external references"; Limitations note current setup omits common research tasks such as finding, downloading, and staging large reference resources
- Why unresolved: Benchmark constraints (4h runtime, 48GB RAM) necessitated providing references as inputs, excluding data curation phase from evaluation
- What evidence would resolve it: Evaluation of agents on new task suite requiring autonomous retrieval of multi-gigabyte references (e.g., human reference genome) from public databases, measured by retrieval accuracy and correct versioning

### Open Question 3
- Question: What is the statistical variance of agent failure modes under adversarial perturbations (e.g., corrupt inputs) across multiple trials?
- Basis in paper: [explicit] Limitations state "Perturbation analysis uses single trial per task and condition, limiting uncertainty estimates... results should be treated as suggestive"
- Why unresolved: Reported robustness metrics (e.g., 7/10 corruption detection) rely on N=1, making it impossible to distinguish systematic errors from stochastic noise or harness-specific artifacts
- What evidence would resolve it: Running perturbation suite with N>10 trials per task/condition to establish confidence intervals for failure rates (e.g., decoy usage rates)

## Limitations

- Subjective LLM grading introduces inherent subjectivity and potential bias, with weak correlation (r=0.31) between manual and automatic grading for pipeline completion
- Model version ambiguity - exact API versions and endpoint configurations for closed-source models are not specified, making exact reproduction difficult
- Domain-specific knowledge requirements - tasks require substantial bioinformatics knowledge that may not be well-represented in general-purpose language models

## Confidence

- High confidence (4/5): Core finding that frontier models achieve high completion rates (>90%) without custom scaffolding is well-supported by direct results across 10 tasks and 3 agent harnesses
- Medium confidence (3/5): Mechanism linking planning quality to completion (r=0.61 correlation) is plausible but lacks direct causal evidence
- Medium confidence (3/5): Conclusion that open-weight models' limitations stem from agentic competence rather than knowledge deficits is partially supported but lacks direct decomposition evidence

## Next Checks

1. **Grading reliability test:** Run the same task three times with identical inputs and compare LLM grader outputs for consistency. Calculate inter-run variance to quantify grading reliability.

2. **Perturbation robustness validation:** For tasks where agents proceeded despite corrupted inputs (7/10), manually inspect final artifacts to verify whether they contain or reflect the corruption, confirming the grader's assessment.

3. **Harness effect isolation:** Take the top two open-weight models and run them through all three harnesses (Codex CLI, Claude Code, OpenCode) on a representative task to quantify harness-specific effects versus model-specific effects.