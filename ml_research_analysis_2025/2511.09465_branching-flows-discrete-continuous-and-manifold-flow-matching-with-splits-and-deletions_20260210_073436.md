---
ver: rpa2
title: 'Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits
  and Deletions'
arxiv_id: '2511.09465'
source_url: https://arxiv.org/abs/2511.09465
tags:
- flows
- branching
- distribution
- which
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Branching Flows is a generative modeling framework that extends
  diffusion and flow matching to variable-length sequences with elements in continuous,
  discrete, manifold, or multimodal spaces. It achieves this by augmenting a base
  Markov generator with a branching and deletion process, where elements evolve along
  binary trees with learned split and deletion rates.
---

# Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions

## Quick Facts
- **arXiv ID**: 2511.09465
- **Source URL**: https://arxiv.org/abs/2511.09465
- **Reference count**: 22
- **Primary result**: Extends flow matching to variable-length sequences via binary tree branching with learned split/deletion rates, achieving superior distribution matching on small molecules and protein structures.

## Executive Summary
Branching Flows is a generative modeling framework that enables variable-length sequence generation over continuous, discrete, manifold, or multimodal state spaces. The key innovation is augmenting a base Markov generator with a branching and deletion process, where elements evolve along binary trees with learned split and deletion rates. This allows control over sequence length during generation while maintaining compatibility with any flow matching base process. The method was demonstrated on three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), showing state-of-the-art distribution matching and the ability to generate variable-length chains.

## Method Summary
The method extends diffusion and flow matching to variable-length sequences by augmenting a base Markov generator with a branching and deletion process. During training, a latent variable Z specifying the complete tree structure is sampled from the data, and the model learns split and deletion rates as hazard functions that approach infinity as t→1. The loss combines three Bregman divergences for base process, split rate, and deletion rate predictions. The framework composes with any flow matching base process, enabling multimodal generation by handling element-wise transitions independently while the branching mechanism handles count changes. At inference, sequences evolve via Euler steps with learned hazards determining when elements split or delete.

## Key Results
- On QM9 small molecules, Branching Flows achieved 1-KS D statistics of 0.91-0.99 compared to 0.87-0.97 for transdimensional jump diffusion models.
- For antibody sequences, Branching Flows matched the distribution matching performance of an oracle-length model with 1-KS D scores of 0.99 for length matching.
- The protein backbone model could generate variable-length chains and perform "unknown-length infix sampling" with generated structures showing geometry consistent with state-of-the-art folding models (scTM scores near 1.0).

## Why This Works (Mechanism)

### Mechanism 1: Binary Tree Branching for Variable-Length Sequences
The model enables variable-length sequence generation by augmenting a base Markov generator with a binary tree structure where elements split and delete stochastically. A forest of binary trees couples initial samples (roots) to data samples (leaves), with each element evolving independently along branches via a base generator while bifurcating or being deleted. Split and deletion rates are learned as hazard functions that approach infinity as t→1, ensuring all required splits/deletions occur by generation end.

### Mechanism 2: Generator Matching with Auxiliary Latent Trees
The model learns the marginal generator via conditional generator matching loss that marginalizes over the branching tree structure. During training, a latent variable Z=(X^ø_1, X_0, T, A) is sampled specifying the complete tree structure. The model trains against conditional generators that know the tree structure, but via the Auxiliary Generator Matching principle, the marginal generator is learned without explicitly exposing G_t to the model.

### Mechanism 3: Composable Base Processes for Multimodal State Spaces
The branching/deletion mechanism operates independently of per-element state evolution. The base generator handles element-wise transitions (e.g., OU process for continuous, DFM for discrete), while the branching mechanism handles count changes. The total generator is a sum of linear parameterizations, allowing per-term Bregman divergences and enabling multimodal generation.

## Foundational Learning

- **Concept: Continuous-Time Markov Chains (CTMCs) and hazard functions**
  - Why needed here: Split and deletion processes are CTMCs where transition rates are hazard functions that must explode near t=1 to guarantee completion.
  - Quick check question: If h(t) = β/(1-t) for t<1, what's the probability of an event occurring before t=0.9?

- **Concept: Generator Matching and Bregman divergences**
  - Why needed here: The framework learns the infinitesimal generator of a process via Bregman divergence losses against conditional generators.
  - Quick check question: For a Poisson process with rate λ, what Bregman divergence corresponds to the log-likelihood loss?

- **Concept: Ornstein-Uhlenbeck bridges**
  - Why needed here: The continuous base process uses endpoint-conditioned OU bridges with explicit transition densities.
  - Quick check question: In an OU bridge from x_0 to x_1 with mean reversion θ, how does the variance at intermediate time t depend on the endpoint variance?

## Architecture Onboarding

- **Component map**: Input (t, X_t) with RFF positions + RoPE sequence order + pair features → Transformer (12L) with adaLN time conditioning, IPA/attention → Heads (parallel): Position predictor → F^base_t (continuous), Rotation predictor → F^base_t (SO3), Token predictor → F^base_t (discrete), Split predictor → R^θ_t, Deletion predictor → ρ^θ_t → Loss: Sum of Bregman divergences per component

- **Critical path**: 1) Sample Z (tree + anchors) from data X_1, 2) Sample X_t from conditional path using OU bridge sampler, 3) Compute conditional targets (R^Z,G_t, ρ^Z,G_t, F^Z,G_t,base), 4) Forward pass through model, 5) Compute per-element losses and backprop

- **Design tradeoffs**: 
  - Anchor strategy: Geodesic interpolation for continuous vs mask tokens for discrete (trade-off between smooth paths and learnability)
  - Tree sampling: Uniform adjacent merge is simple but may not respect domain structure (domain-specific trees could provide stronger inductive bias)
  - Auxiliary vs Augmented GM: Marginalizing over G_t simplifies architecture but loses explicit branch tracking

- **Failure signatures**: Generated sequences have wrong length distribution (check hazard distribution sampling), elements don't reach correct final positions (check base process mean-reversion), training instability with split/deletion losses (verify Bregman divergence scaling)

- **First 3 experiments**: 
  1. Ablate deletion-only vs branching-only vs both on QM9, measure distribution matching (KS statistics)
  2. Vary tree sampling strategies: uniform adjacent merge vs geodesic-weighted merge on protein backbone generation
  3. Test base process noise levels on proteins: sweep OU variance parameters and measure impact on late-time split handling

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does domain-specific tailoring of the branching tree structure and internal anchors improve performance compared to the generic "uniform adjacent merge" strategy?
- **Open Question 2**: What are the relative benefits and trade-offs of using branching-only, deletion-only, or combined branching-and-deletion processes for different data modalities?
- **Open Question 3**: Can Branching Flows be effectively adapted for unordered or permutation-invariant sets without imposing an artificial sequence ordering?
- **Open Question 4**: Can dynamic coupling schemes (updating pairings during the flow) provide a tractable alternative to the current method of pre-sampling the full tree latent Z?

## Limitations

- **Sampling Complexity**: Tree structure and anchor sampling requires O(n²) operations for n-element sequences, potentially significant for long sequences.
- **Theoretical Foundations**: Practical implications of theoretical assumptions about discrete G_t and linear parameterization remain unclear.
- **Scalability Concerns**: Method requires careful hyperparameter tuning for different sequence lengths and modalities.

## Confidence

- **High Confidence** (Mechanism 1 - Binary Tree Branching): Core mathematical framework is well-established in CTMC theory with clearly specified implementation details.
- **Medium Confidence** (Mechanism 2 - Generator Matching): Theoretically sound but involves approximations whose effects on convergence and sample quality are not fully characterized.
- **Medium Confidence** (Mechanism 3 - Composable Base Processes): Conceptually straightforward but interplay between base process noise and branching rates could create training instabilities.

## Next Checks

1. **Ablation Study**: Test deletion-only vs branching-only vs combined mechanisms on QM9 to isolate the contribution of each mechanism to distribution matching performance.

2. **Tree Sampling Sensitivity**: Compare uniform adjacent merge vs domain-informed tree structures on protein backbone generation, measuring scTM scores and chain break rates.

3. **Base Process Noise Sweep**: On protein structures, systematically vary OU variance parameters and measure impact on late-time split handling, where high flow velocities are needed to reach correct final positions.