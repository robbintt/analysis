---
ver: rpa2
title: Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed
  Conformal Prediction
arxiv_id: '2510.15233'
source_url: https://arxiv.org/abs/2510.15233
tags:
- uncertainty
- coverage
- prediction
- intervals
- tessera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TESSERA, a novel uncertainty quantification
  method for protein-ligand affinity prediction that combines Mixture-of-Experts (MoE)
  uncertainty decomposition with conformal prediction calibration. TESSERA addresses
  the challenge of providing reliable, adaptive, and informative per-sample uncertainty
  in drug discovery, where assay noise is heterogeneous, chemical space is imbalanced,
  and distribution shifts are common.
---

# Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction

## Quick Facts
- arXiv ID: 2510.15233
- Source URL: https://arxiv.org/abs/2510.15233
- Authors: Amitesh Badkul; Lei Xie
- Reference count: 40
- Key outcome: TESSERA achieves near-nominal coverage (PICP ≈ 0.91 at 1-α = 0.90), competitive interval efficiency (MPIW = 4.03, NMPIW = 0.17), and strong adaptivity (AUSE = 0.64) on protein-ligand binding affinity prediction under i.i.d. and scaffold-based OOD splits.

## Executive Summary
This work introduces TESSERA, a novel uncertainty quantification method for protein-ligand affinity prediction that combines Mixture-of-Experts (MoE) uncertainty decomposition with conformal prediction calibration. TESSERA addresses the challenge of providing reliable, adaptive, and informative per-sample uncertainty in drug discovery, where assay noise is heterogeneous, chemical space is imbalanced, and distribution shifts are common. The method decomposes uncertainty into epistemic (expert disagreement) and aleatoric (per-expert variance) components, then applies split conformal calibration to produce prediction intervals with finite-sample coverage guarantees.

## Method Summary
TESSERA uses a Mixture-of-Experts architecture with K=4 experts, each producing mean and variance predictions. Expert disagreement (epistemic) is computed as the unweighted standard deviation of expert means, while per-expert variance (aleatoric) is the weighted average of expert variances. Split conformal calibration normalizes residuals by these uncertainty scores to produce prediction intervals. The method is evaluated on CHEMBL31 with both random and scaffold-based splits, using ESM-2 protein encoders with LoRA and SimSGT molecular encoders.

## Key Results
- Near-nominal coverage: PICP ≈ 0.91 at 1-α = 0.90
- Competitive efficiency: MPIW = 4.03, NMPIW = 0.17
- Strong adaptivity: AUSE = 0.64
- Outperforms baselines including MC Dropout, RIO-GP, and eMOSAIC

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Expert disagreement in MoE architectures provides a useful proxy for epistemic uncertainty that correlates with prediction difficulty.
- **Mechanism:** When multiple experts produce divergent mean predictions for the same input, this signals regions of model uncertainty or data scarcity. The disagreement is quantified as the unweighted standard deviation of expert means (Eq. 6), which concentrates in OOD regions.
- **Core assumption:** Expert diversity captures meaningful uncertainty rather than random initialization artifacts; routing patterns reflect genuine ambiguity in the input space.
- **Evidence anchors:**
  - [abstract]: "The MoE learns two key signals: expert disagreement (epistemic uncertainty)"
  - [section B, p.11]: "We call this as expert disagreement, it is a well-used epistemic proxy in ensemble-like systems"
  - [corpus]: Moderate support—related conformal frameworks leverage ensemble disagreement, but direct MoE-conformal combinations remain underexplored.
- **Break condition:** If experts collapse to similar predictions across all inputs (mode collapse), or if gating becomes degenerate (single expert dominates), disagreement signal degrades.

### Mechanism 2
- **Claim:** Per-expert variance heads trained with NLL loss learn input-dependent aleatoric uncertainty that reflects heteroscedastic noise.
- **Mechanism:** Each expert outputs both mean and variance; training with Gaussian NLL (Eq. 4) forces variance heads to scale appropriately with local observation noise. The aleatoric score is the weighted average of expert variances (Eq. 5).
- **Core assumption:** NLL training produces well-specified variance estimates; residual distributions are approximately Gaussian conditional on input.
- **Evidence anchors:**
  - [abstract]: "per-expert variance (aleatoric uncertainty)"
  - [section B, p.11]: "NLL is a proper scoring rule for probabilistic regression and is standard when learning aleatoric uncertainty"
  - [corpus]: Standard approach in heteroscedastic regression; no novel claim here.
- **Break condition:** If variance heads collapse to near-zero (overfitting) or explode (underfitting), calibration fails.

### Mechanism 3
- **Claim:** Normalized split-conformal calibration maps raw MoE uncertainty signals to prediction intervals with finite-sample marginal coverage guarantees under exchangeability.
- **Mechanism:** Nonconformity scores are computed as scaled residuals |y - μ̂| / S(x) where S is either E or A. The (1-α) quantile of calibration scores determines interval width scaling (Eq. 7-8). This preserves coverage while allowing adaptive widths.
- **Core assumption:** Calibration and test data are exchangeable (approximately i.i.d.); scaffold splits violate this, so guarantees degrade under OOD shift.
- **Evidence anchors:**
  - [abstract]: "calibrates these signals via split-conformal prediction to produce per-sample prediction intervals with distribution-free coverage guarantees"
  - [section B, p.12]: "Under the exchangeability assumption, regardless of the choice of S(x), the conformal procedure ensures that the interval covers... with probability 1-α"
  - [section 3, p.6]: "conformal prediction guarantees finite-sample marginal coverage under exchangeability, these guarantees can degrade when the test distribution differs"
  - [corpus]: Temporal Conformal Prediction (TCP) and Domain-Shift-Aware CP papers explicitly address exchangeability violations.
- **Break condition:** Under severe distribution shift where S(x) systematically underestimates or overestimates residual scale, intervals may under-cover or become inefficiently wide.

## Foundational Learning

- **Concept: Split Conformal Prediction**
  - Why needed here: Core calibration mechanism; without understanding quantile-based interval construction, the method appears as a black box.
  - Quick check question: Given calibration scores [0.1, 0.3, 0.5, 0.7, 0.9], what is the 80% prediction interval multiplier for a test point with uncertainty score 2.0?

- **Concept: Aleatoric vs. Epistemic Uncertainty Decomposition**
  - Why needed here: TESSERA explicitly separates these; interpreting results requires knowing which signal to trust for what decision.
  - Quick check question: If you observe high expert disagreement but low per-expert variance, what type of uncertainty dominates and what action does this suggest?

- **Concept: Mixture-of-Experts Routing**
  - Why needed here: Uncertainty signals derive from expert diversity; understanding gating mechanisms explains when/why signals degrade.
  - Quick check question: A gating network outputs [0.97, 0.01, 0.01, 0.01] for an input—what does this imply for the quality of the disagreement signal?

## Architecture Onboarding

- **Component map:**
  [Protein Encoder (ESM-2 + LoRA)] ─┬─> [Concatenation] ─> [MoE Fusion Module] ─> [Expert Heads (μ, σ²)]
  [Ligand Encoder (SimSGT/UniMol)] ─┘                          │
                                           [Gating Network (MLP + Softmax)]

- **Critical path:** The uncertainty signals (E, A) are only as good as expert diversity. If training collapses experts to similarity, conformal calibration has no useful signal to scale.

- **Design tradeoffs:**
  - K=4 experts balances diversity vs. computational overhead; fewer experts reduce disagreement signal
  - Unweighted disagreement (Eq. 6) chosen over weighted because "gating can be peaky"—trades theoretical purity for robustness
  - Separate calibration of E vs. A allows user choice but requires storing two quantiles

- **Failure signatures:**
  - PICP significantly below nominal (e.g., <0.85): Exchangeability violated or uncertainty signal miscalibrated
  - Constant-width intervals across all test points: S(x) not varying; check expert diversity
  - Extremely wide intervals: Calibration set contains outliers inflating quantile

- **First 3 experiments:**
  1. **Sanity check:** On random split (i.i.d.), verify PICP ≈ 0.90 ± 0.02; deviation indicates implementation error
  2. **Ablation:** Compare TESSERA-E vs. TESSERA-A on scaffold split—does one signal dominate for OOD detection?
  3. **Encoder swap:** Run with frozen vs. fine-tuned ligand encoder; assess whether better representations improve efficiency (MPIW) without affecting validity (PICP)

## Open Questions the Paper Calls Out

- **Question:** Does TESSERA maintain reliable uncertainty quantification under protein-conditioned distribution shifts (unseen targets) in addition to ligand-side scaffold shifts?
- **Basis:** [explicit] The authors state, "our OOD evaluation focuses on ligand-side scaffold shift. In drug discovery, other shifts matter: protein splits probe generalization across unseen targets, we plan to test TESSERA under such protein-conditioned splits next."
- **Why unresolved:** The current evaluation is limited to chemical scaffold splits; the method's robustness to shifts in the protein domain (the other half of the protein-ligand interaction) remains untested.
- **What evidence would resolve it:** Empirical results showing coverage (PICP) and efficiency (MPIW) metrics on dataset splits specifically designed to isolate unseen protein targets.

- **Question:** Can synthetic controls and targeted perturbations validate the disentanglement of epistemic and aleatoric uncertainty components?
- **Basis:** [explicit] The paper notes that "ground-truth decompositions are unobservable, disentanglement can only be assessed indirectly. A stronger validation needs synthetic controls and targeted perturbations."
- **Why unresolved:** While the two signals show low correlation, the paper admits this is only indirect evidence that the expert disagreement represents epistemic uncertainty and variance represents aleatoric uncertainty.
- **What evidence would resolve it:** Experiments using datasets where noise is artificially injected (to test aleatoric tracking) or training data is selectively removed (to test epistemic sensitivity) to confirm the components react as theoretically expected.

- **Question:** Can shift-aware conformal variants (e.g., weighted CP) improve TESSERA's guarantees when the exchangeability assumption is violated?
- **Basis:** [explicit] The authors list as a limitation that "conformal prediction guarantees... can degrade when the test distribution differs from calibration... This motivates shift-aware conformal variants... as natural next steps."
- **Why unresolved:** The standard split-conformal method used relies on exchangeability, which may not hold in complex drug discovery distribution shifts; the efficacy of advanced conformal corrections for this specific MoE architecture is unknown.
- **What evidence would resolve it:** Comparative benchmarks showing that a weighted or adaptive conformal implementation yields higher coverage stability than standard split-conformal calibration under severe covariate shift.

## Limitations
- Conformal coverage guarantees degrade under distribution shift, potentially limiting reliability for severe OOD cases
- Expert disagreement signals may fail if MoE training causes expert collapse or gating degeneracy
- Normalized conformal approach efficiency claims lack thorough ablation studies against alternative scaling methods

## Confidence
- **High:** TESSERA's core architecture (MoE with epistemic/aleatoric decomposition) and standard conformal calibration procedure are well-established
- **Medium:** Claims about coverage maintenance under OOD shift and efficiency improvements require more rigorous validation
- **Low:** The assertion that TESSERA provides "reliable" uncertainty quantification for drug discovery decisions in OOD scenarios, given the known limitations of conformal guarantees under exchangeability violations

## Next Checks
1. **Exchangeability Stress Test:** Systematically evaluate TESSERA's coverage degradation across varying degrees of domain shift (e.g., chemical similarity thresholds between calibration and test sets) to quantify the reliability of uncertainty estimates under realistic OOD conditions
2. **Expert Diversity Diagnostics:** Implement monitoring for expert collapse during training and test whether disagreement signals remain meaningful when gating becomes degenerate (e.g., tracking entropy of gating weights)
3. **Calibration Set Size Sensitivity:** Assess how TESSERA's coverage and efficiency metrics vary with calibration set size to determine the minimum data requirements for reliable uncertainty quantification in low-data drug discovery scenarios