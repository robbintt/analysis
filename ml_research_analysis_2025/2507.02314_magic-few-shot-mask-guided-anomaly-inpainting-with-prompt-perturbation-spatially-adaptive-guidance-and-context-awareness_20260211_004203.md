---
ver: rpa2
title: 'MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially
  Adaptive Guidance, and Context Awareness'
arxiv_id: '2507.02314'
source_url: https://arxiv.org/abs/2507.02314
tags:
- anomaly
- image
- mask
- generated
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MAGIC improves few-shot mask-guided anomaly generation by introducing
  three complementary components: Gaussian prompt perturbation (GPP) to prevent model
  overfitting and enhance in-distribution diversity, spatially adaptive guidance (SAG)
  to apply distinct guidance strengths to anomaly and normal regions, and context-aware
  mask alignment (CAMA) to ensure plausible mask placement. MAGIC achieves new state-of-the-art
  results in downstream anomaly detection, localization, and classification tasks,
  with an average anomaly classification accuracy of 78.06% across MVTec-AD, VisA,
  and MVTec 3D-AD datasets, outperforming previous methods by 13.81 percentage points.'
---

# MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness

## Quick Facts
- **arXiv ID:** 2507.02314
- **Source URL:** https://arxiv.org/abs/2507.02314
- **Reference count:** 40
- **Key outcome:** MAGIC achieves state-of-the-art results in few-shot mask-guided anomaly generation, with 78.06% classification accuracy (↑13.81pp) and lowest KID scores (40.27) across MVTec-AD, VisA, and MVTec 3D-AD datasets.

## Executive Summary
MAGIC introduces three complementary components to improve few-shot mask-guided anomaly generation: Gaussian prompt perturbation (GPP) to prevent overfitting and enhance diversity, spatially adaptive guidance (SAG) for region-specific fidelity-diversity tradeoff, and context-aware mask alignment (CAMA) for plausible mask placement. The method achieves new state-of-the-art results in downstream anomaly detection, localization, and classification tasks. MAGIC's design addresses key challenges in industrial anomaly synthesis while maintaining realistic textures and preserving normal backgrounds.

## Method Summary
MAGIC builds on DreamBooth fine-tuned inpainting with Stable Diffusion 2, adding three novel components: (1) GPP applies Gaussian noise to prompt embeddings during both training and inference to learn smooth anomaly manifolds; (2) SAG uses spatially-varying classifier-free guidance scales (lower for anomalies, higher for normal regions) with cosine scheduling; (3) CAMA aligns masks to normal images using semantic correspondence and keypoint matching. The method trains on one-third of anomaly images per class and generates 500 anomaly-normal pairs per type. Key hyperparameters include σ=1.0 for GPP, w_m=4.0 for SAG's initial guidance scale, and 50 DDIM steps for denoising.

## Key Results
- Achieves 78.06% average anomaly classification accuracy across MVTec-AD, VisA, and MVTec 3D-AD datasets
- Outperforms previous methods by 13.81 percentage points in classification accuracy
- Achieves lowest KID scores (40.27) for image fidelity and second-best IC-LPIPS scores (0.30) for diversity

## Why This Works (Mechanism)

### Mechanism 1: Gaussian Prompt Perturbation (GPP) for Manifold Learning
Injecting Gaussian noise into prompt embeddings during both training and inference enables sampling from a smooth anomaly manifold, preventing few-shot overfitting while maintaining diversity. During training, perturbation δ ~ N(0, σ²I) is added to the anomaly-token embedding, forcing the model to learn a continuous manifold rather than memorizing discrete training examples. At inference, sampling from this same manifold produces diverse yet in-distribution anomalies.

### Mechanism 2: Spatially Adaptive Guidance (SAG) for Region-Specific Fidelity-Diversity Tradeoff
Applying spatially-varying classifier-free guidance scales—lower for anomaly regions, higher for normal regions—promotes texture diversity while preserving background consistency. During early noisy diffusion steps, a lower CFG scale (w_m) in masked regions encourages exploration of diverse textures, with a cosine schedule gradually increasing this to default w_d = 7.5 for detail refinement.

### Mechanism 3: Context-Aware Mask Alignment (CAMA) for Semantic Plausibility
Transferring mask locations via multi-keypoint semantic correspondence ensures anomalies appear in structurally plausible regions when object poses vary. Three keypoints from a reference anomaly mask are matched to the target normal image using a pretrained correspondence model. The final mask center maximizes center-similarity while constrained to a semantic candidate line and the foreground mask.

## Foundational Learning

- **Classifier-Free Guidance (CFG)**: Understanding the fidelity-diversity tradeoff controlled by guidance scale w is essential for grasping SAG's spatial manipulation. *Quick check:* If you increase CFG scale from 4.0 to 7.5 in the anomaly region, would you expect more or less texture diversity?

- **Textual Inversion and DreamBooth Fine-Tuning**: MAGIC builds on DreamBooth fine-tuned inpainting; GPP perturbs the learned embedding. Grasping how few-shot personalization works clarifies why overfitting occurs and how perturbation regularizes it. *Quick check:* Why does DreamBooth achieve higher fidelity but lower diversity than textual inversion?

- **Diffusion Inpainting Conditioning**: The backbone conditions on concatenated latents [z_t, b, M] where b is the masked background latent. Understanding this architecture is prerequisite to modifying guidance spatially. *Quick check:* What happens if the mask M is misaligned with the actual object location in the normal image?

## Architecture Onboarding

- **Component map:** Normal image I_N -> CAMA (mask alignment) -> Stable Diffusion 2 inpainting (DreamBooth fine-tuned) -> GPP (prompt perturbation) -> SAG (spatial CFG) -> Anomaly image I_A

- **Critical path:** Input: Normal image I_N, generated mask M (from mask generator with GPP). CAMA: Align M to I_N → M_a. Prepare inpainting latent: z_inp = concat(z_t, E(masked I_N), M_a). Apply GPP to prompt embedding: τ̃(P) = τ(P) + δ. SAG-guided denoising: Use w_m for masked region, w_d for normal region. Output: Anomaly image I_A.

- **Design tradeoffs:** σ = 1.0 balances diversity vs. fidelity; higher values increase KID. w_m = 4.0 optimizes classification accuracy; lower values slightly improve diversity but hurt fidelity. CAMA adds ~4 seconds per image; disable for texture-type defects or when object pose is consistent.

- **Failure signatures:** Unrealistic anomaly textures: Check if GPP is applied during training (not just inference); verify σ isn't too high. Corrupted normal regions: Verify SAG is correctly masking regions; ensure w_d is applied to non-mask areas. Misplaced masks on object-centric categories: Ensure CAMA is enabled; check if foreground extraction (U²-Net) is accurate.

- **First 3 experiments:** 1) GPP ablation: Compare (a) no perturbation, (b) inference-only perturbation, (c) training+inference perturbation on a single MVTec-AD category. Expect KID to follow: (b) > (a) > (c). 2) SAG hyperparameter sweep: Vary w_m ∈ {2.5, 3.0, 3.5, 4.0, 4.5, 5.0} and measure KID + classification accuracy. Confirm w_m ≈ 4.0 is optimal. 3) CAMA alignment validation: On object-centric classes (screw, transistor), measure alignment score (mask-foreground IoU) with and without CAMA. Expect +15-25pp improvement.

## Open Questions the Paper Calls Out

### Open Question 1
How can mask-guided anomaly generation frameworks be adapted to synthesize defects involving global object displacements or large structural changes, such as flipped or missing components? The authors explicitly list this as a limitation: "MAG-based generation fails to produce plausible anomalies" for categories involving "large structural changes or global object displacements" (e.g., metal nut flip, transistor misplaced).

### Open Question 2
To what extent does the performance of MAGIC degrade when applied to visually ambiguous domains where the pre-trained foreground segmentation (U2-Net) or semantic correspondence (GeoAware-SC) models fail? The limitations section states: "MAGIC's performance relies on its pre-trained components... so their inherent limitations in visually ambiguous or unseen domains may lead to less precise anomaly placement."

### Open Question 3
How can the Context-Aware Mask Alignment (CAMA) module be made robust to input masks that deviate significantly from the object's morphology? The authors note: "CAMA's accuracy depends on a plausible input mask; significant deviations can reduce alignment precision."

## Limitations
- **Global structural anomalies:** MAGIC fails to generate realistic anomalies for defects involving large structural changes or global object displacements (e.g., flipped or missing components).
- **Dependency on pretrained components:** Performance relies heavily on U2-Net for foreground segmentation and GeoAware-SC for semantic correspondence, which may fail in visually ambiguous or unseen domains.
- **Prompt template specification:** Exact prompt syntax used during DreamBooth fine-tuning is not specified, limiting exact reproducibility.

## Confidence
- **High Confidence:** Core mechanisms of GPP, SAG, and CAMA are well-supported by ablation studies and achieve state-of-the-art results on multiple benchmarks.
- **Medium Confidence:** Specific hyperparameter choices (σ=1.0, w_m=4.0, cosine schedule) are empirically optimal but may require tuning for different datasets or anomaly types.
- **Low Confidence:** Exact prompt template and mask generator training details are unspecified, preventing exact replication of baseline results.

## Next Checks
1. **GPP distribution shift validation:** Generate anomalies with and without training-time GPP on a single MVTec-AD category. Measure KID and classification accuracy to confirm that inference-only perturbation degrades performance by ~15 KID points and ~8 percentage points in accuracy.

2. **SAG hyperparameter sweep:** Systematically vary w_m ∈ {2.5, 3.0, 3.5, 4.0, 4.5, 5.0} on a subset of categories. Plot KID vs. classification accuracy to verify the optimal tradeoff at w_m ≈ 4.0.

3. **CAMA alignment robustness:** On object-centric classes (e.g., screw, transistor), compute mask alignment scores with and without CAMA. Confirm improvement of +15-25pp in alignment IoU and verify that failures occur primarily on global structural anomalies.