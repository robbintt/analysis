---
ver: rpa2
title: 'Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies'
arxiv_id: '2501.18817'
source_url: https://arxiv.org/abs/2501.18817
tags:
- block
- stack
- tasks
- strategy
- clear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two methods to enhance the reasoning abilities
  of smaller, less resource-intensive Large Language Models (LLMs) by leveraging the
  expertise of larger models. The first method involves providing smaller models with
  a generalized strategy for solving tasks within a domain, generated by a larger
  model.
---

# Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies

## Quick Facts
- arXiv ID: 2501.18817
- Source URL: https://arxiv.org/abs/2501.18817
- Authors: Andrey Borro; Patricia J Riddle; Michael W Barley; Michael J Witbrock
- Reference count: 40
- One-line primary result: Small LLMs achieve performance comparable to larger models using generalized strategies and iterative error correction, reducing reasoning costs by nearly 30% on average.

## Executive Summary
This paper addresses the performance gap between small and large language models in reasoning tasks by introducing two methods: generalized strategy injection and iterative error correction. The approach leverages larger models to generate domain-level strategies that guide smaller models, reducing their need for expensive internal reasoning. Experiments on BlocksWorld planning and mathematical reasoning tasks demonstrate that these methods significantly improve small model performance while reducing costs, with generalized strategies reducing reasoning tokens by over 4000 per task and overall costs by 30% on average.

## Method Summary
The paper proposes two complementary approaches to enhance smaller LLM reasoning capabilities. First, a generalized strategy generated by a larger model (like o1) is injected into the prompt context of smaller models (like o1-mini), providing externalized strategic knowledge that reduces the need for internal reasoning discovery. Second, iterative error correction involves re-prompting the model up to four times when it produces incorrect solutions, with experiments showing that simply repeating the original prompt works as well as providing detailed error feedback. The method is evaluated on BlocksWorld planning tasks (5-6 blocks, 16-18 action solutions) and Type 3 CRT mathematical reasoning problems.

## Key Results
- Small models using generalized strategies achieved success rates of 90-98% compared to 42-67% without strategies
- Error correction improved success rates from 10-16% to 90-94% for smaller models
- Incorporating handwritten strategies decreased average reasoning tokens per task by over 4000 tokens
- Total cost per solved task reduced by nearly 30% on average
- Generated strategies were less effective than handwritten ones (86-94% vs 98-100% success)

## Why This Works (Mechanism)

### Mechanism 1: Externalized Strategy as Cognitive Load Reduction
- **Claim:** Offloading strategic knowledge to the prompt context allows smaller models to bypass internal reasoning deficits.
- **Mechanism:** Smaller models lack the internal "System 2" reasoning capabilities of larger models. By injecting a generalized strategy directly into the context window, the small model utilizes non-parameterized knowledge to guide token generation, reducing the need to "discover" the strategy via internal chain-of-thought.
- **Core assumption:** The bottleneck for smaller models is the discovery of a solution strategy rather than the execution of syntactic steps.
- **Evidence anchors:** Strategy incorporation decreased reasoning tokens by over 4000; generated strategy pays off after only 13 BlocksWorld tasks.

### Mechanism 2: Iterative Stochastic Refinement (Not Error Analysis)
- **Claim:** Performance gains from error correction primarily stem from repeated sampling rather than the model's ability to diagnose specific errors from feedback.
- **Mechanism:** Providing detailed error messages yields similar improvements to simply repeating the original prompt, suggesting the mechanism is likely "Best-of-N" sampling where the model generates a different valid path on a subsequent attempt.
- **Core assumption:** Initial failure is often due to noise or lack of focus, resolved by a fresh generation pass rather than logical debugging.
- **Evidence anchors:** No notable difference between detailed error feedback and prompt repetition; repeating incorrect tasks is effective even without error information.

### Mechanism 3: Cost Amortization via Teacher-Student Distillation
- **Claim:** High inference costs can be amortized by generating reusable "soft" policies (textual strategies) once and deploying them cheaply many times.
- **Mechanism:** A resource-intensive model (Teacher) generates a domain-level strategy once at high cost. A resource-efficient model (Student) then uses this strategy for numerous task instances, where the Student's per-token cost is significantly lower (1/5th or 1/20th).
- **Core assumption:** The domain is stable enough that a single strategy applies effectively to multiple distinct task instances.
- **Evidence anchors:** Generated strategy pays off after only 13 BlocksWorld tasks; solving all 50 tasks with o1-mini and handwritten strategy costs less than solving four tasks with o1.

## Foundational Learning

- **Concept: Automated Planning (PDDL)**
  - **Why needed here:** The paper evaluates reasoning using BlocksWorld and PDDL. Planning involves initial state, goal state, and actions with preconditions/effects, distinct from simple text completion.
  - **Quick check question:** Can you explain why a valid plan in PDDL is not just semantically correct text, but a logically valid sequence of state transitions?

- **Concept: Reasoning Tokens vs. Output Tokens**
  - **Why needed here:** Cost analysis relies on OpenAI's o1 architecture separating hidden "reasoning tokens" (internal monologue) from visible "output tokens."
  - **Quick check question:** If a model uses 5000 reasoning tokens but only 200 output tokens, which set drives the financial cost in the paper's analysis?

- **Concept: Generalized Strategy vs. Fine-Tuning**
  - **Why needed here:** The paper explicitly avoids fine-tuning (modifying weights), instead using "non-parameterized knowledge" (prompts).
  - **Quick check question:** Why does the paper claim this method improves "accessibility" compared to fine-tuning a custom model?

## Architecture Onboarding

- **Component map:** Teacher (Strategy Generator) -> Strategy Repository -> Student (Solver) -> Validator -> Loop Manager
- **Critical path:** 1. Strategy Creation: Prompt Teacher with Domain + Instructions -> Output Strategy. 2. Task Execution: Inject Strategy + Task PDDL into Student -> Output Plan. 3. Validation: Simulator runs Plan. 4. Retry: If Invalid -> Re-prompt Student (with or without error details).
- **Design tradeoffs:**
  - Handwritten vs. Generated: Handwritten strategies are significantly more robust (100% success) than generated strategies (90%) on large tasks. Engineers must weigh human expert time vs. performance hit of auto-generated strategies.
  - Feedback Detail: Since detailed error feedback offers no statistical advantage over simple repetition, default to simpler "repeat prompt" method to save token costs.
- **Failure signatures:**
  - Scaling Collapse: On "Large" tasks (18-19 blocks), generated strategies failed to lift performance (0% -> 10%), whereas handwritten strategies succeeded.
  - Strategy Rigidity: If the strategy is flawed, the Student will likely hallucinate a solution that obeys the strategy but fails execution.
- **First 3 experiments:**
  1. Baseline Establishment: Run Student (o1-mini) on BlocksWorld with no strategy to confirm the "reasoning gap."
  2. Strategy Injection: Run Student with a Handwritten strategy to establish the upper bound of performance improvement via context.
  3. Feedback Ablation: Compare "Re-prompt with Error Details" vs. "Re-prompt with Standard Prompt" to verify that specific error messages are unnecessary.

## Open Questions the Paper Calls Out

- **Can the performance gap between LLM-generated strategies and human-written strategies be closed without improving the strategy-generating model?**
  - Basis: Handwritten strategies achieved near-perfect success (98-100%) while generated strategies achieved 86-94%, but the cause remains unknown.
  - What evidence would resolve it: Experiments comparing different strategy-generation prompting techniques or showing improved prompting closes the gap.

- **Can combining generalized strategies with hierarchical planning or problem decomposition enable smaller LLMs to solve larger planning tasks?**
  - Basis: Current methods fail to address sharp performance decline as task size increases.
  - What evidence would resolve it: Demonstrating that hierarchical decomposition + strategies maintains performance on tasks with 20+ blocks.

- **Do generalized strategies reduce reasoning costs across a wider range of models and domains beyond BlocksWorld and CRT?**
  - Basis: Only two domains (planning and mathematical reasoning) were tested; generalizability remains unproven.
  - What evidence would resolve it: Replicating the ~30% cost reduction finding across diverse reasoning domains.

## Limitations
- Strategy Generalization Boundary: Effectiveness may fail when task domains lack clear hierarchical decomposition or when strategies require domain-specific heuristics not easily captured in text.
- Generated vs. Handwritten Strategy Performance Gap: The paper doesn't fully explain why the o1 model fails to generate robust strategies for complex instances, raising questions about automated strategy generation reliability.
- Error Correction Mechanism Ambiguity: Finding that detailed error feedback provides no advantage challenges conventional assumptions, but the paper doesn't investigate whether this holds across different task types or model architectures.

## Confidence
- **High Confidence**: Cost reduction claims (30% average reduction) and performance improvement metrics are well-supported by controlled experiments with clear baselines and multiple model comparisons.
- **Medium Confidence**: Mechanism explanation for why error correction works (stochastic sampling vs. logical debugging) is plausible but would benefit from additional ablation studies.
- **Medium Confidence**: Cost amortization analysis through strategy generation is mathematically sound but relies on assumptions about strategy reusability not fully stress-tested across diverse task distributions.

## Next Checks
1. **Strategy Transferability Test**: Apply the handwritten BlocksWorld strategy to a modified BlocksWorld variant (different action costs or additional predicates) to test whether effectiveness transfers beyond exact training distribution.
2. **Error Correction Mechanism Dissection**: Run controlled experiments comparing error correction performance when initial failure is due to (a) noisy reasoning, (b) fundamental misunderstanding of constraints, and (c) execution-level mistakes.
3. **Scaling Analysis**: Test the generalized strategy approach on planning tasks with 7-10 blocks (beyond the 5-6 block range studied) to identify the point at which strategy injection no longer compensates for smaller model's reasoning limitations.