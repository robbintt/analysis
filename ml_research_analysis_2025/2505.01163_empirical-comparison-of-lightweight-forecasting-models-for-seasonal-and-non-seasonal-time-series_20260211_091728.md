---
ver: rpa2
title: Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal
  Time Series
arxiv_id: '2505.01163'
source_url: https://arxiv.org/abs/2505.01163
tags:
- forecasting
- rbfnn
- polynomial
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides an empirical comparison between a Polynomial
  Classifier and a Radial Basis Function Neural Network for time series forecasting,
  focusing on both seasonal and non-seasonal patterns. The Polynomial Classifier demonstrated
  superior accuracy and faster execution times for non-seasonal datasets such as gold
  and crude oil prices, while the RBFNN performed better on seasonal data like weather
  and beer production.
---

# Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series

## Quick Facts
- arXiv ID: 2505.01163
- Source URL: https://arxiv.org/abs/2505.01163
- Reference count: 17
- Primary result: Polynomial Classifier outperforms RBFNN on non-seasonal data with 2x speed advantage, while RBFNN better captures seasonal patterns

## Executive Summary
This study empirically compares a Polynomial Classifier and a Radial Basis Function Neural Network for time series forecasting, focusing on both seasonal and non-seasonal patterns. The Polynomial Classifier demonstrated superior accuracy and faster execution times for non-seasonal datasets such as gold and crude oil prices, while the RBFNN performed better on seasonal data like weather and beer production. Statistical tests confirmed significant performance differences between the models. The Polynomial Classifier's simpler, more interpretable structure offers advantages in real-time decision-making, whereas the RBFNN excels at capturing complex seasonal behaviors.

## Method Summary
The study compares two lightweight forecasting models: a Polynomial Classifier using closed-form polynomial expansion and linear solve, and a Radial Basis Function Neural Network using Gaussian radial basis functions with RMSprop optimization. Four datasets were analyzed with 80:20 train-test splits: Vietnam daily weather (3,600/901 samples), gold prices (2,031/508), crude oil prices (3,393/849), and Australian monthly beer production (380/96). Performance was evaluated using MAE, RMSE, CV(RMSE), and execution time, with statistical significance assessed via paired t-tests and Wilcoxon signed-rank tests.

## Key Results
- Polynomial Classifier achieved 2x faster execution times (0.15-0.23s vs 0.28-0.36s) across all datasets
- PC outperformed RBFNN on non-seasonal financial data (gold: CV(RMSE) 1.66% vs 3.65%, oil: 1.84% vs 2.79%)
- RBFNN showed superior performance on seasonal data (weather: CV(RMSE) 5.76% vs 6.25%, beer: 13.09% vs 14.97%)
- First-order polynomials (K=1) consistently yielded optimal results for non-seasonal series

## Why This Works (Mechanism)

### Mechanism 1
- RBFNN's radial basis functions provide superior approximation for cyclical/seasonal patterns through local activation responses
- Each hidden unit computes a Gaussian response R_i(X) = exp(-||X - μ_i||² / (2σ_i²)), creating localized receptive fields that activate strongly only for inputs near their centers
- Seasonal patterns cluster in input space, allowing RBF centers to capture prototypical seasonal motifs
- Evidence: RBFNN performs better on series with pronounced seasonal patterns (abstract, section 4.6)

### Mechanism 2
- First-order polynomial classifiers succeed on non-seasonal data because such series exhibit approximately linear short-term dependencies
- PC constructs feature matrix M through polynomial expansion, then solves w = (M^T M)^-1 M^T t_y analytically
- Non-seasonal financial series showed lowest errors with K=1, suggesting their dominant dynamics are locally linear
- Evidence: PC demonstrated superior accuracy and faster execution times for non-seasonal datasets such as gold and crude oil prices (abstract, section 4.5)

### Mechanism 3
- PC's computational advantage derives from closed-form solutions eliminating iterative optimization overhead
- Unlike RBFNN requiring RMSprop optimization over 60-100 epochs, PC computes weights directly via matrix inversion
- Execution times were 0.15-0.23s vs 0.28-0.36s for RBFNN across datasets
- Evidence: PC demonstrated superior accuracy and faster execution times (abstract, section 3, section 4.6)

## Foundational Learning

- **Radial Basis Functions and Local vs Global Approximation**: Understanding why RBFNN captures seasonal patterns requires grasping that Gaussian RBFs are "local" learners—each unit affects predictions only near its center—versus polynomial terms that contribute globally
  - Quick check: Given input x = [0.5, 0.5] and two RBF centers μ₁ = [0, 0] and μ₂ = [1, 1] with σ = 0.5, which center contributes more to the output?

- **Bias-Variance Tradeoff and Polynomial Degree Selection**: Section 4.5 shows higher degrees (K=3, 4, 5) often perform worse, reflecting overfitting where more polynomial terms increase variance faster than bias reduction
  - Quick check: Why might K=4 work best for gold prices while K=1 works for others? What property of the gold series might justify higher complexity?

- **Statistical Significance Testing for Model Comparison**: The paper uses paired t-tests and Wilcoxon signed-rank tests to claim significant differences; understanding when to use parametric vs non-parametric tests is critical for valid conclusions
  - Quick check: When would you choose Wilcoxon over paired t-test for comparing forecast errors?

## Architecture Onboarding

- Component map:
  Raw Time Series → Sliding Window (d lags) → Feature Transformation → ┌────────────────────┬─────────────────────┐ → Evaluation
                                                                      │                    │                     │
                                                                  PC Path:              RBFNN Path:          MAE/RMSE/
                                                              Polynomial           RBF Centers +            CV(RMSE)
                                                              Expansion            Gaussian                   │
                                                                  │                Activation                   │
                                                                  ↓                    ↓                       ↓
                                                              Linear Solve         RMSprop Opt.          Statistical
                                                              (closed-form)        (iterative)           Validation

- Critical path:
  1. Data preparation: Split into training/testing (80:20 used); construct sliding window matrices Y and target t_Y
  2. For PC: Select polynomial degree K (grid search 1-5); compute M via polynomial expansion; solve for w; predict via t̂_x = w·p(x)
  3. For RBFNN: Configure hidden units, learning rate, batch size, epochs; train with RMSprop; predict via weighted RBF outputs
  4. Validation: Compute MAE, RMSE, CV(RMSE); run paired t-test and Wilcoxon for significance

- Design tradeoffs:
  - Speed vs seasonal accuracy: PC is ~2x faster but loses ~2% CV(RMSE) on seasonal data
  - Interpretability vs flexibility: PC weights are directly interpretable; RBFNN is black-box
  - Hyperparameter burden: PC needs only degree K; RBFNN needs units, lr, batch, epochs

- Failure signatures:
  - PC on seasonal data: Consistent underperformance (CV(RMSE) 14.97% vs 13.09% on beer production) suggests model lacks capacity for cyclical patterns
  - RBFNN on non-seasonal financial data: CV(RMSE) 3.65% vs 1.66% (gold) suggests overfitting or poor center initialization for trending series
  - High-degree polynomials: Sharp error increases at K=5 across all datasets indicate numerical instability/overfitting

- First 3 experiments:
  1. Baseline replication: Implement first-order PC on gold price data with d=10 lags; verify CV(RMSE) ≈ 1.66%
  2. Seasonal stress test: Apply both models to a synthetic sine wave + noise series with known period
  3. Degree sensitivity analysis: For each dataset, plot MAE vs K (1-5) with confidence intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the Polynomial Classifier and RBFNN perform when applied to multivariate and high-frequency time series datasets?
- Basis in paper: The conclusion states that future work will extend the study by evaluating the models on "high-frequency and multivariate time series."
- Why unresolved: The current study restricts its empirical analysis to univariate datasets with daily or monthly frequencies.
- What evidence would resolve it: Empirical results comparing MAE, RMSE, and execution times on millisecond-frequency financial data or multivariate sensor arrays.

### Open Question 2
- Question: Can a hybrid or ensemble approach combining PC and RBFNN outperform individual models by leveraging their complementary strengths?
- Basis in paper: The authors note that "hybrid approaches or model selection based on seasonality characteristics may further enhance performance."
- Why unresolved: The paper only evaluates the models in isolation, despite identifying that PC excels with non-seasonal data while RBFNN handles seasonal data better.
- What evidence would resolve it: A study testing a meta-model that dynamically switches between PC and RBFNN based on detected seasonality, showing improved accuracy over either single model.

### Open Question 3
- Question: How does the accuracy-efficiency trade-off of these lightweight models compare against deep learning architectures like LSTMs or CNNs?
- Basis in paper: The introduction posits that deep learning models are "resource-intensive" and motivates the use of lightweight models, yet the experiments exclude deep learning baselines.
- Why unresolved: Without benchmarking against deep learning on the same hardware, it is unclear if the Polynomial Classifier's speed justifies its lower accuracy on complex seasonal data relative to modern alternatives.
- What evidence would resolve it: Comparative metrics including training time and RMSE for LSTM/CNN models added to the existing experimental setup on the four datasets.

## Limitations
- Missing critical implementation details including sliding window size, RBF center initialization, and data preprocessing specifications
- Statistical analysis lacks multiple comparison corrections across datasets and metrics
- No direct corpus validation for polynomial degree selection mechanisms or RBFNN seasonal approximation theory
- Neighbor papers focus on deep learning approaches rather than lightweight models, providing limited contextual validation

## Confidence
- **High Confidence**: PC computational advantage (~2x faster) and consistent performance patterns across datasets are well-supported by Table 4 execution times and error metrics
- **Medium Confidence**: Seasonal vs non-seasonal performance differentiation is statistically significant but relies on paired t-tests without multiple comparison corrections
- **Low Confidence**: Theoretical mechanisms (polynomial degree selection rationale, RBFNN center initialization) lack direct corpus validation and contain unstated assumptions

## Next Checks
1. **Mechanism validation**: Conduct synthetic experiments with known seasonal patterns (sine waves with varying periods) to quantify the exact RBF unit requirement for matching PC error rates
2. **Statistical robustness**: Apply Bonferroni correction to the paired t-test results and re-evaluate significance across all dataset-metric combinations
3. **Sensitivity analysis**: Systematically vary sliding window size d and polynomial degree K across all datasets to identify the sensitivity of CV(RMSE) to these hyperparameters