---
ver: rpa2
title: 'Evaluating Large Language Models for Functional and Maintainable Code in Industrial
  Settings: A Case Study at ASML'
arxiv_id: '2509.12395'
source_url: https://arxiv.org/abs/2509.12395
tags:
- code
- prompting
- llms
- generated
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance of large language models
  (LLMs) for generating functional and maintainable code in proprietary industrial
  environments, specifically at ASML's leveling department. A custom benchmark was
  created using ASML's internal code, and a new metric, build@k, was introduced to
  assess whether generated code successfully compiles and integrates within real industrial
  repositories.
---

# Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML

## Quick Facts
- arXiv ID: 2509.12395
- Source URL: https://arxiv.org/abs/2509.12395
- Reference count: 40
- Primary result: Few-shot and chain-of-thought prompting yield highest build success rates for code generation in proprietary industrial environments

## Executive Summary
This study evaluates large language models for generating functional and maintainable code in ASML's proprietary leveling department, introducing a custom benchmark and the build@k metric to assess compilation and integration success. The research compares various prompting techniques, generic versus code-specific LLMs, and the impact of model size on code generation quality. Results show that few-shot and chain-of-thought prompting outperform zero-shot approaches, code-specific models generally achieve better build success rates, and larger models (>14B parameters) show diminishing returns. However, ensuring functional correctness remains challenging, with generated code often compiling but failing unit tests.

## Method Summary
The study creates a benchmark from ASML's internal codebase, focusing on "garage" interfaces (data component classes). Context files are recursively collected up to depth 2, with long files summarized using Qwen2.5-Coder and embedded via BGE-M3. Models ranging from 0.5B to 32B parameters are evaluated using zero-shot, few-shot, and chain-of-thought prompting strategies. The primary metric, build@k, measures compilation and integration success, while pass@k assesses unit test compliance. Secondary metrics include CodeBLEU similarity scores and TICS code quality violations.

## Key Results
- Few-shot and chain-of-thought prompting achieve significantly higher build success rates compared to zero-shot approaches
- Code-specific LLMs (DeepSeek-Coder) outperform generic models (DeepSeek) in build success despite lower CodeBLEU scores
- Larger models (>14B parameters) show improved syntax adherence and reduced violations, but functional correctness remains challenging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If few-shot or chain-of-thought (CoT) prompting is used, build success rates increase relative to zero-shot prompting, potentially because these techniques ground the model in proprietary architectural patterns (DCA) that fall outside its pre-training distribution.
- **Mechanism:** In-context learning via examples helps the model infer domain-specific "glue code" structures and naming conventions (e.g., "garage" interfaces) that differ from public code standards.
- **Core assumption:** The model has sufficient capacity to recognize and replicate the structural patterns shown in the prompt examples without weight updates.
- **Evidence anchors:**
  - [abstract] "...few-shot and chain-of-thought prompting yielding the highest build success rates."
  - [section V-A] "Zero-shot prompting consistently underperformed... highlighting the limitations of relying solely on task descriptions."
  - [corpus] "When Many-Shot Prompting Fails" (arXiv:2510.16809) suggests limits to this approach in complex tasks, implying efficacy is task-dependent.
- **Break condition:** If the prompt examples exceed the context window or fail to cover the specific dependencies required by the target file, performance degrades.

### Mechanism 2
- **Claim:** If code is generated by models with parameters â‰¥7B, code quality (violations per build) improves and syntax error rates drop, likely due to better adherence to coding standards and reduced "hallucination" of invalid imports.
- **Mechanism:** Larger models exhibit a stronger prior for syntax correctness and are less prone to the "repeat curse" (repeating imports or headers) observed in sub-3B models.
- **Core assumption:** Syntax validity and stylistic adherence correlate with the model's parameter count independently of the specific fine-tuning dataset.
- **Evidence anchors:**
  - [abstract] "...larger models tend to perform better with diminishing returns beyond 14B parameters."
  - [section V-C] "Larger models... adhere more closely to coding standards... The 0.5B model performed the worst... phenomenon known as the 'repeat curse'."
- **Break condition:** Scaling yields diminishing returns for *functional* correctness (pass@k remained 0%), as size alone does not bridge the gap in proprietary logic.

### Mechanism 3
- **Claim:** If `build@k` is used as a primary metric, it serves as a necessary (but not sufficient) filter for code integration, filtering out syntax errors that similarity metrics (e.g., CodeBLEU) miss.
- **Mechanism:** `build@k` enforces a compilation constraint against the actual industrial repository, exposing dependency hallucinations that match-based metrics cannot detect.
- **Core assumption:** A file that compiles is closer to functional correctness than one that does not, even if it fails unit tests.
- **Evidence anchors:**
  - [section IV-C] "...similarity alone is not a sufficient indicator of functional correctness."
  - [section V-B] DeepSeek generic vs. code-specific results showed that higher CodeBLEU did not guarantee higher build@k.
- **Break condition:** Code may compile (`build@k` = 1) but contain logic that returns `nullptr` or pure virtual functions, failing unit tests (pass@k = 0).

## Foundational Learning

- **Concept: Buildability vs. Functional Correctness**
  - **Why needed here:** The study highlights a critical gap: code can compile (pass `build@k`) yet fail all unit tests (`pass@k` = 0). Distinguishing these prevents premature deployment of "syntactic trash."
  - **Quick check question:** Does the generated code link against existing dependencies, or does it just look structurally similar to valid code?

- **Concept: In-Context Learning (Few-Shot)**
  - **Why needed here:** Industrial codebases (like ASML's) contain proprietary terminology not found in public training data. Few-shot prompting bridges this gap by temporarily teaching the model local dialects during inference.
  - **Quick check question:** Have you provided an example of a "garage" interface in the prompt to demonstrate the specific DCA pattern?

- **Concept: Context Window Management (RAG/Summarization)**
  - **Why needed here:** Industrial files often exceed model context limits (e.g., >2000 lines). Strategies like summarization or embedding-based retrieval are required to fit relevant context.
  - **Quick check question:** Are you prioritizing context files by import depth or semantic similarity to ensure the most critical dependencies are included?

## Architecture Onboarding

- **Component map:** Benchmark Generator -> Context Processor -> LLM Inferencer -> Evaluation Pipeline
- **Critical path:** The flow from **Context Retrieval** to **Prompt Construction** is the bottleneck. If relevant dependencies are excluded due to token limits, the model hallucinates imports, causing build failure.
- **Design tradeoffs:**
  - **Model Size:** Larger models (>14B) offer better quality/syntax but cost significantly more inference time; 1.5B models are faster but hallucinate more.
  - **Prompting Strategy:** Few-shot improves accuracy but consumes context window that could otherwise store more reference files.
- **Failure signatures:**
  - **The "Stub" Failure:** Model generates pure virtual functions or empty classes (high build@k, zero functionality).
  - **The "Repeat Curse":** Smaller models (<3B) generate infinite loops of import statements.
  - **Generic Drift:** Generic models revert to common patterns (returning `nullptr` instead of ASML-specific error handling).
- **First 3 experiments:**
  1. **Baseline Prompting:** Run Zero-shot vs. Few-shot on a small subset (10 garages) using a mid-sized model (7B) to establish a CodeBLEU baseline.
  2. **Context Ablation:** Test retrieval strategies (Depth-first vs. Similarity-first) to see which yields higher `build@k` with a fixed token budget.
  3. **Sanity Check Scaling:** Compare a 0.5B vs. 7B model on a single "garage" to observe the "repeat curse" vs. valid syntax generation qualitatively.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can parameter-efficient fine-tuning (e.g., LoRA) on proprietary data improve functional correctness beyond the capabilities of off-the-shelf models?
  - **Basis in paper:** [explicit] Section VII states that exploring parameter-efficient fine-tuning "present[s] an intriguing avenue for leveraging domain-specific data."
  - **Why unresolved:** This study evaluated pre-trained models using in-context learning (prompting) but did not update model weights using the proprietary ASML codebase.
  - **What evidence would resolve it:** A comparison of build@k and pass@k scores between the baseline models used in this study and versions fine-tuned on the internal "garage" implementations.

- **Open Question 2:** How does the performance of LLMs change when evaluated against comprehensive system-level and integration tests rather than sparse unit tests?
  - **Basis in paper:** [explicit] Section VII suggests "incorporating system-level and integration testing" for future work; Section V notes that pass@k was 0.0 partly due to low test coverage (27%).
  - **Why unresolved:** The lack of associated unit tests for the majority of garages made it impossible to verify functional correctness or draw firm conclusions about the models' true capabilities.
  - **What evidence would resolve it:** Re-running the evaluation on a benchmark enriched with human-written or generated integration tests to capture inter-component behavior.

- **Open Question 3:** Does integrating retrieval-augmented generation (RAG) or agentic pipelines improve context selection and reduce build errors compared to static context allocation?
  - **Basis in paper:** [explicit] Section VII proposes exploring "retrieval-augmented generation" and "agentic pipelines" to enhance reliability and context retrieval.
  - **Why unresolved:** The current methodology relied on a static prioritization of context files (by import depth and cosine similarity), which may have excluded critical dependencies needed for complex generation.
  - **What evidence would resolve it:** An empirical comparison of build success rates between the current static context method and a dynamic RAG-based retrieval mechanism.

## Limitations
- The proprietary nature of the ASML codebase prevents independent validation of the benchmark and results
- Evaluation focuses on syntactic correctness and basic unit tests, not complex business logic or long-term maintainability
- The study does not explore fine-tuning or domain adaptation of LLMs

## Confidence
- **High Confidence:** Few-shot and chain-of-thought prompting improve build success rates
- **Medium Confidence:** Code-specific LLMs outperform generic ones, but magnitude is difficult to quantify
- **Medium Confidence:** Diminishing returns of model scaling beyond 14B parameters is observed
- **Low Confidence:** Larger models inherently produce fewer syntax errors based on qualitative observations

## Next Checks
1. **External Benchmark Replication:** Apply the build@k metric and prompting strategies to a public, complex codebase (e.g., LangChain or TensorFlow) to test generalizability beyond ASML's proprietary environment
2. **Long-Term Maintenance Study:** Track the evolution of generated code over multiple development cycles to measure real-world maintainability impacts, including bug rates and developer comprehension
3. **Fine-Tuning vs. Prompting Comparison:** Train a small code-specific model on a sample of the ASML dataset and compare its performance against the best few-shot prompting results on the same task