---
ver: rpa2
title: 'Improving AGI Evaluation: A Data Science Perspective'
arxiv_id: '2510.01687'
source_url: https://arxiv.org/abs/2510.01687
tags:
- data
- evaluation
- https
- intelligence
- science
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues for a data science-inspired framework for evaluating
  AGI systems, addressing the challenge of measuring progress toward AGI given its
  broad and ambiguous nature. It critiques current approaches that rely on synthetic,
  intuition-driven tasks and instead proposes a pragmatic focus on robust task execution
  and competence demonstration.
---

# Improving AGI Evaluation: A Data Science Perspective

## Quick Facts
- **arXiv ID:** 2510.01687
- **Source URL:** https://arxiv.org/abs/2510.01687
- **Authors:** John Hawkins
- **Reference count:** 40
- **Primary result:** Proposes a data science-inspired framework for evaluating AGI systems using out-of-time testing, group testing, and uncertainty quantification to distinguish genuine competence from memorization.

## Executive Summary
This paper addresses the challenge of evaluating progress toward artificial general intelligence by critiquing current synthetic, intuition-driven evaluation methods. The author proposes a data science-inspired framework that focuses on robust task execution and competence demonstration under conditions of novelty and uncertainty. The framework introduces three key evaluation principles: temporal separation of training and test data to prevent leakage, structured group-based testing to assess cross-domain and cross-language generalization, and uncertainty quantification to measure calibrated decision-making under ambiguity. These methods aim to provide more reliable, real-world relevant evaluations of AGI systems by simulating the conditions they would face in practical applications.

## Method Summary
The proposed evaluation framework consists of three interconnected mechanisms. First, out-of-time testing enforces temporal separation between training data (pre-training and fine-tuning) and target outputs using publication date cutoffs, preventing models from memorizing specific solutions. Second, group testing creates deliberate gaps in training data across domains and languages, requiring systems to demonstrate knowledge transfer rather than pattern matching. Third, uncertainty quantification uses administrative task simulators with conflicting rules to test whether systems can accurately assess their own competence limits and defer appropriately. The framework operates across three agency levels—high (find and solve problems independently), medium (solve specified problem types), and low (solve with explicit constraints)—to measure different aspects of autonomous capability.

## Key Results
- Proposes three data science-inspired evaluation principles for AGI: out-of-time testing, group testing, and uncertainty quantification
- Demonstrates how temporal data separation can prevent training data leakage and ensure genuine problem-solving
- Shows how cross-domain and cross-language testing reveals true knowledge transfer capabilities
- Introduces uncertainty quantification as a measure of system reliability and calibrated decision-making

## Why This Works (Mechanism)

### Mechanism 1: Temporal Separation Prevents Memorization
- **Claim:** Time-based data splits isolate genuine problem-solving from memorization of specific solutions
- **Core assumption:** Meaningful general intelligence requires producing solutions to problems not previously encountered in solved form
- **Evidence anchors:** Abstract mentions out-of-time testing to prevent training data leakage; Section 2.2 describes using time stamps to ensure models recreate solutions without depending on specific human outputs
- **Break condition:** If novel problems share deep structural patterns with training examples, statistical regularities alone may enable solution generation without genuine reasoning

### Mechanism 2: Structured Group Testing Reveals Transfer
- **Claim:** Testing across deliberately novel domains and languages reveals knowledge transfer vs. pattern retrieval
- **Core assumption:** Intelligence implies fluid transfer of conceptual understanding across superficially different contexts
- **Evidence anchors:** Abstract mentions group testing to assess generalization; Section 2.2 describes creating textbooks for subjects never seen in target language
- **Break condition:** If training data contains implicit cross-domain mappings or parallel corpora enabling statistical exploitation, observed "transfer" may not reflect conceptual understanding

### Mechanism 3: Uncertainty Quantification Measures Calibration
- **Claim:** Calibrated deferral behavior indicates whether a system accurately assesses its own competence limits
- **Core assumption:** Trustworthy autonomous operation requires accurate self-assessment of uncertainty
- **Evidence anchors:** Abstract mentions uncertainty quantification to measure decision-making reliability; Section 2.2 describes administrative task simulator with conflicting rules
- **Break condition:** If systems learn to game uncertainty through excessive hedging or if calibration does not generalize beyond specific task structures

## Foundational Learning

**Concept: Data Leakage in Machine Learning Evaluation**
- **Why needed here:** The entire framework presupposes understanding why contamination between training and test data invalidates evaluation
- **Quick check question:** A model trained on data from 2018-2022 is tested on 2023 data. The test questions were publicly discussed on forums in 2021. Has leakage occurred? Why or why not?

**Concept: Generalization vs. Memorization**
- **Why needed here:** The paper's critique of current AGI evaluation hinges on distinguishing systems that reason from those that retrieve
- **Quick check question:** If an LLM performs well on a translated version of a task it trained on in English, what evidence would you need before concluding it "transferred" knowledge versus having seen parallel patterns?

**Concept: Uncertainty Calibration**
- **Why needed here:** Understanding how probability outputs map to actual correctness rates is essential for the uncertainty quantification mechanism
- **Quick check question:** Why might a 95% accurate classifier still be dangerous in high-stakes deployment?

## Architecture Onboarding

**Component map:**
```
Training Data (Time-Locked) -> Out-of-Time Validation -> Group/Cohert Testing -> Uncertainty Quantification -> Agency Calibration Layer
```

**Critical path:**
1. Establish temporal cutoff dates for all training data relative to target tasks
2. Audit for target outputs in pre-cutoff corpus (leakage verification)
3. Design group tests with deliberate domain/language gaps
4. Configure administrative simulator with known conflict rule pairs
5. Execute evaluation across all three mechanisms with agency-level variations

**Design tradeoffs:**
- Temporal strictness vs. data volume: Earlier cutoffs increase test integrity but reduce available training data
- Ecological validity vs. scoring objectivity: Real-world tasks are more meaningful but harder to score consistently
- Agency granularity: Three levels proposed; domain-specific applications may require finer gradations

**Failure signatures:**
- Passes out-of-time test but fails on conceptually similar problems from same period (pattern exploitation suspected)
- High accuracy on group tests with no correlation to confidence scores (uncalibrated)
- Excessive deferral in uncertainty tasks (hedging strategy) or failure to defer when rules genuinely conflict (overconfidence)
- Sharp performance degradation as agency level increases (scaffolding dependency)

**First 3 experiments:**
1. **Bitcoin whitepaper replication:** Train model only on pre-2008 cryptographic and distributed systems literature. Task: Propose peer-to-peer electronic cash system. Compare solution structure to Nakamoto's. Pass criterion: Generates functionally equivalent solution without exposure to post-2008 cryptocurrency literature.

2. **Cross-lingual textbook generation:** Withhold all physics textbooks in Swedish from training. Provide Swedish language corpus plus physics textbooks in English. Task: Generate Swedish physics textbook. Pass criterion: Subject matter accuracy maintained across language transfer; verify no Swedish physics content in training data.

3. **Administrative conflict detection:** Create rule set with 5 known conflict pairs embedded in 50 total rules. Run 1000 simulated cases, 50 containing conflicts. Pass criterion: System defers/flags ≥90% of conflict cases while correctly processing ≥95% of non-conflict cases.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can current models generate novel technical research (e.g., Bitcoin whitepaper) when restricted strictly to training data available prior to the solution's publication?
- **Basis in paper:** [explicit] The paper proposes an "out-of-time" evaluation method using the Bitcoin whitepaper as a specific example, requiring models to recreate solutions using only inputs available before the publication date
- **Why unresolved:** The paper proposes this protocol to prevent memorization but does not report results on whether current architectures can successfully synthesize such complex, novel solutions from pre-existing, disparate concepts without data leakage
- **What evidence would resolve it:** A controlled experiment where a model is fine-tuned exclusively on data prior to October 2008 and evaluated on its ability to derive the proof-of-work chain mechanism

**Open Question 2**
- **Question:** Can AGI systems identify and fill gaps in a target language's knowledge base for subjects completely withheld from that language's training corpus?
- **Basis in paper:** [explicit] The paper outlines a "Group Testing" approach where evaluation involves creating textbooks for subjects never seen in the target language
- **Why unresolved:** The paper defines the high-agency task of recognizing missing subjects and filling them but does not demonstrate if current models can perform this transfer without hallucinating or degrading in quality
- **What evidence would resolve it:** Construction of the proposed multilingual corpora with deliberate subject emissions, followed by a blind assessment of the generated textbooks by native speakers

**Open Question 3**
- **Question:** How effectively can models quantify uncertainty and defer decision-making when faced with conflicting rules in administrative tasks?
- **Basis in paper:** [explicit] The paper proposes an "administrative task simulator" designed to test if systems can recognize unresolvable conflicts and request additional information
- **Why unresolved:** While uncertainty quantification is standard in classification, it remains unverified whether generative agents can reliably exhibit the proposed "human-like" tendency to defer or escalate cases under the proposed ambiguous conditions
- **What evidence would resolve it:** Performance metrics from the proposed simulator showing the frequency of appropriate "referrals to manager" versus false positives in cases with deliberate rule conflicts

## Limitations
- Framework lacks concrete implementation details—no benchmark datasets, scoring rubrics, or specific evaluation protocols are provided
- Assumes availability of timestamped training data and knowledge of target output publication dates, which may not always be feasible
- The relationship between deferral behavior and actual competence is not empirically established

## Confidence
- **High confidence:** Temporal separation to prevent training data leakage is theoretically sound
- **Medium confidence:** Generalization testing approach has theoretical support but lacks empirical validation
- **Low confidence:** Uncertainty quantification mechanism lacks concrete implementation guidance and empirical evidence

## Next Checks
1. **Leakage Audit Validation:** Implement a proof-of-concept with a concrete task (e.g., Bitcoin whitepaper replication) and test whether modern LLMs can generate equivalent solutions from pre-2008 data alone, with rigorous semantic similarity analysis to detect memorization.

2. **Group Testing Feasibility:** Create a small-scale multilingual evaluation using Wikipedia or similar corpus, withholding specific subject-language pairs from training. Measure transfer performance across agency levels and validate that withheld content is truly absent from training data.

3. **Uncertainty Calibration Testing:** Build an administrative task simulator with controlled rule conflicts. Test whether models' deferral behavior correlates with actual conflict presence and whether confidence scores are properly calibrated to correctness rates.