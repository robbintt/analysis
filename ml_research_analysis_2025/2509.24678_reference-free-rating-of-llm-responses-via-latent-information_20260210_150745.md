---
ver: rpa2
title: Reference-Free Rating of LLM Responses via Latent Information
arxiv_id: '2509.24678'
source_url: https://arxiv.org/abs/2509.24678
tags:
- ratings
- latent
- responses
- prompt
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key issues in reference-free LLM-as-a-judge
  evaluations: instability of scores across runs and poor calibration leading to compression
  near the top of the scale. The authors propose "Latent Judges," which derive scalar
  ratings from internal model signals instead of only from generated tokens.'
---

# Reference-Free Rating of LLM Responses via Latent Information

## Quick Facts
- arXiv ID: 2509.24678
- Source URL: https://arxiv.org/abs/2509.24678
- Reference count: 40
- Primary result: Latent methods (probability-weighted scores, verifier-style, and linear probes) outperform standard LLM-as-a-judge prompting on stability, calibration, and discrimination across 8 benchmarks.

## Executive Summary
This paper addresses two key issues in reference-free LLM-as-a-judge evaluations: instability of scores across runs and poor calibration leading to compression near the top of the scale. The authors propose "Latent Judges," which derive scalar ratings from internal model signals instead of only from generated tokens. They explore three methods: probability-weighted ratings using token distributions, verifier-style binary "yes/no" probabilities, and linear probes trained on hidden activations. Across extensive pairwise and single-rating benchmarks, latent methods match or surpass standard prompting baselines, with probability-weighted scores achieving the strongest single-rating correlations.

## Method Summary
The authors propose three methods to extract scalar ratings from LLMs using internal signals rather than generated tokens. Probability-weighted ratings compute the expectation over rating tokens using their probability distribution, providing deterministic real-valued scores. Verifier-style ratings extract binary "yes/no" probabilities from a binary prompt format. Linear probes train on hidden activations at the final token position to recover quality signals when output logits are miscalibrated. These methods address instability (non-determinism) and scale compression (saturation) found in traditional LLM-as-a-judge prompting.

## Key Results
- Probability-weighted scores provide deterministic and more discriminative signals compared to ordinal ratings
- Latent probes recover useful signals when output logits are miscalibrated, particularly for models like Qwen3
- Probability-weighted ratings and latent probes significantly outperform ordinal ratings in listwise ranking tasks
- Latent probing can recover model capabilities inaccessible through training-free methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Probability-weighted ratings reduce instability and improve discriminability compared to discrete sampling.
- **Mechanism:** Instead of sampling a single token, the method computes the expectation over the vocabulary of rating tokens, making the score deterministic and real-valued.
- **Core assumption:** The model assigns meaningful probability mass across the scale such that the expected value reflects a nuanced quality judgment.
- **Evidence anchors:** [abstract] "Probability-weighted scores... provide deterministic and more discriminative signals." [section 3.2] Eq. 1 defines the weighted average. [corpus] Related papers focus on customer satisfaction ratings rather than LLM latent signals.
- **Break condition:** If the model's output distribution is extremely peaked or consistently miscalibrated.

### Mechanism 2
- **Claim:** Latent probes recover quality signals from hidden states when output logits are miscalibrated.
- **Mechanism:** A linear probe is trained on the residual stream activations at the final token position, bypassing potentially biased output heads.
- **Core assumption:** Quality is linearly encoded in the model's hidden activations, even if the model fails to translate this into calibrated output probabilities.
- **Evidence anchors:** [abstract] "Latent probes recover useful signals when output logits are miscalibrated." [page 6] "Probes extract stable and fine-grained information... not accessible through output probabilities alone."
- **Break condition:** If the model does not encode "quality" linearly in the residual stream at the evaluated layer.

### Mechanism 3
- **Claim:** Real-valued latent scores mitigate scale compression (saturation) found in ordinal Likert scales.
- **Mechanism:** Latent methods produce continuous scalars which can be rescaled post-hoc to spread out the distribution and resolve ties.
- **Core assumption:** The raw latent signal varies meaningfully between "good" and "excellent" responses.
- **Evidence anchors:** [page 4] "Real-valued scores can be arbitrarily scaled and shifted... effectively solves the calibration issues." [page 6] Fig. 3 shows variance separation.
- **Break condition:** If the internal representation is itself saturated.

## Foundational Learning

- **Concept:** Logit/Softmax Distribution vs. Sampling
  - **Why needed here:** Understanding that the probability vector over vocabulary contains more information than the single token chosen is central to probability-weighting.
  - **Quick check question:** Why does `temperature=0` (greedy decoding) lose discriminative power compared to using the raw softmax probabilities?

- **Concept:** Linear Probes and Representation Geometry
  - **Why needed here:** The paper assumes quality is a linear direction in activation space, requiring understanding of probing as training a simple head on frozen LLM activations.
  - **Quick check question:** If a model outputs "5" but the internal activation strongly predicts "10" via a probe, what does this imply about the model's output head?

- **Concept:** Reference-Free Evaluation
  - **Why needed here:** General chat responses have no ground truth, so the paper relies on correlating judge scores with external preferences without a "gold standard" answer key.
  - **Quick check question:** How does the lack of a reference answer lead to "compression at the top of the scale" in standard prompting?

## Architecture Onboarding

- **Component map:** Input (Prompt + Response) -> Judge LLM -> Extraction Point (Logits or Residual Stream) -> Scoring Head (Expectation, Binary Probability, or Trained Linear Layer) -> Output (Continuous scalar score)

- **Critical path:**
  1. Implementing the Hook: You must intercept the forward pass to extract `model.output.logits` or `model.layers[-1].output`
  2. Data Alignment: For probes, ensuring activation extraction happens at the correct token position is critical

- **Design tradeoffs:**
  - **Probability-Weighted:** Zero training cost, deterministic. *Tradeoff:* Fails if the model's output distribution is miscalibrated.
  - **Probes:** Requires training data and extraction overhead. *Tradeoff:* More robust to miscalibration and can extract "latent" knowledge the model "knows" but won't say.

- **Failure signatures:**
  - **Prompt Misalignment:** Specialized judge models may fail to follow strict "output exactly one token" instructions.
  - **Saturation:** If the probe or weighted score distribution collapses to a narrow band, check training data separation.

- **First 3 experiments:**
  1. **Baseline Stability Check:** Run a standard 1-10 prompt on 50 responses with temperature > 0. Compute standard deviation across 10 runs.
  2. **Probability-Weighted Implementation:** Modify inference loop to extract logits for tokens 1-10, softmax them, and compute weighted average. Compare to baseline.
  3. **Probe Training:** Extract activations for 1,000 labeled pairs. Train logistic regression on these activations and evaluate held-out accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust are latent judges to reward hacking compared to traditional generative judges, and do internal activation-based signals provide inherent resistance to adversarial manipulation?
- **Basis in paper:** [explicit] The conclusion states: "Future research can investigate... their robustness to known weaknesses of reward models such as reward hacking."
- **Why unresolved:** The paper demonstrates latent judges outperform baselines on standard benchmarks but does not evaluate adversarial scenarios.
- **What evidence would resolve it:** Experiments comparing latent vs. generative judges on adversarially-constructed responses designed to maximize scores while providing low-quality content.

### Open Question 2
- **Question:** Can specialized fine-tuning methods for latent probes improve performance beyond the linear/MLP probes tested, while preserving generalization across diverse prompt distributions?
- **Basis in paper:** [explicit] The conclusion calls for investigating "specific fine-tuning methods for latent judges."
- **Why unresolved:** Current probes use simple architectures trained with BCE loss, which the authors note "squashes outputs toward the extremes, reducing variance and degrading linear correlation."
- **What evidence would resolve it:** Comparison of alternative probe architectures and training objectives across both in-distribution and out-of-distribution benchmarks.

### Open Question 3
- **Question:** How can latent probing signals be effectively integrated into downstream applications like GRPO, multi-teacher distillation, and routing to improve their practical performance?
- **Basis in paper:** [explicit] The conclusion mentions downstream applicability in "Best-of-N, multi-teacher distillation, and routing" as future work.
- **Why unresolved:** The paper demonstrates latent judges work for evaluation but does not implement them in actual training loops or routing systems.
- **What evidence would resolve it:** End-to-end experiments using latent judge scores as reward signals in GRPO training or as routing criteria.

### Open Question 4
- **Question:** Why do specialized judge models like Prometheus fail under probability-weighted and verifier-style prompting, and can this capability degradation be prevented?
- **Basis in paper:** [inferred] The paper notes Prometheus "fails entirely under probability-weighted and verifier setups, as it does not follow these prompts."
- **Why unresolved:** The phenomenon is observed but not analyzed; it's unclear whether this is prompting incompatibility or fundamental representational issue.
- **What evidence would resolve it:** Analysis of Prometheus's internal representations during different prompting styles, or experiments with judge models fine-tuned on diverse output formats.

## Limitations

- Method effectiveness depends critically on model calibration properties, with probability-weighted ratings failing for severely miscalibrated models
- Implementation requires internal model access for extracting hidden activations, limiting applicability to models with exposed internals
- Training latent probes requires labeled preference data and proper hyperparameter selection, adding complexity beyond training-free methods

## Confidence

**High confidence** in the core observation: Standard LLM-as-a-judge evaluations suffer from instability and scale compression, with clear empirical evidence across multiple benchmarks.

**Medium confidence** in probability-weighted ratings: Straightforward to implement with consistent gains in pairwise accuracy, but performance degrades as model calibration worsens.

**Medium confidence** in latent probes: Theoretical justification is sound and demonstrates signal recovery when output logits are miscalibrated, but lack of training hyperparameters reduces reproducibility confidence.

**Low confidence** in general claims about "solving" calibration issues: Improvements are model-dependent and may not generalize universally across domains and architectures.

## Next Checks

1. **Calibration dependency test:** Run probability-weighted ratings on models with varying calibration properties (well-calibrated to severely miscalibrated). Measure performance degradation as calibration worsens and compare against latent probe performance.

2. **Layer sensitivity analysis:** Train latent probes on activations from multiple layers (early, middle, late) and compare performance to identify which layers contain quality signals and test linearity assumptions.

3. **Cross-domain generalization:** Apply methods to a domain not represented in training data (e.g., medical advice evaluation) and measure whether latent probes trained on general preference data transfer effectively.