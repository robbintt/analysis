---
ver: rpa2
title: Text-only adaptation in LLM-based ASR through text denoising
arxiv_id: '2601.20900'
source_url: https://arxiv.org/abs/2601.20900
tags:
- speech
- domain
- text
- target
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of adapting LLM-based ASR systems
  to new domains using only text data, a problem that standard fine-tuning methods
  struggle with due to loss of speech-text alignment. The authors propose a novel
  approach that frames text-only adaptation as a denoising task, where the LLM is
  trained to reconstruct clean transcripts from noisy inputs that mimic the output
  of a speech projector.
---

# Text-only adaptation in LLM-based ASR through text denoising

## Quick Facts
- arXiv ID: 2601.20900
- Source URL: https://arxiv.org/abs/2601.20900
- Reference count: 0
- Primary result: Up to 22.1% relative WER improvement on conversational datasets using text-only adaptation

## Executive Summary
This paper addresses the challenge of adapting LLM-based ASR systems to new domains using only text data, a problem that standard fine-tuning methods struggle with due to loss of speech-text alignment. The authors propose a novel approach that frames text-only adaptation as a denoising task, where the LLM is trained to reconstruct clean transcripts from noisy inputs that mimic the output of a speech projector. They use a multi-view noise-driven batching strategy that mixes audio, projector-induced noise, and synthetic noisy text to maintain alignment while adapting to the target domain. The method is lightweight and requires no architectural changes or additional parameters.

## Method Summary
The proposed approach treats text-only adaptation as a denoising task, where the LLM learns to reconstruct clean transcripts from noisy versions that simulate ASR errors. The noise is generated through a speech projector that mimics the errors introduced during the speech-to-text conversion process. A multi-view batching strategy is employed to maintain the alignment between speech and text representations by mixing audio samples, projector-induced noise, and synthetic noisy text. This allows the model to adapt to the target domain while preserving the learned speech-text alignment. The method is implemented as a fine-tuning procedure without requiring architectural modifications or additional parameters.

## Key Results
- Up to 22.1% relative improvement in word error rate on two conversational datasets
- Outperforms recent state-of-the-art text-only adaptation methods
- Demonstrates effectiveness of denoising-based approach for maintaining speech-text alignment during text-only adaptation

## Why This Works (Mechanism)
The method works by leveraging the denoising framework to maintain the speech-text alignment that is typically lost during standard text-only fine-tuning. By training the LLM to reconstruct clean transcripts from noisy versions that simulate ASR errors, the model learns to preserve the acoustic and linguistic patterns that are crucial for accurate speech recognition. The multi-view batching strategy ensures that the model is exposed to both the original audio-text pairs and their noisy counterparts, allowing it to adapt to the target domain while maintaining the learned alignment.

## Foundational Learning
- **Denoising autoencoders**: Used to train the LLM to reconstruct clean transcripts from noisy inputs, preserving speech-text alignment. *Why needed*: Standard fine-tuning on text data can lead to catastrophic forgetting of speech representations. *Quick check*: Verify that the denoising loss improves over training epochs.
- **Speech projectors**: Generate synthetic noise patterns that mimic ASR errors. *Why needed*: Provide realistic training signals for the denoising task. *Quick check*: Compare the distribution of synthetic noise to actual ASR errors.
- **Multi-view batching**: Mixes audio, noisy text, and clean text to maintain alignment. *Why needed*: Ensures the model sees both original and corrupted versions of the data. *Quick check*: Monitor batch composition statistics during training.
- **Text-only adaptation**: Adapts ASR models without access to speech data. *Why needed*: Enables domain adaptation in scenarios where speech data is unavailable. *Quick check*: Measure WER improvement on target domain test sets.

## Architecture Onboarding

**Component Map**
LLM -> Speech Projector -> Noise Injector -> Multi-view Batcher -> Denoising Loss

**Critical Path**
Audio samples and corresponding transcripts are passed through the speech projector to generate noisy versions. The multi-view batcher combines original and noisy pairs for training. The LLM is fine-tuned using the denoising loss to reconstruct clean transcripts.

**Design Tradeoffs**
- **Noise realism vs. computational cost**: More realistic noise generation requires more computation but provides better training signals.
- **Batch diversity vs. alignment preservation**: Higher diversity in batch composition may improve adaptation but risks losing alignment.
- **Fine-tuning duration vs. performance**: Longer fine-tuning may lead to better adaptation but increases computational cost.

**Failure Signatures**
- **Catastrophic forgetting**: Sudden increase in WER on source domain data during fine-tuning.
- **Overfitting to noise**: Model learns to denoise synthetic noise but fails on real ASR errors.
- **Alignment drift**: Degradation in speech-text alignment, visible as increased uncertainty in speech representations.

**3 First Experiments**
1. Ablation study on noise types to identify the most effective noise patterns for adaptation.
2. Comparison of multi-view batching strategies to optimize alignment preservation.
3. Evaluation on technical and multilingual datasets to assess cross-domain robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic noise patterns may introduce distributional mismatches affecting generalization to truly out-of-domain data
- Experiments limited to conversational datasets, leaving performance on technical, multilingual, or low-resource domains uncertain
- Relative improvements impressive but absolute WER values and comparison to full fine-tuning baselines not provided

## Confidence
- Method validity: Medium (theoretical framework sound but empirical validation limited to specific datasets)
- Performance claims: High (superiority over existing text-only methods demonstrated for tested conditions)
- Generalizability: Low (limited to conversational datasets, uncertain for other domains/languages)

## Next Checks
1. Evaluate the method on technical and multilingual datasets to assess cross-domain robustness
2. Compare performance against full fine-tuning on speech data to establish practical trade-offs
3. Conduct ablation studies on different noise types and mixing strategies to identify optimal configurations