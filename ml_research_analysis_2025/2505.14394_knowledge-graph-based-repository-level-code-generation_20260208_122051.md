---
ver: rpa2
title: Knowledge Graph Based Repository-Level Code Generation
arxiv_id: '2505.14394'
source_url: https://arxiv.org/abs/2505.14394
tags:
- code
- generation
- knowledge
- graph
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating contextually accurate
  code within existing codebases using large language models (LLMs). The proposed
  solution introduces a knowledge graph-based approach to improve code search and
  retrieval by representing repositories as graphs capturing structural and relational
  information.
---

# Knowledge Graph Based Repository-Level Code Generation

## Quick Facts
- arXiv ID: 2505.14394
- Source URL: https://arxiv.org/abs/2505.14394
- Reference count: 32
- Primary result: Graph-based hybrid retrieval achieves 36.36% pass@1 on EvoCodeBench vs 7.27-20.73% for baselines

## Executive Summary
This paper addresses the challenge of generating contextually accurate code within existing codebases using large language models (LLMs). The proposed solution introduces a knowledge graph-based approach to improve code search and retrieval by representing repositories as graphs capturing structural and relational information. The method employs a hybrid retrieval system combining semantic, lexical, and graph-based searches to retrieve relevant code components, which are then provided as context to LLMs for code generation.

## Method Summary
The approach uses a three-stage pipeline: (1) AST-based parsing to extract code elements (Classes, Methods, Functions, Attributes) and store them in a Neo4j graph database with defined node types and relationships; (2) Hybrid retrieval combining LLM-based entity extraction, full-text search, vector similarity search, and n-hop graph traversal with semantic reranking; (3) Code generation using LLMs provided with filtered sub-graphs as context. The system was evaluated on the EvoCodeBench dataset using pass@1 scores.

## Key Results
- Achieved 36.36% pass@1 score with Claude 3.5 Sonnet, significantly outperforming baseline methods
- Baseline GPT-4 with graph-based retrieval achieved 32.00% pass@1 (vs 20.73% without graph context)
- Demonstrates that graph-structured context improves code generation consistency with existing codebases

## Why This Works (Mechanism)

### Mechanism 1
Structured graph representations of code improve retrieval relevance over flat text-based approaches. By parsing repositories using ASTs to extract entities and relationships, the system captures dependencies and usage patterns that pure semantic similarity would miss.

### Mechanism 2
Hybrid retrieval combining lexical, semantic, and graph-based search outperforms any single retrieval modality. Different query types benefit from different strategies: entity-focused queries use lexical search while conceptual queries use semantic search.

### Mechanism 3
Providing relationship-aware sub-graphs as context enables LLMs to generate code that respects existing project conventions and dependencies. This implicitly encodes coding style, usage patterns, and architectural conventions present in the repository.

## Foundational Learning

- **Knowledge Graphs and Graph Databases**: Why needed: The entire approach depends on modeling code as nodes and edges in a queryable graph structure. Quick check: Can you explain the difference between a node property and a relationship in a property graph?

- **Abstract Syntax Trees (ASTs)**: Why needed: AST parsing is the extraction mechanism that identifies code elements to populate the knowledge graph. Quick check: Given a Python function definition, what AST nodes would represent the function name, parameters, and body?

- **Retrieval-Augmented Generation (RAG)**: Why needed: This system is a specialized RAG architecture where the retrieval component uses graph traversal rather than only vector similarity. Quick check: In a standard RAG pipeline, what are the three main components between user query and final output?

## Architecture Onboarding

- **Component map:** Parser/Extractor -> Schema Layer -> Graph Database (Neo4j) -> Embedding Service -> Query Processor -> Hybrid Retriever -> Generator (LLM)

- **Critical path:** Query → Entity Extraction + Embedding → Initial Retrieval (3 parallel paths) → n-hop Expansion → Semantic Filtering → Prompt Construction → LLM Generation

- **Design tradeoffs:** n-hop depth vs latency/noise; schema granularity vs complexity; LLM-generated descriptions vs preprocessing cost; top-k thresholds vs noise inclusion

- **Failure signatures:** Empty retrieval (query entities not found); noisy output (generated code ignores repository style); high latency (large repositories with deep traversals); missing dependencies (generated code references undefined functions)

- **First 3 experiments:** 1) Baseline retrieval comparison: vector-only, full-text-only, graph-only, and hybrid retrieval; 2) n-hop sensitivity analysis: vary traversal depth (1-hop, 2-hop, 3-hop) on EvoCodeBench samples; 3) Schema ablation: remove one node type and measure impact on semantic retrieval quality

## Open Questions the Paper Calls Out

1. **Schema extension**: Does extending the knowledge graph schema to include fine-grained elements (decorators, variable types) and support for multiple programming languages improve retrieval relevance? The paper identifies this as future work but has only evaluated on Python.

2. **Performance scalability**: What is the optimal balance between sub-graph context richness and retrieval latency for large-scale repositories? The paper notes the process is computationally intensive but doesn't analyze scaling.

3. **Task generalization**: Can the proposed knowledge graph structure improve performance on downstream software engineering tasks beyond code generation, such as debugging, documentation generation, and code translation?

## Limitations

- Evaluation restricted to Python repositories, limiting generalizability to other programming languages
- Computational intensity of sub-graph retrieval and filtering for large repositories
- Potential reliability issues with LLM-based entity extraction for ambiguous or domain-specific queries

## Confidence

- **High**: Graph structure improves dependency capture over flat text; hybrid retrieval outperforms single modalities; graph context improves LLM generation consistency
- **Medium**: Entity extraction step reliability; description generation quality impact
- **Low**: Exact retrieval hyperparameter values; generalizability to non-Python languages; performance on repositories with different architectural patterns

## Next Checks

1. **Hyperparameter sensitivity**: Systematically vary top-k thresholds and n-hop depths to identify optimal settings and measure impact on pass@1 scores

2. **Entity extraction robustness**: Test entity extraction on a diverse set of queries including ambiguous and domain-specific examples; measure failure rates and impact on downstream retrieval

3. **Schema completeness analysis**: Evaluate whether the current node types capture all necessary code elements by identifying common missing relationships in failed generation cases