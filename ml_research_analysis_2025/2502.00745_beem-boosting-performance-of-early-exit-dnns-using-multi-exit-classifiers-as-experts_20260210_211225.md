---
ver: rpa2
title: 'BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers
  as Experts'
arxiv_id: '2502.00745'
source_url: https://arxiv.org/abs/2502.00745
tags:
- exit
- early
- layer
- exits
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEEM, an early exit framework that treats
  each exit classifier as an expert and aggregates their weighted confidence scores.
  The method enhances early exit decision-making by combining confidence scores from
  multiple exits and comparing the aggregated score against a threshold, which is
  determined using the error rates of intermediate exits.
---

# BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts

## Quick Facts
- arXiv ID: 2502.00745
- Source URL: https://arxiv.org/abs/2502.00745
- Authors: Divya Jyoti Bajpai; Manjesh Kumar Hanawal
- Reference count: 19
- Primary result: BEEM achieves 1.5× to 2.1× speedups vs final layer on GLUE/COCO with accuracy close to or better than final layer.

## Executive Summary
This paper introduces BEEM, an early exit framework for deep neural networks that treats each exit classifier as an expert and aggregates their weighted confidence scores. The method enhances early exit decision-making by combining confidence scores from multiple exits and comparing the aggregated score against a threshold determined using error rates of intermediate exits. Experiments on GLUE and COCO datasets show that BEEM achieves speed-ups of 1.5× to 2.1× compared to the final layer, with accuracy close to or better than the final layer in many cases. Theoretical analysis guarantees improved performance over the final classifier under specific error rate conditions.

## Method Summary
BEEM implements early exit inference by attaching linear classifiers after each transformer layer in BERT/ALBERT models and Swin Transformer for image captioning. All exits are trained jointly with a loss combining cross-entropy and KL divergence between intermediate and final classifier predictions. During inference, weighted confidence scores are accumulated across layers and compared to a threshold for early exit decisions. The method uses cost-based or accuracy-based weights for aggregation and determines thresholds via grid search or constrained optimization. The framework is evaluated on GLUE benchmark tasks (SST-2, MNLI, QNLI, RTE, MRPC, QQP, CoLA) and COCO captioning, demonstrating superior speed-accuracy trade-offs compared to existing early exit methods.

## Key Results
- Achieves speed-ups of 1.5× to 2.1× compared to final layer inference
- Maintains accuracy close to or better than final layer in most cases
- Outperforms existing early exit methods on both GLUE and COCO datasets
- Demonstrates better stability in the accuracy-speedup trade-off

## Why This Works (Mechanism)
BEEM works by aggregating confidence scores from multiple expert classifiers and using weighted accumulation to make more informed early exit decisions. By treating each exit as an expert and combining their predictions with appropriate weights, the method can leverage the collective knowledge of all classifiers while maintaining computational efficiency. The threshold-based early exit mechanism ensures that predictions are only made when sufficient confidence has been accumulated across multiple layers, reducing the risk of premature exits that could compromise accuracy.

## Foundational Learning
- Early exit mechanisms in DNNs: Needed to understand computational efficiency gains; Quick check: Verify that early exits can reduce inference time while maintaining accuracy.
- Confidence score aggregation: Required for understanding how BEEM combines multiple classifier outputs; Quick check: Confirm that weighted aggregation improves prediction reliability.
- KL divergence regularization: Important for aligning intermediate classifier predictions with the final classifier; Quick check: Ensure KL loss helps intermediate exits learn from the final classifier.
- Threshold-based decision making: Critical for determining when to exit early; Quick check: Validate that threshold selection balances speed and accuracy effectively.

## Architecture Onboarding
- Component map: Input -> Transformer layers -> Linear exit classifiers -> Confidence aggregation -> Threshold comparison -> Output
- Critical path: Data flows through transformer layers, with each layer potentially producing an exit prediction that gets aggregated and compared against the threshold
- Design tradeoffs: Early exit speed vs accuracy, complexity of confidence aggregation vs simple confidence thresholding, fixed vs adaptive thresholds
- Failure signatures: Accuracy drops with aggressive early exits, insufficient speedup due to late exits, instability in threshold selection
- First experiments: 1) Train single exit classifier to establish baseline, 2) Implement confidence aggregation with fixed weights, 3) Test threshold selection on validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on the assumption that early exit error rates are monotonically decreasing, which may not hold for all model architectures or datasets
- Hyperparameter choices for weighted loss (λ_i) and threshold selection (α) are not fully specified, introducing variability in reproduction
- Cost-based weight scheme uses linear depth scaling, but exact formulation is unclear
- Validation split usage for threshold tuning is not detailed, which may affect reproducibility across different data splits

## Confidence
- High confidence in overall framework and experimental setup (datasets, backbone models, and metrics clearly specified)
- Medium confidence in exact training procedure and hyperparameter choices due to missing details on loss weighting and validation split usage
- Medium confidence in theoretical claims, as they depend on assumptions that may not always hold in practice

## Next Checks
1. Verify the monotonicity of early exit error rates on a subset of GLUE tasks to ensure theoretical assumptions hold
2. Test the sensitivity of BEEM performance to different λ_i scaling schemes (e.g., linear vs. exponential) and validate the impact on speed-accuracy trade-off
3. Reproduce the threshold selection process using both grid search and constrained optimization (Eq 3) to compare their effectiveness in achieving the target speedup with minimal accuracy loss