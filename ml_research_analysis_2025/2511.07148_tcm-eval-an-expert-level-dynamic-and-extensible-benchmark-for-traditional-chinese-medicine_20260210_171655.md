---
ver: rpa2
title: 'TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional
  Chinese Medicine'
arxiv_id: '2511.07148'
source_url: https://arxiv.org/abs/2511.07148
tags:
- arxiv
- data
- wang
- medicine
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scarcity of high-quality benchmarks and
  training data for Traditional Chinese Medicine (TCM) in large language models (LLMs).
  The authors introduce TCM-Eval, a dynamic, expert-validated benchmark derived from
  national TCM licensing exams, and construct a large-scale TCM-specific training
  corpus.
---

# TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine

## Quick Facts
- **arXiv ID**: 2511.07148
- **Source URL**: https://arxiv.org/abs/2511.07148
- **Reference count**: 19
- **Primary result**: Introduces TCM-Eval benchmark and SI-CoTE method, achieving 96.32% accuracy with ZMT-M1 on TCM-Eval

## Executive Summary
This work addresses the scarcity of high-quality benchmarks and training data for Traditional Chinese Medicine (TCM) in large language models (LLMs). The authors introduce TCM-Eval, a dynamic, expert-validated benchmark derived from national TCM licensing exams, and construct a large-scale TCM-specific training corpus. To enhance reasoning, they propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE), a method that autonomously generates and validates reasoning chains via rejection sampling, iteratively improving both data and model performance. Using this enriched dataset, they develop ZhiMingTang-M1 (ZMT-M1), a state-of-the-art LLM for TCM, which achieves an impressive 96.32% accuracy on TCM-Eval, significantly exceeding the passing threshold for human practitioners. The study also reveals data leakage issues in existing models, while ZMT-M1 maintains consistent performance across new and old questions. The authors release TCM-Eval with a public leaderboard to encourage further research.

## Method Summary
The authors address TCM-LLM development challenges by creating a comprehensive framework: (1) TCM-Eval benchmark with 6,099 questions across 10 years of national licensing exams plus human-crafted novel questions to prevent data leakage; (2) SI-CoTE method that iteratively enhances question-answer pairs with validated reasoning chains through rejection sampling; (3) a large-scale training corpus from 18 authoritative textbooks and thousands of mock exams. The SI-CoTE pipeline partitions data into K subsets, generates CoT candidates, applies rejection sampling to retain only correct answers, accumulates high-quality reasoning chains, and fine-tunes the model iteratively. This creates a virtuous cycle where model improvements enable better reasoning chain generation in subsequent iterations.

## Key Results
- ZMT-M1 achieves 96.32% accuracy on TCM-Eval, exceeding human passing thresholds
- Existing models show significant data leakage, with performance drops on novel human-crafted questions (e.g., Kimi-K2 drops from 96.05 to 79.70)
- SI-CoTE fine-tuning improves Qwen3-8B accuracy from 66.45% to 82.91% (+24.77% relative)
- TCM-Eval covers 10 exam years (2012-2024) with 6,099 questions across 16 TCM domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative rejection sampling on chain-of-thought (CoT) reasoning creates a virtuous data-model co-evolution cycle.
- **Mechanism**: SI-CoTE partitions the QA dataset into K disjoint subsets. In iteration k, model M_{k-1} generates candidate CoT reasoning; only samples producing the correct ground-truth answer are retained via rejection sampling. The accumulated high-quality CoT data fine-tunes the base model to produce M_k, which then processes the next subset with improved reasoning capability.
- **Core assumption**: Correct-answer CoT samples contain higher-quality reasoning patterns that generalize; the model can internalize these patterns through supervised fine-tuning.
- **Evidence anchors**: [abstract] "propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution"; [section 3.2, SI-CoTE] "This Rejection Sampling step retains a candidate if and only if its generated answer Ai is consistent with the ground-truth answer"
- **Break condition**: If base model M_0 has insufficient TCM knowledge to ever produce correct answers, rejection sampling yields empty training sets and iteration stalls.

### Mechanism 2
- **Claim**: Dynamic benchmark refresh with expert-crafted novel questions mitigates data leakage and exposes memorization versus genuine reasoning.
- **Mechanism**: TCM-Eval includes a dynamic refresh mechanism where licensed TCM professionals periodically design new examination sets matching official exam style/difficulty but never exposed in public training corpora. Performance gaps between old and new questions indicate contamination.
- **Core assumption**: Performance degradation on novel questions primarily reflects memorization of leaked data rather than distribution shift.
- **Evidence anchors**: [section 3.1] "To address this, we collaborated with a panel of licensed TCM professionals to periodically design and release novel examination sets...yet are never exposed in public training corpora"; [table 4] Multiple models show score drops on 2024/HC (Human-Crafted) questions versus older years
- **Break condition**: If new questions differ systematically in style or difficulty from training distribution, performance gaps may reflect distribution shift rather than leakage.

### Mechanism 3
- **Claim**: Combining authoritative textbook extraction with mock exam harvesting produces comprehensive domain coverage that generalizes across TCM sub-domains.
- **Mechanism**: Training corpus built from 18 authoritative TCM textbooks (segmented into knowledge blocks, converted to fill-in-the-blank and MCQ via DeepSeek-v3) plus 60,000+ harvested mock exam questions. Model-in-the-loop validation retains consistently correct samples and flags failures for human review.
- **Core assumption**: Textbook-derived QA pairs and mock exams together approximate the knowledge distribution tested in licensing exams; OCR errors can be controlled via rule-based cleaning.
- **Evidence anchors**: [section 3.2] "we have compiled a comprehensive corpus from 18 authoritative TCM textbooks and thousands of mock examinations...consisting of over 384,807 question-answer pairs"; [table 3] Shows 16 TCM domains with 159M tokens
- **Break condition**: If textbook-to-QA conversion introduces systematic errors or hallucinations, the model may internalize incorrect knowledge patterns.

## Foundational Learning

- **Concept: Rejection Sampling**
  - Why needed here: Core filtering mechanism in SI-CoTE; without understanding this, you cannot diagnose why iteration stalls or produces low-quality CoT data.
  - Quick check question: Given a model that answers correctly 30% of the time, how many samples must you generate per question to have >95% probability of obtaining at least one correct CoT? (Answer: ~9 samples)

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: SI-CoTE's entire value proposition is augmenting simple QA pairs with step-by-step reasoning; you must distinguish CoT data from standard instruction-tuning data.
  - Quick check question: For a medical diagnosis question, what is the difference between a direct answer and a CoT-augmented response? (Answer: CoT includes intermediate reasoning steps—symptom analysis, differential diagnosis, evidence evaluation—before the final answer)

- **Concept: Data Leakage / Benchmark Contamination**
  - Why needed here: The paper's central motivation for dynamic refresh; interpreting Table 4's year-over-year performance patterns requires understanding how memorization inflates scores.
  - Quick check question: If a model scores 95% on 2012 questions but 72% on 2024 questions from the same exam format, what are two possible explanations? (Answer: (1) Data leakage—2012 questions in training data; (2) Distribution shift—2024 questions test different concepts)

## Architecture Onboarding

- **Component map**:
  ```
  Data Pipeline:
  Textbooks → OCR → Segmentation → DeepSeek-v3 QA Generation → Raw QA Pairs
  Mock Exams → Web Crawling → Rule Filtering → Deduplication → Raw QA Pairs
  
  SI-CoTE Pipeline:
  Raw QA → Partition into K subsets → Iteration k:
    → M_{k-1} generates CoT candidates
    → Rejection sampling (keep correct answers)
    → Human annotation for hard cases
    → Accumulate to D_SFT
    → Fine-tune base model → M_k
  
  Evaluation:
  TCM-Eval (6,099 questions across 10 years + HC set)
  ```

- **Critical path**: SI-CoTE iteration quality. If rejection sampling yields <50% retention, you need either more samples per question or a stronger base model. Monitor per-iteration retention rates.

- **Design tradeoffs**:
  - Number of subsets K: More subsets = more iterations = potentially better final model, but longer training time and accumulation of errors
  - Samples per question: More samples = higher probability of finding correct CoT, but higher inference cost
  - Human annotation budget: Hard cases require expert review; trade off between annotation cost and final model quality

- **Failure signatures**:
  - Iteration stalls (retention rate → 0): Base model too weak; switch to stronger checkpoint or increase sampling
  - Model overfits to CoT style but not substance: CoT contains reasoning-like text without improving accuracy on held-out questions
  - Large old-vs-new question gap: Data leakage in training corpus; audit for contamination

- **First 3 experiments**:
  1. **Baseline validation**: Run DeepSeek-R1 (untrained) on TCM-Eval; establish per-year performance profile to identify potential leakage in your own setup
  2. **SI-CoTE ablation**: Train with K=3 vs K=10 subsets on a held-out 10% of corpus; measure final accuracy and per-iteration retention rates
  3. **Generalization test**: Apply SI-CoTE-trained model to a related benchmark (e.g., TCM-3CEval or MTCMB from corpus neighbors) to verify transfer beyond licensing-exam format

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the SI-CoTE framework's reliance on outcome-based rejection sampling inadvertently reinforce reasoning chains that are logically flawed but coincidentally yield the correct answer?
- **Basis in paper**: [inferred] The paper states that SI-CoTE retains reasoning chains "if and only if its generated answer... is consistent with the ground-truth answer," but does not explicitly validate the *logical steps* of the generated CoT.
- **Why unresolved**: Validating the final answer does not guarantee the intermediate reasoning steps are medically sound, potentially leading to "false positive" reasoning capabilities.
- **What evidence would resolve it**: A human or automated qualitative evaluation of the reasoning chains in the final D_SFT dataset to check for medical non-sequiturs in samples where the answer was correct.

### Open Question 2
- **Question**: To what extent does the high performance on TCM-Eval multiple-choice questions translate to effective performance in open-ended, real-world clinical consultation?
- **Basis in paper**: [inferred] While the paper asserts that ZMT-M1 exceeds the passing threshold for human practitioners, it notes that TCM-Eval consists solely of questions "formatted as multiple-choice," whereas clinical practice requires generative and interactive skills.
- **Why unresolved**: Benchmarking via multiple-choice exams tests knowledge retrieval and recognition but does not fully assess the generative capacity required for patient dialogue or complex prescription formulation.
- **What evidence would resolve it**: Evaluation of ZMT-M1 on a separate, open-ended clinical conversational benchmark or via a Turing-test style study with human TCM practitioners.

### Open Question 3
- **Question**: How does the text-only nature of ZMT-M1 and TCM-Eval limit the model's capability in diagnosing conditions that rely heavily on visual or tactile inputs, such as tongue or pulse diagnosis?
- **Basis in paper**: [inferred] The paper contrasts its work with other benchmarks like TCM-Ladder which incorporate "multimodal reasoning," but restricts TCM-Eval and ZMT-M1 to the text domain.
- **Why unresolved**: Traditional Chinese Medicine relies significantly on observation (tongue) and palpation (pulse), modalities that are absent from the current textual training and evaluation pipeline.
- **What evidence would resolve it**: Testing ZMT-M1's diagnostic accuracy on cases where visual or tactile data is essential, or extending the model to a multimodal architecture to compare performance gaps.

## Limitations

- The SI-CoTE framework may reinforce reasoning chains that are logically flawed but yield correct answers, potentially creating false positive reasoning capabilities
- High performance on multiple-choice questions may not translate to effective performance in open-ended clinical consultation scenarios
- The text-only nature of the model and benchmark limits capability in diagnosing conditions requiring visual or tactile inputs like tongue and pulse diagnosis

## Confidence

- **High confidence**: TCM-Eval benchmark construction methodology and the empirical finding of data leakage in existing models (Table 4 performance gaps are directly measurable)
- **Medium confidence**: SI-CoTE's mechanism of action - while the implementation is clear, the assumption that rejection sampling creates genuinely better reasoning patterns needs further validation
- **Medium confidence**: The 96.32% accuracy claim for ZMT-M1, given that this is evaluated on the same dataset used for training (though with dynamic refresh mitigation)

## Next Checks

1. **Ablation study on SI-CoTE iterations**: Train ZMT-M1 with 1, 3, and 5 SI-CoTE iterations, then evaluate on a held-out subset of HC questions to measure whether additional iterations improve genuine reasoning or just reinforce memorization patterns

2. **Cross-benchmark generalization**: Evaluate ZMT-M1 on TCM-3CEval and MTCMB to determine if SI-CoTE improvements transfer beyond licensing-exam formats, revealing whether the model has developed flexible TCM reasoning or just exam-specific patterns

3. **Expert clinical validation**: Have licensed TCM practitioners independently verify that ZMT-M1's top-performing reasoning chains represent sound clinical logic, not just plausible-sounding text, particularly for complex diagnostic scenarios