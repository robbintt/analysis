---
ver: rpa2
title: 'Expect the Unexpected: FailSafe Long Context QA for Finance'
arxiv_id: '2502.06329'
source_url: https://arxiv.org/abs/2502.06329
tags:
- context
- query
- answer
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FailSafeQA, a new benchmark for evaluating
  the robustness of large language models in financial question-answering tasks. The
  benchmark simulates real-world user interactions by perturbing queries and contexts
  through misspellings, incompleteness, domain mismatches, OCR errors, and missing
  or irrelevant documents.
---

# Expect the Unexpected: FailSafe Long Context QA for Finance

## Quick Facts
- arXiv ID: 2502.06329
- Source URL: https://arxiv.org/abs/2502.06329
- Authors: Kiran Kamble; Melisa Russak; Dmytro Mozolevskyi; Muayad Ali; Mateusz Russak; Waseem AlShikh
- Reference count: 21
- Key outcome: FailSafeQA benchmark reveals robustness-context grounding trade-off in financial LLMs

## Executive Summary
This paper introduces FailSafeQA, a new benchmark for evaluating the robustness of large language models in financial question-answering tasks. The benchmark simulates real-world user interactions by perturbing queries and contexts through misspellings, incompleteness, domain mismatches, OCR errors, and missing or irrelevant documents. Results show that while some models excel at handling input perturbations, they often struggle to avoid hallucinations when context is insufficient. The findings highlight significant room for improvement in developing dependable LLMs for financial applications.

## Method Summary
FailSafeQA uses 220 examples from 10-K financial filings (1998-1999, 2017-2018), each 4.1k-27k tokens. The benchmark evaluates models across three dimensions: Robustness (handling query/context perturbations), Context Grounding (refusing when context is insufficient), and Compliance (combined trade-off score). Models are evaluated using an LLM-as-a-judge methodology with Qwen2.5-72B-Instruct rating answers on a 6-point scale. The benchmark includes controlled perturbations: misspelled, incomplete, and out-of-domain queries; plus OCR errors, missing, and irrelevant contexts. Citation-based judging reduces context length for the judge while maintaining evaluation quality.

## Key Results
- Palmyra-Fin-128k-Instruct achieved highest Context Grounding (0.80) but failed 17% of test cases
- OpenAI o3-mini showed highest Robustness (0.90) but hallucinated in 41% of Missing Context scenarios
- Text generation tasks showed higher hallucination rates than question-answering across all models
- Missing Context was the hardest test scenario (scores 0.21-0.68 across models)

## Why This Works (Mechanism)

### Mechanism 1
Perturbing queries across linguistic, structural, and domain dimensions reveals robustness gaps that baseline evaluations miss. By applying controlled transformations (misspellings, incompleteness, out-of-domain rephrasing), the benchmark surfaces models' sensitivity to real-world input variation. Robustness is defined as the minimum compliance score across all perturbation types, penalizing models that perform well only under ideal conditions.

### Mechanism 2
Context grounding failures occur when models prioritize answering over acknowledging insufficient context. By testing Missing Context (empty uploads) and Irrelevant Context (wrong documents), the benchmark measures whether models correctly refuse to answer versus hallucinating. Context Grounding scores average compliance only on these unanswerable scenarios.

### Mechanism 3
Robustness and context grounding exhibit a trade-off; optimizing one degrades the other. The LLM Compliance score (LLMCβ) formalizes this trade-off using a precision-recall-inspired formula where β weights Context Grounding relative to Robustness. With β=0.5, the metric prioritizes avoiding hallucinations.

## Foundational Learning

- **Concept: LLM-as-a-Judge methodology**
  - Why needed here: FailSafeQA uses Qwen2.5-72B-Instruct to rate answers on a 6-point scale; understanding this approach is essential to interpreting scores.
  - Quick check question: Can you explain why the authors use a separate LLM for judging rather than exact string matching?

- **Concept: Robustness vs. Context Grounding trade-off**
  - Why needed here: The core finding is that high robustness often correlates with low context grounding; you must understand both dimensions to evaluate model suitability.
  - Quick check question: If a model scores 0.90 on Robustness but 0.50 on Context Grounding, would you deploy it for financial advisory tasks?

- **Concept: Perturbation-based benchmarking**
  - Why needed here: The benchmark's value comes from controlled input transformations; understanding perturbation types helps interpret failure modes.
  - Quick check question: Why might OCR perturbations cause larger performance drops than misspelling perturbations?

## Architecture Onboarding

- **Component map:** Dataset Generator -> Perturbation Engine -> Model inference -> Judge System -> Scoring Module
- **Critical path:** Query generation → Perturbation application → Model inference → Judge evaluation → Score computation. The judging phase is simplified by using citations rather than full 25k-token contexts.
- **Design tradeoffs:** Citation-based judging reduces context length for the judge but may miss edge cases where answers require document-wide synthesis. 220 examples with 93.64% exceeding 16k tokens balances computational cost with long-context relevance.
- **Failure signatures:**
  - High Robustness, Low Context Grounding: OpenAI o3-mini (0.90 R, 0.59 G, 41% hallucination rate)—risky for compliance-sensitive domains
  - Low Robustness, High Context Grounding: May refuse valid queries; check Palmyra-Fin's 17% failure rate on robust predictions
  - Text Generation tasks: All models show higher hallucination rates on TG vs. QA queries
- **First 3 experiments:**
  1. Baseline your model: Run all 220 examples on your target model, compute R, G, and LLMCβ scores. Compare to Table 1 and Table 2.
  2. Perturbation breakdown: Identify which perturbation type causes the largest drop; for most models, it's OCR or Out-of-Domain queries.
  3. Context failure analysis: Test specifically on Missing Context and Irrelevant Context scenarios; if your model scores below 0.5, implement explicit refusal training before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
Is there a fundamental negative correlation between model Robustness (handling perturbations) and Context Grounding (avoiding hallucinations)? While the trade-off was observed in top performers, the study did not validate this inverse relationship across the full spectrum of model architectures.

### Open Question 2
Does decomposing text generation tasks into a retrieval-then-generate pipeline reduce hallucinations compared to end-to-end generation? Text generation tasks were identified as most susceptible to hallucination, but the proposed multi-step mitigation strategy was not tested.

### Open Question 3
Do robustness and grounding behaviors persist when models must aggregate information from multiple financial documents rather than a single source? The current benchmark is limited to single-document contexts, leaving the complexity of multi-source reasoning untested.

## Limitations

- The LLM-as-a-judge methodology introduces evaluation uncertainties due to unverified correlation with human judgment and potential judge hallucination
- Results may not generalize to broader financial domains beyond 10-K filings
- Perturbation-based evaluation may overestimate real-world robustness if models are trained on similar perturbation patterns

## Confidence

- **High confidence:** Palmyra-Fin-128k-Instruct achieves highest Context Grounding (0.80) while OpenAI o3-mini shows highest hallucination rate (41%) on Missing Context tasks
- **Medium confidence:** The trade-off between Robustness and Context Grounding is demonstrated through comparative analysis, but underlying mechanism remains speculative
- **Low confidence:** Generalizability to broader financial domains and assumption that perturbation-based robustness correlates with real-world performance

## Next Checks

1. **Human validation study:** Recruit financial experts to manually rate 50 randomly selected answers from the benchmark, comparing their judgments against the LLM-as-a-judge scores to assess rating reliability and identify systematic biases.

2. **Cross-domain robustness testing:** Apply the same perturbation methodology to non-financial domains (legal documents, medical records) to determine if the observed trade-off between Robustness and Context Grounding generalizes beyond financial contexts.

3. **Temporal validation:** Retest all models on FailSafeQA after a 3-month period using updated 10-K filings from 2023-2024 to assess whether performance degradation occurs as financial reporting standards and terminology evolve.