---
ver: rpa2
title: 'SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces'
arxiv_id: '2509.00287'
source_url: https://arxiv.org/abs/2509.00287
tags:
- data
- knowledge
- incidents
- graph
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SIGMUS is a system that integrates multimodal urban data streams
  (text, image, and tabular) using Large Language Models to create a semantic knowledge
  graph for incident detection and analysis. The system addresses the fragmentation
  of urban sensor data by automatically inferring relationships between incidents
  and diverse data sources without relying on human-encoded rules.
---

# SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces

## Quick Facts
- **arXiv ID:** 2509.00287
- **Source URL:** https://arxiv.org/abs/2509.00287
- **Reference count:** 40
- **Primary result:** Integrates multimodal urban data using LLMs to create semantic knowledge graph for incident detection, successfully linking 5 data sources to the 2025 Los Angeles Wildfires.

## Executive Summary
SIGMUS addresses the challenge of fragmented urban sensor data by automatically inferring relationships between incidents and diverse data sources using Large Language Models. The system processes text, image, and tabular data through modality-specific pipelines, then uses LLMs to semantically link these reports to urban incidents without requiring human-encoded rules. Demonstrated through the 2025 Los Angeles Wildfires case study, SIGMUS successfully integrates news articles, CCTV images, air quality measurements, weather data, and traffic information into a coherent knowledge graph. Processing latencies range from 0.0043s to 39.38s per report depending on modality, with knowledge graph insertion taking 0.02-30.81s.

## Method Summary
SIGMUS ingests multimodal urban data through modality-specific pipelines that extract structured information (actors, events, captions, trends) which are inserted into a Neo4j knowledge graph. For cross-modal linking, reports are queried alongside candidate incidents with temporal/spatial context, and an LLM determines semantic relationships. Incident disambiguation uses vector similarity search combined with LLM reasoning to merge related incidents. The system employs local DeepSeek-R1 for most tasks and cloud GPT-4o for cross-modality linking, balancing cost and quality.

## Key Results
- Successfully linked 5 diverse data sources (news, CCTV, air quality, weather, traffic) to the 2025 Los Angeles Wildfires
- Processing latencies: 0.0043s-39.38s per report depending on modality
- Knowledge graph insertion: 0.02-30.81s per report
- Demonstrated automatic semantic linking without human-encoded rules

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can identify semantic relationships between urban incidents and multimodal sensory data without human-encoded rules.
- **Mechanism:** Reports are processed through modality-specific pipelines, inserted into a knowledge graph, then an LLM receives the report content alongside candidate incidents (with temporal/spatial context) to determine if a relationship exists.
- **Core assumption:** LLMs possess sufficient world knowledge (e.g., "fire reduces air quality") and contextual reasoning to produce reliable semantic links.
- **Evidence anchors:**
  - [abstract] "SIGMUS uses Large Language Models (LLMs) to produce the necessary world knowledge for identifying relationships between incidents occurring in urban spaces and data from different modalities."
  - [section] 4.2.1 describes cross-modality linking: reports are inserted into KG, then LLM is queried with report info and incident list.
  - [corpus] Related work (e.g., Real-time Spatial RAG) shows similar LLM-based integration for urban environments, but SIGMUS focuses on KG-based semantic linking across heterogeneous sensor modalities.
- **Break condition:** If incidents are highly ambiguous or world knowledge is insufficient (e.g., correlating cooking smoke vs. wildfire smoke purely from air quality), LLM may produce spurious links.

### Mechanism 2
- **Claim:** Modality-specific processing pipelines extract structured, human-readable information suitable for KG insertion and downstream LLM reasoning.
- **Mechanism:** Text undergoes actor/event parsing using LLMs with CAMEO codes; images are captioned via VLM (NVILA); tabular data undergoes statistical trend analysis. Each output is stored as a "report" with standardized attributes.
- **Core assumption:** Processing outputs (captions, trend labels, actor names) are sufficiently accurate and standardized for KG integration.
- **Evidence anchors:**
  - [section] 4.2 describes Actor/Event Parsing, Visual Event Classification, and Time Series Analysis pipelines.
  - [section] Table 2 shows attributes extracted per data source (e.g., actors, PM2.5, image captions).
  - [corpus] MEDMKG benchmarks multimodal KG construction in medical domains, suggesting modality-specific extraction is a common strategy.
- **Break condition:** VLM captioning errors or actor disambiguation failures (e.g., similar names merged incorrectly) propagate into the KG.

### Mechanism 3
- **Claim:** Vector similarity search combined with LLM disambiguation enables efficient incident merging and entity resolution.
- **Mechanism:** For incident disambiguation, text embeddings are retrieved from a vector database (Marqo) to find top-k similar incidents; the LLM then decides if the new incident matches or relates (e.g., IS_PART_OF) to existing ones.
- **Core assumption:** Embedding similarity correlates with semantic relevance, and LLM can reliably disambiguate within top-k candidates.
- **Evidence anchors:**
  - [section] 4.2.2 describes RAG approach for incident merging using vector search + LLM selection.
  - [section] Table 3 reports ~7.87s average latency for incident disambiguation.
  - [corpus] Graph RAG approaches (e.g., "From Local to Global: A Graph RAG Approach") support hybrid retrieval + LLM reasoning patterns.
- **Break condition:** If top-k retrieved incidents are semantically distant, LLM has insufficient context for accurate disambiguation.

## Foundational Learning

- **Concept:** Knowledge Graph Ontologies (RDF/OWL, SOSA, Dublin Core)
  - **Why needed here:** SIGMUS defines a custom ontology with classes like Incident, Report, Observer, Actor; understanding ontology design is necessary to extend or modify the schema.
  - **Quick check question:** Can you explain the difference between `sosa:Observation` and `sigmus:Report` in this system?

- **Concept:** Retrieval-Augmented Generation (RAG) with Vector Databases
  - **Why needed here:** Used for incident merging and entity resolution; requires understanding embedding models, similarity search, and context construction for LLM prompts.
  - **Quick check question:** How would you modify the retrieval step if you needed to prioritize geographic proximity over textual similarity?

- **Concept:** Modality-Specific Neural Processing (VLMs, NER, Time Series Analysis)
  - **Why needed here:** Each data type requires different neural models (NVILA for images, LLMs for text, statistical methods for tabular); integration depends on their outputs.
  - **Quick check question:** What failure modes would you expect from using a VLM trained on general images to caption wildfire-specific CCTV feeds?

## Architecture Onboarding

- **Component map:** Data Ingestion (modality-specific scrapers/API clients) → Data Warehouse (TimescaleDB + SeaweedFS) → Processing Pipelines (Actor/Event Parser, VLM, Trend Analyzer) → KG Integration (Neo4j + Ontology) → Cross-Modal Linking (LLM + KG queries) → Incident Merging (Vector DB + LLM)
- **Critical path:** Ingest → Process → Insert Report → Query KG for incidents → LLM Linking → Disambiguate/Merge. Delays in ingestion or processing latency (e.g., GDELT text at ~39s) affect live monitoring responsiveness.
- **Design tradeoffs:** Local DeepSeek-R1 used for most tasks (cost/control) vs. cloud GPT-4o reserved for cross-modality linking (higher quality); modality-specific pipelines increase complexity but enable richer integration.
- **Failure signatures:** Actor disambiguation errors (~8s latency per report); incident merging confusion when embedding similarity is low; VLM captioning failures on poor-quality CCTV images.
- **First 3 experiments:**
  1. Replicate single-modality processing: ingest PurpleAir data, run trend analysis, verify KG insertion.
  2. Test cross-modal linking: feed a known wildfire-related CCTV image + air quality spike; verify LLM links to correct incident.
  3. Evaluate incident merging: insert duplicate incident reports with slight name variations; measure disambiguation accuracy.

## Open Questions the Paper Calls Out

None

## Limitations

- LLM-based semantic linking may produce spurious connections when incidents are highly ambiguous or world knowledge is insufficient
- VLM-based image captioning may struggle with poor lighting, occlusions, or novel visual scenarios
- Cross-modality linking depends heavily on LLM's ability to correctly interpret temporal and spatial relationships from natural language

## Confidence

- **High:** Knowledge graph construction and data insertion mechanisms
- **Medium:** General urban incident detection capability across diverse incident types
- **Low:** VLM captioning accuracy in challenging urban environments

## Next Checks

1. **Multi-incident Stress Test:** Deploy SIGMUS across a metropolitan area with concurrent incidents (wildfire, chemical leak, traffic accident) to evaluate cross-modality linking accuracy and latency under realistic load conditions.

2. **VLM Robustness Evaluation:** Test the visual event classification pipeline on CCTV feeds with varying quality levels (low light, motion blur, partial occlusion) to quantify performance degradation and establish quality thresholds.

3. **Cross-Modality Generalization:** Validate system performance on diverse urban incident types beyond wildfires, including chemical spills, infrastructure failures, and public health events.