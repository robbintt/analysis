---
ver: rpa2
title: 'IA-VLA: Input Augmentation for Vision-Language-Action models in settings with
  semantically complex tasks'
arxiv_id: '2509.24768'
source_url: https://arxiv.org/abs/2509.24768
tags:
- tasks
- language
- objects
- task
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IA-VLA, a framework for augmenting the input
  of vision-language-action (VLA) models to handle semantically complex tasks, particularly
  those involving duplicate objects. The key idea is to use a large vision-language
  model (VLM) as a pre-processing stage to identify task-relevant objects via semantic
  segmentation and numeric labels, then highlight these objects in the input image
  using semi-transparent masks.
---

# IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks

## Quick Facts
- arXiv ID: 2509.24768
- Source URL: https://arxiv.org/abs/2509.24768
- Authors: Eric Hannus; Miika Malin; Tran Nguyen Le; Ville Kyrki
- Reference count: 30
- Primary result: Semantic offloading via visual highlighting improves VLA success rates from 19% to 72% in duplicate-object manipulation tasks.

## Executive Summary
IA-VLA introduces a framework that augments VLA inputs by leveraging a large VLM for semantic understanding, allowing smaller VLAs to focus on visuomotor control. The approach uses semantic segmentation and numeric labeling to identify task-relevant objects, then highlights these objects in the input image using semi-transparent masks. This enables the VLA to generate actions based on augmented inputs rather than directly parsing complex language instructions. The framework is evaluated on three task settings involving duplicate objects: lifting Lego blocks, putting vegetables into pots, and opening drawers. Results demonstrate significant improvements over baseline VLA performance, particularly in tasks requiring extrapolation from seen concepts.

## Method Summary
IA-VLA processes the initial frame through Semantic-SAM to generate multi-granularity segmentation masks, which are filtered by area and overlap criteria, then overlaid with numeric tags. GPT-4.1 receives the tagged image and instruction, outputs target mask IDs, and these masks are highlighted with semi-transparent gray overlay (alpha=0.8). SAM2 propagates these masks across frames in real-time (~40ms latency). The VLA (OpenVLA) receives the augmented image and (original or simplified) instruction to generate actions. Training uses LoRA finetuning on demonstration data augmented with these highlighting masks. The method includes a relabeled variant where the VLA receives simplified instructions like "lift the highlighted block" instead of the original spatial language.

## Key Results
- IA-VLA improved success rates from 19% to 72% on block-lifting tasks with duplicate objects
- Relabeled variant performed best on extrapolation tasks with unseen instruction combinations
- VLM selection errors accounted for 24% of failures, while execution failures dominated at 70%
- Mask generation failures were minimal at 3%, indicating reliable semantic-SAM performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Offloading via Visual Highlighting
Highlighting task-relevant objects in the input image allows a smaller VLA to leverage the semantic understanding of a larger VLM without increasing inference latency. A large VLM processes the initial frame with numeric segmentation tags, identifies target object masks, and applies a semi-transparent highlight. The VLA receives a pre-processed image where the correct object is visually salient, reducing its semantic burden to simple visual grounding. Core assumption: The VLA can learn to associate highlighted regions with task-relevant actions during finetuning. Break condition: If the VLM selects incorrect masks (24% of failures), the VLA receives misleading input and fails completely.

### Mechanism 2: Responsibility Partitioning via Instruction Relabeling
Simplifying language instructions while providing original instructions to the VLM creates cleaner task boundaries between semantic interpretation and motor execution. In IA-VLA-relabeled, the VLA receives generic instructions like "lift the highlighted block" instead of "lift the second orange block from the right." This forces the VLA to rely primarily on visual highlighting rather than attempting its own semantic parsing. Core assumption: The VLA cannot reliably balance conflicting signals between its own language interpretation and external highlighting when facing novel instruction compositions. Break condition: When masking or VLM selection fails, relabeled models have no redundant signal and fail completely.

### Mechanism 3: Temporal Consistency via Mask Propagation
Propagating segmentation masks across frames using a tracking model (SAM2) maintains visual highlighting throughout the manipulation sequence without repeated VLM calls. SAM2 tracks the initially-identified masks in real-time (~40ms latency per frame), enabling continuous highlighting as objects and the robot move. This preserves the semantic disambiguation benefit across temporal horizons without incurring per-frame VLM costs. Core assumption: SAM2 tracking accuracy remains sufficient throughout the task duration. Break condition: Long-horizon tasks or rapid object motion may cause mask tracking drift.

## Foundational Learning

- Concept: **Vision-Language-Action (VLA) Models**
  - Why needed here: Understanding that VLAs face a fundamental latency-capacity tradeoff—they must output actions at control rates (~10-30 Hz), constraining model size and thus language understanding capacity.
  - Quick check question: Can you explain why a VLA based on a 7B parameter LLM might struggle with "lift the second block from the right" more than "lift the red block"?

- Concept: **Set-of-Mark Visual Grounding**
  - Why needed here: The framework uses numeric tags overlaid on segmented regions (following Yang et al.) to enable the VLM to select objects by reference; understanding this prompting strategy is essential for debugging VLM selection failures.
  - Quick check question: If Semantic-SAM fails to segment the target object, what happens to the numeric tagging pipeline?

- Concept: **LoRA Finetuning for Robot Policies**
  - Why needed here: The experiments use LoRA (rank 32) to adapt OpenVLA; understanding parameter-efficient finetuning helps diagnose whether failures stem from insufficient adaptation or architectural limitations.
  - Quick check question: Would full-model finetuning potentially improve Category 3 performance, or is the semantic limitation fundamental to model capacity?

## Architecture Onboarding

- Component map:
  Input -> Semantic-SAM segmentation -> Mask filtering -> Numeric tagging -> VLM selection -> Mask applicator -> SAM2 tracker -> VLA inference

- Critical path:
  1. Initial frame → Semantic-SAM segmentation → mask filtering (area threshold + overlap handling)
  2. Filtered masks + numeric tags → VLM selection (dominant latency ~10s)
  3. VLM output → highlight application → VLA inference (per-timestep)
  4. Subsequent frames → SAM2 propagation → highlight maintenance → VLA inference

- Design tradeoffs:
  - Highlight density (alpha=0.8): Higher values improve target salience but may obscure fine visual details (gripper fingers noted in paper)
  - Relabeled vs. original instructions: Relabeling improves extrapolation but removes error recovery pathways
  - Segmentation granularity: Levels 1-3 vs 1-4 tuned per environment; higher granularity increases VLM selection complexity

- Failure signatures:
  - VLM selection error (24%): VLM outputs wrong mask IDs → incorrect object highlighted → wrong action
  - Masking failure (3%): Target object unsegmented → no numeric tag → VLM cannot select
  - VLA execution error (70%): Correct highlighting but robot fails grasp/motion → indicates insufficient demonstration diversity or motor learning limits

- First 3 experiments:
  1. Baseline sanity check: Run OpenVLA (no augmentation) on Category 1 tasks to verify finetuning quality; expect >50% success on seen instructions.
  2. Ablation on highlighting density: Test alpha values [0.5, 0.8, 0.95] on block-lifting Category 2 tasks to measure sensitivity to visual salience vs. detail preservation.
  3. Error injection test: Manually swap mask IDs before VLM input on 20% of test cases to quantify recovery capability of non-relabeled vs. relabeled variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does augmenting only the main camera view suffice for VLAs with multiple image inputs, or is consistent augmentation required across all visual streams?
- Basis in paper: [explicit] The conclusion states, "investigating whether, for VLAs with multiple image inputs, augmenting only the main view suffices or whether consistent augmentation must be applied across all streams" is a promising direction.
- Why unresolved: The experiments in this paper were restricted to single-camera viewpoints (tabletop and drawers), so the impact on multi-view architectures is unknown.
- What evidence would resolve it: Comparative evaluations on a multi-view manipulation benchmark where augmentation is applied to the primary view versus all auxiliary views.

### Open Question 2
- Question: Does providing highlighting masks as a separate input channel outperform the current method of overlaying semi-transparent masks directly onto the RGB image?
- Basis in paper: [explicit] The authors suggest "design[ing] a VLA model that can take highlighting masks as input in a separate channel" as a future direction distinct from their implementation.
- Why unresolved: The current approach overlays masks, which may obscure fine visual details (like gripper fingers) or confuse the VLA if the overlay is too dense.
- What evidence would resolve it: Training a VLA with a dedicated mask input channel and comparing its success rate and error modes against the visual overlay baseline on the same tasks.

### Open Question 3
- Question: How can the system optimally balance trusting the VLM's highlighting versus the VLA's internal language understanding when they conflict?
- Basis in paper: [inferred] Section IV-C notes the "relabeled" model failed to recover from VLM errors, while the "original" model failed to trust the mask on unseen instructions, indicating an unresolved trade-off.
- Why unresolved: The paper evaluates two extremes (fully relabeling vs. keeping original text) but does not propose a mechanism for dynamic arbitration between the VLM's visual context and the VLA's instruction following.
- What evidence would resolve it: An ablation study testing a confidence-based weighting mechanism that adjusts the influence of the highlighted region based on the semantic novelty of the instruction.

## Limitations
- Reliance on VLM selection accuracy without clear diagnostic pathways when incorrect masks are chosen
- Relabeled instruction approach removes redundancy, making failures unrecoverable when VLM selection fails
- Mask propagation via SAM2 assumed accurate but lacks quantitative characterization of drift over extended horizons

## Confidence
- **High confidence**: The core mechanism of semantic offloading via visual highlighting works as described, supported by quantitative improvements (19% to 72% success rates) and ablation showing relabeled instructions help with extrapolation tasks.
- **Medium confidence**: The partitioning of responsibility between VLM and VLA through instruction relabeling is supported by failure analysis but lacks direct comparison to alternative partitioning strategies or error recovery mechanisms.
- **Low confidence**: The robustness of mask propagation across frames is assumed based on low reported failure rates (3%) without systematic evaluation of tracking accuracy degradation over time or with varying object motion dynamics.

## Next Checks
1. Implement logging of VLM mask selections and manually verify correctness on 100 random test cases to quantify the actual error rate and identify failure patterns.
2. Evaluate SAM2 tracking accuracy over 100-frame sequences with varying object velocities and occlusions to establish quantitative bounds on mask drift.
3. Compare non-relabeled vs relabeled variants on tasks with induced VLM errors (20% of cases) to measure the practical benefit of retaining original instructions as a fallback mechanism.