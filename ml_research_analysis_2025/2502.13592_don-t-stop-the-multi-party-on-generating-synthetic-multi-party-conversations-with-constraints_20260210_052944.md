---
ver: rpa2
title: Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations
  with Constraints
arxiv_id: '2502.13592'
source_url: https://arxiv.org/abs/2502.13592
tags:
- ecdf
- mpcs
- stance
- speaker
- speak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates generating synthetic multi-party conversations
  using LLMs, addressing the lack of diverse interaction patterns in real-world data.
  The authors explore two strategies: One-Long generation (entire conversation at
  once) and Turn-by-Turn generation (sequential turn generation).'
---

# Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints

## Quick Facts
- arXiv ID: 2502.13592
- Source URL: https://arxiv.org/abs/2502.13592
- Reference count: 40
- Two-generation strategies (One-Long, Turn-by-Turn) for synthetic multi-party conversations with constraints yield high-quality, structurally complex dialogues

## Executive Summary
This paper addresses the scarcity of diverse multi-party conversation (MPC) datasets by proposing a framework for generating synthetic MPCs with controlled characteristics. The authors explore two generation strategies—One-Long (entire conversation at once) and Turn-by-Turn (sequential turn generation)—using four LLMs with 7-8B parameters. They introduce a comprehensive evaluation framework assessing constraint compliance, language variability, structural complexity, and qualitative aspects. Results demonstrate that Qwen2.5 and Llama3.1 best comply with constraints, with Turn-by-Turn yielding better constraint adherence and linguistic variability, while both strategies produce high-quality conversations.

## Method Summary
The paper proposes generating synthetic multi-party conversations with 4-6 speakers, 15 messages, and specific stance distributions using two strategies: One-Long generation (entire conversation at once) and Turn-by-Turn generation (sequential turn generation). The framework uses 38 controversial topics with 76 paired stance statements (progressive/conservative versions). Four models are evaluated: Llama3.1-8B-Instruct, Qwen2.5-7B-Instruct, Ministral-8B-Instruct, and OLMo-2-7B-Instruct. The evaluation assesses constraint compliance (format, interactions, speaker count, message count, stance distribution), language variability (repetition rate, string similarity, semantic coherence), structural complexity (degree centrality, reciprocity, transitivity), and qualitative aspects (naturalness, argumentability, stance consistency/evolution, addressee correctness/preciseness).

## Key Results
- Qwen2.5 and Llama3.1 show highest constraint compliance (77% and 66% for Turn-by-Turn strategy respectively)
- Turn-by-Turn strategy yields better constraint adherence and linguistic variability compared to One-Long
- Generated MPCs exhibit greater structural complexity than real datasets like UbuntuIRC
- Human and LLM-as-a-judge evaluations indicate high-quality conversations with scores around 4.2/5 for naturalness and argumentability

## Why This Works (Mechanism)
The paper's approach works by providing structured prompts that guide LLMs to generate conversations meeting specific constraints. The Turn-by-Turn strategy is particularly effective because it breaks down the generation process into manageable steps, allowing the model to maintain consistency with speaker identities, stances, and interaction patterns throughout the conversation. By iteratively generating speakers, selecting interactions, and creating messages, the model can better track the conversation state and adhere to the specified constraints.

## Foundational Learning
- **Multi-party conversation structure**: Understanding the graph-like nature of MPCs where nodes represent speakers and edges represent interactions
  - Why needed: Essential for modeling complex social dynamics and interaction patterns
  - Quick check: Verify degree centrality, reciprocity, and transitivity metrics on sample MPCs

- **Constraint-based generation**: Using structured prompts to guide LLMs toward specific outputs
  - Why needed: Ensures generated content meets predefined requirements for speaker count, stance distribution, and interaction patterns
  - Quick check: Validate constraint compliance rates across different strategies

- **LLM-as-a-judge evaluation**: Using models to assess generated content quality
  - Why needed: Enables scalable evaluation of naturalness, argumentability, and stance consistency
  - Quick check: Compare LLM judgments with human evaluations on sample conversations

## Architecture Onboarding

**Component map**: Topic statements -> Generation strategy (OL/TT) -> LLM model -> MPC output -> Evaluation framework (4 blocks)

**Critical path**: The evaluation pipeline follows this sequence: (1) constraint validation, (2) language variability metrics, (3) structural analysis (ECDFs vs UbuntuIRC baseline), (4) LLM-as-a-judge for stance consistency/evolution

**Design tradeoffs**: One-Long strategy is simpler but yields lower constraint compliance (66-77% vs <1% for some models); Turn-by-Turn is more complex but better maintains consistency

**Failure signatures**: 
- Stance/speaker count violations (especially with OL strategy)
- Low addressee preciseness (~3.5/5) indicating over-general addressing
- Random seed dependency preventing exact reproduction

**First experiments**:
1. Generate 10 test conversations using provided prompts and verify constraint compliance
2. Implement addressee correctness metric and evaluate sample conversations
3. Reproduce structural complexity ECDFs comparing synthetic and UbuntuIRC datasets

## Open Questions the Paper Calls Out

### Open Question 1
Can synthetic MPCs improve performance of smaller models on downstream discriminative tasks like next-speaker prediction and addressee recognition? The paper demonstrates generation feasibility but doesn't test whether synthetic data transfers to downstream tasks.

### Open Question 2
How does the approach generalize to topics requiring nuanced, non-polarized stances rather than binary pro/con positions? All 38 topics used were deliberately polarizing US-centric debates.

### Open Question 3
What pretraining data characteristics explain why Qwen2.5 and Llama3.1 succeed while Ministral and OLMo2 fail? Models with comparable parameter sizes showed drastically different constraint compliance (77% vs. <1%).

### Open Question 4
How do looser constraints (more speakers, longer conversations, fewer stance restrictions) affect the naturalness and structural complexity of generated MPCs? Experiments were limited to 4-6 speakers, exactly 15 messages, and fixed stance distributions.

## Limitations
- Reproducibility limited by unspecified random seeds and stochastic decoding
- Constraint compliance measurement ambiguity in addressee correctness evaluation
- Evaluation metric sensitivity to implementation details not fully specified
- Limited model comparison (only 4 models with ≤8B parameters tested)
- Human evaluation scope limited to 120 MPCs total

## Confidence

**High confidence**: Generation strategies and overall evaluation framework structure are clearly specified

**Medium confidence**: Quantitative results on constraint compliance and structural complexity are reproducible with some variation

**Low confidence**: Qualitative assessment results depend heavily on LLM-as-a-judge configuration

## Next Checks

1. Generate a small test set (10 topics × 2 prompts × 2 strategies × 2 models) using provided prompts and verify constraint compliance rates match reported ranges (66-77% for TT strategy)

2. Implement the addressee correctness metric using the prompt's definition and evaluate a sample of generated conversations to confirm the reported preciseness score of ~3.5/5

3. Reproduce the structural complexity comparison by generating degree centrality, reciprocity, and transitivity ECDFs for both synthetic and UbuntuIRC datasets using NetworkX