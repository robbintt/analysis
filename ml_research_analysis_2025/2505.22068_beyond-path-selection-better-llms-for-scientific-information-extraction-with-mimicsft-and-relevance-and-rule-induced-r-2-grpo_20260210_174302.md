---
ver: rpa2
title: 'Beyond path selection: Better LLMs for Scientific Information Extraction with
  MimicSFT and Relevance and Rule-induced(R$^2$)GRPO'
arxiv_id: '2505.22068'
source_url: https://arxiv.org/abs/2505.22068
tags:
- reasoning
- arxiv
- relation
- entity
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether reinforcement learning with verifiable\
  \ rewards (RLVR) improves reasoning capacity or merely refines reasoning paths in\
  \ large language models (LLMs), using scientific information extraction (SciIE)\
  \ as a testbed. The authors propose a two-stage training method combining MimicSFT\u2014\
  which uses structured reasoning templates to improve performance without requiring\
  \ high-quality chain-of-thought data\u2014with R2GRPO, a reinforcement learning\
  \ approach that incorporates relevance and rule-induced rewards."
---

# Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R²)GRPO

## Quick Facts
- arXiv ID: 2505.22068
- Source URL: https://arxiv.org/abs/2505.22068
- Reference count: 40
- Primary result: R²GRPO+SFT achieves 66.81 F1 for relation extraction on SciER, surpassing specialized supervised models and other fine-tuned LLMs

## Executive Summary
This paper investigates whether reinforcement learning with verifiable rewards (RLVR) improves reasoning capacity or merely refines reasoning paths in large language models (LLMs), using scientific information extraction (SciIE) as a testbed. The authors propose a two-stage training method combining MimicSFT—which uses structured reasoning templates to improve performance without requiring high-quality chain-of-thought data—with R²GRPO, a reinforcement learning approach that incorporates relevance and rule-induced rewards. Experiments on SciER and OOD datasets show that both methods improve reasoning capacity, with R²GRPO+SFT achieving 66.81 F1 for relation extraction on SciER, surpassing specialized supervised models and other fine-tuned LLMs. The study demonstrates that RLVR can enhance both knowledge memorization and systematic reasoning, challenging the view that it only refines existing reasoning paths.

## Method Summary
The method employs a two-stage training pipeline. First, MimicSFT uses structured pseudo-reasoning templates with multi-task LoRA fine-tuning to improve constrained generation without requiring chain-of-thought annotations. Second, R²GRPO builds on this foundation with composite rewards (F1, span accuracy, relevance, rule adherence) to optimize both knowledge and reasoning capacity. The approach is tested on scientific information extraction tasks using the SciER dataset, with performance measured through micro F1 scores and Best@K/Avg@K metrics to distinguish capacity gains from path selection refinement.

## Key Results
- R²GRPO+SFT achieves 66.81 F1 for relation extraction on SciER, surpassing specialized supervised models
- MimicSFT improves NER performance from 80.8 to 81.7 F1 over standard SFT
- Best@K metrics demonstrate reasoning capacity improvements beyond path selection refinement
- Cross-domain generalization to OOD datasets validates robustness beyond training distribution

## Why This Works (Mechanism)

### Mechanism 1: Structured Reasoning Templates Enable Constraint Decomposition Without CoT Data
The template creates an intermediate reasoning space where schema constraints can be partially satisfied before final output generation. This transforms the optimization landscape from jointly satisfying all constraints to sequential constraint satisfaction, reducing effective search space complexity.

### Mechanism 2: Composite Reward Signals Guide Dual Knowledge-Reasoning Optimization
Multi-component rewards (F1, span accuracy, relevance, rule adherence) provide gradient signals for distinct aspects: F1/span rewards reinforce factual grounding; relevance rewards penalize unsupported citations; rule rewards encourage domain-consistent reasoning patterns.

### Mechanism 3: Hierarchical Two-Level Reasoning Improves Valid Output Probability
Level 1 establishes reasoning framework and schema compliance; level 2 refines with task-specific factual grounding. The hierarchical decomposition enables P(y∈C|hier) ≥ P(y∈C|direct) when intermediate reasoning guides toward valid outputs.

## Foundational Learning

- **Concept: Constrained Generation as Optimization**
  - Why needed here: IE tasks require outputs satisfying both schema constraints and factual constraints
  - Quick check question: Given an IE output with correct entity types but fabricated entity spans, which constraint type is violated?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: R²GRPO builds on GRPO, a PPO variant normalizing rewards across sampled groups
  - Quick check question: How does GRPO's group-based advantage normalization differ from standard PPO's advantage estimation?

- **Concept: Reasoning Capacity vs. Path Selection**
  - Why needed here: The paper's central question is whether RLVR improves underlying capabilities or merely prioritizes existing reasoning paths
  - Quick check question: If a model's pass@1 improves but pass@k declines, does this indicate capacity gain or path selection refinement?

## Architecture Onboarding

- **Component map:** Scientific text → MimicSFT stage (LoRA rank=16) → R²GRPO stage (LoRA rank=64) → JSON validation parser

- **Critical path:**
  1. Pre-train MimicSFT on full SciER (3 epochs, lr=2e-5)
  2. Select 1K subset balancing entity/relation types and difficulty
  3. Initialize R²GRPO from MimicSFT checkpoint
  4. Train R²GRPO with curriculum learning (10 epochs, lr=1e-6)
  5. Evaluate with temperature=0 for deployment, temperature=1.0 for capacity analysis

- **Design tradeoffs:**
  - LoRA rank: Lower (16) for SFT efficiency vs. higher (64) for RL expressiveness
  - Training data: Full dataset for SFT vs. curated subset for RL
  - Temperature: Low (0-0.6) for deterministic extraction vs. high (1.0) for Best@K analysis

- **Failure signatures:**
  - Declining response length during RL training (Figure 5) is normal; increasing length may indicate reward hacking
  - If Avg@K improves but Best@1 degrades, model is over-exploring rather than refining
  - High F1 with low span reward indicates type memorization without boundary precision

- **First 3 experiments:**
  1. Reproduce MimicSFT baseline: Train on SciER with pseudo-templates, compare standard SFT vs. MimicSFT on NER F1
  2. Ablate reward components: Remove one reward at a time and measure RE F1 degradation
  3. Cross-domain transfer: Apply R²GRPO checkpoint to OOD test set without retraining

## Open Questions the Paper Calls Out

- **Question:** To what extent do the performance gains from R²GRPO in scientific information extraction transfer to broader domains and distinct IE tasks like event extraction?
  - Basis: Section 5 explicitly calls for exploring adaptability to broader domains
  - Why unresolved: Current study is confined to SciER dataset and scientific text
  - What evidence would resolve it: Benchmarking R²GRPO on diverse non-scientific IE datasets showing comparable improvements

- **Question:** Can the structured reasoning templates required for MimicSFT be generated or refined automatically, removing the reliance on manual engineering?
  - Basis: Section 5 explicitly calls for investigating more automated methods for generating or refining reasoning templates
  - Why unresolved: Current MimicSFT approach depends on manually defined templates
  - What evidence would resolve it: A framework that autonomously constructs these templates achieving performance parity or superiority

- **Question:** Does the observed synergy between MimicSFT and R²GRPO scale effectively to significantly larger model architectures (70B+ parameters) or different model families?
  - Basis: Section 5 notes the need to explore scalability to larger models and diverse LLM architectures
  - Why unresolved: Experiments were primarily conducted on Qwen2.5-7B
  - What evidence would resolve it: Applying the identical R²GRPO pipeline to 70B+ models demonstrating similar relative improvements

## Limitations

- Study focuses exclusively on scientific information extraction tasks with a specific dataset (SciER) and domain (biomedical/NLP conference papers)
- Template-based reasoning approach relies on human-designed pseudo-CoT templates that may not capture optimal reasoning strategies for all task types
- Composite reward formulation introduces significant hyperparameter complexity with four reward components requiring careful weight tuning

## Confidence

**High Confidence** (supported by direct experimental evidence):
- MimicSFT improves NER performance over standard SFT (81.7 vs 80.8 F1)
- R²GRPO improves RE performance over baseline models (66.81 F1)
- Best@K metrics show reasoning capacity improvements beyond path selection

**Medium Confidence** (supported by experimental results but with limitations):
- The hierarchical reasoning formulation provides theoretical probability bounds
- Composite rewards enable dual knowledge-reasoning optimization
- Cross-domain generalization to OOD datasets

**Low Confidence** (primarily theoretical or with limited validation):
- The specific mechanism by which structured templates enable constraint decomposition
- The exact contribution of each reward component to overall performance
- Temperature sensitivity thresholds and their implications for deployment

## Next Checks

1. **Ablation study on reward components**: Systematically remove each reward component (F1, span, relevance, rule) and measure performance degradation to quantify their individual contributions.

2. **Cross-domain transfer experiment**: Apply the trained R²GRPO model to scientific information extraction tasks in completely different domains (physics, chemistry, social sciences) without domain-specific fine-tuning to assess claimed generalization capabilities.

3. **Temperature robustness analysis**: Systematically evaluate model performance across a wider temperature range (0.1 to 1.5) with more granular sampling to determine exact temperature thresholds where reasoning capacity degrades.