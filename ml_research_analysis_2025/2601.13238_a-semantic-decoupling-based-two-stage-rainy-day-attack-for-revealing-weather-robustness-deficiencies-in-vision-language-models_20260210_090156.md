---
ver: rpa2
title: A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather
  Robustness Deficiencies in Vision-Language Models
arxiv_id: '2601.13238'
source_url: https://arxiv.org/abs/2601.13238
tags:
- image
- semantic
- stage
- attack
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first adversarial framework exploiting
  realistic rainy weather to reveal semantic robustness deficiencies in Vision-Language
  Models (VLMs). The two-stage framework operates by first weakening semantic decision
  boundaries through global rain-layer mixing, then optimizing physically grounded
  rain and illumination parameters to induce stable semantic misalignment.
---

# A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models

## Quick Facts
- arXiv ID: 2601.13238
- Source URL: https://arxiv.org/abs/2601.13238
- Reference count: 40
- Primary result: First adversarial framework exploiting realistic rainy weather to reveal semantic robustness deficiencies in Vision-Language Models (VLMs).

## Executive Summary
This paper introduces a two-stage adversarial framework that exploits realistic rainy weather conditions to reveal semantic robustness deficiencies in Vision-Language Models (VLMs). The approach decouples semantic boundary weakening from physical perturbation synthesis, using global rain-layer mixing in Stage 1 followed by CMA-ES optimization of physically grounded rain and illumination parameters in Stage 2. Evaluations across zero-shot classification, image captioning, and visual question answering show significant performance degradation in mainstream VLMs, with accuracy drops reaching up to 62% in classification tasks. The method generates perturbations that are both interpretable and physically plausible, providing a new benchmark for weather robustness assessment in VLMs.

## Method Summary
The method operates through a two-stage process targeting VLMs' cross-modal embedding spaces. Stage 1 applies a fixed global rain layer with a learnable mixing weight to gradually erode semantic decision boundaries in the embedding space. Stage 2 uses CMA-ES to optimize six raindrop physical parameters (intensity, density, length, width, direction, blur kernel size) plus illumination parameters modeling spatially varying light sources. The framework generates adversarial examples that maintain physical plausibility through perceptual loss and SSIM regularization while achieving semantic misalignment. The attack transfers effectively across tasks by exploiting shared image encoder embeddings.

## Key Results
- Classification accuracy drops up to 62% on OpenCLIP ViT-B/16 models
- Both multi-scale raindrop structures and illumination modeling are key drivers of semantic shifts
- CMA-ES optimization in non-pixel parameter space produces physically plausible perturbations
- Attack successfully transfers to captioning and visual question answering tasks
- Larger VLMs show more severe performance degradation when illumination is included

## Why This Works (Mechanism)

### Mechanism 1
Separating semantic boundary weakening from physical perturbation synthesis improves optimization stability and attack effectiveness compared to joint optimization. Stage 1 operates in a low-dimensional space to erode the discriminative margin between ground-truth and competing semantics, allowing Stage 2's high-dimensional physical parameter search to more reliably induce semantic flips. This conditioned embedding space enables more stable semantic misalignment than simultaneous optimization.

### Mechanism 2
Multi-scale raindrop superposition more effectively disrupts VLM feature extraction than single-scale perturbations by simultaneously corrupting local textures and mid-scale structures. The six optimizable physical parameters create correlated disruptions across spatial frequencies that VLMs rely on for semantic grounding, with ablation studies confirming this component's critical importance.

### Mechanism 3
Coupling illumination modulation with raindrop perturbations amplifies semantic disruption, particularly for larger VLMs. The spatially varying illumination field models real rainfall conditions where illumination is non-uniform, and joint modeling captures compound semantic disruption absent from isolated perturbations. Ablation studies show illumination matters more for larger models.

## Foundational Learning

- **Cross-modal embedding spaces and similarity-based classification in VLMs**: Essential for understanding how the attack targets joint image-text alignment spaces where similarity scores determine classification. Quick check: Given image and text embeddings, how would decreasing one similarity while increasing another affect classification?

- **Covariance Matrix Adaptation Evolution Strategy (CMA-ES)**: Critical for understanding why gradient-free optimization is necessary when optimizing raindrop physical parameters like density and direction that lack differentiable implementations.

- **Perceptual loss functions (VGG-based) and SSIM**: Necessary for understanding how the attack balances semantic disruption with visual realism through constraints that ensure physical plausibility while enabling semantic misalignment.

## Architecture Onboarding

- **Component map**: Clean image → Stage 1 (global rain mixing + embedding extraction) → Stage 2 (CMA-ES optimization of physical parameters) → multi-scale raindrop synthesis + illumination → final adversarial image → evaluation across tasks

- **Critical path**: Stage 1 convergence determines semantic boundary proximity; Stage 2 population initialization affects exploration efficiency; Top-K candidate selection balances attack potential vs. realism; cross-task transfer relies on shared embeddings

- **Design tradeoffs**: λ_reg vs. λ_perc balance affects boundary erosion speed vs. initialization safety; population size >15 shows diminishing returns; raindrops more damaging than illumination overall but illumination more critical for larger models; perceptual constraints limit attack ceiling but ensure transferability

- **Failure signatures**: Stage 1 stagnation indicates tight perceptual constraints; Stage 2 plateau suggests poor initialization or over-constrained parameters; cross-task transfer failure means perturbation affects only low-level features; unrealistic outputs indicate constraint violations

- **First 3 experiments**: 1) Hyperparameter sweep on Stage 1 to validate semantic decoupling assumption; 2) Component ablation to determine primary disruption driver for target model; 3) Population size calibration to identify computational efficiency point

## Open Questions the Paper Calls Out

- **Generalization to other weather conditions**: Does the semantic decoupling framework generalize to fog, haze, or snow with distinct physical scattering properties? The current parametric modeling is tailored specifically to rain characteristics.

- **Defense strategy development**: What specific defenses (adversarial training, augmentation) can mitigate semantic misalignment without degrading standard performance? The paper focuses on attack methodology without implementing defensive mechanisms.

- **Interaction of multiple environmental factors**: How do multiple weather factors interact within the VLM's embedding space—are their combined effects additive or non-linear? While rain and illumination are modeled jointly, complex compound weather effects remain unexplored.

## Limitations

- The study focuses specifically on rainy weather conditions and does not explicitly cover other environmental factors like fog, haze, or snow that may require different physical modeling approaches.

- The evaluation relies on LLM-as-Judge (GPT-4o) for captioning and VQA tasks, which introduces potential variability in judgment consistency across different models and prompts.

- The focus on COCO-80 categories limits generalizability to other semantic domains and real-world deployment scenarios with different label distributions.

## Confidence

**High Confidence**: The two-stage decoupled framework design and general effectiveness in degrading VLM performance; the physical plausibility constraint mechanisms are sound and necessary.

**Medium Confidence**: The specific quantitative results require exact hyperparameter replication; the LLM-as-Judge evaluation introduces some variability though relative trends are likely reliable.

**Low Confidence**: The exact optimal hyperparameter configuration without implementation access; the generalizability to semantic domains outside COCO-80 and significantly different VLM architectures.

## Next Checks

1. **Hyperparameter Calibration on Held-Out Categories**: Run Stage 1 hyperparameter ablation (λ_reg vs. λ_perc) on a held-out set of 5-10 categories to validate the semantic decoupling assumption for your specific target VLM.

2. **Component Ablation on Target Model**: Replicate the raindrop vs. illumination ablation study on your specific target VLM architecture to determine which component drives semantic shifts for your model.

3. **CMA-ES Population Size Efficiency Test**: Run the population size ablation on a small subset of 50 images to identify the point of diminishing returns in attack success vs. computational cost for your setup.