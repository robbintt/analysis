---
ver: rpa2
title: Progressive Depth Up-scaling via Optimal Transport
arxiv_id: '2508.08011'
source_url: https://arxiv.org/abs/2508.08011
tags:
- layers
- layer
- opt-deus
- training
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient scaling of large
  language models (LLMs) by introducing a progressive depth up-scaling method called
  OpT-DeUS. The core idea is to use optimal transport (OT) to align and fuse adjacent
  Transformer blocks from pre-trained layers to create new layers, mitigating neuron
  permutation mismatch that occurs when simply copying or averaging weights.
---

# Progressive Depth Up-scaling via Optimal Transport

## Quick Facts
- arXiv ID: 2508.08011
- Source URL: https://arxiv.org/abs/2508.08011
- Authors: Mingzi Cao; Xi Wang; Nikolaos Aletras
- Reference count: 17
- Key outcome: OpT-DeUS achieves better downstream performance across various model sizes (1.72B and 11.5B parameters) on continual pre-training and supervised fine-tuning tasks compared to baselines like SOLAR, LLaMA PRO, and LESA.

## Executive Summary
This paper addresses the problem of efficient scaling of large language models (LLMs) by introducing a progressive depth up-scaling method called OpT-DeUS. The core idea is to use optimal transport (OT) to align and fuse adjacent Transformer blocks from pre-trained layers to create new layers, mitigating neuron permutation mismatch that occurs when simply copying or averaging weights. OpT-DeUS inserts new layers in the top half of the model and updates only these new layers for training efficiency. Experiments show OpT-DeUS achieves better downstream performance across various model sizes (1.72B and 11.5B parameters) on continual pre-training and supervised fine-tuning tasks compared to baselines like SOLAR, LLaMA PRO, and LESA. The method also demonstrates superior training efficiency, particularly when inserting new layers closer to the top, which reduces backpropagation time.

## Method Summary
OpT-DeUS (Optimal Transport-based Depth Up-scaling) is a method for efficiently scaling Transformer-based language models by adding new layers. The approach uses optimal transport to fuse adjacent pre-trained layers, creating aligned intermediate layers that mitigate neuron permutation issues. New layers are inserted in the top half of the model architecture, and only these new layers are trained while frozen parameters from the original model are retained. This design choice aims to balance computational efficiency with performance gains. The method is evaluated across two model scales (1.72B and 11.5B parameters) and demonstrates improvements over baseline up-scaling approaches on both pre-training and fine-tuning tasks.

## Key Results
- OpT-DeUS outperforms baseline methods (SOLAR, LLaMA PRO, LESA) on downstream tasks for both 1.72B and 11.5B parameter models
- Inserting new layers in the top half of the model reduces backpropagation time compared to bottom-half insertion
- The method achieves better performance with training efficiency, as only new layers are updated during training

## Why This Works (Mechanism)
The method works by using optimal transport to properly align neurons between adjacent Transformer layers during the fusion process. When scaling model depth, simply copying or averaging weights between layers leads to neuron permutation mismatch, where neurons that were functionally equivalent in the original model become misaligned. Optimal transport solves this by finding the optimal alignment between neurons across layers, ensuring that the fused layers preserve functional relationships. By inserting new layers in the top half and only training these new layers, the method reduces computational overhead while still capturing the benefits of increased model depth.

## Foundational Learning
- **Optimal Transport (OT)**: A mathematical framework for finding optimal alignment between probability distributions, used here to align neurons between Transformer layers. Needed to solve neuron permutation mismatch during layer fusion. Quick check: Verify OT implementation correctly computes transport plans between adjacent layers.
- **Transformer Architecture**: The underlying neural network structure being scaled, consisting of self-attention and feed-forward components. Needed as the target architecture for depth scaling. Quick check: Confirm layer insertion maintains proper data flow through the model.
- **Neuron Permutation Mismatch**: The problem where functionally equivalent neurons in different layers become misaligned during simple weight copying/averaging. Needed to understand why naive scaling approaches fail. Quick check: Compare performance of OT-based fusion vs. direct weight copying.
- **Progressive Scaling**: The approach of incrementally adding capacity to pre-trained models rather than training from scratch. Needed to understand the efficiency motivation. Quick check: Measure training time and parameter updates for progressive vs. from-scratch scaling.
- **Layer Fusion**: The process of combining information from multiple layers to create new intermediate representations. Needed to understand how OT creates new layers. Quick check: Analyze activation distributions of fused vs. original layers.
- **Backpropagation Optimization**: Techniques to reduce computational overhead during training. Needed to understand the efficiency gains from top-half insertion. Quick check: Profile memory usage and computation time for different insertion positions.

## Architecture Onboarding
**Component Map**: Pre-trained Transformer -> OT-based Layer Fusion -> New Layer Insertion (top half) -> Partial Training (new layers only)
**Critical Path**: Input sequence -> Original layers (frozen) -> New OT-fused layers (trainable) -> Output
**Design Tradeoffs**: Training only new layers improves efficiency but may limit full model adaptation; top-half insertion reduces backpropagation cost but may affect early-layer representations differently than bottom-half insertion.
**Failure Signatures**: Poor downstream performance indicating OT alignment issues; increased training instability from improper layer fusion; memory bottlenecks from suboptimal insertion positions.
**First Experiments**: 1) Compare OT-fused layer performance against simple weight copying baselines; 2) Test different insertion positions (top vs. bottom half) for computational efficiency; 3) Measure neuron alignment quality using similarity metrics between original and fused layers.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Scalability concerns for models significantly larger than 11.5B parameters due to OT computational overhead
- Limited ablation study on layer insertion positions (only tested top vs. bottom half)
- Unclear generalization to non-Transformer architectures or different attention mechanisms
- Memory consumption trade-offs when inserting layers at different positions not fully characterized

## Confidence
- **High confidence** in the technical novelty of using optimal transport for layer fusion and the empirical superiority over baselines for the tested model sizes
- **Medium confidence** in the training efficiency claims, as they are primarily supported by wall-clock time comparisons without accounting for memory overhead
- **Medium confidence** in the downstream performance improvements, given the relatively small number of tasks (4) and models (2) evaluated

## Next Checks
1. Benchmark OpT-DeUS on larger models (e.g., 30B+ parameters) to assess scalability and computational overhead
2. Conduct memory usage analysis when inserting layers at different positions to validate the claimed backpropagation time savings
3. Evaluate the method on non-LLaMA architectures (e.g., GPT-style or hybrid models) to test architectural generalization