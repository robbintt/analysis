---
ver: rpa2
title: 'In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized
  for Comprehensive Chart Understanding'
arxiv_id: '2507.14298'
source_url: https://arxiv.org/abs/2507.14298
tags:
- data
- chart
- types
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing multimodal language
  models capable of understanding diverse chart types, particularly in cases where
  numerical annotations are not present. It introduces ChartScope, a model optimized
  for both in-depth and in-breadth chart comprehension, using a novel data generation
  pipeline and Dual-Path training strategy.
---

# In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding

## Quick Facts
- **arXiv ID**: 2507.14298
- **Source URL**: https://arxiv.org/abs/2507.14298
- **Reference count**: 4
- **Primary result**: ChartScope achieves state-of-the-art performance on multiple chart comprehension benchmarks including MMC (31.4%), ChartX (45.1%), and PlotQA (34.0%)

## Executive Summary
This paper addresses the challenge of developing multimodal language models capable of understanding diverse chart types, particularly when numerical annotations are absent. The authors introduce ChartScope, a model optimized for both in-depth and in-breadth chart comprehension through a novel data generation pipeline and Dual-Path training strategy. ChartScope demonstrates superior performance across multiple chart comprehension benchmarks, showing particular strength in understanding unannotated charts and diverse chart types compared to previous approaches.

## Method Summary
ChartScope employs a three-stage training approach with a vision-language architecture based on LLaVA. The first stage pre-trains the projector using a combination of CC3M, chart-description pairs, and chart-JSON pairs. The second stage performs end-to-end fine-tuning with Dual-Path data including General QAs, JSON-only QAs, and Data-driven QAs. The third stage applies LoRA fine-tuning per benchmark. The data generation pipeline creates synthetic chart-image pairs using orthogonal generation of code (N=400 scripts) and data (M=1000 JSON files) per chart type, producing ~5M total pairs. Dual-Path training combines Data-driven QAs (extract JSON then answer) with JSON-only QAs (text-only reasoning) to improve both visual-data grounding and reasoning preservation.

## Key Results
- ChartScope achieves state-of-the-art performance on MMC (31.4%), ChartX (45.1%), and PlotQA (34.0%)
- The model demonstrates significant improvement on unannotated charts, with +3% gain on PlotQA compared to previous approaches
- Dual-Path training with combined JSON-only and Data-driven QAs improves ChartQA human performance from 48.96% to 52.28%
- Chart-JSON pairs in projector pre-training improve ChartQA human augmented from 48.56% to 52.28% (+4%)

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Data and Code Generation
The pipeline generates N Python scripts with visual variations and M JSON files with diverse topics using shared templates, enabling N×M synthetic training pairs from N+M generation calls. This quadratic scaling avoids proportional LLM inference costs. The core assumption is that text-only LLMs can produce structurally valid, semantically coherent chart data and rendering code without visual feedback loops.

### Mechanism 2: Dual-Path Training Strategy
Training with both Data-driven QAs (extract JSON then answer) and JSON-only QAs (text-only reasoning) simultaneously improves visual-data grounding while preserving text-based reasoning capabilities. The Data-driven path forces visual-to-data mapping, while JSON-only QAs maintain pure text reasoning without visual shortcuts. This prevents reasoning degradation during domain adaptation.

### Mechanism 3: Projector Pre-training on Chart-JSON Pairs
Pre-training the projector on chart-JSON pairs creates stronger visual-feature-to-data-value grounding compared to only using image-caption pairs. This teaches the model to predict structured JSON (title, axes, values) directly from chart images, encoding explicit visual-to-data mappings rather than just visual-to-caption associations.

## Foundational Learning

- **Vision-Language Alignment (Two-Stage Training)**: Why needed: ChartScope follows LLaVA's paradigm where stage 1 aligns vision encoder outputs to LLM embedding space via projector training, and stage 2 performs end-to-end instruction tuning. Quick check: Why is projector-only pre-training on image-caption pairs typically more effective than end-to-end training from scratch?

- **Instruction Tuning for Domain Adaptation**: Why needed: ChartScope's various QA types are all instruction-tuning data that shape both response format and task distribution. Understanding instruction tuning as "teaching the model how to respond" clarifies why QA diversity matters beyond alignment. Quick check: How does instruction tuning differ from pre-training in terms of data format and training objective?

- **Multi-turn Reasoning and Intermediate Scaffolds**: Why needed: Data-driven QAs explicitly scaffold reasoning by requiring JSON extraction before answering, which is a form of chain-of-thought grounding. Understanding that explicit intermediate steps can reduce reasoning errors explains why this path preserves capability during domain shift. Quick check: Why might prompting a model to "first extract, then answer" improve accuracy compared to direct answering?

## Architecture Onboarding

- **Component map**: Vision Encoder (CLIP ViT) -> Projector (Linear/MLP) -> LLM Backbone (LLaVA-7B/13B, TinyLLaVA-3.1B)
- **Critical path**: Generate templates per chart type → N codes + M data → filter (structure, execution, OCR) → compose pairs → Pretrain projector on CC3M + chart-description + chart-JSON → End-to-end fine-tune with Dual-Path data → Apply LoRA per benchmark
- **Design tradeoffs**: Synthetic vs. real data (synthetic covers 20 types but risks LLM errors; real data limited to 3-6 types), JSON vs. CSV (JSON encodes metadata; CSV is lighter but omits structure), Multi-turn inference vs. direct (better grounding but higher latency), Per-benchmark LoRA vs. unified model (paper uses per-benchmark due to compute constraints)
- **Failure signatures**: OCR shortcut reliance (high performance on annotated charts but drops on unannotated), hallucination from synthetic errors (paper acknowledges "incorrect data can be introduced"), poor generalization beyond 18 chart types (acknowledged limitation), reasoning collapse without JSON-only QAs (Table 4 shows ~3% drop on ChartQA human when JSON-only QAs removed)
- **First 3 experiments**: 1) Pre-training data ablation: Compare projector trained on (a) CC3M only, (b) +chart-description, (c) +chart-JSON. Evaluate on ChartQA and PlotQA. Expected: (c) > (b) > (a), especially on unannotated charts. 2) Dual-Path component ablation: Remove JSON-only QAs, then Data-driven QAs, measuring ChartQA human and PlotQA. Expected: multiplicative degradation, not additive. 3) Inference-time Data Prompting: Compare direct answering vs. extract-then-answer prompting on held-out set. Expected: accuracy gains on reasoning QAs with latency trade-off.

## Open Questions the Paper Calls Out

### Open Question 1: Open-Domain Chart Understanding
The authors explicitly identify developing an open-domain, versatile chart understanding LVLM as future work. The current framework is limited to 18 chart types through predefined templates, raising questions about how to handle the vast variety of chart types found in the wild without specific template fine-tuning.

### Open Question 2: Filtering Semantic Errors in Synthetic Data
The paper acknowledges that synthetic data generated by LLMs cannot be perfect, leading to incorrect data that results in misalignments and incorrect mappings. The current filtering process relies on structural correctness, code execution, and OCR tools, but may not capture semantic inconsistencies or logical errors within the generated text descriptions and data values.

### Open Question 3: Transfer to In-the-Wild Charts
While the model achieves strong performance on clean synthetic charts, the paper doesn't address how well training on Python-generated synthetic charts transfers to real-world charts containing noise, artifacts, or non-standard plotting styles. This domain gap is common in vision-language tasks where models trained on clean data struggle with the noise and stylistic diversity of charts found in scanned documents or presentations.

## Limitations

- The model currently supports understanding only 18 chart types, with limited generalization to more advanced or novel visualization formats
- Synthetic data generated by LLMs may contain systematic errors and misalignments that propagate into the trained model despite filtering
- The approach requires significant computational resources for generating the large synthetic dataset and performing multi-stage training
- Performance gains on unannotated charts suggest reduced OCR shortcut reliance, but the specific mechanisms preventing this behavior are not fully explained

## Confidence

- **High Confidence**: Claims about achieving state-of-the-art performance on established benchmarks (MMC, ChartX, ChartQA) - these are directly measurable results
- **Medium Confidence**: Claims about the Dual-Path training strategy improving both visual-data grounding and reasoning preservation - supported by ablation studies but lacking external validation
- **Medium Confidence**: Claims about quadratic scaling of synthetic data generation - theoretically sound but not empirically validated for this specific application

## Next Checks

1. **Data Quality Validation**: Conduct systematic error analysis on a random sample of synthetic chart-image pairs to quantify misalignment rates and identify failure patterns in the generation pipeline.

2. **Generalization Testing**: Evaluate ChartScope on chart types not included in training (beyond the 20 types) to measure actual generalization capability versus claimed limitations.

3. **Mechanism Isolation**: Design controlled experiments to isolate whether performance gains come from the Dual-Path training specifically or from increased training data diversity more generally.