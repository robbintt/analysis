---
ver: rpa2
title: Learning-based GNSS Uncertainty Quantification using Continuous-Time Factor
  Graph Optimization
arxiv_id: '2503.04933'
source_url: https://arxiv.org/abs/2503.04933
tags:
- gnss
- learning
- methods
- ieee
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two learning-based approaches for quantifying
  GNSS measurement uncertainties, specifically targeting outlier detection and noise
  distribution approximation for pseudorange observations. The first method employs
  a transformer-enhanced LSTM network for offline outlier prediction, leveraging temporal
  and spatial information from satellite observations to detect NLOS signals and predict
  pseudorange errors.
---

# Learning-based GNSS Uncertainty Quantification using Continuous-Time Factor Graph Optimization

## Quick Facts
- **arXiv ID:** 2503.04933
- **Source URL:** https://arxiv.org/abs/2503.04933
- **Reference count:** 25
- **Primary result:** Introduces two learning-based approaches for GNSS uncertainty quantification achieving 40% better accuracy than M-estimators

## Executive Summary
This paper presents two complementary learning-based methods for quantifying GNSS measurement uncertainties in urban environments. The first method employs a transformer-enhanced LSTM network for offline outlier detection, leveraging temporal and spatial information from satellite observations to predict NLOS signals and pseudorange errors. The second method uses online variational Bayesian inference to train a Gaussian mixture model that approximates pseudorange noise distribution from past residuals. Both approaches are validated within a novel continuous-time factor graph optimization framework that enables robust multisensor fusion independent of any specific reference sensor.

## Method Summary
The approach consists of a continuous-time factor graph optimization (FGO) framework using Gaussian Process priors to represent vehicle trajectories, allowing asynchronous integration of GNSS and IMU measurements. Two uncertainty quantification methods operate in parallel: an offline transformer-enhanced LSTM network that classifies NLOS signals and predicts pseudorange errors using historical data, and an online variational Bayesian inference system that fits a Gaussian mixture model to recent measurement residuals. The learned uncertainties are fed back into the FGO to improve state estimation accuracy. The system is designed to handle the challenges of urban GNSS degradation, including multipath effects and signal blockage.

## Key Results
- GMM approach achieves 40% better performance than M-estimators and 11% better than naive GMM implementations
- Transformer-enhanced LSTM shows superior generalization for NLOS classification, particularly effective with out-of-distribution data
- Continuous-time FGO framework enables robust trajectory estimation independent of specific reference sensors
- Experimental validation demonstrates significant improvements in state estimation accuracy in challenging urban environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A continuous-time factor graph optimization using Gaussian Process priors decouples state estimation from specific sensor update rates, reducing information loss during asynchronous GNSS updates.
- **Mechanism:** The system represents the vehicle trajectory as a continuous function of time, allowing the optimizer to query GP-interpolated states whenever GNSS measurements arrive, enabling seamless integration of asynchronous high-frequency IMU data and low-frequency GNSS data.
- **Core assumption:** Vehicle motion between measurement timestamps can be accurately modeled by the chosen GP kernel, and interpolation error is negligible compared to synchronization errors in discrete methods.
- **Evidence anchors:** Section II describes the time-centric factor graph construction that queries GP-interpolated states for measurements not temporally aligned with state variables.

### Mechanism 2
- **Claim:** A Transformer-enhanced LSTM architecture captures both temporal correlations and spatial satellite geometry to predict NLOS signals and pseudorange errors with superior generalization to out-of-distribution data.
- **Mechanism:** The LSTM processes time-series history of GNSS signal features to capture temporal degradation patterns, while the Transformer's attention mechanism evaluates spatial relationships between satellites, dynamically weighing the importance of different satellite signals.
- **Core assumption:** Spatio-temporal features in raw GNSS observations contain sufficient implicit information to distinguish NLOS reflections from direct LOS signals without external 3D city maps.
- **Evidence anchors:** Section III shows the attention-enhanced network achieves best performance even on out-of-distribution data.

### Mechanism 3
- **Claim:** Online Variational Bayesian inference applied to a Gaussian Mixture Model effectively approximates heavy-tailed, multimodal noise distributions, outperforming static robust cost functions.
- **Mechanism:** Instead of assuming fixed Gaussian noise or robust kernels, the system fits a GMM to history of measurement residuals using VB, dynamically constructing a cost function that models outliers as distinct modes in the distribution.
- **Core assumption:** Past residuals are stationary enough over the sliding window to provide valid statistical basis for current measurement's noise distribution.
- **Evidence anchors:** Table I shows GMM with MPMA achieving 0.54m mean error vs 0.90m for M-estimator (Cauchy).

## Foundational Learning

- **Concept: Gaussian Process (GP) Priors for Trajectory Estimation**
  - **Why needed here:** The core estimator relies on continuous-time trajectory representation using GPs to define distributions over functions and allow state interpolation at arbitrary timestamps.
  - **Quick check question:** Can you explain how the covariance matrix of a GP prior changes as the time-gap between two query points increases?

- **Concept: Variational Inference (VI)**
  - **Why needed here:** The online noise approximation uses Variational Bayes rather than standard EM to approximate intractable posterior distributions for GMM parameters by optimizing a lower bound (ELBO).
  - **Quick check question:** Why might Variational Bayes be preferred over standard EM for online parameter updates in a robust control loop?

- **Concept: Self-Attention in Transformers**
  - **Why needed here:** The offline predictor uses a "transformer-enhanced" LSTM, requiring understanding of attention mechanism to debug why the model weighs specific satellites more heavily and processes the spatial aspect of the constellation.
  - **Quick check question:** In the context of GNSS, what does the "Query, Key, Value" concept of attention map to physically?

## Architecture Onboarding

- **Component map:** Raw GNSS/IMU/Lidar -> Core Estimator (FGO with GP priors) -> Residual Generator -> Uncertainty Quantification (TE-LSTM + VB-GMM) -> Feedback to FGO
- **Critical path:** Residual Generator → Online GMM → FGO Update. Biased residuals or slow GMM convergence lead to trajectory drift.
- **Design tradeoffs:** Offline TE-LSTM generalizes well to OOD data but cannot adapt to sudden local geometry changes; online GMM adapts to local conditions but assumes residual stationarity. GMM approach increases computation time by 53%.
- **Failure signatures:** Mode collapse (GMM fits single wide Gaussian), over-rejection (TE-LSTM classifies all signals in new environment as NLOS), interpolation lag in high-dynamics maneuvers.
- **First 3 experiments:**
  1. Sanity Check: Simulate high-dynamic trajectory with asynchronous updates to verify GP-interpolated state error is lower than discrete pre-integration baseline.
  2. Feature Ablation: Run offline model with and without Transformer attention head on Düsseldorf dataset to quantify spatial attention contribution.
  3. Convergence Analysis: Inject synthetic heavy-tailed noise and measure time-steps for VB-GMM to converge to true mixture weights compared to adaptive M-estimators.

## Open Questions the Paper Calls Out

- How can offline (TE-LSTM) and online (Variational GMM) learning paradigms be effectively integrated into a federated framework to enable mutual enhancement? (Section V states intent to integrate both paradigms in federated system)
- How can sample efficiency and augmentation strategies for online VB-GMM be optimized to handle rapid transitions between driving contexts? (Section IV notes need for studies on sample efficiency considering driving contexts)
- Can the proposed offline transformer-enhanced LSTM model be seamlessly integrated into back-end optimization process rather than serving solely as front-end pre-processor? (Section III states not all methods can be seamlessly integrated into state estimator)

## Limitations

- Data distribution constraints: TE-LSTM's generalization relies on training dataset sufficiently capturing urban environment diversity, with performance potentially degrading in radically different environments
- Real-time performance trade-offs: GMM approach incurs 53% increase in computation time (18.27ms vs 11.93ms), potentially prohibitive for resource-constrained systems
- Temporal stationarity assumption: Online VB-GMM assumes past residuals provide valid statistical basis, which may break down during rapid environment transitions

## Confidence

- Data Distribution Constraints: Medium
- Real-time Performance Trade-offs: High
- Temporal Stationarity Assumption: Medium

## Next Checks

1. **Distribution Shift Sensitivity Analysis**: Systematically evaluate TE-LSTM's NLOS classification performance across urban-to-suburban-to-rural gradient to quantify accuracy degradation rate and identify triggering environmental features.

2. **Convergence Speed Benchmarking**: Conduct controlled experiments injecting synthetic heavy-tailed noise with known parameters, measuring time-steps for VB-GMM to converge to true mixture weights compared to adaptive M-estimators.

3. **Computational Complexity Scaling**: Profile computational requirements of GMM-enhanced FGO as function of satellite count (5-20) and graph complexity, identifying when overhead becomes prohibitive for real-time operation on automotive-grade processors.