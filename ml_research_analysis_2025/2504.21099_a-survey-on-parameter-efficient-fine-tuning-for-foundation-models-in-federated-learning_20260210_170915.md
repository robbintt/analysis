---
ver: rpa2
title: A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated
  Learning
arxiv_id: '2504.21099'
source_url: https://arxiv.org/abs/2504.21099
tags:
- federated
- learning
- arxiv
- data
- peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically categorizes and reviews Parameter-Efficient
  Fine-Tuning methods within Federated Learning settings, organizing them into additive,
  selective, and reparameterized approaches. It examines how these methods address
  challenges such as data heterogeneity, communication efficiency, computational constraints,
  and privacy concerns across NLP and computer vision tasks.
---

# A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning

## Quick Facts
- arXiv ID: 2504.21099
- Source URL: https://arxiv.org/abs/2504.21099
- Reference count: 40
- Primary result: Comprehensive survey categorizing PEFT methods for FL across additive, selective, and reparameterized approaches, addressing challenges of data heterogeneity, communication efficiency, and privacy concerns

## Executive Summary
This survey systematically categorizes and reviews Parameter-Efficient Fine-Tuning (PEFT) methods within Federated Learning (FL) settings, organizing them into three main approaches: additive (LoRA, Prefix Tuning), selective (Adapter-based, Prompt-based), and reparameterized (BitFit, Sparse training). The authors examine how these methods address key FL challenges including data heterogeneity, communication efficiency, computational constraints, and privacy concerns across Natural Language Processing (NLP) and Computer Vision (CV) tasks. The survey provides a comprehensive framework for understanding FL-PEFT integration and identifies future research directions including scalability to larger foundation models, theoretical analysis, and sustainable approaches. While the survey itself does not present primary empirical results, it synthesizes existing literature to demonstrate the effectiveness of PEFT methods in reducing computational overhead while maintaining performance comparable to full fine-tuning in federated environments.

## Method Summary
The survey employs a systematic literature review methodology, identifying and categorizing PEFT methods based on their underlying mechanisms and applicability to federated learning scenarios. The authors organize the methods into three main categories: additive methods that introduce trainable components, selective methods that modify input representations, and reparameterized methods that optimize parameter subsets. For each category, the survey examines the specific adaptations required for federated deployment, including communication strategies, privacy-preserving techniques, and computational optimizations. The analysis draws from 40+ references spanning both FL and PEFT literature, with particular emphasis on practical implementation considerations and empirical performance across benchmark datasets in both NLP and CV domains.

## Key Results
- PEFT methods reduce communication overhead by 98-99% compared to full fine-tuning in federated settings while maintaining comparable model performance
- Additive methods (particularly LoRA) demonstrate superior scalability for large foundation models in FL scenarios with minimal accuracy degradation
- Selective and reparameterized methods offer computational advantages on edge devices but face challenges with task diversity and model expressiveness
- Privacy-preserving FL-PEFT methods effectively mitigate gradient leakage risks while maintaining competitive performance on non-IID datasets

## Why This Works (Mechanism)
PEFT methods work by updating only a small subset of parameters while keeping the foundation model's core parameters frozen, dramatically reducing the computational and communication overhead in federated learning. The additive approach inserts low-rank matrices that capture task-specific features, selective methods focus updates on task-relevant input representations, and reparameterized techniques optimize only critical parameters identified through sparsity or importance scoring. In federated settings, these methods address the fundamental tension between model performance and resource constraints by enabling multiple clients to collaboratively fine-tune large models without sharing raw data or updating the entire parameter space.

## Foundational Learning
**Foundation Models** - Pre-trained models (like BERT, GPT, ViT) that serve as starting points for downstream tasks. *Why needed*: Provide strong baseline performance that PEFT can build upon without expensive full fine-tuning. *Quick check*: Verify the model has been pre-trained on large corpus and can perform zero-shot inference.

**Federated Learning** - Decentralized training paradigm where clients collaborate without sharing raw data. *Why needed*: Enables privacy-preserving collaborative model improvement across distributed datasets. *Quick check*: Confirm data remains on client devices and only model updates are communicated.

**Parameter Efficiency** - Strategies that achieve comparable performance while updating only a small fraction of model parameters. *Why needed*: Critical for reducing communication costs and computational burden in resource-constrained FL environments. *Quick check*: Calculate the ratio of trainable parameters to total parameters (typically <1%).

## Architecture Onboarding

**Component Map**: Foundation Model <- Frozen Parameters -> PEFT Modules -> Client Updates -> Server Aggregation -> Global Model

**Critical Path**: Foundation Model Initialization -> PEFT Module Integration -> Client Fine-tuning -> Gradient/Parameter Upload -> Server Aggregation -> Model Synchronization

**Design Tradeoffs**: 
- Communication vs. Performance: Higher parameter counts improve accuracy but increase communication costs
- Privacy vs. Expressiveness: More complex PEFT methods may leak more information through updates
- Scalability vs. Compatibility: Methods optimized for large models may not work well on smaller architectures

**Failure Signatures**:
- Communication bottlenecks when PEFT parameter count exceeds network capacity
- Degraded performance on non-IID data distributions due to conflicting client updates
- Privacy vulnerabilities through gradient inversion attacks on shared PEFT parameters

**First Experiments**:
1. Compare communication overhead of FedAvg vs. FLoRA on standard benchmark datasets with varying client counts
2. Evaluate performance degradation when applying LoRA with different rank values across heterogeneous client devices
3. Test gradient leakage vulnerability of adapter-based methods versus additive methods under standard membership inference attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What convergence guarantees and generalization bounds can be established for PEFT-based federated optimization algorithms under non-IID data distributions?
- Basis in paper: [explicit] "A primary direction is convergence analysis, which aims to establish theoretical guarantees for the convergence behavior of PEFT-based federated optimization algorithms, especially under client heterogeneity... Similarly, deriving generalization bounds under non-IID data distributions would enhance confidence in PEFT's applicability."
- Why unresolved: Existing theoretical frameworks for conventional FL do not account for the reduced parameter update space in PEFT, and the interplay between low-rank parameterizations and federated optimization dynamics remains uncharacterized.
- What evidence would resolve it: Formal proofs of convergence rates for specific FL-PEFT methods (e.g., FedSA-LoRA, PF2LoRA) under defined non-IID conditions, plus empirical validation across standard benchmarks with varying heterogeneity levels.

### Open Question 2
- Question: How can PEFT methods scale to ultra-large foundation models (trillions of parameters) while accommodating heterogeneous client computational capabilities in federated settings?
- Basis in paper: [explicit] "As foundation models continue to scale exponentially—with recent architectures surpassing trillions of parameters—the challenges of efficiently fine-tuning such models in federated settings become increasingly pronounced... their applicability and efficiency in the context of ultra-large models remain limited."
- Why unresolved: Current federated PEFT methods face communication bottlenecks with extremely large models, and memory requirements exceed edge device capabilities. No existing method addresses quantization-aware PEFT with heterogeneous client-adaptive precision.
- What evidence would resolve it: Development and evaluation of quantization-aware federated PEFT methods that dynamically adjust precision based on client capabilities, demonstrating successful fine-tuning of models exceeding 100B parameters across resource-diverse clients.

### Open Question 3
- Question: Can server-side aggregation bias in federated LoRA be eliminated without introducing additional communication overhead or privacy vulnerabilities?
- Basis in paper: [inferred] The paper documents multiple approaches (FFA-LoRA, FLoRA, FedEx-LoRA, LoRA-FAIR) addressing the aggregation bias where ∑pkBkAk ≠ (∑pkBk)(∑pkAk), but notes trade-offs: "FLoRA eliminates the server-side aggregation bias, this method incurs high communication costs proportional to the number of clients and raises privacy concerns."
- Why unresolved: Existing solutions require either limiting tunable parameters (FFA-LoRA), increasing communication (FLoRA), or additional server-side computation (FedEx-LoRA requires SVD each round). A unified solution without these trade-offs remains elusive.
- What evidence would resolve it: A method achieving mathematically exact aggregation with communication costs comparable to baseline FedAvg, formal privacy analysis demonstrating no increased vulnerability, and empirical validation matching full fine-tuning performance.

## Limitations
- The survey primarily synthesizes existing literature rather than presenting original empirical results, introducing potential bias from original studies' methodologies
- Rapid pace of research in this field may lead to incomplete coverage of recent developments and emerging approaches
- Categorization framework (additive, selective, reparameterized) has ambiguous boundaries, with some methods fitting into multiple categories

## Confidence
- High confidence: The classification of PEFT methods into three main categories and their general applicability to federated learning scenarios
- Medium confidence: Claims about communication efficiency improvements and computational overhead reduction, as these depend heavily on specific implementation details and federated learning setups
- Medium confidence: The identified research challenges and future directions, which are reasonable extrapolations but not empirically validated

## Next Checks
1. Conduct a systematic reproducibility study comparing multiple PEFT methods under identical federated learning configurations across both NLP and CV tasks
2. Perform theoretical analysis of convergence rates for federated PEFT methods compared to full fine-tuning approaches
3. Implement and evaluate a benchmark suite that tests the scalability of PEFT methods to foundation models with over 100 billion parameters in realistic federated scenarios