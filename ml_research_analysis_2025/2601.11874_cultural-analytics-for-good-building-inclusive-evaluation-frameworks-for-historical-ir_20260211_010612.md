---
ver: rpa2
title: 'Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for
  Historical IR'
arxiv_id: '2601.11874'
source_url: https://arxiv.org/abs/2601.11874
tags:
- retrieval
- fiction
- non-fiction
- knowledge
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of effective information retrieval
  from historical digital collections, particularly for 19th-century British literature
  and non-fiction. The authors construct a benchmark using the British Library's BL19
  collection, employing expert-driven query design, paragraph-level relevance annotation,
  and LLM assistance to create a scalable evaluation framework.
---

# Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR

## Quick Facts
- **arXiv ID**: 2601.11874
- **Source URL**: https://arxiv.org/abs/2601.11874
- **Reference count**: 40
- **Primary result**: Fiction-derived relevance models improve 19th-century non-fiction retrieval by 3-4% in MAP and recall

## Executive Summary
This work addresses the challenge of effective information retrieval from historical digital collections, particularly for 19th-century British literature and non-fiction. The authors construct a benchmark using the British Library's BL19 collection, employing expert-driven query design, paragraph-level relevance annotation, and LLM assistance to create a scalable evaluation framework. Their approach focuses on knowledge transfer from fiction to non-fiction, exploring how narrative understanding and semantic richness in fiction can improve retrieval for factual materials. The evaluation demonstrates that retrieval models incorporating semantic knowledge from fiction significantly outperform baseline methods, with improvements of over 3% in MAP and 4% in recall when applying relevance models learned from fiction to non-fiction retrieval tasks. This interdisciplinary framework not only enhances retrieval accuracy but also promotes interpretability, transparency, and cultural inclusivity in digital archives.

## Method Summary
The method employs a hybrid expert-LLM annotation pipeline to create graded relevance judgments for the BL19 benchmark. Domain experts design 35 queries reflecting research themes, while gpt-5-mini performs initial 5-point relevance annotation (0-4 scale) on top-100 BM25 results per query. Experts validate 40% of judgments, achieving near 100% inter-assessor agreement. The retrieval framework uses BM25 as baseline and RLM (Relevance-based Language Models) for cross-genre transfer, training on fiction corpus to extract expansion terms that bridge lexical and stylistic gaps when applied to non-fiction retrieval.

## Key Results
- FictionRLM achieves ~3% MAP improvement and ~4% recall gain over NonFictionRLM baseline
- Cross-genre transfer works because fiction encodes broader conceptual relationships absent in factual co-occurrence patterns
- Genre-mixed relevance modeling (Fiction-NonFictionRLM) slightly degrades precision compared to targeted fiction transfer
- Performance gains come from retrieving additional relevant documents lower in rank, not top results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relevance models trained on fiction can improve non-fiction retrieval through cross-genre query expansion.
- Mechanism: Fiction encodes broader conceptual relationships (metaphoric, causal, emotional associations) that are semantically relevant to non-fiction topics but absent in strictly factual co-occurrence patterns. RLM trained on fiction extracts these expansion terms, which bridge lexical and stylistic gaps when applied to non-fiction retrieval.
- Core assumption: Fictional prose from the same historical period shares underlying semantic structures with non-fiction, even when surface vocabulary differs.
- Evidence anchors:
  - [section 4.4] "FictionRLM produces the best overall results... approximate improvement of over 3% in MAP and 4% in recall over the intra-domain model (NonFictionRLM)"
  - [section 4.4] "fiction-derived term distributions help the system retrieve additional related passages lower in rank"
- Break condition: If fiction and non-fiction share minimal vocabulary overlap, or if expansion terms from fiction are too domain-specific, the vocabulary mismatch problem dominates and transfer fails.

### Mechanism 2
- Claim: Hybrid expert-LLM annotation pipelines can produce reliable graded relevance judgments for specialized historical domains.
- Mechanism: Domain experts design queries reflecting research themes; LLM (gpt-5-mini) performs initial graded relevance annotation (0-4 scale); experts validate a random sample to verify reliability.
- Core assumption: LLMs fine-tuned or prompted for domain-specific relevance can approximate expert judgments in cultural-analytic contexts.
- Evidence anchors:
  - [section 3.3] "Inter-assessor agreement between gpt-5-mini and the experts was nearly 100%, with discrepancies observed only at the extreme ends of the scale"
  - [section 3.3] 40% of judgments were expert-validated
- Break condition: If LLM systematically misinterprets historical idioms, archaic terms, or genre-specific conventions, expert validation burden increases dramatically.

### Mechanism 3
- Claim: Genre-mixed relevance modeling can degrade precision compared to targeted cross-genre transfer.
- Mechanism: Combining fiction and non-fiction for RLM feedback (Fiction-NonFictionRLM) introduces competing term distributions—fiction contributes contextual diversity but may dilute topical precision required for factual retrieval.
- Core assumption: Fiction and non-fiction have distinct term-weight hierarchies even for shared vocabulary, and mixing them without weighting reduces discriminative value.
- Evidence anchors:
  - [section 4.4] "When both fiction and non-fiction are merged... performance decreases slightly relative to the fiction-only transfer setting"
  - [table 2] Shows divergent term weights: fiction emphasizes imagery ("festival," "revelry") while non-fiction emphasizes discourse ("temperance," "legislation")
- Break condition: If genre-aware weighting or adaptive fusion is introduced to balance contributions, this trade-off may be mitigated—current mechanism reflects unweighted mixing only.

## Foundational Learning

- Concept: **Pseudo-relevance feedback (PRF) and Relevance-based Language Models (RLM)**
  - Why needed here: The core technical approach uses RLM to extract expansion terms from top-ranked documents. Understanding how PRF assumes top-k documents are relevant, and how RLM estimates term distributions, is essential to interpreting why fiction-to-non-fiction transfer works.
  - Quick check question: If the top-10 retrieved documents are not relevant, what happens to expansion term quality?

- Concept: **Diachronic language variation in historical corpora**
  - Why needed here: 19th-century English exhibits orthographic, lexical, and semantic drift. Baseline BM25 performance (MAP 0.4993) reflects this challenge; the paper argues narrative structures help bridge these gaps.
  - Quick check question: Why would a term like "factory girls" have different semantic associations in 1850 fiction vs. 1850 non-fiction?

- Concept: **Cross-domain / cross-genre transfer in IR and NLP**
  - Why needed here: The paper situates its work within domain adaptation literature. Understanding that models trained on one genre may learn partial patterns useful for another helps contextualize the hypothesis.
  - Quick check question: What is the key risk when applying a relevance model trained on source domain S to target domain T?

## Architecture Onboarding

- Component map:
  - Corpus: BL19 subset (10,210 fiction works, 15,780 non-fiction works, 1830-1899)
  - Topics: 35 expert-designed queries (avg. length 2.8 tokens)
  - Relevance judgments: LLM-assisted 5-point graded scale (0-4), expert-validated on 40% sample
  - Retrieval models: BM25 baseline (Lucene), RM3-based RLM variants (NonFictionRLM, FictionRLM, Fiction-NonFictionRLM)
  - Evaluation: MAP, Recall, nDCG, P@10, MRR

- Critical path:
  1. Index fiction and non-fiction subsets separately
  2. Run BM25 retrieval on non-fiction for each query to generate candidate pools (top-100)
  3. Train RLM on fiction corpus to extract expansion terms
  4. Apply fiction-derived expansion terms to non-fiction retrieval
  5. Compare against NonFictionRLM (intra-domain) and Fiction-NonFictionRLM (mixed) baselines

- Design tradeoffs:
  - **Interpretability vs. neural performance**: Authors intentionally exclude neural re-rankers to isolate cross-genre transfer effects; this sacrifices potential gains from dense retrievers but preserves transparency.
  - **Coverage vs. precision**: FictionRLM improves recall and MAP but not P@10/MRR—gains come from lower-ranked relevant documents, not top results.
  - **Scalability vs. annotation depth**: Hybrid LLM-expert annotation enables larger-scale benchmarks but introduces LLM-specific biases.

- Failure signatures:
  - Low AP on highly symbolic or narrative-specific queries (e.g., "Italian white mice," "Dublin ghost/banshee")—these topics have weak non-fiction presence and high lexical instability.
  - Performance drop in Fiction-NonFictionRLM vs. FictionRLM indicates over-mixing genres dilutes discriminative terms.
  - Discrepancies at extremes of relevance scale (0 vs. 1, 3 vs. 4) suggest boundary conditions where LLM and expert judgments diverge.

- First 3 experiments:
  1. Reproduce Table 1 results: Run NonFictionbase, NonFictionRLM, FictionRLM, and Fiction-NonFictionRLM on the BL19 benchmark. Verify that FictionRLM achieves ~3% MAP improvement over NonFictionRLM.
  2. Ablate term overlap constraint: Test FictionRLM without filtering expansion terms to fiction-non-fiction vocabulary overlap. Measure impact on MAP and recall.
  3. Genre-aware weighting: Implement a weighted combination of fiction and non-fiction RLM term distributions (e.g., α·fiction + (1-α)·nonfiction). Sweep α from 0 to 1 and observe precision-recall trade-off curve.

## Open Questions the Paper Calls Out

- Question: Does cross-genre knowledge transfer from fiction to non-fiction generalize to other centuries, languages, topics, and literary traditions beyond 19th-century British writings?
  - Basis in paper: [explicit] The conclusion states "further research is needed to verify whether similar effects extend to other centuries, languages, topics, and literary traditions."
  - Why unresolved: The study only examined 19th-century British texts from the BL19 collection; no experiments were conducted on other historical periods, languages, or cultural traditions.
  - What evidence would resolve it: Replication of the cross-genre transfer methodology on historical corpora from different time periods (e.g., 18th or 20th century), non-English languages, and non-Western literary traditions.

- Question: Do the observed retrieval improvements stem primarily from richer semantic associations in fiction or from stylistic affinities between fiction and non-fiction genres?
  - Basis in paper: [explicit] The authors state "Future studies could investigate whether performance gains arise primarily from the richer semantic associations of fictional novels or from stylistic affinities between genres."
  - Why unresolved: The current framework does not disentangle semantic richness from stylistic features; both mechanisms could contribute to the observed 3% MAP improvement.
  - What evidence would resolve it: Controlled ablation experiments isolating semantic features (word embeddings, conceptual associations) from stylistic features (syntactic patterns, discourse structure) using genre-controlled datasets.

- Question: Can genre-aware weighting or adaptive fusion methods preserve fiction's imaginative diversity while maintaining topical precision for non-fiction retrieval?
  - Basis in paper: [explicit] The conclusion proposes "introducing genre-aware weighting or adaptive fusion methods may help preserve the imaginative diversity of fiction while maintaining the thematic precision required for effective non-fiction retrieval."
  - Why unresolved: The Fiction-NonFictionRLM experiment showed that naively combining genres dilutes discriminative value (MAP dropped from 0.5743 to 0.5411), but no adaptive methods were tested.
  - What evidence would resolve it: Development and evaluation of retrieval models that dynamically weight fiction-derived versus non-fiction-derived expansion terms based on query characteristics or topic type.

## Limitations

- The dependence on gpt-5-mini for relevance judgments creates a black-box dependency that cannot be independently verified or reproduced
- The framework's generalizability beyond 19th-century British literature to other historical periods or cultural contexts remains untested
- The exact weighting formulas for term selection and the complete implementation details of the hybrid annotation pipeline are not fully specified in the paper

## Confidence

- **High Confidence**: The technical mechanism of RLM cross-genre transfer is well-established in the IR literature. The observed performance improvements (3-4% MAP/Recall gains) are statistically sound and consistent with expected effects of semantic expansion.
- **Medium Confidence**: The specific implementation details of the hybrid annotation pipeline and the exact weighting formulas for term selection are not fully specified in the paper, requiring access to the repository code for complete verification.
- **Low Confidence**: The generalizability of these results beyond 19th-century British literature to other historical periods or cultural contexts remains untested. The framework's effectiveness for different query types and document genres is also not fully explored.

## Next Checks

1. **Reproduce core results**: Run the provided code to verify that FictionRLM achieves the reported 3% MAP improvement over NonFictionRLM on the BL19 benchmark using the pre-computed qrels.

2. **Boundary condition testing**: Test the framework's performance on the "Italian white mice" and "Dublin ghost/banshee" queries to confirm the expected low performance on highly symbolic or narrative-specific topics with weak non-fiction presence.

3. **Annotation pipeline validation**: Conduct an independent expert review of a random sample of LLM-generated judgments to verify the claimed 100% agreement rate and identify any systematic biases in the annotation process.