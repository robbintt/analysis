---
ver: rpa2
title: 'MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural
  Recommendation Models'
arxiv_id: '2509.07319'
source_url: https://arxiv.org/abs/2509.07319
tags:
- learning
- incremental
- recommendation
- data
- megg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in dynamic recommender
  systems where user preferences change over time. While incremental learning is well-studied
  in domains like computer vision, it remains underdeveloped for neural recommendation
  models due to unique challenges such as data sparsity and domain shift.
---

# MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models

## Quick Facts
- arXiv ID: 2509.07319
- Source URL: https://arxiv.org/abs/2509.07319
- Reference count: 40
- Key outcome: Experience replay method using GGscore metric consistently outperforms state-of-the-art incremental learning approaches for neural recommendation models

## Executive Summary
This paper addresses catastrophic forgetting in dynamic recommender systems by proposing MEGG, an experience replay-based incremental learning method. MEGG introduces a novel GGscore metric that quantifies the influence of individual samples on model training through gradient alignment. By selectively replaying highly influential samples from both extremes of the gradient spectrum, MEGG effectively mitigates forgetting while maintaining high recommendation performance. Experiments demonstrate consistent improvements across rating prediction and classification tasks on multiple benchmark datasets.

## Method Summary
MEGG is an experience replay method for incremental learning in neural recommendation models. It computes GGscore for each sample by measuring the dot product between the sample's gradient and a reference gradient (the total dataset loss gradient). Samples are then selected using a "keep-both-ends" strategy, retaining those with maximally extreme positive and negative GGscores. The method uses parameter-localized gradient computation for efficiency, focusing only on user/item embeddings and final layers. Implementation details include 15-stage incremental learning, 5 epochs per stage, batch size 1024, and embedding dimension 64.

## Key Results
- MEGG consistently outperforms state-of-the-art methods across four benchmark datasets
- Strong performance in both rating prediction (RMSE) and classification (AUC) tasks
- Demonstrates excellent scalability and efficiency with near-constant sampling time as embedding size grows
- Shows robustness with significant improvements when replay ratio exceeds 70%

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Influence Estimation
MEGG uses GGscore, defined as the dot product between a sample's gradient and the reference gradient of total dataset loss, to approximate One Step Loss Change. This first-order Taylor expansion approach assumes the model is near convergence and parameter changes are small. The method efficiently estimates sample influence without retraining.

### Mechanism 2: Bimodal "Keep-Both-Ends" Sampling
Unlike methods selecting only hard examples, MEGG retains samples from both extremes of the GGscore distribution. This preserves samples that exert the strongest positive and negative pressure on model convergence, maintaining critical knowledge in opposing directions.

### Mechanism 3: Parameter-Localized Gradient Computation
For efficiency, MEGG computes gradients only for user embeddings, item embeddings, and the final fully connected layer. This exploits the sparsity of collaborative filtering, where individual interactions primarily affect specific parameters, keeping computation constant as embedding size grows.

## Foundational Learning

- **Experience Replay (Reservoir Sampling)**: Required for understanding MEGG's replay buffer mechanism. Quick check: Why does random sampling (GDumb) often outperform complex heuristics in Class-Incremental Learning?
- **Taylor Expansion in Optimization**: Needed to understand GGscore's theoretical justification. Quick check: Why might the approximation ΔL ≈ ∇L · Δθ fail with very large learning rates?
- **Sparsity in Collaborative Filtering**: Explains MEGG's efficiency gains. Quick check: In NCF models, how many parameters typically change when computing gradients for a single user-item pair?

## Architecture Onboarding

- **Component map**: Data Reservoir -> Trainer -> GGscore Engine -> Selector
- **Critical path**: Train on reservoir → Compute reference gradient → Score all samples → Select extreme GGscores → Merge with new data → Repeat
- **Design tradeoffs**: Accuracy vs. efficiency (parameter-localized gradients), robustness vs. noise (penultimate epoch state)
- **Failure signatures**: Uniform near-zero GGscores indicate vanishing reference vector; degraded AUC suggests "keep-both-ends" selecting noise
- **First 3 experiments**: Sanity check correlation between GGscore and loss change, ablation testing "keep-both-ends" vs. alternatives, efficiency benchmark with/without parameter selection

## Open Questions the Paper Calls Out

### Open Question 1
Why does combining MEGG with other incremental methods (SML/IncCTR) degrade performance in NFM models but not in WDL/DCN? The paper reports this empirical observation but lacks analysis of NFM's architectural differences (e.g., bi-interaction pooling) that might cause conflicts with combined approaches.

### Open Question 2
How does the batch size simplification (B-1 to B) in GGscore derivation quantitatively affect influence estimation accuracy compared to exact One Step Loss Change? While the method works empirically, the error bound from this approximation isn't derived or tested.

### Open Question 3
Can GGscore theoretically guarantee optimal reference vector and parameter snapshot selection without grid search? The paper treats snapshot timing as a hyperparameter, lacking a mathematical framework linking convergence stage to information gain.

## Limitations
- Assumes model convergence for accurate gradient alignment, which may not hold in early training stages
- Efficiency gains depend on localized parameter updates, potentially insufficient for models with complex non-local feature interactions
- Bimodal sampling requires sufficient buffer size to represent both extremes meaningfully

## Confidence

- High confidence: Experimental results showing MEGG outperforming baselines across multiple datasets and models
- Medium confidence: Theoretical justification for GGscore as gradient-alignment metric
- Medium confidence: Efficiency claims regarding parameter-localized gradient computation

## Next Checks

1. Verify correlation between GGscore and actual loss change across different training stages
2. Test MEGG's performance with varying reservoir sizes to determine minimum buffer size effectiveness
3. Profile memory and computation overhead when applying MEGG to models with deeper architectures beyond embeddings and final FC layer