---
ver: rpa2
title: 'When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based
  Multi-Agent Systems'
arxiv_id: '2602.00428'
source_url: https://arxiv.org/abs/2602.00428
tags:
- answer
- memory
- effect
- mandela
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the Mandela effect\u2014collective false\
  \ memories\u2014in LLM-based multi-agent systems. It introduces MANBENCH, a benchmark\
  \ with 4,838 questions across four task types, and five interaction protocols to\
  \ measure susceptibility."
---

# When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems

## Quick Facts
- arXiv ID: 2602.00428
- Source URL: https://arxiv.org/abs/2602.00428
- Reference count: 40
- All 13 tested LLMs vulnerable to collective false memory effects, with reality shift rates ranging from 15% to over 99% depending on protocol and model

## Executive Summary
This paper investigates the Mandela effect—collective false memories—in LLM-based multi-agent systems. The authors introduce MANBENCH, a benchmark with 4,838 questions across four task types, and five interaction protocols to measure susceptibility. Evaluated on 13 LLMs, the study finds all models vulnerable to the effect, especially in role-based narratives and specialized domains. Reality shift rates range from 15% to over 99% depending on protocol and model. Two prompt-level defenses (cognitive anchoring, source scrutiny) and a model-level alignment approach reduce the effect by up to 74.40%. Results highlight the need for robust defenses to prevent misinformation spread in collaborative AI systems.

## Method Summary
The study introduces MANBENCH, a benchmark of 4,838 questions across History, Misconceptions, General, and Domain-Specific domains. Five interaction protocols test susceptibility: Baseline (B), Generic Short-term (GS), Generic Long-term (GL), Role-based Short-term (RS), and Role-based Long-term (RL). The key metric is reality shift rate (σP), measuring correct-to-incorrect answer changes after social influence. Defenses include prompt-level cognitive anchoring (establish independent conclusions first) and source scrutiny (evaluate peer credibility), plus model-level SFT alignment training with resilience and cooperative sets.

## Key Results
- All 13 tested LLMs showed vulnerability to collective false memories, with reality shift rates ranging from 15% to over 99%
- Role-based protocols (σRS, σRL) produced higher reality shift rates than generic protocols (σGS, σGL) for nearly all models
- Long-term memory protocols showed persistent false beliefs in some models (e.g., Llama3.1-8B: 85.13% long-term shift) while others showed strong self-correction (GPT-5: near-zero long-term shift)
- Cognitive anchoring and source scrutiny defenses reduced reality shift rates by up to 74.40% across all protocols

## Why This Works (Mechanism)

### Mechanism 1: Social Consensus Contagion
- Claim: Undifferentiated agent groups can systematically override correct individual knowledge through repeated specious evidence
- Mechanism: Agents receiving consistent but incorrect peer responses shift toward the false consensus, with reality shift rates rising from ~37% (single agent) to ~56% (seven-agent groups) before plateauing
- Core assumption: The model treats peer responses as credible evidence rather than unverified claims
- Evidence anchors:
  - [abstract] "false details reinforced through social influence and internalized misinformation"
  - [section 4.3.3] Generic groups show saturation at seven agents with reality shift rates increasing then plateauing
  - [corpus] "Multi-Agent Collaboration Mechanisms: A Survey of LLMs" addresses coordination but does not examine collective bias formation—corpus evidence for contagion is weak
- Break condition: When models are explicitly prompted to establish independent "cognitive anchors" before evaluating peer input, contagion is disrupted

### Mechanism 2: Role-Based Narrative Amplification
- Claim: Strategically differentiated agent roles produce higher reality shift rates than generic consensus through perceived authority and narrative coherence
- Mechanism: Five specialized roles (Error Initiator, Detail Provider, Consensus Reinforcer, Authority Endorser, Questioning Compromiser) construct multi-faceted false realities that agents find more credible than uniform peer pressure
- Core assumption: Models assign higher credibility to structured, role-differentiated discourse than to homogeneous repetition
- Evidence anchors:
  - [section 4.3.1] "role-based protocols (σRS, σRL) is higher than under generic protocols (σGS, σGL) for nearly all LLMs"
  - [section 4.3.3] Inverted-U curve—effect peaks at six agents then declines as "suspicion-induced vigilance" activates
  - [corpus] "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions" confirms persona-induced bias but does not address collective false memory
- Break condition: Source scrutiny prompts that explicitly deconstruct narrative roles and assess credibility reduce role-based reality shift by up to 74%

### Mechanism 3: Memory Consolidation of False Beliefs
- Claim: Short-term situational conformity can crystallize into persistent false beliefs through explicit memory consolidation and retrieval cycles
- Mechanism: When dialogue summaries are distilled and later retrieved as context, false conclusions become "memories" that agents rely on independently of the original social interaction
- Core assumption: The consolidation step preserves conclusions rather than verification processes
- Evidence anchors:
  - [abstract] "memory-related nature of the Mandela effect, involving the consolidation and persistent recall"
  - [section 4.2] Claude 3.5 Haiku retains 55.63% reality shift in long-term vs. 63.67% short-term; Llama3.1-8B shows 85.13% long-term shift
  - [corpus] No corpus papers address memory consolidation in multi-agent systems
- Break condition: Models with strong self-correction capabilities (GPT-5, Llama3.3-70B) show near-zero long-term reality shift even after short-term susceptibility

## Foundational Learning

- Concept: **Reality shift rate (σP)**
  - Why needed here: This metric measures the proportion of originally correct answers that become incorrect after social interaction—the core quantitative definition of the Mandela effect in this benchmark
  - Quick check question: If a model answers 80% correctly in isolation but 50% correctly after role-based interaction, what is its reality shift rate?

- Concept: **Short-term vs. Long-term memory protocols**
  - Why needed here: Distinguishes immediate conformity (same context) from internalized false beliefs (retrieved from consolidated memory), which have different risk profiles and mitigation requirements
  - Quick check question: Why might a model show high short-term susceptibility but low long-term reality shift?

- Concept: **Cognitive anchoring principle**
  - Why needed here: The most effective prompt-level defense requires agents to establish independent conclusions before evaluating social input, reversing the default evidence-assimilation process
  - Quick check question: What three principles define cognitive anchoring as a defense strategy?

## Architecture Onboarding

- Component map:
  Benchmark layer (4,838 questions) -> Protocol layer (5 protocols: B, GS, GL, RS, RL) -> Agent layer (subject + influencing agents) -> Defense layer (prompt-level + model-level)

- Critical path:
  1. Establish baseline reality via isolated questioning (Protocol B)
  2. Identify correctly answered questions as vulnerable population
  3. Apply social influence protocol to vulnerable questions
  4. Measure reality shift rate as (correct→incorrect) / (originally correct)
  5. Apply defense and re-measure to validate mitigation

- Design tradeoffs:
  - Generic vs. Role-based groups: Generic isolates pure consensus pressure; role-based captures narrative complexity but confounds multiple influence mechanisms
  - Short vs. Long timescales: Short-term measures situational conformity; long-term measures belief solidification but requires additional memory processing steps
  - Resilience-only vs. Balanced SFT: Resilience-only training causes dogmatic rejection of all social input (σC increases); balanced training maintains discernment

- Failure signatures:
  - Reality shift >60% in generic protocols indicates high social conformity vulnerability
  - Long-term reality shift approximating short-term indicates poor self-correction
  - σC >10% after resilience-only SFT indicates overfitting to rejection behavior

- First 3 experiments:
  1. **Baseline calibration**: Run Protocol B on target model to establish knowledge baseline and identify vulnerable question set
  2. **Protocol sweep**: Test all four influence protocols (GS, GL, RS, RL) to characterize susceptibility profile across group composition and timescale dimensions
  3. **Defense validation**: Apply cognitive anchoring prompt to highest-vulnerability protocol and measure reduction in reality shift rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the Mandela effect manifest in open-ended, unstructured multi-agent interactions compared to the controlled multiple-choice setting of MANBENCH?
- **Basis in paper:** [explicit] Section 6 (Limitations) notes that actual multi-agent interactions often involve "unstructured dialogue, dynamic role changes, and open-ended tasks," which present significantly greater diversity and are harder to control than the benchmark's structured format
- **Why unresolved:** The current benchmark design prioritized internal validity and precise measurement over ecological validity, leaving the behavior of collective false memories in free-form or long-horizon collaborative tasks unexplored
- **What evidence would resolve it:** A new benchmark or case study evaluating reality shift rates in agents engaging in unrestricted dialogue or complex cooperative planning without forced multiple-choice selection

### Open Question 2
- **Question:** What specific model capabilities or architectural differences cause the Mandela effect to follow an inverse scaling law in some model families (e.g., Qwen3) while decreasing in others (e.g., Claude)?
- **Basis in paper:** [inferred] Section 4.3.5 highlights a counter-intuitive finding where larger Qwen3 models became *more* susceptible, suggesting that "superior narrative understanding" in larger models might make them more vulnerable to internalizing false narratives
- **Why unresolved:** The paper quantifies the correlation between scale and susceptibility but does not isolate the underlying mechanism—whether it is due to training data, reasoning style, or a byproduct of emergent capabilities
- **What evidence would resolve it:** Mechanistic interpretability analysis comparing how models with inverse scaling versus standard scaling process and weigh specious evidence during the attention layers

### Open Question 3
- **Question:** Can the introduction of specialized "critic" agents for cross-verification provide a more adaptive defense against collective false memories than single-agent prompt strategies?
- **Basis in paper:** [explicit] Section 6 (Future work) explicitly states an intention to "develop more generalizable defense mechanisms, such as introducing 'critic' agents for cross-verification and reflection, to ensure alignment with factual ground truth"
- **Why unresolved:** While prompt-level defenses (cognitive anchoring) were effective, they rely on static instructions. An agent-based defense involves dynamic interaction which the paper suggests but does not test
- **What evidence would resolve it:** Experimental results on MANBENCH where a multi-agent group includes a designated "skeptic" role, measuring if this structural addition reduces the reality shift rate more effectively than the baseline or prompt-only defenses

## Limitations

- Controlled experimental conditions may not fully capture real-world multi-agent interaction complexity
- Benchmark questions represent curated scenarios rather than organic multi-agent interactions
- Role-based narratives, while sophisticated, remain artificial constructs
- Evaluation focuses on English-language models and Western knowledge domains, limiting cross-cultural generalizability

## Confidence

- **High Confidence**: Core finding that all tested LLMs are susceptible to collective false memory effects (15%-99% reality shift rates) is robustly supported across 13 models and multiple protocols
- **Medium Confidence**: Role-based amplification mechanism shows strong patterns but relies on specific narrative constructions that may not generalize to all domains
- **Medium Confidence**: Defense mechanisms show measurable effectiveness but may vary significantly with prompt engineering quality and domain specificity

## Next Checks

1. **Ecological Validity Test**: Deploy the role-based Mandela effect protocols in open-ended, task-oriented multi-agent systems (e.g., collaborative writing, problem-solving) to assess whether controlled findings transfer to naturalistic interactions with richer contextual constraints

2. **Cross-Cultural Generalization**: Replicate the benchmark using knowledge domains and historical events specific to non-Western cultures and languages to determine whether the Mandela effect susceptibility pattern holds across different cultural knowledge bases and collective memory structures

3. **Dynamic Defense Evaluation**: Test the cognitive anchoring and source scrutiny defenses under continuous interaction conditions where agents receive mixed streams of correct and incorrect social input over extended periods, measuring both immediate resistance and long-term belief stability