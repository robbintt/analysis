---
ver: rpa2
title: 'From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based
  Medical Calculations'
arxiv_id: '2509.16584'
source_url: https://arxiv.org/abs/2509.16584
tags:
- error
- formula
- answer
- clinical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of existing benchmarks for
  evaluating large language models (LLMs) in medical calculations, which often overlook
  reasoning failures by only checking final numeric answers within a broad tolerance.
  To solve this, the authors introduce a step-by-step evaluation pipeline that independently
  assesses formula selection, entity extraction, and arithmetic computation.
---

# From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations

## Quick Facts
- arXiv ID: 2509.16584
- Source URL: https://arxiv.org/abs/2509.16584
- Reference count: 40
- Primary result: Proposes MedRaC, a training-free modular pipeline that improves LLM accuracy on medical calculations from 16.35% to 53.19% by combining retrieval-augmented generation and Python code execution.

## Executive Summary
This paper addresses the limitations of existing benchmarks for evaluating large language models (LLMs) in medical calculations, which often overlook reasoning failures by only checking final numeric answers within a broad tolerance. To solve this, the authors introduce a step-by-step evaluation pipeline that independently assesses formula selection, entity extraction, and arithmetic computation. They also develop an automatic error analysis framework to attribute mistakes to specific failure modes, validated against human experts. Finally, they propose MedRaC, a modular agentic pipeline combining retrieval-augmented generation and Python-based code execution, which improves LLM accuracy on medical calculations from 16.35% to 53.19% without fine-tuning. The study highlights the importance of granular, explainable evaluation for safer and more reliable LLM deployment in clinical settings.

## Method Summary
The study evaluates and improves LLM performance on medical calculations from MedCalc-Bench using a step-wise evaluation pipeline and a modular agentic pipeline (MedRaC). The method uses 940 cleaned cases from MedCalc-Bench, 55 MDCalc calculators with curated clinical vignettes, and metrics including step-wise conditional correctness across four stages (formula selection, entity extraction, calculation, final answer). The MedRaC pipeline combines Formula RAG using OpenAI embeddings to retrieve relevant formulas and Python code execution where the LLM generates executable code. The approach is evaluated against baselines including Direct, CoT, One-shot, Self-Refine, and MedPrompt using LLM-as-Judge with DeepSeek-chat for evaluation.

## Key Results
- MedRaC pipeline improves LLM accuracy on medical calculations from 16.35% to 53.19% without fine-tuning
- Granular step-wise evaluation reveals accuracy drops from 62.7% to 43.6% under detailed assessment
- RAG component reduces Formula First Error Attribution Rate from 71.96% to 24.65%
- Python code execution eliminates 82.6% of arithmetic errors for Llama3.1-8B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing calculation evaluation into independent steps reveals reasoning failures that final-answer metrics mask.
- **Mechanism:** A pipeline validates Formula selection, Entity extraction, and Arithmetic computation sequentially. If step $S_{i-1}$ is incorrect, step $S_i$ is marked invalid, preventing error propagation from hiding root causes.
- **Core assumption:** An LLM-judge can approximate human expert evaluation of semantic alignment in clinical reasoning (validated in Table 2).
- **Evidence anchors:**
  - [Abstract] Accuracy drops from 62.7% to 43.6% under granular evaluation.
  - [Section 3.1] Defines the dependency $V(S_i) \iff V(S_{i-1})$ and Conditional Correctness metrics.
  - [Corpus] MedMCP-Calc and MedHELM (neighbor papers) corroborate the trend toward multi-stage, holistic evaluation over static scoring.
- **Break condition:** If the LLM-judge exhibits systematic bias not caught by human alignment studies, error attribution becomes unreliable.

### Mechanism 2
- **Claim:** Retrieval-augmented generation (RAG) stabilizes formula selection by grounding the model in verified external knowledge.
- **Mechanism:** A vector store of MDCalc formulas is queried; the retrieved formula is injected into the prompt. This bypasses the need for the model to memorize complex equations, reducing hallucination.
- **Core assumption:** Medical formulas are distinct enough semantically for embedding-based retrieval to be precise (high Top-k accuracy).
- **Evidence anchors:**
  - [Section 5] Ablation shows accuracy drops from 64.68% to 25.64% without RAG; Formula First Error Attribution Rate rises to 71.96%.
  - [Section 3.3] Describes the Formula RAG module targeting hallucination reduction.
  - [Corpus] Weak/missing explicit support for "Formula RAG" specifically in the neighbor list, though general RAG utility is implied by the domain context.
- **Break condition:** If a query is ambiguous or the knowledge base is incomplete, retrieval may fetch an incorrect or irrelevant formula, degrading performance.

### Mechanism 3
- **Claim:** Offloading computation to a Python interpreter eliminates arithmetic and rounding errors inherent in LLM token generation.
- **Mechanism:** The model generates Python code representing the equation and extracted values; a sandboxed executor runs the code to produce the final float.
- **Core assumption:** The LLM can reliably map natural language variables to syntactically correct Python code.
- **Evidence anchors:**
  - [Section 4.3] Figure 4 shows an 82.6% reduction in Arithmetic Errors for Llama3.1-8B.
  - [Section 5] Ablation shows accuracy drops significantly (e.g., Calculation FE rises to 31.88%) without the Code component.
  - [Corpus] Weak direct validation of Python code execution for medical calculation in the provided neighbor abstracts; MedMCP-Calc mentions MCP integration which is analogous but distinct.
- **Break condition:** If variable extraction fails (wrong value/unit), the code executes perfectly on wrong inputs (Garbage In, Garbage Out).

## Foundational Learning

- **Concept:** **Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The MedRaC architecture relies on RAG to solve the "knowledge bottleneck" of memorizing complex medical formulas.
  - **Quick check question:** How does the system handle a query if the relevant formula is missing from the vector store?

- **Concept:** **Program-Aided Language Models (PAL) / Tool Use**
  - **Why needed here:** Understanding why the model generates Python code instead of text output is central to the paper's improvement strategy.
  - **Quick check question:** Does the Python executor guarantee unit consistency, or does the LLM prompt handle unit conversion?

- **Concept:** **LLM-as-Judge**
  - **Why needed here:** The evaluation pipeline depends on an automated judge to scale assessment, replacing costly human annotation.
  - **Quick check question:** What specific bias might an LLM-judge have when evaluating "creative" vs. "deterministic" clinical reasoning?

## Architecture Onboarding

- **Component map:** Input Processor -> Retriever (RAG) -> Planner/Code Generator -> Executor
- **Critical path:** The **Retrieval** component is the most brittle link. The ablation study (Table 5) shows a ~39% absolute accuracy drop without RAG, significantly larger than the drop from removing Code execution. Failure here cascades to all downstream steps.
- **Design tradeoffs:** The paper contrasts MedRaC against "One-shot" prompting. One-shot improves performance by showing examples but cannot fix arithmetic hallucinations. MedRaC fixes arithmetic via code but introduces dependency on external libraries and retrieval quality.
- **Failure signatures:**
  - **Clinical Misinterpretation:** High residual error rate (Figure 4) indicates the architecture struggles with nuance (e.g., "trace ascites" vs. "no ascites") even with correct formulas.
  - **Missing Variables:** Models may fail to extract implicit values (e.g., race, gender) not explicitly stated in the vignette.
- **First 3 experiments:**
  1. **Retrieval Ablation:** Run the pipeline with an empty formula bank to confirm the performance drop mimics the "w/o RAG" baseline (approx. 25% accuracy).
  2. **Error Attribution Spot-Check:** Select 10 random failures; manually verify if the "First Error Attribution" (Formula vs. Extraction) matches human intuition.
  3. **Tolerance Stress Test:** Vary the numerical tolerance in the evaluator (from the paper's strict ±0.005 to the benchmark's ±5%) to visualize the "illusion of correctness" the paper argues against.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MedRaC performance generalize to noisy Electronic Health Record (EHR) data and multilingual clinical settings?
  - **Basis in paper:** [explicit] The authors state, "The generalizability of our results to multilingual settings, noisy EHR data, or patient-facing dialogue remains to be studied."
  - **Why unresolved:** The study relied exclusively on English vignettes from the cleaned MedCalc-Bench dataset, which are structured and do not reflect the noise of real clinical notes.
  - **What evidence would resolve it:** Evaluation results of the MedRaC pipeline applied to unstructured, multilingual EHR extracts or patient dialogues.

- **Open Question 2:** Can the framework be extended to open-ended clinical reasoning tasks that lack well-defined formulas?
  - **Basis in paper:** [explicit] The authors note, "Future work should explore more open-ended clinical reasoning..." as the current benchmark focuses on structured, single-turn tasks.
  - **Why unresolved:** The current evaluation pipeline and MedRaC architecture rely on the existence of specific canonical formulas and structured variables.
  - **What evidence would resolve it:** A modification of the step-wise evaluation pipeline that successfully grades unstructured diagnostic reasoning or management planning without a single numeric ground truth.

- **Open Question 3:** How can the bottleneck of clinical misinterpretation and variable extraction errors be reduced?
  - **Basis in paper:** [inferred] While MedRaC significantly reduced arithmetic and formula errors, the authors observe that "upstream information extraction remains a bottleneck" for errors requiring nuanced clinical understanding.
  - **Why unresolved:** The RAG and code execution components address calculation mechanics but do not resolve errors where the necessary medical context is implicit in the note.
  - **What evidence would resolve it:** An enhanced pipeline component (e.g., clinical context pre-training) that demonstrates a statistically significant reduction in the "Clinical Misinterpretation" and "Incorrect Variable Extraction" error rates.

## Limitations

- The generalizability of LLM-as-judge reliability beyond the tested dataset is uncertain, as the paper does not address how well the evaluation pipeline handles highly ambiguous clinical scenarios or rare diseases where formulas may not exist.
- The scalability of formula retrieval when extending from 55 to 785 calculators remains unverified in the paper, with no performance testing on the full knowledge base.
- The study's dependence on high-quality, consistent variable extraction from clinical text is a bottleneck, as RAG and code execution components do not resolve errors where necessary medical context is implicit in the note.

## Confidence

- **High confidence**: The step-wise evaluation methodology (Mechanism 1) and its ability to expose masked reasoning failures are well-supported by ablation studies and clear logical structure.
- **Medium confidence**: The RAG component's effectiveness is convincingly demonstrated for the 55-formula set, but its performance with the full 785-formula bank remains unverified in the paper.
- **Low confidence**: Claims about the LLM-as-judge's alignment with human experts are based on Table 2 but lack detailed cross-validation methodology or bias analysis.

## Next Checks

1. **Cross-judge reliability test**: Evaluate the same cases using two independent LLM judges (e.g., DeepSeek-chat vs. GPT-4) and compute inter-rater agreement to quantify judge consistency.
2. **Full-bank retrieval accuracy**: Test the formula retrieval component on a held-out subset of the 785-formula bank to measure precision@k and confirm no degradation from the 55-formula set.
3. **Error propagation analysis**: For a stratified sample of 50 failures, manually trace the error attribution chain (Formula → Extraction → Calculation) to verify the LLM judge's assignments align with clinical expert reasoning.