---
ver: rpa2
title: Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity Recognition
  in Low-Resource Languages
arxiv_id: '2501.18750'
source_url: https://arxiv.org/abs/2501.18750
tags:
- languages
- source
- projection
- language
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits projection-based data transfer methods for
  cross-lingual named entity recognition in low-resource languages, introducing two
  key improvements: (1) a target-to-target alignment direction that aligns original
  and back-translated sentences in the same language, and (2) a novel bipartite matching
  approach between source entities and extracted target candidates. The method was
  evaluated across 57 languages using the XTREME and MasakhaNER2 datasets in translate-test
  settings, demonstrating that the proposed candidate matching approach outperforms
  existing projection-based methods in most cases and achieves results comparable
  to or better than model transfer, particularly when the same NER model is used for
  candidate extraction.'
---

# Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity Recognition in Low-Resource Languages

## Quick Facts
- arXiv ID: 2501.18750
- Source URL: https://arxiv.org/abs/2501.18750
- Reference count: 20
- Primary result: Projection-based data transfer with novel bipartite matching outperforms existing methods in most cases and achieves results comparable to or better than model transfer, particularly for NER-based candidate extraction.

## Executive Summary
This paper revisits annotation projection for cross-lingual named entity recognition (NER) in low-resource languages, introducing two key improvements: target-to-target alignment direction and a novel bipartite matching approach. The method aligns original and back-translated sentences in the same language, then formulates entity projection as a constrained bipartite matching problem between source entities and extracted target candidates. Evaluated across 57 languages using XTREME and MasakhaNER2 datasets, the approach demonstrates that the proposed candidate matching formulation outperforms existing projection methods in most cases and achieves results comparable to or better than model transfer, particularly when using the same NER model for candidate extraction.

## Method Summary
The method implements a projection-based data transfer pipeline where target sentences are translated to the source language, labeled using a source NER model, and then labels are projected back to the original target sentences. Key innovations include: (1) target-to-target alignment that aligns original and back-translated target sentences, and (2) a bipartite matching formulation that matches source entities to candidate spans in the target sentence using alignment-based scores. The matching problem is formulated as an integer linear program with overlap constraints and solved using a greedy approximation. Candidate extraction can use either n-grams or NER model predictions, and word alignments are computed using SimAlign or AWESoME with mBERT.

## Key Results
- NER-based candidate extraction with src2tgt alignment achieves 52.0 F1 on Yoruba vs. 32.3 for model transfer
- The proposed bipartite matching approach outperforms existing projection methods in most cases
- Target-to-target alignment generally underperforms source-to-target alignment except for Japanese
- The method struggles with morphologically complex languages like Xhosa and Zulu

## Why This Works (Mechanism)

### Mechanism 1
Reformulating annotation projection as a constrained bipartite matching problem between source entities and target candidates improves transfer quality over heuristic alignment methods. The approach extracts candidate entity spans, computes alignment-based matching scores, then solves an integer linear programming problem to select optimal one-to-one mappings while preventing overlapping projections. Core assumption: Word-to-word alignments contain enough signal to rank candidate-entity pairings correctly on average.

### Mechanism 2
Using the same NER model for both candidate extraction in the target language and source-side labeling enables label correction that can outperform direct model transfer. The source NER model labels the translated sentence and independently extracts candidate spans in the original target sentence, then projection reassigns labels from source entities to matched candidates, potentially fixing misclassifications while preserving span locations. Core assumption: The model's span predictions are more reliable than its class predictions for the target language.

### Mechanism 3
Target-to-target alignment was hypothesized to improve alignment quality but underperformed due to error propagation from back-translation. Instead of aligning across languages, the method aligns within the target language by back-translating the labeled source sentence and aligning it with the original. The intuition is that same-language alignment should be easier, but back-translation errors—especially entity distortion or reordering—invalidate this assumption.

## Foundational Learning

- **Annotation projection in cross-lingual NLP**: The entire method builds on the classic projection pipeline: translate, label in source, project labels back. Understanding this three-step process is prerequisite.
  - Quick check: Can you explain the difference between translate-train and translate-test in projection pipelines?

- **Word-to-word alignment (statistical and neural)**: The scoring function depends on alignment quality. SimAlign and AWESoME are contextualized neural aligners; knowing their limitations helps interpret failure modes.
  - Quick check: Why might neural aligners outperform statistical aligners like FastAlign for low-resource language pairs?

- **Bipartite matching and ILP formulations**: The core contribution formalizes projection as a constrained optimization problem. Basic familiarity with maximum-weight matching and constraint formulation is assumed.
  - Quick check: What constraint prevents projecting two different source entities onto overlapping target spans?

## Architecture Onboarding

- **Component map**: Translation module (NLLB-200) → translates target sentences to source language → Source NER model (XLM-R-Large) → labels translated sentences → Candidate extractor (N-gram or NER-based) → generates target spans → Word aligner (SimAlign or AWESoME) → computes alignments → Matching solver (greedy ILP approximation) → selects optimal entity-candidate pairs → Label assignment → writes projected labels to matched candidates

- **Critical path**: Translation quality → Source NER accuracy → Alignment quality → Candidate extraction quality → Matching correctness. Errors compound; the matching step cannot recover from missing candidates.

- **Design tradeoffs**: N-gram vs. NER candidates (flexibility vs. plausibility); src2tgt vs. tgt2tgt alignment (empirical superiority despite cross-lingual difficulty); Greedy vs. optimal solver (speed vs. optimality).

- **Failure signatures**: Low F1 on morphologically complex languages (Xhosa, Zulu) → alignment and candidate extraction both degrade; tgt2tgt dramatically worse than src2tgt → back-translation corrupts entities; Model transfer outperforms projection on well-represented languages → projection overhead not worth it.

- **First 3 experiments**: 1) Replicate Table 1 on Yoruba, Estonian, Bambara using src2tgt with NER candidates and SimAlign; 2) Ablate candidate extraction by switching from NER-based to N-gram candidates; measure delta on morphologically simple vs. complex language; 3) Test alignment sensitivity by substituting AWESoME for SimAlign.

## Open Questions the Paper Calls Out

### Open Question 1
What alternative matching cost functions beyond alignment-based scores could be integrated into the bipartite matching formulation to improve projection quality? The paper suggests future research could explore alternative scoring mechanisms that can be fused with alignment scores.

### Open Question 2
How can back-translation errors be mitigated to make target-to-target alignment a viable improvement over source-to-target alignment? The paper identifies this as a potential area for future research given that tgt2tgt generally underperforms due to back-translation errors.

### Open Question 3
Can the proposed ILP formulation with both non-overlapping candidate constraints and source entity projection limits be solved optimally in polynomial time? The paper states this remains an open question and uses a greedy approximation.

### Open Question 4
How can the proposed ILP formulation be adapted to better handle morphologically complex languages where it currently underperforms? The paper notes it struggles with languages exhibiting higher morphological complexity and may need additional adaptations.

## Limitations
- The target-to-target alignment direction consistently underperformed source-to-target alignment, contradicting the initial hypothesis about alignment quality
- The method struggles with morphologically complex languages like Xhosa and Zulu, where it underperforms compared to model transfer
- Evaluation focuses exclusively on translate-test scenarios without testing translate-train settings where projection might perform differently

## Confidence
- **High Confidence (90%+)**: NER-based candidate extraction with src2tgt alignment consistently outperforms tgt2tgt alignment across all tested languages
- **Medium Confidence (70-80%)**: The proposed bipartite matching approach outperforms existing projection-based methods, though comparison baseline methods are not fully specified
- **Low Confidence (40-50%)**: The assertion that projection can achieve results comparable to or better than model transfer, as evidence is mixed and conditions for success are not clearly characterized

## Next Checks
1. **Ablation Study on Candidate Extraction Parameters**: Systematically vary n-gram range (1-3, 1-5, 2-4) and filtering thresholds for candidate spans to determine which parameters contribute most to performance differences across language types.

2. **Cross-Domain Robustness Test**: Evaluate the projection pipeline on a held-out domain (e.g., news vs. social media) to assess whether performance gains are domain-specific or generalize across text types.

3. **Alignment Quality Analysis**: Conduct a detailed error analysis comparing src2tgt vs tgt2tgt alignments on a subset of sentences to identify specific error patterns in back-translation that invalidate the tgt2tgt hypothesis.