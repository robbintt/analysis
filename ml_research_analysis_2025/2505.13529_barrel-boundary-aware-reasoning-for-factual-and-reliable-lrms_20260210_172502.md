---
ver: rpa2
title: 'BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs'
arxiv_id: '2505.13529'
source_url: https://arxiv.org/abs/2505.13529
tags:
- answer
- reasoning
- question
- which
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two overthinking patterns in Large Reasoning
  Models (LRMs) that lead to unreliable factual responses: last-minute guessing and
  second-thought spiraling. The authors propose BARREL, a training framework that
  teaches LRMs to recognize knowledge boundaries and express uncertainty when appropriate.'
---

# BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs

## Quick Facts
- arXiv ID: 2505.13529
- Source URL: https://arxiv.org/abs/2505.13529
- Reference count: 40
- Primary result: Reliability of DeepSeek-R1-Distill-Llama-8B improved from 39.33% to 61.48% while maintaining 40.7% accuracy

## Executive Summary
This paper addresses unreliable factual responses in Large Reasoning Models (LRMs) by identifying two overthinking patterns: last-minute guessing and second-thought spiraling. The authors propose BARREL, a training framework that teaches LRMs to recognize knowledge boundaries and express uncertainty when appropriate. BARREL uses a two-stage approach combining Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), employing a reward structure that explicitly rewards uncertainty-aware refusal. Experiments show BARREL significantly improves factual reliability without sacrificing performance on reasoning tasks, demonstrating that teaching models to admit ignorance and providing appropriate rewards for refusal enhances overall reliability.

## Method Summary
BARREL employs a two-stage training approach to teach LRMs boundary-aware reasoning. First, it constructs boundary-aware reasoning traces via Supervised Fine-Tuning using knowledge labeling to distinguish known from unknown questions. For known questions, traces follow a pattern of recalling information, confirming with evidence, and exploring alternatives. For unknown questions, traces acknowledge uncertainty and produce refusal responses. Second, BARREL enhances performance with Group Relative Policy Optimization using a reward structure that assigns intermediate rewards for calibrated refusal (rs), higher rewards for correct answers (rc), and penalties for incorrect answers (rw). This approach teaches models to recognize when they should express uncertainty rather than hallucinate, improving factual reliability while maintaining reasoning accuracy.

## Key Results
- Reliability improved from 39.33% to 61.48% while maintaining 40.7% accuracy on DeepSeek-R1-Distill-Llama-8B
- Outperformed baseline distillation methods in the 3K-test set evaluation
- Ablation study showed rs reward magnitude critically affects refusal behavior - too high causes over-refusal, too low causes under-refusal
- Maintained performance across multiple datasets (TriviaQA, SciQ, NQ-Open)

## Why This Works (Mechanism)

### Mechanism 1
Rewarding uncertainty-aware refusal at an intermediate level (between correct and incorrect answers) enables models to learn when to say "I don't know" rather than hallucinating. The GRPO reward function assigns rc (highest) for correct answers, rw (penalty) for incorrect answers, and rs (intermediate) for calibrated refusal. This intermediate reward creates a viable policy path for abstention when evidence is insufficient. The model possesses latent ability to recognize knowledge boundaries (via internal states) but lacks explicit training signals to express uncertainty.

### Mechanism 2
Structured reasoning traces during SFT establish correct epistemic behaviors by demonstrating explicit evidence-seeking and self-critique before conclusion. For known questions, traces include: RECALL → gold answer with evidence → alternative candidates → CONFIRM. For unknown questions: RECALL → explore candidates → acknowledge uncertainty → refusal. This teaches the model to terminate reasoning appropriately rather than overthink. The sampling strategy correctly classifies questions as known/unknown based on whether any sampled answer matches ground truth.

### Mechanism 3
The combination of SFT (behavioral initialization) and GRPO (policy refinement) outperforms either stage alone due to complementary learning modes. SFT provides cold-start ability to generate refusal responses and follow boundary-aware patterns. GRPO then optimizes the trade-off between accuracy and truthfulness using rule-based rewards, allowing self-adjustment without external knowledge injection. SFT alone cannot achieve both high accuracy and high truthfulness simultaneously—there exists an inherent trade-off ceiling.

## Foundational Learning

- Concept: Knowledge boundary probing via sampling
  - Why needed here: Distinguishes known from unknown questions to construct appropriate training data
  - Quick check question: Can you explain why sampling with K prompts × L repetitions (rather than single-pass) improves boundary detection reliability?

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: RL algorithm that normalizes rewards within groups of trajectories, enabling relative preference learning without a separate reward model
  - Quick check question: How does GRPO differ from PPO in terms of advantage estimation and KL regularization?

- Concept: Factuality metrics triad (Accuracy, Truthfulness, Reliability)
  - Why needed here: Defines trade-offs between correctness, error avoidance, and calibrated abstention
  - Quick check question: Why does Reliability = ans. × Truth. + (1 - ans.) × Acc. capture the goal of "helpful when possible, cautious when uncertain"?

## Architecture Onboarding

- Component map: Knowledge Labeler → Trace Constructor → SFT Trainer → GRPO Optimizer
- Critical path: Knowledge labeling → Trace construction (known vs. unknown templates) → SFT initialization → GRPO refinement → Evaluation
- Design tradeoffs:
  - Known:unknown data ratio in SFT (3:1 used) affects accuracy-truthfulness balance
  - Reward magnitude ordering rc > rw > rs is critical; rs placement determines refusal calibration
  - Training data filtered to only "known" questions to avoid hallucination amplification
- Failure signatures:
  - Excessive refusal: rs too high relative to rw
  - No refusal behavior: rs removed or equal to rw
  - Low accuracy despite training: incorrect known/unknown labeling propagates bad traces
- First 3 experiments:
  1. Ablation on rs reward: Set rs=0 (equal to rw) and verify refusal rate collapses to near zero
  2. SFT-only baseline: Train with varying known:unknown ratios and plot accuracy vs. truthfulness trade-off curve
  3. OOD generalization test: Evaluate refusal rate on SimpleQA (near-zero model accuracy) and SelfAware (unanswerable questions)

## Open Questions the Paper Calls Out

### Open Question 1
Can boundary-aware reasoning frameworks like BARREL be effectively extended to open-ended generation tasks (e.g., article writing, opinion provision) where verifiable ground truth is unavailable? The authors state in Section J (Limitations): "How to teach LRMs to learn knowledge boundary and behave more deliberatively on open-end questions, like writing articles or providing opinions, remains a valuable topic for future work." BARREL's training and evaluation rely on verifiable answers with string-match evaluation; open-ended tasks lack discrete correctness signals and require different reward formulations.

### Open Question 2
Does BARREL's effectiveness generalize to larger LRMs (e.g., 70B+ parameters) and frontier models with different pre-training distributions? Section J acknowledges: "restricted by our limited computing resource, we mainly utilize DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Qwen-7B to perform our study, which are relatively small LRMs." Computational constraints prevented testing on larger models where overthinking patterns and knowledge boundary behaviors may manifest differently.

### Open Question 3
What is the optimal calibration of reward values (r_c, r_s, r_w) for different task domains and model scales? Section 4.3 shows that reward magnitudes critically affect refusal behavior, with excessive rewards causing over-refusal. The paper uses r_c > r_w > r_s but does not systematically explore optimal ratios. The paper establishes that medium refusal reward is necessary but provides limited guidance on precise calibration across different contexts.

## Limitations
- Restricted to relatively small LRMs (8B parameters) due to computational constraints
- Relies on synthetic data generation via GPT-4 for trace construction, raising reproducibility concerns
- Knowledge labeling sampling strategy effectiveness across different knowledge domains remains unverified

## Confidence

**High confidence**: The two overthinking patterns (last-minute guessing and second-thought spiraling) are well-characterized phenomena observed in LRMs. The effectiveness of the intermediate rs reward for calibrated refusal is supported by the ablation showing refusal rate collapses to near-zero when rs is removed or equalized with rw.

**Medium confidence**: The SFT+GRPO two-stage approach providing superior accuracy-truthfulness trade-offs compared to either stage alone. While the paper demonstrates this empirically, the claim that SFT alone cannot achieve both high accuracy and high truthfulness simultaneously (due to an inherent ceiling) is inferred from the results rather than directly proven.

**Low confidence**: The generalizability of the knowledge boundary probing via sampling strategy to other domains beyond the tested QA datasets. The assumption that sampling K×L responses with diverse few-shot prompts reliably distinguishes known from unknown questions across different knowledge domains remains unverified.

## Next Checks

1. **Reward calibration validation**: Systematically vary rs between rw and rc (e.g., rs = 0.1, 0.3, 0.5, 0.7) and measure refusal rate and reliability to empirically confirm the "intermediate value" requirement for calibrated refusal.

2. **Knowledge labeling robustness test**: Run the sampling strategy with varying K (number of few-shot prompts) and L (samples per prompt) on a held-out validation set to quantify false positive/negative rates in known/unknown classification and their impact on SFT trace quality.

3. **OOD generalization evaluation**: Test the trained model on datasets with known ground truth characteristics (e.g., SimpleQA where model accuracy should be near zero, or SelfAware where all questions are designed to be unanswerable) to verify whether refusal behavior appropriately scales with actual knowledge boundaries.