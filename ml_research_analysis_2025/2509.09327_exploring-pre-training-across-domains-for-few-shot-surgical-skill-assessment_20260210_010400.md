---
ver: rpa2
title: Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment
arxiv_id: '2509.09327'
source_url: https://arxiv.org/abs/2509.09327
tags:
- pre-training
- surgical
- data
- performance
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of self-supervised pre-training
  on few-shot surgical skill assessment (SSA), a critical yet data-scarce task in
  surgical computer vision. We annotate a robotic surgery dataset with Objective Structured
  Assessment of Technical Skill (OSATS) scores and evaluate various pre-training sources
  across three few-shot settings (1-, 2-, and 5-shot).
---

# Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment

## Quick Facts
- arXiv ID: 2509.09327
- Source URL: https://arxiv.org/abs/2509.09327
- Reference count: 32
- Primary result: Domain-relevant pre-training outperforms large-scale generic data in few-shot surgical skill assessment

## Executive Summary
This study investigates self-supervised pre-training for few-shot surgical skill assessment, a critical task in surgical computer vision where labeled data is scarce. The researchers annotated a robotic surgery dataset with OSATS scores and evaluated various pre-training sources across 1-, 2-, and 5-shot settings. Their analysis demonstrates that small, domain-relevant datasets (e.g., robotic suturing) can outperform large-scale but less aligned ones (e.g., generic video datasets) in few-shot learning scenarios. The findings highlight the importance of domain alignment over dataset scale for effective few-shot surgical skill assessment.

## Method Summary
The study employs VideoMAEv2 for self-supervised pre-training on various surgical and non-surgical video datasets, then fine-tunes on a few-shot labeled dataset annotated with OSATS scores. Three pre-training datasets were compared: SAR-RARP50U (procedure-specific), RALPN (domain-relevant robotic suturing), and Something-Something-v2 (large-scale generic). The model was evaluated using 1-, 2-, and 5-shot episodic learning protocols with both linear and temporal convolutional network (TCN) classification heads. Domain similarity was quantified using Earth Mover's Distance to understand transferability between pre-training and target domains.

## Key Results
- Small, domain-relevant datasets (e.g., RALPN) outperformed large-scale generic ones (e.g., Something-Something-v2) in few-shot settings
- Domain-relevant pre-training achieved 60.16%, 66.03%, and 73.65% accuracy in 1-, 2-, and 5-shot settings respectively
- Combining procedure-specific data with external datasets improved performance only when domain gap was small
- Temporal reconstruction (VideoMAEv2) was superior to spatial-only approaches for skill assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In low-data regimes, domain alignment between pre-training and target datasets is more predictive of few-shot performance than dataset scale.
- **Mechanism:** Pre-training on domain-relevant data (robotic suturing) reduces the distribution shift the few-shot learner must bridge. The model learns specific visual features (tissue interaction, needle handling) that transfer directly, whereas large-scale generic data learns broad motion features that require significant adaptation.
- **Core assumption:** The downstream task relies on visual features specific to the surgical domain rather than general edge detection or motion optics.
- **Evidence anchors:** Small, domain-relevant datasets (e.g., RALPN) can outperform large-scale but less aligned ones; RALPN (3.5 hours) outperforms Something-Something-v2 (232 hours) in 1-shot and 2-shot temporal evaluation.

### Mechanism 2
- **Claim:** Combining procedure-specific data with external datasets yields performance gains only if the domain gap is small; otherwise, it introduces noise that degrades transfer learning.
- **Mechanism:** Mixing datasets creates a multi-modal feature distribution. If the modes are close (e.g., RALPN + SAR-RARP50U), the combined density reinforces robust features. If distant (e.g., Something-Something-v2 + SAR-RARP50U), the encoder wastes capacity reconciling disparate visual patterns, diluting the signal available for the target task.
- **Core assumption:** The VideoMAEv2 encoder has limited capacity, and optimization struggles to reconcile conflicting visual distributions during self-supervised reconstruction.
- **Evidence anchors:** Combining procedure-specific data improves performance, but less similar combinations can degrade it; combining SAR-RARP50U with Something-Something-v2 results in an average accuracy gain of -0.65% compared to SAR-RARP50U alone.

### Mechanism 3
- **Claim:** Temporal reconstruction (VideoMAEv2) provides superior initialization for skill assessment compared to purely spatial reconstruction (MAE) due to the dynamic nature of surgical skill.
- **Mechanism:** Skill assessment metrics (e.g., "time and motion," "flow of operation") are inherently temporal. Spatial-only pre-training (EndoViT) learns static texture features but fails to encode the motion dynamics critical for distinguishing "proficient" vs. "expert" manipulation.
- **Core assumption:** OSATS scores assessing "flow" and "motion" are encoded in the temporal derivatives of the video frames, not just spatial appearance.
- **Evidence anchors:** EndoViT focuses solely on spatial reconstruction, whereas VideoMAEv2 incorporates temporal reconstruction; EndoViT often underperforms even random initialization in the 1-shot setting.

## Foundational Learning

- **Concept: Self-Supervised Video Masked Autoencoding (VideoMAE)**
  - **Why needed here:** This is the core pre-training technique. Understanding that the model learns by reconstructing masked spatio-temporal patches helps explain why it captures motion dynamics better than image-based models.
  - **Quick check question:** Does the model learn from labeled skill data during pre-training, or from raw video reconstruction? (Answer: Raw video reconstruction).

- **Concept: Few-Shot Learning (FSL) & Episodic Training**
  - **Why needed here:** The evaluation protocol uses "N-way K-shot" episodes rather than standard train/test splits. You must understand that the model is tested on its ability to generalize to new classes (or in this case, skill levels) with only 1-5 examples.
  - **Quick check question:** In a 1-shot setting, how many labeled examples of the "Expert" class does the model see during adaptation? (Answer: One).

- **Concept: Earth Mover's Distance (EMD)**
  - **Why needed here:** The paper uses EMD to quantify "domain gap." Understanding this as a metric of distribution similarity is key to interpreting Fig. 1 and the rationale for dataset selection.
  - **Quick check question:** If Dataset A has a lower EMD to the target than Dataset B, what does that imply about their feature distributions? (Answer: A is more similar/closer in feature space).

## Architecture Onboarding

- **Component map:** SAR-RARP50U video -> VideoMAEv2 encoder (ViT-S) -> Frozen encoder output -> Features -> Linear/TCN classifier -> Skill assessment prediction

- **Critical path:**
  1. Select pre-training dataset (e.g., RALPN)
  2. Train VideoMAEv2 (ViT-S) for 300 epochs to reconstruct masked video snippets
  3. Freeze encoder weights
  4. Extract features from SAR-RARP50L (target)
  5. Train lightweight classifier (Linear or TCN) on top of frozen features using 1-, 2-, or 5-shot sampling

- **Design tradeoffs:**
  - RALPN (Small/Specific) vs. SSv2 (Large/Generic): The paper proves that for SSA, the specific data wins in low-shot scenarios. Do not default to "more data is better" without checking domain alignment (EMD).
  - EndoViT vs. VideoMAE: Do not use image-based foundation models (EndoViT) for video-based skill assessment without temporal fine-tuning, as they may lack motion priors.

- **Failure signatures:**
  - Random Baseline Outperforming Pre-training: If pre-training performance drops below random initialization (as seen with EndoViT in some shots), it indicates "negative transfer"â€”the pre-trained features are actively confusing the classifier.
  - High Accuracy, Low F1: Indicates the model is biasing toward the majority class (likely predicting "Proficient" for everything), a common risk in few-shot unbalanced datasets.

- **First 3 experiments:**
  1. Baseline Reproduction: Pre-train VideoMAEv2 on the provided SAR-RARP50U data and evaluate the linear probe in a 5-shot setting to verify the pipeline.
  2. Domain Gap Validation: Calculate the EMD between your local surgical dataset and the pre-training source to predict transferability before committing compute to training.
  3. Ablation on Temporal Modeling: Compare the Linear probe vs. TCN head on the RALPN-pre-trained features to quantify the value added by temporal aggregation for the specific "flow of operation" OSATS criterion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the finding that domain-relevant pre-training outperforms large-scale generic data hold across multiple surgical centers and diverse procedures?
- Basis in paper: The authors state, "Extending the analysis to additional datasets from different centers would be valuable to validate the generalizability of our conclusions," noting the current limitation to a single dataset (SAR-RARP50).
- Why unresolved: The study was restricted to one dataset due to computational costs, leaving the generalizability of the specific domain alignment strategy unconfirmed in broader clinical contexts.
- What evidence would resolve it: Replicating the pre-training experiments on external surgical datasets from different institutions to verify if small, aligned datasets consistently outperform large, diverse ones.

### Open Question 2
- Question: Can the proposed few-shot framework effectively support fine-grained, multi-class surgical skill assessment rather than just binary classification?
- Basis in paper: The paper notes that "Expanding the number of annotated samples across the score range could allow for more granular skill categories in future work."
- Why unresolved: The current work simplified the task to binary classification (proficient vs. expert) due to a narrow range of Global Rating Scores (GRS) in the dataset and sample size constraints.
- What evidence would resolve it: Applying the framework to a dataset with a wider distribution of skill scores to train and evaluate the model on multi-class labels (e.g., novice, intermediate, expert).

### Open Question 3
- Question: To what extent is the performance of few-shot SSA dependent on spatio-temporal reconstruction versus spatial-only reconstruction during pre-training?
- Basis in paper: The paper observes that EndoViT (spatial-only MAE) underperformed VideoMAEv2 (spatio-temporal), but acknowledges this is not a direct comparison due to differing training data, leaving the specific contribution of temporal modeling uncertain.
- Why unresolved: It is unclear if the lower performance of the foundation model was due to its "excessive diversity" or the lack of temporal reconstruction capabilities required for dynamic skill assessment.
- What evidence would resolve it: An ablation study comparing spatial-only versus spatio-temporal self-supervised learning objectives using identical pre-training data to isolate the effect of temporal modeling on SSA.

## Limitations

- Evaluation constrained to a single surgical procedure (robotic radical prostatectomy) and specific OSATS criteria, limiting generalizability
- Incomplete computational cost analysis - doesn't quantify trade-off between performance gains and training time/resources
- Doesn't explore alternative few-shot learning protocols beyond episodic sampling, such as meta-learning approaches

## Confidence

- **High Confidence:** The core finding that domain-relevant pre-training outperforms generic large-scale datasets in low-shot settings is well-supported by the experimental results (60.16% accuracy for RALPN vs. 53.63% for Something-Something-v2 in 1-shot setting).
- **Medium Confidence:** The assertion that combining procedure-specific data with external datasets yields gains only with small domain gaps is supported by the presented results, though the mechanism could benefit from deeper investigation.
- **Medium Confidence:** The superiority of temporal reconstruction (VideoMAEv2) over spatial-only approaches (EndoViT) for skill assessment is demonstrated, but the analysis could be strengthened by examining which specific OSATS criteria benefit most from temporal modeling versus spatial features.

## Next Checks

1. **Cross-procedure generalization:** Validate the pre-training findings on a different surgical procedure (e.g., cholecystectomy or cardiac surgery) to assess whether domain alignment benefits transfer across surgical specialties or are specific to RARP.

2. **Domain similarity threshold:** Systematically vary the EMD between pre-training and target datasets to identify the threshold at which domain similarity begins to negatively impact performance, providing clearer guidance for dataset selection.

3. **Alternative few-shot protocols:** Compare the episodic sampling approach with meta-learning strategies (e.g., MAML or ProtoNets) to determine if the domain alignment benefits persist across different few-shot learning methodologies.