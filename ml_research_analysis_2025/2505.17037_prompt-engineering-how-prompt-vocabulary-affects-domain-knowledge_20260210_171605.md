---
ver: rpa2
title: 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge'
arxiv_id: '2505.17037'
source_url: https://arxiv.org/abs/2505.17037
tags:
- specificity
- prompt
- nouns
- verbs
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examined how synonym specificity in prompts
  affects LLM performance in domain-specific question-answering and reasoning tasks.
  A synonymization framework was developed to substitute nouns, verbs, and adjectives
  with varying specificity levels across STEM, law, and medicine datasets using four
  LLMs (Llama-3.1-70B-Instruct, Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large
  2).
---

# Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge

## Quick Facts
- arXiv ID: 2505.17037
- Source URL: https://arxiv.org/abs/2505.17037
- Reference count: 40
- Increasing noun specificity generally did not significantly impact LLM performance, while more specific verb synonyms negatively affected reasoning task outcomes

## Executive Summary
This study systematically examined how synonym specificity in prompts affects LLM performance in domain-specific question-answering and reasoning tasks. Using a synonymization framework, the researchers substituted nouns, verbs, and adjectives with varying specificity levels across STEM, law, and medicine datasets using four LLMs. Contrary to expectations, increasing noun specificity generally did not significantly impact performance, while more specific verb synonyms negatively affected reasoning task outcomes. The analysis revealed optimal specificity ranges for both nouns (17.72-19.70) and verbs (8.08-10.57) across all models where performance peaked.

## Method Summary
The study employed a five-step synonymization framework: POS tagging with NLTK, word sense disambiguation using Llama-3.1-70B-Instruct, specificity scoring via WordNet taxonomy, synonym categorization into low/intermediate/high specificity levels, and replacement at 33%/67%/100% ratios. Specificity scores were calculated using taxonomy-based formulas incorporating distance from root nodes and hyponym counts. The framework was applied to MMLU (14 subdatasets), GPQA (448 PhD-level questions), and GSM8K (grade-school math reasoning) datasets, with up to 250 samples per dataset. Performance was measured using Jaccard similarity and Spearman correlations between prompt specificity and accuracy across four LLMs: Llama-3.1-70B-Instruct, Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2.

## Key Results
- Optimal specificity ranges identified: nouns (17.72-19.70), verbs (8.08-10.57) across all four tested models
- Increasing noun specificity generally did not significantly impact performance in question-answering tasks
- More specific verb synonyms negatively affected reasoning task outcomes, with strong negative correlations in GSM8K (-0.78 to -0.89)
- Adjective specificity quantification method introduced but analysis limited due to underrepresentation in datasets

## Why This Works (Mechanism)

### Mechanism 1
LLMs exhibit an optimal specificity range for prompt vocabulary where performance peaks, beyond which both increasing and decreasing specificity degrades accuracy. Specificity scores aggregate via WordNet taxonomy depth (distance from root node) and hyponym count. When prompt specificity falls within the identified optimal ranges (nouns: 17.72-19.70, verbs: 8.08-10.57), models achieve highest correct-answer counts. Outside these bounds, performance declines. This mechanism assumes specificity as defined by WordNet taxonomy position correlates with how LLMs internally represent concept generality/specificity, which is partially validated by cross-model consistency.

### Mechanism 2
Increasing verb specificity harms reasoning task performance more than question-answering performance. Specific verb synonyms may constrain the action-space interpretation in chain-of-thought reasoning, potentially over-specifying how intermediate steps should be conceptualized. This disrupts the model's ability to follow flexible logical progressions. The mechanism assumes verbs encode procedural/relational information critical to reasoning chains, making them more sensitive to specificity changes than nouns which primarily encode entities.

### Mechanism 3
Noun specificity changes have minimal significant impact on LLM performance in QA tasks. Nouns may be processed more flexibly by attention mechanisms, with context disambiguating meaning regardless of specificity level. Models appear robust to noun synonym substitution when sense is preserved via WSD. This assumes noun semantics are more locally recoverable from context than verb semantics in the tested task structures.

## Foundational Learning

- **WordNet Synsets and Taxonomic Structure**: Specificity scoring depends on understanding how WordNet organizes words into synsets (cognitive synonyms) connected via hypernym/hyponym relations. The specificity formula uses distance from root (ENTITY) and hyponym count. Quick check: Given that "cell" in a biological context is more specific than "entity" but less specific than "erythrocyte," which would have a higher specificity score and why?

- **Word Sense Disambiguation (WSD)**: The framework uses WSD to select context-appropriate synonyms, preventing semantic drift. The paper validates Llama-3.1-70B-Instruct as a WSD model (79% accuracy vs. fine-tuned T5's 63%). Quick check: Why would replacing "cell" with "jail cell" in a biology prompt harm LLM performance differently than replacing "study" with "analyze"?

- **Spearman Correlation for Non-Normal Distributions**: Specificity score distributions are non-normal (Kolmogorov-Smirnov test, p<0.05). Spearman rank correlation is used instead of Pearson to assess specificity-performance relationships. Quick check: Why is the correlation coefficient alone insufficient without the p-value, given only 10 data points per model-dataset combination?

## Architecture Onboarding

- **Component map**: POS Retriever -> WSD Module -> Synonym Crawler -> Specificity Calculator -> Synonym Categorizer -> Instruction Synonymizer -> Prompt Specificity Aggregator
- **Critical path**: WSD accuracy -> Synonym selection validity -> Specificity score accuracy -> Performance measurement validity. Errors propagate; the paper acknowledges WSD misclassification as a limitation.
- **Design tradeoffs**: Sample size vs. computational cost (only 250 samples per dataset due to exponential synonym growth); Discrete vs. continuous specificity (categorical approach loses granularity; numerical approach requires more samples for statistical power); WSD model choice (Llama-3.1-70B is faster but still has 21% error rate; fine-tuned T5 is slower but domain-specific)
- **Failure signatures**: WSD errors (synonyms with wrong sense); Insufficient synonym pool (words with <3 unique specificity scores cannot be categorized into all three levels); Underrepresentation (adjectives had only 172 total samples across datasets); Token budget overflow (models set to 500 max tokens; some responses truncated or looped)
- **First 3 experiments**:
  1. Baseline replication: Run original prompts through all four models with exact-match evaluation to establish reference accuracy. Compare against paper's Jaccard-based results.
  2. Verb specificity isolation test: Take 50 GSM8K samples and manually substitute only verbs with progressively more specific synonyms (keeping nouns constant). Measure accuracy degradation slope.
  3. Optimal range validation: Generate synthetic prompts with noun specificity scores at 15, 18, 21, 24 and verb specificity at 6, 9, 12, 15. Plot performance curves to verify whether the claimed optimal ranges (17.72-19.70 nouns, 8.08-10.57 verbs) hold for your target model not in the original study.

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed adjective specificity quantification method effectively predict LLM performance in tasks relying heavily on subjective description, such as sentiment analysis? The conclusion states adjectives were underrepresented in Q&A tasks and suggests future research explore "how adjective specificity affect the LLM performance in other NLP tasks, such as sentiment analysis." This remains unresolved due to the current study's lack of sufficient adjective representation in neutral-toned datasets (GSM8K, MMLU).

### Open Question 2
Do the optimal specificity ranges (17.72â€“19.70 for nouns) generalize to specialized domains outside STEM, law, and medicine? The Limitations section notes the findings may not generalize to all fields and explicitly calls to "extend the scope to include a wider variety of domains such as psychology, finance or engineering." This remains unresolved as the current study restricted datasets to MMLU, GPQA, and GSM8K.

### Open Question 3
Why does increased verb specificity significantly degrade performance in reasoning tasks (GSM8K) compared to question-answering tasks? The Discussion section notes "higher frequency of significant negative correlations observed for verbs in the reasoning task" and states "Further research into the architectural differences... might reveal whether these sensitivities are due to inherent biases." This remains unresolved as the paper quantifies the effect but only hypothesizes that specific terms complicate logical progression.

## Limitations
- WSD model dependency: The framework relies on Llama-3.1-70B-Instruct for word sense disambiguation, which achieved 79% accuracy, with the remaining 21% error rate potentially systematically biasing synonym selection
- Adjective analysis limitations: Only 172 adjective samples remained after preprocessing across all datasets, making comprehensive analysis impossible and severely limiting generalizability of any findings related to adjective specificity
- Sample size constraints: With only 10 data points per model-dataset combination, the statistical power is limited, increasing vulnerability to outliers and reducing confidence in correlation coefficients

## Confidence

**High Confidence**: The existence of optimal specificity ranges (nouns: 17.72-19.70, verbs: 8.08-10.57) across all four tested models. This claim is supported by convergent evidence across multiple architectures and datasets, with clear statistical significance in the reported correlations.

**Medium Confidence**: The differential impact of noun versus verb specificity on performance. While the statistical results are clear, the small sample size and potential WSD confounding factors reduce confidence. The single significant noun result (Granite-13b on GPQA) suggests model-scale may moderate this effect.

**Low Confidence**: The adjective specificity quantification method and any conclusions about adjective impact on LLM performance. With only 172 samples available for analysis, any findings in this domain are highly speculative and should be treated as exploratory rather than conclusive.

## Next Checks

1. **WSD robustness validation**: Create a controlled test set of 100 domain-specific prompts where ground-truth sense disambiguation is known (verified by domain experts). Run the full synonymization pipeline using both Llama-3.1-70B-Instruct and a fine-tuned T5 model to quantify WSD error propagation and isolate its impact from specificity effects.

2. **Expanded sample size replication**: Generate an additional 100 samples per dataset using the optimal specificity ranges identified (noun specificity 17.72-19.70, verb specificity 8.08-10.57). Recompute Spearman correlations to verify whether the original findings hold with increased statistical power, particularly for the noun-verb performance differential.

3. **Context-dependent specificity testing**: Design a multi-turn prompt experiment where the first turn establishes domain context (e.g., "You are a biology expert") and subsequent turns use varied specificity levels. Compare performance against single-turn prompts of equivalent content to determine whether specificity effects are context-dependent.