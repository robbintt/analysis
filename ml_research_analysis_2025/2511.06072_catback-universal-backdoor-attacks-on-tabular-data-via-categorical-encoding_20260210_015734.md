---
ver: rpa2
title: 'CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding'
arxiv_id: '2511.06072'
source_url: https://arxiv.org/abs/2511.06072
tags:
- attack
- data
- dataset
- backdoor
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a universal backdoor attack on tabular data
  by converting categorical features into floating-point representations, enabling
  a gradient-based perturbation that can target any feature type. The method achieves
  up to 100% attack success rate across five datasets and four models while maintaining
  clean accuracy.
---

# CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding

## Quick Facts
- arXiv ID: 2511.06072
- Source URL: https://arxiv.org/abs/2511.06072
- Reference count: 40
- Primary result: Universal backdoor attack achieving 100% ASR across five datasets and four models while maintaining clean accuracy

## Executive Summary
CatBack introduces a novel universal backdoor attack framework for tabular data that converts categorical features into floating-point representations using frequency-based encoding. This encoding enables gradient-based perturbation optimization that can target any feature type (categorical or numerical) without requiring domain-specific modifications. The attack achieves high success rates (up to 100% ASR) while preserving clean data accuracy, and successfully bypasses existing backdoor defense mechanisms. The approach demonstrates that categorical features in tabular data can be leveraged to create effective and stealthy backdoor triggers.

## Method Summary
The CatBack method transforms categorical features into floating-point representations using frequency-based encoding, where each category is represented by its relative frequency in the dataset. This encoding enables the application of gradient-based optimization techniques to both categorical and numerical features simultaneously. The attack optimizes a universal perturbation vector that can be added to any input sample to trigger the backdoor behavior. By operating in the encoded floating-point space, the method achieves efficient preprocessing for neural network models while maintaining semantic relationships between original categories. The perturbation is optimized to maximize backdoor success while minimizing impact on clean data performance.

## Key Results
- Achieves up to 100% attack success rate across five datasets and four neural network models
- Maintains clean accuracy within 1-2% of baseline performance
- Successfully bypasses all tested defenses including neural cleanse, activation clustering, and STRIP
- Works on both categorical and numerical features through unified frequency-based encoding
- Demonstrates stealth through outlier detection resistance

## Why This Works (Mechanism)
The attack exploits the inherent structure of categorical data by converting it to a numerical representation that preserves statistical relationships while enabling gradient-based optimization. Frequency-based encoding maps categories to continuous values based on their occurrence patterns, creating a smooth optimization landscape for gradient descent. This encoding allows the attacker to apply perturbation techniques typically reserved for image or text data to tabular features. The universal perturbation vector learned in this space can be applied to any input sample, triggering the backdoor behavior regardless of the original feature values. The method's effectiveness stems from the fact that frequency-based representations maintain meaningful semantic relationships while being differentiable.

## Foundational Learning
- **Frequency-based encoding**: Converts categorical values to floating-point numbers based on their relative frequency in the dataset. Needed to enable gradient-based optimization on categorical features. Quick check: Verify encoding preserves meaningful relationships between categories.
- **Universal perturbation optimization**: Learns a single perturbation vector that can be applied to any input to trigger backdoor behavior. Needed for efficient attack deployment without per-sample computation. Quick check: Confirm perturbation generalizes across different input samples.
- **Gradient-based backdoor trigger design**: Uses optimization techniques to create triggers that activate specific target behaviors. Needed to achieve high attack success rates. Quick check: Validate trigger effectiveness through controlled experiments.

## Architecture Onboarding

**Component Map**
Frequency Encoding -> Perturbation Optimization -> Universal Trigger Application -> Model Training

**Critical Path**
Categorical features are first transformed using frequency-based encoding, then a universal perturbation vector is optimized through gradient descent, and finally this perturbation is applied to trigger the backdoor during inference.

**Design Tradeoffs**
- Frequency encoding vs one-hot encoding: Frequency encoding enables gradient optimization but may lose fine-grained category distinctions
- Universal vs sample-specific triggers: Universal triggers are efficient but potentially more detectable than dynamic triggers
- Perturbation magnitude: Larger perturbations increase attack success but risk detection and clean accuracy degradation

**Failure Signatures**
- Clean accuracy degradation exceeding 5-10% indicates optimization instability
- Attack success rate below 70% suggests encoding inadequacy or optimization failure
- Detection by outlier methods implies perturbation magnitude is too large

**3 First Experiments**
1. Apply frequency encoding to categorical features and verify numerical relationships are preserved
2. Optimize perturbation vector on a small validation set and measure backdoor activation rate
3. Evaluate clean accuracy impact with different perturbation magnitudes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a standardized imperceptibility metric for tabular data be formulated to effectively capture feature-level similarity and interaction fidelity?
- Basis in paper: [explicit] Section IX states that the lack of an imperceptibility metric in the tabular domain is a limitation. The authors propose a theoretical "Tabular Imperceptibility Score" (TIS) dependent on Feature-Level Similarity (FLS) and Interaction Fidelity (IF) but leave the formal definition as an open question.
- Why unresolved: Unlike image or text domains, tabular data lacks spatial or sequential structure, making standard metrics like Lp-norms or SSIM inapplicable or insufficient for assessing semantic validity.
- What evidence would resolve it: A formal mathematical definition of TIS, validated through user studies or automated detection tests showing correlation with perceived "naturalness" of data records.

### Open Question 2
- Question: Can defenses based on causal feature attribution or preprocessing-agnostic detection strategies successfully mitigate CatBack attacks?
- Basis in paper: [explicit] Section XI explicitly calls for future work to develop "defenses based on causal feature attribution" and "preprocessing-agnostic detection strategies" as a response to the failure of existing defenses like Spectral Signatures.
- Why unresolved: The paper demonstrates that standard backdoor defenses fail because CatBack's frequency-based encoding aligns poisoned samples with clean data modes, bypassing outlier detection.
- What evidence would resolve it: Empirical results showing that a defense mechanism monitoring causal relationships between features can reduce the Attack Success Rate (ASR) without significantly degrading Clean Data Accuracy (CDA).

### Open Question 3
- Question: How does CatBack perform in federated learning or privacy-preserving settings where data is distributed and subject to aggregation algorithms?
- Basis in paper: [explicit] Section XI lists "evaluating these threats in federated or privacy-preserving settings" as a specific direction for future work.
- Why unresolved: The current evaluation focuses on centralized training or outsourced training. Distributed settings introduce factors like model aggregation (averaging weights) and differential privacy noise, which may disrupt the trigger optimization.
- What evidence would resolve it: Experiments measuring ASR and CDA when the poisoned data is split across multiple clients in a federated learning simulation.

### Open Question 4
- Question: Can CatBack be extended to use dynamic, sample-specific triggers while maintaining high attack efficacy?
- Basis in paper: [explicit] Section XI states future work could "extend CatBack to dynamic, sample-specific triggers."
- Why unresolved: The current method relies on a universal perturbation vector. While effective, a static trigger is theoretically easier to reverse-engineer than a dynamic one that changes per sample.
- What evidence would resolve it: An implementation of a conditional trigger generation mechanism that varies perturbations based on input features, achieving comparable ASR to the universal trigger.

## Limitations
- Limited to gradient-based models; non-differentiable methods like decision trees remain untested
- Defense evaluation restricted to only three specific backdoor detection mechanisms
- Clean accuracy preservation claims need verification across broader training configurations
- Practical stealth in real-world deployment scenarios unvalidated beyond controlled experiments

## Confidence

**High**: Attack effectiveness on neural networks demonstrated through extensive experiments with consistent results across multiple datasets and architectures

**Medium**: Clean accuracy preservation results show maintenance within 1-2%, but sensitivity to training parameters untested

**Low**: Universal applicability across all tabular models claimed but limited to gradient-based models; non-differentiable models completely untested

**Medium**: Defense evasion tested against only three defenses; comprehensive security evaluation incomplete

**Low-Medium**: Practical stealth of categorical encoding approach depends on real-world deployment scenarios not validated

## Next Checks

1. Test attack transferability to non-gradient-based models (decision trees, random forests, gradient boosting) to verify true universality claims
2. Evaluate attack performance under different training hyperparameters and optimization settings to assess clean accuracy robustness
3. Assess detection by additional defense mechanisms including input perturbation detection, model watermarking, and anomaly detection specifically designed for tabular data