---
ver: rpa2
title: 'XiHeFusion: Harnessing Large Language Models for Science Communication in
  Nuclear Fusion'
arxiv_id: '2502.05615'
source_url: https://arxiv.org/abs/2502.05615
tags:
- fusion
- nuclear
- large
- xihefusion
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XiHeFusion is the first large language model specifically developed
  for nuclear fusion science communication. Built on Qwen2.5-14B via supervised fine-tuning,
  it integrates multi-source fusion knowledge and employs chain-of-thought reasoning
  to enhance logical, detailed responses.
---

# XiHeFusion: Harnessing Large Language Models for Science Communication in Nuclear Fusion

## Quick Facts
- arXiv ID: 2502.05615
- Source URL: https://arxiv.org/abs/2502.05615
- Reference count: 40
- First LLM specifically developed for nuclear fusion science communication, fine-tuned on 1.2M synthetic QA pairs

## Executive Summary
XiHeFusion is a domain-specific large language model developed for nuclear fusion science communication, built by fine-tuning Qwen2.5-14B on a large-scale dataset of synthetic question-answer pairs generated from nuclear fusion literature. The model employs Chain-of-Thought reasoning to enhance logical and detailed responses, supports bilingual dialogue (Chinese/English), and includes a custom 180+ question test set for evaluation. Experimental results demonstrate strong performance on fusion-related queries, with improved accuracy and depth when using CoT prompting, and competitive results compared to leading LLMs on fusion tasks. The model is open-sourced to accelerate public understanding and engagement in fusion energy research.

## Method Summary
The authors synthesized a large-scale dataset of 1.2 million question-answer pairs using DeepSeek V3 to process nuclear fusion-related text from sources including arXiv, CNKI, and CommonCrawl. This dataset was used to perform supervised fine-tuning (SFT) on Qwen2.5-14B. The model employs Chain-of-Thought (CoT) prompting at inference time, using 8-shot examples to enforce structured reasoning (Background → Definition → Reasoning → Verification → Summary). The approach was evaluated on a custom 184-question test set covering 10 aspects of fusion knowledge.

## Key Results
- XiHeFusion matches or exceeds strong LLMs like DeepSeek V3 and Baichuan 2 on fusion-specific tasks
- CoT prompting significantly improves logical coherence and detail in responses compared to direct answering
- The model demonstrates strong bilingual capabilities in both Chinese and English for fusion science communication
- Custom 184-question test set provides comprehensive evaluation across multiple fusion knowledge domains

## Why This Works (Mechanism)

### Mechanism 1: Domain Knowledge Injection via Synthetic Supervised Fine-Tuning (SFT)
The model is fine-tuned on 1.2M synthetic QA pairs generated by DeepSeek V3 from raw fusion text, allowing it to prioritize fusion-specific patterns and terminology over general knowledge.

### Mechanism 2: Logical Structuring via Chain-of-Thought (CoT) Prompting
CoT prompting enforces a structured generation sequence (Background → Definition → Reasoning → Verification → Summary), improving logical coherence and educational value for complex scientific topics.

### Mechanism 3: Bilingual Alignment via Cross-Lingual Transfer
Fine-tuning on parallel domain data in both Chinese and English enables consistent science communication across languages, leveraging the multilingual capabilities of the base Qwen2.5 model.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) vs. RAG**
  - Why needed: XiHeFusion uses SFT to train knowledge into weights rather than retrieving it at inference time, making it faster but harder to update
  - Quick check: Does the model need access to real-time tokamak sensor data, or just static textbook physics? (XiHeFusion is optimized for the latter)

- **Concept: Synthetic Data Generation (Teacher-Student)**
  - Why needed: The model is trained on QA pairs generated by DeepSeek V3, meaning the "ground truth" is actually a stronger model's interpretation
  - Quick check: If DeepSeek V3 hallucinates a formula in the training data, will XiHeFusion learn it? (Yes)

- **Concept: Transformer Decoders & Attention Masks**
  - Why needed: Understanding the difference between context (CoT prompt/history) and generation is key to debugging why the model might repeat itself
  - Quick check: How does the model handle a context length longer than 128K tokens? (It truncates, potentially losing the "Background" step of the CoT)

## Architecture Onboarding

- **Component map:** Raw Text (CommonCrawl/CNKI) → DeepSeek V3 → 1.2M QA JSONs → Qwen2.5-14B Transformer + LoRA/Full Fine-tuning → System Prompt (CoT Instruction) + 8-shot Examples + User Query

- **Critical path:**
  1. Data Inspection: Verify the 1.2M QA pairs (format: Instruction/Input/Output), checking for CoT structures in the "Output" field
  2. Resource Allocation: Qwen2.5-14B requires ~28GB+ VRAM for inference (BF16) and significantly more for training
  3. Inference Test: Load the model with the specific CoT system prompt and run the 184-question benchmark

- **Design tradeoffs:**
  - Generalization vs. Domain Lock: Heavy SFT on fusion data might degrade general coding or creative writing abilities
  - Synthetic vs. Human Data: Synthetic data scales easily but may lack nuanced student misconceptions
  - CoT Length: Long CoT outputs increase inference latency and cost

- **Failure signatures:**
  - Formula Hallucination: Mathematical derivations appear logical but are factually wrong
  - Language Bleeding: Chinese characters appearing in English responses or vice-versa
  - Repetitive Loops: Model gets stuck repeating the "Summary" or "Verification" step

- **First 3 experiments:**
  1. CoT Ablation: Run the test set with 0-shot, 1-shot, and 8-shot CoT prompts to measure logical coherence improvements
  2. Language Stress Test: Ask complex physics questions in mixed Chinese/English to test reasoning consistency
  3. Negative Constraint Test: Ask the model to "explain pellet injection without using technical jargon" to check if CoT structure prevents following negative constraints

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating multimodal data (images, videos, 1D signals) and physical formula modeling extend XiHeFusion's capabilities from science popularization to expert-level fusion research assistance? (Section V.D explicitly states this as future work)

### Open Question 2
To what extent does Reinforcement Learning fine-tuning improve alignment of XiHeFusion's outputs with high-quality human expectations compared to current SFT approach? (Section V.D notes RL fine-tuning has not been introduced)

### Open Question 3
What specific strategies are required to mitigate generation of inaccurate physical parameter descriptions identified in current outputs? (Paper acknowledges "some responses are not accurate enough" and Figure 14 highlights parameter errors)

## Limitations
- Synthetic QA generation pipeline relies entirely on DeepSeek V3's accuracy, risking hallucination propagation
- Custom 184-question evaluation set is not externally validated, limiting generalizability claims
- CoT reasoning is enforced through prompt engineering rather than learned during training, potentially limiting logical depth

## Confidence
- **High Confidence**: Model successfully fine-tunes on nuclear fusion domain data and shows improved performance on fusion-specific tasks versus general LLMs
- **Medium Confidence**: CoT prompting improves logical coherence in responses, but effect size varies by question complexity
- **Low Confidence**: Long-term retention of knowledge without update mechanisms, and potential degradation in general reasoning abilities

## Next Checks
1. **Ablation Study**: Compare XiHeFusion's performance with and without CoT prompting on the 184-question test set to quantify actual improvement
2. **Hallucination Audit**: Systematically evaluate 100 randomly sampled responses for mathematical formula accuracy and factual consistency against source documents
3. **Generalization Test**: Assess whether fusion expertise comes at cost of degraded performance on non-fusion tasks by benchmarking against Qwen2.5-14B on general LLM test suite