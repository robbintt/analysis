---
ver: rpa2
title: 'QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation'
arxiv_id: '2510.19296'
source_url: https://arxiv.org/abs/2510.19296
tags:
- code
- training
- correct
- signals
- verilog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel reinforcement learning approach for
  Verilog code generation that leverages correct signal implementations within erroneous
  modules. The key insight is that Verilog code's inherent signal independence allows
  partially correct modules to provide meaningful functional rewards.
---

# QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation

## Quick Facts
- arXiv ID: 2510.19296
- Source URL: https://arxiv.org/abs/2510.19296
- Reference count: 39
- Achieves 62.6% pass@1 accuracy on RTLLM v1.1, matching DeepSeek-v3 (671B parameters) with a 7B model

## Executive Summary
QiMeng-SALV introduces a novel reinforcement learning approach for Verilog code generation that leverages the structural independence of hardware signals to extract meaningful rewards from partially correct modules. The method uses signal-aware verification through output comparison, AST-based code extraction to isolate signal dependencies, and signal-aware DPO that computes token probabilities only for correct signal-level code segments. This approach shifts optimization from module-level to fine-grained signal-level, addressing insufficient functional rewards in RL training. Experimental results show the 7B-parameter model achieves state-of-the-art performance on VerilogEval and RTLLM benchmarks, significantly outperforming existing open-source models.

## Method Summary
The method combines supervised fine-tuning (SFT) with signal-aware reinforcement learning. Starting with a 7B Qwen2.5-Coder model fine-tuned on 135k Verilog samples, the approach generates multiple candidate modules per prompt and verifies them using random input stimuli. Yosys parses the Verilog AST to identify signal dependencies, allowing extraction of minimal code segments for each output signal. Signal-aware DPO then computes loss only on tokens belonging to the contrast signal (correct vs. incorrect implementations), masking all other tokens. LoRA fine-tuning is applied for 7k steps using this modified loss function, effectively learning from partial correctness rather than requiring full module correctness.

## Key Results
- Achieves 62.6% pass@1 accuracy on RTLLM v1.1, matching DeepSeek-v3 (671B parameters) with a 7B model
- Significantly outperforms leading open-source CodeV model on both VerilogEval and RTLLM benchmarks
- Ablation studies show 5-7% improvement from signal filtering and 7-9% from signal-aware extraction
- Signal-aware DPO converges faster than naive RL and produces higher quality preference pairs

## Why This Works (Mechanism)

### Mechanism 1: Signal-level Functional Verification Exploits Verilog's Structural Independence
Verilog describes hardware gate/wire interconnections rather than sequential execution, so output signals are relatively independent. When a generated module fails overall, individual signals may still match reference outputs under random test stimuli, enabling reward extraction from otherwise discarded samples.

### Mechanism 2: AST-based Signal Dependency Traversal Isolates Trainable Code Segments
Parse Verilog module → build signal topology graph → traverse backward from target output leaf through all dependent intermediate signals → retain only AST nodes contributing to target signal → produce minimal functional code segment.

### Mechanism 3: Signal-aware DPO Filters Noise by Masking Incorrect Signal Tokens
Restricting DPO loss computation to contrast signal-related tokens prevents gradient contamination from erroneous signal implementations, increasing probability of correct signal code while suppressing incorrect implementations.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Modified to mask non-signal tokens; understand baseline objective (log-σ of probability ratios with KL regularization) to grasp signal-aware modifications
- **Abstract Syntax Trees (AST) for Code Analysis**: Method relies on AST parsing (via Yosys) to decompose modules into signal-specific segments; necessary for understanding extraction pipeline
- **Verilog Hardware Semantics (Signal Parallelism, Module Structure)**: Core insight depends on Verilog's parallel signal semantics vs. sequential software code; misunderstanding leads to incorrect expectations about signal independence

## Architecture Onboarding

- **Component map**: Naive Code Generator (SFT'd Qwen2.5-Coder-7B) → Signal-aware Verification (Yosys + simulator) → Signal-aware Code Extraction (Yosys AST parser) → Signal-aware DPO Training (LoRA fine-tuning)
- **Critical path**: SFT model quality → candidate sample diversity → verification accuracy → segment extraction correctness → DPO reward density. Weakness at any stage propagates downstream.
- **Design tradeoffs**: Complete vs. partial correct datasets (trades quality for quantity), verification rigor (random vs. formal), segment granularity (finer increases reward density but risks losing context)
- **Failure signatures**: Low pass@1 after DPO (insufficient contrast pairs or verification noise), degraded performance on mixed datasets without filtering, AST extraction errors on complex modules
- **First 3 experiments**: 1) Reproduce SFT baseline to verify ~48% pass@1 on RTLLM v1.1, 2) Ablate signal filtering to confirm ~3-4% pass@1 drop, 3) Scale preference dataset to validate monotonic improvement

## Open Questions the Paper Calls Out
The paper acknowledges limitations regarding dataset quality and does not discuss obtaining high-quality reference modules, but does not explicitly call out specific open questions beyond this limitation.

## Limitations
- Relies on reference module correctness for verification; erroneous ground truth can lead to erroneous reward judgments
- Random stimulus generation may miss corner cases in complex sequential logic compared to formal verification
- Assumes signal independence may degrade performance for modules with highly coupled state machines or interdependent outputs

## Confidence
- **Method novelty**: High - Signal-aware DPO with AST-based extraction is novel approach
- **Experimental validity**: Medium - Strong benchmark results but verification method limitations acknowledged
- **Reproducibility**: Medium - Key hyperparameters unspecified (β, random input generation parameters)
- **Generalizability**: Medium - Method specific to Verilog's structural semantics, may not transfer to other languages

## Next Checks
1. Verify signal independence assumption by testing on modules with known signal coupling
2. Compare random verification coverage vs. formal verification on sequential logic circuits
3. Analyze false positive rate in signal verification step by cross-checking with model checking on held-out complex modules