---
ver: rpa2
title: 'From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs
  via Building Zero-Shot Discriminative Embedding Model'
arxiv_id: '2508.00955'
source_url: https://arxiv.org/abs/2508.00955
tags:
- prompt
- embedding
- retrieval
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adapting Multimodal Large
  Language Models (MLLMs) for universal embedding tasks without the computational
  burden of large-scale contrastive pre-training. The proposed framework leverages
  the MLLM''s innate instruction-following capabilities through two key components:
  (1) a hierarchical embedding prompt template with system and user-level instructions
  that enforces discriminative representation, and (2) self-aware hard negative sampling
  (SaHa) that uses the model''s own understanding to mine effective hard negatives
  while filtering out false negatives.'
---

# From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model

## Quick Facts
- arXiv ID: 2508.00955
- Source URL: https://arxiv.org/abs/2508.00955
- Reference count: 40
- One-line primary result: Achieves SOTA on MMEB without contrastive pre-training via hierarchical prompting and self-aware hard negative sampling

## Executive Summary
This paper addresses the challenge of adapting Multimodal Large Language Models (MLLMs) for universal embedding tasks without the computational burden of large-scale contrastive pre-training. The proposed framework leverages the MLLM's innate instruction-following capabilities through two key components: (1) a hierarchical embedding prompt template with system and user-level instructions that enforces discriminative representation, and (2) self-aware hard negative sampling (SaHa) that uses the model's own understanding to mine effective hard negatives while filtering out false negatives. Experiments demonstrate that the hierarchical prompt alone achieves zero-shot performance competitive with contrastively trained baselines. When combined with SaHa for fine-tuning, the approach achieves state-of-the-art results on the MMEB benchmark, outperforming methods that rely on extensive contrastive pre-training.

## Method Summary
The framework builds discriminative embedding capabilities into pre-trained MLLMs through hierarchical prompting and self-aware hard negative sampling. The method first applies a two-level prompt template where system prompts (applied to all inputs) receive privileged attention and act as global controllers, while user prompts provide task-specific instructions. For fine-tuning, the SaHa algorithm preprocesses training data by clustering anchors with hard negatives selected based on semantic dissimilarity of their "owner queries," reducing false negative contamination. The approach uses InfoNCE loss with LoRA adapters (rank 8) and avoids joint training with generation objectives. Key hyperparameters include pool multiplier m=4, hard negatives k=7, temperature τ=0.02, and sub-batch sizes of 16 (2B model) or 4 (7B model) with gradient caching.

## Key Results
- Zero-shot baseline with hierarchical prompt achieves 43.3 avg on MMEB, competitive with contrastively trained methods
- SaHa fine-tuning improves performance by 4.8 points over simple in-batch negative baseline
- Achieves SOTA on MMEB benchmark with 67.1 avg for 2B model and 69.2 avg for 7B model
- Reduces training time significantly compared to methods requiring large-scale contrastive pre-training

## Why This Works (Mechanism)

### Mechanism 1: System Prompts as Global Embedding Controllers
- Claim: System prompts receive substantially higher attention weights than user prompts, enabling them to act as global controllers that enforce consistent embedding structure across all inputs.
- Mechanism: The hierarchical prompt template separates instructions into two levels: (1) a system-level prompt applied universally to queries and candidates that enforces semantic compression, and (2) a representation prompt appended to queries to reinforce the embedding objective. This exploits the differential attention patterns where system tokens receive 5-54× more attention than user tokens across model layers.
- Core assumption: The attention differential observed in Qwen2-VL generalizes to other decoder-based MLLMs.
- Evidence anchors:
  - [abstract] "hierarchical embedding prompt template with system and user-level instructions that enforces discriminative representation"
  - [Section III-B-1] "ϵsys is substantially greater than ϵusr, suggesting the system prompt acts as a global controller"
  - [corpus] Related work (Think Then Embed, U-MARVEL) explores generative context for embeddings but does not specifically analyze system prompt attention mechanisms.
- Break condition: If system prompts do not exhibit privileged attention in your target MLLM architecture, the mechanism may not transfer.

### Mechanism 2: Self-Aware Hard Negative Sampling via Query-Target Coherence
- Claim: Hard negatives mined from candidates whose "owner queries" are semantically dissimilar to the anchor query reduce false negative contamination compared to similarity-based thresholding.
- Mechanism: SaHa retrieves semantically similar candidates for an anchor, then identifies which training queries those candidates serve as positives for. It selects candidates whose owner queries are most dissimilar to the anchor, exploiting the intuition that similar queries share targets. This filters false negatives without external teacher models.
- Core assumption: Training data contains sufficient query-candidate mappings for the owner query identification to be meaningful.
- Evidence anchors:
  - [abstract] "self-aware hard negative sampling (SaHa) that uses the model's own understanding to mine effective hard negatives while filtering out false negatives"
  - [Section III-C] "semantically similar queries are more likely to share the same target and thus may not serve as reliable negatives"
  - [corpus] Related work (VIRTUE, Vela) addresses multimodal embeddings but relies on external models or does not address hard negative contamination explicitly.
- Break condition: If your training corpus has sparse query-candidate associations or high semantic overlap across distinct classes, owner query dissimilarity may not effectively filter false negatives.

### Mechanism 3: Zero-Shot Discriminative Capability via Prompt-Induced Compression
- Claim: Instruction-following capabilities from LLM pre-training can be redirected toward discriminative embedding through prompts that explicitly request compressed ("one word") representations.
- Mechanism: The system prompt instructs the model to "summarize the provided image in one word" or "describe the text in one word," forcing semantic compression into a fixed representation. Last-token pooling then extracts this compressed embedding. This avoids contrastive pre-training by repurposing existing instruction-following abilities.
- Core assumption: The MLLM has sufficiently strong instruction-following capabilities from its base pre-training.
- Evidence anchors:
  - [abstract] "leverages the MLLM's innate instruction-following capabilities"
  - [Section IV-B-3] "system prompt is the single most critical component; configurations that incorporate the system prompt consistently outperform those that omit it"
  - [corpus] U-MARVEL and Think Then Embed similarly explore leveraging MLLM capabilities but focus on generative context rather than prompt hierarchy.
- Break condition: If your MLLM lacks strong instruction-following (e.g., insufficient pre-training scale), the zero-shot baseline may underperform significantly.

## Foundational Learning

- **Contrastive Learning & InfoNCE Loss**
  - Why needed here: SaHa fine-tuning uses InfoNCE loss with hard negatives; understanding how negative sampling affects representation quality is essential for debugging training.
  - Quick check question: Can you explain why false negatives degrade contrastive learning objectives?

- **Attention Mechanisms in Decoder-Only Transformers**
  - Why needed here: The hierarchical prompt relies on system prompts receiving privileged attention; understanding causal attention patterns helps validate whether this transfers to your architecture.
  - Quick check question: Why might early layers vs. late layers show different attention distributions for system vs. user tokens?

- **Hard Negative Mining Trade-offs**
  - Why needed here: SaHa is positioned against traditional HNS methods; understanding computational costs and false negative risks helps evaluate when SaHa is appropriate.
  - Quick check question: What is the computational complexity difference between in-batch negatives and global hard negative mining?

## Architecture Onboarding

- **Component map:**
  Input Processing: Vision encoder (ViT) → Projection layer → LLM backbone with hierarchical prompt template
  Prompt Template: System prompt (global) + User prompt (task instruction + representation reinforcement)
  Embedding Extraction: Last-token pooling → L2 normalization
  Training Data Prep (SaHa): Offline preprocessing that builds clusters of (anchor, positive, k hard negatives) using model's own embeddings
  Fine-tuning: InfoNCE loss with LoRA adapters (rank 8)

- **Critical path:**
  1. Validate system prompt attention patterns on your target MLLM before full implementation
  2. Implement hierarchical prompt template for zero-shot baseline
  3. Run SaHa preprocessing on training corpus (one-time offline)
  4. Fine-tune with LoRA using preprocessed clusters

- **Design tradeoffs:**
  - Pool multiplier (4-8 tested): Higher values increase candidate pool but show marginal degradation; default to 4
  - Hard negatives per anchor (k=7 vs k=15): Higher k slightly improves IND but similar OOD; k=7 is more efficient
  - Batch size vs. GradCache: Sub-batch size 16 (2B model) or 4 (7B model) with gradient caching for memory constraints

- **Failure signatures:**
  - Zero-shot baseline underperforms contrastively trained models → Check if system prompt attention differential exists in your architecture
  - SaHa underperforms in-batch negatives → Likely false negative contamination; reduce pool multiplier or inspect cluster assignments
  - Training instability → Temperature τ=0.02 is sensitive; verify GradCache implementation if using large effective batch sizes

- **First 3 experiments:**
  1. **Attention Analysis:** Measure ϵsys/ϵusr ratio across layers on your target MLLM with sample inputs; if ratio < 5×, hierarchical prompt may not transfer effectively.
  2. **Zero-Shot Baseline:** Apply hierarchical prompt to MMEB evaluation without fine-tuning; compare against Table II baselines to validate prompt-only performance.
  3. **Ablation on k and pool multiplier:** Fine-tune with SaHa using k∈{7,15} and multiplier∈{4,6,8}; identify stable configuration before full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical prompting and SaHa framework effectively generalize to temporal modalities like video and audio?
- Basis in paper: [explicit] The authors state in the conclusion, "For future work, we plan to extend our framework to additional modalities such as video and audio."
- Why unresolved: The current study strictly validates the method on image-text pairs (MMEB benchmark); the impact of temporal dependencies on the static "hierarchical embedding prompt" and the SaHa clustering mechanism remains untested.
- What evidence would resolve it: Experimental results on video-text (e.g., MSR-VTT) and audio-text retrieval benchmarks showing competitive performance without architectural changes.

### Open Question 2
- Question: To what extent does the proposed discriminative fine-tuning degrade the model's innate generative capabilities?
- Basis in paper: [explicit] The conclusion notes the need to "investigate methods to preserve the model's generative capabilities alongside its discriminative performance, particularly under limited computational resources."
- Why unresolved: While the method avoids joint training with generation loss (unlike baselines such as CAFe or VladVA), the paper does not quantify if "catastrophic forgetting" of generation skills occurs during the embedding-specific fine-tuning.
- What evidence would resolve it: A comparative evaluation on standard generation benchmarks (e.g., visual question generation) before and after applying the SaHa fine-tuning.

### Open Question 3
- Question: Does the "global controller" role of system prompts hold across diverse MLLM architectures with different attention mechanisms?
- Basis in paper: [inferred] The hierarchical prompt design is motivated by an attention analysis (Fig. 1) showing system prompts receive higher weights ($\epsilon_{sys} \gg \epsilon_{usr}$), but this analysis is limited to specific layers of the Qwen2-VL model.
- Why unresolved: The assumption that system prompts function as dominant anchors may not transfer to MLLMs with different attention patterns, sparse attention, or non-transformer backbones.
- What evidence would resolve it: A cross-architecture analysis replicating Figure 1 on models like InternVL or Mamba-based MLLMs to verify if the attention efficiency gap persists.

## Limitations
- The framework's reliance on attention differentials between system and user prompts may not generalize across different MLLM architectures with varying attention mechanisms.
- SaHa's effectiveness depends critically on the quality and completeness of query-candidate mappings in the training corpus, making it vulnerable to datasets with sparse associations.
- The zero-shot baseline's competitiveness assumes sufficient instruction-following capability from base MLLM pre-training, which may not hold for smaller or differently trained models.

## Confidence
**High Confidence**: The hierarchical prompt template's contribution to zero-shot performance is well-supported by ablation studies showing consistent superiority over configurations that omit the system prompt. The mechanism of using system prompts to enforce discriminative representation through privileged attention is directly observable and measurable.

**Medium Confidence**: The SaHa algorithm's effectiveness in reducing false negative contamination and improving fine-tuning performance is demonstrated empirically but depends heavily on dataset-specific properties. The claim that owner query dissimilarity effectively filters false negatives assumes training data characteristics that may not generalize.

**Low Confidence**: The generalizability of the attention differential mechanism across different MLLM architectures (beyond Qwen2-VL) remains unproven. The zero-shot baseline's competitiveness may be specific to Qwen2-VL's instruction-following capabilities and may not transfer to models with different pre-training regimes.

## Next Checks
1. **Cross-Architecture Attention Analysis**: Implement the hierarchical prompt on a different MLLM architecture (e.g., LLaVA or MiniGPT-4) and measure system vs. user prompt attention ratios. Verify whether the 5-54× attention differential observed in Qwen2-VL exists; if not, the zero-shot baseline may fail to transfer.

2. **SaHa False Negative Sensitivity Analysis**: On a held-out validation set from the MMEB training data, systematically vary the pool multiplier and k parameters while measuring Precision@1. Identify the point where false negative contamination begins to degrade performance, establishing bounds for when SaHa becomes counterproductive.

3. **Instruction-Following Capability Transfer**: Test the zero-shot baseline on a smaller MLLM variant (e.g., Qwen2-VL-1.5B) and compare performance degradation against the expected scale-dependent instruction-following improvements. This quantifies how much the zero-shot success depends on model scale versus the prompt mechanism itself.