---
ver: rpa2
title: Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe
  Generation and Beyond
arxiv_id: '2505.16060'
source_url: https://arxiv.org/abs/2505.16060
tags:
- learning
- target
- semiconductor
- process
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Feedback Learning (MFL), a test-time
  optimization framework that adapts pre-trained AI models or hardware systems to
  new objectives without retraining or hardware modifications. MFL uses a lightweight
  reverse model to iteratively search for optimal inputs given desired outputs, making
  it particularly valuable in deployment-constrained settings like semiconductor manufacturing.
---

# Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond

## Quick Facts
- arXiv ID: 2505.16060
- Source URL: https://arxiv.org/abs/2505.16060
- Reference count: 40
- This paper introduces Model Feedback Learning (MFL), a test-time optimization framework that adapts pre-trained AI models or hardware systems to new objectives without retraining or hardware modifications.

## Executive Summary
Model Feedback Learning (MFL) is a test-time optimization framework that enables adaptation of pre-trained models to new objectives without retraining. The method uses a lightweight reverse model to iteratively search for optimal inputs given desired outputs, making it particularly valuable in deployment-constrained settings like semiconductor manufacturing. MFL achieves target recipe generation in just five iterations while outperforming both Bayesian optimization and human experts.

The approach demonstrates strong performance across multiple applications including plasma etching, chemical vapor deposition, and wire bonding, with particular advantages in data efficiency and stability under noisy conditions. By eliminating retraining requirements, MFL reduces computational costs and enables real-world deployment in high-dimensional control settings.

## Method Summary
MFL operates through a two-loop training architecture where a reverse model R learns to map target outputs to optimal inputs. Loop A pre-trains R using a differentiable emulator E that approximates the target machine model M, while Loop B fine-tunes R using the actual machine with fewer iterations. The method incorporates conservative learning based on model sensitivity to stabilize convergence in high-sensitivity regions, and uses domain randomization during emulator training to enhance robustness to input noise. The reverse model is lightweight (~7 kB) and can be deployed to generate recipes for new targets without further modification to the underlying machine.

## Key Results
- Achieved target recipe generation in just five iterations for plasma etching applications
- Outperformed both Bayesian optimization and human experts in recipe optimization tasks
- Demonstrated strong performance across three distinct applications: plasma etching (5 iterations), chemical vapor deposition (5 iterations), and wire bonding (9 iterations)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The two-loop training architecture enables efficient reverse model alignment while minimizing expensive interactions with the deployed machine model.
- **Mechanism:** Loop A pre-trains the reverse model R against an emulator E (cheap proxy), establishing approximate inverse mapping. Loop B then fine-tunes R using the actual machine model M with fewer iterations (τ ≪ T), correcting for emulator approximation errors without full retraining.
- **Core assumption:** The emulator E sufficiently approximates machine model M that pre-training transfers meaningfully; gradient information flows correctly through the chain: θ → R_θ(z') → x' → E(x') → y' → loss.
- **Evidence anchors:**
  - [abstract]: "MFL leverages a lightweight reverse model to iteratively search for optimal inputs, enabling efficient adaptation to new objectives under deployment constraints."
  - [section 4.1]: "Since the pre-training in Loop A effectively aligns R with the emulator E, the required number of iterations τ is typically much smaller than T."
  - [corpus]: Weak direct corpus support for two-loop specifically; neighbor papers focus on test-time adaptation broadly but not this dual-loop structure.
- **Break condition:** If emulator-to-machine gap is too large (Figure 4 shows this discrepancy), Loop A pre-training may provide negative transfer, increasing rather than decreasing Loop B iterations.

### Mechanism 2
- **Claim:** Conservative learning based on model sensitivity stabilizes convergence in high-sensitivity input regions critical for precision manufacturing.
- **Mechanism:** During later training stages (t ≥ T₀ in Loop A, h ≥ τ₀ in Loop B), the algorithm computes sensitivity s_E(x) or s_M(x) (Jacobian norm). If sensitivity exceeds threshold δ, learning rate switches from α₁ to α₂ (α₂ < α₁), preventing overshoot in regions where small input changes cause large output variations.
- **Core assumption:** Sensitivity threshold δ appropriately balances exploration vs. stability; high-sensitivity regions correlate with corner cases that destabilize optimization.
- **Evidence anchors:**
  - [section 4.2]: "High sensitivity indicates that even small changes in the input could lead to significant fluctuations in the output."
  - [Algorithm 1, lines 8-12, 20-24]: Explicit conditional logic for learning rate selection based on sensitivity.
  - [corpus]: No direct corpus evidence for sensitivity-aware conservative learning in semiconductor recipe generation.
- **Break condition:** If sensitivity threshold δ is set too low, the algorithm becomes overly conservative, slowing convergence unnecessarily; if too high, instability persists in sensitive regions.

### Mechanism 3
- **Claim:** Domain randomization during emulator training enhances robustness to input noise and distribution shift at test time.
- **Mechanism:** Instead of training emulator E on clean input-output pairs {(xᵢ, zᵢ)}, zero-mean Gaussian noise is added to inputs during training. This forces E to learn smoother mappings less sensitive to perturbations, improving R's generalization when deployed on noisy real-world data.
- **Core assumption:** Noise distribution during training approximates noise encountered at deployment; randomization improves rather than degrades emulator fidelity.
- **Evidence anchors:**
  - [Algorithm 1, line 2]: "Construct emulator model E using {(xᵢ, zᵢ)} via supervised learning with domain randomization."
  - [Figure 8]: Ablation showing randomization model achieving lower output error than fixed model under varying attack noise levels.
  - [corpus]: Neighbor paper [2505.03848] discusses transfer learning in semiconductor analytics but doesn't directly validate domain randomization for this application.
- **Break condition:** If training noise distribution mismatches deployment noise (wrong variance, non-Gaussian), randomization may not transfer and could harm accuracy.

## Foundational Learning

- **Concept: Supervised Learning for Forward Models**
  - Why needed here: MFL assumes existence of a pre-trained forward model M (machine) that maps inputs X to outputs Y ≈ Z. Understanding this forward mapping is prerequisite to inverting it.
  - Quick check question: Given a dataset of semiconductor recipes (inputs) and etching outcomes (outputs), can you train a neural network to predict outcomes from recipes? If not, review standard feedforward network training with MSE loss.

- **Concept: Inverse Problems and Gradient-Based Optimization**
  - Why needed here: The core MFL operation is solving min_{X'} error(M(X'), Z')—finding inputs that produce desired outputs. This requires understanding how to propagate gradients backward through composed functions.
  - Quick check question: Given f(x) = (x - 3)², can you compute the gradient update x ← x - α·∇_x f(x) and explain why this finds the input minimizing f?

- **Concept: Emulators as Differentiable Proxies**
  - Why needed here: MFL trains R using gradients through E (emulator) rather than M directly. Understanding why differentiable proxies enable cheaper optimization is essential.
  - Quick check question: Why might querying a neural network emulator 1000 times be preferable to querying a physical semiconductor simulator 1000 times, even if the emulator is imperfect?

## Architecture Onboarding

- **Component map:**
  Machine Model M -> Emulator E -> Reverse Model R -> Sensitivity Monitor

- **Critical path:**
  1. Collect paired data {(xᵢ, zᵢ)} from machine M (historical runs or designed experiments)
  2. Train emulator E with domain randomization (Line 2)
  3. Initialize reverse model R with random weights θ₀
  4. Loop A (T iterations): Update θ using gradients through E
  5. Loop B (τ iterations): Update θ using gradients through actual M
  6. Deploy R to generate recipes for new targets Z'

- **Design tradeoffs:**
  - **Emulator accuracy vs. training cost**: Better emulator (more data, larger network) reduces Loop B iterations but increases upfront cost.
  - **Loop A length (T) vs. Loop B length (τ)**: Longer Loop A reduces needed Loop B, but if E poorly approximates M, extended Loop A wastes computation.
  - **Sensitivity threshold δ**: Lower δ = more conservative = slower but stabler; higher δ = faster but riskier in sensitive regions.
  - **Reverse model capacity**: Larger R captures more complex inverse mappings but risks overfitting; paper uses small 7 kB model.

- **Failure signatures:**
  - **Diverging loss in Loop A**: Learning rate α₁ too high or emulator E is poorly trained. Check E's prediction error on held-out data first.
  - **Loop B requires many iterations (>50)**: Emulator-machine gap too large. Consider retraining E with more data or architecture changes.
  - **Generated recipes violate constraints (Table 2)**: R not properly regularized; add constraint satisfaction as penalty term or use constrained optimization.
  - **High variance across random seeds (Figure 5 shaded regions)**: R initialization sensitivity; try pre-training R on historical input-output pairs before MFL.

- **First 3 experiments:**
  1. **Emulator validation**: Train E on 80% of historical data, validate MSE on 20% held-out. Target: MSE < threshold acceptable for your precision requirements. If MSE high, increase model capacity or collect more diverse training data.
  2. **Ablate Loop B**: Run MFL with only Loop A (set τ = 0) and compare to full MFL on held-out targets. Quantify how much Loop B improves accuracy vs. additional machine queries.
  3. **Sensitivity threshold sweep**: Run MFL with δ ∈ {0.5, 0.9, 1.5} (Table 10 uses 0.9) on same target set. Plot convergence speed vs. final error to select appropriate δ for your stability requirements.

## Open Questions the Paper Calls Out

- **Question:** Can the Model Feedback Learning (MFL) framework be effectively extended to reinforcement learning (RL) and online learning settings?
- **Basis in paper:** [explicit] Remark 2 states that while the current focus is supervised learning, "the MFL framework can be adapted to scenarios such as online learning, offline learning, and reinforcement learning. The exploration of these extensions is deferred to future work."
- **Why unresolved:** The paper currently validates the method only on supervised-style recipe generation tasks. RL settings involve non-stationary reward landscapes and exploration-exploitation trade-offs not present in the current formulation.
- **What evidence would resolve it:** Successful application of MFL to standard RL benchmarks or simulated control tasks (e.g., robotics) demonstrating stable convergence and sample efficiency.

## Limitations

- **Emulator fidelity dependence:** MFL's success heavily depends on the emulator E providing a sufficiently accurate approximation of the true machine model M, with no clear characterization of performance degradation when emulator fidelity decreases.
- **Sensitivity threshold selection:** The conservative learning mechanism's effectiveness relies on appropriate sensitivity threshold δ selection, which appears to be empirically chosen (δ=0.9) rather than theoretically derived.
- **Simulation-to-real gap:** The method is validated on simulators rather than physical hardware, raising questions about performance in real-world semiconductor manufacturing environments with additional stochasticity and sensor noise.

## Confidence

- **High confidence:** The core two-loop training architecture and its motivation (reducing expensive machine queries) is well-supported by the ablation studies and convergence plots. The data efficiency claims (5 iterations for plasma etching) are directly demonstrated with error bars across random seeds.
- **Medium confidence:** The stability improvements from sensitivity-aware conservative learning are demonstrated but could benefit from more systematic sensitivity threshold ablation studies across different applications.
- **Medium confidence:** The domain randomization's contribution to robustness is shown through controlled noise experiments, but the real-world applicability depends on matching training noise distribution to deployment conditions.

## Next Checks

1. **Emulator fidelity ablation:** Systematically vary emulator accuracy (by training with different dataset sizes or architectures) and measure corresponding Loop B iteration requirements to quantify the emulator-machine gap impact.
2. **Sensitivity threshold sweep:** Test MFL across a range of sensitivity thresholds (δ ∈ [0.5, 1.5]) on all three applications (plasma etching, CVD, wire bonding) to identify optimal values and robustness to threshold selection.
3. **Cross-domain transfer:** Apply MFL to a non-semiconductor domain (e.g., chemical process control or additive manufacturing) to validate the claimed broad applicability beyond the primary validation applications.