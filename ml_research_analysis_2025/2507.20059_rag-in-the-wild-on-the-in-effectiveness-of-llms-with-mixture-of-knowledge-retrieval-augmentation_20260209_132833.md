---
ver: rpa2
title: 'RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge
  Retrieval Augmentation'
arxiv_id: '2507.20059'
source_url: https://arxiv.org/abs/2507.20059
tags:
- retrieval
- arxiv
- llama-3
- mmlu
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effectiveness of retrieval-augmented generation
  (RAG) in realistic, multi-domain scenarios using a large datastore with diverse
  knowledge sources. The study finds that retrieval primarily benefits smaller language
  models, with diminishing returns as model size increases, except for factuality-focused
  tasks.
---

# RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation

## Quick Facts
- arXiv ID: 2507.20059
- Source URL: https://arxiv.org/abs/2507.20059
- Authors: Ran Xu; Yuchen Zhuang; Yue Yu; Haoyu Wang; Wenqi Shi; Carl Yang
- Reference count: 19
- Primary result: Retrieval augmentation benefits smaller models significantly but provides diminishing returns for larger models, except on factuality-focused tasks.

## Executive Summary
This paper evaluates retrieval-augmented generation (RAG) in realistic multi-domain scenarios using a massive heterogeneous datastore. The study finds that retrieval primarily benefits smaller language models, with larger models showing minimal gains except on factuality-focused tasks. Reranking techniques provide only marginal improvements, and no single knowledge source consistently outperforms others across query types. Current large language models struggle to effectively route queries across heterogeneous knowledge sources, highlighting the need for better query routing mechanisms for practical RAG deployment.

## Method Summary
The paper evaluates RAG systems on six benchmarks (MMLU, MMLU-Pro, ARC-Challenge, SciQ, CSBench, SimpleQA) using a MassiveDS datastore containing 1.4T tokens across 10 heterogeneous corpora. Six backbone models spanning different scales are tested: Llama-3.2-3B, Llama-3.1-8B, Qwen3-4B/8B/32B, and GPT-4o-mini/4o. The evaluation uses zero-shot generation with k=5 retrieved passages, employing bge-base-en-v1.5 for retrieval and bge-reranker-v2-m3 for optional reranking. Relative performance gains are measured as Δ(ps) = (ps - ρ)/ρ where ps is RAG performance and ρ is no-retrieval baseline.

## Key Results
- Retrieval augmentation provides substantial gains (+22.87% on STEM MMLU) for smaller models (Llama-3.2-3B) but minimal gains (+0.34%) for larger models (GPT-4o)
- Reranking provides only marginal improvements (e.g., SimpleQA from 29.6% to 32.0% for Llama-3.2-3B)
- No single knowledge source consistently outperforms others across all query types
- Current LLM-based routers underperform "all sources" baseline, sometimes falling below no-retrieval performance

## Why This Works (Mechanism)

### Mechanism 1: Model Scale Determines Retrieval Benefit
- Claim: RAG provides substantial gains for smaller models but shows diminishing returns for larger models, except on factuality-focused tasks.
- Mechanism: Smaller models have limited parametric knowledge capacity; retrieval compensates by providing task-relevant context at inference time. Larger models internalize more knowledge during pretraining, reducing marginal benefit from external context.
- Core assumption: Task-relevant knowledge overlaps substantially with what larger models learned during pretraining.
- Evidence anchors: [abstract] "retrieval mainly benefits smaller models"; [section] Table 1 shows Llama-3.2-3B gains +22.87% on STEM MMLU with retrieval, while GPT-4o gains only +0.34%.

### Mechanism 2: Source Heterogeneity Requires Adaptive Routing
- Claim: No single knowledge source consistently outperforms others across query types; optimal performance requires matching queries to appropriate corpora.
- Mechanism: Different domains have distinct lexical and semantic distributions. Generic retrieval across all sources may introduce noise; targeted retrieval to mismatched sources returns irrelevant passages.
- Core assumption: Query intent can be reliably mapped to corpus type before retrieval occurs.
- Evidence anchors: [abstract] "no single retrieval source consistently excels"; [section] Figure 2 shows 8%–39% of cases for Llama-3.1-8B depend exclusively on retrieval from specific corpora.

### Mechanism 3: Reranking Cannot Compensate for Fundamental Retrieval-Generation Mismatch
- Claim: Improving retrieval quality through reranking yields marginal gains because the bottleneck lies in retrieval coverage and model integration, not passage ordering.
- Mechanism: Reranking refines top-k selection but does not address: (1) retriever's limited access to relevant knowledge in heterogeneous corpora, and (2) the model's ability to synthesize retrieved context with parametric knowledge.
- Core assumption: Retrieved top-k passages contain relevant information that reranking can surface.
- Evidence anchors: [abstract] "rerankers add minimal value"; [section] Figure 3 shows reranking improves SimpleQA from 29.6% to 32.0% for Llama-3.2-3B (marginal).

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: This paper evaluates RAG specifically in heterogeneous, multi-domain settings rather than sanitized Wikipedia-based benchmarks.
  - Quick check question: Why might a 3B model benefit more from RAG than a 70B model on the same factual query?

- Concept: Query Routing / Source Selection
  - Why needed here: The paper finds current LLMs struggle to route queries to appropriate knowledge sources, a critical failure mode for real-world deployment.
  - Quick check question: Given a query about "CRISPR gene editing patent law," which of PubMed, Arxiv, C4, or CommonCrawl would be most relevant?

- Concept: Reranking in Retrieval Pipelines
  - Why needed here: The paper shows reranking provides marginal improvements, challenging the assumption that retrieval quality is the primary bottleneck.
  - Quick check question: If reranking improves top-k precision but downstream task accuracy does not improve, what does this suggest about the system bottleneck?

## Architecture Onboarding

- Component map: Query → Retriever (bge-base-en-v1.5) → [Optional: Reranker (bge-reranker-v2-m3)] → Top-k passages → LLM Backbone → Answer

- Critical path:
  1. Query encoding and retrieval from MassiveDS (1.4T tokens across 10 corpora)
  2. Passage deduplication and filtering (following Shao et al., 2024)
  3. Context formatting with k=5 passages
  4. Zero-shot generation by LLM backbone

- Design tradeoffs:
  - Unified vs. routed retrieval: Unified retrieval is simpler but may introduce noise; routing requires accurate source prediction, which current LLMs fail at
  - Reranking: Adds latency (retrieve 30 → rerank → select 5) for marginal accuracy gains (1–3%)
  - Model scale vs. retrieval dependency: Larger models need less retrieval but have higher per-query cost

- Failure signatures:
  - Negative retrieval gains on large models (GPT-4o: -1.08% on Social Sciences MMLU with retrieval)
  - LLM router underperforms "all sources" baseline, occasionally falling below no-retrieval
  - High variance across source-specific retrieval (Wikipedia helps SimpleQA significantly; PubMed does not)

- First 3 experiments:
  1. Baseline measurement: Run your task on models of different scales (e.g., 3B, 8B, 32B) with and without retrieval to quantify scale-dependent gains on your domain.
  2. Source ablation: For a sample of queries, measure performance when retrieving from each individual corpus vs. all corpora combined to identify which sources matter.
  3. Router evaluation: Test whether a classifier or LLM can predict optimal source per query; compare against "all sources" baseline and random selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learned routing modules trained with supervision or reinforcement learning effectively select among heterogeneous knowledge sources where prompt-based LLM routing currently fails?
- Basis in paper: [explicit] The authors state in Section 2.5 that "Future directions should focus on learned routing modules trained with supervision or reinforcement learning" because current LLMs struggle to route queries effectively using plain or chain-of-thought prompting.
- Why unresolved: The paper only evaluated prompt-based routing strategies (plain and CoT), which consistently underperformed compared to static "all-source" retrieval; the effectiveness of trained routing architectures remains untested.
- What evidence would resolve it: Performance comparison showing that a trained router (using RL or supervision) achieves higher accuracy than the static "all sources" baseline or the oracle upper bound on the MMLU/MMLU-Pro datasets.

### Open Question 2
- Question: Does jointly optimizing source selection and generation improve RAG effectiveness for large LLMs, specifically addressing the integration challenges where reranking offered marginal gains?
- Basis in paper: [inferred] The paper notes in Section 2.4 that reranking provided minimal value, suggesting "deeper integration challenges," and concludes that future work requires "tighter integration between retrieval and generation" rather than improving retrieval quality alone.
- Why unresolved: The current evaluation treats retrieval (and reranking) as a separate step from generation, and the results imply that better retrieval scores do not translate to better generation for large models.
- What evidence would resolve it: Implementation of an end-to-end differentiable RAG system that jointly learns to select sources and generate answers, demonstrating significant gains over the modular pipeline described in the paper.

### Open Question 3
- Question: Do the diminishing returns of retrieval augmentation for large LLMs persist in open-ended generation or long-form reasoning tasks, given the current study focused on short-form QA?
- Basis in paper: [explicit] The "Limitations" section states the study "primarily targets question answering tasks with short-form answers," which may limit applicability to "open-ended generation or long-form reasoning."
- Why unresolved: It is unclear if the finding—that large models internalize knowledge sufficiently to negate retrieval benefits—applies when the required output is more complex and requires synthesizing information over longer contexts.
- What evidence would resolve it: Evaluation of the same mixture-of-knowledge RAG setup on long-form generation benchmarks (e.g., report generation) showing whether large models still exhibit diminishing returns relative to smaller models.

## Limitations
- Unknown chunking and preprocessing strategy for MassiveDS datastore construction
- Limited empirical validation of routing failure mechanisms
- Reranker contribution assessment may not explore alternative approaches or parameter configurations

## Confidence

- **High confidence**: The scale-dependent benefit of RAG (smaller models benefit more) is well-supported by consistent results across multiple benchmarks and models.
- **Medium confidence**: The claim that no single knowledge source consistently excels is supported but may be domain-dependent and could vary with different retrieval configurations.
- **Low confidence**: The assertion that current LLMs struggle to route queries effectively is primarily qualitative and would benefit from more systematic experimental validation.

## Next Checks

1. **Scale-benefit verification**: Replicate the core finding that RAG provides diminishing returns as model size increases by testing the same query set across at least three different model scales (e.g., 3B, 8B, 70B).

2. **Source-specific performance analysis**: Conduct source ablation studies on your domain-specific queries to identify which knowledge sources provide the most value for your particular use case.

3. **Router mechanism evaluation**: Implement and test both LLM-based and classifier-based query routing approaches to verify whether routing performance consistently falls below the "all sources" baseline across different query distributions.