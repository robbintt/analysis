---
ver: rpa2
title: Exploring Solution Divergence and Its Effect on Large Language Model Problem
  Solving
arxiv_id: '2509.22480'
source_url: https://arxiv.org/abs/2509.22480
tags:
- solution
- divergence
- pass
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates solution divergence as a novel factor for
  improving LLM problem-solving. The authors define solution divergence as the variety
  of correct solutions an LLM generates for a single problem and find a positive correlation
  between divergence and performance.
---

# Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving

## Quick Facts
- arXiv ID: 2509.22480
- Source URL: https://arxiv.org/abs/2509.22480
- Reference count: 18
- One-line primary result: Solution divergence improves LLM problem-solving by encouraging diverse valid solutions during training

## Executive Summary
This paper investigates solution divergence as a novel factor for improving LLM problem-solving. The authors define solution divergence as the variety of correct solutions an LLM generates for a single problem and find a positive correlation between divergence and performance. They propose two methods: using divergence for dataset selection in SFT and incorporating it into RL reward functions. Experiments on Math-500, MBPP+, and a newly introduced Maze dataset show that training with higher-divergence samples and using divergence-augmented RL rewards consistently improves Pass@1 and Pass@10 scores across multiple models.

## Method Summary
The method involves quantifying solution divergence using eigenvalues of the Laplacian matrix of a solution similarity graph, where similarity is based on normalized string edit distance. Two training strategies are proposed: (1) SFT dataset selection using high-divergence samples, and (2) RL training with a reward function that incentivizes solution diversity. The divergence metric is calculated as the sum of eigenvalues minus their mean, capturing the spectral properties of the solution relationship graph. The RL reward function augments correctness rewards with diversity terms based on pairwise solution divergence.

## Key Results
- Training on high-divergence SFT data improves Pass@10 scores in 10 out of 12 cases compared to low-divergence baselines
- Divergence-augmented RL rewards improve Pass@10 scores over standard rewards in 11 out of 12 cases
- The positive correlation between solution divergence and performance is strongest for medium-difficulty problems
- Models trained with divergence methods show consistent improvements across Math-500, MBPP+, and Maze datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting SFT data based on solution divergence improves problem-solving performance
- Mechanism: By selecting training samples that maximize the divergence metric, models are exposed to a broader repertoire of valid problem-solving strategies during SFT, improving generalization
- Core assumption: Normalized string edit distance effectively captures strategic or semantic divergence between solutions
- Evidence anchors:
  - [abstract] Using divergence for SFT "consistently improves Pass@1 and Pass@10 scores"
  - [section] Table 2 shows high-divergence SFT outperforms low-divergence baselines in 10/12 cases for Pass@10
  - [corpus] Related work on data curation for problem-solving tasks corroborates importance

### Mechanism 2
- Claim: RL reward function that incentivizes solution diversity enhances exploration of solution space
- Mechanism: The divergence-augmented reward function combines correctness with diversity terms, creating a direct training signal for generating diverse valid outputs
- Core assumption: Rewarding diversity will encourage learning multiple distinct solution paths without degrading correctness
- Evidence anchors:
  - [abstract] Incorporating divergence into RL rewards improves success rates
  - [section] Table 3 shows divergence-augmented reward improves Pass@10 in 11/12 cases
  - [corpus] This is a novel contribution not found in related RL literature

### Mechanism 3
- Claim: Correlation between divergence and performance is strongest for medium-difficulty problems
- Mechanism: Analysis across difficulty levels shows steepest regression slope between divergence and Pass@1 for medium-difficulty subset
- Core assumption: Cognitive science principles about strategy diversity and problem difficulty apply to LLMs
- Evidence anchors:
  - [section] Figure 3 shows steepest slope for medium-difficulty subset across all datasets
  - [abstract] Mentions positive correlation between divergence and performance
  - [corpus] No related papers discuss difficulty-dependent correlation for LLMs

## Foundational Learning

- Concept: Graph Laplacian and Spectral Clustering
  - Why needed here: The divergence metric is derived from eigenvalues of the Laplacian matrix of a solution similarity graph
  - Quick check question: If the eigenvalues of a graph Laplacian are all small, what does that imply about the connectivity of the nodes in the graph?

- Concept: Group-based Reinforcement Learning (e.g., GRPO, DAPO)
  - Why needed here: The divergence reward is integrated into a group-based RL framework
  - Quick check question: In a group-based RL setup, how is the advantage (Â_{i,t}) typically calculated for a given token?

- Concept: Pass@k Metric
  - Why needed here: The paper evaluates performance using Pass@1 and Pass@10 metrics
  - Quick check question: Why might a model have a low Pass@1 score but a high Pass@10 score?

## Architecture Onboarding

- Component map:
  - Divergence Calculator -> SFT Data Selector -> SFT Model -> RL Training Loop -> Divergence-Augmented Reward Function

- Critical path:
  1. Generate candidate solution sets for each question
  2. **SFT Stage**: Use Divergence Calculator to select highest-scoring subset. Fine-tune base model on this high-divergence dataset
  3. **RL Stage**: Initialize from SFT model. Sample solution groups for each question
  4. Calculate rewards using Divergence-Augmented Reward Function
  5. Update model policy via RL Training Loop based on these rewards

- Design tradeoffs:
  - Metric Proxy: String edit distance chosen for computational efficiency vs semantic richness tradeoff
  - Reward Scaling (α): Controls balance between correctness vs diversity; too high risks convoluted solutions
  - Oversampling: Required for consistent Laplacian computation but may bias metric if solutions aren't representative

- Failure signatures:
  - Convoluted Outputs: Model learns to produce correct but overly complex solutions to inflate string edit distance
  - Reward Hacking: Model generates token-wise different but semantically identical solutions
  - Performance Collapse: Too strong diversity reward causes sacrifice of core correctness

- First 3 experiments:
  1. Implement Divergence Calculator and test on hand-crafted solutions to verify it ranks true diversity higher than superficial differences
  2. Train two small models on Math-500: one with random selection, one with divergence-based selection. Compare Pass@1/10 scores
  3. Run RL training comparing baseline vs divergence-augmented reward on logical reasoning task. Monitor Pass@10 improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantic or structural metrics capture solution divergence more effectively than normalized string edit distance?
- Basis in paper: [explicit] Page 2 states "More advanced proxy metrics will be explored as one future work," noting string edit distance offers only a "restricted perspective"
- Why unresolved: Current metric relies on surface-level text overlap, potentially misclassifying semantically identical solutions as divergent
- What evidence would resolve it: Comparative experiments using embedding-based similarity or ASTs for code to evaluate stronger correlation with performance

### Open Question 2
- Question: Does increasing dataset size recover performance gains of divergence-based training in programming tasks?
- Basis in paper: [explicit] Page 7 attributes MBPP+ lack of improvement to "limited size," causing "rapid overfitting and early stopping"
- Why unresolved: Unclear if failure is intrinsic to code generation or methodological artifact of small training set (98 questions)
- What evidence would resolve it: Replicating SFT experiments on larger-scale coding dataset to determine if high-divergence samples improve Pass@1 when overfitting is mitigated

### Open Question 3
- Question: How can divergence-fused reward be optimized to maintain or improve Pass@1 accuracy while maximizing Pass@10?
- Basis in paper: [explicit] Page 8 notes divergence-augmented reward often improves Pass@10 but sometimes lowers Pass@1 compared to binary success reward
- Why unresolved: Current formulation encourages exploration potentially at cost of exploiting most probable correct solution
- What evidence would resolve it: Analyzing reward curves and optimizing scaling factor α or using constrained optimization to ensure Pass@1 doesn't degrade

## Limitations

- The divergence metric based on normalized string edit distance may conflate superficial token-level differences with true semantic diversity
- Maze dataset generation lacks full specification of blocking sets and target distributions, limiting reproducibility
- Small MBPP+ dataset size (98 samples) raises concerns about overfitting and generalizability of results

## Confidence

- **High Confidence**: Empirical finding that training on higher-divergence SFT data consistently improves Pass@10 scores across all three datasets
- **Medium Confidence**: Theoretical mechanism that diversity improves problem-solving by exposing models to varied strategies
- **Low Confidence**: Claim that correlation between divergence and performance is strongest for medium-difficulty problems

## Next Checks

1. Implement the divergence calculator and test it on hand-crafted solution sets with known semantic relationships to verify the metric correctly ranks true semantic diversity higher than superficial differences

2. Apply the divergence-augmented RL reward to a different problem domain (e.g., logical reasoning, commonsense QA) not covered in original experiments to test mechanism generalization

3. Replace normalized string edit distance with a semantic similarity metric (e.g., using code ASTs for MBPP+, embedding-based similarity for math) and re-run SFT selection experiment to compare improvements with more meaningful diversity measure