---
ver: rpa2
title: 'You Never Know a Person, You Only Know Their Defenses: Detecting Levels of
  Psychological Defense Mechanisms in Supportive Conversations'
arxiv_id: '2512.15601'
source_url: https://arxiv.org/abs/2512.15601
tags:
- defense
- level
- defenses
- dmrs
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PSYDEFCONV, the first dialogue corpus with
  help-seeker utterances labeled for DMRS-based psychological defense levels. The
  corpus contains 200 dialogues and 4,709 utterances, with 2,336 help-seeker turns
  annotated with Cohen's kappa 0.639.
---

# You Never Know a Person, You Only Know Their Defenses: Detecting Levels of Psychological Defense Mechanisms in Supportive Conversations

## Quick Facts
- arXiv ID: 2512.15601
- Source URL: https://arxiv.org/abs/2512.15601
- Reference count: 37
- Primary result: Introduced PSYDEFCONV corpus with DMRS-labeled defense mechanisms in 200 dialogues (2,336 help-seeker utterances), achieving 22.4% annotation time reduction with evidence-based pre-annotation system

## Executive Summary
This paper presents the first dialogue corpus annotated for psychological defense mechanisms (PSYDEFCONV) and introduces DMRS CO-PILOT, an evidence-based annotation system. The corpus contains 200 supportive conversations with help-seeker utterances labeled using the Defense Mechanism Rating Scale (DMRS) taxonomy. The authors demonstrate that their pre-annotation approach significantly reduces annotation time while maintaining quality, and provide comprehensive language model benchmarks showing the challenging nature of this classification task.

## Method Summary
The authors developed PSYDEFCONV, a corpus of 200 supportive conversations with 4,709 utterances, where 2,336 help-seeker turns were annotated for psychological defense levels using DMRS taxonomy. They implemented DMRS CO-PILOT, a four-stage pipeline that provides evidence-based pre-annotations to assist human annotators. The system was evaluated through a counterbalanced study measuring annotation time reduction and expert review assessing evidence quality, plausibility, and insight. Language models were benchmarked on this task, establishing baselines for future research.

## Key Results
- Achieved Cohen's kappa of 0.639 for inter-annotator agreement on defense mechanism classification
- Reduced average annotation time by 22.4% using evidence-based pre-annotation system
- Language model benchmarks show best macro F1-score around 30%, with models tending to overpredict mature defenses
- Expert reviewers rated the system 4.62/7 for evidence, 4.44/7 for plausibility, and 4.40/7 for insight

## Why This Works (Mechanism)
The approach works by providing annotators with evidence-based pre-annotations that guide their judgment of psychological defense mechanisms in conversation. The four-stage pipeline systematically processes dialogue context, identifies potential defense mechanisms, and presents supporting evidence to annotators. This scaffolding reduces cognitive load and decision uncertainty, leading to faster annotations without sacrificing quality. The evidence-based approach leverages established psychological frameworks while adapting them to the conversational context of supportive dialogues.

## Foundational Learning
1. **Defense Mechanism Rating Scale (DMRS)** - A taxonomy for classifying psychological defenses (why needed: provides standardized framework for annotation; quick check: can you list the three defense levels?)
2. **Cohen's Kappa** - Inter-rater reliability measure (why needed: quantifies annotation agreement; quick check: what does kappa of 0.639 indicate?)
3. **Counterbalanced study design** - Experimental method comparing conditions (why needed: isolates effect of pre-annotation; quick check: what was the control condition?)
4. **Macro F1-score** - Classification performance metric (why needed: evaluates model across all classes equally; quick check: why is ~30% considered challenging?)
5. **Evidence-based pre-annotation** - Using supporting evidence to guide annotations (why needed: reduces cognitive load; quick check: how does this differ from standard pre-annotation?)
6. **Supportive conversation context** - Dialogues focused on emotional support (why needed: specific domain affects defense mechanism expression; quick check: how might this bias the corpus?)

## Architecture Onboarding

Component map: Pre-processing -> Evidence Generation -> Defense Classification -> Expert Review

Critical path: Evidence Generation -> Defense Classification -> Expert Review (annotation workflow)

Design tradeoffs: Prioritized annotation efficiency over perfect classification accuracy, accepting moderate kappa agreement for practical utility

Failure signatures: Low inter-annotator agreement indicates ambiguous cases; model overfitting to corpus patterns suggests domain specificity

First experiments:
1. Test inter-annotator agreement on a small validation subset using independent raters
2. Measure annotation time difference between pre-annotation and baseline conditions
3. Evaluate model performance on out-of-distribution supportive conversations

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate inter-annotator agreement (Cohen's kappa 0.639) suggests significant subjectivity in classification
- Reliance on single DMRS-based taxonomy without external clinical validation
- Language model performance remains challenging at ~30% macro F1-score
- Corpus may be biased toward mature defenses due to supportive conversation context

## Confidence
- Corpus reliability: Medium confidence (moderate kappa, limited validation)
- Annotation efficiency improvement: High confidence (controlled study design)
- Model performance assessment: High confidence (multiple benchmarks, clear baselines)
- Expert evaluation of system: Medium confidence (subjective ratings, limited sample)

## Next Checks
1. Conduct cross-validation with clinicians using independent psychological assessments to verify the accuracy of defense mechanism labels in PSYDEFCONV
2. Test DMRS CO-PILOT's annotation efficiency in real-world deployment with varied annotator expertise levels over extended periods
3. Evaluate model performance on out-of-distribution conversations (e.g., clinical transcripts, conflict dialogues) to assess generalizability beyond supportive conversations