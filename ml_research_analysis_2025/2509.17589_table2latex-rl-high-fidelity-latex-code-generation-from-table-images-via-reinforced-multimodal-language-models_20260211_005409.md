---
ver: rpa2
title: 'Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via
  Reinforced Multimodal Language Models'
arxiv_id: '2509.17589'
source_url: https://arxiv.org/abs/2509.17589
tags:
- table
- latex
- code
- tables
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of converting table images into
  LaTeX code, a task essential for high-quality document digitization but challenging
  due to the complexity of nested table structures. The authors propose a method combining
  multimodal large language models with a dual-reward reinforcement learning framework
  (VSGRPO), which optimizes both structural accuracy and visual fidelity.
---

# Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models

## Quick Facts
- arXiv ID: 2509.17589
- Source URL: https://arxiv.org/abs/2509.17589
- Authors: Jun Ling; Yao Qi; Tao Huang; Shibo Zhou; Yanqin Huang; Jiang Yang; Ziqi Song; Ying Zhou; Yang Yang; Heng Tao Shen; Peng Wang
- Reference count: 40
- Key outcome: State-of-the-art table image to LaTeX conversion, particularly for complex tables, achieving CW-SSIM 0.6145 and TEDS-Structure 0.9218

## Executive Summary
This paper addresses the challenge of converting complex table images into high-fidelity LaTeX code, a task crucial for document digitization but hindered by LaTeX's syntactic ambiguity and nested table structures. The authors propose Table2LaTeX-RL, which combines multimodal large language models with a dual-reward reinforcement learning framework (VSGRPO) that optimizes both structural accuracy and visual fidelity. By rendering generated LaTeX to images and computing CW-SSIM against ground-truth renders, the method bypasses LaTeX's syntactic ambiguity and directly optimizes visual output quality. The approach achieves state-of-the-art performance, particularly on complex tables, by training reinforcement learning exclusively on complex tables to improve generalization across all complexity levels.

## Method Summary
Table2LaTeX-RL employs a two-stage training approach: first, supervised fine-tuning (SFT) on 1.2M image-LaTeX pairs from arXiv for one epoch to initialize the model; second, VSGRPO reinforcement learning on 5,936 complex tables only, using dual rewards (structural via TEDS-Structure on HTML-converted LaTeX and visual via CW-SSIM on rendered images). The method uses Group Relative Policy Optimization (GRPO) to eliminate the value network and reduce memory overhead, sampling multiple outputs per input and normalizing rewards within groups. The model is built on Qwen2.5-VL-3B or InternVL2-1B base models, with a maximum output length of 8192 tokens and temperature 0 during inference.

## Key Results
- Achieves CW-SSIM score of 0.6145 and TEDS-Structure score of 0.9218 on complex tables, outperforming existing methods
- VSGRPO trained exclusively on complex tables generalizes better across all complexity levels than training on simple or mixed datasets
- 3B model with VSGRPO outperforms 72B/78B baselines on complex tables (TEDS-Structure 0.9218 vs 0.8334), demonstrating RL compensates for scale
- Dual-reward optimization significantly improves performance compared to visual-only or structure-only training (CW-SSIM 0.6145 vs 0.6064 with visual-only)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual-in-the-loop reinforcement learning aligns training objectives with actual output quality by bypassing LaTeX's syntactic ambiguity.
- Mechanism: LaTeX allows semantically equivalent but syntactically different code (e.g., `\textbf{x}` vs. `{\bf x}`, or redundant `{}` wrappers). Text-based metrics penalize these differences even when rendered outputs are identical. By rendering generated LaTeX to images and computing CW-SSIM against ground-truth renders, the reward signal directly reflects visual fidelity rather than token-level matching, correcting the mismatch between training and evaluation.
- Core assumption: Rendering pipeline is deterministic and LaTeX compilation failures are informative negative signals rather than noise.
- Evidence anchors:
  - [abstract] "our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality"
  - [Section 4.3] "the SFT objective focuses solely on text-level alignment and completely ignores the visual similarity between the rendered LaTeX output and the original table image"
  - [Section A, Figure 2-3] Demonstrates visually identical tables with different TEDS scores (0.8047 vs. 1.0) due to syntactic variations
  - [corpus] Related work on Img2LaTeX conversion acknowledges VLM progress but doesn't address this specific ambiguity problem
- Break condition: If rendering latency or compilation failures dominate training time (>50% overhead), the mechanism becomes impractical.

### Mechanism 2
- Claim: Dual-reward optimization provides complementary supervision signals—structure reward captures global layout correctness, visual reward captures local rendering fidelity.
- Mechanism: TEDS-Structure computes tree edit distance on HTML-converted tables, penalizing structural errors (missing rows, wrong cell spans) but ignoring content. CW-SSIM operates on wavelet decompositions of rendered images, capturing local visual patterns but potentially missing high-level structural errors. When combined, they provide coverage across error types.
- Core assumption: HTML conversion from LaTeX preserves structural semantics sufficiently for TEDS-Structure to be meaningful.
- Evidence anchors:
  - [abstract] "dual-reward reinforcement learning strategy...incorporates both a structure-level reward on LaTeX code and a visual fidelity reward"
  - [Table 6] Ablation shows CW-SSIM-only achieves 0.6064 CW-SSIM but 0.9133 TEDS-Structure; dual-reward achieves 0.6145 and 0.9218 respectively—both improve
  - [corpus] Visual-TableQA benchmark also emphasizes need for multimodal evaluation, supporting dual-metric approach
- Break condition: If one reward saturates (e.g., all outputs exceed threshold), gradient signal vanishes for that component.

### Mechanism 3
- Claim: Training reinforcement learning exclusively on complex tables improves generalization across all complexity levels.
- Mechanism: Complex tables contain the structural patterns (nested `\multirow`/`\multicolumn`, 160+ cells) that simple tables lack. By concentrating RL capacity on these edge cases, the model learns transferable structural reasoning without wasting updates on already-solved simple patterns.
- Core assumption: Complex-table patterns subsume simple-table patterns (structural hierarchy holds).
- Evidence anchors:
  - [Section 4.1] Tables classified by complexity: complex = 160+ cells AND 2+ merge commands
  - [Table 5] Training on simple-only achieves 0.5993 CW-SSIM on complex tables; complex-only achieves 0.6145—a 2.5% absolute improvement
  - [Section 5.4] "restricting the RL fine-tuning data to complex tables leads to the best overall performance across all metrics"
  - [corpus] RealHiTBench similarly notes existing benchmarks focus on simple structures, missing real-world complexity
- Break condition: If test distribution shifts toward simple tables with qualitatively different layouts, complex-only training could overfit.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: This is the RL backbone replacing standard RLHF/PPO. Unlike PPO, GRPO eliminates the value network by computing advantages relative to group means, reducing memory overhead.
  - Quick check question: Can you explain why GRPO samples multiple outputs per input and normalizes rewards within groups rather than using a learned value function?

- Concept: **LaTeX table semantics (tabular environment, multirow/multicolumn)**
  - Why needed here: Understanding what makes tables "complex" (merged cells, nesting) is prerequisite to designing reward thresholds and interpreting failure modes.
  - Quick check question: Given a table with `\multirow{3}{*}{A}` spanning 3 rows and `\multicolumn{2}{c}{B}` spanning 2 columns, sketch the HTML tree structure TEDS-Structure would compute.

- Concept: **CW-SSIM for binary document images**
  - Why needed here: Standard SSIM assumes natural image statistics; tables are high-contrast, sparse. The Haar wavelet preprocessing adapts SSIM to this domain.
  - Quick check question: Why does the paper use 2×2 block decomposition with Haar wavelets rather than applying SSIM directly to pixel intensities?

## Architecture Onboarding

- Component map:
  Input Image → Vision Encoder (MLLM backbone) → Language Decoder → LaTeX Output → Compile → PDF → PNG → CW-SSIM (Visual Reward)
  LaTeX Output → HTML Convert → TEDS-Structure (Structure Reward)
  Combined Reward → Policy Update

- Critical path:
  1. **SFT phase**: Fine-tune base MLLM (Qwen2.5-VL-3B or InternVL2-1B) on 1.2M image-LaTeX pairs for 1 epoch. This provides initialization.
  2. **VSGRPO phase**: Sample N outputs per input (N=4-8), render each, compute dual rewards, update policy using group-relative advantages. Train on 5,936 complex tables only.
  3. **Inference**: Single forward pass with temperature=0, max_length=8192 tokens.

- Design tradeoffs:
  - **Model size**: 3B model with VSGRPO outperforms 72B/78B baselines on complex tables (TEDS-Structure 0.9218 vs 0.8334), suggesting RL compensates for scale
  - **Reward thresholds**: 0.6 for CW-SSIM, 0.9 for TEDS-Structure—binary rewards incentivize pushing above threshold but don't differentiate beyond it
  - **Compilation environment**: Requires `latexlive-full` Docker; compilation failures set reward=0, creating sparse gradients

- Failure signatures:
  - **Compilation failure**: Reward=0 for both components; check for undefined macros, mismatched environments
  - **Structural collapse**: High CW-SSIM but low TEDS-Structure suggests correct visual rendering but wrong tree structure (e.g., extra empty rows)
  - **Training instability**: If KL divergence penalty (β=0.02) is too weak, policy may diverge from reference

- First 3 experiments:
  1. **Reproduce SFT baseline**: Fine-tune Qwen2.5-VL-3B on 10K subset for 1 epoch, evaluate TEDS-Structure on 100 complex tables. Target: ~0.90 (per Table 6).
  2. **Ablate reward components**: Train with visual-only vs structure-only rewards, compare to dual-reward on held-out complex set. Expect 1-3% gap per Table 6.
  3. **Test complexity generalization**: Train VSGRPO on medium tables only, evaluate on simple/complex splits. If complex performance drops significantly (>5%), confirms complex-only training necessity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does VSGRPO performance improve significantly when the reinforcement learning phase is scaled to the full dataset?
- Basis in paper: [explicit] The authors state they trained VSGRPO on only 5,936 complex tables due to "computational overhead" and "limited GPU resources."
- Why unresolved: The resource constraints prevented testing whether the current SOTA performance is the upper limit or if further data scaling yields substantial gains.
- What evidence would resolve it: Comparative results of the model trained on the full dataset versus the current subset.

### Open Question 2
- Question: Can the latency of the visual feedback loop be reduced by replacing the physical LaTeX-to-PDF rendering pipeline?
- Basis in paper: [explicit] The paper identifies the "time-consuming process" of compiling LaTeX and converting to PNG for CW-SSIM as a major "training bottleneck."
- Why unresolved: The method relies on actual rendering (a non-differentiable, slow process) to compute visual fidelity, limiting training efficiency.
- What evidence would resolve it: Implementation of a differentiable visual proxy or faster renderer that maintains reward accuracy while increasing throughput.

### Open Question 3
- Question: Would a continuous visual reward signal outperform the currently implemented binary threshold reward?
- Basis in paper: [inferred] The visual reward is currently binary (1 if CW-SSIM > 0.6, else 0). This sparse signal may not provide granular guidance for partially correct outputs compared to the structural reward.
- Why unresolved: The paper does not ablate the binary nature of the reward function against a continuous equivalent.
- What evidence would resolve it: An ablation study comparing the convergence rate and final score of binary rewards versus continuous CW-SSIM value rewards.

## Limitations
- The claim that dual-reward RL significantly improves complex-table performance hinges on the specific reward thresholds (0.6 CW-SSIM, 0.9 TEDS-Structure) being optimal; the paper doesn't explore sensitivity to these values or whether alternative formulations might perform better
- The generalization mechanism (complex-only RL training improving performance across all complexity levels) assumes structural patterns are hierarchical, but this isn't empirically validated—the paper shows correlation but not causation between complex training and simple-table performance
- The comparison with 72B/78B models showing competitive performance may be misleading since the paper doesn't control for pretraining data overlap or architectural differences beyond scale

## Confidence
- High confidence: The fundamental mechanism of visual-in-the-loop RL bypassing LaTeX syntactic ambiguity (Mechanism 1) is well-supported by empirical evidence and logical necessity
- Medium confidence: The dual-reward optimization providing complementary signals (Mechanism 2) is supported by ablation studies, but the specific threshold choices and their optimality remain unverified
- Medium confidence: The claim that complex-only RL training generalizes best (Mechanism 3) is demonstrated empirically but lacks deeper analysis of why this occurs or whether it would hold under distribution shift

## Next Checks
1. **Reward threshold sensitivity**: Systematically vary CW-SSIM and TEDS-Structure thresholds (0.5-0.7 and 0.8-0.95 respectively) during VSGRPO training and measure impact on both complex and simple table performance to verify thresholds aren't coincidentally optimal
2. **Training data distribution analysis**: Train VSGRPO on mixed complexity tables with proportional sampling and compare to complex-only training to determine if performance gains come from complexity focus or simply more diverse training data
3. **Cross-dataset generalization**: Evaluate the model on tables from different domains (e.g., financial reports, web tables) not present in the arXiv training corpus to test whether complex-table structural reasoning truly generalizes beyond scientific LaTeX tables