---
ver: rpa2
title: 'Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor
  Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets'
arxiv_id: '2505.11135'
source_url: https://arxiv.org/abs/2505.11135
tags:
- scheduling
- https
- different
- minifab
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparison of open-source semiconductor
  manufacturing models with real industry data to evaluate reinforcement learning
  (RL) methods for dispatching. The authors investigate scalability by testing policy-gradient
  and evolution strategies approaches across three models: Minifab, SMT2020, and a
  large-scale industry dataset.'
---

# Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets

## Quick Facts
- arXiv ID: 2505.11135
- Source URL: https://arxiv.org/abs/2505.11135
- Reference count: 40
- Reinforcement learning methods for semiconductor dispatching scale better with Evolution Strategies than policy-gradient approaches

## Executive Summary
This paper evaluates reinforcement learning methods for semiconductor dispatching using both open-source and proprietary industry datasets. The authors compare policy-gradient (PPO) and evolution strategies (CMA-ES) approaches across three models: Minifab, SMT2020, and a large-scale industry dataset. Their experiments reveal that CMA-ES scales significantly better than PPO, achieving up to 4% improvement in tardiness and 1% in throughput for the industry dataset. The research identifies bottleneck tool selection and diverse training datasets as crucial factors for optimization success, while noting the computational expense of scaling to complex industrial scenarios.

## Method Summary
The authors implement a reinforcement learning framework for semiconductor dispatching using a scaled dot-product attention neural network to score lots based on 9 features including time to due date, waiting time, and processing information. They compare two optimization approaches: Proximal Policy Optimization (PPO) with an actor-critic architecture, and Evolution Strategies using CMA-ES for black-box optimization. The framework is tested on three models: a modified Minifab open-source model, the SMT2020 competition dataset, and a proprietary industry dataset. Training involves parallel simulation environments running on multiple CPU cores, with CMA-ES sampling parameter perturbations and PPO collecting episodic experiences for policy updates.

## Key Results
- CMA-ES achieves 4% improvement in tardiness and 1% in throughput for the industry dataset, while PPO shows negative improvements
- Controlling bottleneck tools (lithography, diffusion, implantation) yields better results than controlling non-bottleneck equipment
- Diverse training datasets improve generalization across different loading scenarios and stochastic failure patterns
- Computational scaling is favorable with CPU cores, though training time increases significantly with controlled tool set size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolution Strategies (ES) scales better than policy-gradient (PPO) for semiconductor dispatching with delayed rewards and noisy KPIs.
- Mechanism: ES treats the problem as black-box optimization, receiving only episodic rewards rather than requiring step-wise credit assignment. CMA-ES adapts the covariance matrix to sample promising parameter regions, avoiding backpropagation through long trajectories. PPO, by contrast, must estimate value functions for intermediate states; when KPI feedback is sparse and effects of individual dispatching decisions compound over weeks of simulated time, value estimation becomes unreliable.
- Core assumption: The dispatching policy can be represented with a sufficiently small neural network that ES's parameter-space exploration remains tractable.
- Evidence anchors:
  - [abstract] "We show that our proposed Evolution Strategies-based method scales much better than a comparable policy-gradient-based approach."
  - [section 4.2] PPO achieves improvements only for Minifab; for SMT2020 and industry models, PPO yields negative average improvements due to noisy rewards and delayed effects.
  - [corpus] Weak direct evidence; corpus neighbors focus on other domains (mining dispatch, EMS optimization). No comparative ES-vs-PPO studies in semiconductor fabs found.
- Break condition: If policy network parameter count grows substantially (the authors note ES "does not scale well with the number of parameters"), CMA-ES exploration becomes inefficient; gradient-based methods may become necessary.

### Mechanism 2
- Claim: Selecting and controlling the right bottleneck tools is crucial for efficient optimization.
- Mechanism: Semiconductor fabs have heterogeneous equipment with differing constraints. Controlling non-bottleneck tools yields marginal gains because the system's throughput is constrained elsewhere. The authors demonstrate that controlling lithography tools (typical bottlenecks) alone provides improvement, but controlling combinations of tool types (lithography + diffusion + implantation) yields substantially better results because dispatching decisions across interconnected areas can be coordinated.
- Core assumption: The simulation accurately models tool dependencies so that coordinated dispatching across controlled tools translates to real KPI gains.
- Evidence anchors:
  - [abstract] "Moreover, we identify the selection and combination of relevant bottleneck tools to control by the agent as crucial for an efficient optimization."
  - [section 4.3, Figure 8] For Minifab, controlling LIT+IMP+DIF together achieves 29.4% tardiness improvement vs. 15.1% for lithography alone.
  - [corpus] No direct corroborating evidence in corpus.
- Break condition: If the controlled tool set is too small, optimization potential is limited; if too large, computational cost explodes (section 4.4 shows training time scales from hours to days as controlled decisions increase).

### Mechanism 3
- Claim: Diverse training datasets improve generalization across loading scenarios and stochastic failure patterns.
- Mechanism: Real fabs experience variable product mix, volume changes, and random equipment failures. Training on a single scenario causes overfitting to specific WIP patterns. By exposing the agent to multiple loading scenarios and random seeds during training, the policy learns dispatching rules that are robust to distributional shift at deployment.
- Core assumption: The test scenarios share sufficient structure with training scenarios that a single policy can generalize; radical changes (e.g., entirely new product lines) may still require retraining.
- Evidence anchors:
  - [abstract] "For the generalization across different loading scenarios and stochastic tool failure patterns, we achieve advantages when utilizing a diverse training dataset."
  - [section 4.5, Figure 11] Agent trained on scenarios A and B generalizes to unseen scenarios C-F with some degradation but maintains positive improvement.
  - [corpus] Weak evidence; no corpus papers address generalization in RL-based dispatching.
- Break condition: If training scenarios don't adequately cover the operational envelope, the policy may produce dispatching decisions worse than baseline heuristics on out-of-distribution cases.

## Foundational Learning

- Concept: **Semiconductor Frontend Scheduling Problem (SFSP)**
  - Why needed here: Understanding why this is harder than classical job-shop scheduling (reentrant flows, batching, stochastic failures) clarifies why simple heuristics underperform and why RL faces credit-assignment challenges.
  - Quick check question: Can you explain why a wafer revisiting the same lithography tool multiple times increases scheduling complexity?

- Concept: **Evolution Strategies vs. Policy Gradient**
  - Why needed here: The paper's central finding hinges on ES scaling better; understanding the difference (parameter-space vs. action-space exploration, episodic vs. step-wise feedback) is essential for algorithm selection.
  - Quick check question: Why does ES not require backpropagation, and what does that mean for its scalability with CPU parallelization?

- Concept: **Bottleneck Analysis in Manufacturing**
  - Why needed here: The authors emphasize selecting relevant bottleneck tools; without understanding Theory of Constraints basics, you might waste compute controlling non-bottleneck equipment.
  - Quick check question: If a fab's throughput is limited by lithography capacity, why would optimizing diffusion dispatching alone yield minimal improvement?

## Architecture Onboarding

- Component map: Optimizer (CMA-ES or PPO) -> parallel workers (CPU cores) -> Python environment (policy, observation processing) -> Simulator instance (D-SIMCON) via interface -> Queue state + KPIs -> action (lot selection)

- Critical path: Optimizer → parallel environments → simulator calls → lot scoring via attention network → dispatching decision. Latency in the simulator-environment interface dominates training time.

- Design tradeoffs:
  - More controlled tools → higher potential improvement but quadratic increase in compute and memory
  - Longer rollouts (PPO) → better credit assignment but higher sample correlation and memory overflow risk (authors hit memory limits on industry scenario)
  - More training scenarios → better generalization but longer training cycles

- Failure signatures:
  - PPO: Negative average improvement on large models; unstable training curves (Figure 5 shows industry model at -18.8% tardiness improvement)
  - ES: Overfitting to training scenario (test performance drops below baseline on some scenarios in Figure 11)
  - Both: Memory overflow when collecting full-episode samples for large models

- First 3 experiments:
  1. Reproduce Minifab results with CMA-ES controlling lithography only; verify ~15% tardiness improvement. This validates your simulator integration before scaling.
  2. Run PPO on Minifab with truncated vs. complete episodes to observe the stability difference (Figure 5 shows complete episodes yield +8.1% vs. -2.5%).
  3. Test generalization: train on one SMT2020 scenario for 40 episodes, then evaluate on a longer time horizon (180 days) to measure degradation, replicating Figure 12's methodology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Proximal Policy Optimization (PPO) match the scalability of Evolution Strategies on industrial datasets if dedicated hardware allows for policy updates using complete episodes?
- Basis in paper: [explicit] The authors note that using complete episodes yields better results but causes memory overflow for large models. They state, "With more dedicated hardware resources, the PPO potentially would deliver more promising result for the industrial scale scenario."
- Why unresolved: The current experimental setup suffered from memory constraints when attempting to store millions of samples from full episodes of the industry-scale model.
- What evidence would resolve it: A reproduction of the industrial experiments using high-memory hardware capable of processing full episode rollouts, demonstrating stable PPO convergence.

### Open Question 2
- Question: Does a multi-agent reinforcement learning system with policies specialized for specific tool types outperform a single global policy when controlling all dispatching decisions?
- Basis in paper: [explicit] The authors suggest that "the approach might have to be adapted to a multi-agent system with one policy per tool type" because different tools may require different strategies and the number of tool types is too large to encode efficiently.
- Why unresolved: The current study utilized a single policy approach, and the experiments were limited to controlling bottleneck equipment groups rather than the entire factory.
- What evidence would resolve it: A comparative analysis on the industry dataset measuring tardiness and throughput between a multi-agent setup (one policy per tool type) and the current single-agent architecture.

### Open Question 3
- Question: Can a smoother, dense reward function be designed to enable policy-gradient methods to succeed on complex industrial fabs without requiring drastically increased rollout lengths?
- Basis in paper: [inferred] The authors suspect PPO failed on larger models because value estimation is inaccurate and rewards are noisy, stating, "we would need to find a smoother dense reward" to make PPO work for these models.
- Why unresolved: The tested PPO rewards (based on throughput and tardiness rolling means) resulted in negative improvement for the industry model, indicating the specific reward structure required for this complexity remains undiscovered.
- What evidence would resolve it: The identification of a specific dense reward signal that allows PPO to achieve positive improvements in tardiness on the industry dataset comparable to Evolution Strategies.

## Limitations

- The proprietary D-SIMCON simulator used for the industry dataset cannot be reproduced exactly, limiting experimental validation
- No ablation studies on the attention network architecture or systematic hyperparameter sweeps leave questions about whether CMA-ES's superiority is algorithmic or implementation-specific
- Computational scaling shows training times increasing from hours to days as controlled tool set size grows, potentially limiting practical deployment
- The paper does not address policy interpretability, which is crucial for operator trust and deployment in real fab environments

## Confidence

- High Confidence: ES outperforms PPO for this dispatching problem (supported by consistent experimental results across all three models)
- Medium Confidence: Bottleneck tool selection is crucial (evidence from Minifab but not systematically tested across models)
- Medium Confidence: Diverse training datasets improve generalization (supported by controlled experiments but with some performance degradation)
- Low Confidence: Computational scaling with CPU cores (limited to 16 cores in experiments, extrapolation to larger clusters untested)

## Next Checks

1. **Replicate Minifab results with open-source simulation**: Implement a SimPy-based Minifab model matching the described 9 observation features and batching logic, then verify the ~15% tardiness improvement with CMA-ES controlling lithography tools only.

2. **Test bottleneck sensitivity systematically**: Run controlled experiments varying the controlled tool set size on Minifab (lithography only vs. lithography+diffusion vs. all tools) while holding other factors constant to quantify the marginal improvement from each additional tool type.

3. **Evaluate generalization under distributional shift**: Train on scenarios A and B, then test on scenarios C-F with modified product mixes (beyond just loading variations) to assess whether the learned policy degrades catastrophically when faced with novel wafer types or process flows.