---
ver: rpa2
title: Prompting as Scientific Inquiry
arxiv_id: '2507.00163'
source_url: https://arxiv.org/abs/2507.00163
tags:
- prompting
- language
- prompt
- llms
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that prompting should be recognized as a legitimate
  scientific method for understanding large language models (LLMs), rather than being
  dismissed as mere engineering. It distinguishes between "prompt engineering" (optimizing
  prompts for specific tasks) and "prompt science" (using prompts to discover and
  confirm regularities in model behavior).
---

# Prompting as Scientific Inquiry

## Quick Facts
- arXiv ID: 2507.00163
- Source URL: https://arxiv.org/abs/2507.00163
- Authors: Ari Holtzman; Chenhao Tan
- Reference count: 16
- Key outcome: Prompting is a legitimate scientific method for understanding LLMs, distinct from engineering, enabling discovery of capabilities like Chain-of-Thought reasoning and Constitutional AI alignment

## Executive Summary
The paper argues that prompting should be recognized as a scientific method for understanding large language models (LLMs) rather than dismissed as mere engineering. It distinguishes "prompt engineering" (optimizing prompts for tasks) from "prompt science" (using prompts to discover and confirm regularities in model behavior). The authors present three case studies—Chain-of-Thought, Constitutional AI, and emergent capability discovery—showing how prompting has led to major breakthroughs. They argue prompting complements mechanistic interpretability by probing models at the computational level through their natural interface (language) rather than requiring weight-level access.

## Method Summary
The paper establishes prompt science as a behavioral science methodology by demonstrating falsifiability through controlled prompt variations and systematic observation of output distributions. The method involves designing experiments that vary prompts systematically to test hypotheses about model capabilities, using natural language as the intervention mechanism. No training procedures are involved; the approach relies on exploratory prompting to discover new capabilities and prompting studies to confirm hypotheses through behavioral intervention and measurement.

## Key Results
- Prompting discovers capabilities that mechanistic methods cannot access (e.g., Chain-of-Thought reasoning)
- Language serves as LLMs' native interface, enabling scientific probing through linguistic abstractions
- Prompt science complements mechanistic interpretability by addressing computational/algorithmic levels of analysis

## Why This Works (Mechanism)

### Mechanism 1: Language as Native Interface
Prompting works because it intervenes through the channel LLMs were optimized to process—language—rather than artificial numerical abstractions. LLMs develop representations structured by language patterns from training, making linguistic probing effective at revealing capabilities that weight-level analysis may miss.

### Mechanism 2: Productive Vaguity via Linguistic Abstraction
Language prompts enable hypothesis specification at variable precision levels, matching researchers' incomplete understanding. Prompts invoke rich clusters of associations the model has learned, allowing exploration before formalization. This explains how imprecise prompts like "think step-by-step" led to Chain-of-Thought discovery.

### Mechanism 3: Falsifiable Behavioral Intervention
Prompting satisfies scientific rigor through structured manipulation of input variables and measurement of output distributions. Claims like "adding 'let's think step by step' improves math accuracy" are falsifiable and testable across models, distinguishing scientific inquiry from mere optimization.

## Foundational Learning

- **Marr's Three Levels of Analysis**: Needed to position prompt science (computational/algorithmic) vs. mechanistic interpretability (implementation) as complementary. Quick check: Can you explain why discovering LLMs perform few-shot learning is a computational-level insight?

- **Falsifiability in Behavioral Science**: Required to understand how behavioral experiments can be scientific despite black-box access. Quick check: What makes "chain-of-thought improves math accuracy" a falsifiable claim vs. prompt engineering?

- **Training Distribution as Behavioral Determinant**: Important for understanding why prompting exploits correlations learned from training data. Quick check: Why might shift ciphers with common values work better than rare values, and what does this reveal about training data influence?

## Architecture Onboarding

- **Component map**: Input layer (prompt design space) -> Intervention layer (systematic variations) -> Observation layer (output distributions) -> Hypothesis layer (falsifiable claims) -> Validation layer (cross-model testing)

- **Critical path**: 1) Define falsifiable hypothesis 2) Design controlled prompt variations 3) Collect output distributions 4) Test generalization 5) Search for boundary conditions

- **Design tradeoffs**: Precision vs. discovery (mechanistic methods offer precision on known phenomena; prompting discovers unknown capabilities), Scalability vs. depth (prompting scales easily; mechanistic interpretability struggles), Faithfulness vs. accessibility (internal weights are "faithful" but opaque; behavioral outputs are interpretable but may miss internal structure)

- **Failure signatures**: Claims that don't generalize (likely engineering), no systematic variation (anecdotal), performance gains without mechanism (optimization), treating brittleness as noise rather than signal

- **First 3 experiments**: 1) Replicate Chain-of-Thought by testing "let's think step-by-step" on math problems across 3+ model families 2) Design falsifiable probe testing hypothesis about model behavior (e.g., negation handling) 3) Use prompting to discover behavioral anomaly, then search for mechanistic interpretability confirmation

## Open Questions the Paper Calls Out

- Can systematic prompt perturbations map the precise boundaries where model capabilities break down, revealing implicit structure of learned knowledge? (Explicit in Section 7, unresolved due to focus on maximizing scores rather than probing failure boundaries)

- Can a minimal set of "control primitives" in prompt space be identified and composed to achieve arbitrary behavioral modifications? (Explicit in Section 7, unresolved due to lack of comprehensive theory of control dimensions)

- What information do multi-turn prompts fail to capture that single long-prompts retain, and what defines effective multi-turn strategies? (Explicit in Section 7, unresolved because performance differences are observed but mechanisms are not understood)

## Limitations

- The mechanism by which linguistic abstraction maps to model representations remains underspecified and lacks direct empirical validation
- Falsifiability criterion may be too permissive—behavioral claims can be both falsifiable and still reflect prompt engineering
- Weak direct support in corpus literature for prompting as rigorous scientific methodology

## Confidence

**High confidence**: Distinction between prompt engineering (optimization) and prompt science (hypothesis testing) is well-founded and practically useful. Marr framework application for positioning prompt science at computational/algorithmic levels is theoretically sound.

**Medium confidence**: Three case studies demonstrate prompting's discovery potential, though degree to which they represent scientific inquiry vs. clever engineering remains debatable. Falsifiability argument is logically coherent but may not fully address complexity of establishing causal claims.

**Low confidence**: Claim that prompting will remain primary scientific method for LLM understanding is speculative. Mechanism by which language serves as "native interface" is asserted rather than demonstrated. Underestimates potential methodological advances in mechanistic interpretability.

## Next Checks

1. **Falsifiability stress test**: Design experiment testing false hypothesis about LLM behavior (e.g., "LLMs perform better when prompted in Pig Latin") and verify correct failure to reject null hypothesis

2. **Cross-methodological triangulation**: Use prompting to discover behavioral phenomenon, then verify same phenomenon using mechanistic interpretability on small model to assess convergence

3. **Brittleness quantification study**: Systematically vary semantically equivalent prompts to measure stability of behavioral effects and establish empirical bounds on falsifiability