---
ver: rpa2
title: Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality
arxiv_id: '2504.21368'
source_url: https://arxiv.org/abs/2504.21368
tags:
- phase
- training
- diffusion
- noise
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion autoencoders (DAEs) often produce blurry reconstructions
  due to training schedules that emphasize high noise levels early in sampling, recovering
  structures before fine details. The authors address this by dividing training into
  two phases.
---

# Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality

## Quick Facts
- **arXiv ID**: 2504.21368
- **Source URL**: https://arxiv.org/abs/2504.21368
- **Reference count**: 40
- **Primary result**: Two-phase training improves DAE reconstruction PSNR from 26.89 to 30.35 on CIFAR-10

## Executive Summary
Diffusion autoencoders (DAEs) typically produce blurry reconstructions because standard training schedules spend too many steps at high noise levels, recovering structure before fine details. This paper introduces a two-phase training approach: Phase 1 trains the encoder-decoder as a vanilla autoencoder with constant high noise, forcing structural information into the latent code; Phase 2 switches to a shifted cosine noise schedule that allocates more sampling steps to low noise levels for detail refinement. The decoder is also switched from noise prediction to velocity prediction for compatibility. This approach significantly improves reconstruction quality while maintaining latent code manipulability.

## Method Summary
The method involves two distinct training phases. In Phase 1 (25% of training), the DAE is trained as a vanilla autoencoder by always setting the noise level to the highest (t=1), forcing the encoder and decoder to populate the latent code with structural information since the noised input contains pure Gaussian noise. In Phase 2 (75% of training), the model switches to a shifted cosine noise schedule (SC-4) that allocates more sampling steps to low noise levels, enabling the decoder to refine details while leveraging the structural information encoded in the latent code. The decoder is also switched from noise prediction to velocity prediction to ensure compatibility with the shifted cosine schedule. The latent code dimension is 512, and DDIM sampling is used for reconstruction.

## Key Results
- PSNR improves from 26.89 to 30.35 on CIFAR-10
- SSIM increases from 0.905 to 0.955 on CIFAR-10
- FID scores remain comparable to the best single-phase training
- Results hold across CIFAR-10, CelebA, LSUN Bedroom, and ImageNet

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Training Forces Structural Encoding
Phase 1 training with pure Gaussian input (t=1) compels the encoder to compress structural information into the latent code because no signal passes through the noised input. When t=1, the decoder receives xt = pure Gaussian noise. For v-prediction, the target becomes v1 = α1ε - σ1x0 = -x0 (the negated clean image). Since the noised input contains zero information about x0, the decoder must rely entirely on the latent code z, forcing encoder-decoder cooperation.

### Mechanism 2: Shifted Cosine Schedule Reallocates Compute to Detail Refinement
SC-4 noise schedule shifts sampling steps from high-noise (structure recovery) to low-noise (detail refinement), improving texture similarity without sacrificing structure since the latent code already encodes it. High noise levels correlate with recovering large-scale structures; low noise levels with details. Linear-β allocates most steps to high-noise, wasting compute on information already in z. SC-4 shifts the distribution toward low-noise timesteps.

### Mechanism 3: Velocity Prediction Enables Compatibility with Low-Noise-Heavy Schedules
v-prediction is required for SC-4 because ε-prediction fails to learn autoencoding when training spends more time at low noise levels. At low noise levels, ε-prediction targets are similar to the noised input, providing weak gradients for encoder-decoder coordination. v-prediction maintains informative targets across the noise spectrum. At t=1 specifically, v1 = -x0 is a clean prediction task.

## Foundational Learning

- **Diffusion Model Noise Schedules (αt, σt)**: Understanding how αt controls the signal-to-noise ratio over time is essential to grasp why linear-β vs. SC-4 affects structure vs. detail recovery.
  - Quick check: For a shifted cosine schedule with S=4, does αt stay closer to 1 for longer or drop faster compared to S=1?

- **Noise Prediction Types (ε, x, v)**: The paper switches from ε-prediction to v-prediction as a core methodological change; understanding what each predicts at different t values explains why one fails.
  - Quick check: At t=0.9, what does a v-prediction model target versus an ε-prediction model?

- **DDIM Sampling and Inversion**: The paper uses DDIM for reconstruction and mentions "stochastic autoencoding" vs. "inversion"; knowing how the stochastic subcode works clarifies why PSNR differs between them.
  - Quick check: Why does DDIM inversion produce more accurate reconstruction than stochastic autoencoding?

## Architecture Onboarding

- **Component map**: Encoder (U-Net downsampling) -> 512-dim latent vector z -> Decoder (U-Net with attention + AdaGN conditioning on t, z)
- **Critical path**: Phase 1: Train encoder + decoder with t=1 fixed (pure Gaussian input), v-prediction target = -x_0, for 1/4 of total steps; Phase 2: Continue training with t sampled uniformly from [0,1], SC-4 noise schedule, v-prediction target = αtε - σtx0; Inference: Encode x -> z; sample xtK ~ N(0,I); DDIM sample 50 steps conditioned on z
- **Design tradeoffs**: SC-4 vs. SC-2/SC-8 (SC-4 balances PSNR and FID); Phase 1 duration (1/4, not optimized); Latent code size (512, larger may improve reconstruction but not evaluated)
- **Failure signatures**: ε-prediction + SC-4 -> PSNR ~15, washed-out reconstructions; 1-phase SC-4 only -> Sharp but inaccurate structures; Linear-β baseline -> Blurry reconstructions with good structure but poor detail
- **First 3 experiments**: 1) Reproduce Phase 1 alone (t=1 only): Verify PSNR approaches 30.5 but FID is very high (~37); 2) Ablate prediction type: Train v-prediction vs. ε-prediction with SC-4 on CIFAR-10 for 32M examples; 3) Vary phase ratio: Test Phase 1 at 10%, 25%, 50% of training; plot PSNR/FID tradeoff curve

## Open Questions the Paper Calls Out
- **Optimal phase duration**: The authors note they "have not yet optimized the relative duration of the two phases" (fixed at 1/4 of total training) and label determining this hyperparameter an "important open question."
- **Image resolution influence**: The authors suggest "image characteristics, such as resolution, may affect the optimal choice" of the noise schedule scaling factor.
- **SC-4 optimality**: The authors state that SC-4 "may be suboptimal" because the shifted cosine family offers an "infinite number of noise schedules" due to the continuous scaling factor.

## Limitations
- The empirical evidence for why ε-prediction specifically fails with low-noise-heavy schedules is limited to CIFAR-10 experiments
- The choice of 512 latent dimensions and 25% Phase 1 duration appear arbitrary without ablation studies
- The mechanism linking SC-4 to v-prediction necessity is hypothesized but not rigorously tested across different latent dimensionalities or dataset complexities

## Confidence
- **High Confidence**: PSNR and SSIM improvements from 26.89→30.35 and 0.905→0.955 on CIFAR-10 are directly measurable and reproducible
- **Medium Confidence**: The claim that SC-4 improves detail recovery while preserving structure via Phase 1 latent encoding is plausible but relies on assumptions about encoder capacity
- **Low Confidence**: The assertion that v-prediction is required for SC-4 compatibility lacks cross-dataset validation and mechanistic proof

## Next Checks
1. **Phase 1 Ablation**: Train DAE with varying Phase 1 durations (10%, 25%, 50%) on CIFAR-10 and plot PSNR/FID curves to identify optimal structural encoding time
2. **Prediction Type Robustness**: Systematically compare ε-prediction vs. v-prediction across different noise schedules (linear-β, SC-1, SC-4, SC-8) on CelebA and ImageNet
3. **Latent Dimensionality Scaling**: Vary latent code size (256, 512, 1024) while keeping SC-4 and v-prediction fixed to quantify the impact of latent capacity on reconstruction quality