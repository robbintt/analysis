---
ver: rpa2
title: 'RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation'
arxiv_id: '2602.00849'
source_url: https://arxiv.org/abs/2602.00849
tags:
- rmflow
- generation
- meanflow
- arxiv
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RMFlow improves 1-step MeanFlow generation by adding a noise-injection
  refinement stage, balancing Wasserstein path alignment with likelihood maximization.
  It achieves near state-of-the-art performance on text-to-image, context-to-molecule,
  and time-series generation tasks, producing coherent molecular structures and high-fidelity
  images with only 1 neural network evaluation.
---

# RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation

## Quick Facts
- **arXiv ID:** 2602.00849
- **Source URL:** https://arxiv.org/abs/2602.00849
- **Reference count:** 26
- **Primary result:** RMFlow achieves near state-of-the-art 1-step generation on text-to-image, molecule, and time-series tasks, with 98.9% atomic stability and FID 18.91 on COCO.

## Executive Summary
RMFlow improves 1-step MeanFlow generation by adding a noise-injection refinement stage, balancing Wasserstein path alignment with likelihood maximization. It achieves near state-of-the-art performance on text-to-image, context-to-molecule, and time-series generation tasks, producing coherent molecular structures and high-fidelity images with only 1 neural network evaluation. On QM9 molecule generation, it attains 98.9% atomic stability and 93.2% molecular stability, matching or exceeding multi-step methods. On COCO, RMFlow reaches FID 18.91, outperforming many 1-step baselines.

## Method Summary
RMFlow extends 1-NFE MeanFlow by adding a noise-injection refinement step. First, it performs coarse 1-step transport from x₀ to x₁ = x₀ + û₀,₁(x₀;θ). Then, it injects noise to reach x̂tgt = x₁ + √(σ²min - σ²)·ε₂, specifying a Gaussian conditional distribution that admits an ELBO-derived loss term. The joint loss L_RMFlow = L_CMFM + λ₁L_NLL + λ₂E[‖φω(c)‖²] balances distributional alignment with sample fidelity. For large-scale tasks, RMFlow uses pre-training with L_CMFM followed by PEFT fine-tuning with the full objective.

## Key Results
- Achieves 98.9% atomic stability and 93.2% molecular stability on QM9, matching or exceeding multi-step methods
- Reaches FID 18.91 on COCO, outperforming many 1-step baselines
- Closes the gap between 1-NFE and multi-NFE MeanFlow on synthetic mixture Gaussian tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a noise-injection step after 1-NFE MeanFlow transport enables likelihood maximization that standard MeanFlow cannot achieve alone.
- Mechanism: Standard 1-NFE MeanFlow minimizes Wasserstein distance but cannot guarantee likelihood maximization. By transporting to an intermediate state x₁ = x_data + σε₁ with σ < σ_min, then injecting noise to reach x_tgt = x₁ + √(σ²_min - σ²)ε₂, the framework specifies a Gaussian conditional distribution p_θ(x_tgt|x₀) that admits an ELBO-derived loss term.
- Core assumption: The noise-injection formulation accurately approximates the evidence lower bound for the target distribution.
- Evidence anchors:
  - [abstract] "integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step"
  - [section 4.1] Theorem 4.1 shows "-A·L_NLL + C ≤ E[log p_θ(x_tgt)] = -H(p_tgt) - D_KL(p_tgt||p_θ)"
  - [corpus] Weak direct corpus support; neighbor papers focus on MeanFlow training dynamics, not noise-injection variants.
- Break condition: If σ approaches σ_min, the noise term vanishes and likelihood signal disappears; if σ → 0, the intermediate target becomes too noisy.

### Mechanism 2
- Claim: The joint loss L_RMFlow = L_CMFM + λ₁L_NLL + λ₂E[‖φ_ω(c)‖²] balances distributional alignment with sample fidelity.
- Mechanism: L_CMFM (from Boffi et al.) upper-bounds Wasserstein distance (Theorem 3.1). L_NLL provides likelihood lower bound (Theorem 4.1). The regularization term prevents unbounded encoder outputs. Ablations show optimal λ₁ ∈ [1e-2, 5e-2] for QM9 and λ₁ ∈ [1e-2, 1e-1] for synthetic tasks.
- Core assumption: The two objectives do not conflict catastrophically; gradient directions are sufficiently aligned.
- Evidence anchors:
  - [section 4.1.1] Equation 12 defines L_RMFlow with three terms
  - [section B.1-B.3] Ablation tables show λ₁ tuning results across tasks
  - [corpus] AlphaFlow (arxiv 2510.20771) decomposes MeanFlow into trajectory flow matching + consistency, suggesting complementary objectives are viable.
- Break condition: If λ₁ dominates (λ₁ >> 1), Wasserstein constraint weakens; if λ₁ → 0, reverts to standard MeanFlow with degraded 1-NFE quality.

### Mechanism 3
- Claim: MeanFlow's averaged velocity formulation enables 1-NFE generation that standard flow matching cannot achieve without distillation.
- Mechanism: Instead of learning instantaneous velocity u_t(x), MeanFlow learns u_{t,r}(x) = (x_r - x_t)/(r-t), integrating the velocity field. The target u^{tgt}_{t,r} = u_t(x|z) + (r-t)[∇û_{t,r}·u_t + ∂_t û_{t,r}] uses stop-gradient to prevent higher-order optimization while ensuring dynamical consistency.
- Core assumption: The Jacobian-vector product computation accurately captures the time derivative term without numerical instability.
- Evidence anchors:
  - [section 2] Equation 2-4 define mean velocity and training objective
  - [section 1] Figure 1 shows 1-NFE MeanFlow significantly underperforms multi-NFE variants on mixture Gaussian
  - [corpus] Mean Flows (arxiv 2505.13447) derives the identity between average and instantaneous velocities.
- Break condition: If time embedding for (r, t) is poorly conditioned, network cannot distinguish different velocity scales.

## Foundational Learning

- Concept: **Flow Matching (conditional probability paths)**
  - Why needed here: RMFlow builds on MeanFlow which extends FM; understanding how xt = a_t·x1 + b_t·x0 constructs interpolants is prerequisite.
  - Quick check question: Can you explain why FM requires multi-step ODE integration while MeanFlow aims for single-step?

- Concept: **ELBO and variational inference**
  - Why needed here: The noise-injection step's theoretical justification comes from VAE-style ELBO derivation (Theorem 4.1).
  - Quick check question: Why does specifying p_θ(x_tgt|x₀) as Gaussian enable a tractable lower bound on log-likelihood?

- Concept: **Wasserstein distance vs KL divergence**
  - Why needed here: RMFlow explicitly combines Wasserstein control (L_CMFM) with KL control (via L_NLL); understanding their different failure modes is critical.
  - Quick check question: What type of distribution mismatch does Wasserstein penalize that KL might miss?

## Architecture Onboarding

- Component map: Prior x₀ -> [encoder] -> velocity network û_{0,1}(x₀;θ) -> intermediate x₁ -> [noise add] -> x̂tgt -> [decoder] -> output
- Critical path: Prior x₀ → [encoder] → velocity network û_{0,1}(x₀;θ) → intermediate x₁ → [noise add] → x̂tgt → [decoder] → output
- Design tradeoffs:
  - σ controls how much likelihood signal vs. noise injection magnitude; paper uses fixed value
  - λ₁ balances Wasserstein vs. likelihood; requires per-task tuning
  - PEFT (LoRA) for large-scale tasks reduces memory but may limit adaptation capacity
- Failure signatures:
  - Invalid molecular structures (fragmented atoms): λ₁ too small, insufficient likelihood signal
  - Mode collapse in mixture sampling: λ₁ too large, Wasserstein constraint too weak
  - Blurry images: σ_min - σ² too large, excessive noise
- First 3 experiments:
  1. Reproduce synthetic Gaussian mixture sampling (Table 1) to verify 1-NFE RMFlow closes gap to multi-NFE MeanFlow before tackling multimodal tasks.
  2. Ablate λ₁ on QM9 (Table 9) to internalize the Wasserstein-likelihood tradeoff; observe atomic stability plateau around 98.9%.
  3. Train minimal text-to-image model on COCO subset with PEFT, comparing L_CMFM-only baseline vs. full L_RMFlow to measure FID improvement (expect ~8-9 point drop per Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the noise-injection refinement strategy be theoretically generalized to support multi-step MeanFlow transport?
- Basis in paper: [explicit] The conclusion states, "A promising direction for future research is to extend RMFlow to support multiple mean flow transport steps... applying a noise-injection step after each transport step."
- Why unresolved: The current theoretical formulation and loss function (Eq. 12) are derived specifically for the 1-NFE ($t=0$ to $t=1$) scenario. Extending this to intermediate steps requires designing a new loss function to maintain likelihood maximization guarantees across the trajectory.
- What evidence would resolve it: A theoretical derivation of a modified loss function for intermediate steps, accompanied by empirical results showing that multi-step RMFlow outperforms multi-step MeanFlow baselines.

### Open Question 2
- Question: Can the magnitude of the noise-injection step be made adaptive or learnable rather than fixed?
- Basis in paper: [explicit] Section 6 identifies as a limitation that "RMFlow uses a fixed parameter $\sqrt{\sigma^2_{min} - \sigma^2}$ during the noise injection step, which may be suboptimal."
- Why unresolved: The current implementation relies on a hand-tuned constant scalar for the noise variance. It is unknown if a dynamic schedule (dependent on time or state) could better refine the generation quality.
- What evidence would resolve it: A comparative study where the fixed parameter is replaced by a parameterized function (e.g., a small neural network or schedule), demonstrating improved FID or stability metrics on benchmarks like QM9 or COCO.

### Open Question 3
- Question: What is the theoretical relationship determining the optimal balance between the Wasserstein path alignment term and the likelihood maximization term?
- Basis in paper: [inferred] Section 4.1.1 combines $L_{CMFM}$ and $L_{NLL}$ using hyperparameters $\lambda_1, \lambda_2$, which are selected via ablation studies (Appendix B.1). The paper lacks a theoretical justification for why specific weights (e.g., $\lambda_1=5e-2$) are optimal.
- Why unresolved: The objective function balances two distinct theoretical goals (minimizing Wasserstein distance vs. maximizing likelihood), but the interaction between these terms relies on empirical tuning rather than theoretical derivation.
- What evidence would resolve it: A theoretical analysis defining the convergence bounds relative to $\lambda_1$, or a universal rule for setting $\lambda_1$ that holds true across the diverse tasks mentioned (image, molecule, time-series).

## Limitations

- **σ Sensitivity:** The noise-injection step's effectiveness critically depends on the choice of intermediate noise level σ, which the paper fixes but doesn't thoroughly analyze sensitivity to.
- **Theoretical Framework:** The theoretical framework (Theorem 4.1) assumes Gaussian conditionals, but empirical validation of this approximation is limited.
- **Large-Scale Results:** Large-scale COCO results (FID 18.91) have low confidence due to undisclosed SD checkpoint details and PEFT implementation specifics.

## Confidence

- **High Confidence:** Synthetic experiment results showing RMFlow closing the gap between 1-NFE and multi-NFE MeanFlow (Table 1)
- **Medium Confidence:** QM9 molecular stability claims (98.9% atomic, 93.2% molecular) given the strong alignment with RLPF baselines
- **Low Confidence:** Large-scale COCO results (FID 18.91) due to undisclosed SD checkpoint details and PEFT implementation specifics

## Next Checks

1. **σ Sensitivity Analysis:** Systematically vary the intermediate noise level σ in the noise-injection formula to determine its impact on likelihood maximization and sample quality
2. **Cross-Task λ₁ Transferability:** Test whether λ₁ values optimized on synthetic tasks transfer to molecular and image domains, or if per-task tuning is unavoidable
3. **Time Embedding Robustness:** Evaluate RMFlow performance when using different time-sampling strategies (beyond p(t)=2t) to verify the mean velocity formulation's stability