---
ver: rpa2
title: 'Bridging the Gap: From Ad-hoc to Proactive Search in Conversations'
arxiv_id: '2506.00983'
source_url: https://arxiv.org/abs/2506.00983
tags:
- query
- ad-hoc
- conversational
- search
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conv2Query bridges the input gap between ad-hoc search and proactive
  search in conversations (PSC) by mapping conversational context into ad-hoc queries.
  This approach effectively adapts off-the-shelf ad-hoc retrievers to PSC without
  requiring fine-tuning on PSC data.
---

# Bridging the Gap: From Ad-hoc to Proactive Search in Conversations

## Quick Facts
- arXiv ID: 2506.00983
- Source URL: https://arxiv.org/abs/2506.00983
- Authors: Chuan Meng; Francesco Tonolini; Fengran Mo; Nikolaos Aletras; Emine Yilmaz; Gabriella Kazai
- Reference count: 40
- Key outcome: Conv2Query significantly improves PSC retrieval performance, achieving MRR@10 gains of up to 0.355 over conversational context baselines

## Executive Summary
This paper addresses the challenge of proactive search in conversations (PSC) by proposing Conv2Query, a framework that bridges the input gap between ad-hoc search and PSC. The core insight is that off-the-shelf ad-hoc retrievers struggle with lengthy, noisy conversational contexts but excel when provided with concise ad-hoc queries. Conv2Query transforms conversational context into ad-hoc queries using a fine-tuned LLM, enabling existing retrievers to perform effectively in PSC settings without requiring specialized training. The framework includes a novel query filtering mechanism (QF-DC) that selects high-quality pseudo-training queries based on both document relevance and contextual alignment.

## Method Summary
The Conv2Query framework operates by first generating pseudo-query training data using Doc2Query models, then filtering these candidates through a dual-constraint mechanism (QF-DC) that scores queries based on both document relevance and conversational context alignment. A fine-tuned LLM (Mistral-7B) is trained to map conversational contexts to these high-quality pseudo-queries. During inference, the generated queries replace the original conversational context as input to standard ad-hoc retrievers (BM25, ANCE, RepLLaMA). The framework can also be used to fine-tune retrievers directly on PSC data, further improving performance.

## Key Results
- Conv2Query improves MRR@10 by up to 0.355 compared to using conversational context directly
- When used to fine-tune ad-hoc retrievers on PSC data, Conv2Query achieves additional gains of 0.122-0.386 MRR@10
- QF-DC filtering mechanism accelerates convergence and improves quality by ensuring selected queries align with both documents and conversational context
- The approach works effectively across multiple retrievers (BM25, ANCE, SPLADE++, RepLLaMA) and datasets (ProCIS, WebDisc)

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Query Transformation
The paper addresses the fundamental input gap where ad-hoc retrievers pre-trained on short, keyword-heavy queries fail when given long, noisy conversational histories. Conv2Query acts as an adapter that distills conversational context into the format retrievers "expect," unlocking their existing capabilities without fine-tuning the retriever itself. The core assumption is that the primary bottleneck is input format mismatch rather than a lack of semantic knowledge.

### Mechanism 2: Dual-Constraint Filtering (QF-DC) for Learning Stability
The QF-DC mechanism selects pseudo-query training targets based on both Document Relevance and Contextual Alignment, reducing the semantic gap between input (conversation) and target (query). This ensures the mapping function learns a consistent translation from "what is said" to "what is needed," leading to faster convergence and better generalization compared to single-constraint filtering.

### Mechanism 3: Noise Filtration for Lexical Retrieval
For lexical retrievers like BM25, Conv2Query works by removing conversational "stop-words" and noise, leaving high-density keyword signals that maximize sparse matching efficiency. The framework converts natural language into "canonical query language" that mitigates the vocabulary mismatch problem inherent in conversational contexts.

## Foundational Learning

- **Concept: Ad-hoc vs. Conversational Search** - Why needed: This paper bridges these paradigms; Ad-hoc means "single, standalone query" while Conversational implies "context-dependent history + current utterance." Quick check: Is "How about the blue one?" in a shoe conversation Ad-hoc or Conversational? (Answer: Conversational).

- **Concept: Doc2Query (Document-to-Query)** - Why needed: The paper uses Doc2Query to synthesize training data by learning "Doc -> potential Query" instead of simply asking an LLM to write queries. Quick check: Why use Doc2Query instead of LLM query writing? (Answer: To generate pseudo-labels from ground truth relevant documents for high precision).

- **Concept: Distribution Shift (OOD Detection)** - Why needed: The core problem is models trained on MS MARCO queries perform poorly on Reddit/Chat logs. Quick check: Why not just fine-tune retrievers on conversation data? (Answer: The input gap remains during fine-tuning, limiting transfer of pre-trained knowledge).

## Architecture Onboarding

- **Component map:** Doc2Query (T5) + QF-DC Filter (RankLLaMA) -> The Adapter (Conv2Query) -> The Retriever (BM25, RepLLaMA, ANCE)
- **Critical path:** The QF-DC Filtering mechanism, where the paper claims its novelty. If the selected pseudo-query does not balance document relevance and context alignment, the Adapter learns a flawed mapping, causing retrieval failures downstream.
- **Design tradeoffs:** QF-D vs. QF-DC (faster/simpler vs. better performance with overhead), Retriever Choice (neural vs. lexical benefits from different query formats).
- **Failure signatures:** Retriever returns irrelevant docs (check generated query for hallucination), Adapter fails to converge (check QF-DC scores for semantic distance), BM25 MRR drops (generated queries may be too long/verbose).
- **First 3 experiments:** 1) Run raw conversation context through pre-trained retriever to reproduce low baseline scores, 2) Train Conv2Query using Random vs. QF-DC selection to validate dual-filter need, 3) Compare MRR@10 of Neural Retriever vs. Lexical when using Conv2Query outputs.

## Open Questions the Paper Calls Out

1. Can query performance prediction (QPP) methods be effectively utilized to predict the optimal timing for proactive retrieval actions? The authors explicitly identify this limitation and suggest future exploration.

2. To what extent does Conv2Query's performance generalize to conversation types beyond multi-party text threads, such as spoken dialogues or task-oriented interactions? The current evaluation is limited to Reddit threads.

3. Does generating a single, unified query yield better retrieval performance than concatenating multiple queries for contexts with several relevant documents? The authors note this remains unexplored.

## Limitations

- The framework's effectiveness may be context-dependent, particularly for complex conversational scenarios requiring deep semantic understanding beyond keyword matching.
- The attribution of improvements specifically to "distribution alignment" could benefit from more direct ablation studies isolating these effects.
- The claim about universal applicability may be overstated given the evaluation focuses primarily on Reddit-style conversations where keyword-based retrieval remains viable.

## Confidence

- **High Confidence:** Experimental results demonstrating performance improvements across multiple retrievers and datasets are well-supported by reported metrics.
- **Medium Confidence:** Attribution of improvements to "distribution alignment" and "input gap" bridging is plausible but could benefit from more direct ablation studies.
- **Low Confidence:** Claims about universal applicability to all PSC scenarios may be overstated given the Reddit-focused evaluation.

## Next Checks

1. **Ablation Study on Distribution Shift:** Systematically vary context amount provided to retrievers to quantify impact of input length/noise versus semantic understanding requirements.

2. **Cross-Dataset Generalization:** Test Conv2Query on conversational datasets with different characteristics (technical support, medical dialogues) to assess robustness beyond Reddit-style discussions.

3. **Query Quality Analysis:** Conduct human evaluation of generated queries to verify semantic fidelity while reducing noise, addressing potential concerns about over-simplification or hallucination.