---
ver: rpa2
title: 'MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker
  Emotion Recognition'
arxiv_id: '2507.18929'
source_url: https://arxiv.org/abs/2507.18929
tags:
- sticker
- visual
- features
- emotion
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MGHFT, a multi-granularity hierarchical fusion
  transformer for cross-modal sticker emotion recognition. The method leverages Multimodal
  Large Language Models to generate multi-view textual descriptions of stickers, capturing
  semantic cues like intent, style, and character details.
---

# MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition

## Quick Facts
- **arXiv ID:** 2507.18929
- **Source URL:** https://arxiv.org/abs/2507.18929
- **Reference count:** 40
- **Primary result:** MGHFT achieves 73.31% accuracy and 72.52% F1 on SER30K, and 35.13% accuracy on MET-MEME, outperforming existing methods.

## Executive Summary
This paper introduces MGHFT, a novel multi-granularity hierarchical fusion transformer for cross-modal sticker emotion recognition. The method leverages Multimodal Large Language Models (MLLMs) to generate multi-view textual descriptions capturing semantic cues like intent, style, and character details. These descriptions are fused with visual features at multiple stages of a Pyramid Vision Transformer using contrastive learning and attention mechanisms. Experiments on two sticker emotion datasets demonstrate significant performance improvements over existing methods, particularly in large-scale and low-resource scenarios.

## Method Summary
MGHFT generates four distinct textual descriptions (intent, style, main roles, character details) for each sticker using LLaVA-NeXT, then fuses these with visual features from a PVT backbone in a hierarchical manner. The fusion occurs at four stages of the PVT, with each stage receiving text features corresponding to a specific semantic granularity. A Text-Guided Fusion Attention module performs final multimodal integration. The model is trained using a combination of cross-entropy loss and alignment loss (contrastive + MLCE) with AdamW optimizer (LR=1e-3, batch size=16, 50 epochs).

## Key Results
- Achieves 73.31% accuracy and 72.52% F1 score on SER30K dataset
- Achieves 35.13% accuracy on MET-MEME dataset
- Significantly outperforms existing methods on both large-scale (SER30K) and low-resource (MET-MEME) scenarios
- Ablation studies confirm the importance of hierarchical fusion order and text-guided attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing sticker semantics into explicit multi-view textual descriptions creates supervisory signals that guide visual feature extraction more effectively than visual-only processing.
- **Core assumption:** Sticker emotion relies on explicit semantic cues that standard vision encoders fail to capture from pixel data alone.
- **Evidence anchors:** The method uses MLLMs to generate distinct textual embeddings for intent, style, main role, and posture, which are then injected hierarchically into the vision backbone.

### Mechanism 2
- **Claim:** Hierarchical fusion improves performance by matching the granularity of textual guidance to the receptive field of visual stages.
- **Core assumption:** Distinct visual features require different levels of semantic context to interpret emotion correctly.
- **Evidence anchors:** The architecture injects text features into four stages of the PVT backbone, applying "Soft-Fusion" for local features and contrastive alignment for global features.

### Mechanism 3
- **Claim:** Text-Guided Fusion Attention resolves conflicts between visual and textual modalities by using text as a query filter for visual features before classification.
- **Core assumption:** Raw visual features contain significant "emotional noise" that must be filtered out by textual context to isolate the specific emotion.
- **Evidence anchors:** The dual cross-attention mechanism distills visual information down to what is semantically relevant to the text description.

## Foundational Learning

- **Concept: Pyramid Vision Transformer (PVT)**
  - **Why needed here:** The backbone must output features at multiple scales to facilitate hierarchical fusion. Standard ViTs output a single scale.
  - **Quick check question:** Can you identify the output dimensions of Stage 1 through Stage 4 in the backbone and which textual view aligns with each?

- **Concept: Contrastive Learning (CLIP-style)**
  - **Why needed here:** Used for the "Global-Granularity" alignment. You must understand how cosine similarity and temperature scaling pull matching image/text pairs together.
  - **Quick check question:** In Equation 4, what does the temperature parameter Ï„ control, and how does it affect the gradient for hard negatives?

- **Concept: MLLM Prompt Engineering**
  - **Why needed here:** The system relies on the MLLM generating consistent, structured text. If prompts yield "I don't know" or generic descriptions, the pipeline fails.
  - **Quick check question:** Review the prompts in Figure 3. How would you modify them if the stickers contained heavy cultural metaphors that the base MLLM missed?

## Architecture Onboarding

- **Component map:** MLLM (Frozen) -> BERT (Frozen) -> PVT Backbone (Trainable) -> Fusion Blocks (4x stages) -> TGFA Head -> Classification

- **Critical path:**
  1. MLLM descriptions must be generated offline first (slow)
  2. PVT and BERT run in parallel during forward pass
  3. Fusion blocks must wait for corresponding stage output from PVT and text embedding from BERT
  4. Backpropagation flows through fusion blocks into PVT backbone

- **Design tradeoffs:**
  - Using a 7B parameter MLLM provides high semantic quality but makes real-time inference impossible without caching
  - Freezing BERT prevents overfitting to small datasets but limits adaptation to specific sticker slang

- **Failure signatures:**
  - If learning rate is too high, Soft-Fusion attention becomes a one-hot vector, ignoring most textual context
  - If alignment loss is weighted too low, the model ignores text and falls back to visual features only

- **First 3 experiments:**
  1. Run PVT-small on SER30K without text fusion to verify baseline accuracy
  2. Swap the order of text injection to verify the specific ordering [T1, T2, T3, T4] is optimal
  3. Replace MLLM descriptions with generic placeholders to confirm performance boost comes from semantic content

## Open Questions the Paper Calls Out
- The paper acknowledges that stickers frequently appear in dialogue scenarios and cites prior work on conversational sticker understanding, but MGHFT treats stickers as isolated inputs without dialogue context.

## Limitations
- The method's effectiveness is contingent on the MLLM generating high-quality, consistent descriptions
- Generalization to non-English stickers or culturally specific visual metaphors is untested
- Real-time applicability is questionable due to the offline MLLM description generation bottleneck

## Confidence
- **High Confidence:** The hierarchical fusion architecture design and ablation results showing performance degradation when view order is scrambled
- **Medium Confidence:** The claim that MLLM-generated descriptions significantly improve accuracy over visual-only methods
- **Low Confidence:** The scalability of the method to real-time applications due to offline MLLM description generation

## Next Checks
1. Test MGHFT's performance when MLLM descriptions are systematically biased to assess robustness to textual noise
2. Evaluate the model on a dataset of stickers from a different cultural context to verify cross-cultural generalization
3. Measure the end-to-end latency of MGHFT in a deployment scenario, including MLLM description generation time