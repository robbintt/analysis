---
ver: rpa2
title: Deep Multivariate Models with Parametric Conditionals
arxiv_id: '2602.01953'
source_url: https://arxiv.org/abs/2602.01953
tags:
- distribution
- chain
- variables
- limiting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach to deep multivariate modelling
  by representing the joint distribution through conditional distributions for each
  variable given the rest. The method learns these conditionals by maximising the
  data likelihood of the limiting distribution of an MCMC process that alternates
  between sampling from the conditionals.
---

# Deep Multivariate Models with Parametric Conditionals

## Quick Facts
- **arXiv ID:** 2602.01953
- **Source URL:** https://arxiv.org/abs/2602.01953
- **Reference count:** 25
- **Key outcome:** New approach for deep multivariate modeling using conditional distributions and MCMC-based learning achieves 99.04% MNIST classification accuracy and versatile inference capabilities.

## Executive Summary
This paper introduces a novel approach to deep multivariate modeling that represents joint distributions through conditional distributions for each variable given the rest. The method learns these conditionals by maximizing the data likelihood of the limiting distribution of an MCMC process that alternates between sampling from the conditionals. This framework enables versatile downstream tasks including classification, conditional generation, and semi-supervised learning, with the chain length hyperparameter controlling the trade-off between data likelihood and consistency of learned conditionals.

## Method Summary
The method learns a joint distribution through parametric conditional distributions $p_\theta(x_i|x_{-i})$ for each variable group. Training maximizes the data likelihood of the limiting distribution of an MCMC chain that samples from these conditionals. The key innovation is a recurrent lower bound that decomposes into likelihood minus a KL divergence term, implicitly enforcing consistency among conditionals through detailed balance. For semi-supervised learning, unobserved variables are completed via MCMC sampling before gradient updates. The approach is implemented through a blended learning algorithm that maintains a batch of samples, iteratively updates parameters, and resamples variables in-place.

## Key Results
- Achieves 99.04% classification accuracy on MNIST and 88.61% on Fashion MNIST
- Demonstrates ability to learn true joint distributions from incomplete training data (synthetic validation: KL divergence 0.0018±0.0013 vs 0.0007±0.0004 for complete data)
- Shows versatile inference capabilities on CelebA including conditional generation, segmentation from incomplete images, and image demixing
- Outperforms standard VAEs and symmetric equilibrium learning on limiting distribution metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing a recurrent lower bound implicitly enforces consistency among conditional distributions without explicit constraints.
- Mechanism: The lower bound LB(θ, q, n) decomposes into likelihood minus n times a KL divergence term between forward and reverse Markov chains. When q(x'|x) = p_θ(x'|x), the KL term penalizes asymmetry, pushing toward detailed balance. Detailed balance is equivalent to conditionals being mutually consistent.
- Core assumption: Conditional models are expressive enough to achieve consistency within parameter space.
- Evidence: [abstract] shows learning via maximizing limiting distribution likelihood; [Section 3, Proposition 3.1] directly links bound tightness to reverse-chain matching; [Section 5.2] shows matrix asymmetry decreases with longer chains.

### Mechanism 2
- Claim: Chain length n controls a tunable trade-off between data likelihood and conditional consistency.
- Mechanism: The KL penalty in the lower bound is weighted by n. Larger n amplifies consistency enforcement, pushing toward detailed balance. Smaller n relaxes this constraint, allowing likelihood prioritization at cost of inconsistent conditionals.
- Core assumption: Gradient approximation quality degrades gracefully as n decreases, with a useful operating point where both objectives are reasonably satisfied.
- Evidence: [Section 4, Remark 4.3] discusses trade-off between likelihood and consistency; [Section 5.2, Figure 1] shows learned matrix W becomes symmetric for longer chains.

### Mechanism 3
- Claim: Semi-supervised learning emerges naturally from completing unobserved variables via MCMC before gradient updates.
- Mechanism: For partially observed data, MCMC sampler with observed variables clamped draws completion samples. These completed samples are used in standard gradient step, approximating the EM gradient.
- Core assumption: MCMC completion sampler has mixed sufficiently to provide reasonable completions.
- Evidence: [Section 4, Algorithm 1] explicitly samples unobserved variables with observed ones clamped; [Section 5.1] synthetic experiment shows model learns true joint distribution even with missing components.

## Foundational Learning

- **Concept: Gibbs sampling and MCMC limiting distributions**
  - Why needed: The entire model is defined via the limiting distribution of an MCMC process that cycles through conditionals. Understanding convergence is essential.
  - Quick check: Given conditionals p(x|y) and p(y|x), what condition ensures the Gibbs sampler's limiting distribution is the joint p(x,y) having these as conditionals?

- **Concept: ELBO decomposition and variational inference**
  - Why needed: The recurrent lower bound extends ELBO-style reasoning to Markov chains; understanding KL terms helps interpret consistency mechanism.
  - Quick check: In standard VAE ELBO, which term encourages the variational posterior to match the true posterior? How does this relate to the KL term in equation (5)?

- **Concept: Detailed balance and reversibility in Markov chains**
  - Why needed: Theorem 4.2 establishes that detailed balance of the MCMC kernel is equivalent to consistency of the conditionals—a central theoretical result.
  - Quick check: For a transition kernel p(x'|x), what equation defines detailed balance? If satisfied, what does this imply about the limiting distribution?

## Architecture Onboarding

- **Component map:**
  - Conditional networks p_θi(x_i|x_{-i}) -> MCMC sampler -> Completion module -> Training loop
  - For MNIST: p(x|c,z) -> p(c|x,z) -> p(z|x,c)

- **Critical path:**
  1. Define variable groups and implement conditional networks (architectures depend on variable types: CNNs for images, MLPs for attributes/latents)
  2. Implement the Gibbs-style sampler with clamping capability
  3. Set hyperparameters: n_inference (completion steps), expected chain length (controlled by batch replacement ratio)
  4. Run Algorithm 1: for each training batch, complete if needed, then iterate parameter updates with in-place resampling

- **Design tradeoffs:**
  - Longer expected chain length → stronger consistency enforcement but slower training and potential optimization issues if model capacity is limited
  - More conditionals / variable groups → more flexible inference tasks but more networks to train and potentially slower mixing
  - Network capacity vs. consistency: Highly expressive networks can achieve consistency; under-capacity models may require shorter chains to avoid gradient domination by the KL term

- **Failure signatures:**
  - Generated samples from limiting distribution look good, but conditional inference (e.g., classification) is poor: consistency may be too weak; increase chain length
  - Training diverges or stalls with large n: model may lack capacity to achieve consistency; reduce n or increase network size
  - Completions are nonsensical: n_inference too small or model not yet trained enough; pre-train with fully supervised data or increase completion steps

- **First 3 experiments:**
  1. Replicate the synthetic validation (Section 5.1): Train on an Ising model with missing variables. Monitor KL divergence to ground truth and verify both complete and incomplete data scenarios converge.
  2. Ablate chain length on MNIST: Train p(x,c,z) with n=1, 4, 16. Compare FID of generated samples, classification accuracy, and asymmetry measures to confirm the trade-off.
  3. Test semi-supervised regime on Fashion MNIST: Hide class labels for 90% of training data. Compare classification accuracy against a fully supervised baseline to validate the completion mechanism.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does implementing a dynamic schedule for the chain length hyperparameter (e.g., annealing from short to long) improve optimization stability or convergence speed compared to a fixed length?
  - Basis: Section D.3 states, "It might indeed be desirable to start the learning with short (expected) chain lengths and then gradually increase the length... We have not yet experimented with varying it."
  - Why unresolved: Authors used fixed expected chain length throughout training and hypothesize curriculum approach could better approximate gradients as model matures.
  - What evidence would resolve it: Experiments comparing validation likelihood and convergence rates between fixed-length and scheduled-length training regimes.

- **Open Question 2:** Can the proposed learning algorithm be modified to explicitly control or minimize the mixing time of the resulting Markov chain, thereby reducing computational cost of inference?
  - Basis: Section D.1 notes, "Currently, we do not have control of the mixing time... [but] it might be possible to achieve models with both the desired limiting distribution and a fast mixing."
  - Why unresolved: Current method focuses on learning correct limiting distribution but ignores efficiency (speed) of MCMC process, which currently requires many steps (e.g., 100).

## Limitations
- Capacity-Consistency Trade-off: Empirical validation of constraint that conditional network expressiveness must be sufficient to achieve consistency is limited to synthetic cases; real-world model failures due to under-capacity are not demonstrated.
- Hyperparameter Sensitivity: Critical parameters like chain length n and batch replacement ratio are not systematically studied across datasets, leaving uncertainty about optimal settings for different applications.
- Gradient Approximation Quality: Theoretical foundation assumes gradient approximation reliability, but paper provides limited empirical validation of approximation error across different n values.

## Confidence
- **High Confidence**: Core theoretical framework (recurrent lower bound, detailed balance equivalence) and basic synthetic validation showing consistent learning with incomplete data.
- **Medium Confidence**: Performance claims on MNIST and Fashion MNIST, as these are standard benchmarks but comparisons could be more comprehensive.
- **Low Confidence**: CelebA results and specific architectural choices, as implementation details are sparse and complex multi-task evaluation lacks extensive ablation studies.

## Next Checks
1. **Capacity-Consistency Validation**: Systematically vary conditional network capacity on synthetic data and measure the trade-off between likelihood improvement and consistency achievement (as measured by matrix asymmetry).
2. **Hyperparameter Sensitivity Analysis**: Conduct controlled experiments varying chain length n across multiple orders of magnitude on MNIST, measuring classification accuracy, generation quality, and consistency metrics to identify optimal operating points.
3. **Gradient Approximation Study**: For different chain lengths on synthetic data, compare the actual gradient computed via the recurrent bound against the true gradient of the data likelihood to quantify approximation error and its impact on convergence.