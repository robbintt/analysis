---
ver: rpa2
title: 'AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection'
arxiv_id: '2504.21044'
source_url: https://arxiv.org/abs/2504.21044
tags:
- trigger
- adversarial
- triggers
- transform
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGATE introduces a novel black-box watermarking framework for multimodal
  models that addresses stealthiness and robustness challenges in copyright protection.
  The key innovation lies in using in-distribution adversarial triggers instead of
  Out-of-Distribution data, combined with a two-phase cooperative verification mechanism.
---

# AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection

## Quick Facts
- **arXiv ID:** 2504.21044
- **Source URL:** https://arxiv.org/abs/2504.21044
- **Reference count:** 40
- **Primary result:** 0.21-0.32% degradation vs. 0.78-8.04% for baselines, with strong robustness against adversarial attacks

## Executive Summary
AGATE introduces a novel black-box watermarking framework for multimodal models that addresses stealthiness and robustness challenges in copyright protection. The key innovation lies in using in-distribution adversarial triggers instead of Out-of-Distribution data, combined with a two-phase cooperative verification mechanism. The framework generates stealthy adversarial triggers through noise injection in ordinary dataset images, then employs a transform module to correct model outputs by narrowing the distance between adversarial trigger image embeddings and text embeddings. Experiments across five datasets demonstrate superior performance with minimal utility degradation while maintaining strong robustness against adversarial attacks.

## Method Summary
AGATE's framework consists of three main components: (1) adversarial trigger generation that creates in-distribution perturbations maintaining visual fidelity while inducing semantic shifts, (2) a lightweight transform module trained on the original model's embedding space to selectively correct outputs for adversarial triggers, and (3) a two-phase differential verification process that compares model outputs with and without the transform module to detect infringement. The method trains the transform module post-hoc on a frozen original model using contrastive loss to align adversarial image embeddings with text embeddings while preserving basic trigger performance.

## Key Results
- Only 0.21-0.32% performance degradation compared to baselines showing 0.78-8.04% drops
- Strong robustness against adversarial attacks with Res=0 for all forgery attempts in Table 4
- Effective verification across multiple datasets (MS-COCO, Flickr30k, CIFAR-10/100, VOC2007)
- Transform module effectiveness reaches 92.04% cosine distance improvement with 16 triggers

## Why This Works (Mechanism)

### Mechanism 1: In-Distribution Adversarial Trigger Generation
- Claim: Generating triggers from the original training distribution with adversarial noise injection enables stealthy watermarking that resists statistical anomaly detection.
- Mechanism: The method samples basic triggers $T_b$ from the ordinary dataset $D$, then applies adversarial perturbations via a generator $G(z)$ conditioned on the original model's embedding space. The blending operation $(1-m) \odot x + m \odot G(z)$ creates adversarial triggers that maintain visual fidelity ($\ell_2$-norm constraints $\|e_x - x\|_2 \leq \epsilon_1$) while inducing semantic deviation ($D(E_t(y) \| E_v(e_x)) \geq \delta$).
- Core assumption: Adversaries rely on detecting Out-of-Distribution (OoD) statistical anomalies in input-output behavior; in-distribution triggers evade this detection paradigm.
- Evidence anchors:
  - [abstract]: "adversarial trigger generation method to generate stealthy adversarial triggers from ordinary dataset, providing visual fidelity while inducing semantic shifts"
  - [section 3.3]: "Dissimilar to conventional OoD triggers, AGATE's perturbations maintain the original data distribution, thereby avoiding adversaries identifying triggers via abnormal analysis"
  - [corpus]: Weak direct corpus support; related work on adversarial watermarks exists (AdvCLIP, T-Patch) but doesn't validate this specific in-distribution approach.
- Break condition: If adversaries develop input-sanitization methods that detect adversarial perturbations directly (rather than OoD anomalies), stealthiness degrades. Evidence shows GAN-based adversarial noise achieves RMSE=8.73 and SSIM=0.96, suggesting detectability under sophisticated analysis.

### Mechanism 2: Post-Transform Module for Output Correction
- Claim: A lightweight transform module trained on the original model's embedding space can selectively correct outputs for adversarial triggers while preserving normal behavior, creating a verification key.
- Mechanism: The transform module $M$ is a three-layer MLP trained with paired samples $\{(x^{(i)}, y^{(i)}, e_x^{(i)})\}_{i=1}^N$ using contrastive loss (Eq. 3). It minimizes $d(f(e_x), g(y))$ for adversarial alignment while maintaining $d(f(x), g(y)) \geq \eta$ for intrinsic preservation. The module maps adversarial visual embeddings $E_v(e_x)$ toward text embeddings $E_t(y)$ in transform space.
- Core assumption: The transform module learns model-specific feature distributions; cross-model application fails to correct outputs, enabling unique ownership verification.
- Evidence anchors:
  - [abstract]: "post-transform module to correct the model output by narrowing the distance between adversarial trigger image embedding and text embedding"
  - [section 3.4]: "Module $T_B$ showed limited efficacy, failing to correct adversarial triggers' classifications result, demonstrating that only transform modules related to the anterior model were effective"
  - [corpus: RegionMarker]: Similar embedding-space watermarking for EaaS copyright protection validated in text domain; multimodal extension not directly confirmed.
- Break condition: If adversaries successfully reverse-engineer or replicate the transform module through query access, verification security fails. Paper states "adversaries...cannot correctly pass the second verification phase without the knowledge of transform module" but doesn't test against active transform-module extraction attacks.

### Mechanism 3: Two-Phase Differential Verification
- Claim: Comparing model outputs with and without the transform module creates unforgeable ownership evidence, as only legitimate model copies respond correctly to both phases.
- Mechanism: Phase I computes semantic discrepancy $\|E_v(e_x) - E_t(y)\|_{HS} \geq \sigma$ inducing erroneous outputs. Phase II applies transform module enforcing $\|M(E_v(e_x)) - M(E_t(y))\|_{HM} < \tau$. Verification succeeds iff $M(S(e_x)) = S(x)$ (Eq. 5). Adversaries who tamper with Phase I outputs cause phase consistency (violating differential requirement).
- Core assumption: Adversaries cannot simultaneously satisfy both phase conditions without possessing both the trigger set and the specific transform module trained for that model.
- Evidence anchors:
  - [abstract]: "two-phase watermark verification is proposed to judge whether the current model infringes by comparing the two results with and without the transform module"
  - [section 3.5]: "This dual requirement mechanism makes it impossible for adversaries to bypass verification even if the trigger leaks, as expressed in Equation (4) where $\Pr[\text{Verify}(T'_a) = 1 | T'_a \notin M] \leq \text{negl}(\lambda)$"
  - [corpus: SWAP, DNF]: Sequential/dual-layer watermarking approaches show promise for LLM/VLM domains; two-phase multimodal verification not directly validated externally.
- Break condition: Assumption of negligible probability (Eq. 4) is theoretical; empirical robustness tested only against two scenarios (partial trigger knowledge, no transform knowledge). Full white-box attacks with both components exposed remain untested.

## Foundational Learning

- **Concept: Contrastive Learning in Multimodal Embedding Spaces**
  - Why needed here: AGATE relies on manipulating distances between image and text embeddings; understanding cosine/Euclidean distance metrics in shared embedding spaces is essential for interpreting trigger generation and transform module training.
  - Quick check question: Given a CLIP model, can you explain why an image of a "dog" has small cosine distance to the text embedding of "dog" but large distance to "cat"?

- **Concept: Adversarial Perturbations and Visual Fidelity Trade-offs**
  - Why needed here: The framework balances imperceptibility (PSNR, SSIM) against semantic disruption (embedding distance); practitioners must understand how $\ell_p$-norm constraints affect both properties.
  - Quick check question: If you add Gaussian noise with $\epsilon = 0.1$ to an image, would you expect higher or lower SSIM compared to adversarial GAN-generated noise with the same $\ell_2$ bound?

- **Concept: Backdoor Watermarking vs. Ownership Fingerprinting**
  - Why needed here: Distinguishes trigger-based behavioral embedding (AGATE) from parameter-based fingerprinting (white-box methods); clarifies black-box verification constraints.
  - Quick check question: Can a backdoor watermark be verified without access to model parameters? What information must the verifier possess?

## Architecture Onboarding

- **Component map:**
  ```
  Original Dataset D
       ↓
  [Basic Trigger Sampling] → Basic Triggers Tb {x, y}
       ↓
  [Adversarial Generator G(z)] → Adversarial Triggers Ta {e_x, y}
       ↓
  [Original Model O (Visual/Text Encoders)] → Embeddings Ev(·), Et(·)
       ↓
  [Transform Module M (3-layer MLP)] ← Trained on (Tb, Ta) pairs
       ↓
  [Two-Phase Verifier]
      Phase I: S(e_x) output from suspicious model S
      Phase II: M(S(e_x)) output with transform
      Compare: M(S(e_x)) == S(x)?
  ```

- **Critical path:**
  1. Sample basic triggers from dataset (combinatorial randomization, Eq. 1)
  2. Generate adversarial perturbations using GAN conditioned on original model embeddings
  3. Train transform module using contrastive loss (Eq. 3) with learning rate 1e-3 for 1000 epochs
  4. Deploy two-phase verification: without transform (Phase I), with transform (Phase II)
  5. XOR comparison determines infringement (Eq. 6)

- **Design tradeoffs:**
  - **Trigger count vs. effectiveness:** Figure 5 shows fewer triggers increase transform module effectiveness ($D_{cos}$ reaches 92.04 with 16 triggers) but reduce security (easier for adversaries to identify). Paper notes: "trade-off between maintaining high security and achieving maximum effectiveness."
  - **Noise type vs. stealthiness:** Table 2 shows Poisson noise + Local/Blended addition achieves best visual fidelity (PSNR=42.63, SSIM=0.99) but GAN-based adversarial noise achieves best semantic divergence ($D_{cos}=24.02$). Choose based on threat model.
  - **Transform module complexity:** Lightweight 3-layer MLP preserves computational efficiency but may limit correction capacity for complex semantic shifts.

- **Failure signatures:**
  - **Cross-model transform failure:** Applying transform module $T_A$ trained on model $M_A$ to different model $M_B$ produces Res#2=False (Table 3, row 4: "MA 5.96 1.37 False TB 35.52 1.13 False"). Expected behavior—indicates model-specific binding works.
  - **Benign preservation violation:** If transform module causes basic triggers to misclassify, training objective (Eq. 3, intrinsic preservation term) is under-regularized. Increase $\lambda$ or $\eta$.
  - **Phase consistency attack:** If both phases produce identical outputs (Res#1 == Res#2), adversary may have removed triggers or applied counter-perturbations. Verification fails by design (Res=0).

- **First 3 experiments:**
  1. **Trigger visual fidelity validation:** Generate adversarial triggers using different noise types (Gaussian, Poisson, GAN) on CIFAR-10 samples; compute PSNR, SSIM, and $D_{cos}$. Verify GAN-based approach achieves RMSE<10 and $D_{cos}<25$ (Table 2 baseline).
  2. **Transform module dependency test:** Train transform module $T_A$ on CLIP ViT-B-16-quickgelu; test correction on same model vs. CLIP ViT-B-16 (laion400m_e32). Confirm $\Delta_{cos}$ improvement >80 for matching model, <30 for mismatched (Table 3 pattern).
  3. **Two-phase verification robustness:** Simulate adversary with partial knowledge (forged triggers with different noise/shapes per Table 4). Verify Res=0 for all forgery attempts, Res=1 for legitimate triggers. Test with 16, 64, 128 trigger counts to measure effectiveness-security trade-off.

## Open Questions the Paper Calls Out

- **Question:** How can the optimal number of adversarial triggers be determined to balance the trade-off between transform module effectiveness and the risk of trigger detection?
  - **Basis in paper:** [explicit] Section 4.6 concludes that reducing the number of triggers strengthens the transform module's embedding modification but lowers security, noting that "there existed a trade-off between maintaining high security and achieving maximum effectiveness."
  - **Why unresolved:** The paper identifies the inverse relationship between trigger count and security/effectiveness but does not propose a method to optimize this variable for specific deployment scenarios.
  - **What evidence would resolve it:** An algorithm or heuristic that dynamically adjusts the trigger set size based on defined security thresholds and downstream task requirements.

- **Question:** To what extent does AGATE retain verification accuracy when the stolen model undergoes fine-tuning or knowledge distillation?
  - **Basis in paper:** [inferred] The introduction identifies model extraction and parameter replication as key threats, yet the robustness evaluation (Section 4.5) focuses solely on trigger forgery and partial knowledge attacks, omitting model modification attacks like fine-tuning.
  - **Why unresolved:** Backdoor-based watermarks are often fragile to model fine-tuning, and the paper lacks empirical validation of AGATE's persistence when the stolen model's weights are updated or distilled.
  - **What evidence would resolve it:** Experimental results measuring watermark verification success rates after applying standard fine-tuning or model distillation techniques to the stolen surrogate model.

- **Question:** Can the adversarial trigger generation and alignment methodology generalize effectively to generative multimodal models or non-Transformer architectures?
  - **Basis in paper:** [inferred] While the framework claims to be model-agnostic, the Implementation Details (Section 4.1) and experiments restrict evaluation to the CLIP family (ViT-B-16, ViT-B-32).
  - **Why unresolved:** The dependency on CLIP's specific embedding structure for the "adversarial alignment" loss (Eq. 3) suggests potential constraints when applied to generative models (e.g., Stable Diffusion components) or purely convolutional encoders.
  - **What evidence would resolve it:** Application of AGATE to distinct multimodal architectures (e.g., ResNet-based or diffusion-based models) demonstrating consistent stealthiness and verification metrics.

## Limitations
- **Underspecified adversarial generator:** GAN-based trigger generation shows superior performance but only basic noise injection methods are detailed in the paper
- **Transform module security:** No empirical testing against active transform module extraction attacks through query access
- **Architecture dependency:** Results are CLIP-specific without validation on alternative multimodal architectures or modalities

## Confidence
- **High Confidence:** Two-phase verification mechanism design and differential logic - supported by clear mathematical formulation and consistent experimental results
- **Medium Confidence:** In-distribution trigger stealthiness claims - experimental results show good visual fidelity (PSNR=42.63, SSIM=0.99), but limited testing against sophisticated statistical anomaly detection methods
- **Low Confidence:** Generalizability to other multimodal architectures beyond CLIP - results are CLIP-specific without ablation studies on different encoder architectures or modalities

## Next Checks
1. **Adversarial Detector Evaluation:** Test AGATE triggers against state-of-the-art adversarial perturbation detectors (e.g., deep feature analysis, statistical anomaly classifiers) to validate true stealthiness claims
2. **Transform Module Extraction Attack:** Implement query-based attacks attempting to reconstruct the transform module from a black-box victim model to assess actual ownership verification security
3. **Cross-Architecture Transferability:** Apply AGATE framework to alternative multimodal models (e.g., BLIP, Flamingo) to evaluate whether in-distribution trigger generation and transform modules generalize beyond CLIP