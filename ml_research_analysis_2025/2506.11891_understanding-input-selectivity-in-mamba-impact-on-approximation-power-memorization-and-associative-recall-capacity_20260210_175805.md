---
ver: rpa2
title: 'Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization,
  and Associative Recall Capacity'
arxiv_id: '2506.11891'
source_url: https://arxiv.org/abs/2506.11891
tags:
- mamba
- state
- layer
- input
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work provides a theoretical analysis of Mamba's input selectivity,\
  \ proving that its S6 layer can represent projections onto Haar wavelets\u2014enabling\
  \ better approximation of discontinuous functions than its S4D predecessor. Sensitivity\
  \ analysis reveals that while S6 suffers from exponential memory decay like S4D,\
  \ input selectivity allows dynamic adjustment of decay rates."
---

# Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity

## Quick Facts
- arXiv ID: 2506.11891
- Source URL: https://arxiv.org/abs/2506.11891
- Reference count: 40
- This work provides a theoretical analysis of Mamba's input selectivity, proving that its S6 layer can represent projections onto Haar wavelets—enabling better approximation of discontinuous functions than its S4D predecessor.

## Executive Summary
This paper provides a theoretical analysis of Mamba's input selectivity mechanisms, demonstrating how input-dependent discretization enables superior function approximation and selective memorization. The authors prove that Mamba's S6 layer can represent Haar wavelet projections, offering advantages over S4D's Fourier bases for discontinuous functions. They construct analytical solutions for solving MQAR and Induction Heads tasks with constant model sizes independent of sequence length, and validate these findings empirically. The work reveals that convolution and gating components are critical to Mamba's performance and suggests architectural improvements by modifying how input dependence is incorporated into the SSM state matrix.

## Method Summary
The authors analyze Mamba's input selectivity through three synthetic tasks: KEEP n-TH (retrieve the n-th token), MQAR (Multiple-Query Associative Recall), and Induction Heads. They construct analytical solutions for Mamba variants to solve these tasks exactly, requiring model sizes independent of sequence length. The theoretical analysis proves Haar wavelet approximation capability, sensitivity decay bounds, and memorization capacity. Empirical validation involves training 1-layer Mamba models with Adam optimizer for 600 epochs on synthetic data, varying embedding dimensions and state sizes to verify theoretical bounds.

## Key Results
- S6 layer can represent Haar wavelet projections, enabling O(2^{-N/3m}) error for piecewise-constant functions versus O(N^{-1}) for S4D's Fourier bases
- Input selectivity allows dynamic adjustment of exponential memory decay rates, enabling selective retention of relevant information
- Mamba variants can solve MQAR and Induction Heads with model sizes independent of sequence length, requiring d = O(κ + log|V|) for MQAR
- Convolution and gating components are essential for efficient associative recall, with Mamba-2 achieving N = log κ versus N = κ for standard Mamba

## Why This Works (Mechanism)

### Mechanism 1: Haar Wavelet Approximation via Input-Dependent Discretization
- Claim: The S6 layer can represent projections onto Haar wavelets, enabling superior approximation of discontinuous functions compared to S4D's Fourier bases.
- Mechanism: Input-dependent discretization parameter ∆(x) can be driven to ∞ or 0 by manipulating its linear transformation weights, effectively creating Heaviside-like basis functions. Linear combinations of these approximate Haar wavelets with arbitrary precision (3 basis functions per wavelet).
- Core assumption: Input is augmented with time positional encoding to enable ∆(s) to depend directly on position rather than input value.
- Evidence anchors:
  - [abstract] "we prove that the S6 layer of Mamba can represent projections onto Haar wavelets, providing an edge over its Diagonal SSM (S4D) predecessor in approximating discontinuous functions"
  - [Section 4.1, Theorem 1] Formal construction showing ∆(s) → ∞ for s < t̂_k and ∆(s) → 0 otherwise produces Heaviside functions
  - [corpus] Limited direct corpus support for wavelet-based SSM analysis; primarily novel to this work.
- Break condition: Without positional encoding, S6 cannot autonomously determine absolute position (though Proposition 1 notes a separate layer with all-ones input can recover time).

### Mechanism 2: Dynamic Counteraction of Memory Decay via Selective ∆(x)
- Claim: S6 suffers exponential sensitivity decay like S4D, but input selectivity enables dynamic adjustment of the decay rate to preserve relevant information.
- Mechanism: Sensitivity ∂h_t/∂x_j decays as e^{-λ∑∆(x_r)}. By setting λ∆(x_t) → 0 (equivalently e^{-λ∆(x_t)} → 1), the model can "freeze" state updates to retain specific information. The model learns to push e^{-λ∆_t} → 1 for relevant tokens and → 0 for irrelevant ones.
- Core assumption: The model can learn to identify which tokens require preservation vs. forgetting based on task structure.
- Evidence anchors:
  - [abstract] "Sensitivity analysis reveals that while S6 suffers from exponential memory decay like S4D, input selectivity allows dynamic adjustment of decay rates"
  - [Section 4.2, Lemma 2] Formal bound showing lim_{t→∞} |∂h^M_t/∂x_j| ≥ e^{-c} when ∑λ∆(x_r) ≤ c
  - [Figure 1] Empirical distribution showing learned e^{-λ∆_t} clustering near 1 after target position and near 0 before it in KEEP n-TH tasks
  - [corpus] Context-Selective SSM paper (arXiv:2510.14027) discusses similar selective mechanisms but through feedback rather than discretization.
- Break condition: If ∆(x) cannot approach 0 sufficiently (e.g., SoftPlus constraint), long-sequence retention degrades.

### Mechanism 3: Structured Hidden State for Associative Recall
- Claim: Mamba's hidden state matrix (d×N) can be organized with columns indexed by keys storing associated values, enabling constant-model-size solutions to MQAR regardless of sequence length.
- Mechanism: The outer product structure h_t = ∑_{s=1}^t (x̂_s ⊗ B_s) allows B(x) to select which column receives information. C(x) retrieves the appropriate column at query time. Short convolutions (kernel size 2) combine key-value pairs before SSM processing.
- Core assumption: Embedding dimension d is sufficiently large to maintain near-orthogonal key/value representations (Johnson-Lindenstrauss reduction to O(log κ + log |V|)).
- Evidence anchors:
  - [abstract] "The authors construct analytical solutions for Mamba variants to solve Multiple-Query Associative Recall (MQAR)... demonstrating exact solutions with model sizes independent of sequence length"
  - [Section 5.1, Theorem 2] Construction requiring d = O(κ + log|V|), N = κ for Mamba without gating
  - [Table 3] Visual representation of hidden state matrix structure with key-indexed columns
  - [corpus] SeRpEnt (arXiv:2501.11729) explores selective resampling mechanisms but doesn't address MQAR-specific constructions.
- Break condition: Convolution component is essential—without it, S4D cannot efficiently pair keys with values and requires d = O(κ log|V|) instead.

## Foundational Learning

- **Concept: Discrete State-Space Models and Zero-Order Hold Discretization**
  - Why needed here: Understanding Equations 3-5 is prerequisite to grasping how input-dependent ∆, B, C parameters change SSM dynamics.
  - Quick check question: Given h_t = Λ_t h_{t-1} + B_t x_t, what does the product ∏_{r=s+1}^t Λ_r represent in the unrolled solution?

- **Concept: Wavelet vs. Fourier Basis Approximation**
  - Why needed here: The core theoretical contribution compares S6's wavelet-like bases to S4D's exponential/Fourier bases for discontinuous function approximation.
  - Quick check question: Why do Fourier bases achieve only O(N^{-1}) error for piecewise-constant functions while Haar wavelets achieve O(2^{-N/3m})?

- **Concept: Johnson-Lindenstrauss Lemma for Dimensionality Reduction**
  - Why needed here: All three MQAR theorems rely on JL embeddings to reduce embedding dimensions while maintaining near-orthogonality.
  - Quick check question: What embedding dimension p does JL guarantee for preserving ϵ-approximate inner products among d vectors?

## Architecture Onboarding

- **Component map:**
  Embedding → Short Convolution (kernel size 2, shared in Mamba / independent ×3 in Mamba-2) → SSM Layer (S6 with input-dependent ∆, B, C) → Gate branch (elementwise multiply with linear projection of input) → Output linear layer

- **Critical path:**
  1. Embedding must distinguish keys from values (different magnitude scalars or orthogonal subspaces)
  2. Convolution must combine adjacent tokens into key-value pairs while filtering noise
  3. B(x) must project to key subspace to select correct hidden state column
  4. C(x) must project to query/key subspace to retrieve correct column
  5. For INDUCTION HEADS, Mamba-∆⊤ variant required with ∆ varying along state dimension N

- **Design tradeoffs:**
  - Mamba: Single shared convolution, state matrix Λ diagonal (more expressive), requires N = κ for MQAR
  - Mamba-2: Three independent convolutions, Λ scalar (less expressive), achieves N = log κ for MQAR
  - S4D mixer in Mamba architecture: Requires gating, needs d = O(κ log|V|) vs. O(κ + log|V|)
  - Mamba-∆⊤ vs. Mamba: ∆ varies along state dimension N instead of embedding dimension d; better for INDUCTION HEADS

- **Failure signatures:**
  - Model fails MQAR when d < theoretical bound (Figure 2 shows sharp transition)
  - S6 without positional encoding fails KEEP n-TH (Table 1: 0.08 vs. 1.00 with PE)
  - S4D fails long-sequence KEEP n-TH even with PE due to inability to selectively retain (Table A.2: 0.03 accuracy at T=50)
  - Mamba fails INDUCTION HEADS hard cases (special tokens at sequence extremes) when using negative eigenvalues for discounting

- **First 3 experiments:**
  1. **KEEP n-TH ablation**: Train Mamba + PE, Mamba - PE, S4D + PE on sequences of length T ∈ {20, 50} with n = 5. Verify that Mamba+PE learns e^{-λ∆_t} → 1 after position n and → 0 before it. Confirm S4D fails at T=50.
  2. **MQAR scaling sweep**: Train Mamba, Mamba-2, Mamba-S4D with varying embedding dimension d and number of keys κ. Plot accuracy contours against theoretical bounds from Theorems 2-4. Verify sharp transition at predicted d thresholds.
  3. **INDUCTION HEADS with Mamba-∆⊤**: Implement ∆(x) varying along state dimension (Equation 15). Compare standard Mamba vs. Mamba-∆⊤ on hard setting with special tokens at sequence extremes. Verify Mamba-∆⊤ achieves 100% above d = 2|V|, N = |V| bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can standard optimization algorithms like gradient descent reliably discover the analytically constructed solutions for MQAR and Induction Heads, or do these solutions reside in "isolated" regions of the parameter space?
- Basis in paper: [explicit] Section 6 states, "Our current theory does not consider the aspects of optimization and generalization, both of which are interesting future directions to explore."
- Why unresolved: The paper provides existence proofs (mechanistic constructions) for solving tasks but does not analyze the loss landscape or training dynamics required to reach these specific parameter configurations.
- What evidence would resolve it: A theoretical analysis of the loss landscape convexity or empirical demonstrations showing SGD consistently converging to the constructed solutions from random initialization.

### Open Question 2
- Question: How does the theoretical capacity of Mamba's input selectivity scale to complex, multi-step reasoning tasks such as k-Hop Induction Heads or Sequential Function Composition?
- Basis in paper: [explicit] Section 6 lists extending the analysis to "more complicated tasks such as k-HOP INDUCTION HEADS... SEQUENTIAL FUNCTION COMPOSITION... and POINTER VALUE RETRIEVAL would be a natural next step."
- Why unresolved: The current theoretical bounds apply only to single-step associative recall (MQAR) and 1-hop Induction Heads; the architecture's ability to chain these operations theoretically is unproven.
- What evidence would resolve it: Derivations of model size bounds (embed/state dimensions) required to solve k-Hop tasks, similar to the bounds provided for MQAR in Theorems 2-4.

### Open Question 3
- Question: Does generalizing input dependence to the state dimension (as in the proposed Mamba-∆⊤ variant) yield consistent empirical improvements in large-scale language modeling compared to the standard embedding-dimension dependence?
- Basis in paper: [explicit] Section 6 mentions "opportunities to further improve Mamba, such as by changing the way input dependence is incorporated into the SSM state matrix."
- Why unresolved: The Mamba-∆⊤ variant was evaluated only on the synthetic Induction Heads task, showing improved selectivity for "freezing" specific state columns, but its efficiency/performance on general language tasks is unknown.
- What evidence would resolve it: Benchmark comparisons (e.g., perplexity on standard corpora) between standard Mamba and Mamba-∆⊤ at scale.

## Limitations

- The theoretical analysis relies on idealized conditions including perfect orthogonal embeddings and exact constructions that may not hold in practical training scenarios
- The results are derived for specific Mamba variants with particular architectural choices and may not generalize to other input-selective SSM architectures
- The experiments focus on synthetic tasks with controlled properties, and findings may not directly translate to performance on real-world data or more complex downstream tasks

## Confidence

**High Confidence**:
- The formal constructions for MQAR and Induction Heads tasks are mathematically rigorous and the experimental results align with theoretical predictions
- The core mechanism of input-dependent discretization enabling Haar wavelet approximation is supported by both theory and ablation studies
- The critical role of convolution and gating components in enabling efficient associative recall is empirically validated

**Medium Confidence**:
- The sensitivity analysis showing exponential memory decay and its dynamic adjustment via input selectivity is theoretically sound, but the extent to which models learn optimal ∆(x) in practice may vary
- The theoretical bounds on model size for achieving exact solutions are tight in synthetic settings, but may be looser for more complex distributions or noisy data

**Low Confidence**:
- The generalization of Haar wavelet approximation benefits to natural language or vision tasks is speculative without further empirical validation
- The assumption that positional encoding is the only way to achieve absolute time dependence in S6 may be too restrictive; other architectural modifications could potentially address this

## Next Checks

1. **Robustness to Initialization and Training Dynamics**: Systematically vary initialization schemes and training hyperparameters (learning rate, batch size, optimizer) to assess the stability of learned input selectivity patterns (e.g., e^{-λ∆_t} distributions) and task performance across KEEP n-TH, MQAR, and Induction Heads.

2. **Transfer to Real-World Tasks**: Apply the insights from synthetic task analysis to fine-tuning or pre-training Mamba models on established benchmarks (e.g., WikiText, LRA, or vision tasks like CIFAR). Measure if input-selective architectural modifications (e.g., Mamba-∆⊤) provide consistent gains and if learned ∆(x) patterns reflect task-specific selectivity.

3. **Ablation of Input-Dependent Discretization Mechanisms**: Replace the SoftPlus nonlinearity in ∆(x) with alternative activation functions (e.g., sigmoid, tanh) or remove input dependence entirely (fixed ∆) to quantify the exact contribution of input selectivity to approximation power and memorization capacity. Compare performance degradation against theoretical predictions.