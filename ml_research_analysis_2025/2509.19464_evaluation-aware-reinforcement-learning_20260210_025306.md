---
ver: rpa2
title: Evaluation-Aware Reinforcement Learning
arxiv_id: '2509.19464'
source_url: https://arxiv.org/abs/2509.19464
tags:
- policy
- evaluation
- value
- eva-rl
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces evaluation-aware reinforcement learning (EvA-RL),
  a framework that learns policies optimized for both performance and ease of evaluation.
  Unlike standard RL, which learns arbitrary policies then evaluates them afterward,
  EvA-RL explicitly trains policies to minimize evaluation error under a value prediction
  scheme during training.
---

# Evaluation-Aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.19464
- Source URL: https://arxiv.org/abs/2509.19464
- Reference count: 40
- Primary result: Achieves competitive returns while substantially reducing evaluation error through co-learning policies and value predictors

## Executive Summary
This paper introduces evaluation-aware reinforcement learning (EvA-RL), a framework that learns policies optimized for both performance and ease of evaluation. Unlike standard RL, which learns arbitrary policies then evaluates them afterward, EvA-RL explicitly trains policies to minimize evaluation error under a value prediction scheme during training. The authors propose co-learning a transformer-based state-value predictor alongside the policy, enabling accurate policy evaluation from limited assessment rollouts in a proxy environment. Theoretical analysis reveals a tradeoff between performance and evaluation accuracy when using a fixed predictor, which the co-learning approach mitigates. Experiments across discrete (Asterix, Freeway, Space Invaders) and continuous (HalfCheetah, Reacher, Ant) control domains show that EvA-RL achieves competitive returns while substantially reducing evaluation error compared to standard RL plus OPE baselines.

## Method Summary
EvA-RL optimizes policies for both expected return and predictability by adding a penalty term to the standard RL objective. The method co-learns a transformer-based value predictor alongside the policy using assessment environment rollouts as a proxy for evaluation. The predictor conditions on assessment (state, return) pairs to estimate deployment values. Training proceeds in two stages: warmup with standard A2C, then alternating predictor updates via MSE regression and policy updates using a modified policy gradient that includes predictor gradient terms. The approach uses a replay buffer storing deployment states, returns, and assessment datasets from recent policies, maintaining a sliding window of m recent policies.

## Key Results
- EvA-RL achieves competitive returns while substantially reducing evaluation error compared to standard RL plus OPE baselines
- Co-learning the predictor alongside the policy mitigates the performance-accuracy tradeoff identified with fixed predictors
- Assessment environment rollouts with as few as 5 start-states provide sufficient behavioral encoding for accurate deployment-value prediction
- The transformer-based predictor effectively generalizes from assessment returns to deployment state values across diverse discrete and continuous control domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing for predictability alongside return yields policies with lower evaluation error, but creates a tradeoff with performance when using a fixed predictor
- Mechanism: The EvA-RL objective adds a penalty term β(V^π_D(s) - V̂^π_D(s))² to standard return maximization. Increasing β forces the policy toward value functions that the predictor can estimate accurately, constraining the policy space and reducing achievable returns
- Core assumption: The value predictor generalizes sufficiently; if the predictor is a poor estimator, the constraint may not meaningfully improve evaluation accuracy
- Evidence anchors:
  - [abstract] "Theoretical analysis reveals a tradeoff between performance and evaluation accuracy when using a fixed predictor"
  - [section 3.1] Proposition 3.1: "Increasing β in the EvA-RL optimization performed using a fixed value predictor monotonically decreases the evaluation error and the expected return of the resultant policy"
  - [corpus] Weak direct evidence; related work on evaluation-aware LLMs suggests awareness of evaluation context alters behavior, but does not validate this specific mechanism
- Break condition: If the predictor's training distribution diverges significantly from the policy's value landscape, the penalty may optimize for the wrong target, harming both accuracy and return

### Mechanism 2
- Claim: Assessment environment rollouts provide a compressed behavioral encoding that enables accurate deployment-value prediction with limited samples
- Mechanism: A transformer predictor conditions on k assessment start-states and their returns {(s^A_i, g(h^A_i))}, then predicts V^π_D(s) for arbitrary query states. The assessment environment acts as a proxy with shorter horizons or easier access, and the predictor learns to generalize from assessment behavior to deployment performance
- Core assumption: Assessment environment dynamics and start-states are sufficiently representative of deployment-relevant behaviors; if the assessment states are uninformative, the encoding is weak
- Evidence anchors:
  - [abstract] "enables accurate policy evaluation, conditioned on a small number of rollouts in an assessment environment"
  - [section 3.2] "using only start-states and corresponding returns was sufficient"
  - [corpus] No direct validation; OPE methods assume behavior policy coverage, while EvA-RL explicitly shapes policy to match predictor capabilities
- Break condition: If assessment and deployment environments have fundamentally different dynamics or reward structures, the mapping from assessment returns to deployment values becomes unreliable

### Mechanism 3
- Claim: Co-learning the predictor alongside the policy mitigates the accuracy-performance tradeoff by distributing the burden of predictability
- Mechanism: Rather than constraining the policy to match a fixed predictor, the predictor adapts to the policy's evolving value landscape. The two-stage update first trains the predictor on recent deployment data, then updates the policy using the fresh predictor. This allows the policy to maintain performance while the predictor "catches up"
- Core assumption: The predictor can learn sufficiently fast to track policy changes; slow predictor learning creates lag, potentially destabilizing training
- Evidence anchors:
  - [abstract] "co-learn an assessment-conditioned state-value predictor alongside the policy...mitigates this tradeoff"
  - [section 4.1 Figure 2] "Co-learning allows for better performance while maintaining low evaluation error"
  - [corpus] Weak; related work on continual RL discusses non-stationary learning but doesn't validate co-learning dynamics
- Break condition: If predictor learning rate is too low relative to policy updates, the predictor lags, causing the policy to chase stale evaluation targets

## Foundational Learning

- **Off-Policy Evaluation (OPE) methods**:
  - Why needed here: EvA-RL is positioned as an alternative to OPE for policy assessment. Understanding importance sampling (IS), per-decision IS (PDIS), fitted Q-evaluation (FQE), and doubly robust (DR) estimators provides the baseline comparison
  - Quick check question: Why does standard importance sampling suffer exponential variance with horizon length?

- **Policy gradient theorem**:
  - Why needed here: EvA-RL modifies the standard policy gradient by adding predictor-gradient terms. Understanding ∇_θ J(θ) = E[∇_θ log π(a|s) · Q(s,a)] is prerequisite for grasping the expanded gradient in Eq. 7
  - Quick check question: In actor-critic methods, what role does the critic play in variance reduction?

- **Transformer cross-attention**:
  - Why needed here: The value predictor uses a transformer to attend over assessment (state, return) pairs when predicting query-state values. Understanding attention over variable-length contexts is essential
  - Quick check question: How does self-attention differ from cross-attention in conditioning on external context?

## Architecture Onboarding

- **Component map**:
  - Policy network (π_θ) -> Action selection given states
  - Value predictor transformer (ψ_φ) -> Assessment returns + query state -> value estimate
  - Replay buffer (B_ψ) -> Stores (deployment state, deployment return, assessment dataset) tuples
  - Assessment environment (M_A) -> Proxy MDP with fixed start-states, shorter horizon
  - Deployment environment (M_D) -> Target MDP for actual return optimization

- **Critical path**:
  1. Warmup phase: Train policy with standard A2C (β=0) for fixed iterations; collect deployment and assessment data
  2. Predictor warmup: Train ψ_φ on buffered data via MSE loss: E[(g(h_D) - V̂_φ(s_D; Ξ_A))²]
  3. Co-learning loop:
     - Sample batch from B_ψ → update predictor
     - Collect fresh rollouts → compute modified gradient (Eq. 7) → update policy
     - Append new data to B_ψ, maintain sliding window of m recent policies

- **Design tradeoffs**:
  - **β selection**: Higher β → lower evaluation error but potential performance drop; authors use β ∈ {0.01, 0.1} (discrete) and β ∈ {5×10⁻⁴} (continuous)
  - **Assessment start-state selection**: Random sampling from base policy worked, but systematic design (e.g., diverse state coverage) may improve encoding quality
  - **Predictor input richness**: Using only (state, return) pairs sufficed; full trajectories could provide richer signals but increase complexity

- **Failure signatures**:
  - **Predictor collapse**: MAE near zero but returns significantly below baseline → predictor overfitted to recent policies, not generalizing
  - **Policy stagnation**: Returns plateau early while evaluation error remains high → β too high or predictor learning rate too low
  - **Assessment-deployment mismatch**: Low prediction error on assessment but high deployment error → assessment environment not representative

- **First 3 experiments**:
  1. **Validate tradeoff with frozen predictor**: Train predictor on standard RL data, freeze it, run EvA-RL with varying β. Expect: higher β → lower MAE, lower returns (replicates Figure 2)
  2. **Compare co-learned predictor vs. OPE**: Run full EvA-RL with co-learning, evaluate final predictor against FQE/PDIS/DR on same data. Expect: EvA-RL predictor achieves lower MAE (replicates Figure 3)
  3. **Assessment horizon sensitivity**: Vary assessment trajectory length (e.g., 5, 10, 20 steps). Monitor impact on prediction error and returns. Hypothesis: shorter horizons may reduce encoding quality; optimal length depends on environment dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can assessment start-states be systematically designed (rather than randomly sampled from a base policy) to produce stronger behavioral encodings and improve evaluation accuracy?
- Basis in paper: [explicit] "Our work depended on randomly sampled states from base policy for defining the assessment start-states. While effective in our experiments, systematic design may lead to stronger behavioral encodings and improved evaluation."
- Why unresolved: The paper uses random sampling as a simple baseline but does not investigate principled methods for selecting assessment states that maximize information about deployment behavior
- What evidence would resolve it: Experiments comparing random selection against methods such as coverage-based selection, diversity optimization, or information-theoretic state selection, showing improved prediction accuracy or sample efficiency

### Open Question 2
- Question: Would incorporating full assessment trajectories (rather than just start-states and returns) into the value predictor improve evaluation accuracy, and what architectural modifications would be required?
- Basis in paper: [explicit] "Our predictor used only states and returns from the assessment rollouts, rather than full trajectories. Although this design proved sufficient, richer inputs could yield more accurate predictors."
- Why unresolved: The transformer architecture currently processes only state-return pairs; processing full trajectories would require handling variable-length sequential information and potentially different attention mechanisms
- What evidence would resolve it: Ablation studies comparing current predictors against trajectory-conditioned variants, measuring prediction error reductions across diverse environments

### Open Question 3
- Question: Can EvA-RL maintain its benefits when the assessment environment has different transition or reward dynamics from the deployment environment (beyond just different start-states)?
- Basis in paper: [inferred] The paper's experimental setup uses identical dynamics between assessment and deployment environments (differing only in start-state distribution and horizon). The framework allows for different dynamics, but this capability was not empirically validated
- Why unresolved: Real-world scenarios often require evaluating policies in simplified simulations or proxy environments with model discrepancies, which could introduce systematic biases in value predictions
- What evidence would resolve it: Experiments where assessment environments have perturbed dynamics, different observation spaces, or simplified physics, showing whether co-learning can adapt to or compensate for these discrepancies

### Open Question 4
- Question: How does EvA-RL scale to high-dimensional state spaces (e.g., raw pixel observations) and more complex, sparse-reward tasks?
- Basis in paper: [inferred] Experiments use low-dimensional state representations (MinAtar features, Brax proprioceptive states). The transformer predictor may face scalability challenges with high-dimensional inputs requiring different architectural choices
- Why unresolved: The linear attention formulation in theoretical analysis and small hidden dimensions (16) in experiments suggest the approach may not directly transfer to visual domains or tasks requiring long-horizon credit assignment
- What evidence would resolve it: Application to pixel-based control tasks (e.g., Atari from pixels, vision-based robotics) with analysis of computational scaling and prediction accuracy as state dimensionality increases

## Limitations

- The tradeoff between performance and evaluation accuracy under fixed predictors is theoretically proven but may not fully capture real-world predictor generalization limits
- The transformer predictor's performance depends heavily on assessment start-state selection, which was simple random sampling in experiments but may not generalize
- Co-learning effectiveness assumes predictor can track policy changes quickly enough, but learning rate interactions between policy and predictor updates are not fully characterized
- Assessment environment design choices (start states, horizon length) significantly impact results but were not systematically varied

## Confidence

- **High confidence**: The core framework of EvA-RL and its implementation on standard benchmarks; the existence of performance-accuracy tradeoff with fixed predictors
- **Medium confidence**: The extent to which co-learning actually mitigates the tradeoff in all scenarios; the sufficiency of simple assessment encoding (states + returns) across diverse environments
- **Low confidence**: Long-term stability of the co-learning dynamics; performance under assessment-deployment environment mismatch; optimal assessment environment design principles

## Next Checks

1. **Predictor Generalization Stress Test**: Train predictor on policy A, evaluate on policy B (different policy class). Measure whether EvA-RL still achieves low evaluation error, revealing predictor generalization limits
2. **Assessment-Deployment Mismatch Analysis**: Create assessment and deployment environments with controlled differences (e.g., different transition dynamics). Evaluate how prediction error and returns degrade, quantifying environment mismatch sensitivity
3. **Co-learning Stability Study**: Vary predictor learning rate relative to policy learning rate. Identify thresholds where predictor lag causes policy training instability, and measure recovery dynamics when predictors catch up