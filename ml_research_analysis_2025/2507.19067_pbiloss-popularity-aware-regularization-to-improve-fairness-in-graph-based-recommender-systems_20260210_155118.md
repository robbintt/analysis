---
ver: rpa2
title: 'PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based
  Recommender Systems'
arxiv_id: '2507.19067'
source_url: https://arxiv.org/abs/2507.19067
tags:
- items
- popularity
- popular
- fairness
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PBiLoss introduces a regularization-based loss function to mitigate\
  \ popularity bias in graph-based recommender systems. It penalizes over-reliance\
  \ on popular items by integrating two sampling strategies\u2014Popular Positive\
  \ (PopPos) and Popular Negative (PopNeg)\u2014to promote the exposure of less popular\
  \ but relevant items."
---

# PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems

## Quick Facts
- **arXiv ID:** 2507.19067
- **Source URL:** https://arxiv.org/abs/2507.19067
- **Reference count:** 40
- **Primary result:** PBiLoss reduces popularity bias in GNNs by up to 10.5% while maintaining accuracy.

## Executive Summary
PBiLoss introduces a regularization-based loss function to mitigate popularity bias in graph-based recommender systems. It penalizes over-reliance on popular items by integrating two sampling strategies—Popular Positive (PopPos) and Popular Negative (PopNeg)—to promote the exposure of less popular but relevant items. The method employs either a fixed popularity threshold or a threshold-free approach for distinguishing popular items. Experiments on three real-world datasets (Epinions, iFashion, MovieLens) show that PBiLoss significantly reduces popularity bias, improving fairness metrics such as PRU and PRI by up to 10.5%, while maintaining or slightly enhancing recommendation accuracy and ranking performance. The method is model-agnostic and can be seamlessly integrated into state-of-the-art graph-based recommender systems like LightGCN and its variants.

## Method Summary
PBiLoss augments the standard BPR loss with a popularity-aware regularization term to reduce popularity bias in graph-based recommenders. It introduces two sampling strategies: PopPos (sampling unpopular relevant items against popular relevant items) and PopNeg (sampling unpopular relevant items against popular irrelevant items). The method can use either a fixed popularity threshold or a threshold-free probabilistic sampling approach. By explicitly penalizing the model's tendency to over-recommend popular items, PBiLoss encourages exposure of less popular but relevant items while maintaining recommendation accuracy.

## Key Results
- PBiLoss significantly reduces popularity bias metrics (PRU and PRI) by up to 10.5% across three datasets
- The method maintains or slightly improves recommendation accuracy metrics (NDCG, F1, MAP)
- PopNeg sampling strategy generally outperforms PopPos in both fairness improvement and accuracy maintenance
- Fixed threshold approach typically outperforms threshold-free methods in both fairness and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Popularity-Aware Gradient Regularization
PBiLoss augments the optimization objective to $L_{total} = L_{BPR} + w \cdot L_{PBi}$. While standard BPR optimizes for relevance regardless of popularity, the $L_{PBi}$ term explicitly penalizes the model when it ranks popular items higher than unpopular ones. This forces gradient updates to account for item frequency, counteracting the natural amplification of popular items caused by their high degree in graph structure.

### Mechanism 2: Targeted Sampling Strategies (PopPos vs. PopNeg)
PopPos samples a positive unpopular item and a positive popular item, promoting niche content the user has interacted with. PopNeg samples a positive unpopular item and a negative popular item, specifically demoting popular items the user hasn't interacted with. This distinction treats "popular and relevant" (needs diversity) and "popular and irrelevant" (needs suppression) differently.

### Mechanism 3: Probabilistic vs. Fixed Popularity Thresholds
Fixed threshold uses a hyperparameter $\alpha$ to create binary sets of popular and unpopular items, providing stability. No-threshold approach samples items based on probability distributions proportional to degree and inverse degree, offering adaptability to varying data distributions.

## Foundational Learning

- **Concept: Bayesian Personalized Ranking (BPR) Loss**
  - Why needed here: PBiLoss is a wrapper/extension of BPR. Understanding the pairwise ranking objective $(u, i, j)$ is required to interpret how popularity penalty is applied.
  - Quick check question: Can you explain why standard BPR optimizes for relative order rather than absolute score, and how this relates to popularity bias?

- **Concept: Graph Signal Propagation (Message Passing)**
  - Why needed here: The paper targets GNNs like LightGCN. You must understand how message passing aggregates neighbor features, which inherently amplifies signal from high-degree (popular) nodes.
  - Quick check question: If node A has 1000 neighbors and node B has 2 neighbors, how does standard mean-aggregation in a GNN bias representation learning?

- **Concept: Popularity Bias Metrics (PRU & PRI)**
  - Why needed here: Success of PBiLoss is measured by PRU (User-side) and PRI (Item-side) correlation. Without understanding these, you cannot validate if model is becoming "fairer."
  - Quick check question: Does a PRU value closer to 0 or closer to 1 indicate reduction in popularity bias?

## Architecture Onboarding

- **Component map:** Degree Calculator -> Popularity Sampler (PopPos/PopNeg) -> PBiLoss Term; Backbone: LightGCN; Integration: Total Loss = BPR Loss + w * PBiLoss

- **Critical path:** The Popularity Sampler is the critical implementation detail. You must implement logic to calculate node degrees once, and then efficiently sample pairs $(i, j)$ based on constraints defined in Section 4.3.2.

- **Design tradeoffs:**
  - PopNeg is generally more effective but computationally heavier as it samples from large negative sets
  - Thresholding is faster (pre-computable sets) but brittle; no-threshold requires dynamic probability calculations
  - PopPos is faster but often less impactful than PopNeg

- **Failure signatures:**
  - Training Instability: If no-threshold method encounters degree 0 item, inverse probability explodes
  - Accuracy Collapse: If $w$ is too high, model enforces fairness blindly, ranking irrelevant niche items higher than relevant popular ones
  - No Improvement: If alpha threshold is set too low, almost all items are "popular," making regularization term indistinguishable from BPR

- **First 3 experiments:**
  1. Baseline Replication: Train LightGCN on MovieLens with standard BPR. Measure Recall@20 and PRU/PRI to establish bias baseline
  2. Integration Test: Integrate PBiLoss (PopNeg-FT variant) with conservative $w=0.01$ and $\alpha$ at 80th percentile. Verify code runs and PRU decreases
  3. Hyperparameter Sweep: Sweep $w$ (e.g., $[10^{-4}, 10^{-1}]$) to find "elbow" point where PRU improves significantly without dropping NDCG by more than 1-2%

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative centrality metrics, such as HITS (hub and authority) scores, capture nuanced item popularity more effectively than node degree within the PBiLoss framework? The authors explicitly state they plan to explore alternative popularity criteria beyond node degree, such as leveraging HITS scores. The current implementation relies exclusively on node degree as the sole proxy for item popularity.

### Open Question 2
How does PBiLoss perform in non-graph-based recommendation settings, such as sequential or context-aware recommendation tasks? The conclusion notes they aim to apply PBiLoss to other recommendation settings to evaluate its generalizability. The empirical validation is restricted to graph-based collaborative filtering models.

### Open Question 3
Can inherent architectural modifications to Graph Neural Networks (GNNs) mitigate popularity bias more robustly than the proposed loss-based regularization? The paper suggests a key direction is to design or modify model architectures so they inherently mitigate popularity bias, potentially leading to more robust and equitable representations. PBiLoss is a model-agnostic, plug-in regularization component that does not alter the underlying message-passing mechanism.

## Limitations
- The sampling complexity of PopNeg strategy may create scalability issues with large datasets, as the paper lacks runtime or memory analysis
- Fixed popularity threshold approach may not adapt well to datasets with non-stationary popularity distributions, potentially limiting effectiveness in dynamic environments
- Generalizability to non-graph-based recommenders or explicit-feedback datasets is not addressed, and impact on long-tail item discovery remains unexplored

## Confidence

- **High Confidence:** The core mechanism of adding popularity-aware regularization term is well-defined and theoretically sound. Empirical improvement in fairness metrics is supported by experimental results across multiple datasets.
- **Medium Confidence:** Superiority of PopNeg over PopPos is demonstrated, but hybrid strategies or adaptive switching between them could yield further improvements. Hyperparameter choices appear reasonable but lack sensitivity analysis.
- **Low Confidence:** Generalizability to non-graph-based recommenders is not addressed. Impact on long-tail item discovery beyond fairness metrics remains unexplored.

## Next Checks

1. **Scalability Test:** Implement PBiLoss on a dataset with millions of items to measure memory and runtime overhead of PopNeg sampling, particularly construction of negative item sets

2. **Dynamic Dataset Evaluation:** Apply PBiLoss to a temporal dataset where item popularity shifts over time, assessing whether fixed threshold approach maintains effectiveness or requires adaptation

3. **Hybrid Sampling Experiment:** Design experiment that dynamically switches between PopPos and PopNeg based on user-specific popularity bias indicators, measuring if this yields better accuracy-fairness trade-offs than using either strategy alone