---
ver: rpa2
title: Sliced-Wasserstein Distance-based Data Selection
arxiv_id: '2504.12918'
source_url: https://arxiv.org/abs/2504.12918
tags:
- data
- distance
- average
- each
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWAD, a new unsupervised anomaly detection
  method using sliced-Wasserstein distance for training data selection in machine
  learning pipelines. The method identifies outliers by measuring transportation costs
  between individual data points and the rest of the dataset.
---

# Sliced-Wasserstein Distance-based Data Selection

## Quick Facts
- arXiv ID: 2504.12918
- Source URL: https://arxiv.org/abs/2504.12918
- Reference count: 40
- Primary result: FEAD achieves lowest average normalized errors (0.3786) across six UCI regression datasets

## Executive Summary
This paper introduces SWAD, an unsupervised anomaly detection method that uses sliced-Wasserstein distance to identify outliers for training data selection. The method measures transportation costs between individual data points and the rest of the dataset, filtering points that require excessive transport cost. Two scalable approximations are provided: sSWAD for parallel processing of reduced-cardinality representations, and FEAD which uses Euclidean distance for computational efficiency. The methods are evaluated on UCI regression datasets, showing strong performance compared to isolation forest and local outlier factor methods.

## Method Summary
SWAD computes outlier detection scores by comparing the sliced-Wasserstein distance between distributions with candidate outliers removed versus distributions with random samples removed. Points requiring consistently higher transport costs are filtered. sSWAD clusters data and processes each split in parallel, while FEAD approximates Wasserstein distance with Euclidean distance for efficiency. The methods are applied to training data selection before training shallow convex neural networks on UCI regression datasets, with hyperparameters optimized via tree-structured Parzen estimator.

## Key Results
- FEAD achieves lowest average normalized MAE (0.3786) across six UCI regression datasets
- Method effectively filters global outliers while maintaining computational efficiency through approximations
- Authors release first open-source dataset demonstrating localized critical peak rebate demand response in northern climate
- Forecasting benchmark demonstrates practical utility of filtering method

## Why This Works (Mechanism)

### Mechanism 1
Filtering data points with high transportation cost relative to the dataset improves downstream ML model generalization under corruption. SWAD computes voting scores by comparing sliced-Wasserstein distances between distributions with candidate outliers removed and distributions with random samples removed. Points consistently requiring higher transport cost are labeled outliers and excluded from training. Core assumption: corrupted samples are distributionally distinct from inliers, and their removal produces more representative empirical distribution. Break condition: fails on local outliers (low-density regions within main distribution).

### Mechanism 2
Parallelizing anomaly detection across reduced-cardinality splits preserves detection quality while improving scalability. sSWAD clusters the dataset, splits each cluster into S parts, processes each independently in parallel, and unions outlier sets. Core assumption: cluster-then-split preserves enough distributional structure per split for consistent outlier identification. Break condition: poor cluster formation increases false negatives.

### Mechanism 3
Euclidean distance between two samples approximates Wasserstein distance between their leave-one-out distributions for efficient outlier detection. FEAD uses single-sample transport plan as proxy for distribution transport cost. For t=1, proven exact; for t>1, provides upper bound. Core assumption: single-sample transport plan is optimal or near-optimal. Break condition: overestimates distances for t>1 when multiple samples need moving, potentially over-filtering.

## Foundational Learning

- **Wasserstein Distance (Optimal Transport)**: Why needed: SWAD builds on sliced-Wasserstein distance; understanding OT provides intuition for why transportation cost identifies outliers. Quick check: Can you explain why moving a single anomalous point might require higher transport cost than moving a typical point?

- **Sliced-Wasserstein Distance**: Why needed: Core metric in SWAD; makes high-dimensional Wasserstein tractable via 1D projections. Quick check: How does projecting distributions onto random 1D directions enable efficient Wasserstein computation?

- **Unsupervised Anomaly Detection**: Why needed: SWAD operates without labels; understanding AD paradigms contextualizes its design. Quick check: What is the difference between local and global outlier detection, and which does SWAD target?

## Architecture Onboarding

- Component map: Data input -> (optional: cluster & split for sSWAD) -> pairwise distance computation -> voting aggregation -> threshold comparison -> filtered dataset output
- Critical path: Data input → (optional: cluster & split for sSWAD) → pairwise distance computation → voting aggregation → threshold comparison → filtered dataset output
- Design tradeoffs: SWAD vs. sSWAD: exact vs. parallelizable; SWAD vs. FEAD: theoretical fidelity vs. computational efficiency; Conservatism (ϵ/η): lower thresholds increase filtering aggressiveness
- Failure signatures: High false negatives on local outliers; sSWAD degrades with poor cluster formation; FEAD may over-filter on high-dimensional data
- First 3 experiments: 1) Synthetic validation: replicate Figure 3 on Gaussian mixtures—tune ε to isolate minority vs. majority vs. outlier groups. 2) Scalability benchmark: compare runtime of SWAD vs. sSWAD vs. FEAD on increasing dataset sizes; measure filtering consistency. 3) Downstream task impact: train shallow model on UCI datasets with each filter; report normalized MAE and samples filtered.

## Open Questions the Paper Calls Out

1. **Theoretical guarantees**: Can formal theoretical guarantees be established linking SWAD-based data selection to improved out-of-sample predictive performance? [explicit] Section VI states, "One of the biggest limitations of this method is the lack of proper theoretical guarantees for out-of-sample model predictive performance. This is a topic for future work." [inferred] The paper currently relies solely on empirical validation without deriving statistical bounds for generalization error. [resolved by] Derivation of generalization bounds relating sliced-Wasserstein distance of filtered training set to expected test error.

2. **Local outlier extension**: How can SWAD methodology be extended to effectively identify local density-based outliers while maintaining conservative global filtering properties? [explicit] Authors conclude in Section VI that the "proposition is adequate for filtering global outliers yet sometimes fails at detecting local outliers." [inferred] Method measures global transportation costs rather than relative density. [resolved by] Modified version of SWAD demonstrating high accuracy on benchmark datasets containing local anomalies without losing performance on global outlier tasks.

3. **Deep learning generalization**: Does filtering efficacy of FEAD and SWAD generalize to complex, non-convex deep learning architectures beyond shallow convex neural networks tested? [inferred] Numerical experiments exclusively utilize non-regularized SCNNs, leaving interaction with deep networks unexplored. [inferred] Unclear if noise filtering benefits translate to over-parameterized deep models with different robustness properties. [resolved by] Comparative benchmarks on UCI or LCPR datasets using deep neural networks demonstrating consistent performance gains over unfiltered baselines.

## Limitations

- Computational expense of SWAD for large datasets, though FEAD and sSWAD address this
- Theoretical justification for FEAD's approximation is heuristic for t>1
- Method explicitly targets global outliers, missing local anomalies (acknowledged limitation)
- Hyperparameter sensitivity not fully explored; SCNN architecture details lack specificity

## Confidence

- **High confidence**: SWAD's theoretical foundation (Theorem 1, Lemma 1, Proposition 1) and core mechanism are rigorously proven
- **Medium confidence**: Numerical results on UCI datasets demonstrate strong performance, but limited to six datasets and two baseline methods
- **Low confidence**: Scalability claims for sSWAD and FEAD supported by methodology but lack extensive runtime benchmarks or ablation studies

## Next Checks

1. **Hyperparameter robustness**: Systematically vary ε, η, and p across synthetic datasets with known global vs. local outliers to quantify detection accuracy and false positive rates.

2. **Scalability benchmark**: Measure runtime and memory usage of SWAD, sSWAD, and FEAD on datasets ranging from 1,000 to 100,000 samples; assess how cluster quality (K) affects sSWAD's detection consistency.

3. **Cross-dataset generalization**: Test FEAD on additional regression and classification UCI datasets, comparing normalized MAE against more diverse baselines (e.g., autoencoders, clustering-based methods).