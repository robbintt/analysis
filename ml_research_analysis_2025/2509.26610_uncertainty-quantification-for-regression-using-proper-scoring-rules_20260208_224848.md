---
ver: rpa2
title: Uncertainty Quantification for Regression using Proper Scoring Rules
arxiv_id: '2509.26610'
source_url: https://arxiv.org/abs/2509.26610
tags:
- bayes
- score
- uncertainty
- crps
- bpens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends proper scoring rule theory to regression tasks,
  introducing a unified framework for quantifying aleatoric and epistemic uncertainty
  in continuous prediction problems. The core idea is to interpret total, Bayes, and
  excess risks under proper scoring rules as natural uncertainty measures, enabling
  principled decomposition into aleatoric and epistemic components.
---

# Uncertainty Quantification for Regression using Proper Scoring Rules

## Quick Facts
- arXiv ID: 2509.26610
- Source URL: https://arxiv.org/abs/2509.26610
- Authors: Alexander Fishkov, Kajetan Schweighofer, Mykyta Ielanskyi, Nikita Kotelevskii, Mohsen Guizani, Maxim Panov
- Reference count: 40
- Primary result: Extends proper scoring rule theory to regression tasks, introducing a unified framework for quantifying aleatoric and epistemic uncertainty in continuous prediction problems

## Executive Summary
This paper introduces a principled framework for uncertainty quantification in regression by leveraging proper scoring rules. The core innovation is interpreting total, Bayes, and excess risks under these scoring rules as natural measures of uncertainty, enabling a clean decomposition into aleatoric (irreducible noise) and epistemic (model ignorance) components. The framework generalizes existing uncertainty measures and provides closed-form expressions under Gaussian assumptions, with practical approximations for ensemble-based estimation. Experimental results demonstrate effectiveness across selective prediction and OOD detection tasks, with total and excess risks showing task-specific advantages.

## Method Summary
The method interprets proper scoring rules through a risk-theoretic lens, where total risk represents expected score, Bayes risk represents the entropy of the true data distribution (aleatoric uncertainty), and excess risk represents the divergence between predicted and true distributions (epistemic uncertainty). Under Gaussian assumptions, closed-form expressions for these risks are derived for common scoring rules (CRPS, logarithmic, quadratic, squared error). The framework is implemented using heteroscedastic deep ensembles, where each ensemble member predicts both mean and variance. Natural parameterization is used to ensure stable training of the variance network. Risks are estimated by averaging over ensemble predictions, with pairwise terms required for certain measures.

## Key Results
- Total and excess risks perform well in selective prediction and OOD detection tasks respectively
- bR1,1Tot (Bayesian averaging of total risk) is consistently effective across experiments
- Different scoring rules induce similar rankings of uncertainty measures, supporting framework robustness
- CRPS-based measures are more robust than Log or Quadratic scores in certain scenarios
- Epistemic uncertainty (excess risk) is particularly effective for detecting out-of-distribution inputs

## Why This Works (Mechanism)

### Mechanism 1
Interpreting proper scoring rule risks allows for a principled decomposition of uncertainty into distinct aleatoric and epistemic components. The framework equates total uncertainty to the expected score (Total Risk). By exploiting the properties of proper scores, this total is split into the score's entropy (Bayes Risk), which represents irreducible data noise (aleatoric), and the divergence between predicted and true distributions (Excess Risk), which represents model error due to limited knowledge (epistemic).

### Mechanism 2
Deep ensembles provide a tractable Bayesian approximation for computing these risk measures without requiring the true data distribution. An ensemble of M networks is treated as samples from a posterior distribution. Closed-form expressions (e.g., Gaussian mixture approximations) allow the estimation of risk components by averaging over the ensemble outputs (means and variances) rather than solving intractable integrals over the true posterior.

### Mechanism 3
Specific risk components (Total vs. Excess) align with different downstream tasks (Selective Prediction vs. OOD Detection) based on whether the task requires detecting noise or ignorance. Selective prediction benefits from Total Risk (RTot) because it reflects overall expected error. Out-of-distribution (OOD) detection and active learning benefit more from Excess Risk (RExc) because these tasks specifically target the model's lack of knowledge (epistemic gap) rather than data noise.

## Foundational Learning

- **Concept: Proper Scoring Rules**
  - Why needed here: The entire framework relies on the mathematical properties of scores (like Log-Score or CRPS) being "proper," meaning they incentivize honest probabilistic forecasting. Without this, the decomposition into entropy and divergence lacks a theoretical basis.
  - Quick check question: Can you explain why the Squared Error score is only "proper" (not strictly) and how that limits its ability to capture full distributional uncertainty compared to the Log score?

- **Concept: Bayesian Model Averaging (BMA) vs. Ensembles**
  - Why needed here: The paper estimates risk by treating an ensemble as samples from a posterior. Distinguishing between averaging risks (BMA-style) and using the posterior predictive (mixture distribution) is essential for implementing the specific `bR` formulas (e.g., `bR1,1` vs `bR2,1`).
  - Quick check question: If you have 5 models in an ensemble, does `bR1` average their individual entropies, or calculate the entropy of their combined prediction?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper provides a theoretical mapping for these concepts (Aleatoric=Bayes Risk, Epistemic=Excess Risk). Understanding that aleatoric is irreducible noise while epistemic is reducible ignorance is necessary to interpret the experimental results correctly.
  - Quick check question: If you double the dataset size, which component of uncertainty (aleatoric or epistemic) should theoretically decrease, and how would that affect the Excess Risk metric?

## Architecture Onboarding

- **Component map:** Training Loop (M networks) -> Aggregation (collect μi, σi) -> Risk Calculator (implement equations from Tables 2-3) -> Selector/Flagger (compare against thresholds)

- **Critical path:** The training stability of the variance network is the single point of failure. The paper explicitly warns that standard parameterization of the Gaussian NLL leads to collapsed variances. You must implement the **natural parameterization** (η₁ = μ/σ², η₂ = -1/2σ²) as detailed in Appendix E.1 to ensure reliable uncertainty estimates.

- **Design tradeoffs:**
  - **CRPS vs. Log Score:** CRPS is generally more robust but computationally more complex per sample than Log Score. Log score decomposes into standard Entropy/KL-divergence, which may be easier to debug.
  - **Approximation Choice:** `bR1,1` (Bayesian averaging of both risks) is computationally quadratic in M (O(M²)) due to pairwise terms (e.g., Eq. 7), while `bR3` (Gaussian approximation) is linear (O(M)).
  - **Recommendation:** For large ensembles, `bR3` variants may be preferred for speed, but `bR1,1` is theoretically "purer."

- **Failure signatures:**
  - **Constant Variance:** If σi are identical across the ensemble, Excess Risk (Epistemic) may collapse to zero or behave erratically.
  - **Negative Excess Risk:** Check implementation of equations in Table 3; `bR3a,2_Exc` for CRPS can show low magnitudes or instability if the mixture approximation is poor.
  - **Mode Collapse:** If ensemble members converge to the exact same weights, Excess Risk will be near zero, rendering OOD detection useless.

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Replicate the 1D heteroscedastic experiment (Figure 2/Appendix E.2). Verify that Bayes Risk peaks in high-noise regions and Excess Risk peaks outside training support.
  2. **Selective Prediction Baseline:** Implement `bR1,1_Tot` (CRPS) on a standard regression dataset (e.g., UCI Benchmark). Plot the Prediction-Reject Ratio (PRR) to see if rejecting the top 20% highest uncertainty scores drops MSE significantly.
  3. **OOD Detection:** Compare `bR1,1_Exc` (Epistemic) against `bR1_Bayes` (Aleatoric) on an OOD task (e.g., MNIST mosaic vs. CIFAR tiles as in Section 5.4). Calculate AUROC to confirm Epistemic risk is the better signal for anomalies.

## Open Questions the Paper Calls Out

### Open Question 1
Can closed-form uncertainty measures be derived for non-Gaussian parametric assumptions, such as Laplace or multi-modal distributions? The authors state in Section 3.2: "For the scope of this work, we consider the Gaussian assumption... Other parametric assumptions, e.g. Laplace, could be used to derive different measures." The provided framework and all closed-form expressions rely exclusively on Gaussian properties.

### Open Question 2
Does the ordering inequality `bR^1_Bayes ≤ bR^2_Bayes` hold universally for all proper scoring rules? Appendix D notes regarding the relationship between risk estimates: "Whether this result holds for other scores or in general remains open." The paper demonstrates the ordering for the logarithmic score and derives inequalities for CRPS/Quadratic scores, but a general theoretical proof for arbitrary regular scoring rules is missing.

### Open Question 3
Why do certain excess risk measures decrease when the location of predicted variances shifts, and can this behavior be theoretically mitigated? Section 5.1 observes that under a variance location shift, "excess risk measures either decrease or stay the same," contrasting with the expectation that uncertainty should increase. The authors note this is undesirable but do not investigate the theoretical cause or correction for this sensitivity in CRPS and Quadratic scores.

## Limitations
- The theoretical decomposition relies on strict propriety of the scoring rule, which may not hold for all practical choices (e.g., Squared Error is only proper)
- The ensemble-based estimation assumes the ensemble approximates the posterior distribution well, but no theoretical bounds are provided on approximation quality
- Experiments are limited to regression tasks and do not extensively explore cases where the Gaussian assumption for predictive distributions breaks down

## Confidence
- **High Confidence**: The framework's mathematical foundation (proper scoring rules, risk decomposition) and the general effectiveness of total/excess risks for selective prediction and OOD detection tasks
- **Medium Confidence**: The specific performance rankings between different scoring rules (CRPS vs. Log Score) and the robustness of results across diverse datasets
- **Low Confidence**: The exact implementation details of complex risk calculations (e.g., bR1,1_Exc) and their numerical stability in high-dimensional settings

## Next Checks
1. **Numerical Stability Test**: Implement bR1,1_Exc for CRPS on synthetic 1D data with varying ensemble sizes (M=5, 10, 20). Verify that excess risk remains positive and increases with ensemble size as expected, and identify any numerical instability thresholds.
2. **Scoring Rule Robustness**: Re-run the selective prediction experiment on a UCI dataset using both CRPS and Log Score. Compare PRR curves to quantify whether the paper's claim of similar performance rankings holds empirically.
3. **OOD Detection Stress Test**: Design an OOD experiment where the target distribution has higher intrinsic noise (e.g., in-distribution: low-noise MNIST; OOD: high-noise CIFAR). Measure whether Excess Risk still outperforms Total Risk, validating the claim that epistemic uncertainty is the key OOD signal.