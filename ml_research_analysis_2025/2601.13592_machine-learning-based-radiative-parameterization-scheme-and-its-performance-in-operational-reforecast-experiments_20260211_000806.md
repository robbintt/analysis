---
ver: rpa2
title: Machine learning based radiative parameterization scheme and its performance
  in operational reforecast experiments
arxiv_id: '2601.13592'
source_url: https://arxiv.org/abs/2601.13592
tags:
- learning
- ncol
- nlay
- operational
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a machine learning-based parameterization
  scheme for radiative processes in the CMA-GFS model, addressing the challenge of
  computational efficiency in operational weather forecasting. The method employed
  a residual convolutional neural network trained on a comprehensive dataset generated
  from model simulations to approximate the Rapid Radiative Transfer Model for General
  Circulation Models (RRTMG).
---

# Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments

## Quick Facts
- **arXiv ID:** 2601.13592
- **Source URL:** https://arxiv.org/abs/2601.13592
- **Reference count:** 7
- **Primary result:** Hybrid ML-physics model accelerates radiative parameterization by ~8x while maintaining accuracy comparable to traditional RRTMG scheme.

## Executive Summary
This study presents a machine learning-based parameterization scheme for radiative processes in the CMA-GFS global forecast system. The method replaces the computationally expensive RRTMG physics solver with a residual convolutional neural network trained on comprehensive atmospheric datasets. The ML emulator is integrated using a LibTorch-based compilation library tool, enabling real-time operations. A two-month operational reforecast experiment demonstrated that the hybrid model achieves accuracy comparable to traditional physical schemes while accelerating computation speed by approximately eightfold, validating the feasibility of integrating machine learning into operational numerical weather prediction systems.

## Method Summary
The study develops a residual CNN to emulate RRTMG radiative transfer calculations in the CMA-GFS model. A 60-76 TB dataset of atmospheric states and radiative outputs is generated from model simulations and preprocessed into 1D vertical columns. The ResCNN architecture with 3 residual modules is trained using SGD optimization with cosine annealing scheduler on 8 P100 GPUs. Physical constraints are applied to outputs, and experience replay is used to enhance stability by retraining on failure cases. The trained model is compiled with LibTorch and coupled to the Fortran host model via a C++ interface, eliminating IPC overhead.

## Key Results
- Hybrid model achieves 8x computational speedup compared to traditional RRTMG scheme
- Mean Absolute Error remains below 1% compared to RRTMG reference outputs
- 50 independent 10-day forecasts completed successfully after experience replay implementation
- Forecast accuracy for synoptic fields and precipitation comparable to traditional physics-based approach

## Why This Works (Mechanism)

### Mechanism 1: Computational Acceleration via Function Approximation
Replacing iterative RRTMG physics solver with Residual CNN reduces computational cost while maintaining acceptable error bounds. The ResCNN learns the mapping from atmospheric states to radiative fluxes, and inference via optimized matrix multiplications is algorithmically cheaper than solving radiative transfer equations iteratively for multiple spectral bands.

### Mechanism 2: Integration Stability via Experience Replay and Constraints
Long-term stability is maintained by iteratively augmenting the training dataset with "failure" samples from crashed simulations. This addresses out-of-distribution inputs during online coupling that lead to unphysical outputs destabilizing the host model.

### Mechanism 3: Latency Reduction via In-Process Coupling (LibTorch)
Using compiled library (LibTorch) for coupling eliminates overhead of Inter-Process Communication. The neural network is compiled into a shared library callable directly from Fortran, keeping inference in the same memory space as the host model.

## Foundational Learning

**Concept: Residual Convolutional Neural Networks (ResCNN) for 1D Data**
- **Why needed here:** Input data consists of vertical atmospheric columns (1D vectors). Understanding how 1D convolutions extract vertical features and how residual connections prevent gradient degradation in physics-emulating networks.
- **Quick check question:** How does a 1D convolution kernel size of 3 or 5 capture the relationship between adjacent atmospheric pressure layers?

**Concept: Hybrid Modeling & Coupled Stability**
- **Why needed here:** Hybrid models feed ML outputs back into physics solver. Small systematic biases can accumulate over time steps, leading to exponential error growth (model blow-up).
- **Quick check question:** Why does an offline accuracy metric (like RMSE) fail to guarantee the stability of a 10-day online coupled forecast?

**Concept: Fortran-C++ Interoperability**
- **Why needed here:** Legacy weather models are written in Fortran. Integrating modern ML requires understanding ISO_C_BINDING to pass arrays between Fortran and C++ LibTorch interface without copying data.
- **Quick check question:** What is the specific memory layout difference (column-major vs. row-major) that the adapter tool must handle when passing a Fortran array to a C++ tensor?

## Architecture Onboarding

**Component map:** CMA-GFS (Fortran) -> FTA/LibTorch (C++ adapter) -> TorchScript (ML model) -> Data Pipeline (preprocessing)

**Critical path:** 1. Data Generation: Run RRTMG simulations → Extract Input/Output pairs. 2. Offline Training: Train ResCNN in PyTorch → Export to TorchScript. 3. Compilation: Link TorchScript with LibTorch adapter into shared library. 4. Integration: Call library from CMA-GFS radiation driver.

**Design tradeoffs:** Chose LibTorch (In-Process) over IPC to maximize speed but reduce debuggability. Chose ResCNN over FC networks for better representation of local vertical structures and easier training of deeper networks.

**Failure signatures:** Model Interruption: Sudden NaN or Inf values appearing after ~48-72 hours, typically triggered by extreme weather inputs. Heating Rate Drift: Accumulation of small biases leading to unrealistic temperature profiles.

**First 3 experiments:** 1. Unit Validation: Feed held-out test set to TorchScript model and verify MAE <1% against RRTMG. 2. Dry Run: Run hybrid model for 24 hours to check memory leaks and interface stability. 3. Stress Test: Initialize with dates known for extreme events and run for 10 days to verify experience replay prevents crashes.

## Open Questions the Paper Calls Out

**Open Question 1:** Can integrating line-by-line model (LBLRTM) outputs or observational data further improve the ML emulator's forecast accuracy compared to the current approach of merely approximating RRTMG? The current study focused solely on demonstrating feasibility rather than surpassing physical fidelity or incorporating direct observational constraints.

**Open Question 2:** Is it possible to develop a neural network architecture that maintains accuracy across different vertical resolutions without requiring complete retraining? The current model architecture and training data are tightly bound to specific vertical layer dimensions (89 layers), forcing a retraining bottleneck when the host model configuration changes.

**Open Question 3:** To what extent can the slight degradation in 10-meter wind speed and synoptic field forecasts be corrected by incorporating satellite data or LBLRTM physics into the training? The current emulator approximates RRTMG and likely inherits or amplifies existing biases for near-surface variables.

## Limitations

- Long-term stability uncertain for atmospheric states statistically distinct from 2022 training data
- 8x speedup based on single host model and specific computational setup, may vary with different configurations
- Experience replay assumes finite and learnable distribution of extreme weather scenarios

## Confidence

**High Confidence:** Offline accuracy claims (MAE <1% vs. RRTMG) well-supported by training and validation methodology with physical constraints.

**Medium Confidence:** 8x computational speedup plausible but exact gain depends on hardware specifics not fully detailed.

**Medium Confidence:** Experience replay approach for stability theoretically sound but effectiveness contingent on completeness of augmented dataset covering all edge cases.

## Next Checks

1. Implement ResCNN emulator in different NWP model (e.g., WRF) to assess generalizability of 8x speedup and accuracy beyond CMA-GFS.

2. Run hybrid model with historical extreme weather events (e.g., El Niño, polar vortex) not in 2022 training set to evaluate robustness against out-of-distribution inputs.

3. Modify physical constraints to be less restrictive and quantify trade-off between computational speed and model stability to identify optimal bounds for operational use.