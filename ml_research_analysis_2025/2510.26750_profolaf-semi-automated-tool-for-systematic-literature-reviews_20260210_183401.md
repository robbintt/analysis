---
ver: rpa2
title: 'ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews'
arxiv_id: '2510.26750'
source_url: https://arxiv.org/abs/2510.26750
tags:
- profolaf
- articles
- systematic
- reviews
- literature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProfOlaf addresses the labor-intensive nature of systematic literature
  reviews by combining iterative snowballing with LLM-assisted analysis, achieving
  a balance between automation and human oversight. The tool demonstrated efficiency
  with an overall 0.11 rate (111 relevant papers from 1,009 candidates) across seven
  iterations.
---

# ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews
## Quick Facts
- arXiv ID: 2510.26750
- Source URL: https://arxiv.org/abs/2510.26750
- Reference count: 21
- Primary result: Combines iterative snowballing with LLM-assisted analysis to achieve 0.11 efficiency rate while maintaining human oversight

## Executive Summary
ProfOlaf addresses the labor-intensive nature of systematic literature reviews by combining iterative snowballing with LLM-assisted analysis, achieving a balance between automation and human oversight. The tool demonstrated efficiency with an overall 0.11 rate (111 relevant papers from 1,009 candidates) across seven iterations. LLM-assisted tasks showed mixed results: Topic Modeling achieved 0.547 precision and 0.650 recall for topic assignment, and 0.590 precision and 0.710 recall for programming language identification, while summaries scored high on quality metrics (mean 4.5-4.9/5). The tool's approach of combining automated collection with human-in-the-loop verification enables researchers to conduct more efficient, rigorous, and reproducible reviews while maintaining methodological standards.

## Method Summary
ProfOlaf is a Python-based tool that implements semi-automated systematic literature reviews using iterative snowballing (forward/backward citation tracking) combined with LLM-assisted analysis. The workflow alternates between citation expansion and human filtering, terminating when an iteration yields no new relevant papers. The tool performs progressive screening (metadata → title → full text) with multi-rater disagreement resolution, uses TopicGPT for topic modeling and language extraction, and employs a Task Assistant for summarization. Evaluation used a single seed paper on LLM bug benchmarks, achieving 0.11 efficiency rate with mixed LLM performance requiring human validation.

## Key Results
- Overall efficiency rate of 0.11 (111 relevant papers from 1,009 candidates) across seven snowballing iterations
- Topic Modeling precision/recall: 0.547/0.650 for topic assignment, 0.590/0.710 for programming language identification
- LLM-generated summaries scored high quality (4.5-4.9/5) on Faithfulness, Salience, Structure, and Conciseness
- Efficiency declined from 0.43 (iteration 2) to 0.02 (iteration 6), with iteration 7 producing zero new papers

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Iterative snowballing with convergence detection provides efficient literature coverage while maintaining retrieval completeness.
- Mechanism: The tool alternates between citation expansion (forward/backward snowballing) and human filtering, terminating when an iteration yields no new relevant papers. Early iterations capture high-relevance papers (efficiency 0.26–0.43), while later iterations experience diminishing returns (efficiency 0.02 by iteration 6).
- Core assumption: Relevant papers cluster in citation networks; the seed paper's citation neighborhood contains the majority of relevant literature.
- Evidence anchors:
  - [abstract] "ProfOlaf supports iterative snowballing for article collection with human-in-the-loop filtering"
  - [section] Table 1 shows efficiency declining from 0.43 (iteration 2) to 0.02 (iteration 6), with iteration 7 producing zero new papers
  - [corpus] Neighbor paper "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews" similarly addresses corpus filtration efficiency (FMR 0.49)
- Break condition: If citation networks in your domain are sparse or interdisciplinary, snowballing may miss relevant papers that don't cite each other.

### Mechanism 2
- Claim: LLM-based topic modeling accelerates article categorization but requires human validation due to over-assignment and hallucination tendencies.
- Mechanism: TopicGPT generates topic labels and assigns papers to topics using LLM prompting. The model tends toward over-inclusion (e.g., tagging Python when it's used for data processing rather than being the research subject), contributing to lower precision (0.547–0.590) than recall (0.650–0.710).
- Core assumption: LLMs can infer research topics from paper content with sufficient accuracy to reduce (not eliminate) manual labeling effort.
- Evidence anchors:
  - [abstract] "Topic Modeling achieved 0.547 precision and 0.650 recall for topic assignment"
  - [section] Section 3.2.2: "the model often assigned more languages than were actually relevant, suggesting a tendency toward over-assignment and a degree of hallucination"
  - [corpus] Corpus evidence for LLM-based topic modeling accuracy in SLR contexts is limited; neighbor papers focus on screening rather than extraction
- Break condition: For domains with ambiguous terminology or novel research areas lacking established topic vocabularies, TopicGPT's predefined topic generation may miss emerging categories.

### Mechanism 3
- Claim: Progressive screening (metadata → title → full text) with multi-rater disagreement resolution maintains methodological rigor while distributing cognitive load.
- Mechanism: Reviewers first filter by objective metadata (venue ranking, year, language), then by title, then by full text. Disagreements are flagged for collaborative resolution before proceeding. This staged approach reduces the number of full-text reviews required.
- Core assumption: Papers excluded at earlier stages (title, metadata) would also be excluded at later stages; inter-rater reliability is sufficient to trust collaborative resolution.
- Evidence anchors:
  - [abstract] "combining automated collection with human-in-the-loop verification"
  - [section] Section 2.4: "ProfOlaf adopts the approach outlined by Wohlin, which recommends screening articles progressively: first the title, then the abstract, and finally the full text"
  - [corpus] Neighbor paper "AISysRev" similarly implements title-abstract screening with LLM assistance
- Break condition: If your inclusion criteria require nuanced methodological assessment, title/abstract screening alone may incorrectly exclude relevant papers.

## Foundational Learning
- **Snowballing methodology (forward/backward)**
  - Why needed here: ProfOlaf's core retrieval mechanism depends on understanding how citation tracking expands search coverage iteratively
  - Quick check question: Given a seed paper, can you trace which papers would be discovered in one forward vs. backward snowballing step?

- **Systematic literature review protocols (Kitchenham/Wohlin guidelines)**
  - Why needed here: The tool encodes specific methodological choices (progressive screening, efficiency metrics, disagreement resolution) that assume familiarity with SLR standards
  - Quick check question: Why might a systematic review require explicit inclusion/exclusion criteria and multi-rater screening?

- **LLM prompt engineering for extraction tasks**
  - Why needed here: TopicGPT's performance depends on prompt design; understanding prompt-based topic extraction helps diagnose failures
  - Quick check question: What types of extraction errors (over-assignment, hallucination, topic conflation) would you expect from LLM-based topic modeling?

## Architecture Onboarding
- **Component map:**
  - Database (SQLite via Python) -> Snowballing module (Google Scholar, Semantic Scholar, DBLP APIs) -> Venue ranking helper (Scimago/CORE lookup) -> Screening interface (CLI prompts) -> TopicGPT module -> Task Assistant (LLM interface)

- **Critical path:** Initial seed articles → Snowballing (retrieve citations/references) → Metadata filtering → Manual title screening → Manual full-text screening → (loop back to snowballing if new papers found) → Duplicate detection → Topic modeling → Task assistant queries → Manual verification

- **Design tradeoffs:**
  - Snowballing vs. database search: ProfOlaf chose snowballing based on prior evidence of comparable/better performance, but limits coverage to papers in the citation network
  - Full automation vs. human-in-the-loop: Topic modeling is not fully trusted (precision ~0.55–0.59), requiring human validation; summaries score higher (4.5–4.9/5) but still need spot-checking
  - Single-tool integration vs. best-of-breed: ProfOlaf consolidates pipeline stages but may lag behind specialized tools for individual steps (e.g., ChatCite for summarization)

- **Failure signatures:**
  - Over-assignment in topic modeling: Model tags papers with topics/languages tangentially mentioned (e.g., Python for data processing)
  - Missing topics: Generated topic list may omit important categories (e.g., "Benchmark for Code Generation/Repair" was absent in evaluation)
  - Hallucination in extraction: Model may claim languages or topics not actually present in the paper
  - Convergence without completeness: Snowballing may terminate early if citation networks are sparse

- **First 3 experiments:**
  1. Run a single snowballing iteration on a seed paper in your domain; manually verify the ratio of relevant to irrelevant papers retrieved to establish baseline efficiency
  2. Apply TopicGPT to 10–20 papers with known topics; compare generated topics against your ground truth to calibrate expected precision/recall
  3. Test the Task Assistant on 5 papers; evaluate summaries on faithfulness and coverage (using the paper's 4-criterion rubric) to determine if verification can be sample-based rather than exhaustive

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can improved prompting strategies or model selection raise topic modeling precision above 0.547 while maintaining recall?
- Basis in paper: [explicit] "The results indicate that while LLMs can deliver satisfactory outcomes for tasks such as summarization, more advanced models and methodologies are needed before they can be fully trusted for tasks such as topic modeling."
- Why unresolved: The evaluation used only gpt-5-nano with TopicGPT prompting; no alternative models or strategies were tested.
- What evidence would resolve it: Controlled comparison of multiple LLMs and prompting approaches on the same corpus with standardized precision/recall metrics.

### Open Question 2
- Question: To what extent does ProfOlaf's efficiency (0.11 overall rate) and topic modeling accuracy generalize to research domains beyond "Machine Learning for Code"?
- Basis in paper: [inferred] The evaluation used a single seed article from one domain without testing cross-domain applicability or discussing generalizability limitations.
- Why unresolved: Citation patterns, terminology density, and topic granularity may vary significantly across fields.
- What evidence would resolve it: Replication of the methodology across at least three diverse domains with comparative analysis of efficiency rates and extraction metrics.

### Open Question 3
- Question: What mitigation strategies effectively reduce LLM over-assignment and hallucination in structured extraction tasks?
- Basis in paper: [explicit] "The model often assigned more languages than were actually relevant, suggesting a tendency toward over-assignment and a degree of hallucination."
- Why unresolved: The paper identifies the problem but tests no interventions such as confidence thresholds, constrained outputs, or multi-pass verification.
- What evidence would resolve it: Ablation experiments comparing baseline extraction against constrained prompting, confidence filtering, or verification steps, measuring precision changes.

### Open Question 4
- Question: What is the optimal automation-to-oversight ratio that minimizes total human effort while preserving review rigor?
- Basis in paper: [inferred] The paper claims balance is "essential to ensure that precision is not compromised" but evaluates only one configuration—fully manual screening with LLM-assisted analysis.
- Why unresolved: No exploration of partial screening automation or varying human-in-the-loop intensity to identify efficiency-quality tradeoffs.
- What evidence would resolve it: Experiments varying screening automation levels and measuring total time, false positive/negative rates, and inter-rater reliability.

## Limitations
- Evaluation used only one seed paper (Ramos et al. on LLM bug benchmarks), limiting generalizability across domains
- LLM model specification unclear (gpt-5-nano referenced but not publicly documented)
- Missing prompt details for TopicGPT, which significantly affect output quality
- Snowballing may miss relevant papers in sparse citation networks or interdisciplinary fields

## Confidence
- **High**: The snowballing methodology and human-in-the-loop screening approach are well-established (Wohlin guidelines)
- **Medium**: LLM-assisted topic modeling and summarization show measurable performance but require validation across different domains and prompt configurations
- **Low**: Generalizability claims beyond the evaluated domain (ML for Code) remain untested

## Next Checks
1. **Domain Transfer Test**: Apply ProfOlaf to a different research domain (e.g., software engineering methodologies) and compare efficiency metrics and topic modeling precision/recall to establish domain sensitivity.

2. **Prompt Sensitivity Analysis**: Systematically vary TopicGPT prompts (topic definition, extraction instructions) and measure impact on precision/recall to identify optimal prompt configurations.

3. **Human Effort Validation**: Conduct time-motion studies comparing traditional manual SLR versus ProfOlaf-assisted reviews, measuring actual time savings across all workflow stages including LLM verification overhead.