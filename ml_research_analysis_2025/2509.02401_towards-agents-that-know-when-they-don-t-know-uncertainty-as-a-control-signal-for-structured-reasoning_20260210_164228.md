---
ver: rpa2
title: 'Towards Agents That Know When They Don''t Know: Uncertainty as a Control Signal
  for Structured Reasoning'
arxiv_id: '2509.02401'
source_url: https://arxiv.org/abs/2509.02401
tags:
- uncertainty
- arxiv
- summary
- training
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Uncertainty-aware LLM agents improve biomedical multi-omics summarization
  by using retrieval and summary uncertainty as control signals during both training
  and inference. The approach integrates entropy over table-selection rollouts and
  perplexity-based consistency metrics into reinforcement learning with GRPO, then
  applies these signals to filter low-confidence outputs at inference.
---

# Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning

## Quick Facts
- arXiv ID: 2509.02401
- Source URL: https://arxiv.org/abs/2509.02401
- Reference count: 40
- Primary result: Uncertainty-aware LLM agents improve biomedical multi-omics summarization, nearly tripling correct claims and improving survival prediction C-index from 0.32 to 0.63

## Executive Summary
This paper introduces a framework for LLM agents that use uncertainty as a control signal during both training and inference for structured data summarization. The approach integrates retrieval and summary uncertainty into reinforcement learning with GRPO, then applies these signals to filter low-confidence outputs at inference. On two multi-omics benchmarks, this yields substantial improvements in factuality and downstream utility, with the agent able to abstain when evidence is unstable. The method demonstrates that uncertainty estimation can serve as a practical mechanism for building reliable, self-assessing LLM agents in structured data domains.

## Method Summary
The method trains LLM agents using Group Relative Policy Optimization (GRPO) with uncertainty-aware rewards. During training, perplexity-based confidence rewards encourage stable reasoning, while retrieval entropy over table-selection rollouts enables detection of unstable evidence. At inference, the agent samples multiple trajectories and filters outputs based on combined uncertainty thresholds. The approach was validated on MLOmics public benchmark and a proprietary internal dataset, using Qwen2.5-14B-Instruct with the ART framework, achieving significant gains in factuality and downstream survival prediction.

## Key Results
- Factuality improvements: Correct claims increased from 3.0 to 8.4 (internal) and 3.6 to 9.9 (cancer) per summary
- Calibration enhancement: Prediction rejection ratio doubled, enabling better abstention on uncertain outputs
- Downstream utility: Survival prediction C-index improved from 0.32 to 0.63 on both datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating summary uncertainty into the RL reward function improves factual grounding by penalizing low-confidence generations during training.
- **Mechanism:** The agent utilizes Group Relative Policy Optimization (GRPO) with a composite reward $R(\tau)$. A specific component, the confidence reward $R_{conf}$, is defined as the inverse perplexity of the generated summary. By maximizing this term, the policy $\pi_\theta$ is optimized to produce outputs where the model assigns higher probability to its own generated tokens, implicitly reinforcing stable reasoning paths over uncertain ones.
- **Core assumption:** Assumption: Perplexity serves as a valid proxy for summary quality and factual reliability during the training phase, such that minimizing perplexity correlates with reducing hallucinations.
- **Evidence anchors:**
  - [abstract] "Summary uncertainty is incorporated into reinforcement learning (RL) with Group Relative Policy Optimization (GRPO)..."
  - [section 3.4] "...favors low-uncertainty summaries, promoting the exploitation of existing knowledge... $R_{conf}(\tau) = 1/u_{Perp}(s(\tau))$."
  - [corpus] Weak direct link; while the corpus discusses epistemic uncertainty (e.g., "Bayesian Mixture-of-Experts"), it does not specifically validate inverse perplexity as a reward signal in GRPO.
- **Break condition:** If the agent learns to generate generic, high-probability "safe" statements that minimize perplexity but fail to answer the specific query (reward hacking), the mechanism fails to improve utility.

### Mechanism 2
- **Claim:** Measuring entropy over table-selection rollouts enables the detection of unstable evidence retrieval, acting as a viable abstention signal.
- **Mechanism:** During inference, the agent samples $K$ episodes for a given query. It calculates retrieval uncertainty $u_{ret}$ using normalized binary entropy over the frequency of tables accessed across these rollouts. If different rollouts select vastly different tables (high entropy), the agent flags the evidence base as unstable and may abstain or filter the output.
- **Core assumption:** Assumption: Consistency in table selection across multiple stochastic trajectories correlates with the existence of a definitive answer in the database.
- **Evidence anchors:**
  - [abstract] "...retrieval uncertainty--entropy over multiple table-selection rollouts..."
  - [section 3.3] "High $u_{ret}$ indicates inconsistent evidence acquisition... triggering abstention at inference."
  - [corpus] The concept of "knowing when they don't know" via consistency is supported generally by titles like "Large Language Models Do NOT Really Know What They Don't Know," though specific entropy-over-tables methods are not detailed in the provided neighbors.
- **Break condition:** If the database schema is so complex that valid reasoning paths naturally diverge across different tables, high entropy might indicate thoroughness rather than confusion, causing false positives for abstention.

### Mechanism 3
- **Claim:** Post-output filtering based on combined uncertainty thresholds improves the precision of claims in the final summary.
- **Mechanism:** The system employs a conservative filtering rule at inference. It sums the retrieval uncertainty ($u_{ret}$) and summary uncertainty ($u_{CoCoA}$). If this sum exceeds a tuned threshold $\kappa$, the output is suppressed. This effectively separates high-confidence, factually grounded outputs from unstable hallucinations.
- **Core assumption:** Assumption: There exists a threshold $\kappa$ that effectively separates correct from incorrect claims, and that errors are consistently associated with high uncertainty scores.
- **Evidence anchors:**
  - [abstract] "...apply these signals to filter low-confidence outputs at inference."
  - [section 3.6] "abstain if the sum exceeds a tuned threshold $\kappa$; otherwise emit the candidate..."
  - [corpus] Consistent with "Do Retrieval Augmented Language Models Know When They Don't Know?", which links refusal/abstention mechanisms to uncertainty.
- **Break condition:** If the uncertainty scores are poorly calibrated (e.g., the model is confidently wrong), the threshold filter will pass hallucinations as facts.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Standard RLHF might be insufficient for agentic tasks. GRPO compares groups of trajectories to estimate advantages, which is crucial for the paper's specific reward shaping strategy involving code execution and confidence.
  - **Quick check question:** How does GRPO differ from standard PPO in terms of advantage estimation? (Answer: It uses group-relative advantages rather than a value function).

- **Concept: Semantic Entropy / Self-Consistency (CoCoA)**
  - **Why needed here:** The paper relies on CoCoA (Combining Confidence and Consistency) for its inference-time uncertainty. Understanding that this measures semantic agreement across multiple generations (not just token probability) is key to grasping why it works for complex summaries.
  - **Quick check question:** Why is semantic similarity used in CoCoA rather than just checking for exact string matches? (Answer: To detect if different phrasings convey the same meaning, filtering only genuine contradictions).

- **Concept: Exploration vs. Exploitation in RL**
  - **Why needed here:** The paper introduces specific "reward schedules" (Two-Phase, Adaptive) to balance exploring the database ($R_{judge}$) vs. exploiting confident summaries ($R_{conf}$).
  - **Quick check question:** What happens if the "confidence" reward is weighted too heavily from step 0? (Answer: The agent may stop exploring the database schema and produce generic, low-information summaries).

## Architecture Onboarding

- **Component map:**
  - **Environment:** Multi-omics Database (D) + Schema
  - **Agent:** LLM Backbone (Qwen2.5-14B-Instruct)
  - **Tools:** `SQLExecutor`, `PythonTool`, `Schema`, `CommitSummary`
  - **Optimization Engine:** GRPO Trainer with Reward Scheduler
  - **Uncertainty Estimator:** Computes $u_{ret}$ (Entropy) and $u_{CoCoA}$ (Consistency+Perplexity)
  - **Filter:** Inference-time threshold check ($u_{ret} + u_{CoCoA} < \kappa$)

- **Critical path:**
  1. **Query Input** -> Agent generates SQL/Python tools calls (Episode start)
  2. **Execution** -> Environment returns data/state
  3. **Summary** -> Agent commits text summary
  4. **Evaluation (Training)** -> Calculate $R_{code}$, $R_{judge}$, $R_{conf}$ -> GRPO Update
  5. **Evaluation (Inference)** -> Run $K$ rollouts -> Calculate Uncertainties -> Filter/Abstain

- **Design tradeoffs:**
  - **Perplexity vs. CoCoA:** The paper uses Perplexity for training (cheaper) and CoCoA for inference (more accurate but requires sampling $K$ rollouts)
  - **Adaptive vs. Fixed Schedules:** Fixed rewards can cause early collapse in exploration; adaptive schedules (Radapt) stabilize learning but add hyperparameters

- **Failure signatures:**
  - **Reward Hacking:** High correctness ratio but low "useful claims" ratio (model generates safe, trivial facts to minimize uncertainty)
  - **Over-abstention:** Model refuses to answer valid queries because the threshold $\kappa$ is too strict for the given data noise
  - **Schema Overfitting:** Agent performs well on the "tree-like" internal schema but fails on the "flat" public MLOmics benchmark

- **First 3 experiments:**
  1. **Reward Schedule Ablation:** Run $R_{zero}$ (no uncertainty reward) vs $R_{adapt}$ on a validation set to confirm that uncertainty rewards actually increase the "useful claims" ratio (0.30 -> 0.78 as per paper)
  2. **Threshold Sensitivity Analysis:** Vary $\kappa$ (e.g., 0.2, 0.5, 0.8) to plot the Coverage vs. Correctness curve and select the operating point that maximizes PRR (Prediction Rejection Ratio)
  3. **Inference Compute Scaling:** Test performance with $K=3$ vs $K=5$ rollouts to verify if the cost of estimating retrieval entropy is justified by the gain in factuality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the uncertainty-aware agent framework maintain its performance when applied to non-biomedical structured domains such as finance or e-commerce?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the current evaluation is restricted to biomedical multi-omics and that "testing across finance, e-commerce, and other structured environments will demonstrate the broader generality of the framework."
- **Why unresolved:** The agent has only been validated on specific schema topologies (flat and tree-like) found in omics data, leaving its robustness to other data distributions unknown.
- **What evidence would resolve it:** Reporting factuality and calibration metrics on standard financial or e-commerce tabular benchmarks.

### Open Question 2
- **Question:** Can lightweight uncertainty proxies replace computationally expensive CoCoA-based self-consistency checks without degrading calibration?
- **Basis in paper:** [explicit] The authors note that the method "requires multiple rollouts... which add inference cost" and suggest that "leveraging a lightweight uncertainty proxy... could make the approach more efficient."
- **Why unresolved:** The current inference pipeline relies on the costly CoCoA metric, creating a trade-off between reliability and latency.
- **What evidence would resolve it:** A comparative study of inference speed versus Prediction Rejection Ratio (PRR) using perplexity-only or entropy-only signals compared to the full CoCoA baseline.

### Open Question 3
- **Question:** How robust is the reinforcement learning policy against reward hacking when shifting from automated LLM judges to systematic human validation?
- **Basis in paper:** [explicit] The paper acknowledges that they "rely on automated LLM-based judges for reward shaping" which "could potentially induce bias," and identify "Expanding systematic human validation" as a necessary next step.
- **Why unresolved:** The ablation shows moderate correlation ($r \approx 0.64$) with human labels, but optimizing entirely against LLM judges risks overfitting to algorithmic biases rather than human-grounded factuality.
- **What evidence would resolve it:** A validation study comparing policy performance and reward hacking rates when trained via LLM-judges versus human expert feedback (RLHF).

## Limitations

- The method relies on specific proxy metrics (perplexity, entropy) that may not fully capture factual reliability in complex domains
- Performance depends on careful tuning of reward schedules and uncertainty thresholds
- The approach has only been validated on biomedical multi-omics data with specific schema topologies

## Confidence

**High Confidence:** The core claim that uncertainty-aware agents improve multi-omics summarization is well-supported by quantitative results showing nearly threefold improvements in correct and useful claims (from 3.0 to 8.4 internal, 3.6 to 9.9 cancer). The downstream survival prediction improvement (C-index from 0.32 to 0.63) provides strong validation of the approach's practical utility.

**Medium Confidence:** The claim that perplexity serves as a valid proxy for summary quality during training is reasonable but relies on the assumption that minimizing perplexity correlates with reducing hallucinations. While the results support this, the relationship between perplexity and factual reliability in complex biomedical domains warrants further investigation.

**Low Confidence:** The assertion that retrieval uncertainty entropy reliably detects unstable evidence retrieval assumes that table selection consistency directly indicates answer definitiveness. This may not hold in databases with genuinely ambiguous or overlapping evidence structures, potentially leading to false abstention signals.

## Next Checks

1. **Reward Hacking Vulnerability Test:** Design adversarial queries that could exploit the perplexity-based reward signal by generating generic, high-probability statements that technically minimize uncertainty but fail to answer the specific query. Measure whether the agent learns to produce such safe but uninformative responses.

2. **Schema Complexity Generalization:** Test the method on databases with intentionally ambiguous or overlapping table structures to evaluate whether high retrieval entropy might indicate legitimate complexity rather than instability, potentially causing false abstention positives.

3. **Uncertainty Calibration Analysis:** Conduct a detailed calibration study comparing the predicted uncertainty scores against actual error rates across different query types and database regions to identify potential systematic biases in the uncertainty estimation mechanism.