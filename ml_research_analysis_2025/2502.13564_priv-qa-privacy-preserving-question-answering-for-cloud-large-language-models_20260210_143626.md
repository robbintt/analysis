---
ver: rpa2
title: 'PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models'
arxiv_id: '2502.13564'
source_url: https://arxiv.org/abs/2502.13564
tags:
- text
- information
- privacy
- sensitive
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRIV-QA, a privacy-preserving framework for
  cloud-based large language models (LLMs) that protects user data during question-answering
  interactions. The method employs a multi-stage text sanitization pipeline that classifies
  tokens into privacy and importance levels, substitutes sensitive information with
  semantically similar but non-sensitive alternatives, and obfuscates low-risk text
  while preserving key words.
---

# PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models

## Quick Facts
- arXiv ID: 2502.13564
- Source URL: https://arxiv.org/abs/2502.13564
- Authors: Guangwei Li; Yuansen Zhang; Yinggui Wang; Shoumeng Yan; Lei Wang; Tao Wei
- Reference count: 37
- Primary result: Privacy-preserving QA framework achieves 89.40% recall for English and 73.01% for Chinese in sensitive information detection while maintaining high response quality

## Executive Summary
This paper introduces PRIV-QA, a framework that protects user privacy during cloud-based LLM interactions by sanitizing queries and recovering original entities in responses. The method employs a multi-stage pipeline that classifies tokens into privacy and importance levels, substitutes sensitive information with semantically similar alternatives, and obfuscates low-risk text while preserving key words. A recovery module then restores original sensitive information and corrects reasoning errors in LLM responses. The framework is evaluated on a newly constructed SensitiveQA dataset containing 57k bilingual (Chinese/English) interactions with privacy-sensitive content.

## Method Summary
PRIV-QA uses four fine-tuned Qwen2 models (0.5B for detection/substitution/importance, 1.5B for recovery) to process user queries through a multi-stage pipeline. The method first detects sensitive entities using a tri-level classification system (High-Risk, Low-Risk, Key-Words), then substitutes sensitive words with semantically similar alternatives while obfuscating low-risk tokens via differential privacy mechanisms. The sanitized query is sent to a cloud LLM, and the response is processed by a recovery model that restores original entities and corrects any reasoning errors introduced during sanitization. The framework is trained on GPT-4o-generated data and evaluated on a newly constructed SensitiveQA dataset.

## Key Results
- 89.40% recall for English and 73.01% for Chinese in sensitive information detection
- Resists 85.83% of extraction attacks while maintaining high response quality
- BLEU scores of 0.563 (English) and 0.563 (Chinese) for recovered responses, outperforming baseline methods
- Framework achieves privacy protection without significant degradation in answer quality

## Why This Works (Mechanism)

### Mechanism 1: Tri-Level Token Classification and Differential Treatment
The framework processes each query through a fine-tuned generative model (SenM) that identifies five GDPR-aligned privacy categories. High-risk tokens undergo substitution while low-risk tokens are obfuscated via differential privacy mechanisms. This targeted approach preserves semantic coherence while protecting privacy.

### Mechanism 2: Semantic Substitution via Context-Aware Replacement
A substitution model (SubM) generates contextually appropriate replacement pairs trained on 55,000 GPT-4-generated samples. This preserves grammatical structure and semantic relationships, allowing cloud LLMs to process queries correctly despite complete entity replacement.

### Mechanism 3: Dual-Stage Recovery with Reasoning Correction
The recovery model (RcvM) receives the sanitized query, original query, and cloud LLM response, then generates a corrected response that replaces substituted entities back while preserving the reasoning structure. This allows restoration of original information without compromising the cloud LLM's reasoning process.

## Foundational Learning

- **Concept: Local Differential Privacy (LDP) for Text**
  - Why needed here: The obfuscation stage uses Laplacian noise injection on token embeddings to generate candidate replacement sets. Understanding LDP principles helps evaluate the privacy-utility tradeoff.
  - Quick check question: Given a token with embedding emb, what determines whether a candidate token joins the adjacency set CW(ti)?

- **Concept: Entity-Level vs. Token-Level Privacy**
  - Why needed here: PRIV-QA operates at word/entity level for detection/substitution but token level for obfuscation. This distinction affects both protection strength and computational overhead.
  - Quick check question: Why might "thunderstorm" be treated differently than "Springfield" in the same query?

- **Concept: Black-Box LLM Privacy Threats**
  - Why needed here: The framework assumes cloud providers may attempt to extract user information from queries. Understanding extraction attacks (prompt injection, inference attacks) motivates the multi-layer defense.
  - Quick check question: The Extraction Defense Rate metric measures resistance against what specific type of attack?

## Architecture Onboarding

- **Component map:** User Query → [SenM] → Chunk → Detect → Aggregate Sensitive Set → [SubM] → Generate substitution pairs → Apply to Query → [ImpM] → Identify key words → (Optional Obfuscation on non-keywords) → Sanitized Query → Cloud LLM → Response → [RcvM] → (Original Query, Sanitized Query, Response) → Corrected Response

- **Critical path:** Sensitive detection accuracy (SenM) is the primary bottleneck—undetected sensitive words bypass all protection. Recovery quality (RcvM) determines final utility.

- **Design tradeoffs:**
  - Obfuscation enabled: Higher EDR (85.83%) but lower BLEU (0.563 vs 0.641 without obfuscation for English GPT-4-turbo)
  - Chunk size for long documents: Smaller chunks improve detection recall but increase processing overhead
  - Model sizes: 0.5B models for detection/substitution, 1.5B for recovery—larger models may improve accuracy at latency cost

- **Failure signatures:**
  - Detection miss: Original entity appears in sanitized query (check SenM precision/recall on domain-specific test)
  - Substitution incoherence: Cloud LLM returns confused or generic response (SubM training data mismatch)
  - Recovery hallucination: Final response contains entities or facts not in original query (RcvM over-correction)

- **First 3 experiments:**
  1. Validate SenM on your domain: Create 100 manually annotated samples from your target use case; measure precision/recall against the five GDPR categories.
  2. Test substitution plausibility: Send sanitized queries to your target cloud LLM; measure response coherence (BLEU against unsanitized responses) to detect domain-specific substitution failures.
  3. Measure end-to-end latency overhead: Profile chunk processing, substitution, obfuscation, and recovery stages separately with typical query lengths (see Figure 6 for baseline proportions).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can PRIV-QA's multi-stage sanitization and recovery approach be extended to complex reasoning tasks (e.g., mathematical problem-solving, code generation) where precise semantic preservation is critical for correct outputs?
- **Basis in paper:** [explicit] Section 7 states: "Experiments on more complex tasks, such as mathematical reasoning, are not explored."
- **Why unresolved:** The framework was designed and evaluated only on general QA tasks. Mathematical reasoning and code generation require exact preservation of numerical values, variable names, and logical structure—elements that substitution and obfuscation may disrupt irrecoverably.
- **What evidence would resolve it:** Evaluation on datasets like GSM8K (math) or HumanEval (code), measuring both privacy protection (EDR) and task-specific accuracy (pass@k for code, exact match for math).

### Open Question 2
- **Question:** What architectural or training modifications would close the 16+ percentage point gap between English (89.40%) and Chinese (73.01%) sensitive information detection recall?
- **Basis in paper:** [explicit] Table 2 reports substantially lower recall for Chinese. Additionally, Section 5.2 notes Chinese obfuscation results are not reported "due to the lack of open-source support for Chinese tokens" in the Tong et al. (2023) method.
- **Why unresolved:** The paper does not analyze whether the gap stems from tokenizer differences, training data quality, language-specific privacy patterns, or the chunking strategy's interaction with Chinese text segmentation.
- **What evidence would resolve it:** Ablation studies varying chunk size for Chinese, error analysis of missed Chinese sensitive entities, and comparison across multiple Chinese-capable base models beyond Qwen.

### Open Question 3
- **Question:** How does PRIV-QA perform against adversarial attacks beyond extraction attacks, such as membership inference, attribute inference, or reconstruction attacks targeting the recovery module itself?
- **Basis in paper:** [inferred] The evaluation (Section 5.2) focuses exclusively on "Extraction Defense Rate" via information extraction tasks. No other attack vectors are tested. The recovery module has access to original sensitive data, creating a potential vulnerability point.
- **Why unresolved:** An adversary who compromises or queries the local recovery module might extract sensitive mappings or training data. The paper assumes the local component is fully trusted.
- **What evidence would resolve it:** Evaluation against membership inference attacks on the recovery model, analysis of whether substitution patterns leak information, and stress-testing under adversarial queries designed to extract the (s_i, p_i) mapping pairs.

### Open Question 4
- **Question:** To what extent does the framework generalize across different base LLM families (e.g., LLaMA, Mistral, GPT variants) given that all pipeline models are trained exclusively on Qwen?
- **Basis in paper:** [explicit] Section 5.1: "We choose the open-source Qwen2-Chat model as our base model because of its relatively lightweight and strong performance in both Chinese and English." Section 7: "the models in our pipeline are trained solely on the Qwen series models."
- **Why unresolved:** Substitution and recovery models may overfit to Qwen's tokenization, vocabulary, or response patterns. Compatibility with models having different vocabularies or generation styles is unknown.
- **What evidence would resolve it:** Cross-model evaluation where pipeline models trained on Qwen are applied to LLaMA-based or GPT-based cloud LLMs, measuring BLEU, EDR, and win/tie rates.

## Limitations
- The five-category privacy taxonomy may not capture all privacy-sensitive contexts, particularly implicit information or domain-specific privacy concerns
- Semantic substitution assumes cloud LLMs can maintain reasoning quality with entirely fictitious entities, which may fail for complex relational queries
- The recovery module's effectiveness is constrained by the assumption that sanitized queries produce structurally similar reasoning paths to original queries
- Evaluation relies heavily on synthetic data generation via GPT-4o, raising questions about real-world generalization

## Confidence

- **High Confidence:** The multi-stage pipeline architecture and its general approach to privacy preservation through classification, substitution, and recovery are technically sound and well-specified.
- **Medium Confidence:** The specific performance numbers are plausible given the methodology, but their generalizability to real-world scenarios depends on the quality and representativeness of the synthetic training data.
- **Low Confidence:** Claims about the framework's effectiveness across all user demographics and query types are not well-supported, given the limited evaluation scope and reliance on bilingual Chinese/English data without broader language coverage.

## Next Checks

1. **Domain Transfer Test:** Evaluate PRIV-QA on a domain-specific QA dataset (e.g., medical, legal, or financial) with manually annotated privacy labels to assess performance degradation and identify category coverage gaps outside the five GDPR categories.

2. **Adversarial Attack Resistance:** Design and execute targeted inference attacks that exploit semantic relationships between entities to measure whether the framework can prevent attribute inference attacks beyond simple entity extraction.

3. **Real-World User Study:** Conduct a pilot deployment with actual end-users generating diverse queries in their native languages and contexts, measuring both privacy protection effectiveness and user-perceived utility degradation compared to the synthetic evaluation setup.