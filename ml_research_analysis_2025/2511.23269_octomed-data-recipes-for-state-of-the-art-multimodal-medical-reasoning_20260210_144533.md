---
ver: rpa2
title: 'OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning'
arxiv_id: '2511.23269'
source_url: https://arxiv.org/abs/2511.23269
tags:
- reasoning
- multimodal
- arxiv
- medical
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops OctoMed, a multimodal medical reasoning model
  trained via supervised fine-tuning on 8 million examples with 6.8 billion tokens.
  The training recipe leverages diverse structured reasoning traces across imaging
  modalities and task types, enabling the model to achieve state-of-the-art performance
  among open-source models on medical benchmarks.
---

# OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning

## Quick Facts
- arXiv ID: 2511.23269
- Source URL: https://arxiv.org/abs/2511.23269
- Reference count: 40
- Primary result: OctoMed achieves state-of-the-art performance among open-source models on medical benchmarks through supervised fine-tuning on 8M examples with 6.8B tokens

## Executive Summary
This work develops OctoMed, a multimodal medical reasoning model trained via supervised fine-tuning on 8 million examples with 6.8 billion tokens. The training recipe leverages diverse structured reasoning traces across imaging modalities and task types, enabling the model to achieve state-of-the-art performance among open-source models on medical benchmarks. OctoMed also demonstrates task-aware reasoning length adaptation without explicit supervision, producing longer traces for complex reasoning tasks and shorter ones for simpler classification.

## Method Summary
OctoMed is trained through knowledge distillation from DeepSeek-R1 (text-only) and GPT-4o (multimodal) using rejection sampling. The process involves generating 16 reasoning traces per question, keeping only those with correct answers, and fine-tuning for 3 epochs on the filtered dataset. The model uses diverse knowledge sources including text-only QA datasets (MedQA, MedMCQA), multimodal reasoning (PMC-VQA), and medical imaging datasets across multiple modalities.

## Key Results
- Achieves SOTA performance among open-source models on medical benchmarks
- Self-calibrates reasoning trajectory lengths based on task complexity without explicit supervision
- Improves generalization by mixing diverse knowledge sources (text-only, multimodal reasoning, multimodal classification)

## Why This Works (Mechanism)

### Mechanism 1
Combining diverse knowledge sources during SFT improves generalization without degrading in-domain performance. Diverse task exposures provide complementary reasoning patterns that transfer across modalities, teaching the model reusable medical reasoning schemas rather than task-specific heuristics.

### Mechanism 2
Using 16 reasoning traces per question via rejection sampling acts as regularization, improving peak performance more than additional training epochs. Multiple valid reasoning paths expose the model to diverse solution strategies for the same problem, reducing overfitting to specific reasoning patterns.

### Mechanism 3
Training on mixtures of varying reasoning trace lengths enables emergent task-aware reasoning length adaptation without explicit supervision. The model learns to associate complex inputs with longer reasoning patterns and simpler inputs with shorter traces from statistical regularities in the training distribution.

## Foundational Learning

- **Concept: Knowledge Distillation with Rejection Sampling**
  - **Why needed here:** Filters teacher-generated traces by answer correctness before student training
  - **Quick check question:** Can you explain why rejection sampling is critical for medical reasoning versus general instruction tuning?

- **Concept: Chain-of-Thought (CoT) vs. Direct Prompting Trade-offs**
  - **Why needed here:** CoT benefits reasoning tasks (+15 pts) but slightly harms classification tasks (-2 pts)
  - **Quick check question:** For a diabetic retinopathy grading task, which prompting strategy would you expect to perform better?

- **Concept: Multi-Source Data Mixing Without Interference**
  - **Why needed here:** Combining text-only and multimodal data improves rather than degrades performance
  - **Quick check question:** What data preprocessing step was used to prevent benchmark contamination?

## Architecture Onboarding

- **Component map:** Student backbone (Qwen2.5-VL-7B-Instruct) <- Teachers (DeepSeek-R1, GPT-4o) <- Training framework (LLaMA Factory) <- Inference (vLLM)

- **Critical path:**
  1. Curate 8M examples from text-only, multimodal reasoning, and classification sources
  2. Generate 16 reasoning traces per question using appropriate teacher
  3. Apply rejection sampling (keep only traces with correct answers)
  4. Full fine-tune for 3 epochs, batch size 512, LR 5e-5, cosine schedule

- **Design tradeoffs:**
  - Question filtering: Improves early sample efficiency but similar peak performance to no filtering
  - Teacher selection: DeepSeek-R1 superior for text reasoning; GPT-4o required for multimodal
  - Base model timing: SFT more effective on instruction-following models than reasoning-post-trained models

- **Failure signatures:**
  - Uniform reasoning lengths across tasks: Indicates insufficient trace length diversity in training
  - Degraded text-only performance: May indicate multimodal data dominance
  - Low rejection rate (<30%): May indicate teacher overconfidence or data too easy

- **First 3 experiments:**
  1. Ablate knowledge sources: Train three models using only text-only, only multimodal, and combined data
  2. Vary rejection samples: Compare 1 vs. 4 vs. 16 samples per question on MedQA validation
  3. Measure emergent calibration: Compare average token usage across PMC-VQA vs. MedXpertQA

## Open Questions the Paper Calls Out

1. How can reinforcement learning (RL) be effectively integrated with the OctoMed SFT pipeline to further enhance robustness across diverse clinical scenarios?

2. What is the optimal scoring function for rejection sampling on open-ended medical reasoning tasks where final answers cannot be easily verified?

3. Can task-aware reasoning length adaptation be explicitly controlled or enhanced through targeted data curation or training objectives?

4. Does the OctoMed data recipe transfer effectively to other model architectures and parameter scales beyond the tested 7-9B range?

## Limitations

- Data quality and representativeness across medical specialties remains unclear, with potential for interference between modalities
- Heavy reliance on teacher model reliability raises concerns about propagating biases through distillation
- Translation to real-world clinical utility remains unproven despite benchmark performance

## Confidence

**High Confidence**: Core finding that OctoMed achieves SOTA performance among open-source models on medical benchmarks is well-supported by quantitative results

**Medium Confidence**: Mechanism of improved generalization through diverse knowledge source mixing is supported by ablation studies but underlying reasons need investigation

**Low Confidence**: Claim that knowledge distillation with rejection sampling provides better regularization than additional training epochs lacks deeper analysis of why this occurs

## Next Checks

1. **Ablation Study Replication**: Train three separate models using only text-only data, only multimodal data, and the combined dataset to independently verify mixing sources improves performance

2. **Teacher Error Analysis**: Conduct systematic error analysis of DeepSeek-R1 and GPT-4o on medical reasoning tasks to quantify potential for propagating teacher biases

3. **Clinical Realism Test**: Evaluate OctoMed on a held-out set of real clinical cases to assess whether benchmark performance translates to clinically meaningful reasoning capabilities