---
ver: rpa2
title: 'Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment:
  the Patient''s Point of View'
arxiv_id: '2503.15718'
source_url: https://arxiv.org/abs/2503.15718
tags:
- patient
- language
- medical
- trial
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of recruiting patients for\
  \ clinical trials by adopting the patient\u2019s perspective, using patient language\
  \ rather than medical terminology to describe medical profiles. The authors design\
  \ a new dataset and task, NLI4PR, derived from the TREC 2022 Clinical Trial Track,\
  \ where patient profiles are rephrased into natural, lay language and matched to\
  \ clinical trial eligibility criteria framed as a Natural Language Inference problem."
---

# Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View

## Quick Facts
- arXiv ID: 2503.15718
- Source URL: https://arxiv.org/abs/2503.15718
- Authors: Mathilde Aguiar; Pierre Zweigenbaum; Nona Naderi
- Reference count: 23
- Primary result: Patient-initiated recruitment using natural language is feasible, with open-source LLMs achieving 56.5-71.8 F1 on clinical trial matching tasks

## Executive Summary
This paper addresses the challenge of recruiting patients for clinical trials by adopting the patient's perspective, using patient language rather than medical terminology to describe medical profiles. The authors design a new dataset and task, NLI4PR, derived from the TREC 2022 Clinical Trial Track, where patient profiles are rephrased into natural, lay language and matched to clinical trial eligibility criteria framed as a Natural Language Inference problem. They evaluate several open-source Large Language Models on this task in a zero-shot setting, comparing performance between patient language and medical language profiles. Results show that models achieve F1 scores from 56.5 to 71.8 using patient language versus 64.7 to 73.1 using medical language, with the best model (Qwen-2.5-14B) achieving 71.8 F1 in the patient language setting. The small performance gap suggests patient-initiated recruitment using natural language is feasible, paving the way for more accessible clinical trial matching systems.

## Method Summary
The study converts clinical trial eligibility assessment into a 2-way Natural Language Inference task, where patient profiles (statements) are classified as either Entailment (eligible) or Contradiction (excluded) relative to trial eligibility criteria (premises). The NLI4PR dataset contains 7,007 instances derived from TREC 2022 Clinical Trial Track, with patient profiles manually rephrased from medical terminology into natural language while preserving semantic content. Four open-source LLMs (Flan-T5-XXL, Qwen-2.5-7B/14B, Mixtral-8x7B) are evaluated zero-shot using vanilla and persona prompt templates. The zero-shot approach tests whether pretrained models can perform biomedical reasoning without domain-specific fine-tuning, with performance measured using macro-F1 score.

## Key Results
- Qwen-2.5-14B achieves the highest performance: 71.8 F1 using patient language and 73.1 F1 using medical language
- All models perform better on medical language than patient language, with performance gaps ranging from 1.5 to 8.6 F1 points
- Models show asymmetric performance between entailment (57.6-75.8 F1) and contradiction (74.1-86.8 F1), with some models performing up to 26 points better on contradiction
- Flan-T5 exhibits severe label bias, predicting Entailment up to 50 points more frequently than Contradiction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing patient-trial matching as Natural Language Inference (NLI) enables zero-shot evaluation using off-the-shelf LLMs without task-specific training.
- Mechanism: The task converts eligibility assessment into a 2-way classification: patient profile (statement) vs. trial eligibility criteria (premise) → Entailment (eligible) or Contradiction (excluded). The inference rule requires: (1) every patient feature must entail every inclusion criterion, and (2) every patient feature must contradict every exclusion criterion.
- Core assumption: LLMs pretrained on general text can perform biomedical, numerical, and commonsense reasoning without domain-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "We prompt several open-source Large Language Models on our task and achieve from 56.5 to 71.8 of F1 score using patient language, against 64.7 to 73.1 for medical language."
  - [Section 3.2]: Defines the inference relationship formally with entail/contradict conditions mapping to final predictions.
  - [corpus]: Related work (TrialMatchAI, MatchMiner-AI) confirms LLM-based matching is an active research direction, though most systems use structured EHR data rather than free-text patient language.
- Break condition: If patient profiles omit critical features (age, diagnosis, contraindications), the inference chain breaks—models cannot verify entailment across missing information.

### Mechanism 2
- Claim: LLMs exhibit semantic robustness when input shifts from medical terminology to patient language, with only moderate performance degradation.
- Mechanism: Pretraining on diverse corpora exposes models to both technical and lay descriptions of medical concepts. BERTScore of 89.5% between PL and ML versions indicates high semantic overlap despite lexical differences.
- Core assumption: The semantic content of patient profiles is preserved during manual rephrasing—no critical information is lost.
- Evidence anchors:
  - [Section 3.1]: "The patient and medical topics still keep similar features with a high BERTScore of 89.5%."
  - [Section 5]: "All models perform better on medical language than on PL. We believe this loss of performance may come in part from the lack of precision of layman terms."
  - [corpus]: Weak direct evidence—neighbor papers focus on EHR-based matching, not patient language processing specifically.
- Break condition: If patient language introduces ambiguity (e.g., "sclerosis" instead of "ALS"), models cannot determine specific disease type, leading to misclassification.

### Mechanism 3
- Claim: Contradiction detection is easier for models than entailment verification due to asymmetric reasoning requirements.
- Mechanism: For contradiction, a single exclusion criterion violation suffices. For entailment, models must verify ALL inclusion criteria AND verify NO exclusion criteria apply—requiring more computation and knowledge.
- Core assumption: Models correctly identify when to stop reasoning (single-match for contradiction vs. exhaustive-match for entailment).
- Evidence anchors:
  - [Section 6, Figure 5]: "Qwen is performing up to 26 points better on Contradiction than on Entailment... Flan-T5 obtains up to 50 points more in predicting Entailment than Contradiction."
  - [Section 6]: Explicitly states the asymmetry in reasoning complexity between the two labels.
  - [corpus]: No direct corroborating evidence from neighbor papers on this specific asymmetry.
- Break condition: Models may develop label biases (Flan-T5's Entailment bias) if training data or prompting induces systematic preferences, breaking the assumption of balanced reasoning.

## Foundational Learning

- Concept: **Natural Language Inference (NLI)**
  - Why needed here: The entire task architecture depends on understanding entailment (statement follows from premise) vs. contradiction (statement conflicts with premise). Without this, you cannot frame recruitment as classification.
  - Quick check question: Given "Patient has ALS" as statement and "Must have ALS diagnosis" as premise, what label applies?

- Concept: **Patient Language Characteristics**
  - Why needed here: Patient language differs systematically from medical language—lower precision ("sclerosis" vs. "ALS"), longer paraphrases, emotional content, potential typos. Understanding this helps diagnose why models struggle more with PL.
  - Quick check question: Why might a model fail to match "I have sclerosis" to a trial requiring "ALS diagnosis"?

- Concept: **Clinical Trial Eligibility Structure**
  - Why needed here: Trials have inclusion criteria (required features) AND exclusion criteria (disqualifying features). Both must be checked for every patient feature to determine eligibility.
  - Quick check question: If a patient meets all inclusion criteria but has one exclusion criterion condition, are they eligible?

## Architecture Onboarding

- Component map:
  - Input layer: statement_pl (patient language) or statement_medical (medical language) + premise (eligibility criteria)
  - Prompt templates: vanilla (simple instruction) vs. persona (doctor role-play)
  - Model layer: Flan-T5-XXL, Qwen-7B/14B, Mixtral-8x7B
  - Output layer: Entailment/Contradiction binary classification with macro-F1 evaluation
  - Dataset splits: 70/10/20 train/dev/test; 7,007 instances total

- Critical path:
  1. Load patient topic (PL or ML version) → format as statement
  2. Load CTR eligibility section → format as premise
  3. Apply prompt template (vanilla or persona)
  4. Model generates Entailment/Contradiction prediction
  5. Compare to gold label → compute macro-F1

- Design tradeoffs:
  - Manual vs. LLM rephrasing: Authors tried GPT-4o and Llama-3 for automatic rephrasing but found they omitted critical info (e.g., gender); chose manual annotation for consistency
  - Zero-shot vs. fine-tuning: Paper evaluates zero-shot only; fine-tuning on training split is noted as future work
  - 2-way vs. 3-way classification: TREC-CT has "not relevant" label; authors excluded this rather than mapping to "neutral" to maintain binary NLI framing

- Failure signatures:
  - Precision loss: Patient #21 uses "sclerosis" instead of "ALS" → model predicts Contradiction when correct is Entailment
  - Information overload: Patient #30 medical version includes extra details that mislead model into inferring exclusion conditions
  - Label bias: Flan-T5 heavily biased toward Entailment (up to 50-point gap vs. Contradiction accuracy)
  - Persona template backfire: Flan-T5 performs worse with persona + PL; persona framing doesn't universally help

- First 3 experiments:
  1. **Reproduce baseline**: Run Qwen-14B with vanilla template on test set using both statement_pl and statement_medical; verify macro-F1 ~71.8 and ~73.1 respectively
  2. **Ablate prompt template**: Compare vanilla vs. persona templates across all four models to identify which models benefit from role-play framing
  3. **Error analysis on precision gaps**: Identify cases where PL uses broader terms than ML (e.g., "sclerosis" vs. "ALS"); measure how often this causes misclassification vs. other error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning open-source LLMs on the NLI4PR training and development sets significantly improve performance over the zero-shot baseline?
- Basis in paper: [explicit] Section 7 states, "One direction for future work would be to fine-tune the models (using our training and development sets) to see if it would improve performance."
- Why unresolved: The current study restricted its methodology to zero-shot prompting to establish initial feasibility, leaving the potential benefits of supervised learning unexplored.
- What evidence would resolve it: Comparative results showing Macro F1 scores for models like Qwen-14B or Flan-T5-XXL after fine-tuning on the NLI4PR training split.

### Open Question 2
- Question: Do models predict the correct eligibility label based on valid reasoning and evidence retrieval, or do they rely on spurious correlations?
- Basis in paper: [explicit] Section 7 suggests "systematically evaluating models’ explanations would also allow to determine if the model is predicting the right label for the right reason."
- Why unresolved: The paper evaluates output labels (Entailment/Contradiction) but does not assess the faithfulness of the model's internal logic or the specific text evidence used to derive the conclusion.
- What evidence would resolve it: An evaluation using the "LLM-as-a-judge" paradigm or human annotation to verify that the evidence cited in model explanations aligns with the clinical trial criteria.

### Open Question 3
- Question: How does model performance degrade when patient profiles contain missing or incomplete information compared to the current controlled dataset?
- Basis in paper: [inferred] Section 9 (Limitations) notes, "We hypothesize that in a real-world scenario, a patient... might miss some elements, making the task even more challenging."
- Why unresolved: The dataset construction involved manually rephrasing topics while ensuring important information was preserved, which may not reflect the reality of self-reported patient data.
- What evidence would resolve it: A robustness test evaluating models on synthetic patient profiles where key medical features (e.g., demographics, specific symptoms) are systematically omitted.

## Limitations

- Data representativeness uncertainty: The dataset derives from 54 clinical trials, representing a narrow sample of the broader clinical trial landscape, and manual rephrasing may not capture all real-world linguistic variations patients use.
- Model performance variability: The modest performance gap between patient and medical language masks significant variation across models, with Flan-T5 exhibiting a dramatic 50-point Entailment bias suggesting label distribution issues.
- Clinical validity uncertainty: The study assumes original TREC annotations are clinically accurate, but these were created by information specialists rather than physicians, and the NLI conversion may oversimplify complex eligibility logic.

## Confidence

**High confidence**: The NLI framing mechanism works as described—models can perform zero-shot classification of patient-trial matching using either medical or patient language with measurable, reproducible differences in performance. The dataset construction methodology is transparent and the performance gap between PL and ML is consistently observed across multiple models.

**Medium confidence**: Patient-initiated recruitment using natural language is "feasible" based on the modest performance degradation. While the F1 scores suggest reasonable performance, the clinical utility threshold for real-world deployment remains undefined. The semantic preservation claim (BERTScore 89.5%) is methodologically sound but doesn't guarantee clinical equivalence.

**Low confidence**: The assertion that contradiction detection is inherently easier than entailment verification is supported by the data but lacks theoretical grounding. The Flan-T5 label bias appears severe but the paper doesn't explore whether this stems from the model architecture, prompt design, or dataset characteristics.

## Next Checks

1. **Clinical annotation verification**: Have 2-3 physicians independently review 100 random test instances to assess whether model predictions align with clinical judgment, particularly for cases where PL performance drops significantly below ML performance.

2. **Fine-tuning ablation study**: Fine-tune Qwen-2.5-14B on the training split and evaluate on the test set for both PL and ML conditions to establish the gap between zero-shot and fine-tuned performance, identifying whether the current results represent a lower bound.

3. **Linguistic error analysis**: Conduct detailed error analysis on 50 misclassified PL instances to categorize failure modes (semantic ambiguity, missing information, vocabulary mismatch) and determine if specific patient language patterns consistently cause misclassification.