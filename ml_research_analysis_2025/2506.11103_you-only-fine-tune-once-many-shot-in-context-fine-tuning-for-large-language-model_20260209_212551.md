---
ver: rpa2
title: 'You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language
  Model'
arxiv_id: '2506.11103'
source_url: https://arxiv.org/abs/2506.11103
tags:
- fine-tuning
- many-shot
- training
- arxiv
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ManyICL, a novel approach that extends in-context
  fine-tuning from few-shot to many-shot settings by introducing a "mask all targets"
  training strategy. Instead of only predicting the final answer, ManyICL treats every
  answer within the context as a supervised training target, effectively shifting
  the role of many-shot examples from prompts to targets for autoregressive learning.
---

# You Only Fine-Tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model

## Quick Facts
- **arXiv ID**: 2506.11103
- **Source URL**: https://arxiv.org/abs/2506.11103
- **Authors**: Wenchong He; Liqian Peng; Zhe Jiang; Alex Go
- **Reference count**: 40
- **Primary result**: Introduces ManyICL, a training strategy that treats every answer in context as a supervised target, achieving comparable results to task-level fine-tuning with far fewer examples

## Executive Summary
This paper addresses the limitation of traditional in-context learning (ICL) that requires task-specific fine-tuning for optimal performance. ManyICL extends ICL from few-shot to many-shot settings by introducing a "mask all targets" training strategy that treats every answer within the context as a supervised training target. This approach significantly improves training efficiency and performance, achieving results comparable to task-level fine-tuning while using far fewer examples. The method substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning across diverse downstream tasks including classification, summarization, question answering, natural language inference, and math.

## Method Summary
ManyICL modifies the standard autoregressive training objective by computing loss on all target tokens within packed sequences, not just the final answer. The model is trained on 29 held-in datasets from CROSSFIT, constructing prompts by concatenating input-output pairs to fill the 32K context window (averaging ~212 shots per instance). During training, loss is computed on all target positions y₀, y₁, ..., yₙ simultaneously, enabling zero-shot, few-shot, and many-shot capabilities in one forward pass. The approach uses LoRA fine-tuning on Mistral-v0.3 with 8×A100-80G GPUs, per-device batch size=1, global batch size=32, and 4 gradient accumulation steps. At inference, the model generalizes to 14 held-out datasets using only in-context demonstrations.

## Key Results
- ManyICL achieves accuracy comparable to task-level fine-tuning on classification, NLI, and QA tasks while using far fewer examples
- On summarization tasks, ManyICL approaches task-level fine-tuning performance with only ~25 shots in 32K context versus thousands needed for classification
- The approach significantly mitigates catastrophic forgetting, showing only 0.07 perplexity increase on PG-19 at 1K context versus 0.27 for zero/few-shot fine-tuning
- Ablation studies show transfer learning across task categories, with math performance improving even without direct math supervision

## Why This Works (Mechanism)

### Mechanism 1: Mask-All-Targets Training Objective
Treating every answer token in the context window as a supervised target improves training efficiency and ICL capability across shot ranges. During training, loss is computed over all target positions y₀, y₁, ..., yₙ simultaneously, converting demonstration examples from passive prompts into active autoregressive learning signals. This enables a single packed sequence to train zero-shot, few-shot, and many-shot capabilities in one forward pass.

### Mechanism 2: Maximum In-Context Length Training with Dense Packing
Maximizing in-context examples per training instance (filling context window to capacity) improves many-shot ICL performance and preserves long-context capabilities. Training sequences are constructed by concatenating as many input-output pairs as fit within the model's context window, exposing the model to long-range dependencies and dense demonstration patterns during meta-training.

### Mechanism 3: Multi-Task Meta-Learning Transfer
Joint meta-training across diverse tasks enables ICL capability transfer to held-out tasks without task-specific fine-tuning. The model trained on 29 held-in datasets spanning classification, NLI, QA, math, and summarization generalizes to 14 held-out datasets using only in-context demonstrations, with ablation showing transfer benefits across task categories.

## Foundational Learning

- **In-Context Learning (ICL)**: Why needed here - ManyICL fundamentally improves ICL capabilities through fine-tuning. Quick check: Can you explain why ICL is called "training-free" and what the performance gap is compared to dedicated fine-tuning for moderate-sized LLMs?

- **Autoregressive Loss and Masking Strategies**: Why needed here - The core innovation (mask-all-targets) modifies how loss is computed. Quick check: Given a sequence [x₀, y₀, x₁, y₁, x₂, y₂], which tokens contribute to loss in "mask last target" vs. "mask all targets"?

- **Catastrophic Forgetting in Fine-Tuning**: Why needed here - A key claim is ManyICL mitigates forgetting. Quick check: Why does the paper evaluate PG-19 perplexity at varying context lengths, and what would increasing perplexity indicate?

## Architecture Onboarding

- **Component map**: Data pipeline (stratified/random sampling → prompt construction → tokenization) → Training loop (forward pass → mask-all-targets loss → LoRA updates) → Inference (KV-cache → processing → generation) → Evaluation (held-out datasets with accuracy/ROUGE-L)

- **Critical path**: 1) Prompt construction is load-bearing - incorrect formatting breaks ICL; 2) Mask implementation - ensure loss mask correctly identifies target tokens; 3) Context window management - exceeding max length truncates sequences

- **Design tradeoffs**: Mask-all vs. mask-last (~100× more token-efficient but requires careful masking); LoRA vs. full fine-tuning (efficiency vs. potential performance); shot count at inference (diminishing returns beyond certain point)

- **Failure signatures**: Performance worse than no fine-tuning (check mask implementation); no improvement with more shots (verify training included diverse tasks); high perplexity on PG-19 (indicates catastrophic forgetting)

- **First 3 experiments**: 1) Validate mask-all-targets implementation by comparing to mask-last on single task; 2) Ablation on shot count at inference to confirm scaling behavior; 3) Long-context preservation test by measuring PG-19 perplexity before/after fine-tuning

## Open Questions the Paper Calls Out

- **Question 1**: How can the context window be optimized to accommodate many-shot prompts without excessively consuming token budget for user inputs and system prompts? [explicit] The conclusion identifies this as a remaining challenge without proposing specific methods.

- **Question 2**: Why does ManyICL exhibit significantly weaker generalization for summarization tasks compared to classification or NLI when task-specific training data is removed? [inferred] Ablation shows summarization performance degrades to base model levels unlike other tasks.

- **Question 3**: Can inference costs be reduced beyond standard KV caching to make ManyICL practical for real-time applications with extremely long contexts? [explicit] The authors note future work will explore reducing inference costs.

## Limitations

- Empirical evaluation focuses heavily on English-language tasks from curated dataset collection, raising generalization questions to non-English languages and specialized domains
- Ablation studies on catastrophic forgetting use only PG-19 benchmark without exploring other forgetting effects on knowledge or reasoning capabilities
- Computational efficiency claims comparing mask-all-targets to mask-last are based on theoretical token counts rather than wall-clock training time measurements

## Confidence

- **High confidence**: Core mask-all-targets training objective improves many-shot ICL performance compared to zero/few-shot baselines
- **Medium confidence**: ManyICL approaches task-level fine-tuning performance while using far fewer examples
- **Medium confidence**: Catastrophic forgetting mitigation claims are plausible given PG-19 results
- **Low confidence**: Transfer mechanism explanations are speculative without strong mechanistic evidence

## Next Checks

1. **Generalization across domains**: Evaluate ManyICL on non-English datasets, code generation tasks, and specialized domains (medicine, law, STEM) to test whether meta-learning benefits transfer beyond original task distribution

2. **Comprehensive forgetting analysis**: Beyond PG-19 perplexity, evaluate forgetting on knowledge-intensive tasks, reasoning benchmarks, and multilingual capabilities, comparing against multiple fine-tuning baselines

3. **Scaling and efficiency validation**: Test whether ManyICL benefits persist when scaling to larger models and measure actual wall-clock training time rather than theoretical token efficiency