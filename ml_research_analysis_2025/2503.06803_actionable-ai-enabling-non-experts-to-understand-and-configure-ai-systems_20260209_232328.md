---
ver: rpa2
title: 'Actionable AI: Enabling Non Experts to Understand and Configure AI Systems'
arxiv_id: '2503.06803'
source_url: https://arxiv.org/abs/2503.06803
tags:
- cartpole
- influences
- game
- teams
- during
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Actionable AI, a framework enabling non-experts
  to configure black-box AI systems through direct manipulation without requiring
  explanations or domain expertise. The authors conducted a 30-minute gaming experiment
  with 44 participants (22 pairs) using a modified cartpole game where players controlled
  AI behavior via interactive "influences" (circles on screen).
---

# Actionable AI: Enabling Non Experts to Understand and Configure AI Systems

## Quick Facts
- arXiv ID: 2503.06803
- Source URL: https://arxiv.org/abs/2503.06803
- Reference count: 40
- Non-experts can configure black-box AI systems through direct manipulation without explanations or domain expertise

## Executive Summary
This paper introduces Actionable AI, a framework enabling non-experts to configure black-box AI systems through direct manipulation without requiring explanations or domain expertise. The authors conducted a 30-minute gaming experiment with 44 participants (22 pairs) using a modified cartpole game where players controlled AI behavior via interactive "influences" (circles on screen). Their findings showed that 14 out of 22 teams achieved good performance, with 20 out of 22 players developing operational understanding sufficient to configure the AI toward their goals. Players could pass an average of 3.69 consecutive gates in high-performing teams, with some reaching 11.29 consecutive gates. The study demonstrates that non-experts can successfully configure black-box AI systems through direct interaction, developing practical understanding without needing to comprehend the underlying model.

## Method Summary
The experiment used a modified cartpole environment where an RL agent (DiscreteSAC, trained for 710,000 steps) controlled the cart's movement while players used keyboard controls to position and resize two influence circles on screen. The agent's action was determined by a weighted fusion of the model's preference vector and the user's influence vector (Algorithm 1-2). 22 teams of two players each played for 30 minutes, with one player controlling influences and the other providing verbal guidance. Teams progressed through three difficulty levels with increasing gates and environmental disturbances. Performance was measured by consecutive gates passed, with 14 out of 22 teams achieving good performance (more than 3 consecutive gates).

## Key Results
- 14 out of 22 teams achieved good performance (more than 3 consecutive gates)
- 20 out of 22 players developed operational understanding sufficient to configure the AI
- Average of 3.69 consecutive gates passed in high-performing teams, with some reaching 11.29
- Teams exhibited three distinct strategies: "The Bully" (aggressive influence), "The Passenger" (passive observation), and "The Negotiator" (balanced collaboration)

## Why This Works (Mechanism)

### Mechanism 1: Operational Understanding via Direct Manipulation
Non-experts can achieve functional control of black-box AI systems by developing "operating representations" through real-time direct manipulation rather than "correct representations" of internal workings. The system substitutes symbolic explanations with visual interaction elements (circles) that afford direct physical manipulation, creating a causal link between user action and system reaction through trial-and-error.

### Mechanism 2: Hybrid Authority via Influence Weighting
Users configure the system by implicitly negotiating control with the AI model through a weighted fusion of user intent and model stability. The architecture uses a fusion logic where the final action is a product of the model's preference vector and the user's influence vector, creating a symbiotic relationship where the user provides direction and the model provides stability.

### Mechanism 3: Uncertainty-Driven Experiential Learning
The absence of explicit instructions drives faster, deeper engagement and strategy formulation than "explainable" or instructed setups. By removing explanations and clear rules, the system forces users into hypothesis-testing mode, focusing them on interface affordances rather than pre-existing mental models of AI.

## Foundational Learning

- **Reinforcement Learning (RL) Policy**: Understanding that the "agent" is a mathematical policy optimizing a specific reward function, where "configuration" means perturbing inputs to this policy. Quick check: Can you explain why the cart moves on its own even when the user does nothing?

- **Affordance**: Design interactions that suggest their own usage. The "circles" were intuitively understood as "magnets" or "forces" because of their spatial properties. Quick check: Why did users describe the circles as "magnets" even though no text told them so?

- **Operational Understanding vs. Conceptual Understanding**: This is the core metric of success, distinguishing "knowing how to use it" from "knowing how it works." Quick check: If a user thinks the AI is a "ghost" but successfully steers the cart, is the system a success?

## Architecture Onboarding

- **Component map**: User Input (Keyboard) -> Update Influence State -> Fusion Engine (Calculate inf, combine with m) -> Step Physics Engine -> Render Visual Feedback

- **Critical path**: Keyboard inputs update influence circle position/size, Fusion Engine calculates influence vector and combines with model's preference vector, final action selected via argmax of combined vector, physics engine updates cartpole state, visual feedback rendered

- **Design tradeoffs**: Transparency vs. Usability (visibility of action space over transparency of internal logic), Model Strength vs. User Control (balancing influence weights to prevent either user helplessness or model destabilization)

- **Failure signatures**: "The Passenger" (no influence, cart wanders), "The Bully" (max influence, pole falls), "Confusion" (user actions uncorrelated with outcomes, likely calculation bug)

- **First 3 experiments**: 1) Calibration of Influence (vary intensity scalar to find Goldilocks zone), 2) Input Latency Test (measure delay tolerance, hypothesized <100ms required), 3) Noise Floor Test (vary stochastic noise to find threshold where system appears random)

## Open Questions the Paper Calls Out

- Can the Actionable AI framework be effectively applied to agents performing tasks beyond navigation using direct manipulation?
- How does the Actionable AI framework perform in complex, real-world environments compared to simulated ones?
- How does the framework function in diverse interaction models, such as single-user or multi-user setups, distinct from the pair configuration used?

## Limitations

- Exact mathematical formulation of influence weighting algorithms (location_related_weight, speed_related_weight) not specified
- Specific keyboard controls and UI layout details for the Influencer view not provided
- Precise quantitative definitions of environmental disturbances (slope, wind, bumps) lacking

## Confidence

- **High Confidence**: Core finding that non-experts can develop operational understanding through direct manipulation (14/22 teams achieved good performance)
- **Medium Confidence**: Hybrid authority mechanism plausible but exact weight parameters unspecified
- **Low Confidence**: Generalizability beyond cartpole domain remains uncertain

## Next Checks

1. **Algorithm Specification Test**: Implement pilot versions with different influence weighting schemes to determine minimum specification needed for functional influence
2. **Latency Sensitivity Test**: Measure maximum acceptable delay in action-feedback loop (starting at 100ms) to determine threshold where operational understanding breaks down
3. **Cross-Domain Transfer Test**: Adapt influence mechanism to different black-box AI system (e.g., image classification or text generation) to test generalizability beyond reinforcement learning agents