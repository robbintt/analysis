---
ver: rpa2
title: 'NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with
  Surrogate-Free Training'
arxiv_id: '2601.21279'
source_url: https://arxiv.org/abs/2601.21279
tags:
- fp32
- encoding
- gate
- bit-exact
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NEXUS, a framework achieving bit-exact equivalence
  between Spiking Neural Networks (SNNs) and standard Artificial Neural Networks (ANNs)
  through neuromorphic gate circuits. Unlike prior SNN methods that approximate continuous
  values with discrete spikes, NEXUS constructs all arithmetic operations - both linear
  and nonlinear - from pure Integrate-and-Fire (IF) neuron logic gates implementing
  IEEE-754 compliant floating-point arithmetic.
---

# NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training

## Quick Facts
- arXiv ID: 2601.21279
- Source URL: https://arxiv.org/abs/2601.21279
- Authors: Zhengzheng Tang
- Reference count: 40
- Primary result: Achieves 0.00% accuracy degradation on transformer models up to LLaMA-2 70B through bit-exact ANN-to-SNN equivalence using neuromorphic gate circuits

## Executive Summary
NEXUS presents a framework for achieving bit-exact equivalence between Spiking Neural Networks (SNNs) and standard Artificial Neural Networks (ANNs) by constructing all arithmetic operations from pure Integrate-and-Fire (IF) neuron logic gates. Unlike prior SNN methods that approximate continuous values with discrete spikes, NEXUS uses spatial bit encoding to map IEEE-754 floating-point bit patterns directly to parallel spike channels with zero encoding error. This enables surrogate-free training where the Straight-Through Estimator becomes an exact identity mapping. Experiments demonstrate identical task accuracy across models up to LLaMA-2 70B with mean ULP error of only 6.19, while achieving 27-168,000× energy reduction on neuromorphic hardware.

## Method Summary
NEXUS achieves bit-exact ANN-to-SNN equivalence by implementing IEEE-754 compliant floating-point arithmetic using pure IF neuron logic gates. The core innovation is spatial bit encoding, which maps IEEE-754 bit patterns directly to parallel spike channels with zero encoding error by construction. All arithmetic operations (addition, multiplication, transcendental functions) are built from basic IF neuron gates implementing Boolean logic (AND, OR, NOT, XOR). The framework enables surrogate-free training because the SNN forward pass is mathematically equivalent to the ANN, making the Straight-Through Estimator an exact identity mapping rather than an approximation.

## Key Results
- 0.00% accuracy degradation across transformer models up to LLaMA-2 70B (Qwen3-0.6B, Phi-2, LLaMA-2 7B/70B, Mistral)
- Mean ULP error of only 6.19 with 27.7%+ 0-ULP rate across all models
- 27-168,000× energy reduction on Loihi neuromorphic hardware
- 100% accuracy retention across all membrane potential decay factors β ∈ [0.1,1.0]
- >98% gate-level accuracy with synaptic noise up to σ=0.2

## Why This Works (Mechanism)

### Mechanism 1: Spatial Bit Encoding as Lossless Bijection
IEEE-754 floating-point values can be mapped to parallel spike channels with zero encoding error by construction. Each FP32 value is bit-reinterpreted to an integer, then each of the 32 bits is extracted to a separate spike channel (MSB-first). Decoding reverses this exactly. The bijection is lossless because no approximation is involved—only direct bit extraction and reconstruction.

### Mechanism 2: IF Neuron Threshold Selection Implements Boolean Logic
A single IF neuron with appropriate threshold and weights can implement AND, OR, NOT exactly. The IF neuron fires when weighted input sum exceeds threshold θ. AND is IF₁.₅(a+b) (fires only when a+b=2). OR is IF₀.₅(a+b) (fires when a+b≥1). NOT uses bias 1.5 with inhibitory weight −1 and threshold 1.0. These primitives compose into XOR, MUX, and full adders.

### Mechanism 3: Surrogate-Free Training via Exact Forward Equivalence
When the SNN forward pass is bit-exact to the ANN, the STE gradient approximation becomes an exact identity. Since spatial encoding is a lossless bijection and gate circuits implement exact IEEE-754 arithmetic, f(g(x)) = f(x) exactly. Therefore ∂g/∂x = 1 is not an approximation but mathematically correct. No surrogate function is needed.

## Foundational Learning

- **IEEE-754 Floating-Point Representation**
  - Why needed here: Spatial bit encoding directly manipulates sign, exponent, and mantissa bits; understanding FP structure is essential for debugging arithmetic circuits.
  - Quick check question: Given FP32 value 0x40490FDB, what decimal value does it approximate?

- **Integrate-and-Fire (IF) Neuron Model**
  - Why needed here: All logic gates and arithmetic circuits are built from IF neurons with threshold-based firing; the soft-reset mechanism preserves residual potential for correct gate behavior.
  - Quick check question: An IF neuron with threshold 1.5 receives inputs [1, 0] with equal weights. Does it fire?

- **Straight-Through Estimator (STE)**
  - Why needed here: This framework uses STE but claims it is exact rather than approximate; understanding standard STE clarifies why this is novel.
  - Quick check question: In standard quantization-aware training, what does STE approximate and why is it needed?

## Architecture Onboarding

- **Component map:**
  - Level 1: IF neurons → basic gates (AND/OR/NOT)
  - Level 2: Gates → full adders, ripple-carry adders
  - Level 3: Adders/multipliers → IEEE-754 FP32 arithmetic units (~3,348–12,450 neurons per unit)
  - Level 4: FP32 units → nonlinear functions (Softmax, SiLU, RMSNorm)
  - Training: Standard backprop with STE identity; no surrogate functions

- **Critical path:**
  1. Verify spatial encoding/decoding on 10K random FP32 values (0 reconstruction error expected)
  2. Test individual gates (AND/OR/NOT/XOR) with all input combinations
  3. Validate full adder correctness across carry chains
  4. Compare FP32 adder/multiplier outputs against PyTorch (mean ULP < 2 expected)

- **Design tradeoffs:**
  - Precision vs. robustness: FP8 tolerates more noise than FP32 (fewer bits to corrupt)
  - Neuron count vs. latency: Ripple-carry is simple but sequential; parallel prefix adders would reduce depth but increase area
  - Energy vs. accuracy: Lower β (more leakage) is tolerated due to single-timestep design, but noise tolerance remains bounded

- **Failure signatures:**
  - Gate accuracy < 98% at σ=0.1 → check threshold calibration or input scaling
  - Mean ULP > 10 on simple operations → arithmetic circuit logic error
  - Training divergence → verify forward pass equivalence first; if broken, STE assumption is invalid

- **First 3 experiments:**
  1. Replicate encoding verification: encode/decode 1M FP32 values; confirm 0 errors.
  2. Gate truth table validation: test AND/OR/NOT/XOR with all 2-bit inputs; verify 100% correctness at β=1.0, σ=0.
  3. FP32 addition stress test: add 1,024 random pairs; report max ULP and mean ULP against PyTorch reference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the actual latency and throughput characteristics of NEXUS gate circuits compared to GPU implementations, given the massive IF neuron counts (~61.3M per transformer block)?
- Basis in paper: [inferred] The paper thoroughly documents energy reduction (27–168,000×) but does not report inference latency, throughput (tokens/second), or wall-clock time comparisons.
- Why unresolved: Energy efficiency alone is insufficient for deployment decisions; real-time applications require latency bounds.
- What evidence would resolve it: End-to-end latency measurements on actual neuromorphic hardware with tokens/second benchmarks against GPU baselines.

### Open Question 2
- Question: Can NEXUS achieve bit-exact equivalence during training (not just inference) at scales beyond 0.6B parameters?
- Basis in paper: [explicit] "Our experiments demonstrate stable convergence on models as large as Qwen3-0.6B" — training validation is limited to 0.6B, while inference is tested up to 70B.
- Why unresolved: Gradient flow through millions of IF neuron gates at larger scales may exhibit instabilities not observed in smaller models.
- What evidence would resolve it: Successful bit-exact training runs with convergence curves on 7B+ models.

### Open Question 3
- Question: How do theoretical energy estimates translate to actual power consumption on physical neuromorphic hardware like Loihi 2?
- Basis in paper: [inferred] Energy calculations use "Intel Loihi 2 power model (23.6 pJ/SynOp)" from published metrics, not empirical measurements from deployed NEXUS circuits.
- Why unresolved: Simulated energy models may not capture overhead from routing, clock distribution, and inter-chip communication in real deployments.
- What evidence would resolve it: Measured power consumption from NEXUS circuits running on physical Loihi 2 or comparable neuromorphic chips.

### Open Question 4
- Question: What is the precision-robustness-energy tradeoff space for NEXUS, and can lower-precision formats (FP8/FP16) maintain acceptable accuracy under hardware non-idealities?
- Basis in paper: [explicit] "Lower-precision formats are more robust: FP8 operations maintain >95% accuracy at σ=0.05, while FP32 drops to ∼85% — more bits provide more targets for noise-induced flips."
- Why unresolved: The paper demonstrates the tradeoff exists but does not explore whether intentionally reducing precision could yield better energy-robustness operating points.
- What evidence would resolve it: Systematic sweep of precision levels (FP8, FP16, bfloat16) measuring accuracy, noise tolerance, and energy on identical workloads.

## Limitations

- Scalability concerns: LLaMA-2 70B would require approximately 1.5 trillion IF neurons, likely impractical for current neuromorphic hardware
- Hardware noise limits: Reported >98% gate accuracy only holds up to synaptic noise σ=0.2, suggesting a hard upper bound on noise tolerance
- Theoretical energy estimates may not capture real-world routing and communication overhead in physical neuromorphic deployments

## Confidence

- **High Confidence**: Spatial bit encoding achieves zero reconstruction error (verified by construction); IF neuron logic gates implement exact Boolean functions (truth table validation); mean ULP error remains low (~6.19) for small networks.
- **Medium Confidence**: Surrogate-free training identity holds for models up to 7B parameters; energy reduction estimates scale from small proof-of-concept to large models; leakage immunity claims based on single-timestep design.
- **Low Confidence**: Scalability to 70B+ parameter models (trillion-neuron requirement); real-world neuromorphic hardware noise tolerance beyond σ = 0.2; end-to-end equivalence maintenance through entire training pipeline for very deep networks.

## Next Checks

1. Scale stress test: Implement FP32 arithmetic units for a 100M parameter transformer and measure ULP accumulation across all layers.
2. Noise robustness evaluation: Test gate accuracy under σ = 0.3-0.5 noise conditions to establish practical hardware limits.
3. End-to-end training verification: Train a 1B parameter model from scratch using the NEXUS framework and compare intermediate layer activations against floating-point ANN baseline at each step.