---
ver: rpa2
title: 'AverageTime: Enhance Long-Term Time Series Forecasting with Simple Averaging'
arxiv_id: '2412.20727'
source_url: https://arxiv.org/abs/2412.20727
tags:
- time
- series
- forecasting
- information
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AverageTime, a method for long-term time series
  forecasting that leverages simple averaging to enhance performance. The approach
  captures intra-sequence and cross-channel dependencies by generating multiple novel
  sequences through various structural mechanisms, including channel information extraction
  and series decomposition.
---

# AverageTime: Enhance Long-Term Time Series Forecasting with Simple Averaging

## Quick Facts
- arXiv ID: 2412.20727
- Source URL: https://arxiv.org/abs/2412.20727
- Reference count: 25
- Primary result: Achieves up to 3.47% MSE improvement over state-of-the-art models while maintaining near-linear complexity

## Executive Summary
AverageTime introduces a simple yet effective approach to long-term multivariate time series forecasting by leveraging dual-stage averaging operations. The method captures both intra-sequence and cross-channel dependencies through channel information extraction and series decomposition, generating multiple enriched sequences that are fused with the original series via averaging. A channel clustering technique further improves computational efficiency with negligible performance loss. Experiments demonstrate AverageTime outperforms state-of-the-art models across seven real-world datasets while maintaining near-linear complexity.

## Method Summary
AverageTime enhances forecasting by generating enriched sequences through channel information extraction using Transformer encoders and MLPs along the channel dimension, combined with optional series decomposition (FVMD). These extracted sequences are fused with the original series through dual averaging operations - first averaging the extracted sequences together, then averaging the predictions from this fused sequence with predictions from the original series. The LightAverageTime variant adds channel clustering via Spearman correlation and Label Propagation to share prediction MLPs within clusters, significantly improving efficiency. The approach uses RevIN normalization, parameter-independent MLPs for prediction, and MSE loss over 30 epochs with early stopping.

## Key Results
- Achieves up to 3.47% MSE improvement over state-of-the-art baselines across seven datasets
- Maintains near-linear complexity with channel clustering, achieving ~50% memory reduction and 67.5% training time reduction
- Shows strong stability across datasets with varying channel counts (7-500+ channels)
- FVMD decomposition provides additional 0.26-0.91% MSE improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-stage averaging of channel-extracted sequences with original sequences improves forecasting accuracy by preserving information that would otherwise be lost during channel transformation.
- Mechanism: Multiple MLPs and Transformer Encoders extract channel information, generating new sequences averaged together. Predictions from this fused sequence are averaged with predictions from the original series, creating complementary information streams.
- Core assumption: Channel extraction processes are lossy and discard temporal information present in the original sequence that remains predictive.
- Evidence anchors: Abstract states "two straightforward averaging operations"; Table 4 shows models incorporating original sequences outperform those without across all prediction lengths; weak corpus support from neighbor paper proposing learnable complementary sequences.
- Break condition: If channel extraction preserves all predictive information or extracted and original predictions are highly correlated, averaging provides no benefit.

### Mechanism 2
- Claim: Applying Transformer attention and MLPs along the channel dimension captures cross-channel dependencies that improve multivariate forecasting.
- Mechanism: Input series is transposed so channels become the sequence dimension, allowing Transformer encoders with attention to learn which channels influence each other.
- Core assumption: Channels in multivariate time series exhibit dependencies where some channels' patterns predict or correlate with others.
- Evidence anchors: Abstract mentions "captures intra-sequence and cross-channel dependencies"; Section 3.1 describes transposing and using Transformers along channel dimension; iTransformer validates channel-as-token approach.
- Break condition: If channels are truly independent, channel extraction adds noise without signal; performance gains should disappear on univariate data.

### Mechanism 3
- Claim: Clustering channels by Spearman correlation and sharing prediction MLPs within clusters substantially reduces computational cost with negligible accuracy loss.
- Mechanism: Spearman correlation matrix computed on training data, thresholded at T=0.8, channels clustered using Label Propagation Algorithm, and channels in same group share a single MLP predictor.
- Core assumption: Highly correlated channels have similar temporal patterns and can share predictive parameters without significant approximation error.
- Evidence anchors: Abstract states "substantially improves training and inference efficiency with negligible performance loss"; Figure 5 shows threshold ~0.8 achieves comparable performance to TimeXer with 67.5% training time and ~50% memory.
- Break condition: If channels within clusters have meaningfully different temporal dynamics despite correlation, shared parameters will underfit individual channel patterns.

## Foundational Learning

- Concept: **Attention mechanism on non-standard dimensions**
  - Why needed here: Standard Transformers attend over time steps; AverageTime applies attention over channels. Understanding that attention operates on arbitrary sequences is prerequisite.
  - Quick check question: If you transpose a (C=7, T=96) matrix to (T=96, C=7) and apply attention, what relationships is the model learning?

- Concept: **Bias-variance tradeoff in ensemble averaging**
  - Why needed here: The averaging operation is an ensemble technique; understanding when averaging reduces variance without introducing bias explains why it works here.
  - Quick check question: If two predictors have errors that are perfectly correlated, what happens when you average their predictions?

- Concept: **Community detection / graph clustering**
  - Why needed here: LightAverageTime constructs a channel similarity graph and clusters it using Label Propagation; understanding graph-based clustering is necessary to modify or tune this component.
  - Quick check question: Label Propagation iteratively updates node labels based on neighbor majority—what happens if the graph has no clear community structure?

## Architecture Onboarding

- Component map:
  - Input: Multivariate series X∈R^(C×T₁) with RevIN normalization
  - Channel Extraction Branch: Transpose → Transformer Encoder + MLP (on channel dim) → Transpose back → produces enriched sequence
  - Original Branch: Pass-through with optional decomposition (FVMD)
  - Prediction Heads: Parameter-independent MLPs (one per channel) or grouped MLPs (LightAverageTime)
  - Fusion: Average(extracted_predictions, original_predictions)
  - Output: Y∈R^(C×T₂)

- Critical path:
  1. Channel extraction quality determines information gain (Figure 4 shows layer depth matters)
  2. Averaging operation is the fusion point—removing it breaks the mechanism (ablation Table 5 shows 3.47% MSE degradation without it)
  3. Clustering threshold T controls efficiency-accuracy tradeoff (Figure 5)

- Design tradeoffs:
  - Threshold T=0.8 (default): Chosen for balance; T→1.0 behaves like independent channels (slower, more accurate); T→0 forces all channels to share parameters (fastest, underfits)
  - Look-back window=96: Fixed across experiments; paper notes sensitivity to hyperparameters but does not ablate this
  - FVMD decomposition: Optional branch; adds 0.26-0.91% MSE improvement but requires additional computation

- Failure signatures:
  - Performance collapses on datasets with few channels (ETT series show smaller gains)—channel extraction has less information to capture
  - Large performance gap between AverageTime and LightAverageTime indicates threshold T is too aggressive for the dataset's correlation structure
  - Training instability or divergence may occur if learning rate is not tuned per dataset

- First 3 experiments:
  1. Reproduce ablation without averaging (configuration 110 vs 111): Train on Electricity dataset with all components except averaging; should observe ~3.47% MSE increase. Validates fusion mechanism.
  2. Sweep clustering threshold T on Weather: Test T ∈ {0.6, 0.7, 0.8, 0.9, 1.0}, record both MSE and training time. Establishes efficiency-accuracy curve for your infrastructure.
  3. Channel extraction ablation on low-channel dataset: Compare AverageTime with/without channel extraction branch on ETTh1 (7 channels). If gains disappear, confirms mechanism depends on channel count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learnable fusion mechanisms (e.g., gating or attention) outperform the simple averaging operation without significantly increasing computational complexity?
- Basis in paper: The conclusion and Section 3.1 state that the "fusion strategy may also be extended to more sophisticated mechanisms, such as gating operations, to further improve model performance."
- Why unresolved: The paper validates simple averaging as effective, but does not quantify the trade-off between the potential accuracy gains of complex fusion versus the added computational cost.
- What evidence would resolve it: Ablation studies replacing the averaging block with learnable gating or attention mechanisms, comparing MSE/MAE improvements against FLOPs and latency.

### Open Question 2
- Question: What alternative information extraction methods or data sources provide the most synergistic value when integrated into the AverageTime fusion framework?
- Basis in paper: The conclusion suggests future work should "explore providing more effective information for time series fusion."
- Why unresolved: The paper tests FVMD and channel extraction, but the generalizability of the framework to other decomposition techniques or exogenous variables remains unverified.
- What evidence would resolve it: Experiments integrating diverse signal processing techniques (e.g., wavelet transforms) or external covariates into the fusion branch to measure marginal performance improvements.

### Open Question 3
- Question: Can the channel clustering threshold be determined adaptively rather than manually fixed to better handle datasets with varying correlation structures?
- Basis in paper: Section 4.6 notes that model performance "can vary significantly depending on the selected threshold" and fixes $T=0.8$ manually for all datasets.
- Why unresolved: A fixed threshold assumes similar cross-channel correlation densities across all forecasting scenarios, which may limit optimization for datasets with sparse or dense channel relationships.
- What evidence would resolve it: Implementing a learnable or search-based threshold module and comparing its performance and efficiency against the static $T=0.8$ baseline across heterogeneous datasets.

## Limitations

- Channel extraction mechanism's benefits are not fully isolated in ablation studies - it's unclear whether improvements come from the extraction itself or the averaging fusion
- Optimal clustering threshold appears dataset-dependent but the paper uses a fixed T=0.8 without systematic exploration of the efficiency-accuracy Pareto frontier
- Architecture details for channel extraction modules (layer counts, attention heads, hidden dimensions) are underspecified, creating reproducibility challenges

## Confidence

- High confidence in practical effectiveness: Averaging-based fusion shows consistent 3.47% MSE improvement across datasets
- Medium confidence in theoretical justification: Limited exploration of why averaging specifically outperforms alternatives like gating or weighted averaging
- Medium confidence in universal applicability: Clustering threshold T=0.8 may not be optimal across datasets with different correlation structures

## Next Checks

1. Perform targeted ablation study isolating channel extraction benefits by comparing models with only original series vs. only extracted sequences vs. the full averaging approach
2. Systematically sweep the clustering threshold T across all datasets to map the efficiency-accuracy Pareto frontier
3. Test AverageTime on univariate time series to validate whether the channel extraction mechanism provides benefits beyond multivariate correlation capture