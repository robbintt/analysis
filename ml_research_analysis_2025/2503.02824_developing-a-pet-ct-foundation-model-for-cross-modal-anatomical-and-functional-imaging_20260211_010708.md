---
ver: rpa2
title: Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional
  Imaging
arxiv_id: '2503.02824'
source_url: https://arxiv.org/abs/2503.02824
tags:
- foundation
- fratmae
- masked
- training
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose FratMAE, a foundation model for PET/CT imaging
  that integrates anatomical (CT) and functional (PET) information through separate
  encoders and cross-attention decoders. The model uses coronal scan stacks instead
  of traditional axial stacks to capture whole-body context, and incorporates textual
  metadata via contrastive learning.
---

# Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging

## Quick Facts
- arXiv ID: 2503.02824
- Source URL: https://arxiv.org/abs/2503.02824
- Reference count: 25
- Primary result: Coronal-based FratMAE achieves Dice 0.640 (IoU 0.496) for lesion segmentation vs 0.597 (0.454) for baseline

## Executive Summary
FratMAE is a foundation model for PET/CT imaging that integrates anatomical and functional information through separate encoders and cross-attention decoders. The model uses coronal scan stacks instead of traditional axial stacks to capture whole-body context, and incorporates textual metadata via contrastive learning. Pre-trained on the AutoPET III dataset (1,292 cases), FratMAE achieves superior performance on downstream tasks compared to baselines, particularly under limited training data conditions.

## Method Summary
FratMAE employs separate Vision Transformer encoders for PET and CT scans, along with cross-attention decoders that enable synergistic interactions between modalities during masked autoencoder training. The model uses coronal scan stacks instead of traditional axial stacks to capture whole-body context, and incorporates contrastive learning to align PET representations with textual metadata including radiotracer type and demographic information.

## Key Results
- Lesion segmentation: FratMAE achieves Dice 0.640 (IoU 0.496) vs baseline 0.597 (0.454)
- Ann Arbor staging: Accuracy 0.654 vs baseline 0.603
- Particularly strong performance under limited training data (20% of dataset)
- Coronal approach shows significant advantage over axial (Dice 0.795 vs 0.559 without pre-training)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-attention decoders enable the model to learn complementary anatomical-functional relationships by reconstructing one modality using information from the other.
- **Mechanism:** During pre-training, when PET tokens are masked (50%), CT remains fully unmasked. The masked modality's encoded representation serves as the Query, while the unmasked modality serves as Key/Value in cross-attention. This forces the model to learn where metabolic uptake should occur given anatomical structures.
- **Core assumption:** PET uptake patterns have predictable spatial relationships with CT-visible anatomical structures; these correspondences are learnable through reconstruction.
- **Evidence anchors:**
  - [abstract]: "FratMAE employs separate Vision Transformer (ViT) encoders for PET and CT scans, along with cross-attention decoders that enable synergistic interactions between modalities during masked autoencoder training"
  - [Section 2.1]: "Within each decoder block, the encoded representation of the masked modality ZS serves as the query, while the encoded representation of the unmasked modality Z¯S serves as both the key and value"
  - [corpus]: "Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT" similarly uses explicit cross-modal interaction for PET/CT, supporting the premise that modality-aware fusion improves performance
- **Break condition:** If PET-CT pairs are significantly misregistered, or if pathological uptake bears no relationship to visible anatomy, cross-attention provides no useful reconstruction signal.

### Mechanism 2
- **Claim:** Coronal scan stacks capture whole-body metabolic context that axial patches inherently lose through anatomical truncation.
- **Mechanism:** Axial patches fragment the body into local regions, severing long-range spatial dependencies needed to understand disease distribution (critical for staging). Coronal stacks maintain head-to-pelvis continuity in fewer slices, preserving global uptake patterns.
- **Core assumption:** Whole-body context is necessary for accurate lesion characterization and staging; truncating along the axial dimension discards predictive signal about tracer distribution patterns.
- **Evidence anchors:**
  - [abstract]: "coronal scan stacks instead of traditional axial stacks to capture whole-body context, enabling better capture of global uptake patterns"
  - [Section 3.1, Table 2]: Coronal approach achieves Dice 0.795 (95% CI: 0.770-0.817) vs 0.559 (0.492-0.623) for axial without pre-training
  - [corpus]: Limited direct corpus comparison for coronal vs axial in foundation models; appears novel contribution
- **Break condition:** If high-resolution small lesion detection is critical, coronal subsampling (k=2) may lose fine-grained axial detail needed for localization.

### Mechanism 3
- **Claim:** Contrastive alignment between PET representations and clinical metadata (radiotracer type, diagnosis, demographics) encodes prior knowledge about expected uptake patterns.
- **Mechanism:** InfoNCE loss pulls PET CLS tokens toward their associated text embeddings while pushing apart mismatched pairs. This teaches the model that different tracers (FDG vs PSMA) produce different uptake distributions.
- **Core assumption:** Textual metadata contains consistent predictive signal about PET appearance; this signal can be captured through contrastive learning.
- **Evidence anchors:**
  - [Section 2.2]: "contrastive learning to align each PET image representation with its corresponding textual metadata, which includes details such as radiotracer type and basic demographic information"
  - [Section 3.2, Table 3]: FratMAE with ContextAlign achieves Dice 0.640 vs 0.619 without ContextAlign (20% trainset), suggesting metadata provides incremental value
  - [corpus]: "Meta-information Guided Cross-domain Synergistic Diffusion Model" supports that meta-information improves PET reconstruction quality
- **Break condition:** If metadata is noisy, inconsistently formatted, or non-predictive of imaging appearance, contrastive alignment may introduce spurious correlations that harm generalization.

## Foundational Learning

- **Masked Autoencoder (MAE) Pre-training:**
  - Why needed: Enables self-supervised learning on unlabeled medical volumes. The model learns useful representations by predicting masked regions from visible context.
  - Quick check question: Can you explain why the encoder only processes visible (unmasked) tokens while the decoder processes all tokens including mask tokens?

- **Cross-Attention in Transformers:**
  - Why needed: Standard self-attention queries within the same sequence. Cross-attention enables one modality's features to query another's—essential for PET-CT fusion where each provides complementary information.
  - Quick check question: In FratMAE's decoder, which modality provides Query vs Key/Value, and why is this asymmetric design chosen?

- **Contrastive Learning (InfoNCE):**
  - Why needed: Aligns representations across modalities (image-text) by pulling positive pairs together and pushing negative pairs apart. Encodes semantic relationships without explicit labels.
  - Quick check question: What constitutes a "positive pair" vs "negative pair" in the ContextAlign module?

## Architecture Onboarding

- **Component map:**
  - PET Encoder (ViT-B) -> CT Encoder (ViT-B) -> Cross-Attention Decoders (×2) -> Text Encoder (CLIP ViT-B) -> ContextAlign

- **Critical path:**
  1. Resize paired PET/CT to 160×160×192
  2. Extract coronal patches (32×160×192, subsampling k=2)
  3. Randomly select modality for 50% masking; other modality stays unmasked
  4. Dual encoder forward pass
  5. Cross-attention decoder reconstruction → MSE loss
  6. Text encoding → InfoNCE alignment
  7. Combined optimization: L_MSE + L_InfoNCE

- **Design tradeoffs:**
  - Separate encoders preserve modality-specific features but double parameters vs shared encoder
  - Coronal processing captures global context but sacrifices axial resolution through subsampling
  - 50% masking ratio (lower than standard MAE's 75%) may preserve more structure for cross-modal learning
  - Pre-training on 1,292 cases is modest for foundation models; out-of-distribution generalization is unproven

- **Failure signatures:**
  - High false positive rate in segmentation: Cross-attention may not be learning valid anatomical-functional correspondences—visualize attention maps
  - Staging accuracy near random: Whole-body context may not be propagating—verify coronal processing pipeline
  - No gain from ContextAlign: Metadata may be too sparse or inconsistent; check text encoding quality
  - Reconstruction loss plateaus early: Masking ratio may need adjustment for 3D multimodal data

- **First 3 experiments:**
  1. Validate cross-attention contribution: Compare asymmetric masking (one modality fully visible) vs symmetric masking (both partially masked) on held-out reconstruction quality.
  2. Confirm coronal benefit: Replicate axial vs coronal comparison (Table 2) on your target dataset before committing to architecture.
  3. Metadata sensitivity analysis: Train ablations with/without ContextAlign across metadata quality levels to establish when text alignment is worth the complexity.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset scale: The 1,292-case pre-training dataset is relatively modest for foundation model claims. Generalization to larger, more diverse populations remains unproven.
- Clinical validation: Results are benchmarked on internal datasets without external validation or multi-institutional testing, limiting claims about real-world robustness.
- Mechanism specificity: While cross-attention and coronal processing show performance gains, ablation studies don't fully isolate whether gains come from architecture, pre-training, or data augmentation effects.
- Metadata dependency: The incremental benefit of ContextAlign (Dice 0.640 vs 0.619) is modest, suggesting metadata may not be critical for core representation learning.

## Confidence
- **High confidence**: Cross-attention decoder architecture effectively learns PET-CT complementarity (supported by strong lesion segmentation gains and ablation evidence).
- **Medium confidence**: Coronal scan stacks provide meaningful whole-body context advantage (supported by internal comparisons but lacks external validation).
- **Low confidence**: Contrastive metadata alignment provides substantial value (minimal performance delta in controlled ablations).

## Next Checks
1. **External validation**: Test FratMAE on held-out data from different institutions/sites to verify cross-institutional generalization of staging accuracy and lesion detection.

2. **Ablation robustness**: Systematically vary masking ratios (25%, 50%, 75%) and metadata quality to establish minimum requirements for performance gains.

3. **Attention interpretability**: Visualize cross-attention weight distributions to confirm the model is learning physiologically meaningful anatomical-functional correspondences rather than spurious correlations.