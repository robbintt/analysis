---
ver: rpa2
title: 'OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for
  High-Resolution Carbon Emission Prediction Using Open Data'
arxiv_id: '2506.03224'
source_url: https://arxiv.org/abs/2506.03224
tags:
- carbon
- data
- emissions
- emission
- satellite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OpenCarbon, a novel model for predicting high-resolution
  urban carbon emissions using open data. The key innovation is combining satellite
  images and POI data through contrastive learning to capture both the spatial layout
  and functional activity information.
---

# OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data

## Quick Facts
- arXiv ID: 2506.03224
- Source URL: https://arxiv.org/abs/2506.03224
- Authors: Jinwei Zeng; Yu Liu; Guozhen Zhang; Jingtao Ding; Yuming Lin; Jian Yuan; Yong Li
- Reference count: 19
- Primary result: Achieves 26.6% improvement in R² over state-of-the-art methods, with average R² of 0.70

## Executive Summary
OpenCarbon introduces a novel approach for predicting high-resolution urban carbon emissions by combining satellite imagery and POI data through contrastive learning. The model captures both spatial layout and functional activity information to predict emissions at a 1km² resolution across urban areas. By leveraging open data sources, the approach reduces data collection burden while maintaining prediction accuracy, enabling more efficient carbon governance across cities. The model demonstrates strong generalizability across different urban contexts and provides interpretable insights into emission patterns.

## Method Summary
OpenCarbon employs a cross-modality information extraction and fusion module with SE attention to model distinct functional effects from satellite images and POI data. A neighborhood-informed aggregation module with cross-attention captures spatial contiguity and agglomeration effects. The model uses contrastive learning to align representations from both modalities, enabling joint feature extraction. Experiments were conducted on three datasets covering London, Beijing, and Yinchuan, evaluating performance across multiple baselines including LASSO, XGBoost, and GraphSAGE variants.

## Key Results
- Achieves 26.6% improvement in R² over state-of-the-art methods
- Average R² score of 0.70 across three cities (London, Beijing, Yinchuan)
- Successfully ranks emissions across regions despite distribution shifts
- POI data becomes more important for predicting emissions in high-emission areas

## Why This Works (Mechanism)
The model works by jointly modeling spatial contiguity and functional agglomeration effects through multi-modal data fusion. Satellite imagery captures the physical spatial layout and land use patterns, while POI data encodes functional activity distributions. Contrastive learning aligns these complementary representations, allowing the model to capture both structural and functional urban characteristics that drive carbon emissions. The SE attention mechanism enables the model to weigh different functional effects appropriately, while the cross-attention in the aggregation module captures neighborhood interactions.

## Foundational Learning

1. **Contrastive Learning**: Why needed - To align representations from satellite imagery and POI data that capture different aspects of urban function. Quick check - Verify that representations from both modalities converge in shared embedding space.

2. **SE Attention (Squeeze-and-Excitation)**: Why needed - To model distinct functional effects by learning channel-wise feature importance. Quick check - Confirm that attention weights correlate with known emission drivers.

3. **Cross-Attention Mechanisms**: Why needed - To capture spatial contiguity and agglomeration effects between neighboring regions. Quick check - Validate that neighboring regions with similar features show similar emission predictions.

4. **Multi-Modal Fusion**: Why needed - To combine complementary information from satellite imagery (spatial layout) and POI data (functional activity). Quick check - Test performance degradation when either modality is removed.

5. **Graph Neural Networks**: Why needed - To model spatial relationships and neighborhood effects in urban areas. Quick check - Verify that predictions respect known spatial autocorrelation patterns.

## Architecture Onboarding

**Component Map**: Satellite Images -> Feature Extraction -> Cross-Modal Fusion -> SE Attention -> Neighborhood Aggregation -> Emission Prediction

**Critical Path**: Image features and POI features → Cross-modal fusion → SE attention weighting → Neighborhood aggregation → Final prediction

**Design Tradeoffs**: The model trades computational complexity for accuracy by processing high-resolution imagery and rich POI data. While this increases prediction accuracy, it requires significant computational resources and may limit scalability in resource-constrained settings.

**Failure Signatures**: Performance degradation may occur when: 1) Satellite imagery quality is poor, 2) POI data is incomplete or outdated, 3) Urban structures differ significantly from training cities, or 4) The model encounters temporal distribution shifts.

**First Experiments**:
1. Test model performance with satellite imagery only vs POI data only to quantify modality contributions
2. Evaluate cross-city transferability by training on one city and testing on another
3. Conduct temporal validation using historical data to assess model stability over time

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on open data sources with varying quality and completeness across cities
- Computational resource requirements for processing high-resolution imagery and POI data
- Potential oversimplification of complex urban dynamics by focusing primarily on functional zones
- Limited testing to only three cities, raising questions about broader generalizability

## Confidence

| Claim | Confidence |
|-------|------------|
| Model performance improvements (26.6% R2 increase) | High |
| Cross-modality effectiveness | Medium |
| Generalizability claims | Medium |
| Interpretability insights | Low |

## Next Checks

1. Conduct cross-city validation using cities with significantly different urban structures and data availability to test true generalizability
2. Perform ablation studies isolating the contributions of satellite imagery versus POI data to quantify their relative importance
3. Test model performance on temporally distinct datasets to evaluate stability over time and ability to handle urban evolution