---
ver: rpa2
title: 'Unlearning in LLMs: Methods, Evaluation, and Open Challenges'
arxiv_id: '2601.13264'
source_url: https://arxiv.org/abs/2601.13264
tags:
- unlearning
- knowledge
- data
- methods
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews unlearning methods for large language models,
  categorizing them into data-centric, parameter-centric, architecture-centric, hybrid,
  and other approaches. It evaluates the effectiveness of these methods using benchmarks
  like TOFU, focusing on the trade-off between forgetting targeted knowledge and preserving
  overall model utility.
---

# Unlearning in LLMs: Methods, Evaluation, and Open Challenges

## Quick Facts
- **arXiv ID:** 2601.13264
- **Source URL:** https://arxiv.org/abs/2601.13264
- **Reference count:** 7
- **Primary result:** This survey reviews unlearning methods for large language models, categorizing them into data-centric, parameter-centric, architecture-centric, hybrid, and other approaches, and identifies key challenges in providing formal guarantees, efficiency, and robustness.

## Executive Summary
This survey provides a comprehensive overview of unlearning methods for large language models (LLMs), categorizing approaches based on how they target knowledge removal. The paper evaluates these methods using benchmarks like TOFU, focusing on the critical trade-off between successfully forgetting targeted knowledge and preserving overall model utility. It identifies fundamental challenges including the lack of formal guarantees for forgetting, scalability issues, and the need for unlearning methods that work across languages and modalities. The survey highlights the gap between current empirical approaches and the stronger guarantees, efficiency, and robustness needed for trustworthy unlearning in real-world LLM deployments.

## Method Summary
The paper surveys various unlearning methods by categorizing them into five main approaches: data-centric (e.g., Gradient Ascent, NPO), parameter-centric (e.g., Task Vectors, LoRA, Subspace Projection like UNLEARN), architecture-centric (e.g., Contrastive Decoding, Auxiliary Models, RAG), hybrid approaches combining multiple strategies, and other methods. Evaluation is conducted using the TOFU benchmark, which measures forget quality through statistical p-value tests comparing distributions between unlearned and original models, and model utility through normalized harmonic mean of retain set performance. The survey synthesizes results across different forget-set sizes (1%, 5%, 10%) and provides a framework for understanding the trade-offs between forgetting effectiveness and utility preservation.

## Key Results
- Unlearning methods fall into five categories: data-centric, parameter-centric, architecture-centric, hybrid, and other approaches
- TOFU benchmark shows significant trade-offs between forget quality and model utility across all methods
- No current method provides formal guarantees of forgetting while maintaining high utility
- Cross-language and multimodal unlearning remain largely unexplored with limited benchmarks available

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent (Optimization Reversal)
Reversing the standard training objective may degrade the model's ability to reproduce specific data by maximizing the prediction error on the forget set. Standard training minimizes negative log-likelihood; gradient ascent flips this sign to maximize loss, theoretically pushing weights away from representations learned for the forget set. Core assumption: the loss landscape geometry allows parameters to move away from the "forget" region without destroying "retain" regions. Evidence shows traditional GA approaches fail to produce strong forgetting and fall below statistical significance thresholds in TOFU evaluations. Break condition: optimization instability leads to catastrophic collapse or collateral degradation where the model loses general fluency.

### Mechanism 2: Negative Preference Optimization (NPO)
Reframing unlearning as a preference alignment problem—where the model prefers generic or alternative responses over target data—achieves more stable forgetting than direct gradient manipulation. NPO treats forget set data as "dispreferred" outputs, optimizing a preference objective that discourages specific token sequences. Core assumption: the model can be guided to assign lower probability to specific outputs without needing exact influence computation. Evidence shows NPO attains statistically significant forgetting across forget-set sizes, unlike simpler gradient ascent. Break condition: as forget set size increases, NPO exhibits sharp deterioration in utility, indicating struggles to distinguish targeted knowledge from general capabilities.

### Mechanism 3: Subspace Projection (Parameter Isolation)
Knowledge may be removed with high fidelity by identifying the specific low-dimensional subspace of weights responsible for a task and surgically altering it. This approach (e.g., UNLEARN) isolates task-specific subspaces using singular value decomposition and orthogonality constraints, projecting weights to remove only unique components of the forget set. Core assumption: knowledge is localized in a linear or low-rank subspace mathematically separable from other capabilities. Evidence shows UNLEARN preserving normalized utility near 1.0 while maintaining significant forget quality. Break condition: if forget and retain tasks share deep, non-linear dependencies, the orthogonality assumption fails, leading to inadvertent degradation of retained tasks.

## Foundational Learning

- **Concept: Gradient Descent & Loss Landscapes**
  - **Why needed here:** Unlearning methods like Gradient Ascent and NPO operate by manipulating the loss function. Understanding how models minimize loss is required to understand how reversing or constraining that process alters model behavior.
  - **Quick check question:** If you maximize the loss on a specific prompt, does the model output random noise or a specific refusal?

- **Concept: Matrix Decomposition (SVD)**
  - **Why needed here:** Mechanism 3 (Subspace Projection) relies on Singular Value Decomposition to identify principal components in weight space. Without this, the concept of "projecting out" a task is abstract.
  - **Quick check question:** How does projecting a weight matrix onto its principal components differ from simply pruning the smallest weights?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The primary failure mode of unlearning is destroying useful knowledge while trying to remove unwanted knowledge. This is a specific instance of the general catastrophic forgetting problem in continual learning.
  - **Quick check question:** Why is "forgetting" a specific book potentially dangerous to the model's ability to summarize other books?

## Architecture Onboarding

- **Component map:** Data-Centric Layer (handles optimization objectives like GA, NPO) -> Parameter-Centric Layer (operates on weights via Task Vectors, LoRA, Subspace matrices) -> Architecture-Centric Layer (modifies inference flow via Contrastive Decoding, Auxiliary Models, RAG) -> Evaluation Layer (benchmarks measuring Trade-off between Forget Quality and Model Utility)

- **Critical path:**
  1. **Define Scope:** Select the "Forget Set" (e.g., specific author facts) and "Retain Set" (e.g., general world knowledge)
  2. **Select Method:** Choose based on Trade-off. Use NPO for stability on small sets; consider Subspace methods for precision
  3. **Execute Unlearning:** Apply updates (e.g., fine-tune with NPO or compute projection matrices)
  4. **Verify:** Run Forget Quality metrics (p-value tests) and Model Utility checks (MMLU/HELM)

- **Design tradeoffs:**
  - **Speed vs. Guarantees:** Exact unlearning (retraining) is slow but guaranteed; approximate methods (NPO/Subspace) are fast but lack formal guarantees
  - **Forget vs. Retain:** Aggressive forgetting (high Gradient Ascent) destroys utility; conservative methods leave residual knowledge
  - **Modularity:** Architecture-centric methods (Adapters) are reversible but require storing auxiliary components

- **Failure signatures:**
  - **Catastrophic Collapse:** Model outputs incoherent text or excessively generic refusals (e.g., "I cannot answer" for all queries)
  - **Re-extraction:** Adversarial prompts or multimodal attacks successfully recover "forgotten" data (indicates shallow behavioral unlearning vs. deep representational removal)
  - **Utility Drop:** Performance on HELM/MMLU drops >5%, indicating collateral damage

- **First 3 experiments:**
  1. **Baseline Comparison:** Implement Gradient Ascent vs. NPO on a small LLM (e.g., Llama-7B) using TOFU benchmark with 1% forget set to observe stability differences
  2. **Subspace Isolation:** Attempt UNLEARN method on task with clear boundaries (e.g., removing "Harry Potter" knowledge) to test if subspace projection preserves general reasoning better than NPO
  3. **Adversarial Robustness:** After unlearning specific fact, use prompt engineering or "Stealthy Unlearning Attacks" to see if knowledge is truly gone or just suppressed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we bridge the gap between the mathematical rigor of differential privacy and the scale of modern LLMs to provide verifiable guarantees of forgetting?
- **Basis in paper:** [explicit] The paper states that "Formal guarantees remain elusive in large networks" because while differential privacy offers theoretical protection, "bounds degrade for over-parameterized models the size of LLMs."
- **Why unresolved:** Current evaluation relies on empirical probes which cannot ensure data is unrecoverable, and existing formal tools have not been scaled or validated for billion-parameter models.
- **What evidence would resolve it:** A mathematical framework that certifies forgetting in LLMs without prohibitive utility loss or computational costs.

### Open Question 2
- **Question:** How can unlearning be effectively implemented and evaluated across diverse languages and modalities to prevent knowledge re-emergence in non-target domains?
- **Basis in paper:** [explicit] The authors list "cross-language and multimodal unlearning" as an open challenge, noting that "unlearning on information one language generally does not lead to unlearning on other languages."
- **Why unresolved:** Current multilingual benchmarks are limited in diversity, and multimodal settings lack standardized metrics to measure consistency or unintended degradation across modalities.
- **What evidence would resolve it:** Development of broad benchmarks demonstrating consistent forgetting transfer across typologically distinct languages and modalities without collateral damage.

### Open Question 3
- **Question:** How can unlearning methods be hardened against adversarial relearning attacks that exploit residual representations to recover forgotten information?
- **Basis in paper:** [explicit] The paper identifies "robustness against adversarial relearning" as a key challenge, highlighting attacks like "Stealthy Unlearning Attack" that "sidestep" the forgetting process.
- **Why unresolved:** Many current methods leave residual traces or parameter noise that can be exploited by fine-tuning or perturbation attacks, making "forgotten" knowledge recoverable.
- **What evidence would resolve it:** Defense mechanisms that prove robust against standardized adversarial extraction attacks in benchmarks like UnLOK-VQA or SAFEERASER.

### Open Question 4
- **Question:** How can the boundaries between forget and retain sets be precisely defined when target knowledge overlaps significantly with benign utility?
- **Basis in paper:** [inferred] The paper notes that current benchmarks often use disjoint sets, creating "overly optimistic" results, whereas real-world data involves "significant overlap" (e.g., dual-use medical knowledge).
- **Why unresolved:** Formal tools for defining precise knowledge boundaries in over-parameterized models are lacking, leading to either incomplete forgetting or catastrophic degradation of adjacent capabilities.
- **What evidence would resolve it:** Metrics that quantify performance on overlapping queries (like in the BLUR benchmark) where forget-related terms are embedded within retain questions.

## Limitations
- The survey lacks transparency on exact model architectures, hyperparameters, and statistical test details used in TOFU evaluations
- Cross-language and multimodal unlearning feasibility claims are largely theoretical without empirical validation
- No current method provides formal guarantees of forgetting, only probabilistic approximations

## Confidence
- **High confidence:** Basic categorization of methods and identification of key challenges align with multiple corroborating papers
- **Medium confidence:** Specific performance comparisons on TOFU lack transparency on implementation details and hyperparameters
- **Low confidence:** Claims about cross-language and multimodal unlearning feasibility due to limited empirical validation and unexplored territory

## Next Checks
1. **Benchmark Replication:** Implement the TOFU evaluation pipeline with transparent hyperparameter reporting to verify the reported trade-offs between forget quality and model utility across different methods.
2. **Adversarial Robustness Testing:** After applying unlearning methods, systematically attempt to extract "forgotten" knowledge using prompt engineering, knowledge probing, and the "Stealthy Unlearning Attacks" mentioned in Section 5.
3. **Cross-Architecture Validation:** Test the same unlearning methods on multiple model families (e.g., Llama, Mistral, Gemma) and scales (1B-70B parameters) to assess whether performance patterns hold across different architectures.