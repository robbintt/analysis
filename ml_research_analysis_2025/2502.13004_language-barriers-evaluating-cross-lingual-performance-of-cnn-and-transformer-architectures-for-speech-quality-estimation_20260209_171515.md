---
ver: rpa2
title: 'Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer
  Architectures for Speech Quality Estimation'
arxiv_id: '2502.13004'
source_url: https://arxiv.org/abs/2502.13004
tags:
- speech
- across
- quality
- languages
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates cross-lingual generalization in speech quality
  models, comparing a CNN-based NISQA model and a Transformer-based Audio Spectrogram
  Transformer (AST). Both models were trained exclusively on English speech data and
  tested across German, French, Mandarin, Swedish, and Dutch.
---

# Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation

## Quick Facts
- **arXiv ID:** 2502.13004
- **Source URL:** https://arxiv.org/abs/2502.13004
- **Reference count:** 1
- **Primary result:** AST achieves more stable cross-lingual speech quality estimation than CNN-based NISQA when trained only on English data.

## Executive Summary
This paper evaluates cross-lingual generalization in speech quality models by comparing a CNN-based NISQA model with a Transformer-based Audio Spectrogram Transformer (AST). Both models were trained exclusively on English speech data and tested across German, French, Mandarin, Swedish, and Dutch. Results show AST achieves more stable cross-lingual performance, with Mandarin exhibiting the highest correlations and Swedish/Dutch presenting greater challenges. Discontinuity prediction remains difficult across all languages. The study highlights the need for balanced multilingual datasets and architecture-specific adaptations to improve cross-lingual robustness in speech quality assessment models.

## Method Summary
The study compares two architectures: NISQA (CNN-based) and AST (Transformer-based) for cross-lingual speech quality estimation. Both models were trained only on English data (49,062 samples from 34 databases) and evaluated on five languages (German, French, Mandarin, Swedish, Dutch) using 70,000+ test samples. Input is 48kHz audio converted to log-Mel spectrograms (128 bins, 25ms window, 10ms hop). NISQA uses CNN/Mel-Spectrogram with ADAM optimizer (lr=0.001), early stopping at 20 epochs. AST uses 16×16 overlapping patches with 768-dim projection, 5 parallel linear heads for multitask prediction (MOS, coloration, discontinuity, loudness, noise), and MSE loss with ADAM (lr=1e-6). A third-order polynomial calibration per ITU-T P.1401 is applied to predictions.

## Key Results
- AST demonstrates more stable cross-lingual performance with reduced PCC variability across languages compared to NISQA
- Mandarin shows highest correlations for both models, while Swedish and Dutch present greater challenges
- Discontinuity prediction remains consistently difficult across all languages and architectures
- AST's self-attention mechanism enables better capture of long-range dependencies that transfer across languages

## Why This Works (Mechanism)

### Mechanism 1: Self-Attention for Long-Range Temporal Dependency Capture
- **Claim:** Transformer-based architectures provide more stable cross-lingual generalization than CNN-based approaches for speech quality estimation.
- **Mechanism:** The self-attention mechanism in AST captures long-range dependencies in speech signals that CNNs with localized receptive fields struggle to model. This enables better representation of complex degradations that unfold over time (e.g., packet loss patterns, intermittent noise), which transfer more readily across languages than segment-level features.
- **Core assumption:** Speech quality degradations that persist across temporal contexts are critical for accurate MOS prediction, and these temporal patterns share structure across languages.
- **Evidence anchors:** [abstract] "AST achieves a more stable cross-lingual performance"; [section 1] "Transformer-based models...leverage self-attention to model these dependencies more effectively"; [section 5.2] "AST model demonstrates a more stable performance across languages, with reduced PCC variability in most dimensions compared to NISQA"
- **Break condition:** When degradation patterns are highly localized in time and don't require temporal context, or when training data is insufficient for self-attention layers to learn meaningful long-range patterns.

### Mechanism 2: Cross-Modal Transfer Learning via Vision Transformer Initialization
- **Claim:** Pre-trained Vision Transformer (ViT) weights provide useful initialization for audio spectrogram processing, even when source domain is images.
- **Mechanism:** AST initializes weights from ViT trained on ImageNet, treating log-Mel spectrograms as 2D "images." The model then fine-tunes on AudioSet (2M+ audio clips). Attention patterns learned for spatial relationships in images transfer to time-frequency relationships in spectrograms.
- **Core assumption:** Spectrogram representations share structural properties with visual images—local patterns, edges, textures—that can be exploited by attention mechanisms regardless of modality.
- **Evidence anchors:** [section 3.2] "pre-trained version of the AST model leverages cross-modality transfer learning by initializing its weights using pre-trained weights from a Vision Transformer"; [section 3.2] "AST was further fine-tuned for audio classification on the AudioSet dataset"
- **Break condition:** When target audio task requires fundamentally different feature hierarchies than visual tasks, or when spectrogram characteristics (e.g., very narrow frequency bands) don't align with learned visual attention patterns.

### Mechanism 3: Multitask Learning with Shared Backbone and Specialized Heads
- **Claim:** A shared transformer backbone with task-specific linear heads enables learning generalized audio representations while preserving dimension-specific specialization.
- **Mechanism:** The AST backbone output feeds into five separate linear layers, each predicting one quality dimension (MOS, coloration, discontinuity, loudness, noise). Separate loss computation per task ensures each head specializes, while shared backbone updates create unified representations.
- **Core assumption:** Perceptual speech quality dimensions share underlying acoustic features that benefit from joint optimization, and gradients from multiple tasks provide complementary learning signals.
- **Evidence anchors:** [section 3.2] "downstream multitasking by feeding the output of the AST model into five separate linear layers, each dedicated to predicting a specific aspect of speech quality"; [section 3.2] "AST backbone remains fully shared and is updated across all tasks, allowing it to learn a more generalized audio representation"
- **Break condition:** When quality dimensions have conflicting optimization gradients (one dimension's improvement degrades another), or when a dominant task prevents others from learning (imbalance in loss scales).

## Foundational Learning

- **Concept: Log-Mel Spectrogram Representation**
  - **Why needed here:** Both NISQA and AST use log-Mel spectrograms as input. Understanding how audio converts to this 2D time-frequency representation is essential for debugging input pipelines and interpreting model behavior.
  - **Quick check question:** Why are Mel-scale frequency bins used instead of linear frequency bins for speech quality tasks?

- **Concept: Mean Opinion Score (MOS) as Ground Truth**
  - **Why needed here:** MOS is the training target, but it's a noisy subjective measure with language-dependent biases. Understanding its limitations prevents over-interpreting model performance.
  - **Quick check question:** What are two reasons why MOS scores might differ between languages for identical acoustic degradations?

- **Concept: Pearson Correlation vs RMSE as Evaluation Metrics**
  - **Why needed here:** PCC measures ranking correctness (is sample A better than B?), while RMSE measures absolute error. A model can have high PCC but high RMSE if predictions are correctly ordered but systematically shifted.
  - **Quick check question:** If a model achieves PCC=0.85 but RMSE=0.60 for a language, what does this tell you about the nature of prediction errors?

## Architecture Onboarding

- **Component map:**
  Audio (48kHz) → Log-Mel Spectrogram (128 bins, 25ms window, 10ms hop) → Patch Embedding (16×16 overlapping patches → 768-dim vectors) → + Positional Embedding (learnable, 768-dim) → Transformer Encoder (12 layers, self-attention) → CLS Token Extraction → 5 Parallel Linear Heads → MOS, Coloration, Discontinuity, Loudness, Noise → Polynomial Calibration (3rd-order) → Final Predictions

- **Critical path:**
  1. Verify spectrogram extraction matches expected shape (time × 128 Mel bins)
  2. Confirm patch embedding produces correct sequence length with CLS token prepended
  3. Validate attention mask handling for variable-length audio (<12s clips)
  4. Check that multitask loss aggregates correctly across 5 heads

- **Design tradeoffs:**
  - **Pre-trained vs from-scratch:** Pre-trained requires less task-specific data but may carry visual domain biases; from-scratch needs more data but learns audio-native features
  - **48kHz vs 16kHz input:** Higher sample rate captures more high-frequency detail but increases compute by ~3×
  - **CNN (NISQA) vs Transformer (AST):** CNNs have faster inference and smaller memory footprint; Transformers capture long-range dependencies better but require more data

- **Failure signatures:**
  - **Discontinuity PCC near 0.35-0.40:** Both architectures fail here—this dimension is fundamentally difficult, not architecture-dependent
  - **Large PCC range across languages for single dimension (>0.20):** Indicates language-specific bias not addressed by current architecture
  - **Mandarin succeeds, Swedish/Dutch fail:** Suggests tonal languages may have more consistent spectral structure, or dataset quality issues in low-resource languages

- **First 3 experiments:**
  1. **Reproduce English baseline:** Train both models on English training split, validate PCC/RMSE values match paper (AST MOS PCC ~0.70, NISQA ~0.72) to confirm implementation correctness
  2. **Cross-lingual zero-shot evaluation:** Without any retraining, run both models on all 5 test languages to reproduce the Mandarin > French > German > Swedish > Dutch performance ordering
  3. **Discontinuity head isolation:** Train AST with only discontinuity loss (single-task) to determine if multitask interference causes poor discontinuity performance, or if the dimension is inherently difficult

## Open Questions the Paper Calls Out

- **Question:** How do CNN and Transformer architectures differently process linguistic features and degradation artifacts, particularly regarding the "discontinuity" dimension?
  - **Basis in paper:** [explicit] The discussion notes that architectures "process linguistic features and degradations in distinct ways" and that discontinuity trends were opposite for Mandarin between the two models (high PCC for NISQA, low for AST).
  - **Why unresolved:** The study identified the divergence but did not perform the feature attribution or layer-wise analysis necessary to explain the mechanism behind the architectural split in performance.
  - **What evidence would resolve it:** Ablation studies or attention-map visualizations highlighting how each architecture weights temporal features versus spectral features when predicting discontinuity.

- **Question:** To what extent does the reliance on objective scores (e.g., POLQA) for validation skew the ground truth for discontinuity prediction?
  - **Basis in paper:** [explicit] Page 5 states the skew in discontinuity suggests "inconsistencies in objective degradation modeling" and explicitly calls for future work to "prioritize subjective MOS labels over objective scores" to reduce error propagation.
  - **Why unresolved:** The current study relied on objective scores for validation dimensions where subjective ratings were missing, potentially conflating model error with labeler (algorithm) error.
  - **What evidence would resolve it:** A comparative evaluation showing model performance deltas when trained/validated on purely subjective human labels versus objective algorithmic labels for the discontinuity dimension.

- **Question:** Is the lower prediction performance for Swedish and Dutch caused primarily by dataset imbalances or intrinsic linguistic traits?
  - **Basis in paper:** [inferred] While the paper highlights Swedish and Dutch as challenges, it also notes the datasets suffer from "inconsistencies in degradation conditions" and varying sample sizes (e.g., Dutch had the smallest subset), leaving the root cause ambiguous.
  - **Why unresolved:** Without a standardized dataset where the only variable is language, it is impossible to disentangle whether the drop in correlation is due to the language itself or the quality/composition of the test set.
  - **What evidence would resolve it:** Evaluating the models on a controlled multilingual dataset with identical degradation pipelines and balanced sample sizes across all tested languages.

## Limitations
- Relies on ITU-T P.SAMD initiative dataset which may not be publicly accessible, limiting reproducibility
- Discontinuity dimension shows consistently poor performance across all languages and architectures, suggesting fundamental modeling challenges
- Mandarin success pattern could reflect tonal language characteristics or superior dataset quality rather than architectural advantages

## Confidence
- **High Confidence:** AST achieving more stable cross-lingual performance than NISQA (PCC variability reduction observed across multiple languages and dimensions)
- **Medium Confidence:** Self-attention mechanism's role in capturing long-range dependencies (mechanism proposed but not directly validated through ablation)
- **Medium Confidence:** Cross-modal transfer learning effectiveness (initialization strategy described but no comparison to from-scratch training)
- **Low Confidence:** Discontinuity prediction difficulty attributed to architectural limitations (could be dataset quality or annotation noise)

## Next Checks
1. **Ablation Study:** Train AST without pre-training (from-scratch initialization) to quantify cross-modal transfer contribution to cross-lingual performance gains.
2. **Language Subset Analysis:** Compare AST performance on Mandarin tonal vs non-tonal speech segments to determine if success stems from tonal structure or other factors.
3. **Discontinuity Augmentation:** Introduce synthetic discontinuity patterns during training to test whether discontinuity performance improves with targeted data augmentation, distinguishing architectural from data limitations.