---
ver: rpa2
title: A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making
arxiv_id: '2509.14998'
source_url: https://arxiv.org/abs/2509.14998
tags:
- expert
- medical
- experts
- kamac
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KAMAC introduces a knowledge-driven adaptive multi-agent collaboration
  framework that dynamically expands expert teams during discussions to address knowledge
  gaps. Unlike static multi-agent methods, it starts with one or more agents and iteratively
  recruits specialists based on evolving diagnostic context.
---

# A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making

## Quick Facts
- arXiv ID: 2509.14998
- Source URL: https://arxiv.org/abs/2509.14998
- Reference count: 24
- Introduces KAMAC, a knowledge-driven adaptive multi-agent collaboration framework for medical decision-making that dynamically expands expert teams during discussions to address knowledge gaps.

## Executive Summary
KAMAC is a framework that improves medical decision-making by enabling adaptive collaboration among LLM-based expert agents. Unlike static multi-agent systems, KAMAC begins with one or more agents and iteratively recruits additional specialists based on detected knowledge gaps during discussion. This dynamic approach mirrors real-world clinical workflows, allowing the system to address evolving diagnostic contexts efficiently. Experimental results on MedQA and Progn-VQA datasets demonstrate that KAMAC outperforms both single-agent and static multi-agent baselines in accuracy, precision, and recall, particularly in complex cancer prognosis scenarios, while using 53-56% fewer experts than prior methods.

## Method Summary
KAMAC employs a knowledge-driven adaptive multi-agent collaboration framework that dynamically expands a team of LLM-based expert agents based on identified knowledge gaps during discussion. The system uses GPT-4.1-mini or DeepSeek-R1 with temperature set to 0 for deterministic outputs. It begins with a single agent and iteratively runs "Knowledge-driven Collaborative Discussion" for up to three rounds. The framework relies on seven specific prompts (P1-P7) for recruitment, assessment, interaction, knowledge gap detection, and final decision-making using majority voting. The method was evaluated on MedQA (1,273 USMLE-style questions) and Progn-VQA (750 head/neck cancer cases with CT slices and clinical text).

## Key Results
- Achieves higher accuracy, precision, and recall compared to single-agent and static multi-agent baselines on MedQA and Progn-VQA datasets
- Uses 53-56% fewer experts than prior methods while maintaining superior performance
- Demonstrates significant efficiency gains by starting with a single agent and expanding only as needed, avoiding the noise of overlapping perspectives from larger initial teams

## Why This Works (Mechanism)

### Mechanism 1
Dynamically recruiting specialists based on detected knowledge gaps appears to improve diagnostic accuracy over static team configurations. The system initiates with a sparse set of agents and during discussion, if the current agents determine via prompt P4 that their collective expertise is insufficient, they trigger the recruitment of additional specialists. This iterative expansion continues until the knowledge gap is resolved. The core assumption is that LLMs can reliably identify the limits of their own knowledge and select the correct specialist label to bridge that gap. Break condition: If Knowledge Gap detection yields false negatives or hallucinates irrelevant specialties, the adaptive advantage collapses.

### Mechanism 2
Conditioning newly recruited experts on the full preceding conversation history allows for immediate, context-aware contribution rather than isolated analysis. When a new agent is recruited, it receives the current discussion history as few-shot input, creating a cumulative reasoning process where new agents address specific points raised in previous rounds. The core assumption is that the LLM's context window is sufficient to hold the growing dialogue history without degrading reasoning quality. Break condition: If conversation history exceeds the model's effective context length or introduces conflicting noise, new agents may hallucinate or lose focus.

### Mechanism 3
Starting with a minimal team (e.g., one agent) and expanding only as needed optimizes computational efficiency while maintaining performance. By deferring expert recruitment until a specific gap is identified, the system avoids the latency and cost of running inference on fixed, large teams for simple cases. The core assumption is that a single agent is capable of resolving simple cases correctly without the overhead of multi-agent debate. Break condition: If the initial agent is too narrow or biased, it might require multiple rounds of expensive re-recruitment to correct early misunderstandings, negating efficiency gains.

## Foundational Learning

- **Concept: Knowledge Gap Detection (Self-Reflection)**
  - Why needed here: The core engine of KAMAC is not the medical reasoning itself, but the meta-reasoning ability to know what is missing. Without this, the system remains static.
  - Quick check question: Can you design a prompt that forces an LLM to output "Yes/No" regarding the need for a specific specialist, rather than just answering the medical question directly?

- **Concept: Role-Specific System Prompts**
  - Why needed here: The framework relies on agents staying "in character" (e.g., a Radiologist strictly interpreting imaging). The mechanism fails if agents drift outside their expertise.
  - Quick check question: How does the system prevent a "Pathologist" agent from offering treatment advice, which is outside their defined scope in the prompt (P2)?

- **Concept: Consensus vs. Majority Voting**
  - Why needed here: The final decision relies on aggregating potentially conflicting views. Understanding why simple voting outperforms complex weighted aggregation is key to the system's stability.
  - Quick check question: Why does the paper suggest Majority Voting is more robust than Ensemble Refinement for this specific architecture? (Hint: noise sensitivity).

## Architecture Onboarding

- **Component map:** Orchestrator -> Agent Pool -> Prompts (P1-P7) -> Feedback Buffer
- **Critical path:** 1. Initialization: Recruit 1 agent -> Assess (P2). 2. Loop: Agents discuss (P3) -> Check for Knowledge Gaps (P4) -> If Gap: Recruit new agent (P5) -> New Agent Assesses with history (P2) -> Merge into Buffer. If No Gap: Check for Consensus. 3. Exit: Max Rounds reached OR Consensus=True. 4. Finalize: Moderator runs P7 (Voting).
- **Design tradeoffs:** Initial Agent Count: Table 3 suggests 1 initial agent yields the best accuracy/efficiency balance. Starting with 5 adds noise ("overlapping perspectives"). Temperature: Set to 0 for deterministic outputs. Consensus Strategy: The paper explicitly rejects "Ensemble Refinement" for "Majority Voting" because the latter mitigates individual biases/noise better (Table 4).
- **Failure signatures:** Infinite Recruitment: Agents keep detecting "knowledge gaps" and recruiting endless new roles (loops without consensus). Echo Chambers: New recruits simply agree with the existing buffer, failing to provide the new perspective they were recruited for. Context Amnesia: New agents ignore the provided history buffer and generate generic, unrelated advice.
- **First 3 experiments:** 1. Baseline Validation: Run KAMAC on MedQA subset with fixed experts (disable dynamic recruitment) vs. the dynamic mode to verify the contribution of the adaptive loop. 2. Gap Detection Ablation: Manually inspect the output of Prompt P4. Is the model identifying genuine gaps, or is it hallucinating the need for help on simple cases? 3. Stability Test: Run the same complex case 5 times (even with Temp=0, check API stability) to see if the recruitment path (which agents are hired) remains consistent.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's core strength—dynamic expert recruitment—depends heavily on the reliability of Knowledge Gap detection, which may be inconsistent across different medical domains or case complexities
- Efficiency gains (53-56% fewer experts) may be dataset-specific to MedQA and Progn-VQA, requiring validation for generalization to other medical domains
- The robustness of Knowledge Gap detection in preventing false positives (unnecessary recruitment) or false negatives (missing critical gaps) is not fully explored in the paper

## Confidence

**High Confidence:** The iterative recruitment mechanism and its superiority over static baselines (Tables 3-4) are well-supported by empirical results.

**Medium Confidence:** The efficiency claims and the assertion that starting with one agent is optimal are supported, but may depend on task complexity and dataset characteristics.

**Low Confidence:** The robustness of Knowledge Gap detection in preventing false positives or false negatives is not fully explored in the paper.

## Next Checks

1. **Knowledge Gap Detection Ablation:** Manually inspect a sample of Prompt P4 outputs on a held-out MedQA subset to verify if the LLM is correctly identifying genuine knowledge gaps versus hallucinating the need for help on simple cases.

2. **Cross-Dataset Generalization:** Test KAMAC on a different medical benchmark (e.g., MedMCQA or a radiology report dataset) to assess if the efficiency gains and accuracy improvements hold beyond the specific datasets used.

3. **Extreme Case Stress Test:** Design a set of intentionally complex or ambiguous medical cases to push the recruitment mechanism to its limits (e.g., requiring 3 rounds of recruitment) and analyze if the system maintains accuracy without escalating costs.