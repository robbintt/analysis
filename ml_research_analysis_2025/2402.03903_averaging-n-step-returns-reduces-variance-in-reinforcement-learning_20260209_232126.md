---
ver: rpa2
title: Averaging $n$-step Returns Reduces Variance in Reinforcement Learning
arxiv_id: '2402.03903'
source_url: https://arxiv.org/abs/2402.03903
tags:
- return
- returns
- variance
- learning
- compound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that averaging n-step returns into compound returns
  reduces variance while preserving contraction modulus. The authors prove that any
  compound return with the same contraction modulus as a given n-step return has strictly
  lower variance, leading to improved finite-sample complexity in TD learning.
---

# Averaging $n$-step Returns Reduces Variance in Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2402.03903
- **Source URL**: https://arxiv.org/abs/2402.03903
- **Authors**: Brett Daley; Martha White; Marlos C. Machado
- **Reference count**: 40
- **Primary result**: Averaging n-step returns into compound returns reduces variance while preserving contraction modulus

## Executive Summary
This paper establishes that averaging n-step returns into compound returns reduces variance while maintaining the same contraction modulus as the original n-step return. The authors prove that any compound return with the same contraction modulus as a given n-step return has strictly lower variance, leading to improved finite-sample complexity in TD learning. To make this practical for deep RL, they introduce Piecewise λ-Returns (PiLaR), which efficiently approximate λ-returns by averaging just two n-step returns. Experiments demonstrate sample efficiency improvements across both discrete (MinAtar) and continuous (MuJoCo) control tasks.

## Method Summary
The paper introduces Piecewise λ-Returns (PiLaR) as a practical approximation to λ-returns that maintains variance reduction benefits while being computationally efficient. PiLaR averages only two n-step returns to approximate the full λ-return, making it suitable for deep RL applications where computing exact λ-returns is prohibitively expensive. The method is evaluated with DQN on MinAtar games and PPO on MuJoCo environments, showing statistically significant gains in sample efficiency compared to standard n-step returns.

## Key Results
- Averaging n-step returns into compound returns reduces variance while preserving contraction modulus
- PiLaR achieves statistically significant gains in 4/5 MinAtar games when used with DQN
- λ-returns outperform n-step returns in 7/9 MuJoCo tasks when used with PPO
- The theoretical variance reduction property translates to practical sample efficiency improvements in deep RL

## Why This Works (Mechanism)
The mechanism relies on the mathematical property that averaging returns with the same contraction modulus reduces variance without affecting the contraction rate. By constructing compound returns that maintain the desired contraction properties while averaging multiple n-step returns, the method achieves lower variance in value function estimates. This variance reduction directly translates to improved finite-sample complexity in temporal difference learning algorithms.

## Foundational Learning

**Temporal Difference Learning**: The fundamental framework for model-free reinforcement learning that updates value estimates based on bootstrapped targets. *Why needed*: Forms the basis for understanding how variance affects learning stability and sample efficiency. *Quick check*: Verify understanding of TD(0) update rule and its variance properties.

**n-step Returns**: Multi-step returns that balance bias and variance in TD learning by looking ahead n steps before bootstrapping. *Why needed*: The paper builds on n-step returns as the foundation for constructing compound returns. *Quick check*: Understand the bias-variance tradeoff in different n-step returns.

**Contraction Modulus**: The rate at which temporal difference operators contract in value space, determining convergence properties. *Why needed*: Central to the theoretical analysis showing that variance reduction preserves contraction properties. *Quick check*: Verify understanding of Banach fixed-point theorem application to TD learning.

## Architecture Onboarding

**Component Map**: PiLaR algorithm -> n-step return computation -> averaging function -> value update -> policy improvement

**Critical Path**: For each update step: compute two n-step returns → apply PiLaR averaging → generate target → update value function

**Design Tradeoffs**: PiLaR trades exactness of λ-returns for computational efficiency by using only two n-step returns instead of the full geometric mixture, enabling practical application in deep RL while maintaining most variance reduction benefits.

**Failure Signatures**: If PiLaR provides no benefit over n-step returns, this may indicate: (1) insufficient variance in the baseline method, (2) approximation error dominating gains, or (3) task characteristics where variance reduction is less important than other factors.

**First Experiments**:
1. Implement PiLaR with varying λ values on a simple gridworld to verify variance reduction empirically
2. Compare PiLaR against exact λ-returns on a small-scale problem to measure approximation error
3. Test PiLaR with different n values (2, 3, 5) to understand sensitivity to the choice of component returns

## Open Questions the Paper Calls Out

None

## Limitations

- Theoretical analysis assumes tabular settings or linear function approximation, but deep neural networks introduce approximation bias not captured by current analysis
- The variance reduction guarantee is proven only for exact compound returns, while PiLaR approximates λ-returns using piecewise combinations
- Experimental evaluation covers limited environment diversity and does not systematically explore hyperparameter impacts

## Confidence

- **High confidence**: Theoretical variance reduction property for exact compound returns in tabular settings
- **Medium confidence**: PiLaR approximation maintaining similar benefits under function approximation
- **Medium confidence**: Empirical sample efficiency improvements given limited environment diversity

## Next Checks

1. Evaluate PiLaR on more diverse and challenging continuous control benchmarks beyond MuJoCo, including tasks with delayed rewards and stochastic dynamics
2. Systematically measure the approximation error of PiLaR versus exact λ-returns across different environment types and λ values
3. Test the interaction between PiLaR and different function approximation architectures, including recurrent networks and transformers, to assess robustness to approximation bias