---
ver: rpa2
title: Informative Post-Hoc Explanations Only Exist for Simple Functions
arxiv_id: '2508.11441'
source_url: https://arxiv.org/abs/2508.11441
tags:
- explanations
- function
- explanation
- functions
- informative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies when local post-hoc explanations are informative
  about complex decision functions. It defines an explanation as informative if it
  reduces the Rademacher complexity of the set of plausible functions consistent with
  both the prediction and the explanation.
---

# Informative Post-Hoc Explanations Only Exist for Simple Functions

## Quick Facts
- arXiv ID: 2508.11441
- Source URL: https://arxiv.org/abs/2508.11441
- Reference count: 40
- The paper shows that standard local post-hoc explanations (gradients, SHAP, anchors, counterfactuals) are not informative for complex function classes like deep neural networks or unbounded decision trees.

## Executive Summary
This paper establishes theoretical foundations for when local post-hoc explanations are truly informative about complex decision functions. The authors define an explanation as informative if it reduces the Rademacher complexity of the set of functions consistent with both the prediction and the explanation. Their main finding is that popular explanation methods fail to be informative on complex function spaces (all differentiable functions, deep decision trees), but become informative when the function class is restricted to simpler ones (linear functions, shallow trees, Lipschitz-continuous functions) or when explanations are enriched with stability information.

## Method Summary
The paper uses Rademacher complexity as a formal measure of informativeness, comparing the complexity of function spaces before and after applying an explanation. For each explanation method, they analyze whether the constraint imposed by the explanation strictly reduces the complexity of the plausible function set. The analysis involves constructing worst-case functions that satisfy both the prediction and explanation but behave arbitrarily elsewhere, demonstrating non-informativeness when the function class is rich enough to allow such constructions.

## Key Results
- Gradient explanations, SHAP, anchors, and counterfactuals are not informative on complex function classes (differentiable functions, deep trees, high-degree polynomials)
- These explanations become informative when the function class is restricted to simpler models (linear functions, shallow trees, Lipschitz-continuous functions)
- Enriching explanations with stability guarantees (holding over neighborhoods rather than single points) can make them informative even for complex functions
- The paper provides a formal theoretical foundation explaining why post-hoc explanations often fail to provide meaningful insight about complex models

## Why This Works (Mechanism)

### Mechanism 1: Information as Complexity Reduction
- **Claim:** An explanation is theoretically "informative" only if it reduces the Rademacher complexity of the set of functions consistent with the prediction.
- **Mechanism:** The paper defines a function space $\mathcal{F}^{x_0}_{predict}$ (functions matching the prediction) and $\mathcal{F}^{x_0}_{explain}$ (functions matching prediction + explanation). If the complexity of $\mathcal{F}^{x_0}_{explain}$ is strictly smaller than $\mathcal{F}^{x_0}_{predict}$, the explanation has pruned the space of possibilities; otherwise, it is mathematically non-informative.
- **Core assumption:** Rademacher complexity serves as a valid proxy for "knowledge" or "information content" about a decision function.
- **Evidence anchors:**
  - [abstract] "We call an explanation informative if it serves to reduce the complexity of the space of plausible decision functions."
  - [Page 7, Definition 1] Formalizes $R_n(\mathcal{F}^{x_0}_{explain}) < R_n(\mathcal{F}^{x_0}_{predict})$.
  - [corpus] The paper "Are We Merely Justifying Results ex Post Facto?" supports the skepticism regarding the information content of standard post-hoc explanations.
- **Break condition:** If the function class $\mathcal{F}$ is too large (e.g., all differentiable functions), the local constraint imposed by a pointwise explanation is insufficient to lower the complexity measure.

### Mechanism 2: Local Non-Informativeness via Interpolation
- **Claim:** For complex function classes, standard local explanations do not constrain global behavior because one can almost always construct a function that fits the explanation but behaves arbitrarily elsewhere.
- **Mechanism:** The proofs often rely on constructing a function $g$ that matches the prediction and explanation at point $x_0$ but also interpolates any arbitrary labeling of sample points $x_1, \dots, x_n$. Since $\mathcal{F}^{x_0}_{explain}$ can still fit random noise as well as $\mathcal{F}^{x_0}_{predict}$, their complexities are equal, rendering the explanation non-informative.
- **Core assumption:** The function class is rich enough to allow local modification without global structural penalty.
- **Evidence anchors:**
  - [Page 10, Proposition 4] Shows gradients are non-informative on differentiable functions because one can "locally modify a function in a tiny neighborhood... without changing the values it takes on the sample points."
  - [Page 14, Proposition 12] Shows SHAP is non-informative on deep trees because "trees of unlimited depth can fit an arbitrarily big sample of points with arbitrarily small leaves."
- **Break condition:** If the function class is restricted (e.g., linear models, shallow trees), the interpolation capacity is limited.

### Mechanism 3: Informativeness via Stability or Simplicity
- **Claim:** Explanations become informative if the function class is simple or if the explanation itself is "enriched" with stability guarantees.
- **Mechanism:** Stability (e.g., gradients changing slowly) couples the local behavior at $x_0$ to behavior in a neighborhood $B_r(x_0)$. This increases the "effective volume" of the constraint, making it harder to construct a counter-function that fits the explanation locally but diverges globally without violating the stability condition.
- **Core assumption:** The stability parameter $\delta$ or the function simplicity imposes a bound on how fast the function can change.
- **Evidence anchors:**
  - [Page 11, Definition 8] Defines $r$-$\delta$-locally stable explanations.
  - [Page 11, Proposition 9] "Locally stable gradient explanations are informative... The locally stable explanation provides a ball around $x_0$ on which the function... has $\delta$-Lipschitz gradients."
  - [corpus] "Explanations Go Linear" suggests linear constraints as a path to interpretability.
- **Break condition:** If the stability radius $r$ is effectively zero, or the function class allows infinite oscillation within the stability radius.

## Foundational Learning

- **Concept: Rademacher Complexity**
  - **Why needed here:** It is the central metric used to formally define "informativeness." You cannot understand the paper's proofs without grasping that complexity measures the ability of a function class to fit random noise.
  - **Quick check question:** If a function class can interpolate any random labeling of data points, is its Rademacher complexity high or low?

- **Concept: Post-Hoc Explanation Algorithms (Gradients, SHAP, Anchors)**
  - **Why needed here:** The paper analyzes these specific algorithms. You need to know that these are typically local, model-agnostic methods trying to explain individual predictions.
  - **Quick check question:** Does a SHAP value at a single point $x_0$ tell you how the model behaves at a distant point $x_1$?

- **Concept: Function Class Restrictions (Lipschitz, Tree Depth)**
  - **Why needed here:** The central result is conditional: explanations fail for *complex* functions but work for *simple* ones. Understanding constraints like Lipschitz continuity or bounded tree depth is key to identifying "simple."
  - **Quick check question:** Why would a "Lipschitz continuous" function be easier to explain than a generic differentiable function?

## Architecture Onboarding

- **Component map:**
  - Input: Function $f$, Point $x_0$, Function Class $\mathcal{F}$
  - Explanation Generator: Standard Algo (e.g., $\nabla f(x_0)$)
  - Verifier (Theoretical): Check if $R_n(\mathcal{F}^{x_0}_{explain}) < R_n(\mathcal{F}^{x_0}_{predict})$
  - Output: Boolean (Informative vs. Non-informative)

- **Critical path:** The interaction between the **Assumed Function Class ($\mathcal{F}$)** and the **Explanation Algorithm**. If $\mathcal{F}$ is "Deep Neural Networks" (complex) and the Algo is "Gradients", the path ends at "Non-informative". If $\mathcal{F}$ is "Linear Models", the path ends at "Informative".

- **Design tradeoffs:**
  - **Model Complexity vs. Explainability:** The paper proves a hard theoretical tradeoff. You cannot have a model that is arbitrarily complex and expect pointwise post-hoc explanations to be reliably informative.
  - **Stability vs. Generality:** Enriching explanations with stability makes them informative but requires verifying stability properties (expensive) or training stable models (potentially less accurate).

- **Failure signatures:**
  - **The "Local Wiggle":** High confidence in a gradient/SHAP value for a deep tree or DNN. The paper proves this confidence is mathematically unfounded regarding global behavior.
  - **Audit Inflation:** Relying on standard SHAP/Anchors for auditing high-stakes models without asserting model simplicity.

- **First 3 experiments:**
  1. **Sanity Check via Interpolation:** Take a complex model (e.g., unpruned decision tree). Generate an explanation. Can you construct a different model that matches that explanation but predicts the opposite for a specific subset of data?
  2. **Stability Stress Test:** Perturb the input $x_0$ slightly ($x_0 + \epsilon$). Does the explanation change drastically?
  3. **Complexity Constraint Ablation:** Measure the "informativeness" as you reduce model complexity (e.g., max depth of tree). Confirm informativeness rises as complexity falls.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on Rademacher complexity as the sole measure of informativeness, which may not capture all practical notions of explanation quality
- Theoretical results assume perfect knowledge of function class and explanation mechanism, conditions rarely met in practice
- Proofs depend heavily on constructing worst-case functions that may not reflect typical model behaviors

## Confidence
- **High:** Theoretical claims are rigorous and mathematically sound
- **Medium:** Practical implications require additional empirical validation
- **Low:** Paper focuses exclusively on local explanations and doesn't address global explanation methods or hybrid approaches

## Next Checks
1. Empirical validation: Test the theory's predictions on actual complex models by attempting to construct counter-examples that fit explanations but have different behaviors
2. Alternative complexity measures: Replicate key results using different measures of function class complexity (e.g., VC dimension, covering numbers)
3. Practical informativeness test: Design experiments where users must identify model behaviors from explanations alone, comparing performance across simple and complex function classes