---
ver: rpa2
title: 'Automating Thematic Review of Prevention of Future Deaths Reports: Replicating
  the ONS Child Suicide Study using Large Language Models'
arxiv_id: '2507.20786'
source_url: https://arxiv.org/abs/2507.20786
tags:
- reports
- toolkit
- thematic
- future
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that a fully automated LLM-based pipeline
  can reliably replicate manual thematic reviews of coronial data, specifically replicating
  the ONS's child-suicide PFD report analysis. The PFD Toolkit identified 72 child-suicide
  PFD reports (vs.
---

# Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models

## Quick Facts
- arXiv ID: 2507.20786
- Source URL: https://arxiv.org/abs/2507.20786
- Reference count: 14
- Key outcome: Automated LLM pipeline achieved κ = 0.82 agreement with blinded clinical adjudication, identifying 72 child-suicide PFD reports in 8m16s vs. months for manual review.

## Executive Summary
This study demonstrates that a fully automated LLM-based pipeline can reliably replicate manual thematic reviews of coronial data, specifically replicating the ONS's child-suicide PFD report analysis. The PFD Toolkit identified 72 child-suicide PFD reports (vs. 37 found manually by ONS), achieving substantial to almost-perfect agreement (Cohen's κ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%) with blinded clinical adjudication. The end-to-end analysis was completed in 8 minutes and 16 seconds, compared to months for manual review, transforming a months-long process into minutes. The toolkit is openly available and designed for generalizable use across various PFD research applications, enabling scalable, reproducible, and timely public health insights.

## Method Summary
The study processed 4,249 PFD reports from judiciary.uk using the PFD Toolkit v0.3.3 Python package with GPT-4.1 via OpenAI cloud API. Documents were converted to 200 dpi Base64 images for OCR extraction, then classified against a 23 sub-theme/6 theme coding frame with outputs coerced into boolean JSON arrays validated by Pydantic schemas. A stratified sample of 144 reports (72 positive, 72 negative) was clinically validated by three blinded adjudicators with consensus resolution. The pipeline handled both text-based and scanned documents, achieving end-to-end processing in 8m16s.

## Key Results
- Achieved Cohen's κ = 0.82 (95% CI: 0.66-0.98), raw agreement = 91% with blinded clinical adjudication
- Identified 72 child-suicide PFD reports versus 37 found manually by ONS
- Completed end-to-end analysis in 8 minutes and 16 seconds versus months for manual review
- Processed heterogeneous document formats (text-based and scanned PDFs) using vision-enabled LLM OCR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-capable LLMs with OCR can reliably extract structured data from heterogeneous document formats where traditional scraping fails.
- Mechanism: The pipeline converts documents to 200 dpi Base64 images, then uses a vision-enabled LLM (GPT-4.1) to perform OCR and extract both metadata and long-text content, bypassing the need for searchable text or consistent formatting.
- Core assumption: The vision-language model can accurately transcribe scanned documents and infer semantic meaning from layout cues at least as well as manual human review.
- Evidence anchors: [abstract] "All 4,249 PFD reports... processed via PFD Toolkit's large language model pipelines"; [section 3.2] "The toolkit processed both text-based and scanned documents using a multi-stage pipeline that includes 200 dpi Base64 image conversion and vision-enabled LLM text extraction via OCR"

### Mechanism 2
- Claim: LLM-based boolean classification with schema enforcement can achieve substantial-to-almost-perfect agreement with blinded clinical adjudication for case identification.
- Mechanism: The LLM is prompted to classify reports against a predefined coding frame (child suicide relevance, addressee categories, 23 concern sub-themes), with outputs coerced into JSON arrays of boolean values validated by Pydantic schemas.
- Core assumption: The coding frame categories are sufficiently unambiguous that an LLM can apply them consistently, and that the prompts adequately operationalize edge cases.
- Evidence anchors: [abstract] "Cohen's κ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%"; [section 3.3] "outputs were coerced into JSON arrays containing boolean values, where True represented a positive case"

### Mechanism 3
- Claim: Full automation of the screening-to-tabulation pipeline reduces analysis time from months to minutes without proportional loss in accuracy.
- Mechanism: The end-to-end script handles corpus acquisition (web scraping), document processing (OCR/vision), classification (LLM prompting), and output tabulation in a single executable workflow, eliminating handoffs between manual reviewers.
- Core assumption: API latency and rate limits are the primary bottlenecks, and consumer-grade hardware with cloud API access is sufficient for research-scale corpora.
- Evidence anchors: [abstract] "end-to-end analysis was completed in 8 minutes and 16 seconds, compared to months for manual review"; [section 4.3] "end-to-end processing (wall) time for the workflow was 8 minutes and 16 seconds"

## Foundational Learning

- **Concept**: Cohen's κ (kappa) and Fleiss' κ for inter-rater reliability
  - Why needed here: Understanding whether κ = 0.82 represents meaningful agreement requires knowing that κ corrects for chance agreement and that 0.61-0.80 is "substantial" and >0.80 is "almost perfect" on the Landis-Koch scale.
  - Quick check question: If two raters agree 91% of the time but would agree 70% of the time by chance alone, what would κ approximate? (Answer: ~0.70)

- **Concept**: Schema-constrained LLM output (JSON mode / Pydantic validation)
  - Why needed here: The pipeline relies on forcing LLM outputs into boolean arrays; understanding Pydantic validation helps debug cases where the LLM generates malformed outputs that fail schema checks.
  - Quick check question: Why use boolean arrays rather than free-text descriptions for thematic coding at scale? (Answer: Enables automated aggregation, statistical comparison, and reproducible quantification)

- **Concept**: Thematic coding frames and operationalization
  - Why needed here: The study replicates an ONS coding frame with 23 sub-themes under 6 themes; understanding how coding frames are constructed reveals why the LLM can't directly compare counts to ONS (different counting units—"mentions" vs. "report-level presence").
  - Quick check question: Why might the same report contribute multiple "mentions" to a sub-theme but only one "presence" flag? (Answer: A single report may repeat the same concern across multiple paragraphs)

## Architecture Onboarding

- Component map: judiciary.uk → Web Scraper → Document Store (4,249 PDFs) → Image Converter (200 dpi Base64) → Vision-LLM (GPT-4.1) → JSON Arrays → Pydantic Validator → Structured Tables → Output: Screening Results + Theme Counts

- Critical path: Vision-LLM extraction → boolean classification → validation. If Pydantic validation fails, the entire pipeline stalls; error handling for malformed LLM outputs is essential.

- Design tradeoffs:
  - Cloud API (GPT-4.1) vs. local LLM (Ollama): Cloud offers higher accuracy but raises data governance concerns and incurs per-token costs; local deployment trades accuracy for privacy and cost predictability.
  - Report-level presence vs. mention counting: Simpler aggregation but loses within-report frequency information, limiting comparability with ONS methodology.

- Failure signatures:
  - Silent false negatives: Reports where the coroner doesn't explicitly state age or suicidal intent but context implies it (e.g., "Year 10 student" without "child" label).
  - Schema validation errors: LLM outputs that don't conform to expected JSON structure, requiring retry logic or human review.
  - OCR degradation: Scanned documents with low contrast, handwritten annotations, or stamp artifacts that corrupt text extraction.

- First 3 experiments:
  1. Baseline replication: Run PFD Toolkit on a held-out subset of 100 reports with known manual labels to establish precision/recall before scaling to full corpus.
  2. Model ablation: Compare GPT-4.1 vs. a local model (e.g., Llama 3.1 via Ollama) on the same 144-report validation sample to quantify accuracy-cost tradeoffs.
  3. Counting method alignment: Modify the prompt to output "mention counts" rather than boolean presence, enabling direct comparison with ONS aggregate figures and identifying where discrepancies originate.

## Open Questions the Paper Calls Out
None

## Limitations
- Clinical adjudication representativeness: The 144-report validation sample is stratified but not described in terms of diversity of document formats or difficulty levels, which may bias agreement metrics.
- Counting methodology divergence: The study reports report-level thematic presence (boolean flags) rather than ONS's mention-level counts, limiting direct comparability of theme frequency estimates.
- Prompt engineering opacity: The exact GPT-4.1 prompts and few-shot examples are not fully disclosed, making it difficult to assess how edge cases are operationalized.

## Confidence
- **High confidence**: The pipeline's ability to process heterogeneous document formats and achieve substantial-to-almost-perfect agreement (κ = 0.82) with blinded clinical adjudication.
- **Medium confidence**: The runtime efficiency claim (8m16s for 4,249 reports) assumes stable API performance and does not account for potential rate limits or cost variability at scale.
- **Low confidence**: Direct comparability of theme counts to ONS methodology due to differing counting units (report-level presence vs. mention-level frequency).

## Next Checks
1. Prompt replication: Reconstruct and test the exact GPT-4.1 prompts on a held-out sample of 100 reports with known manual labels to validate agreement metrics and identify failure modes.
2. Model ablation study: Compare GPT-4.1 performance against a local LLM (e.g., Llama 3.1 via Ollama) on the same 144-report validation sample to quantify accuracy-cost tradeoffs and assess feasibility for data-governance-sensitive settings.
3. Methodological alignment: Modify the pipeline to output mention-level counts (rather than report-level presence) for the 23 sub-themes and re-compare with ONS aggregate figures to identify where discrepancies originate and whether they stem from classification errors or methodological differences.