---
ver: rpa2
title: Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks
arxiv_id: '2510.03351'
source_url: https://arxiv.org/abs/2510.03351
tags:
- connectivity
- concepts
- cortex
- concept
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpretable neuropsychiatric
  diagnosis using resting-state fMRI data. The authors propose CONCEPTNEURO, a concept-based
  framework that leverages large language models (LLMs) to generate interpretable
  functional connectivity concepts, which are then integrated into a graph neural
  network (GNN) architecture for disorder prediction.
---

# Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks

## Quick Facts
- arXiv ID: 2510.03351
- Source URL: https://arxiv.org/abs/2510.03351
- Reference count: 40
- CONCEPTNEURO framework improves diagnostic accuracy and interpretability for neuropsychiatric disorders using LLM-generated connectivity concepts

## Executive Summary
This paper introduces CONCEPTNEURO, a framework that combines large language models with graph neural networks to enable interpretable diagnosis of neuropsychiatric disorders from resting-state fMRI data. The approach generates disorder-specific connectivity concepts via LLM-guided prompts using neuroimaging terms, filters and encodes these as structured subgraphs, and integrates them into a concept bottleneck classifier. Experiments on ABCD and HCP-D datasets demonstrate consistent accuracy improvements over vanilla GNN baselines while providing clinically meaningful explanations that align with domain knowledge.

## Method Summary
CONCEPTNEURO processes rs-fMRI data through a multi-stage pipeline: (1) Functional connectivity matrices are computed from ROI time-series using Pearson correlation, (2) Disorder-specific connectivity concepts are generated using GPT-4.1 prompted with NeuroQuery terms, (3) Concepts are parsed into region pairs with direction priors and filtered for atlas compatibility, (4) Top-N concept subgraphs are extracted and encoded using a 2-layer GNN backbone, and (5) A concept bottleneck MLP classifier produces interpretable predictions with combined cross-entropy, sparsity, and direction-aware hinge losses. The framework is trained with Adam optimizer, early stopping, and cross-validation.

## Key Results
- Consistently outperforms vanilla GNN baselines across 5 psychiatric disorders on ABCD dataset
- Improves classification accuracy while providing clinically meaningful explanations
- Expert analysis shows strong agreement between model-extracted concepts and domain knowledge
- Ablation studies confirm importance of diverse, high-quality concept sets for both interpretability and predictive performance

## Why This Works (Mechanism)
The framework leverages LLMs to bridge the gap between neuroimaging literature and computational models by extracting clinically relevant connectivity patterns. By encoding these patterns as structured subgraphs within a GNN architecture, the model can learn disorder-specific functional connectivity signatures while maintaining interpretability through the concept bottleneck approach. The direction-aware regularization ensures that learned representations align with established connectivity hypotheses in neuropsychiatric research.

## Foundational Learning
- **Functional connectivity concepts**: Interpretable descriptions of brain region relationships derived from neuroimaging literature - needed to translate clinical knowledge into computational features; quick check: verify concepts can be mapped to specific ROI pairs
- **Concept bottleneck architecture**: Classifier that predicts intermediate concept labels before final diagnosis - needed to maintain interpretability while enabling complex decision-making; quick check: ensure concept predictions are reasonably accurate
- **Direction-aware regularization**: Loss term that enforces consistency with hyper/hypoconnectivity priors - needed to align model learning with established connectivity hypotheses; quick check: monitor direction agreement during training

## Architecture Onboarding

**Component map**: rs-fMRI data -> FC matrices -> Concept generation (LLM) -> Concept parsing/filtering -> Concept subgraph extraction -> GNN encoding -> Concept bottleneck MLP -> Diagnosis

**Critical path**: The LLM-generated concepts must be both anatomically valid and clinically meaningful; failures here cascade through the entire pipeline and degrade both accuracy and interpretability

**Design tradeoffs**: High-quality LLM concepts improve interpretability but increase computational cost and introduce potential bias; simpler concept sets reduce cost but may miss important disorder signatures

**Failure signatures**: 
- High concept filtering rate (>30%) indicates poor LLM-prompt alignment
- Sparse concept subgraphs (<5 edges) suggest atlas resolution issues
- Low expert agreement (<0.6) indicates clinically irrelevant concepts
- Accuracy gains without interpretability gains suggest concept overfitting

**Three first experiments**:
1. Vary LLM temperature and prompt specificity to optimize concept quality vs. filtering rate
2. Compare different GNN architectures (GCN, GAT, GraphSAGE, GIN) for concept encoding
3. Systematically ablate concept sets to identify most informative connectivity patterns

## Open Questions the Paper Calls Out

### Open Question 1
Can the integration of multimodal data (e.g., structural MRI, genetic data) improve the predictive power and robustness of CONCEPTNEURO?
- Basis in paper: [explicit] The authors state in the Limitations section that "Translation to practice would require... integration with multimodal assessments."
- Why unresolved: The current framework exclusively utilizes resting-state fMRI data to generate functional connectivity concepts, ignoring other potential biomarkers.
- What evidence would resolve it: Experiments combining rs-fMRI with structural or genetic data within the concept bottleneck framework, showing improved accuracy over the fMRI-only baseline.

### Open Question 2
How robust is the framework to spurious or hallucinated concepts generated by the Large Language Model?
- Basis in paper: [explicit] The Limitations section notes that "interpretability... is inherently constrained by the quality of the candidate concepts provided by LLMs; ... spurious or incomplete concepts may still arise."
- Why unresolved: While the paper uses filtering and expert validation, it does not quantify the system's resilience against high-confidence but clinically incorrect LLM outputs.
- What evidence would resolve it: An ablation study systematically injecting noise (plausible but false concepts) into the concept set and measuring the resulting drop in diagnostic accuracy or interpretability scores.

### Open Question 3
Does the assumption of consistent direction-aware priors (hyper/hypoconnectivity) hinder performance in heterogeneous populations?
- Basis in paper: [explicit] The authors acknowledge that their direction-aware regularization "assumes that such priors are correct and consistent across individuals, which may not always hold in heterogeneous disorders."
- Why unresolved: The current loss function penalizes violations of these priors, potentially underfitting subgroups of patients who exhibit atypical connectivity directions.
- What evidence would resolve it: A comparative analysis of model performance on patient subgroups clustered by connectivity direction, comparing the rigid regularization against a flexible baseline.

## Limitations
- Relies heavily on LLM-generated concepts, introducing potential variability and bias in concept quality
- Computational cost of generating and ranking concepts may limit scalability to larger disorder taxonomies
- Clinical interpretability claims require systematic evaluation against established diagnostic criteria

## Confidence

**High confidence**: Reported accuracy improvements over vanilla GNN baselines; experimental methodology for data preprocessing and model training; basic concept generation and filtering pipeline

**Medium confidence**: Clinical interpretability claims; expert agreement results; generalization to unseen disorders

**Low confidence**: Specific hyperparameter choices (N_c, MLP architecture); stability of concept rankings across different LLM versions or prompts; computational efficiency claims

## Next Checks

1. Conduct ablation studies varying LLM versions and prompt templates to quantify concept generation stability and its impact on downstream classification accuracy
2. Perform cross-dataset validation by training on ABCD and testing on an independent cohort to assess generalizability beyond the training population
3. Systematically evaluate concept-clinical alignment using standardized neuropsychiatric diagnostic criteria (e.g., DSM-5) with multiple independent expert raters and calculate inter-rater reliability scores