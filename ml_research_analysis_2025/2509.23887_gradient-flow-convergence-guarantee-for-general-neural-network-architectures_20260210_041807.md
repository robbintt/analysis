---
ver: rpa2
title: Gradient Flow Convergence Guarantee for General Neural Network Architectures
arxiv_id: '2509.23887'
source_url: https://arxiv.org/abs/2509.23887
tags:
- neural
- relu
- convergence
- networks
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unified theoretical framework for analyzing
  the convergence of gradient flow in training various neural network architectures.
  The main result shows that, under mild assumptions, gradient flow converges exponentially
  fast to a global minimum for a broad class of neural networks with piecewise nonzero
  polynomial activations, ReLU, or sigmoid activations, provided the network is sufficiently
  over-parameterized (with at least nM parameters, where n is the training set size
  and M is the output dimension).
---

# Gradient Flow Convergence Guarantee for General Neural Network Architectures

## Quick Facts
- arXiv ID: 2509.23887
- Source URL: https://arxiv.org/abs/2509.23887
- Authors: Yash Jakhmola
- Reference count: 15
- Primary result: Gradient flow converges exponentially to global minima for over-parameterized neural networks with piecewise nonzero polynomial activations, ReLU, or sigmoid activations.

## Executive Summary
This paper provides a unified theoretical framework proving that gradient flow converges exponentially fast to a global minimum for a broad class of neural network architectures, including deep networks, ResNets, and graph convolutional networks. The key innovation is showing that the neural tangent kernel (NTK) remains positive definite throughout training with high probability, which guarantees linear convergence of the training loss. The result applies to networks with at least nM parameters (where n is training set size and M is output dimension) using piecewise nonzero polynomial activations, ReLU, or sigmoid activations.

## Method Summary
The paper analyzes gradient flow dynamics in over-parameterized neural networks by establishing that the NTK remains positive definite throughout training. The method involves proving that the set of initializations resulting in a singular NTK has measure zero, and this property is preserved during training via diffeomorphism properties. For ReLU and sigmoid activations, limiting arguments are used where Leaky ReLU serves as a piecewise polynomial proxy for ReLU, and Jackson's inequality constructs piecewise polynomial approximations for sigmoid. The convergence rate is derived by showing the loss dynamics follow a differential inequality that guarantees exponential decay.

## Key Results
- Gradient flow converges exponentially to global minima for neural networks with at least nM parameters
- NTK remains positive definite throughout training with probability 1 under mild initialization conditions
- Results extend to ReLU and sigmoid activations via limiting arguments
- Applies to diverse architectures including deep networks, ResNets, and graph convolutional networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient flow converges to a global minimum with probability 1 under sufficient over-parameterization.
- Mechanism: The paper establishes that the Neural Tangent Kernel (NTK), denoted as $G(t)$, remains positive definite throughout training. This relies on a measure-theoretic argument: the set of initial parameters and data points resulting in a singular NTK has measure zero. Because the gradient flow solution is a diffeomorphism (almost everywhere), it maps this measure-zero "degenerate" set to measure-zero sets at future time steps, preserving the NTK's positive definiteness.
- Core assumption: The network has at least $nM$ trainable parameters (where $n$ is dataset size, $M$ output dimension), and initialization/data are drawn from absolutely continuous distributions.
- Break condition: Initialization occurs exactly on the measure-zero set where the NTK is singular, or the model is under-parameterized ($P < nM$) such that $\frac{\partial F_t}{\partial \theta(t)}$ is rank-deficient.

### Mechanism 2
- Claim: The training loss decays exponentially (linear convergence).
- Mechanism: By deriving the dynamics of the prediction vector $F_t$ via the chain rule, the paper shows $\frac{d}{dt}||y-F_t||^2 \le -2\lambda_0||y-F_t||^2$. Because $\lambda_0$ (the minimum eigenvalue of the NTK) is strictly positive (per Mechanism 1), this differential inequality guarantees an exponential decay bound $||y-F_t|| \le e^{-\lambda_0 t}||y-F_0||$.
- Core assumption: The activation functions are continuous and piecewise polynomial (or approximated by them), allowing the application of ODE solution theorems (Picard's theorem) and ensuring Lipschitz continuity locally.
- Break condition: The NTK eigenvalues approach zero (vanishing gradient), or the activation function violates the continuity/polynomial constraints required to guarantee $\lambda_0$ behaves continuously.

### Mechanism 3
- Claim: Convergence guarantees extend to ReLU and sigmoid activations despite their non-polynomial nature.
- Mechanism: The authors employ a limiting argument. For ReLU, they utilize Leaky ReLU (which is piecewise non-zero polynomial) as a lower bound and show that as the Leaky ReLU converges to ReLU, the convergence properties persist. For sigmoid, they use Jackson's inequality to construct a sequence of piecewise polynomials that uniformly approximate sigmoid.
- Core assumption: The approximation error introduced by the polynomial proxies does not destabilize the positive definiteness of the NTK in the limit.
- Break condition: Using activation functions that cannot be uniformly approximated by piecewise non-zero polynomials or have unbounded derivatives that violate the ODE existence conditions.

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: The entire proof hinges on the behavior of the NTK matrix. You must understand that the NTK characterizes the training dynamics in the infinite-width/linearized regime.
  - Quick check question: Does the NTK stay constant during training in this paper's analysis, or does its positive definiteness simply persist?

- Concept: **Gradient Flow (vs. Gradient Descent)**
  - Why needed here: The theoretical guarantee is exact only for the infinitesimal step size limit (continuous time). Practical application requires understanding the gap between this theory and discrete GD steps.
  - Quick check question: If a learning rate is too high, does Theorem 3.1 still technically apply?

- Concept: **Over-parameterization Threshold ($P \ge nM$)**
  - Why needed here: This is the specific condition required to ensure the Jacobian matrix $\frac{\partial F_t}{\partial \theta(t)}$ can be full rank, preventing the NTK from being singular.
  - Quick check question: If you double the dataset size $n$, what is the minimum required change in parameters $P$ to maintain the convergence guarantee?

## Architecture Onboarding

- Component map: The theory applies to a **General Function Structure**: $f(X, \theta) := g_L(\sigma(\dots g_1(X, \theta)))$. Here, $g_i$ are polynomial layers (e.g., Linear, Conv, ResNet connections) and $\sigma$ is a piecewise non-zero polynomial activation.
- Critical path:
  1. Verify parameter count $P \ge n \times M$.
  2. Ensure initialization is from an absolutely continuous distribution (e.g., Kaiming Uniform).
  3. Use a learning rate small enough to approximate Gradient Flow if strict theoretical convergence is desired.
- Design tradeoffs:
  - **Capacity vs. Speed**: While $P \ge nM$ guarantees convergence, the rate depends on $\lambda_0$. Wider networks tend to have larger $\lambda_0$, potentially speeding up convergence, but increasing compute cost.
  - **Activation Choice**: Leaky ReLU fits the theory directly. Standard ReLU requires the limiting argument, which is theoretically sound but practically adds a layer of abstraction to the guarantee.
- Failure signatures:
  - **Stalling**: Loss plateaus if the NTK eigenvalues become extremely small (near the measure-zero degenerate set), effectively breaking the "positive definite" requirement numerically.
  - **Under-parameterization**: If $P < nM$, the system is rank-deficient, and the proof collapses; the network may not fit the training data.
- First 3 experiments:
  1. **Synthetic Validation**: Train a DNN on random data (Zhang et al. style) with $P$ just above $nM$ and verify the loss curve fits $O(e^{-t})$ on a log plot.
  2. **Ablation on Width**: Train the same architecture while slowly reducing parameters below the $nM$ threshold to observe the transition from convergence to stalling.
  3. **Architecture Stress Test**: Apply the setup to a Graph Convolutional Network (GCN) and a ResNet to confirm the unified theorem holds across these diverse "polynomial layer" structures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical convergence guarantees for Transformer architectures be established with weaker over-parameterization assumptions than standard DNNs?
- Basis in paper: [explicit] The conclusion suggests "showing that transformers have better convergence properties (in the sense of either weaker assumptions or stronger conclusions) would be an interesting line of work."
- Why unresolved: While the paper provides a unified theorem for many architectures, it does not specifically analyze the attention mechanism to determine if it requires fewer parameters than the general $P \geq nM$ bound to guarantee convergence.
- What evidence would resolve it: A formal proof demonstrating linear convergence for Transformers with an over-parameterization threshold strictly lower than $nM$, or showing faster convergence rates compared to fully connected networks.

### Open Question 2
- Question: Does the linear convergence of gradient flow hold for classification tasks utilizing cross-entropy loss?
- Basis in paper: [explicit] The author notes the analysis relies on squared loss and states, "Future works can try extending this result to other losses like cross-entropy."
- Why unresolved: The current proof relies on the dynamics of the squared error function; the mathematical behavior of the Neural Tangent Kernel (NTK) under cross-entropy loss within this framework remains unverified.
- What evidence would resolve it: Extending the unified theorem to derive bounds on the decay of cross-entropy loss, or providing a counter-example where the NTK loses positive definiteness under such conditions.

### Open Question 3
- Question: Can the guarantee of exponential convergence be rigorously extended from continuous gradient flow to discrete gradient descent with a fixed step size?
- Basis in paper: [inferred] The paper explicitly states the results are "only exact in the infinitesimal step size limit" and relies on empirical results to justify practical performance, leaving a theoretical gap regarding discrete updates.
- Why unresolved: The proof utilizes properties of ordinary differential equations which do not perfectly map to the discrete error accumulation inherent in standard gradient descent optimizers.
- What evidence would resolve it: A theoretical bound establishing a maximum step size $\eta$ for which the discrete gradient descent trajectory maintains the exponential convergence rate proven for gradient flow.

## Limitations

- The theoretical guarantee is exact only for continuous gradient flow (infinitesimal learning rate), not practical discrete gradient descent
- Extension to ReLU and sigmoid activations relies on limiting arguments that add theoretical abstraction
- Measure-theoretic arguments for NTK positive definiteness may not hold in practical scenarios with non-absolutely continuous distributions

## Confidence

- **High Confidence**: The core claim that over-parameterization (P â‰¥ nM) ensures NTK positive definiteness and thus convergence, under the assumptions stated.
- **Medium Confidence**: The extension of the theory to ReLU and sigmoid activations via limiting arguments.
- **Medium Confidence**: The empirical validation using gradient descent with a small learning rate as an approximation of gradient flow.

## Next Checks

1. **Empirical NTK Analysis**: For a trained network, compute the eigenvalues of the NTK at initialization and throughout training to empirically verify the theoretical claim that they remain strictly positive.

2. **Under-parameterization Transition**: Systematically reduce the number of parameters P below the nM threshold and observe the point at which the loss curve deviates from exponential decay, confirming the critical role of over-parameterization.

3. **Activation Function Sensitivity**: Compare the convergence behavior of Leaky ReLU, standard ReLU, and sigmoid activations on the same architecture to isolate the impact of the limiting argument used for non-polynomial activations.