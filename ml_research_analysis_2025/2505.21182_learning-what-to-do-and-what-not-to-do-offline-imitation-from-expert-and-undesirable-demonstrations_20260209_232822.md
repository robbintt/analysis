---
ver: rpa2
title: 'Learning What to Do and What Not To Do: Offline Imitation from Expert and
  Undesirable Demonstrations'
arxiv_id: '2505.21182'
source_url: https://arxiv.org/abs/2505.21182
tags:
- expert
- learning
- dataset
- objective
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContraDICE, a novel offline imitation learning
  framework that effectively leverages both expert and undesirable demonstrations.
  The core innovation is a training objective formulated as the difference of two
  KL divergences, enabling the agent to both imitate expert behavior and explicitly
  avoid undesirable behaviors.
---

# Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations

## Quick Facts
- arXiv ID: 2505.21182
- Source URL: https://arxiv.org/abs/2505.21182
- Authors: Huy Hoang; Tien Mai; Pradeep Varakantham; Tanvi Verma
- Reference count: 40
- Primary result: ContraDICE outperforms state-of-the-art baselines on 18 MuJoCo and Adroit tasks, achieving near-expert performance with as few as 3-5 expert trajectories while effectively utilizing undesirable demonstrations.

## Executive Summary
This paper introduces ContraDICE, a novel offline imitation learning framework that effectively leverages both expert demonstrations and undesirable demonstrations. The core innovation is a training objective formulated as the difference of two KL divergences, enabling the agent to both imitate expert behavior and explicitly avoid undesirable behaviors. Extensive experiments show that ContraDICE consistently outperforms state-of-the-art baselines, particularly in settings with limited expert data and mixed-quality demonstrations.

## Method Summary
ContraDICE addresses offline imitation learning from three data sources: expert demonstrations (BG), undesirable/bad demonstrations (BB), and unlabeled mixed-quality data (BMIX). The method trains binary classifiers to estimate occupancy ratios between expert/bad data and the union dataset, then optimizes a convex objective that minimizes divergence from good demonstrations while maximizing divergence from bad ones. A tractable Q-learning algorithm is derived via Lagrangian duality, and policies are extracted using Q-weighted behavioral cloning. The framework demonstrates strong performance across both locomotion and manipulation tasks.

## Key Results
- ContraDICE consistently outperforms state-of-the-art baselines across 18 MuJoCo and Adroit tasks
- Achieves near-expert performance with as few as 3-5 expert trajectories
- Effectively utilizes undesirable demonstrations to improve safety and robustness
- Demonstrates superior performance in limited expert data settings compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing a difference of KL divergences enables simultaneous attraction to expert behavior and repulsion from undesirable behavior.
- Mechanism: The objective min_{dπ} [D_KL(dπ || dG) - α·D_KL(dπ || dB)] minimizes divergence from good demonstrations while maximizing divergence from bad ones. The α parameter controls the strength of repulsion.
- Core assumption: Expert and undesirable demonstrations occupy meaningfully different regions of state-action visitation space.
- Evidence anchors: [abstract] "We propose a novel formulation that optimizes a difference of KL divergences over the state-action visitation distributions of expert and undesirable (or bad) data."

### Mechanism 2
- Claim: Convexity of the learning objective is preserved when the expert outweighs undesirable demonstrations in the optimization.
- Mechanism: Proposition 4.1 proves that when α ≤ 1, f(dπ) remains convex because (1-α)·dπ·log(dπ) dominates (a convex term), while the linear term preserves convexity.
- Core assumption: The occupancy measure space is well-defined and allows Bellman-style decomposition.
- Evidence anchors: [Section 4.1, Proposition 4.1] "If α ≤ 1, then the objective function f(dπ)... is convex in dπ."

### Mechanism 3
- Claim: A lower-bound surrogate objective replaces unstable exponential terms with tractable linear forms while preserving gradient direction.
- Mechanism: Proposition 4.3 uses the inequality exp(t) ≥ t + 1 to replace exp[(Ψ - T^π[Q])/(1-α)] with a linear lower bound, yielding a tractable Q-learning objective.
- Core assumption: The soft Bellman residual T^π[Q](s,a) approximates zero at optimal Q.
- Evidence anchors: [Section 4.2, Proposition 4.3] "eL(Q, π) is a lower bound of L(Q, π), with equality when T^π[Q](s,a) = 0"

## Foundational Learning

- Concept: **Occupancy Measures / State-Action Visitation Distributions**
  - Why needed here: The entire objective operates over dπ(s,a), not policies directly. Understanding how policies induce occupancy measures is essential for interpreting the KL divergence terms.
  - Quick check question: Can you explain why two different policies might induce the same occupancy measure, and what this implies for uniqueness of solutions?

- Concept: **Lagrangian Duality for Constrained MDP Optimization**
  - Why needed here: The convex objective over dπ is transformed into an unconstrained Q-learning problem via duality. Without this, you cannot derive the practical algorithm.
  - Quick check question: Why does convexity of the primal problem matter for strong duality to hold?

- Concept: **Soft Q-Learning / Maximum Entropy RL**
  - Why needed here: The paper adopts MaxEnt framework (soft value functions, entropy-regularized Bellman operators). The V^π_Q definition includes entropy terms; XQL is used for V estimation.
  - Quick check question: How does the soft value function V_Q(s) = β·log(Σ_a μ_U(a|s)·exp(Q(s,a)/β)) differ from standard max-Q, and what role does β play?

## Architecture Onboarding

- Component map:
  - Discriminators cG, cB (2-layer MLP, 256 hidden) -> Binary classifiers distinguishing {BG vs BU} and {BB vs BU}
  - Q-network (2-layer MLP, 256 hidden) -> Estimates Q(s,a), updated via minimizing Ĺ(Q|V) + χ² regularizer
  - V-network (2-layer MLP, 256 hidden) -> Estimated via Extreme Q-Learning objective
  - Policy π_θ (2-layer MLP, 256 hidden, Tanh Gaussian output) -> Extracted via Q-weighted BC
  - Target Q-network -> Soft update with τ=0.005 for stable V-updates

- Critical path:
  1. Train cG, cB for N_µ steps (discriminator pretraining)
  2. Compute Ψ(s,a) = log[dG/dU] - α·log[dB/dU] using discriminator outputs
  3. Alternate: Update Q → Update V → Update π → Soft update target Q
  4. Repeat for N training steps

- Design tradeoffs:
  - α selection: Higher α = stronger bad-avoidance but risks non-convexity. Paper uses 0.2–0.8 per task.
  - QW-BC vs AW-BC: Q-weighted BC avoids V(s) estimation noise but theoretically equivalent. Empirically superior.
  - β (temperature): Controls entropy-regularization strength. Values 10–30 work across tasks.

- Failure signatures:
  - NaN during training: Check discriminator outputs for zeros; add small ε to ratios.
  - Policy collapses to bad behavior: α too high or cB discriminator failed.
  - No learning (flat loss): BMIX quality too low; discriminator cannot distinguish BG from BU.
  - Oscillating Q-values: Increase τ (slower target update) or add stronger χ² regularization.

- First 3 experiments:
  1. Sanity check on simple task: Use CHEETAH-RANDOM+EXPERT with α=0.5, β=20. Verify discriminator AUC > 0.9. Confirm policy achieves >50 normalized score within 500k steps.
  2. Ablate α sensitivity: Fix β=20, sweep α∈{0.0, 0.3, 0.5, 0.7, 0.9}. Plot final normalized score. Expect inverted-U shape with peak at 0.4–0.6.
  3. Compare policy extraction: On HOPPER-RANDOM+EXPERT, run both AW-BC and QW-BC with identical Q/V checkpoints. Measure stability (variance across seeds) and final performance. Expect QW-BC to show lower variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical framework be extended to allow α > 1 while preserving convexity or optimization stability?
- Basis in paper: [explicit] The authors state in the Limitations section: "While our method shows strong empirical performance, it is currently limited to settings where α ≤ 1. Relaxing this constraint would make the learning objective more challenging to optimize, but represents a promising direction for future research."
- Why unresolved: The current convexity proof requires α ≤ 1, and their naive modification for α ≥ 1 does not provide good empirical performance.
- What evidence would resolve it: A new theoretical formulation or approximation scheme that handles α > 1 with provable convergence guarantees, validated empirically across benchmarks.

### Open Question 2
- Question: How can ContraDICE be extended to handle noisy or weakly labeled demonstrations where the expert/bad classification is uncertain?
- Basis in paper: [explicit] The Limitations section states: "we assume access to well-labeled expert and undesirable demonstrations, which may not hold in practice. Developing robust methods that can learn effectively from noisy or weakly labeled data would be a valuable extension of this work."
- Why unresolved: The current method relies on binary classifiers to estimate occupancy ratios dG/dU and dB/dU, which assumes clean labels.
- What evidence would resolve it: Experiments with probabilistic or partially corrupted labels showing maintained performance, ideally with theoretical robustness guarantees.

### Open Question 3
- Question: Can data filtering mechanisms improve ContraDICE's scalability with larger expert datasets, particularly to match ILID's performance in high-data regimes?
- Basis in paper: [explicit] Appendix D.2 notes: "These observations suggest a potential direction for improving ContraDICE by incorporating similar data filtering mechanisms... We leave this exploration for future work."
- Why unresolved: ILID outperforms ContraDICE-G when the good dataset is large because ILID explicitly discards irrelevant transitions.
- What evidence would resolve it: A modified ContraDICE variant with Q-based transition filtering that achieves comparable or better performance than ILID on large-expert-dataset settings.

## Limitations
- Theoretical constraint: The convexity guarantee only holds for α ≤ 1, limiting the strength of bad behavior avoidance.
- Data requirements: Assumes access to well-labeled expert and undesirable demonstrations, which may not hold in practice.
- Scalability issue: Performance degrades with very large expert datasets compared to methods with explicit data filtering mechanisms.

## Confidence

- High confidence: The core mechanism of optimizing a difference of KL divergences and the convexity result are well-supported by theoretical proofs and ablation studies.
- Medium confidence: The surrogate objective derivation is theoretically sound, but empirical superiority relies primarily on single ablation experiments.
- Low confidence: The practical impact of β (temperature) selection across diverse tasks is inadequately characterized - the paper reports specific values per task but provides minimal guidance on selection methodology.

## Next Checks

1. **α sensitivity analysis**: Replicate the ablation study from Appendix D.7 across all 18 tasks to verify the inverted-U performance curve and determine the optimal α range per task type.

2. **Robustness to discriminator failure**: Intentionally degrade cB discriminator performance (e.g., by training on small BB datasets) and measure policy degradation to establish the minimum discriminator quality required for safe learning.

3. **Zero-shot transfer validation**: Test policies trained on D4RL datasets in modified environments (e.g., altered dynamics or reward functions) to assess whether the method genuinely learns robust, transferable behavior rather than overfitting to specific trajectories.