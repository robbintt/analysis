---
ver: rpa2
title: Random-Set Large Language Models
arxiv_id: '2504.18085'
source_url: https://arxiv.org/abs/2504.18085
tags:
- belief
- uncertainty
- sets
- arxiv
- cuzzolin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random-Set Large Language Models (RS-LLMs),
  a novel approach to uncertainty quantification in LLMs by predicting belief functions
  over token sets instead of probability vectors. The method uses hierarchical clustering
  to create a budget of focal token subsets, making it computationally scalable while
  preserving uncertainty representation.
---

# Random-Set Large Language Models

## Quick Facts
- arXiv ID: 2504.18085
- Source URL: https://arxiv.org/abs/2504.18085
- Authors: Muhammad Mubashar; Shireen Kudukkil Manchingal; Fabio Cuzzolin
- Reference count: 40
- Primary result: RS-LLMs achieve 89.60% accuracy on OBQA vs 83.20% for standard models while providing interpretable uncertainty via credal set width

## Executive Summary
This paper introduces Random-Set Large Language Models (RS-LLMs), a novel approach to uncertainty quantification in LLMs by predicting belief functions over token sets instead of probability vectors. The method uses hierarchical clustering to create a budget of focal token subsets, making it computationally scalable while preserving uncertainty representation. RS-LLMs encode epistemic uncertainty via the size and diversity of credal sets associated with predicted belief functions. Evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b, and Phi-2, RS-LLMs outperform standard models in both answer correctness (e.g., 89.60% vs 83.20% accuracy on OBQA) and uncertainty estimation. The approach shows strong potential for detecting hallucinations through credal set width analysis, providing a more interpretable and reliable alternative to traditional probability-based LLM outputs.

## Method Summary
RS-LLMs modify the output layer of standard LLMs to predict belief functions over sets of tokens rather than probability distributions over individual tokens. The method involves hierarchical clustering of token embeddings to create a budget of focal sets (K clusters plus T singletons), resulting in |O| = K + T outputs. During training, the model learns to predict which focal sets contain the correct token using binary cross-entropy loss, with additional regularization terms to ensure belief function validity. At inference, belief functions are converted to pignistic probabilities for token sampling. The approach is evaluated through fine-tuning experiments on CoQA and OBQA datasets with Llama2-7b, Mistral-7b, and Phi-2 base models, using LoRA adapters and 4-bit quantization.

## Key Results
- RS-Llama2 achieves 89.60% accuracy on OBQA vs 83.20% for standard Llama2
- Credal width separation between correct (0.13 ± 0.13) and incorrect (0.28 ± 0.21) context predictions on CoQA
- RS-LLMs show improved uncertainty quantification through better separation of credal widths between correct and incorrect predictions
- Optimal focal set budget K=8,000; K=16,000 degrades performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting belief functions over token sets rather than probability vectors enables explicit encoding of epistemic uncertainty, allowing the model to distinguish between "I don't know which of these is correct" versus "any of these could be valid."
- Mechanism: Instead of outputting a probability distribution P over individual tokens, the model outputs a belief function Bel that assigns belief values to *subsets* of tokens. A belief like Bel({baseball, basketball}) = 1 with singleton beliefs at 0 indicates the model knows the answer is in the set but cannot discriminate further. This provides richer expressivity than standard softmax outputs.
- Core assumption: Token-level uncertainty in language modeling has a meaningful epistemic component that can be captured by set-valued predictions rather than collapsed into point probabilities.
- Evidence anchors:
  - [abstract]: "RS-LLMs encode epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions."
  - [Page 2]: "Allowing the model to predict a belief function... gives the LLM more expressive power to clarify the source of the uncertainty."
  - [corpus]: Related work on epistemic deep learning (arXiv:2510.22261) addresses similar goals of enabling models to "know when they do not know," though via different formalisms.
- Break condition: If belief functions collapse to point probabilities (all mass on singletons), the mechanism provides no advantage over standard LLMs.

### Mechanism 2
- Claim: Hierarchical clustering on token embeddings produces semantically meaningful focal sets, reducing the computational burden from 2^N to tractable K+T outputs while preserving uncertainty representation.
- Mechanism: Token embeddings from a pretrained LLM are clustered hierarchically. The budget O of focal sets combines K clusters (capturing semantic similarity) with T singleton sets (preserving precision). For Llama2-7b with T=32,000 tokens and K=8,000 clusters, this yields 40,000 outputs instead of 2^32,000.
- Core assumption: Tokens that are semantically similar in embedding space are the ones the model should plausibly be confused between; clustering captures the relevant uncertainty structure.
- Evidence anchors:
  - [Page 5]: "The number of clusters K is a hyper-parameter which determines the granularity of the belief function."
  - [Page 13, Appendix A]: Analysis shows focal sets like [cattle, sheep], [shame, pity], [quiet, calm, quietly] with low centroid distances, suggesting semantic coherence.
  - [corpus]: No direct corpus evidence on clustering-for-budgeting in LLMs; this appears novel to the paper.
- Break condition: If clustering produces semantically incoherent sets (e.g., [football, poodle]), the belief function's uncertainty representation becomes meaningless.

### Mechanism 3
- Claim: Credal set width provides a second-order uncertainty signal that correlates with model correctness and hallucination risk.
- Mechanism: Each predicted belief function corresponds to a credal set—a convex set of compatible probability distributions. The width (difference between upper and lower probability bounds) for the predicted token measures epistemic uncertainty. Wider credal sets indicate greater uncertainty.
- Core assumption: Credal width is a meaningful proxy for epistemic uncertainty that can be used downstream for hallucination detection or abstention.
- Evidence anchors:
  - [Page 6, Eq. 14]: "The difference between these bounds, P(t) - P(t), serves as a measure of the size of the credal set."
  - [Page 8, Table 2]: RS-Llama2 shows credal width separation between correct (0.13 ± 0.13) and incorrect context (0.28 ± 0.21) on CoQA.
  - [corpus]: Work on credal networks (arXiv:2507.07619) explores belief function inference in uncertainty propagation, supporting theoretical foundations but not LLM applications.
- Break condition: If credal width does not correlate with prediction quality or hallucination, the uncertainty signal is not actionable.

## Foundational Learning

- Concept: Belief functions and mass functions (Dempster-Shafer theory)
  - Why needed here: RS-LLMs output belief functions; understanding mass-to-belief conversion (Mobius inversion) and validity constraints (non-negative, sum to 1) is essential for interpreting outputs and debugging.
  - Quick check question: Given m({A}) = 0.4, m({B}) = 0.2, m({A,B}) = 0.4, what is Bel({A,B})?

- Concept: Credal sets vs. probability distributions
  - Why needed here: The paper claims credal set width measures epistemic uncertainty. Understanding that a credal set is a *set of distributions* (not a single distribution) clarifies why width captures ambiguity.
  - Quick check question: If a credal set contains only one probability distribution, what does that imply about epistemic uncertainty?

- Concept: Hierarchical clustering and dendrogram cutting
  - Why needed here: The budgeting mechanism requires choosing K (number of clusters). Understanding how cluster granularity affects focal set quality helps tune this hyperparameter.
  - Quick check question: What happens to focal set sizes if you increase K from 4,000 to 16,000?

## Architecture Onboarding

- Component map:
  Input -> Base LLM -> Output head (K+T outputs) -> Belief-to-Mass conversion -> Pignistic probability -> Token sampling

- Critical path:
  1. Pre-compute focal sets: Run hierarchical clustering on token embeddings once; save cluster assignments.
  2. Replace output layer: New linear layer with (T + K) outputs.
  3. Training: Forward pass produces belief predictions; compute loss with Eq. 12 (LBCE + αMr + βMs).
  4. Inference: Convert belief to pignistic probability via Eq. 7; sample or argmax for generation.

- Design tradeoffs:
  - K (focal set budget): Higher K → richer uncertainty representation but larger output head and memory. Paper finds K=8,000 optimal; K=16,000 degraded performance (Table 4).
  - α, β (regularization weights): Control belief function validity. Paper uses α=β=0.01; 0.1 over-constrains, 0.0001 under-constrains (Table 3).
  - Clustering granularity vs. semantic coherence: Agglomerative clustering with average linkage assumed; alternative methods (fuzzy clustering) not explored.

- Failure signatures:
  - Invalid belief functions: Negative mass values or mass sum ≠ 1. Check Mr, Ms losses during training; apply post-processing correction (set negative to 0, add universal set).
  - No uncertainty separation: Credal width distributions overlap for correct/incorrect predictions. May indicate K too small or clustering poor quality.
  - Degraded accuracy vs. baseline: May indicate over-regularization (α, β too high) or output layer initialization issues.

- First 3 experiments:
  1. Verify clustering quality: On a small vocabulary subset (e.g., 1,000 tokens), run clustering and manually inspect focal sets for semantic coherence. Check centroid distances.
  2. Overfit single batch: Train RS-LLM on one batch of 8 examples for 100 steps. Verify loss decreases and predicted beliefs are valid (mass sum ≈ 1, no negative masses).
  3. Compare uncertainty separation: On OBQA validation set, plot credal width distributions for correct vs. incorrect predictions. Verify RS-LLM shows better separation than standard LLM entropy.

## Open Questions the Paper Calls Out

- **Dynamic focal set budgeting**: Can a dynamic strategy adjusting the number of focal elements K based on overlap enhance RS-LLM flexibility?
  - Basis in paper: [explicit] The conclusion identifies manually setting K as a current limitation and states that a dynamic strategy will be "subject to future work."
  - Why unresolved: K is currently a static hyperparameter, which may not optimally represent uncertainty across different contexts or vocabulary sizes.
  - What evidence would resolve it: An adaptive algorithm that varies K per input and demonstrates improved performance or computational efficiency without manual tuning.

- **Alternative clustering algorithms**: Do fuzzy clustering algorithms provide more effective budgeting of focal sets compared to the current hierarchical clustering approach?
  - Basis in paper: [explicit] The conclusion lists "exploration of other clustering algorithms (e.g., fuzzy clustering)" for more effective budgeting as a future direction.
  - Why unresolved: Hierarchical clustering is currently used, but fuzzy clustering might better handle tokens with overlapping semantics or multiple memberships.
  - What evidence would resolve it: Comparative experiments showing fuzzy clustering yields focal sets with lower intra-set variance or higher uncertainty detection accuracy.

- **From-scratch RS-LLM training**: Is it feasible to design a large-scale mathematical framework for training an LLM from scratch using the Random-Set approach?
  - Basis in paper: [explicit] The authors list the "design of a large-scale mathematical framework for training an LLM from scratch" as their "next objective."
  - Why unresolved: The current study modifies only the final layer and uses fine-tuning (LoRA); the behavior of a fully pre-trained RS-LLM is unknown.
  - What evidence would resolve it: A complete pre-training run of an RS-LLM that converges effectively and outperforms standard pre-trained baselines.

- **Set-based vocabulary**: Can random-sets serve as a fundamental alternative to individual tokens by exploiting semantic relations within focal sets?
  - Basis in paper: [explicit] The conclusion suggests investigating the "semantic relation of tokens in a focal set" to potentially propose random-sets as an alternative to tokens.
  - Why unresolved: The vocabulary currently consists of individual tokens; the implications of shifting to a vocabulary based entirely on sets are unexplored.
  - What evidence would resolve it: A theoretical formulation and implementation where the model generates set-based units directly, leading to distinct generative capabilities.

## Limitations

- Hierarchical clustering quality directly determines focal set semantic coherence - poor clustering produces meaningless uncertainty representations
- Computational overhead from 40K+ outputs impacts training efficiency and memory usage
- Performance gains demonstrated only on small datasets (CoQA and OBQA), raising questions about scalability

## Confidence

- **High confidence**: The core mathematical framework (belief functions, credal sets, mass-to-belief conversion) is well-established and correctly applied. The claim that credal set width correlates with prediction quality is strongly supported by experimental results.
- **Medium confidence**: The hierarchical clustering approach for creating focal sets is novel and appears sound, but limited validation of clustering quality introduces uncertainty. Performance improvements are statistically significant but demonstrated on limited datasets.
- **Low confidence**: Long-term practical utility for hallucination detection depends on generalization across domains and model scales. Computational efficiency claims are based on theoretical complexity rather than comprehensive runtime benchmarking.

## Next Checks

1. **Cluster coherence validation**: Systematically evaluate the semantic quality of all generated focal sets by computing average centroid distances within clusters and conducting human evaluation of cluster membership. Identify the threshold of K where clusters become semantically incoherent.

2. **Scalability testing**: Reproduce the OBQA experiment with a 5-10x larger dataset (e.g., Natural Questions or SQuAD) to verify that RS-LLMs maintain their performance advantages while managing computational overhead.

3. **Cross-model generalization**: Apply RS-LLM methodology to larger base models (Llama3-8b, Mistral-7b) and evaluate whether credal width continues to correlate with correctness and whether performance gains scale proportionally.