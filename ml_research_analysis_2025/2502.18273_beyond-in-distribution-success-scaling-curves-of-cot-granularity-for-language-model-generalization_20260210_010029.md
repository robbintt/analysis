---
ver: rpa2
title: 'Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language
  Model Generalization'
arxiv_id: '2502.18273'
source_url: https://arxiv.org/abs/2502.18273
tags:
- generalization
- training
- arxiv
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Chain-of-Thought (CoT) reasoning affects
  out-of-distribution (OOD) generalization in language models for compound tasks.
  The authors conduct controlled experiments showing that QA-trained models achieve
  high in-distribution accuracy but catastrophically degrade in OOD performance, even
  with 10 million training examples.
---

# Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization
## Quick Facts
- arXiv ID: 2502.18273
- Source URL: https://arxiv.org/abs/2502.18273
- Reference count: 40
- Primary result: CoT training improves out-of-distribution generalization while using up to 80% less data

## Executive Summary
This paper investigates how Chain-of-Thought (CoT) reasoning affects out-of-distribution (OOD) generalization in language models for compound tasks. The authors conduct controlled experiments showing that QA-trained models achieve high in-distribution accuracy but catastrophically degrade in OOD performance, even with 10 million training examples. They demonstrate that finer-grained CoT data strongly correlates with better generalization and sample efficiency, with CoT models matching QA performance using up to 80% less data. Theoretically, they prove that compound tasks permit shortcuts in QA data that misalign with true reasoning principles, while CoT forces internalization of valid dependency structures. The analysis shows that transformer positional embeddings amplify generalization by emphasizing subtask condition recurrence in long CoT sequences.

## Method Summary
The authors conduct controlled experiments on two synthetic datasets (Block Towers and Arithmetic Reasoning) using GPT-2 and GPT-3 architectures. They compare QA-trained models against CoT-trained models across varying dataset sizes up to 10 million examples. The experiments systematically vary CoT granularity to test its impact on generalization. The theoretical analysis establishes conditions under which QA data permits shortcut solutions that fail under distributional shift, while CoT data enforces correct reasoning structures. They analyze transformer positional embeddings to show how long CoT sequences create favorable conditions for generalization through subtask condition recurrence.

## Key Results
- QA-trained models show catastrophic OOD performance degradation (0-20% accuracy) despite 10M training examples
- CoT-trained models achieve up to 80% reduction in required training data while matching QA in-distribution performance
- Finer-grained CoT data correlates with improved OOD generalization and sample efficiency
- Transformer positional embeddings amplify generalization by emphasizing subtask condition recurrence in long CoT sequences

## Why This Works (Mechanism)
The mechanism centers on how different training paradigms handle compound tasks under distributional shift. QA training allows models to learn shortcut patterns that work within training distributions but fail when conditions change. These shortcuts exploit correlations in the data without understanding the underlying reasoning principles. CoT training forces the model to internalize the reasoning process itself, creating representations that are invariant to surface-level variations. The theoretical analysis proves that for compound tasks with subtask dependencies, QA data permits solutions that bypass true reasoning, while CoT data constrains the model to learn valid dependency structures. Positional embeddings in transformers further enhance this effect by creating favorable representations when subtask conditions recur throughout long CoT sequences.

## Foundational Learning
- **Compound task structure**: Tasks composed of multiple interdependent subtasks where output depends on subtask conditions
  - Why needed: Understanding task composition reveals why shortcuts fail under distribution shift
  - Quick check: Can the task be decomposed into conditionally dependent subtasks?

- **Shortcut learning**: Models learning patterns that correlate with correct answers without understanding underlying reasoning
  - Why needed: Explains QA models' high in-distribution performance but poor generalization
  - Quick check: Does performance drop significantly under minor input variations?

- **Distributional shift**: Changes in input data distribution between training and test conditions
  - Why needed: The primary threat to real-world deployment of trained models
  - Quick check: Are test conditions systematically different from training conditions?

- **Sample efficiency**: Amount of training data required to achieve target performance
  - Why needed: Practical constraint for real-world applications with limited data
  - Quick check: How does performance scale with training set size?

- **Positional embeddings**: Transformer mechanism for encoding token positions in sequences
  - Why needed: Explains how long CoT sequences create favorable generalization conditions
  - Quick check: Do long sequences with recurring patterns show better generalization?

## Architecture Onboarding
**Component Map:** Data Generation -> Model Training -> Evaluation (In-Distribution and OOD)
**Critical Path:** CoT Granularity Selection -> Training Data Preparation -> Model Training -> Generalization Testing
**Design Tradeoffs:** QA training offers faster convergence but poor generalization; CoT training requires more computation per example but achieves better OOD performance with less total data
**Failure Signatures:** High in-distribution accuracy with catastrophic OOD performance indicates shortcut learning; poor performance on both suggests insufficient model capacity or training
**Three First Experiments:** 1) Train QA and CoT models on small dataset (10K examples) and compare OOD performance; 2) Vary CoT granularity while keeping dataset size constant; 3) Test whether adding noise to training data improves QA model generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to two synthetic datasets and GPT-2/GPT-3 architectures
- Theoretical analysis relies on specific assumptions about task structure and positional embedding behavior
- "80% less data" claim represents in-distribution matching rather than necessarily superior OOD generalization

## Confidence
- High confidence: In-distribution performance comparisons between QA and CoT training, catastrophic OOD performance drop for QA-trained models, correlation between CoT granularity and sample efficiency
- Medium confidence: Theoretical arguments about shortcut vulnerabilities in QA data and role of positional embeddings in generalization
- Medium confidence: Practical recommendation that CoT training is crucial for real-world generalization

## Next Checks
1. Test whether CoT granularity benefits extend to larger models (GPT-4, LLaMA variants) and different architectural families
2. Evaluate findings on human-annotated datasets with naturalistic compound reasoning tasks
3. Design experiments specifically measuring OOD performance improvements with reduced CoT data