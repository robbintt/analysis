---
ver: rpa2
title: Bridging the inference gap in Mutimodal Variational Autoencoders
arxiv_id: '2502.03952'
source_url: https://arxiv.org/abs/2502.03952
tags:
- modalities
- joint
- each
- samples
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes JNF, a novel multimodal variational autoencoder
  that bridges the inference gap in mixture-based models by learning both joint and
  conditional distributions without aggregation. The method consists of two steps:
  first training a joint VAE, then using Normalizing Flows to approximate unimodal
  posteriors.'
---

# Bridging the inference gap in Mutimodal Variational Autoencoders

## Quick Facts
- arXiv ID: 2502.03952
- Source URL: https://arxiv.org/abs/2502.03952
- Reference count: 40
- Key outcome: JNF achieves 0.51 ± 0.02 joint coherence on MNIST-SVHN and outperforms state-of-the-art multimodal VAEs

## Executive Summary
This paper addresses the inference gap problem in multimodal variational autoencoders, where mixture-based aggregation methods create a theoretical lower bound on KL divergence between approximate and true joint posteriors. The proposed JNF method bridges this gap through a two-stage training process: first training a joint VAE, then using Normalizing Flows to approximate unimodal posteriors. An enhanced JNF-Shared variant extracts shared information across modalities via Contrastive Learning or DCCA to further improve conditional generation coherence.

## Method Summary
JNF employs a two-stage training approach: (1) Train a joint VAE with β-weighted ELBO, then freeze all parameters, and (2) Train Normalizing Flows to approximate unimodal posteriors using L_uni loss. For inference on modality subsets, the model combines trained unimodal encoders via Product-of-Experts and samples using Hamiltonian Monte Carlo. The JNF-Shared variant optionally pretrains projectors to extract shared information across modalities before Step 2, simplifying the posterior approximation task.

## Key Results
- Achieves 0.51 ± 0.02 joint coherence on MNIST-SVHN, outperforming state-of-the-art models
- Competitive FID scores across multiple datasets including MNIST-SVHN (0.83 ± 0.04), PolyMNIST (1.39 ± 0.06), and MHD (0.64 ± 0.05)
- Demonstrates superior conditional generation coherence, with JNF-Shared reaching 0.75 coherence for MNIST→SVHN translation

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training Decomposition
Separating joint generative model training from posterior approximation avoids the theoretical inference gap inherent to mixture-based multimodal VAEs. By training the joint encoder first and freezing it, then fitting unimodal encoders separately via L_uni, the model avoids binding these objectives together. The PoE combination for subset posteriors is applied only at inference time, where the gap doesn't apply.

### Mechanism 2: Normalizing Flows for Expressive Unimodal Posteriors
Masked Autoregressive Flows enable q_φj(z|xj) to approximate complex, multi-modal posteriors that Gaussian encoders cannot capture. Starting from a base Gaussian, K flow transformations progressively increase distribution complexity, allowing the unimodal encoder to match the true posterior's shape rather than being forced into a unimodal Gaussian.

### Mechanism 3: Shared Information Projection
Pre-extracting shared information g_j(x_j) via Contrastive Learning or DCCA simplifies the posterior p_θ(z|g_j(x_j)) relative to p_θ(z|x_j), improving conditional generation coherence. Projectors trained to maximize similarity or correlation across modalities extract semantic content while discarding modality-specific noise, making the unimodal posterior approximation task easier.

## Foundational Learning

- **Concept: Variational Inference and the ELBO**
  - Why needed here: The entire framework builds on maximizing a tractable lower bound on log-likelihood. Understanding the decomposition into reconstruction + KL terms is essential to see why mixture aggregation creates problems.
  - Quick check question: Can you explain why minimizing KL(q_φ(z|X)||p_θ(z|X)) brings the approximate posterior closer to the true posterior, and why a positive lower bound on this KL is problematic?

- **Concept: Normalizing Flows**
  - Why needed here: The paper's key innovation is using flows to model complex posteriors. You need to understand how invertible transformations with tractable Jacobians enable density estimation and why autoregressive flows are expressive.
  - Quick check question: Given a base distribution q(z) and transformation z' = f(z), write the density q'(z'). Why does stacking multiple transformations increase expressivity?

- **Concept: Product-of-Experts for Subset Posteriors**
  - Why needed here: At inference time, the model combines trained unimodal encoders via PoE to handle arbitrary subsets of observed modalities. Understanding why this works—and why it doesn't suffer the same gap as PoE aggregation during training—is critical.
  - Quick check question: Derive the PoE combination from the conditional independence assumption. Why can HMC sample from this distribution despite the intractable normalizing constant?

## Architecture Onboarding

- **Component map:**
  ```
  Input X = (x₁, ..., x_M) 
      ↓
  Step 1: Joint Encoder q_φ(z|X) → Gaussian parameters [μ, Σ]
      ↓                                   ↓
  Latent z ~ q_φ(z|X)              Decoders p_θ(xⱼ|z) → reconstruction
      ↓                                   ↑
  ELBO optimization (β-weighted)        |
      ↓                                   |
  Freeze all parameters                   |
      ↓                                   |
  Step 2: Unimodal Encoders               |
      Option A: q_φⱼ(z|xⱼ) via NF          |
      Option B: gⱼ(xⱼ) → q_φⱼ(z|gⱼ(xⱼ)) via NF
      ↓
  L_uni optimization (Eq. 7/15)
      ↓
  Inference: PoE combination + HMC sampling (Eq. 12)
  ```

- **Critical path:**
  1. Step 1 training quality: If the joint VAE doesn't learn meaningful structure, downstream everything fails. Monitor reconstruction quality and latent space organization.
  2. Flow capacity: Too few flow steps → underfitting; too many → overfitting/instability. Tables D.3-D.5 show β and n_flows interact; tune jointly.
  3. Projector pretraining (JNF-Shared only): Must complete before Step 2. CL typically outperforms DCCA on image datasets, but verify with validation metrics.

- **Design tradeoffs:**
  - JNF vs. JNF-Shared: JNF-Shared improves conditional coherence but requires pretraining projectors and assumes shared content exists. Use JNF for heterogeneous modalities where shared semantics are unclear.
  - β selection: Higher β improves joint coherence but degrades conditional coherence. Paper recommends maximizing average coherence; for cross-modal generation tasks, prioritize conditional metrics.
  - Encoder architecture: Paper uses separate modality heads + merging MLP. For many modalities, watch for gradient conflict; concatenation may help.

- **Failure signatures:**
  - Joint VAE collapse: Latent space shows no class structure. Reconstruction loss plateaus high.
  - Flow underfitting: Generated samples lack diversity; q_φⱼ has too-large variance. Increase number of MADE blocks.
  - Projector failure: CL projector accuracy low; JNF-Shared performs worse than JNF.
  - HMC sampling issues: High rejection rate or slow mixing. Tune step size ε and leapfrog steps l.

- **First 3 experiments:**
  1. Reproduce toy dataset visualization: Train joint VAE on circle-square data, visualize 2D latent space. Then train unimodal encoder with/without flows, compare generated samples.
  2. Ablate flow depth: On MNIST-SVHN, compare K∈{1,2,3,4} MADE blocks. Measure coherence and FID.
  3. Compare projector methods: On a dataset with known shared content, train JNF-Shared with CL vs. DCCA vs. random projection baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can the low coherence observed during unconditional generation from the prior be systematically addressed by fitting a distribution on latent embeddings post-training? The authors identify this failure mode on TranslatedPolyMNIST but only suggest post-hoc latent distribution fitting as a possibility without implementing or validating it.

### Open Question 2
Does integrating more expressive posterior/prior distributions or tighter variational bounds into JNF's first training step further improve joint generative modeling? The authors use a basic joint VAE in Step 1 but acknowledge that IWAE bounds, vamp priors, or inverse autoregressive flows remain unexplored within their framework.

### Open Question 3
What principled criteria determine whether DCCA or Contrastive Learning better extracts shared information for a given multimodal dataset? The authors report that CL outperforms DCCA on MNIST-SVHN while both perform comparably on PolyMNIST, yet offer no theoretical guidance for selecting between them.

## Limitations
- The theoretical inference gap is well-defined but its practical impact beyond toy examples remains somewhat heuristic
- Flow effectiveness depends heavily on posterior geometry; the paper doesn't systematically explore when K=2-3 MADE blocks become insufficient
- JNF-Shared assumes meaningful shared information exists; the paper doesn't validate this assumption across truly heterogeneous modalities

## Confidence
- **High confidence:** Two-stage training decomposition successfully avoids mixture-based aggregation problems (supported by consistent improvements across all datasets)
- **Medium confidence:** Normalizing Flows sufficiently approximate complex posteriors (well-supported on tested datasets but not systematically validated for all posterior shapes)
- **Medium confidence:** Shared information projection improves conditional coherence (demonstrated on MNIST-SVHN but limited validation on heterogeneous modalities)

## Next Checks
1. Systematically vary flow depth K and evaluate saturation points on datasets with known multi-modal posteriors
2. Test JNF-Shared on modalities with minimal shared semantic content (e.g., medical imaging + genomic data) to validate projector effectiveness
3. Implement an ablation where Step 1 joint VAE is deliberately under-trained to quantify sensitivity to Step 1 quality