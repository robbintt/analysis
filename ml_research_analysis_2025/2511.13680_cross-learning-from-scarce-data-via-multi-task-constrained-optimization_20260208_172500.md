---
ver: rpa2
title: Cross-Learning from Scarce Data via Multi-Task Constrained Optimization
arxiv_id: '2511.13680'
source_url: https://arxiv.org/abs/2511.13680
tags:
- cross-learning
- data
- which
- learning
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces cross-learning, a multi-task constrained
  optimization framework designed to overcome data scarcity by jointly estimating
  deterministic parameters across multiple related tasks. The method balances the
  bias-variance trade-off by imposing constraints that keep task-specific model parameters
  close to each other while still allowing them to differ, thereby combining information
  from multiple data sources.
---

# Cross-Learning from Scarce Data via Multi-Task Constrained Optimization

## Quick Facts
- **arXiv ID:** 2511.13680
- **Source URL:** https://arxiv.org/abs/2511.13680
- **Reference count:** 34
- **Primary result:** Cross-learning achieves 0.07% peak prediction error on COVID-19 data vs 76.38% (consensus) and 2474% (separate), and 44.5% accuracy on Office-Home image classification vs 33.97% (consensus) and 24.44% (separate)

## Executive Summary
This paper introduces cross-learning, a multi-task constrained optimization framework designed to overcome data scarcity by jointly estimating deterministic parameters across multiple related tasks. The method balances the bias-variance trade-off by imposing constraints that keep task-specific model parameters close to each other while still allowing them to differ, thereby combining information from multiple data sources. The core idea is to introduce a task-specific parameter for each task along with a global parameter shared across all tasks, and enforce a constraint on the closeness between task-specific and global parameters.

The authors prove theoretical guarantees showing that cross-learning outperforms both separate and consensus approaches in terms of mean-squared error under Gaussian noise assumptions. They also extend the method to support arbitrary functional constraints on model outputs rather than parameters, which is particularly useful for neural networks. Experiments demonstrate effectiveness across two applications: fitting an SIR model for COVID-19 data and image classification on the Office-Home dataset.

## Method Summary
Cross-learning addresses data scarcity through a multi-task constrained optimization framework that introduces both task-specific parameters θ_t and a global parameter θ_g for each of T related tasks. The method enforces a constraint ||θ_t - θ_g|| ≤ ε that interpolates between fully separate estimation (ε → ∞) and strict consensus (ε = 0). The optimization can be solved using ADMM for parametric constraints or primal-dual updates for functional constraints on model outputs. The global parameter aggregates information from all tasks and transfers it to data-scarce tasks through the constraint, creating a coupling that allows data-rich tasks to regularize data-scarce ones. The centrality parameter ε controls the bias-variance trade-off, with intermediate values balancing the high variance of separate estimation and high bias of consensus.

## Key Results
- COVID-19 SIR model fitting: Cross-learning achieved 0.07% peak prediction error compared to 76.38% for consensus and 2474% for separate estimation
- Office-Home image classification: Cross-learning achieved 44.5% accuracy compared to 33.97% for consensus and 24.44% for separate training
- Theoretical guarantee: Cross-learning strictly outperforms both separate and consensus estimation in mean-squared error under Gaussian noise assumptions

## Why This Works (Mechanism)

### Mechanism 1: Bias-Variance Trade-off via Centrality Constraint
The centrality parameter ε controls the bias-variance trade-off. When ε → 0, all task parameters are forced equal (consensus, low variance but high bias). When ε → ∞, parameters are unconstrained (separate, low bias but high variance). Intermediate ε values balance both. The core assumption is tasks are related but not identical—their true parameters θ*_t are close but not equal. This is supported by Theorem 1 proving strict inequalities: E[inf_ε E_CL(ε) - E_C] < 0 and E[inf_ε E_CL(ε) - E_S] < 0. Break condition: When tasks are truly unrelated (θ*_t very far apart), any ε < ∞ introduces harmful bias.

### Mechanism 2: Information Transfer via Global Parameter Coupling
Each task-specific parameter θ_t is constrained to remain within distance ε of the global parameter θ_g. This creates a coupling that allows data-rich tasks to influence θ_g, which in turn regularizes θ_t for data-scarce tasks through the projection operation. The core assumption is at least some tasks have sufficient data to produce reliable estimates that can inform the global parameter. This is supported by Lemma 2 showing θ†_t is the projection of θ̂_t onto the ball B(θ†_g, ε). Break condition: When all tasks are equally data-scarce, there is no reliable source of information to aggregate.

### Mechanism 3: Output-Level Regularization for Neural Networks
Constraining model outputs rather than parameters provides more effective regularization for neural networks where parameter proximity doesn't guarantee functional similarity. The functional constraint (1/N_t)∑|f(x_i, θ_t) - f(x_i, θ_g)| ≤ ε is a relaxation of parametric constraints, allowing greater parameter variability while maintaining output consistency. The core assumption is the model f(x, θ) is L-Lipschitz in θ. This is supported by experimental results showing output constraints achieve 44.5% accuracy vs. 33.97% for consensus and 24.44% for separate. Break condition: When tasks require fundamentally different functional behaviors, output constraints may be too restrictive.

## Foundational Learning

- **Concept: Lagrangian Duality and ADMM**
  - **Why needed here:** The cross-learning optimization problem is solved in the dual domain. Understanding Lagrangian multipliers (λ) as "prices" for constraint violations and ADMM as alternating between primal updates (model fitting) and dual updates (constraint enforcement) is essential.
  - **Quick check question:** Can you explain why the dual variable λ_t being zero indicates the constraint is inactive, and why larger λ_t indicates harder constraint satisfaction?

- **Concept: Projection onto Convex Sets**
  - **Why needed here:** Lemma 2 shows the cross-learning estimate θ†_t is a projection onto a ball. Understanding projections as "finding the closest point in the feasible set" explains how constraints regularize estimates.
  - **Quick check question:** Given a point outside a ball of radius ε around center c, where does its projection lie? What does this tell you about how cross-learning modifies task-specific estimates?

- **Concept: Bias-Variance Decomposition**
  - **Why needed here:** The entire theoretical contribution rests on understanding how separate estimation (high variance, low bias) and consensus (low variance, high bias) trade off, and how cross-learning navigates this.
  - **Quick check question:** For the Gaussian noise model y = θ* + η with η ~ N(0, σ²), what happens to bias and variance of the consensus estimator as the true parameters θ*_t become more spread out?

## Architecture Onboarding

- **Component map:**
  - Task-specific models: f(x, θ_t) for each task t ∈ {1,...,T} with parameters θ_t
  - Global model: f(x, θ_g) serving as the shared reference point
  - Constraint layer: Enforces ||θ_t - θ_g|| ≤ ε (parametric) or output-level constraints
  - Dual variables: λ_t track constraint violations (Algorithm 2) or embedded in ADMM structure (Algorithm 1)
  - Loss functions: Task-specific losses ℓ(y_i, f(x_i, θ_t)) combined with constraint penalties

- **Critical path:**
  1. Identify related tasks and collect datasets D_t for each
  2. Choose constraint type: parametric (Algorithm 1, ADMM) for interpretable models; functional (Algorithm 2, primal-dual) for neural networks
  3. Initialize θ_t, θ_g, and dual variables
  4. Run optimization: alternate between task-specific updates, global parameter updates, and dual updates
  5. Tune ε via cross-validation

- **Design tradeoffs:**
  - Parametric vs. functional constraints: Parametric is simpler (closed-form projections) but less effective for neural networks; functional is more flexible but requires more computation
  - ε selection: Small ε → consensus-like behavior (risk: underfitting individual tasks); large ε → separate estimation (risk: overfitting); optimal ε lies in between
  - Centralized vs. distributed: Current formulation is centralized; distributed variants would require communication of θ_g and possibly dual variables

- **Failure signatures:**
  - All λ_t = 0 throughout training: Constraints are never active—ε is too large, reduce it
  - λ_t growing unboundedly: Constraints are infeasible—ε is too small or tasks are fundamentally incompatible
  - Performance worse than consensus: Tasks may be unrelated; verify task relatedness with baseline separate vs. consensus comparison first
  - High variance in results across runs: Initialization sensitivity; try multiple random seeds and average

- **First 3 experiments:**
  1. **Baseline establishment:** On your dataset, train fully separate models and a consensus model. If consensus doesn't outperform separate on any task, tasks may be too unrelated for cross-learning to help.
  2. **ε sensitivity sweep:** For a fixed held-out task with limited data, sweep ε ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 1.0} and plot performance. Identify the ε range where cross-learning outperforms both baselines.
  3. **Ablation on data scarcity:** Systematically reduce training data for one target task (e.g., 100%, 50%, 25%, 10% of original) while keeping auxiliary tasks fixed. Cross-learning should show increasing advantage as data becomes scarcer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can formal performance guarantees be established for the cross-learning formulation with functional constraints (P CLF)?
- **Basis in paper:** The authors state regarding the functional formulation: "While we will not provide a formal analysis of this case, we claim that this cross-learning formulation (P CLF) with functional constraints comes to also exploit the bias-variance trade-off."
- **Why unresolved:** The theoretical analysis (Theorem 1) is derived strictly for the parametric constraint formulation (P CL) under Gaussian noise. The functional variant is supported only empirically and by a relaxation argument (Proposition 4).
- **What evidence would resolve it:** A proof extending the Mean Squared Error (MSE) improvement guarantees of Theorem 1 to the functional constraint space, or a counter-example showing where functional constraints fail to improve upon consensus/separate estimators.

### Open Question 2
- **Question:** Can the centrality parameter $\epsilon$ be optimally selected in an unsupervised manner without relying on ground-truth validation data?
- **Basis in paper:** Theorem 1 proves that an optimal $\epsilon$ exists ($\inf_{\epsilon > 0}$), but the experimental evaluation relies on ablation studies where performance is measured against test data to find the "elbow" or peak accuracy.
- **Why unresolved:** The paper demonstrates sensitivity to $\epsilon$, but provides no algorithmic mechanism to select $\epsilon$ based solely on the training distribution or task covariance.
- **What evidence would resolve it:** An algorithm or heuristic that estimates the optimal $\epsilon$ from training statistics that achieves performance comparable to the oracle selection used in the paper.

### Open Question 3
- **Question:** Do the strict MSE improvement guarantees hold for non-Gaussian noise distributions or non-convex loss landscapes?
- **Basis in paper:** The authors note that their theoretical findings were "formally established within a controlled framework with Gaussian data" and rely on specific properties of this distribution for the proofs.
- **Why unresolved:** Real-world data rarely fits perfect Gaussian assumptions, yet the theoretical guarantees are restricted to this model.
- **What evidence would resolve it:** An extension of Proposition 1 and Theorem 1 to sub-Gaussian or heavy-tailed distributions, demonstrating that the derivative of the error difference remains strictly negative under these broader conditions.

## Limitations
- The effectiveness depends critically on tasks being related but not identical, a condition that is assumed but not always verifiable a priori
- The choice of ε is problem-dependent and requires cross-validation, though the method appears robust to ε variations
- The functional constraint extension assumes L-Lipschitz continuity, which may not hold for all neural network architectures
- The COVID-19 application assumes SIR model validity, while the image classification application assumes the AlexNet backbone is appropriate for the task

## Confidence
- **High confidence:** The theoretical framework for cross-learning under parametric constraints (Theorem 1 and Lemma 2) is rigorous and well-established in optimization theory
- **Medium confidence:** The extension to functional constraints is theoretically sound but relies on Lipschitz assumptions that may be violated in practice
- **Medium confidence:** Empirical results are compelling but evaluated on two specific applications; generalizability to other domains requires further validation

## Next Checks
1. **Task relatedness diagnostic:** Before applying cross-learning, verify that separate and consensus baselines show distinct performance. If they don't differ significantly, tasks may be too unrelated for cross-learning to help.
2. **ε sensitivity verification:** Systematically sweep ε values on a held-out task and confirm that cross-learning performance peaks at intermediate values, validating the bias-variance trade-off mechanism.
3. **Data scarcity progression:** Reduce training data for one task incrementally (100%→50%→25%→10%) while keeping other tasks fixed. Cross-learning should show increasing advantage as data becomes scarcer, matching the pattern in Figure 8.