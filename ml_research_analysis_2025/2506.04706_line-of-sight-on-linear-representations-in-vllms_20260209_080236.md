---
ver: rpa2
title: 'Line of Sight: On Linear Representations in VLLMs'
arxiv_id: '2506.04706'
source_url: https://arxiv.org/abs/2506.04706
tags:
- image
- steering
- features
- linear
- find
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how vision-language models (VLLMs) represent
  images internally by examining LLaVA-Next, a popular open-source model. The authors
  find that ImageNet classes are represented via linearly decodable features in the
  residual stream, and demonstrate that these features are causally linked to model
  outputs through targeted steering interventions.
---

# Line of Sight: On Linear Representations in VLLMs

## Quick Facts
- arXiv ID: 2506.04706
- Source URL: https://arxiv.org/abs/2506.04706
- Reference count: 40
- Primary result: ImageNet classes are represented via linearly decodable features in VLLM residual streams, and steering interventions based on these features causally edit model outputs.

## Executive Summary
This paper investigates how vision-language models represent images internally by examining LLaVA-Next, a popular open-source model. The authors find that ImageNet classes are represented via linearly decodable features in the residual stream, and demonstrate that these features are causally linked to model outputs through targeted steering interventions. They train multimodal Sparse Autoencoders (SAEs) to discover interpretable features, finding that although text and image representations are initially disjoint, they become increasingly shared in deeper layers. The SAEs successfully approximate steering vectors with as few as 20 features, and manual evaluation confirms high interpretability and growing multimodality across layers. These results provide insights into VLLM representations and suggest SAEs as a general tool for understanding visual representations in such models.

## Method Summary
The authors analyze LLaVA-Next 7b by training linear probes on mean-pooled image token activations to classify ImageNet classes, demonstrating competitive accuracy with self-supervised learning methods. They implement contrastive activation addition (CAA) to compute steering vectors from class contrasts and apply these to image tokens, showing causal effects on model outputs. Multimodal SAEs are trained on 1.2M captioned images from ShareGPT4V, decomposing residual stream activations into sparse, interpretable features. The SAEs approximate steering vectors with ~20 features while preserving most efficacy, and manual evaluation reveals increasing multimodality across layers.

## Key Results
- ImageNet classes are linearly decodable from residual stream activations with accuracy competitive with DINO and CLIP
- Steering vectors derived from class contrasts causally edit model outputs when applied to image tokens at layers 8-12
- SAEs trained on joint text-image data recover interpretable features that increasingly merge modalities in deeper layers
- ~20 SAE features can approximate steering vectors while maintaining most causal efficacy

## Why This Works (Mechanism)

### Mechanism 1
ImageNet class information is encoded as linear features in the VLLM residual stream and can be decoded via linear probes. Mean-pooled image token activations form a representation from which a single linear layer can classify ImageNet classes with accuracy competitive with SSL methods. The assumption is that image semantics are approximately linearly decodable.

### Mechanism 2
Steering vectors derived from image-class contrasts causally edit model outputs when applied to image tokens at early-to-mid layers. Contrastive Activation Addition (CAA) computes steering as mean(positive class activations) minus mean(negative class activations). Adding this vector to image tokens shifts outputs toward the positive class while preserving coherence.

### Mechanism 3
Sparse Autoencoders trained on joint text-image data recover monosemantic features that increasingly merge modalities in deeper layers and can approximate steering vectors with ~20 features. SAEs decompose residual stream activations into sparse codes via ReLU+L1 training, with the decoder dictionary spanning interpretable directions.

## Foundational Learning

**Concept: Residual Stream**
- Why needed here: All probing, steering, and SAE analysis operates on residual stream activations; understanding it as the "communication bus" between transformer layers is essential.
- Quick check question: Can you explain why intervening on the residual stream at layer 8 affects generation differently than intervening at layer 20?

**Concept: Linear Probes**
- Why needed here: Probes are used to test whether class information is linearly decodable; this establishes the "legibility" of representations before any causal claims.
- Quick check question: If a linear probe achieves 70% accuracy but steering with the probe direction has no effect, what might you conclude?

**Concept: Sparse Autoencoders (SAEs)**
- Why needed here: SAEs decompose polysemantic activations into interpretable sparse features; they are the main tool for unsupervised feature discovery in this work.
- Quick check question: What does "L0 ≈ 5" mean in terms of how many SAE features are active per token?

## Architecture Onboarding

**Component map:**
CLIP encoder -> MLP projection -> Vicuna-7B decoder -> Residual stream

**Critical path:**
1. Image → CLIP → MLP projection → image tokens injected into decoder
2. Image information flows through early-to-mid layers (≈8–12); steering/ablations are most impactful here
3. After layer ~12, image token information is largely "transferred"; late-layer ablations have minimal effect

**Design tradeoffs:**
- SAE expansion factor (8x): Larger dictionaries increase interpretability granularity but cost more memory/compute
- Dataset balance: ShareGPT4V provides 1.2M captioned images; text/image token ratio affects which modality dominates learned features
- Steering strength vs. coherence: Higher steering magnitude increases class shift but risks incoherence; the Pareto frontier guides optimal strength

**Failure signatures:**
- Text-derived steering fails: Logit Lens and Text Embedding vectors perform no better than random for image steering—likely because they target layers where image information is no longer used
- SAE on all tokens degrades QA: Splicing SAE reconstructions on all tokens (vs. image-only) drops MMMU/AOKVQA to near-chance due to distribution shift between caption training and QA evaluation
- Adversarial attacks are dense: Unlike steering vectors, adversarial perturbations involve hundreds of SAE features with no clear semantic pattern

**First 3 experiments:**
1. Reproduce ImageNet probe accuracy: Train linear probes on mean image token activations at layers 8, 12, 16, 20; compare to CLIP/DINO baselines using the paper's evaluation protocol
2. Steering with CAA on 5 class pairs: Implement contrastive activation addition; sweep steering strengths at layer 10; plot coherence vs. steering score using the GPT-4o-mini judge prompts from Appendix A
3. Train SAE at layer 12: Use the paper's hyperparameters (8x expansion, L0 target ~5, 1.5B tokens from ShareGPT4V); evaluate loss recovered and manually inspect 20 random features for multimodality

## Open Questions the Paper Calls Out

**Open Question 1**
Do linear representations and the layer-wise progression from disjoint to shared multimodal features generalize to cross-attention-based VLLM architectures? This study only examined LLaVA-Next, a decoder-only model with concatenated image tokens; cross-attention architectures process image-text interactions fundamentally differently.

**Open Question 2**
What are the precise computational mechanisms that convert image-only representations into multimodal shared representations during the forward pass? The paper documents that multimodality increases across layers and identifies layer ~12 as the information transfer point, but does not identify the specific circuits or attention heads responsible.

**Open Question 3**
Why are adversarial perturbations expressed as dense, non-sparse changes across hundreds of SAE features rather than sparse, interpretable feature modifications? The paper expected adversarial attacks to be captured by the SAE basis but found the affected features appeared random and uncorrelated with the semantic change.

## Limitations
- SAE interpretability claims are limited by scale of manual evaluation (only 20 features per layer examined)
- Distribution shift exists between SAE training on captioning data and application to VQA tasks
- Steering causality is demonstrated only for early-to-mid layers where image information is actively used

## Confidence
**High confidence:** Linear decodability of ImageNet classes, causal nature of steering vectors, general SAE architecture and training methodology
**Medium confidence:** Interpretability of SAE features, approximation of steering vectors with ~20 features, multimodal emergence across layers
**Low confidence:** Generalizability of SAE-based interpretability across VLLM architectures, SAEs as a "general tool" for understanding visual representations

## Next Checks
1. Comprehensive SAE feature interpretability analysis: Expand manual evaluation to systematic sampling across the entire dictionary with automated metrics
2. Cross-architecture generalization study: Apply same methodology to at least two other VLLM architectures using identical hyperparameters
3. Extended steering intervention analysis: Systematically explore steering effectiveness across all layers and test composition of steering vectors for precise control