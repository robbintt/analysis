---
ver: rpa2
title: Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for
  Training of Partial Bayesian Neural Networks
arxiv_id: '2505.03797'
source_url: https://arxiv.org/abs/2505.03797
tags:
- ohsmc
- neural
- stochastic
- monte
- carlo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GOHSMC LD, a new SMC-based training method
  for partial Bayesian neural networks (pBNNs) that utilizes gradient-based proposals
  and guided sampling to better handle high-dimensional parameter spaces. The method
  builds on the open-horizon SMC framework by incorporating unadjusted Langevin dynamics
  as Markov kernels and a forward proposal L-kernel to improve statistical efficiency.
---

# Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2505.03797
- Source URL: https://arxiv.org/abs/2505.03797
- Authors: Andrew Millard; Joshua Murphy; Simon Maskell; Zheng Zhao
- Reference count: 0
- Key outcome: Gradient-based SMC with Langevin dynamics outperforms state-of-the-art methods for training partial Bayesian neural networks on UCI regression datasets.

## Executive Summary
This paper introduces GOHSMC LD, a new SMC-based training method for partial Bayesian neural networks (pBNNs) that utilizes gradient-based proposals and guided sampling to better handle high-dimensional parameter spaces. The method builds on the open-horizon SMC framework by incorporating unadjusted Langevin dynamics as Markov kernels and a forward proposal L-kernel to improve statistical efficiency. Experiments on six UCI regression datasets show GOHSMC LD outperforms state-of-the-art methods in terms of root mean square error, R², negative log-likelihood, and continuous ranked probability score across most datasets. The method demonstrates particular advantages when the dimensionality of stochastic parameters is high, while also maintaining good performance with larger batch sizes that significantly reduce training times.

## Method Summary
GOHSMC LD combines sequential Monte Carlo sampling with gradient-based proposals for training pBNNs where only the first layer is stochastic. The algorithm maintains a particle approximation of the posterior distribution using unadjusted Langevin dynamics (LD) as proposals, with a forward-proposal L-kernel enabling tractable weight updates. Deterministic parameters are updated via stochastic gradient ascent using a weighted gradient estimate from normalized SMC weights. The method interleaves SMC updates for stochastic parameters with mini-batch gradient updates for deterministic parameters, with resampling triggered when effective sample size drops below half the particle count.

## Key Results
- GOHSMC LD outperforms state-of-the-art methods (VI, SGHMC, SWAG, SVGD) on 6 UCI regression datasets across most metrics
- Method demonstrates particular advantages in high-dimensional parameter spaces where random-walk proposals struggle
- Contrary to typical SGHMC behavior, larger batch sizes improve both training efficiency and predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based proposals (unadjusted Langevin Dynamics) enable more efficient exploration of high-dimensional parameter spaces than random-walk proposals.
- Mechanism: The proposal uses the gradient of the log-probability to move particles toward higher-density regions. A forward-proposal (FP) L-kernel yields a tractable weight update that corrects for the proposal not leaving the target invariant.
- Core assumption: Gradients provide a useful signal, and the posterior geometry is sufficiently smooth for LD moves to be effective.
- Evidence anchors:
  - [abstract] "...utilising a guided proposal and incorporating gradient-based Markov kernels, which gives us better scalability on high dimensional problems."
  - [section 2.2] Contrasts GOHSMC LD with OHSMC RW; derives LD steps (Eq. 8–14) and the FP L-kernel weight update (Eq. 20).
  - [corpus] "Incorporating the ChEES Criterion into Sequential Monte Carlo Samplers" discusses gradient-based proposals (e.g., HMC) in SMC.
- Break condition: Highly multimodal/disconnected posteriors or extremely noisy/vanishing gradients may cause poor mixing or particle collapse.

### Mechanism 2
- Claim: Jointly estimating deterministic parameters (ψ) via stochastic gradient ascent and the posterior of stochastic parameters (θ) via SMC improves training efficiency and predictive performance for pBNNs.
- Mechanism: The algorithm interleaves SMC updates for θ with mini-batch stochastic gradient updates for ψ. Normalized SMC weights provide a weighted gradient estimate for ψ (Eq. 7). Posterior from the previous iteration warm-starts the next SMC step.
- Core assumption: Stochastic parameters capture primary model uncertainty; mini-batch approximations are sufficiently accurate for both posterior and gradient estimation.
- Evidence anchors:
  - [abstract] "We show that our new method outperforms the state-of-the-art in terms of predictive performance and optimal loss."
  - [section 2.1] Describes SGSMC/OHSMC interleaved loops; Eq. 7 defines the weighted gradient; Algorithm 1 outlines the full procedure.
  - [corpus] "Reinforced sequential Monte Carlo for amortised sampling" links SMC to sequential training; "Humble your Overconfident Networks..." combines SGHMC proposals with SMC for scalable training.
- Break condition: High variance in particle weights or poor particle diversity can destabilize ψ updates; very small mini-batches may bias posterior and gradient estimates.

### Mechanism 3
- Claim: The FP L-kernel for unadjusted LD yields a simple, tractable weight update essential for algorithm correctness and performance.
- Mechanism: Exploiting reversibility of the LD step (including a completed momentum update), the Jacobian terms cancel, leaving a weight update that depends only on the target probability ratio and momentum distributions (Eq. 20).
- Core assumption: Reversibility of the numerical LD step holds sufficiently well for the cancellation to be valid.
- Evidence anchors:
  - [abstract] "...using unadjusted Langevin dynamics as proposals with forward-proposal L-kernels..."
  - [section 2.2] "...completing the momentum update facilitates a cancellation in the determinants when using the so-called forwards-proposal (FP) L-kernel thanks to the reversibility of the LD process".
  - [corpus] "Hess-MC2..." discusses advanced proposals in SMC; corpus evidence for FP L-kernel specifically is weak/absent.
- Break condition: Large step sizes (ϵ) or inaccurate integration can break reversibility, leading to biased weights and posterior approximation.

## Foundational Learning

- **Sequential Monte Carlo (SMC) Samplers**:
  - Why needed here: Core inference engine that maintains a weighted particle approximation of a sequence of distributions, culminating in the posterior.
  - Quick check question: How do the particles and weights in an SMC sampler approximate a target distribution, and what triggers resampling?

- **Unadjusted Langevin Dynamics (LD)**:
  - Why needed here: The specific gradient-based Markov kernel used as the proposal to explore the parameter space efficiently.
  - Quick check question: How does unadjusted LD use the gradient of the log-probability to propose a new state, and why is it "unadjusted"?

- **Partial Bayesian Neural Networks (pBNNs)**:
  - Why needed here: The model architecture where only a subset of parameters (θ) are treated as stochastic, while the rest (ψ) are deterministic.
  - Quick check question: In a pBNN, which parameters are assigned a probability distribution, and how does this differ from a fully Bayesian neural network?

## Architecture Onboarding

- **Component map**:
  Particles -> LD Proposal -> Weight Update -> Resampler -> Weighted Gradient -> SGD Optimizer -> Deterministic Parameters

- **Critical path**:
  1.  Initialize `J` particles from the prior `q_0(θ)` and compute initial weights.
  2.  Loop over training iterations:
      a.  Draw a mini-batch of data.
      b.  If `J_eff < J/2`, resample particles and reset weights to `1/J`.
      c.  For each particle `j`:
          i.  Propagate `θ_{t-1}^(j)` to `θ_t^(j)` using the LD kernel.
          ii. Update weight `w_t^(j)` using Eq. 20.
      d.  Normalize all weights.
      e.  Compute the weighted gradient `g(ψ_{t-1})` using Eq. 7.
      f.  Update deterministic parameters `ψ_t = ψ_{t-1} + ϵ g(ψ_{t-1})`.

- **Design tradeoffs**:
  - **Batch Size (`M`)**: Paper claims pBNNs scale well to larger batch sizes, reducing training time and improving performance. Counter to typical SGHMC guidance.
  - **Number of Particles (`J`)**: More particles improve posterior approximation but increase computational and memory costs linearly.
  - **Proposal Step Size (`ϵ`)**: A large `ϵ` speeds up exploration but risks breaking the reversibility assumption and causing numerical instability in the LD and weight updates.
  - **Stochastic Layer Placement**: Paper cites prior work suggesting placing stochasticity on the first layer is empirically best, but this is a design choice that impacts performance.

- **Failure signatures**:
  - **Particle Collapse/Weight Degeneracy**: `J_eff` consistently far below `J/2` after weight updates, resulting in a posterior approximation dominated by a single particle. Check proposal scale and target distribution.
  - **Divergence in ψ Optimization**: Loss for deterministic parameters increases or oscillates violently. May be caused by high variance in the weighted gradient estimate.
  - **Poor Predictive Performance (High RMSE/NLL)**: Model fails to generalize or provide good uncertainty estimates, suggesting the SMC sampler is not finding a good posterior mode or is exploring a flat region.

- **First 3 experiments**:
  1.  **Sanity Check - Synthetic Regression**: Train a small pBNN on a 1D regression problem with known heteroscedastic noise. Visualize particle trajectories and the resulting predictive mean and uncertainty bands. This confirms the SMC loop and gradient proposals are working as intended.
  2.  **Baseline Comparison - UCI Dataset**: Replicate one of the paper's experiments (e.g., Yacht Hydrodynamics). Compare performance (RMSE, NLL) against the reported GOHSMC LD results and a simpler baseline like OHSMC RW to validate the implementation.
  3.  **Ablation Study - Batch Size Impact**: Systematically vary the batch size (`M`) and measure its effect on convergence speed, training time, and final performance metrics (RMSE, NLL) to test the paper's claim about scaling with batch size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Hamiltonian Monte Carlo (HMC) be effectively utilized as a Markov kernel proposal within the GOHSMC framework using a forward-proposal L-kernel?
- Basis in paper: [explicit] The authors state that "there are other gradient based proposals that are also worth exploring such as HMC" and specifically mention using a forward-proposal L-kernel.
- Why unresolved: The current work only implements and tests unadjusted Langevin dynamics (LD); HMC is suggested but not demonstrated.
- What evidence would resolve it: An implementation of GOHSMC using HMC proposals with the derived weight update, evaluated on the same benchmark datasets.

### Open Question 2
- Question: Can adaptive tuning methods like the No-U-Turn Sampler (NUTS) or the ChEES criterion be integrated into the GOHSMC sampler to automate trajectory length selection?
- Basis in paper: [explicit] The paper identifies hyperparameter tuning as an issue for HMC and suggests "Different approaches to solving this could be taken; the No U-Turn algorithm could be used... Alternatively, using the ChEES criterion... would allow us to tune the trajectory length."
- Why unresolved: These specific adaptive algorithms are proposed as potential solutions to the tuning problem but have not yet been applied to this specific SMC framework.
- What evidence would resolve it: Experiments demonstrating that NUTS or ChEES can be successfully incorporated into the GOHSMC algorithm to automatically determine step sizes or trajectory lengths without manual intervention.

### Open Question 3
- Question: Would additional Monte Carlo runs definitively establish GOHSMC LD's superiority over baselines like SWAG, given the current overlap in standard deviations?
- Basis in paper: [explicit] The authors note that "there are overlaps in the standard deviations for certain datasets" and suggest that "Additional Monte Carlo runs may further reduce variance, strengthening the evidence."
- Why unresolved: The current experimental results show performance improvements, but statistical noise prevents a definitive conclusion of superiority on all metrics and datasets.
- What evidence would resolve it: A follow-up study with a significantly higher number of random seeds or runs to tighten the standard deviations and confirm if the performance gap is statistically significant.

## Limitations

- The paper provides limited analysis of how particle degeneracy impacts performance in high-dimensional settings, which could be problematic for very large models.
- Claims about larger batch sizes improving both efficiency and performance contradict typical guidance for SGHMC methods and lack extensive validation across diverse settings.
- The superiority over baseline methods is demonstrated empirically but not rigorously proven, with some performance metrics showing overlapping standard deviations.

## Confidence

- **High confidence**: The theoretical framework for GOHSMC LD is well-established, with clear derivation of the Langevin dynamics proposals and forward-proposal L-kernel weight updates. The experimental methodology follows standard practices.
- **Medium confidence**: Claims about superior performance relative to baselines are supported by empirical results, but the comparison methods and exact architectures used are not fully specified, making direct replication challenging.
- **Low confidence**: The assertion that larger batch sizes improve both training efficiency and performance for pBNNs contradicts typical guidance for SGHMC methods and lacks extensive empirical validation across diverse settings.

## Next Checks

1. **Reproduce core results**: Implement GOHSMC LD and verify performance on the Yacht Hydrodynamics dataset matches reported metrics (RMSE=0.0766, NLL=0.9219).

2. **Batch size scaling validation**: Systematically test the impact of batch size on convergence speed and final performance across multiple UCI datasets to confirm the claimed scaling benefits.

3. **Particle degeneracy analysis**: Monitor effective sample size throughout training on high-dimensional problems to empirically validate the claim that gradient-based proposals prevent particle collapse better than random-walk alternatives.