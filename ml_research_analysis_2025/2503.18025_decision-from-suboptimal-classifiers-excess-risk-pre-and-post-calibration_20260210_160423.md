---
ver: rpa2
title: 'Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration'
arxiv_id: '2503.18025'
source_url: https://arxiv.org/abs/2503.18025
tags:
- gain
- regret
- decision
- correlation
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a theoretical framework to quantify how errors
  in estimated probabilities affect the expected utility of downstream decisions.
  It provides analytical expressions for miscalibration-induced regret and tight bounds
  on the regret of calibrated classifiers, which can be estimated in practice using
  calibration curves and grouping loss estimators.
---

# Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration

## Quick Facts
- arXiv ID: 2503.18025
- Source URL: https://arxiv.org/abs/2503.18025
- Reference count: 40
- This paper introduces a theoretical framework to quantify how errors in estimated probabilities affect the expected utility of downstream decisions, providing analytical expressions for miscalibration-induced regret and tight bounds on the regret of calibrated classifiers.

## Executive Summary
This paper introduces a theoretical framework to quantify how errors in estimated probabilities affect the expected utility of downstream decisions. It provides analytical expressions for miscalibration-induced regret and tight bounds on the regret of calibrated classifiers, which can be estimated in practice using calibration curves and grouping loss estimators. The results identify two regimes: one where recalibration is sufficient and another where more advanced post-training is needed. Experiments on hate-speech detection tasks demonstrate that these estimators better predict the potential gains from recalibration and post-training compared to traditional metrics like Brier score or AUC. The findings enable a new model evaluation procedure to guide cost-effective post-training decisions, showing that recalibration alone often hits diminishing returns, and that methods like stacking or fine-tuning are needed to address grouping loss-induced regret.

## Method Summary
The paper develops a decision-theoretic framework to quantify excess risk from using estimated probabilities instead of true probabilities in binary decision-making. It decomposes expected utility regret into two components: calibration loss (difference between true and estimated probabilities) and grouping loss (variance of true probabilities within estimated probability bins). The framework provides analytical expressions for these regret components and tight bounds on the regret of calibrated classifiers. In practice, calibration curves can be estimated using histogram binning, and grouping loss can be estimated by partitioning the embedding space and computing local variance. The paper proposes a new post-training algorithm (GLAR) that applies isotonic regression or grouping loss correction based on the estimated regret components. Experiments on hate-speech detection tasks with 6 transformer models and 14 datasets demonstrate that the estimated regret components better predict the potential gains from post-training compared to traditional metrics.

## Key Results
- Analytical expressions for miscalibration-induced regret and tight bounds on the regret of calibrated classifiers
- Identification of two regimes: one where recalibration is sufficient and another where more advanced post-training is needed
- Experiments on hate-speech detection tasks show that estimated regret components better predict post-training gains than traditional metrics like Brier score or AUC

## Why This Works (Mechanism)
The framework works by decomposing the expected utility regret into two orthogonal components: calibration loss and grouping loss. Calibration loss captures the error from incorrect probability estimates, while grouping loss captures the error from having heterogeneous true probabilities within estimated probability bins. By quantifying these components, the framework can distinguish between decisions where simple recalibration is sufficient versus those requiring more advanced post-training. The key insight is that the optimal post-training strategy depends on which regret component dominates, enabling cost-effective decision-making about whether to apply recalibration, fine-tuning, or more complex methods like stacking.

## Foundational Learning
- **Expected Utility**: The decision-theoretic foundation for evaluating classifier performance under utility/cost constraints. *Why needed*: To properly evaluate decisions beyond accuracy metrics. *Quick check*: Verify that EU calculations account for different costs of false positives vs false negatives.
- **Calibration Curves**: The relationship between estimated and true probabilities. *Why needed*: To estimate the calibration loss component of regret. *Quick check*: Ensure calibration curves are monotonic before calculating regret.
- **Grouping Loss**: The variance of true probabilities within estimated probability bins. *Why needed*: To quantify the error from heterogeneous true probabilities within bins. *Quick check*: Verify that grouping loss estimates correlate with utility gains.
- **Decision Tree Partitions**: Used to estimate grouping loss by partitioning the embedding space. *Why needed*: To capture local variance differences in true probabilities. *Quick check*: Ensure partitions have sufficient samples to avoid overfitting.
- **Regret Bounds**: Theoretical upper bounds on the regret of calibrated classifiers. *Why needed*: To quantify the maximum potential gain from post-training. *Quick check*: Verify that estimated regret components respect these bounds.
- **Post-training Strategies**: Different approaches (recalibration, fine-tuning, stacking) for improving classifier decisions. *Why needed*: To select the optimal strategy based on estimated regret components. *Quick check*: Compare the effectiveness of different strategies across datasets.

## Architecture Onboarding
**Component Map**: Data -> Embedding Extraction -> Regret Estimation (CL + GL) -> Post-training Selection -> Utility Gain Evaluation

**Critical Path**: The critical path is the estimation of regret components followed by post-training selection. The framework first estimates calibration loss and grouping loss, then uses these estimates to select the optimal post-training strategy. The decision between simple recalibration and more advanced methods depends on whether grouping loss exceeds the threshold of 0.02.

**Design Tradeoffs**: The main tradeoff is between estimation accuracy and computational complexity. Using more bins for calibration curves or more leaves for grouping loss estimation can improve accuracy but increases computational cost. The framework also trades off between simple recalibration (fast, low complexity) and advanced post-training methods (slower, higher complexity).

**Failure Signatures**: 
- High variance in grouping loss estimation when bins have few samples (<50)
- Poor correlation between estimated regret and utility gains (rÂ² < 0.5)
- GLAR degradation when partitions fail to capture meaningful