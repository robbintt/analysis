---
ver: rpa2
title: Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement
  Learning
arxiv_id: '2601.22297'
source_url: https://arxiv.org/abs/2601.22297
tags:
- debate
- terms
- arxiv
- step
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving Multi-Agent Debate
  (MAD) performance by training models to better incorporate diverse reasoning trajectories
  during debate. The core method, Self-Debate Reinforcement Learning (SDRL), is a
  training framework that jointly optimizes both initial responses and debate-conditioned
  responses using verifiable rewards.
---

# Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.22297
- Source URL: https://arxiv.org/abs/2601.22297
- Reference count: 40
- Primary result: SDRL improves both multi-agent debate performance and single-agent accuracy across math benchmarks, with Qwen3-4B-Base showing post-debate accuracy increasing from 52.9% to 57.7% while single-agent maj@32 accuracy rises from 56.2% to 60.2%

## Executive Summary
This paper addresses the problem of improving Multi-Agent Debate (MAD) performance by training models to better incorporate diverse reasoning trajectories during debate. The core method, Self-Debate Reinforcement Learning (SDRL), is a training framework that jointly optimizes both initial responses and debate-conditioned responses using verifiable rewards. SDRL constructs debate pairs from candidate solutions and trains models to critique and revise their reasoning based on alternative perspectives. The primary results show that SDRL consistently improves both MAD performance and single-agent accuracy across multiple math reasoning benchmarks (MATH500, AMC, AIME), with theoretical analysis demonstrating that improvements stem from increasing the "critique advantage" that breaks the martingale neutrality of standard MAD.

## Method Summary
SDRL trains a single LLM to be effective at both standalone reasoning and multi-agent debate by jointly optimizing initial responses and debate-conditioned responses. The method uses DAPO reinforcement learning with verifiable rewards (+1/-1) from math verification. For each prompt, the model generates 8 initial responses, constructs debate pairs using frequency-based or random selection, generates 4-8 debate responses conditioned on the debate context, and jointly optimizes both turns using group advantage normalization. The framework addresses the limitation that standard MAD suffers from martingale neutrality where expected belief remains unchanged across debate rounds.

## Key Results
- SDRL-freq improves Qwen3-4B-Base post-debate accuracy from 52.9% to 57.7% while also increasing single-agent maj@32 accuracy from 56.2% to 60.2%
- On Qwen2.5-3B-Base, SDRL achieves 41.5% debate accuracy on AIME24 vs 38.6% baseline
- Performance gains peak quickly and can decline in later rounds due to diminishing per-round drift and increasing answer correlation

## Why This Works (Mechanism)

### Mechanism 1: Positive Belief Drift via Private Critique
Training models to critique conflicting rationales induces a "positive belief drift" that theoretically breaks the "martingale neutrality" of standard Multi-Agent Debate (MAD). Standard MAD is modeled as a Bayesian belief update where the expected belief in the correct answer remains a martingale unless an external signal is introduced. SDRL trains a "private critique" capability via Reinforcement Learning that, when favoring the correct answer more than a proportional prior belief, generates a positive "critique advantage" causing expected belief in the correct answer to increase across rounds.

### Mechanism 2: Joint Optimization of Solo and Debate Objectives
Merging gradient updates from single-turn responses and debate-conditioned responses allows a single model to simultaneously improve standalone accuracy and collaborative debate performance. SDRL constructs a batch containing both initial rollouts and second-turn "debate rollouts" and optimizes the policy across this mixed batch, incentivizing the model to solve problems independently while also learning to synthesize diverse opinions using the same policy weights.

### Mechanism 3: Diminishing Returns and Correlation Limits
While SDRL induces positive drift, performance gains peak quickly and can decline in later rounds due to diminishing per-round drift and increasing answer correlation. The belief improvement scales inversely with accumulated belief mass, leading to logarithmic accumulation. As rounds progress, agents condition on increasingly similar contexts, raising answer correlation which shrinks the "effective ensemble size," weakening majority-vote amplification.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: SDRL relies on sparse, binary (+1/-1) outcome rewards derived from math verification rather than a learned reward model
  - Quick check: Can you explain how to estimate a policy gradient advantage using only group-level reward statistics (mean/std) without a value function?

- **Concept: Dirichlet Compound Multinomial (DCM)**
  - Why needed: The paper models agent beliefs and the debate process using DCM to theoretically prove why standard debate fails and how SDRL fixes it
  - Quick check: In a DCM model, how does adding pseudo-counts to the Dirichlet parameters shift the expected categorical mean?

- **Concept: DAPO (Decoupled Clip and Dynamic Sampling)**
  - Why needed: This is the specific optimization algorithm used to stabilize training of reasoning models, replacing standard PPO/GRPO to handle long contexts and entropy collapse
  - Quick check: What is the role of "dynamic sampling" constraints in DAPO compared to standard importance sampling clipping?

## Architecture Onboarding

- **Component map:** Policy ($\pi_\theta$) -> Rollout Engine -> Debate Pair Constructor -> Debate Rollouts -> Reward Function -> Optimizer
- **Critical path:** Sample Prompts → Generate Initial Rollouts → Select Debate Pairs (High Disagreement) → Generate Debate Rollouts → Compute Group Advantages → Joint Policy Update
- **Design tradeoffs:** Frequency-based pairing yields higher gains on hard tasks by forcing confrontation with most likely wrong answers, but random pairing offers broader diversity; SDRL adds overhead requiring second-turn rollouts with filtering zero-advantage samples being critical for efficiency
- **Failure signatures:** Martingale Neutrality (Δ ≈ 0 between Majority Vote and Debate); Correlation Collapse (accuracy drops in later rounds as agents converge on same incorrect reasoning); MATH500 Degradation (incorrect reasoning traces in debate context can be longer and dominate)
- **First 3 experiments:** 1) Baseline Validation: Train Qwen2.5-3B with standard DAPO to establish majority voting baseline and verify martingale behavior; 2) Ablation on Pairing: Compare SDRL-Random vs SDRL-Frequency on AIME24 to determine if forcing disagreement is necessary; 3) Round Scaling Analysis: Run inference with T=1,2,3,4 rounds to identify peak accuracy point and verify theoretical diminishing returns

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Performance degradation on easier benchmarks like MATH500 with increased debate rounds due to context contamination from verbose incorrect reasoning
- Theoretical framework relies on simplifying assumptions about agent independence and belief distributions that may not hold in practice
- Limited exploration of trade-offs between frequency-based and random pairing strategies across different model families and dataset characteristics

## Confidence
- **High:** Core claim of SDRL improving both MAD performance and single-agent accuracy is well-supported empirically across multiple benchmarks
- **Medium:** Theoretical framework provides plausible explanation but relies on simplifying assumptions about agent independence
- **Low:** Pairing strategy comparison shows performance differences but analysis doesn't fully explore trade-offs across different contexts

## Next Checks
1. Replicate performance degradation on MATH500 with increased debate rounds to confirm systematic limitation and test whether context pruning mitigates this issue
2. Systematically compare SDRL-freq vs SDRL-rand across all benchmark combinations to determine if pairing strategy effectiveness depends on model scale or task difficulty
3. Test SDRL on non-math reasoning tasks (e.g., code generation) to evaluate whether positive belief drift mechanism generalizes beyond mathematical verification