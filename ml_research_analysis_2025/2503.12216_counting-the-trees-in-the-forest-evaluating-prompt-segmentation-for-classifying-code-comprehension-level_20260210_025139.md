---
ver: rpa2
title: 'Counting the Trees in the Forest: Evaluating Prompt Segmentation for Classifying
  Code Comprehension Level'
arxiv_id: '2503.12216'
source_url: https://arxiv.org/abs/2503.12216
tags:
- code
- responses
- segmentation
- segments
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language model (LLM) segmentation
  to automatically classify responses to "Explain in Plain English" (EiPE) questions
  as either multi-structural (line-by-line) or relational (high-level). The approach
  maps student response segments to code lines, expecting more segments for multi-structural
  and fewer for relational responses.
---

# Counting the Trees in the Forest: Evaluating Prompt Segmentation for Classifying Code Comprehension Level

## Quick Facts
- arXiv ID: 2503.12216
- Source URL: https://arxiv.org/abs/2503.12216
- Reference count: 36
- Primary result: LLM-based segmentation achieves 81% agreement with human labels at threshold of 2 segments, Cohen's kappa 0.61

## Executive Summary
This paper proposes using large language model (LLM) segmentation to automatically classify student responses to "Explain in Plain English" (EiPE) questions as either multi-structural (line-by-line) or relational (high-level) comprehension. The approach maps student response segments to code lines, expecting more segments for multi-structural and fewer for relational responses. Using human-labeled data, the system achieves 81% agreement and a Cohen's kappa of 0.61 when the segment threshold is set to 2, and performance improves further with post-processing that removes function definition segments. The authors release this as an open-source Python package and discuss potential feedback mechanisms for students based on segmentation results.

## Method Summary
The method uses GPT-4o with structured output to segment student explanations and map each segment to specific code lines. The system prompt includes task instructions and two few-shot examples showing the expected output format for both response types. Classification is performed by counting the resulting segments and comparing against a threshold (optimal at 2 segments). A post-processing step removes segments that only map to function definitions, which the model tended to segment separately even for relational responses.

## Key Results
- 81% agreement with human labels when threshold set to 2 segments
- Cohen's kappa of 0.61 indicates "substantial agreement" between automated and human classification
- Performance improves from 68% to 81% agreement with post-processing that removes function-definition-only segments
- Standard deviations for segmentation counts show significant overlap between multi-structural and relational distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segment count correlates with comprehension depth, enabling automated classification of response types.
- Mechanism: An LLM segments student explanations and maps each segment to specific code lines. Multi-structural responses produce more segments (each tied to individual lines), while relational responses produce fewer segments (tied to the whole function). A threshold on segment count distinguishes the two.
- Core assumption: The LLM's segmentation behavior aligns with human judgment of what constitutes a "segment" and that segment count is a valid proxy for comprehension level.
- Evidence anchors:
  - [abstract] "Using a Large Language Model (LLM) to segment both the student's description and the code, we aim to determine whether the student describes each line individually (many segments) or the code as a whole (fewer segments)."
  - [section 5.1] "The model achieved its highest performance at a threshold of 2, with 81% agreement, a Cohen's κ of 0.61, an F1 score of 0.80"
  - [corpus] Weak corpus support—neighbor papers focus on image segmentation and vision-language models, not text-based code comprehension segmentation.
- Break condition: If student responses are ambiguous, incomplete, or contain mixed-level descriptions, segment count may not reliably indicate comprehension depth.

### Mechanism 2
- Claim: Few-shot examples guide the LLM to produce consistent segmentation output format.
- Mechanism: The prompt includes two exemplars—one multi-structural response mapping to individual lines, one relational response mapping to the entire function. This conditions the model on expected behavior and output structure (JSON with "groups" containing "code" and "explanation_portion" fields).
- Core assumption: The model generalizes from these examples to unseen student responses without overfitting to the specific examples.
- Evidence anchors:
  - [section 3] "we provide the model with two, few-shot examples: 1) an example of a mapping for a multi-structural response (Figure 1b) and 2) an example of a mapping for a relational response (Figure 1c)"
  - [section 3] "To guarantee adherence to the desired segmentation format in JSON, we leverage OpenAI's GPT-4o with structured output capabilities."
  - [corpus] Not addressed in corpus.
- Break condition: If the few-shot examples don't cover edge cases (e.g., partially correct responses, mixed abstraction levels), output consistency may degrade.

### Mechanism 3
- Claim: Post-processing removes confounding segments to improve classification accuracy.
- Mechanism: The model sometimes segments function definition descriptions separately from the rest of the code, even for relational responses. A post-processing step removes segments that map only to function definitions, reducing false multi-structural classifications.
- Core assumption: Function definition descriptions are pedagogically non-informative for comprehension level assessment and can be safely excluded.
- Evidence anchors:
  - [section 5.1] "we found that the model was segmenting students' descriptions of function definitions as separate segments from the rest of the code"
  - [section 5.1] "For a threshold of 1, the percent agreement increased from 68% to 81%. Looking at the results for other metrics, the Cohen's κ improved from 0.30 to 0.61"
  - [corpus] Not addressed in corpus.
- Break condition: If instructors value function definition descriptions as part of comprehension assessment, this post-processing could remove pedagogically relevant signal.

## Foundational Learning

- Concept: **SOLO Taxonomy (Structure of Observed Learning Outcome)**
  - Why needed here: The paper uses a modified SOLO taxonomy to classify responses as multi-structural (line-by-line) or relational (abstract purpose). Understanding this framework is essential to interpret what the system measures.
  - Quick check question: Can you explain why a "multi-structural" response indicates lower comprehension depth than a "relational" response?

- Concept: **Few-shot Prompting with Structured Outputs**
  - Why needed here: The approach relies on providing the LLM with example mappings to constrain output format. Understanding how few-shot examples influence model behavior helps diagnose inconsistent outputs.
  - Quick check question: What might happen if the few-shot examples only showed multi-structural responses with no relational examples?

- Concept: **Cohen's Kappa and Inter-rater Agreement**
  - Why needed here: The paper reports κ = 0.61 as "substantial agreement." Interpreting this metric correctly is necessary to evaluate whether the approach is production-ready.
  - Quick check question: Why is percent agreement alone insufficient for evaluating classifier performance against human labels?

## Architecture Onboarding

- Component map:
  - System Prompt -> Few-shot Examples -> LLM (GPT-4o) -> Structured JSON Output -> Post-processor -> Segment Counter -> Classifier -> Output Label

- Critical path:
  1. Construct system prompt with code snippet and task instructions
  2. Append few-shot examples to prompt
  3. Send student response to GPT-4o with structured output enabled
  4. Parse JSON response into segment groups
  5. Apply post-processing to remove function definition segments
  6. Count remaining segments and compare to threshold for classification

- Design tradeoffs:
  - **Threshold selection**: Lower thresholds (1-2) favor recall of relational responses; higher thresholds reduce false positives but may miss some multi-structural responses. Paper suggests threshold of 2 as optimal.
  - **Post-processing aggressiveness**: Removing function definition segments improves agreement but may discard legitimate comprehension signal in some pedagogical contexts.
  - **Model choice**: GPT-4o with structured outputs ensures valid JSON, but adds API dependency and cost. Open-source alternatives not evaluated.

- Failure signatures:
  - **High segment variance within classes**: Section 5 notes "standard deviations for the segmentation counts is reasonably high, meaning there is a significant overlap between the distributions"
  - **Inconsistent segment definitions**: The model may segment differently than a human would expect; "formalized analysis of the segments produced is needed" (Section 7)
  - **Missing segments**: Responses that don't map cleanly to code lines may produce ambiguous or incomplete segmentations

- First 3 experiments:
  1. **Baseline validation**: Run the segmentation approach on a held-out set of responses with human labels, computing Cohen's κ, F1, precision, and recall at multiple thresholds (1-5) to confirm reported results.
  2. **Post-processing ablation**: Compare classification performance with and without the function-definition removal step to quantify its contribution and identify any question types where it hurts accuracy.
  3. **Cross-question generalization**: Test whether a single set of few-shot examples generalizes across all 8 question types, or whether per-question examples improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LLM's approach to segmenting student responses align with human expectations of how explanations should map to code lines?
- Basis in paper: [explicit] "there remains the question of whether its approach to performing segmentation aligns with human expectations."
- Why unresolved: The paper evaluates whether segment counts distinguish response types, but does not validate whether the specific segment-to-code mappings the LLM produces match human judgment about which explanation portions describe which code lines.
- What evidence would resolve it: A study where humans annotate segment-to-code mappings for a set of responses, with inter-rater agreement calculated between human annotations and LLM outputs.

### Open Question 2
- Question: What is the pedagogical impact of providing students with segmentation-based feedback on their code comprehension level?
- Basis in paper: [inferred] The paper proposes two feedback mechanisms (Figure 5) but does not evaluate their effectiveness in improving student learning or guiding students toward relational responses.
- Why unresolved: The paper establishes technical feasibility of segmentation-based classification but does not assess whether exposing students to segment counts or visual mappings actually helps them develop higher-level comprehension skills.
- What evidence would resolve it: A controlled experiment comparing learning outcomes for students who receive segmentation-based feedback versus those who receive standard correctness-only feedback.

### Open Question 3
- Question: How robust is the optimal segmentation threshold across different code complexities, programming languages, and student populations?
- Basis in paper: [inferred] The threshold of 2 segments was optimal for this dataset, but the paper notes "reasonably high" standard deviations and overlap between distributions. The data comes from one course at one university using C-style code snippets.
- Why unresolved: The generalizability of the threshold=2 finding is unknown across different contexts, code lengths, or languages where segmentation behavior may differ.
- What evidence would resolve it: Replication studies across multiple institutions, with different programming languages (e.g., Python, Java), varying code snippet lengths, and diverse student cohorts to determine if threshold tuning is context-dependent.

## Limitations
- Segment counts show substantial overlap between multi-structural and relational responses, limiting perfect discrimination
- The few-shot prompting approach's effectiveness depends heavily on example selection, which is not thoroughly explored
- The function-definition post-processing removes potentially pedagogically relevant signal in some contexts

## Confidence

**Confidence Assessment:**
- **High confidence** in the correlation between segment count and comprehension level for the tested dataset
- **Medium confidence** in the generalizability across different programming exercises and student populations
- **Medium confidence** in the robustness of the few-shot prompting approach, as the effect of example selection is not thoroughly explored

## Next Checks

1. **Segment consistency analysis**: Manually categorize all segments produced by the LLM for a subset of responses to verify whether the model's segmentation aligns with human understanding of what constitutes a meaningful code explanation segment.

2. **Threshold sensitivity testing**: Systematically evaluate classification performance across thresholds 1-5 for each of the 8 question types separately, identifying whether per-question threshold tuning improves accuracy over the single threshold of 2.

3. **Cross-linguistic generalization**: Test the segmentation approach with student responses in languages other than English (e.g., non-native English speakers) to assess whether language proficiency confounds the segment-count relationship with comprehension level.