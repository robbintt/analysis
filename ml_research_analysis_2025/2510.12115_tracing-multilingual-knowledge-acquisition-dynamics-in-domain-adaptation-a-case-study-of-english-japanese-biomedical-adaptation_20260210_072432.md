---
ver: rpa2
title: 'Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation:
  A Case Study of English-Japanese Biomedical Adaptation'
arxiv_id: '2510.12115'
source_url: https://arxiv.org/abs/2510.12115
tags:
- knowledge
- loss
- training
- domain
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multilingual knowledge acquisition dynamics
  in domain adaptation, focusing on English-Japanese biomedical adaptation. The authors
  introduce AdaXEval, an adaptive evaluation pipeline that generates multiple-choice
  QA datasets from the same bilingual domain corpus used for training, enabling direct
  assessment of knowledge memorization, intralingual generalization, and cross-lingual
  transfer.
---

# Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation

## Quick Facts
- arXiv ID: 2510.12115
- Source URL: https://arxiv.org/abs/2510.12115
- Authors: Xin Zhao; Naoki Yoshinaga; Yuma Tsuta; Akiko Aizawa
- Reference count: 40
- Key outcome: Introduces AdaXEval pipeline to evaluate multilingual knowledge acquisition during domain adaptation, revealing "loss shielding" patterns and challenges in cross-lingual transfer despite bilingual training data

## Executive Summary
This paper investigates how large language models acquire and transfer domain knowledge across languages during adaptation, using biomedical English-Japanese adaptation as a case study. The authors develop AdaXEval, an adaptive evaluation pipeline that generates multiple-choice QA datasets from the same bilingual training corpus, enabling direct assessment of knowledge memorization, intralingual generalization, and cross-lingual transfer. Through extensive experiments on a 13B bilingual LLM, they uncover a "loss shielding" phenomenon where correct sequences resist rapid loss growth despite overfitting, and demonstrate that cross-lingual transfer remains challenging even with high-quality bilingual corpora.

## Method Summary
The study introduces AdaXEval, an adaptive evaluation pipeline that generates multiple-choice QA datasets from the same bilingual domain corpus used for training. The pipeline uses multi-LLM agents (Qwen-32B, Llama-3.3-70B, llm-jp variant) with chain-of-thought prompts for fact detection and triple extraction, followed by GPT-4.1 for cloze queries, paraphrased questions, and distractor generation. Training employs continual adaptation on a 13B bilingual LLM using Megatron-LM with 0.5B tokens per language for 4 epochs. The evaluation tracks memorization (cloze), intralingual generalization (paraphrase), and interlingual transfer (cross-lingual) through loss ratio analysis comparing correct vs. all options.

## Key Results
- Knowledge acquisition follows a "loss shielding" pattern where correct sequences resist rapid loss growth despite overfitting
- Cross-lingual transfer remains challenging even with high-quality bilingual corpora and parallel text
- Token overlap in related domains is crucial for effective cross-lingual transfer
- LLMs show extreme sensitivity to training data variations through perturbation studies

## Why This Works (Mechanism)
The study reveals that multilingual knowledge acquisition in domain adaptation follows distinct patterns across different evaluation dimensions. The "loss shielding" mechanism shows that models can maintain knowledge of correct sequences even as overall loss increases due to overfitting. Cross-lingual transfer effectiveness depends critically on token overlap between languages, with English tokens embedded in Japanese documents showing measurable transfer effects during English training. The sensitivity to training data perturbations indicates that small variations can significantly impact model performance, suggesting instability in the knowledge acquisition process.

## Foundational Learning
- AdaXEval pipeline (why needed: enables direct evaluation of knowledge acquisition using training corpus; quick check: verify generated datasets cover all evaluation dimensions)
- Multiple-choice QA generation (why needed: provides controlled evaluation framework; quick check: ensure 4-option format with quality-filtered distractors)
- Loss ratio analysis (why needed: quantifies "loss shielding" phenomenon; quick check: compare loss ratio trends with accuracy curves)
- Token overlap analysis (why needed: explains cross-lingual transfer effectiveness; quick check: measure English token frequency in Japanese documents)
- Continual training framework (why needed: enables systematic study of knowledge dynamics; quick check: verify training stability across epochs)
- Perturbation study methodology (why needed: quantifies model sensitivity to data variations; quick check: ensure perturbations maintain semantic coherence)

## Architecture Onboarding

**Component Map**
- J-STAGE corpus -> AdaXEval pipeline -> Multiple-choice QA datasets -> Continual training -> Evaluation checkpoints

**Critical Path**
AdaXEval generation (NER filtering + multi-LLM fact detection + GPT-4.1 QA generation) -> Continual training (Megatron-LM with specified hyperparameters) -> Loss ratio evaluation (correct vs. all options across 4 evaluation dimensions)

**Design Tradeoffs**
- Using training corpus for evaluation enables direct knowledge tracking but may introduce bias
- Multiple-choice format provides controlled evaluation but limits question complexity
- Continual training enables systematic study but requires careful hyperparameter tuning to avoid catastrophic forgetting

**Failure Signatures**
- Cross-lingual transfer <5% improvement despite bilingual data indicates tokenization or corpus issues
- Loss ratio divergence from accuracy suggests evaluation pipeline implementation problems
- Extreme sensitivity to perturbations (>20% performance drop) indicates model instability

**First Experiments**
1. Verify AdaXEval pipeline generates balanced datasets across all three evaluation dimensions
2. Confirm "loss shielding" pattern by comparing loss ratio and accuracy trends during training
3. Test token overlap analysis by measuring cross-lingual transfer on artificially aligned corpora

## Open Questions the Paper Calls Out
None

## Limitations
- J-STAGE corpus access restricted by license, preventing independent validation
- Multi-LLM prompting strategies for fact detection and triple extraction remain underspecified
- Code repository details incomplete, affecting reproducibility of implementation choices

## Confidence
High: Overall framework of adaptive evaluation pipelines and core finding about token overlap driving cross-lingual transfer
Medium: Specific "loss shielding" pattern characterization and quantitative thresholds for knowledge acquisition
Low: Perturbation study results showing extreme sensitivity, given incomplete implementation specifications

## Next Checks
1. **Token Overlap Analysis Replication**: Extract parallel biomedical documents from public sources and measure token overlap between English and Japanese versions. Train a smaller bilingual model on this corpus and track cross-lingual transfer metrics during monolingual training phases.

2. **Loss Shielding Verification**: Implement loss ratio computation (correct-sequence loss / total loss across 4 options) on a controlled cloze-style dataset. Compare correlation between loss ratio trends and accuracy during training to confirm "loss shielding" pattern independently.

3. **Perturbation Sensitivity Test**: Apply mask-X, random-X, delete-X, reorder-X, and synonyms perturbations to biomedical document sets. Train models on original and perturbed data, measuring performance degradation to quantify sensitivity to training data variations.