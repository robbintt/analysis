---
ver: rpa2
title: Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large
  Language Models
arxiv_id: '2510.26577'
source_url: https://arxiv.org/abs/2510.26577
tags:
- decoding
- cast
- eagle-3
- arxiv
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating inference for
  large language models (LLMs) by introducing a cost-aware dynamic tree decoding approach.
  The method, called CAST, dynamically adjusts the tree structure during speculative
  decoding by considering inference costs such as GPU configurations and batch sizes.
---

# Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models

## Quick Facts
- arXiv ID: 2510.26577
- Source URL: https://arxiv.org/abs/2510.26577
- Reference count: 9
- Outperforms state-of-the-art speculative decoding methods, achieving up to 5.2x speedup over standard decoding and 5-20% improvements over previous best approaches.

## Executive Summary
This paper addresses the challenge of accelerating inference for large language models (LLMs) by introducing a cost-aware dynamic tree decoding approach. The method, called CAST, dynamically adjusts the tree structure during speculative decoding by considering inference costs such as GPU configurations and batch sizes. This is achieved through breadth and depth pruning, along with dynamic reranking, to balance token acceptance rates and computational efficiency. Extensive experiments across six tasks and six LLMs show that CAST consistently outperforms existing state-of-the-art methods.

## Method Summary
CAST constructs a dynamic tree structure during speculative decoding by using breadth pruning (Algorithm 1) and depth pruning based on pre-computed inference cost lookup tables. The method profiles the target and draft model latency to generate lookup tables mapping batch size, context length, and token count to latency. During decoding, it dynamically expands a tree by selecting nodes based on a utility function that balances confidence scores against computational costs, stopping expansion when marginal gains fall below a threshold. After expansion, nodes are reranked by cumulative probability score and a final set is selected for verification with the target model.

## Key Results
- Achieves up to 5.2x speedup compared to standard autoregressive decoding
- Outperforms EAGLE-3 by 5-20% across multiple benchmarks
- Maintains consistent performance improvements across six different LLMs (7B-70B scale)
- Shows graceful degradation as batch size increases compared to fixed-tree approaches

## Why This Works (Mechanism)

### Mechanism 1: Cost-Aware Breadth Pruning via Marginal Utility Thresholding
- Claim: Limiting the number of nodes per layer based on a cost-utility trade-off improves overall speedup compared to heuristically fixing node counts.
- Mechanism: CAST calculates a cumulative utility $u_k^{(i)}$ (sum of confidence scores) and a normalized cost $c_k^{(i)}$ for selecting the top $k$ nodes at layer $i$. It retains nodes only while the marginal utility exceeds a threshold $C_1$. This is implemented via Algorithm 1, which selects the maximum valid index satisfying this condition.
- Core assumption: The acceptance rate of a node is strongly correlated with its confidence score $v(u)$. The utility function is effectively concave, meaning marginal gains decrease as more nodes are added.
- Evidence anchors:
  - [abstract] "This is achieved through breadth and depth pruning, along with dynamic reranking, to balance token acceptance rates and computational efficiency."
  - [Section 4.1] "Drawing inspiration from utility theory in economics, we frame node selection as a utility maximization problem... we introduce a threshold $C_1$ and retain nodes whose marginal utility exceeds this threshold."
  - [Corpus] Weak direct corpus support for this specific utility-based pruning algorithm; related work (EAGLE-2/3) uses dynamic trees but doesn't explicitly model GPU/batch costs.
- Break condition: If confidence scores are not predictive of acceptance rates, or if inference cost is not accurately modeled, the utility/cost calculations become unreliable, potentially leading to suboptimal pruning (e.g., pruning too aggressively or not enough).

### Mechanism 2: Depth Pruning via Predictive Quality Buffer
- Claim: Stopping tree expansion based on a buffer of recent predictive quality gains prevents inefficiently deep trees.
- Mechanism: A buffer $A_i$ tracks the confidence gain ratio $\phi_i$ between consecutive layers. A new layer $(i+1)$ is generated only if $\alpha_i \cdot \frac{u_{n_i}^{(i)}}{c_{n_i}^{(i)}} \ge C_2$, where $\alpha_i$ is the average of the buffer. This adapts tree depth to current performance trends.
- Core assumption: Past confidence gain ratios are predictive of future gains from deeper layers.
- Evidence anchors:
  - [Section 4.1] "Next, we consider depth pruning... We proceed to generate layer $(i+1)$ only if the following condition holds: $\alpha_i \cdot \frac{u_{n_i}^{(i)}}{c_{n_i}^{(i)}} \ge C_2$".
  - [Section 4.1] "We define: $\alpha_i = Avg(A_i)$... update the buffer $A_i$ using a first-in-first-out (FIFO) policy, maintaining up to $R$ recent values of $\phi_i$."
  - [Corpus] No direct corpus support for this specific buffering mechanism.
- Break condition: If confidence gains fluctuate unpredictably or do not correlate with actual token acceptance at deeper levels, the buffer's average will not be a useful stopping criterion, possibly terminating expansion too early or allowing inefficiently deep growth.

### Mechanism 3: Dynamic Reranking with Inference Cost Consideration
- Claim: Selecting the top-$m$ nodes from the entire draft tree based on a cost-aware criterion maximizes acceptance length per verification cost.
- Mechanism: After dynamic expansion, nodes are reranked by their cumulative probability score $v$. Algorithm 1 is reused to determine the final number of nodes $m$ to verify, comparing the cumulative score $u_k = \sum_{j=1}^k v(j)$ against the verification cost $c_k = \frac{S_T(B)[\text{select}(n_0)][k]}{S_T(B)[\text{select}(n_0)][1]}$ with threshold $C_3$.
- Core assumption: Cumulative probability score is a strong proxy for potential acceptance length. Inference cost scales predictably with the number of verified tokens.
- Evidence anchors:
  - [Section 4.2] "From the Figure, it is clear that the accept length and the cumulative probability shares a linear trend. Therefore... choosing the nodes with top probability score is the right choice."
  - [Section 4.2] "...one can also use Algorithm 1 to determine the number of nodes to be verified by the target model... with threshold constant $C_3$."
  - [Corpus] Related work (EAGLE-2/3) also performs reranking, but CAST explicitly incorporates inference cost into the final selection.
- Break condition: If the correlation between cumulative probability and actual acceptance length is weak or breaks down for a specific model/task, or if verification costs are highly non-linear, the reranking will select a suboptimal set of nodes for verification.

## Foundational Learning

- Concept: **Speculative Decoding**
  - Why needed here: CAST is a speculative decoding method; understanding the proposal/verification loop and acceptance criteria is essential.
  - Quick check question: How does speculative decoding ensure the output distribution remains identical to the target model?

- Concept: **Tree-Structured Drafting**
  - Why needed here: CAST builds upon the tree-based drafting approach of EAGLE, which mitigates the "one rejection invalidates all subsequent tokens" problem of chain drafting.
  - Quick check question: In a draft tree, if a node is rejected, what happens to tokens in its sibling subtrees?

- Concept: **Confidence Scores as Acceptance Probability Proxies**
  - Why needed here: CAST uses draft model confidence scores ($v(u)$) to estimate token acceptance probabilities, which drives the entire pruning process.
  - Quick check question: How is the confidence score $v(u)$ of a node in the draft tree calculated?

## Architecture Onboarding

- Component map:
  1.  **Cost Lookup Tables**: Precomputed inference times for draft ($f_D$) and target ($f_T$) models at various batch sizes, context lengths, and sequence lengths.
  2.  **Dynamic Expansion Module**: Implements breadth and depth pruning using the lookup tables and confidence scores to build a draft tree layer-by-layer.
  3.  **Dynamic Reranking Module**: Selects the final set of nodes to verify by applying Algorithm 1 over all nodes in the expanded tree, balancing cumulative probability against verification cost.

- Critical path: The execution flow is: 1) Precompute cost tables. 2) Generate an initial draft token. 3) Enter the Dynamic Expansion loop: at each layer, calculate utilities/costs, apply breadth pruning (Alg 1), decide on next layer via depth pruning condition. 4) Once expansion stops, linearize the tree and run the Dynamic Reranking module (Alg 1 again) to pick the final verification set. 5) Verify the selected nodes with the target model.

- Design tradeoffs:
  - **Precomputation vs. Runtime Overhead**: Building and storing lookup tables adds setup complexity but avoids runtime cost estimation. The paper states, "To save the computation and storage, we only need to maintain the data of $f(B,c,n)$ for $c=kL$ and $n=1,\dots,N$."
  - **Threshold Tuning**: Performance is sensitive to thresholds $C_1$ (breadth), $C_2$ (depth), and $C_3$ (reranking). These may need retuning for new hardware or models.
  - **Batching Optimality**: The method is designed to generalize better across batch sizes, but optimal performance still requires the correct batch size to be known or the lookup tables to be populated for the relevant batch sizes.

- Failure signatures:
  - **Degradation at large batch sizes**: If the method fails to outperform EAGLE-3 at high batch sizes, the cost model may be inaccurate for that regime.
  - **Insufficient speedup**: If speedup is minimal, the thresholds ($C_1, C_2, C_3$) may be too conservative, leading to an overly small tree.
  - **Instability**: If throughput fluctuates wildly, the FIFO buffer in depth pruning may be too small or sensitive to noisy confidence scores.

- First 3 experiments:
  1.  **Cost Table Profiling**: On target hardware, measure $f_D(B,c,n)$ and $f_T(B,c,n)$ for relevant batch sizes $B$, a grid of context lengths $c$, and candidate token counts $n$.
  2.  **Ablation on Thresholds**: With a fixed model and batch size (e.g., batch size 1), vary $C_1$, $C_2$, and $C_3$ to find a stable region that maximizes speedup. Start with values from the paper's appendix.
  3.  **Batch Size Scaling Comparison**: Compare CAST against a baseline (EAGLE-3) across a range of batch sizes (1, 2, 4, 8, 16) on a standard benchmark (e.g., HumanEval). Verify that CAST's performance degrades more gracefully as batch size increases.

## Open Questions the Paper Calls Out
The paper identifies several key limitations and open questions in Appendix E, including the potential limitation of precomputing inference costs requiring lookup tables for specific hardware configurations, the need for careful threshold tuning which may require significant experimentation for new models, and the possibility that the utility-based pruning strategy may not maintain efficiency gains when applied to quantized models where memory bandwidth bottlenecks differ significantly from FP16 models.

## Limitations
- Requires pre-computation of inference cost lookup tables for each hardware configuration, limiting portability
- Performance benefits appear sensitive to careful threshold tuning that may require significant experimentation
- Does not address potential distributional shifts where draft model confidence scores become less predictive of acceptance rates

## Confidence

**High Confidence Claims:**
- CAST consistently outperforms EAGLE-2/3 across multiple benchmarks and models
- The dynamic tree construction approach provides better adaptation to hardware constraints than fixed-tree methods
- The method achieves up to 5.2x speedup relative to standard autoregressive decoding

**Medium Confidence Claims:**
- The specific utility function and threshold values (C1=4 for Vicuna-13B) represent optimal configurations
- The depth pruning buffer mechanism with FIFO policy is the most effective approach
- The linear relationship between cumulative probability and acceptance length holds universally

**Low Confidence Claims:**
- The method will generalize well to models outside the tested distribution (7B-70B scale)
- The profiling requirements (lookup table construction) scale efficiently to larger model families
- The performance gains will remain consistent with future model architectures

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary C1, C2, and C3 across their plausible ranges on a held-out model to quantify performance sensitivity and identify robust operating regions.

2. **Hardware Portability Test**: Implement CAST on a different GPU architecture (e.g., H100 vs A800) and measure the accuracy degradation of pre-computed lookup tables, quantifying the profiling overhead.

3. **Distributional Robustness Check**: Evaluate CAST on tasks with systematically different token distributions (e.g., code generation vs. conversational) to test whether the confidence score proxy remains predictive across domains.