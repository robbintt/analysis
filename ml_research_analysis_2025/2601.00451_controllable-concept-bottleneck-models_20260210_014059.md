---
ver: rpa2
title: Controllable Concept Bottleneck Models
arxiv_id: '2601.00451'
source_url: https://arxiv.org/abs/2601.00451
tags:
- concept
- dadd
- data
- predictor
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Controllable Concept Bottleneck Models (CCBMs)\
  \ to address the static nature of existing Concept Bottleneck Models (CBMs). CCBMs\
  \ enable efficient, post-hoc editing across three granularities\u2014concept-label-level,\
  \ concept-level, and data-level\u2014using influence functions and Kronecker-Factored\
  \ Approximate Curvature (EK-FAC) acceleration."
---

# Controllable Concept Bottleneck Models

## Quick Facts
- **arXiv ID:** 2601.00451
- **Source URL:** https://arxiv.org/abs/2601.00451
- **Reference count:** 40
- **Primary result:** CCBMs achieve near-retraining performance while reducing update times by over 100× through efficient post-hoc editing using influence functions.

## Executive Summary
Concept Bottleneck Models (CBMs) are interpretable models that first predict human-understandable concepts before making a final prediction. However, CBMs are static, requiring full retraining to adapt to new data or correct errors. This paper introduces Controllable Concept Bottleneck Models (CCBMs), which enable efficient post-hoc editing at three granularities: concept-label-level, concept-level, and data-level. By leveraging influence functions and Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC), CCBMs approximate the effect of dataset changes on model parameters without retraining, achieving significant speedups while maintaining accuracy. Experiments on medical, bird, face, and dermatology datasets demonstrate that CCBMs reduce update times by over 100× compared to retraining, with minimal loss in performance.

## Method Summary
CCBMs extend standard CBMs by introducing a closed-form approximation for post-hoc model editing. The method uses influence functions to estimate how changes in the training data (addition, removal, or correction) affect the model's parameters. For scalability, it employs EK-FAC to approximate the Hessian inverse for non-convex neural network components. The editing process involves a two-stage update: first updating the concept predictor (g) to reflect the data change, then updating the label predictor (f) to account for both the direct data change and the shift in concept predictions. This approach supports concept-label corrections, concept insertion/removal, and data unlearning/addition, all without full retraining.

## Key Results
- CCBMs reduce update times by over 100× compared to full retraining across all tested datasets and edit types.
- F1 score performance after editing remains close to fully retrained models, with differences typically less than 0.005.
- The method successfully handles all three edit granularities: concept-label corrections, concept addition/removal, and data unlearning/addition.
- Theoretical error bounds are provided, confirming the accuracy of the influence function approximations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CCBMs achieve efficient post-hoc edits by approximating the effect of dataset changes on model parameters via influence functions, avoiding full retraining.
- **Mechanism:** For a given edit (e.g., removing a data point), the method computes the influence of that point on the model's parameters using the gradient of the loss and the inverse Hessian matrix. The parameter update is estimated as `θ_new ≈ θ_old - H⁻¹ ∇L(z; θ)`. This is extended to the two-stage CBM architecture (concept predictor `g` and label predictor `f`) by applying the influence function to each stage sequentially.
- **Core assumption:** The loss function is twice-differentiable and the Hessian is invertible (locally convex). The approximation assumes that the relationship between parameter change and data perturbation is sufficiently linear around the current optimum.
- **Evidence anchors:**
  - [abstract] "CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining."
  - [section 4, p.4] Theorems 4.2, 4.3, 4.7-4.10 provide the formal derivation of the update rules for concept and label predictors under different edit scenarios.
  - [corpus] The corpus neighbors discuss CBM extensions (stochastic, flexible) but do not provide direct mechanistic validation for the specific influence-function-based editing framework of CCBMs. Evidence is primarily from the paper itself.

### Mechanism 2
- **Claim:** A two-stage editing strategy is required to correctly propagate changes through the CBM's dependent architecture (g then f).
- **Mechanism:** Edits to the training data or concepts first affect the concept predictor `g`. The output of the updated `g` (i.e., the predicted concepts) then serves as the input to the label predictor `f`. Therefore, `f` must be updated not only for the direct data change but also to adapt to the shifted input distribution from the updated `g`. The method first computes `Δg`, then uses the new `g'` to compute a correction term `B` for `f`'s parameters.
- **Core assumption:** The error from the first-stage approximation (`g` update) does not compound catastrophically in the second stage (`f` update), allowing for a sequential approximation to remain close to the true retrained model.
- **Evidence anchors:**
  - [section 4.1, p.4] "Observe that the input data of the label predictor comes from the output of the concept predictor, which is also subject to change. Therefore, we need to adopt a two-stage editing approach."
  - [section 4.3.1, p.6] Theorem 4.8 details the two-term update for the label predictor in data unlearning: `A_unlearn` for direct data removal, and `B_unlearn` for concept drift from `g` to `g'`.
  - [corpus] Not directly addressed. Related works focus on model structure or intervention, not the coupled update mechanism during editing.

### Mechanism 3
- **Claim:** The Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) makes the influence function computation scalable to modern neural networks.
- **Mechanism:** Direct computation and inversion of the Hessian matrix `H` for a deep network (like ResNet-18 used as `g`) is prohibitively expensive. EK-FAC approximates `H` as a Kronecker product of two smaller matrices derived from layer-wise activations and backpropagated errors (`Ω ⊗ Γ`), which are cheap to compute and invert. A damping term `λI` ensures positive definiteness.
- **Core assumption:** The Fisher Information Matrix is a good approximation of the Hessian for non-convex loss surfaces, and the Kronecker-factored decomposition captures the dominant curvature structure.
- **Evidence anchors:**
  - [section 3, p.3] Mentions EK-FAC to accelerate influence function computation for non-convex settings.
  - [Appendix 9.1, p.14-15] Provides the detailed derivation of applying EK-FAC to the convolutional layers of the concept predictor `g`.
  - [corpus] The corpus lacks papers detailing the EK-FAC acceleration method in the context of CBM editing.

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)**
  - **Why needed here:** CCBMs are a direct extension of CBMs. You must understand the standard CBM architecture (input → concept predictor `g` → label predictor `f` → output) and its goal of interpretability through human-understandable concepts.
  - **Quick check question:** Can you explain why a CBM is considered more interpretable than a standard end-to-end neural network?

- **Concept: Influence Functions**
  - **Why needed here:** This is the core theoretical tool enabling parameter estimation without retraining. You need to grasp how `I_up,params(z) = -H⁻¹∇L(z; θ)` approximates the parameter change from upweighting or removing a data point `z`.
  - **Quick check question:** What does the Hessian matrix `H` represent in the context of an influence function, and why is its inversion computationally challenging?

- **Concept: Hessian Approximations & Fisher Information Matrix**
  - **Why needed here:** Since exact Hessian inversion is infeasible, understanding how EK-FAC approximates it using the Fisher Information Matrix is critical for implementing the method efficiently.
  - **Quick check question:** How does the Fisher Information Matrix relate to the Hessian of the loss function, and what structural assumption does Kronecker-Factored approximation make?

## Architecture Onboarding

- **Component map:**
  Concept Predictor (`g`) -> Label Predictor (`f`) -> Output
  Influence Function Engine (EK-FAC for `g`, Direct for `f`) -> Edit Dispatcher

- **Critical path:**
  1. **Define Edit:** Identify the granularity (concept-label, concept, data addition/removal) and the specific indices.
  2. **Compute Gradients:** Calculate `∇L` for the affected samples/concepts with respect to `g`'s parameters.
  3. **Update `g`:** Apply the influence function update: `g_new = g_old - H_g⁻¹ * Σ(gradient_difference)`. Use EK-FAC for `H_g⁻¹`.
  4. **Update `f` (Two-Stage):**
     a. Compute direct influence term `A` based on data/concept change using `H_f⁻¹`.
     b. Compute correction term `B` to account for the shift in `g`'s outputs, using the updated `g_new` and `H_f⁻¹`.
     c. Final `f_new = f_old + A + B`.

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** EK-FAC accelerates computation at the cost of using an approximate Hessian. Larger edits or more complex architectures may increase approximation error.
  - **Edit Granularity:** Concept-level edits require parameter dimensionality changes (inserting zeros), adding complexity compared to data-level edits. Concept-label edits are fine-grained but may require more targeted updates.

- **Failure signatures:**
  - **Large Edit Magnitude:** Removing/adding a large percentage of data at once can violate the linear approximation assumption, leading to significant deviation from the retrained model. Monitor F1 score drop relative to retraining.
  - **Sequential Edit Drift:** Repeated edits without occasional full retraining can cause error accumulation, though the paper's periodic editing tests suggest this is manageable.
  - **Hessian Singularity:** If `H` is nearly singular, the damping term `λI` in EK-FAC is crucial; failure to set it properly can cause numerical instability.

- **First 3 experiments:**
  1. **Single Data Point Removal:** Remove 1 training sample from a small dataset (e.g., Derm7pt). Compare the F1 score and runtime of CCBM's update against a full retrain. Verify the `A_unlearn` and `B_unlearn` terms are computed.
  2. **Concept-Level Edit:** Remove one concept from the CUB dataset. Ensure the parameter dimensionality of `g` is correctly handled (zero-insertion) and that `f` is updated accordingly. Check for shape mismatches.
  3. **Batch Data Addition:** Add a 5% held-out subset back to the training set. Measure the runtime scaling and confirm that `A_add` and `B_add` produce a model whose performance matches the retrained baseline within a small margin (e.g., F1 difference < 0.005).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CCBM scale effectively to extremely large-scale foundation models with billions of parameters without requiring infeasible memory or computation?
- Basis in paper: [inferred] from Section 17.1 (Limitations), which notes that while EK-FAC is used, calculating and inverting the Hessian for extremely large models may still incur significant overhead requiring further optimization.
- Why unresolved: The paper validates CCBM on datasets like CelebA using ResNet-18 backbones, which are orders of magnitude smaller than modern foundation models (e.g., LLMs or ViT), leaving the scalability to billion-parameter regimes unproven.
- What evidence would resolve it: Successful application of CCBM to a Large Language Model (LLM) or large Vision Transformer (ViT), demonstrating runtime and memory usage comparable to the sub-linear scaling seen in the smaller experiments.

### Open Question 2
- Question: How does the theoretical error bound of CCBM change when applied to non-linear label predictors?
- Basis in paper: [explicit] in Section 3: "In this paper, we focus primarily on the scenarios in which the label predictor f is a linear transformation, motivated by their interpretability advantages."
- Why unresolved: The mathematical derivations in Theorems 4.3, 4.6, and 4.8 explicitly rely on the properties of linear transformations (e.g., Lemma 4.5). It is unclear if the influence function approximations hold or if error propagation becomes unstable when the label predictor contains hidden non-linear layers.
- What evidence would resolve it: Theoretical analysis of the approximation error for multi-layer perceptrons (MLPs) used as label predictors, followed by empirical comparisons of editing performance between linear and non-linear predictors.

### Open Question 3
- Question: Is the performance of concept-level editing equally robust for concept *insertion* as it is for concept *removal*?
- Basis in paper: [explicit] in Section 4.2: "For convenience, in this paper, we only consider concept removal; our method can directly extend to concept insertion."
- Why unresolved: The method handles the dimension mismatch in removal by inserting zero-row vectors. Concept insertion requires the inverse operation (expanding dimensions), which may interact differently with the Hessian approximation and pre-existing parameter distributions, potentially causing greater deviation from the retraining baseline.
- What evidence would resolve it: Empirical experiments where new concepts (e.g., new medical risk factors) are added to a pre-trained model, comparing the resulting accuracy and concept alignment against a fully retrained model.

## Limitations
- The linear approximation of influence functions may break down for very large edits (e.g., removing >50% of training data).
- While EK-FAC accelerates computation, its runtime improvement vs. full retraining for very large models (e.g., ViT) is not demonstrated.
- The method's effectiveness for non-binary concept spaces is not validated, though the formulation appears extensible.

## Confidence
- **High Confidence:** The theoretical derivation of the two-stage update rule (g then f) is mathematically rigorous and well-supported by theorems.
- **Medium Confidence:** The empirical runtime and F1 score improvements over retraining are convincing for the tested datasets and model sizes, but may not generalize to all CBM architectures or data regimes.
- **Low Confidence:** The claim that CCBMs enable "fully dynamic" model maintenance without any retraining is overstated; the paper itself resorts to periodic retraining in some experiments to control error accumulation.

## Next Checks
1. **Edit Magnitude Stress Test:** Systematically increase the percentage of data removed/added (1%, 5%, 10%, 25%, 50%) and plot the F1 score drop vs. a full retrain to identify the linear approximation's failure point.
2. **Model Size Scaling:** Apply CCBMs to a CBM with a larger backbone (e.g., ResNet-50 or ViT-Base) and measure if the claimed 100× speedup over retraining is maintained.
3. **Non-Binary Concept Test:** Modify the CUB experiment to use a multi-class attribute space (e.g., replace binary color concepts with a 5-class color attribute) and verify the update rules still produce a functional model.