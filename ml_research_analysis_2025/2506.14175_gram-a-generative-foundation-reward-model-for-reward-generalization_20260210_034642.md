---
ver: rpa2
title: 'GRAM: A Generative Foundation Reward Model for Reward Generalization'
arxiv_id: '2506.14175'
source_url: https://arxiv.org/abs/2506.14175
tags:
- reward
- gram
- data
- generative
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GRAM, a generative foundation reward model
  designed to improve reward generalization by training on both labeled and unlabeled
  data. Unlike traditional discriminative reward models, GRAM uses a two-stage approach:
  unsupervised pre-training on unlabeled input-response pairs to learn general response
  comparison patterns, followed by supervised fine-tuning on labeled preference data.'
---

# GRAM: A Generative Foundation Reward Model for Reward Generalization
## Quick Facts
- arXiv ID: 2506.14175
- Source URL: https://arxiv.org/abs/2506.14175
- Reference count: 40
- GRAM achieves 85.1% accuracy on RewardBench compared to 80.0% for generative baselines

## Executive Summary
This paper introduces GRAM, a generative foundation reward model designed to improve reward generalization by training on both labeled and unlabeled data. Unlike traditional discriminative reward models, GRAM uses a two-stage approach: unsupervised pre-training on unlabeled input-response pairs to learn general response comparison patterns, followed by supervised fine-tuning on labeled preference data. The authors show that label smoothing during training optimizes a regularized Bradley-Terry loss, unifying generative and discriminative approaches. GRAM significantly outperforms strong baselines on out-of-distribution tasks and demonstrates strong adaptability with minimal task-specific data.

## Method Summary
GRAM employs a novel two-stage training methodology for reward modeling. First, it performs unsupervised pre-training on large-scale unlabeled input-response pairs using a contrastive objective to learn general response comparison patterns. This is followed by supervised fine-tuning on limited labeled preference data. The key innovation is the use of label smoothing during training, which the authors prove optimizes a regularized Bradley-Terry loss. This approach allows GRAM to leverage both generative modeling capabilities and discriminative learning objectives, theoretically unifying these two paradigms while achieving superior generalization performance on out-of-distribution tasks.

## Key Results
- GRAM achieves 85.1% accuracy on RewardBench compared to 80.0% for generative baselines
- Significant improvements on out-of-distribution reward modeling tasks
- Demonstrates strong adaptability with minimal task-specific data
- Effectively mitigates overoptimization issues in reinforcement learning fine-tuning

## Why This Works (Mechanism)
GRAM works by combining unsupervised pre-training with supervised fine-tuning, allowing it to learn general response comparison patterns before being specialized on preference data. The label smoothing technique is mathematically shown to optimize a regularized Bradley-Terry loss, which creates a principled bridge between generative and discriminative approaches. By leveraging unlabeled data through pre-training, GRAM develops robust representations that generalize better to new tasks and distributions. The generative pre-training stage enables learning of rich response representations that discriminative models cannot capture without extensive labeled data.

## Foundational Learning
- **Bradley-Terry model**: Pairwise comparison framework for modeling preferences; needed for understanding the theoretical foundation of reward modeling; quick check: verify understanding of preference probability calculations
- **Contrastive learning**: Method for learning representations by comparing similar and dissimilar examples; needed for the unsupervised pre-training stage; quick check: understand how contrastive objectives work in preference learning
- **Label smoothing**: Regularization technique that prevents overconfident predictions; needed to bridge generative and discriminative approaches; quick check: verify how label smoothing affects the Bradley-Terry loss
- **Preference modeling**: Framework for learning from pairwise comparisons; needed as the core task GRAM addresses; quick check: understand how preferences translate to reward signals
- **Foundation models**: Large pre-trained models adapted to downstream tasks; needed to contextualize GRAM's approach; quick check: compare GRAM to other foundation model applications

## Architecture Onboarding
**Component map**: Unlabeled data → Pre-training module → Contrastive objective → General representations → Labeled preference data → Fine-tuning module → GRAM model

**Critical path**: The most critical path is from unlabeled input-response pairs through the pre-training stage to the fine-tuning stage. This path determines GRAM's ability to generalize to new distributions. The contrastive pre-training must successfully capture response comparison patterns before fine-tuning can effectively specialize the model.

**Design tradeoffs**: GRAM trades additional pre-training compute for improved generalization and reduced need for labeled data. The approach requires access to unlabeled preference data, which may not be available in all domains. The two-stage training process adds complexity compared to direct fine-tuning of discriminative models, but provides theoretical benefits through the unified generative-discriminative framework.

**Failure signatures**: GRAM may fail when unlabeled preference data is unavailable or when the pre-training stage doesn't capture relevant response patterns. Performance degradation could occur if the label smoothing parameter is poorly tuned, leading to either underfitting or loss of discriminative power. The model might also struggle with highly specialized domains where general response comparison patterns don't transfer well.

**First experiments**: 
1. Ablation study removing pre-training to quantify its contribution to generalization
2. Sensitivity analysis of label smoothing parameters on final performance
3. Comparison of GRAM's RLHF fine-tuning stability against discriminative baselines

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Requires access to large amounts of unlabeled preference data, which may not be available in all domains
- Computational overhead of pre-training compared to directly fine-tuning discriminative models is not thoroughly discussed
- Limited empirical validation of overoptimization mitigation claims in actual RL fine-tuning scenarios
- Sparse details on evaluation protocol and exact contribution isolation of individual components

## Confidence
- High confidence: GRAM's superior performance on RewardBench and ablation studies showing pre-training benefits
- Medium confidence: Claims about overoptimization mitigation in RLHF and theoretical unification of generative/discriminative approaches
- Low confidence: Practical deployment considerations and real-world RLHF performance

## Next Checks
1. Conduct comprehensive RL fine-tuning experiments comparing GRAM-based rewards versus directly trained discriminative rewards across multiple RLHF tasks
2. Perform scaling studies to understand how GRAM's performance varies with different amounts of unlabeled preference data and compute resources
3. Test GRAM's robustness to noisy or adversarial preference data to validate its generalization claims under realistic conditions