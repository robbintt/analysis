---
ver: rpa2
title: 'JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection
  Using Generative Models'
arxiv_id: '2505.13244'
source_url: https://arxiv.org/abs/2505.13244
tags:
- emotion
- base
- track
- pairwise
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses multilingual multi-label emotion detection
  for SemEval-2025 Task 11. The authors propose a system combining fine-tuned BERT-based
  classifiers and instruction-tuned generative LLMs, using two strategies: a base
  method that maps inputs to all emotion labels and a pairwise method that models
  relationships between text and each emotion category.'
---

# JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models

## Quick Facts
- arXiv ID: 2505.13244
- Source URL: https://arxiv.org/abs/2505.13244
- Reference count: 9
- Primary result: Top-4 performance in 10 languages for Track A, 1st in Hindi, top-5 in 7 languages for Track B

## Executive Summary
This paper addresses multilingual multi-label emotion detection for SemEval-2025 Task 11 by combining fine-tuned BERT-based classifiers with instruction-tuned generative LLMs. The authors propose two strategies: a base method that maps inputs to all emotion labels simultaneously, and a pairwise method that models relationships between text and each emotion category. By reformulating the task as text generation, their approach demonstrates strong cross-lingual effectiveness across 10 languages, achieving top rankings in both emotion detection and intensity prediction tracks.

## Method Summary
The system employs a dual-strategy approach combining traditional fine-tuned classifiers with generative models. The base method reformulates emotion detection as a generation task where the model outputs all relevant emotion labels in one pass. The pairwise method creates individual text-emotion pairs to capture nuanced relationships between input text and each emotion category. Both strategies leverage instruction-tuned LLMs fine-tuned on multilingual emotion datasets, allowing the system to handle multiple languages without language-specific engineering. This generative reformulation enables better generalization across linguistic boundaries compared to traditional classification approaches.

## Key Results
- Achieved top-4 performance across all 10 languages in Track A (multi-label emotion detection)
- Ranked 1st in Hindi for Track A, demonstrating strong performance in low-resource languages
- Secured top-5 performance in 7 languages for Track B (emotion intensity prediction)

## Why This Works (Mechanism)
The approach works by reformulating emotion detection as a generation task rather than classification, which allows LLMs to leverage their strong semantic understanding capabilities. The pairwise strategy explicitly models the relationship between input text and each emotion category, capturing subtle contextual cues that binary classification might miss. Instruction-tuned models bring in-domain knowledge about emotion categories and their relationships, while fine-tuning on task-specific data adapts this knowledge to the specific dataset characteristics. The combination of traditional classifiers and generative models creates a hybrid system that benefits from both approaches' strengths.

## Foundational Learning
- **Cross-lingual emotion representation**: Understanding how emotions map across languages is crucial for multilingual detection. Quick check: Verify emotion lexicons align semantically across target languages.
- **Instruction-tuning for emotion tasks**: LLMs need specialized training to understand emotion detection prompts. Quick check: Test model performance with varying prompt formulations.
- **Multi-label classification challenges**: Emotion texts often contain multiple simultaneous emotions requiring sophisticated modeling. Quick check: Evaluate precision-recall tradeoffs across emotion categories.
- **Generation vs. classification tradeoffs**: Text generation offers flexibility but may sacrifice precision compared to classification. Quick check: Compare F1 scores between generation and classification approaches.

## Architecture Onboarding
- **Component map**: Text Input -> Base Method (BERT fine-tuned classifier) -> Multi-label Output OR Text Input -> Pairwise Method (LLM generation) -> Individual Emotion Scores -> Aggregation
- **Critical path**: For base method: Input tokenization → BERT encoding → Classifier prediction → Label aggregation. For pairwise: Input-emotion pair generation → LLM generation → Confidence scoring → Final prediction.
- **Design tradeoffs**: Base method offers faster inference but may miss nuanced relationships; pairwise method captures better relationships but requires multiple generations. Choice depends on latency requirements vs. accuracy needs.
- **Failure signatures**: Prompt sensitivity leading to degraded performance; slow inference in pairwise method; potential bias toward frequent emotions in training data.
- **First experiments**: 1) Benchmark base vs. pairwise methods on held-out validation set. 2) Test prompt sensitivity by varying wording across 10 languages. 3) Measure inference time overhead for pairwise method across different batch sizes.

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- The pairwise generative strategy exhibits notably slow inference times due to multiple generation passes per input, making it impractical for real-time applications.
- System shows sensitivity to prompt formulation, with performance degrading substantially from suboptimal prompts or formatting errors.
- Cross-lingual generalization claims remain unproven beyond the 10 tested languages, particularly for languages with different scripts or linguistic structures.

## Confidence
High confidence in core performance claims based on verifiable competition results including 1st place in Hindi and top-5 rankings across 7 languages. Medium confidence in cross-lingual generalization claims due to limited testing across diverse language families. Medium confidence in simplicity claims given sensitivity to prompt engineering and system design complexity.

## Next Checks
1. Systematically evaluate performance degradation under prompt variations, including minor wording changes, formatting errors, and adversarial prompt formulations across all languages.
2. Test the approach on languages with non-Latin scripts and different linguistic structures (e.g., Arabic, Mandarin, Finnish) to assess true cross-lingual capabilities beyond the tested Romance/Germanic language set.
3. Quantify the exact computational overhead of the pairwise method versus the base method, including memory usage and wall-clock time, to determine practical deployment constraints.