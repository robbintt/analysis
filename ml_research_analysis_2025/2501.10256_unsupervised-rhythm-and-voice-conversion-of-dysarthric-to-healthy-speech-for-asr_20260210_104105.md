---
ver: rpa2
title: Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech for
  ASR
arxiv_id: '2501.10256'
source_url: https://arxiv.org/abs/2501.10256
tags:
- speech
- rhythm
- dysarthric
- conversion
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dysarthric speech ASR performance is hindered by atypical rhythm
  and voice patterns. This work introduces an unsupervised Rhythm and Voice (RnV)
  conversion framework using self-supervised speech embeddings to map dysarthric to
  typical speech rhythm and voice.
---

# Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech for ASR

## Quick Facts
- arXiv ID: 2501.10256
- Source URL: https://arxiv.org/abs/2501.10256
- Reference count: 23
- Primary result: Unsupervised rhythm and voice conversion significantly improves ASR performance on dysarthric speech, with rhythm normalization showing greater impact than voice conversion alone.

## Executive Summary
This paper addresses the challenge of automatic speech recognition (ASR) on dysarthric speech by proposing an unsupervised framework that converts dysarthric speech to typical, healthy speech patterns. The approach uses self-supervised speech embeddings to perform rhythm and voice conversion without requiring paired or transcribed data. Rhythm modeling segments speech into sonorant, obstruent, and silence types, then adjusts durations to match typical speech patterns. Voice conversion uses k-nearest neighbors to align speaker characteristics. Evaluated on the Torgo corpus with varying dysarthria severity using a frozen Whisper ASR model, the method achieves substantial word error rate (WER) improvements, particularly for severe cases.

## Method Summary
The framework uses WavLM-Large embeddings (6th layer) to represent speech. Rhythm conversion involves clustering speech frames into 100 centroids using KMeans, then hierarchically classifying them into sonorant, obstruent, and silence segments using a VAD. Duration adjustment matches these segments to typical speech distributions via either global speaking rate normalization or fine-grained gamma distribution CDF matching with linear interpolation. Voice conversion employs kNN-based frame-by-frame unit replacement using LJSpeech as the target speaker. The converted features are decoded using a pre-trained HiFi-GAN V1 vocoder. The system is evaluated on the Torgo corpus with Whisper base.en ASR, measuring WER improvements across different dysarthria severity levels.

## Key Results
- Rhythm conversion alone reduces WER by up to 28.4% relative on severe dysarthric speech
- Combined rhythm and voice conversion achieves the best overall performance
- Rhythm normalization is more impactful than voice conversion for ASR improvement
- The unsupervised approach shows consistent improvements across all severity levels

## Why This Works (Mechanism)
Dysarthric speech contains atypical rhythm and voice patterns that mismatch typical ASR models trained on healthy speech. The unsupervised approach leverages self-supervised embeddings to identify and normalize these patterns without requiring paired data. By segmenting speech into meaningful acoustic units (sonorant, obstruent, silence) and matching their durations to typical distributions, the rhythm conversion restores the temporal structure that ASR models expect. The voice conversion aligns speaker characteristics through kNN-based unit replacement, making the converted speech more compatible with models trained on typical speakers. This combination addresses both temporal and spectral mismatches between dysarthric and typical speech.

## Foundational Learning

**KMeans clustering for speech units** - Needed to discretize continuous speech embeddings into learnable units. Quick check: Verify that 100 clusters capture the major acoustic patterns in LJSpeech.

**Voice Activity Detection (VAD)** - Required to distinguish silence from speech segments during clustering. Quick check: Ensure VAD correctly identifies silence segments without false positives on sonorant sounds.

**Gamma distribution fitting** - Used to model and match duration statistics between dysarthric and typical speech. Quick check: Validate that fitted gamma parameters capture the duration differences between speech types.

**k-Nearest Neighbors retrieval** - Core mechanism for voice conversion by finding similar acoustic units. Quick check: Test kNN retrieval accuracy on LJSpeech held-out data.

**HiFi-GAN vocoding** - Converts modified embeddings back to waveforms while preserving naturalness. Quick check: Verify vocoder output maintains speech intelligibility on typical speech.

## Architecture Onboarding

**Component map**: WavLM features -> KMeans clustering -> VAD classification -> Duration matching -> kNN-VC -> HiFi-GAN -> Whisper ASR

**Critical path**: The rhythm conversion path (clustering → classification → duration adjustment) is most critical for ASR improvement, as ablation shows it contributes more than voice conversion.

**Design tradeoffs**: The unsupervised approach trades some potential accuracy for scalability and ease of deployment, avoiding the need for paired data or fine-tuning. The choice of k=8 for kNN and gamma=3 for duration interpolation are empirical parameters that balance quality and computational efficiency.

**Failure signatures**: ASR performance degradation on vocoded data compared to raw audio indicates vocoder incompatibility. Unusually high speaking rates for severe speakers suggest incorrect VAD classification or clustering failures.

**3 first experiments**:
1. Test VAD classification accuracy on clustered centroids using a known VAD algorithm
2. Measure duration statistics before and after gamma distribution matching
3. Evaluate kNN-VC retrieval quality by computing cosine similarity distributions

## Open Questions the Paper Calls Out
None

## Limitations
- The approach assumes LJSpeech speaker characteristics are representative for kNN-VC transfer without testing multiple target speakers
- No comparison to supervised methods that use paired dysarthric-healthy speech data
- The specific VAD algorithm used for segmentation is not specified, introducing potential reproducibility issues
- Claims about scalability and generalizability beyond Torgo corpus are not validated with other datasets

## Confidence
- **High Confidence**: Rhythm conversion provides greater WER improvement than voice conversion alone
- **Medium Confidence**: The unsupervised framework works effectively across different dysarthria severity levels
- **Low Confidence**: The approach will generalize well to other dysarthric speech corpora and multiple target speakers

## Next Checks
1. Reproduce the cluster-to-speech-type mapping using WebRTC VAD and verify segmentation accuracy
2. Measure Whisper WER on LJSpeech speech before and after HiFi-GAN vocoding to establish baseline performance
3. Evaluate the framework with at least two additional healthy speakers beyond LJSpeech to test kNN-VC robustness