---
ver: rpa2
title: Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs
arxiv_id: '2509.12326'
source_url: https://arxiv.org/abs/2509.12326
tags:
- zero
- rows
- random
- hidden
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Kolmogorov-Arnold (KA) geometry
  emerges spontaneously during training of conventional shallow neural networks. KA
  geometry refers to specific structural patterns in the Jacobian of the inner layer
  function, characterized by zero rows and minor concentration.
---

# Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs

## Quick Facts
- arXiv ID: 2509.12326
- Source URL: https://arxiv.org/abs/2509.12326
- Reference count: 0
- Key outcome: KA-like geometry emerges spontaneously during training of shallow MLPs on moderately complex tasks like XOR

## Executive Summary
This paper investigates whether Kolmogorov-Arnold (KA) geometry emerges spontaneously during training of conventional shallow neural networks. KA geometry refers to specific structural patterns in the Jacobian of the inner layer function, characterized by zero rows and minor concentration. The authors develop metrics to quantify these patterns and find that KA-like geometry emerges when learning moderately complex functions like XOR, but not for too simple (linear) or impossible (random) tasks. The results suggest that gradient descent discovers KA-like solutions organically for appropriate tasks, providing insight into how neural networks prepare data for downstream processing.

## Method Summary
The authors train 1-hidden layer MLPs with GeLU activation on 3-variable functions (XOR, linear, random) and measure KA geometry through Jacobian statistics. They use critical batch size (~250) to avoid dead neurons and compute four metrics: zero row percentages, participation ratios, random rotation ratios, and column divergences for size-k minors of the Jacobian. The models have m∈{4,8,16,32} hidden neurons, trained with Adam optimizer and MSE loss, with early stopping based on validation performance.

## Key Results
- KA-like geometry emerges spontaneously when learning moderately complex functions like XOR, characterized by increased zero rows and minor concentration in Jacobian minors
- The effect is most pronounced in a "Goldilocks regime" - neither too simple (linear) nor impossible (random) tasks
- Zero row percentages increase from initialization levels to 12% for k=3 at q(1)=0.01 in xor(32) models, compared to ~3-5% for linear/random tasks
- Participation ratios decrease from initial values of ~6.3 to ~6.0 for k=1, indicating minor concentration
- Random rotation ratios exceed 1.0 for k>1 in xor models, showing alignment beyond chance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent discovers solutions with KA-like geometry when learning functions of appropriate complexity
- Mechanism: Training induces example-dependent zero rows in the Jacobian of the inner layer map, reflecting locally constant coordinates similar to the KA construction
- Core assumption: The KA-like patterns observed correlate with learning useful representations, though causation is not established
- Evidence anchors: Zero row percentages increase from initialization levels to 12% for k=3 at q(1)=0.01 in xor(32) models, compared to ~3-5% for linear/random

### Mechanism 2
- Claim: Minor concentration emerges through combined scale amplification and coordinate alignment during training
- Mechanism: The k-th exterior power of the Jacobian develops concentrated distributions—most minors near zero with heavy-tailed outliers
- Core assumption: Scale and alignment are coupled, but their relative contributions vary with task complexity
- Evidence anchors: Random rotation ratios exceed 1.0 for k>1 in xor models (up to ~25x for k=3), indicating structured alignment beyond chance

### Mechanism 3
- Claim: KA geometry correlates positively with model performance on learnable nonlinear tasks
- Mechanism: As R² increases during training, zero row percentages increase and participation ratios decrease
- Core assumption: Correlation does not imply KA geometry causes improved performance—it may be a byproduct
- Evidence anchors: All three minor concentration metrics show transitions near R² ≈ 0.5 during training

## Foundational Learning

- Concept: **Jacobian matrices in neural networks**
  - Why needed here: The entire analysis rests on interpreting J(x) = ∂σ(x)/∂x and its exterior powers
  - Quick check question: Can you explain why a "zero row" in the Jacobian means a hidden neuron is locally constant with respect to all inputs?

- Concept: **Exterior powers and minors**
  - Why needed here: The paper uses k×k minors (determinants of submatrices) as the primary observable
  - Quick check question: Why would a 2×2 minor being zero indicate that two Jacobian rows are linearly dependent?

- Concept: **Kolmogorov-Arnold representation theorem**
  - Why needed here: The paper measures how closely trained MLPs approximate the structure of theoretical KA networks
  - Quick check question: What is the key structural property of KA inner functions that the authors test for?

## Architecture Onboarding

- Component map: Input (3-dim) -> Linear (n×m) + GeLU -> Jacobian J(x) -> Size-k minor matrices J^(k) -> Four KA metrics -> Comparison with initialization

- Critical path:
  1. Train 1-hidden-layer MLP with Adam, MSE loss, batch size ~250 (critical batch)
  2. Extract Jacobians J(x) for all samples post-training
  3. Compute size-k minor matrices J^(k) (elementwise |∧ⁿJ|)
  4. Calculate four metrics: zero rows, participation ratios, random rotation ratios, column divergence
  5. Compare trained vs. initialization; track evolution during training

- Design tradeoffs:
  - Batch size: Critical batch (~250) avoids "dead neurons"; small batches induce spurious zero rows
  - Hidden dimension m: Larger m shows stronger KA signals but requires more compute for minor enumeration
  - Threshold q(1): Lower values reduce false positives but may miss weak KA geometry

- Failure signatures:
  - No KA geometry for linear/random tasks at critical batch size → expected, not a bug
  - Consistently zero rows (not example-dependent) → check for dead neurons from small batch
  - Random rotation ratios ≤ 1.0 → alignment absent; task may be outside Goldilocks regime

- First 3 experiments:
  1. Replicate xor(16) training with critical batch size, plot zero row percentage vs. training epoch to verify R² ≈ 0.5 transition point
  2. Ablate batch size (32, 64, 250, 1000) on random function to confirm latent KA geometry only emerges at full batch
  3. Interpolate λ in λ-xor (0.5 to 1.5) and plot participation ratios to identify Goldilocks regime boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit regularization toward KA geometry accelerate training, and what are the computationally viable methods for implementing such interventions?
- Basis in paper: The authors attempted to discourage KA-patterning via loss terms (which prevented learning) but found it "easier to break things than to improve them"
- Why unresolved: The positive acceleration direction remains unexplored; computing the 'gradient of KA-ness' might be too costly
- What evidence would resolve it: Experiments showing that KA-encouraging interventions at specific training stages measurably reduce epochs to convergence

### Open Question 2
- Question: In which layers, training regimes, and architectures of large models does spontaneous KA geometry emerge?
- Basis in paper: This study only examined 1-hidden layer MLPs; scaling to attention-based networks, CNNs, and RNNs remains undone
- Why unresolved: Large models present combinatorial explosion of places to look for KA: Jacobians between which pair of layers; minors of which size k
- What evidence would resolve it: Systematic measurements of zero rows and minor concentration metrics across layer pairs in large transformers or CNNs

### Open Question 3
- Question: What mechanisms explain the non-monotonic batch size dependence, particularly the emergence of consistently zero rows at small batches and latent KA geometry in random-function models at full batch?
- Basis in paper: The hypothesis that small batches cause premature neuron death while full batches allow latent KA geometry to develop remains untested
- Why unresolved: The role of gradient noise vs. optimization dynamics is unclear
- What evidence would resolve it: Controlled experiments varying batch size while tracking gradient statistics and neuron activation distributions

## Limitations
- Results are confined to 1-hidden-layer MLPs with GeLU; generalization to deeper networks or other activations is untested
- The causal relationship between KA geometry and improved performance remains unproven—KA could be a byproduct rather than driver of learning
- Minor concentration metrics depend heavily on threshold choice q(1), yet optimal thresholds aren't systematically explored

## Confidence
- **High confidence**: KA geometry emerges spontaneously in the "Goldilocks regime" for moderately complex tasks like XOR
- **Medium confidence**: KA geometry correlates positively with model performance, but causation remains unproven
- **Low confidence**: Claims about scale-amplification and coordinate alignment mechanisms and their relative contributions

## Next Checks
1. Measure KA geometry emergence on varying task difficulties (λ-xor interpolation) to precisely map Goldilocks regime boundaries
2. Test whether forcing KA-like initializations accelerates learning on XOR vs random initializations
3. Evaluate KA geometry emergence in deeper networks (2+ hidden layers) to assess generalizability beyond shallow MLPs