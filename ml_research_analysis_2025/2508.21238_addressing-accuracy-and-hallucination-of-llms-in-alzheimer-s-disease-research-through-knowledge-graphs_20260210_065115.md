---
ver: rpa2
title: Addressing accuracy and hallucination of LLMs in Alzheimer's disease research
  through knowledge graphs
arxiv_id: '2508.21238'
source_url: https://arxiv.org/abs/2508.21238
tags:
- alzheimer
- disease
- graphrag
- knowledge
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of GraphRAG systems for answering
  scientific questions in Alzheimer's disease research. The researchers compared two
  GraphRAG systems (Microsoft GraphRAG and LightRAG) against a standard GPT-4o model
  using a database of 50 papers and 70 expert-curated questions.
---

# Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs

## Quick Facts
- arXiv ID: 2508.21238
- Source URL: https://arxiv.org/abs/2508.21238
- Reference count: 40
- GraphRAG systems outperformed standard GPT-4o on comprehensiveness, diversity, and empowerment metrics for Alzheimer's disease Q&A

## Executive Summary
This study evaluates two GraphRAG systems (Microsoft GraphRAG and LightRAG) against a standard GPT-4o model for answering scientific questions in Alzheimer's disease research. Using a database of 50 papers and 70 expert-curated questions, the researchers found that GraphRAG systems generated more comprehensive, diverse, and empowering responses than standard LLMs, with Microsoft GraphRAG particularly excelling in directness and empowerment metrics. However, both systems struggled with providing detailed traceability to specific sources, a critical need in scientific research. The study also revealed that LightRAG underperformed in most categories except directness, likely due to limitations in its keyword extraction approach.

## Method Summary
The researchers evaluated three conditions: GPT-4o (no context), Microsoft GraphRAG (community-based retrieval), and LightRAG (keyword-guided subgraph retrieval). All systems used GPT-4o for both indexing (entity extraction) and answering. The corpus consisted of 50 Alzheimer's disease research papers converted to text, with 70 expert-curated questions. Quality was assessed using LLM-as-judge (Claude 3.5 Sonnet) comparing pairs of answers across four metrics: comprehensiveness, diversity, empowerment, and directness. Microsoft GraphRAG indexed documents through entity/relation extraction, hierarchical community detection (Leiden algorithm), and community summarization, while LightRAG built a raw knowledge graph with incremental updates using keyword extraction for retrieval.

## Key Results
- Microsoft GraphRAG achieved highest scores on comprehensiveness, diversity, and empowerment metrics
- Both GraphRAG systems outperformed GPT-4o baseline across all quality metrics
- LightRAG only outperformed on directness metric, likely due to keyword extraction limitations
- Both GraphRAG systems struggled with detailed traceability to specific source paragraphs

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Community Summarization for Multi-Source Reasoning
- Claim: Microsoft GraphRAG's community-based indexing enables synthesis across multiple documents by pre-computing intermediate summaries
- Mechanism: Raw documents → text chunks → entity/relation extraction → knowledge graph → Leiden algorithm for hierarchical communities → LLM-generated community summaries. At query time, relevant communities are batched and synthesized into intermediate answers, then aggregated into final response
- Core assumption: Scientific questions often require reasoning across multiple papers rather than retrieving a single passage
- Evidence anchors: [abstract] "integrating domain-specific contextual information before response generation"; [section 3.2] "the system applies the Leiden algorithm to build a hierarchical community structure"

### Mechanism 2: Keyword-Guided Subgraph Retrieval (LightRAG)
- Claim: LightRAG reduces computational overhead by extracting query keywords and retrieving focused subgraphs rather than iterating over all communities
- Mechanism: Query → LLM extracts high-level (concepts) and low-level (entities) keywords → match against knowledge graph → extract subgraphs centered on matched entities → pass to LLM for answering
- Core assumption: Accurate keyword extraction reliably identifies most relevant graph regions
- Evidence anchors: [abstract] "LightRAG underperformed in most categories except directness, likely due to limitations in its keyword extraction approach"; [section 3.3] "For local mode, the LLMs will extract low-level keywords from the query"

### Mechanism 3: Grounding Responses in External Domain-Specific Knowledge
- Claim: Providing LLMs with external, curated knowledge base reduces hallucinations and improves domain accuracy compared to parametric knowledge alone
- Mechanism: RAG systems separate knowledge storage (structured database) from reasoning (LLM), allowing model to access up-to-date, domain-specific information without retraining
- Core assumption: Retrieved context is more reliable than LLM's parametric memory for specialized queries
- Evidence anchors: [abstract] "GraphRAG systems generated more comprehensive, diverse, and empowering responses than standard LLMs"; [section 4.2] "both Microsoft GraphRAG and LightRAG obtain significantly better performance on subtype Results"

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: GraphRAG is variant of RAG; understanding base pattern (index → retrieve → generate) is prerequisite
  - Quick check question: Can you explain why RAG systems retrieve documents before LLM generates response, rather than letting LLM answer directly?

- **Concept: Knowledge Graphs (Entity-Relation Structure)**
  - Why needed here: GraphRAG systems store knowledge as graphs of entities and relationships, not just text chunks
  - Quick check question: Given sentence "Tau aggregates in Alzheimer's disease brain tissue," what entities and relations would be extracted?

- **Concept: Community Detection (Graph Clustering)**
  - Why needed here: Microsoft GraphRAG uses Leiden algorithm to partition graphs into hierarchical communities for efficient retrieval
  - Quick check question: Why might clustering entities into communities improve retrieval compared to searching all nodes directly?

## Architecture Onboarding

- **Component map:** Document ingestion → text chunking → entity/relation extraction (LLM) → knowledge graph construction → community detection (Leiden) → community summaries (LLM) → retrieval → context assembly → LLM generation → response

- **Critical path:** Entity/relation extraction quality → graph structure quality → community coherence → retrieval relevance → answer quality. Errors propagate; poor extraction cannot be recovered downstream

- **Design tradeoffs:**
  - Microsoft GraphRAG: Higher indexing cost, better multi-hop reasoning, difficult to update incrementally (requires re-indexing)
  - LightRAG: Lower indexing cost, incremental updates supported, retrieval quality highly dependent on keyword extraction accuracy
  - Traceability vs. performance: More processing (summaries, graph abstraction) improves answer quality but reduces ability to trace claims to specific source paragraphs

- **Failure signatures:**
  - Generic keyword extraction (e.g., "Alzheimer's disease" in Alzheimer's database) → retrieval returns broad, uninformative context
  - Community level too abstract (level 0) → answers lack specificity
  - Community level too granular (level 2+) → high token costs, potential information overload
  - Incomplete knowledge base → LLM falls back to parametric memory, potentially hallucinating

- **First 3 experiments:**
  1. **Baseline comparison:** Run same 70 expert questions against GPT-4o (no RAG), Microsoft GraphRAG (CL=0, CL=2), and LightRAG (local/global/hybrid modes). Measure comprehensiveness, diversity, empowerment, directness using LLM-as-judge
  2. **Traceability audit:** For subset of questions, manually verify whether claims in GraphRAG responses can be traced to specific source paragraphs. Classify failures by traceability level (cluster, multi-paragraph, single-paragraph)
  3. **Keyword extraction analysis:** For LightRAG failures, inspect extracted keywords. Identify patterns where overly generic keywords caused retrieval degradation; test whether manual keyword refinement improves results

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-as-judge evaluation methodology introduces potential bias and lacks human expert validation
- Small knowledge base (50 papers) limits generalizability to larger-scale scientific domains
- Microsoft GraphRAG's high computational costs (~$0.9 per query) and inability to update incrementally present practical barriers
- Both GraphRAG systems struggle with detailed traceability to specific source paragraphs, a fundamental requirement for scientific research
- Study doesn't address temporal aspects of knowledge—handling rapidly evolving research areas where papers disagree

## Confidence

**High Confidence:** Core finding that GraphRAG systems outperform standard GPT-4o on comprehensiveness, diversity, and empowerment metrics in this specific domain and dataset size

**Medium Confidence:** Superiority of Microsoft GraphRAG over LightRAG, given latter's documented keyword extraction limitations and sensitivity to query formulation

**Low Confidence:** Specific claim that GraphRAG systems adequately address hallucination problems in scientific research; study doesn't provide quantitative measures of hallucination frequency or types

## Next Checks

1. **Human Expert Validation Study:** Conduct blinded evaluation where domain experts assess subset of GraphRAG responses for scientific accuracy, completeness, and source traceability, comparing against LLM-as-judge ratings

2. **Scalability and Cost-Benefit Analysis:** Test Microsoft GraphRAG and LightRAG systems on progressively larger knowledge bases (100, 500, 1000 papers) to measure performance scaling, cost trajectories, and identify breaking points

3. **Traceability Enhancement Experiment:** Implement and evaluate modified GraphRAG pipeline that maintains paragraph-level source attribution through community summarization process, measuring impact on scientific usability and performance tradeoffs