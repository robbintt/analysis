---
ver: rpa2
title: 'Predictive Concept Decoders: Training Scalable End-to-End Interpretability
  Assistants'
arxiv_id: '2512.15712'
source_url: https://arxiv.org/abs/2512.15712
tags:
- concepts
- decoder
- encoder
- concept
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Predictive Concept Decoders (PCDs) are end-to-end trainable architectures
  that learn to predict model behavior from activations using a sparse concept bottleneck.
  The encoder compresses activations into a small set of interpretable concepts, and
  the decoder answers questions about model behavior using only those concepts.
---

# Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants

## Quick Facts
- arXiv ID: 2512.15712
- Source URL: https://arxiv.org/abs/2512.15712
- Authors: Vincent Huang; Dami Choi; Daniel D. Johnson; Sarah Schwettmann; Jacob Steinhardt
- Reference count: 40
- Primary result: End-to-end trainable PCDs learn to predict model behavior from activations using sparse concept bottlenecks, outperforming baselines on interpretability tasks

## Executive Summary
Predictive Concept Decoders (PCDs) are a novel architecture designed to improve model interpretability by learning to predict model behavior from internal activations through a sparse bottleneck of interpretable concepts. The system compresses model activations into a small set of human-understandable concepts, then uses only these concepts to answer questions about the model's behavior. This approach enables scalable interpretability by training end-to-end on web-scale data before finetuning on specific interpretability tasks.

The key innovation is the sparse concept bottleneck that forces the encoder to learn meaningful, interpretable representations while maintaining predictive power. PCDs are pretrained using next-token prediction and then finetuned on question-answering tasks related to model behavior. The architecture demonstrates superior performance on detecting jailbreaks, revealing secret hint usage, and surfacing latent concepts, with interpretability improving as model scale increases.

## Method Summary
PCDs consist of two main components: an encoder that compresses model activations into a sparse set of interpretable concepts, and a decoder that answers interpretability questions using only those concepts. The architecture is trained end-to-end through a two-stage process: pretraining on web data using next-token prediction, followed by finetuning on specific interpretability tasks. The sparse bottleneck constraint ensures that the learned concepts remain interpretable while maintaining predictive accuracy. The encoder maps high-dimensional activations to a small set of discrete concept representations, while the decoder maps these concepts to answers about model behavior.

## Key Results
- PCDs outperform baseline interpretability methods on detecting jailbreaks and secret hint usage
- Encoder concepts become more interpretable as model scale increases
- Sparse bottleneck architecture enables effective human auditing of model behavior
- Performance improves with more training data, demonstrating scalability

## Why This Works (Mechanism)
The sparse concept bottleneck forces the encoder to learn meaningful, interpretable representations by limiting the information that can pass from activations to predictions. This constraint ensures that each concept captures a distinct aspect of model behavior that is both predictive and human-understandable. The end-to-end training approach allows the system to learn which concepts are most relevant for predicting model behavior while maintaining interpretability. Pretraining on web-scale data provides a rich foundation of concepts that can be adapted to specific interpretability tasks during finetuning.

## Foundational Learning
- Sparse bottleneck constraints - Forces learning of interpretable concepts by limiting information flow; check by measuring concept cardinality vs performance
- End-to-end training - Enables joint optimization of interpretability and predictive accuracy; check by comparing to stage-wise training
- Pretraining on web data - Provides diverse concept foundation before task-specific adaptation; check by ablating pretraining
- Discrete concept representations - Enables human-readable interpretation; check by measuring concept coherence scores
- Two-stage training process - Separates broad concept learning from task-specific adaptation; check by varying pretraining vs finetuning ratios

## Architecture Onboarding

Component map: Model Activations -> Encoder -> Sparse Concept Bottleneck -> Decoder -> Interpretability Answers

Critical path: The encoder-decoder path through the sparse bottleneck is critical, as it determines both interpretability and predictive accuracy.

Design tradeoffs: The sparsity level balances interpretability (higher sparsity) against predictive accuracy (lower sparsity). The choice of pretraining data affects concept diversity and transferability.

Failure signatures: Poor concept interpretability suggests inadequate pretraining or overly aggressive sparsity. Low predictive accuracy indicates insufficient capacity or training data.

Three first experiments:
1. Measure concept interpretability using human evaluation on a small subset
2. Ablate the sparse bottleneck to quantify its contribution to interpretability
3. Vary sparsity levels to find the optimal interpretability-predictive accuracy tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Limited testing on real-world model failure cases beyond controlled synthetic datasets
- Unclear generalizability across diverse model architectures and tasks
- Potential diminishing returns on interpretability gains at very large scales
- Reliance on pretraining data may transfer rather than learn truly interpretable concepts

## Confidence
- Core architecture and training methodology: Medium
- Performance claims on synthetic tasks: Medium
- Claims about scalability and real-world applicability: Low
- Interpretability improvements with scale: Medium

## Next Checks
1. Test PCDs on multiple diverse model architectures (not just language models) and real-world failure cases to assess generalizability
2. Conduct human subject studies to measure whether the learned concepts actually improve interpretability and auditing efficiency compared to existing methods
3. Evaluate performance degradation when the concept bottleneck is removed to quantify the true value added by the sparse constraint