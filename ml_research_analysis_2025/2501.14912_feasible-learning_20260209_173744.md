---
ver: rpa2
title: Feasible Learning
arxiv_id: '2501.14912'
source_url: https://arxiv.org/abs/2501.14912
tags:
- loss
- learning
- training
- performance
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Feasible Learning (FL), a learning framework
  where models are trained by solving a feasibility problem that bounds the loss for
  each training sample. Unlike Empirical Risk Minimization (ERM), which optimizes
  for average performance, FL demands satisfactory performance on every individual
  data point.
---

# Feasible Learning

## Quick Facts
- arXiv ID: 2501.14912
- Source URL: https://arxiv.org/abs/2501.14912
- Reference count: 40
- Key outcome: FL trains models by bounding per-sample loss, producing improved tail behavior compared to ERM with only marginal impact on average performance.

## Executive Summary
Feasible Learning (FL) reframes machine learning as a constraint satisfaction problem, requiring each training sample to have bounded loss rather than optimizing average performance. Unlike Empirical Risk Minimization (ERM), which minimizes average loss across all samples, FL demands satisfactory performance on every individual data point. To address practical challenges in setting meaningful thresholds, the authors introduce Resilient Feasible Learning (RFL), which incorporates slack variables with minimal norm penalties. The primal-dual approach dynamically re-weights sample importance during training, resulting in more concentrated loss distributions and fewer excessively high-loss instances across various tasks including image classification, age regression, and preference optimization in language models.

## Method Summary
FL frames learning as minimizing 0 subject to per-sample loss constraints ℓ(yᵢ, h(xᵢ)) ≤ ε for all training samples. The primal-dual algorithm alternates between updating Lagrange multipliers λᵢ for constraint violations and weighted gradient updates on model parameters θ. RFL extends FL by adding slack variables u ≥ 0 with quadratic penalty α‖u‖²/2, providing robustness when the original FL problem is infeasible. The optimization uses alternating gradient descent-ascent: dual variables are updated via projected gradient ascent on constraint violations, then primal parameters are updated using a weighted combination of per-sample gradients. Implementation requires storing O(n) dual variables where n is the training set size, with mini-batch updates for scalability.

## Key Results
- FL consistently produces more concentrated loss distributions across samples, reducing instances with excessively high losses
- Models trained via FL show improved tail behavior compared to ERM while maintaining similar average performance
- Lagrange multipliers in FL provide insights into sample difficulty, with samples near decision boundaries having larger multipliers
- RFL guarantees feasibility and stabilizes optimization through L₂-regularized slack variables, preventing unbounded multiplier growth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-sample constraints with adaptive Lagrange multipliers dynamically re-weight training emphasis toward harder samples.
- Mechanism: The primal-dual formulation assigns one multiplier λᵢ per data point. When gᵢ(θ) > ε, λᵢ increases; when gᵢ(θ) < ε, λᵢ decreases toward zero. Primal updates become a weighted combination of per-sample gradients, where λᵢ captures both instantaneous violation and accumulated difficulty. This concentrates optimization effort on samples near decision boundaries.
- Core assumption: The loss landscape permits non-trivial feasible solutions; non-convex constrained optimization converges via gradient descent-ascent.
- Evidence anchors:
  - [abstract] "a primal-dual approach which dynamically re-weights the importance of each sample during training"
  - [section §2.1] "data points with consistently high losses result in large multipliers, causing the primal updates to focus on reducing their loss"
  - [corpus] Weak support—neighbor papers discuss sample-level re-weighting (Sebra, Adaptive Sample-Level Framework) but not via Lagrangian feasibility formulations.
- Break condition: If the problem is infeasible (ε too tight or mislabeled data), multipliers for unsatisfiable constraints grow unboundedly, destabilizing optimization.

### Mechanism 2
- Claim: Bounding per-sample loss at ε induces functional regularization without explicit complexity penalties.
- Mechanism: Unlike ERM, which drives loss toward zero (potentially overfitting), FL accepts any solution satisfying ℓ(yᵢ, h(xᵢ)) ≤ ε. Once a constraint is satisfied, dual updates reduce λᵢ, diminishing pressure to minimize further. This prevents excessive confidence on easy samples and reduces incentive to memorize noise.
- Core assumption: The optimization algorithm does not overshoot substantially into the feasible interior before λᵢ adjusts.
- Evidence anchors:
  - [section §2] "FL does not inherently favor interpolating or non-interpolating solutions... This introduces a form of functional regularization: to prevent excessive minimization of the loss"
  - [section §5.3] "FL has fewer samples with very high losses... This suggests that while ERM performs better on 'easy' samples, FL ensures more consistent performance, especially in the tail"
  - [corpus] No direct corollary—standard regularization (L₂-regularized ERM) targets weights, not loss thresholds.
- Break condition: Overshoot due to delayed multiplier adjustment can push solutions into the feasible interior, partially negating the regularization effect.

### Mechanism 3
- Claim: Resilient FL (RFL) guarantees feasibility and stabilizes optimization via L₂-regularized slack variables.
- Mechanism: RFL adds slack u ≥ 0 with penalty α‖u‖²/2. The equivalent min-max formulation (Prop. 1) adds −‖λ‖²/(2α) to the Lagrangian, making the inner problem strongly concave. This prevents unbounded multiplier growth on infeasible constraints and provides convergence guarantees for non-convex strongly-concave min-max problems.
- Core assumption: The trade-off parameter α is tuned appropriately; too small allows excessive slack relaxation, too large approaches raw FL dynamics.
- Evidence anchors:
  - [section §3] "RFL guarantees the existence of a feasible solution, even when the original FL problem is infeasible"
  - [section §3.1] "Lα is strongly concave on λ, implying that for a fixed θ, the inner maximization has a unique solution"
  - [corpus] Weak support—Resilient Constrained Learning (Hounie et al., 2024, cited in paper) addresses constraint misspecification but in objective-constraint trade-offs, not constant-objective settings.
- Break condition: If α is too small, excessive slack relaxes constraints excessively; if too large, RFL inherits FL's instability on infeasible problems.

## Foundational Learning

- Concept: **Lagrangian Duality and KKT Conditions**
  - Why needed here: FL is a constrained optimization problem with no objective; solutions are characterized by the Lagrangian saddle-point condition λᵢ ≥ 0, λᵢ(gᵢ(θ) − ε) = 0.
  - Quick check question: Given constraint g(x) ≤ ε and multiplier λ, what is the stationarity condition for x?

- Concept: **Gradient Descent-Ascent for Min-Max Problems**
  - Why needed here: The primal-dual algorithm alternates θ-updates (descent on L) and λ-updates (ascent on L). Understanding convergence properties (non-convex strongly-concave case) is critical.
  - Quick check question: Why does alternating GDA outperform simultaneous GDA for strongly concave inner problems?

- Concept: **Constraint Qualification and Feasibility**
  - Why needed here: FL's solution set is non-empty only if ε is attainable. Understanding Slater's condition and constraint qualification helps diagnose infeasibility.
  - Quick check question: If the dataset contains duplicate samples with different labels, what happens to feasibility at ε = 0?

## Architecture Onboarding

- Component map:
  - Constraint module -> Dual variable store -> Primal optimizer -> Scheduler/controller (optional)

- Critical path:
  1. Sample mini-batch B, compute losses {gᵢ(θ)}ᵢ∈B.
  2. Update dual: λᵢ ← [λᵢ + ηλ · (gᵢ(θ) − ε)]₊ for i ∈ B (for RFL: subtract λᵢ/α).
  3. Compute primal gradient: ∇θ L = Σᵢ∈B λᵢ · ∇θ gᵢ(θ).
  4. Update θ via standard optimizer step.

- Design tradeoffs:
  - **FL vs. RFL**: FL is simpler but fragile to ε misspecification; RFL adds robustness at the cost of tuning α.
  - **Dual step size ηλ**: Too small slows convergence; too large causes oscillatory multipliers.
  - **Constraint level ε**: Task-specific; requires understanding of loss scale (e.g., cross-entropy vs. squared error).
  - **Memory**: Storing λ requires O(n) memory; negligible when dim(θ) ≫ n, but can dominate for small models on large datasets.

- Failure signatures:
  - **Divergent multipliers**: λᵢ → ∞ for some i indicates infeasibility; switch to RFL or relax ε.
  - **Zero gradients everywhere**: All λᵢ → 0 prematurely (α too small in RFL); increase α or decrease ε.
  - **No learning**: λ₀ = 0 and primal update has no effect on first step (fixed by alternating GDA with dual-first update).
  - **Numerical instability in CSERM**: Squaring clamped losses can explode gradients; prefer primal-dual RFL formulation.

- First 3 experiments:
  1. **Sanity check on toy data**: Fit a polynomial to noisy 1D data (as in Fig. 1) with ε = noise std. Verify FL recovers smoother solutions than ERM.
  2. **Infeasibility stress test**: Train on UTKFace with ε = 0 (duplicates with conflicting labels). Compare FL (should struggle) vs. RFL with α ∈ {10⁻³, 10⁻², 10⁻¹}.
  3. **Tail behavior analysis**: Train ResNet-18 on CIFAR-10 with ε = 0.51. Plot CDF and CVaR of per-sample test losses for ERM vs. RFL; verify RFL has fewer high-loss outliers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What statistical learning goals or problem types is Feasible Learning best suited for compared to ERM?
- Basis: [explicit] "In particular, future research should determine which statistical goals FL problems are best suited for."
- Why unresolved: The paper demonstrates empirical success across tasks but provides no theoretical characterization of conditions under which FL should be preferred over ERM.
- What evidence would resolve it: Theoretical analysis establishing when FL's constraint-based formulation provably leads to better generalization, robustness, or tail behavior than average-loss minimization.

### Open Question 2
- Question: Can Lagrange multiplier values be effectively leveraged for dataset pruning or active learning?
- Basis: [explicit] "Beyond simply identifying 'easy' samples, this observation could be leveraged to improve computational efficiency by pruning these samples from the dataset."
- Why unresolved: The paper observes many samples reach zero multipliers but does not test whether removing them preserves performance.
- What evidence would resolve it: Experiments demonstrating that dynamically pruning or downweighting near-zero multiplier samples maintains accuracy while reducing training cost.

### Open Question 3
- Question: Does FL provide fairness and calibration benefits due to its sample-centric nature and avoidance of zero training loss?
- Basis: [explicit] "Other potential benefits of FL—which could be explored in future work—may include fairness, due to its sample-centric nature, and calibration, as it does not demand zero training loss."
- Why unresolved: These hypothesized benefits are mentioned but not empirically validated.
- What evidence would resolve it: Comparative measurements of fairness metrics (e.g., performance parity across subgroups) and calibration error between FL and ERM models.

### Open Question 4
- Question: Why does the primal-dual RFL formulation outperform the equivalent CSERM formulation in practice?
- Basis: [inferred] Proposition 2 proves RFL and CSERM are mathematically equivalent, yet experiments show CSERM degrades with large learning rates while RFL remains stable.
- Why unresolved: The equivalence is at the problem level, but different parameterizations lead to distinct optimization dynamics that lack theoretical characterization.
- What evidence would resolve it: Analysis of gradient dynamics and conditioning differences between multiplier-based updates and clamped-and-squared loss gradients.

## Limitations

- ε selection is task-specific and lacks a principled automated tuning procedure; incorrect choice leads to either infeasible problems or trivial slack usage.
- Multiplier dynamics can become unstable in the presence of noisy or mislabeled data, especially for raw FL; RFL mitigates this but requires careful α tuning.
- Theoretical convergence guarantees apply to non-convex strongly-concave min-max problems, but deep networks often violate strong concavity in practice.
- Empirical validation is limited to a few datasets and architectures; scalability to very large models and datasets remains untested.

## Confidence

- **High confidence**: Per-sample constraint formulation and primal-dual optimization algorithm are correctly specified and implemented.
- **Medium confidence**: The claim that FL improves tail loss behavior is well-supported empirically, though generalization to other domains needs verification.
- **Low confidence**: The assertion that RFL's slack regularization is the only robust solution for infeasible problems; alternative methods may exist.

## Next Checks

1. Test FL/RFL on a large-scale vision benchmark (e.g., ImageNet) to assess scalability and tail loss improvements.
2. Conduct ablation studies on the effect of ε and α across multiple tasks to develop guidelines for hyperparameter selection.
3. Evaluate robustness to label noise by artificially corrupting datasets and comparing FL, RFL, and ERM under varying noise levels.