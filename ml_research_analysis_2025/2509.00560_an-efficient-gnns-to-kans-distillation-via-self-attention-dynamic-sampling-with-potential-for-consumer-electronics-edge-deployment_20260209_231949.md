---
ver: rpa2
title: An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling
  with Potential for Consumer Electronics Edge Deployment
arxiv_id: '2509.00560'
source_url: https://arxiv.org/abs/2509.00560
tags:
- distillation
- sa-dsd
- fr-kan
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying deep learning models
  on resource-constrained edge devices, particularly for consumer electronics, by
  proposing a novel knowledge distillation framework (SA-DSD) that transfers knowledge
  from Graph Neural Networks (GNNs) to Kolmogorov-Arnold Networks (KANs). The proposed
  method improves upon existing Fourier-based KAN (FR-KAN) models by introducing learnable
  frequency bases, phase-shift mechanisms, and a self-attention guided dynamic sampling
  approach.
---

# An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment

## Quick Facts
- arXiv ID: 2509.00560
- Source URL: https://arxiv.org/abs/2509.00560
- Reference count: 30
- Knowledge distillation from GNNs to KANs achieves 3.05%-3.62% accuracy gains over teachers and 16.96x parameter reduction

## Executive Summary
This paper proposes SA-DSD, a knowledge distillation framework that transfers knowledge from Graph Neural Networks (GNNs) to Kolmogorov-Arnold Networks (KANs) for efficient edge deployment. The method introduces learnable frequency bases, phase-shift mechanisms, and self-attention guided dynamic sampling to address the lack of explicit neighborhood aggregation in KANs. Experimental results on six real-world datasets demonstrate significant performance improvements over both GNN teachers and existing KAN baselines while achieving substantial reductions in model size and inference time.

## Method Summary
The SA-DSD framework employs a self-attention mechanism to dynamically sample nodes for distillation based on teacher-student prediction consistency. It introduces FR-KAN+, a Fourier-based KAN with learnable frequency bases and phase shifts, to better capture graph structure. The framework uses an adaptive weighted loss that combines cross-entropy with a consistency-based KL divergence term, where node sampling probabilities are computed from attention scores and teacher-student agreement. This approach addresses the fundamental challenge that KANs lack explicit neighborhood aggregation mechanisms present in GNNs.

## Key Results
- SA-DSD achieves 3.05%-3.62% accuracy improvements over three GNN teacher models (GCN, GraphSAGE, GAT)
- 15.61% accuracy gain over the FR-KAN+ baseline model
- 16.96x reduction in parameter count compared to key benchmarks
- 55.75% decrease in inference time per epoch

## Why This Works (Mechanism)
The method works by leveraging self-attention to identify nodes where teacher-student predictions differ most, focusing distillation efforts where they're most needed. The learnable frequency bases in FR-KAN+ capture multifrequency graph representations that complement the teacher's spectral filters. The adaptive weighted loss balances standard classification with knowledge transfer, while the phase-shift mechanism enables better modeling of complex node relationships. The absence of explicit neighborhood aggregation in KANs is compensated by the self-attention guided sampling that prioritizes structurally informative nodes.

## Foundational Learning

**Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data, using neighborhood aggregation to learn node representations. Needed because the method distills from GNNs to KANs, requiring understanding of how GNNs encode graph structure. Quick check: Verify GCN aggregation equation $\mathbf{h}_i^{(l)} = \sigma(\sum_{j \in \mathcal{N}(i)} \frac{1}{c_{ij}} \mathbf{W}^{(l)} \mathbf{h}_j^{(l-1)})$.

**Kolmogorov-Arnold Networks (KANs)**: Networks that replace fixed activation functions with learnable activation functions on edges, offering better approximation capabilities. Needed as the student architecture being trained. Quick check: Confirm KAN replaces $\sigma(\mathbf{Wx})$ with $\mathbf{K}(\mathbf{x})$ where K is a bivariate function.

**Knowledge Distillation**: Technique where a smaller student model learns from a larger teacher model through softened teacher predictions. Needed as the core training paradigm. Quick check: Verify temperature-scaled softmax $p_i = \frac{\exp(z_i/\tau)}{\sum_j \exp(z_j/\tau)}$.

## Architecture Onboarding

**Component map**: Data loaders -> GNN teacher pre-training -> FR-KAN+ student initialization -> SA-DSD training loop (self-attention computation -> node sampling -> adaptive loss calculation -> optimization)

**Critical path**: Teacher inference on training nodes → Self-attention computation (Eq. 8) → Node importance weighting (Eq. 9) → Consistency-based sampling (Eq. 10) → Adaptive weighted loss (Eqs. 11-13) → Student parameter updates

**Design tradeoffs**: The self-attention mechanism provides adaptive node selection but introduces $O(N^2)$ complexity; learnable frequencies offer better representation capacity but increase parameter count; adaptive weighting improves learning focus but requires careful hyperparameter tuning.

**Failure signatures**: Overfitting without distillation (large train-val gap), numerical instability in attention normalization (NaNs in $\tilde{w}$), poor inductive performance due to test graph structure leakage, and degradation when teacher-student agreement is too high or too low.

**First experiments**: 1) Implement and validate FR-KAN+ with learnable log-frequency bases and phase shifts using exact equations, 2) Test self-attention guided sampling with teacher-student agreement-based node sampling, 3) Verify adaptive weighted loss implementation with normalization fallback when node groups are empty.

## Open Questions the Paper Calls Out

**Scalability to large graphs**: How can the self-attention mechanism be reformulated to handle billion-scale graphs? The current $O(N^2)$ attention computation is prohibitive for massive graphs, requiring exploration of more scalable attention designs.

**Theoretical conditions for spectral advantage**: Under what conditions does the frequency domain parameterization provide superior distillation capacity over MLPs? The paper hypothesizes learnable frequency bases capture multifrequency graph representations but lacks formal theoretical analysis linking spectral convergence to teacher topology encoding.

**Inductive generalization with limited connectivity**: How can the framework prevent performance degradation in inductive settings with limited node connectivity? The current approach may overfit to seen structural contexts when test nodes have completely unseen neighborhood patterns without explicit edge information during inference.

## Limitations

- Missing teacher model hyperparameters (depth/width/heads, learning rate, epochs, early stopping) prevent exact reproduction
- Unspecified distillation hyperparameters (τ, γ, λ, ϵ) affect the magnitude of reported improvements
- Unknown Optuna search space and training termination criteria limit validation of optimization process
- Lack of absolute baseline values for parameter reduction and runtime measurements reduces verification capability

## Confidence

- **High confidence**: Core framework design (self-attention guided sampling + adaptive weighted loss) based on mathematical formulation clarity
- **Medium confidence**: Magnitude of reported improvements due to missing hyperparameters and potential implementation sensitivity
- **Low confidence**: Exact runtime measurements without hardware configuration details and baseline specifications

## Next Checks

1. Implement and validate the FR-KAN+ architecture with learnable log-frequency bases and phase shifts using the exact equations provided
2. Test the self-attention guided sampling mechanism with teacher-teacher agreement-based node sampling probabilities
3. Verify the adaptive weighted loss implementation, particularly the normalization fallback when a node group is empty (Eq. 10)