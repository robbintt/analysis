---
ver: rpa2
title: Byzantine-Robust Federated Learning with Learnable Aggregation Weights
arxiv_id: '2511.03529'
source_url: https://arxiv.org/abs/2511.03529
tags:
- clients
- malicious
- attack
- gradient
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedLAW, a Byzantine-robust federated learning
  framework that treats aggregation weights as learnable parameters. By jointly optimizing
  weights and model parameters with a sparsity constraint, FedLAW effectively neutralizes
  malicious clients while adaptively balancing contributions from benign clients.
---

# Byzantine-Robust Federated Learning with Learnable Aggregation Weights

## Quick Facts
- arXiv ID: 2511.03529
- Source URL: https://arxiv.org/abs/2511.03529
- Authors: Javad Parsa; Amir Hossein Daghestani; André M. H. Teixeira; Mikael Johansson
- Reference count: 40
- Primary result: FedLAW framework achieves up to 8.3 percentage points higher accuracy under label flipping attacks and 3.1 points under inverse-gradient attacks compared to state-of-the-art Byzantine-robust FL approaches

## Executive Summary
This paper introduces FedLAW, a Byzantine-robust federated learning framework that treats aggregation weights as learnable parameters. By jointly optimizing weights and model parameters with a sparsity constraint, FedLAW effectively neutralizes malicious clients while adaptively balancing contributions from benign clients. The framework employs an alternating minimization algorithm that updates weights and model parameters in tandem, providing strong theoretical guarantees for Byzantine resilience and convergence under adversarial attacks.

## Method Summary
FedLAW addresses the Byzantine-robustness problem in federated learning by introducing learnable aggregation weights that are jointly optimized with model parameters. The framework operates by alternating between two steps: updating the model parameters using current weights, and updating the aggregation weights using a sparsity-inducing regularizer to identify and downweight malicious clients. This approach leverages clean validation data available at the server to guide the weight learning process, creating an adaptive defense mechanism that can identify and mitigate various types of Byzantine attacks without requiring prior knowledge of attack patterns.

## Key Results
- FedLAW achieves up to 8.3 percentage points higher accuracy than state-of-the-art methods under label flipping attacks
- The framework shows 3.1 percentage points improvement under inverse-gradient attacks
- Superior performance is demonstrated particularly in highly non-IID and adversarial environments across MNIST and CIFAR10 datasets

## Why This Works (Mechanism)
FedLAW works by transforming the aggregation weights from fixed parameters into learnable variables that can adapt to the trustworthiness of each client. The sparsity constraint forces the optimization to set weights of malicious clients close to zero while preserving contributions from benign clients. This adaptive weighting mechanism allows the system to effectively neutralize various Byzantine attacks by learning which clients to trust based on their impact on validation performance, rather than relying on pre-defined rules or assumptions about attack patterns.

## Foundational Learning

**Federated Learning**: Distributed machine learning where multiple clients train models collaboratively while keeping data local - needed because the paper operates within this distributed paradigm where Byzantine clients can poison the global model.

**Byzantine Attacks**: Malicious behaviors in distributed systems where participants send arbitrary or adversarial messages - needed as the primary threat model that FedLAW aims to defend against.

**Alternating Minimization**: Optimization technique that iteratively minimizes over subsets of variables while fixing others - needed because FedLAW jointly optimizes both model parameters and aggregation weights through alternating updates.

**Sparsity Constraints**: Regularization techniques that encourage solutions with many zero or near-zero values - needed to force the learned weights to effectively "ignore" malicious clients by setting their weights to zero.

**Validation-based Weighting**: Using a held-out validation set to guide parameter updates - needed because FedLAW uses clean validation data to determine which clients are contributing useful information versus poisoning the model.

## Architecture Onboarding

**Component Map**: Client models -> Aggregation weights (learnable) -> Global model -> Validation loss -> Weight updates -> Client models

**Critical Path**: FedLAW's critical path follows: (1) clients compute local updates, (2) server receives updates and current weights, (3) server updates global model parameters, (4) server updates aggregation weights using validation loss, (5) new weights used in next round for model update.

**Design Tradeoffs**: The framework trades computational overhead at the server (for weight learning) against improved robustness to attacks. The requirement for clean validation data provides strong defense capabilities but limits applicability in privacy-sensitive scenarios where server-side data is unavailable.

**Failure Signatures**: The framework may fail when: (1) the clean validation data is insufficient or unrepresentative of the true data distribution, (2) malicious clients collude to mimic benign behavior patterns, (3) the attack patterns change dynamically faster than the weight learning can adapt, or (4) the sparsity constraint is too weak (λ too small) to effectively identify all malicious clients.

**First Experiments**:
1. Run FedLAW on MNIST with label flipping attacks to verify the claimed 8.3 percentage point improvement
2. Test FedLAW under inverse-gradient attacks on CIFAR10 to confirm the 3.1 point accuracy gain
3. Evaluate the framework's performance as the proportion of Byzantine clients increases from 10% to 50%

## Open Questions the Paper Calls Out

None specified in the provided materials.

## Limitations

The framework assumes the availability of clean validation data on the server, which may not be realistic in many practical scenarios. The effectiveness of the sparsity constraint in distinguishing between benign and malicious clients is theoretically justified but requires further empirical validation across diverse attack patterns. The convergence guarantees rely on specific assumptions about the loss landscape and gradient properties that may not hold in all federated learning settings.

## Confidence

**Theoretical guarantees and convergence analysis**: High
**Experimental results on MNIST and CIFAR10**: Medium (limited to two datasets and specific attack types)
**Practical applicability in real-world scenarios**: Low (requires clean validation data and assumes certain attack patterns)

## Next Checks

1. Test FedLAW's performance on additional datasets (e.g., realistic medical imaging or text data) and against a broader range of attack types, including adaptive and collusion-based attacks
2. Evaluate the framework's robustness when server-side validation data is limited or unavailable
3. Conduct large-scale experiments to assess computational overhead and scalability compared to existing Byzantine-robust methods in production-like federated learning environments