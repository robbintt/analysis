---
ver: rpa2
title: 'F-Actor: Controllable Conversational Behaviour in Full-Duplex Models'
arxiv_id: '2601.11329'
source_url: https://arxiv.org/abs/2601.11329
tags:
- speech
- audio
- system
- speaker
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces F-Actor, the first open, instruction-following
  full-duplex speech model that can be trained efficiently on academic resources.
  By freezing the audio encoder and fine-tuning only the LLM, the model requires just
  2,000 hours of data and two days on four A100-40GB GPUs.
---

# F-Actor: Controllable Conversational Behaviour in Full-Duplex Models

## Quick Facts
- arXiv ID: 2601.11329
- Source URL: https://arxiv.org/abs/2601.11329
- Reference count: 40
- Open, instruction-following full-duplex speech model trained efficiently on academic resources

## Executive Summary
F-Actor is the first open, instruction-following full-duplex speech model that achieves controllable conversational behavior with academic-level resources. By freezing the audio encoder and fine-tuning only the LLM, the model requires just 2,000 hours of data and two days on four A100-40GB GPUs. It can follow explicit instructions to control speaker voice, conversation topic, conversational behavior (e.g., backchanneling and interruptions), and dialogue initiation. Systematic analysis shows the best configuration uses word-level alignment, a 2-token audio delay, and a text stream, achieving 99.8% accuracy in dialogue initiation and strong correlations (0.54 for backchannels, 0.25 for interruptions) between prompt-specified and generated behaviors.

## Method Summary
F-Actor uses a frozen NanoCodec encoder with FSQ quantization to produce discrete acoustic units (DAUs), which are processed by a trainable Llama3.2-1B-Instruct backbone. The model conditions on ECAPA-TDNN speaker embeddings and text prompts, with separate embedding matrices for user and system streams. Key design choices include word-level alignment between audio and text with a 2-token audio delay, and training on 2,000 hours of synthetic English conversations (Behavior-SD). The model is trained for ~48 hours on 4Ã—A100-40GB GPUs with only the LLM backbone and linear DAU heads being trained.

## Key Results
- 99.8% accuracy in dialogue initiation with word-level alignment and 2-token audio delay
- 54% cosine similarity for speaker embedding consistency across 52-speaker closed set
- Strong correlations between prompt-specified and generated behaviors: 0.54 for backchannels, 0.25 for interruptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient full-duplex model training is achievable with frozen audio encoder and LLM-only fine-tuning
- Mechanism: The pre-trained NanoCodec encoder produces meaningful discrete acoustic units (DAUs) using finite scalar quantization (FSQ). By freezing this encoder and training only the LLM backbone plus linear prediction heads, the model learns to process/generate these units without learning acoustic representations from scratch. This reduces training to ~2,000 hours and 48 hours on 4 A100s.
- Core assumption: The frozen encoder provides sufficiently expressive audio representations for the target domain
- Evidence anchors:
  - [abstract]: "By keeping the audio encoder frozen and fine-tuning only the LLM, our model requires just 2,000 hours of data"
  - [Section 6.2]: "Only the LLM backbone and the linear DSU heads are trained, while the speech encoder remains frozen"
  - [corpus]: Related papers (Moshi, dGSLM) use similar codec-based approaches but require larger resources; no direct frozen-encoder comparison available
- Break condition: If target domain involves acoustic features poorly represented in the pre-trained codec (unseen languages, specialized audio environments), frozen encoder will fail to capture necessary distinctions

### Mechanism 2
- Claim: Adding a text stream with word-level alignment and 2-token audio delay substantially improves both speech coherence and instruction-following
- Mechanism: The text stream provides explicit linguistic grounding that guides audio generation. Word-level alignment (vs. utterance-level) maintains fine-grained synchronization, while the 2-token audio delay gives the model a "head start" to plan audio tokens after text prediction. This reduces WER from 23%+ to 7-8%.
- Core assumption: Text should temporally precede audio at the word level for coherent generation
- Evidence anchors:
  - [Section 7.1]: "Introducing a text stream alongside audio generation substantially improves performance across all metrics, reducing perplexity from 25.23 to 21-23"
  - [Section 7.1]: "Word-level alignment between audio and text further improves synchronization compared to utterance-level, reducing WER from over 23% to approximately 7.2% when combined with an audio delay of 2 tokens"
  - [corpus]: TurnGuide paper supports text-speech interleaving for meaningful dialogue, corroborating text-audio coupling benefits
- Break condition: If using utterance-level alignment OR zero audio delay, WER degrades to 17-33% and instruction-following accuracy for dialogue initiation drops to ~50% (random)

### Mechanism 3
- Claim: Dedicated embedding matrices per stream and codebook enable reliable speaker disambiguation during full-duplex processing
- Mechanism: Each codebook (1-4) for each stream (user, system) has its own embedding layer. These are summed across codebooks and streams to produce the LLM input. Separate matrices ensure speaker identity persists through summation, enabling overlapping speech processing without speaker confusion.
- Core assumption: Independent embedding spaces for user/system create distinct representations that survive the summation operation
- Evidence anchors:
  - [Section 4]: "Separate embedding matrices for the user and system streams allow the model to reliably distinguish speakers"
  - [Table 2]: Speaker similarity consistently achieves 0.52-0.54 cosine similarity with minimal drift (0.61-0.67)
  - [corpus]: SALMONN-omni uses alternative codec-free approach; direct comparison of embedding strategies unavailable
- Break condition: If speaker embeddings are omitted or shared across streams, speaker similarity drops (Table 2 shows drop to 0.36 without speaker embeddings)

## Foundational Learning

- Concept: **Discrete Acoustic Units (DAUs) and Quantization Methods**
  - Why needed here: The entire architecture relies on converting raw audio to discrete tokens. Understanding RVQ (dependent codebooks, sequential prediction) vs. FSQ (independent codebooks, parallel prediction) determines architecture complexity.
  - Quick check question: Why does FSQ enable parallel prediction across codebooks while RVQ requires hierarchical/sequential prediction?

- Concept: **Full-Duplex Speech Dynamics**
  - Why needed here: Unlike turn-based systems, full-duplex models must handle backchannels (brief acknowledgments while other speaks), interruptions, and overlapping speech simultaneously.
  - Quick check question: What is the key difference between handling user interruptions vs. generating system-initiated interruptions?

- Concept: **Speaker Embedding Consistency and Drift**
  - Why needed here: The model conditions on ECAPA-TDNN speaker embeddings for voice control. Understanding cosine similarity (0.54 achieved) and drift metrics (distance between first/last segments) is essential for debugging voice consistency.
  - Quick check question: If speaker drift increases over a long conversation, what architectural component should be investigated first?

## Architecture Onboarding

- Component map:
  - Raw audio (user + system streams) -> NanoCodec (frozen, FSQ, 4 codebooks @ 12.5fps) -> 8 DAU sequences
  - Separate embedding matrices per stream/codebook -> Sum across codebooks and streams
  - ECAPA-TDNN speaker embedding (5-sec reference) + text prompt (narrative, behavior specs) -> Projected and prepended
  - Llama3.2-1B-Instruct backbone (1B params, trainable)
  - 8 linear heads -> 4 user + 4 system codebook logits -> Sample DAUs -> NanoCodec decoder -> Waveform
  - Word-aligned text tokens via Kaldi forced alignment -> Added to system audio embeddings

- Critical path:
  1. NanoCodec produces valid DAUs in range [0, 4031] for each of 4 codebooks
  2. Separate embedding initialization ensures user/system remain distinguishable post-summation
  3. Word-level text-audio alignment with 2-token audio delay (not utterance-level, not zero delay)
  4. Sampling temperature = 0.9 (Table 6: lower reduces dynamism, higher degrades quality)

- Design tradeoffs:
  - **FSQ vs RVQ codec**: Table 7 shows RVQ (Mimi) drops UTMOS from 3.4 to 2.5 despite comparable perplexity; FSQ preferred for quality
  - **Word vs utterance alignment**: Word-level reduces WER from 23%+ to 7-8%, but requires forced alignment preprocessing (74.4% data retained)
  - **System-only (s) vs system+user (s/u) loss**: s/u improves narrative adherence (2.39 vs 2.27) but may not justify complexity for all use cases

- Failure signatures:
  - **WER >15% with text stream present**: Check alignment granularity (should be word-level) and audio delay (should be exactly 2 tokens)
  - **Dialogue initiation accuracy ~50%**: Text stream likely missing or audio delay = 0; Table 2 shows these configurations achieve random performance
  - **Low backchannel/interruption correlation**: Training data sparsity (only 0.9 interruptions/conversation average); prompt serves as directional signal, not exact count
  - **Speaker similarity <0.40**: Verify speaker embedding projection is applied; Table 2 shows omission drops similarity to 0.36

- First 3 experiments:
  1. **Baseline reproduction**: Train best config (word alignment, delay=2, s/u loss, FSQ) on 10% data subset; target perplexity ~22-24, WER ~10-12%, UTMOS ~3.3-3.5
  2. **Text stream ablation**: Remove text stream entirely; expect perplexity increase to 25+ and WER to 10%+ to validate text stream necessity
  3. **Temperature calibration for use case**: Sweep [0.6, 0.8, 0.9, 1.0] on dev set; higher temperature increases backchannel/interruption correlation but reduces UTMOS (Table 6 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the model be modified to enforce exact counts of backchannels and interruptions rather than treating the prompt merely as a directional signal?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the model "consistently produces fewer such events than requested" and currently relies on the prompt only as a directional indicator rather than an exact counter.
- **Why unresolved:** The training data (Behavior-SD) contains relatively rare instances of these events, preventing the model from learning precise frequency control.
- **What evidence would resolve it:** A model trained on augmented data with higher event density that achieves near 1:1 adherence to prompted numerical instructions.

### Open Question 2
- **Question:** Can the correlation for interruptions (currently 0.25) be improved to match backchannel performance (0.54) through specialized loss functions or data balancing?
- **Basis in paper:** [inferred] The results show a large gap in control performance, which the authors attribute to interruptions being "rare, averaging only 0.9 per conversation" in the training set.
- **Why unresolved:** Standard training objectives do not sufficiently weight these rare, short-duration events, leading to poor correlation with user instructions.
- **What evidence would resolve it:** Experiments showing significantly higher Pearson correlation for interruptions on a held-out test set using weighted losses or oversampling.

### Open Question 3
- **Question:** Does the efficient training recipe (frozen encoder, 2,000 hours) transfer effectively to non-English languages where full-duplex datasets are currently unavailable?
- **Basis in paper:** [explicit] The authors identify in the Limitations section that testing other languages is "currently infeasible" due to the lack of available training data.
- **Why unresolved:** The architecture's efficiency is proven only for English; it is unknown if the "academic budget" approach scales to languages with different prosodic structures without more data.
- **What evidence would resolve it:** Successful replication of F-Actor's instruction-following metrics on a synthetic non-English dataset using the same hardware constraints.

## Limitations

- Data representation bias: Trained on synthetic Behavior-SD dataset with pre-defined conversational behaviors, limiting generalization to naturalistic human conversations
- Closed speaker set: 52-speaker constraint prevents generalization to unseen speakers without additional fine-tuning
- Behavior detection thresholds: Evaluation uses specific thresholds optimized for Parakeet V2, which may not generalize to other detection methods

## Confidence

**High confidence (Mechanistic claims with direct evidence)**:
- Frozen encoder + LLM-only fine-tuning reduces training requirements to 2,000 hours and 48 hours on 4 A100s
- Word-level alignment with 2-token audio delay reduces WER from >23% to ~7%
- Separate embedding matrices per stream enable speaker disambiguation (cosine similarity 0.52-0.54)

**Medium confidence (Behavior correlation claims)**:
- Pearson correlation 0.54 for backchannels and 0.25 for interruptions between prompts and generated content
- These correlations are statistically significant but moderate, suggesting the model captures behavioral direction but not precise counts
- Limited by detection method sensitivity and the directional nature of prompts

**Medium confidence (Subjective quality claims)**:
- UTMOS score of 3.4 on a 5-point scale indicates "acceptable" quality
- This is a relative measure against the training data (Behavior-SD), not absolute human-likeness
- The model generates coherent conversations but quality assessment depends heavily on reference data characteristics

## Next Checks

1. **Speaker generalization test**: Evaluate F-Actor on an unseen speaker dataset (e.g., VCTK Corpus or LibriSpeech test sets) to measure speaker embedding drift and voice consistency outside the 52-speaker training set. This validates whether the 0.54 cosine similarity generalizes beyond the closed set.

2. **Behavior detection method comparison**: Re-run the backchannel and interruption correlation analysis using alternative detection methods (e.g., Silero-VAD vs. Parakeet V2) with multiple threshold configurations. This validates that the 0.54/0.25 correlations are robust to detection methodology rather than artifacts of specific parameter choices.

3. **Open-domain instruction following**: Test the model's ability to follow instructions for topics, voices, and behaviors not represented in Behavior-SD (e.g., emotional tone control, code-switching between languages). This validates whether the instruction-following capability generalizes beyond the synthetic training domain or is overfit to Behavior-SD's specific construction.