---
ver: rpa2
title: 'Plain language adaptations of biomedical text using LLMs: Comparision of evaluation
  metrics'
arxiv_id: '2512.16530'
source_url: https://arxiv.org/abs/2512.16530
tags:
- health
- level
- approaches
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated Large Language Models (LLMs) for simplifying\
  \ biomedical texts to improve health literacy. Using the PLABA dataset of manually\
  \ adapted biomedical abstracts, researchers compared three approaches\u2014baseline\
  \ prompt template, two-AI-agent iterative refinement, and fine-tuning\u2014across\
  \ OpenAI's GPT-4o and GPT-4o-mini models."
---

# Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics

## Quick Facts
- arXiv ID: 2512.16530
- Source URL: https://arxiv.org/abs/2512.16530
- Authors: Primoz Kocbek; Leon Kopitar; Gregor Stiglic
- Reference count: 13
- Primary result: GPT-4o-mini outperformed GPT-4o in biomedical text simplification, with baseline prompt engineering nearly matching two-agent iterative refinement

## Executive Summary
This study evaluates Large Language Models (LLMs) for simplifying biomedical texts to improve health literacy. Using the PLABA dataset of manually adapted biomedical abstracts, researchers compared three approaches—baseline prompt template, two-AI-agent iterative refinement, and fine-tuning—across OpenAI's GPT-4o and GPT-4o-mini models. The evaluation combined quantitative metrics (Flesch-Kincaid grade level, SMOG, SARI, BERTScore, G-Eval) with qualitative assessments by healthcare experts on simplicity, accuracy, completeness, and brevity using 5-point Likert scales.

Results demonstrated that GPT-4o-mini consistently outperformed GPT-4o, while the baseline prompt engineering approach nearly matched the more complex two-agent iterative refinement method. Fine-tuning underperformed despite achieving similar text complexity to ground truth. Notably, G-Eval, an LLM-based evaluation metric, aligned closely with human assessments, suggesting its potential as a scalable alternative for domain-specific text simplification evaluation.

## Method Summary
The study employed a mixed-methods approach to evaluate LLM-based biomedical text simplification. Researchers utilized the PLABA dataset containing manually adapted biomedical abstracts as ground truth. Three approaches were tested: a baseline prompt template, a two-AI-agent iterative refinement process, and fine-tuning of the models. Two OpenAI models (GPT-4o and GPT-4o-mini) were compared across these approaches. Evaluation combined automated metrics including readability scores (Flesch-Kincaid, SMOG), generation quality metrics (SARI, BERTScore), and LLM-based assessment (G-Eval) with human evaluation by healthcare experts rating simplified texts on simplicity, accuracy, completeness, and brevity using 5-point Likert scales.

## Key Results
- GPT-4o-mini consistently outperformed GPT-4o across multiple evaluation metrics in biomedical text simplification
- Baseline prompt engineering achieved nearly equivalent performance to the more complex two-agent iterative refinement approach
- G-Eval, an LLM-based evaluation metric, showed strong alignment with human expert assessments, suggesting its viability as a scalable evaluation alternative

## Why This Works (Mechanism)
The effectiveness of LLMs in biomedical text simplification stems from their ability to understand complex domain-specific terminology and restructure information while preserving semantic meaning. The study demonstrates that simpler prompting strategies can achieve comparable results to more elaborate multi-agent approaches, suggesting that careful prompt engineering may be sufficient for many simplification tasks. The success of G-Eval indicates that LLMs can effectively evaluate their own outputs in domain-specific contexts, potentially reducing reliance on human evaluators for iterative development and quality assessment.

## Foundational Learning
**Biomedical text simplification**: The process of converting complex medical literature into more accessible language while maintaining accuracy - needed to improve health literacy and patient understanding; quick check: evaluate simplified text with target audience comprehension tests
**Readability metrics**: Quantitative measures like Flesch-Kincaid and SMOG that assess text complexity - needed to objectively compare simplification approaches; quick check: verify metric scores against human readability assessments
**SARI metric**: Evaluates text simplification by comparing system output to references and the original text - needed to measure improvement over source material; quick check: test SARI sensitivity to different types of text modifications
**BERTScore**: Uses contextual embeddings to evaluate semantic similarity between texts - needed to assess meaning preservation in simplification; quick check: compare BERTScore with human semantic similarity judgments
**G-Eval**: LLM-based evaluation metric that assesses text quality using the same model that generates it - needed for scalable, cost-effective evaluation; quick check: correlate G-Eval scores with human expert ratings across multiple domains
**Prompt engineering**: The practice of designing effective prompts to guide LLM behavior - needed to optimize model performance without additional training; quick check: A/B test different prompt structures for specific tasks

## Architecture Onboarding

**Component Map**: Input Text -> LLM Model (GPT-4o/GPT-4o-mini) -> Simplification Process (Baseline/Two-Agent/Fine-tuned) -> Output Text -> Evaluation (Automated Metrics + Human Assessment)

**Critical Path**: The sequence from input biomedical abstract through the chosen simplification approach to final evaluation represents the critical path. The most time-consuming elements are typically the human evaluation phase and, for fine-tuned approaches, the model training process. The two-agent iterative refinement adds computational overhead but may improve quality through multiple refinement passes.

**Design Tradeoffs**: The study reveals that simpler approaches (baseline prompt engineering) can match more complex methods (two-agent refinement), suggesting a tradeoff between implementation complexity and performance gain. GPT-4o-mini's superior performance despite lower computational cost indicates a favorable cost-performance ratio. Fine-tuning's underperformance despite significant resource investment highlights the potential limitations of task-specific adaptation for this domain.

**Failure Signatures**: Fine-tuning underperforming expectations suggests potential issues with training data quality, insufficient model capacity, or suboptimal hyperparameter tuning. Human evaluation limitations (three experts) may introduce bias and reduce statistical power. The use of a single dataset (PLABA) limits generalizability and may not capture the full complexity of biomedical text simplification challenges across different document types.

**First Experiments**:
1. Test prompt variations systematically to identify optimal engineering strategies
2. Evaluate G-Eval against human assessments across multiple domains to validate generalizability
3. Compare fine-tuning performance using different model architectures and learning rates

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on a single dataset (PLABA) may limit generalizability to other biomedical text types or domains
- Human evaluation conducted by only three healthcare experts, potentially introducing rater bias and limiting statistical power
- Fine-tuning approach underperformed expectations, suggesting potential issues with training methodology or data quality that remain unexplained

## Confidence

**High confidence**: GPT-4o-mini outperforming GPT-4o across multiple metrics; baseline prompt engineering effectiveness

**Medium confidence**: G-Eval alignment with human evaluations; two-agent approach marginal improvement over baseline

**Low confidence**: Fine-tuning approach results and comparative analysis; specific contribution of each evaluation metric

## Next Checks

1. Replicate the study using additional biomedical datasets (e.g., clinical notes, patient education materials) to assess generalizability
2. Expand human evaluation to 10+ healthcare experts with inter-rater reliability analysis to strengthen qualitative findings
3. Conduct ablation studies on fine-tuning methodology, including different model architectures, learning rates, and data augmentation techniques to identify performance bottlenecks