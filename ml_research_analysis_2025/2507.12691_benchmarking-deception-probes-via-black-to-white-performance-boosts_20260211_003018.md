---
ver: rpa2
title: Benchmarking Deception Probes via Black-to-White Performance Boosts
arxiv_id: '2507.12691'
source_url: https://arxiv.org/abs/2507.12691
tags:
- reasoning
- deception
- probe
- auroc
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new methodology for benchmarking deception
  probes by measuring black-to-white performance boosts - how much better deception
  detection becomes when monitors have access to probe activations versus relying
  only on black-box observations. The authors evaluate eight monitoring strategies
  across five strategic deception datasets using deception probes trained by Goldowsky-Dill
  et al.
---

# Benchmarking Deception Probes via Black-to-White Performance Boosts

## Quick Facts
- arXiv ID: 2507.12691
- Source URL: https://arxiv.org/abs/2507.12691
- Reference count: 40
- Primary result: Deception probes achieve strong detection performance (0.941 and 0.926 AUROC) but show only modest black-to-white performance boosts when compared to text-only baselines

## Executive Summary
This paper introduces a benchmarking framework for evaluating deception probes by measuring black-to-white performance boosts - how much better deception detection becomes when monitors have access to probe activations versus relying only on black-box observations. The authors evaluate eight monitoring strategies across five strategic deception datasets using deception probes trained by Goldowsky-Dill et al. [2025]. They find weak but encouraging black-to-white performance boosts, with probe-based methods like Full Averaging and Critical Averaging achieving strong detection performance (0.941 and 0.926 AUROC respectively). The study also reveals that existing probes activate across entire deceptive responses rather than localizing to specific false claims, and that probe-based methods remain effective even when deceptive responses are padded with irrelevant true facts (deception dilution).

## Method Summary
The paper benchmarks deception probes by comparing white-box monitoring (with access to probe scores from layer 22 residual stream activations) against black-box monitoring (text-only). Eight monitors are evaluated: Full Averaging (mean activation over public tokens), Peak Activation (max activation), follow-up query methods, Critical Averaging (LLM-identified claim subset), White-box Reasoning (Claude-3.5-Sonnet with probe scores), Ensemble (weighted combination), and Black-box Reasoning baseline. The primary metric is AUROC, with black-to-white boost calculated as the difference between white-box and black-box AUROC. Five datasets are used: AI Liar, Deception Dilution, Insider Trading, Sandbagging, and Backdoors, with Alpaca as a control dataset.

## Key Results
- Full Averaging and Critical Averaging achieve strong detection performance (0.941 and 0.926 AUROC respectively) across multiple deception tasks
- Existing probes activate across entire deceptive responses rather than localizing to specific false claims
- Probe-based methods remain effective even when deceptive responses are padded with irrelevant true facts (deception dilution)
- Significant distributional shift challenges when applying text-trained probes to code outputs (Backdoors dataset achieving only 0.55 AUROC)

## Why This Works (Mechanism)

### Mechanism 1
Linear probes capture a latent "deceptive intent" direction in residual stream activations that correlates with strategic deception. Logistic regression projects layer-22 residual stream activations onto a linear direction, producing scalar scores per token. When the model is in a "deceptive mode," activations along this direction systematically differ from honest-mode activations.

### Mechanism 2
Probe activations spread across entire deceptive responses rather than localizing to specific false claims. The probe captures a global "deceptive mode" in the model's internal state, which persists across tokens once activated. Averaging across all public tokens accumulates this diffuse signal.

### Mechanism 3
Black-to-white performance boosts quantify the marginal value of internal state access beyond behavioral observation. By comparing a white-box monitor (with probe scores) to a matched black-box baseline, the methodology isolates the information gain from internal activations.

## Foundational Learning

- **Linear probing of language model internals**: Understanding how linear classifiers extract features from residual stream activations is foundational to interpreting probe outputs and their limitations.
  - Quick check: Given a residual stream vector at layer L and a trained linear probe, how would you compute the per-token deception score?

- **AUROC and class imbalance**: The paper uses AUROC as the primary metric; understanding its properties (threshold-independence, sensitivity to class imbalance) is necessary to interpret results correctly.
  - Quick check: If a detector flags 90% of deceptive samples but also 40% of honest samples, would the AUROC be higher or lower than a detector with 80% TPR at 10% FPR?

- **Distributional shift and out-of-distribution generalization**: Probes trained on one deception distribution are applied to qualitatively different tasks; understanding why performance degrades is critical.
  - Quick check: If a probe trained on text-based deception is applied to code outputs, what failure mode would you expect and why?

## Architecture Onboarding

- **Component map**: Probe extraction (layer 22 residual stream) -> Token classification (public vs private) -> Aggregation strategies (Full Averaging, Peak Activation, Critical Averaging) -> Monitoring layer (numeric-only, follow-up queries, or hybrid) -> Baseline comparison

- **Critical path**: Load pre-trained probe weights -> Run inference capturing layer-22 activations per token -> Filter to public tokens -> Apply aggregation (start with Full Averaging) -> Compare to threshold calibrated on control dataset

- **Design tradeoffs**: Full Averaging vs Peak Activation (robustness vs sensitivity), Pairs vs RP probe (generalization vs roleplay-specific signals), Hybrid vs numeric-only (context incorporation vs interpretability)

- **Failure signatures**: Distributional shift (elevated probe scores on honest samples in new domains), Deception dilution (adversarial padding with true facts), Non-localized signal (cannot pinpoint which claim is false)

- **First 3 experiments**: Replicate Full Averaging baseline on AI Liar and Insider Trading datasets; Stress-test with dilution by appending 100 true facts; Measure black-to-white boost across all five datasets

## Open Questions the Paper Calls Out

- Can deception probes be trained to localize false claims within a response rather than activating broadly?
- How do probe-based monitors perform against more sophisticated adversarial evasion strategies?
- Does free-form multi-turn dialogue improve the detection of deception over single Yes/No follow-up queries?

## Limitations

- Significant distributional shift challenges when applying text-trained probes to code outputs
- Probes cannot localize deception to specific claims, limiting utility for auditing
- Black-box baseline may be unrealistically easy to separate from deception datasets

## Confidence

- Probe-Based Detection Performance: High confidence in strong AUROC scores across multiple tasks
- Distributional Shift Limitations: High confidence in dramatic performance drops and elevated honest sample scores
- Black-to-White Performance Boost Interpretation: Medium confidence due to uncertainty about black-box baseline optimization

## Next Checks

1. Design a new probe training objective that explicitly encourages activation concentration on deceptive claims and validate performance on the Dilution dataset.

2. Systematically evaluate probe performance across multiple code-generation datasets with varying levels of deception complexity.

3. Implement and compare against state-of-the-art text-only deception detection methods to establish whether reported black-to-white boosts represent genuine value-add.