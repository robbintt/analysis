---
ver: rpa2
title: 'Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable
  Action Expert'
arxiv_id: '2510.03896'
source_url: https://arxiv.org/abs/2510.03896
tags:
- action
- arxiv
- expert
- data
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework that decouples high-level VLM
  planning from low-level action execution by introducing a generalizable action expert.
  The expert takes sparse 3D waypoints generated by the VLM and refines them into
  dense, executable action sequences using real-time point cloud observations.
---

# Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert

## Quick Facts
- arXiv ID: 2510.03896
- Source URL: https://arxiv.org/abs/2510.03896
- Authors: Mingyu Liu; Zheng Huang; Xiaoyi Lin; Muzhi Zhu; Canyu Zhao; Zongze Du; Yating Wang; Haoyi Zhu; Hao Chen; Chunhua Shen
- Reference count: 26
- Primary result: Achieves 60% average success rate on long-horizon tasks and 0.75 average success rate in real-world tasks, significantly outperforming generalist baselines.

## Executive Summary
This paper addresses the challenge of bridging high-level visual language model (VLM) planning with low-level robotic action execution by introducing a generalizable action expert. The key innovation is using sparse 3D waypoints as an intermediate representation, allowing the VLM to focus on semantic planning while the action expert handles geometric refinement. The proposed "Action Pre-training, Pointcloud Fine-tuning" (APPF) paradigm significantly improves training efficiency and enables zero-shot generalization across diverse visual domains and camera viewpoints. Extensive experiments demonstrate the method's effectiveness on both simulated and real-world tasks, with performance on par with single-task expert models in short and middle-horizon tasks and superior performance in long-horizon tasks.

## Method Summary
The framework decouples VLM planning from action execution through sparse 3D waypoints. The VLM generates coarse 3D waypoints in the camera frame from RGB images and language instructions. These waypoints are converted to dense, executable trajectories via B-spline interpolation. A generalizable action expert, implemented as a conditional diffusion policy with PointNet-based point cloud encoding, refines these trajectories using real-time point cloud observations. The APPF training paradigm first pre-trains the action expert on pure trajectory data for motor skill learning, then fine-tunes with point cloud conditioning for environment awareness. Noise injection during training improves robustness to imperfect VLM-generated waypoints.

## Key Results
- Achieves 60% average success rate on long-horizon tasks and 0.75 average success rate in real-world tasks
- Outperforms generalist baselines by significant margins across all task horizons
- Matches single-task expert performance on short and middle-horizon tasks while exceeding them on long-horizon tasks
- Demonstrates strong zero-shot generalization across diverse visual domains, camera viewpoints, and language instructions

## Why This Works (Mechanism)

### Mechanism 1
Sparse 3D waypoints as an intermediate representation enable clean decoupling of high-level planning from low-level execution. The VLM generates coarse 3D waypoints in the camera frame, which are then interpolated via B-spline into continuous trajectories. This shifts the action expert's role from semantic-to-action mapping to geometric trajectory refinement. Core assumption: VLMs are inherently better at generating geometric coordinates aligned with their image-based pre-training than predicting actions in robot base frames.

### Mechanism 2
The "Action Pre-training, Pointcloud Fine-tuning" (APPF) paradigm improves data efficiency and convergence by separating motor skill learning from environment-aware refinement. Phase 1 trains the diffusion policy on pure trajectory data to learn trajectory-following. Phase 2 fine-tunes with point cloud conditioning to enable environment-aware refinement. Core assumption: Trajectory-following and point-cloud-based refinement are learnable independently; coupling them slows convergence.

### Mechanism 3
Noise injection during action expert training improves robustness to imperfect VLM-generated waypoints. Gaussian noise is added to goal poses during training (optimal scale 0.1), simulating VLM trajectory variability and preventing overfitting to idealized guidance. Core assumption: VLM-generated waypoints will deviate from ground truth at test time in ways that match the training noise distribution.

## Foundational Learning

- **Conditional Diffusion Policies**: Used to parameterize the action expert as a noise prediction network that iteratively denoises random actions into executable actions given conditioning features. Quick check: Can you explain how a diffusion model iteratively denoises a random action vector into an executable action given conditioning features?

- **PointNet-based Point Cloud Encoding**: Processes cropped point cloud observations through a PointNet-based encoder to produce conditioning features for action generation. Quick check: How does PointNet achieve permutation invariance when processing unordered 3D point sets?

- **B-spline Trajectory Interpolation**: Converts sparse waypoints from the VLM to continuous, smooth end-effector pose trajectories. Quick check: What properties make B-splines suitable for generating smooth trajectories from sparse waypoints?

## Architecture Onboarding

- **Component map**: VLM (SFT fine-tuned) -> 2D anchor points -> 3D waypoints + target gripper pose -> Depth estimation -> Coordinate transform -> B-spline interpolation -> Continuous pose trajectory -> Action expert (PointNet encoder + MLPs) -> Refined action sequence

- **Critical path**: 1. VLM receives RGB image + language instruction -> outputs 2D anchor points; 2. Depth estimation -> 3D coordinates in camera frame; 3. VLM infers 3D waypoints + target pose -> transform to robot frame; 4. B-spline interpolation -> dense guidance trajectory; 5. Action expert samples point cloud + proprioceptive state + guidance pose -> outputs refined action sequence

- **Design tradeoffs**: Camera-frame vs. robot-frame prediction (preserves VLM priors but requires accurate depth); Pre-training batch size (32k) vs. fine-tuning batch size (256) (large batches for motor skills, smaller for point cloud integration); Noise scale (0.10 optimal) balances robustness vs. signal corruption

- **Failure signatures**: VLM over-fine-tuning (language ability degrades, MMLU drops from 70 to 30); Missing/noisy depth (point cloud quality affects action expert refinement); Excessive noise during training (performance collapse at noise scale 0.50)

- **First 3 experiments**: 1. Ablate pre-training vs. end-to-end: Compare APPF paradigm against joint trajectory+pointcloud training on convergence speed and final success rate; 2. Noise scale sweep: Test noise scales {0.0, 0.05, 0.10, 0.20, 0.50} on short/middle/long-horizon tasks to find optimal robustness point; 3. VLM fine-tuning steps vs. performance: Track success rate and MMLU score across SFT steps {0, 500, 1000, ..., 4000} with and without action expert

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the generalizable action expert when deployed with low-quality or sparse native depth sensors compared to the high-fidelity, re-annotated depth maps used in training? The paper validates performance using enhanced depth data but does not quantify the performance degradation when operating on raw sensor data common in consumer robotics.

### Open Question 2
Can the action expert effectively handle dynamic obstacles or moving targets despite being evaluated primarily on static manipulation tasks? While the architecture supports real-time sampling, the paper does not demonstrate the system's ability to react to environmental changes during trajectory execution.

### Open Question 3
To what extent can the action expert correct semantically incorrect or physically impossible 3D waypoints generated by the VLM planner? The paper demonstrates that the expert refines trajectories geometrically, but it is unclear if it can reject semantically wrong waypoints or will faithfully execute flawed plans.

## Limitations
- Performance heavily depends on depth estimation quality, which varies significantly across monocular and stereo conditions
- Real-world generalization to novel environments with different lighting, occlusion patterns, or object appearances remains uncertain
- The APPF training paradigm's effectiveness is primarily empirical rather than theoretically grounded

## Confidence

- **High confidence**: The effectiveness of sparse 3D waypoints as an intermediate representation for decoupling VLM and action expert
- **Medium confidence**: The APPF training paradigm's superiority over end-to-end training for convergence and data efficiency
- **Low confidence**: Generalization claims to completely unseen domains and the robustness of depth estimation across diverse real-world conditions

## Next Checks

1. **Cross-environment transfer test**: Evaluate the same trained model on environments with significantly different visual characteristics than the training set to assess true generalization limits.

2. **Failure mode analysis**: Systematically collect and analyze failure cases across all task horizons to identify whether failures stem from VLM planning errors, depth estimation inaccuracies, or action expert refinement limitations.

3. **Real-world deployment stress test**: Deploy the system in unstructured environments with moving obstacles, variable lighting, and imperfect calibration to measure performance degradation compared to controlled lab settings.