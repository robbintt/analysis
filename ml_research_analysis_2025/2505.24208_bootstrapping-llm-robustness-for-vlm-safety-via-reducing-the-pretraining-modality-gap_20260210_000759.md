---
ver: rpa2
title: Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality
  Gap
arxiv_id: '2505.24208'
source_url: https://arxiv.org/abs/2505.24208
tags:
- safety
- arxiv
- modality
- image
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates safety degradation in large vision-language
  models (LVLMs), where even benign images can trigger harmful responses to prompts
  that text-only models would refuse. It identifies the modality gap between image
  and text representations as a key factor, showing that larger gaps correlate with
  higher unsafe response rates.
---

# Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap

## Quick Facts
- **arXiv ID**: 2505.24208
- **Source URL**: https://arxiv.org/abs/2505.24208
- **Reference count**: 25
- **Primary result**: Simple pretraining regularization method REGAP reduces unsafe response rates in LVLMs by up to 16.3% by aligning image and text embeddings without requiring additional safety data

## Executive Summary
Large vision-language models (LVLMs) face a critical safety challenge: they can generate harmful responses to otherwise benign images that text-only models would refuse. This safety degradation stems from a "modality gap" - the divergence between how LVLMs represent images versus text in their embeddings. The paper introduces REGAP, a lightweight pretraining regularization technique that reduces this gap by applying L2 loss between image and text representations. This approach improves safety without requiring additional safety data or degrading model utility, and can even enhance existing safety fine-tuning methods by up to 18.2%.

## Method Summary
The paper identifies the modality gap between image and text embeddings as a key driver of safety degradation in LVLMs. The proposed REGAP method addresses this by adding a regularization term to the pretraining objective that minimizes the L2 distance between image and text embeddings during contrastive learning. This simple modification encourages the model to learn more aligned representations across modalities without requiring additional safety data or introducing inference overhead. The approach is tested on three different LVLM architectures and shows consistent safety improvements across multiple benchmark datasets.

## Key Results
- REGAP reduces unsafe response rates by up to 16.3% across LLaVA-v1.5, ShareGPT4V, and MiniGPT-4 models
- Safety improvements achieved without any degradation in model utility on standard benchmarks
- REGAP can boost the effectiveness of existing safety fine-tuning methods by up to 18.2%
- The approach generalizes across multiple LVLM architectures and dataset types

## Why This Works (Mechanism)
The modality gap creates a representation divergence where harmful concepts can manifest differently in image versus text embeddings. When this gap is large, the model may fail to recognize safety-relevant patterns consistently across modalities, allowing harmful responses to benign visual inputs. By reducing this gap through L2 regularization during pretraining, REGAP ensures more consistent representation of safety-critical concepts across both image and text inputs, leading to more robust refusal behavior.

## Foundational Learning

**Modality Gap**: The divergence in embedding spaces between different input types (images vs text) in multimodal models. Why needed: Understanding this gap is crucial as it explains why LVLMs can fail safety checks that text-only models pass. Quick check: Verify that embedding similarity between corresponding image-text pairs is lower than within-modal pairs.

**Contrastive Learning**: A training approach that learns representations by pulling together positive pairs (matching image-text) while pushing apart negative pairs. Why needed: This forms the foundation for how LVLMs learn cross-modal alignment during pretraining. Quick check: Confirm that the model correctly matches images to their captions with high accuracy.

**L2 Regularization**: A technique that penalizes large distances between embeddings by adding a squared distance term to the loss function. Why needed: This is the core mechanism REGAP uses to reduce the modality gap. Quick check: Monitor that embedding norms remain stable and distances decrease during training.

## Architecture Onboarding

**Component Map**: Vision encoder -> Projection layers -> LLM backbone -> Text decoder. The modality gap manifests between projection layers and LLM embedding space.

**Critical Path**: Image/text input → Respective encoders → Cross-attention layers → Safety-relevant concepts → Output generation. REGAP modifies the projection layer training to align representations earlier in this path.

**Design Tradeoffs**: Simplicity vs potential for more sophisticated alignment methods (e.g., adversarial training, more complex distance metrics). The paper chooses L2 regularization for its computational efficiency and ease of implementation.

**Failure Signatures**: Large embedding distance between image and text representations of the same concept, inconsistent safety behavior across modalities, degradation in task performance when modality gap is reduced too aggressively.

**First Experiments**:
1. Measure baseline modality gap size across three LVLM architectures using standard metrics
2. Apply REGAP with varying regularization strengths to find optimal balance
3. Evaluate safety performance on established benchmark datasets while monitoring utility metrics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from the work: whether alternative alignment methods might be more effective than L2 regularization, how modality gap reduction affects other safety dimensions like adversarial robustness, and whether the approach generalizes to more complex safety scenarios beyond simple refusal.

## Limitations

- The study only tests L2 regularization for gap reduction, leaving open whether more sophisticated alignment methods might yield better results
- Evaluation focuses primarily on image-based unsafe triggers, not addressing whether modality gap reduction affects other safety dimensions
- No human evaluation is conducted to verify that reduced unsafe response rates correspond to actual harm reduction in real-world scenarios

## Confidence

- **High**: The core claim that REGAP reduces unsafe response rates without degrading utility is well-supported by quantitative results across three datasets and three model architectures
- **Medium**: The assertion that REGAP can boost existing defenses by up to 18.2% is supported, but limited to testing one specific safety training method
- **Low**: The claim that no additional safety data is required may be misleading since safety test sets used for evaluation implicitly define what constitutes "unsafe" behavior

## Next Checks

1. Test REGAP's effectiveness when combined with other safety fine-tuning methods beyond Safety Alignment, such as DPO or RLHF-based approaches
2. Evaluate whether reducing the modality gap introduces new vulnerabilities, such as increased susceptibility to adversarial image perturbations or cross-modal jailbreak attacks
3. Conduct human evaluation studies to verify that reduced unsafe response rates correspond to actual harm reduction in real-world usage scenarios, including multi-turn conversations and ambiguous prompts