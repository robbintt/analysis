---
ver: rpa2
title: 'MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants'
arxiv_id: '2602.00353'
source_url: https://arxiv.org/abs/2602.00353
tags:
- risk
- health
- mental
- evaluation
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MHDash is an open-source platform for evaluating AI assistants
  in mental health contexts. It integrates data collection, expert annotation, multi-turn
  dialogue generation, and risk-aware evaluation into a unified pipeline.
---

# MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants

## Quick Facts
- arXiv ID: 2602.00353
- Source URL: https://arxiv.org/abs/2602.00353
- Reference count: 29
- Primary result: MHDash is an open-source platform for evaluating AI assistants in mental health contexts using structured multi-turn dialogues and risk-stratified evaluation.

## Executive Summary
MHDash addresses a critical gap in AI safety evaluation by providing a comprehensive platform for benchmarking mental health-aware AI assistants. The platform integrates data collection, expert annotation, synthetic dialogue generation, and risk-aware evaluation into a unified pipeline. Through evaluation of both simple baselines and advanced LLM APIs, the platform reveals that conventional accuracy metrics fail to capture safety-critical failure modes, particularly the high false negative rates on severe mental health risk categories.

## Method Summary
The platform uses the MHDialog dataset containing 1,000 multi-turn dialogues with 10 rounds each, annotated across Concern Type, Risk Level, and Dialogue Intent. Data collection combines social media sources with quality filtering, LLM coarse categorization, and expert annotation using C-SSRS protocol. Synthetic dialogues are generated using GPT-4o conditioned on original posts and dialogue intents. Models are evaluated using both standard metrics (accuracy, macro-F1) and risk-specific metrics (High-Risk Recall, FNR, Kendall's Tau) to identify safety-critical failure modes.

## Key Results
- Simple baselines and advanced LLM APIs show comparable overall accuracy but diverge significantly on high-risk cases
- Some models maintain ordinal severity ranking while failing absolute risk classification
- Performance gaps are amplified in multi-turn dialogues where risk signals emerge gradually
- High false negative rates on severe categories persist across all models despite reasonable aggregate scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregate accuracy metrics obscure safety-critical failure modes in mental health AI evaluation.
- Mechanism: Standard metrics weight all errors equally, allowing models to achieve high scores by performing well on majority classes while systematically failing on rare but critical high-risk categories.
- Core assumption: Risk categories have unequal real-world consequences; false negatives on severe cases are qualitatively different from errors on low-risk cases.
- Evidence anchors: [abstract] "simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases"; [section V.C] RoBERTa-HA achieved highest Concern Type accuracy (0.7667) but FNR=1.0 on Attempt and Severe cases.

### Mechanism 2
- Claim: Multi-turn dialogue contexts reveal risk detection failures invisible in single-turn evaluation.
- Mechanism: Risk signals emerge gradually through conversation turns rather than appearing explicitly in single utterances; models must aggregate weak signals across temporal context.
- Core assumption: Mental health risk manifests incrementally in realistic conversations, not as isolated explicit statements.
- Evidence anchors: [abstract] "performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually"; [section III.E] "This process preserves the original risk signals while enabling analysis of how risk evolves across conversational turns."

### Mechanism 3
- Claim: Ordinal severity correlation can coexist with catastrophic absolute classification failures.
- Mechanism: Models learn relative ordering (Severe > Moderate > Minor) through correlation with surface features but lack calibrated thresholds for absolute decisions; Kendall's Tau measures ranking consistency, not calibration.
- Core assumption: Ordinal ranking ability does not imply reliable categorical thresholds.
- Evidence anchors: [abstract] "some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification"; [section V.C] RoBERTa-HA achieved highest Kendall's Tau (0.656) but 0% severe-case recall.

## Foundational Learning

- Concept: False Negative Rate (FNR) in Safety-Critical Systems
  - Why needed here: The paper positions FNR on severe categories as the primary safety metric; understanding this is essential before interpreting any evaluation results.
  - Quick check question: A model achieves 95% accuracy but misses 50% of suicide attempt cases. Is this acceptable for deployment?

- Concept: Ordinal Classification and Kendall's Tau
  - Why needed here: The platform introduces ordinal correlation as a distinct evaluation dimension; without this, you might misinterpret a model with strong Tau but zero severe-case detection as "performing well."
  - Quick check question: If Model A has τ=0.7 with 0% severe recall, and Model B has τ=0.5 with 100% severe recall, which is safer for clinical triage?

- Concept: Multi-Turn Context Aggregation
  - Why needed here: The dataset consists of 10-round dialogues; evaluation requires understanding how evidence accumulates across turns rather than treating each utterance independently.
  - Quick check question: A user says "I'm fine" in turn 3 after mentioning "everything is falling apart" in turn 1. Should a risk classifier weigh turn 3 more heavily?

## Architecture Onboarding

- Component map: Raw data → Quality filtering → Expert annotation → Dialogue Intent assignment → Synthetic dialogue generation → Semantic filtering → Model inference → Risk-stratified evaluation

- Critical path: Raw data → Quality filtering → Expert annotation → Dialogue Intent assignment → Synthetic dialogue generation → Semantic filtering → Model inference → Risk-stratified evaluation

- Design tradeoffs:
  - Synthetic dialogues avoid privacy/ethics constraints but may not capture real user behavior patterns
  - Fixed 10-round length enables controlled comparison but may not reflect natural conversation termination
  - Few-shot prompting for LLMs provides fair comparison but may underestimate fine-tuned LLM potential

- Failure signatures:
  - High aggregate accuracy + FNR > 0.5 on Attempt/Severe categories
  - Strong Kendall's Tau + 0% severe-case recall
  - Performance collapse on Recovery (SU5) and Explicit Help-Seeking (SU3) dialogue intents

- First 3 experiments:
  1. Reproduce the baseline comparison on the provided test split (150 dialogues) to validate pipeline integration; verify your FNR heatmap matches Figure 1.
  2. Ablate dialogue context by evaluating models on turn-1 only vs. full 10-turn context; quantify the multi-turn performance gap.
  3. Calibrate decision thresholds on the validation set to minimize FNR on Severe cases while monitoring precision tradeoffs; compare against the paper's few-shot prompting approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified model architecture be developed that preserves high ordinal severity ranking (Kendall's Tau) while simultaneously minimizing false negative rates on severe risk cases?
- Basis in paper: [explicit] Section V.C notes that RoBERTa-HA achieves the highest ordinal correlation (0.656) but fails completely on severe cases (FNR=1.0), whereas LLMs detect severe cases well but lack consistent ranking.
- Why unresolved: There is an observed trade-off between fine-grained severity ordering and absolute detection of critical classes, and current benchmarks do not identify a model that optimizes both constraints effectively.

### Open Question 2
- Question: To what extent does the use of GPT-4o-generated synthetic dialogues introduce distributional bias that limits the generalizability of risk classifiers to authentic, noisy human interactions?
- Basis in paper: [inferred] Section IV.B acknowledges that dialogues are generated by GPT-4o to simulate "naturalistic" conversations because real data is scarce.
- Why unresolved: The paper validates the platform internally but does not compare performance on synthetic vs. real-world human-AI crisis transcripts, leaving a potential domain gap unexplored.

### Open Question 3
- Question: What specific linguistic or structural factors cause model performance to degrade significantly in "Recovery" (SU5) and "Explicit Help-Seeking" (SU3) dialogue intents compared to other categories?
- Basis in paper: [explicit] Section V.D states that Recovery and Explicit Help-Seeking dialogues "consistently exhibit the lowest and most unstable performance" across all models for both concern type and risk level classification.
- Why unresolved: The paper identifies these categories as "blind spots" where intent-aware evaluation fails, but the underlying mechanism (e.g., lack of explicit distress signals, ambiguity in recovery language) is not analyzed.

## Limitations

- Synthetic dialogue generation may not capture authentic user behavior patterns and risk escalation dynamics
- Few-shot prompting approach may underestimate LLM performance compared to fine-tuning
- Focus on detection accuracy without addressing model calibration, confidence scoring, or clinical validation

## Confidence

- High confidence: The core finding that aggregate metrics obscure safety-critical failures (Mechanism 1) is robustly supported by the data showing consistent FNR > 0.5 on severe categories across models.
- Medium confidence: The multi-turn context advantage (Mechanism 2) is demonstrated but limited to synthetic data; real-world validation is needed.
- Medium confidence: The ordinal vs. absolute classification tradeoff (Mechanism 3) is theoretically sound but based on a single model's performance pattern.

## Next Checks

1. **Real-world dialogue validation**: Test the platform on naturally occurring mental health conversations from anonymized clinical sources to verify synthetic dialogue generation captures authentic risk progression patterns.

2. **Calibration and threshold optimization**: Implement ROC curve analysis for severe-risk detection and optimize decision thresholds rather than using fixed few-shot prompting to establish upper bounds on model performance.

3. **Clinical expert review**: Conduct blind evaluation where mental health professionals assess model outputs against ground truth annotations to validate the annotation protocol and identify any systematic biases in risk detection.