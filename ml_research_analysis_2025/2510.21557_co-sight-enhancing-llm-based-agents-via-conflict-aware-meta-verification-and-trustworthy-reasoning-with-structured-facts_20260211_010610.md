---
ver: rpa2
title: 'Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification
  and Trustworthy Reasoning with Structured Facts'
arxiv_id: '2510.21557'
source_url: https://arxiv.org/abs/2510.21557
tags:
- reasoning
- arxiv
- verification
- facts
- camv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Co-Sight addresses the challenge of long-horizon reasoning failures
  in LLM-based agents by shifting from generation to auditing. It introduces Conflict-Aware
  Meta-Verification (CAMV) to target verification at disagreement hotspots among expert
  agents, and Trustworthy Reasoning with Structured Facts (TRSF) to ground reasoning
  in source-verified, traceable knowledge.
---

# Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts

## Quick Facts
- arXiv ID: 2510.21557
- Source URL: https://arxiv.org/abs/2510.21557
- Reference count: 7
- Achieves 84.4% accuracy on GAIA benchmark

## Executive Summary
Co-Sight addresses long-horizon reasoning failures in LLM-based agents by shifting from generation to auditing. It introduces Conflict-Aware Meta-Verification (CAMV) to target verification at disagreement hotspots among expert agents, and Trustworthy Reasoning with Structured Facts (TRSF) to ground reasoning in source-verified, traceable knowledge. The closed-loop architecture between CAMV and TRSF enables transparent, efficient, and scalable reasoning. Empirically, Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%), Humanity's Last Exam (35.5%), and Chinese-SimpleQA (93.8%), demonstrating that focused verification and structured evidence organization outperform purely generative reasoning.

## Method Summary
Co-Sight employs N expert agents (Planner→Actor→Toolkit) plus one meta-verification agent in a closed-loop system. CAMV identifies disagreement hotspots via Conservative–Radical Ensemble, allocating verification budget only to conflicting nodes rather than full trajectories. TRSF compresses raw tool outputs through three tiers (Tool Records→Notes→Facts), separating provenance-verified facts from assumptions. The system uses integrative synthesis to recombine partial truths from flawed reasoning traces under domain constraints. Temperature diversification (conservative vs. radical agents) exposes diagnostic disagreements, while DAG-based task decomposition with ReAct execution structures multi-step reasoning.

## Key Results
- Achieves 84.4% accuracy on GAIA benchmark (state-of-the-art)
- Outperforms pass@N for small ensemble sizes (N≤2) with better efficiency
- TRSF's three-tier compression improves accuracy by ~2.4% over raw tool outputs
- Demonstrates effective recovery and recombination of partial micro-inferences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shifting verification from full-chain auditing to targeted "disagreement hotspots" appears to bound computational cost while maintaining reliability.
- **Mechanism:** Conservative–Radical Ensemble with low-temperature agents providing stable baselines and high-temperature agents exploring divergent paths. CAMV identifies nodes where agents disagree (Sc) and allocates verification budget only to these points rather than the full trajectory (|S|).
- **Core assumption:** Disagreement among semantically diverse agents correlates with higher probability of error.
- **Evidence anchors:** Abstract states CAMV allocates computation only to disagreement hotspots; section 3.1 describes budget allocation to Sc rather than full chain.
- **Break condition:** If expert agents produce incomplete or homogeneously incorrect plans, conflict set Sc may fail to capture salient errors.

### Mechanism 2
- **Claim:** Structured, hierarchical context compression (TRSF) likely reduces hallucination propagation better than raw context accumulation.
- **Mechanism:** TRSF processes raw tool outputs through three-tier pipeline: Tool Records→Notes→Facts, separating provenance-verified facts from assumptions and low-confidence speculations.
- **Core assumption:** Valid, traceable knowledge can be distilled into structured format without losing critical nuance.
- **Evidence anchors:** Abstract describes TRSF organizing, validating, and synchronizing evidence; section 3.2 explains hierarchical compression.
- **Break condition:** If compression logic is too aggressive, it may discard subtle constraints or edge-case evidence required for correct auditing.

### Mechanism 3
- **Claim:** A closed-loop synthesis of partial truths outperforms selecting a single "best" answer.
- **Mechanism:** CAMV uses Integrative Synthesis to extract valid micro-inferences from otherwise flawed candidate traces, recombining them under domain constraints (K) to construct final answer.
- **Core assumption:** Flawed reasoning trajectories often contain salvageable correct sub-steps; verification is lower-variance than generation.
- **Evidence anchors:** Section 3.1 describes reconstructing coherent reasoning trace by extracting valid micro-inferences; section 4.4 discusses recovering partial micro-inferences.
- **Break condition:** If Consensus Anchoring promotes factually incorrect but universally agreed-upon intermediate (Aθ), synthesis will be grounded in false premises.

## Foundational Learning

- **Concept: Self-Consistency & Consensus Anchoring**
  - **Why needed here:** CAMV relies on agreement among agents to form "anchors" (Aθ) that require no further verification.
  - **Quick check question:** If all agents agree on a specific date for an event, does CAMV verify it? (Answer: No, unless it violates constraint K.)

- **Concept: Constraint-Based Pruning (Elimination-by-Aspects)**
  - **Why needed here:** This is the first filter in CAMV, distinguishing semantic verification from constraint verification.
  - **Quick check question:** Does the system backtrack the entire plan if one step violates a unit constraint?

- **Concept: Provenance Tracking**
  - **Why needed here:** TRSF depends on distinguishing "Given Facts" from "Derived Facts" and "Assumptions."
  - **Quick check question:** Where does a "Derived Fact" (90% confidence) sit in the hierarchy compared to a "Retrieved Fact" (URL source)?

## Architecture Onboarding

- **Component map:** Expert Agents (N): [Planner → Actor → Toolkit] + [Facts Module] → Meta-Verification Agent: [Constraint Pruner → Consensus Anchor → Conflict Auditor → Synthesizer]

- **Critical path:** Query → Expert Ensemble Generation → TRSF Context Compression → CAMV Constraint Pruning → CAMV Conflict Detection (Sc) → CAMV Auditing (using TRSF sources) → Synthesis

- **Design tradeoffs:**
  - Ensemble Size (N): CAMV outperforms pass@N for small N (≤2) but pass@N catches up at scale
  - Temperature: Mix of Conservative (low temp) and Radical (high temp) required; purely conservative agents may miss conflict points

- **Failure signatures:**
  - Silent Consensus Error: All experts agree on wrong fact (false positive anchor)
  - Incomplete Plans: Experts fail to generate steps covering full query scope
  - Context Bloat: TRSF fails to compress tool outputs, overwhelming verifier's context window

- **First 3 experiments:**
  1. Baseline Ensemble: Run N=2 experts with and without CAMV to isolate verification vs. generation gains
  2. TRSF Ablation: Disable three-tier compression and feed raw tool outputs to verifier
  3. Constraint Stress Test: Inject question with subtle unit contradiction to verify Constraint-Based Pruning

## Open Questions the Paper Calls Out

- **How should verification budgets scale with ensemble size and task complexity to maintain CAMV's advantage at larger N?**
  - Basis: Table 1 shows CAMV underperforms pass@N at N≥3 with fixed budget Bmax
  - Why unresolved: Paper demonstrates limitation empirically but doesn't propose adaptive budgeting mechanism
  - What evidence would resolve it: Experiments with budget scaling functions showing sustained accuracy at larger N

- **How robust is Co-Sight in real-world, safety-critical domains where errors have significant consequences?**
  - Basis: Limitations section states robustness in safety-critical settings has yet to be established
  - Why unresolved: All evaluations use academic benchmarks rather than medical, legal, or autonomous systems
  - What evidence would resolve it: Evaluation on safety-critical benchmarks with formal error analysis

- **How can stronger multimodal verifiers be integrated to overcome current vision and parsing module limitations?**
  - Basis: Limitations note multimodal pipeline remains bounded by vision and parsing accuracy
  - Why unresolved: TRSF's compression doesn't specifically address multimodal evidence verification
  - What evidence would resolve it: Ablation studies with dedicated multimodal verification modules

- **Is temperature-based diversity optimal for exposing meaningful conflicts, compared to architectural or prompting-based diversity?**
  - Basis: Conservative-radical ensemble relies solely on temperature variation
  - Why unresolved: No ablation compares temperature diversity against other diversity mechanisms
  - What evidence would resolve it: Controlled experiments comparing conflict quality across diversity strategies

## Limitations
- Silent Consensus Errors may occur when all experts agree on incorrect information
- Performance degrades at larger ensemble sizes (N≥3) due to fixed verification budgets
- No evaluation on safety-critical real-world domains with high-stakes consequences
- Multimodal pipeline remains bounded by current vision and parsing module accuracy

## Confidence
- **High confidence:** Core architectural insight that targeted auditing improves efficiency while maintaining accuracy is well-supported by empirical results across three benchmarks
- **Medium confidence:** Disagreement hotspots among semantically diverse agents correlating with error probability relies on Self-Consistency assumption
- **Low confidence:** Generalizability of 84.4% GAIA accuracy to real-world deployment scenarios without knowing exact parameter configurations

## Next Checks
1. **Parameter sensitivity analysis:** Systematically vary conservative-radical temperature split and consensus threshold θ to identify optimal configurations and understand robustness to hyperparameter choices

2. **Long-horizon failure mode identification:** Design adversarial test cases with subtle unit contradictions, temporal inconsistencies, and incomplete expert plans to measure Co-Sight's ability to detect Silent Consensus Errors and Incomplete Plans failures

3. **Scaling efficiency benchmark:** Compare Co-Sight's computational cost and accuracy against pass@N at varying ensemble sizes (N=2,4,8,16) on GAIA subset to characterize where CAMV's efficiency advantage breaks down