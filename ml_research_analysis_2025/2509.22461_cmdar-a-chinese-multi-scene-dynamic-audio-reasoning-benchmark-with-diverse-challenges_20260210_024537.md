---
ver: rpa2
title: 'CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse
  Challenges'
arxiv_id: '2509.22461'
source_url: https://arxiv.org/abs/2509.22461
tags:
- audio
- reasoning
- question
- answer
- lalms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CMDAR is a Chinese benchmark for evaluating audio-language models
  on complex, multi-scene, and dynamic reasoning tasks. It contains 3,000 high-quality
  question-answer pairs across five reasoning categories and three question types,
  including open-ended and multiple-audio scenarios.
---

# CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse Challenges

## Quick Facts
- arXiv ID: 2509.22461
- Source URL: https://arxiv.org/abs/2509.22461
- Reference count: 40
- Key outcome: CMDAR benchmark evaluates audio-language models on complex Chinese audio reasoning tasks; best open-source model achieves 76.67% accuracy, revealing significant room for improvement in multi-scene, dynamic reasoning

## Executive Summary
CMDAR is a benchmark designed to evaluate audio-language models (LALMs) on complex, multi-scene, and dynamic reasoning tasks using Chinese audio clips from films. It contains 3,000 high-quality question-answer pairs across five reasoning categories and three question types, including open-ended and multiple-audio scenarios. The benchmark was constructed using Chinese films to generate diverse, real-world audio clips paired with expert-curated questions. When evaluated on CMDAR-main, Qwen2.5-Omni (open-source) achieved 76.67% accuracy, while GPT-4o Audio (closed-source) reached 68.47%. GPT-4o Audio substantially outperformed Qwen2.5-Omni on more challenging multiple-audio and open-ended tasks. No model surpassed 80% performance, indicating significant room for improvement in complex audio reasoning. The benchmark revealed limitations in open-source models, differences in perceptual versus reasoning capabilities, and challenges with instruction bias and noise.

## Method Summary
CMDAR was constructed by collecting audio clips from Chinese films and creating expert-annotated question-answer pairs across five reasoning categories: scene understanding, social reasoning, event reasoning, temporal reasoning, and anomaly detection. The benchmark includes three variants: CMDAR-main (1,500 single-choice questions), CMDAR-open (1,500 open-ended questions), and CMDAR-multi (1,500 multi-audio, multiple-choice questions). Audio clips average 25.11 seconds for main/open and 69.90 seconds for multi-audio tasks. Evaluation uses different metrics per variant: regex-based matching for single-choice, LLM-based scoring for open-ended, and set-based metrics for multi-select tasks. The benchmark was evaluated on seven LALMs including Qwen2.5-Omni, GPT-4o Audio, and others.

## Key Results
- Qwen2.5-Omni (open-source) achieved 76.67% accuracy on CMDAR-main, while GPT-4o Audio (closed-source) reached 68.47%
- GPT-4o Audio outperformed Qwen2.5-Omni on more challenging multiple-audio and open-ended tasks
- No model surpassed 80% performance, indicating significant room for improvement in complex audio reasoning
- Reasoning errors constituted the majority of failures (86.68% for GPT-4o Audio), exceeding perception and knowledge errors

## Why This Works (Mechanism)

### Mechanism 1: Audio Source Complexity and Heterogeneity
CMDAR reveals that model performance degrades significantly when processing audio from complex, real-world scenarios involving overlapping heterogeneous sources (speech, music, environmental sounds). The benchmark utilizes clips from Chinese films which contain multi-speaker dialogues, background music, and environmental sounds simultaneously. This creates a challenging "cocktail party problem" where the model must separate and attend to relevant information while filtering noise. The high entropy and long temporal sequences in these clips force the model to maintain context over longer durations (average 25.11 seconds vs. MMAU's ~10s). Real-world film audio is representative of the acoustic complexity an AI agent would face in deployment.

### Mechanism 2: Gap Between Perception and Reasoning
There is a measurable gap between a model's ability to perceive audio features and its ability to perform complex logical reasoning based on those features, with reasoning errors being the dominant failure mode. The error analysis categorizes failures into Perception, Reasoning, Knowledge, and Formatting errors. The finding that reasoning errors constitute the majority (e.g., 86.68% for GPT-4o Audio) suggests that models can often transcribe or identify elements but fail to infer causality, temporal precedence, or social intent. The Gaussian noise experiment further isolates perception: when audio is noise, performance drops to near-random, confirming models use audio input, but the remaining performance on harder tasks is limited by reasoning chains.

### Mechanism 3: Language and Instruction Bias in Evaluation
Evaluation results are significantly influenced by the model's proficiency in the target language (Chinese) and its sensitivity to instruction phrasing, creating potential biases that can be misinterpreted as reasoning failures. The paper highlights that some models lack Chinese training data, affecting fairness. Furthermore, models exhibit "different preferences in answering questions" (e.g., restating option content vs. just giving the letter). The instruction bias experiment shows accuracy varies by ~20% or more with different prompts for the same task. This suggests that measured performance is a composite of audio reasoning, language understanding, and instruction-following abilities.

## Foundational Learning

- **Concept: Audio-Language Model (LALM) Architecture (e.g., Audio Encoder + Adapter + LLM)**
  - Why needed here: Understanding CMDAR's results requires knowing how audio signals are converted into tokens or embeddings that an LLM can process. The performance gap between "cascaded" (separate ASR + LLM) and "non-cascaded" (integrated) models provides insight into where information is lost or reasoning fails.
  - Quick check question: Can you explain the difference in data flow between a cascaded model (ASR output fed to an LLM) and an integrated audio-language model processing a raw waveform?

- **Concept: Chain-of-Thought (CoT) and Multi-Step Reasoning**
  - Why needed here: The benchmark's reasoning categories (temporal, causal, social) require synthesizing information across multiple steps. The error analysis points to "breakage of the multi-step reasoning chain" as a key failure mode. Understanding CoT prompting or reasoning-focused training (like RL) is crucial to interpreting why models fail here.
  - Quick check question: Why would a model that correctly identifies all entities in an audio clip still fail to answer a question about the temporal relationship between two events?

- **Concept: LLM-as-a-Judge for Open-Ended Evaluation**
  - Why needed here: For CMDAR-open, answers are free-form text. The paper uses a state-of-the-art LLM to score these answers on a 0-10 scale. Understanding the limitations and biases of this evaluation method is critical for interpreting the reported scores (e.g., the need to swap positions and run multiple evaluations to reduce randomness).
  - Quick check question: What are two potential biases when using an LLM to score another model's open-ended answers, and how does the paper attempt to mitigate them?

## Architecture Onboarding

- **Component Map:** Audio Clip Input -> Audio Encoder -> Audio Adapter -> LLM Backbone -> Response Processing -> Metric Calculation
- **Critical Path:** The path from Audio Clip Input -> Encoder/Adapter -> LLM Inference -> Response Processing -> Metric Calculation. Any failure or bottleneck in the encoder (missed speech), adapter (poor alignment), or LLM (reasoning failure) will propagate and result in a low score.
- **Design Tradeoffs:**
  - Cascaded vs. Integrated: Cascaded systems (ASR -> LLM) are more modular and easier to debug but lose prosodic and acoustic information not captured by text. Integrated models (e.g., Qwen2.5-Omni) are end-to-end but harder to train and diagnose.
  - Model Size vs. Data Quality: The paper suggests moving from scaling parameters to focusing on data quality (fine-grained processing) and diversity. Larger models (7B vs 3B) show improvement but still have high perceptual errors.
  - English vs. Chinese Corpus Training: Training on Chinese corpora is essential for this benchmark. Models lacking this perform poorly, highlighting a tradeoff between general language capability and specific linguistic expertise.
- **Failure Signatures:**
  - Perception Error: Model misidentifies a sound or mishears dialogue. Signature: Correct reasoning logic applied to wrong premises.
  - Reasoning Error: Model perceives elements correctly but makes faulty inferences about causality or temporality. Signature: Hallucinated connections or incorrect conclusions from accurate premises.
  - Instruction Bias: Model's performance swings wildly with different prompt phrasings. Signature: High variance in accuracy across prompt variants.
  - Formatting Error: Model provides the correct reasoning or content but in a format the evaluation script cannot parse (e.g., "The answer is B because..." instead of "B").
- **First 3 Experiments:**
  1. Baseline Evaluation on CMDAR-main: Run your audio-language model on the 1,500 single-choice questions. Calculate accuracy overall and per category (Scene, Social, Event, Temporal, Anomaly) to identify the weakest reasoning skills. This establishes a baseline using the provided evaluation script.
  2. Instruction Sensitivity Analysis: Select a subset of 100 questions from CMDAR-main. Test your model with the 4 different instruction prompt variants provided in the paper's appendix. Calculate the standard deviation of accuracy to quantify your model's instruction bias.
  3. Noise Robustness Test: Replicate the "Gaussian Noise Replacement" experiment. Replace the audio in a test set with Gaussian white noise of the same length and measure the accuracy drop. This confirms whether your model genuinely utilizes the audio signal versus relying on textual priors in the questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating text-based logical training data, specifically code or mathematical corpora, effectively transfer to and enhance non-textual audio reasoning capabilities?
- Basis in paper: [explicit] The authors state in the Discussion: "We believe that code or mathematical data can help improve audio reasoning ability."
- Why unresolved: This is a hypothesis regarding cross-modal transfer learning that was suggested but not empirically validated in the current experiments.
- What evidence would resolve it: An ablation study training LALMs on code/math data and measuring the resulting performance delta on the CMDAR benchmark.

### Open Question 2
- Question: Will Reinforcement Learning (RL) optimization strategies become significantly more effective as the foundational perceptual capabilities of audio-language models improve?
- Basis in paper: [explicit] The paper notes that while RL provides positive improvements, the "improvement of RL is limited by the current LALMs foundations."
- Why unresolved: It is unclear if the current plateau in RL gains is intrinsic to the method or a symptom of the underlying models' weak audio perception.
- What evidence would resolve it: Re-evaluating the performance gap between RL-trained and supervised models as the base model's perception accuracy (e.g., ASR, sound detection) increases.

### Open Question 3
- Question: Does the reliance on professionally produced Chinese films as the audio source introduce a "production bias" that limits model generalization to unscripted, noisy, real-world environments?
- Basis in paper: [inferred] The data pipeline explicitly uses "Chinese films" with "high-production-value," which implicitly assumes these simulate "real-world scenarios" despite potential differences in acoustic fidelity and scripting compared to spontaneous daily life.
- Why unresolved: Models might overfit to cinematic audio tropes (clear dialogue, dramatic sound effects) rather than learning robust reasoning for chaotic, real-life acoustics.
- What evidence would resolve it: Testing models trained on CMDAR against a dataset of unscripted, amateur, or surveillance-style audio to see if performance holds.

## Limitations

- Language-Specific Bias: The benchmark's exclusive focus on Chinese introduces a significant limitation where models lacking Chinese language training data face inherent disadvantages.
- Evaluation Methodology Concerns: The use of LLM-as-a-judge for CMDAR-open introduces potential evaluator bias, with accuracy variance across different instruction prompts suggesting evaluation instability.
- Real-World Representativeness: The use of Chinese films may not generalize to all deployment contexts, as film audio has specific production characteristics that differ from spontaneous conversations or user-generated content.

## Confidence

- High Confidence: The claim that reasoning errors constitute the majority of failures (86.68% for GPT-4o Audio) is well-supported by detailed error analysis across multiple models.
- Medium Confidence: The conclusion that open-source models lag behind closed-source alternatives is supported but requires caution due to potential language proficiency gaps.
- Low Confidence: The assertion that no model surpassed 80% performance indicating significant room for improvement may be arbitrary rather than reflecting a meaningful capability boundary.

## Next Checks

1. **Cross-Lingual Validation**: Evaluate the same audio clips translated into English to determine whether performance differences stem from audio reasoning limitations versus language proficiency.

2. **Controlled Acoustic Complexity Analysis**: Create systematically varied versions of the same audio clips with controlled levels of acoustic complexity to precisely quantify how different aspects of audio complexity impact performance across model architectures.

3. **Reasoning Chain Visualization**: Implement a visualization tool that traces the model's reasoning process step-by-step for both correct and incorrect answers to identify whether reasoning failures stem from early perceptual errors or genuine logical inference limitations.