---
ver: rpa2
title: LLMs Provide Unstable Answers to Legal Questions
arxiv_id: '2502.05196'
source_url: https://arxiv.org/abs/2502.05196
tags:
- questions
- legal
- llms
- unstable
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the stability of leading LLMs when answering
  legal questions. A novel dataset of 500 difficult legal questions, distilled from
  real U.S.
---

# LLMs Provide Unstable Answers to Legal Questions

## Quick Facts
- **arXiv ID:** 2502.05196
- **Source URL:** https://arxiv.org/abs/2502.05196
- **Authors:** Andrew Blair-Stanek; Benjamin Van Durme
- **Reference count:** 21
- **Primary result:** Proprietary LLMs (gpt-4o, claude-3.5, gemini-1.5) are unstable on legal questions even at temperature=0, with instability rates from 10.6% to 50.4%.

## Executive Summary
This study investigates the stability of leading LLMs when answering legal questions by creating a novel dataset of 500 difficult legal questions distilled from real U.S. court cases with 2-1 split decisions. Each question has two possible answers, and models are called 20 times each with temperature=0. The results show that all three tested models are unstable on some percentage of questions, with most instability being model-specific rather than question-specific. Additionally, while gpt-4o and claude-3.5 perform slightly better than chance at accuracy, gemini-1.5 performs worse than chance, suggesting caution before using these LLMs in real-world legal applications.

## Method Summary
The study curates a dataset of 500 legal questions from 2-1 split U.S. federal Court of Appeals decisions, each summarized into five paragraphs. The questions are posed to three models (gpt-4o, claude-3.5, gemini-1.5) with temperature=0 and specific sampling parameters, running 20 repetitions per question. A follow-up call extracts the binary answer (Party A or Party B). Stability is measured as the percentage of questions where the same answer appears at least 20/20 times, with instability defined as stability less than 100%. Accuracy is measured against the actual court outcomes.

## Key Results
- All three models show instability: gpt-4o (31.4%), claude-3.5 (10.6%), gemini-1.5 (50.4%)
- Most instability is model-specific rather than shared across models
- gpt-4o and claude-3.5 perform slightly better than chance at accuracy
- gemini-1.5 performs worse than chance at accuracy
- Stability correlates with prompt length for gemini-1.5 but not for other models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Setting temperature=0 fails to guarantee deterministic outputs in proprietary cloud-hosted LLMs.
- **Mechanism:** Distributed computing environments introduce noise. Variations in floating-point accumulation across different GPUs or parallelized execution orders on separate servers may alter the final logit selection, even with a fixed seed.
- **Core assumption:** The observed instability is not solely a function of the model weights but of the hardware/software stack serving the API.
- **Evidence anchors:** The authors state this is impossible to say for sure since the models are proprietary, but hypothesize hardware-level determinism failures.
- **Break condition:** If models are run on strictly deterministic local hardware and instability persists, this mechanism is invalid.

### Mechanism 2
- **Claim:** Semantic ambiguity in "hard" legal cases maps to near-equal probability distributions, causing output flipping.
- **Mechanism:** The dataset consists of cases where judges split 2-1, suggesting the legal arguments for Party 1 and Party 2 are nearly balanced. In the model's latent space, the probability gap between the two token sequences is likely small, making the final argmax selection sensitive to minor perturbations or noise.
- **Core assumption:** The "ground truth" of a split court decision correlates with a flatter model probability distribution for the correct answer.
- **Evidence anchors:** The authors note that qualitatively, the legal analysis was sound in unstable cases, just weighing arguments differently.
- **Break condition:** If instability occurs at high rates on simple, non-ambiguous legal questions, this mechanism fails.

### Mechanism 3
- **Claim:** Instability is largely idiosyncratic to specific model architectures or training data rather than the prompt itself.
- **Mechanism:** Different base models likely internalize legal reasoning patterns differently. The low correlation in stability between models implies that a prompt causing instability in one model does not necessarily trigger it in another; the "fault line" lies in the model's specific weights/alignment.
- **Core assumption:** Legal reasoning stability is an emergent property of specific training pipelines, not just prompt complexity.
- **Evidence anchors:** The authors observe that much of the instability is idiosyncratic to each model with low correlations between which questions destabilized different models.
- **Break condition:** If a prompt causing instability in Gemini-1.5 always causes instability in GPT-4o, the idiosyncratic claim is false.

## Foundational Learning

- **Concept:** **Temperature vs. Determinism**
  - **Why needed here:** Engineers often equate `temp=0` with guaranteed reproducibility. This paper demonstrates that in black-box APIs, `temp=0` is a necessary but insufficient condition for determinism.
  - **Quick check question:** Does setting `seed=42` and `temp=0` guarantee the same response from OpenAI's API if called from two different regions? (Answer: No, per this paper's findings).

- **Concept:** **Stability vs. Confidence**
  - **Why needed here:** The paper explicitly distinguishes these. A model can be "confident" (low entropy in a single run) yet "unstable" (flipping answers across runs). Relying on single-call confidence scores is dangerous in legal tech.
  - **Quick check question:** If a model outputs "I am 99% sure Party A wins" in Run 1, but "Party B wins" in Run 2, is it stable?

- **Concept:** **Ambiguity Datasets (Split Decisions)**
  - **Why needed here:** Standard benchmarks often have "clear" answers. Legal reality involves edge cases. Understanding how to construct datasets from split judicial decisions (2-1 votes) is critical for stress-testing AI in high-stakes domains.
  - **Quick check question:** Why is a dataset of 500 unanimous decisions less useful for testing LLM stability than 500 split decisions?

## Architecture Onboarding

- **Component map:** 5-paragraph legal prompt (Facts + Arguments) -> API Call (Temp=0) x 20 repetitions -> Secondary LLM call to summarize winner from "Chain of Thought" output -> Aggregator logic to count frequency of Party A vs. Party B

- **Critical path:** The extraction step is vital. The paper notes that "think step by step" produces long, unstructured text. A reliable parser (or second LLM call) is required to map this verbose output to a binary label for stability analysis.

- **Design tradeoffs:**
  - **Cost vs. Reliability:** Detecting instability requires N=20 calls per prompt. In production, this increases latency and cost by 20x.
  - **Reasoning vs. Stability:** The "think step by step" prompt improves reasoning quality but likely increases the surface area for divergent generation paths compared to single-token prompting.

- **Failure signatures:**
  - **The "Waffle":** A 50/50 split in responses over 20 runs indicates a query right on the model's decision boundary.
  - **Length Instability:** Gemini-1.5 showed significant correlation between longer prompts and instability (Table 3). This is a specific failure signature for that architecture.

- **First 3 experiments:**
  1. **Sanity Check (Reproducibility):** Select 10 questions; run them 20 times each on a local model (e.g., Llama-3) with fixed seed/temp=0 to isolate API noise from model behavior.
  2. **Consensus Thresholding:** Run the dataset with a "self-consistency" loop (e.g., majority vote of 5 runs) to see if accuracy improves over a single run.
  3. **Prompt Length Ablation:** Truncate the 5-paragraph prompt to 1 paragraph (facts only) and measure if stability improves (specifically testing the Gemini correlation finding).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific technical mechanisms (e.g., floating-point accumulation, distributed hardware parallelization) cause instability in proprietary LLMs even when temperature is set to 0?
- **Basis in paper:** The authors state, "Why are some leading LLMs unstable...? It is impossible to say for sure, since the models are proprietary," and list hardware and mathematical hypotheses.
- **Why unresolved:** The "black-box" nature of the proprietary models (gpt-4o, claude-3.5, gemini-1.5) prevents researchers from inspecting internal inference pipelines or hardware configurations.
- **What evidence would resolve it:** Access to model weights, inference code, and hardware logs, or a controlled reproduction of the experiment on open-weight models with known distributed computing setups.

### Open Question 2
- **Question:** Does the "reasoning" architecture of models like o1 inherently provide greater stability than standard foundational models, and does this stability depend on the temperature setting?
- **Basis in paper:** The authors note that o1 (hardwired to temperature 1.0) was more stable than gpt-4o (temperature 0), stating, "We have several hypotheses... Given the closed nature... we cannot test these hypotheses."
- **Why unresolved:** o1's API does not allow temperature modification, making it impossible to isolate whether the stability stems from the model's reasoning process or other factors.
- **What evidence would resolve it:** An API update allowing temperature control for o1, or the release of a "reasoning" model where temperature can be fixed to 0.

### Open Question 3
- **Question:** Does the process of summarizing complex legal cases into five-paragraph prompts systematically bias LLMs toward outcomes that differ from actual court decisions?
- **Basis in paper:** The authors speculate on why models agree with each other more than the court, asking if "summarization... biases each question in one direction," but they do not test the models against the full original text.
- **Why unresolved:** The experimental design relied exclusively on summarized versions of the cases to manage context windows and standardize input, meaning no comparison to full-text analysis was conducted.
- **What evidence would resolve it:** A comparative study where LLMs rule on the same cases using the full court opinions versus the five-paragraph summaries.

## Limitations

- **API-level determinism uncertainty:** The findings hinge on the assumption that temperature=0 with specified parameters adequately controls for stochasticity, which cannot be fully validated without access to the models' internal determinism controls.
- **Dataset construction bias:** The binary evaluation and summarization process using GPT-4o creates a dependency chain that could systematically advantage certain model architectures.
- **Fundamental binary limitation:** The binary nature of the evaluation (Party A vs. Party B) doesn't capture the nuanced partial agreements, dissenting opinions, or remand decisions common in real legal outcomes.

## Confidence

- **High Confidence:** The finding that different models exhibit different stability profiles is well-supported by the data. The observation that 20 repetitions are needed to detect instability is robust and technically sound.
- **Medium Confidence:** The accuracy results showing models performing slightly better than chance are valid, but the interpretation that this represents "modest competence" should be tempered by the binary nature of the evaluation and the potential summarization bias in the dataset construction.
- **Low Confidence:** The specific mechanism explanation for temperature=0 instability (hardware-level determinism failures) is plausible but cannot be independently verified given the black-box nature of the APIs.

## Next Checks

1. **Determinism Validation:** Run a subset of questions (e.g., 50) through a local, open-source model with fixed seed and temperature=0 to isolate whether instability is truly model-agnostic or specific to the API infrastructure.

2. **Granularity Assessment:** Expand the evaluation beyond binary outcomes to capture partial credit scenariosâ€”for instance, measuring whether models correctly identify the majority reasoning even when selecting the wrong party, or whether they recognize when remand is appropriate.

3. **Stability Cost-Benefit Analysis:** Systematically vary the number of repetitions (5, 10, 15, 20) to quantify the relationship between stability detection reliability and computational cost, providing practical guidance for deployment scenarios.