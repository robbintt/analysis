---
ver: rpa2
title: 'Forgetting-MarI: LLM Unlearning via Marginal Information Regularization'
arxiv_id: '2511.11914'
source_url: https://arxiv.org/abs/2511.11914
tags:
- unlearning
- information
- mari
- unlearn
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Forgetting-MarI, a method for unlearning
  specific data from large language models (LLMs) by targeting only the marginal information
  contributed by the data to be forgotten. Unlike existing approaches that indiscriminately
  remove all information linked to unlearn data, Forgetting-MarI uses a mutual information-based
  penalty to suppress only the unique contribution of the unlearn set beyond what
  is already supported by the retain set.
---

# Forgetting-MarI: LLM Unlearning via Marginal Information Regularization

## Quick Facts
- arXiv ID: 2511.11914
- Source URL: https://arxiv.org/abs/2511.11914
- Authors: Shizhou Xu; Yuan Ni; Stefan Broecker; Thomas Strohmer
- Reference count: 40
- One-line primary result: Forgetting-MarI achieves better utility preservation and forgetting performance than state-of-the-art unlearning methods while providing theoretical guarantees on undetectability.

## Executive Summary
Forgetting-MarI introduces a novel approach to unlearning specific data from LLMs by targeting only the marginal information contributed by the unlearn set, rather than removing all information linked to it. The method uses a mutual information-based penalty to suppress the unique contribution of unlearn data beyond what's supported by the retain set, providing theoretical guarantees on unlearning effectiveness. Experiments on mid-scale LLMs demonstrate superior performance compared to existing methods, with robustness to sequential unlearning and effectiveness against white-box copyright detectors.

## Method Summary
Forgetting-MarI fine-tunes LLMs to unlearn specific data while preserving utility on retain data. The method computes marginal information as the Jensen-Shannon divergence between next-token marginals from retain and a mixture distribution containing both retain and unlearn data. A pooled estimator stabilizes training by averaging marginals across sequence positions. The loss combines this MarI penalty with KL divergence to a frozen original model on retain data. The approach requires simultaneous access to both retain and unlearn sets during training.

## Key Results
- Achieves better utility preservation and forgetting performance than state-of-the-art unlearning methods on GPT-2 Large and Llama-3.2-1B
- Provides theoretical guarantees on unlearning effectiveness through bounds on residual mutual information
- Successfully defeats white-box copyright detectors, demonstrating practical effectiveness for privacy-compliant AI systems
- Robust to sequential unlearning requests without catastrophic utility degradation

## Why This Works (Mechanism)

### Mechanism 1
The method isolates and removes only the "marginal information" (unique contribution) of the data to be forgotten, preserving shared knowledge with the retain set. Unlike standard methods that maximize loss on the entire unlearn set, Forgetting-MarI minimizes the Jensen-Shannon Divergence between the model's output distribution on the retain set and a mixture distribution containing both retain and unlearn data. By driving the JSD to zero, the model ensures that the presence of unlearn data provides no distinguishable signal beyond what is already in the retain set. The mechanism assumes the retain set is available and sufficiently representative of knowledge that should be preserved.

### Mechanism 2
Minimizing mutual information provides a theoretical upper bound on the ability of white-box detectors to identify the forgotten data. The authors derive that residual mutual information directly limits the Bayes accuracy of a discriminator trying to detect if the model was trained on the unlearn data. Theorem 2.1 proves that minimizing this mutual information explicitly bounds the "self-perplexity gap," preventing the model from having suspiciously high confidence on the unlearn set. The mechanism assumes the pathwise non-vanishing condition holds, ensuring token probabilities do not hit zero.

### Mechanism 3
A "pooled" estimator stabilizes unlearning in heterogeneous data regimes by averaging out token-specific noise. Instead of calculating JSD for every token position, the algorithm averages the next-token marginals across the sequence length to form pooled distributions. It computes JSD on these pooled distributions, acting as a variational lower bound to the token-wise loss but offering smoother gradients. The mechanism assumes the heterogeneity of the dataset creates sufficient noise to warrant position-averaging.

## Foundational Learning

- **Concept**: **Jensen-Shannon Divergence (JSD) vs. KL Divergence**
  - **Why needed here**: The method relies on JSD rather than KL divergence to measure distributional shift between retain and unlearn data. Understanding why JSD is symmetric and bounded (while KL is not) is critical to understanding why training remains stable.
  - **Quick check question**: Why would a symmetric distance metric be preferred over a directional one when measuring the "distance" between a mixed distribution and a pure distribution?

- **Concept**: **White-Box Membership Inference**
  - **Why needed here**: The paper's success metric is defeating "white-box" detectors (specifically perplexity-based ones). You must understand that these detectors flag data as "memorized" if the model assigns it anomalously low perplexity compared to unseen data.
  - **Quick check question**: If a model minimizes the perplexity gap between the "unlearn set" and the "retain set," does the model necessarily "forget" the content, or does it just hide the statistical signature of training?

- **Concept**: **Variational Bounds**
  - **Why needed here**: The paper uses "pooled MarI" which acts as a "variational lower bound" to the true token-wise marginal information.
  - **Quick check question**: If the pooled loss is a *lower bound* on the true information leakage, does minimizing the pooled loss guarantee the true information leakage is minimized?

## Architecture Onboarding

- **Component map**: Data Loader -> Logit Extractor -> Distribution Calculator -> MarI Loss Head -> Utility Loss Head -> Optimizer
- **Critical path**:
  1. Load batch (X_r, X_u)
  2. Pass through model to get logits
  3. Convert logits to probabilities and average across batch/sequence to get p_r and p_u (if using pooled)
  4. Compute mixture p_d
  5. Calculate JSD(p_d, p_r) + KL(p_current || p_frozen)
  6. Backpropagate

- **Design tradeoffs**:
  - **Pooled vs. Token-wise**: Use Pooled for large, heterogeneous corpora (more stable gradients). Use Token-wise only if unlearn targets are highly structured and aligned in length/position.
  - **Retain Access**: The method requires access to D_r during unlearning. If D_r is unavailable, this architecture cannot be deployed as described.

- **Failure signatures**:
  - **Oscillating Loss**: Likely caused by using Token-wise MarI on heterogeneous data; switch to Pooled.
  - **Catastrophic Utility Drop**: The mixing parameter α or loss weight λ may be misconfigured, causing the model to prioritize the mixture p_d over the retain p_r too aggressively.
  - **Re-learning**: If sequential unlearning is performed without freezing the "original" reference model correctly, the utility anchor may drift.

- **First 3 experiments**:
  1. **Sanity Check (Toy Data)**: Fine-tune a small GPT-2 on two distinct datasets (A and B). Verify that applying Forgetting-MarI to unlearn B makes the perplexity of B statistically indistinguishable from A.
  2. **Ablation (Pooled vs. Token)**: Run the unlearning process on a dataset with variable sentence lengths (e.g., WikiText). Plot the variance of the loss gradient for Pooled vs. Token-wise modes to verify the stability claim.
  3. **Detector Robustness**: Implement a simple "Min-K%" perplexity detector. Run it against the unlearned model. Confirm that the AUC-ROC drops to ~0.5 (random guess) as shown in Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
Can Forgetting-MarI maintain its theoretical guarantees and utility preservation when scaled to Large Language Models with significantly larger parameter counts (e.g., 7B to 70B+ parameters)? The authors state that future work should explore scalability to even larger models and datasets, as current experiments are restricted to mid-scale models and no existing method provides unlearning guarantees that scale to models in the 7B–70B+ range.

### Open Question 2
Is it possible to achieve marginal information unlearning without requiring access to the original retain set (D_r), perhaps by substituting it with external or synthetic data? The conclusion explicitly lists replacing the retain set with a new dataset to remove the retain access assumption as a direction for future work, as the current definition of Marginal Information relies on distinguishing the distribution of the unlearn set from the retain set.

### Open Question 3
How can the theoretical gap between the derived bounds and empirical performance be bridged to guarantee that Forgetting-MarI finds the "unlearned baseline" with certainty? The conclusion notes it is still unknown how Forgetting-MarI finds the unearthed baseline with certainty, and identifies a need to bridge the gap between theoretical bounds and observed performance, as the paper provides an upper bound on residual mutual information but not a convergence proof.

### Open Question 4
Can a principled framework be developed for the optimal selection of regularization parameters (specifically λ and γ) based on theoretical guidance rather than empirical tuning? The conclusion states that developing principled approaches for optimal parameter tuning, especially with theoretical guidance, remains an open challenge, as the current method relies on setting a "tolerable perplexity gap" and tuning regularization magnitudes until a threshold is met.

## Limitations

- The method requires access to the original retain set during unlearning, which may not be available in real-world scenarios where unlearn requests arrive sequentially
- Experiments are limited to mid-scale models (GPT-2 Large and Llama-3.2-1B), leaving scalability to truly large LLMs (7B-70B+ parameters) untested
- Theoretical guarantees focus on white-box detectors using perplexity-based attacks, with effectiveness against black-box membership inference techniques remaining unclear

## Confidence

**High Confidence**:
- The mathematical formulation of marginal information and its relationship to mutual information is sound
- The pooled estimator provides more stable gradients than token-wise computation for heterogeneous datasets
- The method successfully defeats white-box perplexity-based detectors in tested experimental conditions

**Medium Confidence**:
- The claim of superior utility preservation compared to baseline methods is supported by experimental evidence on tested models and datasets
- The method's effectiveness against sequential unlearning requests is demonstrated but requires more extensive validation
- The theoretical bounds on residual mutual information translate to practical undetectability in controlled experiments

**Low Confidence**:
- Generalization to truly large-scale LLMs without modification
- Robustness against all types of membership inference attacks beyond the white-box detectors tested
- Performance when retain sets are small, incomplete, or unavailable

## Next Checks

1. **Scalability Validation**: Implement and evaluate Forgetting-MarI on a GPT-3.5-class model (e.g., 175B parameters) to assess computational requirements, convergence speed, and memory constraints. Measure whether theoretical guarantees scale with model size.

2. **Black-Box Attack Resistance**: Design and implement a black-box membership inference attack that checks for verbatim string regurgitation or semantic similarity. Evaluate whether the unlearned model still leaks information about the unlearn set through these alternative channels.

3. **Sequential Unlearning Stress Test**: Develop a protocol that simulates realistic unlearning scenarios with multiple, potentially overlapping unlearn requests over time. Track utility drift, memory requirements, and whether theoretical bounds degrade with each successive unlearning operation.