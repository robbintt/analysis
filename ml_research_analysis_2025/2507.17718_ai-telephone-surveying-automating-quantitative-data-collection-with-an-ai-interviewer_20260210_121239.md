---
ver: rpa2
title: 'AI Telephone Surveying: Automating Quantitative Data Collection with an AI
  Interviewer'
arxiv_id: '2507.17718'
source_url: https://arxiv.org/abs/2507.17718
tags:
- survey
- wave
- interviewer
- respondents
- surveys
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: An AI interviewer system was built using automatic speech recognition,
  large language models, and speech synthesis to conduct quantitative telephone surveys.
  It was deployed in two pilot waves with 104 U.S.
---

# AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer

## Quick Facts
- arXiv ID: 2507.17718
- Source URL: https://arxiv.org/abs/2507.17718
- Reference count: 6
- Primary result: AI interviewer system completed 30-minute, 123-question telephone surveys with 86% of respondents rating experience as neutral or positive

## Executive Summary
This study presents an AI interviewer system that automates quantitative telephone surveys using automatic speech recognition, large language models, and speech synthesis. The system was deployed in two pilot waves with 104 U.S. adults from a probability-based panel, administering a comprehensive 30-minute survey. Results showed the AI interviewer demonstrated advantages in handling ambiguous responses, tolerating interruptions, and managing noisy audio, with 86% of completers rating their experience positively in the second wave. Follow-up surveys indicated respondents perceived the AI interviewer as more natural and better at understanding them compared to the first wave, particularly when survey length was reduced.

## Method Summary
The AI interviewer system was built using automatic speech recognition, large language models, and speech synthesis to conduct telephone surveys. The system was deployed in two pilot waves with 104 U.S. adults from a probability-based panel, administering a 123-question, 30-minute survey. The second wave included improvements based on initial findings, with 70 people answering at least one call and 30 completing the full survey. Evaluation relied on self-reported satisfaction measures and follow-up surveys comparing wave 1 and wave 2 performance.

## Key Results
- 86% of survey completers rated their experience with the AI interviewer as neutral or positive in wave 2
- Follow-up surveys showed respondents perceived the AI interviewer as more natural and better at understanding them in wave 2 compared to wave 1
- System demonstrated advantages in handling ambiguous responses, tolerating interruptions, and managing noisy audio

## Why This Works (Mechanism)
The AI interviewer leverages speech recognition to convert spoken responses into text, uses large language models to interpret and respond appropriately to survey questions, and employs speech synthesis to deliver questions and responses verbally. This creates a conversational flow that mimics human interviewers while maintaining consistency and scalability. The system's ability to handle ambiguous responses and tolerate interruptions stems from the flexibility of language models to interpret context and manage conversational turn-taking, while speech synthesis provides natural-sounding delivery that improves respondent comfort.

## Foundational Learning
- Automatic Speech Recognition (ASR): Converts spoken language into text for processing; needed for understanding respondent answers; quick check: verify transcription accuracy across different accents and noise levels
- Large Language Models (LLMs): Interpret questions, generate appropriate responses, and manage conversational flow; needed for natural interaction and handling ambiguity; quick check: test model's ability to handle unexpected responses and maintain context
- Speech Synthesis: Converts text responses into natural-sounding speech; needed for delivering questions and maintaining conversational engagement; quick check: evaluate naturalness and clarity of synthesized speech across different contexts

## Architecture Onboarding

**Component Map**
ASR -> LLM -> Speech Synthesis -> ASR (feedback loop)

**Critical Path**
Audio input → ASR transcription → LLM processing → Response generation → Speech synthesis → Audio output to respondent

**Design Tradeoffs**
- ASR accuracy vs. real-time processing speed: Higher accuracy models may introduce latency that disrupts conversational flow
- LLM model size vs. cost: Larger models provide better understanding but increase operational expenses
- Speech synthesis naturalness vs. computational efficiency: More natural voices require more processing power and may introduce delays

**Failure Signatures**
- Poor ASR transcription leads to misinterpretation of responses and inappropriate follow-up questions
- LLM context loss results in repetitive or irrelevant questions despite previous answers
- Speech synthesis artifacts create unnatural pauses or pronunciation errors that reduce respondent engagement

**First 3 Experiments**
1. Test ASR accuracy across different accents and background noise levels using controlled audio samples
2. Evaluate LLM response appropriateness by feeding it typical survey responses and measuring relevance
3. Measure respondent satisfaction with different speech synthesis voices and speaking rates

## Open Questions the Paper Calls Out
The paper identifies several areas requiring further investigation, including systematic measurement of data quality outcomes such as response accuracy, consistency, and completion rates across different question types. The study also notes the need to better understand how to balance strictness with flexibility in the AI interviewer's response requirements, and to develop more robust methods for detecting respondent misbehavior during surveys.

## Limitations
- Small sample size of 104 participants limits generalizability of findings
- 30-minute survey length likely contributed to high dropout rates and may not reflect typical survey conditions
- Evaluation relies heavily on self-reported satisfaction measures that may be subject to response bias

## Confidence

**High confidence**: The technical architecture description and implementation details are well-documented and verifiable. The comparative results between wave 1 and wave 2 regarding perceived naturalness and understanding are supported by follow-up survey data, though sample sizes are modest.

**Medium confidence**: Claims about system advantages in handling ambiguous responses, tolerating interruptions, and managing noisy audio are plausible based on technical design but lack systematic empirical validation. The reported 86% positive/neutral experience rate is specific but may be influenced by selection bias.

## Next Checks
1. Conduct systematic validation of data quality outcomes (response accuracy, consistency, completion rates) across different question types and respondent demographics
2. Perform controlled experiments comparing AI interviewer performance with human interviewers on identical survey content
3. Test system robustness with larger, more diverse samples and shorter survey durations to assess scalability and real-world applicability