---
ver: rpa2
title: 'COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization
  and Headline Generation'
arxiv_id: '2506.15372'
source_url: https://arxiv.org/abs/2506.15372
tags:
- comments
- article
- dataset
- summarization
- headline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces COSMMIC, a pioneering comment-sensitive multimodal
  multilingual dataset for Indian languages, containing 4,959 article-image pairs
  and 24,484 reader comments across nine major Indian languages. It evaluates summarization
  and headline generation using configurations integrating article text, user comments,
  images, and their combinations, employing models like GPT-4 and Llama-3.
---

# COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation

## Quick Facts
- arXiv ID: 2506.15372
- Source URL: https://arxiv.org/abs/2506.15372
- Reference count: 40
- Introduces COSMMIC dataset with 4,959 article-image pairs and 24,484 reader comments across nine Indian languages

## Executive Summary
COSMMIC is a pioneering dataset for comment-sensitive multimodal summarization and headline generation in Indian languages. It contains 4,959 article-image pairs with 24,484 reader comments across nine major Indian languages. The study evaluates summarization performance using configurations that integrate article text, user comments, images, and their combinations with models like GPT-4 and Llama-3. Results show that incorporating relevant comments and images enhances summarization quality, with "Article + Images + Relevant Comments" yielding the highest improvements. A back-translation study confirms that preserving the original language is crucial for maintaining summarization quality, underscoring the dataset's value for advancing NLP research in Indian languages.

## Method Summary
The study employs zero-shot multimodal prompting with GPT-4 and Llama-3 to generate summaries and headlines from article text, images, and reader comments. Comment filtering uses a fine-tuned IndicBERT classifier to distinguish between supporting, enriching, and disconnected comments. Image classification employs multilingual CLIP to categorize images as supplementary (reinforcing) or complementary (contextual). The pipeline includes preprocessing, filtering, generation, and evaluation using ROUGE and BERTScore metrics with Polyglot tokenization.

## Key Results
- "Article + Images + Relevant Comments" configuration achieves highest ROUGE and BERTScore improvements
- Filtering out disconnected comments significantly improves performance (R1 58.22 vs 35.62 for enriching vs disconnected)
- Back-translation study shows direct language processing preserves quality better than translation pipelines
- Multilingual CLIP classifies 36.84% of images as reinforcing and 63.16% as adding extra information

## Why This Works (Mechanism)

### Mechanism 1: Comment Filtering Improves Signal-to-Noise Ratio
Filtering reader comments to retain only "enriching" or "supporting" inputs improves summarization performance compared to using all comments. A classifier (IndicBERT) assigns labels to comments, and excluding "disconnected" comments increases the signal-to-noise ratio in the input context. The generation model can then focus on content that expands upon or reinforces the article. Evidence shows enriching comments yield R1 58.22 vs disconnected comments dropping to R1 35.62. Break condition: If the comment classifier error rate is high, generation quality degrades to levels similar to the "All Comments" baseline.

### Mechanism 2: Image Categorization Enhances Multimodal Understanding
Categorizing images as "supplementary" (reinforcing) or "complementary" (contextual) allows for better utilization of visual data in text generation. A multilingual CLIP classifier scores image-text alignment, where complementary images allow the model to generate summaries with background information not present in the text. Evidence shows 36.84% of images reinforce the article while 63.16% add extra information. Break condition: If the image is "complementary" but the model lacks specific visual grounding, it may hallucinate details.

### Mechanism 3: Direct Language Processing Preserves Quality
Processing Indian languages directly yields higher quality summarization than translation-based pipelines. Preserving the original language maintains linguistic nuances and sentence structure that are lost during machine translation to English and back. Evidence from back-translation shows noticeable drops in performance when using translation, highlighting loss of linguistic nuances. Break condition: If the model's native proficiency in a specific low-resource language is too low, direct processing may fail.

## Foundational Learning

**Concept: ROUGE and BERTScore in Morphologically Rich Languages**
Why needed: Standard ROUGE can be misleading for Indian languages due to agglutination and morphology. BERTScore is often required to capture meaning where exact word matches fail.
Quick check: If a model generates a synonym or morphological variant of the ground truth, which metric penalizes it more heavily?

**Concept: Zero-Shot Multimodal Prompting**
Why needed: The paper relies on prompting LLMs to ingest text, images, and comments simultaneously without fine-tuning. Understanding how to structure these prompts is critical.
Quick check: How does the order of "Article Text" vs. "Comments" in the prompt typically influence the model's attention weight?

**Concept: IndicBERT and Multilingual CLIP**
Why needed: These are the specific encoder models used for pre-filtering components. IndicBERT is optimized for Indian languages, while multilingual CLIP aligns images with non-English text.
Quick check: Why is a standard English-only CLIP model insufficient for classifying the relationship between a Hindi article and its image?

## Architecture Onboarding

**Component map:** DailyHunt Scraper -> Raw (Article, Image, Comments) -> Comment Filter: IndicBERT -> Image Filter: Multilingual CLIP -> LLM (GPT-4 or Llama-3) via Zero-Shot Prompting -> Polyglot Tokenizer -> ROUGE/BERTScore calculation

**Critical path:** The Comment Classification stage is the primary driver of performance delta. The paper demonstrates that "Disconnected" comments drastically lower scores, meaning the robustness of the IndicBERT filter is the single point of failure for the "Reader-Aware" aspect.

**Design tradeoffs:**
- Llama-3 vs. GPT-4: Llama-3 (8B) is cost-effective but shows instability (generating English or incomplete summaries for 1-2% of inputs). GPT-4 is robust but expensive.
- Strict vs. Loose Filtering: Strictly filtering for only "Enriching" comments yields the highest scores but risks losing the "Supporting" sentiment that aids headline generation.

**Failure signatures:**
- Language Mixing: Llama-3 outputs containing mixed English and target language words
- Translation Hallucination: Significant drop in R1/R2 scores when back-translation is used
- Comment Noise: Sudden drop in BERTScore often indicates "Disconnected" comments have leaked into the prompt context

**First 3 experiments:**
1. Baseline Validation: Run "Article Only" vs. "Article + All Comments" to confirm baseline noise level
2. Filter Ablation: Implement IndicBERT comment classifier and compare "All Comments" vs. "Filtered Relevant Comments" on Hindi data
3. Language Robustness Test: Generate summaries using Llama-3 for a low-resource language (e.g., Odia or Malayalam) and check for "Language Mixing" failure mode

## Open Questions the Paper Calls Out

**Open Question 1:** How can comment-sensitive headline or summary generation be effectively achieved for articles that have not yet accumulated user comments? The paper proposes using dynamic feedback repositories but provides no experimental validation.

**Open Question 2:** Does the performance of the IndicBERT-based comment classifier and multilingual CLIP image classifier transfer effectively to the other eight languages in the dataset? Classifiers were only trained and tested on Hindi data, leaving performance on other languages unknown.

**Open Question 3:** Why does the addition of filtered relevant comments improve summarization performance for GPT-4 but often degrade performance for Llama-3? The paper notes this discrepancy but doesn't determine if it's due to the smaller model's inability to handle longer contexts or confusion from the prompt structure.

## Limitations

- Dataset covers only 4,959 articles across nine languages with uneven distribution (Hindi 34.43%, Bengali 28.61%)
- Comment classifier trained on only 250 manually labeled Hindi comments, raising concerns about generalization to other languages
- Llama-3 tendency to generate mixed-language or English outputs represents a significant failure mode
- Relies entirely on zero-shot prompting without exploring few-shot learning or fine-tuning

## Confidence

**High Confidence:** Comment Filtering mechanism - Empirical evidence clearly demonstrates filtering out "Disconnected" comments improves ROUGE and BERTScore scores

**Medium Confidence:** Image Categorization mechanism - Conceptually sound with supporting data, but specific threshold and limited validation reduce confidence in exact implementation

**Medium Confidence:** Direct Language Processing - Back-translation results provide clear evidence, but only tested on Hindi and specific losses not fully characterized across all nine languages

## Next Checks

1. **Classifier Generalization Test:** Replicate comment classification experiment across all nine languages using same 250-sample training approach, measuring accuracy and F1 scores for each language

2. **Multimodal Ablation Study:** Systematically test all combinations of article text, comments (all vs. filtered), and images (present vs. absent) for subset of articles in each language

3. **Human Evaluation of Summary Quality:** Conduct small-scale human evaluation (50 articles) where native speakers rate summaries on informativeness, coherence, and cultural appropriateness