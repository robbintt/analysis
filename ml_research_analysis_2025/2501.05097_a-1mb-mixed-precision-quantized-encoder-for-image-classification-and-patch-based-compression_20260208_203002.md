---
ver: rpa2
title: A 1Mb mixed-precision quantized encoder for image classification and patch-based
  compression
arxiv_id: '2501.05097'
source_url: https://arxiv.org/abs/2501.05097
tags:
- image
- weights
- compression
- quantization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mixed-precision (3b/2b/1b) quantized encoder
  for both image classification and patch-based compression, designed to be hardware-efficient
  for embedded systems. The key contributions include an automatic adaptive quantization
  framework based on histogram-equidistributed quantization to stabilize quinary and
  ternary weights training, and a layer-shared Bit-Shift Normalization that simplifies
  Batch Normalization.
---

# A 1Mb mixed-precision quantized encoder for image classification and patch-based compression

## Quick Facts
- arXiv ID: 2501.05097
- Source URL: https://arxiv.org/abs/2501.05097
- Authors: Van Thien Nguyen; William Guicquero; Gilles Sicard
- Reference count: 40
- Key outcome: 87.5% CIFAR-10 accuracy with 1Mb memory; 0.25 bpp patch-based compression without block artifacts

## Executive Summary
This paper proposes a reconfigurable mixed-precision (3b/2b/1b) quantized encoder for dual purposes: image classification and patch-based compression. The key innovations include histogram-equidistributed quantization to stabilize low-precision weight training, and layer-shared Bit-Shift Normalization that simplifies Batch Normalization for hardware efficiency. The system achieves state-of-the-art results for its memory footprint, delivering 87.5% classification accuracy while enabling compression at 0.25 bits-per-pixel with superior visual quality compared to existing methods.

## Method Summary
The approach centers on a VGG-7 based Nonlinear Quantized Encoder (NQE) with mixed-precision weights (quinary/ternary/binary) optimized through histogram-equidistributed quantization. Training occurs in two phases: initial standard Batch Normalization for 100 epochs, followed by replacement with layer-shared Bit-Shift Normalization and 120 additional epochs. For compression, the encoder processes non-overlapping 32x32 patches independently, while a dedicated PURENET decoder reconstructs full-frame images using a refinement module to eliminate block artifacts. The hardware-efficient design targets embedded systems with strict memory constraints.

## Key Results
- Achieves 87.5% top-1 accuracy on CIFAR-10 using only 1Mb of memory
- Delivers patch-based compression at 0.25 bits-per-pixel without visible block artifacts
- Outperforms state-of-the-art compression methods in visual quality metrics (PSNR/MS-SSIM)
- Demonstrates successful dual-purpose operation through reconfigurable architecture

## Why This Works (Mechanism)

### Mechanism 1: Histogram-Equidistributed Quantization
Dynamically adjusts quantization step $\Delta$ based on weight distribution statistics to stabilize quinary/ternary weight training. This prevents weight collapse by ensuring quantized weights are distributed with nearly equi-probabilities across available levels, maximizing entropy during training.

### Mechanism 2: Layer-Shared Bit-Shift Normalization
Replaces Batch Normalization with a layer-shared power-of-2 scaling factor, eliminating per-channel multipliers and biases at inference. The scale is derived from the 0.9-quantile of channel scales learned during initial training, significantly reducing hardware complexity.

### Mechanism 3: Spatial Propagation in Decoder
PURENET decoder uses a refinement module with convolutional layers having large receptive fields to propagate information across patch boundaries, eliminating block artifacts typically associated with patch-based compression approaches.

## Foundational Learning

- **Concept: Straight-Through Estimator (STE)**
  - Why needed: Quantization functions are non-differentiable; STE enables backpropagation through quantized weights
  - Quick check: Can you explain how gradients pass through sign or floor functions during backward pass?

- **Concept: Linear Symmetric Quantization**
  - Why needed: Mathematical basis for weight quantization mapping continuous values to discrete levels
  - Quick check: How does quantization step $\Delta$ define clipping threshold for linear symmetric quantizer?

- **Concept: Group-wise Convolution**
  - Why needed: Structural pruning technique reducing parameters in encoder's later stages
  - Quick check: How does reducing groups ($G$) affect MAC operations versus standard dense convolution?

## Architecture Onboarding

- **Component map:** NQE (3b→2b→1b Conv blocks → Bottleneck → Binary Code) → PURENET (Patch Upsampling → Patch Aggregation → Refinement → Full-frame Image)
- **Critical path:** Histogram Equidistribution in encoder training loop; robust $\Delta$ calculation is essential for stable convergence
- **Design tradeoffs:** Accuracy vs. Memory (binary weights in deep layers); Hardware Cost vs. Normalization (BSN vs. standard BN)
- **Failure signatures:** Weight collapse (all weights zero), block artifacts (visual grid lines), hardware overflow (activation overflow)
- **First 3 experiments:** 1) Verify quantization stability by plotting $\tau$ evolution over epochs, 2) BSN ablation study comparing with standard BN, 3) Bottleneck profiling accuracy vs. parameter count

## Open Questions the Paper Calls Out
- How effectively can quantized RNN units extend the encoder's capabilities to handle frame sequences for both classification and compression?
- To what extent do skip connections and self-attention mechanisms improve NQE performance while maintaining hardware efficiency?
- Can reconfigurability be leveraged using ensemble learning to enhance patch-based compression quality?

## Limitations
- Training stability for mixed-precision weights lacks empirical evidence showing performance under non-symmetric or multi-modal weight distributions
- BSN accuracy preservation doesn't include ablation studies against alternative low-precision normalization schemes or sensitivity analysis of quantile selection
- PURENET decoder generalizability untested at extreme compression ratios or different patch sizes beyond demonstrated scenarios

## Confidence
- High Confidence: Classification accuracy of 87.5% on CIFAR-10 with 1Mb memory constraint
- Medium Confidence: Hardware efficiency claims regarding BSN implementation
- Low Confidence: Claim of "no block artifacts" in compression without quantitative blockiness metrics

## Next Checks
1. Train same architecture with fixed quantization parameters (no histogram equalization) and compare convergence speed and final accuracy
2. Systematically vary BSN quantile threshold (0.7, 0.8, 0.9, 0.95) and measure accuracy degradation to determine optimal selection
3. Measure mean squared difference between adjacent patch boundaries in compressed outputs, comparing PURENET against standard patch-based compression with and without refinement