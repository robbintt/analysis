---
ver: rpa2
title: 'MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs
  Toward AGI'
arxiv_id: '2506.23563'
source_url: https://arxiv.org/abs/2506.23563
tags:
- reasoning
- arxiv
- mllms
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMReason is a benchmark designed to precisely evaluate long-chain
  reasoning in multimodal large language models. It addresses three limitations in
  existing benchmarks: lack of difficulty and diversity, susceptibility to guessability
  and memorization, and inadequate assessment of intermediate reasoning steps.'
---

# MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI

## Quick Facts
- arXiv ID: 2506.23563
- Source URL: https://arxiv.org/abs/2506.23563
- Reference count: 40
- Primary result: GPT-4o achieves 25.7% final-answer reasoning accuracy and 42.1% intermediate steps reasoning score on this challenging benchmark

## Executive Summary
MMReason is a benchmark designed to precisely evaluate long-chain reasoning in multimodal large language models by addressing key limitations in existing benchmarks: lack of difficulty and diversity, susceptibility to guessability and memorization, and inadequate assessment of intermediate reasoning steps. The benchmark achieves this through curated challenging questions from multiple disciplines and difficulty levels, reformulated into open-ended formats, and filtered using a multi-model voting technique to eliminate guessable or memorized questions. It also includes detailed step-by-step solutions and a reference-based ternary scoring mechanism to assess intermediate reasoning steps. Evaluation on 13 leading MLLMs shows GPT-4o achieving the highest scores, demonstrating MMReason's effectiveness as a challenging benchmark.

## Method Summary
MMReason curates 1,384 challenging multimodal questions from 6 disciplines (Math, Business, Science, Engineering, Social Sciences, Health) after filtering 2,941 initial questions. Questions are reformulated from multiple-choice to open-ended format and filtered using a multi-model voting technique where K=4 powerful MLLMs attempt to answer using only text input. If any model answers correctly without visual input, the question is discarded as potentially memorized or visually irrelevant. The benchmark employs a reference-based ternary scoring mechanism using GPT-4o to decompose responses into steps and score them against manually annotated reference solutions (correct=1, unverifiable=0.5, incorrect=0).

## Key Results
- GPT-4o achieves the highest final-answer reasoning accuracy of 25.7% among 13 evaluated MLLMs
- GPT-4o achieves the highest intermediate steps reasoning score of 42.1%
- The newly collected diverse data proves most challenging, with models showing distinct error patterns in perception (34%) versus reasoning (44%)
- MMReason demonstrates that leading MLLMs still struggle with integrating visual parsing and long-chain logic

## Why This Works (Mechanism)

### Mechanism 1: Elimination of Guessability and Memorization via Data Filtering
- **Assumption:** If a model can answer a multimodal question correctly using only text, it has either memorized the answer or the question doesn't require visual reasoning
- **Evidence:** Multi-model voting technique removes questions answerable without visual input, ensuring genuine multi-modal reasoning
- **Break condition:** Fails if models can reason through problems using textual priors without needing images

### Mechanism 2: Reference-Based Ternary Scoring for Intermediate Steps
- **Assumption:** A capable LLM (GPT-4o) can reliably judge semantic correctness of reasoning steps when provided with ground-truth references
- **Evidence:** Ternary scoring (correct=1, unverifiable=0.5, incorrect=0) acknowledges ambiguity in step evaluation
- **Break condition:** Fails if LLM judge exhibits systematic biases or reference solutions are flawed

### Mechanism 3: Difficulty Stratification through Curation and Reformulation
- **Assumption:** Performance on academic and competition problems is a strong proxy for general reasoning ability
- **Evidence:** Questions span pre-university to university levels across 6 disciplines, creating challenging dataset
- **Break condition:** Fails if academic problem-solving skills don't transfer to broader reasoning tasks

## Foundational Learning

- **Multi-Modal Reasoning (in MLLMs):** The core capability being measuredâ€”integrating text and images for logical deduction. *Quick check:* Can you explain why an MLLM might fail to solve a geometry problem even if it correctly identifies all shapes in the diagram?

- **Data Contamination / Memorization:** Central problem MMReason addresses. *Quick check:* How would you test if a model's impressive performance is due to genuine reasoning or training exposure?

- **LLM-as-a-Judge:** Method used to score intermediate reasoning steps. *Quick check:* What are two potential failure modes of using one LLM to evaluate another LLM's output?

## Architecture Onboarding

- **Component map:** Dataset (questions, images, references) -> Filtering Pipeline (multi-model voting) -> Evaluation Pipeline (GPT-4o ternary scoring)

- **Critical path:** Understanding the evaluation logic in Section 4.3, particularly prompts for step extraction and ternary scoring, as reliability depends on correct implementation

- **Design tradeoffs:**
  - Open-ended vs. MCQ: Essential for eliminating guessing but requires LLM judge, introducing judge biases
  - Automatic vs. Manual Scoring: Scalable but imperfect; "Unverifiable" category (0.5) acknowledges limitations
  - Committee Filtering: K=4 models for T=2 rounds increases robustness but is computationally expensive

- **Failure signatures:**
  - High final score but very low intermediate step score suggests shortcuts or flawed answer extraction
  - High score with obvious logical errors in reasoning steps indicates overly lenient LLM judge

- **First 3 experiments:**
  1. **Judge Consistency Check:** Manually score 50 random responses and compare to automated GPT-4o scores
  2. **Filtering Ablation:** Evaluate on unfiltered dataset (2,941 questions) to measure impact of memorization filter
  3. **Cross-Discipline Analysis:** Analyze failure modes in poorly performing disciplines (e.g., Engineering) to identify bottlenecks

## Open Questions the Paper Calls Out
- How can MLLM architectures be optimized to reduce distinct error rates in perception (34%) versus reasoning (44%)?
- Does GPT-4o-based ternary scoring introduce ceiling effects or bias against divergent reasoning paths?
- Does text-only voting filtering remove questions where visual data serves as necessary confirmatory cue?

## Limitations
- Missing GPT-4o prompt details for step extraction and ternary scoring makes verification difficult
- Multi-model voting threshold may be too aggressive or too lenient against models with strong textual priors
- Academic and competition-style problems may not fully capture AGI-level reasoning capabilities

## Confidence

**High Confidence:** Benchmark addresses stated limitations; GPT-4o achieves highest scores; dataset composition well-documented

**Medium Confidence:** Multi-model voting effectively eliminates guessable questions; ternary scoring provides reliable assessment; curated dataset represents challenging problems

**Low Confidence:** Performance predicts AGI-level reasoning; difficulty stratification accurately reflects complexity; results directly comparable to other benchmarks

## Next Checks

1. **Judge Consistency Validation:** Manually score 50 random responses using ternary system and compare against automated GPT-4o scores to quantify agreement rates

2. **Cross-Benchmark Comparison:** Evaluate same models on MMReason and MMMU-Pro to assess correlation and identify dataset-specific biases

3. **Practical Reasoning Transfer:** Test whether high MMReason performers also excel at practical reasoning tasks outside academic domain to validate generalizability claims