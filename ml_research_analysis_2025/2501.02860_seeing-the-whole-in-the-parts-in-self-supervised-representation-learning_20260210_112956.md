---
ver: rpa2
title: Seeing the Whole in the Parts in Self-Supervised Representation Learning
arxiv_id: '2501.02860'
source_url: https://arxiv.org/abs/2501.02860
tags:
- representations
- co-byol
- learning
- local
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CO-SSL, a new family of self-supervised learning
  methods that align local and global image representations to model spatial co-occurrences
  of visual features. Unlike previous methods that rely on aggressive cropping or
  masking, CO-SSL applies an SSL loss between local representations (before final
  pooling) and global image representations.
---

# Seeing the Whole in the Parts in Self-Supervised Representation Learning

## Quick Facts
- **arXiv ID:** 2501.02860
- **Source URL:** https://arxiv.org/abs/2501.02860
- **Reference count:** 40
- **Primary result:** 71.5% top-1 ImageNet-1K accuracy with 100 pre-training epochs

## Executive Summary
This paper introduces CO-SSL, a family of self-supervised learning methods that align local and global image representations to model spatial co-occurrences of visual features. Unlike previous approaches relying on aggressive cropping or masking, CO-SSL applies a self-supervised loss between local representations (before final pooling) and global image representations. The authors demonstrate that this approach achieves state-of-the-art results on ImageNet-1K with 71.5% top-1 accuracy after 100 epochs, while also showing improved robustness to noise corruption, internal masking, small adversarial attacks, and large training crop sizes.

## Method Summary
CO-SSL extends existing self-supervised learning frameworks (BYOL and DINO) by adding a local-to-global alignment loss. The method computes representations at two levels: local feature maps before global pooling and global pooled representations. An SSL loss is then applied between each local embedding and the global embedding, encouraging local representations of co-occurring features to align. To ensure local representations capture truly local information, the authors introduce RF-ResNet, a modified ResNet architecture with bounded receptive fields (typically 99×99 pixels). The total loss combines standard global instance discrimination with the new local-global alignment term weighted by hyperparameter ws.

## Key Results
- Achieves 71.5% top-1 ImageNet-1K accuracy with 100 pre-training epochs
- Outperforms previous methods including BYOL, DINO, and SimSiam on ImageNet-1K
- Shows improved robustness to noise corruption, internal masking, small adversarial attacks, and large training crop sizes
- RF size analysis reveals optimal performance for receptive fields between 67×67 and 163×163 pixels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning local representations with a global representation via SSL loss enables learning of spatial co-occurrence statistics without aggressive cropping or masking.
- **Mechanism:** The method computes an SSL loss between each local embedding (pre-pooling) and the global embedding. All local representations of co-occurring features are attracted toward the same global representation center. This creates invariance to which specific features are present—if a trunk is visible, the representation aligns with "elephant" regardless of whether a tusk is also visible.
- **Core assumption:** Visual features that frequently co-occur in natural images should have similar representations; the global representation serves as an anchor for its constituent parts.
- **Evidence anchors:**
  - [abstract]: "aligning local representations (before pooling) with a global image representation"
  - [section 3.1]: "CO-BYOL 'pushes' a global image representation (elephant) to lie at the center of and 'attract' its co-occurring local representations (trunk, tusk, etc.)"
  - [corpus]: Related work "How PARTs assemble into wholes" explores spatial-aware pretext tasks but does not directly validate this specific local-global alignment mechanism.
- **Break condition:** If local representations already have receptive fields covering the entire image, they capture global information and the mechanism collapses to standard instance discrimination.

### Mechanism 2
- **Claim:** CO-SSL produces highly redundant local representations within an image, which explains improved robustness to corruption and partial occlusion.
- **Mechanism:** Through triangle inequality—all local representations are pushed toward the same global representation, they are consequently pushed toward each other. To corrupt a global feature, multiple local representations must be altered rather than just one.
- **Core assumption:** Robustness emerges when multiple redundant "votes" encode the same semantic content; corruption of individual votes is tolerable.
- **Evidence anchors:**
  - [abstract]: "CO-SSL learns highly redundant local representations, which offers an explanation for its robustness"
  - [section 4.4]: "an important difference between CO-BYOL and BYOL is the increased similarity between intra-image local representations... a corruption must then alter several local representations instead of one"
  - [corpus]: "Semantic Concentration for Self-Supervised Dense Representations Learning" identifies over-dispersion of patches as problematic, suggesting convergence (redundancy) is desirable, though this paper does not directly validate the robustness claim.
- **Break condition:** If the loss weight coefficient ws is too low (near 0), local representations do not converge toward similarity and redundancy is lost.

### Mechanism 3
- **Claim:** Bounding receptive field size is necessary for local representations to capture truly local information; standard CNNs have RFs exceeding image size, making "local" representations effectively global.
- **Mechanism:** RF-ResNet restricts receptive fields (e.g., 99×99 pixels) by replacing most 3×3 convolutions with 1×1 convolutions while maintaining parameter count through additional layers and skip connections.
- **Core assumption:** Meaningful co-occurrence learning requires that local representations encode spatially bounded regions; if RF ≥ image size, the local-global distinction is meaningless.
- **Evidence anchors:**
  - [section 3.2]: "Modern CNN architectures learn local representations with RF size beyond 400×400 pixels... each 'local' representation may potentially be a representation of the whole image"
  - [section 4.3]: "CNNs with RF sizes between 67×67 and 163×163 yield better results with CO-BYOL than tiny (33×33) or large (425×425)"
  - [corpus]: No direct corpus evidence on receptive field effects in SSL.
- **Break condition:** If RF is too small (33×33), insufficient semantic information is captured; if too large (425×425), the mechanism provides no benefit over standard SSL.

## Foundational Learning

- **Concept: Self-Supervised Instance Discrimination (BYOL/DINO)**
  - Why needed here: CO-SSL extends BYOL and DINO; understanding asymmetric online/target networks, stop-gradient, and projection heads is prerequisite.
  - Quick check question: Why does BYOL use a target network with exponentially moving average weights rather than shared weights?

- **Concept: Receptive Field Computation in CNNs**
  - Why needed here: RF-ResNet design requires computing how kernel size, stride, and depth determine RF size via the formula RF(L) = Σ(kl-1)×Πsi + 1.
  - Quick check question: Given kernel sizes k=[7,3,3] and strides s=[2,1,1], what is the RF size after layer 3?

- **Concept: Spatial Co-occurrence Statistics in Vision**
  - Why needed here: The paper draws explicit motivation from human statistical learning (Fiser & Aslin, 2001); understanding why co-occurrence modeling aids categorization clarifies the design rationale.
  - Quick check question: How would learning "trunk co-occurs with tusk" help a model recognize elephants from partial views?

## Architecture Onboarding

- **Component map:**
  Image x → Two augmentations (t, t') → Online Network fθ, g1θ, g2θ, q1θ, q2θ → Local repr Lθ (n² feature maps before pooling) → Global hθ=pool → Global proj/pred (g1θ→q1θ) → Local proj/pred (g2θ→q2θ, 1×1) → L_global (standard BYOL) → Local repr L'ξ (before pooling) → Global h'ξ=pool → Global proj (g1ξ) → Local proj (g2ξ, 1×1) → L_local (CO-SSL) → L_CO-BYOL = L_global + ws × L_local

- **Critical path:**
  1. Configure RF-ResNet hyperparameters (m, s', s'', s''') for target RF size
  2. Local projection heads (g2, 1×1 conv) must process all n² local representations per image
  3. Set weight coefficient ws ∈ [0.2, 0.5] to balance local vs global loss
  4. Set minimum crop ratio cmin (default 0.2) for augmentation diversity

- **Design tradeoffs:**
  - RF size: smaller = more local but less semantic; optimal range 67–163 pixels for 224×224 images
  - ws value: too high overwhelms global loss; too low loses co-occurrence benefit
  - Compute: RF99-ResNet requires ~3× backbone FLOPs (106G vs 32.8G) due to larger feature maps; standard R50 variant adds only local head overhead
  - CO-BYOL (R50): +1.4% accuracy, same backbone FLOPs as BYOL; CO-BYOL (RF99-R50): +1.5% accuracy, 3× FLOPs

- **Failure signatures:**
  - Accuracy degrades if RF < 67×67 (insufficient semantics) or > 163×163 (too global per Section 4.3)
  - If ws ≈ 0, method reduces to standard BYOL with no local-global benefit
  - Aggressive local head downsampling (reducing n²) drops accuracy: 84.2%→87.6% as n² increases from 1 to 196 (Table 5)
  - CO-BYOL (R50) may use shortcut strategy where ERF covers distinct but large subparts; RF99-R50 forces bounded local inference (Figure 4, Section 4.4)

- **First 3 experiments:**
  1. **Baseline verification on ImageNet-100:** Implement CO-BYOL with standard ResNet50, compare ws=0 (BYOL baseline) vs ws=0.2 to confirm ~3–4% improvement.
  2. **RF size sweep:** Train RF-ResNet variants (RF67, RF99, RF163) with fixed ws=0.2, cmin=0.2 to identify optimal RF for target resolution.
  3. **Robustness validation:** Apply ImageNet-C noise/blur corruptions and internal masking (remove 50–90% of local representations) to verify claimed robustness over BYOL baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural design choices in the original DINO paper cause CO-DINO to underperform compared to DINO-mc or CO-BYOL?
- Basis in paper: [explicit] "The reason is unclear to us, but we suspect that the design choices made in the original paper may favor DINO-mc... and leave to future work an analysis of how these designs impact CO-DINO."
- Why unresolved: The authors observed the performance gap but did not isolate whether specific components (e.g., bottleneck layers, deep projection heads, or normalization) caused the suboptimal alignment in their CO-DINO implementation.
- What evidence would resolve it: An ablation study varying DINO-specific architectural components within the CO-SSL framework to identify which design hinders the local-global alignment.

### Open Question 2
- Question: Can exploiting lower-level visual co-occurrences (e.g., between neighboring pixels or low-level representations) further improve robustness against corruptions and adversarial attacks?
- Basis in paper: [explicit] "Given our results, we speculate that this may make the representations even more robust to corruptions and adversarial attacks."
- Why unresolved: The current work focuses on the whole object and large object parts (local vs. global); the hypothesis that extending this to lower-level features improves robustness remains untested.
- What evidence would resolve it: Experiments applying the CO-SSL loss to earlier intermediate layers with very small receptive fields, followed by evaluation on robustness benchmarks like ImageNet-C.

### Open Question 3
- Question: What are the performance boundaries of CO-SSL when using a bag of small patch representations with deeper ResNets and more training epochs?
- Basis in paper: [explicit] "Yet, we lacked computational resources to touch the boundaries of what can be learned with a bag of small patch representations. That would require even deeper ResNets and more training epochs."
- Why unresolved: The authors were resource-constrained and could not determine if the high performance of patch representations scales indefinitely or plateaus with increased model capacity and training duration.
- What evidence would resolve it: Scaling laws analysis using significantly deeper RF-ResNet architectures trained for extended durations (e.g., 1000+ epochs) on large-scale datasets.

## Limitations

- **Computational Overhead:** RF99-ResNet requires approximately 3× the FLOPs of standard ResNet50, limiting scalability to larger datasets or higher resolutions.
- **Hyperparameter Sensitivity:** The local loss weight ws critically affects performance and stability, with the optimal value likely dependent on architecture and dataset specifics.
- **Receptive Field Precision:** Precise implementation of RF-ResNet is crucial; errors in stride calculations can cause RF leakage, collapsing the method to standard SSL.

## Confidence

- **Local-global alignment mechanism (Mechanism 1):** High - Directly supported by ablation studies showing performance drop when ws→0.
- **Redundant local representations explaining robustness (Mechanism 2):** Medium - Correlation observed but causation not rigorously proven; alternative explanations possible.
- **Bounded RF necessity (Mechanism 3):** High - Clear empirical evidence showing performance degradation outside 67-163 pixel range.

## Next Checks

1. **RF size verification:** Compute and visualize the Effective Receptive Field for RF99-ResNet to confirm it stays within the 99×99 target and doesn't leak to full image coverage.

2. **Local redundancy quantification:** Measure pairwise cosine similarity between local representations within images across different ws values to empirically verify the redundancy claim and its relationship to robustness.

3. **Computational efficiency analysis:** Profile memory usage and training time for RF99 vs standard R50 on the same hardware to quantify the practical overhead and identify bottlenecks.