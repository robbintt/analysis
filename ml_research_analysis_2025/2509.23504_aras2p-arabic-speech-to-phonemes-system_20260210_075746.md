---
ver: rpa2
title: 'AraS2P: Arabic Speech-to-Phonemes System'
arxiv_id: '2509.23504'
source_url: https://arxiv.org/abs/2509.23504
tags:
- arabic
- arxiv
- pretraining
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents AraS2P, a speech-to-phonemes system that ranked
  first in the Iqra''Eval 2025 shared task for Quranic pronunciation assessment. The
  system adapts Wav2Vec2-BERT through a two-stage training strategy: first, task-adaptive
  pretraining on large-scale Arabic speech-phoneme datasets generated using the MSA
  Phonetiser from Common Voice, SADA, and MASC; second, fine-tuning on the official
  task data augmented with XTTS-v2 synthesized recitations featuring varied Ayat segments,
  speaker embeddings, and textual perturbations to simulate human errors.'
---

# AraS2P: Arabic Speech-to-Phonemes System

## Quick Facts
- arXiv ID: 2509.23504
- Source URL: https://arxiv.org/abs/2509.23504
- Authors: Bassam Matar; Mohamed Fayed; Ayman Khalafallah
- Reference count: 11
- Primary Result: First place in Iqra'Eval 2025 shared task with F1-score of 0.4726

## Executive Summary
AraS2P is a speech-to-phonemes system designed for Quranic pronunciation assessment that achieved first place in the Iqra'Eval 2025 shared task. The system addresses the challenge of Arabic mispronunciation detection by adapting Wav2Vec2-BERT through a two-stage training strategy: task-adaptive pretraining on large-scale Arabic speech-phoneme datasets, followed by fine-tuning on augmented official task data. The approach effectively handles the complex Arabic phonemic inventory and diacritic significance, demonstrating that phoneme-aware pretraining combined with targeted augmentation yields strong performance in phoneme-level mispronunciation detection.

## Method Summary
The system employs Wav2Vec2-BERT as its base model and implements a two-stage training strategy. Stage 1 involves task-adaptive pretraining using Common Voice Arabic (157h), SADA (668h), and MASC (1,000h) datasets, with transcripts converted to phonemes via MSA Phonetiser. Stage 2 fine-tunes on official task data augmented with XTTS-v2 synthesized recitations featuring varied Ayat segments, speaker embeddings, and textual perturbations to simulate human errors. The model uses AdamW optimizer with learning rate 1e-5, weight decay 0.01, and CTC loss. Training progresses through 800k iterations for pretraining and 2.5 epochs for fine-tuning, with the final checkpoint selected based on overall performance rather than best validation score.

## Key Results
- Achieved first place in Iqra'Eval 2025 shared task with F1-score of 0.4726
- Demonstrated effectiveness of phoneme-aware pretraining for Arabic speech tasks
- Showed that synthetic data augmentation improves mispronunciation detection performance
- Successfully handled complex Arabic phonemic inventory including diacritics and shadda

## Why This Works (Mechanism)
The system's success stems from addressing the fundamental challenge of Arabic speech recognition: the complex phonemic inventory with significant diacritics and the gap between Modern Standard Arabic (MSA) used in pretraining data and Quranic Arabic in the target domain. By using MSA phonetiser to convert text to phonemes for pretraining, then fine-tuning on task-specific data with synthetic augmentation, the model learns both general Arabic phonemic patterns and domain-specific pronunciation characteristics. The two-stage training approach allows the model to first learn broad phonetic representations before specializing to the Quranic recitation domain.

## Foundational Learning
- **CTC Loss Function**: Needed for sequence-to-sequence phoneme prediction; quick check: verify output length matches input audio duration
- **Wav2Vec2-BERT Architecture**: Needed for speech representation learning; quick check: confirm model has been pretrained on multilingual data
- **Phoneme Inventory Mapping**: Needed to convert MSA phonemes to task-specific phonemes; quick check: validate mapping table completeness
- **Synthetic Data Generation**: Needed to simulate human mispronunciation patterns; quick check: verify noise algorithm parameters
- **AdamW Optimization**: Needed for stable training with weight decay; quick check: confirm learning rate and weight decay values

## Architecture Onboarding

### Component Map
Wav2Vec2-BERT (pretrained) -> MSA Phonetiser (text→phoneme conversion) -> Task-Adaptive Pretraining (800k iterations) -> Synthetic Data Augmentation (XTTS-v2) -> Fine-Tuning (2.5 epochs) -> F1-score evaluation

### Critical Path
The critical path for reproduction follows: pretraining data preparation (phoneme conversion) → task-adaptive pretraining (800k iterations) → synthetic data generation (textual noise + XTTS-v2 synthesis) → fine-tuning (2.5 epochs) → evaluation. The checkpoint selection at 2.5 epochs (not best validation) is crucial for final performance.

### Design Tradeoffs
The system trades off between general Arabic phonetic knowledge (from large pretraining datasets) and domain-specific Quranic pronunciation patterns (from fine-tuning). Using synthetic augmentation instead of more real Quranic recitation data balances the need for diverse mispronunciation patterns against data availability constraints.

### Failure Signatures
- Distribution mismatch between pretraining phonemes and task data (rare diacritics like "shadda" not well-represented)
- Validation-test distribution shift causing overfitting to validation set
- Incomplete phoneme mapping table leading to conversion errors
- Suboptimal noise probability parameters reducing synthetic data quality

### First Experiments
1. Train pretraining-only model and analyze phoneme frequency histograms to identify distribution gaps
2. Generate synthetic data with varying p_noise values (0.1, 0.2, 0.3) to find optimal noise level
3. Fine-tune on training data only vs training data + synthetic augmentation to measure augmentation impact

## Open Questions the Paper Calls Out
1. **Qualitative Failure Modes**: The paper explicitly states it leaves qualitative analysis of the top-performing system (fine-tuned on Tuning dataset with synthetic augmentation) for future work, comparing only pretraining-only and training-data-only setups.
2. **Phonological Accuracy of Noise Generation**: The paper uses random perturbations rather than phonologically informed rules, validating only through final F1-scores without comparing against realistic human mispronunciation patterns.
3. **Pretraining vs Fine-tuning for Rare Phonemes**: The paper addresses diacritic detection via fine-tuning but doesn't evaluate whether domain-specific pretraining data could resolve these deficiencies without fine-tuning.

## Limitations
- Incomplete phoneme mapping table (Table 2) hinders faithful reproduction
- Unspecified hyperparameters for synthetic data generation (p_noise, max_noise, XTTS-v2 speaker embeddings)
- Lack of qualitative error analysis for the best-performing system
- No evaluation of alternative approaches for handling rare phonemes

## Confidence

**High Confidence**: The two-stage training methodology with specific hyperparameters is clearly described and verifiable through leaderboard results. CTC loss and AdamW parameters are fully specified.

**Medium Confidence**: The approach to handling Arabic phonemic complexity is sound, but exact implementation details for phoneme conversion and synthetic data generation are incomplete.

**Low Confidence**: The impact of pretraining data distribution on generalization to rare diacritics and the optimal configuration for synthetic augmentation remain unclear.

## Next Checks
1. Reconstruct the complete phoneme mapping table by analyzing phoneme frequency distributions in provided datasets
2. Implement synthetic data generation pipeline with multiple noise probability configurations to determine optimal settings
3. Compare phoneme frequency histograms of pretraining data versus fine-tuning data to identify distribution mismatches