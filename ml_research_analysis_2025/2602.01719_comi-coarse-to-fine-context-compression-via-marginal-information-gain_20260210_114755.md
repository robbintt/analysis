---
ver: rpa2
title: 'COMI: Coarse-to-fine Context Compression via Marginal Information Gain'
arxiv_id: '2602.01719'
source_url: https://arxiv.org/abs/2602.01719
tags:
- compression
- comi
- information
- context
- redundancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes COMI, a coarse-to-fine context compression
  framework that dynamically optimizes for both task relevance and semantic diversity
  under high compression rates. By introducing Marginal Information Gain (MIG), COMI
  adaptively reallocates compression budgets between segments and performs token merging
  within groups.
---

# COMI: Coarse-to-fine Context Compression via Marginal Information Gain

## Quick Facts
- arXiv ID: 2602.01719
- Source URL: https://arxiv.org/abs/2602.01719
- Reference count: 25
- Key outcome: Achieves up to 25-point EM gain under 32x compression

## Executive Summary
This paper introduces COMI, a coarse-to-fine context compression framework that dynamically optimizes both task relevance and semantic diversity under high compression rates. The framework introduces Marginal Information Gain (MIG) as a criterion for adaptively reallocating compression budgets between segments while performing token merging within groups. Through extensive experiments, COMI demonstrates significant improvements over existing methods, particularly under extreme compression scenarios. The work establishes MIG as a critical metric for efficient long-context modeling in large language models.

## Method Summary
COMI operates through a two-stage compression process that first divides long contexts into coarse segments, then applies fine-grained compression within each segment. The framework introduces Marginal Information Gain (MIG) as a dynamic metric to allocate compression budgets adaptively based on information density and task relevance. Token merging within groups is performed using a combination of semantic similarity scoring and importance weighting, allowing the system to maintain critical information while achieving high compression ratios.

## Key Results
- Achieves up to 25-point EM gain compared to baselines under 32x compression
- Demonstrates superior performance across NarrativeQA, QuALITY, and GiantMix datasets
- Outperforms existing compression methods in both retrieval and generation tasks

## Why This Works (Mechanism)
The effectiveness of COMI stems from its dynamic budget allocation strategy enabled by MIG. Rather than applying uniform compression across all segments, the framework identifies information-rich regions that warrant higher compression budgets while aggressively compressing less critical areas. The coarse-to-fine approach allows for global context understanding before local optimization, preventing the loss of cross-segment dependencies that often plague uniform compression methods.

## Foundational Learning

**Marginal Information Gain (MIG)**
- Why needed: To quantify the value of retaining specific information under compression constraints
- Quick check: Can be validated by measuring information preservation vs compression ratio across different segments

**Coarse-to-fine segmentation**
- Why needed: To balance global context understanding with local optimization efficiency
- Quick check: Evaluate segment boundary detection accuracy and its correlation with downstream task performance

**Token merging strategies**
- Why needed: To reduce context length while preserving semantic integrity
- Quick check: Compare merged token semantic similarity scores against original tokens using embedding distance metrics

## Architecture Onboarding

**Component Map**
Coarse Segmentation -> MIG Budget Allocation -> Fine-grained Token Merging -> Compressed Context Output

**Critical Path**
The most critical path is the MIG computation and budget allocation stage, as it directly determines compression efficiency and information preservation. Errors in this stage cascade through the token merging phase, leading to suboptimal compressed contexts.

**Design Tradeoffs**
The framework trades computational overhead in the MIG calculation phase for significant gains in compression quality. The coarse-to-fine approach requires additional memory for segment-level processing but enables more intelligent compression decisions compared to uniform methods.

**Failure Signatures**
Performance degradation typically manifests as information loss in middle segments where MIG scores are borderline. The system may also struggle with highly heterogeneous contexts where information density varies dramatically between adjacent tokens.

**First Experiments**
1. Compare MIG-based budget allocation against uniform allocation across synthetic datasets
2. Evaluate token merging quality using human evaluation of semantic preservation
3. Test compression performance on long documents with known critical information locations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on synthetic and manually constructed datasets, limiting real-world applicability assessment
- Performance comparisons are mainly against baselines with similar architectural constraints, not emerging adaptive attention mechanisms
- Lack of ablation studies isolating MIG's impact from the coarse-to-fine segmentation strategy

## Confidence
- High confidence in technical validity of MIG formulation and integration
- Medium confidence in absolute performance gains due to limited real-world dataset evaluation
- Medium confidence in scalability claims without multi-domain testing

## Next Checks
1. Evaluate COMI on diverse real-world long-document datasets (legal contracts, technical documentation) to assess generalization beyond synthetic benchmarks
2. Conduct controlled ablation studies comparing MIG against alternative diversity metrics within the same coarse-to-fine architecture
3. Test robustness under extreme compression ratios (>64x) to identify performance saturation points and failure modes