---
ver: rpa2
title: Accelerating RL for LLM Reasoning with Optimal Advantage Regression
arxiv_id: '2505.20686'
source_url: https://arxiv.org/abs/2505.20686
tags:
- policy
- training
- arxiv
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A-PO, a two-stage reinforcement learning
  framework that achieves up to 2x faster training and over 30% memory reduction compared
  to state-of-the-art methods for LLM reasoning tasks. The method estimates the optimal
  value function V using offline sampling from a reference policy, then performs on-policy
  updates via least-squares regression with a single generation per prompt.
---

# Accelerating RL for LLM Reasoning with Optimal Advantage Regression

## Quick Facts
- arXiv ID: 2505.20686
- Source URL: https://arxiv.org/abs/2505.20686
- Reference count: 40
- Key result: Achieves 2x faster training and 30% memory reduction compared to PPO/GRPO while maintaining competitive performance

## Executive Summary
This paper introduces A*-PO, a two-stage reinforcement learning framework for fine-tuning LLMs on reasoning tasks. The method estimates the optimal value function V* using offline sampling from a reference policy, then performs on-policy updates via least-squares regression with a single generation per prompt. Theoretical analysis shows near-optimal performance with polynomial sample complexity without requiring explicit exploration strategies. Empirical results on GSM8K, MATH, and competition-level benchmarks demonstrate competitive or superior performance across multiple model sizes (1.5B, 3B, 7B), achieving the lowest KL-divergence to base models while maintaining training efficiency.

## Method Summary
A*-PO is a two-stage framework for KL-regularized RL on LLM reasoning tasks. In Stage 1 (offline), it estimates the optimal value function V* per prompt by sampling N responses from a reference policy and computing V̂*(x) = β₁ ln(1/N Σ exp(r(x,yᵢ)/β₁)). In Stage 2 (online), it performs on-policy updates using a single generation per prompt, optimizing the least-squares loss (β₂ ln(π/πref) - (r - V̂*))² with AdamW and learning rate 1e-6. The method eliminates online value estimation and multiple generations per prompt, achieving 2x speedup and 30% memory reduction compared to state-of-the-art methods.

## Key Results
- 2x faster training compared to PPO and GRPO on GSM8K and MATH benchmarks
- 30%+ memory reduction during training across 1.5B, 3B, and 7B model sizes
- Competitive or superior performance on out-of-domain tasks while maintaining lowest KL-divergence to base models
- Single generation per prompt sufficient for convergence (vs 16 generations for PPO/GRPO)

## Why This Works (Mechanism)

### Mechanism 1
- Pre-computing the optimal value function V* offline eliminates costly online value estimation during training
- The KL-regularized optimal value has closed form V*(x) = β ln E_{y∼πref}[exp(r(x,y)/β)], estimated from N i.i.d. samples
- Core assumption: Reference policy has non-zero probability of generating correct solutions (vref > 0)
- Evidence: Abstract states offline sampling eliminates online value estimation; Section 3 describes offline computation without gradient computation

### Mechanism 2
- Regressing log-probability ratio to optimal advantage via least-squares yields simpler, more stable training than policy gradient with clipping
- Loss ℓ_t(π) = E[(β ln(π/πref) - (r - V*))²] has optimal policy π* as global minimizer
- Core assumption: Policy class is realizable and loss is bounded
- Evidence: Section 3 states no clipping mechanism is applied; Theorem 1 shows convergence under realizability

### Mechanism 3
- KL regularization with positive β enables polynomial-sample convergence without explicit exploration under mild conditions
- KL constraint ensures π* has support overlap with πref; when vref > 0, no optimism-based exploration needed
- Core assumption: min_x E_{y∼πref}[r(x,y)] ≥ vref > 0 and decoupling coefficient is bounded
- Evidence: Section 5 states KL-regularized RL with β > 0 and vref > 0 can be solved without sophisticated exploration

## Foundational Learning

- **KL-regularized RL objective**: Why needed? Builds on max_π E[r] - β·KL(π||πref); understanding closed-form π* and V* is essential
  - Quick check: Can you derive why π*(y|x) ∝ πref(y|x)exp(r(x,y)/β)?

- **Advantage functions (A^π vs A*)**: Why needed? A*-PO uses fixed optimal advantage A* instead of current-policy advantage A^{π_t}
  - Quick check: Why does using A* instead of A^{π_t} eliminate need for online critic networks?

- **No-regret online learning basics**: Why needed? Theoretical analysis frames algorithm as no-regret learning with sublinear regret Reg(T)
  - Quick check: If Reg(T) = O(√T), what does Theorem 1 imply about performance gap after T iterations?

## Architecture Onboarding

- **Component map**: Prompt set X → N samples per prompt from πref → Compute V̂*(x) → Current π_t → Sample y ∼ π_t(·|x) → Compute loss ℓ_t → SGD update → π_{t+1}

- **Critical path**: Stage 1 latency ∝ |X| × N × generation_time (parallelizable); Stage 2 latency per iteration ∝ batch_size × 1 generation per prompt

- **Design tradeoffs**:
  - N (offline samples): Higher N reduces V* bias but increases Stage 1 cost; N=8 sufficient per experiments
  - β₁ vs β₂: Large β₁ (e.g., 0.5) smooths V* estimation; small β₂ (e.g., 1e-3) relaxes KL constraint
  - Filtering: Removes unsolvable prompts, speeds training 28%, but may reduce diversity

- **Failure signatures**:
  - Training loss plateaus high → check if vref ≈ 0 for many prompts (V* estimates unreliable)
  - Policy collapses to πref → β₂ may be too large or learning rate too small
  - Memory still high → verify only 1 generation per prompt is being stored during Stage 2

- **First 3 experiments**:
  1. **V* estimation ablation**: Vary N ∈ {1, 2, 4, 8, 16, 32} and plot squared regression loss and MATH500 accuracy
  2. **β sensitivity**: Vary β₁ ∈ {∞, 2, 1, 0.5, 0.25, 0.125} while fixing β₂=1e-3
  3. **Baseline comparison on GSM8K**: Replicate Figure 2 results with PPO, GRPO, REBEL, and A*-PO on Qwen2.5-1.5B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does pre-computing V* over training prompts harm generalization to out-of-domain tasks, and can principled regularization mitigate this?
- Basis: Section 4.2 mentions concern about overfitting to training set
- Why unresolved: Authors show empirical generalization but no theoretical characterization of when overfitting occurs
- What evidence would resolve it: Systematic study varying dataset size, prompt diversity, and V* estimation noise

### Open Question 2
- Question: How does A*-PO's performance degrade when base model has near-zero success probability (vref ≈ 0) on significant fraction of prompts?
- Basis: Assumption 3 requires vref > 0 for theoretical guarantees; Section 4.4 filtering ablation suggests hard prompts may be unsolvable
- Why unresolved: Theory assumes vref > 0, but real-world training sets may contain many prompts base model cannot solve
- What evidence would resolve it: Controlled experiments varying vref distribution; analysis of failure modes when assumption violated

### Open Question 3
- Question: What is theoretically optimal number N of offline samples for V* estimation given compute budget and prompt difficulty distribution?
- Basis: Section 4.4 ablation shows N=8 works empirically, but plateau behavior lacks theoretical explanation
- Why unresolved: Authors observe convergence at N≈8 but do not derive formal guidance for selecting N across different settings
- What evidence would resolve it: Theoretical analysis of bias-variance tradeoff in V* estimation; experiments mapping optimal N to observable prompt statistics

## Limitations

- Evaluation limited to mathematical reasoning tasks (GSM8K, MATH) with limited out-of-domain validation
- Offline stage scalability untested for larger models (e.g., 70B+) and longer contexts (beyond 16K tokens)
- Theoretical guarantees assume realizability and bounded decoupling coefficients that may not hold for complex LLMs

## Confidence

- **High Confidence (8/10)**: 2x faster training and 30% memory reduction claims well-supported by ablation studies and baseline comparisons
- **Medium Confidence (6/10)**: Theoretical guarantees of polynomial sample complexity without exploration are mathematically sound under stated assumptions
- **Low Confidence (4/10)**: Superior performance across diverse reasoning tasks weakly supported due to focus on mathematical reasoning benchmarks

## Next Checks

1. **Generalization Stress Test**: Evaluate A*-PO on non-mathematical reasoning tasks (e.g., HellaSwag, strategy game reasoning) to validate generalization beyond mathematical domains

2. **Scaling Analysis**: Test method with larger model sizes (34B, 70B) and measure how Stage 1 offline sampling cost scales for frontier models

3. **Realizability Assessment**: Conduct controlled experiments with intentionally suboptimal reference policies (near-zero pass@N) to test performance when vref > 0 assumption is violated