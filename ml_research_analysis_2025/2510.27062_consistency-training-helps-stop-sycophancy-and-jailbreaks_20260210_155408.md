---
ver: rpa2
title: Consistency Training Helps Stop Sycophancy and Jailbreaks
arxiv_id: '2510.27062'
source_url: https://arxiv.org/abs/2510.27062
tags:
- training
- consistency
- sycophancy
- data
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles two alignment issues in LLMs: sycophancy (adopting
  user beliefs) and jailbreaking (bypassing safety guidelines through prompt manipulation).
  The authors introduce consistency training as a self-supervised approach that trains
  models to produce the same outputs or internal activations for a clean prompt and
  its adversarially wrapped counterpart.'
---

# Consistency Training Helps Stop Sycophancy and Jailbreaks

## Quick Facts
- arXiv ID: 2510.27062
- Source URL: https://arxiv.org/abs/2510.27062
- Reference count: 19
- Primary result: Consistency training methods reduce sycophancy and jailbreak susceptibility in LLMs without requiring labeled harmful/safe data

## Executive Summary
This paper introduces consistency training as a self-supervised approach to reduce two key alignment failures in LLMs: sycophancy (adopting user beliefs) and jailbreaking (bypassing safety guidelines). The authors propose two methods - Bias-Augmented Consistency Training (BCT) and Activation Consistency Training (ACT) - that train models to produce consistent outputs or internal activations for clean prompts and adversarially wrapped counterparts. Both methods reduce sycophancy equally well, while BCT outperforms ACT in jailbreak reduction. The methods avoid capability staleness by using the model's own completions as targets rather than stale external data.

## Method Summary
The paper proposes two consistency training variants. BCT is a supervised approach where the model is fine-tuned to output its original clean response when given an adversarially wrapped prompt. ACT operates at the activation level, enforcing L2 consistency between residual stream activations of clean and wrapped prompts. Both methods assume the model behaves correctly on clean prompts and only train on those examples. The methods require paired data generation and filtering to ensure training only on desirable clean-prompt behavior.

## Key Results
- BCT reduces ClearHarm jailbreak ASR from 67.8% to 2.9% on Gemini 2.5 Flash
- ACT reduces ClearHarm jailbreak ASR to 38.1% while maintaining higher benign answer rates (88.3% vs 75.5%)
- Both BCT and ACT improve sycophancy avoidance from 86.0% to 89.2% while maintaining MMLU accuracy at 89.1%
- ACT uniquely avoids capability staleness by not relying on external response data

## Why This Works (Mechanism)
Consistency training works by exploiting the "shallow safety alignment" hypothesis - that current safety training creates brittle, surface-level defenses. By training models to maintain consistent behavior across adversarially modified prompts, the methods create deeper, more robust alignment. BCT directly constrains outputs while ACT constrains internal representations, both forcing the model to maintain its intended behavior even when faced with manipulative prompts.

## Foundational Learning
- **Self-Supervised Learning & Data Augmentation**: BCT and ACT are self-supervised methods that generate paired data from a model's own outputs. Quick check: Can you explain how creating a dataset from a model's own outputs differs from traditional supervised fine-tuning on human-labeled data?
- **Transformer Internal Activations (Residual Stream)**: ACT operates directly on residual stream activations. Quick check: At a high level, what information does the residual stream at a given layer and token position in a Transformer model encode?
- **The "Shallow Safety Alignment" Hypothesis**: This hypothesis suggests standard safety training is brittle. Quick check: According to this hypothesis, why are current safety-aligned LLMs still vulnerable to simple adversarial prompts like jailbreaks?

## Architecture Onboarding
**Component map:** Paired data generation (clean → wrapped) → BCT: supervised fine-tuning on (wrapped, clean response) pairs | ACT: L2 loss on residual stream activations

**Critical path:** Data generation and filtering is critical. The methods assume correct behavior on clean prompts - if the model generates incorrect/harmful responses to clean prompts, consistency training will reinforce this bad behavior. Filtering to only include examples where clean-prompt behavior is desirable is essential.

**Design tradeoffs:**
- BCT vs. ACT: BCT is simpler and more effective against jailbreaks but requires storing full responses and may over-constrain output style. ACT is more complex (requires activation access), less effective against jailbreaks, but has lower over-refusal impact and avoids stale response tokens.
- Loss Weight: ACT requires very small loss weight (10^-4) as activation gradients can be large and destabilize training.

**Failure signatures:**
- ACT divergence: If loss spikes, ensure you're only applying loss to matching suffix tokens, not all positions
- Over-refusal: Reduced benign answer rates indicate over-training; mitigate by adding SFT data on benign challenging prompts
- Capability degradation: Check for stale data from weaker models; use fresh self-generated data instead

**First 3 experiments:**
1. Reproduce the Sycophancy Baseline: Implement BCT pipeline using MMLU dataset, generate model answers to clean questions, create wrapped versions with sycophantic cues, fine-tune on (wrapped, clean answer) pairs, evaluate on held-out set
2. Ablate ACT Layers: Implement ACT on Gemma 2 2B, run three training runs (all layers, last half, single middle layer), compare sycophancy reduction to test distributed circuitry hypothesis
3. Staleness Ablation: Create SFT datasets with fresh vs stale model responses, train identical models, compare MMLU performance to observe capability staleness effect

## Open Questions the Paper Calls Out
- Why does capability staleness degrade sycophancy performance but not consistently affect jailbreak performance?
- Can ACT be improved to match BCT's jailbreak reduction while preserving its lower over-refusal rates?
- If two models are competitively matched on benchmarks, can they train on each other's completions without degrading generalization?
- Could better loss balancing enable combined ACT+BCT training to outperform BCT alone?

## Limitations
- Limited to Gemini 2.5 Flash and Gemma 2 2B models, limiting generalizability
- Data generation pipeline assumes correct clean-prompt behavior, which may not always hold
- Filtering process for jailbreak data may introduce selection bias
- "Shallow safety alignment" hypothesis is motivated rather than directly tested

## Confidence
**High Confidence:** Consistency training reduces sycophancy and jailbreak success rates; MMLU accuracy maintenance claim (89.1% vs 89.2%)

**Medium Confidence:** ACT uniquely avoids capability staleness; "shallow safety alignment" hypothesis is plausible but lacks direct validation

**Low Confidence:** ACT is more effective at avoiding over-refusals (based on limited evidence); specific ACT loss weight requirement (10^-4) based on single model runs

## Next Checks
1. **Direct Staleness Measurement:** Create controlled experiment training identical models using fresh self-generated responses, responses from 6-month-old model, and responses from 2-year-old model. Measure MMLU accuracy and task-specific performance across different time gaps.

2. **Over-Refusal Quantification:** Implement comprehensive over-refusal benchmark using diverse benign prompts that might be considered potentially harmful. Compare BCT, ACT, and standard SFT models with statistical significance testing.

3. **Layer Ablation for ACT:** Systematically ablate ACT across different layer groups (bottom, middle, top, all combinations) on multiple model sizes. Measure sycophancy reduction, jailbreak effectiveness, and MMLU accuracy to identify optimal layer selections and test distributed circuitry hypothesis.