---
ver: rpa2
title: 'VIKING: Deep variational inference with stochastic projections'
arxiv_id: '2510.23684'
source_url: https://arxiv.org/abs/2510.23684
tags:
- should
- viking
- posterior
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of applying variational inference\
  \ to overparameterized deep neural networks, where traditional mean-field approximations\
  \ fail due to strong parameter correlations induced by model reparameterizations.\
  \ The authors propose a novel variational family that explicitly reflects this geometry\
  \ by decomposing the parameter space into two orthogonal subspaces\u2014one capturing\
  \ uncertainty over the training data and another capturing general model uncertainty\u2014\
  via projections onto the kernel and image of the Fisher-Rao metric."
---

# VIKING: Deep variational inference with stochastic projections

## Quick Facts
- arXiv ID: 2510.23684
- Source URL: https://arxiv.org/abs/2510.23684
- Reference count: 40
- Primary result: Achieves state-of-the-art variational inference for overparameterized deep networks by decomposing posterior uncertainty into kernel and image subspaces of the Fisher-Rao metric

## Executive Summary
This paper addresses a fundamental challenge in Bayesian deep learning: performing accurate variational inference in overparameterized neural networks where traditional mean-field approximations fail due to strong parameter correlations induced by model reparameterizations. The authors propose VIKING (Variational Inference with a Known and Interpretable Geometry), which decomposes the parameter space into two orthogonal subspaces using the Fisher-Rao metric—one capturing uncertainty over the training data and another capturing general model uncertainty. By explicitly modeling this geometry through stochastic alternating projections and matrix-free solvers, VIKING achieves state-of-the-art or competitive performance across image classification, out-of-distribution detection, and generative modeling tasks, demonstrating that accurate posterior inference in deep networks is achievable when respecting overparameterization structure.

## Method Summary
VIKING addresses the challenge of variational inference in overparameterized neural networks by leveraging the geometric structure of the parameter space. The method decomposes the posterior into two orthogonal subspaces defined by the Fisher-Rao metric: the kernel (nullspace) and image of the loss-Jacobian matrix. The variational family is parameterized as q(θ) = N(θ|θ̂, σ²_ker·UU^T + σ²_im·(I-UU^T)), where U projects onto the kernel of the loss-Jacobian. To scale inference to large models, VIKING employs stochastic alternating projections using reparameterized sampling, with conjugate gradient methods for kernel projection. The ELBO is computed using Hutchinson trace estimation for the kernel dimension and closed-form KL divergence. A key innovation is the use of a β-scaled KL term to balance posterior uncertainty calibration with model performance.

## Key Results
- Achieves state-of-the-art accuracy and uncertainty calibration on CIFAR-10, Imagenette, and Fashion-MNIST classification tasks
- Demonstrates superior out-of-distribution detection performance with AUROC scores exceeding baseline methods
- Shows competitive negative log-likelihood on VAE generative modeling benchmarks
- Validates that respecting Fisher-Rao geometry improves inference quality compared to mean-field approximations

## Why This Works (Mechanism)
The method works by recognizing that overparameterization in neural networks creates a specific geometric structure in the parameter space. When models are overparameterized, the loss-Jacobian matrix J_θ̂ has a non-trivial kernel representing directions in parameter space that do not affect the loss. VIKING exploits this by decomposing the variational posterior into two independent components: one for the kernel subspace (capturing data-dependent uncertainty) and one for the image subspace (capturing general model uncertainty). This decomposition is achieved through projections onto the kernel and image of the Fisher-Rao metric, which naturally captures the geometry induced by reparameterizations. The stochastic alternating projections allow efficient sampling from this structured variational family, while the matrix-free solver approach scales the method to large models.

## Foundational Learning

**Fisher-Rao metric**: A Riemannian metric on the manifold of probability distributions that measures the infinitesimal distance between distributions. Why needed: Provides the geometric framework for decomposing parameter space into meaningful subspaces. Quick check: Verify that the Fisher-Rao metric reduces to the expected Fisher information matrix for exponential family distributions.

**Kernel and image subspaces**: The nullspace and range of the loss-Jacobian matrix J_θ̂. Why needed: These subspaces capture different types of uncertainty in overparameterized models—kernel directions don't affect the loss, while image directions do. Quick check: Confirm that the rank of J_θ̂ equals the number of linearly independent training examples for a well-trained model.

**Stochastic alternating projections**: An iterative method for sampling from a structured distribution by alternating projections onto subspaces. Why needed: Enables efficient sampling from the variational family without explicitly computing expensive matrix decompositions. Quick check: Verify that the projection operator P_U satisfies P_U² = P_U and P_U + P_(I-U) = I.

**Hutchinson trace estimator**: A stochastic method for estimating the trace of a matrix using random vectors. Why needed: Provides an efficient way to estimate the kernel dimension R without explicitly computing eigendecompositions. Quick check: Confirm that E[η^T A η] = trace(A) for η ~ N(0,I).

## Architecture Onboarding

**Component map**: Data → Loss-Jacobian computation → Kernel projection (CG solver) → Stochastic alternating projections → Variational posterior sampling → ELBO computation → Model parameters update

**Critical path**: The computational bottleneck is the kernel projection via conjugate gradient, which requires multiple backward passes per sample. The Hutchinson trace estimator and closed-form KL computation are comparatively inexpensive.

**Design tradeoffs**: The method trades computational efficiency for improved uncertainty quantification. The matrix-free approach avoids explicit decompositions but requires more iterations than direct methods. The β-scaled KL provides flexibility but requires tuning.

**Failure signatures**: Training instability when σ_im is too large, poor uncertainty calibration when the kernel projection is inaccurate, and convergence issues when the MLE warmup is too aggressive.

**Three first experiments**:
1. Verify kernel dimension estimation on a small overparameterized linear model
2. Compare stochastic alternating projections against direct sampling on a simple 2D example
3. Test ELBO computation with and without Hutchinson trace estimation on a small network

## Open Questions the Paper Calls Out

**Open Question 1**: Does MLE warmup to a sharp minimum inhibit subsequent ELBO optimization, and can this be mitigated? The paper conjectures that sharp minima negatively affect Monte Carlo ELBO estimates but lacks theoretical or empirical proof. Resolution would require comparing Hessian spectra of models stopped at "sweet spot" versus fully converged models.

**Open Question 2**: Can reorthogonalized CG computational overhead be reduced to match standard backpropagation? The paper suggests future work on matrix-free least squares solvers or custom gradients. Resolution would require implementation and benchmarking of these alternatives to demonstrate wall-clock speedups.

**Open Question 3**: Is the independence assumption between kernel and image subspaces a safe approximation for all architectures? The paper admits this may be overly simplistic but performs well empirically. Resolution would require comparative analysis on tasks requiring fine-grained covariance structure or comparison against low-rank plus diagonal approximations.

## Limitations

- Computational overhead from kernel projection via conjugate gradient requires multiple backward passes per sample
- Reorthogonalization procedure adds implementation complexity and potential numerical instability
- Assumption of independence between kernel and image subspaces may discard important covariance information
- Performance sensitive to hyperparameters (γ, β, warmup duration) requiring careful tuning

## Confidence

- Theoretical framework: High - The geometric arguments connecting overparameterization to Fisher-Rao structure are well-grounded
- Empirical results: Medium - Strong performance across benchmarks but limited comparison with other structured variational approaches
- Implementation scalability: Medium - Matrix-free approach is promising but computational overhead remains significant
- Generalizability: Low - Limited evaluation on larger-scale datasets and deeper architectures

## Next Checks

1. Implement systematic ablation comparing full VIKING against variants with random subspace projections versus Fisher-Rao informed projections
2. Extend evaluation to larger-scale datasets (ImageNet) and deeper architectures to test scalability limits
3. Perform detailed sensitivity analysis on the γ parameter in stochastic alternating projections and its effect on convergence and posterior quality