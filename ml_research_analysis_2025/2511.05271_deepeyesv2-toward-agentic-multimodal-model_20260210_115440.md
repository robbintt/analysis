---
ver: rpa2
title: 'DeepEyesV2: Toward Agentic Multimodal Model'
arxiv_id: '2511.05271'
source_url: https://arxiv.org/abs/2511.05271
tags:
- reasoning
- search
- arxiv
- tool
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepEyesV2, an agentic multimodal model that
  integrates tool invocation (code execution and web search) into reasoning loops.
  It addresses the limitation of passive MLLMs by enabling dynamic, context-aware
  tool use for enhanced reasoning.
---

# DeepEyesV2: Toward Agentic Multimodal Model

## Quick Facts
- arXiv ID: 2511.05271
- Source URL: https://arxiv.org/abs/2511.05271
- Authors: Jack Hong; Chenxiao Zhao; ChengLin Zhu; Weiheng Lu; Guohai Xu; Xing Yu
- Reference count: 40
- Key outcome: Agentic multimodal model with dynamic tool use (code execution, web search) via two-stage training, outperforming both general-purpose MLLMs and grounded reasoning models on perception, reasoning, and search tasks.

## Executive Summary
DeepEyesV2 addresses the limitation of passive multimodal models by enabling active, context-aware tool invocation for enhanced reasoning. The authors propose a two-stage training pipeline: supervised fine-tuning (SFT) on a curated dataset to establish tool-use patterns, followed by reinforcement learning (RL) to strengthen and adapt tool invocation. The model achieves strong performance across real-world understanding, mathematical reasoning, and search-intensive tasks, outperforming both general-purpose MLLMs and grounded reasoning models. A new benchmark, RealX-Bench, is introduced to evaluate coordinated perception, search, and reasoning integration.

## Method Summary
The authors propose a two-stage training pipeline to enable agentic multimodal reasoning. First, supervised fine-tuning (SFT) is performed on a cold-start dataset curated by filtering source benchmarks for difficulty (base model ≤2/8 correct) and tool-benefit, then synthesizing multi-turn trajectories with tool calls using strong models and including long CoT text-only data. The SFT uses batch_size=128, lr=1e-5, 3 epochs, AdamW with cosine decay. Second, RL with DAPO is applied using a reward combining accuracy and format (R_acc + R_format), batch_size=256, 16 rollouts/prompt, lr=1e-6, KL=0.0, max_length=16384 tokens, clip_ratio=[0.20, 0.30]. The model uses Qwen2.5-VL-7B as backbone and integrates tool invocation (code execution, web search) into reasoning loops.

## Key Results
- DeepEyesV2 achieves strong performance across real-world understanding, mathematical reasoning, and search-intensive tasks, outperforming both general-purpose MLLMs and grounded reasoning models.
- The model demonstrates task-adaptive tool invocation, with RL enabling more complex tool combinations and flexible, context-aware reasoning.
- DeepEyesV2 introduces a new benchmark, RealX-Bench, to evaluate coordinated perception, search, and reasoning integration.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Tool invocation patterns**: Why needed: To enable the model to dynamically decide when and how to use external tools (code execution, web search) during reasoning. Quick check: Verify the model generates valid tool calls in appropriate contexts during inference.
- **Reward shaping (accuracy + format)**: Why needed: To guide the model toward correct and well-formatted outputs, especially for code execution and search results. Quick check: Inspect reward values and model outputs to ensure both accuracy and format are improving.
- **DAPO (Distributional Actor-Critic Policy Optimization)**: Why needed: To stabilize RL training and prevent reward hacking or tool abandonment. Quick check: Monitor training stability and tool invocation frequency over RL epochs.

## Architecture Onboarding
- **Component map**: Qwen2.5-VL-7B -> SFT -> RL (DAPO) -> DeepEyesV2
- **Critical path**: SFT establishes basic tool-use patterns → RL adapts and strengthens tool invocation → final model performs agentic reasoning
- **Design tradeoffs**: SFT ensures stable initial behavior but may overfit to training patterns; RL enables adaptation but risks reward hacking or tool abandonment
- **Failure signatures**: Reward hacking (placeholder/non-executable code), tool abandonment (short chains without tools), early RL instability (buggy code generation)
- **First experiments**: (1) Verify tool invocation format and execution success rate; (2) Monitor tool usage frequency during RL to detect abandonment; (3) Compare performance on perception, reasoning, and search tasks with and without tools

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What mechanisms cause direct reinforcement learning to fail at inducing robust tool-use behavior in multimodal models?
- Basis in paper: [explicit] The authors state "direct reinforcement learning alone fails to induce robust tool-use behavior" and observe that models either abandon code generation or converge to placeholder code (reward hacking).
- Why unresolved: The paper documents the failure modes empirically but does not isolate whether the issue stems from sparse rewards, insufficient exploration, base model capability gaps, or architectural constraints.
- What evidence would resolve it: Ablation studies varying reward density, exploration strategies, and base model size with controlled experiments isolating each factor's contribution to tool-use acquisition failure.

### Open Question 2
- Question: Can the two-stage training pipeline transfer effectively to larger model scales (e.g., 32B, 72B) or different base architectures?
- Basis in paper: [inferred] All experiments use Qwen2.5-VL-7B as the backbone. The paper compares against larger models but does not train DeepEyesV2 on them.
- Why unresolved: Scaling effects on tool-use emergence, cold-start data requirements, and RL stability remain unknown. Larger models may exhibit different failure modes or require less supervision.
- What evidence would resolve it: Training DeepEyesV2 variants on 32B and 72B backbones with identical data and hyperparameters, comparing convergence speed, final performance, and tool-use patterns.

### Open Question 3
- Question: How can models be improved at integrating image-search results compared to text-search results?
- Basis in paper: [explicit] "Text-only search provides larger improvements than image-only search, suggesting that current models still have limited ability to integrate image-search results effectively."
- Why unresolved: The paper identifies the asymmetry but does not investigate whether the bottleneck lies in query formulation, result parsing, cross-modal grounding, or result aggregation.
- What evidence would resolve it: Controlled analysis of image-search vs. text-search pipelines measuring retrieval relevance, model attention patterns on search outputs, and performance when ground-truth image-search results are provided.

### Open Question 4
- Question: What architectural or training modifications could reduce the substantial performance gap on tasks requiring integration of all three capabilities (perception, search, reasoning)?
- Basis in paper: [explicit] Models show severe limitations on integration tasks (e.g., Gemini achieves 27.8% on integration vs. 46.0% average; DeepEyesV2 achieves 18.1% vs. 28.3%).
- Why unresolved: The integration bottleneck may arise from competing optimization objectives, context length limitations, or lack of explicit multi-skill training signals.
- What evidence would resolve it: Probing experiments measuring intermediate representations for each capability, and training with explicit integration-focused rewards or auxiliary tasks.

## Limitations
- Missing implementation details: exact tool invocation markers, trajectory structure, and integration of tool outputs into observations are unspecified.
- Data transparency: cold-start dataset curation and RL data synthesis lack details on filtering thresholds and number of examples retained per source.
- Tool integration: specifics of code execution sandbox and web search (SerpAPI) setup are not provided.

## Confidence
- High confidence: reported model capabilities and performance gains across benchmarks
- Medium confidence: RL training pipeline and reward design
- Low confidence: exact reproducibility of training data and tool invocation patterns

## Next Checks
1. Reconstruct the tool invocation and observation format by examining the source benchmarks and synthesizing example trajectories that match the described training data.
2. Implement a minimal version of the tool stack (code execution, web search) using open APIs to validate the feasibility of the described integration.
3. Conduct ablation studies on the cold-start dataset filtering criteria and RL reward design to isolate the impact of each training stage on model performance.