---
ver: rpa2
title: 'MLLM-based Speech Recognition: When and How is Multimodality Beneficial?'
arxiv_id: '2507.19037'
source_url: https://arxiv.org/abs/2507.19037
tags:
- modalities
- speech
- modality
- noise
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines when and how multiple input modalities improve
  automatic speech recognition (ASR) accuracy in noisy environments. Using a multi-modal
  large language model (DMLM) architecture, the authors systematically evaluate the
  impact of adding different modalities (audio, lip movements, images, OCR text) under
  varying noise conditions.
---

# MLLM-based Speech Recognition: When and How is Multimodality Beneficial?

## Quick Facts
- arXiv ID: 2507.19037
- Source URL: https://arxiv.org/abs/2507.19037
- Reference count: 40
- Primary result: Adding more modalities improves ASR accuracy under noise, with synchronized modalities most helpful at high noise and unsynchronized modalities peaking at moderate noise levels.

## Executive Summary
This paper investigates how and when multimodal large language models (MLLMs) improve automatic speech recognition (ASR) in noisy environments. The authors systematically evaluate the impact of adding different modalities—audio, lip movements, images, and OCR text—under varying acoustic noise conditions. Using a multi-modal large language model (DMLM) architecture with frozen modality-specific encoders, they demonstrate that harnessing more modalities generally improves ASR accuracy, with the benefit varying by noise level and modality type. The study also reveals important design considerations including visual encoder quality, input ordering effects, and architectural tradeoffs between Transformers and Mamba backbones.

## Method Summary
The method employs a Discrete Multi-modal Language Model (DMLM) architecture where modality-specific frozen encoders (Seamless for audio, AV-HuBERT for lips, DALL-E/ViT for images, EasyOCR for text) convert raw inputs to discrete tokens. These tokens are concatenated with absolute positional encodings and processed by an autoregressive decoder-only LLM (OPT-125M or Mamba-130M). Training uses modality-weighted cross-entropy loss with λS=0.3, λT=0.5, λI=0.5. The primary dataset is a synthetic 3-Equations corpus (8K train/1K val/1K test) with MUSAN noise added to the second half of utterances at varying SNR levels. Additional validation uses SlideAVSR and Spoken Moments in Time datasets.

## Key Results
- Adding more modalities consistently improves ASR accuracy across noise levels, with RB values of +3.1% to +20.4% depending on modality combinations.
- Synchronized modalities (lip movements) are most beneficial at high noise levels, while unsynchronized modalities (images, OCR) peak at moderate noise levels.
- Higher-quality visual encoders (DALL-E vs ViT) yield better multimodal integration due to more semantically-aligned representations.
- Audio-first input ordering provides increasing benefits as noise increases, due to primacy effects with absolute positional encodings.
- Both Transformers and Mamba architectures show similar multimodality benefit patterns, though Mamba offers faster inference but is more hyperparameter-sensitive.

## Why This Works (Mechanism)

### Mechanism 1: Synchronized vs. Unsynchronized Modality Benefits
- Claim: Synchronized modalities (e.g., lip movements) become more beneficial as auditory noise increases, while unsynchronized modalities (e.g., images, OCR) show peak benefit at moderate noise levels with diminishing returns at extreme noise.
- Mechanism: Synchronized modalities maintain temporal alignment with audio, allowing direct cross-modal disambiguation even when audio is severely corrupted. Unsynchronized modalities require the model to first establish correspondence between visual context and audio content—a process that fails when audio is too noisy to provide reliable alignment cues, but also provides limited marginal value when audio is already clean.
- Core assumption: The model has learned meaningful cross-modal representations during pre-training that enable it to map between visual cues and linguistic content.
- Evidence anchors:
  - [abstract] "Synchronized modalities (e.g., lip movements) are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most helpful at moderate noise levels."
  - [Section V-B] "when the audio is too noisy, the unsynchronized modalities (image, OCR) cannot establish a correspondence with the audio, and the visual information is useless."
  - [corpus] Weak direct corpus support; related papers focus on speech enhancement and accent robustness rather than multimodal fusion mechanisms.
- Break condition: If audio noise is so extreme that temporal structure is lost, even synchronized modalities may fail if the model cannot segment the visual stream appropriately.

### Mechanism 2: Visual Encoder Quality Impact
- Claim: Visual encoder quality directly impacts downstream ASR accuracy, with more semantically-aligned encoders (e.g., DALL-E) outperforming structural encoders (e.g., ViT) for multimodal speech tasks.
- Mechanism: Encoders trained with language-aligned objectives produce token representations that more readily integrate with the LLM's existing linguistic knowledge, reducing the learning burden during multimodal fusion. Structural encoders capture visual details but require additional learned transformations to map into semantic space.
- Core assumption: The frozen visual encoder's representations are sufficiently expressive that the downstream LLM can learn useful cross-modal attention patterns without encoder fine-tuning.
- Evidence anchors:
  - [abstract] "Higher-quality visual representations consistently improve ASR accuracy, highlighting the importance of developing more powerful visual encoders."
  - [Section V-D] "DALL-E, with a relative benefit of +8.9%, performs better than ViT on this task... DALL-E produces richer semantic features more aligned with language, thus facilitating multi-modal integration."
  - [corpus] No direct corpus evidence on visual encoder selection for speech tasks.
- Break condition: If the visual encoder's tokenizer produces sequences too long for the LLM context, sequence noise effects may negate encoder quality benefits.

### Mechanism 3: Audio-First Input Ordering
- Claim: Audio-first input ordering provides increasing benefits as noise levels increase, due to interaction between positional encoding, primacy effects, and noise distribution.
- Mechanism: With absolute positional encodings, tokens at known positions (sequence start) are easier for the model to attend to reliably. When audio is placed first, the clean portion of noisy utterances occupies fixed early positions, enabling the model to locate usable signal. Placing audio after variable-length visual tokens pushes the clean audio to unpredictable positions.
- Core assumption: The model exhibits primacy bias—stronger attention to early sequence positions—and the noise distribution leaves some portion of audio relatively clean (in the controlled 3-Equations dataset, noise was added to the second half).
- Evidence anchors:
  - [Section VII-B] "RB of the audio-first models keeps increasing as the noise level increases... and that audio-first is more beneficial in noisy situations."
  - [Section VII-B] "With the A + L → T model, the clean part (first half) of the first utterance is at a known location... In contrast, with the L + A → T model, the audio begins only after a variable-length lip sequence."
  - [corpus] No corpus papers address modality input ordering effects.
- Break condition: If using relative positional encodings or if noise uniformly corrupts the entire audio signal, the ordering effect may diminish or disappear.

## Foundational Learning

- Concept: **Word Error Rate (WER) and Relative Benefit (RB)**
  - Why needed here: These are the primary evaluation metrics. WER measures absolute transcription accuracy; RB quantifies the marginal contribution of adding modalities. Without understanding these, you cannot interpret the experimental results.
  - Quick check question: If an audio-only model achieves 20% WER and adding OCR reduces WER to 16%, what is the RB?

- Concept: **Synchronized vs. Unsynchronized Modalities**
  - Why needed here: This distinction is central to the paper's main finding about when different modalities help. Synchronized modalities (lip movements) have temporal alignment with audio; unsynchronized modalities (static images) provide context without temporal correspondence.
  - Quick check question: Would video of a speaker's hands gesturing be considered synchronized or unsynchronized with their speech?

- Concept: **Autoregressive Decoder-Only LLMs with Discrete Tokenization**
  - Why needed here: The DMLM architecture tokenizes all modalities into discrete tokens processed by a standard language model backbone. Understanding this unified token-space approach is essential for grasping how multimodal fusion occurs.
  - Quick check question: Why might discrete tokenization of continuous audio features lose information compared to continuous feature fusion approaches?

## Architecture Onboarding

- Component map: Raw inputs → Frozen encoders → Discrete token sequences → Token concatenation → LLM backbone → Positional encoding → Autoregressive generation → Text prediction
- Critical path: 1) Raw inputs → frozen encoders → discrete token sequences per modality 2) Token sequences concatenated in specified order with positional encodings 3) Combined sequence processed by LLM backbone autoregressively 4) Text tokens predicted; loss computed with modality weights (optimized: λS=0.3, λT=0.5, λI=0.5)
- Design tradeoffs:
  - Transformer vs. Mamba backbone: Transformers more stable and often more accurate; Mamba ~2× faster inference, less sensitive to sequence length, but harder to tune hyperparameters
  - More modalities vs. sequence length: Adding modalities provides complementary information but increases sequence noise (irrelevant token ratio), which can hurt at moderate noise levels
  - Interleaved vs. blocked input: Blocked (all visual tokens, then all audio) outperforms interleaved for synchronized dense modalities; interleaving provides in-context learning benefits for vision-language tasks but not ASR
- Failure signatures:
  - WER degradation when adding modalities: Likely sequence noise (irrelevant tokens overwhelming useful signal) or suboptimal loss weights
  - Mamba training instability: Hyperparameter sensitivity significantly higher than Transformers; requires per-modality-combination tuning
  - Visual modality showing near-zero Perceptual Score: Regularization benefit only, not complementary information—the encoder may not extract task-relevant features
- First 3 experiments:
  1. Baseline comparison: Train audio-only (A→T) model on target dataset; measure WER across SNR levels to establish noise-sensitivity curve
  2. Single modality addition: Add one secondary modality (e.g., I+A→T or L+A→T) and compute RB across SNR levels; identify whether synchronized or unsynchronized pattern emerges
  3. Loss weight sweep: With a 2-modality model, grid search modality loss weights (λS, λT, λI) on validation set; verify that non-zero weights on non-text modalities improve convergence even for text-only generation

## Open Questions the Paper Calls Out

### Open Question 1: Positional Encoding for 2-D Representations
- Question: Can novel positional encoding schemes be developed to effectively collate information across modalities and process 2-D visual representations (e.g., rasters) in MLLMs?
- Basis in paper: [explicit] The authors conclude that "MLLMs with absolute positional encoders struggle to handle 2-D representations" and explicitly suggest developing "more powerful positional encodings" as future work.
- Why unresolved: The experiments revealed that "perfect" 2-D raster representations performed worse than expected, likely because standard absolute encodings fail to capture necessary spatial structures.
- What evidence would resolve it: A new positional encoding architecture that enables MLLMs to achieve lower WER on raster inputs compared to current baselines.

### Open Question 2: Stabilizing SSM Architectures
- Question: Can state space models (SSMs) like Mamba be stabilized to achieve the "best of both worlds" of fast inference and high accuracy in multimodal ASR?
- Basis in paper: [explicit] The conclusion explicitly proposes exploring "more recent state space models that might obtain the 'best of both worlds' of fast inference and high accuracy."
- Why unresolved: The study found that while Mamba was faster, it was significantly less stable and more sensitive to hyperparameters than Transformers.
- What evidence would resolve it: An SSM-based MLLM that matches Transformer robustness (hyperparameter insensitivity) while retaining linear inference speed.

### Open Question 3: Scaling Effects on Multimodality Benefits
- Question: Do the noise-dependent benefits of synchronized vs. unsynchronized modalities persist when scaling model capacity beyond 130M parameters?
- Basis in paper: [inferred] The experiments were restricted to relatively small models (OPT-125M and Mamba-130M), leaving the interaction between model scale and modality utility unexplored.
- Why unresolved: Larger models may possess greater capacity to filter irrelevant "sequence noise" or extract relevant features from unsynchronized modalities at high noise levels, potentially shifting the observed "sweet spots."
- What evidence would resolve it: Replicating the modality/noise analysis on significantly larger LLM backbones (e.g., 7B+ parameters) to observe if the relative benefit curves shift.

## Limitations

- Dataset Generalization: Primary conclusions drawn from synthetic 3-Equations dataset with fixed speakers and vocabulary may not generalize to natural speech with variable speaker characteristics and accent diversity.
- Visual Encoder Evaluation: Performance claims about DALL-E vs ViT rest on single encoder comparisons without exploring other architectures or establishing causal mechanisms.
- Input Order Effects: Audio-first advantage demonstrated under controlled noise conditions (second-half corruption) may not hold in real-world environments with distributed noise.

## Confidence

**High Confidence**: The observation that adding more modalities generally improves ASR accuracy, and that this benefit varies with noise level. These are empirical findings directly supported by experimental results across multiple datasets.

**Medium Confidence**: The mechanism explaining synchronized vs. unsynchronized modality behavior. The experimental evidence supports the SNR-dependent benefit patterns, but the causal explanation about temporal alignment and correspondence establishment is inferred rather than directly tested.

**Medium Confidence**: The visual encoder quality impact. The performance differences between DALL-E and ViT are measurable, but the explanation about semantic alignment is theoretical and could be influenced by other encoder differences.

**Low Confidence**: The input order primacy effect explanation. While the experimental results show ordering matters, the mechanism assumes specific noise characteristics and primacy bias without directly validating these assumptions.

## Next Checks

1. **Cross-Dataset Generalization Test**: Replicate the synchronized/unsynchronized modality SNR benefit curves on a natural speech dataset with variable noise characteristics (e.g., Librispeech with MUSAN noise added at random positions within utterances rather than to fixed halves). This would test whether the mechanism holds when the primacy effect is disrupted.

2. **Encoder Ablation Study**: Replace DALL-E with CLIP and BEiT encoders while holding all other variables constant. Compare semantic alignment scores (e.g., using image-text retrieval metrics) against ASR performance to establish whether semantic alignment correlates with multimodal benefit across multiple visual architectures.

3. **Positional Encoding Manipulation**: Train models with relative positional encodings instead of absolute encodings to test whether the audio-first advantage persists. Additionally, train models where noise is added to random utterance positions rather than fixed halves to determine if the primacy effect is essential to the ordering benefit.