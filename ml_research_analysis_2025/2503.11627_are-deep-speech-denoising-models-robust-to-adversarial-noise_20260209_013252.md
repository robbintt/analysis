---
ver: rpa2
title: Are Deep Speech Denoising Models Robust to Adversarial Noise?
arxiv_id: '2503.11627'
source_url: https://arxiv.org/abs/2503.11627
tags:
- speech
- attacks
- adversarial
- audio
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors demonstrate that four recent deep speech denoising
  models are vulnerable to imperceptible adversarial attacks that can reduce their
  outputs to unintelligible gibberish. Using psychoacoustic masking constraints, they
  successfully attack models in various settings including with and without background
  noise, reverb, and simulated over-the-air conditions.
---

# Are Deep Speech Denoising Models Robust to Adversarial Noise?

## Quick Facts
- arXiv ID: 2503.11627
- Source URL: https://arxiv.org/abs/2503.11627
- Reference count: 40
- Primary result: Four deep speech denoising models are vulnerable to imperceptible adversarial attacks that can reduce their outputs to unintelligible gibberish

## Executive Summary
This paper demonstrates that recent deep speech denoising models (Demucs, Full-SubNet+, FRCRN, MP-SENet) are vulnerable to imperceptible adversarial attacks. Using psychoacoustic masking constraints, the authors successfully attack models in various conditions including background noise, reverb, and simulated over-the-air scenarios. The attacks are most effective with model-specific (white-box) access and show limited transferability between models. While universal perturbations are not feasible, targeted attacks appear promising. Simple defenses like Gaussian noise offer only limited protection, suggesting speech denoising models are more vulnerable to adversarial attacks than comparable image denoising models.

## Method Summary
The authors generate adversarial perturbations using Projected Gradient Descent (PGD) with psychoacoustic masking constraints to ensure imperceptibility. They compute time-frequency masking thresholds from input audio's power spectral density, then constrain perturbation energy to remain below these thresholds. The attack uses STOI (Short-Term Objective Intelligibility) as a differentiable loss function, iteratively updating perturbations to degrade speech intelligibility. They evaluate across multiple conditions: with/without background noise at various SNRs, with/without reverb, and with simulated over-the-air conditions using room impulse responses. The attack is tested on four DNS Challenge 4 models using 20 seeds per setting, with perturbations parameterized in time domain (STFT domain for over-the-air attacks).

## Key Results
- White-box adversarial attacks successfully reduce STOI scores below -1.0 across all four models
- Attacks show no significant transferability between different architectures or even different checkpoints of the same model
- Psychoacoustic constraints (12dB threshold reduction) ensure perturbations remain imperceptible
- Gaussian noise defense only partially mitigates attacks, requiring high SNR (50-60dB) for effectiveness
- Targeted attacks using STOI fail to produce recognizable target speech despite high objective scores

## Why This Works (Mechanism)

### Mechanism 1: Psychoacoustic Masking for Imperceptibility
Adversarial perturbations can be hidden within audio using psychoacoustic models that exploit human auditory masking effects across time and frequency. The attack computes time-frequency masking thresholds from input PSD, then constrains perturbation energy to remain below these thresholds using a projection operator that scales magnitude while preserving phase. The MP3 psychoacoustic model with temporal pre-masking (20ms decay at 0.16 ms⁻¹) and post-masking (100ms decay at 0.02 ms⁻¹) predicts human perceptibility thresholds.

### Mechanism 2: STOI-Based Gradient Optimization via PGD
STOI serves as an effective differentiable loss function for degrading speech enhancement, though it breaks down for maximization objectives. PGD iteratively updates perturbation δ to maximize STOI(clean, output) - STOI(clean, input) for untargeted attacks, with Adam optimizer and gradient clipping (norm ≤ 10). While STOI correlates with human-perceived intelligibility when minimized, this correlation does not hold bidirectionally for targeted attacks.

### Mechanism 3: Model-Specific Vulnerability via Gradient Access
White-box attacks succeed because DNS models lack inherent robustness, but vulnerabilities do not transfer across architectures due to model-specific gradient landscapes. Attack success depends on computing gradients through the specific model; different architectures produce incompatible gradient directions. FSN+ shows apparent resilience due to exploding gradients (norm ~10³⁰) causing numerical instability.

## Foundational Learning

### Concept: Psychoacoustic Masking (Temporal and Frequency)
Why needed: Designing imperceptible attacks requires understanding how louder frequency components at time τ mask quieter components at adjacent frequencies and times (τ±Δt).
Quick check: If a 1kHz tone at 60dB plays at t=100ms, what is the masking threshold for a 990Hz tone at t=50ms vs t=150ms?

### Concept: Projected Gradient Descent with Non-Convex Constraints
Why needed: Standard gradient descent cannot enforce PSD(δ)τ,ω ≤ θτ,ω; the projection operator is required after each update.
Quick check: Why does the projection operator clip magnitude while preserving phase in the STFT domain?

### Concept: Intrusive vs. Non-Intrusive Speech Quality Metrics
Why needed: STOI and ViSQOL require ground-truth reference; DNSMOS and NISQA do not—this affects attack objective design and evaluation.
Quick check: Why can't DNSMOS be used as an attack objective even though it correlates reasonably with output quality?

## Architecture Onboarding

### Component Map:
| Component | Input → Output | Key Parameters |
|-----------|---------------|----------------|
| Masking Threshold Calculator | Audio x → Thresholds θτ,ω | 12dB reduction, Hann window, 512 FFT |
| PGD Optimizer | x, θ, model f → Perturbation δ | Adam, lr=0.01, gradient clipping @ 10 |
| Projection Operator | δ, θ → Clipped δ | Per-bin PSD enforcement |
| STOI Loss | (output, target) → scalar | Differentiable implementation |
| Over-the-Air Projector | δ, RIR r → Feasible δ | Wiener deconvolution + gradient descent |

### Critical Path:
1. Compute masking thresholds from input (MP3 psychoacoustic model + temporal pre/post-masking)
2. Run PGD optimization (20K-5K iterations depending on model)
3. Project perturbations to satisfy PSD constraints
4. Evaluate using STOI and perceptual metrics

### Design Tradeoffs:
- 12dB vs 6dB threshold reduction: Standard attacks use 12dB; over-the-air attacks loosen to 6dB for convergence at cost of slight audible crackling
- Time-domain vs STFT parameterization: Standard attacks use time-domain δ; over-the-air attacks use STFT for smoother gradient behavior
- STOI vs learned metrics: STOI is differentiable and correlates with intelligibility degradation; learned metrics (DNSMOS, NISQA) are gamed by adversarial noise against the metric network

### Failure Signatures:
- Exploding gradients in FSN+: Gradient norms reaching 10³⁰, causing NaN/Inf
- Over-the-air projection divergence: Wiener deconvolution fails when RIR is poorly conditioned
- Targeted attack metric failure: High STOI between output and target but human listeners hear only "faint robotic hints"

### First 3 Experiments:
1. Reproduce untargeted attack on Demucs: Run 20K PGD iterations on clean speech (70dB SNR, no reverb) and verify STOI enhancement drops below -1.0; check perturbation imperceptibility via listening test
2. Measure attack transfer across checkpoints: Train attack on Demucs "master64" checkpoint and evaluate on two other Demucs checkpoints to confirm intra-architecture non-transfer
3. Test Gaussian noise defense: Apply white noise at SNRs 15-60dB to attacked inputs and measure STOI recovery; identify SNR threshold where defense becomes effective

## Open Questions the Paper Calls Out

### Open Question 1
Can truly imperceptible targeted attacks induce arbitrary output speech from DNS models? The authors note STOI "broke down when used as a metric for maximization (high STOI does not imply high intelligibility)" and that "future attackers may find ways to generate targeted attacks using more sophisticated objectives." This remains unresolved because current intelligibility metrics work poorly as maximization objectives; targeted attack outputs still sound like "faint robotic hints" rather than recognizable target speech.

### Open Question 2
Can pure black-box attacks succeed against DNS models without gradient access? "Further research is urgently required, particularly on pure black-box attacks" since attacks "appear to be entirely model-specific and do not transfer to other models." This remains unresolved because white-box attacks showed no transferability between architectures or even between different checkpoints of the same model.

### Open Question 3
Are discrete token-based DNS models vulnerable to adversarial attacks? "We only attack fully differentiable DNS models; discrete token-based DNS models will require different techniques to attack." This remains unresolved because token-based models have non-differentiable components that break standard gradient-based attack methods like PGD.

### Open Question 4
Do the generated adversarial perturbations pass formal human perceptibility tests? "Although we provide samples and use well-established methods for generating imperceptible noise, we do not run an ABX user study to prove the imperceptibility of the generated perturbations." This remains unresolved because prior work found psychoacoustic masking still produces perceptible perturbations in many cases; the authors tightened constraints by 12 dB but did not verify imperceptibility with human subjects.

## Limitations

- Attack success depends entirely on white-box gradient access with no transferability between models
- Targeted attacks fail to produce recognizable target speech despite high objective scores
- Over-the-air attacks require manual tuning of threshold reduction for convergence

## Confidence

- **High confidence**: Untargeted attack effectiveness and model-specific vulnerability (consistent STOI degradation across conditions)
- **Medium confidence**: Psychoacoustic imperceptibility (masking thresholds theoretically sound but exact implementation unclear)
- **Low confidence**: Targeted attack mechanism and over-the-air projection (targeted attacks fail human perception test, over-the-air requires manual tuning)

## Next Checks

1. Verify psychoacoustic masking implementation by comparing computed thresholds against reference MP3 psychoacoustic model implementations
2. Test targeted attack with alternative objectives (learned metrics or perceptual losses) to determine if STOI's limitations are fundamental
3. Evaluate defense robustness by systematically testing Gaussian noise, frequency masking, and adaptive attacks across all four models to identify most effective countermeasures