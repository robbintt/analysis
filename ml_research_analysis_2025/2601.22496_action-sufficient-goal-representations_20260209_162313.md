---
ver: rpa2
title: Action-Sufficient Goal Representations
arxiv_id: '2601.22496'
source_url: https://arxiv.org/abs/2601.22496
tags:
- value
- goal
- learning
- representation
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of action sufficiency to analyze
  goal representations in hierarchical offline goal-conditioned reinforcement learning.
  The authors argue that value-sufficient representations, commonly used in existing
  methods, do not guarantee action sufficiency, which is necessary for optimal control.
---

# Action-Sufficient Goal Representations

## Quick Facts
- arXiv ID: 2601.22496
- Source URL: https://arxiv.org/abs/2601.22496
- Reference count: 40
- Primary result: Actor-derived goal representations outperform value-based ones in hierarchical offline RL by provably preserving action-relevant information

## Executive Summary
This paper introduces the concept of action sufficiency to analyze goal representations in hierarchical offline goal-conditioned reinforcement learning. The authors argue that value-sufficient representations, commonly used in existing methods, do not guarantee action sufficiency, which is necessary for optimal control. They formalize this through information-theoretic analysis and show that representations learned solely from value objectives can collapse states that need to be differentiated for action learning. The proposed method learns actor-based goal representations by minimizing log-loss during low-level policy training, which provably induces action-sufficient representations. Experiments on both a discrete cube environment and the OGBench benchmark demonstrate that actor-derived representations consistently outperform value-based ones, particularly in complex manipulation tasks.

## Method Summary
The authors propose learning goal representations directly from the actor objective rather than the value function in hierarchical offline goal-conditioned RL. They modify the standard HIQL framework by making the goal encoder learnable and training it jointly with the low-level policy using the negative log-likelihood loss. This approach ensures the representation preserves information needed for action selection (action sufficiency) rather than just value prediction (value sufficiency). The method is evaluated on OGBench cube tasks with both oracle and predicted subgoals, showing consistent improvements over value-based representations.

## Key Results
- Actor representations (ϕA) consistently outperform value representations (ϕV) across all OGBench cube tasks
- Performance gap widens in higher-difficulty scenarios (quadruple cube vs double/triple)
- Oracle subgoal evaluation confirms low-level controllability improvements are due to better representations
- Actor representations maintain action sufficiency while value representations collapse directionally-informative features

## Why This Works (Mechanism)

### Mechanism 1: Value Collapse Hides Action-Relevant Information
Representations optimized solely for value prediction can discard information critical for action selection, even if value estimation is perfect. In hierarchical RL, value functions map state-goal pairs to a scalar distance/return. If two distinct goals are equidistant from a state (e.g., "left 1 step" vs "right 1 step"), they produce identical values. A value-based encoder is incentivized to map these distinct goals to the same latent code to minimize value loss. However, the optimal actions are opposite. When the representation collapses these goals, the policy receives identical inputs for conflicting targets, making optimal control impossible.

### Mechanism 2: Log-Loss Gradients Enforce Action Sufficiency
Minimizing the standard log-loss of the low-level policy induces an approximately action-sufficient representation. Theorem 7.2 proves a direct bound: the "action sufficiency gap" is upper-bounded by the excess actor loss. When training the encoder jointly with the policy using NLL, gradients must flow through the encoder to lower the negative log-likelihood of the correct action. To minimize this loss, the encoder must retain features that distinguish goals requiring different actions; otherwise, the policy head cannot resolve the correct action distribution.

### Mechanism 3: Decoupling Training and Deployment Access
The architecture allows the low-level policy to learn from "oracle" goals during training to build a sufficient representation space, which is then used for compressed subgoals at deployment. During training, the system has access to the true future state (subgoal). It trains the encoder and policy using this dense signal. At deployment, the high-level planner predicts a latent subgoal, and the low-level policy conditions on this compressed code. Because the encoder was trained to be action-sufficient, the latent contains all information needed for control, even if it is a compressed code predicted by a high-level planner.

## Foundational Learning

- **Conditional Mutual Information (CMI)**
  - Why needed: This is the mathematical language used to define the "gap" between value and action. Specifically, I(A;G|S,Z) measures how much information about the Goal is lost by the representation Z that is necessary to predict the Action.
  - Quick check: If I(A;G|S,Z) > 0, does the policy have enough information to act optimally? (Answer: No)

- **Hierarchical Goal-Conditioned RL (HGCRL)**
  - Why needed: The method relies on a specific structural assumption: a High-Level policy sets subgoals (temporal abstraction) and a Low-Level policy executes primitive actions. Understanding this interface is critical to seeing why the "representation" acts as a bottleneck.
  - Quick check: In HIQL, does the low-level policy condition on the final goal directly or the subgoal representation?

- **Offline RL / Distributional Shift**
  - Why needed: The paper operates in the offline setting (learning from a fixed dataset). The guarantees on "Action Sufficiency" rely on the data distribution. If the dataset doesn't cover the necessary trajectories, the learned representation might be insufficient for parts of the state space not present in the dataset.
  - Quick check: Why might a representation that is "action-sufficient" on the training dataset fail at test time?

## Architecture Onboarding

- **Component map:** (S, G) -> Encoder φ -> Z -> Low-level Policy π_ℓ -> A
- **Critical path:**
  1. Sample (S, G, A) from the replay buffer/dataset
  2. Pass (S, G) through the encoder to get Z = φ(S, G)
  3. Compute the policy log-loss: -log π_ℓ(A | S, Z)
  4. Backpropagate gradients to both π_ℓ and φ
  5. Freeze φ after training if using a separate High-Level planner, or train π_h to predict in the space of Z
- **Design tradeoffs:**
  - Actor-Rep (φA) vs. Value-Rep (φV): φA optimizes for control (Action Sufficiency) but might be harder for the high-level planner to predict. φV is smooth and easy for planning but loses directional info.
  - Latent Dimensionality: Too small forces information collapse. Too large retains "nuisance" info, making high-level planning difficult.
- **Failure signatures:**
  - High Value Accuracy, Low Success Rate: Policy understands "how far" the goal is but takes the wrong action to get there.
  - Policy Oscillation: Agent moves toward a subgoal but cannot pinpoint the final location because the representation compressed spatial resolution.
- **First 3 experiments:**
  1. Implement the 1D Line Validation example. Train φV and φA on a simple integer line task. Verify that φV collapses direction while φA retains sign.
  2. Compute I(A;G|S,Z) for a random encoder vs. an actor-trained encoder in a gridworld. Verify that the actor loss correlates with a drop in this CMI metric.
  3. Run HIQL on a standard offline GCRL benchmark. Compare: (a) Standard HIQL (frozen value-based encoder) vs. (b) Modified HIQL (unfrozen encoder trained with actor loss). Check for the specific performance gap mentioned in the paper.

## Open Questions the Paper Calls Out

- **Question 1:** How can a unified objective balance the trade-off between preserving action-relevant information for low-level control and maintaining the compressibility required for efficient high-level subgoal planning?
  - Basis: The Conclusion states future work should explore "balancing action-relevant information with the compressibility required for efficient high-level planning," noting that action sufficiency is necessary but not sufficient for the entire hierarchy.
  - Why unresolved: The paper establishes actor-based representations improve low-level control but doesn't ensure these representations remain easily predictable for the high-level policy.
  - What evidence would resolve it: A modified loss function that penalizes representation complexity while retaining high success rates.

- **Question 2:** To what extent does limited dataset coverage in offline RL widen the gap between the theoretical guarantee of approximate action sufficiency and practical control performance?
  - Basis: Section 9.1 notes that the theoretical analysis assumes access to the true distribution, whereas offline learning relies on static datasets with potentially limited coverage.
  - Why unresolved: The theoretical bound relies on optimizing the actor NLL to near-optimality, a condition that may be impossible to satisfy if the offline dataset doesn't adequately cover the state-action space.
  - What evidence would resolve it: Experiments varying dataset coverage diversity measuring the correlation between achieved NLL loss and the resulting action sufficiency gap.

- **Question 3:** Does the requirement for action sufficiency hold or require modification in hierarchical settings where the goal space G does not coincide with the state space S?
  - Basis: Section 2 states, "We assume that the goal space coincides with the state space, i.e., G=S."
  - Why unresolved: The theoretical framework and the derived condition are predicated on the assumption G=S. It's unclear if the definitions translate directly to abstract goal spaces where the goal-state relationship is less deterministic.
  - What evidence would resolve it: Theoretical analysis or empirical validation of actor-based representations in environments with abstract goal spaces.

## Limitations

- The theoretical bounds are information-theoretic rather than performance-guaranteed, establishing that actor training bounds the action-sufficiency gap but not directly bounding downstream task performance.
- The framework assumes sufficient dataset coverage; if the offline dataset lacks necessary state-action-goal tuples, the representation may be insufficient in deployment despite low training loss.
- The paper uses Gaussian policies for the low-level actor, which may limit performance on tasks requiring multimodal action distributions.

## Confidence

- **High Confidence:** The theoretical framework (Proposition 5.2 showing value sufficiency ≠ action sufficiency) and the 1D integer line counterexample are mathematically sound and clearly demonstrated.
- **Medium Confidence:** The empirical results showing actor representations outperform value representations are compelling, but the effect size varies across tasks, and ablation studies don't fully isolate the representation contribution.
- **Low Confidence:** The claims about hierarchical planning in actor latent space are supported by success rate comparisons but lack direct analysis of whether high-level planners actually benefit from the learned representation geometry.

## Next Checks

1. **Controlled distribution shift test:** Train ϕA and ϕV on a subset of goals, then evaluate both on held-out goals to measure generalization of action sufficiency beyond training distribution.
2. **Representation geometry analysis:** Visualize and quantify the latent space structure for both ϕA and ϕV to empirically verify that ϕA preserves action-relevant distinctions that ϕV collapses.
3. **Ablation of joint training:** Implement a variant where ϕA is trained with NLL loss but the policy is fixed, to isolate whether representation learning or policy adaptation drives the performance gains.