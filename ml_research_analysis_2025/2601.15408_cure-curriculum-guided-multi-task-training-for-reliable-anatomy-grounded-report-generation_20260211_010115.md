---
ver: rpa2
title: 'CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded
  Report Generation'
arxiv_id: '2601.15408'
source_url: https://arxiv.org/abs/2601.15408
tags:
- report
- cure
- maira-2
- training
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CURE improves medical vision-language model reliability by dynamically
  prioritizing underperforming samples during multi-task training, without additional
  data. It reformulates tasks into fine-grained instructional formats and uses error-aware
  curriculum learning to balance grounding accuracy and report quality.
---

# CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation

## Quick Facts
- **arXiv ID:** 2601.15408
- **Source URL:** https://arxiv.org/abs/2601.15408
- **Reference count:** 40
- **Primary result:** Improves anatomy-grounded localization by +0.37 IoU and reduces hallucinations by 18.6% using error-aware curriculum learning

## Executive Summary
CURE introduces a curriculum-guided multi-task training framework for medical vision-language models that dynamically prioritizes underperforming tasks during training. By reformulating report generation into fine-grained anatomy-grounded instructions and using error-driven sampling, CURE achieves superior grounding accuracy without additional data. The framework demonstrates significant improvements in both visual localization and report quality across multiple chest X-ray datasets while reducing hallucinations by 18.6% across six anatomical regions.

## Method Summary
CURE uses MedGemma-4B-IT as the base model with LoRA adapters (rank 16) for multi-task training on three tasks: Phrase Grounding, Grounded Report Generation, and Anatomy-Grounded Report Generation. The key innovation is an error-aware curriculum that evaluates validation subsets every 3,000 steps and re-weights sampling probabilities based on a combined error score (0.8×IoU + 0.2×CXRFEScore). Training proceeds in two phases: 3,000-step pre-training on AGRG data only, followed by 6,000-step multi-task fine-tuning with curriculum sampling. The framework uses aggressive data augmentation (CLAHE, affine transforms) with bounding-box-aware text supervision and a high learning rate (2×10⁻⁴) to enable grounding transfer.

## Key Results
- Achieves +0.37 IoU improvement in anatomy-grounded localization compared to baselines
- Boosts report quality by +0.188 CXRFEScore while maintaining clinical text accuracy
- Reduces hallucinations by 18.6% across six anatomical regions
- Demonstrates effective zero-shot generalization to unseen dataset (VinDr-CXR)

## Why This Works (Mechanism)

### Mechanism 1: Error-Driven Sampling Re-balancing
The framework dynamically re-weights sampling probabilities based on validation errors, preventing performance collapse in imbalanced multi-task training. Every 3,000 steps, it evaluates small validation subsets, computes an error score combining IoU and CXRFEScore, and updates sampling probabilities proportionally. This forces the optimizer to revisit underperforming data sources, maintaining multi-task competence.

### Mechanism 2: Decomposed Anatomy-Grounded Instruction Tuning
By decomposing report generation into fine-grained "Locate," "Describe," and "Locate and describe" instructions, CURE reduces hallucination by explicitly grounding normal anatomy. This approach breaks the bias of standard phrase grounding toward abnormalities, forcing the model to process specific anatomical regions regardless of pathology and learn to output "No acute findings" for normal regions.

### Mechanism 3: Inter-Dataset Curriculum for Task Stabilization
The curriculum addresses task collapse by preventing the largest dataset from dominating training. Without curriculum learning, natural sampling (proportional to dataset size) causes the model to overfit to the dominant dataset and forget minority tasks. The error-aware curriculum up-weights minority datasets when their validation performance drops, maintaining multi-task competence.

## Foundational Learning

- **Curriculum Learning (Self-Paced Learning)**: Understanding error-aware dynamic scheduling vs. standard easy-first curricula. Quick check: How does error-aware curriculum differ from standard easy-first curriculum learning?
- **Visual Grounding (Phrase Grounding)**: Distinguishing between mapping phrases to boxes vs. generating text with boxes. Quick check: Why does standard phrase grounding lead to higher hallucination rates?
- **Multi-Task Learning (MTL) & Negative Transfer**: Understanding why naive merging fails and motivates architectural design. Quick check: Why did natural sampling strategy cause GRG task to collapse completely?

## Architecture Onboarding

- **Component map:** Base Model (MedGemma-4B-IT) -> Adapter (LoRA Rank 16) -> Curriculum Manager (Evaluates validation -> Computes error_i -> Updates weights) -> Data Loader (Custom sampler with weights)
- **Critical path:** Initialization (uniform sampling) -> Evaluation (run inference on validation subsets) -> Re-weighting (calculate error e_i -> update sampler probabilities) -> Training (continue with new sampling distribution)
- **Design tradeoffs:** IoU vs. Semantic Quality (α=0.8 prioritizes grounding), Update Frequency (3k steps optimal), Learning Rate (2e-4 required for grounding transfer)
- **Failure signatures:** Task Collapse (GRG IoU drops to 0.0), Hallucination Spikes (high abnormality hallucination rates)
- **First 3 experiments:** 1) Sanity Check (uniform sampling baseline), 2) Curriculum Ablation (v11 vs v15 on GRG task), 3) Hallucination Probe (Left Clavicle subset)

## Open Questions the Paper Calls Out

1. **Rare Clinical Findings Distribution:** How can the intra-dataset curriculum be extended to explicitly re-balance rare clinical findings alongside anatomical diversity? The current design balances anatomical locations but doesn't account for abnormal finding distribution, identified as future work.

2. **Clinical Text Metrics Regression:** Can training be modified to prevent regression in clinical text metrics (F1-Score) when optimizing for high visual grounding accuracy? Aggressive learning rates boost grounding but often cause text metric regression.

3. **Full-Scale Training Stability:** Does performance stability hold when scaling from 1.74% subset to full 12.9M training instances? Curriculum dynamics may change drastically with dataset scale, requiring validation.

## Limitations

- Validation error on small subsets (200-300 samples) may be unreliable for true task imbalance assessment
- Framework requires separate annotated datasets for each task, limiting zero-shot applicability
- Potential overfitting to "hard" samples when curriculum is applied to very large datasets

## Confidence

- **High Confidence:** Multi-task collapse under natural sampling (direct ablation study evidence)
- **Medium Confidence:** Error-aware curriculum prevents task collapse (controlled experiments but relies on validation subset representativeness)
- **Medium Confidence:** Hallucination reduction through instruction decomposition (strong quantitative reduction but causal link needs deeper analysis)
- **Low Confidence:** Generalization to unseen datasets (promising zero-shot results but limited cross-domain validation)

## Next Checks

1. **Robustness Test:** Replicate curriculum ablation study with 5 different random seeds to confirm consistent task collapse prevention
2. **Hallucination Analysis:** Perform per-anatomy breakdown of hallucination rates on held-out test set to verify 18.6% reduction across all six regions
3. **Curriculum Sensitivity:** Test performance across different validation subset sizes (100, 200, 400) and update frequencies (1.5k, 3k, 6k steps)