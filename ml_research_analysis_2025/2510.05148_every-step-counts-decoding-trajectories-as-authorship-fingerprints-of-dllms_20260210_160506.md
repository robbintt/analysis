---
ver: rpa2
title: 'Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs'
arxiv_id: '2510.05148'
source_url: https://arxiv.org/abs/2510.05148
tags:
- attribution
- decoding
- different
- arxiv
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first exploration of model attribution
  for discrete diffusion large language models (dLLMs), leveraging their unique decoding
  trajectories. The key insight is that dLLM decoding behavior embeds model-specific
  structural signals that can be used for attribution.
---

# Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs

## Quick Facts
- **arXiv ID:** 2510.05148
- **Source URL:** https://arxiv.org/abs/2510.05148
- **Reference count:** 40
- **Primary result:** DDM+GTA fingerprinting achieves up to 30% AUC improvement in model attribution across three challenging scenarios

## Executive Summary
This paper introduces the first model attribution framework for discrete diffusion large language models (dLLMs) by leveraging their unique decoding trajectories. The core insight is that dLLM decoding behavior embeds model-specific structural signals that can be used as attribution fingerprints. The authors propose the Directed Decoding Map (DDM) to encode token influence patterns during decoding, and Gaussian-Trajectory Attribution (GTA) to generate compact probabilistic fingerprints. Extensive experiments across Cross-Model, Independent-Run, and Cross-Checkpoint scenarios demonstrate significant improvements over baselines, with robust performance even in highly challenging attribution tasks.

## Method Summary
The paper introduces a two-component attribution system for dLLMs. First, the Directed Decoding Map (DDM) captures how newly decoded tokens influence the confidence of previously decoded tokens, creating a fine-grained representation of decoding dependencies. Second, Gaussian-Trajectory Attribution (GTA) fits cell-wise Gaussian distributions to these DDMs to generate compact probabilistic fingerprints for each model. Attribution is performed by computing which model's distribution best matches the target trajectory's likelihood. The approach is tested across three attribution scenarios: distinguishing between different models, identifying runs from the same model, and differentiating models from the same checkpoint.

## Key Results
- DDM+GTA combination achieves up to 30% AUC improvements over baselines
- Maintains 81% AUC even when distinguishing models fine-tuned from identical checkpoints
- Demonstrates robust performance across Cross-Model, Independent-Run, and Cross-Checkpoint scenarios
- Outperforms all baseline attribution methods in every tested scenario

## Why This Works (Mechanism)
The mechanism works because discrete diffusion decoding creates unique trajectory patterns that are intrinsically tied to a model's learned probability distributions and structural properties. As tokens are decoded sequentially, each model's internal uncertainty and token selection patterns create distinctive influence maps between tokens. The DDM captures these fine-grained dependencies, while GTA compresses them into stable probabilistic fingerprints that remain consistent across different generations from the same model.

## Foundational Learning
- **Discrete diffusion decoding:** Why needed - Understanding the token-by-token generation process; Quick check - Can identify the order and selection criteria for token generation
- **Directed Decoding Map (DDM):** Why needed - Captures token influence patterns during decoding; Quick check - Can explain how new tokens affect confidence of previous tokens
- **Gaussian-Trajectory Attribution (GTA):** Why needed - Creates compact probabilistic fingerprints from decoding patterns; Quick check - Can describe how Gaussian distributions are fitted to DDMs
- **Model attribution scenarios:** Why needed - Tests method robustness across different attribution challenges; Quick check - Can distinguish between Cross-Model, Independent-Run, and Cross-Checkpoint settings
- **Decoding trajectory fingerprinting:** Why needed - Leverages model-specific structural signals for attribution; Quick check - Can explain how decoding behavior serves as authorship fingerprints

## Architecture Onboarding

**Component Map:** Input Text -> DDM Generator -> Gaussian Fitting -> Model Likelihood Computation -> Attribution Decision

**Critical Path:** Text input → DDM generation (capturing token influence) → Gaussian distribution fitting (fingerprint creation) → Likelihood comparison across models → Attribution decision

**Design Tradeoffs:** The DDM captures fine-grained decoding dependencies but increases computational complexity; GTA provides compact fingerprints but may lose some trajectory detail; the approach trades some attribution granularity for robustness and efficiency.

**Failure Signatures:** Attribution failures occur when models share similar training processes, when decoding trajectories are highly variable, or when computational constraints limit DDM resolution. The 81% AUC in identical checkpoint scenarios indicates systematic limitations in extreme attribution challenges.

**Three First Experiments:**
1. Generate attribution results on a held-out dataset with known model origins
2. Compare DDM+GTA performance against baseline methods on identical checkpoint attribution
3. Test attribution accuracy when varying the length of decoding trajectories

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Generalization beyond tested dLLM architectures remains uncertain
- Computational overhead for DDM generation and processing not thoroughly addressed
- No evaluation of robustness against adversarial obfuscation or manipulation techniques
- 81% AUC in identical checkpoint scenarios indicates remaining attribution uncertainty

## Confidence
- **High confidence:** The core insight that dLLM decoding trajectories contain model-specific structural signals is well-supported by empirical results
- **Medium confidence:** The DDM+GTA combination's superiority over baselines is demonstrated, but generalization to unseen model architectures remains untested
- **Low confidence:** The robustness of the approach against deliberate obfuscation or adversarial manipulation has not been evaluated

## Next Checks
1. Test DDM+GTA attribution accuracy across a broader range of dLLM architectures, including those with varying attention mechanisms and tokenization strategies
2. Evaluate the method's performance when models share similar training datasets or are fine-tuned with minimal hyperparameter differences
3. Assess computational efficiency and scalability for real-time attribution tasks, including memory usage and processing time for large-scale deployments