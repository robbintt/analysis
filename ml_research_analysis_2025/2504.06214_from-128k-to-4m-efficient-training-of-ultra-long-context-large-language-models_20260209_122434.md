---
ver: rpa2
title: 'From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models'
arxiv_id: '2504.06214'
source_url: https://arxiv.org/abs/2504.06214
tags:
- context
- arxiv
- tokens
- long-context
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an efficient training recipe for extending
  the context length of large language models (LLMs) from 128K to ultra-long lengths
  of 1M, 2M, and 4M tokens while maintaining strong performance on standard tasks.
  The method uses a two-stage approach: continued pretraining with special document
  separators and YaRN-based RoPE scaling to handle ultra-long sequences, followed
  by instruction tuning on short-context data to preserve instruction-following and
  reasoning capabilities.'
---

# From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models

## Quick Facts
- arXiv ID: 2504.06214
- Source URL: https://arxiv.org/abs/2504.06214
- Reference count: 14
- Primary result: Extends Llama-3.1-8B context from 128K to 1M/2M/4M tokens while maintaining strong performance

## Executive Summary
This paper introduces an efficient training recipe for extending large language model context windows from 128K to ultra-long lengths of 1M, 2M, and 4M tokens while maintaining strong performance on standard tasks. The method uses a two-stage approach: continued pretraining with special document separators and YaRN-based RoPE scaling to handle ultra-long sequences, followed by instruction tuning on short-context data to preserve instruction-following and reasoning capabilities. Evaluated on Llama-3.1-8B-Instruct, the resulting UltraLong-8B models achieve state-of-the-art performance on long-context benchmarks and maintain competitive performance on standard benchmarks.

## Method Summary
The method extends context length through a two-stage approach. Stage 1 uses continued pretraining on 1B tokens with YaRN-based RoPE scaling (α=1, β=4, scale factors s=128/256/512) and special document separators, training with full attention across concatenated documents. Stage 2 applies instruction tuning on 100K short-context examples (<8K tokens) covering general, math, and code domains to preserve reasoning capabilities. Both stages use Adam optimizer with specific learning rates and batch sizes, trained on distributed systems with context and tensor parallelism.

## Key Results
- UltraLong-8B models achieve state-of-the-art performance on long-context benchmarks (RULER, LV-Eval, InfiniteBench)
- Models maintain 100% accuracy on Needle-in-a-Haystack retrieval tests across all tested context lengths
- One-step context extension (1B tokens direct to target) outperforms multi-step training by 2-3 points on RULER
- Standard benchmark performance preserved (62.47 average vs 61.45 for base Llama-3.1-8B-Instruct)

## Why This Works (Mechanism)

### Mechanism 1: YaRN-Based Positional Scaling with Aggressive Scale Factors
YaRN modifies RoPE frequencies with temperature scaling to compress positional information, using aggressive scale factors (s=128/256/512) that prevent representation collapse when input length approaches 128K tokens. This maintains discriminative positional signals across millions of tokens by allowing the attention mechanism to interpolate to unseen positions during fine-tuning.

### Mechanism 2: Cross-Document Attention Without Masking
Removing cross-document attention masks and using special separator tokens (`<s>`) enables more efficient context extension than masked approaches. The model learns to distinguish document boundaries through separator embeddings while maintaining global attention capability, reducing training complexity and allowing discovery of useful cross-document patterns.

### Mechanism 3: Short-Context Instruction Tuning for Capability Preservation
Instruction tuning exclusively on short-context data (<8K tokens) preserves reasoning and instruction-following capabilities lost during continued pretraining, without requiring synthetic long-context instruction data. The short-context constraint is computationally efficient and sufficient because the context extension was already achieved in Stage 1.

## Foundational Learning

- **Rotary Position Embeddings (RoPE):** Understanding how RoPE encodes position through complex-valued rotation is essential for debugging scaling failures and selecting hyperparameters. *Quick check:* Given a RoPE base frequency of 500K and sequence position 100, at what rotation angle (in radians) does the highest-frequency dimension encode this position?

- **Tensor Parallelism and Context Parallelism:** Training 4M token sequences requires distributed attention computation across 256 H100s. *Quick check:* For a 4M token sequence with CP=16, how many tokens does each context parallel rank process locally, and what communication pattern is required during the attention computation?

- **Needle-in-a-Haystack (NIAH) Evaluation:** NIAH is the primary diagnostic for long-context retrieval. *Quick check:* If a model achieves 100% NIAH accuracy at all depths for 1M tokens but fails on LV-Eval multi-hop QA, what capability gap does this reveal?

## Architecture Onboarding

- **Component map:** Llama-3.1-8B-Instruct (128K) → [RoPE Scaling Layer] YaRN with α=1, β=4, s∈{128,256,512} → [Continued Pretraining] 1B tokens, LR=3e-5, CP={4,16}, TP=8 → [Instruction Tuning] 100K examples, LR=5e-6, TP=8 → UltraLong-8B-{1M,2M,4M}-Instruct

- **Critical path:** RoPE scaling factor selection → continued pretraining stability (monitor perplexity at 128K boundary) → SFT data quality/decontamination → NIAH 100% accuracy validation → full benchmark suite

- **Design tradeoffs:** One-step vs multi-step (one-step outperforms by 2-3 points), short vs long SFT data (short preserves capabilities efficiently), YaRN vs NTK-aware scaling (YaRN provides 3-4 point improvement at >512K lengths)

- **Failure signatures:** NIAH failures at specific depths indicate attention sink issues or position bias; standard benchmark degradation >5 points suggests insufficient SFT data quality; perplexity spike near 128K tokens indicates conservative RoPE scaling factor; OOM during 4M training suggests insufficient context parallelism

- **First 3 experiments:**
  1. Validate NIAH at 512K with s=128: Train UltraLong-8B-512K on 500M tokens, verify 100% NIAH accuracy. If failures occur at depths >80%, increase β from 4 to 6 and retrain.
  2. Ablate separator tokens on 256K model: Train two 256K models (with/without `<s>` separators), evaluate on RULER and LV-Eval. A <1 point difference suggests separators are optional; >2 points confirms their necessity.
  3. Stress-test short-context SFT transfer: After Stage 1 pretraining to 1M, evaluate on MMLU/GSM-8K before SFT. If degradation >10 points, increase SFT data from 100K to 200K or add synthetic long-context instructions.

## Open Questions the Paper Calls Out

- **Question:** Can reinforcement learning or preference optimization techniques (e.g., RLHF, DPO) further improve ultra-long context model performance beyond supervised fine-tuning? *Basis:* Section 7 states the current work focuses on SFT and does not explore RLHF/DPO.

- **Question:** How can safety alignment be integrated into ultra-long context models without degrading their extended context capabilities? *Basis:* Section 7 notes the work does not address safety alignment, leaving potential risks unmitigated.

- **Question:** Does this training recipe scale effectively to larger model sizes (70B+ parameters)? *Basis:* All experiments use Llama-3.1-8B-Instruct; no validation on larger variants despite Llama-3.1 family including 70B and 405B models.

- **Question:** How well does benchmark performance predict real-world utility on tasks like multi-document reasoning, codebase analysis, or long-form video understanding? *Basis:* Introduction acknowledges evaluations rely on synthetic benchmarks that don't fully capture real-world long-context tasks.

## Limitations
- Computational requirements: Training on 256 H100s with CP=16/TP=8 may not be accessible to all research groups
- Generalization to other model families: Approach validated only on Llama-3.1-8B-Instruct; specific hyperparameters may need adjustment
- Data distribution effects: Performance impact of specific upsampling/downsampling strategy versus alternative corpus compositions is not fully explored

## Confidence
- **High Confidence:** YaRN-based RoPE scaling enables stable ultra-long context adaptation; one-step context extension outperforms multi-step training; short-context instruction tuning preserves reasoning capabilities; NIAH 100% accuracy validates retrieval capability
- **Medium Confidence:** Cross-document attention without masking is more efficient; special document separators significantly improve performance; approach scales to 4M tokens without catastrophic forgetting
- **Low Confidence:** Performance on real-world long-context tasks beyond synthetic benchmarks; generalizability to model families with different attention mechanisms; impact of different corpus distributions on long-context learning

## Next Checks
1. **Scaling Factor Sensitivity Analysis:** Systematically vary YaRN scale factors (s=64, 128, 256, 512) on 1M token model and measure NIAH accuracy across different depth ranges to determine optimal scaling factors.

2. **Cross-Document Attention Ablation with Realistic Data:** Train models with and without cross-document masking using a corpus containing both related and unrelated documents, comparing performance on multi-hop QA tasks versus NIAH retrieval.

3. **Transfer to Different Base Models:** Apply the exact methodology to Llama-2-7B-Instruct and Qwen-7B-Instruct to test generalizability, comparing scaling factor sensitivity curves across models.