---
ver: rpa2
title: Investigating Task Arithmetic for Zero-Shot Information Retrieval
arxiv_id: '2505.00649'
source_url: https://arxiv.org/abs/2505.00649
tags:
- task
- arithmetic
- retrieval
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Task Arithmetic for zero-shot information
  retrieval, addressing the challenge of domain and language mismatch in large language
  models. The authors propose combining weights of domain-specialized and IR-tuned
  models via simple arithmetic operations, specifically by adding domain-specific
  task vectors to an IR-tuned baseline.
---

# Investigating Task Arithmetic for Zero-Shot Information Retrieval

## Quick Facts
- arXiv ID: 2505.00649
- Source URL: https://arxiv.org/abs/2505.00649
- Reference count: 40
- Primary result: Up to 18% NDCG@10 and 15% P@10 gains in zero-shot domain/language adaptation via parameter arithmetic

## Executive Summary
This paper proposes Task Arithmetic as a lightweight method for zero-shot information retrieval adaptation across domains and languages. By computing the parameter difference between domain-specialized and pre-trained models, then adding this "task vector" to an IR-tuned baseline, the approach transfers specialized knowledge without additional training. The method demonstrates consistent improvements across scientific, biomedical, and multilingual datasets, achieving up to 18% NDCG@10 gains while remaining computationally efficient and architecture-agnostic.

## Method Summary
Task Arithmetic computes a task vector as the element-wise difference between a domain-fine-tuned model and its pre-trained baseline. This vector is added to the weights of an IR-fine-tuned model, with a scaling factor α controlling the magnitude of injection. The approach uses a two-stage retrieval pipeline: BM25 retrieves top-100 documents, which are then re-ranked using the adapted model. Scores from both stages are combined via λ-weighted fusion. The method requires only three model checkpoints and avoids additional fine-tuning, making it suitable for zero-shot scenarios where labeled data or computational resources are limited.

## Key Results
- Task Arithmetic consistently outperforms strong baselines across scientific, biomedical, and multilingual datasets
- Performance gains reach up to 18% NDCG@10 and 15% P@10 compared to IR-tuned models alone
- Optimal scaling factor α varies by model and dataset (0.3-0.7 range), with α=1.0 rarely optimal
- The approach works across encoder-only architectures without architectural modifications

## Why This Works (Mechanism)
The method operates on the principle that domain adaptation knowledge can be encoded as parameter-space differences and linearly added to existing task-specific models. This works because domain and IR capabilities may reside in approximately orthogonal subspaces within the parameter space, allowing additive combination without catastrophic forgetting. The scaling factor α enables controlled interpolation between the original IR task performance and the injected domain knowledge, while the architecture-agnostic nature stems from operating directly on weight matrices rather than requiring model-specific modifications.

## Foundational Learning
- **Parameter Arithmetic**: Adding weight differences between models to transfer knowledge; needed because traditional fine-tuning is expensive for zero-shot scenarios; quick check: verify weight shapes match before subtraction
- **Task Vector Computation**: τ_D = Θ_D - Θ_0 captures domain-specific knowledge; needed to isolate the adaptation component from base model; quick check: ensure pre-trained and domain models share identical architecture
- **Scaling Factor Interpolation**: α controls injection strength; needed to balance domain adaptation with preserving IR capabilities; quick check: sweep α values to find optimal performance
- **Two-Stage Retrieval**: BM25 first stage followed by neural re-ranking; needed to leverage both sparse and dense retrieval strengths; quick check: verify λ-weighted score fusion produces reasonable rankings
- **Architecture-Agnostic Transfer**: Operating on weight matrices bypasses architectural constraints; needed for broad applicability across model types; quick check: confirm parameter names align exactly across models
- **Zero-Shot Adaptation**: No target-task training required; needed for scenarios with limited labeled data; quick check: validate performance without any domain-specific fine-tuning

## Architecture Onboarding

### Component Map
Pre-trained model -> Domain model -> Task Vector computation -> IR model -> Adapted model -> BM25 retriever -> Score fusion -> Evaluation metrics

### Critical Path
The essential sequence is: (1) load Θ_0 and Θ_D, (2) compute τ_D = Θ_D - Θ_0, (3) load Θ_T and add α·τ_D to create Θ', (4) run BM25 retrieval, (5) re-rank with Θ', (6) fuse scores and compute metrics. The Task Vector computation and addition are the core operations that enable the transfer.

### Design Tradeoffs
- **Efficiency vs. Performance**: Task Arithmetic avoids expensive fine-tuning but requires careful α tuning; simpler than adapters but may be less precise
- **Zero-Shot vs. Few-Shot**: The method truly requires no target-task training, but optimal performance often needs dev set tuning
- **Architecture Flexibility vs. Implementation Complexity**: Operating on weights enables broad applicability but requires exact architecture matching and parameter alignment

### Failure Signatures
- **Catastrophic Forgetting**: Model loses IR ranking capability when α is too high
- **No Improvement**: Performance matches or degrades from baseline when domain-task alignment is poor
- **Shape Mismatches**: Errors when parameter dimensions don't align between models
- **Score Fusion Artifacts**: BM25+LLM pipeline improvements may stem from normalization rather than genuine re-ranking gains

### Three First Experiments
1. **Basic Arithmetic Validation**: Compute τ_D for a simple case (e.g., T5-base biomedical) and verify Θ' weights are correctly formed with α=1.0
2. **α-Sensitivity Analysis**: Run BM25+re-ranking with α ∈ {0.1, 0.3, 0.5, 0.7, 1.0} on a single dataset to identify optimal scaling
3. **Architecture Transfer Test**: Apply Task Arithmetic to a different encoder-only model (e.g., RoBERTa-base) and verify cross-architecture functionality

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimal scaling factor $\alpha$ be predicted automatically for fully zero-shot scenarios without a labeled development set? The current reliance on development sets for calibration contradicts the zero-shot constraint when no labeled data exists for a target domain. An algorithm that estimates $\alpha$ based on weight statistics or model uncertainty would resolve this.

### Open Question 2
Does the injection of domain-specific task vectors degrade the model's effectiveness on general-domain or out-of-distribution retrieval tasks? The authors note that "excessive emphasis on domain parameters can overshadow the IR-specific knowledge," implying a potential trade-off. Evaluation on general-purpose datasets like MS-MARCO would provide evidence.

### Open Question 3
Can multiple domain-specific task vectors be combined (e.g., biomedical + multilingual) into a single model without causing interference? While the abstract suggests the method can "synthesize diverse tasks," experiments limit arithmetic to single domain vector addition. Testing $\tau_{D1} + \tau_{D2}$ addition would reveal whether constructive or destructive interference occurs.

## Limitations
- Performance heavily depends on careful α tuning, which requires labeled development data that may not exist in true zero-shot scenarios
- The fixed BM25+LLM pipeline makes it difficult to isolate Task Arithmetic's contribution from retrieval-stage effects
- Only encoder-only models are tested, leaving the architecture-agnostic claim unverified for decoder-only or encoder-decoder models
- No comparison to other parameter-efficient transfer methods limits claims of superiority

## Confidence

### High Confidence
- The arithmetic operation itself is correctly implemented and α-tuning improves over α=1.0 on multiple datasets
- The orthogonality assumption between domain and IR subspaces is supported by the absence of catastrophic forgetting

### Medium Confidence
- Cross-dataset and cross-lingual gains are robust but depend heavily on base model and domain-task alignment
- The "zero-shot" claim holds only under the narrow definition of "no target-task training"

### Low Confidence
- The claim that Task Arithmetic is universally architecture-agnostic lacks empirical support beyond encoder-only models
- The absence of comparison to other adapter-based or parameter-efficient transfer methods limits claims of superiority

## Next Checks
1. **Baselines & Ablations**: Compare Task Arithmetic against adapter fusion, prompt tuning, and low-rank adaptation under identical compute budgets to quantify efficiency gains
2. **End-to-End Retrieval**: Replace BM25+LLM with a learned dense retriever (e.g., Contriever) and evaluate Task Arithmetic's impact on the full pipeline to isolate re-ranking from retrieval-stage effects
3. **Cross-Architecture Generalization**: Test the method on decoder-only (GPT-2) and encoder-decoder (T5) models across diverse tasks (summarization, QA) to confirm the architecture-agnostic claim