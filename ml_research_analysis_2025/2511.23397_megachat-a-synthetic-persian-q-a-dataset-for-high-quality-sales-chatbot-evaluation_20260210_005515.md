---
ver: rpa2
title: 'MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation'
arxiv_id: '2511.23397'
source_url: https://arxiv.org/abs/2511.23397
tags:
- generation
- dataset
- channels
- posts
- megachat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MegaChat introduces the first fully synthetic Persian Q&A dataset
  for evaluating sales chatbots in Telegram-based e-commerce. Using a novel multi-agent
  architecture with specialized generators, validators, and refiners, the system produces
  persona-aware questions grounded in real Telegram shopping channels.
---

# MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation

## Quick Facts
- arXiv ID: 2511.23397
- Source URL: https://arxiv.org/abs/2511.23397
- Reference count: 26
- The first fully synthetic Persian Q&A dataset for sales chatbot evaluation, outperforming RAG in 4/5 Telegram channels

## Executive Summary
MegaChat introduces the first fully synthetic Persian Q&A dataset for evaluating sales chatbots in Telegram-based e-commerce. Using a novel multi-agent architecture with specialized generators, validators, and refiners, the system produces persona-aware questions grounded in real Telegram shopping channels. Answer generation combines classic RAG approaches with an advanced agentic pipeline featuring multi-query retrieval and persona alignment, evaluated by GPT-5.1. The agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating superior scalability and quality without expensive human annotation or fine-tuning.

## Method Summary
MegaChat employs a multi-agent system to generate synthetic Persian Q&A pairs from Telegram shopping channel posts. The process involves three specialized agents for question generation: Generator produces persona-aligned questions with confidence scores, Validator verifies factual grounding, and Refiner enhances conversational naturalness while filtering outputs below 50% confidence. Answer generation uses both classic RAG (FAISS + text-embedding-3-large with top-5 retrieval) and an advanced agentic system that generates 5-8 diverse queries, performs parallel retrieval, reranks using SLM, extracts persona preferences, and synthesizes persona-aligned responses. GPT-5.1 evaluates outputs across six weighted dimensions to establish ground truth labels.

## Key Results
- Agentic answer generation outperformed classic RAG models in 4 out of 5 diverse Telegram shopping channels
- The Two-Pass question generation pipeline (Generator→Validator→Refiner) with 50% confidence filtering ensured high-quality synthetic questions
- Multi-query retrieval with SLM reranking and persona-aligned synthesis demonstrated superior performance for sales chatbot evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent question generation with sequential validation-refinement produces higher-quality synthetic questions than single-pass generation
- Mechanism: A Two-Pass pipeline where (1) Generator Agent produces persona-aligned questions with confidence scores, then (2) Validator Agent verifies factual grounding while Refiner Agent enhances conversational naturalness and filters outputs below 50% confidence
- Core assumption: Decomposing generation, validation, and refinement across specialized agents catches more errors than monolithic generation
- Evidence anchors:
  - [abstract] "The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data"
  - [section IV] "A representative example of the generated question-answer pairs is illustrated in Fig. 11"
  - [corpus] Weak direct evidence—FarsiMCQGen addresses Persian MCQ generation but uses different methodology; no direct comparison to multi-agent pipelines exists
- Break condition: If validation and refinement agents share identical underlying models/prompts as generator, error propagation may negate quality gains

### Mechanism 2
- Claim: Agentic answer generation with multi-query retrieval outperforms classic RAG for persona-aligned sales responses
- Mechanism: Five-stage workflow—(1) SLM generates 5–8 diverse queries covering specs/pricing/delivery, (2) parallel retrieval aggregates candidate pool, (3) SLM reranks to top-5, (4) LLM extracts persona preferences, (5) LLM synthesizes persona-aligned response
- Core assumption: Multi-query expansion captures more relevant context; SLM reranking approximates relevance better than pure embedding similarity
- Evidence anchors:
  - [abstract] "Our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis...outperformed traditional RAG models in 4 out of 5 diverse channels"
  - [section IV] Agentic system generated best-ranked responses in 4/5 channels (8, 22, 18, 15 wins respectively)
  - [corpus] No direct corpus validation for this specific agentic-vs-RAG comparison in Persian sales contexts
- Break condition: If source posts lack sufficient coverage of query dimensions (e.g., delivery info absent), multi-query expansion yields diminishing returns

### Mechanism 3
- Claim: LLM-as-judge evaluation using multi-dimensional criteria produces reliable ground truth labels without human annotation
- Mechanism: GPT-5.1 ranks four candidate answers per question across six weighted dimensions—Factual Correctness (primary), Persona Alignment, Emotional Sensitivity, Tone Preference, Interaction Style, Content Preferences
- Core assumption: GPT-5.1's ranking correlates with human preference; dimension weighting reflects actual user priorities
- Evidence anchors:
  - [section III.C.3] "The evaluator considers six dimensions when ranking answers, with the following order of importance: Factual Correctness..."
  - [section IV] "Despite some individual components underperforming, the GPT-5.1 evaluation layer ensured the selection of optimal answers, maintaining high dataset quality"
  - [corpus] Gilardi et al. [20] cited in paper shows ChatGPT outperforms crowd workers for text annotation—provides indirect support but not specific to Persian sales QA
- Break condition: If GPT-5.1 exhibits systematic bias toward certain response styles (e.g., longer answers), ground truth labels may not reflect actual user preferences

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Classic RAG serves as the baseline architecture; understanding retriever-LLM interaction is essential for comparing against agentic approaches
  - Quick check question: Given a query, can you trace how FAISS retrieves top-k documents and how the LLM synthesizes them into a grounded response?

- Concept: **Multi-Agent System Design**
  - Why needed here: MegaChat's core innovation is decomposing tasks across specialized agents; understanding agent roles, handoffs, and failure modes is critical
  - Quick check question: If the Validator Agent incorrectly rejects 80% of questions, where would you diagnose the issue—prompt design, threshold calibration, or upstream generator quality?

- Concept: **Persona-Driven Generation**
  - Why needed here: Questions and answers are explicitly designed around user personas (demographics, psychographics, purchase behavior); this drives evaluation criteria
  - Quick check question: Given a persona "budget-conscious student looking for laptop," what query dimensions should multi-query expansion prioritize?

## Architecture Onboarding

- Component map:
  ```
  Telegram Channels (48) → Data Collection → Post Filtering
                                              ↓
                              Question Generation Pipeline:
                              Generator → Validator → Refiner
                                              ↓
                              Answer Generation (parallel):
                              ├─ Classic RAG (GPT-4.1, GPT-4o, GPT-4-turbo)
                              └─ Agentic System (5-stage LLM+SLM)
                                              ↓
                              GPT-5.1 Evaluator → Ground Truth Labels
  ```

- Critical path: Question generation quality → Answer candidate diversity → Evaluation reliability. If questions lack realism, downstream evaluation is compromised regardless of answer quality.

- Design tradeoffs:
  - Fully synthetic vs. human-validated: Scalable but may introduce subtle biases (paper acknowledges this)
  - Multi-query retrieval vs. single-query: Higher coverage but increased latency and API costs
  - SLM for reranking vs. LLM: Faster/cheaper but potentially lower relevance accuracy

- Failure signatures:
  - Low question diversity: Generator Agent may overfit to channel patterns; check persona variation in prompts
  - Hallucinated answers: RAG grounding failure; verify retriever returns relevant posts before synthesis
  - Evaluator bias: Same model architecture consistently wins; check if GPT-5.1 favors response length or style over content

- First 3 experiments:
  1. **Ablate the Two-Pass pipeline**: Generate questions with Generator only vs. full Generator→Validator→Refiner. Compare question quality on 100-sample subset using manual review.
  2. **Vary retrieval depth**: Test agentic system with 3, 5, 8 queries. Measure answer quality vs. latency tradeoff on a single channel.
  3. **Cross-validate evaluator**: Have GPT-5.1 and human annotators rank the same 50 answer sets. Compute agreement rate to validate LLM-as-judge reliability for this domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific subtle biases does the fully automated, LLM-based generation process introduce into the dataset regarding user personas or product representation?
- Basis in paper: [explicit] The comparison with existing datasets notes that while automation enables scalability, it "may introduce subtle biases, which merit future study."
- Why unresolved: The paper focuses on the technical architecture and performance metrics but does not conduct a bias audit or qualitative analysis of the generated content's fairness.
- What evidence would resolve it: A comparative analysis between MegaChat outputs and human-curated datasets to identify statistical skews in demographic representation or product sentiment.

### Open Question 2
- Question: To what extent does the GPT-5.1 "LLM-as-a-judge" evaluation align with human Persian speakers' preferences for emotional sensitivity and persona alignment?
- Basis in paper: [inferred] The methodology relies exclusively on GPT-5.1 to rank answers and determine ground truth, yet provides no human validation to confirm the model aligns with actual user satisfaction in a low-resource language.
- Why unresolved: While the paper demonstrates internal consistency via GPT-5.1, it does not verify if the model's definition of "Persona Alignment" or "Emotional Sensitivity" matches human perceptions.
- What evidence would resolve it: Correlation scores between GPT-5.1 rankings and human evaluator rankings for a sample of the generated Q&A pairs.

### Open Question 3
- Question: What specific content characteristics in the @LBASs2 channel caused the agentic architecture to underperform compared to classic RAG models?
- Basis in paper: [inferred] The results section states the agentic architecture outperformed RAG in "4 out of 5" channels, implying it failed or matched RAG in the remaining channel (@LBASs2) without explanation.
- Why unresolved: The paper highlights the aggregate success but does not provide an error analysis for the failure case, leaving the conditions under which the complex agentic system degrades unknown.
- What evidence would resolve it: An ablation study or qualitative review of the @LBASs2 channel posts to identify features (e.g., post length, ambiguity) that break the multi-query retrieval or synthesis pipeline.

## Limitations
- The fully synthetic nature of the dataset introduces potential biases from model-generated questions and answers, as acknowledged by the authors
- GPT-5.1 was used for evaluation but is not publicly available, requiring approximation with GPT-4o
- Limited diversity in source channels (45.8% clothing) may constrain question variety and answer quality across underrepresented categories

## Confidence
- **High Confidence**: Multi-agent generation architecture design, classic RAG baseline implementation, Telegram channel data collection methodology
- **Medium Confidence**: Agentic system superiority claims (4/5 channel wins), SLM reranking effectiveness, GPT-5.1 evaluation reliability
- **Low Confidence**: Exact SLM model specifications, persona template effectiveness, long-term generalization to unseen channels

## Next Checks
1. **Ablate the Two-Pass pipeline**: Compare question quality from Generator-only vs. full Generator→Validator→Refiner on 100-sample subset using manual review to verify error-catching effectiveness
2. **Cross-validate evaluator**: Have GPT-4o and human annotators rank the same 50 answer sets to compute agreement rate and validate LLM-as-judge reliability for Persian sales QA
3. **Test retrieval depth**: Evaluate agentic system with 3, 5, 8 queries to measure answer quality vs. latency tradeoff on a single channel, confirming multi-query expansion benefits