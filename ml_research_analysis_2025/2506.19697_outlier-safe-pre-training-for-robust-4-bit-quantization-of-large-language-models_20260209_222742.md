---
ver: rpa2
title: Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models
arxiv_id: '2506.19697'
source_url: https://arxiv.org/abs/2506.19697
tags:
- channel
- layers
- proj
- training
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Outlier-Safe Pre-Training (OSP), a framework
  that prevents activation outliers in large language models during training to improve
  4-bit quantization robustness. OSP combines three innovations: the Muon optimizer
  (which avoids privileged bases), Single-Scale RMSNorm (which eliminates channel-wise
  scaling), and learnable embedding projections (which redistribute embedding magnitudes).'
---

# Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models

## Quick Facts
- arXiv ID: 2506.19697
- Source URL: https://arxiv.org/abs/2506.19697
- Reference count: 40
- This paper introduces Outlier-Safe Pre-Training (OSP), a framework that prevents activation outliers in large language models during training to improve 4-bit quantization robustness

## Executive Summary
This paper introduces Outlier-Safe Pre-Training (OSP), a framework that prevents activation outliers in large language models during training to improve 4-bit quantization robustness. OSP combines three innovations: the Muon optimizer (which avoids privileged bases), Single-Scale RMSNorm (which eliminates channel-wise scaling), and learnable embedding projections (which redistribute embedding magnitudes). A 1.4B-parameter model trained on 1 trillion tokens with OSP achieved near-zero excess kurtosis (0.04) compared to 1818.56 in standard models, and scored 35.7 on 10 benchmarks under 4-bit quantization versus 26.5 for an Adam-trained baseline. OSP reduced memory usage by 33% and maintained only 2% training overhead. The framework demonstrates that outliers are preventable rather than inherent, enabling efficient LLM deployment without architectural changes.

## Method Summary
OSP modifies LLM pre-training by replacing Adam optimizer with Muon (spectral optimizer), standard normalization with Single-Scale RMSNorm (uniform channel scaling), and adding learnable embedding projections initialized orthogonally. The framework trains a 1.4B parameter model on 1T tokens using TPU v4-512 with FSDP, achieving near-zero activation kurtosis that enables robust 4-bit quantization without post-training mitigation.

## Key Results
- OSP achieves near-zero excess kurtosis (0.04) compared to 1818.56 in standard models
- 1.4B model scores 35.7 on 10 benchmarks under 4-bit quantization vs 26.5 for Adam baseline
- OSP reduces memory usage by 33% and maintains only 2% training overhead
- Demonstrates outliers are preventable rather than inherent to LLM training

## Why This Works (Mechanism)

### Mechanism 1: Eliminating Optimizer-Induced Privileged Bases
Standard adaptive optimizers like Adam create privileged bases through element-wise scaling, systematically amplifying certain channels. The Muon optimizer uses Newton-Schulz iteration to orthogonalize gradient matrices before applying them, ensuring parameter updates are full-rank linear transformations that prevent systematic channel amplification. This eliminates the initial seeds of outlier formation.

### Mechanism 2: Unifying Activation Scaling to Prevent Channel Amplification
Standard normalization layers with per-channel learnable scaling factors create explicit basis alignment leading to outlier channels. OSP replaces these with Single-Scale RMSNorm (SSNORM), using a single scalar to scale all channels uniformly. This preserves dynamic magnitude adjustment while preventing any single dimension from being preferentially scaled over time.

### Mechanism 3: Redistributing Embedding-Derived Outliers via Learnable Projections
Even with spectral optimizers, embedding layers can generate outliers due to high dimensionality and separate optimization dynamics. OSP adds a learnable full-rank projection matrix (EMBPROJ) after the embedding layer, initialized as orthogonal. This redistributes concentrated activation magnitudes across multiple dimensions, preventing outliers from propagating into main transformer blocks.

## Foundational Learning

- **Concept: Activation Outliers and Kurtosis**
  - Why needed here: The entire OSP framework is predicated on the idea that high kurtosis in activation distributions makes low-bit quantization ineffective
  - Quick check question: Can you explain why a single activation value that is 100x larger than the mean would cause a 4-bit quantization scheme to fail for the entire layer?

- **Concept: Quantization Fundamentals (Scale Factor and Zero-Point)**
  - Why needed here: Outliers inflate the quantization scale factor, which increases rounding errors for normal-valued activations
  - Quick check question: In the quantization formula $\hat{x}_{int} = \text{clip}(\text{round}(x/s) + z, 0, 2^n-1)$, how does a large outlier in $x$ affect the precision of non-outlier values?

- **Concept: Privileged Basis in Neural Networks**
  - Why needed here: This is the core theoretical justification for why Adam and standard normalization layers create outliers
  - Quick check question: Why does performing element-wise operations in a specific coordinate basis encourage certain dimensions to grow disproportionately?

## Architecture Onboarding

- **Component map:** Input Embeddings (Adam) -> EMBPROJ (orthogonal projection) -> Transformer Blocks (Muon + SSNORM) -> Output
- **Critical path:** Correct initialization and isolation of EMBPROJ layer and consistent application of SSNORM throughout the model. The decoupling of optimizers (Adam for embeddings/projection, Muon for the rest) is a vital implementation detail.
- **Design tradeoffs:** 
  - Efficiency vs. Outlier Prevention: Muon offers 98% training throughput efficiency compared to Adam
  - Stability vs. Uniformity: SSNORM ensures uniformity but requires careful initialization
  - Isolation vs. Contamination: Adam for embeddings is faster but introduces outliers; EMBPROJ adds parameters to "clean" activations
- **Failure signatures:**
  - Outlier Recurrence: If excess kurtosis climbs during training, check if EMBPROJ was omitted or SSNORM was replaced
  - Training Divergence: If loss explodes, verify orthogonal initialization of EMBPROJ and Muon learning rate
  - No Quantization Benefit: If model still performs poorly under 4-bit quantization, ensure all three OSP components were enabled
- **First 3 experiments:**
  1. Baseline Reproduction: Train small model with standard Adam/RMSNorm to establish failure baseline
  2. Component Ablation: Implement full OSP stack and run ablation study removing one component at a time
  3. Downstream Validation: Train 1.4B model on larger dataset and evaluate under full precision and 4-bit quantization

## Open Questions the Paper Calls Out

- Does the Outlier-Safe Pre-Training (OSP) framework effectively prevent outlier formation in models larger than 1.4B parameters (e.g., 3B or 7B scales)? The authors state they have not yet explored the impact across a range of model sizes.

- What are the underlying mechanisms linking attention patterns to outlier formation, given that attention sinks persist in outlier-free models? The paper finds OSP models use concentrated positive attention instead, but does not fully explain this mechanistic shift.

- How does the Muon optimizer compare to other second-order methods (Shampoo, SOAP) in balancing outlier prevention with computational efficiency? The study focuses primarily on Muon without extensive comparisons due to practical constraints.

## Limitations
- Model size generalization remains unproven beyond 1.4B parameters
- Effectiveness for alternative architectures (Mamba, RWKV) is unknown
- Computational overhead claims appear optimistic given typical second-order optimization costs

## Confidence
- **High Confidence**: Adaptive optimizers create privileged bases; single-scale normalization prevents channel amplification; correlation between outliers and quantization degradation
- **Medium Confidence**: Outliers are "preventable rather than inherent"; Muon's 98% training throughput efficiency; generalizability across model scales
- **Low Confidence**: Long-term stability of orthogonal projection matrix; effectiveness on non-English datasets; impact on convergence speed

## Next Checks
1. Implement OSP on a 7B parameter model and compare excess kurtosis and quantization performance to validate scaling effectiveness
2. Apply OSP to a non-Transformer architecture (e.g., Mamba or RWKV) to test framework generalizability
3. Run OSP training for 10 trillion tokens instead of 1 trillion to monitor long-term stability and potential gradual outlier emergence