---
ver: rpa2
title: 'OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language
  Models'
arxiv_id: '2504.01789'
source_url: https://arxiv.org/abs/2504.01789
tags:
- reasoning
- training
- otg-1
- otg-r1
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1),
  Thai-centric Large Language Models developed to address the challenge of achieving
  optimal performance for Thai language tasks. OTG-1.6 employs Task Arithmetic model
  merging to combine specialized models, improving generalization across diverse tasks
  without increasing computational requirements.
---

# OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language Models

## Quick Facts
- arXiv ID: 2504.01789
- Source URL: https://arxiv.org/abs/2504.01789
- Reference count: 3
- Primary result: OTG-R1 (32B) outperforms 70B models on reasoning benchmarks

## Executive Summary
This paper presents OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large Language Models developed to address the challenge of achieving optimal performance for Thai language tasks. OTG-1.6 employs Task Arithmetic model merging to combine specialized models, improving generalization across diverse tasks without increasing computational requirements. OTG-R1 integrates multi-stage training with the Less-Is-More Reasoning Hypothesis (LIMO) to enhance reasoning capabilities. Benchmark evaluations demonstrate that OTG-1.6 achieves superior performance in generalization tasks, with an average score of 52.34 across benchmarks, and OTG-R1 achieves competitive or superior results on reasoning benchmarks, with an average score of 71.59. Notably, OTG-R1 outperforms larger models like DeepSeek-R1 and Typhoon2-R1 despite having a smaller model size (32B compared to 70B).

## Method Summary
The authors develop two Thai-centric LLMs using parameter-efficient fine-tuning on 8x H100 GPUs. OTG-1.6 combines three specialized models (general instructions, translation pairs, Thai exams) via Task Arithmetic merging with weights [0.15, 0.15, 0.70]. OTG-R1 uses multi-stage training: Phase 1 fine-tunes on Thai datasets at 8192 tokens, then Phase 2 extends to 16384 tokens with LIMO reasoning data. Both models employ LoRA (rank 64, alpha 128) for efficient adaptation of the base models (Qwen2.5-72B-Instruct for OTG-1.6, DeepSeek-R1-Distill-Qwen-32B for OTG-R1).

## Key Results
- OTG-1.6 achieves an average score of 52.34 across OpenThaiEval benchmarks, demonstrating superior generalization
- OTG-R1 achieves an average score of 71.59 on reasoning benchmarks, outperforming larger models like DeepSeek-R1 and Typhoon2-R1
- OTG-R1 (32B) demonstrates that model size is less critical than training methodology for reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Task Arithmetic Model Merging for Cross-Domain Generalization
- Claim: Weighted linear combination of task-specialized models enables broad generalization without increased inference cost.
- Mechanism: Fine-tune separate models on domain-specific datasets (Thai exams, translation, general instructions), then merge parameters using weighted arithmetic. The dominant weight (0.70 for Thai exams) prioritizes domain-specific knowledge while smaller weights (0.15 each) preserve auxiliary capabilities.
- Core assumption: Task-specific knowledge resides in linearly composable parameter directions without catastrophic interference.
- Evidence anchors:
  - [abstract] "OTG-1.6 employs Task Arithmetic model merging to combine specialized models, improving generalization across diverse tasks without increasing computational requirements"
  - [section 2.1] "merging weights were assigned as follows: General Instructions Model (0.15), Translation Pairs Model (0.15), and Thai Exams Model (0.70)"
  - [corpus] Limited direct corpus evidence on Task Arithmetic efficacy; neighbor papers do not replicate this specific merging approach
- Break condition: When merged tasks require contradictory representations (e.g., formal vs. colloquial language patterns), or when weight distribution misaligns with target domain priorities.

### Mechanism 2: LIMO (Less-Is-More) for Reasoning Enhancement
- Claim: High-quality curated reasoning data achieves stronger reasoning gains than large-scale uncurated datasets.
- Mechanism: LIMO dataset provides structurally diverse, contextually rich reasoning examples that teach chain-of-thought patterns more efficiently than volume-based training. Applied without translation to preserve reasoning structure integrity.
- Core assumption: Reasoning capability is bottlenecked by data quality and structural diversity, not quantity.
- Evidence anchors:
  - [abstract] "OTG-R1 integrates multi-stage training with the Less-Is-More Reasoning Hypothesis (LIMO) to enhance reasoning capabilities"
  - [section 2.2] "This dataset was applied without translation to preserve contextual integrity. The integration of LIMO aimed to leverage high-quality, contextually relevant data"
  - [corpus] Weak corpus validation; LIMO hypothesis cited from Ye et al. 2025 but not independently verified in neighbor literature
- Break condition: When target reasoning domains diverge from LIMO's distribution, or when translation is required (breaking the no-translation preservation principle).

### Mechanism 3: Progressive Context Extension for Complex Reasoning
- Claim: Incrementally extending maximum sequence length during training enables stable learning of long-chain reasoning.
- Mechanism: Phase 1 trains at 8192 tokens to establish reasoning foundations; Phase 2 extends to 16384 tokens with LIMO data, allowing the model to process multi-step problems without destabilizing previously learned patterns.
- Core assumption: Reasoning patterns learned at shorter contexts transfer to longer contexts when extended gradually.
- Evidence anchors:
  - [section 2.2] "The initial training phase utilized a maximum sequence length of 8192 tokens, which was extended to 16384 tokens in the subsequent phase. This progression allowed the model to process increasingly complex queries"
  - [abstract] "OTG-R1 integrates multi-stage training"
  - [corpus] No direct corpus evidence on progressive context extension specifically
- Break condition: When target tasks require reasoning structures fundamentally different from training progression (e.g., very long multi-hop reasoning not seen in Phase 2).

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Both models use LoRA (rank 64, alpha 128) for parameter-efficient fine-tuning of 72B and 32B models on 8x H100s.
  - Quick check question: Can you explain why LoRA enables training large models with limited GPU memory compared to full fine-tuning?

- **Concept: Task Arithmetic in Parameter Space**
  - Why needed here: OTG-1.6's core innovation combines models via weighted parameter addition; understanding this requires grasping how fine-tuning creates task vectors.
  - Quick check question: If you merge two models with weights 0.7 and 0.3, what happens to parameters where the models have opposite update directions?

- **Concept: Chain-of-Thought Reasoning in LLMs**
  - Why needed here: OTG-R1's LIMO integration targets reasoning enhancement; evaluating results requires understanding what constitutes reasoning benchmarks (AIME, MATH500).
  - Quick check question: How does a reasoning benchmark differ from a general knowledge benchmark in what it evaluates?

## Architecture Onboarding

- **Component map:**
  - Base models: Qwen2.5-72B-Instruct (OTG-1.6) / DeepSeek-R1-Distill-Qwen-32B (OTG-R1)
  - Training framework: MS Swift + DeepSpeed
  - Fine-tuning method: LoRA (r=64, α=128, lr=1e-4)
  - Merging tool: mergekit (OTG-1.6 only)
  - Datasets: Thai-translated Alpaca, OASST1, GPTeacher, ONET/TGAT exams, LIMO (OTG-R1 Phase 2)

- **Critical path:**
  1. Prepare domain-specific datasets (Thai exams, translation pairs, instructions)
  2. Fine-tune separate LoRA adapters per domain (3 epochs each)
  3. Merge adapters using Task Arithmetic with specified weights
  4. For OTG-R1: Train Phase 1 (8192 tokens) → Phase 2 (16384 tokens + LIMO)

- **Design tradeoffs:**
  - Model size vs. reasoning efficiency: 32B OTG-R1 outperforms 70B baselines, suggesting training methodology matters more than scale for reasoning tasks
  - Weight distribution in merging: 0.70 Thai exams weight optimizes for domain performance but may underweight general capabilities
  - LIMO without translation: Preserves reasoning structure but may limit Thai-specific reasoning patterns

- **Failure signatures:**
  - Out-of-domain queries: Paper explicitly notes models "may struggle with out-of-domain inputs or highly specialized queries"
  - Weight imbalance in merging: Over-weighting one domain may cause knowledge interference in others
  - Insufficient context at inference: If queries exceed training context (16384 tokens), reasoning quality may degrade

- **First 3 experiments:**
  1. Replicate OTG-1.6 merging with modified weights (e.g., 0.50/0.25/0.25) to measure sensitivity to weight distribution on OpenThaiEval.
  2. Ablate LIMO integration: Train OTG-R1 without Phase 2 LIMO data and compare MATH500-TH scores to isolate LIMO's contribution.
  3. Test context ceiling: Evaluate OTG-R1 on reasoning tasks at 16384+ tokens to identify where progressive training breaks down.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Task Arithmetic weight distributions be optimized to prevent the loss of domain-specific knowledge during model merging?
- Basis in paper: [explicit] The authors state in the Limitations section that the merging technique "may overlook domain-specific knowledge due to weight distribution limitations."
- Why unresolved: The paper applies fixed weights (0.15, 0.15, 0.70) but does not explore adaptive weighting strategies that might preserve niche knowledge better than the chosen static ratios.
- What evidence would resolve it: Ablation studies comparing static weight merging against dynamic or learned weighting mechanisms on a diverse set of held-out Thai domain tasks.

### Open Question 2
- Question: What specific linguistic capabilities remain uncaptured by current Thai LLM benchmarks?
- Basis in paper: [explicit] The paper notes that "Current evaluation benchmarks do not cover all aspects of Thai language understanding and reasoning, potentially overlooking certain capabilities."
- Why unresolved: The evaluation relies on math, coding, and general knowledge benchmarks (e.g., AIME24-TH, OpenThaiEval), which may fail to assess cultural nuances or complex linguistic structures unique to Thai.
- What evidence would resolve it: The development and application of a new benchmark suite specifically designed to test currently unmeasured aspects, such as Thai cultural pragmatics or low-resource dialectal variations.

### Open Question 3
- Question: What training modifications are required to improve model robustness on out-of-domain or highly specialized queries?
- Basis in paper: [explicit] The Limitations section identifies that "The models may struggle with out-of-domain inputs or highly specialized queries that were not adequately addressed during training."
- Why unresolved: The current training relies on standardized examination datasets (ONET, TGAT) and general instructions, which creates a distribution mismatch with highly specialized real-world inputs.
- What evidence would resolve it: Evaluation of models fine-tuned with curriculum learning or domain-adversarial training on a held-out set of specialized, out-of-domain expert queries.

## Limitations

- The evaluation relies heavily on Thai-centric benchmarks without external validation from English-language or multilingual benchmarks
- ONET and TGAT exam datasets are referenced but not publicly linked, preventing independent verification of the domain-specific training data
- The LIMO hypothesis, while novel, lacks independent validation in the cited literature and may represent overfitting to the specific reasoning patterns present in the LIMO dataset

## Confidence

- **High Confidence:** The Task Arithmetic merging methodology (weighted parameter combination) is well-established and directly implementable from published code.
- **Medium Confidence:** The performance improvements over baseline models are statistically significant on the reported benchmarks, though the benchmarks themselves are not widely adopted in the broader research community.
- **Low Confidence:** The claim that OTG-R1 outperforms DeepSeek-R1 and Typhoon2-R1 on reasoning tasks is difficult to verify without access to the exact benchmark implementations and evaluation protocols.

## Next Checks

1. **Independent Benchmark Replication:** Re-run OpenThaiEval and reasoning benchmarks using standardized implementations to verify the claimed average scores of 52.34 (OTG-1.6) and 71.59 (OTG-R1).

2. **Weight Sensitivity Analysis:** Systematically vary the Task Arithmetic weights in OTG-1.6 merging to determine the robustness of performance improvements and identify optimal weight distributions.

3. **Out-of-Domain Testing:** Evaluate both models on non-Thai benchmarks (e.g., English MMLU, GSM8K) to assess whether the claimed generalization improvements transfer beyond the Thai-centric domain.