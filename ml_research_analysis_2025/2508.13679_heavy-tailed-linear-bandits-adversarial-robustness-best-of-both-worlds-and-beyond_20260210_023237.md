---
ver: rpa2
title: 'Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds,
  and Beyond'
arxiv_id: '2508.13679'
source_url: https://arxiv.org/abs/2508.13679
tags:
- heavy-tailed
- regret
- adversarial
- linear
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing algorithms for
  adversarial heavy-tailed bandits, a setting where losses can be heavy-tailed and
  chosen adversarially. Existing approaches struggle with this problem due to difficulties
  in handling the potential bias introduced by loss clipping, particularly on the
  optimal arm, and the lack of finite support of the losses.
---

# Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond

## Quick Facts
- arXiv ID: 2508.13679
- Source URL: https://arxiv.org/abs/2508.13679
- Authors: Canzhe Zhao; Shinji Ito; Shuai Li
- Reference count: 40
- Primary result: Proposes first FTRL-based BOBW algorithm for heavy-tailed MABs without truncated non-negativity assumption

## Executive Summary
This paper addresses the challenge of designing algorithms for adversarial heavy-tailed bandits, where losses can be heavy-tailed and chosen adversarially. Existing approaches struggle due to difficulties in handling bias introduced by loss clipping and the lack of finite support. The authors propose a general framework using follow-the-regularized-leader (FTRL) over clipped loss estimates shifted by a carefully designed bonus function. This approach cancels the bias introduced by clipping while maintaining optimal regret bounds. The framework achieves significant improvements over existing methods in both worst-case adversarial and stochastic regimes.

## Method Summary
The paper proposes a general framework for adversarial heavy-tailed bandits that performs FTRL over clipped loss estimates shifted by a carefully designed bonus function. The key innovation is the bonus function, which cancels the bias introduced by loss clipping, especially on the optimal arm, while keeping its magnitude controlled. For MABs, this yields a BOBW algorithm achieving O(T^(1/ε)) worst-case regret in adversarial regimes and O(log T) gap-dependent regret in stochastic regimes. For linear bandits, the framework extends to achieve O(d^(1/2)T^(1/ε)) regret, matching the best-known worst-case bound in stochastic regimes. The Heavy-tailed Noise Aware Stability-Penalty Matching (HT-SPM) learning rate is a crucial component for balancing bias and variance.

## Key Results
- Devises the first FTRL-type BOBW algorithm for heavy-tailed MABs without truncated non-negativity assumption, achieving O(T^(1/ε)) worst-case regret in adversarial regimes and O(log T) gap-dependent regret in stochastic regimes
- Proposes the first algorithm for adversarial heavy-tailed linear bandits with finite arm sets, achieving O(d^(1/2)T^(1/ε)) regret
- Introduces HT-SPM, a general data-dependent learning rate framework that enables tighter regret analysis by matching stability and penalty terms

## Why This Works (Mechanism)
The framework works by using FTRL with a carefully designed bonus function that cancels the bias introduced by loss clipping. The bonus function is derived through a reverse analysis that quantifies the bias on the optimal arm and compensates for it while maintaining overall regret control. The HT-SPM learning rate balances the stability of the algorithm with the penalty from the bonus terms, enabling tighter regret bounds. The key insight is that by shifting the loss estimates with this bonus function, the algorithm can maintain unbiasedness on the optimal arm while still benefiting from the variance reduction of clipping.

## Foundational Learning
- **Heavy-tailed distributions**: Losses have finite (ε-1)-th moments but potentially infinite ε-th moments. Needed to understand the distributional assumptions and why standard concentration inequalities fail. Quick check: Verify that the loss distribution satisfies the given moment condition.
- **Adversarial bandit framework**: Losses are chosen by an adversary rather than being i.i.d. Needed to understand the worst-case nature of the problem and the distinction from stochastic settings. Quick check: Confirm the adversary's choice is made without knowledge of the algorithm's internal randomness.
- **Follow-the-regularized-leader (FTRL)**: An online optimization algorithm that minimizes cumulative loss plus a regularization term. Needed as the core algorithmic framework. Quick check: Ensure the algorithm's update rule correctly implements the FTRL principle with the specified regularizer.
- **Clipping mechanism**: Truncating losses at a threshold to reduce variance. Needed to understand how heavy-tailed noise is handled. Quick check: Verify the clipping threshold is set appropriately based on the known parameters.
- **Bonus function design**: A carefully constructed function that cancels bias while maintaining control over the magnitude. Needed to understand how the framework achieves unbiasedness on the optimal arm. Quick check: Confirm the bonus function satisfies the bias cancellation property through calculation.
- **Tsallis entropy regularization**: A non-standard regularizer that enables tighter analysis in MABs. Needed to understand why this specific regularizer is chosen over alternatives. Quick check: Verify the regularizer satisfies the conditions required for the HT-SPM framework.

## Architecture Onboarding

**Component Map:**
HT-SPM Learning Rate -> FTRL Algorithm -> Bonus-shifted Loss Estimates -> Clipped Losses

**Critical Path:**
The critical path is the computation of the bonus function and its integration into the FTRL update. The algorithm proceeds as follows: (1) Observe the clipped loss z_t,a for each arm, (2) Compute the bonus function b_t,a using the HT-SPM learning rate and the current estimates, (3) Update the FTRL distribution using the shifted loss estimates (z_t,a + b_t,a), and (4) Select the next arm based on the updated distribution. The bonus function computation is the most critical step as it directly affects the bias-variance trade-off.

**Design Tradeoffs:**
- The choice of regularizer affects both the computational complexity and the regret bounds. Tsallis entropy is chosen for its favorable properties in the MAB setting but may not generalize well to all problems.
- The HT-SPM learning rate requires careful tuning of parameters like η_t and β_t. The current choice balances stability and adaptability but may be suboptimal for specific problem instances.
- The framework assumes known parameters ε and σ for the heavy-tailed distribution. While this enables tighter bounds, it limits the algorithm's applicability when these parameters are unknown.

**Failure Signatures:**
- If the bonus function is not properly calibrated, the algorithm may suffer from high regret due to either excessive bias or variance. This can be detected by monitoring the magnitude of the bonus terms relative to the clipped losses.
- In the linear bandit case, if the feature vectors are not well-conditioned, the algorithm may struggle to distinguish between arms, leading to higher regret. This can be detected by analyzing the condition number of the design matrix.
- If the adversary is too strong (e.g., changing losses rapidly), the algorithm's regret may approach the worst-case bound, indicating that the bonus function is not providing sufficient protection against adversarial corruption.

**First 3 Experiments:**
1. Test the MAB algorithm on a synthetic heavy-tailed distribution with known parameters to verify the theoretical regret bounds.
2. Evaluate the linear bandit algorithm on a problem with well-separated optimal arms to assess its performance in the stochastic regime.
3. Stress-test the algorithm with an adaptive adversary that changes losses based on the algorithm's past behavior to evaluate its robustness in the adversarial regime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FTRL-based BOBW algorithms for heavy-tailed bandits achieve instance-optimal regret in stochastic regimes that adapts to the solution c(A, ℓ) of the arm geometry optimization problem, rather than the O(d/Δ_min)-style dependence?
- Basis in paper: [explicit] The authors state: "achieving a BOBW algorithm that adapts to the solution of this optimization problem has only been realized by Lee et al. (2021), via a detect-and-switch-based strategy. To the best of our knowledge, no FTRL-based BOBW algorithm has been shown to attain such instance-optimal regret, even in the simpler case of linear bandits with bounded or sub-Gaussian noises."
- Why unresolved: The detect-and-switch approach achieves this but lacks adaptivity in intermediate regimes. The FTRL framework's self-bounding analysis inherently yields O(d/Δ_min)-type bounds.
- What evidence would resolve it: An FTRL-based algorithm whose stochastic regret scales with c(A, ℓ) rather than d/Δ_min, verified on problem instances where c(A, ℓ) << d/Δ_min.

### Open Question 2
- Question: Can the worst-case regret lower bound for heavy-tailed bandits with known ε and σ be achieved without knowing these parameters, under minimal additional assumptions on the heavy-tailed distribution?
- Basis in paper: [explicit] The authors note: "Genalti et al. (2024) prove that it is not possible to match the worst-case regret lower bound of the case with known ε and σ, if either ε or σ is unknown. It remains an interesting but also challenging future direction to explore whether it is feasible to match the worst-case lower bound...by imposing (minimal) additional assumptions."
- Why unresolved: The bonus function construction requires explicit knowledge of ε and σ to set appropriate clipping thresholds and bonus magnitudes. Without these, the bias-variance trade-off cannot be properly calibrated.
- What evidence would resolve it: An algorithm achieving O(σ^(1/ε)d^(1/2)T^(1/ε)) regret without knowing ε or σ, possibly under mild conditions like symmetric distributions or bounded mean.

### Open Question 3
- Question: Can the adversarial heavy-tailed linear bandit framework be extended to infinite arm sets while maintaining the O(d^(1/2)T^(1/ε)) regret rate?
- Basis in paper: [inferred] The paper focuses exclusively on finite arm sets. Table 2 notes that existing stochastic algorithms like HEAVY-OFUL achieve O(d^((3ε-2)/(2ε))T^(1/ε)) for infinite sets, while the finite-set lower bound is Ω(σ^(1/ε)(log K)^(1-1/ε)d^(1-1/ε)T^(1/ε)).
- Why unresolved: The log K factors in the finite-set analysis arise from the Shannon entropy regularizer. For infinite sets, a continuous analogue of the exploration distribution and loss estimator would be needed.
- What evidence would resolve it: An algorithm for infinite arm sets (e.g., unit ball) achieving O(σ^(1/ε)d^(1/2)T^(1/ε)) regret, or a lower bound showing additional d-dependence is necessary.

### Open Question 4
- Question: Can the variance-reduced linear loss estimator and HT-SPM learning rate be combined with more general regularizers (beyond Tsallis entropy) to handle heavy-tailed bandits with different feedback structures?
- Basis in paper: [inferred] The authors mention their techniques "may be of independent interest for other heavy-tailed bandit optimization problems with structured loss feedback." The HT-SPM framework (Proposition 1) is stated generally but only instantiated for MABs and linear bandits.
- Why unresolved: The conditions in Proposition 1 (particularly Eq. 12 linking h_t^(ε-1)z_t to ⟨Δ, p_t⟩) must be verified for each feedback structure, and the bonus function design depends on the specific form of the loss estimator.
- What evidence would resolve it: Application of the bonus-shifted FTRL framework with HT-SPM to settings like combinatorial bandits, graph feedback, or linear contextual bandits under heavy-tailed noise, with corresponding regret guarantees.

## Limitations
- The theoretical analysis relies on specific assumptions about the clipping mechanism and the structure of the bonus function, which may not hold in all practical scenarios
- The computational complexity of the proposed algorithms, particularly for the linear bandit case, is not explicitly addressed and could be prohibitive for large-scale applications
- The empirical validation is limited to synthetic experiments, with no real-world data or extensive simulations provided to demonstrate practical performance

## Confidence
- **High confidence** in the theoretical regret bounds for both MAB and linear bandit settings, as these are rigorously derived using established techniques
- **Medium confidence** in the practical applicability of the algorithms, given the lack of comprehensive empirical evaluation and discussion of computational considerations
- **Low confidence** in the generalizability of the results to continuous action spaces or more complex bandit structures beyond the considered finite-arm linear bandit setting

## Next Checks
1. Conduct extensive empirical studies on synthetic and real-world datasets to evaluate the practical performance of the proposed algorithms, comparing them with state-of-the-art methods under various heavy-tailed and adversarial conditions
2. Analyze the computational complexity of the algorithms, particularly for the linear bandit case, and explore potential optimizations or approximations to improve scalability
3. Extend the theoretical framework to continuous action spaces and investigate the applicability of the proposed methods to more complex bandit structures, such as contextual bandits with rich feature spaces