---
ver: rpa2
title: 'Digital Player: Evaluating Large Language Models based Human-like Agent in
  Games'
arxiv_id: '2502.20807'
source_url: https://arxiv.org/abs/2502.20807
tags:
- game
- arxiv
- agents
- civilization
- civagent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CivSim and CivAgent as a testbed for studying
  human-like agents in strategy games, focusing on digital players capable of human-AI
  interaction. The testbed leverages the open-source game Unciv to enable LLM-based
  agents to engage in complex decision-making involving numerical reasoning, long-term
  planning, and diplomatic negotiations.
---

# Digital Player: Evaluating Large Language Models based Human-like Agent in Games

## Quick Facts
- arXiv ID: 2502.20807
- Source URL: https://arxiv.org/abs/2502.20807
- Reference count: 40
- Primary result: Introduces CivSim and CivAgent as testbed for human-like LLM agents in strategy games

## Executive Summary
This paper presents CivSim and CivAgent as a novel testbed for studying human-like agents in strategy games. The framework leverages the open-source game Unciv to enable LLM-based agents to engage in complex decision-making involving numerical reasoning, long-term planning, and diplomatic negotiations. By integrating LLMs with game simulators and reflection mechanisms, CivAgent demonstrates improved performance in both full-game scenarios and specialized mini-games focused on diplomacy and negotiation. The testbed also supports data flywheel construction through human feedback, offering a scalable platform for advancing LLM-based human-like agents.

## Method Summary
The paper introduces CivSim and CivAgent as a testbed for evaluating LLM-based human-like agents in strategy games. CivAgent integrates LLMs with tools like game simulators and reflection mechanisms to enhance decision-making capabilities. The framework uses Unciv as the game environment, enabling agents to engage in complex tasks requiring numerical reasoning, long-term planning, and diplomatic negotiations. Experiments compare different CivAgent variants, demonstrating that those with simulator and reflection modules outperform others in full-game tasks and specialized mini-games.

## Key Results
- CivAgent variants with simulator and reflection modules outperform others in full-game tasks
- Notable success in diplomatic skills and negotiation/deception mini-games
- Testbed supports data flywheel construction through human feedback

## Why This Works (Mechanism)
CivAgent works by combining LLM reasoning capabilities with game-specific tools that enhance decision quality. The simulator module allows agents to predict game state changes before committing to actions, reducing costly mistakes. The reflection mechanism enables self-evaluation of decisions, leading to improved strategic planning. The integration with Unciv provides a structured environment where these capabilities can be systematically tested and refined.

## Foundational Learning
- Unciv game mechanics - Why needed: Provides the strategic game environment for testing; Quick check: Understand basic gameplay elements like resource management and diplomacy
- LLM integration patterns - Why needed: Forms the cognitive core of the agent; Quick check: Know how LLMs process game state and generate actions
- Simulator tool integration - Why needed: Enables forward planning and risk assessment; Quick check: Verify the simulator can accurately predict game state changes
- Reflection mechanisms - Why needed: Improves decision quality through self-evaluation; Quick check: Confirm reflection can identify suboptimal decisions

## Architecture Onboarding

**Component Map:** Unciv Game Environment -> CivAgent (LLM Core + Tools) -> Game Actions -> Feedback Loop

**Critical Path:** Game State Observation → LLM Processing → Tool Utilization (Simulator/Reflection) → Action Generation → Game State Update

**Design Tradeoffs:** Balance between computational cost of simulation versus decision quality; integration complexity versus modularity of tool components

**Failure Signatures:** Poor performance in numerical reasoning tasks indicates simulator integration issues; repeated strategic mistakes suggest reflection mechanism inadequacy; diplomatic failures point to insufficient training data in negotiation scenarios

**First 3 Experiments:**
1. Run CivAgent in Unciv with only LLM core (no tools) to establish baseline performance
2. Enable simulator tool and measure improvement in numerical reasoning tasks
3. Activate reflection mechanism and evaluate changes in strategic decision quality

## Open Questions the Paper Calls Out
None

## Limitations
- Performance in full-game scenarios remains limited, suggesting gaps in long-term strategic planning capabilities
- Results are specific to Unciv game mechanics and may not generalize to other game genres
- The human-like behavior claim requires further validation through direct comparison with actual human player behaviors

## Confidence
- High Confidence: Integration of LLMs with game-specific tools demonstrably improves decision-making in structured environments
- Medium Confidence: CivAgent achieves human-like behavior based on performance metrics but needs human evaluation validation
- Medium Confidence: Data flywheel concept through human feedback is theoretically sound but lacks empirical validation

## Next Checks
1. Conduct systematic human evaluation studies comparing CivAgent's diplomatic behaviors against those of human players across diverse scenarios
2. Test CivAgent's performance in multiple strategy game environments to assess generalizability beyond the Unciv framework
3. Implement and measure the effectiveness of the proposed human feedback loop over extended training periods to validate the data flywheel concept