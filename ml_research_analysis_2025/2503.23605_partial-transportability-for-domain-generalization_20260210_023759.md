---
ver: rpa2
title: Partial Transportability for Domain Generalization
arxiv_id: '2503.23605'
source_url: https://arxiv.org/abs/2503.23605
tags:
- risk
- domain
- source
- scms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain generalization, specifically
  providing performance guarantees for predictions made in unseen domains when only
  data from source domains is available. The authors propose a framework called partial
  transportability, which allows bounding the generalization error of a classifier
  in a target domain based on data from source domains and assumptions about the underlying
  data generating mechanisms encoded in causal diagrams.
---

# Partial Transportability for Domain Generalization

## Quick Facts
- arXiv ID: 2503.23605
- Source URL: https://arxiv.org/abs/2503.23605
- Reference count: 40
- One-line primary result: Provides worst-case risk bounds for domain generalization using partial transportability and causal diagrams

## Executive Summary
This paper addresses domain generalization by providing performance guarantees for predictions in unseen target domains when only source domain data is available. The authors introduce partial transportability, a framework that bounds the generalization error of a classifier in a target domain based on source data and causal assumptions encoded in selection diagrams. They propose Neural-TR, a gradient-based optimization algorithm using Neural Causal Models to compute tight upper-bounds for generalization error, and Causal Robust Optimization (CRO), an iterative method to find predictors with optimal worst-case risk.

## Method Summary
The method extends transportability theory to partial transportability, allowing bounds on target risk when exact identification is impossible. It uses selection diagrams to encode structural invariances across domains, with Neural Causal Models implementing parameter sharing constraints (θᵢᵥ = θⱼᵥ for invariant mechanisms). Neural-TR maximizes the target risk subject to source likelihood constraints and structural invariances, while CRO iteratively generates adversarial target distributions to robustify the classifier against worst-case scenarios.

## Key Results
- Theorem 1 shows partial transportability can be solved using canonical models
- Theorem 2 demonstrates neural causal models' expressiveness for solving partial transportability tasks
- Proposition 1 proves the correctness of the Neural-TR algorithm
- Theorem 3 establishes the optimality of CRO for domain generalization
- Experiments on synthetic examples and colored MNIST show effective worst-case risk bounding and optimal predictor finding

## Why This Works (Mechanism)

### Mechanism 1: Selection Diagrams as Constraint Encoders
The framework uses Selection Diagrams ($G_\Delta$) to mark variables with domain discrepancies ($\Delta_{ij}$). In Neural Causal Models, parameters for invariant mechanisms are shared across domains, reducing the hypothesis space to compatible target distributions. This mechanism relies on correct specification of the causal graph and selection variables.

### Mechanism 2: Partial Identification via Neural Bounds
When target risk is non-transportable, a tight worst-case bound is computed by optimizing over NCMs consistent with source data and structural constraints. The optimization maximizes the functional (e.g., risk) w.r.t. target NCM parameters, providing a performance guarantee. This assumes the underlying SCM is expressible by the NCM class.

### Mechanism 3: Adversarial Robustification (CRO)
CRO treats Neural-TR as an adversary, iteratively finding the NCM maximizing risk for the current classifier, then retraining using group DRO on data from this worst-case distribution. This adversarial game between finding worst-case distributions and learning robust classifiers reaches equilibrium to minimize worst-case risk.

## Foundational Learning

- **Structural Causal Models (SCMs) and C-components**: Needed to understand the ground truth data generating process and efficient optimization decomposition. Quick check: Can you identify c-components in a causal graph with bi-directed edges?

- **Transportability Theory**: Essential for extending standard transportability (identifying unique target values) to partial transportability (bounding values). Quick check: What does a selection node $S \to V$ imply about mechanism $V$ across domains?

- **Partial Identification**: Critical for understanding how the paper bounds rather than point-estimates quantities. Quick check: If a query is identifiable, what happens to the lower and upper bounds?

## Architecture Onboarding

**Component map:**
- Input: Source datasets {D₁...Dₖ}, Selection Diagram G∆, Functional ψ
- Neural-TR: Gradient-based optimizer managing NCMs
  - Pre-processing: Identifies c-components; substitutes identifiable parts with pre-trained constant models
  - Optimization: Tunes non-identifiable NCM parameters to maximize ψ subject to likelihood and sharing constraints
- CRO: Outer loop wrapper that iteratively calls Neural-TR to generate adversarial data and retrains classifier

**Critical path:**
1. Define G∆ and discrepancy sets Δ
2. Decompose query into c-components
3. Separate: Pre-train identifiable components; Initialize NCMs for non-identifiable components
4. Optimize: Run Neural-TR to find worst-case NCM
5. Robustify: (Optional) Run CRO loop to train classifier using sampled adversarial data

**Design tradeoffs:**
- Canonical vs. Neural Models: Canonical models are linear/convex but discrete and unscalable; NCMs are scalable but non-convex with risk of local optima
- Modularity: Alg 1 substitutes identifiable factors with constants, improving speed but assuming pre-trained models are accurate

**Failure signatures:**
- Bounds > 1.0: Indicates constraint violations or insufficient NCM capacity
- CRO Divergence: Classifier risk oscillates, suggesting contradictory domains or high learning rate
- Loose Bounds: Neural-TR converges to significantly higher value than true risk

**First 3 experiments:**
1. Unit Test (Example 2/3): Replicate synthetic binary experiments to validate Neural-TR returns exact theoretical bounds
2. CMNIST Spurious Correlation: Train on Colored MNIST with 90%/80% correlation; show ERM worst-case risk ~0.95 vs optimal bound 0.25
3. Ablation on Knowledge: Run experiments with incorrect selection diagram (e.g., removing shifting mechanism node); check if bounds become invalid

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the framework extend to partially known graph structures or equivalence classes rather than fully specified selection diagrams? The current results assume known fixed causal structure, limiting applicability when expert knowledge is incomplete.

- **Open Question 2**: Does incorporating specific parametric assumptions (e.g., linear Gaussian models) into Neural Causal Models improve computational efficiency for high-dimensional data? The non-parametric neural approach may face scalability challenges compared to representation learning methods.

- **Open Question 3**: How sensitive is CRO to misspecification of the domain discrepancy set Δ? Underspecified discrepancy sets could lead to incorrect optimization constraints, rendering worst-case bounds invalid or non-tight.

## Limitations
- Requires correct specification of selection diagram and causal structure
- Empirical validation limited to two tasks (synthetic examples and CMNIST)
- No evaluation on real-world domain generalization benchmarks
- Non-convex optimization may yield loose bounds due to local optima

## Confidence
- **High**: Framework correctly extends partial transportability theory to neural models; connection between selection diagrams and parameter sharing is well-established
- **Medium**: Neural-TR effectively bounds worst-case risk in controlled settings; bounds are valid but may be loose due to local optima
- **Low**: CRO consistently finds classifiers that minimize worst-case risk across diverse domains; iterative game reaching equilibrium is assumed but not empirically verified

## Next Checks
1. **Selection Diagram Sensitivity**: Systematically test method's sensitivity to incorrect selection diagram specifications by deliberately removing or adding selection nodes and measuring bound and CRO performance degradation.

2. **Generalization to Real Data**: Evaluate on established domain generalization benchmarks (e.g., PACS, Office-Home) with ground-truth interventions to test practical utility of bounds and CRO-trained classifier robustness.

3. **Scalability and Constraint Satisfaction**: Investigate impact of regularization coefficient Λ on bound tightness and constraint satisfaction through ablation studies varying Λ and measuring feasibility of generated target distributions.