---
ver: rpa2
title: 'Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis
  of Methodologies'
arxiv_id: '2507.13875'
source_url: https://arxiv.org/abs/2507.13875
tags:
- language
- catalan
- speech
- data
- spanish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work improves automatic speech recognition (ASR) for Catalan-Spanish
  code-switching, a task challenged by limited training data and linguistic similarities.
  We explore three strategies: generating synthetic CS data, concatenating monolingual
  audio, and leveraging real CS data with language tokens.'
---

# Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies

## Quick Facts
- **arXiv ID:** 2507.13875
- **Source URL:** https://arxiv.org/abs/2507.13875
- **Reference count:** 0
- **Primary result:** Synthetic CS data + dominant `<ca>` token yields best WER (14.48% ParlamentParla, 16.38% Corts Valencianes)

## Executive Summary
This work tackles the challenge of automatic speech recognition (ASR) for Catalan-Spanish code-switching, where limited training data and linguistic similarities hinder performance. The authors explore three strategies: generating synthetic CS data via TTS, concatenating monolingual audio, and leveraging real CS data with language tokens. Fine-tuning OpenAI's Whisper models on these approaches, they find that combining synthetic CS data with the dominant `<ca>` token achieves the lowest word error rates, outperforming both monolingual concatenation and small real CS datasets. The synthetic approach proves especially effective with only 17 hours of data, making it a promising solution for low-resource CS scenarios.

## Method Summary
The authors investigate three methodologies for improving ASR on Catalan-Spanish code-switching: generating synthetic CS data by translating noun chunks and synthesizing speech, concatenating monolingual audio samples, and leveraging real CS data extracted using a BERT-based detector. They fine-tune OpenAI's Whisper Large v3 model on these datasets, with a focus on the impact of language token conditioning (`<ca>` vs `<es>`) during inference. The synthetic pipeline uses spaCy for chunking, MarianMT for translation, Matcha-TTS for speech synthesis, and audiomentations for noise injection.

## Key Results
- Synthetic CS data + dominant `<ca>` token yields WER of 14.48% on ParlamentParla and 16.38% on Corts Valencianes.
- Synthetic approach outperforms 100 hours of concatenated monolingual audio and 4 hours of real CS data.
- Forcing `<es>` token on Catalan-dominant speech causes WER to spike above 50%.

## Why This Works (Mechanism)

### Mechanism 1
Synthetic code-switching (CS) data generation, combined with language token conditioning, appears to outperform raw monolingual data concatenation for low-resource pairs. The authors propose that synthesizing CS audio by translating noun chunks (using MarianMT) and generating speech (using Matcha-TTS) creates specific "cross-lingual" acoustic patterns. By augmenting this with environmental noise (audiomentations), the model (Whisper) learns to handle intra-sentential switches in a way that randomly concatenated monolingual audio ("Tuples") fails to replicate, likely because the latter lacks semantic coherence.

### Mechanism 2
Enforcing the dominant language token at inference time significantly stabilizes transcription for linguistically similar code-switched pairs. Whisper relies on language tokens to condition the decoder. In Catalan-Spanish CS, where acoustic and linguistic features overlap heavily, forcing the model to decode with the minority language token (<es>) appears to create a "conflict" in the decoder, leading to massive error rates. Constraining the decode to the dominant context language (<ca>) seems to act as a global anchor, allowing the model to "borrow" Spanish phonemes only when necessary without losing the sentence context.

### Mechanism 3
Fine-tuning on targeted extraction of real CS data (using BERT) creates a domain-specific specialist but may lack generalization compared to synthetic augmentation. By fine-tuning a BERT model to detect language tokens and extracting ~5 hours of real CS data from TV3, the model adapts specifically to the acoustic profile of that broadcast domain. However, the limited volume (4 hours training) restricts its ability to generalize to other parliamentary domains (like Corts) as effectively as the broader synthetic approach.

## Foundational Learning

- **Code-Switching (CS) Taxonomy (Inter- vs Intra-sentential)**
  - Why needed here: The paper's synthetic strategy specifically targets intra-sentential switching (translating noun chunks within sentences). Understanding this distinction is required to evaluate why simple audio concatenation (simulating inter-sentential switching) failed to improve performance.
  - Quick check question: Does concatenating two monolingual audio files simulate intra-sentential code-switching?

- **Whisper Prompt Engineering / Token Conditioning**
  - Why needed here: A core finding is that the order and presence of language tokens (<ca>, <es>) in the prompt drastically alter WER. You must understand that Whisper accepts a prompt prefix to condition the decoder.
  - Quick check question: What happens to the WER if you force a Catalan-dominant model to decode with a Spanish token?

- **Text-to-Speech (TTS) Data Augmentation Pipeline**
  - Why needed here: The winning methodology relies on synthesizing audio. One must understand that this involves text manipulation (MarianMT) -> phonemization -> spectrogram generation (Matcha-TTS) -> audio augmentation (noise/clipping).
  - Quick check question: Why is additive noise (audiomentations) added to the TTS output before training?

## Architecture Onboarding

- **Component map:**
  - Base Model: OpenAI Whisper Large v3 (Encoder-Decoder Transformer)
  - Synthetic Generator: spaCy (chunking) -> MarianMT (translation) -> Matcha-TTS (multilingual synthesis) -> Audiomentations (noise injection)
  - CS Detector: Multilingual BERT (fine-tuned for token classification) to extract validation data

- **Critical path:**
  1. Data Prep: Extract noun chunks from Catalan text -> translate to Spanish -> Synthesize + Augment
  2. Training: Fine-tune Whisper Large v3 on 17h Synthetic + 105h Common Voice (optional context)
  3. Inference: Pass audio with prompt prefix `<|startoftranscript|><|ca|><|notimestamps|>`

- **Design tradeoffs:**
  - Synthetic vs. Real: Synthetic allows unlimited data generation but risks "sim-to-real" gap (TTS artifacts). Real CS data is authentic but extremely scarce (only ~5h found).
  - Token Choice: `<ca>` is safe for this dataset but fails if the user intends to speak primarily Spanish. A dynamic language detector might be needed for production.

- **Failure signatures:**
  - Token Mismatch: WER spikes to >50% if the wrong language token is forced (e.g., `<es>` on Catalan audio).
  - Tuple Confusion: Random concatenation of monolingual audio results in higher WER than synthetic data, likely due to prosodic discontinuity confusing the attention mechanism.

- **First 3 experiments:**
  1. Baseline Replication: Run the pre-trained Whisper Large v3 on the ParlamentParla test set with no prefix, then with `<|ca|>` prefix to verify the performance jump noted in Table 2.
  2. Synthetic Validity Check: Generate 1 hour of synthetic CS data using the MarianMT + Matcha-TTS pipeline. Visually inspect spectrograms or listen to 10 random samples to ensure noun-chunk translation sounds natural.
  3. Ablation on Token Order: Fine-tune a model on the Synthetic data and evaluate two decoding passes: one with `<|ca|><|es|>` and one with just `<|ca|>` to confirm the paper's finding that the single dominant token is superior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can aggregating the extracted real-world code-switched data from ParlamentParla, Corts Valencianes, and TV3 into a single fine-tuning set surpass the performance of the synthetic data approach?
- Basis in paper: The authors state in Section 5 that "integrating the code-switched data in ParlamentParla, Corts Valencianes and TV3 Parla for fine-tuning in future research could potentially lead to even better performance."
- Why unresolved: The experiments evaluated datasets separately (Synthetic vs. TV3 vs. Tuples) but did not test a unified model fine-tuned on all extracted real CS data combined.
- What evidence would resolve it: Ablation studies showing WER results for a model fine-tuned on the union of all real extracted CS subsets compared to the synthetic-only baseline.

### Open Question 2
- Question: Does the efficiency of the "noun-chunk translation" synthetic strategy hold for language pairs with lower linguistic similarity or different syntactic structures than Catalan-Spanish?
- Basis in paper: Section 5 notes that the efficiency of using only 17 hours of synthetic data "warrants further exploration for other languages in future experiments."
- Why unresolved: Catalan and Spanish are linguistically similar; it is untested whether this specific low-resource synthetic method works for distinct language pairs (e.g., Catalan-English).
- What evidence would resolve it: Replicating the methodology on a disparate language pair and comparing the performance delta against monolingual baselines.

### Open Question 3
- Question: Why does enforcing the Spanish language token (`<es>`) cause catastrophic failure, and can this prompt bias be corrected?
- Basis in paper: Table 2 shows the `<es>` token causes WER to jump from ~23% to ~51% in fine-tuned models, a degradation the authors describe as the model "struggling" without offering a technical solution.
- Why unresolved: The paper identifies the anomaly but does not determine if the failure is due to training data imbalance, token precedence, or model architecture.
- What evidence would resolve it: Experiments varying token ordering during training or balancing the loss function to penalize errors specifically when the secondary language token is active.

## Limitations
- The "best" model is trained on synthetic CS data, but no human evaluation of synthetic audio naturalness is reported, and the real CS data extracted (~5h) is insufficient for a fair head-to-head comparison.
- All datasets are parliamentary or broadcast speech, which may not reflect spontaneous conversational CS, and the "dominant token" strategy (<ca>) may fail in balanced bilingual or Spanish-dominant contexts.
- The synthetic approach hinges on translating noun chunks, but CS behavior varies by speaker, topic, and region; if Spanish intrusions are not primarily noun-based, the synthetic data may miss critical patterns.

## Confidence
- **High Confidence:** Forcing the wrong language token (<es> on Catalan-dominant speech) causes WER to spike above 50%.
- **Medium Confidence:** Synthetic CS data + dominant token (<ca>) outperforms 100h of concatenated monolingual audio.
- **Low Confidence:** The BERT-based extraction of real CS data is accurate and representative.

## Next Checks
1. **Synthetic Audio Quality Audit:** Generate 20 random samples from the synthetic CS pipeline. Conduct a forced-choice A/B test where annotators identify which clips are TTS-generated vs. real human speech. Compute the "naturalness" score to quantify the sim-to-real gap.
2. **Token Strategy Robustness:** Create a parallel test set by translating ParlamentParla utterances to Spanish-dominant form. Evaluate the "best" model (synthetic + <ca>) on this set to confirm whether the <ca> token strategy fails as predicted when the dominant language is inverted.
3. **CS Logic Ablation:** Modify the synthetic pipeline to translate verb phrases instead of noun chunks. Train a new model on this data and compare WER to the original synthetic model on ParlamentParla to test if noun-chunk translation is the critical factor or if the mere presence of CS is sufficient.