---
ver: rpa2
title: 'LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks
  Training'
arxiv_id: '2601.15079'
source_url: https://arxiv.org/abs/2601.15079
tags:
- graph
- prompt
- quantization
- aggregation
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRAP, a novel method that leverages prompt
  learning to improve the performance of quantized graph neural networks. LoRAP injects
  lightweight, input-dependent prompts into aggregated features to optimize the results
  of quantized aggregations, addressing the limitations of existing prompting strategies
  that only manipulate node features.
---

# LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training

## Quick Facts
- arXiv ID: 2601.15079
- Source URL: https://arxiv.org/abs/2601.15079
- Authors: Chenyu Liu; Haige Li; Luca Rossi
- Reference count: 35
- Primary result: LoRAP consistently enhances low-bit quantized GNN performance across 4 QAT frameworks and 9 datasets, often surpassing full-precision baselines.

## Executive Summary
This paper introduces LoRAP, a novel method that leverages prompt learning to improve the performance of quantized graph neural networks. LoRAP injects lightweight, input-dependent prompts into aggregated features to optimize the results of quantized aggregations, addressing the limitations of existing prompting strategies that only manipulate node features. By combining low-rank decomposition and input-dependent weighting, LoRAP efficiently generates prompts that compensate for quantization errors. Extensive experiments on 4 QAT frameworks and 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing minimal computational overhead. The method bridges the gap between full-precision and quantized models, often surpassing full-precision baselines. Additionally, a fused GPU kernel is proposed to further reduce latency, making LoRAP practical for real-world deployment.

## Method Summary
LoRAP is a post-aggregation prompting technique that generates input-dependent prompts using low-rank decomposition to compensate for quantization errors in GNNs. The method works by dequantizing aggregated features, generating prompt weights through a shared linear layer and softmax, constructing prompts as weighted combinations of low-rank basis matrices, adding the prompts to the dequantized features, and re-quantizing the result. The approach is compatible with existing QAT frameworks (QAT, Degree-Quant, A2Q, MixQ) and introduces minimal parameter overhead. A fused GPU kernel is also proposed to reduce inference latency. The method is evaluated across multiple datasets and QAT frameworks, demonstrating consistent accuracy improvements over quantized baselines and often surpassing full-precision performance.

## Key Results
- LoRAP consistently improves INT4 quantized GNN accuracy across 4 QAT frameworks and 9 datasets
- The method often surpasses full-precision baseline performance while maintaining minimal computational overhead
- A fused GPU kernel reduces latency by approximately 2x compared to unfused implementation
- Low-rank decomposition enables efficient parameterization with optimal rank typically between 4-8

## Why This Works (Mechanism)

### Mechanism 1
Injecting prompts after aggregation decouples the error correction objective from the graph topology. In standard "node prompting" (pre-aggregation), a prompt $P$ must transit through the adjacency matrix $A$, requiring the model to learn a complex "pre-inverse" transformation ($A(X+P)$) to correct the error. LoRAP applies prompts to the aggregated feature directly ($AX + P$). This allows the prompt $P$ to directly target the propagated quantization error $A\epsilon_X$ without being modulated by $A$, simplifying the optimization landscape.

### Mechanism 2
Post-aggregation prompting enables targeted correction of systematic bias for high-degree nodes. In GNNs, high-degree nodes aggregate many messages, often accumulating significant systematic bias. Pre-aggregation prompts affect neighbors, making targeted self-correction difficult (a "credit assignment" problem). Post-aggregation prompts modify the node's final summed feature directly, allowing the model to learn a specific bias offset for that node without influencing the aggregation of its neighbors.

### Mechanism 3
Low-rank decomposition parameterizes the prompt space to efficiently span the diverse error distributions across nodes. Instead of a single static prompt or a full-rank matrix (computationally expensive), LoRAP generates prompts as a weighted combination of low-rank basis matrices ($P_A P_B$). The weights are derived dynamically from the aggregated features via a softmax layer. This creates an "input-dependent" prompt capable of adapting to specific local errors while maintaining parameter efficiency.

## Foundational Learning

- **Message Passing (Aggregation vs. Update)**
  - Why needed: LoRAP acts specifically between the aggregation phase and the update phase. You must understand where information flows to know where to inject the prompt.
  - Quick check: In a standard GNN layer, if you inject a prompt before the `aggregate` function, which nodes' representations change? If you inject it after, which nodes change?

- **Quantization-Aware Training (QAT) & Fake Quantization**
  - Why needed: The paper targets INT4 quantization. The mechanism relies on differentiating through the quantization operator (STE) to train prompts that compensate for the precision loss.
  - Quick check: Why can't we simply train a prompt on a full-precision model and expect it to work perfectly on a 4-bit quantized model without QAT-style fine-tuning?

- **Low-Rank Matrix Factorization ($U \Sigma V^T$)**
  - Why needed: The "Low-Rank" in LoRAP refers to the parameterization $P_A P_B$. Understanding this helps diagnose why the parameter count is low and what the "rank" hyperparameter $r$ controls (the expressivity of the prompt).
  - Quick check: If rank $r=1$, what does that imply about the diversity of the prompts generated for different nodes?

## Architecture Onboarding

- **Component map:** Quantized Node Features ($X_q$) -> Standard Aggregation (e.g., Sum/Mean) -> Dequantize -> LoRAP Module (Weight Generation + Prompt Construction) -> Add Prompt -> Quantize -> Standard GNN Update

- **Critical path:** The gradient flow through the LoRAP Module. During backprop, gradients flow through the re-quantization node (using Straight-Through Estimator) into the prompt generation weights ($P_A, P_B, \phi$). If the fused kernel is used, ensure gradients are still correctly computed for the prompt parameters.

- **Design tradeoffs:**
  - Rank ($r$) vs. Accuracy: Higher rank allows better error approximation but increases parameter count ($L \times r \times (k+d)$)
  - Latency vs. Recovery: Unfused LoRAP adds 93.5μs overhead; Fused kernel reduces this to 44.5μs
  - Prompt Count ($k$) vs. Complexity: More basis prompts ($k$) allow finer-grained error correction but require a larger projection matrix

- **Failure signatures:**
  - Accuracy Collapse: If learning rate is too high for the prompt weights, the "correction" may diverge, destabilizing the GNN
  - High Latency: If the fused kernel is not installed/enabled, the sequential kernel launches (GEMM -> Softmax -> Add) will dominate inference time
  - Spectral Bias: If the rank $r$ is too low, the model may correct low-frequency errors but fail on high-frequency noise

- **First 3 experiments:**
  1. Sanity Check (FP32 vs INT4): Train a standard GNN (FP32), then quantize to INT4. Confirm the accuracy drop (baseline). Apply LoRAP and verify recovery.
  2. Ablation (Rank vs. Accuracy): Sweep rank $r \in \{1, 2, 4, 8\}$ on a fixed dataset to find the "elbow" where parameter efficiency saturates.
  3. Latency Benchmark: Measure single-layer latency for "INT4 + LoRAP (Unfused)" vs "INT4 + LoRAP (Fused)" vs "FP32". Verify the speedup claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can LoRAP maintain its efficacy when applied in a strictly parameter-efficient manner with frozen backbone weights? The authors deliberately avoid freezing quantized GNN weights, contrasting with typical PEFT strategies. It remains unclear if performance gains are primarily due to prompt compensation or joint optimization of backbone weights. Ablation studies comparing joint training against frozen backbone setups would resolve this.

### Open Question 2
Is LoRAP applicable to Post-Training Quantization (PTQ) settings where full backpropagation is infeasible? The methodology relies on backpropagation through QAT frameworks, but the paper distinguishes QAT from PTQ as requiring only small calibration sets. Since LoRAP requires gradient-based optimization of prompt matrices, its utility in data-limited PTQ settings is unknown. Evaluation on pre-trained, frozen quantized models using small calibration datasets would clarify this.

### Open Question 3
Is there a principled heuristic to determine the optimal number of prompt bases ($k$) and rank ($r$) without dataset-specific manual tuning? Section 5 mentions these are "subjected to tuning," and Appendix A.6 notes the optimal trade-off depends on performance and computational budget. The experimental results show varying optimal configurations, suggesting sensitivity that could hinder plug-and-play deployment. An analysis correlating graph structural properties with optimal $k$ and $r$ values would help establish adaptive selection rules.

## Limitations
- The core assumptions about quantization error (additive, low-rank recoverable) are primarily validated through synthetic analysis rather than systematic error distribution analysis
- The proposed fused GPU kernel's performance claims are based on specific infrastructure implementation details that may not generalize across hardware
- The paper doesn't provide a principled method for determining the optimal rank hyperparameter for different datasets or GNN architectures

## Confidence

- **High confidence:** Empirical performance claims across 9 datasets and 4 QAT frameworks; latency improvements from fused kernel implementation; basic architectural design of post-aggregation prompting
- **Medium confidence:** Claims about targeted correction capability for high-degree nodes; effectiveness of low-rank decomposition for parameter efficiency
- **Low confidence:** Generalization of additive error assumption; optimality of rank hyperparameter selection; real-world deployment latency benefits

## Next Checks

1. **Error Distribution Analysis:** Conduct systematic analysis of quantization error distributions across different graph datasets and quantization levels to validate the additive error assumption and low-rank structure claim.

2. **Cross-Architecture Testing:** Evaluate LoRAP on GNN architectures beyond GCN/GIN (e.g., GAT, GraphSAGE, Transformer-based GNNs) to assess generalizability across different aggregation mechanisms.

3. **Hardware Profiling:** Measure end-to-end latency and memory usage on different GPU architectures (e.g., NVIDIA A100, AMD MI250) to validate the claimed performance benefits and identify potential bottlenecks in real deployment scenarios.