---
ver: rpa2
title: 'FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation'
arxiv_id: '2510.13598'
source_url: https://arxiv.org/abs/2510.13598
tags:
- generation
- table
- data
- pages
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces FreshTab, a dynamic benchmark generation method
  for table-to-text tasks using recent Wikipedia tables to avoid LLM training data
  contamination and enable domain-specific evaluation. FreshTab collects datasets
  in multiple languages (English, German, Russian, French, Swedish, Dutch, Spanish)
  and assigns domain and logical operation labels.
---

# FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation

## Quick Facts
- **arXiv ID**: 2510.13598
- **Source URL**: https://arxiv.org/abs/2510.13598
- **Reference count**: 20
- **One-line primary result**: FreshTab enables domain-balanced, LLM-training-data-free table-to-text evaluation with moderate LLM-judge correlation to human judgments

## Executive Summary
FreshTab introduces a dynamic benchmark generation method that sources recent Wikipedia tables (Feb-May 2025) to evaluate table-to-text generation while avoiding LLM training data contamination. The approach uses Wikidata SPARQL queries to identify tables created after model knowledge cutoffs, enabling evaluation on truly novel content. FreshTab provides datasets in multiple languages with domain and logical operation labels, addressing the critical issue of benchmark contamination that inflates LLM performance scores on static datasets.

Experiments comparing FreshTab to the earlier LoTNLG benchmark reveal that recent LLMs perform worse on FreshTab based on automatic metrics like TAPEX, but this performance gap disappears in LLM-judge and human evaluations. Domain-balanced FreshTab data proves more challenging than sport-heavy LoTNLG data, and automatic metrics show poor correlation with human judgments while LLM-as-a-judge achieves moderate correlation (0.53). The results demonstrate that FreshTab enables more reliable and diverse benchmarking for table-to-text generation tasks.

## Method Summary
FreshTab generates dynamic benchmarks by querying Wikidata for pages created after configurable cutoff dates or covering events occurring after those dates. The pipeline scrapes, cleans, and filters tables (with size limits and empty cell thresholds), then assigns domain labels (sport, politics, culture, other) and samples logical operation labels per table. Evaluation uses Direct CoT and Choice prompting with temperature 0.7, generating structured outputs via Ollama with 8-bit quantization. The method supports multilingual datasets (English, German, Russian, French, Swedish, Dutch, Spanish) and reference-free evaluation using TAPEX and LLM-as-a-judge, with optional human annotation via Factgenie.

## Key Results
- FreshTab.diverse data are more challenging than sport-heavy LoTNLG data, with domain effects stronger than freshness effects
- Automatic metrics show low correlation with human judgments, while LLM-as-a-judge achieves moderate correlation (0.53)
- Recent LLMs perform worse on FreshTab based on automatic metrics, but this gap disappears in LLM-judge and human evaluations
- Domain-specific effects are visible across all evaluation methods (TAPEX, LLM-judge, human)

## Why This Works (Mechanism)

### Mechanism 1: Contamination Avoidance Through Temporal Filtering
FreshTab avoids LLM training data contamination by sourcing tables published after model knowledge cutoff dates. The pipeline queries Wikidata via SPARQL for pages created after a configurable cutoff date or covering events occurring after that date. Since the content did not exist during pretraining, models cannot rely on memorization. Core assumption: LLMs cannot have memorized tables that were created after their training cutoff. Evidence: "based on recent tables collected by our method...newer than the LLM's knowledge cutoff date" and "pages on events taking place between the cutoff date and the present." Break condition: If Wikipedia pages are translations of older articles from other languages, novelty may be compromised.

### Mechanism 2: Reference-Free Automatic Metrics and Human Evaluation Divergence
Reference-free automatic metrics (TAPEX, TAPAS) show low correlation with human judgments because they penalize valid but simpler outputs. TAPEX is a table entailment model that assesses whether generated text is entailed by the table. When models produce simpler insights on unfamiliar data, TAPEX may score them lower even if they are factually correct. Core assumption: TAPEX performance reflects entailment accuracy, not hallucination rate. Evidence: "insights generated by LLMs from recent tables collected by our method appear clearly worse by automatic metrics, but this does not translate into LLM and human evaluations" and "TAPEX performs better on simple logical operations than on more complex ones." Break condition: If the task requires complex aggregation or superlative insights, simpler outputs—while accurate—may not meet task requirements.

### Mechanism 3: Domain Balance Reveals Capability Limitations
Domain-balanced benchmarks are more challenging and reveal broader capability limitations than domain-skewed benchmarks. Prior benchmarks like LoTNLG are sport-heavy (73% sport tables). Models may overfit to common sports patterns. FreshTab.diverse balances four domains (sport, politics, culture, other), forcing broader reasoning. Core assumption: Domain-specific patterns are learnable and transfer less across domains. Evidence: "Domain-balanced FreshTab data are more challenging than the sport-heavy LoTNLG data" and "the domain change (lot vs. diverse) having a stronger effect than the freshness of the tables." Break condition: If model training data has heavy coverage across all target domains, domain balance effects may diminish.

## Foundational Learning

- **Table-to-Text Generation**: The core task is generating natural language insights from structured tabular data. Understanding this clarifies why evaluation is difficult—there are many valid outputs for one table. Quick check: Given a sports results table, can you identify three different valid insights using aggregation, comparison, and superlative operations?

- **Benchmark Contamination**: LLMs memorize training data, inflating performance on public benchmarks. FreshTab's purpose is to evade this by design. Quick check: If an LLM was trained on Wikipedia dumps from 2023, why would testing it on February-May 2025 tables yield more trustworthy scores?

- **LLM-as-a-Judge Evaluation**: The paper relies on LLM-judge for scalable evaluation, achieving moderate correlation (0.53) with humans. Understanding its limitations is critical for interpreting results. Quick check: Why might a Llama-70B judge correlate better with human judgments than TAPEX, but still not reach near-perfect correlation?

## Architecture Onboarding

- **Component map**: Wikidata SPARQL Query Module -> Table Scraper & Cleaner -> Label Assignment -> Evaluation Pipeline -> Configuration Layer
- **Critical path**: 1. Define cutoff date → 2. Execute SPARQL queries → 3. Scrape and clean tables → 4. Assign labels → 5. Run model inference → 6. Evaluate outputs → 7. Aggregate metrics per domain/model
- **Design tradeoffs**: No human reference texts (enables on-the-fly generation but prevents use of reference-based metrics); Random logical operation sampling (provides controllability but may not match natural insight distribution); 8-bit quantization via Ollama (reduces hardware costs but introduces quantization artifacts)
- **Failure signatures**: Empty outputs (reasoning models produce 5-10% empty outputs on diverse data); Table misinterpretation (models misread column labels, subtables, or row/column order); Inconsistent claims (outputs contain self-contradictory assertions)
- **First 3 experiments**: 1. Reproduce the TAPEX gap between LoTNLG and FreshTab.en.lot for one model (e.g., Llama 3.3 70B) to validate your pipeline; 2. Run LLM-as-a-judge on the same outputs and compute Pearson correlation with TAPEX scores to confirm metric divergence; 3. Extend to a new language (e.g., German or French) using FreshTab's multilingual pipeline and compare domain-specific TAPEX scores to English

## Open Questions the Paper Calls Out

- **Cross-Lingual Translation Impact**: To what extent does cross-lingual translation of Wikipedia articles compromise the "freshness" and uncontaminated nature of non-English FreshTab benchmarks? The authors note that "Some of the data novelty effect may have been compromised by new articles being only translated from another language... but needs to be further evaluated." This matters because if an LLM's training data included the original English version of a table, it may have effectively "seen" the content even if the specific French or German Wikipedia page was created after the cutoff date.

- **Automatic Metric vs. Human Evaluation Divergence**: Why do reference-free automatic metrics like TAPEX indicate a significant performance drop on fresh data while human and LLM-judge evaluations do not? The paper notes that "insights generated by LLMs from recent tables... appear clearly worse by automatic metrics, but this does not translate into LLM and human evaluations." This discrepancy suggests a fundamental bias in current automatic metrics regarding novel data or domain shifts.

- **Prompting Strategy Optimization**: How can prompting strategies be optimized to reduce high failure rates in reasoning models and prevent the bias toward "simple" logical operations on diverse domains? The authors identify prompting optimization as future work, noting "the current prompting strategy could be refined" and observing that "reasoning LLMs... run into a dead end" while other models over-produce "simple" insights.

## Limitations
- Table novelty verification is limited as translated articles from older sources may still appear novel in Wikipedia metadata, compromising contamination avoidance
- Exact prompt templates for Direct CoT and Choice prompting setups are not provided, preventing exact reproduction
- Structured output JSON schema used for constrained generation via Ollama is not specified
- No human reference texts are generated, preventing use of reference-based metrics (BLEU, ROUGE)

## Confidence

- **High Confidence**: Domain balance claim is well-supported by reported domain distribution (73% sport in LoTNLG vs balanced domains in FreshTab) and statistically significant performance differences (p ≤ 0.05)
- **Medium Confidence**: Low correlation between automatic metrics and human judgments is supported by reported Pearson correlation of 0.11, but lacks direct corpus evidence for table-to-text specific metric-human divergence
- **Low Confidence**: Contamination avoidance mechanism's effectiveness is assumed based on date filtering but lacks empirical verification that collected tables are truly absent from training data

## Next Checks

1. **Cross-Lingual Contamination Analysis**: Examine Russian, German, and French FreshTab datasets for evidence of translated content from older sources. Run semantic similarity checks between FreshTab tables and older Wikipedia dumps to quantify contamination risk across languages.

2. **Prompt Template Impact Study**: Replicate the FreshTab generation pipeline using multiple prompt variations (including variations in reasoning depth, output format, and temperature) to measure the sensitivity of TAPEX scores and LLM-judge correlations to prompt design choices.

3. **Domain Transfer Experiment**: Train or fine-tune a model on FreshTab.diverse data and test on both FreshTab.diverse and LoTNLG datasets to empirically measure domain generalization effects and validate whether domain-balanced training improves cross-domain performance.