---
ver: rpa2
title: 'Towards Efficient Agents: A Co-Design of Inference Architecture and System'
arxiv_id: '2512.18337'
source_url: https://arxiv.org/abs/2512.18337
tags:
- agent
- context
- reasoning
- latency
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AgentInfer is a unified framework for accelerating autonomous
  agents by addressing end-to-end latency across reasoning, context, and execution
  stages. It integrates four synergistic components: AgentCollab uses hierarchical
  dual-model reasoning with self-evaluation-driven escalation; AgentCompress performs
  lightweight context summarization to manage memory growth; AgentSched implements
  cache-aware scheduling to optimize KV cache reuse; and AgentSAM employs suffix automaton-based
  speculative decoding to accelerate token generation using cross-session memory.'
---

# Towards Efficient Agents: A Co-Design of Inference Architecture and System

## Quick Facts
- **arXiv ID**: 2512.18337
- **Source URL**: https://arxiv.org/abs/2512.18337
- **Reference count**: 40
- **One-line result**: AgentInfer achieves 1.8-2.5× end-to-end speedup and >50% ineffective token reduction in Deep Research Agents

## Executive Summary
AgentInfer addresses the challenge of accelerating LLM-based autonomous agents operating in Think–Act–Observe loops. The framework co-designs four synergistic components: AgentCollab balances large- and small-model usage through self-evaluation-driven escalation; AgentCompress manages memory growth via lightweight context summarization; AgentSched optimizes scheduling using cache-aware hybrid policies; and AgentSAM accelerates token generation through cross-session memory-enhanced speculative decoding. Evaluated on BrowseComp-zh and DeepDiver benchmarks, AgentInfer demonstrates significant end-to-end latency improvements while preserving task accuracy.

## Method Summary
AgentInfer is a unified framework accelerating autonomous agents by optimizing the full end-to-end latency across reasoning, context, and execution stages. The method employs four synergistic components: (1) AgentCollab - a hierarchical dual-model reasoning framework that dynamically routes between large and small models based on self-evaluation signals; (2) AgentCompress - lightweight context summarization that filters and distills search results to manage memory growth; (3) AgentSched - cache-aware scheduling with shadow-price controller switching between SJF and KV-aware modes; and (4) AgentSAM - suffix automaton-based speculative decoding enhanced with cross-session semantic memory. The system was evaluated using openPangu-DeepDiverV2 models on BrowseComp-zh benchmark, demonstrating 1.8-2.5× end-to-end speedup while reducing ineffective token consumption by over 50%.

## Key Results
- 1.8-2.5× end-to-end speedup on BrowseComp-zh benchmark while preserving accuracy
- 50%+ reduction in ineffective token consumption during agent reasoning
- 72% KV cache hit rate (vs. 63% FCFS, 58% SJF) with 0.904× end-to-end latency
- 26% LLM latency reduction using cross-session semantic memory in AgentSAM

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-evaluation-driven escalation preserves large-model accuracy while reducing its usage to bottleneck segments only.
- **Mechanism:** After each Think step, the active model emits a structured PROGRESS block with binary judgment. When the small model signals stagnation (value=FALSE), the controller escalates to the large model; once progress resumes (value=TRUE), it de-escalates. This concentrates expensive capacity on genuinely hard segments.
- **Core assumption:** Progress-check signals reliably correlate with actual trajectory quality; models can self-diagnose stagnation accurately.
- **Evidence anchors:**
  - [abstract] "AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment"
  - [Section 5.1.1] "When <value> is FALSE, this self-diagnostic signal is interpreted as a sign of difficulty... and the session is escalated to ML"
  - [Table 4] AgentCollab achieves 33.8% accuracy (vs. 34.61% large-model-only) with 1.32× speedup
- **Break condition:** If progress-check signals become noisy or if tasks have uniformly high difficulty (few "easy" segments), escalation frequency approaches large-model-only baseline, negating efficiency gains.

### Mechanism 2
- **Claim:** Shadow-price-based hybrid scheduling improves both latency and cache hit rate by dynamically balancing SJF and KV-aware modes.
- **Mechanism:** The scheduler monitors effective usable capacity (U) and total new demand (D). When D > U (tight capacity), shadow price λ increases, penalizing requests requiring new cache blocks and rewarding cache hits. When D ≤ U, λ approaches zero and SJF behavior dominates.
- **Core assumption:** The D/U ratio meaningfully predicts cache pressure; smooth transitions prevent thrashing between modes.
- **Evidence anchors:**
  - [Section 5.3] "The shadow-price λ translates this binary state into a continuous control signal"
  - [Table 10] AgentSched achieves 72% KV cache hit rate (vs. 63% FCFS, 58% SJF) with 0.904× end-to-end latency
  - [corpus] Related work on cache-aware dispatching (avg neighbor FMR=0.44) supports viability but no direct validation of shadow-price mechanism
- **Break condition:** If workload has minimal context variance (all short or all long requests), hybrid switching provides no advantage over fixed policy.

### Mechanism 3
- **Claim:** Cross-session semantic memory enhances speculative decoding hit rates by expanding the SAM corpus beyond current context.
- **Mechanism:** AgentSAM constructs a composite suffix automaton from (1) current session context and (2) top-K semantically similar historical contexts retrieved via dense/sparse retrieval. Merged SAM provides more matching subsequences for speculative proposals.
- **Core assumption:** Semantically similar queries produce similar token patterns; cross-session retrieval identifies genuinely useful continuations.
- **Evidence anchors:**
  - [Section 5.4.2] "By initializing SAM with contextually relevant tokens drawn from prior agent interactions... AgentSAM achieves a higher speculative hit rate"
  - [Table 11] AgentSAM reduces LLM latency by 26% (vs. 20.7% for session-only SAM)
  - [Figure 5] OTE and SHR improve with longer contexts and cross-session memory
- **Break condition:** If queries are highly novel or diverse (low inter-session similarity), retrieved contexts add noise without useful patterns, potentially degrading hit rates.

## Foundational Learning

- **Concept: Think–Act–Observe execution loops**
  - **Why needed here:** AgentInfer optimizes the full multi-turn reasoning lifecycle, not isolated inference. Each iteration accumulates latency and context.
  - **Quick check question:** Can you trace where inference latency, tool execution, and context growth each contribute to end-to-end delay in a 10-turn research agent?

- **Concept: KV-cache persistence and prefill costs**
  - **Why needed here:** AgentSched's design hinges on the cost of cache eviction—recomputing 32K+ token prefixes causes multi-second latency spikes.
  - **Quick check question:** Explain why SJF scheduling, despite reducing average latency, can increase total compute for multi-turn agents.

- **Concept: Speculative decoding with draft-then-verify**
  - **Why needed here:** AgentSAM extends standard speculative decoding by enriching the draft source with cross-session memory.
  - **Quick check question:** What two conditions determine whether speculative decoding provides net benefit? (Hint: see Section 5.4.3 adaptive switch.)

## Architecture Onboarding

- **Component map:** Request arrives -> AgentCollab determines model tier -> AgentSAM speculates during Think (if SAM ready) -> AgentCompress filters search results, triggers async compression -> AgentSched selects next request -> Loop until termination

- **Critical path:**
  1. Request arrives → AgentCollab determines model tier
  2. During Think: AgentSAM speculates (if SAM ready), falls back otherwise
  3. After Act: AgentCompress filters search results, triggers async compression
  4. AgentSched selects next request based on λ-controlled scoring
  5. Loop until termination

- **Design tradeoffs:**
  - Compression ratio vs. reasoning continuity (Table 9: removing reasoning traces increases turns by 26%)
  - SAM construction latency vs. speculation availability (sync: +1.4s TTFT; async: minimal TTFT impact)
  - SJF responsiveness vs. cache preservation (Table 10: SJF improves latency but degrades hit rate)

- **Failure signatures:**
  - Accuracy collapse with aggressive compression (omitting reasoning memory → 1.8× latency increase, Section 5.2)
  - Quantization trap: INT8 improves TPS but increases time-to-solution by 70% due to retry loops (Table 1)
  - Speculation thrashing: enabling SAM at high batch sizes slows inference (Section 5.4.3 adaptive switch)

- **First 3 experiments:**
  1. **AgentCollab ablation:** Run BrowseComp-zh with (a) large-only, (b) small-only, (c) AgentCollab. Measure accuracy vs. speedup tradeoff frontier. Validate Table 4 reproduction.
  2. **AgentSched stress test:** Inject mixed short/long requests with 128K max sequence length. Monitor λ oscillation, cache hit rate, and whether long requests starve under different thresholds.
  3. **AgentSAM hit rate analysis:** Compare three SAM sources—current-context-only, session-level, cross-session—across varying prompt lengths. Plot SHR vs. context length to validate Figure 5 trends.

## Open Questions the Paper Calls Out

- **Question:** How robust is AgentCollab's self-evaluation-driven escalation when models produce incorrect Progress Check signals (false positives claiming progress when stagnating, or false negatives triggering unnecessary escalation)?
- **Question:** To what extent do the AgentInfer optimizations generalize beyond Deep Research Agents to other agentic workloads (coding agents, dialogue agents, robotics)?
- **Question:** Can the shadow-price controller in AgentSched be extended to handle heterogeneous hardware with varying KV-cache capacities and memory bandwidths?
- **Question:** What is the optimal trade-off between compression ratio and trajectory stability when AgentCompress is applied to agents with longer reasoning horizons (>50 turns)?

## Limitations

- Evaluation limited to single Chinese benchmark (BrowseComp-zh) and one agent task (Deep Research), raising generalizability concerns
- No rigorous validation of cross-session memory retrieval quality or semantic relevance in AgentSAM
- Shadow-price mechanism lacks empirical validation of dynamic behavior under varying workloads
- Model quality and latency contributions not separated from system-level gains

## Confidence

- **High Confidence**: AgentCompress's role in reducing context growth and AgentCollab's hierarchical dual-model design with self-evaluation signals are well-supported by ablation studies (Tables 4, 9) and the mechanism is clearly specified.
- **Medium Confidence**: AgentSched's shadow-price-based hybrid scheduling shows strong cache hit improvements (72% vs 63% FCFS), but the theoretical link between D/U ratio and cache pressure, and the smoothness of λ transitions, are not empirically validated under stress.
- **Low Confidence**: AgentSAM's cross-session memory integration lacks rigorous retrieval quality assessment. While Table 11 shows latency gains, the semantic relevance of retrieved contexts is not verified, and the impact of noisy retrievals on speculation hit rate is not explored.

## Next Checks

1. **Retrieval Quality Audit for AgentSAM**: Measure the semantic similarity between retrieved cross-session contexts and the current query using embedding-based metrics (e.g., cosine similarity). Test whether high-similarity retrievals correlate with higher speculative hit rates, and whether low-similarity retrievals degrade performance.

2. **Shadow-Price Dynamics under Load**: Instrument AgentSched to log λ values, D/U ratios, and cache hit rates over time under varying workloads (short/long request mixes). Validate that λ increases smoothly with D/U and that switching between SJF and KV-aware modes reduces thrashing.

3. **Generalization Stress Test**: Apply AgentInfer to a non-Chinese, non-web-search task (e.g., code generation or multi-hop reasoning in English). Measure whether the same efficiency gains (1.8–2.5× speedup, >50% ineffective token reduction) hold, and whether prompt compression triggers and self-evaluation signals remain effective.