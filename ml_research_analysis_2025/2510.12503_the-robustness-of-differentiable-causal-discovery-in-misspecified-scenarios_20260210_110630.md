---
ver: rpa2
title: The Robustness of Differentiable Causal Discovery in Misspecified Scenarios
arxiv_id: '2510.12503'
source_url: https://arxiv.org/abs/2510.12503
tags:
- causal
- discovery
- nodes
- data
- differentiable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Robustness of Differentiable Causal Discovery in Misspecified Scenarios

## Quick Facts
- **arXiv ID:** 2510.12503
- **Source URL:** https://arxiv.org/abs/2510.12503
- **Reference count:** 40
- **Primary result:** Differentiable causal discovery methods show robust performance under missing data but fail under scale variation due to Var-Sortability breakdown.

## Executive Summary
This paper benchmarks 12 causal discovery algorithms across 8 misspecified scenarios, revealing that differentiable methods (e.g., NOTEARS, DAGMA) are surprisingly robust to missing data but fail catastrophically under scale variation. The authors propose that linear differentiable methods maintain performance under missingness due to constant noise ratios, while failing under scale variation because variance scaling disrupts the correlation between causal order and node variance (Var-Sortability). Nonlinear differentiable methods also struggle with scale variation, though linear methods unexpectedly perform well on nonlinear data. The study highlights critical limitations and suggests future research directions for temporal causal discovery and scale-invariant nonlinear methods.

## Method Summary
The paper evaluates 12 causal discovery algorithms (constraint, score, functional, and differentiable) across 8 misspecified scenarios (e.g., latent confounders, measurement error, scale variation) using synthetic linear and nonlinear SEMs (ER and SF graphs, 10–50 nodes, 2000 samples) and the Sachs real-world dataset. Performance is measured via Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Differentiable methods use Augmented Lagrangian optimization to enforce DAG constraints, with hyperparameters tuned via grid search. The benchmark systematically corrupts data with specific violations and compares algorithmic robustness, revealing that differentiable methods excel in messy scenarios but fail under standardized data.

## Key Results
- Differentiable methods (NOTEARS, DAGMA) achieve SHD ~1.5 on linear vanilla data but degrade significantly under scale variation (SHD doubles/triples).
- Linear differentiable methods maintain robustness under MCAR missingness due to constant noise ratios.
- Nonlinear differentiable methods (NOTEARS-MLP, GraN-DAG) also fail under scale variation, with no established scale-invariant remedy.
- Linear methods surprisingly perform well on nonlinear data, contradicting expectations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting combinatorial DAG constraints into smooth equality constraints allows gradient-based methods to optimize graph structure using standard continuous solvers.
- **Mechanism:** The algorithm transforms the discrete acyclicity constraint ($G \in DAG$) into a differentiable form, typically $h(W(G)) = 0$ (e.g., $Tr(e^{W \circ W}) - d = 0$). This allows the use of the Augmented Lagrangian Method (ALM) to solve the optimization problem iteratively, balancing the score function (fit) against the DAG constraint.
- **Core assumption:** The equality constraint $h(W(G)) = 0$ must be an exact proxy for acyclicity, and the resulting optimization landscape must be navigable via gradient descent.
- **Evidence anchors:**
  - [Section 2.3]: "NOTEARS... convert combinatorial acyclic constraints into smooth equality constraints and solve the optimization by transforming... into unconstrained optimization through the augmented Lagrangian method."
  - [Section 3.3]: Mentions tuning hyperparameters like the Lagrange multiplier penalty to control this process.
  - [Corpus]: Weak support; corpus neighbors focus on general differentiable discovery but lack specific details on the ALM mechanism in this context.
- **Break condition:** If the optimization falls into local minima where the DAG constraint is satisfied but the graph structure is incorrect (common in scale-variant scenarios), the mechanism fails.

### Mechanism 2
- **Claim:** Linear differentiable methods maintain robustness in "Missing Completely At Random" (MCAR) scenarios because the relative noise ratio between variables remains theoretically constant.
- **Mechanism:** In linear models, the identifiability of the DAG depends on the noise ratio $r = \max(\sigma^2)/\min(\sigma^2)$. Under MCAR, rows are deleted randomly; while sample size drops, the variance of the underlying noise terms ($\sigma^2$) and thus the ratio $r$ does not change. Consequently, the theoretical guarantee (minimizing least squares returns the true DAG) remains intact.
- **Core assumption:** The data must adhere to the linear Gaussian equal noise variance assumption (or a bounded ratio $r < 1 + \xi/d$).
- **Evidence anchors:**
  - [Section 4.1.2]: "Under the MCAR case... The noise ratio $r=1$ remains constant before and after data imputation. This explains the superior performance..."
  - [Section 3.1.1]: Defines the MCAR mechanism as random deletion following a Bernoulli distribution.
  - [Corpus]: N/A.
- **Break condition:** If the missingness mechanism is Not Missing At Random (NMAR), the noise ratio or covariance structure might shift, breaking the assumption and potentially degrading performance.

### Mechanism 3
- **Claim:** Linear differentiable methods fail under "Scale Variation" because they implicitly rely on "Var-Sortability," confusing variance scaling with causal order.
- **Mechanism:** These methods often use Least Squares (L2) loss. Prior work (Reisach et al., 2021) shows L2 minimization tends to order variables by increasing marginal variance. If input data is standardized (scale variation), the natural variance order aligning with the causal order is destroyed or inverted, leading the optimization to converge to incorrect DAGs.
- **Core assumption:** The causal order must be correlated with the variance of the variables (Var-Sortability) for the L2 loss to effectively guide the search.
- **Evidence anchors:**
  - [Section 4.1]: "The explanation for this is that... after the causal direct effect of $X_i \to X_k$ cancels out, the variance of node $X_k$ changes significantly. This reduces the Var-Sortability..."
  - [Table 2.2]: Shows SHD/SID scores for NOTEARS/GOLEM doubling or tripling in the "Scale-variant" column compared to "Vanilla".
  - [Corpus]: N/A.
- **Break condition:** If data normalization or scaling destroys the correlation between node variance and causal ordering, the loss function provides a misleading gradient signal.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & SEMs**
  - **Why needed here:** The paper evaluates performance based on recovering the "true DAG" generated by a specific SCM (Eq 1). Understanding how coefficients ($U(-2, -0.5)$) and noise distributions ($N(0,1)$) generate data is crucial for interpreting "Vanilla" vs. "Misspecified" scenarios.
  - **Quick check question:** Can you explain how the "noise ratio" $r$ changes if you add measurement error $\epsilon_i$ to a variable $X_i$?

- **Concept: DAG Constraints & Acyclicity**
  - **Why needed here:** The core innovation of differentiable discovery is approximating this constraint (Section 2.3). You must understand what a Directed Acyclic Graph is to evaluate if the algorithm has correctly identified the structure or merely found a cyclic graph with low loss.
  - **Quick check question:** Why is a cyclic graph problematic for causal inference (intervention)?

- **Concept: Evaluation Metrics (SHD & SID)**
  - **Why needed here:** The paper uses Structural Hamming Distance (SHD) and Structural Intervention Distance (SID) to quantify "robustness." You need to distinguish between structural similarity (SHD) and the correctness of causal effects (SID).
  - **Quick check question:** If an algorithm correctly identifies edges but reverses their direction, which metric (SHD or SID) would likely be more penalized?

## Architecture Onboarding

- **Component map:** Synthetic Dataset (ER or SF graphs) -> Preprocessing (Optional scaling/missingness injection) -> Differentiable Solver (Score Function $L_{rec}$ + Constraint $h(W)$) -> Augmented Lagrangian loop updating $W$ -> Weighted Adjacency Matrix $W$ -> Thresholding -> Evaluator (SHD/SID calculation against Ground Truth $W^*$).
- **Critical path:** The optimization loop (Section 2.3, Eq 4). The model iteratively updates $W$ to minimize $L_{rec}$ while the Lagrange multipliers $\alpha_t$ and $\mu_t$ enforce the DAG constraint $h(W) \approx 0$.
- **Design tradeoffs:**
  - **Linear vs. Non-Linear:** Linear models (NOTEARS) are faster but fail on non-linear data (Mechanism Violation). Non-linear models (NOTEARS-MLP) are flexible but computationally heavier and still sensitive to scale variation (Table 4.2).
  - **Robustness vs. Precision:** The paper suggests differentiable methods are robust to "Messy" data (missing/confounding) but brittle to "Standardized" data (scale variation).
- **Failure signatures:**
  - **Scale Variation:** Look for a massive spike in SHD/SID in Table 2.2 & 3.2. The graph often becomes fully connected or acyclic but structurally inverted.
  - **Mechanism Mismatch:** If you feed linear data to a non-linear model (or vice versa), performance degrades, though interestingly, linear methods handle non-linear data better than expected (Table 2.2).
- **First 3 experiments:**
  1. **Baseline Reproduction:** Run NOTEARS on the "Linear Vanilla" model (Section 3.1) to ensure you can achieve the low SHD (~1.5 in Table 2.1) stated in the paper.
  2. **Stress Test (Scale):** Introduce the "Scale-variant" transformation (Eq 5) to the data. Observe the degradation in SHD to confirm the "Var-Sortability" failure mechanism.
  3. **Robustness Check (Missing):** Apply the "Missing Model" (MCAR with $\beta=0.1$). Verify that the performance remains stable compared to the baseline, validating the "noise ratio" theory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can performance degradation in nonlinear differentiable causal discovery methods be mitigated under scale variation?
- Basis in paper: [explicit] The authors note in Section 4.2 that while linear methods can achieve scale invariance via specific loss functions (Deng et al., 2024), the "performance of nonlinear differentiable methods under scale variation remains challenging and warrants further investigation."
- Why unresolved: Current nonlinear benchmarks (e.g., NOTEARS-MLP, GraN-DAG) show significant performance drops compared to linear counterparts when data is standardized, and no loss-function remedy has been established for the nonlinear case.
- What evidence would resolve it: The development of a scale-invariant loss function or optimization constraint for nonlinear models that maintains Structural Hamming Distance performance on standardized datasets comparable to linear methods like GOLEM-EV.

### Open Question 2
- Question: How robust are differentiable causal discovery methods when applied to time series and event sequence data under model assumption violations?
- Basis in paper: [explicit] The conclusion states, "Finally, our study confines itself to non-temporal causal discovery algorithms. Equally crucial is the conduct of benchmark assessments for causal discovery in time series and event sequences under model assumption violations."
- Why unresolved: The current benchmark is strictly limited to i.i.d. observational data across 12 algorithms; the interaction between temporal dynamics and assumption violations (e.g., autoregressive errors) remains untested.
- What evidence would resolve it: A comprehensive evaluation of temporal causal discovery algorithms (e.g., DYNOTEARS) across the eight misspecified scenarios defined in the paper.

### Open Question 3
- Question: Why do linear differentiable causal discovery algorithms achieve competitive performance when applied to nonlinear data (mechanism violation)?
- Basis in paper: [explicit] In Section 4.1, under "Mechanism violation," the authors state, "we are surprised to observe that linear differentiable causal discovery algorithms achieve competitive performance" despite the data being generated by a nonlinear Gaussian process.
- Why unresolved: This phenomenon contradicts the expectation that linear models should fail on nonlinear mechanisms, and the paper does not provide a theoretical explanation for this robustness.
- What evidence would resolve it: A theoretical analysis explaining why the least-squares score function or DAG constraint in linear DCD is less sensitive to nonlinearity than traditional score-based methods like GES in these specific scenarios.

### Open Question 4
- Question: What are the theoretical guarantees for the robustness of nonlinear differentiable causal discovery methods in misspecified scenarios?
- Basis in paper: [inferred] Section 4.1.2 provides theoretical explanations for the performance of *linear* differentiable methods (based on noise ratios), but the paper relies entirely on empirical results to demonstrate the robustness of nonlinear methods (NOTEARS-MLP, GraN-DAG).
- Why unresolved: The theoretical framework involving noise ratios (r) derived from Loh & Bühlmann (2014) applies to linear models, leaving the success of neural network-based methods under mechanism violations theoretically unexplained.
- What evidence would resolve it: An extension of the identifiability and consistency proofs to nonlinear additive noise models that accounts for the optimization landscape of gradient-based methods.

## Limitations

- The robustness claims are primarily validated on synthetic data, with only one real-world dataset (Sachs) used, limiting generalizability.
- The theoretical explanation for linear method success on nonlinear data is absent, relying instead on empirical observation without mechanistic understanding.
- Hyperparameter tuning strategies are vaguely described as "optimal relative to the specific dataset," suggesting possible oracle selection that could inflate performance claims.

## Confidence

- **High Confidence:** The experimental methodology for benchmarking across 8 misspecification scenarios is well-defined and reproducible. The synthetic data generation process is explicitly stated.
- **Medium Confidence:** The proposed mechanisms (MCAR noise ratio constancy, Var-Sortability failure) are logically sound and supported by results, but lack extensive ablation studies or mathematical proofs directly in this paper.
- **Low Confidence:** Claims about robustness are primarily tested on synthetic data; performance on the single real-world dataset (Sachs) may not generalize to other domains.

## Next Checks

1. **Ablation on Scale Variation:** Systematically vary the scale factor in Eq (5) and plot SHD/SID against NOTEARS to quantify the exact degradation curve, testing the Var-Sortability hypothesis.
2. **Sensitivity Analysis on Hyperparameters:** Re-run the experiments using only a single, fixed sparsity $\lambda_1$ value (e.g., 0.5) to assess how much the "robustness" claims depend on oracle hyperparameter tuning.
3. **Real-World Generalization:** Apply the benchmark to at least two additional real-world causal discovery datasets (e.g., from the causeme.net repository) to validate if the synthetic data conclusions hold outside controlled environments.