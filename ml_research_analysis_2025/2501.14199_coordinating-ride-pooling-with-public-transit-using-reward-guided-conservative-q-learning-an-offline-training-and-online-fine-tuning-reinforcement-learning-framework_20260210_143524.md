---
ver: rpa2
title: 'Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative
  Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework'
arxiv_id: '2501.14199'
source_url: https://arxiv.org/abs/2501.14199
tags:
- transit
- ride-pooling
- online
- vehicle
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework, RG-CQL,
  to coordinate ride-pooling with public transit in multimodal transportation networks.
  The method trains an offline Conservative Double Deep Q Network (CDDQN) policy and
  a reward estimator (Guider Network) from historical data, then fine-tunes the policy
  online using the Guider to guide exploration.
---

# Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework

## Quick Facts
- **arXiv ID**: 2501.14199
- **Source URL**: https://arxiv.org/abs/2501.14199
- **Reference count**: 40
- **Primary result**: RG-CQL achieves 81.3% improvement in data efficiency with 4.3% increase in total rewards and 5.6% reduction in overestimation errors

## Executive Summary
This paper introduces RG-CQL, a reinforcement learning framework that coordinates ride-pooling with public transit to optimize multimodal transportation. The method trains a Conservative Double Deep Q Network (CDDQN) and a reward estimator (Guider Network) offline from historical data, then fine-tunes the policy online using the Guider to guide exploration. A case study using Manhattan taxi and transit data demonstrates significant improvements in system rewards and data efficiency compared to traditional online RL methods.

## Method Summary
The RG-CQL framework consists of two phases: offline training and online fine-tuning. During offline training, a CDDQN learns conservative Q-values by penalizing out-of-distribution actions, while a Guider Network learns to predict immediate rewards. In the online phase, the CDDQN policy is fine-tuned using the Guider to filter exploration actions that predict low rewards. The framework uses bipartite matching to coordinate vehicle-passenger assignments, with actions including both direct drop-offs and transit zone drop-offs. The environment combines driving routing (OSRM) and transit routing (Dijkstra on schedules).

## Key Results
- Ride-pooling coordinated with transit improves system rewards by 17% compared to solo rides coordinated with transit
- Ride-pooling coordinated with transit outperforms ride-pooling without transit coordination by 22%
- RG-CQL achieves 81.3% improvement in data efficiency compared to traditional online RL methods

## Why This Works (Mechanism)

### Mechanism 1: Conservative Q-Learning Prevents Overestimation
- **Claim**: The system prevents overestimation of unseen state-action pairs during offline training by penalizing out-of-distribution (OOD) actions.
- **Mechanism**: CDDQN adds a regularization term to the standard TD loss that minimizes Q-values of actions not present in the historical dataset while maximizing Q-values of observed actions.
- **Core assumption**: Historical dataset contains sufficient distribution of high-quality trajectories to establish a reliable baseline policy.
- **Evidence anchors**: Abstract mentions CDDQN development; Section 5.1 describes conservative regularization term; Mildly Conservative Regularized Evaluation supports conservative approaches.
- **Break condition**: If historical data is sparse or biased, conservative penalty may suppress valid but unobserved optimal actions, causing suboptimal behavior.

### Mechanism 2: Guider Network Bridges Offline-Online Gap
- **Claim**: A supervised "Guider" network bridges the "pessimism-optimism gap" between offline training and online fine-tuning by filtering exploration actions.
- **Mechanism**: Guider learns to predict immediate rewards via supervised learning on historical data. During online fine-tuning, it filters out actions that predict reward below threshold during exploration.
- **Core assumption**: Immediate rewards are strong proxy for long-term value validity during initial phases of online exploration.
- **Evidence anchors**: Abstract mentions Guider Network as exploration guide; Section 5.2 details Guider's loss function and filtering logic; Behavior-Adaptive Q-Learning discusses similar needs.
- **Break condition**: If reward function is sparse or Guider fails to generalize to new states, it may prune valuable exploratory moves, stalling convergence.

### Mechanism 3: Flexible Transit Drop-off Actions
- **Claim**: Modeling transit stations as flexible drop-off points increases ride-pooling efficiency compared to fixed destination delivery.
- **Mechanism**: MDP action space includes dropping passenger at final destination OR at transit zone. Centralized bipartite matching solver uses Q-values to solve vehicle-passenger matching and drop-off selection.
- **Core assumption**: Passengers have defined tolerance for detours and are willing to switch modes if total utility is favorable.
- **Evidence anchors**: Abstract states coordinated ride-pooling with transit outperforms ride-pooling without transit coordination by 22%; Section 4.1.1 defines action space including transit zones; BMG-Q validates bipartite matching utility.
- **Break condition**: If transit schedules are unreliable or travel times inaccurately modeled, pooling-transit action values will be miscalculated, leading to poor service.

## Foundational Learning

**Concept: Conservative Q-Learning (CQL)**
- **Why needed here**: Standard offline RL suffers from "extrapolation error," overestimating value of actions not seen in dataset. CQL ensures policy learns safely from historical data without hallucinating success.
- **Quick check question**: Can you explain why standard Q-learning tends to overestimate OOD actions when trained offline?

**Concept: Offline-to-Online Fine-Tuning Gap**
- **Why needed here**: Paper specifically targets performance drop that occurs when conservatively trained offline policy is suddenly exposed to online exploration.
- **Quick check question**: What specific conflict arises between pessimistic policy learned offline and optimistic exploration required online?

**Concept: Bipartite Matching for Dispatch**
- **Why needed here**: RL agent doesn't dispatch directly; it provides value estimates to centralized ILP. Understanding Q-values to edge weights translation is necessary to interpret system outputs.
- **Quick check question**: How does exploration strategy (epsilon-greedy) modify edge weights in bipartite matching graph?

## Architecture Onboarding

**Component map:**
Offline Trainer (CDDQN + Guider) -> Environment Simulator (Vehicle Router + Transit Router) -> Online Agent (CDDQN + Bipartite Matcher)

**Critical path:**
1. Pre-train CDDQN and Guider on historical dataset D
2. Initialize online environment; compute state s_t
3. CDDQN estimates Q(s,a) for all actions; Guider estimates G(s,a)
4. Construct bipartite graph (Vehicles vs. Riders)
5. If exploring: Use Guider to prune low-reward actions before random selection
6. Solve ILP for matching; execute routes; update replay buffer

**Design tradeoffs:**
- Conservatism (C) vs. Learnability: High C prevents OOD errors but may make agent too timid to explore better strategies
- Guider Threshold (b_r): Too high prevents almost all exploration; too low fails to bridge offline-online gap
- Mode Share (p_pool): Enforcing strict "pooling-only" preferences reduces algorithm's flexibility and total system reward

**Failure signatures:**
- Initial Unlearning: Reward drops sharply at start of online fine-tuning (indicates Guider not effectively filtering exploration)
- Stagnant Service Rate: Service rate fails to exceed 70-80% (indicates matching distance R_match or fleet size insufficient for demand)
- Excessive Detours: Average detour exceeds delay tolerance κ (indicates penalty coefficients β3, β4 are too low)

**First 3 experiments:**
1. **Overestimation Check**: Train DDQN vs. CDDQN offline and plot Q-value estimates vs. actual returns to verify conservative regularization is working
2. **Ablation on Guider**: Run online fine-tuning with and without Guider module to isolate improvement in "data efficiency" (episodes to reach target reward)
3. **Transit Utility Validation**: Compare "Pooling-Only" vs. "Pooling-with-Transit" modes specifically on "Order Service Rate" metric to verify transit integration increases capacity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can RG-CQL framework be extended to simultaneously address both first-mile and last-mile challenges in transit coordination?
- **Basis in paper**: [explicit] Conclusion section explicitly identifies investigating order-matching problem for both first and last-mile challenges as primary future avenue
- **Why unresolved**: Current model and experiments focus exclusively on first-mile scenario, overlooking last-mile leg of journey
- **What evidence would resolve it**: Modified MDP formulation and simulation results demonstrating framework's efficiency in bidirectional multimodal context

### Open Question 2
- **Question**: Can Guider Network's exploration efficiency be improved by incorporating environment transition predictions rather than relying solely on instant reward regression?
- **Basis in paper**: [explicit] Conclusion states enhancing Guider through improved transition predictions offers promising research direction
- **Why unresolved**: Current Guider trained via supervised learning to estimate immediate rewards (r) but doesn't explicitly model state transition dynamics
- **What evidence would resolve it**: Comparative experiments showing transition-aware Guider accelerates convergence or increases total rewards compared to reward-only Guider

### Open Question 3
- **Question**: How does RG-CQL framework perform in low-demand scenarios where assumption of agent independence fails due to increased competition?
- **Basis in paper**: [inferred] Remark 1 states independence assumption holds primarily during peak hours (demand > supply) and may not hold when supply exceeds demand
- **Why unresolved**: Validation study focuses on morning peak hours, and model explicitly ignores agent interdependencies to reduce dimensionality
- **What evidence would resolve it**: Simulation results in off-peak settings comparing current independent-agent approach against multi-agent baselines accounting for competitive interdependence

## Limitations

- Framework performance heavily depends on quality and representativeness of historical dataset; sparse or biased data may permanently suppress valuable actions
- Assumes transit schedules are reliable and waiting times are accurately modeled; real-world disruptions could significantly degrade performance
- Guider's filtering threshold is set empirically without sensitivity analysis, raising questions about robustness across different operational conditions

## Confidence

- **High confidence**: 17% improvement in system rewards from coordinating ride-pooling with transit versus solo rides, and 22% improvement over ride-pooling without transit coordination
- **Medium confidence**: 81.3% improvement in data efficiency and 4.3% increase in total rewards during online fine-tuning
- **Low confidence**: 5.6% reduction in overestimation errors claim, as paper doesn't clearly define how overestimation rate is calculated or benchmarked

## Next Checks

1. **Ablation study on conservatism coefficient**: Systematically vary Conservative Coefficient C during offline training and measure resulting overestimation rates and final policy performance to determine optimal tradeoff between safety and exploration

2. **Sensitivity analysis of Guider threshold**: Test multiple values for reward threshold (b_r) during online fine-tuning to quantify impact on exploration efficiency and identify whether chosen value of 100 is universally optimal

3. **Real-world transit disruption scenario**: Simulate scenarios with transit delays or service interruptions to evaluate how coordination framework degrades and whether fallback mechanisms exist to maintain service quality