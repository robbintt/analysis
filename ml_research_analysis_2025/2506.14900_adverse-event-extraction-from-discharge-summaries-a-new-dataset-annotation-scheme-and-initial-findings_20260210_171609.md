---
ver: rpa2
title: 'Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation
  Scheme, and Initial Findings'
arxiv_id: '2506.14900'
source_url: https://arxiv.org/abs/2506.14900
tags:
- fracture
- should
- annotated
- haemorrhage
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a manually annotated dataset for extracting
  Adverse Events (AEs) from discharge summaries of elderly patients. The dataset includes
  14 clinically significant AEs and contextual attributes like negation, diagnosis
  type, and in-hospital occurrence.
---

# Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings

## Quick Facts
- arXiv ID: 2506.14900
- Source URL: https://arxiv.org/abs/2506.14900
- Reference count: 40
- Manually annotated dataset for extracting 14 Adverse Events (AEs) from elderly patient discharge summaries, supporting discontinuous and overlapping entities with contextual attributes like negation and diagnosis type.

## Executive Summary
This study introduces a manually annotated dataset for extracting Adverse Events (AEs) from discharge summaries of elderly patients. The dataset includes 14 clinically significant AEs and contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, it supports discontinuous and overlapping entities, which are rarely addressed in prior work. Evaluation using FlairNLP with multiple models across fine-grained, coarse-grained, and coarse-grained with negation annotations showed strong performance for document-level coarse-grained extraction (BERT-cased F1 = 0.943) but notably lower performance for fine-grained entity-level tasks (BERT-uncased F1 = 0.675). Challenges persist in detecting rare events and capturing complex clinical language, particularly for underrepresented AEs. The dataset, developed in a Trusted Research Environment, is available upon request via DataLoch and serves as a robust benchmark for AE extraction and future cross-dataset generalization.

## Method Summary
The study developed a dataset of 2,040 discharge summaries annotated for 14 Adverse Events with contextual attributes (negation, in-hospital occurrence, diagnosis type, low confidence). The annotation scheme supports discontinuous and overlapping entities, addressing a gap in clinical NER research. Evaluation used FlairNLP's LSTM-CRF architecture with five embedding types (GloVe, BERT-uncased, BERT-cased, BioBERT, BioClinicalBERT) across three annotation granularities. Data preprocessing adapted Dai et al. (2020) to convert Brat format to CoNLL format for discontinuous entity handling. The dataset is accessible via DataLoch Trusted Research Environment upon request.

## Key Results
- Document-level coarse-grained extraction achieved high performance (BERT-cased F1 = 0.943)
- Entity-level fine-grained extraction showed lower performance (BERT-uncased F1 = 0.675), particularly for rare AEs
- BERT-cased outperformed other embeddings in most tasks, while BioClinicalBERT showed competitive results
- Discontinuous and overlapping entity handling presented unique challenges not commonly addressed in clinical NER

## Why This Works (Mechanism)
The study's success stems from its comprehensive annotation schema that captures both entity types and contextual attributes, enabling more nuanced AE extraction. The use of multiple embedding types and evaluation granularities provides a thorough assessment of model capabilities. The dataset's focus on elderly patients in a specific clinical setting creates a specialized benchmark for this population. The FlairNLP framework's LSTM-CRF architecture effectively handles sequential labeling tasks while supporting complex entity representations through its preprocessing adaptations.

## Foundational Learning
- **Discontinuous Entity Handling**: Critical for capturing multi-fragment clinical concepts like "left-sided hemiparesis" split across sentences; verify by checking if fragmented labels maintain semantic coherence
- **Contextual Attribute Annotation**: Negation and in-hospital occurrence provide crucial semantic context; validate by ensuring attribute consistency with entity spans
- **Trusted Research Environment Constraints**: Data access limitations affect reproducibility; confirm compliance requirements before attempting reproduction
- **Clinical NER Granularity Levels**: Different evaluation levels serve distinct use cases; test by comparing document-level vs entity-level performance metrics
- **Class Imbalance Impact**: Rare AEs significantly affect model performance; assess by examining frequency distributions and their correlation with F1 scores

## Architecture Onboarding

**Component Map**: DataLoch (raw discharge summaries) -> Brat Annotation -> CoNLL Conversion -> FlairNLP LSTM-CRF -> Evaluation Metrics

**Critical Path**: The annotation and preprocessing pipeline is critical, as incorrect handling of discontinuous entities will propagate errors through all subsequent stages. The FlairNLP training configuration must be precisely replicated to match reported results.

**Design Tradeoffs**: The choice between fine-grained entity extraction and coarse-grained document-level extraction reflects different clinical use cases. Fine-grained extraction provides detailed information but suffers from lower performance, while document-level extraction offers higher accuracy but less specificity.

**Failure Signatures**: Low F1 scores on rare entities (e.g., Seizures_inhospital=6 instances) indicate class imbalance issues. Poor performance on discontinuous entities suggests preprocessing errors in the Brat-to-CoNLL conversion. High document-level scores with low entity-level scores indicate correct implementation but highlight the inherent difficulty of fine-grained extraction.

**First Experiments**:
1. Verify Brat-to-CoNLL conversion by inspecting converted files for correct BIO tagging of discontinuous entities
2. Train BERT-cased model on coarse-grained task to confirm document-level F1 â‰ˆ 0.943
3. Test entity-level extraction on rare AEs to validate reported performance degradation

## Open Questions the Paper Calls Out
- **Cross-dataset Generalization**: The authors plan to assess cross-domain generalization by applying their annotation guidelines to the MIMIC-IV dataset to determine how well models trained on DataLoch generalize to US hospital data
- **LLM Performance**: State-of-the-art large language models like GPT-4 or Mistral were not available for testing in the Trusted Research Environment, leaving their potential advantages for handling overlapping entities unknown
- **Data Balancing Impact**: The authors did not explore data balancing or class weighting techniques for underrepresented categories due to computational constraints, leaving open the question of whether these approaches could improve rare entity detection

## Limitations
- Incomplete specification of Brat-to-CoNLL preprocessing logic for discontinuous and overlapping entities
- Data access restricted to DataLoch Trusted Research Environment requiring formal approval
- No exploration of data balancing techniques for rare entity classes
- Computational constraints limited experimentation with advanced models and techniques

## Confidence

**High confidence**: Document-level coarse-grained extraction results (F1 = 0.943) - aggregated metrics less sensitive to annotation nuances

**Medium confidence**: Entity-level fine-grained extraction results (F1 = 0.675) - acknowledged challenges with rare entities and preprocessing uncertainties

**Low confidence**: Practical reproducibility of discontinuous entity handling without exact preprocessing implementation

## Next Checks
1. Obtain DataLoch access and verify the exact Brat-to-CoNLL conversion implementation matches the paper's description, particularly for handling multi-fragment and overlapping entity spans
2. Replicate the entity-level fine-grained extraction results for underrepresented AEs (e.g., Seizures, Constipation) to assess whether class imbalance or preprocessing artifacts drive performance differences
3. Conduct ablation studies with different embedding types (BERT-cased vs BioClinicalBERT) on the coarse-grained+negation task to confirm the relative performance differences reported in Table 4