---
ver: rpa2
title: 'IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric
  Assistants'
arxiv_id: '2511.19684'
source_url: https://arxiv.org/abs/2511.19684
tags:
- step
- skipping
- task
- steps
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndEgo, a multimodal egocentric and exocentric
  dataset for industrial tasks, including assembly/disassembly, logistics and organisation,
  inspection and repair, and woodworking. The dataset contains 3,460 egocentric recordings
  (approximately 197 hours) and 1,092 exocentric recordings (approximately 97 hours),
  with detailed annotations, processed outputs, and benchmarks.
---

# IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants

## Quick Facts
- **arXiv ID**: 2511.19684
- **Source URL**: https://arxiv.org/abs/2511.19684
- **Reference count**: 40
- **Primary result**: Introduces IndEgo, a large-scale multimodal dataset with 3,460 egocentric and 1,092 exocentric recordings for industrial tasks, with benchmarks showing SOTA models struggle on mistake detection (F1 0.29-0.44) and action recognition (57.6%-64.1%).

## Executive Summary
This paper introduces IndEgo, a large-scale multimodal dataset for industrial scenarios combining egocentric and exocentric views. The dataset includes 3,460 egocentric recordings (~197 hours) and 1,092 exocentric recordings (~97 hours) across assembly/disassembly, logistics, inspection/repair, and woodworking tasks. The authors establish three key benchmarks: Mistake Detection, Reasoning-based Video Question Answering, and Collaborative Task Understanding. Baseline evaluations using state-of-the-art VLMs reveal significant challenges, with F1 scores ranging from 0.29 to 0.44 for mistake detection and accuracy between 57.6% and 64.1% for action recognition. The dataset is available at https://huggingface.co/datasets/FraunhoferIPK/IndEgo.

## Method Summary
The IndEgo dataset was collected using Project Aria devices with egocentric and exocentric cameras, capturing industrial tasks with detailed annotations including actions, mistakes, and QA pairs. For baseline evaluations, the authors tested zero-shot and fine-tuned approaches using VLMs (VideoLLaMA3, Intern-VL-2.5, Qwen2.5-VL-7B, Gemini 2.0 Flash Thinking) on three benchmarks. Mistake Detection used binary classification and sub-categories (Severe, Process Failure, Impact Future), QA used LLM judges for answer evaluation, and Collaborative Task Understanding evaluated user attribution in joint work scenarios.

## Key Results
- Mistake Detection F1 scores range from 0.29 to 0.44 across models and settings
- Action recognition accuracy ranges from 57.6% to 64.1% for fine-tuned models
- Joint egocentric and exocentric views improve performance but remain context-dependent
- Models struggle to differentiate between wearer and coworker actions in collaborative settings (35.2% zero-shot accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Complementary View Geometry for Occlusion Handling
Joint egocentric and exocentric views provide a robust signal for mistake detection by capturing fine-grained hand-object interactions (ego) and stable global context (exo). This fusion allows disambiguation of "hand masking tool" from "incorrect tool usage." The mechanism degrades if the exocentric camera is static while the user moves out of frame, or if synchronization drifts.

### Mechanism 2: Task Graph Constraints for Error Taxonomy
Industrial mistake detection functions effectively when models utilize strict temporal dependencies of procedural task graphs. The dataset annotates specific failure modes (Severe, Process Failure, Impact Future) requiring models to learn graph topology to detect omissions versus execution errors. This fails in unscripted or highly variable tasks where the "correct" graph is not unique or linear.

### Mechanism 3: Semantic Disambiguation via Multimodal Intent
Narration and eye gaze serve as priors to disambiguate visually similar inverse actions (e.g., "screwing in" vs. "unscrewing") in low-frame-rate video. The dataset links video to "inner monologue" and attention, allowing semantic grounding when visual motion is ambiguous. This signal is lost if the user is silent or gaze data is noisy during heavy labor.

## Foundational Learning

- **Concept: Egocentric Video Synchronization & Calibration**
  - Why needed here: The dataset relies on Project Aria data requiring specialized tooling (MPS) to align temporal streams
  - Quick check question: Can you explain the difference between the raw VRS file stream and the processed MPS output for the Aria device?

- **Concept: Graph Neural Networks (GNNs) or Structured State Spaces**
  - Why needed here: The "Task Graphs" defined in the paper are directed acyclic graphs requiring architectures that respect graph connectivity
  - Quick check question: How would you enforce a temporal dependency constraint (e.g., Step B requires Step A) in a loss function?

- **Concept: Multimodal Fusion Strategies (Late vs. Early)**
  - Why needed here: The paper evaluates "RGB only" vs "RGB + Audio + Gaze" requiring understanding of how to fuse high-dimensional video features with sparse sensor data
  - Quick check question: When fusing audio and video features for the Mistake Detection task, would you concatenate embeddings before the classifier (Early Fusion) or average the predictions (Late Fusion), and why?

## Architecture Onboarding

- **Component map:** Project Aria VRS Files -> Machine Perception Services (MPS) -> Extracted 10FPS RGB, Gaze Vectors, Point Clouds -> VideoLLaMA3 / InternVL / QwenVL Backbone -> Transformer or MLP classifier
- **Critical path:**
  1. Sync: Aligning Ego (10FPS) and Exo (30FPS) streams is manual and prone to drift
  2. Processing: Running MPS on raw Aria data is computationally expensive; pre-processed data must be checked for "semi-dense point cloud" quality
  3. Prompting: For Zero-Shot MD, prompt design significantly alters performance

- **Design tradeoffs:**
  - 10 FPS vs. 30 FPS: Limits Ego RGB to 10 FPS to manage storage for 197 hours of recording, sacrificing fine temporal resolution but allowing longer task sequences
  - Zero-Shot vs. Fine-Tuning: Zero-shot allows rapid testing of SOTA models but performance is poor (F1 ~0.40); fine-tuning improves this (~0.44) but requires labeled data

- **Failure signatures:**
  - Inverse Action Confusion: High confusion between "tighten" vs "untighten" or "attach" vs "detach" due to low frame rate and visual similarity
  - Collaborative Attribution: Models struggle to attribute actions to the "wearer" vs. the "coworker" when hands overlap in the frame
  - Modality Noise: In noisy shop-floor environments, audio can degrade performance rather than help if not filtered

- **First 3 experiments:**
  1. Reproduce Zero-Shot MD Baseline: Run VideoLLaMA3 on the 15 MD tasks (Ego view) to confirm the F1 ~0.24 baseline
  2. Ablate Modality Input: Compare "RGB Only" vs "RGB + Gaze Overlay" using the Gemini Flash model on the "Singular Actions" subset
  3. Proximity Analysis (Collaborative): Train a simple classifier to distinguish "Worker 1 Hand" vs "Worker 2 Hand" in collaborative videos

## Open Questions the Paper Calls Out

- **Open Question 1:** How can models effectively disambiguate between the egocentric wearer's actions and those of a coworker in close-proximity collaborative industrial tasks? (Basis: Section 4.3 notes VLMs struggle to differentiate users, yielding only 35.2% zero-shot accuracy; unresolved due to occlusions and visual ambiguity)
- **Open Question 2:** To what extent does joint egocentric and exocentric modeling improve mistake detection when the ego-view is occluded? (Basis: Section 4.1 states joint view outperforms single views but utility is context-dependent; unresolved as simple feature concatenation yields only marginal gains)
- **Open Question 3:** Can reliable "early mistake detection" be achieved using only the initial portion of an action sequence before the error is fully executed? (Basis: Table 3 and Section 4.1 define "Early Mistake Detection" as a specific challenge where baseline performance drops significantly; unresolved as current models rely heavily on visual confirmation of erroneous outcomes)

## Limitations

- Manual synchronization between egocentric and exocentric streams introduces potential drift, particularly for long sequences
- The dataset focuses on specific industrial domains (woodworking, electronics assembly) which may limit generalization to other factory settings
- Gaze annotation quality is not validated against ground truth in manipulation tasks, raising questions about its utility for fine-grained hand-object interaction analysis

## Confidence

- **High Confidence**: Dataset statistics and benchmark methodology are well-documented and reproducible
- **Medium Confidence**: Mistake Detection results show clear performance gaps, but the exact prompt engineering for zero-shot evaluation could influence outcomes
- **Low Confidence**: Collaborative task understanding results are limited by small sample sizes (10 tasks) and unclear attribution metrics

## Next Checks

1. Measure and report the temporal alignment error between ego/exo streams across the dataset to quantify the impact on joint-view analysis
2. Test the Mistake Detection models on a held-out task from a different industrial domain to assess generalization beyond the training distribution
3. Conduct a controlled experiment validating gaze accuracy during manipulation tasks against expert annotations to establish its reliability as a disambiguation signal