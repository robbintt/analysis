---
ver: rpa2
title: 'Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative
  Texts'
arxiv_id: '2504.07459'
source_url: https://arxiv.org/abs/2504.07459
tags:
- causal
- graph
- action
- event
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a linguistic framework for generating causal
  graphs from narrative texts, addressing the gap between high-level causality and
  event-specific relationships. The method first extracts concise, agent-centered
  vertices using LLM-based summarization.
---

# Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts

## Quick Facts
- **arXiv ID**: 2504.07459
- **Source URL**: https://arxiv.org/abs/2504.07459
- **Reference count**: 30
- **Key outcome**: A linguistic framework for generating causal graphs from narrative texts, combining LLM summarization, RoBERTa embeddings, and an Expert Index of linguistic features to outperform GPT-4o and Claude 3.5 in causal graph quality.

## Executive Summary
This paper introduces a linguistic framework for generating causal graphs from narrative texts, addressing the gap between high-level causality and event-specific relationships. The method first extracts concise, agent-centered vertices using LLM-based summarization. It then employs a hybrid system combining RoBERTa embeddings with an "Expert Index" of seven linguistically grounded features, integrated into a STAC classification model for precise causal link identification. A five-iteration prompting process refines and constructs connected causal graphs. Experiments on 100 narrative chapters and short stories show the approach consistently outperforms GPT-4o and Claude 3.5 in causal graph quality across key dimensions, while maintaining comparable readability. The open-source tool offers an interpretable and efficient solution for capturing nuanced causal chains in narratives.

## Method Summary
The framework uses a four-stage pipeline: (1) LLM-based summarization to extract concise, agent-centered vertices from narrative paragraphs, (2) RoBERTa-based classifier to identify seven Expert Index features per sentence, (3) XGBoost classifier using concatenated RoBERTa embeddings and one-hot Expert Index vectors to assign STAC labels (Situation, Task, Action, Consequence), and (4) a five-iteration LangChain prompting process to construct connected causal graphs through bond learning, pruning, and refinement. The system was trained on 750 annotated sentences for Expert Index features and 1,000 sentences for STAC labels, with an 80/20 train/test split.

## Key Results
- Outperforms GPT-4o and Claude 3.5 across 8 evaluation dimensions including Causality, Motivations, and Completeness
- Achieves STAC classification F1 scores of 0.7-0.8 for most categories, with Expert Index features providing a 13-point F1 gain for Consequence detection
- Maintains comparable readability while producing more precise causal graphs than baseline LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing narrative text into concise, agent-centered vertices reduces noise and focuses the causal graph on salient plot points.
- **Mechanism**: An LLM (GPT-4o) rewrites raw narrative paragraphs into simple sentences. This process replaces pronouns, splits clauses, and enforces active voice, ensuring every vertex represents a distinct, attributable event rather than a complex state.
- **Core assumption**: Complex sentence structures and passive voice obscure direct causal agency; simplifying them makes causal links more explicit for downstream models.
- **Evidence anchors**: [abstract] "...extracts concise, agent-centered 'vertices' using an LLM-based summarization strategy." [section 3.1] "Requirements for Each Vertex... Concise... Agent-Centered... Active Voice."
- **Break condition**: If the summarization step hallucinates events or fails to resolve pronouns correctly, the resulting graph will depict disconnected or erroneous agents.

### Mechanism 2
- **Claim**: Augmenting semantic embeddings with an explicit "Expert Index" of linguistic features improves the classification of causal roles (STAC) compared to semantic embeddings alone.
- **Mechanism**: The system calculates seven linguistically grounded features (e.g., Eventivity, Boundedness) per sentence. These discrete features are concatenated with RoBERTa embeddings and fed into an XGBoost classifier. This provides structural signals (e.g., "is this a habit or a specific event?") that dense vector embeddings might miss.
- **Core assumption**: Causal role (STAC) correlates strongly with specific linguistic aspectual classes (e.g., a "Task" is likely future-oriented and agent-initiated).
- **Evidence anchors**: [abstract] "...hybrid system combining RoBERTa embeddings with an 'Expert Index'... achieves superior precision in causal link identification." [section 5.3] "Models that incorporate the Expert Index features consistently outperform their counterparts... most pronounced for the Consequence (C) category."
- **Break condition**: If the text is highly metaphorical or uses non-standard grammar, the rule-based Expert Index features may misclassify the sentence aspect.

### Mechanism 3
- **Claim**: A structured, multi-iteration prompting process using counterfactual reasoning reduces spurious causal edges.
- **Mechanism**: Instead of asking an LLM to generate a graph in one shot, the system iterates five times. Crucially, Iteration 3 applies counterfactual reasoning ("If A did not occur, would B still happen?") to prune weak links identified in previous steps.
- **Core assumption**: LLMs are better at verifying specific logical constraints (via critique) than generating complex structures from scratch.
- **Evidence anchors**: [abstract] "...structured five-iteration prompting process refines and constructs connected causal graphs." [section 3.4] "Iteration 3: Logical Consistency and Pruning... applied counterfactual reasoning... to filter out any bonds that did not have explicitly causal relationship."
- **Break condition**: If the LLM's context window is exceeded or if the narrative is non-linear (e.g., time-travel), the sequential iteration logic may fail to converge.

## Foundational Learning

- **Concept**: **Aspectual Features (Vendlerian Classes)**
  - **Why needed here**: The "Expert Index" relies on distinguishing if a verb is stative (state of being), habitual (recurring), or episodic (one-time event). Without understanding this, you cannot calculate the Boundedness or Eventivity features required for the STAC classifier.
  - **Quick check question**: In the sentence "She was reading the book," is the event bounded (episodic) or unbounded (static/habitual)?

- **Concept**: **STAC Framework (Situation, Task, Action, Consequence)**
  - **Why needed here**: This is the target label schema. It differs from standard causality extraction by introducing "Task" (intent/requirement) as a distinct causal node type, separating it from the "Action" itself.
  - **Quick check question**: Does the sentence "He needed to open the door" represent an Action or a Task in this framework?

- **Concept**: **Graph Topology in NLP**
  - **Why needed here**: The paper aims for a "connected graph" $G=(V,E)$. Understanding that edges represent specific bonds (e.g., Action $\to$ Consequence) rather than generic relations is vital for interpreting the 11 valid STAC bonds.
  - **Quick check question**: According to the paper's definition, can a "Situation" node directly cause an "Action" node, or must it go through a "Task"?

## Architecture Onboarding

- **Component map**: Pre-processor (GPT-4o summarization) -> Feature Encoder (RoBERTa + Expert Index) -> Classifier (XGBoost) -> Graph Builder (5-iteration LangChain prompting)
- **Critical path**: The STAC Classification (Component 3) is the bottleneck. The paper notes that LLM-only labeling resulted in a Kappa of only 0.63. The system relies heavily on the XGBoost model correctly labeling nodes before the Graph Builder attempts to link them.
- **Design tradeoffs**:
  - Interpretability vs. Complexity: Uses XGBoost + Hand-crafted features instead of an end-to-end Transformer to allow inspection of *why* a node was labeled (e.g., "Classified as Action because Eventivity=Dynamic")
  - Cost: Uses 5 LLM iterations per graph, increasing latency and cost compared to single-prompt solutions, to gain higher logical consistency
- **Failure signatures**:
  - Habitual Confusion: The classifier struggles to distinguish "Habitual" from "Static" events (F1 < 0.4 for Habitual), potentially mislabeling recurring plot points as permanent states
  - Isolated Vertices: If Iteration 4 fails to find a cause for an isolated node, the graph becomes disconnected, violating the problem setting $G$
- **First 3 experiments**:
  1. Ablation on Vertex Quality: Feed raw text vs. LLM-summarized vertices into the STAC classifier to measure the drop in classification F1-score
  2. Expert Index Validation: Run the XGBoost classifier with RoBERTa embeddings *only* (dropping the Expert Index) to reproduce the ~13% drop in Consequence detection mentioned in Section 5.3
  3. Iteration Depth Test: Truncate the graph prompting to 3 iterations (removing the counterfactual pruning) and measure the increase in spurious edges (False Positives)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the classification accuracy for the "Boundedness" feature be improved by redefining the overlapping "habitual" and "static" categories?
- **Basis in paper**: [explicit] The paper states that "Boundedness posed greater challenges due to conceptual overlap between the habitual and static classes, leading to reduced precision and recall."
- **Why unresolved**: The current RoBERTa-based classifier struggles to distinguish these specific linguistic aspects using the existing feature definitions.
- **What evidence would resolve it**: A comparative study showing improved F1-scores for Boundedness after modifying the feature taxonomy or introducing intermediate categories.

### Open Question 2
- **Question**: To what extent can prompt engineering or fine-tuning improve LLM performance on STAC labeling to eliminate the need for human annotation?
- **Basis in paper**: [explicit] The authors note that STAC labels generated by the LLM had a Cohen's Kappa of only 0.63 ("moderate agreement"), making them "inadequate for reliable model training" compared to human annotators.
- **Why unresolved**: The paper relies on human ground truth for training but does not explore if specific fine-tuning could close the performance gap for this intermediate step.
- **What evidence would resolve it**: Experiments demonstrating that a fine-tuned LLM achieves a Kappa score > 0.8 (substantial agreement) against human labels for the STAC task.

### Open Question 3
- **Question**: Does the hybrid Expert Index approach maintain its performance advantage when applied to non-literary domains such as news reports or technical documentation?
- **Basis in paper**: [inferred] The dataset is strictly limited to "narrative chapters and short stories" published between 1800 and 1950.
- **Why unresolved**: The linguistic features (Expert Index) were selected for narrative flow (Situation-Task-Action-Consequence); it is unclear if this logic holds for expository or descriptive texts.
- **What evidence would resolve it**: Evaluation results from applying the model to a corpus of news articles or modern non-fiction, comparing graph quality against the current baselines.

## Limitations
- The five-iteration prompting process relies on LLM reasoning that may not generalize to highly non-linear narratives or texts with ambiguous causality
- The linguistic feature extraction depends on syntactic patterns that may not hold for metaphorical or poetic language
- The evaluation framework's "causal" and "motivational" dimensions are judged by human annotators without standardized rubrics

## Confidence
- **High Confidence**: The STAC classification results (F1 scores around 0.7-0.8 for most categories) and the measurable improvement from adding Expert Index features (~13% gain for Consequence detection)
- **Medium Confidence**: The pairwise comparison results against GPT-4o and Claude 3.5, given the subjective nature of 8 evaluation dimensions
- **Low Confidence**: The claim that the approach "maintains comparable readability" since this dimension was not directly benchmarked against baseline systems

## Next Checks
1. **Generalization Test**: Apply the framework to non-narrative texts (e.g., news articles) to assess if the Expert Index features remain predictive across domains
2. **Error Analysis**: Conduct a systematic error analysis on the 11 valid STAC bonds to identify which specific causal patterns the system consistently misses
3. **Human Study**: Recruit 5-10 additional annotators to independently evaluate the same graphs on the "causality" and "motivations" dimensions to establish inter-rater reliability