---
ver: rpa2
title: Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion
  Science
arxiv_id: '2511.02092'
source_url: https://arxiv.org/abs/2511.02092
tags:
- online
- data
- uncertainty
- learning
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining machine learning
  performance for predicting toroidal field coil deflection in fusion devices, where
  data streams exhibit non-stationary behavior due to experimental evolution and equipment
  wear. The authors apply online learning techniques to continuously adapt ML models
  to drifting data distributions, demonstrating an 80% reduction in prediction error
  compared to static models.
---

# Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science

## Quick Facts
- **arXiv ID:** 2511.02092
- **Source URL:** https://arxiv.org/abs/2511.02092
- **Authors:** Kishansingh Rajput; Malachi Schram; Brian Sammuli; Sen Lin
- **Reference count:** 17
- **Primary result:** Online learning reduces TF coil deflection prediction error by 80% compared to static models

## Executive Summary
This paper addresses the challenge of maintaining accurate machine learning predictions for toroidal field coil deflection in fusion devices under non-stationary conditions. The authors propose an uncertainty-guided online ensemble method that combines online learning with calibrated uncertainty estimates to adapt to concept drift caused by equipment wear and experimental evolution. The approach demonstrates an 80% reduction in prediction error over static models, with an additional 10% improvement from the uncertainty-weighted ensemble compared to standard online learning.

## Method Summary
The method uses a CNN with Deep Gaussian Process Approximation (DGPA) output layer, trained incrementally using sliding windows of recent data (buffer sizes: 1, 5, 20, 40, 200 shots). Five parallel models are maintained, each with different buffer sizes to capture drift at various temporal scales. Uncertainty estimates from DGPA models are calibrated and used to weight ensemble predictions via inverse-variance weighting. The ensemble combines the rapid adaptation of short-horizon models with the stability of long-horizon models, achieving robust performance across different drift regimes.

## Key Results
- Online learning reduces prediction error by 80% compared to static models
- Uncertainty-guided ensemble reduces error by an additional 10% over single-model online learning
- Pearson correlation of 0.58 between predicted uncertainty and actual prediction error
- Statistical validation confirms robust improvements across multiple trials

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incremental model updates using sliding-window batched training reduce prediction error by approximately 80% compared to static models when data distributions drift over time.
- **Mechanism:** The model performs fine-tuning from previously learned weights using the most recent sample combined with a small buffer of historical data. Different buffer sizes (1, 5, 20, 40, 200 shots) capture drift at different temporal scales—shorter buffers handle abrupt drift; longer buffers handle gradual drift.
- **Core assumption:** The drift rate is bounded such that the chosen buffer window contains sufficient signal for adaptation before the next distribution shift.
- **Evidence anchors:**
  - [abstract] "Online learning... reduces error by 80% compared to a static model"
  - [Section 3.2] "For abrupt drifts, shorter buffer size is favored, whereas for slow and gradual drifts, longer buffer size is more appropriate."
  - [corpus] MARLINE paper confirms concept drift is "a major problem in online learning" requiring proactive adaptation.
- **Break condition:** When drift velocity exceeds the model's capacity to adapt within a single buffer window, or when ground truth delay exceeds prediction horizon requirements.

### Mechanism 2
- **Claim:** DGPA provides uncertainty estimates that correlate with prediction error (Pearson r = 0.58), enabling reliable confidence assessment for out-of-distribution samples.
- **Mechanism:** Random Fourier Features approximate an RBF kernel in a fixed-size matrix, combining GP distance-awareness with DNN expressivity. A bi-Lipschitz constraint (L₁ = 0.75, L₂ = 1.25) enforces distance preservation between input and final hidden layer, ensuring OOD samples remain detectable. Post-training calibration scales variance to match observed coverage.
- **Core assumption:** Distance preservation in hidden representations is sufficient proxy for detecting distributional shift from training data.
- **Evidence anchors:**
  - [Section 3.3] "bi-Lipschitz constraint is applied to the transformation from input x to the final hidden layer output h(x)"
  - [Results, Figure 4] "Pearson correlation coefficient between the mean of the two bands is 0.58" (error vs. uncertainty)
  - [corpus] Limited direct corpus validation for DGPA; most related work uses Bayesian Neural Networks or MC Dropout for UQ.
- **Break condition:** When miscalibration area exceeds ~20% (see Figure 2b: future data at 21.7% vs. test data at 3%), indicating uncertainty no longer reliably predicts error.

### Mechanism 3
- **Claim:** Inverse-variance weighting of ensemble predictions (wᵢ ∝ 1/σᵢ²) yields an additional ~10% MAE reduction over single-model online learning.
- **Mechanism:** An ensemble of models trained on different buffer horizons produces predictions (ŷᵢ) and uncertainties (σᵢ). The meta-algorithm computes weighted average with weights inversely proportional to variance, combining the short-horizon model's rapid adaptation with the long-horizon model's stability.
- **Core assumption:** Calibrated uncertainty values serve as proxy for inverse confidence, where lower σᵢ reliably indicates higher accuracy.
- **Evidence anchors:**
  - [abstract] "uncertainty guided online ensemble reduces predictions error by about 10% respectively over standard single model based online learning"
  - [Section 3.4, Algorithm 1] "Compute ensemble prediction: ŷ = Σwᵢŷᵢ with wᵢ ∝ 1/σᵢ²"
  - [corpus] IncA-DES confirms dynamic ensemble selection "showing good results" for concept drift, validating ensemble approach.
- **Break condition:** When uncertainty calibration degrades across ensemble members, causing misweighted combinations that perform worse than naive averaging.

## Foundational Learning

- **Concept: Gaussian Process Kernels and Random Fourier Features**
  - Why needed here: DGPA replaces exact GP kernel computation with RFF approximation; understanding this trade-off is essential for diagnosing uncertainty quality issues.
  - Quick check question: Given a dataset with 100K samples, explain why exact GP inference (O(n³)) is infeasible and how RFF reduces this to O(m) where m = 512 features.

- **Concept: Concept Drift Types (Abrupt vs. Gradual vs. Incremental)**
  - Why needed here: Buffer size selection depends on drift type; selecting the wrong buffer leads to either overfitting to noise or lagging behind rapid shifts.
  - Quick check question: If equipment failure causes sudden sensor bias at shot 155,000, which buffer size (1, 5, 20, 40, or 200) would adapt fastest? What's the trade-off?

- **Concept: Uncertainty Calibration (Expected vs. Observed Coverage)**
  - Why needed here: The ensemble weighting mechanism assumes calibrated uncertainties; uncalibrated models will produce misleading confidence scores.
  - Quick check question: If a model predicts 90% confidence intervals but only 70% of true values fall within them, is it overconfident or underconfident? How would you adjust the scaling factor α?

## Architecture Onboarding

- **Component map:**
  Input (9 plasma vars, 100-point window) -> Conv1D Blocks × 3 (128→64 filters, kernel=3, ReLU, MaxPool) -> Dense Layers × 3 (128→128→1 nodes, dropout=0.05, ReLU) -> DGPA Layer (RFF kernel approximation, m=512) -> Output: Mean prediction ŷ ± Uncertainty σ

  Five parallel models with buffer sizes [1, 5, 20, 40, 200] shots -> Ensemble weighting (wᵢ ∝ 1/σᵢ²)

- **Critical path:**
  1. Initialize base model on shots 148,043–149,999 (train/val/test: 70/15/15)
  2. Deploy ensemble with pre-trained weights
  3. For each new shot t: predict -> receive ground truth -> update each model with its buffer -> recalibrate uncertainties
  4. Monitor miscalibration area; retrain from scratch if >15%

- **Design tradeoffs:**
  - **Ensemble size vs. latency:** 5 concurrent models require ~5× memory and inference time; may be prohibitive for real-time control (<10ms latency).
  - **Buffer size vs. stability:** Shorter buffers adapt faster but amplify noise; longer buffers smooth gradual drift but miss abrupt shifts.
  - **Calibration frequency vs. compute:** Per-shot calibration (Algorithm 2) adds overhead but maintains uncertainty quality.

- **Failure signatures:**
  - **Miscalibration spike:** Miscalibration area jumps from <5% to >15% after maintenance event -> trigger full retraining.
  - **Uncertainty-error decoupling:** Pearson correlation drops below 0.3 -> uncertainty no longer guides ensemble effectively; fall back to naive averaging.
  - **MAE regression:** Online ensemble MAE exceeds single-model baseline -> likely buffer mismatch with current drift regime.

- **First 3 experiments:**
  1. **Buffer ablation:** Train single models with buffer sizes [1, 5, 20, 40, 200] on held-out shots 150,000–155,000; plot MAE vs. shot number to identify optimal buffer for each drift regime.
  2. **Calibration validation:** Compute miscalibration area before/after Algorithm 2; verify reduction from baseline (target: <5% on in-distribution data).
  3. **Ensemble weighting comparison:** On shots 155,000–160,000, compare (a) single best model, (b) naive average, (c) uncertainty-weighted ensemble; compute statistical significance via 10-trial bootstrap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating heterogeneous model architectures into the online ensemble improve performance over the current approach of varying buffer sizes?
- Basis in paper: [explicit] The authors state they aim to "extend our approach to include other types of models in the ensemble, including different architectures."
- Why unresolved: The current study exclusively uses an ensemble of models with the same architecture but different training horizons (buffer sizes).
- What evidence would resolve it: A comparative analysis showing error rates and uncertainty calibration when distinct architectures (e.g., CNNs vs. LSTMs) are combined in the ensemble.

### Open Question 2
- Question: Can attention mechanisms with various scales effectively address distribution drifts in fusion data streams?
- Basis in paper: [explicit] The authors plan to "investigate the use of attention mechanisms with various scales to address drifts."
- Why unresolved: The current framework relies on sliding windows and buffer-based averaging rather than attention-based weighting for handling drift.
- What evidence would resolve it: Benchmarks comparing the adaptation speed and prediction accuracy of attention-based models against the current uncertainty-guided ensemble.

### Open Question 3
- Question: How does the uncertainty-guided online ensemble perform under real-time operational constraints at a live fusion facility?
- Basis in paper: [explicit] The authors intend to "deploy this approach at the DIII-D facility to evaluate its performance in real-time."
- Why unresolved: All reported results are derived from offline processing of historical shot data, which does not account for real-time latency or data transmission issues.
- What evidence would resolve it: Successful deployment during an experimental campaign demonstrating low-latency inference and sustained accuracy without pipeline interruptions.

### Open Question 4
- Question: Can the computational efficiency of the ensemble be improved without sacrificing the prediction accuracy gained through uncertainty guidance?
- Basis in paper: [inferred] The discussion notes that the proposed ensemble requires "substantially greater computational and memory resources," creating a trade-off with predictive performance.
- Why unresolved: The paper does not explore model compression, distillation, or dynamic selection techniques to mitigate the resource costs of running multiple concurrent models.
- What evidence would resolve it: An implementation that achieves similar MAE reduction (approx. 10%) with significantly reduced memory footprint and FLOPS.

## Limitations

- **Moderate uncertainty correlation:** Pearson correlation of 0.58 between predicted uncertainty and actual error indicates imperfect guidance for ensemble weighting.
- **Computational overhead:** Maintaining five parallel models with different buffer sizes requires substantial memory and inference time.
- **Dataset-specific validation:** Results demonstrated only on a single fusion device dataset, limiting generalizability claims.

## Confidence

**High confidence:** The 80% error reduction from online learning over static models is well-supported by established online learning theory and the fundamental challenge of concept drift in fusion experiments. The methodology for incremental model updates using sliding windows is standard practice.

**Medium confidence:** The DGPA uncertainty estimation mechanism has theoretical grounding in GP literature, but the specific implementation details (bi-Lipschitz constraint formulation, RFF integration) lack sufficient specification for exact reproduction. The moderate correlation (r = 0.58) between uncertainty and error suggests the approach works but with room for improvement.

**Medium confidence:** The 10% ensemble improvement is demonstrated statistically but depends heavily on the quality of uncertainty calibration. When miscalibration exceeds 15%, the ensemble can underperform single-model baselines, creating a critical failure mode.

## Next Checks

1. **Cross-dataset validation:** Test the ensemble approach on at least two additional non-stationary datasets (e.g., industrial sensor data, climate monitoring) to verify the 10% improvement generalizes beyond fusion science applications.

2. **Uncertainty calibration robustness:** Systematically measure Pearson correlation between predicted uncertainty and actual error across different drift regimes (abrupt vs. gradual) to identify conditions where uncertainty guidance fails.

3. **Latency-constrained deployment:** Profile inference time for the five-model ensemble and implement model pruning/compression techniques to determine if the approach can meet real-time control requirements (<10ms) while maintaining performance benefits.