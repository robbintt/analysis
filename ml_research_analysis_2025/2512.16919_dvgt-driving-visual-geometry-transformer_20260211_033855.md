---
ver: rpa2
title: 'DVGT: Driving Visual Geometry Transformer'
arxiv_id: '2512.16919'
source_url: https://arxiv.org/abs/2512.16919
tags:
- driving
- geometry
- point
- dvgt
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DVGT, a Driving Visual Geometry Transformer
  designed for autonomous driving that reconstructs global dense 3D point maps from
  unposed multi-view image sequences. The core innovation lies in its 3D prior-free
  architecture that directly predicts metric-scaled geometry in ego-vehicle coordinates
  without requiring camera parameters or post-alignment with external sensors.
---

# DVGT: Driving Visual Geometry Transformer

## Quick Facts
- **arXiv ID:** 2512.16919
- **Source URL:** https://arxiv.org/abs/2512.16919
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art 3D point map reconstruction from unposed multi-view image sequences, with 0.457 Abs Rel and 0.953 δ<1.25 on nuScenes.

## Executive Summary
DVGT is a Driving Visual Geometry Transformer that reconstructs global dense 3D point maps from unposed multi-view image sequences for autonomous driving. The core innovation is a 3D prior-free architecture that directly predicts metric-scaled geometry in ego-vehicle coordinates without requiring camera parameters or post-alignment with external sensors. Trained on a large mixture of driving datasets, DVGT significantly outperforms existing methods and demonstrates strong generalization across diverse camera configurations.

## Method Summary
DVGT employs a DINOv3 ViT-L encoder and a 24-layer Geometry Transformer with factorized attention to process multi-frame, multi-view inputs. It jointly predicts 3D point maps and ego poses by alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention. The model is trained on pseudo-ground truth depth generated from MoGe-2 and aligned with LiDAR using ROE, with metric regression stabilized through linear scaling of coordinates.

## Key Results
- Achieves 0.457 Abs Rel and 0.953 δ<1.25 on nuScenes benchmark
- Outperforms VGGT and other baselines across multiple metrics
- Demonstrates strong generalization to diverse camera configurations without requiring explicit geometric priors

## Why This Works (Mechanism)

### Mechanism 1: Factorized Spatial-Temporal Attention
The model decomposes global attention into structured local groups (intra-view, cross-view, cross-frame) to preserve geometric consistency while reducing computational complexity. This restricts attention to relevant geometric constraints within views and between structured time/stereo relationships, avoiding the prohibitive cost of full attention on multi-frame, multi-view driving data.

### Mechanism 2: Prior-Free Ego-Centric Coordinate Prediction
By predicting 3D points directly in ego-vehicle coordinates rather than camera coordinates, the model decouples geometry from specific sensor rig configurations. This enables zero-shot generalization to new camera setups by learning a generalized visual-to-metric mapping without relying on fixed geometric priors like camera calibration.

### Mechanism 3: Metric Regression with Coordinate Normalization
Direct regression of metric 3D coordinates is stabilized through linear scaling of target coordinates (division by 10), preventing numerical instability from the large dynamic range of driving scenes. This approach maintains precision for near-field geometry while compressing the large coordinate space.

## Foundational Learning

- **Concept: Vision Transformers (ViT) & Tokenization**
  - **Why needed here:** Understanding how images become sequences of tokens with positional embeddings is required to grasp how "Intra-view" attention works.
  - **Quick check question:** How does adding a learnable "Ego Token" to the sequence allow the transformer to regress a global property like vehicle pose?

- **Concept: Attention Mechanisms (Self vs. Cross)**
  - **Why needed here:** The core innovation is the *alternating* attention structure, requiring distinction between tokens looking at themselves (Intra-view) vs. looking at different modalities/frames (Cross-view/Frame).
  - **Quick check question:** In the "Cross-Frame Temporal Attention" block, which keys and values does a query token from frame $t$ attend to?

- **Concept: Coordinate Systems (SE(3) Transformations)**
  - **Why needed here:** The output is a metric point map in the ego-frame of the *first* frame, plus ego-poses for subsequent frames, requiring understanding of transformations between coordinate systems.
  - **Quick check question:** If the model predicts a point at $(x, y, z)$ in the frame $t$ coordinate system, how do you use the predicted ego-pose $\hat{T}_t$ to find its location in the global ego-frame?

## Architecture Onboarding

- **Component map:** Multi-frame, Multi-view Images -> DINOv3 (ViT-L) Encoder -> Patch Tokens + Ego Token -> 24-layer Geometry Transformer (Alternating Intra-View -> Cross-View -> Cross-Frame Attention) -> Point Map Head + Pose Head
- **Critical path:** The **alternating attention blocks**. If this custom attention mask is implemented incorrectly, the geometric reasoning will collapse.
- **Design tradeoffs:** Factorized attention is ~4x faster than global attention but sacrifices some accuracy; removing explicit geometric priors gains generalization but relies entirely on data scale to learn projection implicitly.
- **Failure signatures:** Metric Drift if linear scaling is omitted or wrong; Temporal Inconsistency if temporal positional embeddings are removed; Waymo Performance Drop due to dataset sampling imbalance.
- **First 3 experiments:**
  1. Overfit Single Scene: Train on one nuScenes sequence to verify loss converges and 3D point cloud matches ground truth.
  2. Ablate Attention: Disable Cross-Frame attention and run inference on video sequence to observe temporal stability.
  3. Zero-Shot Rig Test: Train on nuScenes only, test on KITTI without code changes to validate prior-free generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can optimizing dataset sampling weights during training significantly improve reconstruction accuracy on large-scale datasets like Waymo?
- Basis in paper: [explicit] Authors state performance on Waymo is "less competitive" due to "disproportionate data sampling" and expect optimizing sampling weights will bridge this gap.
- Why unresolved: Current training assigned equal weight to Waymo despite it being 5x larger than other datasets, causing underfitting.
- What evidence would resolve it: Re-training with sampling weights proportional to dataset size/diversity, followed by improved Accuracy and Completeness metrics on Waymo validation.

### Open Question 2
- Question: How can ego-pose estimation be improved for camera setups with high overlap but limited field of view (e.g., dual-camera rigs)?
- Basis in paper: [explicit] Section 4.2 notes DVGT's pose prediction on KITTI is "slightly lower," attributing this to KITTI's "high-overlap dual-camera setup."
- Why unresolved: Current model appears optimized for surround-view configurations, not fully leveraging geometric constraints in high-overlap, narrow-baseline setups.
- What evidence would resolve it: Ablation study introducing geometric constraints or feature fusion mechanisms tailored for high-overlap views, showing improved AUC@30 scores on KITTI.

### Open Question 3
- Question: Can the factorized spatial-temporal attention mechanism be refined to fully close the performance gap with global attention while maintaining its efficiency advantage?
- Basis in paper: [inferred] Section 4.4 and Table 6 show factorized attention improves speed but still yields slightly lower accuracy compared to global attention baseline.
- Why unresolved: Decomposition trades some capacity for long-range dependencies for computational feasibility, a trade-off only partially mitigated by temporal positional embeddings.
- What evidence would resolve it: Development of hybrid attention scheme or more expressive positional encoding matching Abs Rel and δ<1.25 metrics of global attention without returning to O(N²) complexity.

## Limitations
- Relies heavily on pseudo-ground truth depth from MoGe-2, potentially propagating errors
- Claims of zero-shot generalization to novel camera configurations not empirically validated beyond known datasets
- Performance drops on Waymo reveal potential overfitting to specific dataset characteristics

## Confidence
- **High confidence:** Core architectural innovation (factorized attention mechanism) well-specified and supported by ablation studies; performance gains over baselines clearly demonstrated
- **Medium confidence:** Zero-shot generalization claim compelling but based on validation within known datasets; linear scaling factor empirically validated but may not generalize to all scenarios
- **Low confidence:** Robustness to truly novel camera configurations remains untested; filtering criteria for pseudo-ground truth quality vaguely specified

## Next Checks
1. **Cross-Dataset Generalization Test:** Train DVGT exclusively on nuScenes (6 views) and evaluate on a dataset with drastically different camera configurations (e.g., front-facing only or fisheye lenses) to test the prior-free claim.
2. **Attention Ablation on Temporal Consistency:** Systematically disable each attention type (Intra-view, Cross-view, Cross-frame) during inference on a video sequence and quantify the impact on temporal stability of reconstructed 3D points.
3. **Scaling Factor Sensitivity Analysis:** Train identical models with different coordinate scaling factors (1x, 10x, 100x) on a challenging highway dataset and analyze the trade-off between numerical stability and fine-grained geometry accuracy.