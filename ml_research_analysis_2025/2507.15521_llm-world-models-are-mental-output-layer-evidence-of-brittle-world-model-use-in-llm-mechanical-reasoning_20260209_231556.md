---
ver: rpa2
title: 'LLM world models are mental: Output layer evidence of brittle world model
  use in LLM mechanical reasoning'
arxiv_id: '2507.15521'
source_url: https://arxiv.org/abs/2507.15521
tags:
- system
- pulley
- systems
- llms
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) construct
  and manipulate internal "world models" or rely solely on statistical associations
  in their output layer token probabilities. The authors adapt cognitive science methodologies
  from human mental models research to test LLMs on pulley system problems using TikZ-rendered
  stimuli across three studies.
---

# LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning

## Quick Facts
- arXiv ID: 2507.15521
- Source URL: https://arxiv.org/abs/2507.15521
- Authors: Cole Robertson; Philip Wolff
- Reference count: 9
- Primary result: LLMs employ pulley-counting heuristics for mechanical reasoning but lack fine-grained structural connectivity representations, with performance collapsing when surface cues are removed

## Executive Summary
This paper investigates whether large language models construct internal world models or rely solely on statistical associations when reasoning about mechanical systems. Using cognitive science methodologies adapted from human mental models research, the authors test LLMs on pulley system problems using TikZ-rendered stimuli across three studies. The findings suggest LLMs can exploit statistical correlations between pulley count and mechanical advantage and distinguish functional from obviously jumbled systems, but fail when both comparison systems appear structurally coherent. This indicates models manipulate approximate spatial representations sufficient for coarse discrimination but lack the facility to reason over nuanced structural connectivity.

## Method Summary
The study uses TikZ-rendered pulley system diagrams to evaluate LLM mechanical reasoning across three studies with claude-3-opus-20240229, gpt-4-0314, gpt-4-vision-preview, and gpt-4o-2024-11-20. Study 1 tested 810 unique pulley systems (MA=1-5) with distractor elements, asking models to estimate mechanical advantage. Studies 2-3 used 30 pairs each to compare functional vs. jumbled systems and functional vs. connected-but-non-force-transferring systems. Models were prompted with temperature=0.5, max_tokens=2500, and specific formatting instructions. Performance was evaluated through exact accuracy, correlation with ground truth, regression analysis, and binary classification metrics.

## Key Results
- Study 1: Models showed marginal but significant above-chance performance in estimating MA, with estimates correlating significantly with ground truth (r=0.27-0.40), but regression revealed pulley count was the strongest predictor (β=0.168-0.464)
- Study 2: Models distinguished functional from jumbled systems with high accuracy (F1=0.8), identifying the functional system as having greater MA
- Study 3: When comparing functional systems to connected-but-non-force-transferring systems, performance collapsed to chance (F1=0.46), suggesting models cannot reason over nuanced structural connectivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs approximate mechanical advantage through statistical correlation heuristics rather than explicit physical simulation.
- Mechanism: Token-level predictions leverage surface-level statistical regularities (e.g., pulley count correlates with MA) from training data rather than constructing and manipulating internal world models. The paper shows number of pulleys was the strongest predictor of model estimates (β = 0.168–0.464 across models), while true MA was often negative or non-significant when controlling for pulley count.
- Core assumption: Training corpora contain sufficient co-occurrences of pulley count and mechanical advantage discussions for statistical learning.
- Break condition: When pulley count no longer predicts MA (e.g., systems with redirect pulleys that don't contribute to force transfer), heuristic fails. Study 3 confirms this—performance drops to chance when visual complexity matches functional systems.

### Mechanism 2
- Claim: LLMs encode coarse spatial relations sufficient for gross distinction tasks but lack fine-grained structural connectivity representations.
- Mechanism: Networks develop approximate spatial representations from training data that support "gestalt" level discrimination (functional vs. obviously jumbled), but these representations lack the fidelity required for force-transfer reasoning.
- Core assumption: TikZ code provides sufficient spatial structure signals through coordinate patterns and drawing commands.
- Break condition: When both comparison stimuli appear structurally coherent (Study 3), coarse spatial representation becomes insufficient, exposing representational brittleness.

### Mechanism 3
- Claim: Latent representations may exist but are incompletely coupled to output layer reasoning.
- Mechanism: Models can represent system features implicitly (influencing behavior) without being able to articulate or reliably reason over them at the output layer—suggesting a dissociation between what the network "knows" and what it can express.
- Core assumption: Output tokens don't fully reflect internal network representations.
- Break condition: Without mechanistic interpretability tools, this remains an inference from behavior rather than a demonstrated mechanism.

## Foundational Learning

- Concept: Mechanical Advantage (MA)
  - Why needed here: Core evaluation metric; MA quantifies force reduction in pulley systems (MA = input distance / load displacement). Understanding that MA depends on rope routing and supporting line count—not just pulley count—is essential to interpret Study 1 and 3 failures.
  - Quick check question: If a system has 4 pulleys but only 2 supporting rope segments, what's the likely MA?

- Concept: Mental Models vs. Statistical Pattern Matching
  - Why needed here: The central theoretical question—do LLMs construct manipulable internal representations (mental models) or rely on surface correlations? Human mental simulation involves step-wise animation of physical systems; the paper tests whether LLMs show analogous capacity.
  - Quick check question: What behavioral signature would distinguish mental simulation from statistical association in a novel pulley configuration?

- Concept: Out-of-Distribution (OOD) Evaluation
  - Why needed here: The study uses TikZ-rendered pulley systems specifically because they're unlikely to appear in training data, testing generalization rather than retrieval. This paradigm guards against false positives from memorization.
  - Quick check question: Why is TikZ code a better OOD test than natural language descriptions of pulley systems?

## Architecture Onboarding

- Component map: TikZ tokens -> latent spatial representation -> heuristic selection (count-based vs. connectivity-based) -> output prediction
- Critical path: The bottleneck appears at the connectivity representation stage, where coarse spatial encoding fails to support fine-grained force-transfer reasoning.
- Design tradeoffs:
  - Training on more mechanical diagrams might improve performance, but risks memorization rather than genuine world model acquisition
  - Explicit world model modules (LeCun 2022) would trade end-to-end simplicity for robustness
  - Interpretability integration could reveal whether failures are representational or reasoning-based
- Failure signatures:
  - High confidence with incorrect reasoning (Study 2: correct answers with hallucinated justifications)
  - Negative correlation between mirrored trial accuracy (Study 3: r = -0.28), indicating unstable representations
  - Performance collapse when surface cues (disorganization) are removed
- First 3 experiments:
  1. Control probe: Test same models on natural language descriptions of identical pulley systems to isolate representation modality effects
  2. Ablation study: Vary pulley count independently from MA (more redirect pulleys) to quantify heuristic vs. genuine reasoning contributions
  3. Interpretability integration: Apply sparse autoencoder probing (per Templeton et al.) to identify whether force-transfer concepts exist in network circuitry but fail to influence outputs

## Open Questions the Paper Calls Out

- Question: Are the observed performance patterns the result of true out-of-distribution reasoning or memorized training data?
  - Basis in paper: The Limitations section states there is "uncertainty regarding whether tested pulley stimuli are truly out-of-distribution; exposure during training cannot be ruled out."
  - Why unresolved: The authors could not audit the training data of proprietary models (e.g., GPT-4, Claude) to confirm these specific TikZ pulley diagrams were not present during pre-training.
  - What evidence would resolve it: Testing models on entirely novel synthetic diagramming languages or auditing training corpora to verify the absence of specific structural patterns.

- Question: Do LLMs possess brittle world models for other physical systems beyond pulleys?
  - Basis in paper: The General Discussion suggests that "extending the paradigm to other physical systems (e.g., levers, gear trains) may help assess the breadth of these internal representations."
  - Why unresolved: This study only assessed mechanical reasoning within the narrow domain of pulley systems represented by TikZ code.
  - What evidence would resolve it: Replicating the three-study paradigm (heuristic check, connectivity check, force transfer check) using diagrams of levers, gears, or electrical circuits.

- Question: Can interpretability techniques identify the specific neural circuits responsible for mechanical reasoning?
  - Basis in paper: The authors advocate for moving from a "cognitive approach to a neuro-cognitive science of AI," specifically calling for "interpretability research which unpacks knowledge represented in deep network weights."
  - Why unresolved: Probing output tokens cannot deconfound latent world models from statistical token probabilities; only internal network analysis can verify if a "mental model" is explicitly encoded.
  - What evidence would resolve it: Applying methods like activation patching or feature extraction (e.g., identifying "pulley" or "force" features) to map internal activations to mechanical ground truth.

## Limitations

- The TikZ code may encode statistical regularities that LLMs learn as generic spatial reasoning heuristics, potentially confounding the OOD evaluation.
- The dissociation between latent knowledge and output layer reasoning remains speculative without mechanistic interpretability validation.
- The relatively small sample sizes in Studies 2-3 (30 pairs each) and binary nature of the tasks may not fully capture the richness of mechanical reasoning failure modes.

## Confidence

- High Confidence: Study 1 findings showing pulley-counting heuristic use (β = 0.168–0.464 for pulley count vs. non-significant or negative MA coefficients).
- Medium Confidence: Study 2 and 3 results suggesting representational brittleness, though interpretation requires additional validation.
- Low Confidence: Mechanism 3 claims about latent knowledge not reflected in outputs, as this interpretation is unverified without circuit-level analysis.

## Next Checks

1. **Modality Transfer Test**: Evaluate the same pulley systems presented as natural language descriptions rather than TikZ code. If models show similar performance patterns, this would strengthen the case that deficits are world-model-related rather than TikZ-specific.

2. **Pulley Count Ablation**: Create matched pairs where pulley count correlates with MA in one system but not the other (e.g., systems with redirect pulleys). If performance drops when pulley count no longer predicts MA, this would validate the heuristic explanation over genuine physical reasoning.

3. **Mechanistic Probing**: Apply sparse autoencoder interpretability tools to identify whether force-transfer concepts exist in network representations. If the necessary concepts are present but fail to influence outputs, this would provide direct evidence for the latent knowledge hypothesis.