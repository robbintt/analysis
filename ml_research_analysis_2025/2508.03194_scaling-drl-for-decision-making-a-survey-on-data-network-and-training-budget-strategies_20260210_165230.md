---
ver: rpa2
title: 'Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget
  Strategies'
arxiv_id: '2508.03194'
source_url: https://arxiv.org/abs/2508.03194
tags:
- scaling
- learning
- data
- training
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews scaling strategies for deep
  reinforcement learning across data, network, and training budget dimensions. It
  identifies parallel data collection and synthetic generation as key methods for
  enhancing sample efficiency, while network scaling through width expansion, transformer
  architectures, and ensemble methods improves model capacity and stability.
---

# Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies

## Quick Facts
- arXiv ID: 2508.03194
- Source URL: https://arxiv.org/abs/2508.03194
- Authors: Yi Ma; Hongyao Tang; Chenjun Xiao; Yaodong Yang; Wei Wei; Jianye Hao; Jiye Liang
- Reference count: 27
- Primary result: Systematic review of scaling strategies across data, network, and training budget dimensions in DRL

## Executive Summary
This survey provides a comprehensive taxonomy of scaling strategies for deep reinforcement learning, categorizing approaches along three dimensions: data scaling (parallel collection, synthetic generation), network scaling (width expansion, transformer architectures, ensemble methods), and training budget scaling (distributed training, high replay ratios, auxiliary objectives). The authors identify key trade-offs between performance gains and computational costs, emphasizing the need for adaptive frameworks to balance scalability with efficiency. They also explore scaling's role in large language model training and outline critical open challenges, particularly the interdependencies between scaling dimensions and generalization to complex tasks.

## Method Summary
The survey synthesizes existing research on DRL scaling through a generalized off-policy actor-learner framework. It examines three primary scaling dimensions: data scaling via parallel data collection (Ape-X) and synthetic data generation (SYNTHER, Diffusion models); network scaling through width expansion using architectures like SimBa (residual blocks + LayerNorm) and transformers (GTrXL); and training budget scaling via distributed training, high replay ratios (UTD 10-20) with ensemble critics (REDQ) or regularized critics (BRO). The methodology emphasizes validation on decision-making benchmarks (DeepMind Control, Atari, MuJoCo) while highlighting theoretical foundations and practical implementation considerations for each scaling approach.

## Key Results
- Parallel data collection and synthetic generation are identified as primary methods for enhancing sample efficiency
- Network scaling through width expansion and transformer architectures improves model capacity and stability
- Training budget scaling via distributed training and high replay ratios optimizes resource utilization
- Significant trade-offs exist between performance gains and computational costs across all scaling dimensions
- Scaling strategies show varying effectiveness across different DRL domains, with vision-based RL lagging behind state-based methods

## Why This Works (Mechanism)
The survey demonstrates that DRL scaling effectiveness stems from addressing fundamental bottlenecks: data scarcity through parallel collection and synthetic generation, representational limitations through network capacity expansion, and inefficient resource utilization through distributed training and optimized replay mechanisms. The mechanism relies on systematic dimension-specific interventions that compound when applied together, though interdependencies between dimensions remain poorly understood. Success depends on maintaining stability through normalization techniques and adaptive frameworks that can dynamically balance the three scaling dimensions.

## Foundational Learning
**Off-policy actor-learner framework** - Why needed: Provides the baseline architecture for scaling interventions. Quick check: Verify SAC/TD3 implementation converges on DMC Walker-Walk.
**Replay buffer management** - Why needed: Critical for data efficiency and stability. Quick check: Monitor value error on new vs. old data to detect primacy bias.
**Network normalization** - Why needed: Essential for stabilizing large networks. Quick check: Ensure LayerNorm or Batch Renormalization is active in scaled architectures.
**Distributed training** - Why needed: Enables parallel data collection and computation scaling. Quick check: Verify actor-learner communication and synchronization.
**Synthetic data generation** - Why needed: Addresses sample efficiency limitations. Quick check: Compare performance with synthetic vs. real interaction data.

## Architecture Onboarding
**Component map:** Actor -> Replay Buffer -> Learner -> Network (SimBa/Transformer) -> Critic/Actor
**Critical path:** Data collection → buffer filling → learning updates → policy improvement
**Design tradeoffs:** Width vs. depth for network scaling; replay ratio vs. plasticity for budget scaling; synthetic vs. real data quality for data scaling
**Failure signatures:** Performance degradation at high replay ratios (plasticity loss); divergence in large networks without normalization; overfitting in vision-based methods with increased capacity
**First experiments:** 1) Baseline SAC with MLP on DMC Walker-Walk, 2) Replace MLP with SimBa architecture, 3) Increase replay ratio from 1 to 20 with LayerNorm stabilization

## Open Questions the Paper Calls Out
**Open Question 1:** How can adaptive frameworks be designed to jointly optimize the interdependent dimensions of data, network, and training budget scaling?
Basis: Section 7.3 states "Unclear Interdependencies between Scaling Dimensions... Most existing works treat data, network, and training budget scaling as independent axes, but their interdependencies are poorly understood."
Why unresolved: Current research isolates these axes; synthetic data effectiveness relies on policy capacity, and optimal replay ratios vary non-linearly with network width.
What evidence would resolve it: Development of meta-learning or differentiable architecture search framework that dynamically balances three dimensions for Pareto-optimal performance.

**Open Question 2:** Can a unified theory of capacity scaling in RL be established that accounts for non-stationary objectives and exploration-exploitation trade-offs?
Basis: Section 7.3 notes "The Scalability-Efficiency Paradox... A unified theory of capacity scaling in RL — connecting representational power, optimization landscape geometry, and exploration-exploitation trade-offs — remains an open challenge."
Why unresolved: Unlike supervised learning, DRL involves non-stationary data and delayed credit assignment, making standard scaling laws inapplicable.
What evidence would resolve it: Theoretical framework relating network capacity and training dynamics to emergent capabilities, accurately predicting performance scaling.

**Open Question 3:** How can visual reinforcement learning overcome "cross-modal scaling disparities" to achieve scaling benefits comparable to state-based methods?
Basis: Section 7.3 highlights "Cross-Modal Scaling Disparities," noting vision-based RL lags behind state-based methods as pixel inputs exacerbate overfitting when increasing network capacity.
Why unresolved: Increasing network capacity in vision-based RL often leads to overfitting on pixel inputs rather than learning robust spatial-temporal representations.
What evidence would resolve it: Novel architectures or training techniques integrating video diffusion models that allow vision-based agents to scale parameters without performance degradation.

## Limitations
- Lacks empirical validation of claimed relationships between scaling dimensions
- Missing quantitative evidence for effectiveness of scaling methods across domains
- Does not provide derived scaling coefficients for DRL (only references LLMs)
- Critical hyperparameters for proposed architectures unspecified
- No systematic comparison of scaling strategies across different DRL domains

## Confidence
- **High confidence:** Taxonomy of scaling dimensions is well-grounded and internally consistent
- **Medium confidence:** Identification of parallel data collection and synthetic generation as key methods, based on cited works
- **Low confidence:** Specific architectural recommendations and their purported benefits without empirical validation

## Next Checks
1. **Empirical scaling coefficient estimation:** Implement proposed scaling framework on DMC Walker-Walk to measure performance as function of network width, data volume, and compute budget
2. **Ablation study of scaling interventions:** Systematically test individual strategies (LayerNorm at high replay ratios, ensemble critics) to quantify contribution to performance gains
3. **Cross-domain generalization test:** Validate whether scaling strategies effective in control tasks transfer to partially observable domains (Atari) or sparse-reward environments (MuJoCo)