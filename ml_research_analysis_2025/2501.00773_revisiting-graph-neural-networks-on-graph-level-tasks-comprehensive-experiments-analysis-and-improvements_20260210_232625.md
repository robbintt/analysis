---
ver: rpa2
title: 'Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments,
  Analysis, and Improvements'
arxiv_id: '2501.00773'
source_url: https://arxiv.org/abs/2501.00773
tags:
- graph
- node
- graphs
- gnns
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of evaluating and improving graph
  neural networks (GNNs) for graph-level tasks. The authors identify three key issues
  in existing evaluation frameworks: insufficient coverage of data domains, insufficient
  graph-level tasks and scenarios, and lack of a unified evaluation pipeline.'
---

# Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements

## Quick Facts
- arXiv ID: 2501.00773
- Source URL: https://arxiv.org/abs/2501.00773
- Reference count: 40
- Primary result: Proposed GNN model achieves superior performance across 27 graph datasets, demonstrating robustness to noise, imbalance, and few-shot scenarios.

## Executive Summary
This paper addresses critical limitations in evaluating graph neural networks (GNNs) for graph-level tasks by proposing a unified framework that includes diverse datasets from multiple domains, supports various graph tasks, and ensures fair comparisons across challenging scenarios. The authors identify three key issues in existing evaluation frameworks: insufficient coverage of data domains, lack of comprehensive graph-level tasks and scenarios, and absence of a unified evaluation pipeline. To address these gaps, they introduce a novel GNN model that enhances expressivity through a k-path rooted subgraph approach for counting paths and cycles, and improves generalization using adaptive graph contrastive learning that removes unimportant edges. The method is rigorously evaluated against 14 baselines on 27 graph datasets, establishing it as a robust and generalizable solution for graph-level tasks.

## Method Summary
The proposed method combines enhanced expressivity through k-path rooted subgraph extraction with adaptive contrastive learning for improved generalization. The k-path approach identifies all k-length path tuples in a graph, extracts rooted subgraphs, and appends positional identifiers to nodes in the path. This enables counting of specific substructures (paths and cycles) that standard GNNs struggle with. For generalization, the method calculates edge importance based on node degrees and drops unimportant edges to create augmented views, then applies contrastive learning with task-aware consistency regularization. The model is trained to maximize similarity between augmented views while maintaining consistency with original graph predictions.

## Key Results
- Achieves superior performance across 27 graph datasets against 14 baselines
- Demonstrates strong robustness to noisy graphs, imbalanced data, and few-shot learning conditions
- Near-perfect counting performance on synthetic datasets (MAE approaching 0.0)
- Robustness to 20-30% random edge deletion on real-world datasets

## Why This Works (Mechanism)

### Mechanism 1: Expressivity via k-Path Rooted Subgraphs
- **Claim**: Enhancing standard GNNs with a k-path rooted subgraph approach improves the model's ability to count specific substructures (paths and cycles), which standard MPNNs often fail to distinguish.
- **Mechanism**: The model extracts subgraphs rooted at every k-length path tuple, appends positional identifiers to nodes in the path, and aggregates information from the local neighborhood of this path.
- **Core assumption**: Standard GNNs lack the "counting power" to distinguish graphs with identical local neighborhoods but different global substructures.
- **Evidence anchors**: Theoretical proof (Theorem 3.1) shows the model can count k+2-Paths and k+3-Cycles; demonstrated through near-perfect counting performance on synthetic datasets.
- **Break condition**: Performance degrades on graphs where relevant features are not local to k-paths or require global context beyond the extracted subgraph radius.

### Mechanism 2: Generalization via Adaptive Edge Dropping
- **Claim**: Removing "unimportant" edges via adaptive contrastive learning improves generalization by forcing the model to learn robust structural patterns rather than overfitting to noise or spurious connections.
- **Mechanism**: Edge importance is calculated based on the degrees of its endpoint nodes (imp_{v,u} = log((D(v)+D(u))/2 + 1)), with lower-degree edges dropped with higher probability to create augmented views.
- **Core assumption**: High-degree nodes represent structural "bridges" or "hubs" crucial for functionality across domains, while low-degree nodes are less critical or potentially noisy.
- **Evidence anchors**: Demonstrated robustness to edge deletion and noise across multiple datasets; the dropping probability equation is explicitly defined.
- **Break condition**: Critical features encoded in low-degree edges may be deleted, risking loss of important signal.

### Mechanism 3: Consistency-aware Regularization
- **Claim**: Combining contrastive loss with task-aware consistency loss stabilizes training, particularly for regression tasks where standard contrastive learning may conflict with quantitative changes in properties.
- **Mechanism**: The model maximizes similarity between augmented views while ensuring predictions remain consistent with the original graph, using ReLU-based loss for regression to account for decreased substructure counts after edge dropping.
- **Core assumption**: Augmentation preserves the label (classification) or changes the label predictably (regression).
- **Evidence anchors**: Distinct consistency loss defined for regression vs. classification; Figure 3 shows robustness to noise attributed to this training scheme.
- **Break condition**: If augmentation is too aggressive, the consistency loss forces prediction of the original label for a graph that structurally no longer supports it.

## Foundational Learning

- **Concept**: **Weisfeiler-Lehman (WL) Isomorphism Test & GNN Expressivity**
  - **Why needed here**: The paper explicitly frames its contribution around enhancing "expressivity" and counting power, which is directly tied to the limitations of the WL test in standard GNNs.
  - **Quick check question**: Why would a standard GCN fail to distinguish a 6-cycle from two separate 3-cycles if node features are identical?

- **Concept**: **Graph Contrastive Learning (GCL)**
  - **Why needed here**: The proposed method relies on GCL principles (generating views, maximizing similarity) to achieve generalization without labels.
  - **Quick check question**: How does the "positive view" generation strategy in this paper differ from standard random edge dropping?

- **Concept**: **Graph-Level Readout / Pooling**
  - **Why needed here**: The model must aggregate node/subgraph representations into a single graph label; the paper discusses limitations of simple pooling vs. hierarchical/subgraph methods.
  - **Quick check question**: What is the "Readout" function used in Equation (19) to combine the k-path rooted subgraph representations?

## Architecture Onboarding

- **Component map**: Input -> Augmenter (edge importance -> edge dropping -> views) -> Subgraph Sampler (k-tuples -> rooted subgraphs -> ID features) -> Encoder (GNN processes subgraphs) -> Aggregator (Readout functions combine embeddings) -> Loss Heads (Contrastive + Consistency)

- **Critical path**: The k-tuple extraction and subgraph feature augmentation is the rate-limiting step. Unlike standard GNNs, this architecture effectively multiplies the batch size by the number of k-paths per graph.

- **Design tradeoffs**:
  - **Accuracy vs. Efficiency**: The k-path approach increases expressivity (Table 4 shows near-perfect counting) but significantly increases memory/compute load (Table 5 shows high memory usage compared to GCN).
  - **Heuristic vs. Learned Augmentation**: Using degree for edge importance is a fixed heuristic (Section 3.1). It is computationally cheaper than learning edge importance but may be brittle for domain-specific graphs where degree does not correlate with structural importance.

- **Failure signatures**:
  - **OOM (Out of Memory)**: Likely to occur on dense graphs where the number of k-paths explodes (Table 5 shows OOM on REDDIT-B for similar models).
  - **Underfitting on Small Molecules**: If k is set too high relative to the graph diameter, the "subgraph" becomes the whole graph, negating the local focus benefits.

- **First 3 experiments**:
  1. **Expressivity Check (Regression)**: Run on the "4-Cycle" or "5-Path" datasets (Table 4) to verify if the MAE approaches 0.0, confirming the theoretical counting power.
  2. **Noise Robustness (Classification)**: Perturb IMDB-B or ENZYMES with 20-30% random edge deletion (Figure 3) to verify if the adaptive augmentation provides resilience over baselines like GCN/GIN.
  3. **Ablation on k**: Vary k (e.g., k=1 vs k=3) on a dataset with complex substructures to find the breaking point where computational cost outweighs expressivity gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical expressivity of the k-path rooted subgraph GNN be extended to provably count or distinguish more complex substructures beyond paths and cycles?
- Basis in paper: Theorems 3.1 and 3.3 provide strict proofs only for counting k+2-Paths and k+3-Cycles, leaving the model's capability regarding other motifs unverified.
- Why unresolved: The theoretical analysis establishes a lower bound of expressivity ("at a minimum") but does not define the upper bound or isomorphic distinguishing power for general subgraphs.
- What evidence would resolve it: A formal proof of equivalence to the Weisfeiler-Leman hierarchy or empirical results on datasets requiring the counting of complex motifs like cliques or stars.

### Open Question 2
- Question: Does the degree-based edge importance heuristic fail to generalize to domains where structural role is not correlated with node degree (e.g., regular grids or computer vision graphs)?
- Basis in paper: Section 3.1 explicitly defines edge importance using node degree, justified by domain-specific intuition (e.g., influencers in social networks), which may not hold for all graph types.
- Why unresolved: The evaluation uses social, biology, and chemistry datasets where degree is often a proxy for importance, but excludes domains like geometric meshes or regular lattices where degree is constant.
- What evidence would resolve it: Experiments on regular graph datasets where edge importance should be determined by positional embedding or structural context rather than degree.

### Open Question 3
- Question: Can the computational complexity of the k-path rooted subgraph encoder be reduced to allow scaling to graphs with millions of nodes or extremely dense connectivity?
- Basis in paper: Table 5 shows the proposed model (Ours) incurs high memory costs (e.g., 4327MB on 4-Cycle) and slower inference times compared to simple baselines.
- Why unresolved: The method samples subgraphs for every k-length path, leading to a complexity that scales with O(n Â· n_e^k), which becomes prohibitive for large n or k.
- What evidence would resolve it: Integration of a scalable approximation method (e.g., path sampling) that maintains counting power while operating in linear time relative to edges.

### Open Question 4
- Question: How sensitive is the model's performance to the choice of the path length hyperparameter k relative to the graph diameter?
- Basis in paper: Section 4.3 fixes k=1 for classification and k=3 for regression without providing an ablation study or theoretical guidance on selecting k based on dataset properties.
- Why unresolved: The choice of k determines the size of the rooted subgraph and the theoretical counting power (e.g., k+3-Cycle), but there is no analysis of the trade-off between overhead and accuracy for different k values.
- What evidence would resolve it: A sensitivity analysis reporting performance and runtime changes as k varies across graphs of differing diameters and densities.

## Limitations
- High computational costs due to k-path rooted subgraph approach, using nearly twice the memory of GCN baselines
- Degree-based edge importance heuristic lacks strong empirical validation across diverse domains
- Framework complexity makes implementation and scaling to very large graphs challenging

## Confidence
- **High Confidence**: Analysis of existing GNN evaluation limitations is well-supported; unified framework addresses gaps systematically; robustness to noise/imbalance/few-shot scenarios is convincingly demonstrated
- **Medium Confidence**: Theoretical expressivity improvements are sound, but practical gains beyond counting substructures are less clearly established; adaptive edge dropping effectiveness depends on degree correlation assumption
- **Low Confidence**: Scalability claims limited by computational burden of k-path approach; performance on graphs with diameter smaller than k is not thoroughly addressed

## Next Checks
1. **Ablation Study on k-value**: Systematically test different k values on a dataset with complex substructures to identify the optimal balance between expressivity gains and computational costs
2. **Alternative Edge Importance Metrics**: Compare the degree-based edge dropping with learned edge importance scores to validate whether the heuristic approach is optimal or just computationally convenient
3. **Scalability Benchmark**: Test the method on a large-scale graph dataset (>100k nodes) to quantify the memory and time complexity trade-offs in practice