---
ver: rpa2
title: 'From Context to EDUs: Faithful and Structured Context Compression via Elementary
  Discourse Unit Decomposition'
arxiv_id: '2512.14244'
source_url: https://arxiv.org/abs/2512.14244
tags:
- context
- compression
- structural
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LingoEDU, an explicit context compression
  framework that transforms unstructured text into hierarchical discourse trees using
  Elementary Discourse Units (EDUs). Unlike implicit compression methods that suffer
  from positional bias or explicit token-removal approaches that disrupt local coherence,
  LingoEDU preserves both global structure and fine-grained details through coordinate-anchored
  EDU decomposition.
---

# From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition

## Quick Facts
- arXiv ID: 2512.14244
- Source URL: https://arxiv.org/abs/2512.14244
- Reference count: 40
- LingoEDU achieves DLA 49.60% vs Claude-4-Sonnet +6.45%, TED 4.77

## Executive Summary
This paper introduces LingoEDU, an explicit context compression framework that transforms unstructured text into hierarchical discourse trees using Elementary Discourse Units (EDUs). Unlike implicit compression methods that suffer from positional bias or explicit token-removal approaches that disrupt local coherence, LingoEDU preserves both global structure and fine-grained details through coordinate-anchored EDU decomposition. A lightweight ranking module then selects query-relevant sub-trees for linearization.

The authors construct StructBench, a manually annotated dataset of 248 diverse documents, to evaluate structural understanding. LingoEDU achieves a Tree Edit Distance of 4.77 and Document Level Accuracy of 49.60%, significantly outperforming state-of-the-art LLMs like Claude-4-Sonnet (DLA +6.45%) while maintaining low latency (1.20s per document) and cost ($0.0007 per document). The method reduces hallucinations in downstream tasks: +14.94% on HotpotQA, +51.11% relative improvement on HLE Deep Search, and +47.13% on noisy Chinese web browsing tasks. LingoEDU demonstrates optimal efficiency-robustness balance, particularly in long contexts, and remains compatible with API-based models.

## Method Summary
LingoEDU operates through three stages: EDU decomposition, ranking, and linearization. The decomposition stage uses a bi-level approach that first extracts explicit layout structures (tables, lists, figures) then performs deep semantic segmentation to create hierarchical discourse trees anchored to source EDU indices. A lightweight ranking module (Qwen3-Reranker-0.6B) selects query-relevant sub-trees within a budget constraint (k=10 nodes) using greedy selection. Finally, the linearization stage reorders selected spans by original start indices and concatenates them for downstream use. The model is trained on ~100k synthetic samples using a Solver-Critic refinement loop, followed by supervised fine-tuning on manually annotated documents from StructBench.

## Key Results
- Tree Edit Distance of 4.77 and Document Level Accuracy of 49.60% on StructBench
- +6.45% DLA improvement over Claude-4-Sonnet
- +14.94% accuracy on HotpotQA, +51.11% relative improvement on HLE Deep Search
- Low latency (1.20s per document) and cost ($0.0007 per document)

## Why This Works (Mechanism)
LingoEDU works by explicitly modeling discourse structure through EDU decomposition, which preserves both global coherence and local details that implicit methods lose. The coordinate-anchored approach ensures faithful reconstruction by maintaining precise source-document references, preventing hallucinations. The lightweight ranking module operates efficiently within budget constraints while the linearization protocol preserves original ordering, maintaining semantic flow.

## Foundational Learning
- Elementary Discourse Units (EDUs): Minimal discourse units that preserve both semantic completeness and structural coherence. Why needed: They serve as atomic units for building hierarchical discourse trees. Quick check: Verify EDU boundaries don't split semantic concepts.
- Tree Edit Distance (TED): Measures structural similarity between discourse trees. Why needed: Primary metric for evaluating compression quality. Quick check: Calculate TED between ground truth and generated trees.
- Solver-Critic loop: Iterative refinement framework for synthetic data generation. Why needed: Creates high-quality training data at scale. Quick check: Monitor synthetic data quality through validation metrics.

## Architecture Onboarding
**Component map:** EDU decomposition -> Ranking module -> Linearization -> Downstream tasks

**Critical path:** Document → Bi-level decomposition → Solver-Critic refinement → Synthetic data generation → Model training → Inference pipeline

**Design tradeoffs:** Explicit structure preservation vs. compression efficiency; coordinate anchoring vs. flexibility; synthetic data vs. manual annotation costs.

**Failure signatures:** Hallucinated indices outside source EDU range; fragmented context after linearization; ranking module misses relevant information.

**First experiments:** 1) Test EDU decomposition on 5-10 documents with ground truth; 2) Validate ranking module independently on StructBench subset; 3) Verify synthetic data quality meets training standards.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation pipeline details remain unspecified, requiring ~100k samples for reproduction
- Manual annotation process for StructBench lacks detailed guidelines and inter-annotator agreement metrics
- Long document scalability claims (100k+ words) untested in evaluation

## Confidence
- **High confidence**: Core methodology, metrics (TED=4.77, DLA=49.60%), efficiency figures (1.20s/doc, $0.0007/doc)
- **Medium confidence**: Downstream task improvements, comparison to Claude-4-Sonnet (+6.45% DLA)
- **Low confidence**: Claims about robustness in "noisy Chinese web browsing tasks" without detailed methodology

## Next Checks
1. Reproduce EDU decomposition on 5-10 documents and verify synthetic data quality meets specified standards
2. Validate ranking module independently on StructBench subset to confirm DLA of 49.60%
3. Conduct systematic error analysis on downstream tasks to quantify hallucination reduction vs. relevance improvement