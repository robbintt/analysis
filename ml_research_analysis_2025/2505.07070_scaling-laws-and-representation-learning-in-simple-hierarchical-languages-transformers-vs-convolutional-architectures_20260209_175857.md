---
ver: rpa2
title: 'Scaling Laws and Representation Learning in Simple Hierarchical Languages:
  Transformers vs. Convolutional Architectures'
arxiv_id: '2505.07070'
source_url: https://arxiv.org/abs/2505.07070
tags:
- data
- hidden
- learning
- training
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how neural language models learn the hierarchical
  structure of languages through next-token prediction. The authors introduce the
  Random Hierarchy Model (RHM), an analytically tractable ensemble of probabilistic
  context-free grammars that captures the compositional structure of natural language.
---

# Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures

## Quick Facts
- **arXiv ID**: 2505.07070
- **Source URL**: https://arxiv.org/abs/2505.07070
- **Reference count**: 0
- **Key outcome**: CNNs exhibit faster performance scaling than transformers on hierarchical language data, with sample complexity P ~ m^ℓ vs P ~ m^2ℓ for depth ℓ structure.

## Executive Summary
This paper investigates how neural language models learn hierarchical structure through next-token prediction on synthetic data generated by the Random Hierarchy Model (RHM). The authors develop a theoretical framework based on token correlations to predict learning dynamics. They demonstrate that convolutional networks outperform transformers on this task, attributed to architectural alignment with the generative process through locality and weight sharing. The study provides both theoretical predictions and empirical validation of scaling laws, revealing how architectural biases shape representation learning efficiency.

## Method Summary
The authors introduce the Random Hierarchy Model (RHM), an ensemble of probabilistic context-free grammars with fixed tree topology, to generate synthetic hierarchical language data. They train hierarchical CNNs and transformers on last-token prediction tasks, comparing learning dynamics through test loss scaling. CNNs use fixed local receptive fields matching the tree structure, while transformers employ global self-attention. The theoretical framework predicts sample complexity based on correlation detection between tokens at different tree distances, with CNNs achieving better scaling due to weight sharing that exploits translation-invariant structure.

## Key Results
- CNNs show significantly faster decay of test loss with training steps than transformers on RHM data, following P ~ m^ℓ vs P ~ m^2ℓ scaling
- Both architectures progressively encode deeper hierarchical structure during training, but CNNs achieve this more efficiently
- Hidden representations become invariant to rule replacements (preserving symbol identity) but sensitive to symbol replacements, evidencing staged learning of structure
- Locally connected networks (without weight sharing) initially match CNNs but transition to transformer-like scaling, confirming weight sharing as the key advantage

## Why This Works (Mechanism)

### Mechanism 1
Deep networks learn hierarchical structure sequentially, one layer at a time, by exploiting observable token correlations. Token correlations decay exponentially with tree distance. Networks detect correlations between context tuples and target token to cluster spans belonging to same hidden variable. Once depth-ℓ structure is inferred, correlations with tuples of inferred hidden variables become accessible. Training dynamics allow networks to exploit detectable correlations before memorizing noise—a distributional simplicity bias.

### Mechanism 2
Weight sharing in convolutional architectures provides access to stronger correlations, improving sample complexity from P ~ m^2ℓ to P ~ m^ℓ. In RHM, identical production rules apply at all positions within a level. CNNs exploit this by sharing weights across positions, allowing them to aggregate statistical evidence from all occurrences of equivalent hidden variables. This enables direct correlation with tuples of inferred hidden variables (shorter tree distance = stronger signal) rather than observable tokens.

### Mechanism 3
Architectural alignment between model structure and generative process determines scaling efficiency. CNNs mirror the RHM's fixed tree topology through local receptive fields (filter size = stride = branching factor s) and hierarchical pooling. Each CNN layer corresponds to one level of the derivation tree. Transformers use global attention, which is architecturally agnostic and must learn the locality structure from data.

## Foundational Learning

- **Probabilistic Context-Free Grammars (PCFGs)**: The RHM is an ensemble of PCFGs; understanding derivation trees, production rules, and hidden vs. terminal symbols is essential to grasp what networks are learning.
  - Quick check: Can you explain how a derivation tree represents the hierarchical generation of a sentence, and what makes a grammar "context-free"?

- **Sample complexity for correlation detection**: The paper's theoretical contribution links correlation strength (decaying with tree distance) to minimum training samples needed for inference at each depth.
  - Quick check: Given correlation variance ~1/v²(vm)^d, how does increasing vocabulary size v affect samples needed to detect a correlation at tree distance d?

- **Translation invariance in neural architectures**: The CNN advantage hinges on weight sharing exploiting positional equivalence; distinguishing this from mere locality is critical.
  - Quick check: How does a locally connected network differ from a convolutional network, and what statistical property of data would make this difference matter?

## Architecture Onboarding

- **Component map**: RHM generator (L=depth, s=arity, v=vocab, m=rules) -> Hierarchical CNN (L conv layers, filter=stride=s, width=512) -> Test loss vs. steps; RHM generator -> Transformer (L layers, d_K=512, 16 heads, MHA only) -> Test loss vs. steps.

- **Critical path**: Generate RHM dataset with fixed (L, s, v, m); ensure mv ≤ v^s (unambiguity). Train models in online setting with cross-entropy loss. Monitor test loss vs. training steps; identify transitions between s^ℓ-gram performance plateaus. Probe hidden representations using symbol/rule replacement transformations to verify hierarchical encoding.

- **Design tradeoffs**: CNNs offer 2× better scaling exponent but require known tree structure. Transformers are flexible but sample-inefficient on rigid hierarchies. Width must be sufficient to avoid bottlenecks; depth must match or exceed L to represent full hierarchy.

- **Failure signatures**: Loss plateaus above s^L-gram baseline indicates failing to learn deepest structure. LCN matching transformer rather than CNN after early stages suggests weight sharing not providing expected benefit. No staged transitions in loss curve may indicate data lacks hierarchical structure or model is memorizing.

- **First 3 experiments**:
  1. Train depth-4 CNN and transformer on RHM with (L=4, s=2, v=16, m=4). Verify CNN achieves ~P^(-log f / log m) scaling vs. transformer's ~P^(-log f / 2 log m).
  2. Compare CNN vs. LCN on same data. Confirm LCN initially matches CNN but transitions to transformer-like scaling as in Fig. 8.
  3. At multiple training checkpoints, compute cosine similarity q_T(h^L) for symbol and rule replacements at each depth. Verify staged acquisition of invariance to rule replacements.

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scaling differ between Transformers and CNNs when data exhibits variable tree topologies rather than the fixed regular structure of the RHM? The authors state a natural extension is to consider variable tree topologies, suggesting this might be better exploited by attention mechanisms.

### Open Question 2
Can the correlation-based theoretical framework be extended to characterize learning in models that capture context-sensitive dependencies? The conclusion proposes incorporating context-sensitive dependencies to push beyond the current context-free structure.

### Open Question 3
Do the architectural biases and scaling laws identified in synthetic RHM data generalize to real-world hierarchical datasets like natural language? The authors write that applying analyses to real-world datasets would test the generality of the framework.

## Limitations
- Theoretical framework assumes sequential depth-by-depth learning without rigorous justification of this trajectory
- Architecture-specific advantages demonstrated only on fixed-tree RHM data, not on natural language
- Sample complexity predictions rely on idealized assumptions (infinite width, online learning, SGD with lr=1)

## Confidence
- **High confidence**: Empirical demonstration that CNNs outperform transformers on RHM data with statistically significant differences in scaling exponents
- **Medium confidence**: Theoretical mechanism explaining why CNNs benefit from weight sharing (LCN ablation experiment provides strong evidence)
- **Low confidence**: Claim that architectural alignment is the primary driver of performance differences (doesn't address whether transformer flexibility could eventually match CNN performance)

## Next Checks
1. Test sequential learning hypothesis by training networks with controlled noise levels to empirically verify depth-ℓ structure is only learned after depth-(ℓ-1) structure is mastered
2. Vary architectural priors systematically beyond LCN ablation—test partial weight sharing and CNNs with learned receptive fields to isolate which aspects of architectural alignment contribute to the performance gap
3. Test on non-rigid hierarchical data by generating RHM variants with variable tree topologies, context-sensitive dependencies, or non-uniform rule distributions to quantify how much the CNN advantage degrades when assumptions are violated