---
ver: rpa2
title: On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer
  Detection
arxiv_id: '2511.07700'
source_url: https://arxiv.org/abs/2511.07700
tags:
- dataset
- calibration
- adae
- fairness
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses performance disparities in AI-based melanoma
  detection across demographic subgroups by incorporating calibration as a complementary
  metric to AUROC-based fairness benchmarks. The authors evaluated three top-performing
  ISIC 2020 Challenge models (ADAE, 2nd Place, 3rd Place) on both ISIC 2020 and PROVE-AI
  datasets, focusing on subgroups defined by sex, age, and Fitzpatrick Skin Tone.
---

# On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection

## Quick Facts
- arXiv ID: 2511.07700
- Source URL: https://arxiv.org/abs/2511.07700
- Reference count: 38
- Primary result: Calibration metrics reveal subgroup biases missed by AUROC-based fairness benchmarks in AI melanoma detection

## Executive Summary
This study investigates fairness in AI-based melanoma detection by incorporating calibration metrics alongside traditional AUROC benchmarks. The authors evaluated top-performing ISIC 2020 Challenge models on both ISIC 2020 and PROVE-AI datasets across demographic subgroups. Their analysis revealed that while all models significantly outperformed a baseline ERM algorithm in discrimination, calibration analysis exposed systematic overestimation biases, particularly related to age. The findings demonstrate that high AUROC scores alone are insufficient for ensuring equitable AI healthcare solutions, highlighting the critical need for comprehensive fairness auditing that includes both discrimination and calibration metrics.

## Method Summary
The study employed a three-part fairness audit pipeline using pre-trained models from the ISIC 2020 Challenge. Discrimination analysis computed AUROC per subgroup using DeLong's test for statistical comparison. Calibration analysis applied a score-based CUSUM test with Variable Importance plots to detect and explain miscalibration. The CUSUM test used residual models trained on metadata and embeddings to predict true event rates, identifying subgroups where predictions diverged from observed outcomes. Models were evaluated on both ISIC 2020 (10,982 images) and PROVE-AI (603 images with Fitzpatrick Skin Tone data) datasets, with subgroups defined by sex, age, and Fitzpatrick Skin Tone.

## Key Results
- All evaluated models significantly outperformed ERM baseline in discrimination on ISIC 2020
- ADAE ensemble showed the best overall performance, particularly on PROVE-AI dataset
- Calibration analysis revealed ADAE and constituent models were prone to overestimation, with age being the most significant factor for miscalibration
- 3rd Place model showed performance not significantly better than ERM on PROVE-AI, suggesting overfitting to ISIC data
- Large ensemble size (90 models) in ADAE retained discrimination advantage over smaller ensembles on out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1
Calibration metrics can reveal subgroup biases that discrimination metrics (AUROC) miss. AUROC measures a model's ability to rank positive cases higher than negative ones but is insensitive to whether predicted probability scores are accurate. A model can have high AUROC but systematically over- or under-predict risk for specific subgroups. The score-based CUSUM test detects such miscalibration, while Variable Importance plots identify contributing features like age.

### Mechanism 2
Incorporating clinical metadata during model training may improve discrimination but can introduce calibration trade-offs, particularly related to age. ADAE was trained with metadata and achieved excellent discrimination but was prone to overestimating risk. Variable Importance plots identified age as the most significant factor for this miscalibration. Constituent models showed that metadata's role in calibration is complex, with some showing less age-driven overestimation.

### Mechanism 3
Model performance degrades on out-of-distribution data, but large ensembles may retain discrimination advantage over simpler models. When applied to PROVE-AI dataset, all models showed performance drops compared to ISIC 2020. However, ADAE's large ensemble retained significantly better discrimination than 2nd and 3rd place models, which were no better than ERM baseline on PROVE-AI, indicating that model choice affects severity of degradation.

## Foundational Learning

- **Calibration vs. Discrimination in Clinical AI**: Why needed - The core premise is that high discrimination (AUROC) is insufficient for clinical fairness; predictions must also be calibrated to avoid systematic over/under-treatment. Quick check - Can a model with AUROC of 0.95 still be unfair? If so, how?

- **Score-based CUSUM Test for Calibration**: Why needed - This is the specific method used to detect miscalibrated subgroups. Understanding it is key to interpreting results and applying the approach. Quick check - What does a rising CUSUM test statistic indicate, and what does the Variable Importance plot reveal?

- **Out-of-Distribution (OOD) Generalization**: Why needed - The study evaluates models on a new dataset with different demographics. Understanding OOD challenges explains performance drops observed. Quick check - Why might a model that performs well on one dataset fail or show bias on another?

## Architecture Onboarding

- **Component map**: Pre-trained Models (ADAE, 2nd Place, 3rd Place, ERM) -> Datasets (ISIC 2020, PROVE-AI) -> Fairness Audit Pipeline (Discrimination Analysis -> Calibration Analysis -> Result Analysis)

- **Critical path**: Load pre-trained models and datasets -> Run inference to get predictions and extract intermediate embeddings -> Perform discrimination analysis (AUROC, DeLong's test) -> For calibration: Split data, train residual models, compute CUSUM test statistic, generate VI plots -> Analyze results to identify miscalibrated subgroups and their drivers

- **Design tradeoffs**: Complexity vs. Interpretability (ADAE ensemble vs. ERM baseline), Metric Choice (AUROC vs. Calibration - both needed but calibration requires more data), Subgroup Definition (predefined vs. automatic discovery)

- **Failure signatures**: High AUROC, Low Specificity (over-prediction), Significant AUROC Drop on OOD Data (poor generalization), CUSUM Test Fails to Converge (small subgroup sizes or insufficient metadata)

- **First 3 experiments**: Reproduce Baseline Comparison (run ERM, 2nd, 3rd place models on ISIC 2020), OOD Discrimination Analysis (apply all models to PROVE-AI, compute subgroup AUROCs), Calibration Driver Ablation (re-run CUSUM test while removing one metadata feature at a time)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does training deep learning models with varying combinations of demographic and clinical metadata impact the trade-off between discrimination and calibration? The study evaluated pre-trained models as black boxes without ablation studies to isolate causal effects of specific metadata inclusion.

- **Open Question 2**: Do state-of-the-art melanoma detection models exhibit significant calibration and discrimination disparities across darker Fitzpatrick Skin Tones (FST 4-6)? The PROVE-AI dataset lacked sufficient samples for FST categories 4, 5, and 6, preventing statistical analysis of fairness across darker skin tones.

- **Open Question 3**: Can calibration testing methods be developed to better detect miscalibration within intersectional subgroups? The current CUSUM-based method may lack sensitivity when subgroup sizes are small or when statistical signals are diluted across overlapping identities.

- **Open Question 4**: Is the substantial computational cost of large ensembles like ADAE justified by significant improvement in fairness metrics compared to simpler baseline models? The study focused on predictive performance but did not quantify resource consumption required to achieve results relative to ERM baseline.

## Limitations
- Small sample size in PROVE-AI dataset (603 images) limits detection of intersectional biases and reduces statistical power for calibration analysis
- Complex ensemble architectures make exact reproduction of ADAE's 90-model system challenging
- External validity limited to single out-of-distribution dataset (PROVE-AI)

## Confidence
- **High Confidence**: AUROC-based discrimination results on ISIC 2020 dataset; CUSUM test methodology for calibration detection
- **Medium Confidence**: OOD performance degradation patterns; age as primary driver of miscalibration (based on VI plots)
- **Low Confidence**: Calibration results on PROVE-AI dataset due to small sample size; intersectional bias detection

## Next Checks
1. **Sensitivity Analysis**: Perform bootstrapping on PROVE-AI dataset to quantify uncertainty in calibration metrics and test statistical robustness of reported miscalibration patterns

2. **Cross-Dataset Validation**: Apply the complete fairness audit pipeline (AUROC + CUSUM calibration) to an additional external melanoma dataset to verify generalizability of performance degradation and bias patterns

3. **Feature Ablation Study**: Systematically remove individual metadata features (age, sex, image size) from the residual models in the CUSUM analysis to confirm the attribution of miscalibration to specific demographic factors