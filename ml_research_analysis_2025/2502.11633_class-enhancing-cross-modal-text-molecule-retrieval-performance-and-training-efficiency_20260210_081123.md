---
ver: rpa2
title: 'CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training
  Efficiency'
arxiv_id: '2502.11633'
source_url: https://arxiv.org/abs/2502.11633
tags:
- training
- learning
- retrieval
- molecule
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-modal text-molecule retrieval, where
  molecules are represented by SMILES and text descriptions. Existing methods overlook
  adaptive training adjustments and efficiency.
---

# CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training Efficiency

## Quick Facts
- arXiv ID: 2502.11633
- Source URL: https://arxiv.org/abs/2502.11633
- Reference count: 40
- Primary result: CLASS improves cross-modal retrieval Hits@1 by 1.0%-1.9% while reducing training data usage to 84.44%-86.73%

## Executive Summary
This paper introduces CLASS, a curriculum learning framework for cross-modal text-molecule retrieval that addresses efficiency limitations in existing methods. The approach quantifies sample difficulty based on modality similarities, progressively introduces training samples from easy to difficult, and employs adaptive intensity learning to adjust training objectives dynamically. Experiments on ChEBI-20 demonstrate CLASS improves retrieval performance while significantly reducing early-stage training data requirements compared to state-of-the-art baselines.

## Method Summary
CLASS operates by first encoding text descriptions using SciBERT and molecules using Mol2vec, then quantifying sample difficulty through pairwise cosine similarity comparisons across both modalities. Samples are sorted by difficulty and introduced progressively via a linear scheduler that controls the proportion of training data per epoch. An adaptive intensity mechanism scales the loss function throughout training using either sigmoid or inverse proportion curves. The framework wraps around existing contrastive or adversarial models (AMAN, ORMA) without modifying their core alignment objectives, focusing instead on optimizing the training schedule and loss weighting.

## Key Results
- Improves Hits@1 by 1.0%-1.9% on ChEBI-20 dataset
- Reduces training data usage to 84.44%-86.73% compared to baselines
- Improves Mean Rank by 1.76-3.35 positions
- Adaptive intensity curve γ₂ (k/(1+k)) outperforms γ₁ (sigmoid) with 51.1% vs 50.5% Hits@1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Samples with more semantically similar neighbors in the training set pose greater retrieval difficulty for the model.
- Mechanism: For each text-molecule pair, cosine similarity is computed against all other pairs in both text and molecule embedding spaces. The mean similarity is thresholded (σ=0.99), and samples with more neighbors exceeding this threshold are classified as harder. This creates a difficulty ranking: high-similarity samples create more confusable retrieval targets.
- Core assumption: Retrieval difficulty correlates with the density of similar samples in the embedding space; samples with many near-duplicates are harder to distinguish during contrastive learning.
- Evidence anchors:
  - [abstract] "we quantify the sample difficulty considering both text modality and molecule modality"
  - [section 3.3] "samples with a larger number of similar samples pose a greater challenge to the model during training. The model will have more difficulty in learning to distinguish between these similar samples."
  - [corpus] Weak direct corpus support; TextME addresses novel modalities but not difficulty quantification.
- Break condition: If similarity computations in embedding space do not correlate with actual retrieval confusion (e.g., near-duplicates are actually easy for the model), the difficulty ranking becomes noise.

### Mechanism 2
- Claim: Progressive introduction of training samples from easy to difficult improves convergence and reduces early-stage data requirements.
- Mechanism: A linear scheduler controls sample proportion per epoch via λ = α + β·k, where α is initial ratio (e.g., 40%), β is growth factor (e.g., 3%), and k is epoch index. Only the top λ|Z_sort| easiest samples are used at epoch k. This allows the model to establish basic cross-modal alignment before confronting hard negatives.
- Core assumption: Easy samples provide a stable foundation for learning cross-modal representations before the model encounters confusable hard negatives that could destabilize early training.
- Evidence anchors:
  - [abstract] "remarkably reducing the scale of training samples at the early stage of training and improving training efficiency"
  - [section 4.4] "solely using a sample size of 84.44% in the text-molecule retrieval task"
  - [corpus] Adjacent work on curriculum strategies is absent; corpus focuses on modality bridging, not training schedules.
- Break condition: If easy samples do not provide sufficient diversity to learn generalizable representations, the model may overfit to early distributions and fail to adapt when hard samples are introduced.

### Mechanism 3
- Claim: Dynamically scaling loss intensity across curriculum stages prevents overfitting to simple samples while ensuring adequate gradient signal for complex samples.
- Mechanism: A multiplicative factor γ scales the aggregated loss per epoch. Two curves were tested: γ₁ = 1/(1+e^(-k-1)) (sigmoid) and γ₂ = k/(1+k) (inverse proportion). γ₂ outperformed γ₁, attributed to more uniform progression and later convergence.
- Core assumption: Training intensity should be calibrated to sample difficulty; low intensity on easy samples prevents premature convergence, while high intensity on hard samples forces discrimination among confusable pairs.
- Evidence anchors:
  - [section 3.5] "the training of the model is strengthened in the complex sample learning stage"
  - [table 4] γ₂ achieves 51.1% Hits@1 vs. γ₁'s 50.5%
  - [corpus] No corpus evidence directly addresses intensity scaling in curriculum learning.
- Break condition: If the intensity curve grows too slowly, the model may underfit hard samples; if too quickly, early-stage instability may persist.

## Foundational Learning

- Concept: Curriculum Learning (easy-to-hard training)
  - Why needed here: CLASS depends on the premise that neural networks benefit from structured exposure to increasingly difficult samples rather than uniform sampling.
  - Quick check question: Can you explain why training on only easy samples first might harm final performance if the curriculum stops too early?

- Concept: Cross-Modal Contrastive Alignment
  - Why needed here: The backbone models (AMAN, ORMA) align text and molecule embeddings via contrastive or adversarial objectives; CLASS modifies the loss weighting, not the alignment objective itself.
  - Quick check question: Given a batch of text-molecule pairs, how would you compute the contrastive loss that pulls matching pairs together and pushes non-matching pairs apart?

- Concept: Molecular Representations (SMILES, Mol2vec)
  - Why needed here: Molecules are encoded via SMILES strings processed by Mol2vec, which converts graph substructures into embedding vectors analogous to word2vec for molecular "sentences."
  - Quick check question: How does Mol2vec's substructure-based encoding differ from treating SMILES as raw character sequences?

## Architecture Onboarding

- Component map:
  - Multimodal Encoder: SciBERT (text) + Mol2vec (molecule) → h_t, h_m
  - Difficulty Quantification: Pairwise cosine similarity → threshold σ → count similar neighbors N_i → sort Z_sort
  - Sample Scheduler: λ = α + β·k → select top λ|Z_sort| samples for epoch k
  - Adaptive Intensity: γ curve → scale epoch loss L_epoch = γ · Σ(V_i · L_i)
  - Backbone: Any contrastive/adversarial model (AMAN, ORMA) providing base loss L

- Critical path:
  1. Pre-compute all pairwise similarities once before training (O(L²) cost).
  2. Sort training set by difficulty (ascending N_i).
  3. Per epoch: select subset D_k via scheduler, compute backbone loss, scale by γ_k, backprop.

- Design tradeoffs:
  - α/β selection: Higher α uses more data early (faster but potentially noisier); lower α saves compute but risks undertrained early representations. Grid search required per backbone.
  - Similarity threshold σ: 0.99 is conservative (few neighbors counted as similar); lower thresholds increase difficulty granularity but may mislabel easy samples as hard.
  - γ curve choice: γ₂ (k/(1+k)) more uniform; γ₁ (sigmoid) converges faster but underperformed in experiments.

- Failure signatures:
  - Training data ratio never reaches 100%: α + β·E < 100, where E is total epochs. Check that final λ ≥ 1.0.
  - Hits@1 degrades vs. baseline: Likely difficulty ranking is inverted or γ scaling too aggressive early.
  - No time savings: Scheduler not reducing batch size; ensure D_k actually shrinks samples per batch, not just per-epoch count.

- First 3 experiments:
  1. Reproduce baseline (AMAN or ORMA) on ChEBI-20 to establish reference Hits@1, Mean Rank.
  2. Integrate CLASS with fixed α=40, β=3, σ=0.99, γ=γ₂; measure both performance and training time reduction.
  3. Ablate one component (disable adaptive intensity by fixing γ=1) to isolate contribution of scheduler vs. intensity scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating more sophisticated molecular similarity metrics improve the accuracy of sample difficulty quantification compared to the current cosine similarity on Mol2Vec embeddings?
- Basis in paper: [explicit] The authors state in the Conclusion and Limitations, "we plan to investigate more potent molecular similarity calculation methods" because the current method "may not be able to fully capture the intricate structural and chemical properties."
- Why unresolved: The current method relies on simple embedding similarity, which may overlook complex chemical nuances required for precise difficulty estimation.
- What evidence would resolve it: Experiments comparing retrieval performance when using domain-specific similarity metrics (e.g., Tanimoto similarity on fingerprints) versus the current embedding-based method.

### Open Question 2
- Question: Can alternative, potentially learnable intensity curves outperform the fixed sigmoid (γ₁) and inverse proportion (γ₂) functions in coordinating training stages?
- Basis in paper: [explicit] The Conclusion notes, "In the future, we will explore a more effective learning intensity curve for adaptive intensity learning."
- Why unresolved: The study only tested two fixed mathematical functions; a dynamic or learned schedule might adapt better to specific convergence patterns.
- What evidence would resolve it: A comparative study integrating a learned pacing function or a reinforcement learning-based scheduler into the CLASS framework.

### Open Question 3
- Question: How can adaptive intensity learning be extended to dynamically adjust distinct learning tasks (e.g., contrastive vs. alignment losses) within multi-objective training?
- Basis in paper: [explicit] The Conclusion states, "Subsequently, we extend the adaptive intensity learning to adaptively focus on adjusting different learning tasks in training objectives during various stages."
- Why unresolved: The current framework applies intensity learning globally to the loss scalar, rather than modulating individual loss components based on stage.
- What evidence would resolve it: Implementation of a multi-task weighting mechanism within the CLASS scheduler that varies loss weights per task over epochs.

## Limitations

- Computational complexity: The pairwise similarity computation requires O(L²) operations, making it expensive for large datasets
- Domain specificity: The difficulty quantification method may not generalize well to datasets with different molecular diversity or similarity distributions
- Fixed hyperparameters: The effectiveness of α=40%, β=3%, and σ=0.99 parameters may not transfer to other datasets or backbone models without re-tuning

## Confidence

- **High confidence**: The core mechanism of progressive sample introduction (Mechanism 2) is well-established in curriculum learning literature and the empirical results (reduced training data usage to 84.44%-86.73%) are directly measurable.
- **Medium confidence**: The difficulty quantification method (Mechanism 1) shows logical consistency but depends on the assumption that embedding space similarity correlates with retrieval difficulty, which may vary across datasets.
- **Medium confidence**: The adaptive intensity scaling (Mechanism 3) shows measurable improvement (γ₂ achieving 51.1% vs γ₁'s 50.5% Hits@1) but the choice between γ curves lacks theoretical justification.

## Next Checks

1. **Generalization test**: Apply CLASS to a larger molecule dataset (e.g., PubChem) to verify that training data reduction and performance gains scale beyond ChEBI-20.
2. **Ablation on similarity threshold**: Systematically vary σ from 0.90 to 0.99 to quantify sensitivity of difficulty ranking and verify that the method works across different similarity strictness levels.
3. **Cross-backbone validation**: Test CLASS with a different backbone model (e.g., CLIP-based approach) to confirm that curriculum benefits are not specific to AMAN/ORMA architectures.