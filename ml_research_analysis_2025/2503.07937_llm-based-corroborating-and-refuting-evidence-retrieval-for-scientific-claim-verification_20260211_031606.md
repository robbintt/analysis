---
ver: rpa2
title: LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim
  Verification
arxiv_id: '2503.07937'
source_url: https://arxiv.org/abs/2503.07937
tags:
- ciber
- claim
- responses
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CIBER, an extension of Retrieval-Augmented
  Generation (RAG) for scientific claim verification. CIBER uses multi-aspect interrogation
  to probe LLM responses from diverse perspectives, then fuses evidence through weighted
  proportions, information gain, or belief update methods to determine claim validity.
---

# LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim Verification

## Quick Facts
- arXiv ID: 2503.07937
- Source URL: https://arxiv.org/abs/2503.07937
- Authors: Siyuan Wang; James R. Foulds; Md Osman Gani; Shimei Pan
- Reference count: 6
- Primary result: CIBER achieves 0.667 accuracy and 0.623 F1 on combined climate change and vaccination-autism claim datasets

## Executive Summary
This paper introduces CIBER, an unsupervised extension of Retrieval-Augmented Generation (RAG) for scientific claim verification. CIBER uses multi-aspect interrogation to probe LLM responses from diverse perspectives, then fuses evidence through weighted proportions, information gain, or belief update methods to determine claim validity. Evaluated on synthetic and real datasets for climate change and vaccination-autism claims, CIBER achieves accuracy of 0.667 and F1 of 0.623 on combined datasets, outperforming traditional RAG approaches. The system is unsupervised, requires no internal model access, and works with both white-box and black-box LLMs. Performance improves significantly with advanced models like GPT-3.5 and GPT-4 compared to smaller models like GPT-2.

## Method Summary
CIBER extends standard RAG by adding three modules: Multi-Aspect Interrogation (MAI) generates original, agree, and conflict probes; Response Resolution (RR) maps LLM outputs to canonical labels via regex/lexicon; Verdict & Confidence (V&C) fuses K=10 runs per probe using Weighted Proportions, Weighted Information Gain, or Weighted Belief Update (Dempster-Shafer). The system tests three LLMs (GPT-2, GPT-3.5-Turbo, GPT-4-Turbo) on four datasets (CSyn, CReal, ASyn, AReal) with 60 abstracts each, containing claims about climate change and vaccination-autism links.

## Key Results
- CIBER-WIG achieves 0.667 accuracy and 0.623 F1 on combined datasets, outperforming RAG
- Synthetic datasets show 0.85 accuracy vs. real datasets at ~0.59 accuracy
- GPT-3.5 and GPT-4 significantly outperform GPT-2 (near-random at ~0.35 accuracy)
- Conflict probes (PCF) degrade performance by 19-23% with GPT-3.5/4 due to negation ambiguity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Probing an LLM with diverse logical perspectives (Multi-Aspect Interrogation) exposes response inconsistency, which correlates with lower reliability in claim verification.
- **Mechanism:** The system generates "Agree" (PAG) and "Conflict" (PCF) probes regarding a scientific claim. If an LLM supports a claim in a positive probe but fails to refute it in a negative probe, the inconsistency signals uncertainty or hallucination, allowing the system to downgrade confidence or revert to "Neutral."
- **Core assumption:** LLMs generate more consistent outputs when they possess the necessary knowledge (context) than when they are hallucinating.
- **Evidence anchors:**
  - [abstract] "CIBER addresses the inherent uncertainty in Large Language Models (LLMs) by evaluating response consistency across diverse interrogation probes."
  - [section] Page 3: "MAI is designed to assess the consistency of LLM responses under various probes with diverse lexical and logical variations."
  - [corpus] "Retrieval and Argumentation Enhanced Multi-Agent LLMs" supports the efficacy of multi-perspective reasoning in forecasting.
- **Break condition:** If the LLM fails to follow negative instructions (e.g., answering "Yes" to "Is this false?"), the Conflict probes (PCF) may yield misleading signals, degrading performance.

### Mechanism 2
- **Claim:** Aggregating multiple stochastic responses using evidence fusion (Weighted Information Gain or Belief Update) filters out noise more effectively than single-pass generation.
- **Mechanism:** By iterating each probe $K$ times (e.g., 10 runs) and mapping responses to probabilities, the system calculates entropy and information gain. High entropy across responses lowers the confidence score, preventing the system from committing to a hallucinated verdict.
- **Core assumption:** Hallucinations are stochastic and inconsistent, whereas correct reasoning paths are stable across multiple generations.
- **Evidence anchors:**
  - [section] Page 4: "To address this uncertainty, we iterate each probe in PAG and PCF K times... we explored three fusion strategies."
  - [section] Page 8: "CIBER-WIG... exhibited a substantial 14.8% increase in accuracy... compared to RAG."
  - [corpus] "MedRAGChecker" (neighbor) similarly utilizes claim-level verification to isolate unsupported assertions in RAG outputs.
- **Break condition:** If the retrieval context is universally irrelevant, the LLM may hallucinate consistently (low entropy but wrong), causing fusion methods to reinforce a false verdict.

### Mechanism 3
- **Claim:** Anchoring verification in specific retrieved scientific documents (context grounding) reduces the error rate compared to parametric knowledge alone.
- **Mechanism:** Instead of asking the LLM if a claim is true broadly, CIBER asks, "Based on the study described in Paper A, is Claim C true?" This constrains the reasoning space to the provided context.
- **Core assumption:** The LLM's reading comprehension exceeds its internal recall accuracy for specific scientific citations.
- **Evidence anchors:**
  - [section] Page 3: "By anchoring the LLM's responses in a specific scientific study, this contextual grounding enables the LLM to provide more accurate and relevant information."
  - [abstract] "...designed to identify corroborating and refuting documents as evidence for scientific claim verification."
  - [corpus] Evidence in corpus is weak for this specific mechanism; neighbors focus on general RAG rather than specific "anchoring" prompts.
- **Break condition:** If the retrieval step fetches a document that superficially matches keywords but is semantically opposed, the LLM may be "distracted" by the negative context.

## Foundational Learning

- **Concept:** **Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** CIBER is architected as an extension of standard RAG; understanding the baseline retrieval-generation loop is required to see where CIBER inserts its interrogation and fusion modules.
  - **Quick check question:** Can you distinguish between the *retrieval* phase (finding documents) and the *generation* phase (synthesizing an answer) in a standard LLM query?

- **Concept:** **Dempster-Shafer Theory (DST)**
  - **Why needed here:** One of the core fusion strategies (Weighted Belief Update) uses DST to combine evidence masses from different probes. Without this, the formulas for $m(S)$ and $K$ (conflict factor) will be opaque.
  - **Quick check question:** How does Dempster-Shafer theory handle the combination of conflicting evidence compared to simple probability averaging?

- **Concept:** **Information Gain (Entropy)**
  - **Why needed here:** The Weighted Information Gain (WIG) strategy relies on calculating entropy ($E$) to determine the uncertainty of the LLM's response distribution.
  - **Quick check question:** If an LLM outputs "Support" 50% of the time and "Refute" 50% of the time for the same probe, would the Information Gain be high or low?

## Architecture Onboarding

- **Component map:** Input (Claim + Vector Database) -> MAI Module (generates pO, PAG, PCF probes) -> LLM Worker (executes K times) -> RR (Response Resolution via regex/lexicon) -> V&C (Verdict & Confidence via WP, WIG, or WBU)

- **Critical path:** The flow moves from *Prompt Engineering* (MAI) -> *Sampling* (LLM) -> *Parsing* (RR). The paper notes that failure in parsing (specifically for negative probes in GPT-3.5/4) is a bottleneck.

- **Design tradeoffs:**
  - **Black-box vs. White-box:** CIBER trades the depth of internal state analysis for generalizability (works on API-only models like GPT-4).
  - **Cost vs. Accuracy:** Running $K=10$ iterations per probe with multiple probes (PAG + PCF) drastically increases inference cost and latency compared to single-shot RAG.

- **Failure signatures:**
  - **Negative Probe Ambiguity:** As seen on Page 7, GPT-3.5/4 may answer "Yes" to "Is this claim false?" in a way that confirms the claim (interpreting "Yes, it is false" vs "Yes, the claim is true"). The parser must handle these linguistic flips.
  - **Synthetic Data Bias:** Performance on synthetic abstracts (CSyn) is significantly higher (0.85 Acc) than real abstracts (CReal ~0.59 Acc), suggesting the system may struggle with the nuance/density of real scientific text.

- **First 3 experiments:**
  1. **Ablation on Probes:** Run CIBER using only $P_{AG}$ vs. only $P_{CF}$ vs. Both on a small validation set to measure the contribution of logical conflict checking (Page 7, Table 3).
  2. **Fusion Strategy Correlation:** Implement WP, WIG, and WBU simultaneously and calculate the correlation matrix of their verdicts to see if they provide complementary signals (Page 8, Figure 3).
  3. **Parser Robustness Check:** Inject manual "messy" LLM responses (e.g., "According to the abstract, the statement is false") into the RR module to verify the regex parser handles negations correctly before running full inference.

## Open Questions the Paper Calls Out
- **Question:** Can sophisticated ensemble methods for verdict aggregation improve upon the simple majority voting baseline currently used in CIBER?
- **Question:** To what extent does the ambiguity in negation handling (e.g., "Yes" vs. "No" to negative probes) limit the performance of conflict probes (PCF) in advanced LLMs?
- **Question:** How robust is the CIBER framework against misinformation or hallucinated content present within the retrieved documents themselves?

## Limitations
- Performance gap between synthetic (0.85 accuracy) and real datasets (~0.59 accuracy) suggests poor generalization to real scientific literature
- Lack of detailed implementation specifications (embedding models, exact probe templates, grid search parameters) creates uncertainty in faithful reproduction
- Negative probes degrade GPT-3.5/4 performance by 19-23% due to negation ambiguity in response parsing

## Confidence
- **High Confidence:** The core mechanism of multi-aspect interrogation improving response consistency (Mechanism 1) is well-supported by the theoretical framework and ablation results.
- **Medium Confidence:** The effectiveness of weighted fusion strategies (Mechanisms 2 and 3) is demonstrated but may be dataset-dependent, particularly given the synthetic data bias.
- **Low Confidence:** The specific implementation details required for exact reproduction (embedding models, parsing regex patterns, probe templates) are insufficiently specified.

## Next Checks
1. **Synthetic vs. Real Data Gap Analysis:** Replicate the experiments separately on synthetic and real datasets to quantify the generalization gap and identify specific failure modes in real abstracts.

2. **Parser Robustness Testing:** Create a controlled test suite with manually crafted "messy" LLM responses containing various negation patterns and confirm the RR module correctly handles all cases before running full experiments.

3. **Fusion Strategy Correlation:** Implement all three fusion strategies (WP, WIG, WBU) simultaneously and compute inter-strategy correlation coefficients to determine whether they provide complementary evidence or redundant signals.