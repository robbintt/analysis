---
ver: rpa2
title: Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance
  Parameters
arxiv_id: '2507.07941'
source_url: https://arxiv.org/abs/2507.07941
tags:
- learning
- task
- parameters
- nuisance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a late fusion multi-task learning framework
  for semiparametric inference with nuisance parameters, addressing the challenge
  of integrating information from heterogeneous datasets while preserving privacy.
  The framework operates in two steps: initial double machine learning estimators
  are obtained from individual task learning, followed by adaptive aggregation to
  exploit task similarities while remaining robust to task-specific differences.'
---

# Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance Parameters

## Quick Facts
- arXiv ID: 2507.07941
- Source URL: https://arxiv.org/abs/2507.07941
- Authors: Sohom Bhattacharya; Yongzhuo Chen; Muxuan Liang
- Reference count: 40
- Primary result: Framework achieves lower MSE and improved prediction accuracy compared to individual-task learning, with greatest improvements when tasks are highly similar

## Executive Summary
This paper introduces a late fusion multi-task learning framework for semiparametric inference with nuisance parameters across heterogeneous datasets. The method enables distributed estimation while preserving privacy by aggregating summary statistics rather than raw data. The framework operates in two stages: initial double machine learning estimators are computed locally, then adaptively aggregated to exploit task similarities while maintaining robustness to task-specific differences. Theoretical guarantees demonstrate improved convergence rates when nuisance parameters exhibit similarity across tasks, with extensive simulations showing effectiveness particularly in moderate sample sizes.

## Method Summary
The framework consists of two key steps: (1) local double machine learning estimation where each task computes initial estimators and associated loss gradients using cross-fitting, and (2) central aggregation where these estimates are fused through a penalized optimization that shrinks them toward a global mean while guarding against outliers. The method extends to nuisance parameter fusion by transmitting kernel regression derivatives rather than raw data, enabling improved estimation when nuisance functions are similar across tasks. The approach relies on Neyman near-orthogonal estimating equations to prevent nuisance estimation errors from propagating to target parameters.

## Key Results
- The method achieves lower mean squared error compared to individual-task learning, particularly when tasks share similar parametric components
- Extensive simulations demonstrate improved prediction accuracy over parametric multi-task learning methods
- The framework shows effectiveness in moderate sample sizes, with the greatest improvements observed when tasks are highly similar
- Nuisance parameter fusion improves convergence rates by a factor of K^{-1/2} when nuisance functions exhibit similarity across tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive aggregation of initial estimators reduces mean squared error (MSE) by exploiting task similarity while maintaining robustness to outlier tasks.
- **Mechanism:** Local tasks compute initial Double Machine Learning (DML) estimators $\hat{\theta}_k$ and associated loss gradients. A central server solves an optimization problem (Eq. 2.5) that shrinks these estimates toward a global mean $u_0$ via an $\ell_2$ penalty $\lambda$. This "late fusion" shares statistical strength across tasks without sharing raw data.
- **Core assumption:** Tasks belong to a subset $S$ where parameters are similar ($\|\theta_k - \theta_0\| \le \delta$), while outlier tasks are limited in number (Assumption 3.1).
- **Evidence anchors:**
  - [Abstract]: "estimators are adaptively aggregated to exploit task similarities while remaining robust to task-specific differences."
  - [Section 2.2]: "The penalty term encourages similarity between the resulting estimators... while the tuning parameter $\lambda$ guards against possible outliers."
  - [Corpus]: Neighbor papers on "Rescuing double robustness" support the underlying DML robustness but do not validate the specific fusion aggregation logic.
- **Break condition:** If the tuning parameter $\lambda$ is set too high relative to the true task heterogeneity, the bias induced by shrinking to a global mean $u_0$ will dominate, increasing error for outlier tasks.

### Mechanism 2
- **Claim:** Fusing nuisance parameter estimates improves convergence rates for the target parameters when nuisance functions (e.g., propensity scores) are similar across tasks.
- **Mechanism:** The framework extends fusion to infinite-dimensional nuisance parameters $\eta_k$ (e.g., kernel regression estimates). Instead of sharing raw data, local servers transmit derivatives of the kernel loss computed on a grid covering the feature space. The central server aggregates these to minimize a penalized loss, effectively denoising the nuisance estimates before plugging them into the target parameter estimation.
- **Core assumption:** Nuisance parameters share a similarity structure (Assumption 3.4) where $\max_{k \in S_\eta} \|\eta_k - \eta_0\| \le \delta_\eta$.
- **Evidence anchors:**
  - [Section 2.4]: "We leverage the optimization (2.9) and propose an algorithm without directly sharing local data."
  - [Theorem 3.2]: Provides convergence rates for nuisance estimates showing improvement by factor $K^{-1/2}$ for similar tasks.
  - [Corpus]: Related work on "Bayesian Semiparametric Causal Inference" addresses similar nuisance estimation issues but via different mechanisms.
- **Break condition:** If nuisance parameters are highly heterogeneous across tasks ($\delta_\eta$ is large), the fused nuisance estimates will be biased, potentially degrading the final target parameter accuracy compared to local estimation.

### Mechanism 3
- **Claim:** The use of Neyman near-orthogonal estimating equations prevents the errors in nuisance parameter estimation from propagating to the target parameter estimation.
- **Mechanism:** By constructing score functions $m(Z, \theta, \eta)$ where the Gateaux derivative with respect to $\eta$ is near zero, the first-order impact of nuisance estimation error is nullified. This allows the target parameter $\theta$ to be estimated at parametric rates ($n^{-1/2}$) even if nuisance estimates converge at slower nonparametric rates.
- **Core assumption:** The score function satisfies Neyman near-orthogonality (Assumption 3.2) and regularity conditions (Assumption 3.3).
- **Evidence anchors:**
  - [Remark 2.1]: "Neyman near-orthogonal estimating equations are particularly advantageous because they minimize the impact of nuisance parameter estimation errors."
  - [Section 3.1]: "This assumption is pretty mild and required to ensure that each $\theta_k$ is sufficiently separated."
  - [Corpus]: "An Introduction to Double/Debiased Machine Learning" confirms this is a standard requirement for valid inference in semiparametric models.
- **Break condition:** If the orthogonality condition fails (e.g., due to model misspecification), the estimation error of the nuisance parameters will "leak" into the target parameter, potentially preventing $\sqrt{n}$-consistency.

## Foundational Learning

- **Concept: Double Machine Learning (DML) & Cross-fitting**
  - **Why needed here:** The paper builds its initial estimators using DML. Understanding cross-fitting (splitting data to estimate nuisance vs. target parameters) is required to implement the "local" step before fusion.
  - **Quick check question:** Can you explain why we cannot estimate the nuisance function and the target parameter on the same training fold without introducing bias?

- **Concept: Semiparametric Efficiency & Orthogonality**
  - **Why needed here:** The theoretical guarantees rely on orthogonal score functions. Without this concept, one cannot verify if a new estimating equation is valid for the framework.
  - **Quick check question:** Does the gradient of the score function with respect to the nuisance parameter vanish (or nearly vanish) at the true parameter values?

- **Concept: Distributed Optimization (Late Fusion)**
  - **Why needed here:** The core architecture relies on transmitting gradients/Hessians instead of data.
  - **Quick check question:** If the local objective functions are strictly convex, what summary statistics are minimally required by the central server to solve the global fused objective?

## Architecture Onboarding

- **Component map:** Local Nodes -> Central Server -> Fused Estimates
- **Critical path:**
  1. Local cross-fitting to generate initial estimates and variance estimates
  2. Transmission of local sufficient statistics (Gradients/Hessians)
  3. Central solving of the regularized regression (tuning λ via CV)
  4. (Optional) Iteration if nuisance fusion is required (Section 2.4 grid optimization)
- **Design tradeoffs:**
  - **Penalty λ:** High λ increases bias (forces similarity) but reduces variance; low λ treats tasks independently
  - **Bandwidth ħ:** Controls smoothness of nuisance estimates. Must be tuned locally vs. globally
  - **Assumption:** The framework assumes K tasks are available simultaneously; it does not support streaming/incremental tasks without re-solving the central optimization
- **Failure signatures:**
  - **High Bias / "Mean Collapse":** MSE increases for specific tasks after fusion. Likely cause: λ is too large, pulling distinct tasks toward a global mean
  - **No Improvement:** Fusion performs same as local. Likely cause: λ is too small or tasks are truly unrelated
  - **Divergence:** Optimization fails. Likely cause: Local Hessians W_k are not positive definite (poor local data quality)
- **First 3 experiments:**
  1. **Sanity Check (Homogeneity):** Generate K=5 tasks with identical parameters (δ=0). Verify MSE decreases by factor of K (Theorem 3.1)
  2. **Stress Test (Heterogeneity):** Introduce 1 outlier task. Vary λ to confirm the method remains robust (error for outlier remains bounded) while similar tasks improve
  3. **Nuisance Fusion Validation:** Setup Partial Linear Model (Example 2.1) with similar propensity scores but distinct outcome models. Test Section 2.4 method vs. local estimation to verify improvement in nuisance estimation translates to final parameter accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed late fusion framework be extended to settings where the independent and identically distributed (i.i.d.) assumption does not hold, such as in longitudinal data analysis?
- Basis in paper: [explicit] The authors state in the Discussion that extending the framework to address dependence structures in data, specifically for longitudinal data, would broaden the method's applicability.
- Why unresolved: The current theoretical guarantees and estimation procedures rely on the assumption that samples are i.i.d. within each task.
- What evidence would resolve it: A theoretical extension of the convergence proofs that accounts for within-subject correlation or time-series dependence, along with simulations on longitudinal datasets.

### Open Question 2
- Question: How can alternative forms of task similarity, such as clustering or low-rank structures, be incorporated into the multi-task learning framework?
- Basis in paper: [explicit] The Discussion identifies incorporating structures like clustering as a "compelling direction," suggesting that data-driven clustering could help estimate treatment effects more efficiently if such structures exist.
- Why unresolved: The current method relies on a similarity assumption defined by a subset S of tasks being close to a parameter θ₀, but it does not explicitly model or estimate hierarchical or grouped relationships among tasks.
- What evidence would resolve it: A modified optimization objective that includes a clustering penalty or low-rank constraint, demonstrating improved estimation accuracy when tasks form distinct groups.

### Open Question 3
- Question: How can black-box machine learning methods be effectively utilized to improve nuisance parameter estimation when those parameters are similar across tasks?
- Basis in paper: [explicit] The authors note on Page 13 that obtaining improved estimation of nuisance parameters using ML methods when they are similar "requires significantly new ideas and we leave it for future explorations."
- Why unresolved: While the paper proposes a kernel regression method for similar nuisance parameters, it lacks a generalized pipeline for integrating off-the-shelf ML algorithms (e.g., Random Forests, Neural Networks) into the late fusion step for nuisance estimation.
- What evidence would resolve it: A theoretical analysis showing convergence rates for a late-fusion-compatible ML nuisance estimator, or an empirical study comparing the kernel approach against adapted ML methods.

## Limitations
- The framework assumes near-orthogonality and parametric similarity across tasks, which may not hold in real-world settings with substantial task heterogeneity or model misspecification
- The choice of fusion penalty λ requires cross-validation that may be computationally expensive in large-scale settings
- Theoretical guarantees depend on specific conditions (bounded parameters, restricted eigenvalue conditions) that may be violated in practice

## Confidence
- **High confidence** in the fundamental mechanism of using Neyman orthogonality to prevent nuisance error propagation to target parameters
- **Medium confidence** in the aggregation algorithm's robustness to outliers, as theoretical bounds are asymptotic and simulation evidence is limited to controlled settings
- **Medium confidence** in the practical benefits of fusing nuisance parameters, as this adds computational complexity and the improvement depends on the degree of nuisance similarity

## Next Checks
1. **Stress test heterogeneity:** Systematically vary the number and magnitude of outlier tasks beyond what was tested in simulations to verify robustness claims
2. **Model misspecification sensitivity:** Evaluate performance when the assumed semiparametric model is partially misspecified to test the limits of the orthogonality protection
3. **Communication efficiency:** Measure the computational and communication overhead of transmitting gradients/Hessians versus the accuracy gains, particularly for high-dimensional nuisance parameters