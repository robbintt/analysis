---
ver: rpa2
title: CLAWS:Creativity detection for LLM-generated solutions using Attention Window
  of Sections
arxiv_id: '2510.17921'
source_url: https://arxiv.org/abs/2510.17921
tags:
- uni00000013
- uni00000011
- claws
- auroc
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting creativity in LLM-generated
  mathematical solutions, a task often overlooked compared to writing tasks. The proposed
  method, CLAWS, leverages attention weights across prompt sections and output to
  classify solutions into Hallucinated, Creative, and Typical categories without requiring
  human evaluation.
---

# CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections

## Quick Facts
- arXiv ID: 2510.17921
- Source URL: https://arxiv.org/abs/2510.17921
- Reference count: 40
- This paper proposes CLAWS, an attention-based method for detecting creativity in LLM-generated mathematical solutions, achieving weighted F1 scores up to 63.20 on five 7-8B math RL models.

## Executive Summary
This paper addresses the challenge of detecting creativity in LLM-generated mathematical solutions, a task often overlooked compared to writing tasks. The proposed method, CLAWS, leverages attention weights across prompt sections and output to classify solutions into Hallucinated, Creative, and Typical categories without requiring human evaluation. CLAWS outperforms five existing white-box detection methods on five 7-8B math RL models, achieving weighted F1 scores of up to 63.20 on the test set. It demonstrates superior performance in macro-average metrics, indicating effective detection of creative solutions while maintaining robust classification of typical and hallucinated ones.

## Method Summary
CLAWS extracts attention weights from the last decoder layer during generation of mathematical solutions. The method computes section-wise attention averages across five semantic sections (Guideline, Problem, Reference Solutions, Instruction, Response), normalizes these to create a 5-dimensional feature vector, and classifies solutions using prototype matching, XGBoost, MLP, or TabM. The approach requires only a single forward pass and no re-generation, making it efficient for deployment. Reference sets are constructed with 20 generations per problem, and solutions are labeled by two LLM evaluators using a union criterion for creativity detection.

## Key Results
- CLAWS achieves weighted F1 scores up to 63.20 on test sets across five 7-8B math RL models
- The method outperforms five existing white-box detection methods in both weighted and macro-average F1 metrics
- CLAWS effectively distinguishes creative solutions as distributed between hallucinated and typical patterns in attention space

## Why This Works (Mechanism)

### Mechanism 1: Section-wise Attention Differentiation
- **Claim**: Different solution types (Hallucinated/Creative/Typical) exhibit distinguishable attention patterns across prompt sections.
- **Mechanism**: CLAWS computes normalized attention ratios per section (G, P, S, I, R) from the decoder's last layer during generation. Hallucinated solutions show elevated attention to Guideline and Problem sections, while Typical solutions attend more to Solutions, Instruction, and Response sections—Creative solutions fall between these patterns.
- **Core assumption**: The section of the prompt a model attends to most during generation causally influences whether the output is hallucinated, creative, or typical.
- **Evidence anchors**:
  - [abstract] "CLAWS... leverages attention weights across prompt sections and output to classify solutions"
  - [section 5, p.8] "The visualization illustrates the mean per method for each class... CLAWS effectively distinguishes Creative solutions as distributed between Hallucinated and Typical solutions"
  - [corpus] Weak direct support; related papers focus on reasoning evaluation via other modalities, not attention-based detection
- **Break condition**: If attention patterns show no statistical separation across classes, or if layer selection significantly alters class separation (layer-wise analysis shows relative robustness but this is model-dependent).

### Mechanism 2: White-Box Internal State Extraction Without Re-generation
- **Claim**: Single-pass attention extraction during generation is sufficient for classification without multiple model calls or external models.
- **Mechanism**: During inference, CLAWS extracts attention weights from the last decoder layer (A^(L)_t,h), computes section averages via summation over token positions, then normalizes. No additional forward passes or ensemble sampling required.
- **Core assumption**: Last-layer attention captures sufficient signal for three-way classification; deeper layers are unnecessary.
- **Evidence anchors**:
  - [section 3, p.5] "CLAWS extracts only the attention layer in the single response generation process and just performs sum and average operations"
  - [section 5, p.10, Table 5] Layer-wise analysis shows last layer achieves best overall performance with no significant degradation across layers
  - [corpus] No direct validation; this is an empirical finding specific to this paper's evaluation
- **Break condition**: If other layers or hidden states provide stronger discriminative signal for different model architectures or tasks beyond mathematics.

### Mechanism 3: LLM-Evaluator Labeling with Union Creativity Criterion
- **Claim**: Dual-LLM evaluation with union criterion (either evaluator judges creative → creative label) reduces false negatives while maintaining reliability.
- **Mechanism**: Two frontier LLMs (GPT-o4-mini, Gemini-1.5-Pro) evaluate correctness first, then creativity against reference solutions. Cohen's kappa of 0.741 indicates substantial inter-evaluator agreement.
- **Core assumption**: LLM-based evaluation reliably substitutes for human expert judgment on mathematical creativity; reference solutions (always human-written) ground the creativity comparison.
- **Evidence anchors**:
  - [section 2.2.2, p.3] Detailed classification criteria based on structural/methodological differences between solutions
  - [appendix B.1.2, p.22] "Cohen's kappa score between the two evaluators was 0.741, which corresponds to substantial agreement"
  - [corpus] Adjacent work (CreativeMath) validates LLM evaluators for creativity, but human ground truth comparison is absent in this study
- **Break condition**: If evaluator agreement drops below 0.6 or if systematic bias emerges (e.g., favoring longer solutions as creative).

## Foundational Learning

- **Concept**: Self-Attention Mechanism in Transformers
  - **Why needed here**: CLAWS fundamentally relies on understanding how attention weights represent token-to-token dependencies during autoregressive generation.
  - **Quick check question**: Can you explain why the attention matrix at layer L has shape T × (len(X) + T), where T is output length and len(X) is prompt length?

- **Concept**: White-Box vs. Black-Box Detection Methods
  - **Why needed here**: The paper positions CLAWS against white-box baselines (Perplexity, Entropy-based methods) that access internal states, distinct from black-box methods requiring multiple generations.
  - **Quick check question**: Why would a white-box method like CLAWS be preferred over black-box methods like SelfCheckGPT in latency-sensitive deployments?

- **Concept**: Prototype-Based Classification
  - **Why needed here**: CLAWS uses prototype matching (computing Euclidean distance to class centers in learned embedding space) rather than simple thresholding.
  - **Quick check question**: How does prototype classification differ from threshold-based binary detection, and why might it better handle the three-class problem?

## Architecture Onboarding

- **Component map**:
  - Generator (RLM) -> Attention Extractor -> Section Tokenizer -> Feature Normalizer -> Classifier (Prototype/MLP/XGBoost/TabM)

- **Critical path**:
  1. Prompt construction with 4 sections (G, P, S, I)
  2. Single-pass generation with attention extraction
  3. Response tokens appended as section R
  4. Section-wise attention averaging and normalization
  5. Classification via pre-computed prototypes or trained classifier

- **Design tradeoffs**:
  - **Last-layer vs. all-layers**: Paper shows last layer is sufficient, reducing computation but potentially missing hierarchical features
  - **Prototype vs. MLP**: Prototype requires only reference set (low-resource), MLP achieves higher F1 but needs training data
  - **Union vs. intersection creativity criterion**: Union reduces false negatives (more inclusive), intersection would be stricter

- **Failure signatures**:
  - High hallucination rate in generator (>70%) causes prototype imbalance, degrading classification (observed with OREAL)
  - Gray-shaded cells in results tables indicate detection of only 1-2 of 3 classes—signals classifier collapse
  - Significant layer-wise performance variation may indicate model-specific attention distribution patterns

- **First 3 experiments**:
  1. **Baseline reproduction**: Run all 5 white-box methods (PPL, Logit Entropy, Window Entropy, Hidden Score, Attention Score) with threshold strategy on DeepSeek-Math reference set to establish baseline metrics.
  2. **Ablation by section**: Remove each of the 5 sections (G/P/S/I/R) individually and measure F1 degradation to validate section contribution claims.
  3. **Balanced vs. imbalanced reference set comparison**: Construct balanced reference set (equal samples per class) and compare Prototype vs. MLP performance to assess sensitivity to class imbalance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CLAWS effectively generalize to creative domains outside of mathematical reasoning, such as coding or creative writing, where solution "correctness" is more subjective?
- Basis in paper: [explicit] Appendix A states that defining creativity based on math problems is a limitation and "expanding it to a various tasks will be our future work."
- Why unresolved: The current prompt structure relies on a "Problem," "Reference Solutions," and binary "Correctness" criteria, which may not map directly to open-ended tasks without clear ground truth.
- What evidence would resolve it: Evaluation of the CLAWS attention window method on non-mathematical benchmarks (e.g., creative writing prompts or coding tasks) with adapted evaluation criteria.

### Open Question 2
- Question: Do the attention patterns correlating with creativity and hallucination in 7–8B parameter models hold for significantly larger frontier models (>20B parameters)?
- Basis in paper: [explicit] Appendix A notes that the white-box approach requires substantial resources, and therefore experiments were limited to 7–8B models, leaving the behavior of "large-sized models" untested.
- Why unresolved: Larger models may exhibit different attention distributions or rely on different internal mechanisms for reasoning, potentially reducing the discriminative power of section-wise attention ratios.
- What evidence would resolve it: Replicating the experimental framework on 70B+ parameter models to determine if the relationship between "Guideline" attention and hallucination persists.

### Open Question 3
- Question: Can the identified attention patterns be causally manipulated to prevent hallucinations or induce creativity, rather than just detecting them post-hoc?
- Basis in paper: [inferred] Page 8 observes that "hallucinations may be caused by over-focusing on a part of the input prompt," suggesting a mechanistic link that implies potential for intervention.
- Why unresolved: The study establishes a correlation for classification purposes but does not test whether forcing the model to reduce attention on the "Guideline" section actively prevents hallucinated outputs.
- What evidence would resolve it: Experiments using attention steering or constrained decoding to penalize the "Guideline-heavy" attention pattern to see if it alters the output class from Hallucinated to Correct.

### Open Question 4
- Question: How well does the LLM-based evaluation protocol align with human expert judgment in distinguishing "Creative" from "Typical" mathematical solutions?
- Basis in paper: [inferred] Appendix B.1.1 justifies the use of LLM evaluators (GPT-o4-mini, Gemini) due to cost, noting that LLM evaluation may involve "intrinsic biases" but assuming it reflects human diversity.
- Why unresolved: The ground truth for "creativity" is subjective; while the paper validates the *method* against the *LLM labels*, it does not validate the *LLM labels* against the gold standard of human mathematicians.
- What evidence would resolve it: A comparative study calculating the correlation between the LLM Evaluator's creativity labels and those assigned by a sample of human math experts on the same dataset.

## Limitations
- The evaluation focuses exclusively on mathematical problem-solving, leaving uncertainty about performance on other domains
- The LLM-based creativity labeling lacks direct human ground truth validation despite showing substantial inter-evaluator agreement
- Prototype method shows particular sensitivity to class imbalance, with Creative solutions being the rarest class

## Confidence
- **High confidence**: Attention extraction mechanism and normalization procedures are well-specified and reproducible
- **Medium confidence**: Three-way classification performance claims, given strong results but limited cross-domain validation
- **Medium confidence**: Creativity detection effectiveness, given substantial but not perfect LLM-evaluator agreement

## Next Checks
1. Cross-domain evaluation: Test CLAWS on non-mathematical tasks (e.g., creative writing, code generation) to assess generalizability of attention-based detection
2. Human validation study: Compare LLM-labeled creativity scores against human expert judgments on a subset of solutions to validate the labeling mechanism
3. Model architecture ablation: Evaluate CLAWS on transformer variants (e.g., Mamba, RWKV) to test robustness beyond standard attention-based LLMs