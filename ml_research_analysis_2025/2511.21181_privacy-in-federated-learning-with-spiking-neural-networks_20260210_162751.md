---
ver: rpa2
title: Privacy in Federated Learning with Spiking Neural Networks
arxiv_id: '2511.21181'
source_url: https://arxiv.org/abs/2511.21181
tags:
- gradient
- spiking
- attacks
- inversion
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the privacy vulnerability of gradient inversion
  attacks on Spiking Neural Networks (SNNs) in federated learning settings. The authors
  adapt three classical gradient inversion attacks (DLG, iDLG, and GRNN) to the spike
  domain and evaluate them on both image and event-based datasets.
---

# Privacy in Federated Learning with Spiking Neural Networks

## Quick Facts
- arXiv ID: 2511.21181
- Source URL: https://arxiv.org/abs/2511.21181
- Reference count: 36
- Primary result: SNNs demonstrate 49% higher resistance to gradient inversion attacks compared to ANNs

## Executive Summary
This paper investigates privacy vulnerabilities in federated learning when using Spiking Neural Networks (SNNs) as opposed to conventional Artificial Neural Networks (ANNs). The authors adapt three classical gradient inversion attacks (DLG, iDLG, and GRNN) to the spike domain and evaluate their effectiveness on both traditional image datasets and event-based spiking data. The results show that SNNs exhibit significantly stronger resistance to privacy attacks, with attack success rates dropping by approximately half compared to ANNs. The study identifies the non-differentiable nature of spike functions and temporal encoding as the primary mechanisms for this enhanced privacy protection.

## Method Summary
The researchers adapted three gradient inversion attacks originally designed for ANNs to work with SNNs by modifying them to operate in the spike domain. They conducted experiments on two datasets: LFW (traditional images) and DVS128 Gesture (event-based spiking data). The evaluation compared attack success rates between ANN and SNN implementations under identical federated learning conditions. The study systematically analyzed how the temporal encoding and non-differentiable spike functions in SNNs affect the gradient inversion process.

## Key Results
- iDLG attack achieved 97.3% ASR on LFW with ANNs but only 32.2% with SNNs
- Across all attack variants, ASRs decreased by approximately 49% on average when using SNNs
- On natively spiking DVS128 Gesture dataset, ASRs remained near-random levels (7-10%) for all attacks
- SNNs demonstrated inherent privacy-preserving properties without requiring additional mechanisms

## Why This Works (Mechanism)
The enhanced privacy protection stems from two key properties of SNNs: their non-differentiable spike functions and temporal encoding schemes. Unlike ANNs that use continuous activation functions, SNNs employ discontinuous spike functions that create information loss during gradient propagation. The temporal encoding further obscures the relationship between input data and model gradients, making it significantly harder for attackers to reconstruct original inputs from shared gradients.

## Foundational Learning
- **Federated Learning**: Distributed ML paradigm where clients train models locally and share only gradients - needed to understand the attack surface and privacy implications
- **Gradient Inversion Attacks**: Methods that reconstruct training data from model gradients - critical for evaluating privacy vulnerability
- **Spiking Neural Networks**: Bio-inspired neural networks using discrete spike events - fundamental to understanding the privacy mechanism
- **Temporal Encoding**: Data representation using timing of events rather than amplitude - key to SNNs' privacy advantage
- **Event-based Vision**: Sensor technology that captures changes rather than frames - relevant for evaluating SNNs on native spiking data

## Architecture Onboarding
- **Component Map**: Client devices -> Local SNN training -> Gradient computation -> Spike-domain attack attempts -> Central server aggregation
- **Critical Path**: Data encoding → SNN forward pass → Spike generation → Gradient calculation → Privacy protection
- **Design Tradeoffs**: SNNs offer better privacy but may sacrifice some accuracy compared to ANNs; temporal encoding adds complexity but enhances security
- **Failure Signatures**: High attack success rates indicate privacy vulnerability; near-random ASRs suggest effective protection
- **First Experiments**: 1) Replicate ANN vs SNN privacy comparison on standard datasets 2) Test attack effectiveness with different spike encoding schemes 3) Evaluate privacy-utility tradeoff curves

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Attack implementations were adapted from ANN methods without optimization for spike domain
- Effectiveness may vary with different spike encoding schemes not systematically evaluated
- Results limited to specific datasets and architectures, limiting generalizability

## Confidence
- SNNs exhibit higher resistance to gradient inversion attacks: High
- Non-differentiable spike functions and temporal encoding are primary privacy mechanisms: Medium
- SNNs can reduce need for external privacy mechanisms: Medium

## Next Checks
1. Test attack effectiveness across diverse SNN architectures and spike encoding schemes to establish robustness bounds
2. Evaluate privacy-utility tradeoffs under varying levels of communication compression and gradient quantization
3. Implement adaptive attack strategies specifically designed for spike-domain gradients to identify potential vulnerabilities