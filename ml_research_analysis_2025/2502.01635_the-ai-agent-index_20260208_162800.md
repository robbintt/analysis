---
ver: rpa2
title: The AI Agent Index
arxiv_id: '2502.01635'
source_url: https://arxiv.org/abs/2502.01635
tags:
- systems
- arxiv
- agentic
- agent
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the AI Agent Index, the first public database
  documenting deployed agentic AI systems. The authors created a structured framework
  covering system components, application domains, and risk management practices,
  collecting data from 67 systems via public sources and developer correspondence.
---

# The AI Agent Index

## Quick Facts
- arXiv ID: 2502.01635
- Source URL: https://arxiv.org/abs/2502.01635
- Reference count: 34
- Primary result: First public database of deployed agentic AI systems, revealing limited safety disclosure despite ample capability information

## Executive Summary
The AI Agent Index is the first public database documenting deployed agentic AI systems, providing structured documentation of 67 systems as of December 31, 2024. The authors created a standardized framework covering system components, application domains, and risk management practices, collecting data through public sources and developer correspondence. They found that while developers provide ample information about capabilities and applications, there is limited disclosure about safety evaluations and guardrails—fewer than 10% report external safety testing. Most indexed systems are developed by US-based companies and focus on software engineering or computer use tasks.

## Method Summary
The authors systematically identified agentic systems using a decision graph with criteria including being named systems, accomplishing diverse tasks, and having "meaningfully higher degree of agency than ChatGPT-4o." They populated 33-field agent cards from public documentation, contacted developers for corrections (achieving a 36% response rate), and archived all sources with stable timestamps. The index uses Chan et al. (2023) agency characteristics (underspecification, directness of impact, goal-directedness, long-term planning) to distinguish agentic systems from chatbots and narrow tools.

## Key Results
- 67 agentic systems documented as of December 31, 2024 cutoff
- Only 19.4% disclose formal safety policies; fewer than 10% report external safety evaluations
- 48 of 67 systems (71.6%) are developed by US-based companies
- Most systems focus on software engineering (25) and computer use (23) applications

## Why This Works (Mechanism)

### Mechanism 1: Structured Transparency Gap Detection
By defining a fixed 33-field taxonomy across 6 categories and applying it uniformly to all systems, the index surfaces patterns in what developers choose to disclose versus withhold. The structure forces comparison along identical dimensions, revealing systematic disclosure asymmetries across the field.

### Mechanism 2: Boundary Definition via Multi-Criteria Decision Graph
The inclusion criteria combine absolute thresholds (must be a named system, must accomplish diverse tasks) with relative thresholds (must have "meaningfully higher degree of agency than ChatGPT-4o"). This operationalizes contested definitions like "agentic" while handling edge cases through a discretionary node.

### Mechanism 3: Developer Correspondence as Data Augmentation
Direct developer feedback on draft entries improves accuracy and increases buy-in. The 36% response rate provides partial validation while revealing which developers engage with transparency efforts, though it may introduce selection bias.

## Foundational Learning

- **Agentic properties (underspecification, directness, goal-directedness, long-term planning)**: The entire indexing framework depends on distinguishing "agentic" systems from chatbots and narrow tools. Without understanding these four dimensions, you cannot evaluate inclusion decisions.
  - Quick check: Can you explain why a customer service chatbot might fail the "directness of impact" criterion even if it uses the same base model as an indexed agent?

- **Compound AI systems and scaffolding**: The index documents agents as "foundation models augmented by scaffolding" rather than standalone models. Understanding this architecture is necessary to interpret the "system components" category.
  - Quick check: If an agent uses GPT-4o as its backend but adds custom planning modules, which fields in the agent card would differ from a raw GPT-4o model card?

- **Red-teaming vs. internal safety evaluation**: The key finding concerns the gap in safety disclosure. Distinguishing between developer-conducted testing and external auditing is essential for interpreting the 9% vs. 7.5% figures.
  - Quick check: Why might external red-teaming provide different information than internal safety evaluations, even if both cover the same harm categories?

## Architecture Onboarding

- **Component map**: Web search → academic literature → benchmark leaderboards → first draft agent cards → Contact developers → 36% response rate → incorporate corrections → Archive all cited sources via Wayback Machine/perma.cc with timestamps near Dec 31, 2024 cutoff → Finalize and publish

- **Critical path**: Identify candidate systems meeting "agentic" criteria via decision graph → Populate 33-field agent card template from public sources → Contact developers for corrections → Archive all citations with timestamps → Finalize and publish before cutoff date

- **Design tradeoffs**: 
  - Snapshot vs. living database: Fixed cutoff (Dec 31, 2024) ensures consistency but sacrifices real-time updates
  - Breadth vs. depth: 67 systems with 33 fields each rather than deeper analysis of fewer systems
  - Inclusion discretion: Final discretionary node adds subjectivity but allows tracking announced systems from major labs

- **Failure signatures**:
  - Scope creep: Including development frameworks or non-agentic chatbots would dilute the sample
  - Stale data: Database rapidly loses relevance given deployment pace shown in Figure 4
  - Gaming: Developers could selectively disclose information while obscuring other practices
  - Geographic/language bias: Underrepresents non-Western systems

- **First 3 experiments**:
  1. Inter-rater reliability test: Have multiple reviewers independently apply the decision graph to 10 edge-case systems; measure agreement on inclusion decisions
  2. Response rate analysis: Compare safety disclosure rates between systems whose developers responded vs. didn't respond to correspondence requests
  3. Longitudinal tracking: Re-sample the same 67 systems 6 months later to measure how disclosure practices change

## Open Questions the Paper Calls Out

- **Do developers actually lack safety practices, or simply not disclose them publicly?**: Only 36% of developers responded to requests for feedback, and the index relies on publicly available information which may not reflect internal practices. Direct audits of developer organizations would resolve this.

- **How can documentation frameworks avoid incentivizing "cosmetic compliance" from developers?**: The tension between transparency goals and performative disclosure is inherent to voluntary reporting frameworks. Longitudinal studies comparing substantive safety outcomes versus disclosure patterns would help resolve this.

- **What agentic systems exist outside the English-language, Western-centric ecosystem captured by the index?**: The field is "highly decentralized and poorly documented," making comprehensive global coverage infeasible with current methods. Systematic multilingual searches would help address this gap.

## Limitations

- Geographic and language bias likely underrepresents non-Western agentic systems
- 36% developer response rate may introduce selection bias toward more engaged or compliant developers
- Fixed December 31, 2024 cutoff creates a static view of a rapidly evolving field

## Confidence

- **High**: Finding that developers provide more information about capabilities than safety practices is supported by systematic data collection
- **Medium**: US dominance finding is well-documented but may reflect indexing methodology limitations
- **Low**: "Fewer than 10% report external safety evaluations" may undercount actual practices if developers conduct evaluations without disclosure

## Next Checks

1. **Inter-rater reliability test**: Have multiple independent reviewers apply the decision graph to 10 edge-case systems to measure agreement on inclusion decisions
2. **Response rate analysis**: Compare safety disclosure rates between systems whose developers responded to correspondence requests versus those that didn't
3. **Longitudinal tracking study**: Re-sample the same 67 systems 6 months after the index cutoff to measure changes in disclosure practices and test whether the index influences developer behavior