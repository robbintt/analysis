---
ver: rpa2
title: Single-pass Adaptive Image Tokenization for Minimum Program Search
arxiv_id: '2507.07995'
source_url: https://arxiv.org/abs/2507.07995
tags:
- token
- tokens
- image
- reconstruction
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KARL introduces a single-pass adaptive image tokenizer that predicts
  the appropriate number of tokens for each image in one forward pass, rather than
  requiring iterative search. Inspired by Kolmogorov Complexity, it learns to halt
  token generation once the desired reconstruction quality is achieved.
---

# Single-pass Adaptive Image Tokenization for Minimum Program Search

## Quick Facts
- arXiv ID: 2507.07995
- Source URL: https://arxiv.org/abs/2507.07995
- Reference count: 40
- Authors: Shivam Duggal; Sanghyun Byun; William T. Freeman; Antonio Torralba; Phillip Isola
- Primary result: KARL matches or exceeds recent adaptive tokenizers on reconstruction metrics while being uniquely single-pass

## Executive Summary
KARL introduces a single-pass adaptive image tokenizer that predicts the appropriate number of tokens for each image in one forward pass, rather than requiring iterative search. Inspired by Kolmogorov Complexity, it learns to halt token generation once the desired reconstruction quality is achieved. The training strategy mimics upside-down reinforcement learning: first attempting lossless compression, then using the resulting loss as a condition to learn when to stop. This enables efficient, adaptive inference without multiple encoder-decoder runs.

## Method Summary
KARL implements a two-phase loss-conditioned training procedure where it first estimates image complexity by attempting near-lossless compression with a random token budget, then learns to tokenize conditioned on the achieved reconstruction error and an increased token budget. The model outputs both token embeddings and halting probabilities in a single forward pass, allowing it to mask out redundant tokens at inference. A latent distillation transformer architecture processes 2D image tokens alongside learnable 1D latent tokens, with a learned embedding for the desired reconstruction loss threshold conditioning the entire encoding process. The approach is trained on ImageNet or ImageNet-100 using pre-tokenized 2D grids from VQGAN/VAE, with a two-stage training pipeline including latent distillation pretraining followed by GAN-based finetuning.

## Key Results
- KARL achieves competitive or superior performance to recent adaptive tokenizers (FlexTok, Matryoshka) on LPIPS, SSIM, PSNR, and DreamSim metrics
- The method demonstrates strong correlation between predicted token counts and human judgments of image complexity
- A small encoder / large decoder configuration provides optimal efficiency without sacrificing reconstruction quality
- Token counts serve as practical approximations of Kolmogorov Complexity, enabling adaptive allocation based on image inherent complexity

## Why This Works (Mechanism)

### Mechanism 1: Loss-Conditioned Halting Predictions via Upside-Down Reinforcement Learning
KARL treats reconstruction error thresholds as task-specifying input conditions, allowing the encoder to predict halting probabilities for tokens in one forward pass. The training generates supervised pairs where reconstruction error from a token budget becomes the conditioning input for a larger budget, teaching the model to halt extra tokens. This converts adaptive allocation into a supervised mapping from desired loss to token selection.

### Mechanism 2: Kolmogorov Complexity Approximation via Self-Supervised Curriculum
By randomly sampling token budgets and observing reconstruction error, KARL naturally generates a distribution where simple images achieve low error with few tokens while complex images require more. This implicit curriculum aligns learned token counts with algorithmic complexity of the data, as the model is never tasked with impossible compression goals.

### Mechanism 3: Efficient Inference via One-Shot Halting Probability Prediction
The encoder outputs halting probabilities alongside token embeddings through a differentiable mechanism trained with loss-conditioned supervision. This allows the network to learn which tokens are essential versus redundant for a given target quality, enabling single-pass inference by masking tokens above a halting probability threshold.

## Foundational Learning

- **Kolmogorov Complexity (KC) and Minimum Description Length (MDL)**: Why needed - KARL's theoretical justification is grounded in AIT, clarifying why it uses approximate KC via token counts and reconstruction loss thresholds. Quick check - Why can't KARL directly compute true Kolmogorov Complexity and what does it use as a practical substitute?

- **Upside-Down Reinforcement Learning (UDRL)**: Why needed - The paper frames its two-phase training as analogous to UDRL, where rewards (desired reconstruction loss) are inputs to a policy rather than goals to maximize. Quick check - What information from Phase 1 is fed back into Phase 2 as a conditioning signal similar to a "desired reward" in UDRL?

- **Matryoshka vs. Recurrent vs. Single-Pass Adaptive Tokenization**: Why needed - Understanding differences between nested subsets, iterative search, and one-shot prediction is critical to appreciate KARL's efficiency claim. Quick check - How does KARL's approach differ from both Matryoshka and recurrent methods at inference time?

## Architecture Onboarding

- **Component map**: VQGAN/VAE -> 2D Token Grid -> Latent Distillation Encoder -> Token Embeddings + Halting Probabilities -> Latent Distillation Decoder -> Reconstructed 2D Token Grid -> Image

- **Critical path**: Image converted to 2D tokens via VQGAN, concatenated with learnable 1D latent tokens and loss-conditioning embedding. Encoder processes input with epsilon=0 in Phase 1 to get epsilon_0, then reprocesses with epsilon=epsilon_0 and T+DeltaT in Phase 2 to learn halting. At inference, encoder outputs embeddings and halting probabilities in single pass; tokens with probability >=0.75 discarded; remaining tokens passed to decoder.

- **Design tradeoffs**: Small Encoder/Large Decoder configuration optimal as encoder's job is simple distillation while decoder's reconstruction is more complex. Continuous 1D tokens yield better reconstruction than quantized ones. Choice of conditioning loss (L1 vs perceptual) affects token allocator behavior.

- **Failure signatures**: Over-allocation occurs if model under-trained or loss condition too strict, using maximum budget always. Under-allocation/inefficiency happens if loss condition too loose or halting loss weight too low. OOD failure occurs when model misallocates tokens for structured but out-of-distribution content.

- **First 3 experiments**: 
  1. Train KARL models with small (6 layers) and large (12 layers) encoders while keeping decoder size constant; compare LPIPS/SSIM on validation set at epsilon=0.05 to confirm small encoder uses fewer tokens for equivalent quality.
  2. Train two models: standard loss-conditioned training vs. control without LTC phase; evaluate both on test set varying inference-time epsilon to show standard model's adaptability.
  3. Evaluate trained KARL on in-distribution images, pure Gaussian noise, and structured OOD patterns; plot assigned token count vs. reconstruction quality to analyze if model correctly handles both noise and structure.

## Open Questions the Paper Calls Out
- Can adaptive tokenizers bridge the gap between learned complexity estimates and true Kolmogorov Complexity for algorithmically simple but out-of-distribution inputs?
- Can the tokenization process be formalized to explicitly distinguish between "useful structure" and "noise," operationalizing AIT concepts of Sophistication and Logical Depth?
- How does choice of halting condition (L1 vs LPIPS) affect semantic utility of learned "minimal program" in downstream tasks?

## Limitations
- Theoretical claims about Kolmogorov Complexity approximation lack rigorous proof despite strong conceptual alignment
- Architecture specifications omit exact layer counts, hidden dimensions, and attention heads needed for reproduction
- Training procedure complexity with unspecified hyperparameters (loss weights, learning rates, durations) affects reproducibility
- OOD generalization claims based on limited examples require broader validation across diverse scenarios

## Confidence
- **High**: Single-pass adaptive inference mechanism, efficiency improvements over alternatives, objective performance metrics
- **Medium**: Kolmogorov Complexity approximation conceptual link, loss-conditioned training effectiveness, scaling law findings
- **Low**: Generalization to novel image types, robustness to threshold choice, behavior on diverse OOD distributions

## Next Checks
1. **Scaling Law Replication Test**: Implement KARL with multiple encoder/decoder size configurations (6/12, 12/12, 12/6 layers) on ImageNet-100; verify small encoder achieves comparable quality with fewer tokens at epsilon=0.05.

2. **Loss-Conditioning Ablation Study**: Train standard two-phase loss-conditioned model vs. control without LTC phase; evaluate both on test set across multiple epsilon values (0.02, 0.05, 0.10) to demonstrate adaptability difference.

3. **OOD and Noise Sensitivity Analysis**: Evaluate trained KARL on in-distribution images, pure Gaussian noise, structured OOD patterns, and high-frequency natural images; plot token count distributions and reconstruction quality curves to validate KC proxy claims.