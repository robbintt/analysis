---
ver: rpa2
title: Natural Context Drift Undermines the Natural Language Understanding of Large
  Language Models
arxiv_id: '2509.01093'
source_url: https://arxiv.org/abs/2509.01093
tags:
- similarity
- semantic
- accuracy
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how the natural evolution of context paragraphs
  impacts question answering performance in generative Large Language Models (LLMs).
  The authors introduce a framework that curates naturally evolved, human-edited variants
  of reading passages from QA benchmarks and measures LLM performance across varying
  levels of semantic similarity to content seen during pretraining.
---

# Natural Context Drift Undermines the Natural Language Understanding of Large Language Models

## Quick Facts
- arXiv ID: 2509.01093
- Source URL: https://arxiv.org/abs/2509.01093
- Authors: Yulong Wu; Viktor Schlegel; Riza Batista-Navarro
- Reference count: 40
- Large Language Models (LLMs) experience significant accuracy drops when reading passages semantically diverge from their training corpus versions, even when all necessary information remains available.

## Executive Summary
This study investigates how natural evolution of context paragraphs impacts question answering performance in generative Large Language Models (LLMs). The authors introduce a framework that curates naturally evolved, human-edited variants of reading passages from QA benchmarks and measures LLM performance across varying levels of semantic similarity to content seen during pretraining. By analyzing six QA datasets and eight LLMs with open training data, the research finds that LLM accuracy declines as reading passages semantically diverge from their training corpus versions, while human annotators maintain stable performance. For example, average accuracy on BOOLQ drops by over 30% from the highest to lowest similarity bins, with slopes exceeding 70 across several models.

## Method Summary
The study constructs a framework that extracts Wikipedia edit history to create naturally evolved variants of reading passages used in QA benchmarks. It then matches these passages to their specific versions in LLM training corpora (Dolma, Pile) and computes semantic similarity scores between trained and current versions. The evaluation uses zero-shot inference on six QA datasets with eight different LLMs, filtering out instances where models answer correctly without passage access to ensure genuine reading comprehension is being tested. Performance is analyzed across 10 similarity bins to measure degradation patterns.

## Key Results
- LLM accuracy on BOOLQ drops by over 30% from highest to lowest similarity bins
- Performance slopes exceed 70 across several models, indicating steep degradation
- Human annotators maintain relatively stable performance regardless of passage similarity
- Multi-hop reasoning tasks (HotpotQA) show greater resilience with shallower slopes (~24) compared to single-hop extraction tasks

## Why This Works (Mechanism)

### Mechanism 1: Surface-Form Anchoring in Pretraining
LLMs bind question-answering capabilities to specific lexical patterns encountered during pretraining rather than abstract logical structures. When reading passages diverge from their form in the training corpus, models fail to retrieve or integrate necessary information despite semantic content remaining valid. Evidence shows surface-level tasks like SQuAD are more sensitive to text evolution than reasoning tasks.

### Mechanism 2: Differential Resilience via Reasoning Complexity
Tasks requiring multi-step reasoning exhibit greater resilience to context drift than single-hop extraction tasks. Complex reasoning forces models to synthesize information across contexts, potentially activating broader reasoning mechanisms less dependent on exact surface forms. Hotpot QA demonstrates a much shallower 23.94 average slope, indicating greater robustness to textual edits.

### Mechanism 3: Parametric Knowledge Conflict
Performance degradation is exacerbated when evolved context contradicts or fails to trigger specific parametric knowledge memorized during pretraining. The study filters out questions answerable without context, implying remaining instances require passage information. When passages drift, they fail to align with internal "source of truth" regarding entities, leading to extraction failures.

## Foundational Learning

- **Distributional Shift / Natural Perturbation**: Understanding the distinction between natural temporal drift and adversarial noise is key to interpreting results. Quick check: How does "natural context drift" differ from "adversarial robustness" testing?

- **Parametric vs. Contextual Knowledge**: The methodology separates cases where models rely on internal weights versus provided text. Quick check: Why does the framework exclude questions the model answers correctly *without* the passage?

- **Semantic Similarity Metrics (Sentence Embeddings)**: The independent variable is semantic similarity scores measured by models like all-MiniLM-L6-v2. Quick check: What does a slope of "70" mean in terms of the relationship between embedding similarity and model accuracy?

## Architecture Onboarding

- **Component map**: Revision Crawler -> Training Matcher -> Similarity Engine -> Evaluation Harness
- **Critical path**: The Training Matcher is most brittle; accurate matching to exact training corpus snapshots is essential for valid similarity scores
- **Design tradeoffs**: Inclusion Match (IM) for extraction tasks is strict but may miss semantic equivalents; semantic similarity for generative tasks is more flexible but introduces noise
- **Failure signatures**: False positives (high accuracy on context-free questions) indicate memorization contamination; human-model performance gaps confirm model-specific failures
- **First 3 experiments**:
  1. Manually trace 5 random passages from BOOLQ to verify Training Matcher logic in Dolma
  2. Run context-free prompting on BOOLQ subset to observe data filtering volume
  3. Reproduce slope analysis for SQuAD 1.1 and HotpotQA to verify reasoning task robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Temporal alignment uncertainty: Framework depends on accurately matching benchmark passages to exact training corpus snapshots
- Single-context design: Results may not generalize to multi-document or dialogue contexts
- Semantic similarity proxy: Sentence embeddings may not fully capture semantic equivalence for logical relationships

## Confidence
- High Confidence: Core finding of accuracy degradation with semantic drift while human performance remains stable
- Medium Confidence: Mechanism attribution (surface-form anchoring vs. parametric conflict) is plausible but not definitively proven
- Medium Confidence: Differential resilience of reasoning tasks appears consistent but could reflect architectural preferences

## Next Checks
1. Manually verify Training Matcher accuracy by tracing 20 random passages from BOOLQ and SQuAD to exact locations in Dolma training corpus
2. Run context-free prompting experiment across all six datasets to measure percentage of questions answered correctly without passage access
3. Replicate slope analysis for HotpotQA across all eight models to determine consistency of differential resilience