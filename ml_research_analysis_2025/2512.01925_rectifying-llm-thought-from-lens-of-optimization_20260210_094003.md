---
ver: rpa2
title: Rectifying LLM Thought from Lens of Optimization
arxiv_id: '2512.01925'
source_url: https://arxiv.org/abs/2512.01925
tags:
- density
- same
- earth
- reasoning
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Rectifying LLM Thought from Lens of Optimization

## Quick Facts
- **arXiv ID**: 2512.01925
- **Source URL**: https://arxiv.org/abs/2512.01925
- **Authors**: Junnan Liu; Hongwei Liu; Songyang Zhang; Kai Chen
- **Reference count**: 40
- **Primary result**: RePro improves reasoning accuracy on AIME24/25, MATH500, LiveCodeBench, and GPQA-Diamond while reducing token usage by 17-54% compared to baseline GRPO methods.

## Executive Summary
This paper introduces RePro, a plug-in module that refines long-chain-of-thought (CoT) reasoning during RLVR post-training by treating reasoning trajectories as implicit optimization processes. The framework computes process-level rewards based on optimization quality (magnitude and stability of progress toward ground truth) and integrates these with outcome rewards to reduce overthinking. Experiments show consistent accuracy improvements across math, science, and coding benchmarks while significantly reducing token usage through entropy-based segment selection.

## Method Summary
RePro computes a proxy objective $\tilde{\mathcal{J}}$ measuring average log-probability of ground-truth tokens during reasoning. It then evaluates each reasoning segment using dual scores: magnitude (net progress) and stability (monotonic improvement via Kendall's Tau). Top-k high-entropy segments are selected for reward computation, with process rewards normalized and combined with outcome rewards via a small weight (Œ±=0.1). The method is compatible with PPO, GRPO, and REINFORCE++ variants, requiring only KV-cache reuse for computational efficiency.

## Key Results
- RePro achieves 35.7‚Üí36.9% improvement on AIME24 with DeepSeek-R1-Distill-Qwen-1.5B
- Reduces token usage by 17-54% across benchmarks while maintaining or improving accuracy
- Shows consistent gains on MATH500 (65.6‚Üí66.8%), LiveCodeBench (65.4‚Üí66.6%), and GPQA-Diamond (29.4‚Üí29.8%)
- Ablation shows magnitude score more critical than stability for math tasks
- Effective on models from 950M to 8B parameters

## Why This Works (Mechanism)

### Mechanism 1: CoT as Implicit Gradient Descent
- Claim: Reasoning trajectory decoding can be modeled as an iterative optimization process over internal model states.
- Mechanism: Each reasoning step updates the model's implicit state, increasing the likelihood of the correct answer. The proxy objective $\tilde{\mathcal{J}} = \frac{1}{|a|}\sum_{i=1}^{|a|}\log\pi_\theta(a^{(i)}|q,\tau^{(\leq t)})$ tracks perplexity on ground-truth tokens. Suboptimal reasoning manifests as oscillations around saddle points or local optima rather than convergence.
- Core assumption: The probability of generating the ground truth answer serves as an effective proxy for an underlying optimization objective‚Äîthis is a conceptual analogy, not a proven biological or computational equivalence.
- Evidence anchors:
  - [abstract] "framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution"
  - [section 3.1] Eq. 4 formalizes implicit parameter updates via $\tilde{\eta} \cdot \tilde{\nabla}_\theta \mathcal{J}$
  - [section 3.2] Figure 2 shows empirical evidence: $-\tilde{\mathcal{J}}$ decreases monotonically along correct reasoning trajectories
  - [corpus] Related work (Dai et al., 2023; Liu et al., 2025a) similarly treats reasoning as optimization; corpus evidence is consistent but not independently validating the specific proxy choice
- Break condition: If $\tilde{\mathcal{J}}$ does not correlate with reasoning quality (e.g., for tasks where ground truth is multi-token and conditional), the analogy fails. Not tested on open-ended generation without verifiable answers.

### Mechanism 2: Dual Scoring for Optimization Quality
- Claim: Process-level reward based on optimization intensity and stability improves RL training signals beyond terminal outcome rewards alone.
- Mechanism: 
  - **Magnitude Score** $\mathcal{S}_{magn,(t)} = \tanh(\Delta + 1) + 1$ measures relative increase in $\tilde{\mathcal{J}}$ from baseline (direct prediction without CoT). Higher score = greater reasoning benefit.
  - **Stability Score** $\mathcal{S}_{stab,(t)}$ uses Kendall's Tau correlation between $\tilde{\mathcal{J}}$ values and step indices; captures monotonic improvement vs. oscillation.
  - Combined as $\mathcal{S} = (1-w) \cdot \mathcal{S}_{magn} + w \cdot \mathcal{S}_{stab}$ with $w=0.5$ default.
- Core assumption: Effective reasoning should show both net progress toward the answer and smooth trajectory without excessive backtracking. This assumes the "correct" reasoning path is roughly monotonic.
- Evidence anchors:
  - [section 3.3] Formal definitions in Equations 7-10
  - [section 4.3] Ablation: RePro outperforms baseline across all $w$ values; slightly higher performance at lower $w$ suggests Magnitude Score may be more critical
  - [section 4.4] Figure 5 shows reduction in "backtracking" pattern frequency during training
  - [corpus] Limited corpus validation; related work on efficient reasoning (Hou et al., 2025; Aggarwal & Welleck, 2025) uses length-based penalties rather than trajectory dynamics
- Break condition: If reasoning inherently requires non-monotonic exploration (e.g., hypothesis testing with controlled backtracking), penalizing instability may harm performance on tasks requiring systematic search.

### Mechanism 3: Entropy-Based Segment Selection and Reward Integration
- Claim: Computing process rewards only at high-entropy decision points balances signal quality and computational cost.
- Mechanism:
  1. Segment thinking tokens by `\n\n` delimiters
  2. Select top-$k$ segments by first-token entropy $\mathcal{H}(c_{i,(0)})$
  3. Compute $\mathcal{S}$ for each selected segment with ground truth appended
  4. Rectifying reward $\tilde{r}_j = \mathcal{S}_j - \mathcal{S}_{j-1}$ (incremental gain)
  5. Normalize within group/batch (Eq. 15-18 per algorithm)
  6. Combined advantage: $\hat{A}_t = A + \alpha \cdot \tilde{A}_t$ with $\alpha=0.1$
- Core assumption: High-entropy tokens indicate decision points where suboptimal reasoning is more likely‚Äîuncertainty correlates with potential for error or wasted computation.
- Evidence anchors:
  - [section 3.4] Eqs. 11-14 define selection and reward computation
  - [section 4.3] Table 2: Increasing $k$ from 5‚Üí30 yields marginal improvements (AIME24: 35.7‚Üí36.9), suggesting diminishing returns
  - [section 4.4] Figure 6 shows 17-54% token reduction across benchmarks
  - [corpus] Wang et al. (2025b) and Cui et al. (2025) similarly identify high-entropy tokens as critical decision points in RL for reasoning
- Break condition: If entropy does not correlate with decision importance (e.g., in highly deterministic tasks or when entropy reflects noise rather than meaningful uncertainty), selection becomes random and adds computational overhead without benefit.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and Critic-Free RL Variants**
  - Why needed here: RePro is a plug-in module compatible with PPO, GRPO, REINFORCE++. Understanding advantage estimation, clipping, and normalization is required to correctly integrate $\hat{A}_t = A + \alpha \cdot \tilde{A}_t$ without destabilizing training.
  - Quick check question: Given a trajectory with outcome reward $r=1$ and process rewards $\tilde{r}=[0.2, -0.1, 0.3]$, compute the combined advantage at step 2 with $\alpha=0.1$ under GRPO normalization.

- **Concept: Perplexity and Log-Likelihood as Confidence Proxies**
  - Why needed here: The proxy objective $\tilde{\mathcal{J}}$ is average log-probability of ground-truth tokens. Interpreting this as model confidence in the answer is foundational to the entire framework.
  - Quick check question: If $\tilde{\mathcal{J}}_0 = -2.5$ and $\tilde{\mathcal{J}}_3 = -1.8$ after 3 reasoning steps, compute the relative improvement $\Delta$ and corresponding $\mathcal{S}_{magn,(3)}$.

- **Concept: Kendall's Tau and Rank Correlation**
  - Why needed here: Stability Score uses Kendall's Tau to measure monotonicity of $\tilde{\mathcal{J}}$ progression. Understanding this metric is necessary to diagnose why a trajectory receives low stability scores.
  - Quick check question: For $\tilde{\mathcal{J}}=[1.0, 1.2, 1.1, 1.3, 1.4]$, compute the Kendall's Tau correlation with indices $[1,2,3,4,5]$. Would this receive a high or low Stability Score?

## Architecture Onboarding

- **Component map:** Question q ‚Üí LLM Rollout ‚Üí Trajectory œÑ = [œÑ_thinking; œÑ_conclusion] ‚Üí Segment by \n\n ‚Üí N segments ‚Üí Entropy computation ‚Üí Select top-k high-entropy segments ‚Üí For each selected segment: Append ground truth a, compute log-prob ‚Üí Compute ≈¥ÃÉùí• sequence ‚Üí Magnitude Score + Stability Score ‚Üí ùíÆ ‚Üí Rectifying reward rÃÉ_j = ùíÆ_j - ùíÆ_{j-1} ‚Üí Normalize per algorithm ‚Üí Combined advantage √Ç_t = A (outcome) + Œ± ¬∑ √É_t (process) ‚Üí Policy update via clipped objective (Eq. 2)

- **Critical path:** The forward passes to compute $\tilde{\mathcal{J}}$ at each segment. These require KV-cache reuse (vLLM/SGLang) to avoid O(k √ó |trajectory|) re-computation. Without this optimization, training becomes prohibitively slow.

- **Design tradeoffs:**
  - **k (segments):** Higher k = more granular supervision but more forward passes. Paper shows marginal returns beyond k=20; default k=10 balances cost/performance.
  - **w (magnitude/stability balance):** Lower w emphasizes intensity over smoothness. Ablation suggests magnitude more critical for math tasks; stability may matter more for tasks requiring exploration.
  - **Œ± (process reward weight):** Default 0.1 prevents process rewards from overwhelming outcome signals. Sensitive domains may require tuning.

- **Failure signatures:**
  - Training instability with high Œ±: Process rewards introduce noise, destabilizing policy updates. Mitigate by reducing Œ± or increasing normalization group size.
  - No token reduction observed: Entropy-based selection may not be identifying meaningful decision points. Check entropy distribution‚Äîmay need domain-specific segmentation.
  - Performance degradation on simple tasks: Overthinking is less prevalent; RePro adds overhead without benefit. Consider conditional application (e.g., only for problems above difficulty threshold).

- **First 3 experiments:**
  1. **Baseline validation:** Train DeepSeek-R1-Distill-Qwen-1.5B with vanilla GRPO on DeepScaleR-Preview-Dataset (40K math samples). Record AIME24/MATH500 accuracy and average token count. This establishes the overthinking baseline RePro is designed to address.
  2. **Ablation on k:** With fixed Œ±=0.1, w=0.5, run k‚àà{5,10,20,30} on held-out validation set. Plot accuracy vs. training time to identify the k where marginal accuracy gain < 0.5% per doubling of compute. Target: justify default k=10.
  3. **Cross-domain transfer:** Train on math-only data, evaluate on GPQA-Diamond (science) and LiveCodeBench (code). Compare RePro vs. vanilla GRPO on out-of-domain token efficiency. Hypothesis: RePro's optimization framing generalizes; if not, may need domain-specific calibration of w or Œ±.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal values for hyperparameters (weight w balancing magnitude/stability scores, RePro weight Œ±, and number of selected segments k) be determined theoretically rather than empirically?
- **Basis in paper:** [inferred] The ablation study (¬ß4.3, Table 2, Figure 4) shows performance varies with w and Œ±, but values are set empirically (w=0.5, Œ±=0.1, k=10) without principled guidance.
- **Why unresolved:** No theoretical framework connects these parameters to model architecture, task type, or training dynamics.
- **What evidence would resolve it:** A systematic study showing relationships between optimal hyperparameter values and model/task characteristics, or a theoretical bound on their selection.

### Open Question 2
- **Question:** To what extent does RePro benefit base models without pre-existing reasoning capabilities compared to models already fine-tuned for reasoning?
- **Basis in paper:** [explicit] Section C.3 states RePro and GRPO "achieve comparable performance" on Qwen3-4B-Base, contrasting with consistent improvements on reasoning-capable models.
- **Why unresolved:** The mechanism by which RePro provides process-level supervision may depend on existing reasoning structure that base models lack.
- **What evidence would resolve it:** Controlled experiments across model scales comparing RePro's relative improvement on base vs. instruction-tuned vs. reasoning-distilled models.

### Open Question 3
- **Question:** How does RePro's computational overhead scale with reasoning trajectory length, and what are the practical limits of its applicability?
- **Basis in paper:** [inferred] The paper mentions KV caching mitigates costs (Appendix A) but provides no quantitative overhead analysis or scaling curves.
- **Why unresolved:** Long-CoT trajectories can exceed thousands of tokens; forward passes for surrogate objective computation may become prohibitive.
- **What evidence would resolve it:** Detailed timing/memory benchmarks showing training overhead as a function of trajectory length and segment count k.

## Limitations

- The optimization analogy between CoT reasoning and gradient descent lacks rigorous mathematical proof beyond empirical observation.
- Entropy-based segment selection may not universally identify decision points across all reasoning domains, particularly for tasks where uncertainty manifests differently than token entropy.
- The framework's computational overhead during training may be prohibitive for very long reasoning trajectories without efficient KV-cache reuse.

## Confidence

**High Confidence:**
- The empirical results showing RePro's effectiveness across multiple benchmarks (AIME24, MATH500, LiveCodeBench) are robust and reproducible given the described methodology.
- The dual scoring mechanism (magnitude + stability) provides a reasonable framework for identifying suboptimal reasoning patterns.
- The token efficiency improvements (17-54% reduction) are directly measurable and significant.

**Medium Confidence:**
- The optimization analogy for CoT reasoning is conceptually sound but lacks rigorous mathematical validation beyond empirical observation.
- The choice of hyperparameters (k=10, w=0.5, Œ±=0.1) appears reasonable but may not be optimal across all domains.
- The entropy-based selection method correlates with decision points but may miss important reasoning segments in certain task types.

**Low Confidence:**
- The generalizability of RePro to non-mathematical reasoning domains remains largely untested.
- The framework's behavior on highly exploratory reasoning tasks (where backtracking is intentional) is not well-characterized.
- The long-term stability of models trained with process rewards versus outcome rewards alone is not evaluated.

## Next Checks

1. **Cross-Domain Robustness Test:** Apply RePro-trained models to GPQA-Diamond (science reasoning) and LiveCodeBench (code generation) to quantify whether the optimization framing generalizes beyond mathematical reasoning. Measure both accuracy and token efficiency to determine if process rewards maintain their effectiveness when ground truth format and reasoning patterns differ substantially.

2. **Backtracking Task Evaluation:** Design a controlled benchmark with tasks requiring intentional hypothesis testing and exploration (e.g., logic puzzles with multiple solution paths). Compare RePro versus vanilla RL to determine whether penalizing stability actually harms performance on tasks where controlled backtracking is essential for correctness.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary Œ± (0.01‚Üí0.5), w (0.0‚Üí1.0), and k (5‚Üí30) on a single benchmark (e.g., MATH500) while measuring three outcomes: final accuracy, training stability (gradient norm variance), and computational overhead. Identify regions where RePro provides net benefit versus where it introduces instability or unnecessary cost.