---
ver: rpa2
title: Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity
  in the Internal Circuitry of LLMs
arxiv_id: '2509.21044'
source_url: https://arxiv.org/abs/2509.21044
tags:
- arxiv
- llms
- internal
- activation
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the question of why reinforcement learning
  (RL) fine-tuning enhances the capabilities of large language models (LLMs) beyond
  supervised fine-tuning (SFT). To explore the underlying mechanisms, the authors
  adapt the Edge Attribution Patching (EAP) framework to analyze internal differences
  in LLMs before and after RL fine-tuning.
---

# Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs

## Quick Facts
- arXiv ID: 2509.21044
- Source URL: https://arxiv.org/abs/2509.21044
- Authors: Honglin Zhang; Qianyue Hao; Fengli Xu; Yong Li
- Reference count: 29
- Primary result: RL fine-tuning increases activation intensity and diversity in LLM internal circuitry, with PPO/GRPO showing stronger effects than DPO

## Executive Summary
This study investigates why reinforcement learning (RL) fine-tuning enhances LLM capabilities beyond supervised fine-tuning by analyzing internal circuitry changes using the Edge Attribution Patching framework. The research reveals two robust effects across multiple model families and mathematical datasets: increased average activation intensity indicating stronger engagement of internal pathways, and greater diversity in activation patterns reflected by higher entropy and less concentrated edge distributions. These findings suggest RL reshapes information flow to be both more redundant and flexible, potentially explaining its advantage in mathematical generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, showing weaker or inconsistent internal changes compared to PPO- and GRPO-based training.

## Method Summary
The authors adapt the Edge Attribution Patching (EAP) framework to analyze internal differences in LLMs before and after RL fine-tuning. This mechanistic interpretability approach measures how edge activations contribute to model outputs, allowing researchers to quantify changes in activation intensity and diversity. The analysis was conducted across multiple model families and mathematical datasets to establish robustness. By comparing models fine-tuned with different RL methods (PPO, GRPO, and DPO), the study identifies systematic patterns in how RL alters internal circuitry, with a particular focus on understanding why RL shows advantages in mathematical reasoning tasks.

## Key Results
- RL fine-tuning produces an overall increase in average activation intensity across internal pathways
- Greater diversity in activation patterns emerges after RL fine-tuning, shown by higher entropy and less concentrated edge distributions
- DPO-based fine-tuning shows weaker or inconsistent internal changes compared to PPO- and GRPO-based methods

## Why This Works (Mechanism)
RL fine-tuning appears to work by reshaping the information flow within LLMs, making internal pathways both stronger and more diverse. The increased activation intensity suggests that RL strengthens engagement across multiple routes in the model's circuit, while the diversity increase indicates that RL creates multiple parallel pathways for information processing rather than relying on single dominant routes. This combination of redundancy and flexibility may explain RL's superior performance in tasks requiring mathematical generalization, where multiple solution paths and robust reasoning are valuable.

## Foundational Learning
- **Edge Attribution Patching Framework**: A mechanistic interpretability method that measures how individual connections (edges) between neurons contribute to model outputs; needed to quantify internal changes, quick check: verifies that edge attribution correlates with actual impact on predictions
- **Activation Intensity**: The strength or magnitude of neuron activations in the model; needed to measure pathway engagement, quick check: confirms higher values correspond to stronger information flow
- **Entropy in Activation Patterns**: A measure of diversity or unpredictability in how information flows through the network; needed to quantify flexibility, quick check: higher entropy indicates multiple active pathways rather than single dominant ones
- **DPO vs PPO/GRPO Methodology**: Different RL approaches where DPO uses preference-based learning while PPO/GRPO use online reinforcement; needed to understand why DPO shows different patterns, quick check: verifies methodological differences persist across implementations
- **Mathematical Reasoning Datasets**: Specialized evaluation sets used to test model capabilities; needed to demonstrate generalization benefits, quick check: ensures tasks require genuine reasoning rather than pattern matching
- **Model Families**: Different LLM architectures tested to establish generalizability; needed to rule out architecture-specific effects, quick check: confirms patterns hold across diverse designs

## Architecture Onboarding
Component Map: Input -> Embedding Layer -> Transformer Blocks -> Output Layer
Critical Path: Edge Attribution Patching measures influence along activation pathways through transformer layers
Design Tradeoffs: Balancing activation intensity (strength) with diversity (flexibility) versus computational efficiency
Failure Signatures: DPO showing inconsistent patterns suggests methodological limitations in preference-based approaches
First 3 Experiments: 1) Replicate activation intensity findings on non-mathematical domains, 2) Test entropy measures with alternative interpretability methods, 3) Isolate DPO methodology variables in controlled comparisons

## Open Questions the Paper Calls Out
None

## Limitations
- Findings focus primarily on mathematical reasoning tasks, limiting generalizability to other domains
- Results depend on the Edge Attribution Patching framework, which may influence outcomes through experimental design
- DPO versus other RL methods comparison is based on limited model families and datasets

## Confidence
- High confidence in RL fine-tuning increasing activation intensity and diversity across tested conditions
- Medium confidence in interpreting these changes as explaining RL's mathematical generalization advantage
- Medium confidence in DPO versus PPO/GRPO distinctions pending broader validation

## Next Checks
1. Test whether activation intensity and diversity increases extend to non-mathematical domains and diverse datasets
2. Replicate findings using alternative mechanistic interpretability methods to verify robustness of the Edge Attribution Patching approach
3. Conduct controlled experiments isolating DPO methodology from other variables to determine whether observed differences are intrinsic to the approach