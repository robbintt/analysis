---
ver: rpa2
title: 'SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich
  Industrial Assembly Tasks'
arxiv_id: '2509.13949'
source_url: https://arxiv.org/abs/2509.13949
tags:
- learning
- reinforcement
- share
- human
- assembly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHaRe-RL is a structured, interactive reinforcement learning framework
  for contact-rich industrial assembly. It combines manipulation primitives, human
  demonstrations and online corrections, and per-axis adaptive force limits to enable
  safe, sample-efficient learning in real-world tasks.
---

# SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks

## Quick Facts
- **arXiv ID:** 2509.13949
- **Source URL:** https://arxiv.org/abs/2509.13949
- **Reference count:** 40
- **Key result:** 95% success rate and 5.4 s cycle time for Harting connector insertion within 3 h of real-world interaction

## Executive Summary
SHaRe-RL addresses the challenge of learning contact-rich industrial assembly tasks by combining structured manipulation primitives, human demonstrations, online corrections, and adaptive force limits. The framework decomposes complex assembly into a state machine of primitives, where only specific parameters are exposed to reinforcement learning while others are fixed. This structure, combined with off-policy learning from both demonstrations and interventions, enables efficient exploration in sparse reward settings. The adaptive safety mechanism provably bounds contact forces while preserving free-space dynamics, allowing aggressive exploration without risking damage.

## Method Summary
SHaRe-RL structures assembly tasks into Manipulation Primitive Nets (MP-Nets) - a state machine where each state corresponds to a specific phase of assembly. Within these primitives, Adaptive Manipulation Primitives (AMPs) expose only relevant parameters to the RL policy. The system uses RLPD (an off-policy algorithm) to learn from both demonstration buffers and online experience, with human operators intervening when failures are imminent. A key innovation is the adaptive force limit mechanism that applies per-axis compliance, using a recurrence relation to dynamically adjust force bounds based on measured contact forces. This allows the system to maintain aggressive free-space motion while guaranteeing safe contact forces during exploration.

## Key Results
- Achieves 95% success rate for 0.2-0.4 mm clearance Harting connector insertion
- Reduces cycle time to 5.4 seconds compared to 6.5 seconds for skilled human demonstrator
- Outperforms HG-DAgger baseline (80% success, 7.4 seconds) with only 3 hours of real-world interaction
- Ablation studies confirm necessity of each component for achieving reliable high-precision assembly

## Why This Works (Mechanism)

### Mechanism 1: Search Space Compression via Manipulation Primitives
Structuring tasks into Manipulation Primitive Nets (MP-Nets) reduces the effective dimensionality of the learning problem by decomposing long-horizon tasks into distinct phases. Only specific parameters within Adaptive Manipulation Primitives (AMPs) are exposed to the RL policy, while fixed primitives handle deterministic phases. This prevents the policy from wasting samples on solved sub-problems and localizes uncertainties to specific axes during relevant phases.

### Mechanism 2: Off-Policy Acceleration with Human-in-the-Loop Residuals
Combining offline demonstrations with online interventions accelerates convergence in sparse reward settings by seeding the replay buffer with high-reward trajectories and correcting distribution shift. RLPD samples equally from demonstration and online buffers, while interventions inject expert actions directly into trajectories. This guides the agent through the "ignorance window" where random exploration would fail to find the sparse goal state.

### Mechanism 3: Adaptive Compliance for Safe Force Convergence
Adaptive per-axis force limits enable faster, safer exploration by allowing aggressive free-space motion while guaranteeing bounded contact forces upon collision. The controller applies a recurrence relation where the force limit is high when no force is measured but decays exponentially upon contact, creating a "soft landing" dynamic where commanded forces converge to a safe equilibrium rather than spiking destructively.

## Foundational Learning

- **Concept: Task Frame Formalism (TFF)**
  - Why needed here: MP-Nets and compliance mechanism rely on defining control modes relative to a specific task frame
  - Quick check question: Can you define a coordinate frame where the z-axis aligns with the insertion direction, and distinguish which axes require position control vs. force control?

- **Concept: Off-Policy Reinforcement Learning (RLPD/SAC)**
  - Why needed here: Method relies on reusing old data (demonstrations) interleaved with new experience
  - Quick check question: Explain why an off-policy algorithm can learn from a static dataset of demonstrations while an on-policy algorithm generally cannot

- **Concept: Sparse vs. Dense Rewards**
  - Why needed here: Industrial tasks often lack intermediate feedback, making the "sparse reward" problem critical
  - Quick check question: In a sparse reward setting (success = 1, else 0), why would pure random exploration fail to ever discover the goal in a high-dimensional space?

## Architecture Onboarding

- **Component map:** Calibration -> MP-Net state machine (Approach -> Insert -> Press) -> RL Policy (within InsertAMP) -> Safety Layer (adaptive force limits) -> Impedance Controller -> Robot

- **Critical path:**
  1. Calibration: Define Task Frame and MP-Net transitions
  2. Data Seeding: Collect 20 teleoperated demos in Demonstration Buffer
  3. Training Loop: Start autonomous rollouts with operator monitoring and interventions
  4. Safety Tuning: Verify adaptive force limit stability before high-speed exploration

- **Design tradeoffs:**
  - Structure vs. Generality: MP-Nets make learning faster but require manual engineering for new geometries
  - Intervention vs. Autonomy: Interventions ensure safety but may bias policy toward human sub-optimality
  - Vision vs. Proprioception: Vision necessary for lateral alignment but adds observation noise

- **Failure signatures:**
  - Oscillating Forces: If stability condition violated, contact forces may oscillate violently
  - Plateauing at ~80%: Policy converges to demonstrator's skill level if RL updates too weak
  - "Blind" Misalignment: If wrist-camera occluded, policy cannot resolve rotational alignment

- **First 3 experiments:**
  1. Validate Safety Limits: Run InsertAMP with zero-velocity policy to verify force bounds
  2. Ablate Structure: Train flat RL policy (no MP-Net) to confirm sample efficiency drop-off
  3. Compare Data Sources: Train variants with demos only, interventions only, and both

## Open Questions the Paper Calls Out

### Open Question 1
Can SHaRe-RL scale to longer-horizon assembly processes with multiple adaptive primitives operating concurrently? The current evaluation involves only one adaptive primitive; error propagation and credit assignment across multiple learned primitives remain untested.

### Open Question 2
How well does SHaRe-RL generalize across connector geometries, tolerances, and part variations without retraining? Only HanDD connectors with 0.2-0.4 mm clearance were tested; vision system may overfit to specific geometry.

### Open Question 3
Can human supervision requirements be reduced through model-based exploration or self-supervised rewards? The method still depends on 20 demonstrations and early-stage interventions; this overhead may limit adoption in SME settings.

### Open Question 4
Can MP-Net structure be automatically generated from CAD models or task specifications? Currently, MPs and their transition conditions require manual engineering; this represents remaining setup cost.

## Limitations
- MP-Net decomposition requires careful manual engineering for each new task geometry
- Performance heavily depends on quality and quantity of human demonstrations (20 required)
- Evaluation focuses on single connector type, leaving generalization to other geometries untested
- Adaptive force limit may face practical challenges from sensor noise and control latency

## Confidence
- **High Confidence:** Adaptive force limit stability proof and practical effectiveness in bounding forces
- **Medium Confidence:** Sample efficiency improvements relative to baselines
- **Medium Confidence:** Generalization claims, as experiments focus on one connector type

## Next Checks
1. Test SHaRe-RL on a geometrically different connector assembly task to assess generalization
2. Systematically vary demonstration quality to quantify robustness to imperfect human input
3. Measure performance degradation when removing the safety layer to validate its necessity in real-world operation