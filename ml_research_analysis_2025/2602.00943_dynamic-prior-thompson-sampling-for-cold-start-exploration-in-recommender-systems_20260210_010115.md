---
ver: rpa2
title: Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems
arxiv_id: '2602.00943'
source_url: https://arxiv.org/abs/2602.00943
tags:
- prior
- exploration
- dynamic
- thompson
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cold-start problem in recommender systems
  where new items must be explored while minimizing user impact. The authors propose
  Dynamic Prior Thompson Sampling, which replaces uniform priors with adaptive priors
  that control exploration probability.
---

# Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems

## Quick Facts
- arXiv ID: 2602.00943
- Source URL: https://arxiv.org/abs/2602.00943
- Authors: Zhenyu Zhao; David Zhang; Ellie Zhao; Ehsan Saberian
- Reference count: 13
- Key outcome: +0.19% lift in Qualified Play-Through Rate and 21% reduction in regretted impressions in online experiment

## Executive Summary
This paper addresses the cold-start problem in recommender systems where new items must be explored while minimizing user impact. The authors propose Dynamic Prior Thompson Sampling, which replaces uniform priors with adaptive priors that control exploration probability. The key innovation is a closed-form quadratic solution that ensures new arms have a target probability ε of outperforming the current best arm. The method is validated through Monte Carlo simulations, batched simulations, and a large-scale online experiment on thumbnail personalization.

## Method Summary
The method computes dynamic Beta priors for new arms using a closed-form quadratic solution that calibrates the prior mean to achieve a target probability ε of outperforming the current best arm's posterior. The prior strength parameter r controls the effective sample size. The approach preserves Thompson Sampling's Bayesian update structure while providing controlled exploration during the "prior-dominant" window when feedback hasn't yet been incorporated due to batched updates.

## Key Results
- Monte Carlo simulations validated that calibrated priors achieve target ε with <0.01 absolute error
- Batched simulations showed dynamic priors outperform uniform priors (Beta(1,1)) in cumulative reward when exploring weak new arms
- Online experiment demonstrated +0.19% lift in Qualified Play-Through Rate and 21% reduction in regretted impressions

## Why This Works (Mechanism)

### Mechanism 1: Target Probability Enforcement via Closed-Form Prior Computation
- Claim: Computing a dynamic prior mean via quadratic solution enforces P(X_j > Y_k) = ε at new arm introduction.
- Mechanism: The method derives prior mean q_j from coefficients A_q, B_q, C_q that encode the best arm's observed success rate (p̂_k), sample size (n_k), target probability (ε), and prior strength (r). Setting Beta(α_prior,j, β_prior,j) = (n_k·r·q_j, n_k·r·(1-q_j)) calibrates the new arm's initial Thompson sample distribution against the incumbent's posterior.
- Core assumption: Normal approximation to Beta posteriors is sufficiently accurate when n_k is large (thousands to millions of observations).
- Evidence anchors:
  - [abstract] "closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time"
  - [Section 3.3] Full derivation with coefficients A_q, B_q, C_q and solution formula
  - [corpus] Weak direct evidence; related cold-start papers focus on learned exploration rather than prior calibration
- Break condition: When n_k is small (<1000), normal approximation degrades; fallback q_j = ε·p̂_k activates (Algorithm 1, line 4).

### Mechanism 2: Batched Deployment Latency Compensation
- Claim: Calibrated priors reduce impression waste during pipeline delay windows when feedback hasn't yet been incorporated.
- Mechanism: Under batched updates with interval Δt (2–8 hours), new arms remain in a "prior-dominant" state. Uniform Beta(1,1) implicitly assumes 50% success probability, causing systematic over-allocation to weak items when true base rates are 1–5%. Dynamic priors align the prior-dominant window with realistic competition levels.
- Core assumption: The best arm's posterior is stable during the batch interval and representative of the competitive baseline.
- Evidence anchors:
  - [Section 1.1] "Throughout [t0, t0+Δt), the new arm remains governed by its prior"
  - [Section 1.2] "dynamic priors explicitly design this 'prior-dominant' window to prevent prolonged over-exposure"
  - [corpus] Item Level Exploration paper mentions production constraints but doesn't analyze prior design
- Break condition: If the incumbent arm's performance shifts dramatically mid-batch (non-stationarity), the computed prior may be misaligned.

### Mechanism 3: Smooth Posterior-Driven Transition Without Policy Discontinuities
- Claim: Dynamic priors preserve Thompson Sampling's Bayesian update structure, avoiding sharp phase transitions.
- Mechanism: Unlike forced-exploration heuristics that guarantee traffic for K batches then abruptly stop, dynamic priors are intrinsic to the TS decision rule. As real data accumulates (α_j, β_j increase from feedback), the prior's influence naturally diminishes and allocation adapts continuously.
- Core assumption: Feedback pipeline eventually delivers data; if data is permanently delayed, prior influence persists indefinitely.
- Evidence anchors:
  - [Section 1.2] "dynamic priors remain intrinsic to the TS decision rule and adapts smoothly as posteriors evolve"
  - [Section 5.3] Figure 5 shows forced exploration's sharp boundaries vs. Figure 3's smooth dynamic prior curves
  - [corpus] No direct corpus comparison on transition smoothness
- Break condition: If prior strength r is set too high relative to incoming data rate, prior may dominate for too many batches.

## Foundational Learning

- Concept: **Thompson Sampling with Beta-Bernoulli conjugacy**
  - Why needed here: The entire method builds on Beta posteriors updating with Bernoulli rewards; you must understand how α (success count + prior) and β (failure count + prior) parameterize the distribution.
  - Quick check question: Given Beta(α=10, β=90), what's the posterior mean and how does it change after observing 1 success?

- Concept: **Exploration-exploitation tradeoff in bandits**
  - Why needed here: The ε parameter directly controls this tradeoff; understanding why over-exploration harms user experience while under-exploration delays winner discovery is essential for tuning.
  - Quick check question: If ε=0.01 vs. ε=0.10, how does the new arm's initial traffic share differ, and what's the regret implication for a weak arm?

- Concept: **Batched policy updates and pipeline latency**
  - Why needed here: Production systems don't update per-impression; the paper's core contribution addresses this deployment constraint specifically.
  - Quick check question: If your batch interval is 6 hours and a new item launches at hour 0, when does its first posterior update occur?

## Architecture Onboarding

- Component map: Prior Computation Module -> TS Sampling Engine -> Feedback Aggregator -> Batch Scheduler
- Critical path:
  1. New arm introduced -> Prior Computation Module calculates dynamic prior
  2. TS Sampling Engine uses prior until first batch update
  3. Feedback Aggregator collects data during batch interval
  4. Batch Scheduler triggers posterior update, prior influence fades
  5. Repeat sampling with updated posteriors
- Design tradeoffs:
  - **ε selection**: Lower (0.01–0.03) minimizes waste but slows discovery; higher (0.10) provides visible exploration signal for developer dashboards
  - **r selection**: Controls how many batches before data overtakes prior; r=0.01 yields ~1% of n_k effective sample size, r=0.1 yields ~10%
  - **Fallback behavior**: When quadratic solution yields invalid q_j ≤ 0 or q_j ≥ p̂_k, falls back to q_j = ε·p̂_k
- Failure signatures:
  - **Persistent over-allocation**: ε set too high or r too strong; new weak arms maintain traffic share
  - **Starvation of promising new arms**: ε set too low; breakout rate drops
  - **Non-convergence**: Batch interval too long relative to traffic volume; posteriors never stabilize
- First 3 experiments:
  1. **Monte Carlo validation of P(X_j > Y_k)**: With n_k=10^5, p̂_k=0.05, ε=0.03, r=0.01, run 100K samples to verify empirical probability matches target within 0.01
  2. **Batched simulation comparison**: 10 batches, weak new arm (p_new=0.01), compare dynamic prior (ε=0.01, r=0.001) vs. uniform Beta(1,1) on cumulative reward
  3. **A/A test for online experiment**: Before treatment launch, verify no statistically significant difference between control and treatment on QPTR (p>0.05)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic priors be extended to contextual bandit architectures while preserving exact exploration probability guarantees?
- Basis in paper: [explicit] "Future research will explore the integration of this methodology with contextual bandit architectures to leverage high-dimensional side information while maintaining the exact exploration guarantees established in this work."
- Why unresolved: The current derivation assumes non-contextual Beta posteriors; contextual settings require conditional distributions that may not yield closed-form solutions.
- What evidence would resolve it: A theoretical derivation showing exploration probability control under contextual Thompson Sampling, validated empirically.

### Open Question 2
- Question: Can dynamic priors jointly optimize multiple objectives such as engagement, fairness, and ecosystem diversity?
- Basis in paper: [explicit] "We plan to investigate multi-objective dynamic priors that can simultaneously optimize for engagement, fairness, and long-term ecosystem diversity."
- Why unresolved: The current formulation targets a scalar success probability; multi-objective settings introduce trade-offs with no single optimal prior.
- What evidence would resolve it: Pareto-optimal prior formulations with controlled exploration across multiple metrics.

### Open Question 3
- Question: What is the principled approach for setting the prior strength parameter r across different deployment contexts?
- Basis in paper: [inferred] The paper tests r ∈ {0.01, 0.05, 0.1, 0.001} across experiments without deriving optimal values; r=0.1 was chosen based on production pipeline constraints but the generalization is unclear.
- Why unresolved: r balances prior stability against posterior responsiveness, yet no theoretical framework guides its selection for new domains.
- What evidence would resolve it: A systematic study relating r to pipeline delay, base rates, and regret bounds.

### Open Question 4
- Question: How do dynamic priors perform when multiple new arms are introduced simultaneously rather than sequentially?
- Basis in paper: [inferred] The online experiment addresses "mid-flight insertion" of a single new arm; simulation introduces one new arm at batch 5. Real systems often introduce many new items concurrently.
- Why unresolved: Simultaneous introduction creates competition among new arms, and the calibration P(X_j > Y_k) = ε may not generalize to pairwise comparisons among new arms.
- What evidence would resolve it: Simulation and online experiments with multiple concurrent new arm introductions.

## Limitations

- The method's performance depends critically on the stability of the best arm's posterior during batch intervals; significant performance shifts can misalign the computed prior.
- The normal approximation used for closed-form computation degrades when the best arm's sample size is small (<1000), requiring fallback mechanisms.
- The approach assumes the best arm's posterior represents the competitive baseline, which may not hold in highly dynamic recommendation environments.

## Confidence

- **High confidence**: The Monte Carlo simulation results showing calibrated priors achieve target ε with <0.01 absolute error (Section 5.1), and the quadratic solution derivation (Section 3.3) are mathematically sound and empirically validated.
- **Medium confidence**: The batched simulation results comparing dynamic priors to uniform priors show expected behavior, but the simulation assumptions (stationary arms, known p_new) may not fully capture production complexity.
- **Medium confidence**: The online experiment results (+0.19% QPTR lift, 21% reduction in regretted impressions) are statistically significant, but the short 7-day duration and specific context (thumbnail personalization) may limit generalizability.

## Next Checks

1. **Non-stationarity stress test**: Run Monte Carlo simulations where the best arm's true success rate drifts by ±50% during the batch interval. Measure how this affects the new arm's actual P(X_j > Y_k) compared to the target ε.
2. **Small-sample robustness validation**: For n_k < 1000, conduct experiments to quantify the error between actual P(X_j > Y_k) and target ε when using the fallback mechanism q_j = ε·p̂_k.
3. **Cross-domain Generalization**: Deploy the method in a different recommendation context (e.g., movie recommendations) and validate whether the optimal ε and r parameters transfer, or if they require domain-specific tuning.