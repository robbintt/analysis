---
ver: rpa2
title: Modeling Layered Consciousness with Multi-Agent Large Language Models
arxiv_id: '2510.17844'
source_url: https://arxiv.org/abs/2510.17844
tags:
- consciousness
- emotional
- unconsciousness
- example
- self-awareness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-agent framework for modeling artificial
  consciousness in large language models (LLMs), grounded in psychoanalytic theory.
  The proposed Psychodynamic Model simulates self-awareness, preconsciousness, and
  unconsciousness through agent interaction, guided by a Personalization Module combining
  fixed traits and dynamic needs.
---

# Modeling Layered Consciousness with Multi-Agent Large Language Models

## Quick Facts
- arXiv ID: 2510.17844
- Source URL: https://arxiv.org/abs/2510.17844
- Reference count: 5
- This study introduces a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory.

## Executive Summary
This paper proposes a Psychodynamic Model that simulates layered consciousness through multi-agent dialogue among specialized LLM agents representing self-awareness, preconsciousness, and unconsciousness. The system employs a Personalization Module that combines fixed personality traits with dynamic needs and emotional states to generate context-sensitive responses. Using parameter-efficient fine-tuning on emotionally rich dialogues, the framework was evaluated across eight personalized conditions, demonstrating improved emotional depth and reduced output variance compared to baseline models. An LLM-as-a-Judge approach showed 71.2% preference for the fine-tuned model, suggesting potential for adaptive, personalized cognition.

## Method Summary
The Psychodynamic Model employs three specialized LLM agents (Self-awareness, Preconsciousness, Unconsciousness) that engage in multi-turn dialogue through an orchestrator agent. The Personalization Module encodes character traits as Fixed State and dynamic needs/states as Flexible State, both represented in natural language. The unconsciousness agent underwent parameter-efficient fine-tuning (LoRA, rank 16) on emotionally rich dialogue data to enhance emotional expressiveness. The system was evaluated using an LLM-as-a-Judge approach across eight personalized conditions, comparing against GPT-4o baseline and analyzing output coherence, emotional depth, and personalization scores.

## Key Results
- Fine-tuned model preferred in 71.4% of evaluations (SD=3.7) vs. GPT-4o baseline for emotional expressiveness
- Personalization scores improved from 68.7% to 70.9% with Flexible State implementation
- Output variance decreased by 37.8% when needs/states were incorporated
- 71.2% overall preference for fine-tuned model in LLM-as-a-Judge evaluation

## Why This Works (Mechanism)

### Mechanism 1: Interconscious Reasoning via Multi-Agent Dialogue
- Claim: Structured dialogue between specialized consciousness agents produces more psychologically coherent outputs than single-model generation.
- Mechanism: Three distinct LLM agents engage in multi-turn chat-based exchanges, each articulating role-specific perspectives. An orchestrator agent routes turns and checks for consensus before generating a Final Action with explicit emotional tagging.
- Core assumption: Psychodynamic theory's tripartite consciousness model can be productively approximated through agent role-specialization and negotiated consensus.
- Evidence anchors:
  - [abstract] "simulates self-awareness, preconsciousness, and unconsciousness through agent interaction"
  - [section 3.1.2] "The process unfolds as a multi-turn exchange, resembling a discussion among three entities... We refer to this as Interconscious Reasoning"
  - [corpus] Neighbor paper "Emergence of Self-Awareness in Artificial Systems" proposes a three-layer model, suggesting layered approaches are an active research direction, though empirical validation remains limited.

### Mechanism 2: Personalization via Fixed and Flexible State Components
- Claim: Combining stable trait representations with dynamic need/state encoding enables context-sensitive, individualized response generation.
- Mechanism: Fixed State Component encodes personality traits and long-term memory as stable text. Flexible State Component represents short-term memory, fluctuating needs (Maslow's hierarchy), and current emotional states in natural language. Both feed into consciousness agents as conditioning context.
- Core assumption: Human behavioral variability arises from interactions between enduring dispositions and transient internal states, both representable in text.
- Evidence anchors:
  - [section 3.2] "Fixed State Component... encodes stable, long-term characteristics... Flexible State Component captures short-term, dynamic elements"
  - [section 4.2.1] "natural language format yielded the most consistent performance improvements" for encoding needs/states
  - [section 5.2.1] With needs/states implemented, personalization scores improved from 68.7% to 70.9%, and output variance decreased by 37.8%
  - [corpus] Limited direct corpus validation for this specific personalization approach; related work focuses on personality simulation without psychodynamic structure.

### Mechanism 3: Unconsciousness Agent Fine-Tuning for Emotional Expressiveness
- Claim: Parameter-efficient fine-tuning on emotionally rich dialogue data enhances the unconsciousness agent's ability to express latent emotional content that baseline safety-aligned models suppress.
- Mechanism: Filter EmpatheticDialogues dataset for deeply internalized emotions (14,804 instances). Apply LoRA-based PEFT to LLaMA 3.1 8B with rank 16, learning rate 2×10⁻⁴, 2 epochs, 4-bit quantization. This relaxes suppression of emotionally charged content while preserving base capabilities.
- Core assumption: Suppression mechanisms in aligned LLMs prevent authentic simulation of unconscious processes; targeted fine-tuning can selectively relax these constraints.
- Evidence anchors:
  - [section 4.1] "the unconsciousness agent... often fell short... due to the LLM's alignment and safety mechanisms"
  - [section 5.1.3] Fine-tuned model preferred in 71.4% of evaluations (SD=3.7) vs. GPT-4o baseline
  - [Table 1] Qualitative example shows fine-tuned model produces more direct emotional expression ("This is stupid!") vs. baseline's more filtered response
  - [corpus] No corpus papers directly validate this specific fine-tuning approach for unconsciousness simulation.

## Foundational Learning

- **Multi-Agent LLM Orchestration**
  - Why needed here: The architecture requires coordinating three specialized agents with distinct system prompts, managing turn-taking, and detecting consensus.
  - Quick check question: Can you explain how agent role differentiation differs from single-model multi-step prompting?

- **Parameter-Efficient Fine-Tuning (LoRA/PEFT)**
  - Why needed here: Fine-tuning the unconsciousness agent requires modifying emotional expression patterns without full retraining or catastrophic forgetting.
  - Quick check question: What is the trade-off between LoRA rank size and preservation of base model capabilities?

- **Psychodynamic Theory Fundamentals (Freud, Maslow)**
  - Why needed here: System design presupposes understanding of conscious/preconscious/unconscious distinctions and hierarchical need structures.
  - Quick check question: How does Maslow's need hierarchy map to the Flexible State Component's input structure?

## Architecture Onboarding

- **Component map:**
  Consciousness Module: Self-awareness, Preconsciousness, Unconsciousness agents + Orchestrator (routing, termination check, final action generation)
  Personalization Module: Fixed State (traits, long-term memory) + Flexible State (short-term memory, needs, states)
  Evaluation Layer: LLM-as-Judge (GPT-4o) with 10-item assessment across three groups

- **Critical path:**
  1. Define character profile (Fixed State text)
  2. Specify current condition (Flexible State: needs fulfillment, emotional state)
  3. Present situation to consciousness agents
  4. Run Interconscious Reasoning loop until consensus
  5. Generate Final Action with emotional tag

- **Design tradeoffs:**
  - GPT-4o baseline vs. fine-tuned LLaMA:前者更强的general reasoning，后者更好的emotional expressiveness for unconsciousness
  - Natural language vs. numeric encoding for needs/states: Natural language provides semantic richness but may introduce ambiguity
  - Multi-turn vs. single-turn reasoning: Multi-turn enables richer internal dialogue but increases latency and cost

- **Failure signatures:**
  - Unconsciousness agent produces overly rational/polite responses (insufficient fine-tuning or safety override)
  - Agents talk past each other without convergence (poor orchestrator routing logic)
  - Final Action contradicts established personality traits (insufficient Fixed State conditioning)
  - Repetitive agent outputs (consensus detection too aggressive or turn limits too permissive)

- **First 3 experiments:**
  1. **Baseline replication:** Run the three-agent system with base GPT-4o for all agents; compare output coherence and emotional depth against paper's reported metrics.
  2. **Ablation on Flexible State:** Test with and without needs/states encoding to validate personalization improvement claims (68.7% → 70.9%).
  3. **Alternative unconsciousness fine-tuning:** Experiment with different LoRA ranks (8, 32) or datasets to characterize the sensitivity of emotional expressiveness to training configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Psychodynamic Model generalize its simulation of consciousness across a broader range of character types and demographic contexts?
- Basis in paper: [explicit] The authors state in the Limitations section that the "application of the model was limited to two character profiles" and that "future work will focus on extending the model to a broader range of character types and contexts."
- Why unresolved: The current study restricted the scope to two profiles to enable "detailed validation of the system's functionality."
- What evidence would resolve it: Successful evaluation of the model's Consciousness Fidelity and Personalization scores across a statistically diverse dataset of character profiles.

### Open Question 2
- Question: How does the Flexible State Component adapt to repeated or overlapping experiences, specifically regarding sensitivity or desensitization over time?
- Basis in paper: [explicit] The authors note that subsequent studies need to "examine how the system adapts to repeated or overlapping experiences" and how "repeated exposure to similar stimuli influences sensitivity or desensitization."
- Why unresolved: The Personalization Module was implemented in a "controlled manner to isolate and evaluate its contribution," rather than testing dynamic, long-term state changes.
- What evidence would resolve it: Longitudinal simulations demonstrating how short-term memory updates modify behavioral responses to identical recurring stimuli.

### Open Question 3
- Question: How does the multi-agent architecture perform in multi-character interactions compared to single-agent internal reasoning?
- Basis in paper: [explicit] The authors identify "multi-character interactions" as a specific area for future investigation and assessment.
- Why unresolved: The current framework focuses exclusively on "Interconscious Reasoning" (internal negotiation) within a single entity.
- What evidence would resolve it: Metrics on social coherence and psychodynamic consistency in dialogue scenarios involving multiple Psychodynamic Model agents.

### Open Question 4
- Question: To what extent does the "LLM-as-a-Judge" evaluation align with assessments from human experts in consciousness studies or psychology?
- Basis in paper: [inferred] The paper relies entirely on GPT-4o for evaluation, arguing that "individuals without specialized knowledge... would be unable to reliably assess" validity. However, it does not validate if the LLM judge agrees with human experts.
- Why unresolved: Validation was performed against a baseline model, not against a "ground truth" provided by human clinical or psychological experts.
- What evidence would resolve it: A correlation study comparing the LLM judge's "Consciousness Fidelity" scores with ratings from a panel of psychologists or consciousness researchers.

## Limitations
- The theoretical foundation linking psychodynamic theory to multi-agent LLM systems remains largely conceptual
- The 71.2% LLM-as-Judge preference represents a modest improvement over baseline models
- Long-term stability of fine-tuned unconsciousness agent outputs and potential safety boundary relaxation are unknown

## Confidence
- **High Confidence**: Multi-agent architecture implementation and Interconscious Reasoning mechanism are well-documented and reproducible
- **Medium Confidence**: Personalization improvements through Fixed and Flexible State components are supported by quantitative results, though effect size is modest
- **Low Confidence**: Theoretical claims about artificial consciousness modeling exceed empirical evidence provided

## Next Checks
1. **Human Expert Validation**: Conduct blind evaluations with psychology and psychoanalysis experts to assess whether multi-agent outputs demonstrate authentic psychodynamic reasoning patterns
2. **Longitudinal Stability Testing**: Implement automated monitoring of fine-tuned unconsciousness agent outputs over extended conversations (1000+ turns) to detect degradation patterns
3. **Cross-Domain Generalization**: Apply the Psychodynamic Model framework to non-dialogue tasks (creative writing, decision-making scenarios, therapeutic simulations) to test consciousness modeling generalization