---
ver: rpa2
title: 'LADY: Linear Attention for Autonomous Driving Efficiency without Transformers'
arxiv_id: '2512.15038'
source_url: https://arxiv.org/abs/2512.15038
tags:
- lady
- driving
- linear
- autonomous
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of transformer-based
  models in end-to-end autonomous driving by introducing LADY, the first fully linear
  attention-based generative model for autonomous driving. LADY employs RWKV-7 for
  efficient multi-frame sensor feature fusion with constant memory and time complexity,
  enabling long-range temporal context integration.
---

# LADY: Linear Attention for Autonomous Driving Efficiency without Transformers

## Quick Facts
- arXiv ID: 2512.15038
- Source URL: https://arxiv.org/abs/2512.15038
- Reference count: 40
- Achieves state-of-the-art planning performance with significantly reduced computational cost on NA VSIM and Bench2Drive benchmarks using fully linear attention architecture

## Executive Summary
This paper introduces LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY addresses the computational inefficiency of transformer-based models by employing RWKV-7 for multi-frame sensor feature fusion with constant memory and time complexity, and a lightweight linear cross-attention mechanism for effective cross-modal interactions. The model achieves state-of-the-art planning performance while maintaining constant inference time and memory usage regardless of input frame count, validated on both simulation benchmarks and edge devices.

## Method Summary
LADY processes multi-frame camera and LiDAR inputs through ResNet-34 backbones, fuses them using RWKV-7 blocks with recurrent temporal state updates, projects to BEV space, and generates multi-modal trajectories via a diffusion decoder with truncated denoising. The model employs Lightweight Linear Cross-Attention (LICA) for cross-modal interactions and maintains constant computational complexity through its linear attention architecture. Training uses 10-frame temporal history with truncated diffusion (50 steps training, 2 steps inference), while inference can handle arbitrary frame counts through sequential state updates.

## Key Results
- Achieves state-of-the-art planning performance on NA VSIM and Bench2Drive benchmarks
- Maintains constant inference time and memory usage regardless of input frame count
- Validated on edge devices, confirming practicality in resource-constrained scenarios
- Scorer achieves PDMS of 90.9, with best-of-N reference evaluation reaching 99.2

## Why This Works (Mechanism)

### Mechanism 1
RWKV-7 enables constant-time, constant-memory multi-frame sensor fusion through recurrent state updates that compress key-value history into a fixed-size state, eliminating dependency on total frame count. The temporal hidden state S_t is computed via sequential update: S_t = S_{t-1}(diag(w_t) - κ̂_t^T(a_t ⊙ κ̂_t)) + v_t^T · k̃_t.

### Mechanism 2
Lightweight Linear Cross-Attention (LICA) achieves cross-modal attention without quadratic cost by encoding queries through RWKV-7, concatenating with BEV features, and processing through sequential RWKV-7 blocks where recurrent updates naturally attend to both feature and query tokens.

### Mechanism 3
Diffusion-based decoder with truncated denoising (50 training steps from 1000, 2 inference steps) generates diverse, high-quality multi-modal trajectories while reducing computational overhead and mitigating mode collapse.

## Foundational Learning

- **Linear Attention and State Space Models**: Understanding the difference between O(T²d) transformer attention and O(Td) linear attention is essential for grasping LADY's core innovation. *Quick check: Can you explain why RWKV-7 achieves O(d) memory at inference while transformers require O(T·d)?*

- **Bird's Eye View (BEV) Feature Representation**: Multi-frame camera and LiDAR features are fused into BEV space for downstream planning; this is the canonical representation for end-to-end driving. *Quick check: Why is BEV preferred over perspective-view features for trajectory planning?*

- **Diffusion Models for Trajectory Generation**: The decoder uses truncated diffusion to generate multi-modal trajectories; understanding the forward/reverse process and truncation tradeoffs is critical. *Quick check: What is the effect of reducing denoising steps from 1000 to 2 at inference?*

## Architecture Onboarding

- **Component map**: Multi-frame sensor input -> ResNet-34 features -> RWKV-7 temporal fusion -> BEV projection -> ego status concatenation -> LICA cross-attention -> diffusion decoder -> scored trajectories

- **Critical path**: Multi-frame sensor input → ResNet-34 features → RWKV-7 temporal fusion → BEV projection → ego status concatenation → LICA cross-attention → diffusion decoder → scored trajectories

- **Design tradeoffs**: Parallel training (all frames at once) vs. sequential inference (frame-by-frame state update) — requires careful state management; Fixed training history (10 frames) but arbitrary inference history — zero-padding handles shorter sequences; Truncated diffusion (2 steps) for speed vs. full diffusion (1000 steps) for quality — empirical validation supports truncation

- **Failure signatures**: State saturation with long sequences containing many dynamic agents; Scorer suboptimal selection (90.9 PDMS vs 99.2 best-of-N); Conservative behavior in single-frame variants at intersections

- **First 3 experiments**:
  1. **Ablate frame count**: Train with 10 frames; evaluate with 1, 4, 6, 8, 10, 12, 15, 20 frames. Expected: performance increases with frame count, plateaus near training length.
  2. **Compare inference scaling**: Measure latency and memory on NVIDIA Orin for LADY vs. Transfuser, DRAMA, DiffusionDrive across 1-20 frames. Expected: LADY constant; others grow quadratically.
  3. **Validate LICA necessity**: Replace LICA with transformer cross-attention; compare PDMS and inference cost. Expected: performance similar but cost increases significantly.

## Open Questions the Paper Calls Out

### Open Question 1
How can the trajectory scoring mechanism be improved to better identify optimal trajectories from multi-modal candidates? The authors note their scorer achieves PDMS of 90.9 but reference evaluation reaches 99.2, indicating the existing scorer fails to accurately identify the optimal trajectory.

### Open Question 2
Would LADY's advantages over transformer-based methods be more pronounced on datasets containing more scenarios requiring long-range temporal context? The authors state modest performance gains are mainly attributed to insufficient samples requiring long-range temporal context in current datasets.

### Open Question 3
How does LADY perform with alternative linear attention architectures (e.g., Mamba-2, RetNet, Gated Linear Attention) compared to RWKV-7? Future work will focus on incorporating alternative linear-attention architectures into the model.

### Open Question 4
How does LADY perform in real-world autonomous driving deployments beyond simulation benchmarks? The authors mention validating LADY in real-world autonomous driving scenarios as future work, with current validation primarily on NAVSIM and Bench2Drive.

## Limitations

- Architectural specifications lack precise details for RWKV-7 depth, hidden dimensions, and LICA block count, preventing exact reproduction
- Validation relies exclusively on simulation benchmarks (NAVSIM, Bench2Drive) without real-world deployment testing
- Complexity quantification gaps exist, with missing detailed runtime benchmarks across varying frame counts and hardware configurations

## Confidence

**High Confidence**: Mechanism 1 - Temporal Fusion (RWKV-7 formulation for constant-memory temporal fusion is well-established with explicit mathematical formulation)
**Medium Confidence**: Mechanism 2 - LICA Cross-Attention (novel sequential attention mechanism lacks comparative analysis against transformer cross-attention)
**Medium Confidence**: Overall Performance Claims (state-of-the-art results reported but without ablation studies isolating component contributions)

## Next Checks

**Validation Check 1**: Implement exact architectural specifications for RWKV-7 and LICA blocks, then conduct ablation studies comparing full LADY, LADY without LICA, LADY with transformer cross-attention, and LADY with full diffusion (1000 steps).

**Validation Check 2**: Scale inference testing across 1-50 frames on target hardware (NVIDIA Orin/Jetson AGX) measuring latency, memory usage, and throughput to verify claimed constant complexity.

**Validation Check 3**: Transfer LADY to an alternative autonomous driving benchmark (e.g., CARLA or nuScenes) with different sensor configurations to assess generalization beyond training domain.