---
ver: rpa2
title: 'MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders'
arxiv_id: '2510.26411'
source_url: https://arxiv.org/abs/2510.26411
tags:
- medical
- interpretability
- neurons
- medclip
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Medical Sparse Autoencoders (MedSAEs) to the
  latent space of MedCLIP, a vision-language model trained on chest radiographs and
  reports. The goal is to improve interpretability in medical AI by extracting clinically
  meaningful and monosemantic features.
---

# MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2510.26411
- Source URL: https://arxiv.org/abs/2510.26411
- Reference count: 0
- Primary result: MedSAE extracts 21 clinically meaningful features from MedCLIP with >70% detection accuracy, achieving lower entropy (2.25 vs 2.38) and higher class selectivity than raw embeddings

## Executive Summary
This paper introduces Medical Sparse Autoencoders (MedSAE) to improve interpretability of MedCLIP, a vision-language model for chest radiographs. By decomposing polysemantic MedCLIP embeddings into monosemantic features through L1-regularized sparse autoencoders, the authors demonstrate significant improvements in feature interpretability. Using correlation metrics, entropy analysis, and automated neuron naming via MedGEMMA, they identify clinically meaningful features that show superior alignment with medical concepts compared to raw MedCLIP representations.

## Method Summary
MedSAE trains a ReLU-based sparse autoencoder on normalized MedCLIP-ResNet embeddings from chest radiographs. The architecture expands the 512-dimensional embedding space to 8192 dimensions while applying L1 regularization to enforce sparsity. Trained on CheXpert with λ=3e-4 and expansion factor 16×, the model selects checkpoints based on minimal neuron entropy while maintaining 0.98 feature variance explained. Evaluation combines Pearson correlation analysis between neuron activations and clinical labels with entropy-based monosemanticity metrics, followed by automated semantic validation using MedGEMMA's detection task on top-activating images.

## Key Results
- MedSAE neurons achieve lower entropy (2.25) compared to raw MedCLIP features (2.38)
- 21 medically meaningful features identified with detection accuracy above 70% via MedGEMMA
- Neurons exhibit higher class selectivity with monosemantic representations compared to polysemantic raw embeddings
- 30% dead neuron rate observed at current hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders decompose polysemantic MedCLIP embeddings into monosemantic features by forcing activation sparsity in an expanded latent space. The ReLU-based SAE encoder projects normalized embeddings into a higher-dimensional space (expansion factor 16×) while L1 regularization constrains active neurons. This incentivizes the network to allocate dedicated neurons to individual concepts rather than superposing multiple concepts per neuron. Lower entropy in neuron-label correlations indicates genuine monosemanticity rather than artefactual sparsity patterns.

### Mechanism 2
Pearson correlation between neuron activations and clinical labels identifies concept-selective neurons that can be validated via entropy analysis. For each neuron-label pair, the correlation coefficient quantifies linear relationship strength. Normalizing absolute correlations into a probability distribution per neuron enables entropy computation—lower entropy means the neuron's activation concentrates on fewer labels, indicating selectivity. Linear correlation adequately captures neuron-concept relationships in the normalized embedding space.

### Mechanism 3
MedGEMMA generates and validates semantic interpretations for SAE neurons through structured prompting and a detection task. Top-activating images for each neuron are presented to MedGEMMA, which generates concept descriptions. Validation occurs via a binary detection task: given the description and a balanced set of activating/non-activating images, MedGEMMA classifies which match the concept. Detection accuracy quantifies semantic alignment. The VLM's detection performance reflects genuine semantic grounding rather than exploiting image artifacts or spurious correlations.

## Foundational Learning

- **Concept: Polysemantic vs. Monosemantic Neurons**
  - Why needed: The core premise is that raw MedCLIP embeddings contain superposed features (one neuron responding to multiple unrelated concepts). Understanding this distinction explains why SAE expansion helps.
  - Quick check: Can you explain why a neuron activating for both "pleural effusion" and "image artifact" would harm interpretability?

- **Concept: L1 Regularization and Sparsity**
  - Why needed: The SAE objective (Equation 2) balances reconstruction fidelity against sparsity. The λ hyperparameter directly controls the trade-off between dead neurons and feature disentanglement.
  - Quick check: What happens to the L0 activation rate if you increase λ by 10×?

- **Concept: Embedding Normalization for Cross-Modal Alignment**
  - Why needed: MedCLIP embeddings require centering and scaling before SAE training to ensure consistent λ effects and convergence stability.
  - Quick check: Why would unnormalized embeddings from different modalities cause training instability?

## Architecture Onboarding

- **Component map:**
  MedCLIP-ResNet (frozen) → 512-dim embeddings → Normalization layer (center + scale) → ReLU-MedSAE encoder → 8192-dim sparse activations → MedSAE decoder → Reconstructed embeddings
  Evaluation branch: Sparse activations → Pearson correlation with labels → Entropy computation
  Interpretation branch: Top-k activating images per neuron → MedGEMMA prompting → Concept description → Detection validation

- **Critical path:**
  1. Extract and normalize MedCLIP embeddings from CheXpert training split
  2. Train MedSAE with λ=3e-4, expansion=16, learning rate=7e-6 for 200 epochs
  3. Select checkpoint with lowest average neuron entropy while maintaining FVE >0.95
  4. Run correlation analysis to identify candidate neurons per clinical concept
  5. Generate MedGEMMA interpretations for top-activating neurons
  6. Validate via detection task on held-out balanced images

- **Design tradeoffs:**
  - Expansion factor (16×): Higher expansion increases disentanglement capacity but raises compute and dead neuron risk
  - L1 coefficient (3e-4): Controls sparsity; paper reports 30% dead neurons at this setting—acceptable given 0.98 FVE
  - Neuron selection: Entropy-based selection prioritizes monosemanticity but may miss distributed representations

- **Failure signatures:**
  - High dead neuron rate (>40%): Reduce λ or increase learning rate
  - Low FVE (<0.90): Increase expansion factor or reduce λ
  - Low detection accuracy (<60%): Check MedGEMMA prompt quality; verify activating images are semantically coherent
  - High entropy across all neurons: Normalization may be incorrect; check embedding statistics

- **First 3 experiments:**
  1. **Baseline replication:** Train MedSAE with paper hyperparameters on CheXpert; verify entropy (2.25) and FVE (0.98) match reported values
  2. **Ablation on expansion factor:** Compare expansion=[8, 16, 32] while holding λ constant; measure entropy, FVE, and dead neuron rate tradeoffs
  3. **Label correlation validation:** For the 21 neurons with >70% detection accuracy, compute per-class ROC-AUC against CheXpert labels to confirm clinical alignment beyond the reported correlation metric

## Open Questions the Paper Calls Out

### Open Question 1
Can more expressive SAE architectures reduce the 30% dead neuron rate observed in MedSAE without compromising the monosemanticity of the extracted features? The study utilized a standard ReLU-based architecture, which is prone to feature dormancy during training, and did not test alternative architectures that might mitigate this. Training MedSAE using alternative architectures (e.g., JumpReLU, Gated SAEs) and comparing the percentage of active neurons against the current 30% baseline while monitoring entropy scores would resolve this.

### Open Question 2
Do the monosemantic features learned by MedSAE from CheXpert generalize to chest radiographs from different institutions or to different imaging modalities? The experiments were restricted to the CheXpert dataset; features highly selective for CheXpert labels may be learning site-specific artifacts or may not transfer to different data distributions like MIMIC-CXR. Zero-shot application of the trained MedSAE to external datasets (e.g., MIMIC-CXR, PadChest) to measure if neuron-class correlations and detection accuracies remain consistent would resolve this.

### Open Question 3
How does the MedGEMMA automated naming pipeline compare to human expert annotation in terms of semantic fidelity and susceptibility to hallucination? While the detection task confirms the internal consistency of the concept, it does not verify if the text descriptions accurately reflect clinical reality or if they omit subtle but critical visual features. A reader study where radiologists blindly rank or correct the MedGEMMA-generated labels for the top-activating images to quantify semantic alignment and hallucination rates would resolve this.

## Limitations
- Limited validation across modalities and institutions; results may not generalize beyond CheXpert dataset
- Presence of inactive neurons (30% dead neuron rate) suggests inefficiencies in current sparse encoding approach
- Clinical utility of extracted features for downstream tasks (diagnosis, treatment planning) remains unvalidated

## Confidence
- **High confidence:** Core mechanism of SAE-based feature decomposition (entropy reduction from 2.38 to 2.25 is directly measurable)
- **Medium confidence:** MedGEMMA-based semantic validation (detection accuracy >70% is reported, but validation methodology details are sparse)
- **Medium confidence:** Clinical interpretability claims (21 meaningful features identified, but clinical expert validation is not detailed)

## Next Checks
1. **Cross-modal validation:** Apply MedSAE to a different medical imaging dataset (e.g., dermatology or radiology CT) to assess generalizability of the extracted features and entropy improvements
2. **Expert clinical validation:** Have board-certified radiologists independently verify the semantic descriptions of the 21 high-accuracy neurons to assess clinical accuracy beyond automated detection
3. **Downstream task evaluation:** Test whether MedSAE-extracted features improve performance on clinically relevant tasks (e.g., disease classification, progression monitoring) compared to raw MedCLIP embeddings