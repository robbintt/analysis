---
ver: rpa2
title: 'The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate
  and Compete'
arxiv_id: '2505.03961'
source_url: https://arxiv.org/abs/2505.03961
tags:
- agents
- maxreward
- noinstruct
- nscarrot
- nsplumber
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether narrative priming can influence
  the behavior of LLM agents in collaborative settings. The researchers use a repeated
  public goods game where agents decide how many tokens to contribute to a shared
  pool, with contributions multiplied and redistributed equally.
---

# The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete

## Quick Facts
- arXiv ID: 2505.03961
- Source URL: https://arxiv.org/abs/2505.03961
- Reference count: 37
- Primary result: Narrative priming significantly influences cooperation levels in LLM agents playing repeated public goods games, with shared stories increasing collaboration by up to 100% compared to control conditions.

## Executive Summary
This study investigates whether narrative priming can influence the behavior of LLM agents in collaborative settings. The researchers use a repeated public goods game where agents decide how many tokens to contribute to a shared pool, with contributions multiplied and redistributed equally. Agents are primed with different stories—some emphasizing teamwork and others focusing on self-interest or serving as controls—to assess how narratives affect cooperation. In homogeneous groups where all agents share the same story, those primed with teamwork-focused narratives achieved significantly higher collaboration scores (up to 0.96) compared to control conditions (around 0.48–0.72). However, when agents were assigned different stories (heterogeneous groups), the effect reversed, and agents primed for self-interest achieved higher payoffs. The study also tested scalability with larger groups and robustness against selfish agents, finding that while collaboration generally declined in larger or adversarial settings, the overall trend persisted. The findings suggest that shared narratives can effectively promote cooperation among LLM agents, but mismatched narratives may undermine it. These results highlight the potential of narrative priming for designing multi-agent systems and AI alignment strategies.

## Method Summary
The study uses a repeated public goods game with N LLM agents (4, 16, or 32) contributing tokens (0-10) to a shared pool over R=5 rounds. The pool is multiplied by m=1.5 and redistributed equally. Agents are primed with one of 12 story prompts (8 cooperation-themed, 4 baselines) injected into their system prompt. The study tests homogeneous groups (all agents receive same story) and heterogeneous groups (agents receive different stories). Collaboration scores are measured as total contributions divided by maximum possible contributions. The experiments use meta-llama-3.1-70b-instruct-fp8 via LangChain with temperature=0.6, running 100 trials per homogeneous condition and 400 trials for heterogeneous conditions.

## Key Results
- Homogeneous groups with cooperation-themed stories achieved collaboration scores up to 0.96, compared to 0.48–0.72 for control conditions
- In heterogeneous groups, self-interested agents achieved higher individual payoffs (~90.87) by exploiting cooperative agents (~63-67)
- Larger group sizes (16-32 agents) showed reduced collaboration but maintained the overall trend of narrative influence
- The effect persisted even when including selfish agents in homogeneous groups, though collaboration scores declined

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Narrative context semantically biases the model's output distribution toward cooperation or defection.
- **Mechanism:** Cooperation-themed stories likely contain vocabulary (e.g., "teamwork," "unity") that activates prosocial token probabilities during generation. This functions as a semantic prime rather than a logical instruction, shifting the "personality" or strategy the model simulates.
- **Core assumption:** The LLM associates narrative themes with behavioral patterns due to correlations in the training data (e.g., stories about teamwork often precede descriptions of cooperative acts).
- **Evidence anchors:** [discussion] The authors note that cooperative narratives "contain teamwork-related words even at a statistical (bag-of-words) level" and may activate related contexts. [results] Agents primed with "Turnip" or "OldManSons" (cooperation-themed) showed collaboration scores up to 0.96, whereas "maxreward" (self-interest) dropped to ~0.48. [corpus] Weak direct support; neighbor papers focus on story generation coherence (e.g., "SCORE") rather than behavioral priming in game theory.

### Mechanism 2
- **Claim:** Shared narratives establish a "common knowledge" state that facilitates reciprocity.
- **Mechanism:** In homogeneous groups, identical primes reduce strategic ambiguity. Agents recognize (via the prompt context) that they are operating under the same "cultural" rules, making cooperative strategies less risky and easier to sustain over repeated rounds.
- **Core assumption:** LLMs can implicitly recognize symmetry in prompting conditions and adjust strategy expectations accordingly.
- **Evidence anchors:** [abstract] "Common stories improve collaboration, benefiting each agent." [results] Homogeneous groups maintained high contributions, whereas heterogeneous groups saw cooperation collapse. [corpus] Weak support; no direct neighbor papers on multi-agent common knowledge processing.

### Mechanism 3
- **Claim:** In mixed-narrative populations, self-interested agents exploit the predictability of cooperative agents.
- **Mechanism:** Cooperation-primed agents adopt a "generous" default strategy. When paired with self-interested agents ("maxreward"), the cooperative agents fail to retaliate sufficiently, allowing the defectors to extract higher individual payoffs from the shared pool.
- **Core assumption:** Cooperative priming reduces the agent's sensitivity to exploitation (low "retaliation" threshold) compared to self-interested priming.
- **Evidence anchors:** [results] In heterogeneous experiments, "maxreward" agents achieved the highest payoffs (~90.87), effectively preying on the lower payoffs of cooperative agents (~63-67). [discussion] The study notes that in heterogeneous settings, "collaboration declines... and selfish agents become more successful."

## Foundational Learning

- **Concept:** Public Goods Game (Game Theory)
  - **Why needed here:** This is the core environment. You must understand the tension between individual rationality (contribute 0) and collective optimality (contribute max) to interpret the collaboration scores.
  - **Quick check question:** In a single round, why is contributing zero the "dominant strategy" for a rational agent, even though everyone loses if everyone does it?

- **Concept:** Context/Priming in LLMs
  - **Why needed here:** The paper manipulates behavior not by changing the rules, but by changing the *story* preceding the rules. You need to distinguish between logical instruction and semantic association.
  - **Quick check question:** Does the "maxreward" prompt change the game's payoff formula, or does it change the agent's internal prioritization of that formula?

- **Concept:** Homogeneous vs. Heterogeneous Multi-Agent Systems
  - **Why needed here:** The efficacy of the intervention flips entirely based on group composition. A strategy that works for identical agents fails in a diverse population.
  - **Quick check question:** Why does a "cooperative" agent perform worse in a heterogeneous group than in a homogeneous group, even if it plays the exact same strategy?

## Architecture Onboarding

- **Component map:** Agent Core (LLaMA 3.1 70B) -> Priming Module (story injection) -> Game Engine (pool calculation) -> Memory (round-by-round feedback)
- **Critical path:** 1. Initialization: Inject story + game rules into agent prompt. 2. Action: Agent generates an integer contribution t ∈ [0, 10]. 3. Update: Engine calculates pool, redistributes rewards, and feeds summary back to agents. 4. Iteration: Repeat for R=5 rounds.
- **Design tradeoffs:** Temperature (0.6 vs 1.0): Lower temp (0.6) used for consistency; higher temps increase variance but also differentiation between priming effects. Story Selection: Stories were "ad hoc" and not controlled for emotional valence, potentially confounding semantic cooperation with narrative quality.
- **Failure signatures:** Format deviation: Agents failing to output an integer token. Strategic collapse: In heterogeneous groups, total pool contributions dropping near zero as agents adapt to defectors.
- **First 3 experiments:** 1. Baseline Validation: Run homogeneous groups with "noinstruct" vs. "maxreward" to verify the model understands the game logic. 2. Semantic vs. Explicit: Compare a subtle cooperative story vs. an explicit instruction "You must cooperate" to see if narrative offers a different magnitude of effect. 3. Robustness Check: Introduce a "dummy" agent (hard-coded to contribute 0) into a homogeneous cooperative group to see how quickly the LLM agents abandon their narrative priming in the face of exploitation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the competitive advantage of self-interested agents persist in heterogeneous groups where all agents are primed with meaningful, collaboration-focused stories, rather than a mix of meaningful and baseline stories?
- **Basis in paper:** [explicit] Section 3.2 states, "Testing whether this effect persists in a heterogeneous population where all agents are primed with meaningful, collaboration-focused stories remains an avenue for future work."
- **Why unresolved:** The current heterogeneous experiment (Exp. 2) randomly assigned stories from a pool containing both meaningful narratives and baseline controls (e.g., nonsense or self-interest), but did not test groups consisting solely of different pro-cooperation narratives.
- **What evidence would resolve it:** Running heterogeneous experiments where every agent receives a distinct narrative that all share a strong cooperation theme, and analyzing if individual payoffs equalize or if competition still emerges.

### Open Question 2
- **Question:** What are the causal mechanisms by which narrative priming alters agent behavior, and to what extent is this dependent on RLHF training?
- **Basis in paper:** [explicit] Section 4 notes, "The causal mechanisms underlying this phenomenon are still unclear," and lists "understanding causal mechanisms (e.g., via mechanistic interpretability)" and "Comparative cross-model analysis (non-RLHF variants)" as future work.
- **Why unresolved:** It is currently unknown if cooperation stems from the activation of specific semantically related contexts in the training data or from the model's alignment training (RLHF) to follow instruction-like narratives.
- **What evidence would resolve it:** Mechanistic interpretability analysis (e.g., examining attention heads) to trace how narratives modify value representations, coupled with experiments on base models without RLHF.

### Open Question 3
- **Question:** Do the effects of narrative priming decay over extended repeated games, and can malicious adversarial narratives destabilize established cooperation?
- **Basis in paper:** [explicit] Section 6 lists future work to "evaluate whether priming effects decay over repeated games" and to assess if "priming with malicious narratives destabilizes multi-agent systems."
- **Why unresolved:** The study limited games to R=5 rounds, which prevents observation of long-term strategy stability or the erosion of priming effects as the game context begins to outweigh the initial prompt.
- **What evidence would resolve it:** Experiments extending the number of rounds significantly (e.g., 20+ rounds) to observe score trends over time, and the introduction of agents specifically primed with adversarial, destructive narratives into cooperative groups.

## Limitations
- Story selection appears ad hoc without systematic control for emotional valence or narrative quality, making it unclear whether effects stem from semantic priming or simply more engaging stories.
- The homogeneous setting assumes agents can reliably recognize shared priming conditions—a capability not directly tested.
- Lack of formal mechanism for agents to adapt strategies based on observed opponent behavior limits ecological validity of heterogeneous group results.

## Confidence
- **High Confidence:** The basic observation that narrative context influences LLM agent behavior in public goods games. This is directly observable and robust across multiple experimental conditions.
- **Medium Confidence:** The interpretation that semantic priming (not just explicit instruction) drives cooperation differences. While supported by the contrast between story types, the lack of controlled narrative manipulation introduces uncertainty.
- **Low Confidence:** The claim that shared narratives create "common knowledge" enabling sustained cooperation. This mechanism is inferred rather than directly measured, and alternative explanations (like story quality or length) cannot be ruled out.

## Next Checks
1. **Narrative Control Experiment:** Replace the 8 cooperative stories with semantically neutral stories of identical length and emotional tone to isolate whether semantic content or narrative quality drives effects.
2. **Transparency Manipulation:** In heterogeneous groups, explicitly tell agents what priming their partners received to test whether the breakdown of cooperation depends on agents' awareness of narrative mismatch.
3. **Adaptation Mechanism Test:** Implement a simple reputation system where agents can track and respond to previous contributions, then re-run heterogeneous experiments to see if cooperative agents can defend against exploitation.