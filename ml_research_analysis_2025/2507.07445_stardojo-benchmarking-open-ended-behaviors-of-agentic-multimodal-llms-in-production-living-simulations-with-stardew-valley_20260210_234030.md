---
ver: rpa2
title: 'StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in
  Production-Living Simulations with Stardew Valley'
arxiv_id: '2507.07445'
source_url: https://arxiv.org/abs/2507.07445
tags:
- save
- agents
- tasks
- game
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces StarDojo, a benchmark for evaluating agentic\
  \ multimodal large language models in open-ended production-living simulations using\
  \ Stardew Valley. StarDojo presents 1,000 tasks across five domains\u2014farming,\
  \ crafting, exploration, combat, and social interactions\u2014requiring agents to\
  \ manage both productive activities and social engagement."
---

# StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley

## Quick Facts
- arXiv ID: 2507.07445
- Source URL: https://arxiv.org/abs/2507.07445
- Reference count: 40
- Primary result: GPT-4.1 achieves only 12.7% success rate on StarDojo-Lite subset of 100 tasks

## Executive Summary
StarDojo introduces a comprehensive benchmark for evaluating agentic multimodal large language models in open-ended production-living simulations using Stardew Valley. The benchmark presents 1,000 tasks across farming, crafting, exploration, combat, and social domains, requiring agents to balance productive activities with social engagement. Evaluations of state-of-the-art MLLMs reveal substantial limitations, with the best-performing model (GPT-4.1) achieving only 12.7% success on the 100-task Lite subset. The work highlights the gap between current MLLM capabilities and the demands of complex simulations, aiming to drive research toward robust, adaptive agents in production-living environments.

## Method Summary
StarDojo evaluates MLLM agents on 1,000 tasks in Stardew Valley, using a unified Python interface with parallel execution across operating systems. Agents receive multimodal observations (720P screenshots plus structured textual game state) and output actions from a 10-skill action space. The benchmark includes a configurable pause-and-resume mechanism during model inference, enabling deliberate planning. Tasks range from easy (30 max steps) to hard (150 max steps) across five domains. Evaluation uses incremental state comparison to track progress, with agents tested zero-shot using ReAct-style prompting.

## Key Results
- GPT-4.1 achieves 12.7% success rate on StarDojo-Lite subset, with 0% success on hard tasks
- All models show near-zero success on medium and hard difficulty tasks requiring extended action sequences
- Multimodal observations (text + image) outperform either modality alone, particularly for navigation and spatial reasoning tasks
- Real-time execution without pausing during inference causes significant performance degradation due to target movement and time progression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining textual game state with visual observations improves MLLM agent performance over either modality alone.
- Mechanism: Textual observations provide precise tile-level spatial coordinates and object properties that ground visual interpretation. Agents use visual input for coarse navigation until targets appear in textual surroundings, then switch to textual descriptions for precise interaction.
- Core assumption: MLLMs struggle with pixel-level spatial reasoning but can reliably parse structured text representations.
- Evidence anchors:
  - [abstract] "failures primarily due to challenges in visual understanding, multimodal reasoning"
  - [section 4.2] "removing textual input (Image Only) significantly affects agents' performance across all tasks... eliminating visual input (Text Only) substantially reduces success in tasks that require navigation"
  - [corpus] No direct corpus evidence for this specific mechanism; related benchmarks focus on different modalities.
- Break condition: If an agent can learn robust visual-spatial representations without textual scaffolding, this multimodal advantage would diminish.

### Mechanism 2
- Claim: The pause-and-resume game control mechanism enables deliberate planning by decoupling inference time from game time.
- Mechanism: The StarDojoMod directly modifies internal game states to pause execution during model inference (which can exceed 10 seconds for GPT-4.1 API calls), then resumes before action execution. This prevents the game state from drifting during reasoning.
- Core assumption: Real-time reactivity is not the primary capability being evaluated; strategic decision-making is.
- Evidence anchors:
  - [section 3.2] "implemented a configurable pause-and-resume mechanism by directly modifying the inner states of the game"
  - [section 4.2] "Disabling the feature to pause the environment (Real-time) remarkably affects performance... the target (bug) continues moving during model inference"
  - [corpus] No corpus papers examine this specific temporal decoupling mechanism.
- Break condition: If deployment requires real-time interaction, the evaluated capabilities would not transfer.

### Mechanism 3
- Claim: Incremental observation comparison enables scalable, automated evaluation across heterogeneous tasks.
- Mechanism: The evaluator maintains the previous game state, compares it with the current state after each action, detects task-relevant changes (e.g., items harvested, NPCs contacted), and accumulates progress toward completion thresholds.
- Core assumption: Task completion can be determined from observable state changes rather than requiring process verification.
- Evidence anchors:
  - [section 3.4] "the evaluator maintains the previous game state information, compares it with the current state... identifies incremental changes indicating progress"
  - [Appendix C.5] Algorithm 1 shows the incremental comparison procedure
  - [corpus] Corpus papers reference benchmarking infrastructure but not this specific incremental evaluation pattern.
- Break condition: If tasks require verifying the correctness of intermediate reasoning rather than just final outcomes, this mechanism would be insufficient.

## Foundational Learning

- Concept: **Production-living systems** (simultaneous resource-generating activities and social exchanges)
  - Why needed here: StarDojo uniquely evaluates agents on balancing productive output, resource management, and social integration within a unified environment, unlike prior benchmarks that isolate these skills.
  - Quick check question: Can you explain why a farming simulation game is a better proxy for real-world "production-living" than a pure combat or puzzle game?

- Concept: **Action space abstraction** (mapping complex human inputs to discrete callable functions)
  - Why needed here: The paper abstracts keyboard/mouse inputs into 10 fundamental actions (move, use, interact, craft, etc.) that preserve decision-making challenges while eliminating redundant operations.
  - Quick check question: What is the trade-off between using `navigate(name)` (high-level shortcut) versus explicit `move(x,y)` calls for evaluating exploration ability?

- Concept: **State-grounded evaluation** (comparing consecutive observations rather than checking fixed end states)
  - Why needed here: The benchmark's 1,000 tasks have varying initial conditions; incremental comparison accommodates this variability while providing step-level feedback.
  - Quick check question: Why would comparing only the final game state against a gold standard be problematic for this benchmark?

## Architecture Onboarding

- Component map: Stardew Valley (C#) -> SMAPI (C#) -> StarDojoMod (socket server + shared memory, C#) -> Python Wrapper -> Task Manager -> Evaluator -> Agent

- Critical path: Agent → Python Wrapper → Socket Server (port-specific) → StarDojoMod → Game Engine → Observation → Python Wrapper → Agent. Evaluator runs in parallel after each action.

- Design tradeoffs:
  - **Text-only vs. multimodal observations**: Text provides precision; vision enables spatial reasoning. Default uses both.
  - **Paused vs. real-time**: Paused enables deliberate planning; real-time tests reactivity. Default is paused.
  - **Navigate action inclusion**: Excluded in experiments to evaluate realistic exploration, but available for other research questions.

- Failure signatures:
  - **Near-zero success on medium/hard tasks**: Indicates long-horizon planning failure; check if agent is reaching step limits without reaching distant targets.
  - **Repeated interaction attempts on inaccessible objects**: Indicates visual-textual inconsistency; check if textual surroundings clearly show the object is not adjacent.
  - **Combat failures in real-time mode**: Target moves during inference; expected behavior when pause is disabled.

- First 3 experiments:
  1. Run GPT-4.1 on 5 easy farming tasks with `image_obs=True` and full textual observations to establish baseline; expect 20-30% success per Table 3.
  2. Ablate to `image_obs=False` (text only) on the same tasks; expect navigation-related tasks to degrade significantly per Figure 4.
  3. Run one medium combat task (e.g., "kill_5_green_slime") in both paused and real-time modes; expect real-time mode to fail due to target movement during inference latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agents achieve robust performance in real-time (non-paused) environments where inference latency directly impacts task success, as opposed to turn-based settings?
- Basis in paper: [explicit] The ablation study states that disabling the pause feature "remarkably affects performance across tasks demanding timely reactions or prolonged action sequences," with combat targets moving during inference and in-game time advancing, sometimes causing "completion into the night or spanning multiple in-game days."
- Why unresolved: Current MLLM inference times (over 10 seconds for GPT-4.1 via API) are incompatible with dynamic environments where targets move and time progresses, yet most benchmarks use turn-based settings.
- What evidence would resolve it: Development of agents that maintain comparable success rates between paused and real-time conditions, or architectural innovations that reduce inference latency to sub-second levels.

### Open Question 2
- Question: How can multimodal agents be improved to achieve consistent reasoning between visual observations and structured textual state information?
- Basis in paper: [explicit] The paper reports "consistent discrepancies between image-based reasoning and textual observations even in advanced models," such as interpreting a tree "visually located several tiles away" as immediately accessible despite textual observations "clearly indicating an empty tile directly above the character."
- Why unresolved: Advanced models like GPT-4.1 and Claude 3.7 Sonnet still exhibit these inconsistencies, suggesting fundamental challenges in cross-modal grounding.
- What evidence would resolve it: Agents that demonstrably weight textual and visual cues appropriately, achieving higher success rates on tasks requiring precise spatial reasoning.

### Open Question 3
- Question: What architectural or training improvements are needed for agents to successfully complete medium and hard difficulty tasks requiring extended action sequences (50-150 steps)?
- Basis in paper: [explicit] The paper states that "all evaluated models show near-zero success rates on medium and hard tasks" and that "efficient and precise long-distance navigation remains the primary bottleneck," with agents unable to "autonomously navigate complex scenarios, such as returning home or sleeping on time."
- Why unresolved: Even the best-performing model (GPT-4.1 at 12.7% overall) achieves 0% on hard tasks across all categories, indicating current MLLMs lack the planning depth for multi-step reasoning.
- What evidence would resolve it: Agents achieving non-trivial success rates (e.g., >20%) on medium/hard tasks in StarDojo-Lite or completion of the extended Playthrough task (accumulating 1 million in-game currency).

### Open Question 4
- Question: How can agent performance on production-living benchmarks like StarDojo be improved through specialized visual understanding of stylized game art styles?
- Basis in paper: [explicit] The paper notes that "the most common failures come from agents' inability to accurately locate and approach target objects in visual observations" and that models "struggle significantly with accurately identifying crucial visual elements, such as character locations, entrances, trees, NPCs, and crops," with recent work highlighting "difficulties that current MLLMs encounter when interpreting Stardew Valley's distinctive art style."
- Why unresolved: State-of-the-art MLLMs trained primarily on natural images and photographs fail to generalize to stylized pixel art, limiting their applicability to game environments and simulations.
- What evidence would resolve it: Fine-tuned or specially adapted models demonstrating improved object detection and spatial reasoning metrics on stylized visual domains without relying on textual observations.

## Limitations
- Cannot distinguish whether failures stem from fundamental MLLM limitations versus suboptimal prompting or incomplete action-space coverage
- Does not systematically analyze failure modes or compare against human baselines to establish whether performance gaps reflect fundamental limitations
- Tests zero-shot performance without investigating potential improvements through prompt engineering, action space augmentation, or model adaptation

## Confidence
- **High Confidence**: The benchmark infrastructure is functional and the reported success rates are reproducible with the provided configuration. The observation that current MLLMs struggle with open-ended production-living tasks is well-supported by the experimental results.
- **Medium Confidence**: The claim that multimodal observations outperform unimodal approaches is supported by the ablation study, but the paper does not explore which specific modalities are most critical for different task types or whether alternative observation representations could yield better performance.
- **Low Confidence**: The assertion that failures are "primarily due to challenges in visual understanding, multimodal reasoning, and low-level manipulation" is plausible but not directly tested. The paper does not systematically analyze failure modes or compare against human baselines to establish whether the performance gap reflects fundamental limitations or implementation details.

## Next Checks
1. **Failure Mode Analysis**: Instrument the evaluation pipeline to log and categorize failures by type (visual confusion, reasoning errors, action execution failures) across all task categories to identify whether specific failure modes dominate and whether they vary by task difficulty or domain.

2. **Human Baseline Evaluation**: Have human participants complete a subset of StarDojo-Lite tasks using the same observation interface to establish whether the performance gap reflects MLLM limitations versus environmental challenges, and to identify which aspects of the tasks are most difficult for human players.

3. **Alternative Observation Formats**: Evaluate the impact of different observation representations (e.g., JSON vs. natural language text, different image resolutions, alternative spatial encodings) on agent performance to determine whether the current multimodal approach is optimal or whether simpler representations could achieve comparable results.