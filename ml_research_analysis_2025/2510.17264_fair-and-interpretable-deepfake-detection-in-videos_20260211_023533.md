---
ver: rpa2
title: Fair and Interpretable Deepfake Detection in Videos
arxiv_id: '2510.17264'
source_url: https://arxiv.org/abs/2510.17264
tags:
- deepfake
- detection
- concept
- data
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses fairness and interpretability challenges in
  deepfake detection, where existing methods exhibit demographic bias, lack transparency,
  and fail to capture temporal information. The proposed solution integrates temporal
  feature learning with a novel frequency-aware data augmentation method that operates
  in the low-frequency domain while preserving high-frequency deepfake artifacts.
---

# Fair and Interpretable Deepfake Detection in Videos

## Quick Facts
- arXiv ID: 2510.17264
- Source URL: https://arxiv.org/abs/2510.17264
- Authors: Akihito Yoshii; Ryosuke Sonoda; Ramya Srinivasan
- Reference count: 40
- Primary result: Proposes frequency-aware data augmentation and temporal clustering to achieve fair and interpretable deepfake detection while maintaining high accuracy

## Executive Summary
This paper addresses the critical challenges of fairness and interpretability in deepfake detection systems. The authors identify that existing detection methods exhibit demographic bias and lack transparency, while also failing to capture temporal information crucial for video analysis. They propose an integrated solution that combines temporal feature learning with a novel frequency-aware data augmentation method. The approach operates by selectively preserving high-frequency deepfake artifacts while mitigating bias-inducing low-frequency features, using sequence-based clustering to model temporal changes and concept extraction for interpretable explanations.

## Method Summary
The proposed method integrates temporal feature learning with frequency-aware data augmentation to address fairness and interpretability in deepfake detection. It uses sequence-based clustering that incorporates temporal difference vectors to model temporal changes, combined with concept extraction via linear classifiers to provide interpretable explanations. The frequency-aware augmentation operates in the low-frequency domain while preserving high-frequency deepfake artifacts, forcing the model to learn artifact-based features rather than demographic shortcuts. The system generates synthetic concept banks using Stable Diffusion to enable bias monitoring without explicit demographic labels, measuring variance of concept influences across different data clusters to infer bias.

## Key Results
- Achieves best tradeoff between fairness and accuracy compared to state-of-the-art techniques
- Demonstrates consistent improvements across multiple demographic groups while maintaining high detection performance
- Shows significant gains in Equalized Odds ($F_{EO}$) fairness metric
- Validated on FaceForensics++, DFDC, Celeb-DF, and DFD datasets using Xception and ResNet architectures

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Augmentation
The method selectively preserves deepfake artifacts while disrupting demographic correlates by operating in the frequency domain. Deepfake artifacts reside primarily in high-frequency components, whereas spurious demographic correlations (e.g., race or gender linked to "fake" labels) reside in low-frequency components. By applying a low-pass filter to isolate and mix low-frequency regions between image pairs while leaving high-frequency residuals intact, the model learns artifact-based features rather than demographic shortcuts.

### Mechanism 2: Temporal Difference Vectors in Clustering
Standard clustering groups features by spatial similarity, but the authors enhance this by concatenating a "temporal difference" vector (cosine distance between frame $t$ and $t-1$) with spatial features before clustering. This creates clusters sensitive to temporal instability, exposing time-variant spurious correlations. If a concept is highly correlated with "fake" labels in high-temporal-variance clusters but not low-variance ones, it is flagged as a spurious correlation.

### Mechanism 3: Synthetic Concept Banks for Bias Monitoring
Instead of requiring expensive labeled demographic attributes, the system uses a "Concept Bank" generated via Stable Diffusion prompts like "face with glasses." It trains linear classifiers (Concept Activation Vectors) for these concepts and measures the variance of these concepts' influence across different data clusters. High variance implies the model relies on that concept inconsistently, indicating a spurious correlation (bias).

## Foundational Learning

- **Fourier Transforms (FFT) in Vision**: Essential for understanding how the augmentation separates high-freq (artifacts) from low-freq (bias/demographics). *Quick check*: If you apply a low-pass filter to an image and subtract it from the original, what does the residual represent?

- **Spurious Correlation & Shortcut Learning**: Critical for grasping why the model might "cheat" by using easy demographic shortcuts rather than complex forgery features. *Quick check*: Why might a deepfake detector achieve 99% accuracy on training but fail on new data if the training set had gender imbalance?

- **Concept Activation Vectors (CAVs)**: Key to understanding the interpretability module, where linear directions in feature space correspond to human concepts like "baldness" or "skin tone." *Quick check*: How do you train a linear classifier to define a "concept vector" using concept-positive images and random counter-examples?

## Architecture Onboarding

- **Component map**: Input video frames → Feature Extractor (Xception/ResNet) → Embeddings → Temporal Module (cosine sim between frames) → Clustering (PCA/UMAP + Δ) → Concept Bank (Stable Diffusion) → Bias Scoring (CSS) → Sampling Weights → Augmentation Engine (FFT mix) → Output Classifier + Heatmaps + CSS

- **Critical path**: The Bias-aware Sampling (Eq. 11) driving the Frequency Mixing (Eq. 12). If sampling weights ($W$) are calculated incorrectly, the augmentation will mix random pairs rather than targeted debiasing pairs, neutralizing the fairness gain.

- **Design tradeoffs**: 
  - Accuracy vs. Fairness: Explicitly trades minor accuracy drops for significant gains in Equalized Odds
  - Concept Granularity: Larger Concept Bank increases computational overhead but improves interpretability
  - Frequency Threshold ($\alpha$): Set at 3/4; lowering preserves more low-freq info (risking bias), raising might destroy structural context

- **Failure signatures**:
  - NaN Loss: Check FFT implementation; mixing frequency domains can cause numerical instability
  - Stagnant Fairness ($F_{EO}$): If CSS values remain low/uniform, clustering may have failed
  - Visual Artifacts: If augmented images look "ghostly," the frequency mask or low-pass filter may be too aggressive

- **First 3 experiments**:
  1. Visualize "Low Frequency" vs "High Frequency" decomposition of a sample frame to verify deepfake artifacts are in high-freq residual
  2. Run clustering pipeline on validation set and visualize clusters colored by "Real/Fake" and "Demographic"
  3. Sweep frequency threshold $\alpha$ (e.g., 0.5, 0.75, 0.9) on small subset to measure sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the method's performance degrade when forgery artifacts are primarily located in low-frequency components rather than the high-frequency domain? The conclusion explicitly identifies reliance on the assumption that deepfake-specific artifacts are predominantly present in the high-frequency domain as a limitation.

### Open Question 2
Does the framework maintain its balance of fairness and accuracy when integrated with modern transformer-based architectures? The authors state they will investigate generalization capabilities beyond CNNs to SoTA classifiers.

### Open Question 3
How can the concept extraction module be adapted for deepfake detection in non-face contexts where demographic attributes are undefined? The authors note plans to extend the method to detect deepfakes across non-face datasets.

## Limitations

- The frequency-based augmentation mechanism assumes deepfake artifacts predominantly reside in high-frequency components, which needs empirical validation across different deepfake generation methods
- The temporal clustering approach relies on cosine similarity of frame embeddings, but optimal temporal window size and frame sampling rate are not specified
- Claims of achieving "best tradeoff" are limited by relatively small number of baseline methods evaluated (6 total)

## Confidence

- **High confidence**: Mathematical formulation of frequency-domain augmentation and experimental methodology are well-defined and reproducible
- **Medium confidence**: CSS-based bias detection mechanism is theoretically sound, though sensitivity to different concept bank compositions requires further validation
- **Low confidence**: Claims about achieving best tradeoff compared to state-of-the-art are limited by evaluation scope

## Next Checks

1. **Frequency domain validation**: Analyze actual frequency distribution of artifacts across different deepfake generation methods to verify high-frequency artifact assumption

2. **Concept bank sensitivity**: Test bias detection performance using different concept bank sizes and compositions to assess robustness to concept selection

3. **Temporal window analysis**: Evaluate sensitivity of temporal clustering mechanism to different frame sampling rates and temporal window sizes across various video lengths