---
ver: rpa2
title: "Towards a Principled Muon under $\u03BC\\mathsf{P}$: Ensuring Spectral Conditions\
  \ throughout Training"
arxiv_id: '2601.01306'
source_url: https://arxiv.org/abs/2601.01306
tags:
- spectral
- arxiv
- muon
- training
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of ensuring spectral conditions\
  \ for the Muon optimizer under the maximal-update parameterization (\u03BCP) framework\
  \ throughout the entire training process. The authors propose Muon++, a variant\
  \ of Muon that maintains these conditions without explicit spectral normalization\
  \ of model weights, thereby reducing computational overhead."
---

# Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training

## Quick Facts
- arXiv ID: 2601.01306
- Source URL: https://arxiv.org/abs/2601.01306
- Reference count: 11
- Primary result: Proposes Muon++ that maintains spectral conditions without explicit weight normalization through gradient projection onto orthogonal subspace of top singular vectors

## Executive Summary
This work addresses a critical limitation in the Muon optimizer under maximal-update parameterization (μP) where spectral conditions can break down during late-stage training, particularly for moderately large models. The authors introduce Muon++, which preserves these conditions by projecting gradients onto the orthogonal subspace of the top singular vectors of weight matrices, eliminating the need for explicit spectral normalization of weights. The key insight is that controlling spectral norms at the optimizer update level is sufficient for maintaining μP-compatible scaling. Experimental results demonstrate that Muon++ reliably satisfies the required spectral conditions across the entire training horizon, bridging the theoretical-practical gap for matrix-based optimizers in large language model training.

## Method Summary
Muon++ modifies the standard Muon optimizer by introducing a gradient projection step that maintains spectral conditions without explicit weight normalization. The method projects gradients onto the orthogonal subspace of the top singular vectors of the weight matrix, effectively controlling the spectral norm through optimizer updates rather than weight constraints. This approach reduces computational overhead compared to explicit spectral normalization while preserving the theoretical guarantees needed for μP scaling. The projection is designed to work with moderately large models where the spectral gap remains non-vanishing, though the authors acknowledge limitations for ultra-large models where singular values may cluster.

## Key Results
- Muon++ maintains spectral conditions throughout training without explicit weight spectral normalization
- The gradient projection approach reduces computational overhead compared to weight-level spectral normalization
- Spectral conditions are reliably satisfied across the training horizon for moderately large models
- The method bridges the gap between theoretical μP guarantees and practical deployment of matrix-based optimizers

## Why This Works (Mechanism)
The mechanism relies on controlling the spectral norm at the optimizer update level rather than constraining weights directly. By projecting gradients onto the orthogonal subspace of top singular vectors, the update direction inherently respects the spectral condition requirements. This approach exploits the fact that under μP, the effective scaling behavior depends on how gradients propagate through the network, not just on the static weight norms. The projection ensures that updates don't violate the spectral constraints while maintaining sufficient degrees of freedom for effective optimization.

## Foundational Learning
- **Maximal-update parameterization (μP)**: A scaling framework where width and depth scale jointly to maintain optimal learning dynamics; needed to understand the theoretical context and why spectral conditions matter.
- **Spectral normalization**: A technique to constrain the spectral norm of weight matrices; important as the baseline approach that Muon++ aims to avoid.
- **Singular value decomposition (SVD)**: Used to identify top singular vectors for gradient projection; fundamental to the core algorithmic mechanism.
- **Gradient projection**: A method to constrain optimization updates to specific subspaces; central to how Muon++ maintains conditions.
- **Orthogonal subspace projection**: The specific mathematical operation used to preserve spectral constraints while allowing optimization; the key technical innovation.

## Architecture Onboarding
- **Component map**: Weight matrices -> Singular value decomposition -> Gradient projection -> Optimizer update
- **Critical path**: Input → Weight matrix → Singular value computation → Gradient projection → Update application → Output
- **Design tradeoffs**: Reduced computational overhead vs. increased algorithmic complexity; theoretical guarantees vs. practical scalability limitations
- **Failure signatures**: Degraded training stability when singular values cluster; breakdown of spectral conditions in ultra-large models; increased overhead from SVD computation
- **First experiments**: 1) Verify spectral condition satisfaction across training epochs, 2) Compare training stability with and without projection, 3) Measure computational overhead vs explicit spectral normalization

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the correlation between activation vectors and weight matrices (Issue 2) be modeled and mitigated to maintain spectral conditions during late-stage training?
- Basis in paper: Section 5.1 states that a "neat and sound mitigation of Item Issue 2 is left as a future work," and Section 4.4 notes it is "hard to model it in a principled way."
- Why unresolved: The paper identifies that as training progresses, the activation vector $h_{\ell-1}$ and weight $W_{\ell}$ correlate in complex ways that break standard spectral condition derivations.
- What evidence would resolve it: A principled framework or update rule that accounts for data-dependent activation-weight correlations without relying on the independence assumptions used in the current Muon++ derivation.

### Open Question 2
- Question: Can the minimalist $\rho$-correlated weight model be expanded into a comprehensive framework for estimating internal weight correlations?
- Basis in paper: Section 5.1 explicitly lists the limitation that "the modeling framework in Section 4.4 for alleviating Item Issue 1 is far from comprehensive."
- Why unresolved: The current proposal uses a simple moment estimator for a scalar correlation $\rho$, which may not capture the full complexity of dependencies arising in long-horizon training.
- What evidence would resolve it: A robust estimation method (beyond method of moments) or a richer correlation model that successfully adapts the spectral condition dynamically in diverse training scenarios.

### Open Question 3
- Question: Is it possible to guarantee spectral conditions for Muon under $\mu$P in ultra-large models without resorting to explicit spectral normalization?
- Basis in paper: Section 4.3 discusses the failure of the projection approach when $\sigma_1 \approx \sigma_2$, and Section 5.1 notes, "For Muon++, we still need explicit spectral normalization for super large-width cases in Algorithm 2."
- Why unresolved: The proposed projection method (Algorithm 1) relies on a non-vanishing spectral gap ($\sigma_1 - \sigma_2$), a condition that theoretically fails as width approaches infinity (Fact A.2).
- What evidence would resolve it: A theoretical guarantee or modified projection operator that maintains the spectral norm constraint $S$ even when the singular values of the weight matrix degenerate or cluster.

## Limitations
- Limited validation scope restricted to moderately large models, with no testing on extremely large-scale transformer architectures
- Computational overhead from gradient projection scales with the number of singular vectors computed
- Experimental focus on spectral condition satisfaction rather than downstream task performance comparisons
- Theoretical guarantees break down when singular values cluster in ultra-large models

## Confidence
- **High confidence**: The mathematical formulation of spectral condition preservation through optimizer updates is sound and follows logically from established μP framework principles
- **Medium confidence**: The experimental demonstration that Muon++ maintains spectral conditions across training is well-supported, though limited to moderate model sizes
- **Medium confidence**: The claim about reduced computational overhead compared to explicit weight spectral normalization is plausible but requires broader empirical validation across different model architectures

## Next Checks
1. Evaluate Muon++ on extremely large transformer models (>10B parameters) to verify spectral condition preservation at scale and measure any degradation in training stability or convergence speed
2. Conduct ablation studies comparing downstream task performance (e.g., language modeling loss, perplexity) between Muon++ and standard Muon across multiple model sizes to quantify practical benefits beyond spectral condition satisfaction
3. Benchmark the computational overhead of the gradient projection step across different weight matrix dimensions to establish scaling behavior and identify potential bottlenecks for very large models