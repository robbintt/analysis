---
ver: rpa2
title: 'LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services'
arxiv_id: '2512.07436'
source_url: https://arxiv.org/abs/2512.07436
tags:
- search
- arxiv
- data
- across
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LocalSearchBench, the first comprehensive
  benchmark for agentic search in local life services, featuring over 1.3M merchant
  entries across 6 service categories and 9 cities, with 900 multi-hop QA tasks derived
  from real user queries. The benchmark includes LocalPlayground, a unified evaluation
  environment integrating LocalRAG and web search tools for LLM interaction.
---

# LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services
## Quick Facts
- arXiv ID: 2512.07436
- Source URL: https://arxiv.org/abs/2512.07436
- Reference count: 40
- First comprehensive benchmark for agentic search in local life services with 1.3M+ merchant entries and 900 multi-hop QA tasks

## Executive Summary
LocalSearchBench introduces the first comprehensive benchmark for agentic search in local life services, featuring over 1.3M merchant entries across 6 service categories and 9 cities, with 900 multi-hop QA tasks derived from real user queries. The benchmark includes LocalPlayground, a unified evaluation environment integrating LocalRAG and web search tools for LLM interaction. Experiments demonstrate that even state-of-the-art models struggle on this domain-specific benchmark, with the best model (DeepSeek-V3.2) achieving only 35.60% correctness, while average completeness reaches 60.32% and faithfulness 30.72%. The results highlight the need for specialized benchmarks and domain-specific agent training in local life services.

## Method Summary
The benchmark constructs a comprehensive evaluation framework for agentic search in local life services by first building a knowledge base from Baidu Map API covering 1.3M+ merchants across 6 service categories in 9 cities. The QA dataset consists of 900 multi-hop questions collected from Baidu Know, designed to test complex reasoning over local service information. The LocalPlayground evaluation environment provides a unified interface with LocalRAG for database queries and web search capabilities for external information retrieval. Models interact with this environment through function calling, generating search queries, processing results, and producing final answers. Evaluation employs three metrics: correctness (answer accuracy against ground truth), completeness (whether all query aspects are addressed), and faithfulness (faithfulness to provided evidence).

## Key Results
- DeepSeek-V3.2 achieves only 35.60% correctness on the benchmark
- Average completeness across models is 60.32%
- Average faithfulness across models is 30.72%
- Even state-of-the-art models struggle significantly on domain-specific multi-hop queries

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on real-world multi-hop reasoning requirements in local services, where users need to synthesize information from multiple sources to make decisions. The integration of both structured database queries (LocalRAG) and unstructured web search mirrors actual user behavior when researching local services. The three-metric evaluation system (correctness, completeness, faithfulness) provides comprehensive assessment of both factual accuracy and reasoning quality. The use of real user queries from Baidu Know ensures ecological validity and captures authentic information needs in local service contexts.

## Foundational Learning
**LocalRAG Systems**: Why needed - enables efficient retrieval from structured merchant databases; Quick check - verify database indexing and query response times
**Multi-hop Reasoning**: Why needed - local service queries often require synthesizing information from multiple sources; Quick check - test with increasingly complex query chains
**Function Calling**: Why needed - allows LLMs to interact with external tools systematically; Quick check - validate function call accuracy and parameter passing
**Ground Truth Extraction**: Why needed - provides reliable answer verification for evaluation; Quick check - cross-validate with multiple annotators
**Domain-specific Evaluation**: Why needed - general benchmarks don't capture local service domain challenges; Quick check - compare with general QA benchmarks

## Architecture Onboarding
Component map: User Query -> LocalRAG/→ Web Search → Result Processing → Answer Generation → Evaluation Metrics
Critical path: Query reception → Tool selection (LocalRAG vs Web Search) → Information retrieval → Response synthesis → Metric calculation
Design tradeoffs: LocalRAG provides fast, structured access but limited scope vs web search offers broader coverage but noisier results
Failure signatures: Incorrect merchant information, missed query aspects, hallucination of non-existent services, over-reliance on single information source
First experiments:
1. Test single-hop queries to establish baseline performance
2. Compare LocalRAG-only vs web search-only performance
3. Evaluate model behavior on queries spanning multiple service categories

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general need for specialized benchmarks and domain-specific agent training in local life services.

## Limitations
- Benchmark covers only 6 service categories and 9 cities, limiting generalizability
- Evaluation relies heavily on RAG-based ground truth extraction which may introduce systematic errors
- Results may not reflect deployment scenarios in regions outside China due to Chinese web search integration

## Confidence
High: Performance metrics showing state-of-the-art models struggling are directly measurable and reproducible
Medium: Claim of being first comprehensive benchmark lacks comparative analysis with existing local service benchmarks
Medium: Need for specialized benchmarks extrapolates from results without extensive ablation studies

## Next Checks
1. Conduct cross-city and cross-category generalization studies to assess performance patterns in other local service domains
2. Perform ablation studies on RAG-based ground truth extraction to quantify impact on correctness measurements
3. Test benchmark with models using different search APIs and latency conditions to understand retrieval quality impact