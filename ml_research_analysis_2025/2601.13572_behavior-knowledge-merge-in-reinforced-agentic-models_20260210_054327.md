---
ver: rpa2
title: Behavior Knowledge Merge in Reinforced Agentic Models
arxiv_id: '2601.13572'
source_url: https://arxiv.org/abs/2601.13572
tags:
- task
- merging
- agents
- memory
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a critical limitation in applying standard\
  \ model merging techniques to RL-trained agentic models: the mismatch between dense\
  \ SFT updates and sparse, heterogeneous RL task vectors causes signal dilution,\
  \ degrading task-specific capabilities. To address this, the authors propose Reinforced\
  \ Agent Merging (RAM), a distribution-aware method that disentangles shared and\
  \ unique parameter updates\u2014averaging shared regions while preserving and rescaling\
  \ unique ones to counteract dilution."
---

# Behavior Knowledge Merge in Reinforced Agentic Models

## Quick Facts
- arXiv ID: 2601.13572
- Source URL: https://arxiv.org/abs/2601.13572
- Reference count: 40
- This paper proposes RAM, a distribution-aware merging method for RL-trained agentic models that outperforms baselines by disentangling shared and unique parameter updates.

## Executive Summary
This paper identifies a critical limitation in applying standard model merging techniques to RL-trained agentic models: the mismatch between dense SFT updates and sparse, heterogeneous RL task vectors causes signal dilution, degrading task-specific capabilities. To address this, the authors propose Reinforced Agent Merging (RAM), a distribution-aware method that disentangles shared and unique parameter updates—averaging shared regions while preserving and rescaling unique ones to counteract dilution. Experiments across coding, tool-use, and memory domains with Qwen and Llama architectures show that RAM consistently outperforms existing merging baselines, achieving state-of-the-art results and even surpassing the original specialized agents in many tasks. RAM also demonstrates superior robustness and efficiency compared to prior methods.

## Method Summary
RAM addresses the signal dilution problem in merging RL-trained agents by first computing task vectors (fine-tuned minus base model parameters) for each agent. It then probes the parameter distribution using a threshold to identify active parameters and counts overlaps across tasks. Parameters are partitioned into shared (updated by ≥2 tasks) and unique (updated by exactly 1 task) subsets. Shared parameters are averaged across tasks, while unique parameters are preserved at full magnitude and rescaled proportionally to each task's shared-to-unique ratio to compensate for averaging-induced contraction. The final merged model is assembled by adding the modified task vector to the base model. Key hyperparameters include threshold ϵ=10^-5, rescaling strength r=0.1, and clip bound α=2.0.

## Key Results
- RAM achieves 64.82 average score vs 63.33 for best baseline DARE+TA on Qwen-based agents
- RAM+ with rescaling surpasses specialized agents on 9/12 tasks, including beating coding specialist on LiveBench (40.23 vs 37.70)
- RAM maintains or improves instruction-following capabilities while TIES/DARE baselines cause significant degradation
- RAM demonstrates superior efficiency, completing in 75.4s vs >400s for TIES/DARE variants

## Why This Works (Mechanism)

### Mechanism 1
When standard model merging methods apply global averaging to RL-trained agents, task-specific unique parameter updates are divided by N, causing signal dilution that degrades specialized capabilities. On-policy RL induces highly sparse and heterogeneous task vectors that often modify non-overlapping parameter regions across tasks. Standard merging (e.g., Task Arithmetic's τ_merged = 1/N Σ τ_i) averages all parameters uniformly. For a unique parameter updated only by task t, averaging with N-1 zero-valued updates scales its magnitude to 1/N without any balancing benefit, since unique regions hardly interfere with out-of-domain tasks. Core assumption: RL task vectors are sparse because on-policy optimization targets narrow behaviors via task-specific reward signals, unlike SFT which updates global parameters.

### Mechanism 2
RAM preserves specialized behavior knowledge by disentangling shared and unique parameter regions via distribution statistics, then applying differentiated merging strategies to each. RAM computes binary masks for each task vector using threshold ϵ (10^-5), then counts overlap per parameter: c = Σ m_t where c_i indicates how many agents update parameter i. Parameters are partitioned into Shared (c_i ≥ 2) and Unique (c_i = 1) subsets. Shared parameters are averaged to balance multi-task consensus; unique parameters are preserved at full magnitude with task-specific rescaling (λ_t) to compensate for signal contraction in shared regions. Core assumption: The binary threshold ϵ = 10^-5 accurately distinguishes meaningful parameter changes from noise, and the shared/unique partition captures functional differences in knowledge encoding.

### Mechanism 3
Distribution-aware rescaling (λ_t = 1 + r·clip(ρ_t, 0, α)) amplifies unique parameter regions proportionally to each task's shared-to-unique ratio, recovering functional equivalence to pre-merge performance. RAM calculates Overlap-Unique Ratio ρ_t = (shared params)/(unique params) for each task. Tasks with higher ρ_t have more shared parameters that suffer averaging-induced contraction. Under functional equivalence hypothesis (∆f̂_t ≈ ∆f_t), the rescaling factor λ_t ≈ 1 + r·ρ_t compensates for this contraction. Clipping bounds α prevents numerical instability for extreme ρ_t values. Core assumption: Parameter importance is approximately isotropic (g_i·τ_t,i ≈ const on average), allowing ratio of parameter counts to approximate ratio of functional contributions.

## Foundational Learning

- **Task Vectors (τ = θ_fine-tuned - θ_pretrained)**: Why needed: RAM operates entirely on task vectors—understanding that these represent learned behavioral knowledge differences from the base model is prerequisite to grasping why dilution matters and how disentanglement works. Quick check: Given a base model θ_pre and fine-tuned model θ_task, can you compute the task vector and explain what it represents functionally?

- **RL vs SFT Training Dynamics**: Why needed: The paper's core problem stems from RL producing sparse, heterogeneous updates while SFT produces dense, redundant ones. Understanding this difference explains why standard merging assumptions fail for RL agents. Quick check: Why would on-policy RL with task-specific reward signals produce sparser parameter updates than supervised fine-tuning on broad datasets?

- **Parameter Sparsity and Overlap Analysis**: Why needed: RAM's disentanglement requires computing which parameters are active (non-zero) for each task and counting overlaps. Without understanding sparsity metrics and overlap statistics, the probing and selective merging mechanisms are opaque. Quick check: Given three task vectors with 3.2%, 46.2%, and 54.3% density respectively, what challenges arise when applying a uniform trimming threshold (as in TIES-Merging)?

## Architecture Onboarding

- **Component map**: Probing Module -> Rescaling Module -> Selective Merge Module -> Final Model Assembly

- **Critical path**: 
  1. Load all N RL-trained agent checkpoints and base model checkpoint
  2. Compute task vectors τ_t = θ_t - θ_pre for all agents
  3. Probe distribution: compute masks, overlap counts, partition parameters
  4. Calculate rescaling factors λ_t based on ρ_t
  5. Apply selective merge rule to construct τ_merged
  6. Assemble final model and validate on held-out benchmarks

- **Design tradeoffs**:
  - Threshold choice (ϵ=10^-5): Too low includes noise as "active" parameters, inflating overlap; too high misses small but meaningful updates. PyTorch default used for gradient checking provides reasonable baseline.
  - Rescaling strength (r): Higher r better compensates for dilution but risks over-amplification. Ablation shows r=0.10 optimal; r=0.20 causes degradation.
  - Clip bound (α=2.0): Prevents extreme rescaling for tasks with very high overlap ratios, trading theoretical optimality for numerical stability.
  - Efficiency vs complexity: RAM (without rescaling) completes in 75.4s vs >400s for TIES/DARE variants, achieving Pareto-optimal efficiency-performance tradeoff.

- **Failure signatures**:
  1. Catastrophic forgetting on general tasks: If RAM severely degrades instruction-following, check IFEval scores. Paper shows RAM maintains or improves on base model (Qwen: +1.44 Loose Instruction Accuracy), while TIES/DARE collapse on smaller models (-11.15 drop).
  2. In-domain performance regression: If specialized capabilities drop after merging, verify unique regions are being preserved (not averaged). Check that |T_i| = 1 case applies rescaling, not averaging.
  3. Numerical instability: If merged model produces NaN/Inf outputs, check for extreme ρ_t values exceeding α, or verify rescaling implementation doesn't compound across iterations.
  4. No improvement over baselines: If RAM ≈ Task Arithmetic performance, verify threshold ϵ is correctly filtering near-zero updates and that unique regions are being identified (c_i = 1 should be non-trivial fraction for sparse agents).

- **First 3 experiments**:
  1. Baseline comparison on single architecture: Merge Qwen2.5-7B-based agents (CURE, ToolRL, MemAgent) using RAM, Task Arithmetic, TIES, DARE+TA. Evaluate on 12 tasks across coding/tool/memory domains. Expected: RAM > all baselines (paper shows 64.82 vs 63.33 best baseline).
  2. Ablation on rescaling factor: Test RAM with r ∈ {0.00, 0.05, 0.10, 0.15, 0.20} to validate peak at r=0.10 and degradation at extremes. Monitor both average score and per-domain stability.
  3. Architecture generalization test: Apply RAM to Llama3.2-3B-based agents (Math, Tool, Search) to verify mechanism transfers across architectures and scales. Expected: RAM outperforms baselines and shows positive synergy (merged > specialist on some tasks).

## Open Questions the Paper Calls Out
None

## Limitations
- The core sparsity claim for RL-trained agents is only indirectly supported—while coding agent sparsity is measured (3.2%), tool and memory agent sparsity values are cited from unpublished arXiv sources without detailed methodology.
- The functional equivalence assumption (∆f̂_t ≈ ∆f_t) underpinning the rescaling derivation lacks direct empirical validation.
- The binary thresholding approach assumes ϵ = 10^-5 perfectly separates signal from noise, but sensitivity to this threshold is not thoroughly explored.

## Confidence

**Mechanism 1**: High - Directly measured with coding agent; mechanism is intuitive and well-explained.
**Mechanism 2**: Medium - Implementation details are clear, but the partitioning relies on threshold choice not extensively validated.
**Mechanism 3**: Low-Medium - Derivation is logical but assumes isotropy without direct evidence; ablation shows some support but doesn't validate functional equivalence hypothesis.

## Next Checks
1. **Threshold sensitivity analysis**: Systematically vary ϵ (10^-4 to 10^-6) and measure impact on overlap distribution, unique parameter preservation, and final performance to determine if 10^-5 is optimal or arbitrary.
2. **Functional contribution verification**: For parameters in unique regions, measure their individual contribution to task performance (e.g., via ablation or reparameterization) to validate the isotropy assumption underlying the rescaling mechanism.
3. **Dense RL training comparison**: Train an RL agent with explicit parameter regularization to produce dense updates, then test if RAM still outperforms standard merging, directly validating that sparsity is the root cause of signal dilution.