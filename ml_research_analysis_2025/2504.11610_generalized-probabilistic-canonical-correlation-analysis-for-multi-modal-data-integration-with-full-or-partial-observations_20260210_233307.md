---
ver: rpa2
title: Generalized probabilistic canonical correlation analysis for multi-modal data
  integration with full or partial observations
arxiv_id: '2504.11610'
source_url: https://arxiv.org/abs/2504.11610
tags:
- data
- gpcca
- missing
- modalities
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPCCA extends probabilistic CCA to multiple modalities and missing
  data by embedding missingness into the probabilistic model and estimating parameters
  with an EM algorithm. It incorporates ridge regularization to stabilize covariance
  estimation in high dimensions.
---

# Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations

## Quick Facts
- **arXiv ID:** 2504.11610
- **Source URL:** https://arxiv.org/abs/2504.11610
- **Reference count:** 38
- **Key outcome:** GPCCA outperforms PPCA, MOFA, SNF, and NEMO in clustering accuracy for multi-modal data with missing values.

## Executive Summary
GPCCA extends probabilistic canonical correlation analysis (CCA) to multiple modalities and handles missing data by embedding missingness into the probabilistic model. The method uses an EM algorithm with ridge regularization to stabilize covariance estimation in high dimensions. Simulations and real-world applications show GPCCA yields more stable and accurate clusters than existing methods, even with modality-wise missingness. The method is implemented as an R package.

## Method Summary
GPCCA generalizes probabilistic CCA to R modalities, modeling each modality as a linear transformation of shared latent factors plus modality-specific noise: X^(r) = W^(r)Z + μ^(r) + E^(r). The model handles missing values by treating them as latent variables. Parameters are estimated using an EM algorithm with ridge regularization on the error correlation matrix to prevent instability in high dimensions. The latent dimension is selected by maximizing clustering stability across multiple random initializations. Default ridge parameter λ is 1/2, and clustering is performed using the Louvain algorithm.

## Key Results
- GPCCA achieves higher clustering accuracy (ARI) than PPCA, MOFA, SNF, and NEMO across various missingness patterns and correlation structures
- Applied to multi-view image and TCGA multi-omics datasets, GPCCA produces more stable and accurate clusters than alternatives
- GPCCA identifies cancer-associated genes in TCGA data while handling modality-wise missingness

## Why This Works (Mechanism)
GPCCA works by explicitly modeling the shared latent structure across modalities while accounting for missing data through probabilistic embedding. The ridge regularization stabilizes covariance estimation in high dimensions where sample sizes may be smaller than feature counts. The EM algorithm iteratively estimates missing values and model parameters, converging to a solution that maximizes the likelihood of observed data. The consensus score-based dimension selection ensures the model captures meaningful shared structure without overfitting.

## Foundational Learning
- **EM algorithm:** Needed to handle missing data by iteratively estimating missing values and parameters. Quick check: Verify convergence of log-likelihood across iterations.
- **Ridge regularization:** Stabilizes covariance matrix inversion when feature dimensions approach or exceed sample size. Quick check: Monitor condition number of estimated covariance matrices.
- **Consensus score:** Measures clustering stability across multiple random initializations to select optimal latent dimension. Quick check: Plot consensus score vs. candidate dimensions to identify peak.
- **Probabilistic CCA:** Extends standard CCA by modeling data generation process with noise and latent factors. Quick check: Compare recovered latent factors to ground truth in simulations.
- **Missing data mechanisms:** MCAR (missing completely at random) vs MNAR (missing not at random) affect how missingness should be modeled. Quick check: Verify missingness patterns in real datasets match assumed mechanisms.

## Architecture Onboarding

### Component Map
Data matrices (X^(r)) -> GPCCA model (EM algorithm with ridge regularization) -> Latent factors (Z) -> Louvain clustering -> ARI score

### Critical Path
EM initialization -> E-step (impute missing values) -> M-step (update parameters with ridge regularization) -> Convergence check -> Latent factor extraction -> Clustering evaluation

### Design Tradeoffs
- **Ridge parameter λ:** Higher values increase stability but may oversmooth correlations; default 1/2 balances this
- **Latent dimension selection:** Maximizing consensus score prevents overfitting but increases computational cost through multiple initializations
- **Clustering resolution:** Louvain resolution parameter affects cluster granularity; 0.8 provides reasonable balance

### Failure Signatures
- Slow or non-convergence of EM algorithm indicates poor initialization or inappropriate λ value
- Unstable latent factors across random starts suggest insufficient signal or too many latent dimensions
- Low ARI scores despite convergence may indicate model misspecification for the data structure

### First Experiments
1. Run GPCCA on simulated data with known latent structure to verify recovery accuracy
2. Test sensitivity to ridge parameter λ by comparing results across λ ∈ {1/2, 2/3, 0.8}
3. Evaluate clustering stability by comparing ARI scores across different random initializations

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability is a concern for high-dimensional modalities due to matrix inversions in the M-step
- Method's ability to handle MNAR missingness is asserted but not thoroughly validated
- Missing methodological details in preprocessing and initialization for real-world applications

## Confidence
- Simulated data results: High confidence (controlled conditions, clear benchmarks)
- Real-world applications (TCGA): Medium confidence (missing methodological details in preprocessing and initialization)
- Computational scalability: Low confidence (limited testing on high-dimensional data)

## Next Checks
1. Re-run simulations with varying sample sizes and feature dimensions to assess scalability and robustness of the EM algorithm
2. Replicate clustering results on the "Multiple Features" dataset or other multi-view benchmarks using the provided R package
3. Validate initialization sensitivity by testing different random seeds and comparing consistency in latent embeddings and ARI scores