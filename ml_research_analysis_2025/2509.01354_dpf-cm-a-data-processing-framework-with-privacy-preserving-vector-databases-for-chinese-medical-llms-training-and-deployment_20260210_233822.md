---
ver: rpa2
title: 'DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases
  for Chinese Medical LLMs Training and Deployment'
arxiv_id: '2509.01354'
source_url: https://arxiv.org/abs/2509.01354
tags:
- data
- medical
- training
- evaluation
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DPF-CM introduces a holistic data processing framework for Chinese
  medical LLMs that addresses the lack of attention to data processing in existing
  pipelines. The framework includes two core modules: a data processing pipeline for
  model training and a privacy-preserving vector database approach for deployment.'
---

# DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment

## Quick Facts
- arXiv ID: 2509.01354
- Source URL: https://arxiv.org/abs/2509.01354
- Reference count: 23
- Key outcome: DPF-CM improves Chinese medical LLM accuracy and reduces training data privacy leakage by 27%

## Executive Summary
DPF-CM introduces a holistic data processing framework for Chinese medical LLMs that addresses the lack of attention to data processing in existing pipelines. The framework includes two core modules: a data processing pipeline for model training and a privacy-preserving vector database approach for deployment. For training, it employs chained examples context-learning to generate high-quality, question-oriented instructions and an ensemble-based filtering mechanism to denoise preference data. For privacy, it uses model memory search, high-risk database construction, secure database construction, and match-and-replace to minimize privacy leakage. Experimental results show that DPF-CM significantly improves model accuracy, achieving state-of-the-art performance among open-source Chinese medical LLMs, and reduces training data privacy leakage by 27%.

## Method Summary
DPF-CM processes Chinese medical LLM training data through three stages: continued pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO). The framework uses Minhash-LSH deduplication with threshold 0.8, LLM-based quality scoring, and chained examples context-learning for instruction generation. For DPO training, it employs an ensemble of five reward models to filter noisy preference data. For deployment, the privacy-preserving vector database (PPVD) mechanism identifies memorized outputs via ROUGE-L similarity matching and replaces them with secure responses from a general LLM. The entire pipeline is implemented using LLaMAFactory on 24 A800-80G GPUs with bf16 precision.

## Key Results
- Achieves state-of-the-art performance among 7B open-source Chinese medical LLMs
- Reduces training data privacy leakage by 27% through the PPVD mechanism
- Improves instruction quality through chained examples context-learning for medical Q&A data

## Why This Works (Mechanism)

### Mechanism 1: Chained Examples Context-Learning for Instruction Generation
Structuring few-shot examples as a quality-ordered chain with explicit refinement prompts improves instruction generation for medical Q&A data. Examples are scored by an LLM, ranked from lowest to highest quality (E1 < E2 < ... < EN), and concatenated with interleaved instruction prompts. This encourages the generating model to reason about why later examples improve on earlier ones, rather than treating examples as independent demonstrations. The core assumption is that the generating LLM can infer and apply a progressive optimization pattern from chained examples.

### Mechanism 2: Ensemble-Based Preference Data Denoising via Average Preference Distance
Filtering preference pairs by the average preference distance across multiple reward models removes noisy samples (contradictory or indistinguishable pairs) and improves DPO training. Five reward models are trained with different random seeds on the preference dataset. For each pair (chosen vs. rejected), the preference distance Dis_i = (1/N) Σ(rθ_k(x, y_c) − rθ_k(x, y_r)) is computed. Samples with distances near zero (indistinguishable) or excessively large (trivially different) are excluded. The core assumption is that noisy annotations produce inconsistent reward model outputs.

### Mechanism 3: Privacy-Preserving Vector Database (PPVD) for Inference-Time Memorization Mitigation
Detecting and replacing model outputs that closely match memorized training data reduces privacy leakage with minimal impact on response quality. Training samples are split; the first half prompts the model, and ROUGE-L similarity to the second half identifies memorized (high-risk) samples. Their embeddings populate a high-risk vector database. A secure database stores alternative responses from a general LLM. During inference, cosine similarity between user prompts and high-risk entries triggers replacement with secure responses. The core assumption is that memorization is detectable via partial prompting.

## Foundational Learning

- **Concept: Three-Stage LLM Training (Continued Pre-Training → SFT → DPO/RLHF)**
  - Why needed here: DPF-CM optimizes data processing separately for each stage; understanding the role of each stage is essential to apply the correct processing technique.
  - Quick check question: Given a raw medical dialogue corpus, which stage-specific processing steps would you apply before SFT vs. before DPO?

- **Concept: Reward Models and Preference Distance**
  - Why needed here: The ensemble denoising mechanism relies on interpreting reward model outputs and preference distances; misunderstanding these leads to incorrect thresholding.
  - Quick check question: If a preference pair has a near-zero average preference distance across five reward models, what does this imply about the pair's quality for DPO training?

- **Concept: Vector Database Search and Cosine Similarity**
  - Why needed here: PPVD depends on embedding-based similarity matching at inference time; errors in embedding extraction or similarity calculation break privacy protection.
  - Quick check question: How would you validate that the high-risk and secure vector databases are aligned so that replacement responses are contextually appropriate?

## Architecture Onboarding

- **Component map:** Data cleaning → Deduplication → Quality selection/optimization → Chained instruction generation → Preference data creation → Ensemble denoising → Stage-wise training (CPT/SFT/DPO) → Memory search → High-risk DB construction → Secure DB construction → Inference-time match-and-replace

- **Critical path:**
  1. Verify continued pre-training data cleaning thresholds (repetition, perplexity)
  2. Validate chained-example instruction generation on a held-out medical Q&A split
  3. Train multiple reward models; confirm preference distance distributions before filtering
  4. Execute memory search to identify high-risk samples; validate ROUGE-L threshold calibration
  5. Deploy PPVD with A/B testing for response quality and leakage metrics

- **Design tradeoffs:**
  - Privacy vs. Utility: Aggressive high-risk thresholds reduce leakage but may over-replace clinically relevant responses
  - Generation Scale vs. Quality: LLM-generated pre-training data expands coverage but risks factual errors per Section 3.1
  - Ensemble Size vs. Cost: More reward models improve denoising robustness but increase compute overhead

- **Failure signatures:**
  - Instruction generation produces generic or off-domain instructions (likely seed scoring failure)
  - Preference distances cluster near zero across all pairs (reward model undertraining or data imbalance)
  - Inference responses become evasive or irrelevant (secure DB misalignment or threshold too low)
  - High false-positive rate in memory search (ROUGE-L threshold missetting for Chinese text)

- **First 3 experiments:**
  1. **Instruction Generation Ablation:** Compare chained-example instructions vs. standard few-shot on a 500-sample medical Q&A validation set; measure GPT-4 win rate and semantic similarity
  2. **Preference Denoising Threshold Sweep:** Train DPO models with varying preference distance thresholds (exclude top/bottom 5%, 10%, 20%); evaluate on single-turn and multi-turn dialogue benchmarks
  3. **PPVD Leakage and Quality Tradeoff:** Vary the high-risk similarity threshold (0.75, 0.85, 0.95); measure reduction in memorization similarity and change in response quality via AI and human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can lightweight feature representations effectively replace high-dimensional embeddings in the PPVD to mitigate storage costs without degrading privacy protection?
- Basis in paper: [explicit] The authors state in the Limitations section that substantial private data leads to significant storage demands for the High-Risk Database.
- Why unresolved: The current implementation relies on storing full embeddings, creating a scalability bottleneck for large datasets.
- What evidence would resolve it: Experiments comparing storage size versus privacy leakage rates using compressed or hashed feature representations.

### Open Question 2
- Question: What domain-specific methods can ensure the accuracy and ethical safety of generated medical pre-training data beyond using general-purpose LLMs?
- Basis in paper: [explicit] The paper notes that general LLMs used for data generation may produce inaccurate or ethically inappropriate content due to limited medical knowledge.
- Why unresolved: The current framework utilizes general models (Qwen2.5-72B) for generation, inheriting their potential for medical hallucinations.
- What evidence would resolve it: A comparative analysis of medical-specific generation models versus general models regarding hallucination rates and safety compliance.

### Open Question 3
- Question: How resilient is the PPVD "match-and-replace" mechanism against adversarial prompt engineering designed to bypass vector similarity detection?
- Basis in paper: [inferred] The privacy evaluation relies on ROUGE-L and cosine similarity metrics using specific prompt splits, potentially missing active attack vectors.
- Why unresolved: The paper does not test against adversarial attacks that might paraphrase sensitive inputs to lower similarity scores below the threshold.
- What evidence would resolve it: Penetration testing results using adversarial examples that intentionally attempt to evade the high-risk database matching.

## Limitations
- The privacy protection mechanism lacks detailed evaluation of false-positive rates during inference
- Chained example generation effectiveness depends heavily on initial seed example quality
- Ensemble denoising assumes reward models capture diverse perspectives, but correlation may limit effectiveness

## Confidence
- **High Confidence:** Experimental results showing 27% reduction in privacy leakage and state-of-the-art performance on Chinese medical benchmarks
- **Medium Confidence:** Effectiveness of chained examples context-learning and ensemble-based preference denoising mechanisms, as core assumptions are reasonable but not extensively validated
- **Low Confidence:** Real-world deployment scenarios where medical professionals might encounter over-replacement of legitimate responses or where the secure database responses might lack domain specificity

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary the ROUGE-L threshold (0.75, 0.85, 0.95) and preference distance filtering percentages (5%, 10%, 20%) to quantify tradeoffs between privacy protection and response quality degradation
2. **Secure Database Quality Validation:** Evaluate the semantic alignment between high-risk samples and their secure replacements using both automated metrics (BERTScore, ROUGE-L) and human medical experts to ensure clinical adequacy
3. **Chained Example Robustness Test:** Create controlled experiments with varying seed example qualities to measure how instruction generation quality degrades when initial examples are noisy or contradictory