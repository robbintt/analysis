---
ver: rpa2
title: 'Quake: Adaptive Indexing for Vector Search'
arxiv_id: '2506.03437'
source_url: https://arxiv.org/abs/2506.03437
tags:
- query
- partitions
- recall
- search
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quake is an adaptive vector search index that maintains low latency
  and high recall under dynamic, skewed workloads. It employs a cost-driven multi-level
  partitioning scheme that splits or merges partitions based on access patterns and
  sizes, uses Adaptive Partition Scanning (APS) to dynamically adjust the number of
  partitions scanned per query, and utilizes NUMA-aware parallelism to maximize memory
  bandwidth.
---

# Quake: Adaptive Indexing for Vector Search

## Quick Facts
- arXiv ID: 2506.03437
- Source URL: https://arxiv.org/abs/2506.03437
- Reference count: 40
- Primary result: 1.5-38× lower query latency, 4.5-126× lower update latency vs. state-of-the-art

## Executive Summary
Quake introduces an adaptive vector search index that dynamically maintains low latency and high recall under changing, skewed workloads. Unlike static partitioning methods or expensive graph-based indexes, Quake uses a cost-driven multi-level partitioning scheme with Adaptive Partition Scanning (APS) to minimize query overhead while meeting recall targets. It achieves significant performance gains over HNSW, DiskANN, SVS, and SCANN on real-world Wikipedia and synthetic datasets while maintaining 90% recall targets without offline tuning.

## Method Summary
Quake extends the IVF (Inverted File Index) paradigm with a dynamic, multi-level partition hierarchy. It employs a cost model that predicts query latency based on partition sizes and access frequencies, triggering adaptive split/merge operations to optimize for skewed workloads. APS estimates recall probability using geometric intersection volumes, scanning only the minimum number of partitions needed to meet recall targets. NUMA-aware parallelism distributes partition scanning across memory nodes to maximize bandwidth utilization. The system matches oracle-level nprobe efficiency without requiring offline parameter tuning.

## Key Results
- Achieves 1.5-38× lower query latency compared to state-of-the-art methods
- Reduces update latency by 4.5-126× while maintaining 90% recall targets
- Matches oracle-level nprobe efficiency without offline tuning across diverse workloads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cost-model guided partition maintenance reduces query latency under skewed workloads by targeting high-impact partitions.
- Mechanism: A cost model estimates per-partition latency contribution as $C_{lj} = A_{lj} \cdot \lambda(s_{lj})$ (access frequency × scan latency). Partitions exceeding a cost threshold trigger split/merge actions, with a verify-then-commit guard ($\Delta C < -\tau$) preventing harmful restructuring.
- Core assumption: Access frequencies and partition sizes are the primary predictors of query latency; future access patterns resemble recent sliding-window observations.
- Evidence anchors:
  - [abstract] "guided by a cost model that predicts query latency based on partition sizes and access frequencies"
  - [section 4.1] Eq. (1)-(2) define cost model; [section 4.2.3] describes commit threshold guard
  - [corpus] Limited direct validation; HONEYBEE addresses dynamic partitioning for access control but not cost-model optimization
- Break condition: If access patterns shift rapidly (faster than maintenance interval) or $\lambda(s)$ becomes non-monotonic due to hardware effects, the cost model may mispredict benefits.

### Mechanism 2
- Claim: Adaptive Partition Scanning (APS) meets recall targets with minimal partitions scanned by estimating recall probability from geometric intersection volumes.
- Mechanism: Approximates partition-query intersection as hyperspherical caps using the regularized incomplete beta function. Probability $p_i$ that partition $P_i$ contains a neighbor is updated as query radius $\rho$ shrinks; scanning terminates when cumulative probability $\sum p_i \geq \tau_R$.
- Core assumption: Uniform density within partitions; half-space approximation of Voronoi boundaries is sufficiently accurate.
- Evidence anchors:
  - [section 5] Eq. (7)-(9) define geometric recall estimation; Algorithm 1 details APS
  - [section 7.6] Table 5 shows APS achieves 17-29% latency overhead vs. oracle without tuning
  - [corpus] Auncel (cited in paper) uses similar geometric approach but is conservative; no corpus papers validate APS's specific beta-function approximation
- Break condition: High-dimensional spaces where volume calculations become unstable, or highly non-uniform data distributions within partitions.

### Mechanism 3
- Claim: NUMA-aware intra-query parallelism saturates memory bandwidth, closing the performance gap with graph indexes.
- Mechanism: Partitions are distributed round-robin across NUMA nodes. Worker threads scan only locally-placed partitions; main thread coordinates partial results and triggers early termination when recall estimate meets target.
- Core assumption: Query processing is memory-bandwidth-bound; partition-level work can be cleanly divided without coordination overhead dominating.
- Evidence anchors:
  - [section 6] Algorithm 2 describes NUMA-aware execution with APS integration
  - [section 7.5] Figure 6 shows 4× improvement over non-NUMA at 64 workers, achieving 200GBps throughput
  - [corpus] No corpus papers specifically validate NUMA-aware vector search; general NUMA principles from relational DBs (cited as [17, 35]) are assumed transferable
- Break condition: Small datasets where memory bandwidth isn't the bottleneck, or highly skewed access patterns that cause NUMA node imbalances.

## Foundational Learning

- **Partitioned (IVF) vs. Graph-based ANN indexes**
  - Why needed here: Quake extends IVF-style partitioning; understanding baseline trade-offs (graph = fast reads, slow updates; partitioned = fast updates, slow reads) is essential.
  - Quick check question: Why does Faiss-IVF degrade under write skew while HNSW degrades under frequent inserts?

- **Recall@k metric**
  - Why needed here: APS optimizes for recall targets; cost model trades off recall vs. latency.
  - Quick check question: If $\tau_R = 0.9$ and APS scans partitions until estimated recall = 0.9, what happens if the geometric model underestimates $p_i$?

- **k-means clustering and centroid drift**
  - Why needed here: Quake uses k-means for partition creation and refinement; drift occurs when data distribution shifts away from initial centroids.
  - Quick check question: Why does Quake's refinement step run k-means on neighboring partitions after a split?

## Architecture Onboarding

- **Component map**: Query → Centroid search → Candidate partitions → APS probability estimation → NUMA-distributed scanning → Partial result merge → Recall check → Terminate
- **Critical path**: Query arrives → Identify candidate partitions via centroid search → APS estimates $p_i$ per partition → Workers scan local partitions in parallel → Main thread merges partial results → Terminate when $\sum p_i \geq \tau_R$. For updates: Insert/Delete modifies partition → batched maintenance evaluates cost deltas → split/merge commits if $\Delta C < -\tau$.
- **Design tradeoffs**:
  - **Refinement radius $r_f$**: Higher $r_f$ improves partition quality but increases maintenance cost (Table 7: refinement = 75% of maintenance time)
  - **Initial candidate fraction $f_M$**: Too low = may miss recall target; too high = wasted scan overhead
  - **Threshold $\tau$**: Lower = aggressive maintenance (potential over-splitting); higher = tolerates imbalance
- **Failure signatures**:
  - Recall degrades over time: APS model mismatch or $f_M$ too low; check if partition geometry has drifted
  - Latency increases despite maintenance: Access patterns shifting faster than maintenance interval; consider smaller window $W$ or lower $\tau$
  - NUMA scaling plateaus: Work imbalance across nodes; check for hot partitions concentrated on one node
- **First 3 experiments**:
  1. Reproduce Table 5 on SIFT 1M: Compare APS vs. fixed nprobe vs. oracle at $\tau_R \in \{0.8, 0.9, 0.99\}$ to validate geometric recall estimation.
  2. Ablate maintenance on WIKIPEDIA-12M: Run Quake with/without cost model, with/without refinement (per Table 7) to isolate component contributions.
  3. Scale NUMA workers on MSTURING 100M: Vary threads from 1 to 64, measure latency and throughput (replicate Figure 6) to identify saturation point on target hardware.

## Open Questions the Paper Calls Out
None

## Limitations
- Cost model may mispredict benefits if access patterns shift faster than maintenance intervals can adapt
- APS geometric recall estimation may become inaccurate in high-dimensional spaces or with non-uniform data distributions
- NUMA-aware parallelism may not generalize to small datasets or workloads with extreme partition skew causing node imbalances

## Confidence

- **High confidence**: Query latency improvements (1.5-38×) and update latency improvements (4.5-126×) against baselines are well-supported by experimental results across multiple datasets and recall targets.
- **Medium confidence**: The cost model's ability to maintain 90% recall targets without offline tuning is demonstrated, but relies on specific workload characteristics and may not generalize to all access patterns.
- **Medium confidence**: APS's geometric recall estimation achieving 17-29% overhead vs. oracle is validated, but the beta-function approximation for high-dimensional spaces needs further validation.

## Next Checks

1. **Dynamic Workload Validation**: Test Quake on datasets with rapidly shifting access patterns (e.g., temporal data streams) to evaluate cost model adaptation speed and APS recall accuracy under distribution drift.

2. **High-Dimensional Stress Test**: Evaluate APS recall estimation accuracy on datasets with dimensions > 1024, measuring breakdown points for the spherical approximation and beta-function calculations.

3. **NUMA Imbalance Analysis**: Create workloads with extreme partition skew (hot partitions on single nodes) to measure performance degradation and identify NUMA-aware mechanisms' limits.