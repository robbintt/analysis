---
ver: rpa2
title: 'Demo: Generative AI helps Radiotherapy Planning with User Preference'
arxiv_id: '2512.08996'
source_url: https://arxiv.org/abs/2512.08996
tags:
- dose
- mean
- planning
- prediction
- rapidplan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel two-stage training framework for radiotherapy
  dose prediction that incorporates user preferences through interactive sliders.
  The approach uses a foundational dose decoder pretraining stage followed by a flexible
  dose prediction stage with multi-conditional inputs including user-defined trade-offs
  between organs-at-risk (OARs) and planning target volumes (PTVs).
---

# Demo: Generative AI helps Radiotherapy Planning with User Preference

## Quick Facts
- **arXiv ID**: 2512.08996
- **Source URL**: https://arxiv.org/abs/2512.08996
- **Reference count**: 40
- **Primary result**: Novel two-stage training framework with user preference sliders achieves superior OAR sparing and PTV conformity compared to Varian RapidPlan in head-and-neck radiotherapy

## Executive Summary
This paper introduces a generative AI system for radiotherapy dose prediction that incorporates real-time user preferences through interactive sliders. The approach employs a two-stage training framework: Stage I pre-trains a VQ-VAE dose decoder on 31K dose distributions, while Stage II fine-tunes with multi-conditional inputs including user-defined trade-offs between organs-at-risk and planning target volumes. The method was validated on head-and-neck cancer cases and demonstrated better plan quality than Varian RapidPlan, with 14 better vs 0 worse OAR sparing cases and 1 better vs 0 worse PTV homogeneity cases.

## Method Summary
The method uses a two-stage training approach. Stage I involves VQ-VAE decoder pre-training using reconstruction, VQ, adversarial, and uniformity losses on 31K dose distributions. Stage II fine-tunes a flexible dose prediction system with a MedNext-style encoder and pre-trained decoder, incorporating multi-conditional inputs through AdaIN: CT images, RT structures, beam/angle plates, and user preference sliders. The user preferences (homogeneity index and OAR sparing weights) are encoded via adaptive instance normalization and injected into the image encoder. The model outputs 3D dose predictions that are translated into Eclipse-compatible optimization objectives through DVH extraction, achieving deliverable plans that outperform RapidPlan in both intra-patient and inter-patient scenarios.

## Key Results
- Interactive slider interface enables real-time customization of dose predictions with measurable DVH shifts between preference modes
- 14 better vs 0 worse OAR sparing cases and 1 better vs 0 worse PTV homogeneity cases compared to Varian RapidPlan
- Lower standard deviations in dose-volume histogram differences indicating more consistent plan quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Two-stage training with foundational dose decoder stabilizes predictions under complex, multi-conditional inputs.
- **Mechanism**: Stage I VQ-VAE pretraining regularizes the latent space, creating a realistic dose prior that constrains Stage II predictions even when conditional signals vary.
- **Core assumption**: Pre-training on large corpus encodes generalizable prior that regularizes Stage II learning.
- **Evidence anchors**: Abstract states "stabilize training, particularly under complex conditions"; section 2.2 describes regularization benefits; corpus support is weak.

### Mechanism 2
- **Claim**: Interactive sliders enable real-time customization of OAR vs. PTV trade-offs through conditioning.
- **Mechanism**: User preferences encoded via AdaIN and injected into encoder; random sampling during training creates smooth latent manifold.
- **Core assumption**: Random sampling creates controllable latent space where slider positions map to predictable dose metric changes.
- **Evidence anchors**: Abstract mentions "customizable preferences"; section 3 shows measurable DVH differences between preferences; corpus support is weak.

### Mechanism 3
- **Claim**: Translating 3D dose predictions to Eclipse objectives produces deliverable plans outperforming RapidPlan.
- **Mechanism**: Predicted 3D dose converted to DVH-based objectives plus margins, fed into Eclipse optimizer.
- **Core assumption**: Translation preserves enough dosimetric intent for final plan to reflect prediction quality.
- **Evidence anchors**: Abstract claims "outperforms Varian RapidPlan"; Table 4 shows better outcomes in 14/15 OAR cases; corpus support is limited.

## Foundational Learning

- **Concept: VQ-VAE (Vector Quantized Variational Autoencoder)**
  - **Why needed here**: Learns discrete latent space for dose distributions, enforcing realistic reconstructions and providing stable decoder for Stage II.
  - **Quick check question**: Can you explain why discretizing latent space via codebook quantization might help regularize dose prediction compared to continuous VAE?

- **Concept: Adaptive Instance Normalization (AdaIN)**
  - **Why needed here**: Injects user preferences and beam configurations into encoder by modulating feature statistics based on conditional inputs.
  - **Quick check question**: How does AdaIN differ from standard batch normalization when conditioning on external signals like user preferences?

- **Concept: DVH (Dose-Volume Histogram) Metrics**
  - **Why needed here**: Evaluates plans using DVH differences and translates 3D predictions into Eclipse-compatible objectives.
  - **Quick check question**: Given a DVH curve for an OAR, what does lower mean dose but higher max dose imply about dose distribution's clinical trade-offs?

## Architecture Onboarding

- **Component map**: CT (1 channel) + RT structures (7 channels) + beam/angle plates + user preference sliders → MedNext encoder with AdaIN → Stage I pre-trained decoder → 3D dose prediction → DVH extraction → Eclipse objectives → Eclipse optimizer → deliverable plan

- **Critical path**:
  1. Ensure Stage I pre-training converges with realistic dose reconstructions
  2. Freeze/warm-start decoder in Stage II; train encoder with preference conditioning
  3. Validate preference alignment: slider changes should produce predictable DVH shifts
  4. Run translation pipeline into Eclipse; verify achieved plans align with predictions

- **Design tradeoffs**:
  - One-step GAN vs. diffusion: Chosen for fast inference (~30 ms) but may sacrifice sample diversity
  - VQ-VAE pretraining vs. end-to-end: Extra stage increases complexity but stabilizes training under heterogeneous conditions
  - DVH objectives vs. direct 3D optimization: Current TPS lacks native 3D dose objectives, forcing translation step

- **Failure signatures**:
  - Boundary artifacts in dose maps: Often indicates insufficient Stage I pre-training
  - Non-monotonic slider responses: Suggests inadequate preference sampling or conditioning
  - Large intra-patient DVH variance: Indicates poor alignment between prediction and achievable plans

- **First 3 experiments**:
  1. Ablate Stage I pre-training: Train Stage II from scratch; compare MAE and artifact rates
  2. Preference controllability test: Systematically vary sliders; plot resulting mean OAR doses and PTV HI/CI
  3. Translation fidelity check: Compare predicted vs. Eclipse-achieved DVHs for 10 patients

## Open Questions the Paper Calls Out
None

## Limitations
- Two-stage training framework's effectiveness primarily validated internally with limited external evidence for VQ-VAE pretraining in RT dose prediction
- Translation from 3D dose to Eclipse objectives may not fully preserve AI's dosimetric intent due to TPS optimizer constraints
- Generalizability to other cancer sites, institutions, or treatment modalities remains untested

## Confidence
- **High Confidence**: Interactive slider methodology for real-time preference adjustment is technically sound
- **Medium Confidence**: Superior performance over RapidPlan supported by quantitative metrics but limited to head-and-neck cases
- **Low Confidence**: Stability claims for two-stage framework under complex conditions rely heavily on internal validation

## Next Checks
1. Cross-institutional validation: Test on dose distributions from different institutions with varying planning styles
2. Alternative TPS integration: Implement translation pipeline into open-source TPS to verify approach generality
3. Real-time slider response testing: Systematically vary slider positions for diverse anatomies and measure DVH changes for smoothness and clinical reasonableness