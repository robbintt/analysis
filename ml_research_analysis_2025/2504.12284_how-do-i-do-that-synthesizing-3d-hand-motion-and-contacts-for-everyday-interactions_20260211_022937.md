---
ver: rpa2
title: How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions
arxiv_id: '2504.12284'
source_url: https://arxiv.org/abs/2504.12284
tags:
- contact
- hand
- interaction
- image
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce LatentAct, a novel approach to predicting 3D hand
  motion and contact maps (interaction trajectories) from a single RGB image, action
  text, and a 3D contact point on an object. Our method consists of an Interaction
  Codebook, a VQVAE model that learns a latent codebook of hand poses and contact
  points, and an Interaction Predictor, a transformer-decoder module that retrieves
  relevant embeddings from the codebook and generates future interaction trajectories.
---

# How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions

## Quick Facts
- arXiv ID: 2504.12284
- Source URL: https://arxiv.org/abs/2504.12284
- Reference count: 40
- Introduces LatentAct, a novel approach to predicting 3D hand motion and contact maps from a single RGB image, action text, and 3D contact point

## Executive Summary
This paper introduces LatentAct, a novel approach for predicting 3D hand motion and contact maps from a single RGB image, action text, and a 3D contact point on an object. The method combines an Interaction Codebook (VQVAE) that learns latent embeddings of hand poses and contact points, with an Interaction Predictor (transformer-decoder) that retrieves relevant embeddings and generates future interaction trajectories. The authors develop a data engine to extract 3D hand poses and contact trajectories from the HoloAssist dataset, which they claim is 2.5-10X larger than existing works in terms of object and interaction diversity. Experiments show that LatentAct outperforms transformer and diffusion baselines across four generalization settings with MPJPE scores of 6.72-8.14 cm and F1-scores of 0.74-0.80.

## Method Summary
LatentAct consists of two main components: an Interaction Codebook (VQVAE) that learns a latent codebook of hand poses and contact points, and an Interaction Predictor (transformer-decoder) that retrieves relevant embeddings from the codebook and generates future interaction trajectories. The method takes as input a single RGB image, action text, and a 3D contact point on an object, and predicts the 3D hand motion and contact map for the interaction. The authors develop a data engine to extract 3D hand poses and contact trajectories from the HoloAssist dataset, which is used to train and evaluate the method.

## Key Results
- Outperforms transformer and diffusion baselines across four generalization settings
- Achieves MPJPE scores of 6.72-8.14 cm
- Achieves F1-scores of 0.74-0.80 for contact prediction
- Claims dataset is 2.5-10X larger than existing works in terms of object and interaction diversity

## Why This Works (Mechanism)
The paper does not provide a detailed mechanism section.

## Foundational Learning
- VQVAE: Vector Quantized Variational Autoencoder, used for learning discrete latent representations
  - Why needed: To learn a codebook of hand poses and contact points that can be efficiently retrieved and used for interaction prediction
  - Quick check: Review VQVAE architecture and training process
- Transformer decoder: A sequence-to-sequence model used for generating future interaction trajectories
  - Why needed: To model the temporal dependencies in hand motion and contact maps
  - Quick check: Understand transformer decoder architecture and attention mechanisms
- 3D hand pose estimation: The task of predicting the 3D position and orientation of hand joints from images or other inputs
  - Why needed: To accurately represent the hand's interaction with objects in 3D space
  - Quick check: Review state-of-the-art methods for 3D hand pose estimation
- Contact map prediction: The task of predicting which parts of the hand are in contact with the object during an interaction
  - Why needed: To capture the fine-grained details of hand-object interactions
  - Quick check: Understand different representations and evaluation metrics for contact maps

## Architecture Onboarding

Component map:
Input (RGB image, action text, 3D contact point) -> Interaction Codebook (VQVAE) -> Interaction Predictor (Transformer Decoder) -> Output (3D hand motion, contact map)

Critical path:
The critical path involves extracting features from the input using the Interaction Codebook, retrieving relevant embeddings, and generating the output using the Interaction Predictor.

Design tradeoffs:
The paper does not discuss specific design tradeoffs.

Failure signatures:
The paper does not discuss specific failure signatures.

First experiments:
1. Train and evaluate the Interaction Codebook (VQVAE) on the HoloAssist dataset to learn latent embeddings of hand poses and contact points.
2. Train and evaluate the Interaction Predictor (transformer decoder) on the learned embeddings to generate future interaction trajectories.
3. Perform ablation studies to quantify the individual contributions of the codebook and transformer components.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Evaluation methodology for contact prediction lacks detail on F1-score metric and contact map representation
- Dataset construction process for extracting 3D hand poses and contact trajectories from HoloAssist lacks clarity on annotation quality and potential biases
- Generalization claims across novel objects, actions, tasks, and scenes are difficult to assess without explicit cross-validation protocols or held-out test sets
- No statistical significance testing provided for performance improvements over baselines

## Confidence
- Core technical approach (VQVAE codebook with transformer decoder): Medium
- Performance improvements over baselines (6.72-8.14 cm MPJPE): Medium (requires independent replication)
- Claim of 2.5-10X dataset size increase compared to prior work: Low (without explicit dataset statistics and comparisons)

## Next Checks
1. Conduct ablation studies removing either the codebook or transformer components to quantify their individual contributions
2. Implement cross-dataset evaluation on standard benchmarks like HO-3D or FPHA to verify generalization claims
3. Perform user studies comparing synthesized hand motions against ground truth videos for qualitative assessment of naturalness and plausibility