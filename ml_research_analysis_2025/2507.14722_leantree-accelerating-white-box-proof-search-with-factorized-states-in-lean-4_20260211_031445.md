---
ver: rpa2
title: 'LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean
  4'
arxiv_id: '2507.14722'
source_url: https://arxiv.org/abs/2507.14722
tags:
- proof
- lean
- tactic
- search
- goals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeanTree addresses the challenge of automated theorem proving in
  formal systems like Lean 4 by introducing a white-box approach that factorizes complex
  proof states into simpler, independent branches. The core method involves building
  a tool that incrementally executes proofs while detecting and managing metavariable
  coupling between goals, along with a data extraction pipeline that transforms human-written
  and autoformalized proofs into structured proof trees.
---

# LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4

## Quick Facts
- arXiv ID: 2507.14722
- Source URL: https://arxiv.org/abs/2507.14722
- Reference count: 34
- Key outcome: White-box rollouts achieve 18.36% success rate on MiniF2F, significantly outperforming black-box (5.32%) and matching whole-proof generation (9.59%)

## Executive Summary
LeanTree addresses the challenge of automated theorem proving in Lean 4 by introducing a white-box approach that factorizes complex proof states into simpler, independent branches. The core method involves building a tool that incrementally executes proofs while detecting and managing metavariable coupling between goals, along with a data extraction pipeline that transforms human-written and autoformalized proofs into structured proof trees. This enables parallel search, efficient state reuse, and richer training data compared to black-box methods. The LeanTree dataset contains 74,706 factorized proofs from Mathlib and 26,201 from DeepSeek-Prover-V1, structured for direct use in machine learning pipelines.

## Method Summary
LeanTree introduces a white-box automated theorem proving approach for Lean 4 that factorizes complex proof states into simpler, independent branches by detecting metavariable dependencies. The method builds a proof tree incrementally through three stages: singleton trees from tactic applications, simplification of nested tactics, and merging of metavariable-coupled goals. This enables parallel search and efficient state reuse. The system includes a Python interaction module, a BetterParser for tactic-goal mapping, and a proof tree builder. The LeanTree dataset extracts 74,706 factorized proofs from Mathlib and 26,201 from DeepSeek-Prover-V1, providing structured training data for machine learning approaches to theorem proving.

## Key Results
- White-box rollouts with Llemma-7B achieve 18.36% success rate on MiniF2F-test
- Black-box rollouts (without intermediate state feedback) achieve only 5.32% success rate
- Whole-proof generation achieves 9.59% success rate
- LeanTree dataset contains 74,706 factorized proofs from Mathlib and 26,201 from DeepSeek-Prover-V1

## Why This Works (Mechanism)

### Mechanism 1: Proof State Factorization
- Claim: Breaking complex proof states into independent, parallelizable branches reduces search complexity and enables state reuse.
- Mechanism: When tactics produce multiple goals, LeanTree allows each to be explored independently rather than treating them as a monolithic state.
- Core assumption: Goals produced by tactics are generally independent unless they share unassigned metavariables.
- Evidence anchors: Abstract states factorization enables "parallel search, efficient state reuse"; section 3.2 shows "simplifying states reduces the difficulty of the policy task".
- Break condition: Goals share metavariables (handled by Mechanism 2); otherwise, factorization yields independent search branches.

### Mechanism 2: Metavariable Coupling Detection
- Claim: Detecting shared metavariables between goals prevents incorrect independent exploration of dependent subproblems.
- Mechanism: When tactics create goals sharing an unassigned metavariable (e.g., `2 ≤ ?m` and `?m ≤ 5`), LeanTree keeps them coupled until the metavariable is assigned.
- Core assumption: Metavariable dependencies can be reliably detected and tracked during incremental proof execution.
- Evidence anchors: Section 3.3 describes detection of "all dependencies between goals caused by metavariables"; section 4.1.3 shows "sibling nodes in a singleton tree are merged if they share a metavariable".
- Break condition: Coupling detection fails or is incomplete, leading to incorrect independent exploration.

### Mechanism 3: White-Box Feedback Loop
- Claim: Providing intermediate proof state feedback to LLMs during search substantially improves success rates over black-box approaches.
- Mechanism: After each tactic, the model receives the verified current goal state, conditioning the next tactic on grounded intermediate context rather than hallucinating progress.
- Core assumption: Pretrained LLMs can effectively use intermediate state information for tactic selection.
- Evidence anchors: Abstract shows "white-box rollouts achieve a 18.36% success rate, significantly outperforming black-box rollouts (5.32%)"; section 6 demonstrates "supplying the prover with internal proof states improves proof search performance".
- Break condition: Intermediate states become too complex or distribution shift degrades model conditioning.

## Foundational Learning

- Concept: Lean Tactic Mode and Metavariables
  - Why needed here: LeanTree factorizes proof states defined by metavariables; understanding that metavariables are unassigned "holes" in proof terms is essential.
  - Quick check question: In Lean, what does it mean for a metavariable to be "assigned" during tactic execution?

- Concept: AND-OR Tree Search
  - Why needed here: Factorized states map naturally to AND-OR trees where OR nodes select tactics and AND nodes require proving all subgoals.
  - Quick check question: In an AND-OR proof tree, what must hold at an AND node versus an OR node?

- Concept: Black-box vs. White-box ATP
  - Why needed here: The paper advances white-box methods; understanding the distinction explains why intermediate state access matters.
  - Quick check question: How does a white-box prover's interaction with the verifier differ from a black-box prover's?

## Architecture Onboarding

- Component map: Lean REPL -> LeanTree Interaction Module -> PaperProof BetterParser -> Proof Tree Builder -> Dataset Storage
- Critical path: 1. User initiates proof search via Python API (theorem statement or Lean file). 2. LeanTree queries Lean REPL for initial proof state. 3. Factorization module splits goals where possible, respecting metavariable coupling. 4. Policy model proposes tactics conditioned on current factorized state. 5. Tactic applied via REPL; new state(s) returned. 6. Repeat until no open goals or budget exhausted. 7. Final proof verified via Lean kernel.
- Design tradeoffs: Coverage vs. correctness (23% of Mathlib tactic proofs not converted due to `calc`/`conv` constructs); Granularity vs. preprocessing complexity (breaking nested/merged tactics simplifies modeling but increases pipeline complexity); Verification thoroughness vs. speed (LeanTree's incremental assignment verification avoids false negatives from full-term kernel checks on branched proofs).
- Failure signatures: False positives (library search tactics like `apply?` left in proofs; banned by default); False negatives (full-term kernel verification fails on syntactically branched proofs; mitigated by incremental assignment verification); Parsing failures (complex tactic combinators like `try`, `iterate`, `any goals` may not translate cleanly to tree structure).
- First 3 experiments: 1. Reproduce MiniF2F linear rollouts with Llemma-7B to validate white-box (18.36%) vs. black-box (5.32%) vs. whole-proof (9.59%) comparisons. 2. Ablate factorization: Compare factorized vs. non-factorized search on a held-out subset to isolate gains from state factorization. 3. Verify metavariable coupling detection: Construct test cases with known shared metavariables and confirm correct coupling/decoupling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proof tree building process be extended to support tactics like `calc` and `conv` that fundamentally alter proof structure?
- Basis in paper: Section 4.2 identifies that 28.6% of Mathlib conversion failures stem from these tactics, and Section 7 lists "not all tactic proofs can be converted" as a specific limitation.
- Why unresolved: These constructs do not fit the standard goal-tactic model used by the current singleton tree builder, causing structural parsing failures.
- What evidence would resolve it: An updated extraction algorithm that successfully converts `calc` and `conv` blocks into factorized proof trees without losing semantic correctness.

### Open Question 2
- Question: Does factorized proof search scale effectively when integrated with sophisticated tree search algorithms like Best-First Search (BFS)?
- Basis in paper: Section 6 notes that experiments were restricted to linear rollouts, while citing Best-First Search as a "natural upper bound" that remains untested with LeanTree's specific factorization.
- Why unresolved: The paper validates the dataset and tool using simple linear sampling but does not evaluate the intended AND-OR search mechanisms on the new factorized states.
- What evidence would resolve it: Benchmarks on MiniF2F or ProofNet showing success rates of LeanTree-integrated BFS/MCTS compared to the reported linear rollout baseline (18.36%).

### Open Question 3
- Question: Does the lack of premise information in LeanTree limit its performance compared to retrieval-augmented approaches?
- Basis in paper: Section 5 contrasts LeanTree with LeanDojo, noting that LeanDojo extracts premise information for retrieval-augmented generation (RAG), a feature LeanTree currently lacks.
- Why unresolved: The paper isolates the benefit of factorization but does not ablate the impact of missing premise retrieval, which is a standard component in modern automated theorem proving.
- What evidence would resolve it: A comparative study measuring performance when LeanTree's factorized states are combined with premise retrieval versus using factorization alone.

## Limitations
- 23% of Mathlib tactic proofs cannot be converted due to unsupported constructs like `calc` and `conv`
- Sampling hyperparameters for Llemma-7B (temperature, top-p) are not specified, affecting reproducibility
- Dataset coverage limitations may introduce bias in model training and evaluation

## Confidence
- **High Confidence**: The core mechanism of using white-box feedback (intermediate state access) to improve proof search performance is well-supported by the 18.36% vs 5.32% success rate comparison. The mathematical soundness of metavariable coupling detection is also well-established.
- **Medium Confidence**: The factorization approach's effectiveness is supported by the overall results, but the specific contribution of factorization versus other factors isn't isolated in experiments. The claim that factorized states simplify policy tasks is theoretically sound but lacks direct empirical validation.
- **Low Confidence**: The completeness of metavariable coupling detection in all edge cases isn't empirically validated. The impact of the 23% dataset coverage gap on model generalization is uncertain.

## Next Checks
1. Reproduce the MiniF2F linear rollouts with Llemma-7B to verify the 18.36% white-box vs 5.32% black-box success rate comparison under controlled conditions.
2. Conduct an ablation study comparing factorized vs non-factorized search on a held-out subset to isolate the specific contribution of state factorization.
3. Test the metavariable coupling detection mechanism on edge cases with complex shared metavariable dependencies to verify correct coupling/decoupling behavior across all scenarios.