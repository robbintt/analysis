---
ver: rpa2
title: Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning
arxiv_id: '2506.05418'
source_url: https://arxiv.org/abs/2506.05418
tags:
- learning
- data
- dynamics
- performance
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Self-Predictive Dynamics (SPD), a method for
  vision-based reinforcement learning that addresses data efficiency and generalization
  challenges. SPD uses weak and strong augmentations in parallel, and learns representations
  by predicting inverse and forward transitions across two-way augmented versions.
---

# Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.05418
- Source URL: https://arxiv.org/abs/2506.05418
- Reference count: 40
- Primary result: SPD achieves 22% and 396% performance gains on Hopper Hop compared to other RL methods for each background setup, and 29% higher performance than PAD which is fine-tuned for testing observations on Finger Spin.

## Executive Summary
This paper proposes Self-Predictive Dynamics (SPD), a method for vision-based reinforcement learning that addresses data efficiency and generalization challenges. SPD uses weak and strong augmentations in parallel, and learns representations by predicting inverse and forward transitions across two-way augmented versions. It consists of two-way data augmentations, a discriminator, and dynamics chaining. In experiments on MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD outperforms previous studies in complex observations and significantly improves generalization performance for unseen observations.

## Method Summary
SPD is implemented on top of Soft Actor-Critic (SAC) and adds a shared encoder with discriminator (relativistic GAN) and dynamics chaining (inverse + forward dynamics) using two-way augmentations. The method alternates between updating the SPD components (encoder, discriminator, dynamics) and the SAC policy. The SPD loss combines adversarial invariance learning (forcing weak and strong augmented views to have indistinguishable representations) with inverse and forward dynamics prediction losses. The key innovation is using strong augmentations during training to force the encoder to learn background-invariant representations while maintaining task-relevant information through dynamics modeling.

## Key Results
- SPD outperforms previous studies in complex observation environments like MuJoCo visual control tasks with distractor backgrounds
- SPD significantly improves generalization performance for unseen observations, achieving 22% and 396% performance gains on Hopper Hop compared to other RL methods
- SPD achieves 29% higher performance than PAD (which is fine-tuned for testing observations) on Finger Spin task
- The method demonstrates effectiveness in autonomous driving task (CARLA) with complex visual observations

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Invariance Learning via Relativistic GAN
- Forcing representations of weakly and strongly augmented views to be indistinguishable encourages the encoder to discard task-irrelevant features like background textures and lighting
- The discriminator tries to distinguish between $z_w$ (weak augmentation) and $z_s$ (strong augmentation), while the encoder optimizes $J(\phi) = -\log(\sigma(D(z_s) - D(z_w)))$ to fool it
- Core assumption: Task-relevant features (robot pose, object positions) are invariant to color/texture perturbations, while distractors are not
- Evidence anchors: [abstract], [section 3.3], related work "Salience-Invariant Consistent Policy Learning"

### Mechanism 2: Inverse Dynamics for Task-Relevant Feature Selection
- Predicting the action that connects two states forces the encoder to retain only features causally related to agent behavior
- Inverse dynamics $I(z_t, z_{t+1}) \rightarrow \tilde{a}_t$ must predict the executed action $a_t$
- Core assumption: Actions are determined primarily by task-relevant state changes; distracting features are temporally uncorrelated with actions
- Evidence anchors: [section 3.4], [figure 4 left], "Dream to Generalize" paper

### Mechanism 3: Cross-Augmentation Forward Prediction for Consistent Latent Dynamics
- Predicting next latent states across different augmentation levels enforces a consistent latent dynamics model, improving generalization
- Forward dynamics $F(z_t, \tilde{a}_t) \rightarrow \tilde{z}_{t+1}$ uses actions inferred from cross-augmented pairs
- Core assumption: A well-structured latent space exists where dynamics are approximately linear and augmentation-invariant
- Evidence anchors: [section 3.4], [figure 4 left], weak corpus support

## Foundational Learning

- **Soft Actor-Critic (SAC) and off-policy RL**: SPD is implemented on top of SAC; understanding entropy-regularized actor-critic updates is essential for debugging the full system
  - Why needed: SPD builds on SAC framework
  - Quick check: Can you explain why SAC uses a stochastic policy and how the entropy term affects exploration?

- **Contrastive vs. Adversarial Representation Learning**: The paper compares its relativistic GAN discriminator against contrastive methods (CURL)
  - Why needed: Understanding when each works helps interpret ablation results
  - Quick check: What is the key difference between maximizing mutual information (contrastive) and fooling a discriminator (adversarial) for learning invariances?

- **Data Augmentation Strength in RL**: The core innovation is combining weak and strong augmentations
  - Why needed: Knowing why strong augmentations alone hurt RL performance clarifies the two-way design
  - Quick check: Why might strong augmentations like random convolution degrade policy learning when applied naively in RL?

## Architecture Onboarding

- **Component map**: Image -> Aug$_w$ + Aug$_s$ -> Encoder -> $z_w$, $z_s$ -> Discriminator + Inverse Dynamics + Forward Dynamics -> SPD Loss -> Encoder update
- **Critical path**: 1) Observe image → apply Aug$_w$ and Aug$_s$ → encode to $z_w$ and $z_s$ 2) Train discriminator to distinguish $z_w$/$z_s$ while encoder tries to fool it 3) From consecutive observations, infer actions via inverse dynamics on cross-augmented pairs 4) Predict next latent states via forward dynamics; enforce consistency 5) Update encoder with combined gradients from SPD loss + SAC Q-function loss
- **Design tradeoffs**: $\lambda_\psi$ (dynamics loss weight) vs. $\lambda_A$ (adversarial loss weight): Paper uses $\lambda_\psi=0.1$, $\lambda_A=0.001$; higher $\lambda_A$ may destabilize training
- **Failure signatures**: Discriminator loss collapses to 0 (encoder and discriminator may have reached equilibrium too quickly); inverse dynamics loss remains high (latent space may not capture action-relevant features)
- **First 3 experiments**: 1) Sanity check without distractors: Train SPD on default DMC (no background) to verify it matches DrQ/SAC baseline performance 2) Ablation on augmentation strength: Compare using only random-shift vs. full strong augmentation set 3) Generalization test: Train on Simple Distractor, test on Natural Video (unseen)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided. However, based on the discussion and future work mentioned, potential open questions include: extending the single-step dynamics chaining to multi-step predictions for long-term temporal dependencies, analyzing robustness to strong augmentations that might distort task-relevant features, and testing applicability to high-dimensional, realistic robotic control without manual augmentation tuning.

## Limitations
- Exact augmentation parameters (e.g., random convolution kernel size, color jitter strength) are unspecified, making exact reproduction difficult
- The extremely large performance improvements (e.g., 396%) without detailed variance reporting or ablation on augmentation strength
- The claim of 396% performance gain on Hopper Hop appears extremely high and may be sensitive to initialization or background generation specifics

## Confidence
- **High Confidence**: The overall architecture design (two-way augmentations + adversarial invariance + dynamics chaining) is internally consistent and mechanistically plausible
- **Medium Confidence**: Empirical results showing SPD outperforming baselines on DMC and CARLA generalization tasks, though exact replication depends on unspecified implementation details
- **Low Confidence**: The extremely large performance improvements (e.g., 396%) without detailed variance reporting or ablation on augmentation strength

## Next Checks
1. Replicate the ablation study comparing discriminator-only vs. discriminator+inverse vs. full SPD on Hopper Hop to verify the claimed improvement trajectory
2. Test SPD on unseen backgrounds (Natural Video) after training on Simple Distractor to confirm generalization claims in Table 1
3. Vary the adversarial loss weight λA systematically (0.0001, 0.001, 0.01) to assess sensitivity and potential overfitting