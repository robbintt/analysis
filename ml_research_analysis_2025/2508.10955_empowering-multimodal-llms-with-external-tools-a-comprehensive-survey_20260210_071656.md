---
ver: rpa2
title: 'Empowering Multimodal LLMs with External Tools: A Comprehensive Survey'
arxiv_id: '2508.10955'
source_url: https://arxiv.org/abs/2508.10955
tags:
- multimodal
- arxiv
- mllms
- large
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey presents a comprehensive review of how external tools
  can enhance Multimodal Large Language Models (MLLMs) across four key dimensions:
  high-quality data acquisition, challenging task performance, thorough evaluation,
  and future research directions. The authors categorize external tools into knowledge
  bases, expert models, APIs, physical tools, and program tools, highlighting their
  benefits for knowledge acquisition, expertise enhancement, efficiency, adaptability,
  interpretability, and robustness.'
---

# Empowering Multimodal LLMs with External Tools: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2508.10955
- **Source URL**: https://arxiv.org/abs/2508.10955
- **Reference count**: 40
- **Primary result**: Comprehensive review of how external tools enhance MLLMs across data acquisition, task performance, evaluation, and future directions

## Executive Summary
This survey presents a comprehensive review of how external tools can enhance Multimodal Large Language Models (MLLMs) across four key dimensions: high-quality data acquisition, challenging task performance, thorough evaluation, and future research directions. The authors categorize external tools into knowledge bases, expert models, APIs, physical tools, and program tools, highlighting their benefits for knowledge acquisition, expertise enhancement, efficiency, adaptability, interpretability, and robustness. The survey systematically examines how external tools can be leveraged for data collection, synthesis, annotation, and cleaning to address the data bottleneck in MLLM development. It then explores how these tools can improve MLLM performance on complex tasks such as multimodal retrieval-augmented generation, reasoning, hallucination detection and mitigation, safety, agent-based applications, and video perception. The survey also reviews external tool-based approaches for MLLM evaluation, including keyword extraction, embedding-based metrics, MLLM-based evaluation, and evaluation platforms. Finally, it identifies key challenges including efficiency, privacy, proactivity, trustworthiness, cost, fairness, and diversity, while outlining promising future research directions for developing more capable and reliable tool-augmented MLLMs.

## Method Summary
This survey paper systematically reviews the literature on external tool augmentation for MLLMs without conducting original experiments. It categorizes tools into knowledge bases, expert models, APIs, physical tools, and program tools, then examines their applications across four dimensions: data acquisition (collection, synthesis, annotation, cleaning), task performance (RAG, reasoning, hallucination detection, safety, agents, video perception), evaluation methods (keyword extraction, embedding-based metrics, MLLM-based evaluation, platforms), and future research directions. The survey synthesizes findings from 40+ papers, identifying mechanisms like retrieval-augmented grounding, synthetic data injection, and agentic task decomposition. No unified training procedure is provided; instead, it describes multiple approaches including training-free (in-context learning) and instruction-tuning methods for RAG, input-augmented and calibration-based approaches for hallucination mitigation, and detector-based and LLM-based approaches for various tasks.

## Key Results
- External tools can significantly reduce MLLM hallucinations through retrieval-augmented grounding using CLIP/BLIP embeddings
- Synthetic data generation via expert models (GPT-4, Stable Diffusion) enables instruction-tuning for specialized capabilities like reasoning
- Agentic task decomposition allows MLLMs to delegate sub-tasks to specialized tools, improving performance on complex multimodal tasks
- Tool integration introduces efficiency trade-offs, with agentic systems offering greater capability at the cost of increased latency
- Privacy concerns arise when using closed-source APIs for annotation and evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Grounding
If an MLLM is augmented with a retrieval mechanism, it appears to reduce hallucinations by conditioning generation on external evidence rather than internal parametric memory alone. A retriever (e.g., CLIP or BLIP) encodes the multimodal query to fetch relevant knowledge items (text/image) from an external database. These items are injected into the MLLM's context window, explicitly providing ground-truth or high-probability associations before generation. The core assumption is that the retrieval model can accurately map the query to relevant knowledge, and the MLLM can effectively attend to and utilize this context over its internal priors. This mechanism is supported by MRAG approaches that improve factual accuracy and reduce hallucinations by retrieving and incorporating knowledge from external knowledge bases.

### Mechanism 2: Capability Injection via Synthetic Data
If high-quality instruction-tuning data is scarce, expert models can synthesize it to teach MLLMs specific behaviors (e.g., reasoning, object detection). Powerful teacher models (e.g., GPT-4, Stable Diffusion) process raw inputs (images/text) to generate complex outputs (CoT rationales, edited images). The student MLLM is then fine-tuned on this synthetic corpus to internalize the expert's reasoning or perceptual alignment. The core assumption is that the teacher model's outputs are sufficiently high-quality and error-free that they do not propagate hallucinations or bias into the student model. This approach is validated by LLaVA-Reasoner, which uses GPT-4o to generate rationales from minimal supervision and shows strong generalization.

### Mechanism 3: Agentic Task Decomposition
If a complex multimodal task exceeds the MLLM's native capacity, the model can act as an agent to decompose the task and delegate sub-tasks to specialized tools. The MLLM interprets a user request, plans a sequence of actions, invokes external tools (APIs, code) for execution, and synthesizes the final response. The core assumption is that the MLLM possesses sufficient planning and instruction-following capabilities to generate valid tool calls and handle the returned structured data. This mechanism is supported by multimodal agents that perceive information, reason and plan over complex tasks, and make informed decisions to execute appropriate actions.

## Foundational Learning

- **Concept: Multimodal Embeddings (CLIP/BLIP)**
  - **Why needed here**: These are the foundation for Retrieval (MRAG) and Evaluation (similarity scoring). You must understand how images and text map to a shared vector space.
  - **Quick check question**: Can you explain why CLIP Score might be used to detect hallucinations in an image caption?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here**: The survey emphasizes CoT as a primary method for enhancing Reasoning and Hallucination Analysis. Tools are often used to generate or verify CoT steps.
  - **Quick check question**: How does prompting an MLLM to "think step-by-step" change its reliance on external tools like a calculator or search engine?

- **Concept: Hallucination vs. Grounding**
  - **Why needed here**: A major portion of the survey addresses the "grounding" problem. You need to distinguish between object hallucination (inventing objects) and relation hallucination.
  - **Quick check question**: In the context of this survey, how does a "Grounding DINO" tool help differentiate a "real" object from a "hallucinated" one?

## Architecture Onboarding

- **Component map**: Core: LLM Backbone (e.g., LLaMA) + Multimodal Projector + Vision Encoder -> Interface: ToolFormer/Function-calling layer (parses text to API calls) -> Tools: External Expert Models (Grounding DINO, Stable Diffusion), Knowledge Bases (Vector DBs), Program Tools (Python/CLIP)

- **Critical path**:
  1. Input: Multimodal query (Image + Text) enters the MLLM
  2. Decision: MLLM determines if internal knowledge is sufficient (often via a "Reflection" token or classifier)
  3. Retrieval: If insufficient, query Vector DB via CLIP embeddings
  4. Tool Execution: If action is needed, execute API/Code
  5. Synthesis: MLLM generates final answer based on Tool Output

- **Design tradeoffs**:
  - Efficiency vs. Accuracy: Tool integration adds inference latency. Agentic systems are more capable but significantly slower than monolithic MLLMs
  - Privacy vs. Capability: Using closed-source APIs (GPT-4V) for annotation/evaluation offers high capability but risks data privacy

- **Failure signatures**:
  - Tool Snowballing: An error in one tool (e.g., bad OCR) propagates through the reasoning chain, leading to a hallucinated conclusion
  - Over-reliance: The MLLM ignores visual input and blindly trusts retrieved text that is contextually irrelevant

- **First 3 experiments**:
  1. Baseline vs. MRAG: Implement a simple CLIP-based retriever to feed similar images/captions to an MLLM and measure the drop in object hallucination rate on a benchmark like POPE
  2. Synthetic Fine-tuning: Use GPT-4o (via API) to generate CoT rationales for a small image dataset; fine-tune an open-source MLLM (e.g., LLaVA) on this data and test for improved reasoning consistency
  3. Tool-Integration Stress Test: Give an MLLM agent access to a calculator and web search; ask it multi-step math questions requiring current data (e.g., "What is the inflation rate times the population of France?") to verify tool-calling logic

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can MLLMs be developed to autonomously decide when to invoke tools and which specific tools to select without explicit human instructions?
**Basis in paper**: Section 6 states that current MLLMs "lack both the initiative to invoke tools autonomously and the ability to make informed decisions about which tools to use."
**Why unresolved**: Existing systems rely on manual configuration or prompt-based triggers; developing the agency for self-governed tool selection is a nascent field.
**What evidence would resolve it**: An agent architecture that dynamically selects the optimal tool from a diverse set based solely on the context of the multimodal input.

### Open Question 2
**Question**: To what extent can the capabilities of external tools be "internalized" into MLLM parameters during training to eliminate inference-time latency?
**Basis in paper**: Section 6 identifies "Efficiency" as a challenge, proposing research into integrating tools during training to "internalize expert capabilities."
**Why unresolved**: There is a trade-off between the flexibility of external tools and the speed of a standalone model; the capacity of model weights to absorb tool functionalities without degradation is unknown.
**What evidence would resolve it**: A study demonstrating that a fine-tuned MLLM can perform specialized tasks (e.g., detection) with accuracy comparable to a tool-augmented version but with significantly reduced latency.

### Open Question 3
**Question**: How can MLLMs adaptively determine the reliability of external tool outputs to prevent error propagation?
**Basis in paper**: Section 6 notes under "Trustworthiness" that performance depends on external models that "may not be trustworthy," suggesting a need for adaptive trust mechanisms.
**Why unresolved**: Current pipelines often treat tool outputs as ground truth; mechanisms for the MLLM to "verify" or "distrust" external data are underexplored.
**What evidence would resolve it**: A framework that successfully identifies and ignores noisy or hallucinated outputs from an unreliable tool without sacrificing overall task performance.

## Limitations
- Performance improvements from tool integration are reported without independent verification across different benchmarks and domains
- The survey does not address edge cases where tool failures cascade or where multimodal context is insufficient for tool selection
- Real-world cost-benefit trade-offs of tool-augmented systems are not deeply analyzed beyond noting efficiency concerns

## Confidence
- **High Confidence**: The categorization of external tools (knowledge bases, expert models, APIs, physical tools, program tools) and their general benefits for MLLMs is well-supported by the literature
- **Medium Confidence**: Claims about specific performance improvements from tool integration are moderately supported but vary significantly across different approaches and benchmarks
- **Low Confidence**: Assertions about future research directions and emerging challenges are speculative and not yet empirically validated

## Next Checks
1. **Empirical Replication**: Implement a minimal CLIP-based retrieval pipeline with LLaVA and measure actual hallucination reduction on POPE benchmark versus reported improvements
2. **Cost-Benefit Analysis**: Profile the latency and compute costs of tool-augmented inference versus baseline MLLM performance across multiple tasks
3. **Cross-Domain Generalization**: Test synthetic data fine-tuning approaches (e.g., LLaVA-Reasoner methodology) on datasets outside their training distribution to verify robustness claims