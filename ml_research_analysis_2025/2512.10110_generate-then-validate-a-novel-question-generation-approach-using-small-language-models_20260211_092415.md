---
ver: rpa2
title: 'Generate-Then-Validate: A Novel Question Generation Approach Using Small Language
  Models'
arxiv_id: '2512.10110'
source_url: https://arxiv.org/abs/2512.10110
tags:
- answer
- question
- questions
- human
- phi-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a \u201Cgenerate-then-validate\u201D pipeline\
  \ for automated multiple-choice question generation using small language models.\
  \ The approach first generates an abundance of candidate questions via Phi-2, then\
  \ applies syntactic filtering, answer confidence evaluation, and learning-objective\
  \ alignment checks."
---

# Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models

## Quick Facts
- **arXiv ID:** 2512.10110
- **Source URL:** https://arxiv.org/abs/2512.10110
- **Reference count:** 34
- **Primary result:** A pipeline using Phi-2 (2.7B parameters) to generate and validate multiple-choice questions, achieving high inter-rater agreement on answers (kappa ≈ 0.76) and moderate agreement on learning-objective alignment (kappa ≈ 0.58).

## Executive Summary
This paper introduces a "generate-then-validate" pipeline for automated multiple-choice question (MCQ) generation using small language models. The approach first generates an abundance of candidate questions via Phi-2, then applies syntactic filtering, answer confidence evaluation, and learning-objective alignment checks. Human experts and Gemini-2.5-Pro served as judges, achieving substantial inter-rater agreement on question answers (Cohen's kappa ≈ 0.76) and moderate agreement on learning-objective alignment (Cohen's kappa ≈ 0.58). The ablation study confirmed that validation steps improve question quality, with agreement rising to 0.91 kappa at high confidence thresholds. The method demonstrates that small models, when guided by systematic validation, can produce high-quality questions suitable for educational use.

## Method Summary
The method uses a five-step incremental prompting process with Phi-2 to generate MCQs from action-based learning objectives (LOs) in AP Environmental Science. First, the model generates a question seed, then a stem, followed by answer choices, the correct answer, and an explanation. Validation includes syntactic filtering (e.g., ensuring choices follow "a) option" format), answer confidence evaluation (retaining questions where the model's confidence in the correct answer exceeds 0.9), and learning-objective alignment (retaining questions most likely generated from the intended LO). Human experts and Gemini-2.5-Pro judge the final outputs for answerability and alignment.

## Key Results
- High inter-rater agreement on question answers (Cohen's kappa ≈ 0.76).
- Moderate inter-rater agreement on learning-objective alignment (Cohen's kappa ≈ 0.58).
- Ablation study shows validation steps significantly improve question quality, with agreement reaching 0.91 kappa at high confidence thresholds.
- The pipeline retains about 42% of generated questions after all validation steps.

## Why This Works (Mechanism)
The generate-then-validate approach works by leveraging the strong reasoning capabilities of Phi-2 within a controlled generation framework, then using systematic validation to filter out errors that small models are prone to. The answer confidence threshold (>0.9) acts as a guardrail against low-quality distractors and ambiguous correct answers. Learning-objective alignment ensures that the generated questions actually address the intended educational goals rather than drifting into unrelated topics. This two-phase approach compensates for the limited general knowledge of small models by focusing their generation on quality over quantity.

## Foundational Learning
- **Incremental Prompting**: Why needed: To guide the model step-by-step through complex question structure. Quick check: Model outputs follow the expected format at each stage.
- **Answer Confidence Thresholding**: Why needed: To ensure the model is certain about the correct answer before accepting it. Quick check: Probability of correct answer exceeds 0.9.
- **Learning-Objective Alignment via Log-Probability**: Why needed: To verify that the question semantically matches the intended LO. Quick check: Log-probability for the intended LO exceeds all others.
- **Syntactic Filtering**: Why needed: To enforce consistent formatting for downstream processing. Quick check: All choices match the "a) option" pattern.

## Architecture Onboarding

### Component Map
Phi-2 (Generation) -> Syntactic Filter -> Answer Confidence Validator -> LO Alignment Validator -> Final Question Set

### Critical Path
The critical path is the generation and validation sequence: incremental prompting must complete successfully before any validation can occur, and all three validation steps must pass for a question to be retained. The most computationally expensive step is the answer confidence evaluation, which requires multiple forward passes to compute probabilities.

### Design Tradeoffs
- **Model Size vs. Quality**: Using Phi-2 (2.7B) instead of a larger model trades raw capability for efficiency and domain-specific training benefits.
- **Validation Stringency vs. Retention**: Higher confidence thresholds improve quality but reduce the number of usable questions.
- **Human vs. Model Validation**: Using Gemini-2.5-Pro as a surrogate judge reduces cost but may miss subtle pedagogical issues.

### Failure Signatures
- **Format Drift**: Model fails to output choices in "a) option" format, breaking downstream parsing.
- **Low Confidence Filtering**: Most questions are discarded because the model is uncertain about correct answers.
- **LO Misalignment**: Generated questions do not semantically match the intended learning objectives despite passing formatting checks.

### First Experiments
1. Test Phi-2 incremental prompting on a single LO to verify output format compliance.
2. Measure answer confidence distribution to set an appropriate threshold.
3. Validate that LO alignment computation correctly ranks the intended objective highest.

## Open Questions the Paper Calls Out
- **Synergy of SLM and LLM**: Would combining an SLM for generation with an LLM for validation improve quality or throughput? (The paper suggests this as future work.)
- **Pedagogical Utility**: Does high expert agreement on question quality translate to effective learning gains for actual students? (No classroom study was conducted.)
- **Generalizability of Confidence Evaluation**: Is the effectiveness of answer confidence evaluation dependent on Phi-2's domain-specific training, or would it work similarly with other SLMs? (Only Phi-2 was tested.)

## Limitations
- The pipeline relies on strict formatting rules that may be brittle to model output variations.
- Learning-objective alignment agreement (kappa ≈ 0.58) is only moderate, suggesting room for improvement in this validation step.
- No comparison is made to using a large model without validation, so it's unclear whether gains come from validation or model scale.

## Confidence
- **Answer Validation Claims**: High - Strong human-model agreement and ablation results support this.
- **Learning-Objective Alignment Claims**: Medium - Lower inter-rater agreement and lack of detail on LO encoding consistency reduce confidence.
- **Novelty of Framework**: Medium - Similar incremental prompting exists, but integration with small models and strict validation thresholds is distinctive.

## Next Checks
1. Implement a control experiment using a large model (e.g., Gemini-2.5-Pro) with the same incremental generation pipeline but without validation, to isolate the effect of model size versus validation rigor.
2. Conduct an inter-annotator reliability study on a subset of questions to confirm that the observed kappa values are not inflated by specific rater pairs.
3. Test the impact of varying the answer-confidence threshold (e.g., 0.8, 0.95) on both retention rate and downstream alignment quality to map the trade-off curve.