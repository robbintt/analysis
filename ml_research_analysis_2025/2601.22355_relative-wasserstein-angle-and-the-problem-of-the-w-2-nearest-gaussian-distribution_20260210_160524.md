---
ver: rpa2
title: Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution
arxiv_id: '2601.22355'
source_url: https://arxiv.org/abs/2601.22355
tags:
- gaussian
- angle
- distribution
- wasserstein
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the problem of quantifying deviation from Gaussianity
  using optimal transport theory. The authors introduce two geometric quantities:
  the relative Wasserstein angle and the orthogonal projection distance, which measure
  how far an empirical distribution deviates from Gaussianity in the relative translation
  invariant Wasserstein space.'
---

# Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution

## Quick Facts
- arXiv ID: 2601.22355
- Source URL: https://arxiv.org/abs/2601.22355
- Reference count: 40
- This paper introduces geometric quantities to measure deviation from Gaussianity using optimal transport theory and demonstrates that the commonly used moment-matching Gaussian is not the W2-nearest Gaussian approximation for non-Gaussian distributions.

## Executive Summary
This paper addresses the problem of quantifying deviation from Gaussianity in empirical distributions using optimal transport theory. The authors introduce two geometric quantities - the relative Wasserstein angle and the orthogonal projection distance - to measure how far an empirical distribution deviates from Gaussianity in the relative translation invariant Wasserstein space. They prove that the filling cone between two rays in this space is flat, ensuring that angles, projections, and inner products are well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone. The authors derive closed-form expressions for these quantities in one dimension and develop an efficient stochastic manifold optimization algorithm for high-dimensional settings.

## Method Summary
The method introduces a geometric framework for measuring Gaussianity using the relative translation invariant Wasserstein space (RW2). The key innovation is defining the relative Wasserstein angle and projection distance as measures of deviation from Gaussianity. For one-dimensional cases, closed-form expressions are derived for various distribution families. For high-dimensional cases, a stochastic manifold optimization algorithm is developed that leverages the rotation invariance property of the Gaussian distribution. The algorithm optimizes over the special orthogonal group to find the nearest Gaussian approximation in the W2 sense. The method is evaluated through experiments comparing the robustness of the relative Wasserstein angle versus Wasserstein distance, and demonstrating that the proposed nearest Gaussian provides better approximations than moment matching for evaluating FID scores.

## Key Results
- The filling cone between two rays in the RW2 space is proven to be flat, ensuring well-defined angles and projections
- Closed-form expressions for relative Wasserstein angle and projection distance are derived for 1D location-scale families (Gaussian, uniform, Laplace, logistic)
- A stochastic manifold optimization algorithm is developed for finding the W2-nearest Gaussian in high-dimensional settings
- Experiments show the relative Wasserstein angle is more robust than Wasserstein distance and the proposed nearest Gaussian provides better FID score approximations than moment-matching Gaussian

## Why This Works (Mechanism)
The method works by leveraging the geometric structure of the relative translation invariant Wasserstein space. By proving the filling cone between two rays is flat, the authors establish a well-defined geometry where angles and projections can be meaningfully computed. The relative Wasserstein angle captures the "angle" between an empirical distribution and the Gaussian cone, while the projection distance measures how far the distribution is from its nearest Gaussian approximation. This geometric interpretation allows Gaussian approximation to be reformulated as an optimization problem over the special orthogonal group, exploiting the rotation invariance property of Gaussian distributions.

## Foundational Learning

**Relative Translation Invariant Wasserstein Space (RW2)**: A modification of the standard Wasserstein space that removes translation effects, making it suitable for measuring shape differences between distributions. Why needed: Standard Wasserstein distance conflates location and shape differences, making it unsuitable for measuring pure distributional deviation from Gaussianity. Quick check: Verify that RW2 distance between two identical distributions up to translation is zero.

**Filling Cone Geometry**: The geometric structure formed by all distributions lying on geodesics between two rays in the Wasserstein space. Why needed: Establishes whether angles and projections are well-defined in the Wasserstein space. Quick check: Confirm that the filling cone construction is consistent for the two-ray case as proven in Theorem 3.2.

**Location-Scale Families**: Distributions parametrized by location (mean) and scale (variance) parameters. Why needed: These families have tractable geometry in the Wasserstein space, allowing for closed-form solutions. Quick check: Verify that the projection onto the Gaussian ray for a uniform distribution follows the derived 1D formula.

## Architecture Onboarding

**Component Map**: Empirical Distribution → RW2 Embedding → Relative Wasserstein Angle + Projection Distance → Nearest Gaussian Approximation (via manifold optimization)

**Critical Path**: The optimization over the special orthogonal group is the computational bottleneck, as it requires iteratively updating rotation matrices to minimize the projection distance.

**Design Tradeoffs**: The method trades computational complexity for geometric interpretability - closed-form solutions exist only for 1D cases, while high-dimensional cases require iterative optimization.

**Failure Signatures**: Poor convergence of the manifold optimization algorithm may indicate ill-conditioned distributions or numerical instability in high dimensions.

**First Experiments**: 1) Compute relative Wasserstein angle between a Gaussian and a non-Gaussian distribution; 2) Verify closed-form projection distance for a uniform distribution in 1D; 3) Test convergence of the manifold optimization algorithm on a simple 2D non-Gaussian distribution.

## Open Questions the Paper Calls Out

### Open Question 1
How can the relative Wasserstein angle and projection distance be computed for distribution families that are not parametrized solely by mean and scale? The paper only derives closed-form solutions for location-scale families, and the problem for other families corresponds to projecting onto more general curves, which is left for future investigation.

### Open Question 2
Can a consistent "filling cone" geometry be constructed for three or more rays in the RW2 space? The paper demonstrates that the filling-cone construction cannot be extended to three rays in general due to inconsistent intermediate distributions.

### Open Question 3
Are there specific high-dimensional distribution classes, beyond the coordinate-separable case, where the W2-nearest Gaussian admits an analytical solution? The paper notes that analytic solutions are unlikely to exist in general for high dimensions but leaves open the possibility for special structures.

## Limitations

- Closed-form expressions are only available for one-dimensional cases, with high-dimensional cases requiring iterative optimization
- The manifold optimization algorithm's convergence properties and computational complexity are not thoroughly analyzed
- Experiments focus on specific datasets and may not generalize to all types of non-Gaussian distributions
- Comparison with moment-matching Gaussian approximations is limited to FID score evaluation

## Confidence

- **High confidence**: The theoretical framework and geometric interpretation of Gaussian approximation as a projection problem onto the Gaussian cone
- **Medium confidence**: The closed-form expressions for one-dimensional cases and the proposed stochastic manifold optimization algorithm for high-dimensional settings
- **Low confidence**: The generalization of results to all types of non-Gaussian distributions and the comparison with moment-matching Gaussian approximations in terms of FID scores

## Next Checks

1. Conduct experiments on a wider range of datasets and non-Gaussian distributions to assess the robustness and generalization of the proposed method
2. Perform a thorough analysis of the convergence properties and computational complexity of the stochastic manifold optimization algorithm for high-dimensional settings
3. Compare the proposed nearest Gaussian approximation with other metrics and applications beyond FID score evaluation to demonstrate its effectiveness and limitations