---
ver: rpa2
title: 'Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness
  and Knowledge Graphs'
arxiv_id: '2509.09522'
source_url: https://arxiv.org/abs/2509.09522
tags:
- semantic
- title
- graph
- https
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of matching job titles in resume
  recommendation systems by leveraging semantic textual relatedness (STR) and knowledge
  graphs. The authors propose a hybrid architecture combining fine-tuned SBERT embeddings
  with domain-specific knowledge graphs to improve both semantic alignment and explainability.
---

# Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2509.09522
- **Source URL:** https://arxiv.org/abs/2509.09522
- **Reference count:** 40
- **Primary result:** Fine-tuned SBERT models augmented with knowledge graphs produce consistent improvements in the high-STR region, reducing RMSE by 25% over strong baselines.

## Executive Summary
This paper addresses the challenge of matching job titles in resume recommendation systems by leveraging semantic textual relatedness (STR) and knowledge graphs. The authors propose a hybrid architecture combining fine-tuned SBERT embeddings with domain-specific knowledge graphs to improve both semantic alignment and explainability. The evaluation uses a stratified approach, partitioning STR scores into low, medium, and high regions to enable fine-grained analysis. The results show that fine-tuned SBERT models augmented with knowledge graphs produce consistent improvements in the high-STR region, reducing RMSE by 25% over strong baselines. The study highlights the importance of regional performance analysis and demonstrates the benefits of combining knowledge graphs with text embeddings for explainable job title matching.

## Method Summary
The method combines SBERT embeddings with knowledge graph integration for job title matching. It uses a self-supervised training pipeline that generates training pairs from cosine similarities between job descriptions, eliminating the need for manual labeling. The approach includes BART summarization of job descriptions, SBERT encoding, pairwise cosine similarity computation, and fine-tuning with cosine similarity loss. A bipartite job-skill graph is constructed from the ESCO taxonomy and processed through RGCN to learn node embeddings. An MLP alignment layer maps SBERT embeddings to the knowledge graph space. The evaluation uses stratified RMSE across three STR regions (low: 0.0-0.5, medium: 0.5-0.75, high: 0.75-1.0) with paired t-tests for regional differences.

## Key Results
- Fine-tuned SBERT models augmented with knowledge graphs reduce high-STR RMSE by 25% compared to text-only baselines
- Stratified evaluation reveals distinct performance patterns across semantic regions
- MPNET+RGCN achieves lowest high-STR RMSE (0.11) vs. MPNET-F (0.18) and MPNET (0.44)
- Low-STR performance favors text-only models, while high-STR performance favors hybrid approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-supervised training pairs generated from cosine similarity can substitute for manually labeled relatedness scores in job title matching.
- **Mechism:** SBERT encodes job descriptions → pairwise cosine similarities computed → top-k similar pairs become pseudo-labels for STR training.
- **Core assumption:** Cosine similarity between full job description embeddings is a reasonable proxy for job title semantic relatedness.
- **Evidence anchors:**
  - [abstract] "self-supervised data pipeline that eliminates the need for manually labeled similarity scores by generating training pairs from cosine similarities between job descriptions"
  - [section 3.2, Algorithm 1 Steps 2-3] Explicitly describes pairwise similarity computation and dataset construction
  - [corpus] Weak direct evidence; corpus neighbors focus on supervised/contrastive approaches rather than self-supervised pseudo-labeling
- **Break condition:** If job descriptions are noisy, boilerplate-heavy, or poorly aligned with titles, pseudo-labels will introduce systematic noise—especially in low-STR pairs where the paper acknowledges "weak supervision introduces potential label noise."

### Mechanism 2
- **Claim:** Knowledge graph augmentation selectively improves performance in high-STR regions by encoding structured skill relationships.
- **Mechanism:** Bipartite job-skill graph constructed from ESCO taxonomy → RGCN learns node embeddings → MLP maps SBERT embeddings to KG space → cosine similarity in unified space.
- **Core assumption:** Structured skill ontologies capture hierarchical and functional relationships (e.g., career progressions) that dense text embeddings miss.
- **Evidence anchors:**
  - [abstract] "fine-tuned SBERT models augmented with KGs produce consistent improvements in the high-STR region, where the RMSE is reduced by 25%"
  - [Table 2] MPNET+RGCN achieves lowest high-STR RMSE (0.11) vs. MPNET-F (0.18) and MPNET (0.44)
  - [corpus] "Multilingual JobBERT" and "TalentCLEF 2025" papers show related contrastive approaches but do not test KG integration directly
- **Break condition:** If skill taxonomy is incomplete, outdated, or misaligned with target job domain, graph edges will mislead rather than guide—particularly for emerging roles or cross-domain positions.

### Mechanism 3
- **Claim:** Stratified regional evaluation (low/medium/high STR) reveals model behavior patterns that global RMSE obscures.
- **Mechanism:** Partition STR continuum into [0.0–0.50), [0.50–0.75), [0.75–1.0] → compute region-specific RMSE → paired t-tests across regions.
- **Core assumption:** Different STR regions represent distinct semantic challenges requiring different model capabilities.
- **Evidence anchors:**
  - [abstract] "stratified evaluation enables a fine-grained analysis of model performance across semantically meaningful subspaces"
  - [Section 4.1] Shows MPNET excels in low-STR (t=-24.21) while MPNET-RGCN excels in high-STR (t=23.77)
  - [corpus] No direct comparable stratified evaluation found in neighbors; papers typically report aggregate metrics
- **Break condition:** If regional thresholds (0.5, 0.75) are poorly calibrated to the target domain, stratification will misdiagnose model strengths/weaknesses.

## Foundational Learning

- **Concept: Semantic Textual Relatedness (STR) vs. Similarity (STS)**
  - Why needed here: STR captures associative relationships (e.g., "hand"–"glove") beyond surface synonyms; critical for matching functionally related but lexically distant job titles like "CEO"–"Managing Director."
  - Quick check question: Would "Python Developer" and "Data Engineer" be more STR-related or STS-similar?

- **Concept: Knowledge Graph Embeddings (RGCN)**
  - Why needed here: Relational Graph Convolutional Networks encode heterogeneous edge types in bipartite job-skill graphs, enabling structured reasoning paths for explainability.
  - Quick check question: How does RGCN differ from standard GCN in handling multiple relation types?

- **Concept: Self-Supervised Learning from Pairwise Similarities**
  - Why needed here: Enables training without manual annotations by treating cosine similarity scores as soft labels; essential for scaling to large job corpora.
  - Quick check question: What failure mode arises when pseudo-labels are derived from noisy embeddings?

## Architecture Onboarding

- **Component map:** Raw job titles + descriptions (Kaggle dataset) -> BART summarization -> SBERT encoding -> pairwise cosine similarity computation -> STR pair generator -> bipartite job-skill graph (ESCO) -> RGCN embeddings -> MLP alignment -> cosine similarity in unified space

- **Critical path:** The MLP alignment layer determines whether text embeddings successfully leverage graph structure; poor alignment negates KG benefits.

- **Design tradeoffs:**
  - Text-only models (MPNET-F): Lower complexity, better low-STR discrimination, no explainability
  - Hybrid (MPNET+RGCN): Higher complexity, superior high-STR precision, full explainability via skill paths
  - Skill pruning threshold (>20% job share removal): Improves explanation specificity but may lose coverage for cross-functional roles

- **Failure signatures:**
  - Generic skills dominating explanations (specificity ≈ 0.0) → skill pruning ineffective
  - High error variance in medium-STR band (0.50–0.75) → model struggles with borderline cases
  - Low-STR RMSE spike in KG-augmented models → graph noise overwhelming text signal

- **First 3 experiments:**
  1. **Baseline comparison:** Evaluate pre-trained MPNET vs. fine-tuned MPNET-F vs. MPNET+RGCN on held-out job title pairs; report per-region RMSE and paired t-tests.
  2. **Skill pruning ablation:** Vary skill job-share threshold (10%, 20%, 30%) and measure impact on explanation specificity score and high-STR RMSE.
  3. **Alignment layer diagnosis:** Visualize SBERT→KG embedding space with t-SNE before/after MLP alignment; quantify improvement using silhouette score on known job clusters.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach depends on the completeness and currency of the ESCO taxonomy, limiting applicability to emerging roles or cross-domain positions
- The MLP alignment architecture is underspecified, making faithful reproduction uncertain
- Regional evaluation thresholds (0.5, 0.75) are fixed without calibration to the target domain, risking misclassification of borderline cases

## Confidence

- **High confidence**: SBERT fine-tuning with cosine similarity loss improves semantic alignment; stratified evaluation reveals performance patterns; low-STR models outperform in discrimination tasks.
- **Medium confidence**: Knowledge graph augmentation improves high-STR matching; skill pruning enhances explanation specificity; MLP alignment layer successfully bridges text and graph embeddings.
- **Low confidence**: Absolute RMSE values; exact hyperparameter choices for R-GCN and MLP; generalizability to non-European job markets.

## Next Checks
1. **Cross-domain robustness test**: Apply the pipeline to a US-centric job dataset (e.g., LinkedIn or Indeed data) and compare high-STR RMSE degradation; quantify impact of taxonomy mismatch.
2. **Skill pruning ablation study**: Systematically vary the job-share threshold (10%, 20%, 30%) and measure trade-offs between explanation specificity and high-STR RMSE; plot Pareto frontier.
3. **Alignment layer ablation**: Remove the MLP alignment and evaluate high-STR RMSE; if KG benefits disappear, this confirms the alignment layer is the critical integration mechanism.