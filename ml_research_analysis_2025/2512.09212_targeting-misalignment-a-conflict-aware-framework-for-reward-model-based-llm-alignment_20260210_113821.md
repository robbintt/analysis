---
ver: rpa2
title: 'Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based
  LLM Alignment'
arxiv_id: '2512.09212'
source_url: https://arxiv.org/abs/2512.09212
tags:
- reward
- base
- proxy
- alignment
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward-model misalignment in LLM alignment,
  where imperfect proxy rewards lead to unintended model behavior. The authors propose
  a conflict-aware framework that treats fine-tuning as knowledge integration, identifying
  QA pairs where the base policy and proxy reward strongly disagree (proxy-policy
  conflicts).
---

# Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment

## Quick Facts
- arXiv ID: 2512.09212
- Source URL: https://arxiv.org/abs/2512.09212
- Reference count: 12
- Key outcome: Targeting high-conflict QA pairs via PACS improves alignment efficiency, with optimal thresholds δ=1.6 (safety) and δ=1.5 (helpfulness)

## Executive Summary
This paper addresses reward-model misalignment in LLM alignment by proposing a conflict-aware framework that treats fine-tuning as knowledge integration. The authors introduce PACS (proxy-policy conflict score) and Kendall-Tau Distance to detect disagreements between base policy log-probabilities and proxy reward scores. Their Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) algorithm targets high-conflict examples for human feedback, refining both reward model and policy. Experiments on safety and helpfulness alignment tasks show that targeting high-conflict QA pairs improves alignment performance, with PACS thresholds of 1.6 (safety) and 1.5 (helpfulness) yielding optimal results, demonstrating efficient allocation of human feedback resources.

## Method Summary
The framework detects proxy-policy conflicts where the base model strongly disagrees with the proxy reward, using PACS (local) and Kendall-Tau Distance (global) metrics. SHF-CAS samples N responses per prompt, filters by K-T threshold τ, then selects high-PACS examples for human feedback. The proxy reward is fine-tuned on collected feedback, then used for PPO-based policy optimization. This iterative process continues until feedback budget H is exhausted or no high-conflict examples remain.

## Key Results
- SHF-CAS with δ=1.6 achieves gold reward -2.90 on safety task vs. PPO baseline 3.92
- SHF-CAS with δ=1.5 achieves 3.36 on helpfulness task vs. baseline -2.32
- Random sampling consistently underperforms SHF-CAS across both tasks
- Higher PACS thresholds select fewer but more impactful examples

## Why This Works (Mechanism)

### Mechanism 1: Proxy-Policy Conflict Detection via PACS
PACS normalizes both reward scores and log-probabilities to a common scale, computing absolute divergence to surface QA pairs where models disagree. High PACS values indicate regions of shared ignorance or complementary knowledge. The paper empirically finds optimal PACS thresholds of δ=1.6 (safety) and δ=1.5 (helpfulness).

### Mechanism 2: Global Ranking Alignment via Kendall-Tau Distance
Beyond point-wise disagreement, Kendall-Tau Distance quantifies global ranking mismatch between policy and reward preferences. For each prompt, N completions are ranked by both policy probability and reward score, with K-T measuring concordant vs. discordant pairs. This captures systematic ranking inversions across response distributions.

### Mechanism 3: Iterative Human-in-the-Loop Feedback via Conflict-Aware Sampling (SHF-CAS)
SHF-CAS selectively targets high-conflict QA pairs for human feedback, concentrating supervision where both models are uncertain. The algorithm samples responses, filters by K-T threshold τ, then selects QA pairs exceeding PACS threshold δ. Human feedback on these examples refines the proxy reward, which then guides policy optimization.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - Why needed: The reward model is trained via pairwise preference comparisons using BT model p*(y₁≻y₂|x) = exp(r(x,y₁))/(exp(r(x,y₁))+exp(r(x,y₂)))
  - Quick check: Given r(x,y₁)=2.0 and r(x,y₂)=0.5, what probability does BT assign to preferring y₁ over y₂?

- **Concept: KL-Regularized Policy Optimization**
  - Why needed: The RL objective balances reward maximization with KL divergence from base policy: max E[r(x,y)] - β·D_KL[π_θ||π_base]
  - Quick check: If β→∞, what policy does the optimal π_θ approach?

- **Concept: Reward-Probability Duality**
  - Why needed: Optimal policy under reward r satisfies π_r(y|x) ∝ exp(r(x,y)), motivating PACS as comparison between log-probability and reward
  - Quick check: If a reward function gives scores [1, 3, 6] for three responses, what relative probabilities should an optimal policy assign?

## Architecture Onboarding

- **Component map:** Base policy (π_base) -> Conflict detector (PACS/K-T) -> Human feedback interface -> Reward refiner -> Policy optimizer -> Base policy
- **Critical path:** (1) Sample N responses per prompt from π_base → (2) Compute K-T Distance, filter by threshold τ → (3) Compute PACS for remaining QA pairs, filter by δ → (4) Collect human feedback on top-H conflicts → (5) Fine-tune r_proxy → (6) Fine-tune π_base via PPO → (7) Iterate until budget exhausted
- **Design tradeoffs:** PACS threshold δ (higher = fewer examples), K-T threshold τ (lower = more prompts), N samples per prompt (more = better estimates), feedback budget H (larger = better alignment)
- **Failure signatures:** False positive conflicts, shared ignorance not resolved, base model degradation, threshold mismatch
- **First 3 experiments:**
  1. Threshold sweep: Run SHF-CAS with δ ∈ {1.0, 1.2, 1.4, 1.5, 1.6, 1.8} on validation set
  2. Ablation: Random vs. Conflict-aware sampling with matched sample sizes
  3. Iterative refinement test: Run SHF-CAS for I ∈ {1, 2, 3} iterations with fixed budget

## Open Questions the Paper Calls Out

- **Open Question 1:** Can proxy-policy conflicts be programmatically classified as "complementary knowledge" versus "shared ignorance" without external human review?
  - Basis: The authors treat all high-conflict pairs as candidates for external supervision due to lack of automated classification mechanism
  - What evidence would resolve: A classifier predicting conflict source with high accuracy

- **Open Question 2:** Do the optimal PACS thresholds (δ) generalize across varying model architectures, parameter scales, and domains?
  - Basis: Different optimal thresholds for safety (1.6) vs. helpfulness (1.5) raise questions about universality
  - What evidence would resolve: Consistent optimal threshold ranges across distinct model families and parameter counts

- **Open Question 3:** How does the reliability of the PACS metric degrade when the "strong base model" assumption is violated?
  - Basis: The framework assumes base policy behaviors in "agreement" regions are trustworthy
  - What evidence would resolve: Ablation study using a deliberately weakened base model

## Limitations
- PACS thresholds (1.6, 1.5) are empirically determined but lack theoretical justification for why these specific values work best
- Evaluation relies on simulated human feedback via GPT-4o rather than real human annotation
- Framework assumes base policy provides trustworthy priors, which may not hold for misaligned base models

## Confidence

- **High confidence:** Framework architecture is internally consistent and well-specified
- **Medium confidence:** Experimental results showing SHF-CAS outperforms random sampling
- **Low confidence:** Claims about PACS thresholds being optimal without broader validation

## Next Checks

1. **Cross-task generalization test:** Apply SHF-CAS with reported optimal thresholds to a held-out third alignment task and verify performance benefits persist
2. **Human vs. GPT-4o supervision comparison:** Run identical SHF-CAS experiments with real human feedback versus GPT-4o oracle feedback, measuring both alignment performance and feedback consistency
3. **Conflict detection ablation:** Systematically vary PACS thresholds from 0.5 to 2.0 and measure false positive/negative rates using a manually annotated validation set with known alignment issues