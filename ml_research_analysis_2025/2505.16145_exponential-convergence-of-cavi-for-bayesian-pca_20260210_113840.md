---
ver: rpa2
title: Exponential Convergence of CAVI for Bayesian PCA
arxiv_id: '2505.16145'
source_url: https://arxiv.org/abs/2505.16145
tags:
- where
- have
- matrix
- convergence
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes exponential convergence of coordinate ascent
  variational inference (CAVI) for Bayesian principal component analysis (BPCA) with
  mean-field variational approximation. The authors first prove precise exponential
  convergence for the single-component case (k=1) by establishing a novel connection
  to the classical power iteration algorithm, showing that traditional PCA is recovered
  as point estimates of BPCA parameters.
---

# Exponential Convergence of CAVI for Bayesian PCA

## Quick Facts
- arXiv ID: 2505.16145
- Source URL: https://arxiv.org/abs/2505.16145
- Reference count: 40
- Primary result: Establishes exponential convergence of CAVI for Bayesian PCA with mean-field approximation, showing traditional PCA is recovered as point estimates

## Executive Summary
This paper proves exponential convergence of coordinate ascent variational inference (CAVI) for Bayesian principal component analysis (BPCA) with mean-field variational approximation. The authors establish precise exponential convergence for the single-component case (k=1) by connecting CAVI to the classical power iteration algorithm, showing that traditional PCA is recovered as point estimates of BPCA parameters. For the general case with any number of components, they leverage recent abstract tools to prove exponential convergence, introducing a novel lower bound on the symmetric KL divergence between multivariate normal distributions that may be of independent interest.

## Method Summary
The method implements CAVI with sequential block updates for Bayesian PCA. The model assumes data matrix X ∈ ℝ^(n×d) with x_i = Wz_i + τ₀^(-1/2)ε_i, where W is a d×k loading matrix and Z is an n×k latent variable matrix. The algorithm alternates between optimizing variational distributions q_W and q_Z using closed-form matrix normal updates. For k=1, the directional updates converge exponentially to the leading principal component with rate determined by the eigenvalue gap. For k≥1, local exponential contraction is established when certain regularity conditions hold, particularly that the loss function Hessian is non-singular at stationary points.

## Key Results
- k=1 case: CAVI directional updates converge exponentially to the leading principal component with rate (1 - ρ₁/λ₁)^t
- General k case: Local exponential contraction KL(q_Z^(t+1)||q*_Z) ≤ C_Z κ^t when generalized correlation GCorr_{1/2}(r₀) ∈ (0, 2)
- Novel lower bound on symmetric KL divergence between multivariate normals enables uniform control of convergence constants
- Traditional PCA is retrieved as point estimates of BPCA parameters in the k=1 case

## Why This Works (Mechanism)

### Mechanism 1: Power Iteration Equivalence (k=1 Case)
- Claim: For single-component BPCA, CAVI directional updates converge exponentially to the leading principal component with rate (1 - ρ₁/λ₁)^t
- Mechanism: Normalized iterates μ_Z^(t)/||μ_Z^(t)|| follow the classical power iteration algorithm (XX')^t μ_Z^(0)/||(XX')^t μ_Z^(0)||, which converges to the leading eigenvector μ₁ of XX'. The eigenvalue gap ρ₁ = λ₁ - λ₂ determines convergence speed.
- Core assumption: Data matrix X has full rank with strictly ordered eigenvalues (λ₁ > λ₂ > ... > λ_d), and initialization has non-zero component c₁ along μ₁ (probability 1 for random initialization)
- Break condition: If c₁ = 0 (initialization orthogonal to μ₁), iterates converge to a different eigenvector μ_i where i = min{j : c_j ≠ 0}

### Mechanism 2: Generalized Correlation Contraction (k ≥ 1 Case)
- Claim: CAVI achieves local exponential contraction KL(q_Z^(t+1)||q*_Z) ≤ C_Z κ^t when generalized correlation GCorr_{1/2}(r₀) ∈ (0, 2)
- Mechanism: The Δ_q* term measures posterior covariance between W and Z blocks. When small relative to symmetric KL divergences D_KL,1/2, the blocks are nearly decoupled and CAVI behaves like independent coordinate descent on each block.
- Core assumption: Stationary point q* exists in set I (satisfying condition (16) with τ₀||Σ*||^(1/2)||Σ*_Z||^(1/2)||X|| + 2||μ*_W||·||μ*_Z|| < γ₀). Hessian non-singular at stationary points (requires Λ ∝̸ I_k when k ≥ 2)
- Break condition: If Λ ∝ I_k, rotational ambiguity creates continuous manifold of equivalent stationary points, Hessian becomes singular, and Theorem 2 guarantees fail

### Mechanism 3: Symmetric KL Lower Bound for Uniform Control
- Claim: D_KL,1/2(q_W||q*_W) ≥ L*²_W||Σ_W - Σ*_W||² + M*²_W||μ_W - μ*_W||² enables ratio cancellation in generalized correlation
- Mechanism: The lower bound produces product terms like ||μ_W - μ*_W||·||μ_Z - μ*_Z|| that match the numerator structure |Δ_q*|, allowing the ratio GCorr_{1/2}(r₀) to be bounded uniformly independent of iteration parameters
- Core assumption: Variational iterates remain within KL balls B*_W(r₀) and B*_Z(r₀); r₀ sufficiently small so constants L*_W, M*_W remain bounded
- Break condition: If r₀ too large (initialization far from q*), lower bound constants degrade and condition (16) verification may fail

## Foundational Learning

- **Concept: Mean-Field Variational Inference**
  - Why needed here: CAVI alternates between optimizing q_W and q_Z blocks while holding the other fixed; understanding why ELBO increase guarantees KL decrease is essential
  - Quick check question: Why does minimizing KL(q||π) via coordinate ascent on blocks q_W, q_Z guarantee monotonic ELBO improvement?

- **Concept: Matrix Normal Distribution**
  - Why needed here: Variational distributions have structured covariances (I_d ⊗ Σ_W) implying row independence; this structure arises automatically from updates and simplifies KL computations
  - Quick check question: If W ~ N(μ_W, I_d ⊗ Σ_W), what is the covariance structure across rows versus columns?

- **Concept: Power Iteration Convergence**
  - Why needed here: The k=1 analysis directly maps CAVI to power iteration; convergence rate depends on eigenvalue gap λ₁ - λ₂ relative to λ₁
  - Quick check question: Why does power iteration converge to the leading eigenvector, and what happens if initialization is orthogonal to it?

## Architecture Onboarding

- **Component map:**
  - μ_W (d×k), Σ_W (k×k): Mean and row covariance for loading matrix W
  - μ_Z (n×k), Σ_Z (k×k): Mean and row covariance for latent variables Z
  - τ₀: Fixed noise precision (assumed known)
  - Λ: Diagonal prior precision (MUST be non-identity when k ≥ 2)
  - ELBO: Monitored quantity (KL + log p(X))

- **Critical path:**
  1. Initialize q_Z^(0) = N(μ_Z^(0), I_n ⊗ Σ_Z^(0)) with μ_Z^(0) ≠ 0 (random initialization)
  2. W-update: Σ_W^(t+1) = (τ₀(nΣ_Z^(t) + (μ_Z^(t))'μ_Z^(t)) + Λ)^(-1), μ_W^(t+1) = τ₀X'μ_Z^(t)Σ_W^(t+1)
  3. Z-update: Σ_Z^(t+1) = (τ₀(dΣ_W^(t+1) + (μ_W^(t+1))'μ_W^(t+1)) + I_k)^(-1), μ_Z^(t+1) = τ₀Xμ_W^(t+1)Σ_Z^(t+1)
  4. Compute ELBO; terminate when relative increase < ε

- **Design tradeoffs:**
  - k=1: Precise characterization + orthogonality guaranteed, but limited to single PC
  - k>1: General applicability, but requires verifying condition (16) and no orthogonality (BPCA ≠ traditional PCA point estimates)
  - Λ selection: Non-proportional to I_k avoids singularity but requires tuning

- **Failure signatures:**
  - μ_Z^(0) = 0 → trivial fixed point (all subsequent means = 0)
  - Λ ∝ I_k when k ≥ 2 → manifold of equivalent stationary points, Hessian singular
  - Large τ₀||Σ_W||^(1/2)||Σ_Z||^(1/2)||X|| relative to γ₀ → condition (16) violated, no convergence guarantee

- **First 3 experiments:**
  1. k=1 directional convergence: Generate synthetic X with known μ₁; verify ||μ_Z^(t)/||μ_Z^(t)|| - μ₁|| decreases as (1 - ρ₁/λ₁)^t
  2. k=1 fixed point check: Solve polynomial P(u) from Proposition 2; confirm (||μ_Z^(t)||, Σ_Z^(t)) → (a*, b*); compute Jacobian eigenvalues at fixed point
  3. Hessian singularity (k=2): Compare Λ = I_2 vs. Λ = diag(1,2); verify zero eigenvalue in first case, non-singular Hessian in second case using Newton refinement after CAVI

## Open Questions the Paper Calls Out

- Can the non-singularity of the loss function Hessian at stationary points be theoretically proven for cases where k > 1?
  - Basis: The authors state that theoretically proving non-singularity is "infeasible using standard tools due to its complexity" and leave it beyond the scope of the paper.
  - Why unresolved: The complexity of the loss function derivation makes standard verification tools ineffective for theoretical guarantees.
  - What evidence would resolve it: A rigorous mathematical proof confirming the Hessian is non-singular under specific prior conditions, such as when Λ is not proportional to the identity matrix.

- Can exponential convergence of CAVI be established without relying on the assumption of a non-singular Hessian?
  - Basis: The authors suggest that Assumption 2 is likely unnecessary but would require "a significantly different proof technique" involving direct trajectory analysis.
  - Why unresolved: Current abstract tools used in the paper rely on this regularity condition to ensure stationary points are isolated.
  - What evidence would resolve it: A convergence proof derived from analyzing CAVI trajectories directly, rather than depending on local contraction mapping frameworks.

- Does exponential convergence hold when the number of principal components k is learned via model selection rather than fixed?
  - Basis: The authors list establishing convergence for learned k as a "possible extension of our work."
  - Why unresolved: The theoretical results in the paper rely on k being a fixed hyperparameter known in advance.
  - What evidence would resolve it: Extending the convergence analysis to variational inference procedures that treat k as a random variable or model selection target.

## Limitations

- Condition (16) verification for general k case requires careful parameter tuning; the bound may be violated for poorly scaled data or inappropriate Λ choices
- Fixed point characterization for k=1 relies on numerical solution of polynomial P(u); analytical guarantees are limited to convergence within neighborhood
- The symmetric KL lower bound for multivariate normals is novel and requires empirical validation of the constants c₂, c₃ across different dimensionalities

## Confidence

- High confidence: k=1 exponential convergence via power iteration equivalence (direct connection to well-established theory)
- Medium confidence: General k exponential convergence (relies on verifying condition (16) which depends on problem-specific parameters)
- Medium confidence: Symmetric KL lower bound utility (novel theoretical contribution but constants may degrade for certain parameter regimes)

## Next Checks

1. **Condition (16) verification**: Systematically vary τ₀, Λ, and initialization radius r₀ to identify parameter regimes where the condition fails; measure impact on convergence behavior
2. **Fixed point stability analysis**: Compute the Jacobian at the k=1 fixed point (a*, b*) for synthetic data with varying eigenvalue gaps λ₁ - λ₂; verify that the largest eigenvalue magnitude remains bounded by 1 - ρ₁/λ₁
3. **Symmetric KL bound robustness**: Test the lower bound constants c₂, c₃ across different dimensions (d, k) and KL ball radii r₀; identify when the bound becomes vacuous and affects generalized correlation guarantees