---
ver: rpa2
title: 'Your thoughts tell who you are: Characterize the reasoning patterns of LRMs'
arxiv_id: '2509.24147'
source_url: https://arxiv.org/abs/2509.24147
tags:
- reasoning
- behavior
- output
- taxonomy
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOT, an inductive method that constructs
  human-readable taxonomies of reasoning features to characterize how large reasoning
  models (LRMs) think. LOT uses a generative language model to compare reasoning traces
  from two LRMs, identifying distinctive reasoning traits in natural language, then
  builds classifiers to predict the source model based on these features.
---

# Your thoughts tell who you are: Characterize the reasoning patterns of LRMs

## Quick Facts
- arXiv ID: 2509.24147
- Source URL: https://arxiv.org/abs/2509.24147
- Reference count: 32
- Key outcome: LOT inductively discovers reasoning features that enable 80-100% classification accuracy between LRMs and improves accuracy by 3.3-5.7% via test-time reasoning style alignment.

## Executive Summary
This paper introduces LOT, an inductive method that constructs human-readable taxonomies of reasoning features to characterize how large reasoning models (LRMs) think. LOT uses a generative language model to compare reasoning traces from two LRMs, identifying distinctive reasoning traits in natural language, then builds classifiers to predict the source model based on these features. The method is iterated to refine the taxonomy until reliable classification is achieved. LOT is applied to classify reasoning traces from 12 open-source LRMs across math, science, and coding tasks, achieving 80-100% accuracy in distinguishing LRMs that differ in scale, base model family, or objective domain.

## Method Summary
LOT is a three-stage iterative method for discovering distinguishing reasoning features from LRMs. First, a generative LLM (Llama3.3-70B-Instruct) compares reasoning traces from two LRMs on the same question to propose distinguishing features in natural language. Second, these features are encoded as binary (PoR) or frequency-based (BoR) vectors using an expanding taxonomy. Third, a logistic regression classifier predicts the source model. When classification fails, the LLM proposes new features, expanding the taxonomy iteratively until convergence (20 iterations without change). The method achieves 80-100% accuracy in distinguishing LRMs across different scales, base families, and domains.

## Key Results
- LOT achieves 80-100% classification accuracy between LRMs differing in scale, base family, or objective domain
- Smaller models exhibit more circular reasoning; domain-specialized models show reasoning inertia (e.g., coding-style reasoning for math)
- Test-time reasoning style alignment improves GPQA accuracy by 3.3-5.7% for smaller Qwen3 models
- BoR encoding outperforms PoR by 3-14% by capturing feature frequency
- LOT outperforms few-shot prompting, VML, and human-defined taxonomy baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An LLM can inductively discover distinguishing reasoning features from paired traces that enable accurate source model classification.
- **Mechanism:** LOT uses a generative LLM to compare two reasoning traces on the same question, articulate differences in natural language, and convert these into binary/frequency feature vectors. A logistic classifier trained on these vectors predicts source identity. When classification fails, the LLM proposes new features, expanding the taxonomy iteratively until convergence.
- **Core assumption:** Distinguishing features are consistently present across traces from the same model family, scale, or domain; the LLM annotator can reliably detect and verbalize them.
- **Evidence anchors:** [abstract] "LOT uses a generative language model to compare reasoning traces from two LRMs and articulate their distinctive features in words." [section 3.1-3.3] Algorithm 1 details the initialization, encoding (PoR/BoR), and iterative update process.
- **Break condition:** Features are model-specific artifacts (e.g., output formatting habits) rather than reasoning behaviors; annotator LLM introduces systematic bias; convergence occurs before discriminative features are found.

### Mechanism 2
- **Claim:** Reasoning patterns systematically differ by model scale, base family, and fine-tuning domain—and these differences are quantifiable.
- **Mechanism:** LOT achieves 80–100% classification accuracy between models differing along these axes. Smaller models exhibit more circular reasoning; domain-specialized models show inertia (e.g., Seed-Coder uses Python for math problems).
- **Core assumption:** Scale, family, and domain leave stable behavioral signatures in reasoning traces.
- **Evidence anchors:** [section 4.1] "Qwen3-32B more reliably recalls problem-relevant knowledge... smaller variants often redundantly evaluate the same information." [section 4.3] "Seed-Coder sometimes borrows its coding-oriented reasoning style for mathematics... in 20% of cases, Seed-Coder goes further by implementing a Python function."
- **Break condition:** Differences are confounded by training data, hyperparameters, or sampling settings not controlled in the study.

### Mechanism 3
- **Claim:** Modifying reasoning behaviors associated with correctness causally improves accuracy.
- **Mechanism:** Odds ratios identify features correlated with correct answers. The intervention pipeline summarizes reasoning, edits the summary to add/remove high/low-odds features, and re-expands into full traces. This improves GPQA accuracy by 3.3–5.7% for smaller Qwen3 models.
- **Core assumption:** The correlation between features and correctness reflects a causal relationship; the edit-and-re-expand process preserves reasoning coherence.
- **Evidence anchors:** [section 5] "Figure 6C shows that the intervention improves the accuracy of Qwen3-0.6B, Qwen3-4B, and Qwen3-8B on GPQA." [section 5] "We link the reasoning differences to performance: aligning the reasoning style of smaller Qwen3 models with that of the largest Qwen3 during test time improves their accuracy on GPQA by 3.3–5.7%."
- **Break condition:** Correlation does not imply causation; re-expansion introduces noise; models cannot reliably follow reasoning-content instructions (Appendix C shows instruction-following failures).

## Foundational Learning

- **Concept: Inductive vs. Deductive Taxonomy Construction**
  - **Why needed here:** LOT contrasts with prior work using fixed, researcher-defined taxonomies. Understanding this distinction clarifies why LOT discovers features like "visualizing molecular structure" that predefined schemes miss.
  - **Quick check question:** Can you explain why a fixed taxonomy might miss domain-specific reasoning inertia?

- **Concept: Presence of Reasoning (PoR) vs. Bag of Reasoning (BoR) Encodings**
  - **Why needed here:** PoR uses binary feature presence; BoR incorporates frequency. BoR consistently outperforms PoR (3–14% improvement), suggesting how often a behavior occurs matters as much as whether it occurs.
  - **Quick check question:** For a long trace with repeated backtracking, which encoding captures more information?

- **Concept: Test-Time Reasoning Intervention**
  - **Why needed here:** The paper demonstrates that reasoning style can be modified at inference time without retraining. The summarize-modify-re-expand pipeline is the key mechanism.
  - **Quick check question:** Why must summarization precede modification for traces exceeding 20K tokens?

## Architecture Onboarding

- **Component map:** Annotator LLM -> Feature Taxonomy -> Encoder -> Classifier -> Update Loop
- **Critical path:** Initialization from one trace pair → annotate batch → train classifier → predict on new pairs → if wrong, propose new features → re-encode → retrain → repeat until convergence (N=20 iterations without change).
- **Design tradeoffs:**
  - **PoR vs. BoR:** BoR is more accurate but requires KNN imputation for missing dimensions during training.
  - **Batch size (40 pairs):** Balances annotation cost against stable classifier updates.
  - **Convergence threshold:** N=20 may stop early if local minimum reached; M=2|D_train| limits total updates.
- **Failure signatures:**
  1. **Low accuracy despite many iterations:** Features may be too generic; annotator not discovering discriminative traits.
  2. **Taxonomy explosion:** Features are not consolidating; annotator generates redundant variations.
  3. **Intervention hurts performance:** Odds ratios from training split don't generalize; re-expansion corrupts reasoning.
- **First 3 experiments:**
  1. **Reproduce Qwen3-0.6B vs. Qwen3-32B classification** on GPQA-Diamond with PoR encoding; verify 80%+ accuracy and inspect top-weighted features.
  2. **Ablate the update loop:** Compare taxonomy initialized from single pair vs. after 5/10/20 updates; measure accuracy gain per iteration.
  3. **Test intervention on held-out dataset:** Apply the summarize-modify-re-expand pipeline to MATH-500; check if gains transfer beyond GPQA or if effects are domain-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do specific meta-attributes (e.g., parameter scale) causally determine reasoning patterns independently of training recipes?
- **Basis in paper:** [explicit] The authors state they "did not establish causal links between the meta-attributes of LRMs (e.g., size) and their reasoning patterns" because the compared models often differ in multiple hidden variables.
- **Why unresolved:** Current open-source models have heterogeneous, non-public training data and procedures, making it impossible to isolate variables like scale or architecture.
- **What evidence would resolve it:** A controlled ablation study training models from scratch where only the target meta-attribute (e.g., size) varies while holding data and training steps constant.

### Open Question 2
- **Question:** How does the choice of the LLM annotator influence the stability and completeness of the generated reasoning taxonomy?
- **Basis in paper:** [explicit] The authors note that "The effects of the LLM annotator on LOT's performance remain underexplored," despite all experiments relying solely on Llama3.3-70B-Instruct.
- **Why unresolved:** Different LLMs possess different inductive biases and instruction-following capabilities, which may cause them to propose different features for the same reasoning traces.
- **What evidence would resolve it:** Running the LOT pipeline on the same reasoning traces using different state-of-the-art annotators (e.g., GPT-4, Claude) and comparing the overlap and divergence of the resulting taxonomies.

### Open Question 3
- **Question:** Can LOT-derived reasoning features be utilized as reward signals to improve model reasoning during training, rather than just for test-time alignment?
- **Basis in paper:** [explicit] The authors suggest that "future work may explore how to leverage the identified reasoning differences in training such as... incorporating them as processed reward for reinforcement learning."
- **Why unresolved:** The paper only demonstrates performance gains (3.3–5.7% on GPQA) through a complex test-time intervention pipeline (summarize-modify-expand), not through weight updates.
- **What evidence would resolve it:** A training run where a reward model is trained to distinguish "expert" reasoning features identified by LOT, used to guide the RL fine-tuning of a smaller model.

## Limitations
- Some reasoning traces exceed 20K tokens, requiring summarization that may lose feature granularity
- The method requires iterative human-in-the-loop refinement, making it computationally expensive compared to fixed taxonomy approaches
- Distinguishing models with similar scale/base can be more challenging, with accuracy dropping below the 80-100% range

## Confidence
- **High confidence:** The LOT method successfully classifies reasoning traces from LRMs differing in scale, base model family, or objective domain (80-100% accuracy demonstrated).
- **Medium confidence:** The causal claim that modifying reasoning behaviors improves accuracy (3.3-5.7% gains on GPQA) is supported but requires validation across additional domains beyond the GPQA case study.
- **Medium confidence:** The inductive feature discovery mechanism works as described, though the convergence threshold (N=20 iterations) and KNN imputation parameters remain underspecified.

## Next Checks
1. Reproduce the Qwen3-0.6B vs. Qwen3-32B classification on GPQA-Diamond with PoR encoding to verify baseline accuracy and inspect feature distributions.
2. Test the intervention pipeline on a held-out domain (e.g., MATH-500) to assess whether reasoning-style alignment transfers beyond GPQA.
3. Run 5 random seeds of the update loop and measure feature taxonomy overlap to assess stability and convergence consistency.