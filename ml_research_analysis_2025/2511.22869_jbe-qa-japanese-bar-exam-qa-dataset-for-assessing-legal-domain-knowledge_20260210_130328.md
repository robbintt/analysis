---
ver: rpa2
title: 'JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge'
arxiv_id: '2511.22869'
source_url: https://arxiv.org/abs/2511.22869
tags:
- legal
- code
- dataset
- japanese
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors created JBE-QA, the first comprehensive Japanese legal
  question-answering dataset that includes the Civil Code, Penal Code, and Constitution.
  They converted multiple-choice bar exam questions into binary classification tasks,
  resulting in 3,464 balanced instances.
---

# JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge

## Quick Facts
- arXiv ID: 2511.22869
- Source URL: https://arxiv.org/abs/2511.22869
- Reference count: 0
- Authors created first comprehensive Japanese legal QA dataset from bar exam questions

## Executive Summary
This paper introduces JBE-QA, the first comprehensive Japanese legal question-answering dataset derived from bar exam materials. The dataset covers the Civil Code, Penal Code, and Constitution, converting multiple-choice questions into binary classification tasks. The authors evaluate 26 large language models across zero-shot and few-shot settings, finding that proprietary reasoning models significantly outperform other model families, while Japanese-specialized models show only marginal improvements.

## Method Summary
The authors converted Japanese bar exam multiple-choice questions into binary classification tasks, creating 3,464 balanced instances. They evaluated 26 LLMs including proprietary, open-weight, Japanese-specialized, and reasoning models using zero-shot and four-shot settings. The evaluation used F1 score as primary metric with faithfulness tracking to measure instruction compliance. Proprietary reasoning models achieved highest performance with F1 scores up to 0.861.

## Key Results
- Proprietary reasoning models achieved F1 scores up to 0.861, significantly outperforming other model families
- Constitution questions were consistently easier than Civil Code or Penal Code questions across all model types
- Japanese-specialized models showed only marginal improvements (less than 0.03 advantage) over base models

## Why This Works (Mechanism)

### Mechanism 1
Enabling reasoning capabilities in LLMs systematically improves legal judgment accuracy on bar exam questions. Reasoning models generate intermediate analytical steps before producing binary classifications, allowing systematic decomposition of legal statements rather than pattern-matching on surface text. This mechanism assumes legal determinations require multi-step application of statutes and precedents that benefit from explicit intermediate reasoning.

### Mechanism 2
Japanese-domain specialization through continual pre-training provides only marginal improvements on legal reasoning tasks. General Japanese language exposure improves surface fluency but does not substantially encode legal domain knowledge; base model's legal reasoning capacity remains the bottleneck. This assumes legal knowledge is largely orthogonal to general language proficiency in the target language.

### Mechanism 3
Few-shot exemplars help proprietary models but can degrade open-weight model performance due to instruction-following failures. High-quality instruction-tuned models extract semantic patterns from exemplars while maintaining format compliance; lower-quality open-weight models instead copy exemplar surface content, producing invalid outputs. This assumes exemplar utility depends on model's ability to separate task structure from exemplar content.

## Foundational Learning

- **Binary classification decomposition of multi-choice questions**
  - Why needed here: Converts complex multiple-choice combinations into independent true/false judgments, enabling granular evaluation and simplified metrics
  - Quick check question: Given a 3-statement question with options 1-8 representing truth combinations, how many binary classification instances does this produce?

- **Legal term non-literal interpretation**
  - Why needed here: Case study shows legal terms like "uninhabited" and "belonging to another" have precedent-defined meanings contradicting literal readings; models relying on surface semantics fail
  - Quick check question: Why does the arson case classify an owner-occupied, owner-owned insured building as "uninhabited belonging to another"?

- **Faithfulness-to-instruction metric**
  - Why needed here: Many models generate explanations despite instructions to output only binary values; unfaithful outputs default to 0, artificially lowering F1 scores
  - Quick check question: A model outputs "1. Explanation: According to case law..."—is this faithful, and how does it affect scoring?

## Architecture Onboarding

- **Component map**: PDF→XML extraction → field segmentation (instruction/question/theme/lead_in/remark) → binary instance creation → balanced label verification (47.6% True, 52.4% False) → evaluation harness with faithfulness tracking

- **Critical path**: Data quality assurance (manual label verification) → prompt template construction → model inference (temperature=0 for non-reasoning) → output parsing → binary conversion → F1/faithfulness computation

- **Design tradeoffs**: Zero-shot vs few-shot (provides baseline but may not reflect optimal prompting); binary decomposition loses inter-statement dependencies present in original exam; excluded 52 questions that couldn't convert to binary format

- **Failure signatures**: (1) Low faithfulness + low F1 → model generating explanations despite instructions (e.g., Haiku-3.5, Sonnet-4.5 in zero-shot); (2) Few-shot F1 drop + faithfulness drop → exemplar copying behavior (LLM-jp models); (3) Consistent underperformance on Penal/Civil Code vs Constitution → insufficient statutory knowledge

- **First 3 experiments**:
  1. Establish zero-shot baseline F1 across model families (proprietary vs open-weight vs Japanese-specialized) to quantify the capability gap
  2. Compare reasoning-enabled vs disabled modes on same proprietary models (Opus-4.1, Sonnet-4/4.5) to isolate reasoning contribution
  3. Analyze per-subject performance variance (Constitution vs Civil Code vs Penal Code) to identify knowledge gaps and validate that Constitution is systematically easier across all model types

## Open Questions the Paper Calls Out

### Open Question 1
How can essay-type (ronbun-shiki) bar exam questions be effectively adapted for automated LLM evaluation? The authors note this would be an interesting future direction since essay questions require evaluating extended legal reasoning and argumentation quality, which is difficult even for human experts.

### Open Question 2
Can an automatic system detect and update dataset instances when relevant Japanese laws are revised? The paper identifies this as an ideal solution since questions are not automatically updated when laws change, and detecting semantic impact of amendments requires nuanced legal understanding.

### Open Question 3
What causes Constitution questions to be consistently easier for LLMs than Civil Code or Penal Code questions? The dataset spans three distinct legal domains with different reasoning patterns, but the source of difficulty differences remains unexplained.

### Open Question 4
Why do some open-weight models (especially LLM-jp-3.1) show substantially worse performance in few-shot settings compared to zero-shot? The authors hypothesize this may stem from pretraining differences but haven't empirically validated the mechanism.

## Limitations
- Binary decomposition approach loses semantic relationships between statements that may be contextually interdependent in original multi-choice format
- Faithfulness metric substantially impacts F1 scores, with models generating invalid outputs despite high reasoning capability
- Proprietary model evaluations depend on undocumented reasoning parameters that may not be reproducible

## Confidence
- **High confidence**: Proprietary reasoning models significantly outperform all other model families (F1 up to 0.861 vs 0.642 for best open-weight)
- **Medium confidence**: Japanese-specialization provides only marginal improvements (less than 0.03 advantage)
- **Low confidence**: The mechanism by which reasoning tokens improve legal judgment accuracy and whether binary decomposition adequately captures legal reasoning complexity

## Next Checks
1. Reconstruct multi-choice evaluation: Take a random sample of the 52 excluded questions and manually evaluate them with reasoning models to quantify the performance gap between binary decomposition and original multi-choice formats

2. Faithfulness ablation study: Run the same models with relaxed faithfulness criteria (accept explanations if first token is correct) to determine how much F1 score inflation comes from output format compliance versus actual reasoning capability

3. Legal term interpretation test: Extract all questions where literal vs legal meaning diverges (like the arson case) and evaluate model performance specifically on these instances to isolate whether failures stem from semantic understanding or legal knowledge gaps