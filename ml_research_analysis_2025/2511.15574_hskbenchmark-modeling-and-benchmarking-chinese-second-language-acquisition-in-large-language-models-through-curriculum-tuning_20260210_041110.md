---
ver: rpa2
title: 'HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition
  in Large Language Models through Curriculum Tuning'
arxiv_id: '2511.15574'
source_url: https://arxiv.org/abs/2511.15574
tags:
- language
- chinese
- llms
- grammar
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HSKBenchmark, the first benchmark for staged
  modeling and writing assessment of LLMs in Chinese second language acquisition.
  It covers HSK levels 3-6 and includes 6.76M tokens of authentic textbooks, 16K synthetic
  instruction samples, 30 test topics, and a linguistically-grounded evaluation system.
---

# HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning

## Quick Facts
- arXiv ID: 2511.15574
- Source URL: https://arxiv.org/abs/2511.15574
- Authors: Qihao Yang; Xuelin Wang; Jiale Chen; Xuelian Dong; Yuxin Hao; Tianyong Hao
- Reference count: 40
- Key outcome: First benchmark for staged modeling of Chinese SLA in LLMs, achieving human-like acquisition through curriculum tuning

## Executive Summary
This paper introduces HSKBenchmark, the first comprehensive benchmark for modeling Chinese second language acquisition in large language models. The framework covers HSK levels 3-6 and employs a curriculum-tuning approach that progressively trains models from beginner to advanced levels, mirroring human acquisition trajectories. Through a combination of textbook pretraining, synthetic instruction tuning, and a specialized evaluator (HSKAgent), the authors demonstrate that fine-tuned LLMs can achieve writing performance comparable to advanced human learners while exhibiting human-like acquisition characteristics.

## Method Summary
The methodology employs a two-phase curriculum-tuning framework: first, models undergo textbook pretraining using 6.76M tokens from authentic HSK textbooks segmented by difficulty level; second, they receive synthetic instruction tuning using 16K samples generated from 591 grammar items. The training proceeds sequentially through HSK levels 3→4→5→6, with each stage building on the previous one. Evaluation is performed by HSKAgent, a specialized evaluator fine-tuned on 10K learner compositions that assesses writing across grammar coverage, error count, lexical complexity, syntactic complexity, and holistic scoring dimensions.

## Key Results
- Curriculum-tuned LLMs achieve writing performance on par with advanced human learners
- Models exhibit human-like acquisition trajectories through progressive difficulty levels
- HSKAgent achieves high agreement with human raters (QWK = 0.7969)
- Curriculum tuning significantly outperforms shuffled training across all assessment metrics

## Why This Works (Mechanism)

### Mechanism 1: Progressive Complexity Anchoring (Curriculum Tuning)
- **Claim:** Sequential exposure to graded linguistic complexity yields superior high-level proficiency compared to mixed-difficulty training.
- **Mechanism:** Implements Krashen's "i+1" hypothesis by constraining training data at each stage to level-appropriate materials, building foundational grammar scaffolds before attempting complex structures.
- **Core assumption:** LLMs require similar developmental trajectory to human learners to achieve robust generalization in SLA.
- **Evidence anchors:** Abstract confirms progressive training simulating human trajectories; Figure 3 shows curriculum tuning surpasses shuffled training in later stages; supports difficulty of instilling complex writing skills without specific guidance.

### Mechanism 2: Input-Output Coupling (Synthetic Instruction Tuning)
- **Claim:** High-quality synthetic instruction data derived from grammar lists enables precise "output-based" learning to complement "input-based" textbook pretraining.
- **Mechanism:** Converts passive pattern recognition from textbooks into active production capabilities by forcing generation using specific grammar items.
- **Core assumption:** State-of-the-art LLMs can generate instruction data of sufficient validity (95%) to serve as ground truth without introducing hallucinated grammar rules.
- **Evidence anchors:** Abstract reports 95% validity rate; sequential optimization explicitly couples input and output learning; supports need for targeted output exercises to master complex structures.

### Mechanism 3: Specialized Evaluator Grounding (HSKAgent)
- **Claim:** A specialized evaluator fine-tuned on learner corpora is required to detect specific error distributions and complexity metrics relevant to SLA.
- **Mechanism:** Learns to recognize non-native error patterns and grade based on SLA-specific linguistic dimensions rather than generic fluency.
- **Core assumption:** Features from 10K learner corpus are representative of broader population of L2 acquisition errors.
- **Evidence anchors:** HSKAgent achieves high agreement with human raters (QWK = 0.7969); supports need for specific reliability studies when using LLMs for scoring.

## Foundational Learning

**Concept:** Curriculum Learning (Self-paced Learning)
- **Why needed:** The core contribution is the curriculum-tuning framework; understanding that training data order matters is essential.
- **Quick check:** Can you explain why training on HSK 6 data immediately might degrade performance compared to starting with HSK 3?

**Concept:** Second Language Acquisition (SLA) Metrics
- **Why needed:** Paper evaluates models on linguistically grounded indices like lexical complexity (MATTR-50) and syntactic complexity (MDD).
- **Quick check:** Why might an LLM score highly on "Holistic Scoring" but poorly on "Syntactic Complexity" compared to humans?

**Concept:** LoRA (Low-Rank Adaptation)
- **Why needed:** Experiments use LoRA to fine-tune models on consumer-grade GPUs, enabling parameter-efficient adaptation.
- **Quick check:** How does LoRA allow authors to update the model without catastrophic forgetting of base capabilities?

## Architecture Onboarding

**Component map:**
79 Textbooks (cleaned) + 591 Grammar Items -> Synthetic Generator (GPT/DeepSeek) -> Base Model -> (Pretrain Textbooks L -> Instruct Tune L) for L in [3, 4, 5, 6] -> HSKAgent Evaluation

**Critical path:** Data Cleaning & Alignment is most sensitive component; ensuring 6.76M tokens are strictly segmented by HSK level is prerequisite for entire curriculum logic.

**Design tradeoffs:**
- Scale vs. Authenticity: Traded massive web-scraped data for smaller, authentic textbooks to ensure level purity
- Synthetic vs. Human Gold: Used synthetic instruction data for scalability rather than limited human essays, risking model-specific biases but gaining coverage over 591 grammar items

**Failure signatures:**
- Early Plateau: If model performance at HSK 6 is statistically similar to HSK 4, curriculum data likely lacks sufficient complexity differentiation
- Fluent but Wrong: If "Writing Errors" is low but "Holistic Scoring" is low, model generates fluent nonsense not grounded in prompt's grammar requirements

**First 3 experiments:**
1. Baseline Calibration: Run raw base models on 30 test topics to establish zero-shot floor
2. Curriculum vs. Shuffle Ablation: Train two identical models—one sequential HSK 3-6 pipeline, one with shuffled data—to reproduce performance gap
3. Grammar Coverage Probe: Test specifically for presence of "Advanced" grammar items in output to verify generalization beyond explicit training items

## Open Questions the Paper Calls Out

**Open Question 1:** Does curriculum-tuning framework generalize to SLA modeling for other languages or proficiency frameworks beyond Chinese?
- Basis: Paper intends to scale framework to broader range of languages and suggests extending to other frameworks like CEFR
- Unresolved: Current study restricted to Chinese; requires successful application to Indo-European language

**Open Question 2:** Can integration of multimodal inputs (images and audio) improve SLA modeling fidelity compared to text-only approach?
- Basis: Conclusion identifies incorporating multimodal inputs as future work; methodology notes images deleted because not objective
- Unresolved: Framework relies exclusively on text-based tokens, ignoring visual/auditory context present in actual human learning materials

**Open Question 3:** Can LLMs be further optimized to match human lexical and syntactic complexity?
- Basis: Results note LLMs generate shorter, more typical sentences while optimizing for predictive likelihood
- Unresolved: Inherent bias toward high-probability tokens limits ability to mimic deliberate complexity humans use to demonstrate competence

**Open Question 4:** How can benchmark extend to model earliest stages (HSK 1-2) given scarcity of authentic learning materials?
- Basis: Paper excludes levels 1-2, citing fewer learning materials available at lower levels as major issue
- Unresolved: Current benchmark only covers levels 3-6, leaving simulation of complete "beginner to advanced" trajectory incomplete

## Limitations

- Reliance on synthetic instruction data generated by other LLMs may encode model-specific biases not representative of human learning patterns
- Benchmark focuses on intermediate-to-advanced learners (HSK 3-6), leaving beginner acquisition pathway unmodeled
- HSKAgent evaluation framework may not capture creative or unconventional but grammatically correct expressions that human learners sometimes produce

## Confidence

**High Confidence:** Curriculum-tuning framework's effectiveness is well-supported by experimental results showing progressive improvement and clear superiority over shuffled training
**Medium Confidence:** Specialized evaluator shows reasonable agreement with human raters, but generalizability requires further validation; 95% synthetic data validity claim lacks detailed methodology
**Low Confidence:** Claims about models achieving "writing performance on par with advanced human learners" rely on automated evaluation without comprehensive human expert validation across all dimensions

## Next Checks

1. **Human Expert Validation:** Commission human Chinese language experts to independently evaluate stratified sample of model-generated essays across all HSK levels, comparing against HSKAgent scores
2. **Cross-Model Synthetic Data Consistency:** Generate synthetic instruction data using multiple LLM providers and fine-tune separate models to assess whether performance differences arise from instruction data source
3. **Generalization Stress Test:** Create adversarial test cases containing grammar combinations not explicitly covered in 591-item grammar list to evaluate whether curriculum-tuned models can handle novel linguistic constructions