---
ver: rpa2
title: Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled
  Decoder-Only Transformer
arxiv_id: '2510.11926'
source_url: https://arxiv.org/abs/2510.11926
tags:
- locaris
- indoor
- telemetry
- localization
- wi-fi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Locaris, a decoder-only large language model
  (LLM) for indoor Wi-Fi localization. The core idea is to treat each Wi-Fi access
  point measurement as a token, enabling the model to ingest raw telemetry without
  pre-processing.
---

# Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer

## Quick Facts
- arXiv ID: 2510.11926
- Source URL: https://arxiv.org/abs/2510.11926
- Authors: Nayan Sanjay Bhatia; Pranay Kocheta; Russell Elliott; Harikrishna S. Kuttivelil; Katia Obraczka
- Reference count: 40
- Key outcome: Locaris achieves sub-meter accuracy for indoor Wi-Fi localization using a frozen LLM backbone with LoRA adapters, demonstrating robust cross-environment transfer and graceful degradation under missing access points.

## Executive Summary
Locaris introduces a novel approach to indoor Wi-Fi localization by treating each access point measurement as a textual token, enabling direct ingestion of raw telemetry without preprocessing. The method leverages a frozen LLM backbone (LLaMA-3.2-1B) with lightweight LoRA adaptation modules and a regression head to predict 2D/3D coordinates. This architecture achieves state-of-the-art performance across multiple telemetry types (RSSI, FTM) while requiring minimal calibration data for few-shot adaptation to new environments.

## Method Summary
The approach serializes raw Wi-Fi telemetry into text tokens where each access point measurement becomes a token sequence (e.g., "AP1 RTT: 12.5 AP2 RSS: -67"). A frozen LLaMA-3.2-1B decoder-only transformer processes these variable-length sequences, with LoRA adapters inserted into attention projections providing task-specific adaptation. The final token's hidden state passes through a two-layer MLP regression head to output location coordinates. Training optimizes only LoRA parameters and the regression head using MSE loss, achieving parameter efficiency while maintaining strong performance.

## Key Results
- Sub-meter median accuracy (0.77m MAE) across diverse indoor environments
- 10x reduction in calibration requirements compared to baselines (5-10% target data vs. 50-75%)
- Graceful degradation under RSSI-only conditions (sub-5m accuracy) while baselines collapse catastrophically
- Cross-environment transfer learning maintains near-optimal performance with minimal target environment data

## Why This Works (Mechanism)

### Mechanism 1
Token-based representation enables schema-free, variable-length telemetry ingestion without information loss. Each AP measurement is serialized as a textual token, allowing the transformer to process any combination of APs and modalities sequentially. Missing APs are omitted rather than imputed, preserving natural signal relationships.

### Mechanism 2
Frozen LLM backbone with LoRA adaptation provides parameter-efficient cross-environment transfer via preserved pre-trained attention patterns. The pre-trained decoder retains learned attention mechanisms for sequential reasoning, while LoRA adapters modify attention weights for localization-specific patterns without disrupting core representations.

### Mechanism 3
Self-attention captures implicit spatial relationships (multipath, NLOS, interference) that improve robustness under degraded conditions. Self-attention computes pairwise dependencies across all token positions, enabling the model to learn that nearby APs or specific signal combinations correlate with spatial locations.

## Foundational Learning

- **Decoder-only transformers**: Understand causal masking vs. bidirectional attention for streaming Wi-Fi telemetry processing.
  - *Quick check*: Can you explain why a decoder-only architecture is preferred over BERT-style encoders for streaming Wi-Fi telemetry?

- **Low-Rank Adaptation (LoRA)**: Learn rank decomposition, adapter placement, and freezing strategy for efficient model adaptation.
  - *Quick check*: If LoRA rank is set too low, what symptom would you expect during few-shot adaptation to a new building?

- **Wi-Fi telemetry types (RSSI, FTM, CSI)**: Understand tradeoffs between ubiquitous but noisy RSSI, ranging-capable FTM, and high-accuracy CSI requiring specialized hardware.
  - *Quick check*: Why does the paper show graceful degradation under RSSI-only conditions but baselines collapse? What property of the model enables this?

## Architecture Onboarding

- **Component map**: Raw telemetry -> Text tokenization -> Frozen LLM backbone -> LoRA adapters -> Last token embedding -> MLP regression head -> (x,y) coordinates
- **Critical path**: 1) Collect raw telemetry scan (variable APs, modalities) 2) Format as token sequence 3) Tokenize with LLaMA tokenizer 4) Forward pass through frozen backbone + LoRA adapters 5) Extract last-token embedding 6) MLP regression head outputs coordinates 7) Backpropagate MSE loss to LoRA + MLP parameters only
- **Design tradeoffs**: FP16 offers best throughput and energy efficiency vs. 4-bit quantization saving memory but degrading accuracy; LoRA rank selection balances expressiveness vs. parameters; single-environment training is faster but less generalizable than cross-environment.
- **Failure signatures**: Catastrophic error spikes under RSSI-only if trained primarily on FTM; collapse when AP count drops below 3 due to geometric constraints; cross-device heterogeneity bias without adaptation data; memory overflow on mobile CPUs requiring distillation.
- **First 3 experiments**: 1) Reproduce single-environment baseline on SODIndoorLoc, validating ~25-30% improvement over KNR/KNN baselines 2) Ablate telemetry modalities on FTM-RSSI dataset, confirming sub-5m RSSI-only vs. baseline collapse 3) Few-shot cross-environment transfer from lecture+office to corridor, validating 10x reduction in calibration data.

## Open Questions the Paper Calls Out
- Extension to 3D localization using adapter modules for richer tasks like environment mapping
- Performance with Channel State Information (CSI) telemetry offering highest accuracy but requiring specialized hardware
- Elimination of inference slowdown in quantized models through quantization-aware training or integer-only LoRA adapters
- Maintenance of accuracy under extended temporal deployment with environmental changes like furniture rearrangement

## Limitations
- Computational overhead may be prohibitive for mobile/embedded devices despite parameter efficiency
- Claims about pre-trained language representations providing useful inductive biases for RF signals lack empirical validation
- Temporal stability and real-world deployment challenges (device heterogeneity, environmental drift) remain untested

## Confidence
- **High confidence**: Core performance claims (sub-meter accuracy, graceful degradation, few-shot transfer) are well-supported by experimental results
- **Medium confidence**: LoRA-based parameter efficiency and decoder-only architecture necessity are plausible but lack direct comparative validation
- **Low confidence**: Assertion that pre-trained language representations transfer to RF signals lacks empirical support

## Next Checks
1. **Architecture ablation study**: Train small transformer from scratch with comparable parameters to determine if LLM backbone provides benefits beyond parameter count
2. **Temporal stability experiment**: Collect data over weeks with controlled environmental changes to assess real-world deployment readiness and calibration requirements
3. **Edge deployment benchmark**: Implement 4-bit quantized Locaris on Raspberry Pi with Coral USB accelerator to validate practical deployment tradeoffs in throughput, memory, and accuracy