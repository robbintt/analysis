---
ver: rpa2
title: 'AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State
  Space Models'
arxiv_id: '2602.00534'
source_url: https://arxiv.org/abs/2602.00534
tags:
- pruning
- energy
- state
- aire-prune
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AIRE-Prune introduces a post-training pruning method for state
  space models (SSMs) that reduces state dimension by ranking and removing low-importance
  states based on their asymptotic impulse-response energy. The core idea is to assign
  each state a closed-form energy score representing its long-term output contribution,
  normalize these scores layer-wise, and prune the least energetic states using a
  single global threshold.
---

# AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models

## Quick Facts
- arXiv ID: 2602.00534
- Source URL: https://arxiv.org/abs/2602.00534
- Reference count: 40
- Primary result: Achieves 60.8% average pruning ratio with only 0.29% average accuracy loss across six LRA tasks and Speech Commands

## Executive Summary
AIRE-Prune introduces a post-training pruning method for state space models (SSMs) that reduces state dimension by ranking and removing low-importance states based on their asymptotic impulse-response energy. The core idea is to assign each state a closed-form energy score representing its long-term output contribution, normalize these scores layer-wise, and prune the least energetic states using a single global threshold. This method extends modal truncation to deep stacks and aligns pruning with typical-case energy rather than worst-case gain. Across six Long Range Arena tasks and Speech Commands, AIRE-Prune achieves an average pruning ratio of 60.8% with only 0.29% average accuracy loss, outperforming existing baselines. It also yields 1.2×-2.9× inference speedups and 19%-65% parameter reductions, demonstrating substantial redundancy in trained SSMs and effective compression with negligible accuracy drop.

## Method Summary
AIRE-Prune computes per-state asymptotic impulse-response energy using $E_i = \frac{\|C_{:,i}\|_2^2 \|B_{i,:}\|_2^2}{1 - |\lambda_i|^2}$, then normalizes scores layer-wise via prefix normalization to enable global cross-layer pruning with a single threshold. This one-shot method preserves diagonal structure and achieves aggressive compression without retraining.

## Key Results
- Achieves 60.8% average pruning ratio across six LRA tasks and Speech Commands
- Maintains only 0.29% average accuracy loss with 60% pruning
- Provides 1.2×-2.9× inference speedups and 19%-65% parameter reductions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A state's contribution to long-term signal transmission can be quantified by its asymptotic impulse-response energy, allowing low-contribution states to be safely removed.
- Mechanism: The method calculates a closed-form energy score $E_i = \frac{\|C_{:,i}\|^2 \|B_{i,:}\|^2}{1 - |\lambda_i|^2}$ for each state (mode) in the diagonal state space model. This score sums the squared magnitude of a state's output over an infinite horizon under unit impulse excitation. It captures controllability ($B$), observability ($C$), and memory/damping ($\lambda$). States with the lowest $E_i$ contribute least to the total output energy and are ranked lowest for pruning.
- Core assumption: The "importance" of a state is better approximated by its total energy over time (typical-case behavior) rather than its peak gain at a specific frequency (worst-case behavior).
- Evidence anchors:
  - [Abstract]: Assigns every state a closed-form asymptotic impulse-response energy-based score... representing total impulse-response energy.
  - [Section 4]: "This aligns pruning with steady-state distortion and empirically enables aggressive compression..."
  - [Corpus]: Corpus offers related context on SSM pruning challenges (e.g., "On Pruning State-Space LLMs") but does not evaluate AIRE-Prune's specific energy metric.
- Break condition: If a task relies heavily on rare, high-frequency features that might be suppressed in typical energy calculations but are critical for specific predictions.

### Mechanism 2
- Claim: Layer-wise prefix normalization enables a single global threshold to effectively prune states across layers with vastly different parameter scales.
- Mechanism: Raw energy scores $E_i$ vary significantly between layers (e.g., encoder vs. decoder). To compare them globally, the method sorts states in a layer by energy and computes a "prefix-normalized" score: the ratio of the current state's energy to the cumulative energy of all higher-ranked states ($E_{(i)} / \sum_{j \le i} E_{(j)}$). This creates a monotonic score where a global threshold $\tau$ retains a contiguous "head" of important states in each layer.
- Core assumption: A single global threshold is sufficient to identify the "elbow" of diminishing returns across all layers simultaneously.
- Evidence anchors:
  - [Abstract]: "...normalizes these scores layer-wise to enable global cross-layer comparison and selection."
  - [Section 4]: "This 'hazard-rate' ratio is monotonically non-increasing... enabling an elbow-style, layer-adaptive rule..."
  - [Corpus]: No direct corpus evidence validates the specific prefix-normalization technique for SSMs.
- Break condition: If certain layers require significantly finer or coarser granularity than the global threshold permits, potentially over-pruning sensitive layers or under-pruning redundant ones.

### Mechanism 3
- Claim: Pruning low-energy states creates a smaller, dense model that preserves the input-output mapping of the original model while accelerating inference.
- Mechanism: Unlike unstructured weight pruning which creates sparse matrices, this method removes specific dimensions from the state vector $x$ and corresponding rows/columns from $B$, $C$, and $\Lambda$. This results in a dense, lower-dimensional SSM that runs faster on standard hardware without specialized sparse kernels.
- Core assumption: Trained SSMs contain significant redundancy (states with near-zero energy contribution) that can be removed without retraining.
- Evidence anchors:
  - [Section 5]: "...aggressive state pruning translates into meaningful computational and memory savings, not just abstract sparsity."
  - [Figure 5]: Shows layer-wise pruning profiles, indicating some layers can be pruned heavily (e.g., >80%) while others retain more states.
  - [Corpus]: Confirms general interest in "Accelerating LLM Inference" and "Systolic Array-based Accelerator" for efficient SSM deployment.
- Break condition: If the base model is already optimally compressed or under-parametrated, forcing a high pruning ratio will sever essential signal paths.

## Foundational Learning

- Concept: **Diagonal State Space Models (SSMs)**
  - Why needed here: AIRE-Prune relies on the diagonal structure ($\Lambda$) to derive a closed-form energy solution. Understanding that a complex system is decomposed into independent scalar modes ($\lambda_i$) is critical.
  - Quick check question: If a state matrix is not diagonal, can you apply the $E_i = \frac{\|C_{:,i}\|^2 \|B_{i,:}\|^2}{1 - |\lambda_i|^2}$ formula directly?

- Concept: **Impulse Response & Energy ($H_2$ Norm)**
  - Why needed here: The core pruning metric is the total energy of the impulse response. You must understand that stability ($|\lambda| < 1$) ensures the geometric series converges, making the energy score finite.
  - Quick check question: What happens to the energy score $E_i$ if a pole $\lambda_i$ lies exactly on the unit circle (marginally stable)?

- Concept: **Model Order Reduction (MOR)**
  - Why needed here: This paper is an application of control theory's "modal truncation" to deep learning. Knowing the difference between "balanced truncation" (destroys diagonal structure) and "modal truncation" (preserves it) explains the design choice.
  - Quick check question: Why does the paper reject balanced truncation in favor of modal truncation for modern SSMs?

## Architecture Onboarding

- Component map: Input -> Score Engine -> Normalizer -> Pruning Masker -> Output
- Critical path:
  1. **Stability Verification:** Ensure $|\lambda_i| < 1$; otherwise, energy scores diverge.
  2. **Prefix Normalization:** The sorting and prefix-sum logic determines the "elbow"; implementation errors here destroy the global thresholding property.
- Design tradeoffs:
  - **One-shot vs. Iterative:** AIRE is one-shot (fast) but assumes redundancy exists; iterative pruning might find better compression but costs more compute.
  - **Global vs. Local:** Using a global threshold simplifies tuning but removes agency from layer-specific dynamics (observed in ListOps sensitivity).
- Failure signatures:
  - **Sudden Accuracy Drop:** The "elbow" is sharp; picking a threshold just past the elbow causes catastrophic forgetting.
  - **Low Compression on Small Models:** If the model is already small, the energy distribution may be flat, preventing aggressive pruning.
- First 3 experiments:
  1. **Validation Run:** Apply AIRE-Prune to a pre-trained S5 model on PathX. Verify that a 70% pruning ratio maintains accuracy within 0.5% of the baseline (per Table 1).
  2. **Threshold Sweep:** Plot accuracy vs. pruning ratio for a specific task. Identify the "elbow" point visually to confirm the step-function degradation shown in Figure 4.
  3. **Inference Benchmark:** Measure wall-clock time for the pruned model vs. the original on an H100/A100. Confirm the 1.2x-2.9x speedup claims match the theoretical FLOP reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AIRE-Prune be adapted for input-selective models like Mamba where state matrices are input-dependent?
- Basis in paper: [explicit] Page 11 (Future Work) states the need to analyze AIRE on "input-selective models like Mamba, where state matrices are input dependent."
- Why unresolved: The current closed-form solution assumes Linear Time-Invariant (LTI) dynamics, whereas input-selective models have time-varying dynamics (gating).
- What evidence would resolve it: A derivation of energy scores for time-varying systems, perhaps using Monte-Carlo estimation over gates, applied to Mamba benchmarks.

### Open Question 2
- Question: Does the high compressibility found in Long Range Arena (LRA) tasks transfer to large-scale language modeling datasets?
- Basis in paper: [explicit] Page 11 (Future Work) explicitly suggests AIRE-Prune "can be analyzed on language datasets."
- Why unresolved: Current experiments are limited to LRA and Speech Commands; language modeling involves different data distributions and dependency structures.
- What evidence would resolve it: Evaluation of AIRE-Prune on standard language modeling benchmarks (e.g., The Pile, WikiText-103) measuring perplexity vs. compression.

### Open Question 3
- Question: Can the negligible accuracy loss be further improved or the pruning ratio increased via lightweight retraining or iterative pruning?
- Basis in paper: [inferred] Page 7 and 8 emphasize "one-shot pruning (no retraining)," leaving the potential benefits of fine-tuning unexplored.
- Why unresolved: While one-shot pruning is efficient, standard pruning literature often utilizes fine-tuning to recover capacity lost during removal.
- What evidence would resolve it: A comparison of one-shot AIRE-Prune against an iterative AIRE-Prune with gradient-based fine-tuning steps.

## Limitations

- **Global Threshold Sensitivity**: The single global threshold approach may not work optimally for all architectures and tasks, as evidenced by ListOps being an outlier.
- **Stability Constraint Enforcement**: The paper doesn't explicitly detail how stability is verified or enforced in practice, which could lead to numerical instability.
- **Complex Conjugate Handling**: The exact implementation details for handling complex conjugate pairs to preserve real-valued outputs are not fully specified in the main text.

## Confidence

**High Confidence**: The core mathematical formulation of the energy score $E_i = \frac{\|C_{:,i}\|^2 \|B_{i,:}\|^2}{1 - |\lambda_i|^2}$ and its derivation from impulse response theory. The empirical results showing substantial pruning ratios (60.8% average) with minimal accuracy loss (<0.29% average) across multiple LRA tasks are well-supported by the presented tables and figures.

**Medium Confidence**: The prefix normalization technique's universal applicability across different model architectures and tasks. While effective for the tested S5 models, the method's performance on non-S5 SSMs or models with different state space parameterizations needs broader validation.

**Low Confidence**: The claim that a single global threshold works optimally across all layers for all tasks. The sensitivity shown in ListOps suggests this may not hold universally, and the paper doesn't provide comprehensive guidelines for when layer-specific thresholds might be necessary.

## Next Checks

1. **Stability Verification Test**: Implement explicit stability checking for all poles in pre-trained models. For any pole with $|\lambda_i| \geq 1 - \delta$ (where $\delta$ is a small tolerance), either reject the model or apply damping. Verify that all models used in the paper pass this stability check.

2. **Complex Pair Handling Implementation**: Implement and test two different strategies for handling complex conjugate pairs: (a) treating them as a single pruning unit, and (b) summing their individual energies. Compare the resulting pruning ratios and accuracy retention to determine which approach better preserves model performance.

3. **Threshold Sensitivity Analysis**: For a single task (e.g., PathX), systematically vary the global threshold $\tau$ across its full range and plot accuracy vs. pruning ratio. Identify the "elbow" point and test whether it occurs at the same relative position across different layers. Compare this empirical elbow location to the threshold values used in the paper's results.