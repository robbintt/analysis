---
ver: rpa2
title: 'Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs'
arxiv_id: '2512.05648'
source_url: https://arxiv.org/abs/2512.05648
tags:
- forget
- data
- retain
- sgtm
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective Gradient Masking (SGTM) is a pretraining-time technique
  that localizes target knowledge into dedicated model parameters for later removal,
  addressing dual-use risks in large language models. SGTM zero-masks gradients from
  target domain examples so they only update designated parameters, then removes these
  parameters post-training.
---

# Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs

## Quick Facts
- **arXiv ID**: 2512.05648
- **Source URL**: https://arxiv.org/abs/2512.05648
- **Reference count**: 40
- **Primary result**: SGTM maintains 99% equivalent forget loss compared to perfect data filtering even with 20% unlabeled forget data, while achieving lower retain loss than previous gradient routing variants

## Executive Summary
Selective Gradient Masking (SGTM) is a pretraining-time technique that addresses dual-use risks in large language models by localizing target knowledge into dedicated model parameters for later removal. Unlike traditional data filtering that requires complete knowledge of dangerous domains, SGTM uses a small set of labeled dangerous data to create parameter specialization through gradient masking, then removes these parameters post-training. The method demonstrates strong performance on bilingual TinyStories experiments and Wikipedia biology removal tasks, achieving better forget/retain trade-offs than data filtering while showing robustness to adversarial fine-tuning. SGTM's effectiveness increases with model scale, with leakage from unlabeled dangerous data decreasing from 0.02 to 0.005 as models grow from 64M to 254M parameters.

## Method Summary
SGTM operates during pretraining by partitioning transformer parameters into forget and retain sets, then applying selective gradient masking during training. For dangerous domain samples, gradients to retain parameters are zeroed in the backward pass, forcing knowledge to be stored only in forget parameters. For safe domain samples, forget parameters are zeroed in the forward pass. This creates functional specialization where forget parameters learn dangerous domain patterns while retain parameters remain unaffected. After training, forget parameters are permanently removed by setting to zero. The method uses parameter-gradient masking rather than activation-gradient masking, making it less disruptive while achieving stricter isolation. SGTM shows strong robustness to adversarial fine-tuning, requiring 7× more optimization steps to recover removed capabilities compared to post-training unlearning methods.

## Key Results
- SGTM achieved 99% equivalent forget loss compared to perfect data filtering on bilingual TinyStories even with 20% unlabeled forget data
- On Wikipedia biology removal, SGTM achieved higher biology forget loss at any given retain loss than both weak (biology-only) and strict (biology + related) data filtering
- SGTM showed only 6% compute penalty on general knowledge compared to data filtering methods
- SGTM demonstrated strong robustness to adversarial fine-tuning, requiring 7× more steps to reach baseline forget loss than RMU unlearning

## Why This Works (Mechanism)

### Mechanism 1: Selective Gradient Masking for Parameter Isolation
- **Claim**: Zero-masking parameter gradients from target domain examples restricts knowledge encoding to designated parameters.
- **Mechanism**: During backward pass on D_forget samples, gradients for θ_retain are zeroed before optimizer update, forcing forget knowledge to be stored only in θ_forget. This differs from activation-gradient masking (prior Gradient Routing) which is more disruptive and allows information flow to down-projection layers.
- **Core assumption**: Knowledge can be functionally isolated through gradient restrictions during training, and this localization persists through training completion.
- **Evidence anchors**: [abstract], [Section 3.2], GRAIL (2504.12681) addresses gradient-based unlearning but operates post-training; no pretraining-time gradient masking approaches found in corpus.
- **Break condition**: If forget knowledge can be reconstructed from retain parameters through composition of independently-learned concepts, or if gradient masking fails to prevent implicit encoding through retain data that contains forget concepts.

### Mechanism 2: Self-Reinforcing Knowledge Localization (Absorption)
- **Claim**: Once initial localization establishes specialized pathways, even unlabeled forget data naturally updates θ_forget more strongly than θ_retain.
- **Mechanism**: Early masking creates functional specialization in θ_forget for processing forget-domain patterns. When unlabeled forget samples arrive (without masking), they naturally activate these specialized pathways, generating stronger relative gradient norms to θ_forget than θ_retain—reinforcing localization without explicit intervention.
- **Core assumption**: Gradient norm distributions reflect meaningful knowledge localization rather than just optimization dynamics; the "absorption" property conjectured by Cloud et al. (2024) operates mechanistically.
- **Evidence anchors**: [Section 4.4], [Figure 4(b)], Relative gradient norm distributions show forget data primarily updates forget parameters (top-left panel), retain data primarily updates retain parameters (top-right).
- **Break condition**: If gradient norms don't correlate with actual knowledge storage, or if unlabeled forget data proportion exceeds absorption capacity (paper shows robustness to 20% unlabeled in TinyStories).

### Mechanism 3: Structural Knowledge Absence Enables Adversarial Robustness
- **Claim**: Knowledge removed via parameter zeroing is fundamentally harder to recover than post-training suppression because the knowledge was never encoded in θ_retain.
- **Mechanism**: Unlike RMU (post-training unlearning) which suppresses existing representations, SGTM prevents encoding during pretraining. Adversarial fine-tuning must discover entirely new circuitry in θ_retain rather than reactivating suppressed pathways. This requires ~7× more optimization steps.
- **Core assumption**: Training-time prevention creates qualitatively different knowledge absence than post-hoc removal; the structural difference manifests in fine-tuning dynamics.
- **Evidence anchors**: [Section 5.3], [Figure 6(a)], Shows RMU recovers baseline forget loss in ~50 steps; SGTM requires ~350 steps (comparable to strict data filtering).
- **Break condition**: If θ_retain has sufficient capacity and architectural flexibility to rapidly learn forget knowledge from scratch during fine-tuning, or if fine-tuning data distribution closely matches original forget distribution.

## Foundational Learning

- **Concept: Gradient masking vs. activation masking**
  - **Why needed here**: SGTM's key innovation is masking parameter gradients (∇θ_retain = 0) rather than activation gradients. Activation masking blocks backpropagation through masked activations, altering gradients for all downstream parameters and permitting information flow to down-projection layers (W_2, W_O). Parameter-gradient masking is less disruptive while achieving stricter isolation.
  - **Quick check question**: Given a 4-layer transformer, if you mask attention output activations on layer 2 for forget data, which parameters can still receive gradient updates from forget samples?

- **Concept: Leakage quantification methodology**
  - **Why needed here**: The paper introduces "leakage" as the ratio of equivalent forget token exposure (from baseline training) to actual unlabeled forget tokens seen. A leakage of 0.02 means 100M unlabeled forget tokens contribute as much knowledge as 2M tokens in standard training.
  - **Quick check question**: If an SGTM model with 50M unlabeled forget tokens achieves forget loss equivalent to a baseline model trained on 1M forget tokens, what is the leakage rate?

- **Concept: Forget/retain trade-off curves and compute penalty**
  - **Why needed here**: SGTM evaluation uses Pareto-style curves plotting forget loss (higher = better forgetting) vs. retain loss (lower = better capability preservation). Compute penalty is expressed as equivalent FLOPs reduction on baseline model: 5% penalty means achieving that retain loss on unfiltered data would require 5% less compute.
  - **Quick check question**: Why does strict data filtering show negative compute penalty (-15%) on general knowledge while SGTM shows positive penalty (+5%)?

## Architecture Onboarding

- **Component map**:
  ```
  Per Transformer Block:
  ├── Attention Heads: h_total (e.g., 32)
  │   ├── θ_forget: h_forget heads (e.g., 1)
  │   └── θ_retain: (h_total - h_forget) heads
  ├── MLP Layer: d_MLP hidden units (e.g., 4096)
  │   ├── θ_forget: d_forget units (e.g., 64)
  │   └── θ_retain: (d_MLP - d_forget) units
  └── Parameter split affects: W_1, b_1, W_2, b_2, W_O, W_QKV

  Data Pipeline:
  ├── D_forget: High-confidence target domain (apply gradient masking)
  ├── D_retain: High-confidence safe domain (apply parameter masking)
  └── D_unlabeled: Uncertain labels (no masking, relies on absorption)
  ```

- **Critical path**:
  1. **Classifier setup**: Train/obtain domain classifier for forget set identification (paper used Wikipedia's article topic service for biology)
  2. **Parameter partitioning**: Designate h_forget attention heads and d_forget MLP units per block (default: 1/32 heads, 64/4096 MLP units)
  3. **Training loop modification**:
     - Forward pass on D_retain: Zero θ_forget parameters (selective parameter masking)
     - Backward pass on D_forget: Zero θ_retain gradients before optimizer step (selective gradient masking)
     - D_unlabeled: Normal forward/backward (no intervention)
  4. **Post-training ablation**: Set θ_forget = 0 for safe model deployment
  5. **Optional logit calibration**: Train bias layer to stabilize post-ablation loss values

- **Design tradeoffs**:
  - **Forget capacity (d_forget, h_forget)**: Larger allocation improves forget-domain learning pre-ablation but increases removal footprint. Paper used ~3% of MLP units (64/2048), 3% of heads (1/32).
  - **Joint parameters variant**: Including embeddings or down-projections in θ_joint (updated by both forget/retain) improves pre-ablation capability but risks knowledge leakage. Full masking variant shows 85% compute penalty on biology pre-ablation vs. 30% for joint embeddings variant.
  - **Retain data sampling**: Paper used 10-25% confident retain samples with parameter masking; higher sampling improves retain performance but requires more labeled data.

- **Failure signatures**:
  - **Superficial suppression**: High forget loss post-ablation but low adversarial fine-tuning resistance (<2× RMU steps to recover) → absorption mechanism failed, knowledge leaked to θ_retain
  - **Excessive retain degradation**: >10% compute penalty on general knowledge → d_forget/h_forget too small or masking too aggressive; consider joint parameter variants
  - **Scale-dependent leakage**: Leakage increases with model scale (opposite of expected) → fundamental architecture mismatch; check if forget capacity scales with model size
  - **Per-token loss outliers**: Forget loss driven by few extreme tokens rather than broad distribution → localized token suppression rather than knowledge removal (verify with Figure 6(b)-style analysis)

- **First 3 experiments**:
  1. **Label noise robustness sweep**: Replicate TinyStories bilingual setup with 1%, 5%, 10%, 20%, 40% undiscovered forget rates. Measure forget loss, retain loss, and leakage. Validate absorption property and scaling trends.
  2. **Forget capacity ablation**: Train models with d_forget ∈ {32, 64, 128, 256} on Wikipedia biology removal task. Plot forget/retain trade-off curves to identify optimal capacity for target domain complexity.
  3. **Adversarial fine-tuning comparison**: Train SGTM, weak filter, strict filter, and RMU models. Fine-tune all on 50/50 forget/retain mixture. Record steps-to-baseline-forget-loss. Confirm SGTM achieves ≥5× RMU robustness.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does SGTM's leakage reduction trend continue at frontier model scales (billions of parameters), and does the localization mechanism remain effective as models grow larger?
  - **Basis in paper**: [explicit] The authors state "While these models remain relatively small, the observed trend suggests that SGTM's localization becomes stronger as scale increases" and note experiments are "orders of magnitude below the size of frontier systems."
  - **Why unresolved**: Experiments only cover 8M–254M parameter models; scaling behavior to billion-parameter regimes is unknown.
  - **What evidence would resolve it**: Training models with SGTM at progressively larger scales (1B+, 10B+) and measuring leakage rates and forget/retain trade-offs.

- **Open Question 2**: What are the geometric properties of gradient updates that enable self-reinforcing knowledge localization, and why do gradient norms alone not fully capture parameter update behavior?
  - **Basis in paper**: [explicit] "Future work should investigate the geometric properties of these gradient updates to better understand the knowledge localization mechanisms."
  - **Why unresolved**: The paper observes discrepancies between gradient norms and effective knowledge transfer, suggesting direction and alignment of gradients matter beyond magnitude.
  - **What evidence would resolve it**: Analysis of gradient direction/alignment (e.g., cosine similarity, gradient projection) between forget and retain parameters across data types.

- **Open Question 3**: How susceptible is SGTM to in-context attacks where adversaries supply dangerous information at inference time, compared to data filtering?
  - **Basis in paper**: [explicit] "Data filtering has been shown to be susceptible to this (O'Brien et al., 2025), and we expect SGTM to behave similarly, given the nature of our method."
  - **Why unresolved**: The authors explicitly state this vulnerability is expected but not tested experimentally.
  - **What evidence would resolve it**: Red-teaming experiments measuring whether in-context dangerous information restores removed capabilities in SGTM models versus data-filtered models.

- **Open Question 4**: Does SGTM's performance on proxy tasks (language/biology removal) translate to genuine CBRN capability removal on benchmarks like WMDP?
  - **Basis in paper**: [explicit] "Our models are not large enough to yield meaningful results on evaluations that directly probe dangerous capabilities, like WMDP. Our evaluation is thus based on loss metrics as a proxy."
  - **Why unresolved**: Proxy tasks may not capture the complexity of actual dual-use capabilities; loss metrics may not reflect downstream task performance.
  - **What evidence would resolve it**: Training sufficiently large models with SGTM targeting CBRN-relevant domains and evaluating on WMDP or similar dangerous capability benchmarks.

## Limitations

- **Scale dependency**: Experiments were conducted on relatively small models (8M-254M parameters), and the effectiveness of SGTM at frontier scales remains unknown.
- **Proxy evaluation**: The paper relies on loss metrics as proxies for dangerous capability removal rather than direct evaluation on benchmarks like WMDP due to model size constraints.
- **In-context vulnerability**: SGTM is expected to be susceptible to in-context attacks similar to data filtering, though this was not experimentally verified.

## Confidence

- **Mechanism 1 (Selective Gradient Masking)**: High confidence - well-defined implementation and clear distinction from prior work.
- **Mechanism 2 (Self-Reinforcing Localization)**: Medium confidence - supported by gradient norm analysis but underlying geometric properties require further investigation.
- **Mechanism 3 (Adversarial Robustness)**: High confidence - strong experimental evidence showing 7× increase in fine-tuning steps required.

## Next Checks

1. **Gradient masking verification**: Print and verify that gradients to θ_retain are exactly zero during backward pass on D_forget samples.
2. **Leakage measurement**: Calculate leakage ratio (equivalent forget tokens / actual unlabeled forget tokens) across different model scales to confirm decreasing trend.
3. **Adversarial fine-tuning comparison**: Fine-tune SGTM, weak filter, strict filter, and RMU models on 50/50 forget/retain mixture and measure steps-to-baseline-forget-loss to confirm 7× robustness advantage.