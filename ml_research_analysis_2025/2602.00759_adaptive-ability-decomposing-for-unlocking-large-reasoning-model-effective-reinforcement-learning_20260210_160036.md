---
ver: rpa2
title: Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective
  Reinforcement Learning
arxiv_id: '2602.00759'
source_url: https://arxiv.org/abs/2602.00759
tags:
- rlvr
- training
- decomposer
- reasoning
- reasoner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Ability Decomposing (A2D), a method
  to enhance reinforcement learning with verifiable rewards (RLVR) for large reasoning
  models (LRMs). The core idea is to train a decomposer model via RLVR to break down
  complex questions into simpler sub-questions, which then guide the reasoning model
  during RLVR training.
---

# Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.00759
- Source URL: https://arxiv.org/abs/2602.00759
- Reference count: 40
- Primary result: A2D improves accuracy on 8 mathematical reasoning benchmarks compared to RLVR baselines like GRPO and LUFFY.

## Executive Summary
This paper introduces Adaptive Ability Decomposing (A2D), a method to enhance reinforcement learning with verifiable rewards (RLVR) for large reasoning models (LRMs). The core idea is to train a decomposer model via RLVR to break down complex questions into simpler sub-questions, which then guide the reasoning model during RLVR training. This approach addresses the limitation of RLVR where models struggle with complex reasoning due to limited exploration. The method uses an in-context distillation loss to integrate sub-question guidance without relying on external teacher models. Experiments on eight mathematical tasks with models like Qwen2.5-7B and LLaMA show that A2D consistently outperforms baselines such as GRPO, LUFFY, and Scaf-GRPO, achieving higher accuracy scores. For example, A2D improves Pass@k scores significantly compared to vanilla RLVR. The method is also shown to be adaptable to different RLVR algorithms and acts as a plug-and-play module. Analysis reveals that coarse-grained hints improve exploration, while fine-grained hints enhance exploitation.

## Method Summary
A2D consists of three phases: (1) Train a decomposer model using GRPO to generate sub-questions from complex questions, with rewards based on both formatting and quality (measured by a proxy reasoner's Pass@k score). (2) Use the trained decomposer to annotate the training dataset with sub-questions. (3) Train the reasoner using a hybrid loss combining standard RLVR with an in-context distillation loss (IDL) that conditions on the original question but optimizes for correct responses generated with sub-question hints. The method uses a threshold to conditionally apply IDL only for "hard" samples to prevent over-reliance on hints.

## Key Results
- A2D outperforms GRPO, LUFFY, and Scaf-GRPO on 8 mathematical reasoning benchmarks
- Pass@k scores improve significantly compared to vanilla RLVR
- The method is adaptable to different RLVR algorithms and acts as a plug-and-play module
- Coarse-grained hints improve exploration while fine-grained hints enhance exploitation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting sub-question hints increases information density during RLVR, reducing blind exploration.
- **Mechanism:** RLVR provides sparse rewards. Decomposing complex questions into sub-questions conditions the policy on intermediate reasoning steps, acting as high-density information signals without requiring an external teacher model.
- **Core assumption:** Models possess knowledge to solve sub-problems but fail to compose them due to large search spaces.
- **Evidence anchors:**
  - [abstract] "...model can only engage in largely blind exploration... provide additional information... without relying on a teacher model."
  - [section 1] "...model lacks compositional generalization... sub-question guidance can serve as one of the effective forms of additional information."
- **Break condition:** If sub-questions are irrelevant or hallucinatory, they add noise rather than useful information.

### Mechanism 2
- **Claim:** IDL allows models to retain hint benefits during training while remaining robust to their absence at inference.
- **Mechanism:** IDL optimizes likelihood of correct responses generated with hints, conditioned on the original question. This forces mapping problem structure to solution paths without becoming hint-dependent.
- **Core assumption:** Correct reasoning trajectories with hints are transferable skills that can be learned by base model parameters.
- **Evidence anchors:**
  - [section 2.3] "...internalize the correct problem-solving strategies into the inner knowledge of the reasoner... preventing overfitting."
  - [section 3.3.3] "A2D does not lead to a rapid decline in the model's exploratory ability... [compared to] GRPO + Prompt w/ SQ."
- **Break condition:** Insufficient IDL coefficient or diversity prompt may cause catastrophic forgetting or overfitting to hint wording.

### Mechanism 3
- **Claim:** Training decomposer to maximize Pass@k (vs. Pass@1) encourages coarse-grained hints that preserve exploration potential.
- **Mechanism:** Pass@1 reward encourages step-by-step solutions (fine-grained hints forcing exploitation). Pass@k encourages abstract sub-questions guiding toward correct subspace without dictating exact path.
- **Core assumption:** Trade-off exists between hint granularity and reasoner's exploration ability; total supervision limits exploration.
- **Evidence anchors:**
  - [section 3.3.5] "The model trained with our method... decomposes this problem into two sub-questions... [while] Pass@1 Reward... degenerates into step-by-step reasoning solutions."
  - [abstract] "...coarse-grained hints improve exploration, while fine-grained hints enhance exploitation."
- **Break condition:** Weak proxy reasoner makes quality reward noise, failing to select useful sub-questions.

## Foundational Learning

- **Concept:** **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** A2D is a wrapper around RLVR. Understanding baseline limitations (sparse, outcome-based rewards) is essential to grasp why A2D introduces a decomposer.
  - **Quick check question:** Can you explain why vanilla GRPO might lead to "blind exploration" and conservative policies on hard math problems?

- **Concept:** **Compositional Generalization**
  - **Why needed here:** The paper posits models fail not from lacking knowledge but from inability to compose simple skills for complex problems. A2D bridges this gap.
  - **Quick check question:** How does the paper distinguish between "solving a problem" and "composing sub-problems"?

- **Concept:** **Distillation vs. In-Context Learning**
  - **Why needed here:** A2D uses "In-context Distillation Loss" (IDL). Distinguishing between copying teacher data (SFT) and this hybrid approach (learning from own outputs under temporary guidance) is crucial.
  - **Quick check question:** Why does the paper argue that simply adding sub-questions to the prompt during training (Prompt w/ SQ) is inferior to using IDL?

## Architecture Onboarding

- **Component map:** Decomposer -> Proxy Reasoner -> Reasoner -> Verifier
- **Critical path:**
  1. **Phase 1 (Decomposer Training):** Train decomposer via GRPO with reward $R = R_{format} \times R_{quality}$ (Pass@k of proxy reasoner).
  2. **Phase 2 (Annotation):** Use trained decomposer to pre-annotate training dataset with sub-questions.
  3. **Phase 3 (Reasoner Training):** Train reasoner via RLVR + IDL. If rollout avg reward is low (< $k_1$), activate IDL: generate response with hints, and if correct, backprop on that response without hints (using diverse prompt).

- **Design tradeoffs:**
  - **Pass@k vs. Pass@1 Reward:** Pass@1 creates "solution-style" hints (high exploitation, low exploration). Pass@k creates "inspiration-style" hints (high exploration). Paper recommends Pass@k for better scaling.
  - **Prompt dependency:** Training with hints always present makes model dependent on them. A2D uses conditional threshold ($k_1$) and diverse prompts to prevent dependency.

- **Failure signatures:**
  - **Hint Collapse:** Decomposer generates full solution instead of sub-questions (check for high token count or detailed math steps).
  - **Overfitting:** Reasoner performs well on training questions with hints but fails on test questions without hints (verify IDL applied with diverse prompts).
  - **Conservative Policy:** High threshold $k_1$ causes model to use hints too often and lose own exploration ability.

- **First 3 experiments:**
  1. **Decomposer Sanity Check:** Train decomposer on small subset. Verify it generates `<subquestion>` tags and these questions are simpler than original (manual inspection).
  2. **Ablate the Reward:** Train two decomposers (Format Reward only vs. Format + Quality Reward). Check if Quality Reward version helps frozen Reasoner solve problems better.
  3. **ID Loss Validation:** Train Reasoner with "Always Hint" vs. "A2D (Conditional Hint)". Plot performance curve over steps to see if "Always Hint" plateaus early (loss of exploration).

## Open Questions the Paper Calls Out
- Can the A2D framework be effectively extended to online settings where decomposer and reasoner evolve simultaneously?
- How should the decomposer's hint granularity evolve to remain compatible with an improving reasoner during long-term training?
- Does the benefit of sub-question decomposition in RLVR transfer to non-mathematical domains like code generation or logical reasoning?

## Limitations
- Key hyperparameters (IDL coefficient α, thresholds k₁, k₂, learning rates) are unspecified, making faithful reproduction difficult
- Effectiveness of coarse vs. fine-grained hints is empirically shown but lacks deeper theoretical grounding for why Pass@k vs. Pass@1 creates this distinction
- The method may not transfer to domains outside math or to non-answer-verifiable tasks

## Confidence
- **High Confidence:** Core experimental result that A2D improves accuracy over baselines on math benchmarks is well-supported
- **Medium Confidence:** Three mechanisms are logically sound and supported by internal evidence, but critical design choices are underspecified
- **Medium Confidence:** Plug-and-play adaptability to different RLVR algorithms is plausible from design but only mentioned, not empirically tested

## Next Checks
1. **Decomposer Robustness Test:** Train two decomposers (one with only Format Reward, one with Format + Quality Reward) and verify that the Quality Reward version produces sub-questions that actually help a frozen Reasoner solve problems better. Check for reward hacking.
2. **ID Loss Dependency Test:** Compare "Always Hint" training vs. A2D's conditional hint approach. Plot performance curves to confirm that unconditional hints cause early plateauing (loss of exploration).
3. **Hint Granularity Analysis:** Implement both Pass@1 and Pass@k reward schemes for the decomposer. Analyze generated sub-questions to confirm Pass@1 yields step-by-step solutions while Pass@k yields abstract "inspiration-style" hints.