---
ver: rpa2
title: 'RAPTOR: Ridge-Adaptive Logistic Probes'
arxiv_id: '2602.00158'
source_url: https://arxiv.org/abs/2602.00158
tags:
- raptor
- steering
- accuracy
- concept
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RAPTOR is a ridge-regularized logistic probe for concept extraction\
  \ in LLMs that matches or exceeds strong baselines in accuracy while providing better\
  \ directional stability under small data perturbations and substantially lower training\
  \ cost. It uses validation-tuned \u21132 regularization to produce stable, reusable\
  \ concept vectors for activation steering."
---

# RAPTOR: Ridge-Adaptive Logistic Probes

## Quick Facts
- arXiv ID: 2602.00158
- Source URL: https://arxiv.org/abs/2602.00158
- Reference count: 40
- Primary result: Ridge-regularized logistic probes achieve ~87% accuracy, superior stability, and lower training cost for concept extraction in LLMs.

## Executive Summary
RAPTOR introduces ridge-regularized logistic probes to extract stable concept vectors from large language model (LLM) activations. By tuning the ℓ2 regularization parameter via validation, RAPTOR produces reusable activation vectors for intervention-based steering. Empirical results show that RAPTOR matches or exceeds strong baselines in both accuracy and directional stability under small data perturbations, while also reducing training time per layer. Theoretical analysis via CGMT explains the tradeoff between accuracy and stability in high-dimensional regimes. Experiments demonstrate near-perfect steering control with low intervention rates, though large per-layer strengths occur in tail cases.

## Method Summary
RAPTOR uses logistic regression with ℓ2 regularization to learn concept vectors from LLM activations. The regularization strength λ is tuned on a validation set to balance accuracy and stability. Ridge regularization shrinks weights toward zero, which dampens sensitivity to noise and small data perturbations, improving directional stability of the learned concept vectors. This makes the extracted vectors more reusable for activation steering across different contexts. RAPTOR trains faster than baselines per layer and achieves high accuracy across diverse models and datasets.

## Key Results
- Achieves ~87% best-layer accuracy across five datasets and six LLM families.
- Outperforms GCS and xRFM in stability under synthetic perturbations (e.g., 8% sign flips).
- Trains faster per layer and produces reusable activation vectors for intervention-based steering.

## Why This Works (Mechanism)
Ridge regularization reduces overfitting and improves generalization by penalizing large weights, which stabilizes the learned concept vectors against small perturbations in the input data. In high-dimensional settings, this leads to a favorable tradeoff between accuracy and robustness, as explained by the CGMT framework. The validation-tuned λ ensures the probe adapts to each concept's structure, yielding more stable and reusable vectors for downstream steering tasks.

## Foundational Learning
- **Logistic regression**: Binary classification model; needed for mapping activations to concept presence/absence; quick check: verify loss is convex and solvable via gradient methods.
- **Ridge regularization (ℓ2)**: Adds penalty on weight magnitude; needed to stabilize solutions and reduce sensitivity to noise; quick check: confirm λ controls overfitting vs. underfitting.
- **CGMT (Convex Gaussian Min-max Theorem)**: Theoretical tool for analyzing high-dimensional random systems; needed to explain accuracy-stability tradeoff; quick check: ensure assumptions (e.g., Gaussian features) approximately hold for activations.
- **Activation steering**: Intervention-based control of model behavior via vector arithmetic on activations; needed to leverage extracted concept vectors; quick check: validate steering changes target behavior as expected.
- **Directional stability**: Consistency of concept vector under small data perturbations; needed for reusability; quick check: measure cosine similarity before/after perturbation.

## Architecture Onboarding
- **Component map**: LLM activations -> Logistic probe (with ridge) -> Concept vector -> Steering intervention
- **Critical path**: Extract activations -> Train ridge probe (validate λ) -> Apply steering
- **Design tradeoffs**: Ridge vs. Lasso (stability vs. sparsity), logistic vs. linear probe (calibration vs. simplicity), tuning λ vs. fixed value (adaptivity vs. simplicity)
- **Failure signatures**: Over-regularization (low accuracy), under-regularization (instability), poor λ tuning (suboptimal balance)
- **First experiments**:
  1. Compare ridge probe accuracy with/without regularization on a simple dataset.
  2. Measure stability (cosine similarity) under synthetic perturbations for different λ.
  3. Test steering intervention rate and behavioral change for a single concept pair.

## Open Questions the Paper Calls Out
None explicitly stated in the source material.

## Limitations
- Stability results rely on a specific synthetic perturbation model (8% sign flips), not natural distribution shifts.
- CGMT analysis assumes i.i.d. Gaussian features, which may not reflect sparse, structured transformer activations.
- Steering experiments limited to a single concept pair; broader behavioral and safety implications untested.

## Confidence
- Accuracy claims: High (controlled experiments across multiple datasets/models).
- Stability under perturbations: Medium (depends on synthetic shift model).
- CGMT theoretical claims: Medium (high-dimensional limit assumptions may not hold for sparse activations).
- Steering robustness: Low (limited to single concept pair, no safety analysis).

## Next Checks
1. Test stability under natural distribution shifts (e.g., different domains or corrupted inputs) rather than synthetic sign flips.
2. Benchmark against dense linear probes and other regularization schemes on the same datasets to isolate RAPTOR's unique benefits.
3. Extend steering experiments to multiple concept pairs and downstream tasks to assess generalization and safety of interventions.