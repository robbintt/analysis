---
ver: rpa2
title: 'AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets'
arxiv_id: '2509.26185'
source_url: https://arxiv.org/abs/2509.26185
tags:
- cell
- dataset
- blood
- classification
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AttriGen introduces an automated multi-attribute annotation framework
  for blood cell datasets that combines CNN-based cell type classification with ViT-based
  morphological attribute recognition. The system achieves 94.62% accuracy on multi-attribute
  classification, approaching human expert performance while reducing annotation time
  from weeks to minutes for large datasets.
---

# AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets

## Quick Facts
- arXiv ID: 2509.26185
- Source URL: https://arxiv.org/abs/2509.26185
- Reference count: 33
- Primary result: Dual-model architecture achieves 94.62% accuracy on multi-attribute blood cell classification

## Executive Summary
AttriGen introduces an automated multi-attribute annotation framework for blood cell datasets that combines CNN-based cell type classification with ViT-based morphological attribute recognition. The system achieves 94.62% accuracy on multi-attribute classification, approaching human expert performance while reducing annotation time from weeks to minutes for large datasets. By integrating two complementary datasets (PBC for cell types and WBCAtt for morphological attributes), AttriGen produces comprehensive 12-attribute characterizations of blood cells, enabling scalable expansion of annotated medical imaging datasets with minimal human intervention.

## Method Summary
AttriGen employs a dual-model architecture that separates cell type classification (using VGG16 CNN) from morphological attribute recognition (using Swin-S Vision Transformer). The system trains on two specialized datasets: PBC (17,092 images, 8 cell types) and WBCAtt (10,298 images, 11 morphological attributes). Images are resized to 224×224 and normalized. The CNN classifies cell type with 98.83% accuracy, while the ViT recognizes 11 morphological attributes with 94.62% Global Average Accuracy. The framework enables rapid annotation of unlabeled datasets through pseudo-labeling at approximately 20 milliseconds per cell, transforming weeks of expert work into minutes.

## Key Results
- Dual-model architecture achieves 94.62% Global Average Accuracy across 11 morphological attributes
- Cell type classification accuracy reaches 98.83% using VGG16 on PBC dataset
- Annotation time reduced from weeks to approximately 2.26 minutes for 6,784 cells
- Performance gap of only 1.48% compared to human expert annotation (96.1%)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Model Complementary Feature Extraction
Separating cell type classification (CNN) from morphological attribute recognition (ViT) improves multi-attribute accuracy by leveraging architecture-matched models. CNN architectures excel at hierarchical local features via convolution, while Vision Transformers capture long-range dependencies through self-attention. This design capitalizes on the specialized information available in two distinct datasets.

### Mechanism 2: Bootstrapped Dataset Expansion via Pseudo-Labeling
A ViT trained on small expert-annotated seed data can reliably annotate unlabeled images, enabling dataset expansion at minimal human cost. The pipeline trains on WBCAtt (10,298 expert-labeled images), validates against human performance, then applies inference to unlabeled PBC images at ~20ms per cell. Once the model reaches near-human accuracy (94.62% vs. 96.1% human), generated labels become trustworthy proxies for expert annotation.

### Mechanism 3: Hierarchical Attention for Fine-Grained Attribute Differentiation
Swin Transformer's shifted-window attention enables superior performance on attributes requiring multi-scale spatial reasoning. The hierarchical windows with shifted partitions allow information exchange across regions, benefiting attributes like "granule type" (local features) and "nuclear-cytoplasmic ratio" (global spatial relationship).

## Foundational Learning

- **Convolutional Neural Networks (CNNs) and Transfer Learning**
  - Why needed here: The cell type classifier uses VGG16 pre-trained on ImageNet and fine-tuned on PBC; understanding feature extraction hierarchies and freezing strategies is essential for reproducing results.
  - Quick check question: Can you explain why early convolutional layers learn generic edge/texture detectors while deeper layers capture domain-specific patterns, and how this justifies transfer learning?

- **Vision Transformers and Self-Attention Mechanisms**
  - Why needed here: Swin-S, ViT-B/16, and DeiT variants form the attribute recognition backbone; understanding patch embeddings, multi-head attention, and shifted windows is critical for debugging and model selection.
  - Quick check question: How does the shifted-window mechanism in Swin Transformer differ from standard ViT patch processing, and why might this benefit multi-scale morphological features?

- **Multi-Label/Multi-Attribute Evaluation Metrics**
  - Why needed here: The paper introduces Global Average Accuracy (GAA) across 11 attribute heads; understanding precision/recall/F1 per attribute and aggregation methods is necessary for proper benchmarking.
  - Quick check question: Why might GAA be preferable to AUC or ACAcc for multi-attribute evaluation, and what limitations does it have when attribute classes are imbalanced?

## Architecture Onboarding

- **Component map:**
  Input Image (224×224) -> [CNN] VGG16 -> Cell Type (8 classes) -> Concatenated Output
                            -> [ViT] Swin-S -> 11 Attributes -> (12-attribute vector)

- **Critical path:** (1) Preprocessing pipeline—resize to 224×224, normalize to [0-1], convert labels via bijective mappings; (2) CNN training on PBC (80/10/10 split) with shear/zoom augmentation; (3) ViT training on WBCAtt (60/10/30 split) with flip/crop augmentation; (4) Parallel inference at 20ms/cell; (5) Output fusion.

- **Design tradeoffs:**
  - VGG16 vs. Swin-S: CNN is computationally lighter but less effective for attributes requiring global context; ViT achieves higher GAA (94.62% vs. 91.42%) at greater compute cost.
  - Dataset split ratios: WBCAtt uses 30% test (larger than PBC's 10%) due to higher attribute variability—prioritizes evaluation robustness over training data volume.
  - Assumption: The 1.48% accuracy gap vs. human experts is acceptable for time-critical applications; not validated for clinical diagnosis.

- **Failure signatures:**
  - Nucleus Shape accuracy drops to 80.51%—may indicate insufficient spatial resolution or ambiguous annotation criteria.
  - Cell Size at 84.03%—suggests sensitivity to preprocessing variations; check resizing consistency.
  - If pseudo-labeled data degrades downstream model performance, suspect systematic label noise or distribution shift between seed and target datasets.

- **First 3 experiments:**
  1. **Reproduce baseline metrics:** Train VGG16 on PBC and Swin-S on WBCAtt using specified splits; verify 98.83% and 94.62% accuracies within ±0.5% tolerance.
  2. **Ablation by architecture:** Compare Swin-S vs. ViT-B/16 vs. DeiT-B on WBCAtt to validate hierarchical attention contribution; expect Swin-S to outperform by 0.2-0.4% GAA.
  3. **Pseudo-label quality check:** Apply trained Swin-S to held-out PBC images with expert labels; measure agreement rate and identify systematic error patterns (e.g., specific attributes or cell types with higher disagreement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AttriGen maintain high accuracy when extended to pathological cells with abnormal morphologies found in leukemia and other hematologic diseases?
- Basis in paper: [explicit] The conclusion explicitly states, "Future research should explore extension to pathological cells with abnormal morphologies, which are critical for leukemia and other hematologic disease diagnosis."
- Why unresolved: The current models are trained on the PBC dataset (derived from healthy donors) and WBCAtt, which may not encompass the distribution shifts or visual anomalies present in diseased cells.
- What evidence would resolve it: Evaluation of the pre-trained dual-model architecture on independent datasets containing confirmed leukemia or myelodysplastic syndrome samples to measure performance retention.

### Open Question 2
- Question: Can active learning techniques be integrated into the AttriGen pipeline to reduce the initial seed data requirement or improve the annotation performance gap?
- Basis in paper: [explicit] The authors suggest in the conclusion that "active learning techniques could further optimize the annotation process."
- Why unresolved: The current framework relies on a standard bootstrapping approach using a fixed seed dataset rather than iterative expert-in-the-loop refinement based on model uncertainty.
- What evidence would resolve it: A comparative study showing that an active learning variant achieves higher accuracy (closer to the 96.1% human baseline) or requires fewer seed labels to achieve the current 94.62% accuracy.

### Open Question 3
- Question: Can specific architectural modifications improve the recognition of shape-based attributes, which currently show lower performance than texture-based attributes?
- Basis in paper: [inferred] Table III reveals a significant performance disparity, with "Nucleus Shape" (80.51%) and "Cell Size" (84.03%) substantially underperforming compared to attributes like "Granularity" (99.81%).
- Why unresolved: The selected Swin-S transformer may inherently prioritize local texture and color features over global geometric or shape-based morphological cues.
- What evidence would resolve it: Ablation studies utilizing shape-aware attention mechanisms or specialized geometric data augmentation demonstrating improved accuracy on the low-performing shape attributes.

### Open Question 4
- Question: Does the dual-model CNN-ViT architecture generalize effectively to other medical imaging domains with different morphological taxonomies?
- Basis in paper: [explicit] The paper claims the framework "establishes a generalizable paradigm for expanding attribute-based image classification datasets across diverse medical imaging domains."
- Why unresolved: The empirical validation is strictly limited to peripheral blood cell microscopy; cross-domain transferability remains theoretical.
- What evidence would resolve it: Successful application of the AttriGen pipeline to a distinct domain (e.g., lung CT or histopathology) using its specific attribute sets, yielding comparable annotation efficiency and accuracy.

## Limitations
- Critical hyperparameters and architectural details remain unspecified, particularly the multi-head implementation for Swin-S and learning rate schedules
- Pseudo-labeling validation assumes distribution similarity between WBCAtt and PBC datasets without rigorous domain adaptation assessment
- 1.48% accuracy gap vs. human experts is presented as acceptable but not validated for clinical decision-making contexts

## Confidence

- **High Confidence**: Dual-model architecture design (CNN for cell type, ViT for attributes) and reported baseline accuracies (98.83% for cell type, 94.62% GAA for attributes) are reproducible given the specified datasets and preprocessing.
- **Medium Confidence**: Swin-S superiority over other ViT variants is plausible based on hierarchical attention mechanisms, but requires ablation studies to confirm.
- **Low Confidence**: Pseudo-labeling quality and clinical applicability remain uncertain without systematic validation on held-out expert-annotated data and assessment of label noise propagation.

## Next Checks
1. **Reproduce baseline metrics**: Train VGG16 on PBC and Swin-S on WBCAtt using specified splits and augmentations; verify 98.83% and 94.62% accuracies within ±0.5% tolerance.
2. **Ablate Swin-S architecture**: Compare Swin-S vs. ViT-B/16 vs. DeiT-B on WBCAtt to validate hierarchical attention contribution; expect Swin-S to outperform by 0.2-0.4% GAA.
3. **Validate pseudo-label quality**: Apply trained Swin-S to held-out PBC images with expert labels; measure agreement rate and identify systematic error patterns (e.g., specific attributes or cell types with higher disagreement).