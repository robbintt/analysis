---
ver: rpa2
title: Can VLMs Detect and Localize Fine-Grained AI-Edited Images?
arxiv_id: '2505.15644'
source_url: https://arxiv.org/abs/2505.15644
tags:
- image
- images
- edited
- editing
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel vision-language approach for detecting
  and localizing fine-grained AI edits in images, addressing the limitations of traditional
  detectors that only provide global real/fake labels without spatial localization.
  To support this, the authors construct FragFake, a large-scale benchmark of AI-edited
  images spanning multiple editing models, diverse edit types, and several source
  datasets, generated via a fully automated pipeline.
---

# Can VLMs Detect and Localize Fine-Grained AI-Edited Images?

## Quick Facts
- arXiv ID: 2505.15644
- Source URL: https://arxiv.org/abs/2505.15644
- Authors: Zhen Sun; Ziyi Zhang; Zeren Luo; Zhiyuan Zhong; Zeyang Sha; Tianshuo Cong; Zheng Li; Shiwen Cui; Weiqiang Wang; Jiaheng Wei; Xinlei He; Qi Li; Qian Wang
- Reference count: 40
- Primary result: VLMs achieve near 0.99 accuracy and 70%+ object precision in detecting and localizing AI-edited images

## Executive Summary
This paper introduces a vision-language approach for detecting and localizing fine-grained AI edits in images, addressing the limitation of traditional detectors that only provide global real/fake labels. The authors construct FragFake, a large-scale benchmark of AI-edited images spanning multiple editing models and diverse edit types, generated via an automated pipeline. Through supervised fine-tuning and RLVR training of VLMs like Qwen2.5-VL, they achieve close to 0.99 accuracy and over 70% object precision in both classification and localization tasks. User studies confirm that fine-tuned VLMs significantly outperform non-expert humans in both detection and localization accuracy.

## Method Summary
The method involves constructing a large-scale benchmark called FragFake using an automated pipeline where GPT-4o generates object descriptions for edited images created by six different editing models. VLMs are adapted through supervised fine-tuning with LoRA adapters and optionally refined using RLVR with a GRPO-based reward function that includes format, classification, and localization components. The approach leverages multi-component rewards to improve output interpretability while maintaining high detection accuracy. The models are evaluated on both image-level classification and object-level localization tasks using automated and human evaluation metrics.

## Key Results
- Fine-tuned VLMs achieve image-level detection accuracy between 0.97-0.99
- Object-level localization precision exceeds 70% across all settings
- RLVR yields modest but consistent gains (F1 from 0.96 to 0.98) while improving output interpretability
- Data balancing and training size significantly affect localization performance but not binary classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning of VLMs on task-specific edited-image data enables both classification and localization
- Mechanism: LoRA adapters inject task-specific parameters while preserving pretrained knowledge, learning to map visual inconsistencies to structured text outputs
- Core assumption: Edited images leave detectable visual artifacts that VLMs can learn to associate with textual descriptions
- Evidence anchors: Fine-tuning yields very strong image-level detection (Acc typically between 0.97-0.99); limited corpus support for this specific mechanism
- Break condition: If edits become indistinguishable from natural image variation at pixel level

### Mechanism 2
- Claim: GRPO-based RLVR training improves output interpretability with modest metric gains
- Mechanism: Multi-component reward function (format 0.1, classification 0.6, localization 0.3) guides policy optimization toward structured, verifiable outputs
- Core assumption: Verifiable rewards can meaningfully shape VLM behavior for spatial reasoning tasks
- Evidence anchors: RLVR yields modest but consistent gains over SFT (F1 increases from 0.96 to 0.98); no direct corpus evidence for RLVR in image forensics
- Break condition: If automatic judge fails to capture subtle semantic differences, reward hacking may occur

### Mechanism 3
- Claim: Data balancing and training set diversity are critical for fine-grained localization but not for binary classification
- Mechanism: Balancing original and edited images prevents classifier bias; diverse editing models force model to generalize rather than memorize artifacts
- Core assumption: Editor-specific artifacts exist and can be learned; generalization requires exposure to multiple editing paradigms
- Evidence anchors: Classification accuracy remains nearly 1.00 across all sample sizes while RP and OP improve with dataset growth; detectors trained on single editing style don't generalize reliably
- Break condition: If test edits come from fundamentally novel architectures with different artifact distributions, transfer fails

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables efficient fine-tuning of large VLMs (7B parameters) on single GPUs while preserving pretrained capabilities
  - Quick check question: Can you explain why LoRA rank affects localization performance differently across model architectures?

- Concept: **Vision-Language Model Structured Output**
  - Why needed here: The task requires both binary classification and spatial localization in a single text response format
  - Quick check question: How would you design a prompt template that enforces both reasoning explanation and verifiable output format?

- Concept: **Cross-Domain Transfer in Detection**
  - Why needed here: Real-world deployment requires generalization to unseen editing models and image sources
  - Quick check question: Why does accuracy remain high (~0.8) during cross-editor transfer while object precision drops significantly?

## Architecture Onboarding

- Component map: Original images → GPT-4o instruction generation → 6 editing models → image-text pairs → LoRA adapters on Qwen2.5-VL-7B/Gemma3-4B → optional RLVR refinement → VLM-based evaluation
- Critical path: Dataset quality → balanced sampling → LoRA rank selection → SFT convergence → optional RLVR refinement
- Design tradeoffs:
  - Higher LoRA rank: More capacity but risk of overfitting (optimal varies: rank 8 for Qwen2.5-VL, rank 32 for Gemma3)
  - UQ vs UF split: UQ enforces unique objects (harder, more realistic); UF allows repetition (easier, less generalizable)
  - SFT vs RLVR: SFT efficient and strong; RLVR adds interpretability at 8× GPU cost
- Failure signatures:
  - Binary accuracy high (>0.95) but OP near random: Model learned dataset bias, not localization
  - Cross-editor OP collapse to <10%: Training data overfit to single editor's artifacts
  - Inconsistent output format: Reward function format component not being enforced
- First 3 experiments:
  1. Reproduce SFT baseline on Gemini-IG UF split with Qwen2.5-VL, targeting 0.98+ accuracy and 75%+ OP
  2. Ablate LoRA rank (8, 16, 32, 64) to identify optimal setting for chosen model architecture
  3. Test zero-shot transfer: train on Step1X-Edit, evaluate on Flux and MagicBrush to quantify generalization gap

## Open Questions the Paper Calls Out
None

## Limitations
- Automated data generation pipeline relies on GPT-4o for object localization, potentially missing fine-grained spatial details
- Limited to six editing models and four source datasets, restricting generalizability to real-world editing diversity
- RLVR approach requires 8× GPU computational overhead for modest gains, raising deployment efficiency concerns

## Confidence

- **High Confidence**: Binary classification accuracy (0.97-0.99) and cross-dataset transfer for Step1X-Edit models
- **Medium Confidence**: Object-level localization performance and cross-editor generalization
- **Low Confidence**: RLVR's practical value proposition and real-world applicability

## Next Checks
1. Conduct human evaluation of GPT-4o-generated object descriptions across different editing models to quantify annotation accuracy and consistency
2. Evaluate fine-tuned models on real-world social media images containing unknown editing tools, measuring both detection accuracy and localization precision
3. Compare SFT and RLVR models on downstream task performance per GPU-hour to quantify the practical trade-off between training cost and output interpretability improvements