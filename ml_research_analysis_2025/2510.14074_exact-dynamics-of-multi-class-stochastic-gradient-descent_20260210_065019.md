---
ver: rpa2
title: Exact Dynamics of Multi-class Stochastic Gradient Descent
arxiv_id: '2510.14074'
source_url: https://arxiv.org/abs/2510.14074
tags:
- lemma
- assumption
- bound
- then
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a framework for analyzing the training dynamics
  of one-pass stochastic gradient descent (SGD) on high-dimensional multi-class classification
  problems where data comes from a Gaussian mixture model with anisotropic covariances.
  The authors extend existing theory of high-dimensional SGD to handle Gaussian-mixture
  data with multiple classes (growing with parameter size) and general covariance
  structures.
---

# Exact Dynamics of Multi-class Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2510.14074
- Source URL: https://arxiv.org/abs/2510.14074
- Reference count: 40
- Key outcome: Derives exact deterministic dynamics for one-pass SGD on high-dimensional multi-class classification with GMM data, showing phase transitions in learning behavior based on covariance structure.

## Executive Summary
This paper develops a framework for analyzing the training dynamics of one-pass stochastic gradient descent (SGD) on high-dimensional multi-class classification problems where data comes from a Gaussian mixture model with anisotropic covariances. The authors extend existing theory of high-dimensional SGD to handle Gaussian-mixture data with multiple classes (growing with parameter size) and general covariance structures. The core method involves deriving exact deterministic dynamics for key quantities like the norm of iterates and their overlap with class means, expressed as solutions to a system of ODEs. This allows prediction of learning curves and risk for general means and covariances.

## Method Summary
The paper analyzes one-pass SGD in the proportional regime where data dimension $d$ and sample size $n$ scale linearly. The method employs a "resolvent formulation" where statistics of the SGD iterate are encoded using the resolvents of the data covariance matrices. By showing that the stochastic process evaluated on the resolvent is an "approximate solution" to an integro-differential equation, and leveraging the stability of these solutions, the authors prove that the stochastic noise averages out in high dimensions, leaving a deterministic curve. The key assumption is that covariance matrices commute, enabling simultaneous diagonalization and transformation of high-dimensional matrix dynamics into a system of $d$ coupled scalar ODEs for quantities tracking the norm in each eigendirection and overlap with class means.

## Key Results
- Derives exact deterministic ODEs describing the dynamics of one-pass SGD on GMM data with commuting covariances
- Identifies a structural phase transition: for zero-one covariance models and extreme power-law models, SGD aligns more closely with class means projected onto low-variance "clean directions"
- Shows binary logistic regression loss can decay to zero in extreme power-law regimes while saturating at non-zero constants for isotropic covariances
- Provides explicit error bounds showing SGD trajectories concentrate around deterministic curves as dimension increases

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Limit via Resolvent Approximation
In the proportional regime where data dimension $d$ and sample size $n$ scale linearly, the stochastic trajectory of one-pass SGD concentrates around a deterministic path described by a system of Ordinary Differential Equations (ODEs). The paper employs a "resolvent formulation" where statistics of the SGD iterate are encoded using the resolvents of the data covariance matrices. By showing that the stochastic process evaluated on the resolvent is an "approximate solution" to an integro-differential equation (Proposition 5), and leveraging the stability of these solutions, the authors prove that the stochastic noise averages out in high dimensions, leaving a deterministic curve.

### Mechanism 2: Alignment with Low-Variance "Clean Directions"
SGD preferentially aligns the learned parameter vector with the projection of class means onto eigenspaces of the covariance matrix with small eigenvalues (low variance), effectively treating these as "clean" signal directions. In the derived ODEs (specifically Eq. 8), the growth of the overlap $m_\rho(t)$ with the mean is coupled with the eigenvalue $\lambda^{(i)}_\rho$. For "zero-one" models (eigenvalues 0 or 1), the dynamics in the $\lambda=0$ subspace ($m_{00}$) experience no noise penalty from the covariance term, allowing for continuous (e.g., logarithmic) growth in alignment, whereas high-variance directions saturate.

### Mechanism 3: Spectral Decoupling via Commuting Covariances
The learning dynamics decouple into independent scalar ODEs for each eigenvalue mode of the data covariance. The assumption that all class covariance matrices commute ($[K_i, K_j] = 0$) allows for a simultaneous diagonalization. This transforms the high-dimensional matrix dynamics into a system of $d$ coupled scalar ODEs for quantities $V_\rho$ (norm in eigendirection $\rho$) and $m_{\rho,j}$ (overlap with mean), which are computationally tractable and analyzable.

## Foundational Learning

- **Concept: Gaussian Mixture Models (GMMs) & Stein's Lemma**
  - Why needed here: The derivation of the ODEs (Eq. 8) relies heavily on computing expectations of gradients (e.g., $\mathbb{E}[\nabla f(\theta_{t,i})]$) where the argument is Gaussian. Stein's Lemma is the standard tool for these computations in high-dimensional statistics.
  - Quick check question: Given a function $f$ and a Gaussian random vector $z \sim N(0, \Sigma)$, can you express $\mathbb{E}[\nabla f(z)]$ as an expectation involving the Hessian and the covariance?

- **Concept: Resolvents in Random Matrix Theory**
  - Why needed here: The core proof strategy (Section 5) uses the resolvent $(K_i - zI)^{-1}$ to encode the covariance structure. Understanding the analytic properties of resolvents (complex integration, contours) is essential to follow the proof from Proposition 5 to Theorem 1.
  - Quick check question: What information does the trace of the resolvent, $\text{Tr}((K - zI)^{-1})$, reveal about the eigenvalue distribution of $K$?

- **Concept: Online (One-pass) vs. Multi-pass SGD**
  - Why needed here: The paper specifically analyzes *streaming* SGD where data is seen once. The "time" parameter $t = k/d$ measures samples-per-dimension. This is distinct from the optimization dynamics on a fixed dataset (multi-pass), which is covered in other literature.
  - Quick check question: In the proportional regime ($n = \alpha d$), how does the sample complexity of one-pass SGD differ qualitatively from the generalization error of empirical risk minimization on a fixed set of $n$ samples?

## Architecture Onboarding

- **Component map:** GMM Parameters ($\mu_i, K_i$) and Loss Function ($f_i$) -> Simultaneous diagonalization of commuting covariances -> ODE Solver for system (Eq 8) tracking $V_\rho(t)$ and $m_{\rho,j}(t)$ -> Risk and statistics aggregation
- **Critical path:** Solving the ODE system (8) is the bottleneck. The system is stiff for spectra with large condition numbers (power-law). Stability requires careful time-stepping.
- **Design tradeoffs:**
  - *Tractability vs. Realism:* The commuting covariance assumption enables the exact ODE formulation but restricts direct application to datasets where classes have distinct, non-aligned covariance structures.
  - *One-pass vs. Convergence:* The framework optimizes for understanding learning curves (risk over time) rather than the final convergence to a global optimum on a fixed dataset.
- **Failure signatures:**
  - *Non-concentration:* If $d$ is small, the SGD path will show high variance around the ODE prediction (Theorem 1 bounds the error by $d^{-\epsilon}$).
  - *Explosive Norm:* If the learning rate $\gamma$ is too large relative to the data covariance, the norm $V(t)$ may explode, breaking the non-explosiveness condition (Proposition 8).
- **First 3 experiments:**
  1. **Validation of Deterministic Equivalence:** Run high-dimensional SGD ($d=1000$) on a binary logistic regression task with isotropic data. Plot the trajectory of the loss $L(t)$ and overlap $m(t)$ against the numerical solution of the ODEs. Verify the error bound shrinks as $d$ increases.
  2. **Phase Transition Analysis (Zero-One Model):** Simulate the "zero-one" model (Section 3.1). Plot the projection $m_{00}(t)$ (zero-variance subspace) vs. $m_{11}(t)$ (high-variance subspace). Confirm that $m_{00}$ grows unboundedly while $m_{11}$ saturates, demonstrating the "clean direction" mechanism.
  3. **Power-Law Scaling:** Test the "extreme power-law" regime (Proposition 4). Vary the power-law exponent $\alpha$ and $\beta$. Confirm that for extreme values, the loss decays polynomially ($L(t) \sim t^{-c}$) rather than saturating, and correlate this with the spectral properties of $K$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the structural phase transition in SGD alignment hold when the covariance matrices for different classes are non-identical ($K_1 \neq K_2$)?
- Basis in paper: [explicit] Remark 5 states, "We expect that similar results hold when the two classes have different covariance matrices... We conjecture that what will determine the transition is the maximal power in $\alpha$... we leave this for future work."
- Why unresolved: The authors note the analysis becomes "not trivial" when matrices differ.
- What evidence would resolve it: An analytical proof extending the dynamics to the non-identical covariance case.

### Open Question 2
- Question: Does the structural phase transition with respect to the power-law exponent extend to general models beyond binary logistic regression?
- Basis in paper: [explicit] Page 2 and the Abstract state, "We suspect this phase transition holds in a larger generality for other models as well."
- Why unresolved: The detailed analysis is restricted to binary logistic regression and least squares.
- What evidence would resolve it: Deriving the exact dynamics and phase transition conditions for non-linear multi-class problems with general loss functions.

### Open Question 3
- Question: Can the boundedness of the ratio $a(t) = E[w_{12}]/(E[w_{12}] - E[w_{12}^2])$ (Assumption 12) be proven generically rather than assumed?
- Basis in paper: [explicit] Remark 9 notes, "We suspect that it actually holds generically in these models (i.e., there should be a way to prove it rather than assuming it)."
- Why unresolved: The proofs of Propositions 2, 3, and 4 currently rely on this technical assumption to bound the risk and alignment.
- What evidence would resolve it: A rigorous proof that the ratio $a(t)$ remains bounded for all $t$ in identity, zero-one, and power-law models.

## Limitations

- The commuting covariance assumption, while enabling exact analytic tractability, represents a significant restriction that limits direct applicability to real-world datasets where class covariances typically have misaligned eigenvectors.
- The high-dimensional limit ($d \to \infty$) analysis assumes $d$ is sufficiently large that stochastic fluctuations become negligible, with practical thresholds depending on specific covariance structures.
- The one-pass (streaming) setting studied here differs fundamentally from the more commonly analyzed multi-epoch SGD optimization setting, making direct comparison with standard SGD convergence results challenging.

## Confidence

**High Confidence:** The deterministic equivalence result (Theorem 1) is mathematically rigorous under the stated assumptions. The proof technique combining resolvent analysis with stability arguments for integro-differential equations is sound, and the concentration phenomenon in high dimensions is well-established in random matrix theory.

**Medium Confidence:** The identification of the "clean direction" mechanism and the structural phase transition is supported by both analytical results and numerical simulations, but the commuting covariance assumption limits generalizability. The phase transition behavior is exact for the restricted model class but may manifest differently in more realistic settings.

**Low Confidence:** The specific predictions for power-law covariance models (Proposition 4) rely on precise asymptotic analysis of the ODEs that may be sensitive to numerical integration errors and the treatment of expectations over Gaussian variables. Small changes in the power-law exponent could shift behavior between regimes in ways that are difficult to verify numerically.

## Next Checks

1. **Commuting Covariance Construction:** Develop a systematic method for generating GMM data with commuting covariance matrices that spans the full range of spectral properties (isotropic, zero-one, power-law). Validate that the constructed covariances actually commute numerically before running SGD simulations.

2. **Finite-Dimensional Error Analysis:** Quantify the convergence of SGD trajectories to ODE predictions as a function of dimension $d$ for different covariance structures. Systematically vary $d$ and measure both the mean squared error and the variance of the stochastic paths around the deterministic curve.

3. **Covariance Misalignment Sensitivity:** Study how the ODE predictions change when the commuting covariance assumption is violated. Introduce controlled misalignment between class covariance eigenvectors and measure the degradation in prediction accuracy, establishing when the exact solution breaks down.