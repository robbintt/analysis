---
ver: rpa2
title: Just-in-time and distributed task representations in language models
arxiv_id: '2509.04466'
source_url: https://arxiv.org/abs/2509.04466
tags:
- task
- representations
- tasks
- transferrable
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how language models represent and use tasks
  during in-context learning. It compares two types of task representations: one that
  identifies the task type, and another that is transferrable and enables models to
  perform the task without examples.'
---

# Just-in-time and distributed task representations in language models

## Quick Facts
- arXiv ID: 2509.04466
- Source URL: https://arxiv.org/abs/2509.04466
- Reference count: 28
- This paper studies how language models represent and use tasks during in-context learning, finding that transferrable task representations form sporadically just before answer generation while task identity representations persist throughout the prompt.

## Executive Summary
This paper investigates how language models represent tasks during in-context learning, comparing task identity (stable, decodable throughout) with transferrable task representations (enable zero-shot recontextualization). The key finding is that transferrable representations form sporadically at key tokens rather than accumulating monotonically, while task identity representations remain stable. Evidence accrual across examples is reflected in transferrable representations, but the effect is inconsistent and often limited to small task scopes. These findings suggest models use both passive task sensitivity and active, localized task representations, with implications for understanding and controlling model behavior.

## Method Summary
The study uses activation patching to extract residual stream activations from the colon token preceding answer generation in few-shot prompts, then injects these into zero-shot prompts to test transferrability. Researchers search for the "best layer" for each task using development sets, then evaluate recontextualization accuracy (exact string match) versus few-shot baselines. The approach is applied across 14 simple tasks and longer-generation tasks, comparing transferrable task representations with task identity representations decoded via linear probes. Temporal analysis examines representation formation at various token positions, while scope analysis investigates representation behavior in multi-subtask contexts.

## Key Results
- Transferrable task representations form sporadically at key tokens rather than accumulating monotonically across the context
- Task identity representations are stable and decodable throughout the prompt
- Evidence accrual across examples is reflected in transferrable representations, but the effect is inconsistent and often limited to small task scopes
- In complex or multi-subtask cases, representations are more distributed or fail to transfer fully

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferrable task representations form sporadically at key tokens rather than accumulating monotonically across the context.
- Mechanism: Models maintain two parallel task representations—a passive, persistent task identity signal decodable throughout the context, and an active, transferrable representation that only crystallizes at specific tokens (primarily the colon before answer generation). This "just-in-time" formation allows models to remain task-sensitive without continuously maintaining computationally expensive transferrable states.
- Core assumption: The residual stream at intermediate layers can encode task information that, when patched to a new context, reactivates the appropriate inference circuits.
- Evidence anchors:
  - [abstract] "transferrable task representations evolve in non-monotonic and sporadic ways, while task identity representations persist throughout the context"
  - [section 4.2] "transferrable task representations generally do not form in the residual activations across layers in these [non-key] tokens"
  - [corpus] Related work on induction heads (Olsson et al., 2022) shows ICL relies on specialized circuits, but corpus evidence on sporadic activation timing is limited.
- Break condition: Tasks requiring state-tracking (e.g., counting tasks) may not form effective local transferrable representations; more complex tasks may require distributed representations across multiple tokens/layers.

### Mechanism 2
- Claim: Evidence accrual from multiple in-context examples condenses into transferrable representations that converge toward stable task encodings.
- Mechanism: As more examples are provided, models aggregate demonstration information into the key token representation. This condensed representation decreases in variance and magnitude, suggesting denoising toward a stable task vector. The transferrable accuracy ratio (recontextualized/few-shot) remains relatively stable across shot counts, indicating proportional evidence condensation.
- Core assumption: The extraction method (single token, single layer) captures sufficient task information; distributed representations don't dominate for the studied tasks.
- Evidence anchors:
  - [abstract] "They successfully condense evidence when more examples are provided in the context. But this evidence accrual process exhibits strong temporal locality"
  - [section 4.1] "Task vectors extracted from the last colon token in each example capture evidence accrual on most tasks (12 out of 14)"
  - [corpus] Bakalova et al. (2025) identify a "contextualize-then-aggregate" circuit that supports evidence aggregation.
- Break condition: "Hard-to-transfer" tasks (2/14 simple tasks: counting tasks) show minimal evidence accrual despite behavioral improvement with more shots—suggesting these tasks require state-tracking not capturable in local representations.

### Mechanism 3
- Claim: Transferrable task representations exhibit semantic scope locality, often capturing only minimal task boundaries (e.g., first subtask in multi-task contexts).
- Mechanism: In longer-generation or mixed-task contexts, transferrable representations extracted from the pre-answer colon token primarily encode the immediate subtask. Subsequent subtask representations form at later delimiter tokens (e.g., commas before subsequent answers). The reinstated context "fades" over generation, requiring re-activation at subtask boundaries.
- Core assumption: Task scope is defined by semantically independent subtask boundaries; the model defers representing later subtasks until needed.
- Evidence anchors:
  - [abstract] "In some cases, transferrable task representations also show semantic locality, capturing a small task 'scope' such as an independent subtask"
  - [section 4.3] "In mixed-generation tasks... all models form strong local task representations that only encapsulate the first subtask"
  - [corpus] Corpus evidence on scope locality is limited; Tikhonov et al. (2025) suggest multi-vector approaches may be needed.
- Break condition: Tasks with repeated identical operations (e.g., ANTONYM X3) show better sustained transfer than mixed-task chains, suggesting scope locality is modulated by semantic heterogeneity across subtasks.

## Foundational Learning

- Concept: **Activation Patching (Causal Intervention)**
  - Why needed here: The paper's core methodology relies on extracting residual stream activations from one context and injecting them into another to test transfer. Understanding this is essential to interpret all reported results.
  - Quick check question: If you patch a task vector from an 8-shot antonym prompt onto a zero-shot prompt, what behavior would indicate successful transfer?

- Concept: **Residual Stream & Layer Selection**
  - Why needed here: Task vectors are extracted from specific layers (typically middle layers) of the residual stream. The paper searches for "best layers" per task, and results vary by extraction/injection site.
  - Quick check question: Why might middle layers capture more transferrable task information than early or late layers?

- Concept: **In-Context Learning (ICL) Dynamics**
  - Why needed here: The paper investigates how task representations evolve as more examples are added. Understanding baseline ICL behavior (performance improves with more shots) provides context for interpreting evidence accrual findings.
  - Quick check question: What behavioral pattern would you expect if transferrable representations were NOT condensing evidence across examples?

## Architecture Onboarding

- Component map:
  - Task vector extraction site: Colon token (`:`) preceding answer generation in few-shot prompts
  - Task identity decoding sites: All format tokens (Q, `:`, A, `\n`) across layers
  - Injection target: Corresponding token position in zero-shot prompts at the "best layer" (identified via development set search)
  - Alternative extraction: Function vectors (summed attention head activations) show similar locality patterns

- Critical path:
  1. Identify task set and prepare few-shot prompts (k ∈ {0, 1, 2, 4, 8, 16, 32})
  2. For each task, search for best layer using 50 dev queries at k=8
  3. Extract task vectors from last colon token at best layer
  4. Patch onto zero-shot prompt colon token at same layer
  5. Evaluate recontextualization accuracy vs. few-shot baseline
  6. For temporal analysis: repeat extraction from other format tokens

- Design tradeoffs:
  - Single-token vs. multi-token extraction: Paper uses single-token extraction for tractability; acknowledges some tasks may require distributed representations
  - Patching vs. additive injection: Task vectors use overwriting patch; function vectors use additive injection—results broadly similar but Gemma3 incompatible with function vector approach
  - Exact match evaluation: Underestimates performance on tasks with multiple valid outputs but enables consistent comparison

- Failure signatures:
  - Hard-to-transfer tasks: Counting tasks (COUNT_COLOR_IN_3, COUNT_FRUIT_IN_3) show <50% accuracy recovery even at 32-shot
  - Scope decay: Mixed-generation tasks show near-zero accuracy on outputs beyond the first subtask
  - Model-specific patterns: Qwen3 shows partial transfer from non-key tokens; Gemma3 does not
  - Layer mismatch: Using wrong layer reduces transfer accuracy substantially

- First 3 experiments:
  1. Replicate basic transfer on your model: Extract task vectors from 8-shot antonym prompts at middle layers, patch onto zero-shot, verify recontextualization accuracy >60% of few-shot baseline.
  2. Temporal locality test: Extract from non-colon tokens (Q, first `:`, A, `\n`), patch onto corresponding zero-shot positions—expect near-zero recontextualization for most tasks.
  3. Evidence accrual curve: Plot recontextualization accuracy vs. k (0, 1, 2, 4, 8, 16, 32 shots) for 3-5 simple tasks; verify monotonic increase for easy-to-transfer tasks but flat curve for counting tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic factors drive the strong temporal and scope locality of transferrable task representations?
- Basis in paper: [explicit] The discussion section explicitly identifies understanding the "mechanistic bases for the strong temporal and scope locality" as an "intriguing direction for future work," hypothesizing it might relate to residual stream stability.
- Why unresolved: The paper observes the "just-in-time" phenomenon empirically but does not identify the specific circuit elements or training pressures that cause models to defer task representation instantiation.
- What evidence would resolve it: Ablation studies tracing the gradients or attention heads responsible for suppressing task vectors at non-key tokens, or training analysis showing that locality emerges to optimize computational efficiency.

### Open Question 2
- Question: Do distributed intervention methods reveal global task representations that single-token patching misses?
- Basis in paper: [explicit] The limitations section notes that the conclusions are "bounded by the effectiveness of these methods" and speculates that "representations for some task contexts may be more distributed, either across tokens and/or across model layers."
- Why unresolved: The study relies primarily on single-token, single-layer activation patching, which may fail to capture the full task state required for complex, state-tracking tasks.
- What evidence would resolve it: Applying multi-site activation patching (e.g., simultaneously intervening on multiple layers or tokens) to the "hard-to-transfer" tasks to see if global representations exist but are spatially distributed.

### Open Question 3
- Question: Do these sporadic representation dynamics persist in naturalistic, non-decomposable tasks?
- Basis in paper: [explicit] The limitations section states, "It's possible that many of the dynamics we observe here would not generalize to settings with naturalistic languages, especially when the tasks are not cleanly decomposable."
- Why unresolved: The experiments utilize structured tasks (e.g., antonyms, translation) with clear subtask boundaries, whereas real-world reasoning often involves semantic ambiguity and continuous context.
- What evidence would resolve it: Replicating the temporal analysis of task vectors on open-ended generation tasks (e.g., summarization or creative writing) to determine if "just-in-time" activation occurs without distinct subtask boundaries.

## Limitations

- The study focuses exclusively on task representation formation before the first answer token, leaving open questions about how representations evolve during multi-token or multi-step reasoning.
- Results depend heavily on single-token, single-layer extraction, which may miss distributed representations required for complex, state-tracking tasks.
- Findings are based on three Gemma V3 models and 14 simple tasks, raising questions about generalizability to more complex tasks or different model architectures.

## Confidence

**High Confidence**:
- Task identity representations are stable and decodable throughout the prompt
- Transferrable representations form sporadically rather than monotonically
- Evidence accrual is reflected in transferrable representations for simple tasks
- Hard-to-transfer tasks exist (particularly counting tasks)

**Medium Confidence**:
- Evidence accrual is temporally localized to key tokens
- Semantic locality of transferrable representations
- Model-specific patterns in representation formation

**Low Confidence**:
- Generalization to more complex or mixed-task scenarios
- The relationship between distributed representations and transferrability
- Whether current extraction methods capture the full representational landscape

## Next Checks

1. **Multi-token Representation Test**: Extract and patch task vectors from multiple consecutive tokens (rather than single tokens) around the answer-colon position. Compare transfer accuracy to single-token extraction to assess whether distributed representations improve recontextualization.

2. **Extended Temporal Analysis**: Perform activation patching at non-delimiter tokens during multi-token answer generation. Test whether representations formed at the initial colon successfully transfer to intermediate answer tokens, or whether new representations form during generation.

3. **Cross-task Transferability**: Extract task vectors from one task (e.g., ANTONYM) and patch them onto prompts from semantically related tasks (e.g., SYNONYM). Measure whether transferrable representations capture task family information beyond individual task identity.