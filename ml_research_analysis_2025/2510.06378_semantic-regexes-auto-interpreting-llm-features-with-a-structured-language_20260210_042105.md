---
ver: rpa2
title: 'Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language'
arxiv_id: '2510.06378'
source_url: https://arxiv.org/abs/2510.06378
tags:
- feature
- semantic
- example
- language
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semantic regexes, a structured language for
  automatically describing LLM features. The core idea is to use a constrained set
  of primitives (symbols, lexemes, fields) combined with modifiers (context, composition,
  quantification) to capture activation patterns more precisely than natural language.
---

# Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language

## Quick Facts
- arXiv ID: 2510.06378
- Source URL: https://arxiv.org/abs/2510.06378
- Reference count: 40
- Key outcome: Semantic regexes perform on par with natural language baselines for describing LLM features, yielding more concise (median 41 chars vs 139) and consistent (33.6% identical vs 0.0%) descriptions while helping users build more accurate mental models

## Executive Summary
This paper introduces semantic regexes, a structured language for automatically describing LLM features extracted via sparse autoencoders. The method uses a constrained vocabulary of primitives (symbols, lexemes, fields) with modifiers (context, composition, quantification) to generate concise, consistent descriptions that match natural language accuracy. Semantic regexes are embedded in a standard automated interpretability pipeline with an explainer model generating descriptions and an evaluator model scoring them. Across three models and 100 features per layer, semantic regexes achieve parity with natural language baselines while offering advantages in conciseness, consistency, and ability to quantify feature complexity across layers.

## Method Summary
The method operates within an automated interpretability pipeline where an explainer model (gpt-4o-mini) generates semantic regex descriptions from activating examples, and an evaluator model (gpt-4o-mini) scores these descriptions using detection, fuzzing, clarity, responsiveness, purity, and faithfulness metrics. For each SAE feature, the top 10 activating examples are collected, highlighted with 30% activation thresholds, and formatted with delimiters. The explainer is prompted with few-shot semantic regex examples and asked to produce descriptions in a structured format. The evaluator then assesses generated descriptions across multiple metrics using positive and random examples. The approach is tested on GPT-2-Small with 24,576 SAE features and Gemma-2-2B with 16k and 65k SAE features.

## Key Results
- Semantic regexes match natural language accuracy on discrimination (detection, fuzzing, responsiveness, purity), generation (clarity), and faithfulness metrics
- Semantic regex descriptions are significantly more concise (median 41 characters vs 139 for max-acts) and consistent (33.6% identical vs 0.0%)
- User study with 24 participants shows semantic regex descriptions help build more accurate mental models for 9 of 12 tested features
- Feature complexity, measured by component count and abstraction level, increases steadily across layers

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Induced Consistency
- Claim: Limiting the description language to a structured vocabulary increases consistency across descriptions of similar features
- Mechanism: The finite set of primitives and modifiers restricts the hypothesis space, reducing variability in how the explainer model describes equivalent activation patterns
- Core assumption: Feature activation patterns that are functionally similar can be mapped to the same constrained expression without significant loss of information
- Evidence anchors: Semantic regexes yield identical descriptions 33.6% of the time vs 0.0% for max-acts; related work identifies consistency as a key failure mode of natural language descriptions

### Mechanism 2: Abstraction Hierarchy Mirrors Feature Complexity
- Claim: The layered abstraction in semantic regex primitives tracks increasing feature complexity across model depth
- Mechanism: Early-layer features map to low-abstraction primitives (symbols); later-layer features require higher-abstraction primitives (fields) and compositional combinations
- Core assumption: Feature complexity correlates with the number and abstraction level of components needed to describe them accurately
- Evidence anchors: Average number of components per semantic regex steadily increases across layers; decrease in low-level primitives and increase in fields observed

### Mechanism 3: Explicit Patterning Reduces Human Ambiguity
- Claim: Structured descriptions help users form more accurate mental models by explicitly encoding pattern boundaries
- Mechanism: Natural language descriptions often include irrelevant details or leave boundaries implicit; semantic regexes expose exact composition and quantification rules
- Core assumption: Users can learn the minimal syntax quickly and apply it to reason about activation boundaries
- Evidence anchors: Participants scored higher using semantic regex descriptions on 9 of 12 features; natural language often introduced extraneous details that misled participants

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs) and Feature Extraction**
  - Why needed here: The method operates on SAE-extracted features, assuming these features represent monosemantic concepts
  - Quick check question: Can you explain why SAE features are preferred over raw neurons for interpretability, and what "monosemanticity" means in this context?

- Concept: **Automated Interpretability Pipelines**
  - Why needed here: Semantic regexes plug into the standard pipeline of explainer → evaluator → subject model
  - Quick check question: Describe the three roles (explainer, evaluator, subject) and how they interact in the described methodology

- Concept: **Regular Expression Semantics**
  - Why needed here: Semantic regexes extend regex concepts (quantification, alternation, sequencing) to semantic primitives
  - Quick check question: What is the difference between a traditional regex character class and a semantic regex "field" primitive?

## Architecture Onboarding

- Component map: Subject model -> SAE features -> Activating examples -> Explainer model -> Semantic regexes -> Evaluator model -> Evaluation metrics
- Critical path: 1) Sample activating examples for each SAE feature, 2) Format examples with highlighted activating regions, 3) Prompt explainer with few-shot semantic regex examples + activating data, 4) Parse and validate generated semantic regex syntax, 5) Run evaluation metrics using evaluator model
- Design tradeoffs: Conciseness vs. completeness (lower thresholds produce more consistent highlights but may include noise), number of examples (more may improve accuracy but increases cost), language expressiveness (more primitives increase coverage but reduce consistency benefits)
- Failure signatures: Overly broad descriptions (high detection/recall but low clarity), overly narrow descriptions (high clarity but low detection), syntactic errors (malformed semantic regexes), context misapplication (context modifier applied when redundant)
- First 3 experiments: 1) Apply semantic-regex to different architecture (Llama, Mistral) with existing SAE features; compare against max-acts, 2) Vary highlighting threshold (20%, 30%, 50%, 70%) and measure impact on description consistency and accuracy, 3) Select pairs of features from different layers with high activation correlation; measure whether semantic regexes produce identical descriptions more often than natural language baselines

## Open Questions the Paper Calls Out

- Can semantic regexes effectively describe features in models larger than 2B parameters (e.g., frontier LLMs with 70B+ parameters)? The authors expect the language will evolve and only studied GPT-2-Small and Gemma-2-2B-RES-16k/65k, but feature complexity patterns may differ qualitatively in much larger models.
- Can the semantic regex language be extended to describe non-residual-stream components such as attention heads or MLP neurons? Current primitives assume token-adjacent activation patterns; attention head features may require different primitives capturing query-key-value relationships.
- How can the conciseness-completeness trade-off be addressed when minimal semantic regexes fail to convey sufficient context to human users? The user study found semantic regexes were "accurate but too minimal to elicit strongly activating phrases" for 2 of 12 features.
- Do semantic regexes improve consistency and redundancy detection in circuit identification tasks compared to natural language descriptions? While consistency at individual feature level was demonstrated, whether this translates to practical benefits in downstream circuit analysis workflows remains untested.

## Limitations

- The semantic regex language may not fully capture complex or polysemantic features that require nuanced natural language descriptions
- The automated evaluation pipeline using gpt-4o-mini as both explainer and evaluator introduces potential bias, as the same model judges its own outputs
- The user study sample size (24 participants) is small and may not represent diverse user populations with varying regex familiarity
- The method's dependence on pre-extracted SAE features means its effectiveness is bounded by the quality and monosemanticity of those features

## Confidence

- **High Confidence**: Core empirical findings comparing semantic regexes to natural language baselines (conciseness, consistency, accuracy metrics) are well-supported by experimental results and methodology is clearly specified
- **Medium Confidence**: User study results showing improved mental model accuracy with semantic regex descriptions are suggestive but limited by sample size and lack of long-term retention testing
- **Low Confidence**: Claims about semantic regexes providing a "readout of feature complexity across layers" are correlational rather than causal, and the mechanism linking abstraction hierarchy to actual feature complexity is not rigorously validated

## Next Checks

1. **Cross-Architecture Generalization Test**: Apply semantic regexes to a completely different model family (e.g., Llama or Mistral) with their own SAE features, comparing against max-acts across all evaluation metrics to test if the structured language approach generalizes beyond the tested architectures.

2. **Ablation Study on Activation Thresholds**: Systematically vary the activation threshold for highlighting activating examples (20%, 30%, 50%, 70%) and measure the impact on description consistency, accuracy, and user mental model formation to determine optimal parameters.

3. **Polysemanticity Stress Test**: Select known polysemantic features from the datasets and evaluate whether semantic regexes can capture multiple interpretations without collapsing them inappropriately, comparing against natural language descriptions that might handle polysemanticity better.