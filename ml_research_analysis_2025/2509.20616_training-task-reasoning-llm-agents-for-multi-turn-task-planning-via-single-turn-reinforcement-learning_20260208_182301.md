---
ver: rpa2
title: Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn
  Reinforcement Learning
arxiv_id: '2509.20616'
source_url: https://arxiv.org/abs/2509.20616
tags:
- policy
- task
- expert
- grpo
- single-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training LLM agents for complex
  multi-turn task planning by transforming it into single-turn task reasoning problems.
  The core method uses Group Relative Policy Optimization (GRPO) with dense, verifiable
  rewards derived from expert trajectories to efficiently optimize policies.
---

# Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.20616
- Source URL: https://arxiv.org/abs/2509.20616
- Reference count: 26
- Primary result: 1.5B model trained with single-turn GRPO achieves 70% success rates on long-horizon planning tasks, outperforming 14B baseline models

## Executive Summary
This paper addresses the challenge of training LLM agents for complex multi-turn task planning by transforming it into single-turn task reasoning problems. The core method uses Group Relative Policy Optimization (GRPO) with dense, verifiable rewards derived from expert trajectories to efficiently optimize policies. Theoretical analysis proves that GRPO improvements on single-turn reasoning provide a lower bound for multi-turn success probability. Experiments on the Robotouille benchmark demonstrate that a 1.5B parameter model trained with single-turn GRPO achieves 70% success rates for long-horizon planning tasks, outperforming larger baseline models up to 14B parameters.

## Method Summary
The method transforms multi-turn task planning into single-turn task reasoning problems using a two-stage pipeline. First, expert trajectories are generated by a large model (Llama3.3-70B) via rejection sampling and filtered for minimal steps. Second, a 1.5B model undergoes SFT (8 epochs, batch 8) followed by GRPO (batch 256, lr=1e-6, Î²=0.001, 50 epochs). The GRPO uses group-based advantage estimation with binary rewards that check if the action matches the expert action. The approach includes a KL penalty to prevent policy collapse and operates on ReAct-formatted state-action tokens.

## Key Results
- 1.5B GRPO model achieves 70% success rate on 23-step Double Cheese Burger task
- Outperforms 14B baseline models across all three Robotouille tasks (Burger, Cheese Burger, Double Cheese Burger)
- Strong cross-task generalization when trained on most complex task
- Demonstrates robustness to noisy training trajectories

## Why This Works (Mechanism)

### Mechanism 1
Converting sparse, long-horizon MDPs into dense, single-turn bandit problems enables efficient policy optimization. The method decomposes multi-turn tasks into single-turn MDPs where the agent receives binary rewards at every step by matching expert actions, transforming the difficult credit assignment problem into a per-step classification problem.

### Mechanism 2
Group Relative Policy Optimization amplifies success probability by using group-based advantage estimation without a critic network. For each state, the policy generates multiple actions and calculates advantage by normalizing reward against group mean and variance, providing stable gradients that up-weight correct actions relative to group performance.

### Mechanism 3
Improving single-turn reasoning success provides a mathematically proven lower bound on multi-turn success probability. The paper establishes a recursion where multi-turn success is the product of single-turn successes, proving that GRPO's guaranteed increase in single-turn success probability provides a lower bound on multi-turn success that degrades linearly with task horizon.

## Foundational Learning

### Concept: Markov Decision Processes (MDP) & Rewards
- **Why needed here**: The paper reformulates standard multi-turn MDP (sparse rewards) into single-turn MDP (dense rewards). Understanding this distinction is required to grasp why standard RL struggles with long horizons and how this method solves it.
- **Quick check**: Can you explain the difference between sparse reward signals (given only at the end) and dense reward signals (given at every step) and how it affects learning speed?

### Concept: On-Policy Reinforcement Learning (GRPO)
- **Why needed here**: GRPO is the engine of this method. Unlike Supervised Fine-Tuning which mimics data, GRPO explores alternatives and optimizes for relative improvement within a group of sampled outputs.
- **Quick check**: In GRPO, why do we compare a specific action's reward to the group mean rather than just using the raw reward value?

### Concept: Expert Trajectories & Optimality
- **Why needed here**: The theoretical guarantees rely on "unique optimality" of expert trajectories. If training data contains sub-optimal paths, the binary reward signal becomes noisy or misleading.
- **Quick check**: Why does the paper insist on "minimal trajectory length" for defining the expert policy?

## Architecture Onboarding

### Component map:
Expert Generator (Llama3.3-70B) -> Rejection Sampling & Filtering -> Single-Turn Dataset (s_t, a^GT_t) -> SFT (8 epochs) -> GRPO (50 epochs) -> Policy Model (Qwen2.5-1.5B)

### Critical path:
The Rejection Sampling & Filtering step is critical. If the "expert" trajectories are not truly optimal or minimal, the theoretical lower bound does not hold, and the agent learns sub-optimal habits.

### Design tradeoffs:
- Efficiency vs. Robustness: Single-turn training is computationally efficient but assumes training state distribution covers execution distribution
- Model Size: Demonstrates 1.5B model beating 14B baselines, suggesting reasoning-specific optimization is more efficient than raw parameter scaling

### Failure signatures:
- State Drift: Performance degrades if execution states are far from training states in TV distance
- Generalization Gap: Training on simpler tasks (Burger) does not generalize to complex tasks (Double Cheeseburger); must train on hardest expected horizon

### First 3 experiments:
1. SFT Baseline: Train 1.5B model on expert trajectories using standard SFT (no RL). Measure Success Rate (SR).
2. GRPO Uplift: Apply GRPO on SFT checkpoint. Verify if SR increases and step count decreases (approaching minimal turns).
3. Noise Ablation: Inject noise into expert trajectories and observe GRPO training robustness to sub-optimal data.

## Open Questions the Paper Calls Out
- Can the single-turn GRPO framework be effectively adapted for environments where optimal expert trajectories are unavailable or expensive to generate?
- How does the theoretical lower bound on success probability degrade when the environment transition function is stochastic rather than deterministic?
- Does the performance lower bound remain tight when the state distribution shift violates the Bounded Policy with State Proximity assumption?

## Limitations
- Theoretical lower bound relies on Assumption 3.1 regarding bounded Total Variation distance between training and execution state distributions, which may not hold for diverse environments
- Evaluation limited to three relatively simple cooking tasks with known object locations and limited combinatorial complexity
- Binary reward function assumes expert trajectories are uniquely optimal without rigorous validation beyond minimum-step filtering

## Confidence
- High Confidence: Experimental results showing 1.5B GRPO model outperforming 14B baseline models on Robotouille benchmark
- Medium Confidence: Theoretical analysis proving the lower bound on multi-turn success probability, depending heavily on Assumption 3.1
- Medium Confidence: Cross-task generalization results, limited to three tasks making broader claims tentative

## Next Checks
1. **State Distribution Coverage Analysis**: Quantify Total Variation distance between training and test state distributions across multiple environments to empirically validate Assumption 3.1 and measure performance degradation as distance increases.

2. **Reward Signal Robustness Test**: Generate and evaluate trajectories with known sub-optimal steps to measure how the binary reward function handles imperfect expert demonstrations. Compare GRPO performance with noisy vs. clean expert trajectories.

3. **Horizon Scalability Experiment**: Evaluate the method on tasks with significantly longer horizons (50+ steps) to test the linear degradation claim in Theorem 3.3 and identify practical limits of the single-turn decomposition approach.