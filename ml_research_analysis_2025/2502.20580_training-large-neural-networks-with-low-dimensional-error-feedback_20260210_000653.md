---
ver: rpa2
title: Training Large Neural Networks With Low-Dimensional Error Feedback
arxiv_id: '2502.20580'
source_url: https://arxiv.org/abs/2502.20580
tags:
- feedback
- error
- learning
- latexit
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a low-dimensional feedback alignment (LDFA)
  framework that enables training large neural networks with significantly compressed
  error signals. The method factorizes the feedback pathway as B = QP with rank r,
  constraining the error dimensionality while maintaining high-dimensional forward
  representations.
---

# Training Large Neural Networks With Low-Dimensional Error Feedback

## Quick Facts
- **arXiv ID:** 2502.20580
- **Source URL:** https://arxiv.org/abs/2502.20580
- **Authors:** Maher Hanut; Jonathan Kadmon
- **Reference count:** 40
- **Primary result:** LDFA achieves near-backpropagation performance on CNNs and transformers with feedback ranks proportional to task dimensionality rather than network width.

## Executive Summary
This work introduces Low-Dimensional Feedback Alignment (LDFA), a framework that enables training large neural networks using significantly compressed error signals. The key innovation is factorizing the feedback pathway as B = QP with rank r, constraining error dimensionality while maintaining high-dimensional forward representations. Through theoretical analysis of linear networks and extensive experiments, the authors demonstrate that learning the feedback subspace via Oja's rule recovers backpropagation-like dynamics, achieving BP-level accuracy while reducing backward-pass FLOPs by up to 20%. The approach also reveals that error dimensionality acts as an inductive bias, shaping learned representations from orientation-selective to center-surround receptive fields.

## Method Summary
LDFA factorizes the feedback matrix B as B = QP where P ∈ R^(r×m) projects the m-dimensional output error into an r-dimensional channel, and Q ∈ R^(k×r) mixes this compressed signal back to the k-dimensional hidden layer. The method supports two variants: a normative version that trains Q and P to minimize ||QP - W^⊤||²_F (requiring weight access), and a local version using Oja's rule to learn P while keeping Q fixed random. For convolutional layers, P operates as a 1×1 convolution projecting error channels, with covariance aggregated over batch and spatial dimensions. The approach maintains standard forward weights W updated via backpropagation, while the backward pass uses the low-rank factorization to dramatically reduce computation.

## Key Results
- LDFA achieves near-backpropagation accuracy on CIFAR-10 and CIFAR-100 with feedback ranks proportional to task dimensionality (number of classes) rather than network width
- Backward-pass FLOPs reduced by up to 20% without accuracy loss when using optimal rank constraints
- Error dimensionality acts as inductive bias, transforming receptive fields from orientation-selective to center-surround patterns as rank decreases
- Direct error projection (dLDFA) enables non-reciprocal feedback pathways while maintaining accuracy, supporting biologically plausible implementations

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Feedback Factorization Creates a Controllable Error Bottleneck
Decomposing feedback as B = QP with rank r constrains error dimensionality while preserving high-dimensional forward representations, decoupling feedback cost from network width. The matrix P ∈ R^(r×m) projects the m-dimensional output error into an r-dimensional channel, and Q ∈ R^(k×r) mixes this compressed signal back to the k-dimensional hidden layer. This reduces backward-pass multiplication cost from O(km) to O(r(k+m)). Most task-relevant error information resides in a low-dimensional subspace tied to task complexity rather than network capacity. Fixed random P fails when r < m; spurious fixed points emerge because the stationarity condition becomes underdetermined.

### Mechanism 2: Learning the Error Subspace via Oja's Rule Recovers BP-Like Convergence
Training P to track the principal subspace of the error signal eliminates spurious fixed points and enables convergence matching backpropagation. The Oja update drives P toward the top-r eigenvectors of the error covariance, allocating limited rank budget to directions where the network makes the largest mistakes. The dominant modes of the current residual contain sufficient information for effective credit assignment. If r < task dimensionality d, performance degrades because insufficient directions are available to transmit task-relevant errors.

### Mechanism 3: Direct Error Projection Enables Non-Reciprocal Feedback Pathways
Learning only P (with fixed random Q) suffices for accurate learning even when errors bypass intermediate layers, supporting biologically plausible indirect feedback. In direct LDFA, error δ_{l'} from a source layer l' projects directly to target layer l via δ_l = (Q_{ll'} P_{ll'} δ_{l'}) ⊙ f'(a_l). The Oja rule for P_{ll'} uses only locally available teaching signals, requiring no access to forward weights. The error subspace learned at each target layer generalizes across different source layers. CNNs may require additional Hebbian adaptation of Q for robust performance; pure Oja learning of P alone shows larger gaps in convolutional settings.

## Foundational Learning

- **Concept: Feedback Alignment (FA)**
  - **Why needed here:** LDFA extends FA by adding rank constraints and subspace learning. Understanding FA's core idea—replacing W^⊤ with fixed random B—clarifies why fixed low-rank feedback fails and what the learning rules must fix.
  - **Quick check question:** Can you explain why random feedback matrices can work in FA, and why constraining them to low rank breaks this property?

- **Concept: Principal Component Analysis (PCA) via Oja's Rule**
  - **Why needed here:** The local learning rule uses Oja's rule to extract the top-r principal components of the error signal. Understanding how Hebbian learning plus orthonormalization yields online PCA is essential for implementing the P updates correctly.
  - **Quick check question:** What role does the (I - P^⊤P) term play in Eq. 15, and what biological mechanism might implement it?

- **Concept: Linear Network Dynamics and SVD Analysis**
  - **Why needed here:** The paper's theoretical foundation analyzes learning dynamics in linear networks by rotating into the singular basis of the input-output covariance. This framework predicts the r ≥ d scaling requirement and characterizes fixed points.
  - **Quick check question:** In Eq. 7, what does Λ_i = 1 signify, and why does r < m allow Λ_i to deviate from 1 for some modes?

## Architecture Onboarding

- **Component map:** Forward weights W_l unchanged → Nonlinearity f(·) → Batch norm → Backward pass with B_l = Q_l P_l (rank r) → P_l updated via Oja rule → Q_l optionally updated via Hebbian rule
- **Critical path:**
  1. Compute output error δ_L = ∂L/∂ŷ
  2. For each layer l from L-1 to 0: compute δ_l = (Q_l P_l δ_{l+1}) ⊙ f'(a_l)
  3. Update W_l: ΔW_l = η δ_{l+1} h^⊤_l
  4. Update P_l: compute error covariance C, apply Oja update
  5. Update Q_l (optional): reconstruction loss or Hebbian rule
- **Design tradeoffs:**
  - Lower r → fewer backward FLOPs but slower convergence; total compute follows U-shaped curve
  - Normative vs. local rules: normative (minimizing ||QP - W^⊤||) gives faster convergence but requires weight access; local (Oja) is biologically plausible but may need more epochs
  - Direct vs. layerwise feedback: direct enables parallel error broadcast but may require higher r for same accuracy
- **Failure signatures:**
  - Accuracy plateaus below BP level: r < task dimensionality; increase rank
  - Training instability with local rules: learning rate for P too high relative to W; use separate optimizers
  - CNN underperformance: Q may need Hebbian adaptation; pure fixed Q with Oja P insufficient
- **First 3 experiments:**
  1. Reproduce linear network mode overlap curves with r=m vs. r<m, fixed P vs. learned P; verify that learned P recovers top-r modes
  2. Train MLP on CIFAR-10 with rank constraints on all layers (r ∈ {64, 32, 16, 10}); confirm performance degrades when r drops below number of classes
  3. Train VGG-style CNN with layer-wise rank fractions (r = n/2, n/4, n/8); verify r=n/8 underperforms due to r < d=10

## Open Questions the Paper Calls Out
- **Question:** Can imposing specific structure (e.g., topographic, modular, or convolutional constraints) on the low-rank feedback pathway serve as an inductive bias to improve generalization or robustness?
- **Question:** Does LDFA remain computationally efficient when applied to tasks with high-dimensional outputs, such as semantic segmentation or generative modeling?
- **Question:** Can the proposed local learning rule for the feedback projection (P) be implemented in spiking neural networks or neuromorphic hardware?

## Limitations
- Theoretical framework relies on linear network approximations that may not fully capture nonlinear deep learning dynamics
- Experimental validation focuses primarily on CNNs and MLPs, with limited ablation studies on Hebbian Q adaptation necessity for convolutional layers
- Connection between error dimensionality as inductive bias and actual representation quality remains largely qualitative

## Confidence
- **High confidence:** Linear network theory predictions about r ≥ d requirement and spurious fixed points with fixed low-rank feedback
- **Medium confidence:** Generalization to CNNs and transformers showing BP-level accuracy with rank proportional to task complexity
- **Low confidence:** Biological plausibility claims and indirect feedback pathway benefits

## Next Checks
1. **Architecture Ablation:** Systematically test LDFA across diverse architectures (RNNs, ResNets, Vision Transformers) to verify the r ∝ d scaling holds beyond CNNs and standard MLPs
2. **Inductive Bias Quantification:** Beyond receptive field visualization, measure representation quality using canonical correlation analysis between representations learned with different feedback dimensionalities
3. **Hebbian Q Necessity:** Conduct controlled experiments isolating the impact of Q adaptation in CNNs by comparing fixed Q vs. Hebbian Q vs. learned Q (via reconstruction loss)