---
ver: rpa2
title: 'GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs'
arxiv_id: '2503.23598'
source_url: https://arxiv.org/abs/2503.23598
tags:
- genvp
- rule
- puzzle
- rules
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenVP is a generative model for Raven's Progressive Matrices (RPM)
  that learns to create and solve visual puzzles by jointly modeling the generation
  of complete RPM matrices from abstract rules and the inference of those rules from
  given puzzles. The method uses a hierarchical VAE with contrastive learning and
  a Mixture of Experts (MoE) to robustly predict rules from partial puzzle views.
---

# GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs

## Quick Facts
- arXiv ID: 2503.23598
- Source URL: https://arxiv.org/abs/2503.23598
- Reference count: 40
- Key outcome: State-of-the-art RPM-solving accuracy on five datasets with strong OOD generalization and novel puzzle generation capability.

## Executive Summary
GenVP is a hierarchical VAE that jointly learns to generate and solve Raven's Progressive Matrices by modeling rule abstraction and application. It uses contrastive learning and a Mixture of Experts to robustly predict rules from partial puzzle views, achieving state-of-the-art performance across multiple RPM datasets. The model demonstrates strong generalization to unseen attribute values and can generate coherent RPM instances from rule specifications.

## Method Summary
GenVP is a hierarchical VAE with contrastive learning that generates and solves RPM puzzles through a two-phase training process. It uses a CNN-based encoder to map 64×64 pixel images to latent variables, which are split into rule-relevant (Zo) and distractor (Z̄o) components. Four rule predictors infer rules from different puzzle views (full context, context-missing, partial rows, and puzzle-level relationships), whose outputs are aggregated via Mixture of Experts. The model is trained with ELBO loss plus global and local contrastive losses that enforce rule consistency discrimination between valid and invalid puzzle configurations.

## Key Results
- Achieves state-of-the-art RPM-solving accuracy on RAVEN, I-RAVEN, RAVEN-FAIR, PGM, and VAD datasets
- Outperforms prior generative approaches by significant margins in OOD scenarios with unseen attribute values and rule sets
- Successfully generates novel, coherent RPM instances from abstract rule specifications
- Robustness stems from multi-view rule prediction, contrastive learning, and explicit modeling of rule-relevant vs. irrelevant visual features

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Latent Disentanglement for Rule-Relevant Feature Separation
- Claim: Explicitly separating rule-relevant attributes (Zo) from distractor attributes (Z̄o) enables robust generalization to out-of-distribution scenarios with unseen attribute values.
- Mechanism: The model splits image-level latents Z into Zo (rule-relevant dimensions) and Z̄o (rule-irrelevant dimensions). During inference, only Zo feeds into rule prediction, shielding the reasoning process from distracting visual variations.
- Core assumption: Rule-relevant and rule-irrelevant features can be linearly separated in latent space through architectural constraint.
- Evidence anchors: Achieves 90.2%/91.7% accuracy on ANGLE-I (unseen angle distractors) vs. RAISE's 62.4%/73.8%.

### Mechanism 2: Mixture of Experts (MoE) for Multi-View Rule Prediction Reduces Noise Sensitivity
- Claim: Aggregating rule predictions from multiple partial puzzle views prevents single-predictor failure modes and reduces error propagation.
- Mechanism: Four predictors infer rules from: (1) full Zo, (2) context-only Z_ctx^o (one image missing), (3) partial rows Z_prow^o (one row excluded), (4) puzzle-level Zr. These are combined via weighted averaging or neural network.
- Core assumption: Each individual predictor captures complementary information; errors are uncorrelated across views.
- Evidence anchors: MoE achieves 95.0% vs. best individual predictor R(Zr) at 93.2%; R(Zo) alone drops to 48.7%.

### Mechanism 3: Dual-Level Contrastive Learning Enforces Rule Consistency Discrimination
- Claim: Contrasting valid puzzles against invalid completions (using wrong candidates) teaches the model to distinguish rule-violating configurations.
- Mechanism: Global masked contrastive loss compares rule matrices across different valid puzzles; local masked contrastive loss contrasts valid puzzle P against invalid puzzles P⁻ formed by inserting negative candidates.
- Core assumption: Negative candidates violate at least one rule; the model can learn to identify which specific attribute-rule pairs differ.
- Evidence anchors: Without contrastive terms, GenVP drops from 94.7% to 50.5% on RAVEN; PGM Neutral drops from 79.5% to 47.4%.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and ELBO Optimization
  - Why needed here: GenVP is fundamentally a hierarchical VAE; understanding ELBO terms (reconstruction, KL regularization) is required to modify the objective or debug latent collapse.
  - Quick check question: Can you explain why β-weighting in ELBO (Eq. 6) affects disentanglement quality?

- Concept: Contrastive Learning Foundations
  - Why needed here: The dual-level contrastive scheme builds on SimCLR-style positive/negative pair logic; understanding why contrastive gradients flow through R to latents is critical.
  - Quick check question: What happens to contrastive learning if all samples in a batch share identical rule matrices?

- Concept: Raven's Progressive Matrices (RPM) Structure
  - Why needed here: RPMs have specific structure (context matrix + choice list, row/column rules); understanding rule types and distractor attributes is prerequisite to debugging rule prediction failures.
  - Quick check question: Why does the O-IG configuration (nested 2×2 mesh) show lower accuracy than C-S (center single)?

## Architecture Onboarding

- Component map:
  - Encoder_Z (CNN) -> Z -> [Zo, Z̄o] -> Encoder_Zr -> Zr
  - Rule Predictors (MoE): Four NN heads predict R from Zo, Z_ctx^o, Z_prow^o, Zr; combined via weighted average or NN
  - Decoder_X (CNN) <- Z <- Zr <- R (generative path)
  - Encoder_Z and Decoder_X are CNNs mapping images to/from latents

- Critical path:
  1. Inference: X -> Z -> [Zo, Z̄o] -> Zr -> MoE(R) -> solution selection via rule-set maximization
  2. Generation: Sample R, Z̄o ~ N(0,I) -> Zr -> Zo -> Z -> decode X (complete RPM)

- Design tradeoffs:
  - Non-parametric MoE (weighted avg) vs. parametric NN: Non-parametric is more stable; NN slightly better on PGM/VAD but requires additional training
  - Zo/Z̄o split ratio: K_Zo=54 vs. K_Z̄o=10 for RAVEN; over-allocating to Zo may include distractor noise
  - Contrastive loss weights βG, βL: Too low (β=1) drops accuracy to 92.1%; too high may destabilize ELBO optimization

- Failure signatures:
  - OOD size extrapolation failure: SIZE-E accuracy drops to 45.0%/65.5% due to encoder struggling with extremely small objects
  - MoE context predictor failure: R(Z_ctx^-7), R(Z_ctx^-8) drop to 16.3%/16.7% when missing image is in same row as negative candidate
  - No contrastive training: Accuracy collapses to ~50% as model cannot distinguish valid from invalid completions
  - Complex grid configurations: O-IG, 2×2, 3×3 show lower accuracy (86.0%, 93.3%, 84.1%) due to numerical/positional rule complexity

- First 3 experiments:
  1. Ablate MoE components: Train GenVP with each individual rule predictor (R(Zo), R(Zr), R(Z_prow), R(Z_ctx)) and compare accuracy against full MoE on RAVEN-FAIR. Expected: MoE > any individual predictor.
  2. OOD stress test: Evaluate trained model on interpolated/extrapolated attribute values (angle, color, size) to verify Zo/Z̄o disentanglement. Compare against baseline without latent split.
  3. Contrastive weight sweep: Train with βG, βL ∈ {1, 10, 20, 40} and plot accuracy vs. weight. Identify stability region and check for ELBO-contrastive tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can visual reasoning models improve their ability to perform visual comprehension of numerical relationships and intricate arithmetic rules in complex grid configurations?
- Basis in paper: The authors state in the conclusion: "Improving visual comprehension of numerical relationships is an ongoing challenge, and we reserved this for future work," specifically noting struggles with intricate arithmetic rules.
- Why unresolved: Current generative models, including GenVP, treat arithmetic constraints as standard attribute rules but fail to capture the underlying quantitative logic as effectively as humans, particularly in complex layouts.
- What evidence would resolve it: An architectural extension or training paradigm that achieves significantly higher accuracy on "Arithmetic" rule types within the O-IG and 3x3 configurations compared to the current results.

### Open Question 2
- Question: Can the reliance on explicit rule annotations for training be eliminated while maintaining state-of-the-art solving and generation performance?
- Basis in paper: Section 3.3 describes training using ground-truth rule observations R_i, and Appendix G reveals a performance drop from 94.7% to 70.3% on RAVEN when these labels are withheld.
- Why unresolved: The current hierarchical structure relies on supervised alignment between the latent rule variable Zr and the provided rule matrix R to enforce disentanglement; removing this supervision degrades the model's ability to isolate relevant features.
- What evidence would resolve it: A modification to the contrastive or ELBO objective that allows the model to discover rule clusters in an unsupervised manner, recovering the performance gap observed in Appendix G.

### Open Question 3
- Question: What mechanisms are required to bridge the generalization gap between attribute interpolation and attribute extrapolation?
- Basis in paper: Table 2 demonstrates a significant disparity in out-of-distribution (OOD) performance, where Size Interpolation (83.1%) is handled well, but Size Extrapolation (45.0%) performs poorly.
- Why unresolved: The model likely fails to linearly organize the latent space for attributes like size, preventing it from generating or recognizing values outside the training distribution's statistical support.
- What evidence would resolve it: A modified decoder or regularization term that successfully maintains high accuracy (>80%) on extrapolation test sets involving attribute values strictly outside the training range.

## Limitations

- Architectural opacity: Exact CNN architectures for EncoderZ and DecoderX are referenced but not specified, creating reproducibility challenges
- Dataset specificity: Strong performance on synthetic RPM datasets may not generalize to real-world visual reasoning tasks
- OOD size extrapolation failure: Model struggles significantly with extremely small objects outside training distribution

## Confidence

- High Confidence: The hierarchical VAE framework and contrastive learning implementation are technically sound and well-validated through ablation studies
- Medium Confidence: The MoE aggregation strategy and latent disentanglement claims are supported by controlled experiments but lack direct mechanistic analysis
- Low Confidence: Claims about robustness to OOD scenarios, while supported by specific dataset results, may be overfitting to synthetic distribution shifts

## Next Checks

1. Implement GenVP with RAISE-equivalent CNN architecture and verify baseline accuracy on RAVEN dataset before adding contrastive components
2. Visualize t-SNE embeddings of Zo vs Z̄o dimensions across puzzles with different rule types to verify proposed feature separation
3. Evaluate trained GenVP model on Eye-Q or SPaRC datasets (non-RPM puzzles) to test generalization beyond synthetic RPM structures