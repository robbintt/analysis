---
ver: rpa2
title: Data-Efficient Ensemble Weather Forecasting with Diffusion Models
arxiv_id: '2509.11047'
source_url: https://arxiv.org/abs/2509.11047
tags:
- sampling
- data
- training
- weather
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how curated data selection impacts the
  performance of autoregressive diffusion models for ensemble weather forecasting.
  The authors evaluate several sampling strategies under a fixed 20% training data
  budget, comparing them against full-data training.
---

# Data-Efficient Ensemble Weather Forecasting with Diffusion Models

## Quick Facts
- **arXiv ID:** 2509.11047
- **Source URL:** https://arxiv.org/abs/2509.11047
- **Reference count:** 28
- **Primary result:** Stratified time sampling at 20% data budget achieves performance comparable to full-data training, with superior calibration (SSR) in ensemble weather forecasting.

## Executive Summary
This paper investigates how curated data selection impacts the performance of autoregressive diffusion models for ensemble weather forecasting. The authors evaluate several sampling strategies under a fixed 20% training data budget, comparing them against full-data training. Stratified time sampling—which ensures uniform representation across calendar months—achieves performance comparable to full-data training on most metrics and even outperforms it on the Spread/Skill Ratio (SSR), a key calibration metric. The approach demonstrates that simple, domain-informed sampling can significantly reduce data requirements without sacrificing accuracy, motivating future work on adaptive or model-aware data selection strategies in scientific forecasting.

## Method Summary
The study evaluates ensemble weather forecasting with autoregressive diffusion models using ERA5 reanalysis data at 5.625° resolution. The authors compare multiple data sampling strategies at a fixed 20% training budget: Random, K-Means clustering, Greedy Diverse Sampling, Stratified Time (uniform per calendar month), Spatial sampling, and Herding. The diffusion model predicts 24-hour ahead forecasts autoregressively rolled out to 10 days. Performance is measured using CRPS (probabilistic skill), RMSE (accuracy), and SSR (calibration) on a 2018 test set.

## Key Results
- Stratified time sampling achieves performance comparable to full-data training on RMSE and CRPS metrics
- Stratified time sampling outperforms full-data training on the Spread/Skill Ratio (SSR), indicating better-calibrated uncertainty estimates
- K-Means and Greedy Diverse sampling strategies show degraded performance compared to stratified time sampling
- Spatial sampling shows limited success in capturing relevant weather diversity patterns

## Why This Works (Mechanism)

### Mechanism 1: Seasonal Coverage Improves Calibration
- **Claim:** Stratified time sampling improves model calibration (SSR) by ensuring uniform exposure to seasonally-varying atmospheric patterns, preventing over-representation of any single season
- **Core assumption:** Weather phenomena exhibit strong seasonal dependence, and a representative mix of these conditions is critical for the diffusion model to learn well-calibrated uncertainty
- **Evidence anchors:** Abstract shows stratified time sampling outperforms full-data on SSR; section 4.4 attributes this to coverage across all months exposing the model to balanced atmospheric phenomena
- **Break condition:** If the forecasting task is not dominated by seasonal patterns or if the data has no temporal autocorrelation

### Mechanism 2: Weather Data Redundancy Enables Data Efficiency
- **Claim:** A representative 20% subset of training data can achieve comparable predictive accuracy to full-data training because densely-sampled weather data contains significant redundancy
- **Core assumption:** The core dynamics of the weather system can be learned from a sparse sampling of states, and additional densely-packed data points contribute more to computational cost than to generalization
- **Evidence anchors:** Abstract states 20% achieves comparable performance; section 3.2 explains random sampling may overrepresent certain phenomena while K-Means ensures broad coverage while avoiding redundancy
- **Break condition:** If the forecasting task requires learning very rare, extreme events not captured by standard stratified sampling

### Mechanism 3: Domain-Specific Heuristics Outperform Generic Methods
- **Claim:** Simple, domain-informed heuristics (stratified time) can outperform more complex, general-purpose feature-based selection methods (K-Means, Greedy Diverse) for this spatiotemporal task
- **Core assumption:** Temporal/seasonal stratification is a better proxy for weather diversity than generic spatial or feature-space clustering
- **Evidence anchors:** Abstract motivates future work on adaptive methods beyond random or temporal sampling; section 3.2 describes domain-specific nature of stratified time; section 4.4 shows stratified time performs best among 20% baselines
- **Break condition:** If feature engineering for clustering is highly domain-aware and demonstrably superior to simple time stratification

## Foundational Learning

- **Concept: Diffusion Models for Weather Forecasting**
  - **Why needed here:** This is the core architecture; understanding they are generative models trained to denoise data is essential to grasp how they produce ensemble forecasts by sampling from the learned distribution
  - **Quick check question:** How does a diffusion model generate a probabilistic ensemble forecast from a single initial condition?

- **Concept: Autoregressive Forecasting & Error Accumulation**
  - **Why needed here:** The paper evaluates models that forecast 24 hours ahead and are then rolled out to 10 days; understanding this iterative process is key to interpreting the 5-day vs. 10-day evaluation metrics
  - **Quick check question:** If the model has a small bias, what happens to the forecast error at a 10-day lead time compared to a 1-day lead time?

- **Concept: Ensemble Forecast Metrics (RMSE, CRPS, SSR)**
  - **Why needed here:** The paper's central claim is improving the Spread/Skill Ratio (SSR); one must know that RMSE measures accuracy of the mean, CRPS measures probabilistic skill, and SSR measures calibration to understand the results
  - **Quick check question:** If a model has a low RMSE but a very low SSR (e.g., 0.5), what does that imply about its uncertainty estimates?

## Architecture Onboarding

- **Component map:** Input data -> Data Selection (Stratified Time) -> Diffusion Model Training -> Autoregressive Rollout -> Metric Evaluation (SSR)
- **Critical path:** Input data -> Data Selection (Stratified Time) -> Diffusion Model Training -> Autoregressive Rollout -> Metric Evaluation (SSR)
- **Design tradeoffs:**
  - Data Volume vs. Diversity: Using less data (20%) is more efficient but risks missing critical modes; stratified sampling mitigates this by enforcing temporal diversity
  - Heuristic vs. Adaptive Selection: Simple heuristics (stratified time) are cheap and robust but static; adaptive methods (future work) could be more powerful but are computationally costly
  - Metric Optimization: Optimizing for SSR (calibration) may yield different data subsets than optimizing for RMSE (accuracy)
- **Failure signatures:**
  - High SSR Variance: Indicates an unstable sampling method (e.g., random sampling)
  - Low SSR (Under-dispersion): The ensemble is too confident, likely because the training data didn't cover enough variance (e.g., over-sampling calm weather)
  - Degraded RMSE/CRPS: The sampling method discarded too much important data, failing to capture the full system dynamics
- **First 3 experiments:**
  1. Reproduce Baseline: Train the model on the full ERA5 training set (1979-2015) and record RMSE, CRPS, and SSR on the 2018 test year to establish the performance upper-bound
  2. Reproduce Main Result: Implement stratified time sampling to select 20% of the training data; train an identical model and compare metrics, specifically looking for the SSR improvement shown in Tables 1 & 2
  3. Ablation on Data Budget: Run stratified time sampling at 10%, 30%, and 50% data budgets to determine the point where performance significantly degrades compared to the full-data baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive, model-aware sampling strategies outperform static temporal stratification in ensemble forecasting?
- **Basis in paper:** The authors state the work "motivates future work on adaptive or model-aware sampling methods that go beyond random or purely temporal sampling"
- **Why unresolved:** The current study evaluates only static heuristics (e.g., random, K-Means) and does not test dynamic selection methods that react to model training dynamics
- **What evidence would resolve it:** Developing a gradient-based or uncertainty-aware sampling method that yields higher Spread/Skill Ratios or lower CRPS than stratified time sampling at the same data budget

### Open Question 2
- **Question:** What is the minimum data budget required for stratified time sampling to maintain performance parity with full-data training?
- **Basis in paper:** The study evaluates a fixed 20% budget but does not test the lower bounds of this efficiency
- **Why unresolved:** It is unclear if the 20% threshold is optimal or if the model could achieve similar results with 10% or 5% of the data
- **What evidence would resolve it:** A sweep of data budgets (e.g., 1%, 5%, 10%, 15%) identifying the specific inflection point where RMSE and SSR degrade significantly compared to the full-data baseline

### Open Question 3
- **Question:** Why do hybrid sampling methods combining temporal stratification with spatial clustering underperform compared to simple temporal stratification?
- **Basis in paper:** Appendix A.2 mentions that hybrid variants (Stratified KMeans, Stratified Entropy) showed "lacking" performance despite theoretical justifications
- **Why unresolved:** The authors do not provide an analysis explaining why adding spatial diversity or entropy criteria to the temporal prior fails to improve results
- **What evidence would resolve it:** An ablation study analyzing the loss landscapes or feature distributions of hybrid methods to determine if they introduce noise or conflicting optimization signals

## Limitations
- Model Architecture Dependence: Results are specific to the fixed diffusion model architecture from prior work and may not generalize to different model configurations
- Static Budget Evaluation: Study focuses on a single 20% budget without exploring the full performance curve across varying data fractions
- Limited Sampling Strategy Diversity: Analysis does not explore adaptive or model-aware selection methods, which the authors identify as future work

## Confidence
- **High Confidence:** Stratified time sampling outperforms random and feature-based selection methods at 20% data budget (supported by Tables 1 & 2 metrics)
- **Medium Confidence:** 20% data achieves "comparable" performance to full data for most metrics, though absolute RMSE/CRPS gaps exist
- **Low Confidence:** Generalization of findings to other diffusion model configurations, weather variables, or forecasting tasks is not established

## Next Checks
1. **Ablation on Data Budget:** Train models using stratified time sampling at 10%, 30%, and 50% data budgets to map the performance curve and identify the break point where full-data training becomes necessary
2. **Cross-Architecture Validation:** Replicate the stratified sampling experiment using a different diffusion model architecture (e.g., LaDCast) to test the robustness of the data efficiency claim
3. **Extreme Event Sensitivity:** Evaluate the stratified sampling method's performance on a dataset enriched with rare, high-impact weather events to assess its ability to capture low-probability, high-consequence scenarios