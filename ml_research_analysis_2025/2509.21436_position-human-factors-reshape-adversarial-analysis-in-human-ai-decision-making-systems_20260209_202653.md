---
ver: rpa2
title: 'Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making
  Systems'
arxiv_id: '2509.21436'
source_url: https://arxiv.org/abs/2509.21436
tags:
- human
- adversarial
- attack
- reliance
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that human factors fundamentally reshape adversarial
  analysis in human-AI decision-making systems and must be incorporated into evaluating
  robustness. The authors identify four key human factors (self-confidence, task risk,
  task complexity, and time sensitivity) that influence AI reliance behaviors.
---

# Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems

## Quick Facts
- arXiv ID: 2509.21436
- Source URL: https://arxiv.org/abs/2509.21436
- Reference count: 40
- This paper argues that human factors fundamentally reshape adversarial analysis in human-AI decision-making systems and must be incorporated into evaluating robustness.

## Executive Summary
This paper challenges traditional adversarial machine learning by demonstrating that human factors fundamentally reshape how attacks succeed in human-AI collaborative systems. The authors introduce a novel robustness analysis framework that captures both AI performance and human reliance dynamics through two interconnected assessments. Their timing-based adversarial attack case study reveals that attack effectiveness exhibits non-monotonic behavior—more frequent attacks are not necessarily more effective. Instead, strategic, sparse attacks exploiting early-stage trust before user skepticism is triggered achieve the highest impact through a self-limiting trust-performance feedback loop.

## Method Summary
The framework implements two interconnected assessments: AI reliance assessment (computing trust scores from performance feedback and model-irrelevant factors like self-confidence, task risk, complexity, and time sensitivity) and AI performance assessment (evaluating output accuracy through human feedback). A timing-based adversarial attack exploits temporal dynamics in human-AI collaboration by perturbing AI outputs at strategically chosen moments. The attack score (AS) measures cumulative adversarial impact across sequential tasks, with simulations showing that two optimally-timed attacks outperform more frequent attack strategies due to the self-limiting nature of trust degradation.

## Key Results
- Attack effectiveness exhibits non-monotonic behavior—increasing attack frequency can reduce overall attack success in human-AI systems
- Optimal attack timing, rather than frequency, maximizes effectiveness through exploiting early-stage trust before user skepticism triggers
- Four key human factors (self-confidence, task risk, complexity, time sensitivity) create context-dependent attack surfaces independent of model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks in human-AI systems trigger a self-limiting trust-performance feedback loop that paradoxically constrains long-term attack effectiveness.
- Mechanism: When an attack causes AI errors, humans detect degraded performance and reduce trust. This reduced trust leads humans to override AI decisions, which prevents subsequent attacks from influencing outcomes—even if the attacker continues perturbing inputs.
- Core assumption: Humans can reliably detect degraded AI performance over time and adjust reliance behavior accordingly.
- Evidence anchors:
  - [abstract] "The analysis shows that strategic, sparse attacks exploiting early-stage trust before user skepticism is triggered achieve the highest impact."
  - [Section II-A] "After observing just a few conspicuous errors, users often hesitate to use the AI system on subsequent tasks... once users stop relying on the model, the attack can no longer influence decisions, resulting in a self-limiting trust–performance feedback loop."
  - [corpus] Related work on appropriate reliance (arXiv:2501.10909) supports that fostering appropriate user reliance is critical for team performance, but does not directly address adversarial settings.
- Break condition: If humans have poor error detection capabilities, or if task constraints (extreme time pressure) prevent performance monitoring, the feedback loop weakens.

### Mechanism 2
- Claim: Attack effectiveness exhibits non-monotonic behavior—increasing attack frequency can reduce overall attack success in human-AI systems.
- Mechanism: Each attack erodes human trust. More frequent attacks accelerate trust degradation, causing humans to abandon AI reliance sooner. Sparse, strategically-timed attacks preserve trust long enough to achieve higher cumulative impact.
- Core assumption: Human trust updates based on observed performance history, with negative experiences weighted more heavily than positive ones (as noted in Section III-B citing prior literature).
- Evidence anchors:
  - [abstract] "The experimental results reveal that attack effectiveness exhibits non-monotonic behavior: more frequent attacks are not necessarily more effective."
  - [Section V-A, Figure 2] Simulation shows the most effective attack strategy occurs with only two attacks over 10 tasks; beyond this, attack scores plateau or decline.
  - [corpus] No direct corpus evidence on non-monotonic attack frequency effects; this appears to be a novel finding in this paper.
- Break condition: If humans have extremely high initial trust thresholds or fail to update trust based on performance, frequency may have linear rather than non-monotonic effects.

### Mechanism 3
- Claim: Cognitive and situational factors (self-confidence, task risk, complexity, time sensitivity) create context-dependent attack surfaces independent of model accuracy.
- Mechanism: These "model-irrelevant factors" modulate human reliance decisions. For example, time-critical tasks increase default AI reliance, expanding attack windows; high-risk tasks trigger more scrutiny, requiring subtler perturbations. Attackers who ignore these factors misestimate vulnerability.
- Core assumption: Humans integrate multiple signals (performance history + contextual factors) into reliance decisions, rather than basing trust solely on observed accuracy.
- Evidence anchors:
  - [Section II-B] Identifies four factors and provides scenario examples (low-risk tasks allow more aggressive attacks; time-critical tasks create vulnerability windows).
  - [Section III-B, Eq. 4-10] Formalizes how factors combine with performance feedback into a reliance score.
  - [corpus] arXiv:2505.19220 (DeCoDe) addresses defer-vs-collaborate decisions but focuses on complementary strengths, not adversarial exploitation of human factors.
- Break condition: If humans rely uniformly regardless of context (automation bias), situational factors have reduced modulating effect.

## Foundational Learning

- Concept: **Trust calibration in AI systems**
  - Why needed here: The entire framework depends on understanding how humans dynamically adjust trust based on observed performance. Without this, the self-limiting feedback loop and timing-based attack strategies make little sense.
  - Quick check question: Can you explain why observing one AI error might reduce trust more than observing one success increases it?

- Concept: **Sequential decision-making with feedback**
  - Why needed here: Unlike single-shot adversarial attacks, this framework models human-AI collaboration as a sequence where each decision affects future reliance. The timing-based attack exploits this temporal structure.
  - Quick check question: Why would an attack at task 1 have different consequences than the same attack at task 10 in a 10-task sequence?

- Concept: **Adversarial robustness fundamentals**
  - Why needed here: The paper positions itself against traditional adversarial ML, which assumes fully autonomous systems. Understanding what conventional attacks optimize for (single-task prediction manipulation) clarifies what's different here.
  - Quick check question: In a traditional adversarial attack against an image classifier, does the attacker need to consider whether a human will override the model's prediction?

## Architecture Onboarding

- Component map:
  - AI Reliance Assessment (Section III-B) -> AI Performance Assessment (Section III-C) -> Decision Execution Logic (Section IV-B)

- Critical path:
  1. Initialize r*_1 (initial reliance, set high in simulations)
  2. For each task T_i: compute instantaneous reliance r_i → apply momentum update → compare to threshold → determine trust decision
  3. If trusted: execute AI prediction; performance assessment is model-only (indirect)
  4. If not trusted: human generates prediction; performance assessment compares human vs. AI
  5. Performance assessment result updates D_i for next task's reliance computation

- Design tradeoffs:
  - **Momentum weight α**: High α (e.g., 0.8) smooths trust slowly, making system resistant to single errors but slow to recover trust after attack sequences. Low α makes trust volatile
  - **Reliance threshold r̂**: Low threshold = humans default to trusting AI (more efficient but more vulnerable). High threshold = humans verify more often (more robust but slower)
  - **Assumes white-box attacker knowledge** (Section IV-A2): Framework analyzes worst-case robustness; real attackers may have partial knowledge, changing optimal attack timing

- Failure signatures:
  - **Trust collapse**: Consecutive attacks early in sequence cause r* to drop below threshold and stay there; all subsequent AI predictions are overridden regardless of quality (Figure 3a, "Worst Attack" trajectory)
  - **False trust recovery**: If α is too high and non-attacked tasks have good performance, trust can recover between attacks, enabling the "Best Attack" pattern where spaced attacks succeed (Figure 3a)
  - **Threshold mismatch**: If r̂ doesn't match actual human behavior, simulations mispredict attack effectiveness. Sensitivity analysis (Figure 4d) shows attack success varies substantially with threshold

- First 3 experiments:
  1. **Replicate the two-attack timing sweep**: Implement the simulation with 10 tasks, two attacks allowed. Enumerate all C(10,2) = 45 attack position combinations. Verify that attacks at positions (1, 10) yield higher AS than (1, 2) or (9, 10), matching Figure 3
  2. **Sensitivity to human vs. model accuracy**: Rerun simulations varying p_h (human accuracy) and p_m (model accuracy) as in Figure 4a-c. Confirm that low human accuracy amplifies vulnerability more than low model accuracy
  3. **Stress-test the reliance update rule**: Replace the momentum-based update (Eq. 5) with alternative trust models (e.g., Bayesian updating, threshold-based step functions). Observe whether non-monotonic attack effectiveness persists or is an artifact of the specific update rule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the non-monotonic effectiveness patterns of timing-based attacks hold in empirical user studies with actual human participants?
- Basis in paper: [inferred] The experimental results rely on simulated human reliance (Equations 4–10) and mathematical models rather than real-world behavioral data.
- Why unresolved: Simulations use fixed parameters (e.g., momentum weight α=0.8) that may not capture the full stochastic variability of actual human trust dynamics.
- What evidence would resolve it: Replication of the optimal attack timing findings with human subjects in a controlled, safety-critical simulation.

### Open Question 2
- Question: How can heterogeneous individual traits (e.g., domain expertise, cognitive load) be integrated into adaptive reliance models to improve analysis accuracy?
- Basis in paper: [explicit] Section VII.A states that current static modeling assumes fixed reliance update rules and ignores heterogeneous, context-sensitive human behaviors.
- Why unresolved: Capturing fine-grained personal characteristics and distinct reliance patterns for different task types remains difficult to formalize mathematically.
- What evidence would resolve it: A dynamic model that successfully predicts reliance shifts based on varying individual user profiles and task contexts.

### Open Question 3
- Question: How do collective reliance dynamics in multi-stakeholder groups alter vulnerability to adversarial misinformation compared to single-user settings?
- Basis in paper: [explicit] Section VII.C identifies "Multi-Agent Collaboration and Group Reliance" as a challenge, noting that reliance may be socially reinforced or diluted in teams.
- Why unresolved: Current analysis focuses on single-user collaboration, ignoring consensus mechanisms and social proof found in clinical or financial teams.
- What evidence would resolve it: An analysis of attack propagation through multi-human, multi-AI teams to identify emergent collective vulnerabilities.

## Limitations

- Simulation-based results rely on specific trust update models that may not generalize to real human behavior
- Assumes white-box attacker knowledge representing worst-case scenario rather than realistic partial knowledge
- Binary decision model (trust/not trust) oversimplifies the continuous spectrum of human reliance behaviors

## Confidence

**High Confidence**: The conceptual framework distinguishing between AI performance assessment and AI reliance assessment is well-founded. The observation that attack effectiveness is non-monotonic and that timing matters more than frequency is theoretically sound given the self-limiting feedback loop mechanism.

**Medium Confidence**: The simulation results showing optimal attack timing at specific positions are robust to the model assumptions but may not translate directly to human subjects. The formalization of human factors into quantifiable metrics provides a useful analytical framework but requires empirical validation.

**Low Confidence**: The specific numerical thresholds and parameter values used in the simulations are somewhat arbitrary without human-subject data anchoring them to actual behavior patterns.

## Next Checks

1. **Human Subject Validation**: Conduct a user study where participants collaborate with an AI system on sequential tasks, measuring actual trust calibration and reliance decisions when subjected to controlled perturbations. Compare human behavior patterns to the simulation predictions.

2. **Alternative Trust Models**: Implement and test the framework using different trust update mechanisms (Bayesian updating, threshold-based step functions, exponential smoothing) to determine whether the non-monotonic attack effectiveness is robust across models or specific to the momentum-based approach.

3. **Real-World Attack Scenarios**: Apply the framework to a specific domain (medical diagnosis, fraud detection, content moderation) with actual human-AI collaboration data to validate whether the identified human factors and timing-based attack strategies manifest in practice.