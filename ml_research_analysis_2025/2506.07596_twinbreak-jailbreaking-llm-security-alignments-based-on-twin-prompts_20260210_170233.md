---
ver: rpa2
title: 'TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts'
arxiv_id: '2506.07596'
source_url: https://arxiv.org/abs/2506.07596
tags:
- pruning
- prompts
- twinbreak
- safety
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TwinBreak, a white-box method for removing\
  \ safety alignment from LLMs by targeting and pruning specific parameters responsible\
  \ for harmful content rejection. TwinBreak uses twin prompts\u2014highly similar\
  \ harmless and harmful prompt pairs\u2014to identify safety parameters through activation\
  \ differences, while also identifying utility parameters to preserve model functionality."
---

# TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts

## Quick Facts
- **arXiv ID**: 2506.07596
- **Source URL**: https://arxiv.org/abs/2506.07596
- **Reference count**: 40
- **Primary result**: 89–98% ASR on unseen harmful prompts with minimal utility degradation across 16 models (1B–72B parameters)

## Executive Summary
TwinBreak is a white-box parameter pruning method that removes safety alignment from LLMs by targeting and pruning specific parameters responsible for harmful content rejection. The method uses twin prompts—highly similar harmless and harmful prompt pairs—to identify safety parameters through activation differences, while also identifying utility parameters to preserve model functionality. TwinBreak achieves high attack success rates (89–98%) on unseen harmful prompts with minimal utility degradation, outperforming related works in both attack success and runtime efficiency.

## Method Summary
TwinBreak identifies safety parameters by analyzing activation differences between highly similar harmful and harmless prompts (twin prompts) across MLP layers, then iteratively prunes the top-ranked parameters while excluding those critical for utility. The method uses a 5-iteration process, pruning 1% of remaining candidate parameters per iteration, and generates the first 50 output tokens with the pruned model before optionally reverting to the original model. Safety parameters are identified from the last 6 input token activations across Gate and Up layers of middle MLP blocks, while utility parameters are identified from activation differences between harmless prompt pairs.

## Key Results
- Achieves 89–98% ASR on unseen harmful prompts (AdvBench, HarmBench, JailbreakBench, StrongREJECT)
- Minimal utility degradation: <2% drop on HellaSwag, ARC-Challenge, OpenBookQA, RTE, WinoGrande
- Outperforms related works in both attack success and runtime efficiency
- Works across 16 models (1B–72B parameters) from five different vendors

## Why This Works (Mechanism)

### Mechanism 1: Twin Prompt Activation Differencing
Parameters with large activation differences between structurally similar harmful and harmless prompts encode safety refusal behavior. TwinBreak generates a single output token for both prompts, collects activations from the last 6 input tokens across targeted MLP layers, computes L2-normalized activation differences, and ranks parameters by difference magnitude. The top 1% per iteration are flagged as safety parameters.

### Mechanism 2: Utility Parameter Exclusion via Harmless Prompt Pair Activation
Parameters with high activation differences between two harmless prompts encode general language understanding and must be preserved. Randomly pair harmless prompts from TwinPrompt, collect activations, and exclude the top 0.1% of parameters with largest differences from pruning.

### Mechanism 3: Iterative Fine-Grained Pruning with Progressive Parameter Exposure
Safety parameters become identifiable only after dominant safety parameters are removed, requiring multiple small pruning iterations. Run 5 iterations, each pruning 1% of remaining candidate parameters, while aggregating pruned parameters across iterations. Use the pruned model for the first 50 output tokens, then optionally revert to the unpruned model.

## Foundational Learning

- **Decoder-only Transformer Architecture (MLP Blocks)**: Why needed here: TwinBreak targets specific MLP sub-layers (Gate, Up) while excluding self-attention and first/last decoder blocks. Quick check: Can you explain why the Gate layer controls information flow and the Up layer expands representations in an MLP block?

- **Safety Alignment as Refusal Behavior**: Why needed here: The method assumes safety alignment trains models to emit refusal responses for harmful prompts, analogous to backdoor triggers. Quick check: How does RLHF create refusal behavior, and why might this concentrate in specific parameters?

- **Activation-Based Pruning**: Why needed here: TwinBreak uses activation magnitudes to rank parameters rather than weight magnitudes or gradient-based importance. Quick check: Why might activation differences between similar inputs better isolate behavior-specific parameters than weight-based pruning?

## Architecture Onboarding

- **Component map**: TwinPrompt construction → Utility parameter identification (once) → Loop: activation collection → safety parameter ranking → exclusion of utility parameters → pruning → validation → (repeat 5x)

- **Critical path**: TwinPrompt dataset creation → Activation collection from twin prompt pairs → Safety parameter identification via activation differences → Utility parameter identification from harmless pairs → Iterative 5-round pruning (1% per round) → ASR and utility validation

- **Design tradeoffs**:
  - Pruning rate (1% vs. higher): Lower rates preserve utility but require more iterations; higher rates risk removing overlapping utility parameters
  - Token window (last 6 tokens): Captures prompt-final context but may miss early prompt signals; averaging over top 5 reduces noise
  - Pruned inference length (50 tokens): Longer generation with pruned model slightly improves ASR but risks utility degradation; switching to unpruned model is safer

- **Failure signatures**:
  - ASR remains low (<70%): Likely safety parameters not concentrated in Gate/Up layers; try including self-attention or adjusting utility exclusion rate
  - Outputs become incoherent: Utility parameter exclusion too aggressive or pruning rate too high; increase utility rate to 1% or reduce pruning rate
  - ASR high but StrongREJECT scores low: Model generating repetitive or incoherent harmful responses; likely over-pruned utility parameters

- **First 3 experiments**:
  1. Baseline validation: Run TwinBreak on LLaMA 2 7B with full TwinPrompt (100 pairs), measure ASR on HarmBench validation split and utility on HellaSwag
  2. Ablation on pruning targets: Prune only Gate, only Up, and Gate+Up (default) on LLaMA 2 7B
  3. Utility exclusion sensitivity: Test utility parameter rates of 0%, 0.1% (default), and 1% on Gemma 2 2B

## Open Questions the Paper Calls Out

- **Can entangling safety mechanisms with core model utility effectively serve as a defense against parameter pruning attacks?**: Section 4.7 states that enhancing robustness by distributing alignment broadly so that removal degrades model integrity is a promising direction for future research.

- **Can the manual creation of the TwinPrompt dataset be automated without compromising the attack's success rate?**: Section 4.4 notes that generating the dataset requires a "one-time effort of around nine hours" of manual labor to ensure high similarity.

- **Does the localization of safety mechanisms in MLP Gate and Up layers generalize to encoder-decoder or multimodal architectures?**: Section 2.1 explicitly limits the scope to "decoder-only" architectures, and Section 3.3 bases the pruning strategy on the structure of these specific blocks.

## Limitations

- Dataset specificity: Method's performance hinges on TwinPrompt dataset construction quality, which requires manual crafting of highly similar prompt pairs
- Parameter localization assumption: Assumes safety alignment parameters are concentrated in MLP Gate/Up layers, which may not generalize to all model architectures or safety training regimes
- Utility parameter identification heuristics: The 0.1% utility parameter exclusion rate appears arbitrary and lacks theoretical justification

## Confidence

- **High Confidence**: Core iterative pruning methodology (1% per iteration × 5 iterations) is well-specified and produces consistent results across multiple models and benchmarks
- **Medium Confidence**: TwinPrompt dataset construction methodology is reasonable but not extensively validated; assumption that top 1% activation differences reliably identify safety parameters could be sensitive to implementation details
- **Low Confidence**: Generalization claims to "any decoder-only LLM" lack validation beyond the 16 tested models; cross-vendor transfer and performance on non-chat models untested

## Next Checks

1. **TwinPrompt Dataset Sensitivity Analysis**: Systematically vary dataset size (10, 25, 50, 100 pairs) and composition on LLaMA 2 7B to identify minimum viable dataset size and composition sensitivity

2. **Cross-Architectural Parameter Localization**: Test TwinBreak on encoder-decoder models (T5, BART) and hybrid architectures to validate MLP layer targeting assumption

3. **Utility Parameter Overlap Quantification**: Implement parameter tracking to measure overlap between safety and utility parameters, varying utility exclusion rates (0.01%, 0.1%, 1%, 5%) to quantify trade-off space