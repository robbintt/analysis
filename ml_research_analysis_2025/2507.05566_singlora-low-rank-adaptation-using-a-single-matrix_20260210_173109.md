---
ver: rpa2
title: 'SingLoRA: Low Rank Adaptation Using a Single Matrix'
arxiv_id: '2507.05566'
source_url: https://arxiv.org/abs/2507.05566
tags:
- lora
- learning
- sing
- stable
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses instability issues in Low-Rank Adaptation
  (LoRA) caused by scale disparities between adaptation matrices during training of
  large language models. The authors propose SingLoRA, which reformulates low-rank
  adaptation using a single matrix decomposition A A^T instead of the traditional
  two-matrix product BA, eliminating inter-matrix scale conflicts and halving the
  parameter count.
---

# SingLoRA: Low Rank Adaptation Using a Single Matrix

## Quick Facts
- arXiv ID: 2507.05566
- Source URL: https://arxiv.org/abs/2507.05566
- Reference count: 40
- Key result: 91.3% accuracy on MNLI using 60% parameters vs LoRA's 89.1%

## Executive Summary
This paper addresses instability issues in Low-Rank Adaptation (LoRA) caused by scale disparities between adaptation matrices during training of large language models. The authors propose SingLoRA, which reformulates low-rank adaptation using a single matrix decomposition A A^T instead of the traditional two-matrix product BA, eliminating inter-matrix scale conflicts and halving the parameter count. Theoretical analysis shows that SingLoRA guarantees stable feature learning in infinite-width neural networks by design, unlike standard LoRA which requires careful learning rate tuning. Extensive experiments demonstrate consistent performance improvements across multiple tasks.

## Method Summary
SingLoRA reformulates LoRA's low-rank adaptation by replacing the two-matrix product BA with a single symmetric matrix decomposition AA^T. This eliminates the scale disparity between matrices B and A that causes optimization instability in standard LoRA. The method includes a warm-up ramp function u(t) to ensure the adapter starts near zero while maintaining stable gradients. By constraining updates to a single matrix, SingLoRA halves the parameter count while theoretically guaranteeing stable feature learning in infinite-width neural networks.

## Key Results
- On MNLI comprehension reasoning, Llama-7B fine-tuned with SingLoRA achieves 91.3% accuracy (vs 89.1% for LoRA and 90.2% for LoRA+) using only 60% of the parameters
- On DreamBooth image generation, SingLoRA improves DINO similarity to 0.151 (vs 0.143 for LoRA and 0.148 for DoRA)
- SingLoRA demonstrates stable performance across different learning rates without the sensitivity issues that plague standard LoRA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing BA with AA^T eliminates training instability from learning rate sensitivity to matrix scale disparities
- **Mechanism:** Standard LoRA optimization suffers from "transformation ambiguity" where scaling one matrix up and the other down results in the same forward pass but vastly different gradient scales. SingLoRA constrains updates to AA^T, making gradient updates transformation-invariant and allowing standard optimizers to converge without matrix-specific learning rate tuning
- **Core assumption:** The primary bottleneck in LoRA is optimization instability due to scale mismatch, not representational capacity
- **Evidence anchors:** Abstract states "inherently removes inter-matrix scale conflicts"; Section 4.3 shows gradient descent is transformation-invariant for SingLoRA; Neighboring paper "RefLoRA" supports premise that LoRA suffers from "imbalanced weight updates"
- **Break condition:** If target layer requires strictly non-symmetric updates that cannot be decomposed into symmetric interactions

### Mechanism 2
- **Claim:** Symmetric update AA^T maintains expressiveness in Transformer attention despite structural constraint
- **Mechanism:** While AA^T is symmetric, attention computes QK^T. The product of two symmetric matrices (AqAq^T and AkAk^T) is not necessarily symmetric unless they commute, allowing complex non-symmetric attention patterns
- **Core assumption:** Expressiveness in self-attention is captured by non-commutative interaction of symmetric query/key updates
- **Evidence anchors:** Section 5 states "product of two symmetric matrices is not necessarily symmetric unless they commute"; Figure 1 shows SingLoRA converging faster on attention approximation task
- **Break condition:** If downstream task relies heavily on specific non-symmetric weight updates in non-attention layers that differ from attention dynamics

### Mechanism 3
- **Claim:** "Warm-up" ramp function u(t) preserves pre-trained knowledge while allowing stable gradient flow
- **Mechanism:** Unlike LoRA which initializes B=0, SingLoRA initializes A with Kaiming scaling (non-zero). To ensure W_update ≈ 0 at t=0, it multiplies adapter by time-dependent scalar u(t) that ramps from 0 to 1
- **Core assumption:** Optimization landscape allows smooth transition from pre-trained weights to adapted state without shock from ramp-up
- **Evidence anchors:** Section 4.1 requires u(0)=0 and adopts simple ramp function; Appendix A.1 ablation shows robustness to ramp duration choice
- **Break condition:** If ramp duration T is too short relative to learning rate (causing early instability) or too long (delaying convergence)

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA)
  - **Why needed here:** SingLoRA is direct architectural modification of LoRA; understanding W0 + BA formulation is required to grasp why AA^T change impacts stability
  - **Quick check question:** Can you explain why standard LoRA initializes matrix B to zero?

- **Concept:** Gradient Scale Sensitivity (in Infinite Width)
  - **Why needed here:** Paper justifies design using "infinite-width neural network" theory, arguing standard LoRA gradients vanish/explode as model width n increases
  - **Quick check question:** Why does multiplying two learned matrices (B and A) require careful learning rate tuning compared to a single matrix?

- **Concept:** Symmetric Matrix Properties
  - **Why needed here:** To understand Section 5, must know symmetric matrices can be decomposed (AA^T) but their products are not necessarily symmetric
  - **Quick check question:** Does the product of two symmetric matrices always result in a symmetric matrix?

## Architecture Onboarding

- **Component map:**
  - W0 (Frozen): Pre-trained weights
  - A (Trainable): Single low-rank matrix ∈ Rd×r
  - u(t) (Scalar): Ramp function controlling adapter magnitude

- **Critical path:**
  1. Initialize A with Kaiming initialization (Non-zero)
  2. Define forward pass: Wfinal = W0 + α/r · u(t) · (AA^T)
  3. Ensure u(t) = min(t/T, 1) so at step 0, Wfinal ≈ W0

- **Design tradeoffs:**
  - **Stability vs. Parameter Count:** Gain optimization stability and halve parameters, but AA^T produces symmetric low-rank matrix (subset of all possible low-rank matrices producible by BA)
  - **Rank Efficiency:** To match parameter count of rank r LoRA, SingLoRA technically needs higher rank (≈√2r), but often wins even with same rank (fewer parameters)

- **Failure signatures:**
  - Immediate divergence at Step 0: Likely forgot to implement ramp u(t) or initialized u(0)≠0
  - Slow convergence: Ramp period T may be set too conservatively (too large)
  - Performance Cap: If applying to non-attention layers where symmetry is highly restrictive

- **First 3 experiments:**
  1. **Sanity Check (MNLI):** Fine-tune Llama-7B on MNLI using exact hyperparameters from Table 2 (Rank 8, LR 1e-3) to verify accuracy lift (>91%) vs standard LoRA
  2. **Learning Rate Sweep:** Run sweep of learning rates (5e-5 to 2e-3) on both LoRA and SingLoRA; plot accuracy to verify SingLoRA's "flat" stability curve vs LoRA's variance
  3. **Ramp Ablation:** Test different values of T (ramp steps) to verify claim in Appendix A.1 that performance is robust to this hyperparameter

## Open Questions the Paper Calls Out
None

## Limitations
- Applicability beyond attention layers and image generation tasks remains unproven
- Theoretical analysis using infinite-width neural networks may not fully capture finite-width behavior where matrix symmetry could impose practical constraints
- Learning rate stability claims could depend on specific optimizer configurations (AdamW) and may not generalize to all training regimes

## Confidence
- **High confidence**: Claims about reduced parameter count (50% reduction) and elimination of inter-matrix scale conflicts
- **Medium confidence**: Claims about consistent performance improvements across all tested tasks and learning rates
- **Medium confidence**: Theoretical claims about infinite-width network behavior

## Next Checks
1. Test SingLoRA on non-attention layers (MLPs, feed-forward networks) to verify symmetric constraint doesn't impair performance in these components
2. Evaluate SingLoRA's stability across different optimizers (SGD, Adafactor) and learning rate schedules to confirm robustness isn't specific to AdamW with linear warm-up
3. Conduct ablation studies comparing different symmetric decomposition strategies to determine if performance gains are specifically due to symmetric constraint