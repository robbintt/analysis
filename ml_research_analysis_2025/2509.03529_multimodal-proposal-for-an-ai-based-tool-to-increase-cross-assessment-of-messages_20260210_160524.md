---
ver: rpa2
title: Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages
arxiv_id: '2509.03529'
source_url: https://arxiv.org/abs/2509.03529
tags:
- embeddings
- financial
- each
- multimodal
- earnings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hierarchical multimodal framework for embedding
  earnings calls using discourse trees, capturing both affective tone and conversational
  structure. Each call is segmented into monologues and question-answer pairs, with
  each node enriched by text, audio, and video emotion features, plus structured metadata
  like topic labels and coherence scores.
---

# Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages

## Quick Facts
- arXiv ID: 2509.03529
- Source URL: https://arxiv.org/abs/2509.03529
- Reference count: 20
- Primary result: Hierarchical multimodal framework embedding earnings calls using discourse trees, capturing affective tone and conversational structure

## Executive Summary
This paper presents a hierarchical multimodal framework for embedding earnings calls using discourse trees, capturing both affective tone and conversational structure. Each call is segmented into monologues and question-answer pairs, with each node enriched by text, audio, and video emotion features, plus structured metadata like topic labels and coherence scores. A two-stage transformer architecture processes node-level multimodal content via contrastive learning and aggregates them into a global call-level embedding. Experimental results show that the embeddings form stable, semantically meaningful representations reflecting sentiment, structure, and thematic alignment. The approach generalizes to other high-stakes communication domains such as telemedicine, education, and political discourse, offering a robust, explainable method for multimodal discourse analysis.

## Method Summary
The framework segments earnings calls into hierarchically structured discourse trees with monologue and Q&A nodes. Each node receives multimodal emotion features (7-dimensional: anger, disgust, fear, joy, neutral, sadness, surprise) from text (DistilRoBERTa), audio (emotion2vec), and optional video (ViT-face). Metadata from LLM ensembles provides topic labels, coherence scores, and answer coverage. A two-stage transformer processes node-level content with contrastive learning, then aggregates node embeddings into conference-level representations using another contrastive layer. The final embeddings are evaluated qualitatively through UMAP projections showing semantic clustering by sentiment and topic.

## Key Results
- Hierarchical discourse tree embeddings capture both structural semantics and affective tone beyond flat sequence models
- Two-stage contrastive learning produces semantically aligned embeddings at node and conference levels with stable clustering
- Multimodal fusion of text, audio, and video emotion features enriches discourse representations compared to text-only approaches

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical discourse tree representation captures structural semantics that flat sequence models miss by preserving speaker roles, turn-taking, and rhetorical relationships. Each node carries structural metadata that informs the attention mechanism about discourse function beyond content.

### Mechanism 2
Two-stage contrastive learning produces semantically aligned embeddings at both local (node) and global (conference) levels by forcing same-node representations together while pushing different nodes apart across subsampled views.

### Mechanism 3
Multimodal emotion fusion enriches discourse representations beyond text alone by aggregating sentence-level emotion vectors from audio, text, and video into a shared latent space that captures affective signals complementary to lexical content.

## Foundational Learning

- **Contrastive Learning & NT-Xent Loss**
  - Why needed here: Core training objective at both node and conference levels; requires understanding how positive/negative pair construction and temperature scaling affect embedding geometry.
  - Quick check question: Given two augmented views of the same node, what should their cosine similarity approach after training? What if they're from different nodes in the same batch?

- **Transformer Attention & Positional Encoding**
  - Why needed here: Both transformers use multi-head self-attention with positional encodings; conference-level transformer uses learnable [CLS] token for aggregation.
  - Quick check question: Why does the conference-level transformer need positional encodings if the node-level transformer already processed sequence order?

- **Multimodal Fusion Strategies**
  - Why needed here: Three modalities must be projected into shared latent space before fusion; understanding early vs. late fusion trade-offs matters.
  - Quick check question: If video is unavailable for some calls, should you (a) zero-fill video embeddings, (b) train a separate text-audio model, or (c) mask the modality? What are the implications?

## Architecture Onboarding

- **Component map**: Data Pipeline -> Feature Extraction -> Node-Level Transformer -> Conference-Level Transformer -> Output
- **Critical path**: Data acquisition → Transcription quality → Accurate Q&A pairing → Emotion feature extraction → Node-level contrastive convergence → Conference-level contrastive convergence
- **Design tradeoffs**: Sentence-level vs. intervention-level emotion extraction; single LLM vs. ensemble for metadata; video availability and graceful degradation
- **Failure signatures**: Embeddings cluster by speaker identity rather than semantic content; UMAP shows random scatter; high uncertainty in metadata labels; attention weights concentrated on single node
- **First 3 experiments**:
  1. Ablation study: Train text-only, text+audio, and full multimodal variants on same data. Measure embedding quality via downstream sentiment classification accuracy and UMAP cluster coherence.
  2. Structural perturbation: Randomly shuffle node order before conference-level encoding. If performance degrades significantly, positional structure matters; if not, the model may rely primarily on content aggregation.
  3. Cross-domain transfer: Apply trained model (without retraining) to corpus neighbor domain (e.g., MiMIC Indian earnings calls or political discourse transcripts). Measure embedding similarity patterns to assess generalization claims.

## Open Questions the Paper Calls Out

- What is the predictive or explanatory power of the generated discourse embeddings when integrated with market data for financial forecasting? (The paper states future research will focus on integrating these embeddings with market and company-level financial data to assess their utility in financial forecasting contexts.)

- To what extent does the proposed framework generalize to other high-stakes communication domains such as telemedicine, education, or political discourse? (The abstract and conclusion claim the system generalizes to other high-stakes unscripted communicative domains, but experimental validation is restricted to financial earnings calls.)

- What is the marginal contribution of specific modalities (video, text, audio) and structured metadata (coherence scores, answer coverage) to the final embedding quality? (RQ2 asks about the contribution of emotional trajectories, answer coherence, and interaction structure, but ablation studies quantifying performance impact are not reported.)

- How robust are the final embeddings to noise or errors in the LLM-generated structural metadata (topic labels and coherence scores)? (The methodology relies on uncertainty-aware LLM ensemble for metadata, but does not analyze how errors in generated ground truth propagate through the transformer and affect final embedding.)

## Limitations

- The two-stage contrastive learning mechanism's effectiveness across domains remains unproven, as experiments focus primarily on earnings calls
- The assumed complementarity of multimodal emotion signals may not generalize to contexts where vocal or facial cues are less informative or available
- The reliance on LLM-generated metadata introduces potential noise that could propagate through the embedding pipeline

## Confidence

- **High confidence**: The hierarchical discourse tree representation can be constructed from earnings call transcripts with existing NLP tools, and the framework architecture is technically feasible
- **Medium confidence**: Multimodal emotion fusion provides complementary signals to text, based on related work but not directly validated in this specific architecture
- **Medium confidence**: Two-stage contrastive learning produces semantically meaningful embeddings, though quantitative validation beyond UMAP visualization is needed
- **Low confidence**: The approach generalizes robustly to other high-stakes communication domains without domain-specific adaptation

## Next Checks

1. **Ablation study with downstream task evaluation**: Train and evaluate text-only, text+audio, and full multimodal variants on a held-out test set using a concrete downstream task (e.g., sentiment classification or question-answer coherence prediction). Compare both embedding quality metrics and task performance to quantify the marginal contribution of each modality.

2. **Structural perturbation analysis**: Systematically shuffle node order at the conference level and retrain (or test with frozen node embeddings). Measure degradation in downstream task performance and embedding quality metrics. Additionally, compare against a flat sequence model that ignores discourse structure to quantify the structural contribution.

3. **Cross-domain transfer validation**: Apply the trained model to transcripts from related domains (e.g., MiMIC Indian earnings calls, political debates, or telemedicine consultations) without fine-tuning. Evaluate embedding quality through UMAP visualization and semantic similarity tasks, and measure performance drop compared to in-domain testing to assess generalization claims.