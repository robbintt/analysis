---
ver: rpa2
title: Low-Rank Filtering and Smoothing for Sequential Deep Learning
arxiv_id: '2410.06800'
source_url: https://arxiv.org/abs/2410.06800
tags:
- tasks
- learning
- task
- low-rank
- diagonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Bayesian filtering and smoothing framework
  for sequential deep learning, treating neural network parameters as states in a
  nonlinear Gaussian state-space model. The key innovation is using diagonal plus
  low-rank Laplace approximations of the precision matrix to enable efficient filtering
  and smoothing operations.
---

# Low-Rank Filtering and Smoothing for Sequential Deep Learning

## Quick Facts
- arXiv ID: 2410.06800
- Source URL: https://arxiv.org/abs/2410.06800
- Reference count: 40
- The paper proposes a Bayesian filtering and smoothing framework for sequential deep learning using diagonal plus low-rank Laplace approximations.

## Executive Summary
This paper introduces a Bayesian filtering and smoothing framework for sequential deep learning by treating neural network parameters as states in a nonlinear Gaussian state-space model. The key innovation is using diagonal plus low-rank Laplace approximations of the precision matrix, enabling efficient filtering and smoothing operations while capturing useful parameter interactions. The method allows incorporating domain knowledge about task relationships via the process noise matrix Q and enables task-specific models informed by all tasks via smoothing without requiring data access. Experiments demonstrate that the approach outperforms baseline regularization methods, with smoothing providing significant performance gains on earlier tasks.

## Method Summary
The method frames sequential neural network training as a Bayesian state estimation problem where parameters are treated as states in a state-space model with Gaussian transitions. After each task, a Laplace approximation of the posterior is computed using the Generalized Gauss-Newton (GGN) matrix, which is compressed via truncated SVD to a diagonal plus low-rank form. Filtering proceeds through predict-update cycles, while smoothing applies a backward pass to compute task-specific models informed by all tasks. The diagonal plus low-rank structure enables efficient operations via Woodbury matrix identities, reducing computational complexity from O(D²) to O(Dk) where k is the rank.

## Key Results
- The low-rank GGN approximation captures useful parameter interactions while maintaining computational efficiency
- Structured process noise Q improves current task performance while maintaining average performance across tasks
- Smoothing provides significant performance gains on earlier tasks without requiring data access

## Why This Works (Mechanism)

### Mechanism 1: State-Space Formulation Enables Recursive Posterior Computation
The method maps sequential neural network training to a Bayesian state estimation problem, enabling exact recursive computation of parameter posteriors through predict-update cycles. Network parameters θt are treated as latent states evolving via Gaussian transition pp(θt+1|θt) = N(θt+1; θt, Q). Task likelihoods pp(Dt|θt) ∝ exp(-1/λ L(θt, Dt)) encode supervised learning. The filtering distribution pp(θt|D1:t) is computed recursively: predict integrates over the previous posterior, and update multiplies by the current likelihood.

### Mechanism 2: Diagonal Plus Low-Rank Precision via GGN Truncation
The GGN matrix's inherent low-rank structure (rank C << D where C is output dimension) enables efficient diagonal plus low-rank precision approximations that capture important parameter correlations at O(Dk) cost. The GGN JȞJ^T approximates the Hessian and has rank at most C. After each update, the precision P = D⁻ + Σ J(b)Ȟ(b)J(b)^T is compressed via truncated SVD to rank k, yielding P = D + UΣU^T. All subsequent operations preserve this structure through Woodbury identity applications.

### Mechanism 3: Structured Process Noise Q Encodes Task Relationship Priors
The diagonal process noise covariance Q provides a principled mechanism to inject domain knowledge about which parameters should adapt between tasks, directly controlling the stability-plasticity trade-off. Q adds uncertainty to parameter predictions: C⁻_t = C_{t-1} + Q. When Q=0, parameters are assumed constant across tasks (strong retention). Layer-structured Q encodes beliefs about where task shifts manifest—e.g., brightness changes primarily affect early convolutional layers.

### Mechanism 4: Backward Smoothing Enables Retroactive Knowledge Transfer Without Data Access
Bayesian smoothing computes task-specific models θt^s informed by all tasks (past and future) through backward message passing, without requiring renewed access to any task's data. The Rauch-Tung-Striebel smoother applies backward recursion: ms_t = m_t + G_t(ms_{t+1} - m⁻_{t+1}) where G_t = C_t(C⁻_{t+1})^{-1} is the smoothing gain. This propagates posterior information from later tasks back to earlier ones through the transition model structure.

## Foundational Learning

- **Bayesian Filtering (Predict-Update Cycle)**: Why needed here: The entire framework builds on recursive posterior computation. Understanding how predict adds uncertainty (C⁻_t = C_{t-1} + Q) and update incorporates likelihood information (P_t = P⁻_t + Hessian) is essential for debugging and extending the method.
  - Quick check question: Why does the predict step always increase covariance, and why is this necessary for model flexibility on new tasks?

- **Laplace Approximation**: Why needed here: The update step approximates the intractable non-Gaussian posterior with a Gaussian via second-order Taylor expansion around the MAP estimate. The precision matrix is the Hessian of the loss.
  - Quick check question: Why does the Laplace approximation require the loss to be locally convex near θ*, and what would happen if the Hessian had negative eigenvalues?

- **Woodbury Matrix Identity**: Why needed here: All efficient operations—predict, update (GGN addition), and smooth (gain computation)—rely on the Woodbury identity to invert and manipulate (D + UΣU^T) matrices without materializing the full D×D matrix.
  - Quick check question: Given (D + UΣU^T)^{-1}, derive the Woodbury form D^{-1} - D^{-1}U(Σ^{-1} + U^T D^{-1}U)^{-1}U^T D^{-1}. What is the computational saving when k << D?

## Architecture Onboarding

- **Component map**: State representation (m_t, P_t) -> Predict step (Woodbury twice) -> Train with P⁻_t regularizer -> Compute GGN over batches -> Truncate SVD -> Store (m_t, D_t, U_t, Σ_t) -> (Optional) Backward smoothing pass
- **Critical path**: Initialize P_0 = D_0 + U_0 Σ_0^T (diagonal scaled by λ, or from pre-trained Laplace approximation). For each task t = 1...T: Predict → Train with P⁻_t regularizer → Compute GGN over batches → Truncate SVD → Store (m_t, D_t, U_t, Σ_t). (Optional) After all tasks: Backward smoothing pass to compute task-specific ms_t for all t.
- **Design tradeoffs**: Rank k higher captures more parameter correlations but costs O(Dk) memory and O(k²D) compute. Paper shows k ≈ 10 (≈ number of classes) is often sufficient; diminishing returns beyond. Regularization strength λ controls prior precision scaling. Too large → rigid, cannot adapt; too small → forgetting.
- **Failure signatures**: Exploding eigenvalues in Σ_t: Precision becomes overconfident; model loses plasticity. Remedy: increase Q or decrease λ. Current task accuracy collapses with regularization: Q too small relative to prior precision. Remedy: increase Q, especially for layers expected to change. Smoothing degrades early task performance: Gaussian approximation poor, or tasks fundamentally dissimilar. Remedy: check filtering posteriors for multimodality; consider task clustering.
- **First 3 experiments**: 1. Rank sensitivity on Permuted MNIST: Sweep k ∈ {1, 2, 5, 10, 20, 50} with Q=0, λ tuned. Plot final average accuracy vs k. Expect elbow near k ≈ C (number of classes). 2. Structured Q on GRADUALCAMELYON: Compare Q=0, Q=scalar·I (uniform), Q structured (non-zero for conv layers only). Measure: (a) average accuracy across tasks, (b) current task accuracy. 3. Filtering vs Smoothing gain: Train on 5-task sequence with limited data (750 points/task). After training, compare filtered m_t vs smoothed ms_t accuracy on each task. Expect consistent gains on early tasks.

## Open Questions the Paper Calls Out

- **Can the framework be extended to support efficient filtering and smoothing with block-diagonal K-FAC approximations?**: The paper notes that OSLA (which uses K-FAC) outperforms the proposed method on some benchmarks, suggesting that extending the framework to support block-diagonal approximations is a "promising direction for future work." The current diagonal plus low-rank structure allows for efficient operations, but it is unclear how to maintain this efficiency while incorporating the layer-wise correlations captured by K-FAC.

- **Can the framework accurately infer model parameters for tasks for which no data was ever observed?**: Section 5.2 states that inferring neural network parameters for tasks with intermediate characteristics (e.g., brightness levels between two trained tasks) provides an "interesting avenue for future work." While the smoother updates parameters for previously seen tasks, the ability to interpolate states to synthesize models for unobserved task IDs remains hypothetical.

- **How does the curvature overestimation caused by truncated SVDs impact the fidelity of the posterior approximation?**: The Conclusion lists as a limitation that "low-rank approximations with truncated SVDs can lead to an overestimated curvature, which can hurt performance." The paper utilizes truncated SVDs for efficiency but does not quantify the error introduced by discarding smaller eigenvalues or propose methods to mitigate this bias.

## Limitations
- The method relies on Gaussian approximations that may not capture the true posterior well in non-convex loss landscapes
- Performance depends heavily on correctly designing the structured process noise matrix Q
- The approach has only been validated on relatively simple architectures and benchmark datasets
- Memory requirements still scale with model size, limiting applicability to very large models

## Confidence
- **High confidence**: The mathematical framework (predict-update cycle, Woodbury operations, truncated SVD compression) is well-established and correctly applied
- **Medium confidence**: The performance claims on benchmark datasets are convincing, but generalization to more challenging scenarios needs validation
- **Low confidence**: The claims about structured Q encoding domain knowledge and smoothing providing substantial gains without data access are supported by limited evidence

## Next Checks
1. **Sensitivity analysis on Q design**: Systematically test incorrect Q structures to quantify performance degradation and establish guidelines for Q design
2. **Long-sequence scaling experiment**: Evaluate the method on 20+ task sequences with permuted MNIST or CIFAR-100 to assess how rank truncation and memory requirements scale with task count
3. **Modern architecture validation**: Apply the method to ResNet-18 or ViT on Split CIFAR-100 to verify performance on deeper architectures and compare against modern continual learning methods