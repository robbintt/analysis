---
ver: rpa2
title: 'Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining
  Dataset'
arxiv_id: '2508.15096'
source_url: https://arxiv.org/abs/2508.15096
tags:
- will
- math
- heat
- solution
- calculated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Nemotron-CC-Math, a large-scale, high-quality
  math pretraining dataset built from Common Crawl. The key innovation is a modular
  pipeline using Lynx for layout-aware HTML-to-text rendering combined with LLM-based
  cleaning to reliably preserve mathematical equations and code blocks across diverse
  web formats.
---

# Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset

## Quick Facts
- arXiv ID: 2508.15096
- Source URL: https://arxiv.org/abs/2508.15096
- Authors: Rabeeh Karimi Mahabadi; Sanjeev Satheesh; Shrimai Prabhumoye; Mostofa Patwary; Mohammad Shoeybi; Bryan Catanzaro
- Reference count: 40
- Primary result: 133B token math pretraining dataset with 5.5× more high-quality tokens than previous best, improving math reasoning by +4.8 to +12.6 on MATH

## Executive Summary
Nemotron-CC-Math is a large-scale math pretraining dataset constructed from Common Crawl using a novel modular pipeline. The key innovation is using Lynx for layout-aware HTML-to-text rendering combined with LLM-based cleaning to reliably preserve mathematical equations and code blocks across diverse web formats. The resulting corpus contains 133B tokens, with a high-quality subset (Nemotron-CC-Math-4+) containing 5.5× more tokens than the previous best, FineMath-4+. Pretraining a Nemotron-T 8B model on this data yields substantial improvements across multiple benchmarks, demonstrating that high-quality math data enhances not only mathematical reasoning but also code generation and general knowledge.

## Method Summary
The dataset construction pipeline processes 98 Common Crawl snapshots (2014-2024) through several stages: URL filtering using existing math datasets as seeds, Lynx rendering to preserve layout and mathematical content, Phi-4 LLM cleaning to normalize math to LaTeX and remove boilerplate, quality classification using FineMath-4+ classifier, MinHash deduplication, and decontamination against benchmark prompts. The modular design allows for domain-agnostic application, and the full pipeline is released to support open-source research.

## Key Results
- Nemotron-CC-Math contains 133B tokens total, with 52B tokens in the highest quality subset (4+)
- +4.8 to +12.6 gains on MATH benchmark when used for continued pretraining
- +4.6 to +14.3 gains on MBPP+ code generation benchmark
- Consistent improvements across MMLU benchmarks for general knowledge

## Why This Works (Mechanism)

### Mechanism 1: Layout-aware HTML rendering via Lynx preserves mathematical structure better than DOM-based parsers
- Mechanism: Lynx executes HTML layout rules to produce output mirroring human-perceived page structure, reliably capturing equations and maintaining code indentation—unlike DOM parsers that often strip or corrupt technical content
- Core assumption: Text-based rendering provides sufficient signal for downstream language models even without visual formatting
- Evidence anchors: [abstract]: "leverages layout-aware rendering with lynx"; [section 2.1.2]: "lynx executes HTML layout rules to produce output that mirrors the human-perceived page structure, reliably capturing equations and maintaining code indentation"
- Break condition: If HTML lacks sufficient semantic structure (e.g., math rendered purely as images without alt text), Lynx may not recover equations

### Mechanism 2: LLM-based cleaning standardizes heterogeneous math representations into consistent LaTeX while removing boilerplate
- Mechanism: A lightweight LLM (Phi-4, 14B) normalizes MathJax, KaTeX, MathML, and inline LaTeX into unified LaTeX notation, corrects typos, and discards navigation/boilerplate—avoiding brittle heuristic rules
- Core assumption: The cleaning task is simple enough that smaller instruction-tuned models can perform it effectively
- Evidence anchors: [abstract]: "targeted LLM-based cleaning stage... standardizing notation into LaTeX representation"; [section 3.2]: "Phi-4 performs competitively across all domains, often matching or exceeding the results of much larger models"
- Break condition: If LLM introduces hallucinations or over-edits technical content, mathematical fidelity degrades

### Mechanism 3: High-quality math pretraining data improves math reasoning, code generation, and general knowledge
- Mechanism: Structured mathematical content trains reasoning patterns that transfer to code (logic/structure) and general domains (abstract reasoning). Incidental code preservation in the corpus (~4.3M samples) provides additional code-specific signal
- Core assumption: Reasoning capabilities learned from math transfer across domains requiring structured logical thinking
- Evidence anchors: [abstract]: "yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+... also improving general-domain performance on MMLU"; [section 3.1]: "we find that Nemotron-CC-Math-3+ and Nemotron-CC-Math-4+ contain approximately 4.3M and 1.44M samples with code snippets"
- Break condition: If code samples are malformed or math lacks diversity, transfer benefits diminish

## Foundational Learning

- Concept: **HTML extraction paradigms (DOM parsers vs. rendering engines)**
  - Why needed here: Understanding why Trafilatura/Resiliparse fail on math content (they strip equations) vs. why Lynx succeeds (layout-aware rendering)
  - Quick check question: Given an HTML page with MathJax equations, would jusText preserve the equations? Why or why not?

- Concept: **LaTeX mathematical notation and delimiters**
  - Why needed here: The pipeline normalizes all math formats (MathML, KaTeX, Unicode) into unified LaTeX with `$...$` delimiters
  - Quick check question: Convert `\begin{equation} E = mc^2 \end{equation}` to the inline format used by Nemotron-CC-Math

- Concept: **Pretraining data quality vs. scale tradeoffs**
  - Why needed here: The paper shows Nemotron-CC-Math-4+ (52B tokens) outperforms larger but lower-quality datasets (MegaMath-Web, 264B tokens) on multiple benchmarks
  - Quick check question: If you have 100B token compute budget, when would you choose a 50B high-quality corpus over a 200B medium-quality corpus?

## Architecture Onboarding

- Component map: Common Crawl (98 snapshots) → URL filtering (leverage existing math datasets) → Lynx rendering → LLM cleaning (Phi-4) → Quality classification (FineMath classifier) → Fuzzy deduplication (MinHash LSH) → Decontamination (embedding similarity) → Nemotron-CC-Math-3+/4+

- Critical path: 1. Lynx rendering is the bottleneck for equation preservation—if this fails, downstream LLM cleaning cannot recover lost content. 2. Quality classification determines which subset (3+ vs 4+) a document enters; misclassification propagates to training data mix.

- Design tradeoffs: Lynx vs. DOM parsers: Lynx preserves structure but includes boilerplate (requires LLM cleanup); DOM parsers are faster but lose technical content. Phi-4 vs. larger LLMs: Phi-4 (14B) is 40× smaller than DeepSeek-V3 (671B) with competitive cleaning performance—computational efficiency vs. marginal quality gains. 3+ vs. 4+ subsets: 3+ (133B tokens) provides scale; 4+ (52B tokens) provides highest quality for late-stage training.

- Failure signatures: Degenerate repetitions in output (see Appendix A.2.1 MegaMath-Pro examples)—indicates upstream extraction or scoring failure. Missing inline equations—Lynx rendering failed to capture dynamically rendered math. Corrupted code indentation—LLM over-edited code blocks; check prompt constraint #6.

- First 3 experiments: 1. Reproduce Lynx extraction on 100 sample HTML pages with diverse math formats (MathJax, KaTeX, MathML); manually verify equation preservation rate. 2. Ablate LLM cleaner: compare Phi-4 vs. Qwen2.5-32B on 10K documents; measure cleaning quality and throughput. 3. Train a small model (e.g., 1B parameters) on Nemotron-CC-Math-4+ vs. FineMath-4+ with 10B token budget; evaluate on MATH and GSM8K to validate quality claims before scaling.

## Open Questions the Paper Calls Out

- **Can the pipeline effectively extract high-quality pretraining data for technical domains other than mathematics, such as biology, chemistry, or legal texts?**
  - Basis in paper: [explicit] The conclusion states, "Importantly, the modular, domain-agnostic design enables application to other technical fields," framing the pipeline's utility beyond math as a future capability
  - Why unresolved: The paper exclusively evaluates the pipeline on mathematical content, and the LLM-based cleaning prompt (Appendix A.4) is specifically tuned to "Preserve all mathematical content" and "Format all mathematical expressions using LaTeX"
  - What evidence would resolve it: A reproduction of the dataset construction pipeline targeting a distinct domain (e.g., organic chemistry) with a modified cleaning prompt, followed by pretraining ablations to compare downstream performance against existing domain-specific datasets

- **Does the reliance on pre-existing math dataset URLs (e.g., from OpenWebMath and FineMath) for seeding the extraction significantly limit the discovery of novel mathematical content in Common Crawl?**
  - Basis in paper: [inferred] Section 2 notes that the authors "leverage community-filtered datasets: extracting URLs from OWM, InfiMM-WebMath, FineMath, and MegaMath" to avoid the precision issues of their own classifiers. This implies the dataset may be bounded by the recall of these prior, often "brittle," pipelines
  - Why unresolved: The authors did not quantify the volume of mathematical content in Common Crawl *missed* by these seed URLs, nor did they attempt to expand the URL set using their own classifiers
  - What evidence would resolve it: Running a math-specific classifier on a random sample of Common Crawl documents excluded by the seed URL lists to estimate the false negative rate and analyzing the quality of the excluded documents

- **Does the LLM-based "correction" and standardization of mathematical notation inadvertently introduce hallucinations or remove valid, albeit non-standard, mathematical syntax?**
  - Basis in paper: [inferred] Section 2.1.2 describes the LLM stage as one that "corrects typographical errors" and "standardizes mathematical expressions," while Appendix A.4 instructs the model to "Fix typos... Rewrite sentences when necessary"
  - Why unresolved: While the paper demonstrates improved downstream benchmark scores, it does not qualitatively or quantitatively analyze the trade-off between syntactic normalization and the potential loss of original notation diversity or the introduction of semantic errors by the cleaning model (Phi-4)
  - What evidence would resolve it: A human or automated evaluation of the "cleaned" output vs. the "raw" Lynx output for a random sample of documents to identify instances where the cleaning model altered the semantic meaning of an equation or discarded valid content

## Limitations

- The exact dataset composition cannot be fully verified due to unknown details about the intersection of seed URLs with available Common Crawl snapshots
- The effectiveness of Phi-4 cleaning across all mathematical domains is assumed rather than empirically validated for edge cases
- Quality improvement claims depend on the specific Nemotron-T 8B "mid-training checkpoint" used as the base model, which is not publicly specified

## Confidence

- **High Confidence:** The architectural approach using Lynx for layout-aware rendering is well-justified and technically sound
- **Medium Confidence:** The claim that Nemotron-CC-Math-4+ contains 5.5× more tokens than FineMath-4+ is verifiable, but quality improvement claims depend on specific implementation details
- **Low Confidence:** The exact composition of the Nemotron-CC-Math dataset cannot be fully verified without access to the specific seed URLs and Common Crawl snapshots used

## Next Checks

1. **Verify Lynx rendering on diverse math formats:** Extract 100 sample HTML pages containing different mathematical rendering methods (MathJax, KaTeX, MathML, inline LaTeX) from Common Crawl. Process them through the Lynx rendering pipeline and manually verify equation preservation rates. Document cases where mathematical content is lost or corrupted.

2. **Ablate the LLM cleaning stage:** Select 10,000 documents from the rendered corpus and process them through two different cleaning approaches: (a) the full Phi-4 cleaning pipeline, and (b) a simpler heuristic-based cleaning. Compare the mathematical accuracy, LaTeX consistency, and computational efficiency of both approaches. This validates whether the LLM cleaning provides measurable benefits over simpler methods.

3. **Validate quality improvement with alternative base models:** Using a publicly available 8B model (e.g., Llama-3-8B) that has been pretrained on ~9T tokens, perform the same annealing experiment with 30% Nemotron-CC-Math-4+ and 70% general data for 100B tokens. Evaluate on MATH, GSM8K, and MBPP+ to verify whether the quality improvements are consistent across different base models or specific to the Nemotron-T architecture.