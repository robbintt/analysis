---
ver: rpa2
title: Interpretability in Deep Time Series Models Demands Semantic Alignment
arxiv_id: '2602.02239'
source_url: https://arxiv.org/abs/2602.02239
tags:
- time
- series
- concepts
- alignment
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that true interpretability in deep time series
  models requires semantic alignment - where model variables and mechanisms correspond
  to human-understandable concepts and reasoning patterns. The authors formalize this
  requirement through two conditions: (1) instantaneous concepts must align with encoder
  outputs, and (2) dynamic concepts must maintain alignment through temporal propagation.'
---

# Interpretability in Deep Time Series Models Demands Semantic Alignment

## Quick Facts
- **arXiv ID:** 2602.02239
- **Source URL:** https://arxiv.org/abs/2602.02239
- **Reference count:** 39
- **Primary result:** Semantic alignment in deep time series models requires model variables and mechanisms to correspond to human-understandable concepts and reasoning patterns.

## Executive Summary
This paper argues that true interpretability in deep time series models requires semantic alignment - where model variables and mechanisms correspond to human-understandable concepts and reasoning patterns. The authors formalize this requirement through two conditions: (1) instantaneous concepts must align with encoder outputs, and (2) dynamic concepts must maintain alignment through temporal propagation. They propose a blueprint for semantically aligned models that encode raw observations into interpretable concepts, propagate these concepts through time using aligned mechanisms, and optionally decode them for task-specific outputs. The framework enables properties like actionability, verifiability, and fairness analysis.

## Method Summary
The authors formalize semantic alignment through two conditions: instantaneous concepts must align with encoder outputs, and dynamic concepts must maintain alignment through temporal propagation. They propose a blueprint architecture consisting of three components: a concept encoder that maps raw observations to interpretable source concepts, a propagation module that computes derived concepts through time using aligned mechanisms, and a task decoder that produces final predictions. The framework extends concept bottleneck models from static to temporal domains by enforcing alignment constraints at each time step through supervised losses on both concept prediction and temporal propagation.

## Key Results
- Semantic alignment requires both instantaneous concept alignment and dynamic concept propagation alignment
- Existing interpretability methods like attention weights and saliency maps fail to achieve semantic alignment because they operate at the level of internal computations rather than domain-relevant concepts
- The framework enables actionability, verifiability, and fairness analysis through interpretable concept variables
- Semantic alignment need not trade off against performance, citing concept bottleneck models that match black-box accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoder outputs can align with human-interpretable instantaneous concepts through explicit supervision of concept variables.
- **Mechanism:** A concept encoder maps raw observations x≤t to source concepts satisfying P(Ut = CU_t | X≤t) = 1. This is enforced via a concept loss term L_concept that compares predicted concepts to ground-truth labels at each timestep.
- **Core assumption:** Human reasoning can be formalized as random variables operating over interpretable concepts; concept annotations are available or can be obtained.
- **Evidence anchors:**
  - [abstract]: "instantaneous concepts must align with encoder outputs"
  - [Section 3.2, Definition 1]: Formal definition P(Ut = CU_t | X≤t) = 1 a.s., ∀t
  - [corpus]: Paper 107245 ("Towards Interpretable Concept Learning over Time Series") supports concept-based interpretability but focuses on temporal logic, not validating this specific mechanism.
- **Break condition:** If concept annotations are unavailable, unreliable, or if concepts are inherently subjective/ambiguous, alignment cannot be verified or enforced.

### Mechanism 2
- **Claim:** Dynamic concepts can maintain semantic alignment through temporal propagation when explicitly supervised at each time step.
- **Mechanism:** Propagation mechanisms (temporal and spatio-temporal) compute derived concepts such that P(Zt+1 = CZ_t+1 | X≤t) = 1. A temporal propagation loss L_prop compares propagated concepts against future ground-truth labels, preserving alignment over time.
- **Core assumption:** The semantics of human concepts remain consistent over time; temporal dependencies between concepts can be captured by parametric mechanisms.
- **Evidence anchors:**
  - [abstract]: "dynamic concepts must maintain alignment through temporal propagation"
  - [Section 4.2]: Decomposes propagation into temporal mechanisms P(c_t+1^(k) | c_≤t^(k)) and spatio-temporal mechanisms P(c_t+1^(k) | c_≤t^(j), ...)
  - [corpus]: Weak direct evidence—neighbor papers address semantic alignment in LLM contexts but don't validate temporal concept propagation.
- **Break condition:** If concept semantics drift over time, or if propagation mechanisms cannot capture complex temporal dependencies, alignment decays exponentially (authors explicitly note this risk).

### Mechanism 3
- **Claim:** Mechanisms relating concepts can be aligned by constraining them to satisfy human-specified constraints (monotonicity, physical laws, sparsity).
- **Mechanism:** A mechanism P(V|V') is aligned if it belongs to M^(h)_{V|V'}, the set of admissible distributions under human constraints. This can be enforced by: (a) constraining hypothesis space, (b) regularization/auxiliary losses, or (c) composing primitive modules with known properties.
- **Core assumption:** Human knowledge about mechanisms can be expressed as mathematical or physical constraints; such constraints don't overly reduce model expressivity.
- **Evidence anchors:**
  - [Section 3.2, Definition 2]: "Mechanism alignment as constraint satisfaction" formalization
  - [Section 4.3]: Authors speculate on regularization or composing primitive modules with known properties
  - [corpus]: Paper 50333 ("Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment") mentions semantic alignment but doesn't validate constraint-based mechanism alignment.
- **Break condition:** If constraints are too restrictive (severely reducing expressivity), or if domain knowledge cannot be formalized as constraints, this approach fails to produce useful models.

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed here: The entire framework extends CBMs from static to temporal domains; understanding the static case is prerequisite.
  - Quick check question: Can you explain how a CBM forces task predictions to be mediated by interpretable concept variables?

- **Concept: State-Space Models**
  - Why needed here: The encoding-propagation-decoding template generalizes classical state-space representations (Equations 1-3).
  - Quick check question: What is the difference between an observation equation and a state transition equation in a state-space model?

- **Concept: Conditional Independence and Causal Graphs**
  - Why needed here: Section 4.1 discusses source vs. derived concepts with causal dependencies; understanding P(c|x) factorization is essential.
  - Quick check question: If concept A causally influences concept B, how does this affect the factorization of P(A, B | x)?

## Architecture Onboarding

- **Component map:**
  Raw observations (x≤t) -> [Concept Encoder] -> Source concepts (c_t, k∈S) -> [Propagation Module] -> Derived concepts (c_t, k∈D) -> [Task Decoder] -> Output (ŷ)

- **Critical path:**
  1. Define domain concepts (instantaneous vs. dynamic) with domain experts
  2. Collect or generate concept annotations at required temporal granularity
  3. Design propagation mechanisms respecting known constraints
  4. Train with three-term loss: L = αL_task + βL_concept + γL_prop

- **Design tradeoffs:**
  - **Residual pathways:** Add parallel opaque pathway to recover black-box performance if concepts are incomplete (Observation 4.3), trading interpretability for accuracy.
  - **Constraint strictness:** Hard constraints guarantee alignment but may reduce expressivity; soft constraints via regularization provide flexibility with weaker guarantees.
  - **Temporal resolution:** Concept propagation can operate at coarser/finer scales than input sampling (Observation 4.1), trading annotation cost against temporal precision.

- **Failure signatures:**
  - Concept drift over time despite accurate per-timestep predictions → indicates L_prop is missing or insufficiently weighted
  - Predictions accurate but concept values nonsensical → concept encoder not learning proper alignment, check L_concept supervision
  - Model fails to capture complex dynamics → propagation mechanisms too constrained; consider relaxing constraints or adding residual pathways

- **First 3 experiments:**
  1. **Ablation study:** Train model with only L_task + L_concept (omit L_prop) on a synthetic dataset with known dynamic concepts; measure concept alignment degradation over time.
  2. **Constraint comparison:** Compare hard physical constraints vs. soft regularization for mechanism alignment on a physics-informed time series task; measure both accuracy and constraint satisfaction.
  3. **Annotation sensitivity:** Vary concept annotation granularity (full supervision vs. sparse labels vs. LLM-annotated) and measure impact on semantic alignment metrics and task performance.

## Open Questions the Paper Calls Out

None

## Limitations

- The requirement for concept annotations at each timestep creates a potentially prohibitive annotation burden, particularly for high-frequency time series.
- The claim that semantic alignment need not trade off against performance relies on cited literature rather than demonstrated in this work.
- The framework assumes domain knowledge can be formalized as mathematical or physical constraints, which may not be feasible for many real-world domains.

## Confidence

- **High Confidence:** The formalization of semantic alignment through the two conditions (instantaneous concept alignment and dynamic concept propagation) is internally consistent and mathematically rigorous.
- **Medium Confidence:** The proposed blueprint architecture (encoder-propagation-decoder) is a reasonable generalization of concept bottleneck models to temporal domains, but the specific mechanisms for temporal propagation and constraint satisfaction are not fully specified or validated.
- **Low Confidence:** The claim that semantic alignment need not trade off against performance is the weakest link - it's based on cited literature rather than demonstrated in this work.

## Next Checks

1. **Empirical Validation of No Performance Trade-off:** Conduct controlled experiments comparing semantically aligned models against black-box baselines across multiple time series datasets. Measure not just final accuracy but also the degradation in alignment metrics as model capacity increases to match black-box performance.

2. **Robustness to Concept Annotation Quality:** Systematically vary the quality and completeness of concept annotations (full supervision, sparse labels, noisy labels, LLM-generated labels) and measure the resulting degradation in both semantic alignment and task performance. This would quantify the practical feasibility of the approach.

3. **Long-term Temporal Alignment Stability:** Design experiments on synthetic datasets with known dynamic concepts to measure how alignment degrades over time horizons. Test whether the temporal propagation loss L_prop effectively prevents exponential decay of alignment, and identify the conditions under which alignment breaks down.