---
ver: rpa2
title: Score-based Metropolis-Hastings for Fractional Langevin Algorithms
arxiv_id: '2602.00835'
source_url: https://arxiv.org/abs/2602.00835
tags:
- fractional
- score
- mafla
- langevin
- proposal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of sampling from heavy-tailed\
  \ and multimodal distributions when neither the target density nor the proposal\
  \ density can be evaluated, specifically for \u03B1-stable L\xE9vy-driven fractional\
  \ Langevin algorithms. The authors propose MAFLA, a Metropolis-Adjusted Fractional\
  \ Langevin Algorithm that uses a score-based Metropolis-Hastings correction mechanism."
---

# Score-based Metropolis-Hastings for Fractional Langevin Algorithms

## Quick Facts
- arXiv ID: 2602.00835
- Source URL: https://arxiv.org/abs/2602.00835
- Reference count: 36
- Primary result: MAFLA achieves 8-35x lower Wasserstein-1 distance and significantly better tail-quantile errors compared to FULA on heavy-tailed α-stable mixtures

## Executive Summary
This paper addresses the challenge of sampling from heavy-tailed and multimodal distributions when neither the target density nor the proposal density can be evaluated, specifically for α-stable Lévy-driven fractional Langevin algorithms. The authors propose MAFLA, a Metropolis-Adjusted Fractional Langevin Algorithm that uses a score-based Metropolis-Hastings correction mechanism. MAFLA approximates fractional proposal scores using the location-family structure of isotropic symmetric α-stable distributions and learns an acceptance function via Score Balance Matching without evaluating any densities. The method significantly improves finite-time sampling accuracy over unadjusted fractional Langevin dynamics.

## Method Summary
MAFLA combines fractional Langevin dynamics with a learned Metropolis-Hastings correction. First, a target score network is trained via sliced score matching to estimate ∇log p(x). Proposals are generated using α-stable noise: x' = x + τb̃(x) + τ^(1/α)ξ where ξ ~ SαS(1). Instead of evaluating intractable densities, MAFLA approximates fractional proposal scores using the location-family structure and trains an acceptance network via Score Balance Matching, which enforces detailed balance at the gradient level. The method is evaluated on heavy-tailed mixtures and combinatorial optimization problems including MaxCut and minimum vertex cover.

## Key Results
- On 2D α-stable mixtures (α=1.95), MAFLA reduces W1 from 4.87 to 0.52 and 0.99-quantile error from 80.33 to 0.80 compared to FULA
- MAFLA shows robustness to step-size and dimension scaling, with consistent 20-35% W1 reduction across dimensions
- For combinatorial optimization, MAFLA achieves better solution quality and feasibility on MaxCut and minimum vertex cover problems
- The acceptance network trained via Score Balance Matching effectively filters poor proposals while preserving good exploration

## Why This Works (Mechanism)

### Mechanism 1
Approximating fractional proposal scores via location-family structure enables gradient-based MH corrections without tractable proposal densities. For isotropic symmetric α-stable distributions, the fractional score S_q^(α)(x'|x) = -1/(ατ)·r admits closed form where r = x' - x - τb̃(x). The proposal score gradients are computable from samples and drift without evaluating the intractable density f_α. This works for α ∈ (1,2] and small step sizes τ.

### Mechanism 2
Score Balance Matching enforces detailed balance at the gradient level, enabling learning of acceptance functions without density evaluation. The gradient form of detailed balance depends only on target scores, proposal score gradients, and acceptance score gradients. SBM minimizes L2(a) = E[||∇log a(x',x) - ∇log a(x,x') - Δp - Δq||²] with optional regularization. This ensures the learned acceptance function approximately satisfies detailed balance.

### Mechanism 3
Combining heavy-tailed fractional dynamics with MH-style correction improves exploration-stability trade-offs. α-stable noise (α < 2) produces occasional long jumps enabling escape from local minima; the learned acceptance function selectively rejects unlikely proposals, stabilizing tail behavior and reducing discretization bias. This yields lower Wasserstein-1 distances and tail-quantile errors compared to unadjusted FULA.

## Foundational Learning

- **Concept: α-stable distributions and Lévy processes**
  - Why needed here: Proposals use symmetric α-stable noise; understanding tail behavior and the absence of closed-form densities is essential
  - Quick check question: For α = 1.5, what moment orders exist, and why does this preclude classical MH acceptance computation?

- **Concept: Score matching (sliced and denoising variants)**
  - Why needed here: Target scores s_θ(x) ≈ ∇log p(x) are learned via sliced score matching; understanding the Fisher divergence and Jacobian trace approximations informs training choices
  - Quick check question: Why does sliced score matching use random projections v ~ N(0, I_d) to estimate tr(∇_x s(x))?

- **Concept: Detailed balance in MCMC**
  - Why needed here: The gradient-form detailed balance is the core theoretical constraint; SBM is a relaxation of this condition
  - Quick check question: Given the standard detailed balance equation p(x)q(x'|x)a(x,x') = p(x')q(x|x')a(x',x), derive the gradient form by taking log and ∇.

## Architecture Onboarding

- **Component map:** Score network -> Fractional drift computation -> Proposal mechanism -> Acceptance network -> SBM loss minimization
- **Critical path:** (1) Train score network on target samples → (2) Generate training pairs via curriculum interpolation → (3) Train acceptance network with SBM loss → (4) Run MAFLA sampling with parallel particles
- **Design tradeoffs:** K=0 drift approximation is robust but potentially lower accuracy ceiling vs K≥1: Higher ceiling, sensitive to h; λ_α regularization stabilizes tail errors but may underfit core distribution; smaller τ provides better approximation but slower mixing
- **Failure signatures:** Acceptance probabilities collapsing to 0 or 1 → increase entropy regularization λ; large W1 with good q95 → mode imbalance; training instability with K≥1 → fall back to K=0
- **First 3 experiments:** 1) Validate on 2D α-stable mixture: compare empirical vs target mixture weights; 2) Step-size sweep: plot W1, q95, q99 curves; 3) Dimension scaling: verify ~20-35% W1 reduction holds across dimensions

## Open Questions the Paper Calls Out

- **Question 1:** What theoretical bounds link score/proposal approximation errors to stationarity error in MAFLA?
  - Basis: The paper provides no theoretical bounds linking approximations to stationarity error, relying on empirical validation only
  - Resolution: Theoretical bounds on total variation or Wasserstein distance between MAFLA's stationary distribution and the true target

- **Question 2:** Can MAFLA be extended to anisotropic or non-isotropic α-stable distributions?
  - Basis: The proposal score approximations rely explicitly on the isotropic location-family structure
  - Resolution: Derivation of computable proposal score proxies for anisotropic stable distributions with empirical validation

- **Question 3:** Can the methodology extend to α ≤ 1 where fractional score formulas may not hold?
  - Basis: The mathematical machinery relies on α ∈ (1, 2]; different derivations are needed for super-heavy-tailed regime
  - Resolution: Alternative proposal score constructions valid for α ≤ 1 with experiments on targets with infinite mean

- **Question 4:** What are the convergence rates and mixing time guarantees for MAFLA under standard assumptions?
  - Basis: The paper demonstrates empirical finite-time improvements but provides no theoretical convergence analysis
  - Resolution: Provable mixing time bounds for MAFLA under assumptions on score approximation quality, step size, and target regularity

## Limitations

- The method assumes symmetric isotropic α-stable proposals, limiting applicability to more complex proposal structures
- Neural network architectures and training hyperparameters are not specified, requiring assumptions for reproduction
- Score Balance Matching provides gradient-level detailed balance but does not guarantee exact distributional correctness

## Confidence

**High Confidence Claims:**
- MAFLA significantly improves Wasserstein-1 distance and tail-quantile errors over FULA on heavy-tailed mixtures
- The proposed SBM approach enables learning acceptance functions without density evaluation, verified through experimental performance
- The fractional Langevin framework with α-stable noise can escape local minima more effectively than Gaussian-driven dynamics

**Medium Confidence Claims:**
- MAFLA's robustness to step-size and dimension scaling holds across diverse problem settings
- Performance gains on combinatorial optimization transfer to real-world instances
- The combination of heavy-tailed dynamics and MH correction provides optimal exploration-stability trade-offs

**Low Confidence Claims:**
- Generalization of SBM to non-symmetric or non-isotropic proposal distributions
- Behavior in extremely high dimensions (d > 32) where score estimation may become unreliable
- Stability of the method for α very close to 1 (Cauchy-like tails)

## Next Checks

1. **Curriculum Training Stability**: Run MAFLA with varying η schedules (linear vs. sigmoid) and monitor SBM loss convergence. Verify that acceptance probabilities remain well-calibrated throughout training.

2. **Gradient Balance Verification**: For a simple 2D Gaussian target, compute empirical ∇log a(x',x) - ∇log a(x,x') and compare against Δp + Δq from training data. Quantify how closely the learned acceptance satisfies the gradient detailed balance condition.

3. **Score Estimation Sensitivity**: Replace the learned target score with ground-truth gradients (when available) and measure the impact on MAFLA performance. This isolates the contribution of score estimation error versus the SBM correction mechanism.