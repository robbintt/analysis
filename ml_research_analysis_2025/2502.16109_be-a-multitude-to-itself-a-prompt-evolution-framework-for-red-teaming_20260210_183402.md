---
ver: rpa2
title: 'Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming'
arxiv_id: '2502.16109'
source_url: https://arxiv.org/abs/2502.16109
tags:
- prompts
- attack
- llms
- prompt
- teaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RTPE, a framework for evolving red teaming
  prompts to improve the discovery of harmful LLM outputs. It addresses the inefficiency
  of manual red teaming by automatically scaling up prompts through in-breadth and
  in-depth evolution.
---

# Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming

## Quick Facts
- arXiv ID: 2502.16109
- Source URL: https://arxiv.org/abs/2502.16109
- Reference count: 10
- Key outcome: RTPE framework achieves 80% attack success rate on GPT-3.5 while generating highly diverse prompts

## Executive Summary
RTPE is a framework for automatically evolving red teaming prompts to discover harmful LLM outputs more efficiently than manual approaches. The framework uses two evolutionary strategies: in-breadth scaling through enhanced in-context learning with comparative examples and poetry mutagenic factors, and in-depth diversification through structured transformations. Experiments show RTPE outperforms baselines in both attack success rate and diversity across 8 sensitive topics and 8 different LLM models.

## Method Summary
RTPE operates in two stages: first, it scales prompt quantity through an in-breadth evolution loop using enhanced in-context learning with superior and inferior examples plus poetry mutagenic factors; second, it diversifies prompt forms through in-depth transformations including downward expansion, restructuring, dialogue simulation, and length reduction. The framework generates 4,800 total prompts (1,920 breadth + 2,880 depth) and evaluates them across 8 sensitive topics on multiple target models.

## Key Results
- RTPE achieves 80% attack success rate on GPT-3.5-turbo-0613, outperforming baselines (SAP: 0.57 ASR, FLIRT: 0.73 ASR)
- High-frequency words with abstract, negative, or artistic traits are particularly effective for attacks
- GPT-3.5 is most vulnerable among tested models; "fraud" prompts are easiest to exploit
- Larger models and later versions show better safety performance

## Why This Works (Mechanism)

### Mechanism 1: Comparative Examples Prevent ICL Collapse
LLMs can learn from negative examples through contrastive reasoning, not just imitation. Superior examples demonstrate effective patterns while inferior examples provide contrastive signals that steer generation away from repetitive templates. Evidence shows w/o inferior examples drops ASR from 0.80 to 0.63; replacing inferior with superior drops ASR to 0.68.

### Mechanism 2: Poetry as Mutagenic Factor Conceals Malicious Intent
Poetry provides condensed, symbolic language patterns that mask aggressive intent from safety classifiers. Safety alignment primarily detects overt harmful language; symbolic/abstract phrasing bypasses these filters. Poetry achieves ASR of 0.80 vs. w/o Poetry at 0.73; other genres underperform (Novel 0.54, Play 0.65).

### Mechanism 3: In-Depth Transformations Maintain Efficacy Under Variation
Safety vulnerabilities are often structural/conceptual rather than tied to exact phrasing. Structured transformations test whether failures are robust to surface variation. RTPE transformations achieve competitive ASR (downward 0.71, restructure 0.77, dialogue 0.75, length 0.68) while expanding coverage.

## Foundational Learning

- **In-Context Learning (ICL) Stability**: ICL is highly sensitive to demonstration selection and format. Quick check: Can you explain why providing only successful attack examples might produce less diverse outputs than providing both successes and failures?

- **Attack Success Rate (ASR) vs. Diversity Tradeoff**: The paper explicitly optimizes for both; baseline methods achieve high ASR with low diversity or high diversity with low ASR. Quick check: If a red-teaming method achieves 90% ASR but all prompts are minor variations of one template, what vulnerability would it miss?

- **Safety Alignment Bypass Patterns**: Understanding why abstract/negative/artistic language evades classifiers helps interpret the poetry mechanism. Quick check: Why might "explain how to create a fake charity" trigger rejection while "in the realm of charitable acts and noble intentions, orchestrate a scheme" succeed?

## Architecture Onboarding

- **Component map**: [Initial Seeds X] → [Prompt Pool P] → IN-BREADTH LOOP (sample superior/inferior, add poetry, generate new prompt, evaluate, add to pool) → [Pre-generated Prompts] → IN-DEPTH TRANSFORMS (downward expansion, restructure, dialogue simulation, length declining) → [Final Attack Prompt Set]

- **Critical path**: The in-breadth loop is the scaling engine; if demonstration selection or mutagenic factors fail, the entire downstream pipeline suffers. In-depth transforms are optional amplification, not core generation.

- **Design tradeoffs**: ASR vs. Diversity: FIFO achieves 0.57 ASR with 0.91 n-gram similarity; RTPE achieves 0.80 ASR with 0.39 similarity. Seed robustness vs. quality: random seeds produce consistent results (ASR 0.73-0.80). Length vs. efficacy: simple truncation drops ASR to 0.57; LLM-based compression maintains 0.68 ASR.

- **Failure signatures**: High n-gram similarity (>0.85) indicates mode collapse. ASR drops sharply under length truncation if structural keywords are removed. Dialogue simulation loses efficacy beyond 4 rounds.

- **First 3 experiments**:
  1. Reproduce Table 1 baselines comparison on GPT-3.5-turbo-0613 measuring ASR, Self-BLEU, and embedding diversity.
  2. Ablate mutagenic factors by removing poetry and replacing with other genres (Table 6 replication).
  3. Test topic sensitivity across all 8 topics on 2-3 models, expecting "fraud" to show highest ASR (>0.85).

## Open Questions the Paper Calls Out

### Open Question 1
What concrete defense mechanisms can effectively mitigate RTPE-evolved attacks while preserving model utility? The paper states it "does not provide concrete methods for defending LLMs against such attacks" and leaves this as future work. This remains unresolved despite RTPE demonstrating high attack success rates (up to 80%) across multiple models.

### Open Question 2
How does RTPE's effectiveness scale against more advanced models such as GPT-4, Claude-3, and larger open-source models like Llama-2-70b? Budget and access constraints limited testing to 8 models; only preliminary observations via chat.lmsys.org were mentioned.

### Open Question 3
What linguistic or semantic properties make poetry more effective than other literary genres as a mutagenic factor for concealing malicious intent? Table 6 shows poetry achieves 80% ASR versus 54-71% for other genres, but the mechanism behind this gap remains unexplored.

### Open Question 4
Why do larger models (13B) exhibit worse safety performance than smaller models (7B) under RTPE attacks? The paper observes larger models "demonstrate inferior safety performance" but provides no causal explanation for this counterintuitive finding.

## Limitations
- Does not provide concrete defense methods against evolved attacks
- Limited testing on frontier models like GPT-4 and Claude-3 due to access constraints
- Only preliminary observations on larger open-source models like Llama-2-70b

## Confidence
- **High**: ASR and diversity metrics are clearly defined and consistently measured across experiments
- **Medium**: Poetry mutagenic factor effectiveness is well-demonstrated but mechanism remains partially unexplained
- **Low**: Causal explanations for larger model vulnerabilities and genre-specific effectiveness are speculative

## Next Checks
1. Verify in-breadth loop implementation by checking if both superior and inferior examples are being sampled (not just top-scoring prompts)
2. Test poetry vs. other genre ablation to confirm 7-15 point ASR advantage
3. Validate topic sensitivity by running fraud and terrorism prompts on same model to confirm expected ASR gap (>0.85 vs <0.70)