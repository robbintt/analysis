---
ver: rpa2
title: 'ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning'
arxiv_id: '2512.00305'
source_url: https://arxiv.org/abs/2512.00305
tags:
- chart
- reasoning
- data
- grounding
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of numerical hallucinations in
  multimodal large language models (MLLMs) when interpreting charts with sparse textual
  annotations. The core method, PointCoT, integrates reflective interaction into chain-of-thought
  reasoning by prompting MLLMs to generate bounding boxes and re-render charts based
  on location annotations, establishing connections between textual reasoning steps
  and visual grounding regions.
---

# ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning

## Quick Facts
- **arXiv ID**: 2512.00305
- **Source URL**: https://arxiv.org/abs/2512.00305
- **Reference count**: 40
- **Key outcome**: Achieves state-of-the-art chart reasoning performance, improving accuracy on ChartBench by +5.04% through integrating bounding box grounding into chain-of-thought reasoning.

## Executive Summary
This paper addresses the persistent problem of numerical hallucinations in multimodal large language models (MLLMs) when interpreting charts with sparse textual annotations. The authors propose PointCoT, a method that extends chain-of-thought reasoning by requiring MLLMs to generate bounding boxes for specific reasoning steps, creating a visual feedback loop that grounds textual reasoning in chart regions. They construct ChartPoint-SFT-62k, a high-quality dataset of 19.2K chart samples with explicit grounding and reasoning steps, and use it to fine-tune two instruction-tuned models (ChartPointQ2 and ChartPointQ2.5) that achieve state-of-the-art performance across multiple chart benchmarks.

## Method Summary
The method constructs a dataset by generating chain-of-thought reasoning steps from chart code, identifying "Grounding" steps that require visual localization, and modifying the code to insert '@' markers at relevant coordinates. After rendering, OCR tools extract these coordinates and normalize them to [0-999] integers. The resulting dataset includes four instruction formats: standard VQA, localization tasks, reasoning with edited charts, and a mix of all types. Two Qwen2-VL-based models are fine-tuned using a two-stage approach: first on general chart data, then on the ChartPoint-SFT-62k dataset using LLaMA-Factory with specific hyperparameters.

## Key Results
- ChartPointQ2.5 achieves 61.1% accuracy on ChartBench, outperforming previous state-of-the-art by +5.04%
- On ChartQA, ChartPointQ2.5 achieves 78.1% accuracy with the PointCoT method, compared to 77.6% without
- The method shows significant improvements across multiple chart benchmarks including PlotQA and InfoQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating bounding boxes for reasoning steps creates a visual feedback loop that aligns textual reasoning with chart elements.
- Mechanism: PointCoT extends chain-of-thought reasoning by requiring the model to output bounding boxes for each "Grounding" step (e.g., locating an axis, legend, or data point). This forces the model to attend to specific image regions to produce coordinates. The paper suggests this creates a "reflective interaction," where the act of localizing may implicitly verify the reasoning step's validity against the visual input.
- Core assumption: The model possesses sufficient pre-existing localization capabilities to generate meaningful bounding boxes.
- Evidence anchors:
  - [abstract] "...prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions."
  - [section 3.1] "...incorporate coordinate-based cues at each step to justify the model’s focus region."
  - [corpus] Related work (e.g., ChartLens, ChartAgent) also explores visual attribution and grounding in charts, suggesting this is a recognized problem space.
- Break condition: The mechanism breaks if the model cannot generate accurate bounding boxes or if the grounding steps are too abstract to be localized to a specific region.

### Mechanism 2
- Claim: Structured data supervision with explicit "Grounding" vs. "Reasoning" step labels teaches the model a human-like, verifiable process for chart interpretation.
- Mechanism: The authors use a pipeline (an LLM and code editing) to generate a dataset where each reasoning step is labeled. "Grounding" steps are paired with bounding boxes, explicitly teaching the model to look at the chart for specific information before making logical inferences in "Reasoning" steps. This separates data extraction from logical deduction.
- Core assumption: Chart reasoning can be decomposed into a sequence of discrete grounding and reasoning steps, and this structure is learnable.
- Evidence anchors:
  - [section 3.2] "...classify each sub-step into two categories: Grounding and Reasoning... This classification helps incorporate specific bounding boxes for steps that require element localization..."
  - [section 3.4] "Type 3: Reasoning with Edited Chart... aiding the model in learning the correct visual reasoning logic."
  - [corpus] No direct corpus evidence for this specific training signal.
- Break condition: This breaks for questions requiring a non-decomposable, holistic understanding of the chart or where the grounding/reasoning boundary is blurry.

### Mechanism 3
- Claim: Training on multiple data formats (VQA, localization, reasoning with edited charts) forces the model to internalize the connection between visual regions and reasoning.
- Mechanism: The dataset includes four instruction types, including one where the correct bounding box is rendered onto the chart as input. This multi-format training is designed to make the model robust and teach it to use visual cues, rather than relying solely on OCR-extracted text.
- Core assumption: Training on these specific formats generalizes to new, unseen charts and questions.
- Evidence anchors:
  - [abstract] "...introduce an automated pipeline to construct ChartPoint-SFT-62k..."
  - [section 3.4] "Fig 4 illustrates the process... which primarily includes four formats..."
  - [corpus] No direct corpus evidence for this specific multi-format training scheme.
- Break condition: The model overfits to the specific data formats or styles in the training set and fails to generalize to real-world charts.

## Foundational Learning

- Concept: **Visual Grounding** in MLLMs.
  - Why needed here: The entire method rests on the model's ability to link textual concepts (e.g., "the bar for 2018") to specific spatial coordinates in an image.
  - Quick check question: Can your base model (e.g., Qwen2-VL) accurately point to an object in an image given a textual description?

- Concept: **Chain-of-Thought (CoT) Reasoning**.
  - Why needed here: PointCoT is a modification of CoT. Understanding the basic idea of step-by-step decomposition is critical to grasp how adding bounding boxes enhances the process.
  - Quick check question: Can your model solve a multi-step arithmetic word problem by explaining its steps before giving a final answer?

- Concept: **Supervised Fine-Tuning (SFT)**.
  - Why needed here: The ChartPoint models are produced by fine-tuning base MLLMs on a specific, constructed dataset. The performance gain is attributed to this training, not just a prompting technique.
  - Quick check question: Are you familiar with fine-tuning a pre-trained model on a task-specific dataset of input-output pairs?

## Architecture Onboarding

- Component map:
  - Input: A chart image and a question
  - ChartPoint Model (MLLM): A fine-tuned model (e.g., based on Qwen2-VL)
  - Output: A text response containing a CoT with interleaved bounding boxes (`<bbox>...</bbox>`) and a final answer
  - Training Pipeline (Offline): LLM-based step generator, code editor, chart renderer, and OCR tool to create the `ChartPoint-SFT-62k` dataset

- Critical path:
  1. **Training**: A base MLLM with grounding abilities is fine-tuned on the multi-format SFT dataset
  2. **Inference**: The model receives a chart and question, and generates a CoT. At each step, it may output a bounding box to "ground" its reasoning
  3. **Final Answer**: The model produces a final answer based on its CoT

- Design tradeoffs:
  - **Reliance on Base Model**: The method is critically dependent on the base model having strong innate grounding capabilities (paper shows failure on models without it)
  - **Pipeline Complexity**: The data generation pipeline is complex and has a low yield
  - **Format Sensitivity**: Performance is sensitive to how coordinates are formatted (0-999 integer format works best)

- Failure signatures:
  - The model ignores instructions to generate bounding boxes and produces text-only CoT
  - The model generates irrelevant or inaccurate bounding boxes, indicating weak localization
  - Performance drops on charts with sparse text annotations where OCR is unreliable

- First 3 experiments:
  1. **Baseline Check**: Run a strong baseline MLLM (e.g., Qwen2.5-VL) on ChartBench. Observe if it generates bounding boxes when prompted and check its accuracy
  2. **Ablation on Data Formats**: Train a model using only the standard VQA data (Type 1) vs. one using the full `ChartPoint-SFT-62k` dataset and compare performance on ChartQA
  3. **Ablation on Base Model**: Train two versions of ChartPoint—one on Qwen2-VL (strong grounder) and one on a model with weaker grounding (e.g., ChartMoE)—to verify the grounding capability prerequisite

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PointCoT be adapted to improve chart reasoning in MLLMs that lack strong pre-existing grounding capabilities?
- Basis in paper: [inferred] Page 7, Section 5.2 states that "PointCoT is highly dependent on the underlying localization capabilities," noting that models like Qwen-VL and ChartMoE showed no improvement because their reflection based on BBox failed.
- Why unresolved: The method currently fails to enhance models with weak localization, suggesting the proposed self-reflection mechanism requires a minimum threshold of visual grounding competence to be effective.
- What evidence would resolve it: Demonstrating that specific pre-training or auxiliary losses can equip weak models with the necessary grounding to benefit from PointCoT.

### Open Question 2
- Question: To what extent does tokenizer design impact the effectiveness of coordinate-based visual grounding?
- Basis in paper: [inferred] Page 7, Section 5.3 notes that normalizing coordinates to 4-decimal values (Type A) failed because the tokenizer splits decimals into segments, whereas integer-based coordinates (Type C) yielded the best results.
- Why unresolved: The performance variance (0.52% vs. 1.68%) suggests that numerical representation fidelity is a bottleneck, but it is unclear if this is a fundamental limitation of current tokenizers or a solvable data-formatting issue.
- What evidence would resolve it: A comparative analysis of PointCoT on models utilizing specialized math/coordinate-aware tokenizers versus standard LLM tokenizers.

### Open Question 3
- Question: Does visual grounding reflection scale to complex reasoning tasks beyond direct data extraction?
- Basis in paper: [inferred] Page 3, Section 3.2 explicitly states the dataset focuses on "straightforward chart comprehension, centered around chart data points Q&A" rather than "complex numerical reasoning."
- Why unresolved: While grounding improves value extraction, it remains untested whether forcing the model to generate bounding boxes aids or hinders high-level abstract reasoning (e.g., summarizing trends or multi-hop inference).
- What evidence would resolve it: Evaluation results on benchmarks requiring complex logical inference (like ChartQA's "human" subset) analyzing the correlation between grounding step accuracy and final reasoning correctness.

## Limitations

- The method's effectiveness is critically dependent on the base model having strong pre-existing visual grounding capabilities, failing to improve models without this foundation
- The automated data generation pipeline has a relatively low yield (77% success rate for OCR extraction) and relies on complex orchestration between multiple components
- The paper does not thoroughly evaluate performance on charts with very sparse annotations where OCR-based text extraction would be unreliable

## Confidence

**High Confidence**: The core observation that integrating bounding boxes into CoT reasoning improves chart understanding is well-supported by quantitative results (+5.04% on ChartBench). The mechanism of using visual grounding to reduce hallucinations is theoretically sound and consistent with established MLLM research.

**Medium Confidence**: The claim that structured data supervision with explicit grounding/reasoning labels is the primary driver of improvement. While the ablation study supports this, the complexity of the training pipeline makes it difficult to isolate the exact contribution of this specific training signal versus other factors.

**Low Confidence**: The assertion that the multi-format training approach (including edited charts with bounding boxes) generalizes to real-world charts. The paper provides limited evidence for how well this approach transfers to charts outside the training distribution, particularly those with different visual styles or annotation densities.

## Next Checks

1. **Base Model Dependency Test**: Systematically evaluate PointCoT across a broader range of base models with varying grounding capabilities to quantify the minimum threshold required for the method to be effective.

2. **Format Contribution Analysis**: Design an ablation study that isolates the contribution of each training format (VQA only, VQA + localization, full multi-format) to determine which specific components drive performance improvements.

3. **Real-World Generalization**: Test the trained models on a diverse set of real-world charts from sources outside the training distribution, particularly those with minimal textual annotations, to validate claims about reducing numerical hallucinations in sparse-text scenarios.