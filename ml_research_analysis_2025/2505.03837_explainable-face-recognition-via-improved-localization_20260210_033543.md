---
ver: rpa2
title: Explainable Face Recognition via Improved Localization
arxiv_id: '2505.03837'
source_url: https://arxiv.org/abs/2505.03837
tags:
- class
- face
- image
- recognition
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of transparency in deep learning-based
  face recognition systems by proposing a method for explainable face recognition
  using the Scaled Directed Divergence (SDD) technique. The SDD technique performs
  fine localization of class-specific features in scenarios with overlapping classes,
  which is common in face recognition.
---

# Explainable Face Recognition via Improved Localization

## Quick Facts
- arXiv ID: 2505.03837
- Source URL: https://arxiv.org/abs/2505.03837
- Authors: Rashik Shadman; Daqing Hou; Faraz Hussain; M G Sarwar Murshed
- Reference count: 21
- One-line primary result: SDD CAMs achieve 62.33% average confidence drop vs 42.50% for random CAMs in face recognition explanation

## Executive Summary
This paper addresses the critical challenge of transparency in deep learning-based face recognition systems by proposing the Scaled Directed Divergence (SDD) technique. The method performs fine localization of class-specific features in overlapping class scenarios, which is particularly relevant for face recognition where individuals often share similar facial characteristics. The authors fine-tune the AdaFace model on the FaceScrub dataset and achieve 96.33% test accuracy, then apply SDD to generate visual explanations that highlight the most relevant discriminative face features for model predictions.

The effectiveness of SDD is validated through a deletion-and-retention scheme, demonstrating that SDD CAMs outperform random CAMs in terms of average confidence drop (62.33% vs. 42.50% for deletion) and prediction change percentage (51.36% vs. 29.73% for deletion). These results validate that SDD successfully localizes significant face features, enhancing the transparency and trustworthiness of deep learning-based face recognition systems by providing more focused and interpretable visual explanations.

## Method Summary
The SDD technique addresses explainable face recognition by performing fine localization of class-specific features through discriminative subtraction. The method fine-tunes AdaFace (ResNet100 backbone pre-trained on WebFace12M) on FaceScrub dataset (530 classes, 80/10/10 split) achieving 96.33% test accuracy. SDD generates class activation maps by calculating divergence between target class CAM and weighted sum of other class CAMs, then applying non-linear amplification. The technique uses top-5 predicted classes for subtraction and is evaluated through deletion-retention metrics where top 20% CAM values are removed/retained to measure confidence drop and prediction change.

## Key Results
- SDD CAMs achieve 62.33% average confidence drop vs 42.50% for random CAMs when deleting top 20% highlighted pixels
- SDD CAMs cause 51.36% prediction changes vs 29.73% for random CAMs in deletion evaluation
- The method achieves 96.33% test accuracy on FaceScrub dataset after fine-tuning AdaFace model

## Why This Works (Mechanism)

### Mechanism 1: Discriminative Subtraction for Class Separability
The Scaled Directed Divergence (SDD) technique isolates identity-specific features by subtracting activation maps of non-target classes from the target class map. This subtraction acts as a filter where features common to multiple identities cancel out, leaving only discriminative features unique to the target identity. The method calculates divergence as $(x_tF - \sum x_k)$ where $x_t$ is target and $x_k$ are other classes, effectively removing shared facial features like general face shape while preserving specific ones like birthmarks.

### Mechanism 2: Adaptive Scaling and Signal Amplification
The technique includes dynamic scaling factor $F$ and amplification parameter $\alpha$ to stabilize subtraction and sharpen visualization. Raw subtraction could lead to negative values or signal loss, so scaling factor $F$ normalizes target map relative to others, while exponentiation ($e^{\alpha \dots}$) non-linearly amplifies remaining divergence. This turns subtle differences into distinct hot spots, making the final SDD CAM visually interpretable and semantically meaningful.

### Mechanism 3: Semantic Sensitivity via Deletion Metrics
The reported effectiveness relies on causal link between highlighted pixels and model confidence score. By removing top 20% of SDD-highlighted pixels and observing large confidence drop (62.33%), the paper demonstrates these spatial regions genuinely drive model decision logic. This validates that explanations match behavior, as good explanation maps must contain pixels whose removal maximally degrades performance.

## Foundational Learning

- **Concept: Class Activation Mapping (CAM)**
  - Why needed: This is the primitive signal SDD modifies; understanding standard CAMs helps see why "divergence" subtraction is needed
  - Quick check: Can you explain why a standard CAM might highlight the whole face rather than just distinguishing features?

- **Concept: The Class Overlap Problem**
  - Why needed: Core motivation for SDD is that face recognition involves classes (people) that look very similar
  - Quick check: Why would standard visualization struggle to distinguish between two people with similar glasses or hairstyles?

- **Concept: Deletion/Retention Metrics**
  - Why needed: This is the validation standard; understanding that good explanations must "break" the model is crucial
  - Quick check: If removing highlighted pixels does not change model prediction, what does that imply about the explanation?

## Architecture Onboarding

- **Component map:** FaceScrub dataset -> AdaFace (ResNet100) backbone -> Fine-tuning layer (FaceScrub classes) -> SDD Generator (feature maps + class weights) -> Heatmap -> Deletion Evaluator (masks input) -> Backbone -> Confidence Drop measurement

- **Critical path:** Generation of "Top-5" CAMs is essential as SDD requires generating CAMs for predicted class and most similar distractor classes to perform subtraction, not just single-map calculation

- **Design tradeoffs:** SDD provides narrower localization but depends heavily on choice of "other" classes used in subtraction set; computational overhead requires inference for multiple class weightings (Target + Top-K others) rather than just target class

- **Failure signatures:** Empty maps if scaling factor F is too low or divergence is negative; over-highlighting if top-5 classes aren't actually visually similar to target; incorrect parameter settings can suppress entire map or fail to distinguish specific features

- **First 3 experiments:** 1) Run standard CAM on correctly classified face to confirm broad localization as baseline; 2) Visualize SDD map with different alpha values to see focus area control; 3) Mask top 20% SDD pixels on original image and feed back to model to verify confidence drop matches claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDD quantitatively compare to other state-of-the-art explainability methods (e.g., Grad-CAM, LIME) using deletion-and-retention evaluation scheme?
- Basis in paper: Experimental evaluation only benchmarks SDD against "Random CAM" baseline, omitting comparisons with other established visual explanation techniques
- Why unresolved: Paper demonstrates SDD is better than random but lacks statistical evidence it outperforms existing discriminative localization methods
- What evidence would resolve it: Comparative results from deletion-and-retention tests using Grad-CAM and LIME baselines on same FaceScrub dataset

### Open Question 2
- Question: Does SDD's narrow localization actually enhance user trust compared to broader traditional explanations?
- Basis in paper: Abstract and Conclusion claim method ensures "transparency and trust," yet evaluation relies solely on computational metrics without human subject experiments
- Why unresolved: Computational faithfulness doesn't automatically correlate with human interpretability or increased trust
- What evidence would resolve it: User study measuring subjective trust scores or ability to predict model failures based on SDD vs traditional CAM visualizations

### Open Question 3
- Question: Is SDD effective for non-CNN architectures like Vision Transformers (ViT) increasingly common in face recognition?
- Basis in paper: Methodology relies specifically on ResNet100 backbone to generate feature maps, and SDD depends on spatial feature maps typical of CNNs
- Why unresolved: Paper doesn't test technique on architectures lacking spatial convolutional structure required by current SDD formula
- What evidence would resolve it: Application of SDD technique to ViT-based face recognition model and analysis of resulting localization maps

## Limitations

- The computational mechanism relies on somewhat arbitrary design choices (scaling factor formula, top-5 class selection, alpha parameter) without sensitivity analysis
- Experimental validation is limited to single dataset (FaceScrub) and single backbone model (AdaFace ResNet100), raising generalizability concerns
- Claims about enhanced trust and interpretability lack qualitative validation through user studies or expert evaluation

## Confidence

**High Confidence:** Technical implementation of SDD is mathematically coherent and reproducible; deletion-and-retention evaluation methodology is standard practice

**Medium Confidence:** Claim that SDD achieves better feature localization than random CAMs is supported by quantitative metrics, though practical significance of 20 percentage point difference warrants further investigation

**Low Confidence:** Assertion that SDD provides truly interpretable explanations aligning with human understanding lacks qualitative validation through user studies or expert evaluation

## Next Checks

1. **Cross-Dataset Validation:** Test SDD on multiple face recognition datasets (LFW, VGGFace2) to assess generalizability beyond FaceScrub

2. **Parameter Sensitivity Analysis:** Systematically vary alpha parameter, scaling factor F, and number of top classes used in subtraction to determine robustness to hyperparameter choices

3. **Human Evaluation Study:** Conduct user study where human observers rate whether SDD-highlighted regions correspond to genuinely discriminative features they would use to distinguish between similar-looking individuals