---
ver: rpa2
title: 'DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition
  based on Active Learning'
arxiv_id: '2507.12011'
source_url: https://arxiv.org/abs/2507.12011
tags:
- class
- learning
- dataset
- duse
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUSE is a data expansion framework for low-resource automatic modulation
  recognition that uses an uncertainty scoring function and active learning strategy
  to selectively transfer informative samples from an auxiliary dataset to a target
  dataset. It iteratively trains a model, scores samples by uncertainty (difference
  between top two class probabilities), and selects the most uncertain samples for
  transfer.
---

# DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning

## Quick Facts
- arXiv ID: 2507.12011
- Source URL: https://arxiv.org/abs/2507.12011
- Reference count: 40
- Key outcome: DUSE improves AMR accuracy up to 23.81% under class-imbalance using uncertainty-based active learning

## Executive Summary
DUSE is a data expansion framework designed to address the challenge of low-resource Automatic Modulation Recognition (AMR) by selectively transferring informative samples from a large auxiliary dataset to a small target dataset. It uses an uncertainty scoring function based on the softmax probability margin (difference between top two class probabilities) to identify samples near decision boundaries. These uncertain samples are then iteratively transferred using an active learning strategy, continuously refining the scorer's estimates. Extensive experiments demonstrate DUSE's superiority over 8 coreset selection baselines, with significant accuracy improvements and strong cross-architecture generalization.

## Method Summary
DUSE operates by iteratively training a model on a small target dataset (D_T), scoring the remaining samples in an auxiliary dataset (D_A) by their uncertainty (softmax margin), and transferring the most uncertain samples back to D_T. This active learning loop refines uncertainty estimates over multiple rounds, with the number of rounds and transfer batch size determined by the expansion rate. The method maintains class balance better than alternatives and selects samples that are informative across different network architectures. Implementation requires partitioning data into D_T and D_A, implementing the uncertainty scoring function, running the iterative selection loop, and evaluating the expanded dataset on a held-out test set.

## Key Results
- DUSE outperforms 8 coreset selection baselines across three AMR datasets
- Accuracy improvements up to 23.81% under class-imbalance settings
- Strong cross-architecture generalization: datasets selected with 2D-CNN perform well on five other architectures (67-93% accuracy)
- Maintains class balance better than alternatives and ensures tight clustering within classes

## Why This Works (Mechanism)

### Mechanism 1
Samples with low uncertainty scores (small margin between top two class probabilities) lie near decision boundaries and refine them more effectively than high-confidence samples. The uncertainty scoring function computes u = p*_1 - p*_2 from softmax outputs. Lower u indicates the model struggles to distinguish between the top two classes, signaling boundary-proximal samples. Selecting these forces the model to clarify ambiguous regions. Core assumption: Decision boundary refinement on uncertain samples yields greater generalization gains than reinforcing already-confident predictions.

### Mechanism 2
Iterative retraining on progressively augmented data improves uncertainty estimates and mitigates early selection errors. Active learning loop alternates between (1) training on current D*_T, (2) re-scoring remaining auxiliary samples with updated model, and (3) transferring top-K uncertain samples. This dynamic feedback adapts scoring to evolving data distribution. Core assumption: Model trained on larger, incrementally-improved dataset provides better uncertainty estimates than one trained only on initial small target set.

### Mechanism 3
Uncertainty-based sample selection is architecture-agnostic; datasets selected with one model transfer effectively to others. Decision boundaries, while model-specific in exact geometry, reflect shared task structure. Samples near boundaries for one architecture tend to be informative for others because they capture fundamental class ambiguities. Core assumption: Uncertainty correlates with intrinsic sample difficulty or class overlap, not just model-specific artifacts.

## Foundational Learning

- **Concept: Active Learning**
  - Why needed here: DUSE's iterative selection loop is an active learning paradigm applied to data expansion rather than labeling.
  - Quick check question: Can you explain why retraining the model after each batch of selections might improve subsequent uncertainty estimates?

- **Concept: Softmax Probability Margin**
  - Why needed here: The uncertainty score u = p*_1 - p*_2 is a margin-based measure; understanding softmax calibration is essential.
  - Quick check question: If a model outputs softmax probabilities [0.45, 0.40, 0.15], what is the margin-based uncertainty score?

- **Concept: Coreset Selection**
  - Why needed here: DUSE is compared against 8 coreset baselines (Entropy, Margin, GraNd, Forgetting, etc.); understanding these clarifies the design space.
  - Quick check question: How does coreset selection (reducing dataset size) differ from data expansion (increasing dataset with external samples)?

## Architecture Onboarding

- **Component map**: Target dataset D_T (small, labeled) -> Auxiliary dataset D_A (large, labeled) -> Backbone model f(·; θ) -> Uncertainty scoring module -> Selection module -> Training loop -> Iteration controller

- **Critical path**:
  1. Implement uncertainty scoring function (Algorithm 1) — ensure numerical stability in softmax
  2. Verify active learning loop correctly removes selected samples from D_A and appends to D_T
  3. Validate class distribution after selection (Figure 3) — DUSE maintains better balance than baselines

- **Design tradeoffs**:
  - Higher K (samples per round): Faster expansion but risk of noisy selections if model undertrained
  - More rounds R: Finer-grained adaptation but higher compute cost
  - Backbone choice: 2D-CNN used in paper; other architectures may yield different selections but cross-architecture results suggest robustness

- **Failure signatures**:
  - Selected samples cluster in few classes (check per-class counts as in Figure 3)
  - Accuracy plateaus or degrades at high expansion rates (possible overfitting or distribution drift)
  - Large variance across runs (check random seeds, initialization)

- **First 3 experiments**:
  1. Reproduce ablation (Table IV) on one dataset: Compare one-shot vs. active learning selection at fixed budget to verify iterative refinement gain
  2. Visualize class distribution (Figure 3 style) for DUSE vs. one baseline (e.g. Entropy) to confirm balance preservation
  3. Test cross-architecture transfer (Table III subset): Train 2D-CNN for selection, evaluate expanded dataset on at least two other architectures

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DUSE perform in low signal-to-noise ratio (SNR) regimes (e.g., below 10dB) where uncertainty estimates may be noisy or unreliable? The authors exclude SNR ≤ 10dB samples, leaving robustness to channel noise unproven.

- **Open Question 2**: Can DUSE effectively transfer samples when the auxiliary dataset has a significantly different distribution or class imbalance compared to the target domain? Current experiments use a split of the same dataset, not testing domain shift scenarios.

- **Open Question 3**: What is the computational trade-off between DUSE's iterative active learning rounds and the final performance gain compared to single-pass selection methods? The paper shows accuracy improvements but doesn't quantify the computational cost of repeatedly retraining the scorer model.

## Limitations
- Model architecture ambiguity: The paper references 2D-CNN without specifying exact layer configurations, making faithful reproduction challenging.
- Active learning parameters: Number of rounds R is not explicitly stated, which affects the granularity of iterative refinement and total computation.
- Distribution drift risk: Cross-architecture generalization assumes shared boundary structures, but radical architectural differences may weaken this correspondence.

## Confidence
- **Mechanism 1 (Boundary Refinement)**: Medium - Well-supported by margin-based scoring theory and ablation results, but assumes auxiliary and target distributions align sufficiently.
- **Mechanism 2 (Iterative Retraining)**: High - Clear empirical gains in Table IV and consistent with active learning literature; the feedback loop is the core innovation.
- **Mechanism 3 (Cross-Architecture Generalization)**: Medium - Strong quantitative support in Table III, but limited exploration of extreme architectural mismatches.

## Next Checks
1. **Ablation on Round Count**: Reproduce Table IV results while varying R (e.g., 3 vs 10 rounds) to quantify the trade-off between refinement quality and computational cost.
2. **Per-Class Balance Analysis**: Generate class distribution plots (Figure 3 style) for DUSE vs. Entropy at r=19% to confirm that DUSE consistently preserves class balance across all three datasets.
3. **Architectural Robustness Test**: Train 2D-CNN for selection, then evaluate the expanded dataset on a non-CNN architecture (e.g., GRU or transformer) to stress-test the cross-architecture generalization claim.