---
ver: rpa2
title: 'Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization'
arxiv_id: '2505.21321'
source_url: https://arxiv.org/abs/2505.21321
tags:
- benchmarks
- bencher
- optimization
- benchmark
- cont
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bencher introduces a modular benchmarking framework for black-box
  optimization that isolates each benchmark in its own virtual Python environment,
  accessed via a unified RPC interface. This architecture eliminates dependency conflicts
  and simplifies integration of diverse real-world benchmarks.
---

# Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization
## Quick Facts
- arXiv ID: 2505.21321
- Source URL: https://arxiv.org/abs/2505.21321
- Reference count: 12
- Bencher enables reproducible benchmarking for black-box optimization with 80 benchmarks across continuous, categorical, and binary domains

## Executive Summary
Bencher addresses the challenge of reproducible benchmarking in black-box optimization by introducing a modular framework that isolates each benchmark in its own virtual Python environment. This approach eliminates dependency conflicts and simplifies the integration of diverse real-world benchmarks. The framework uses Docker and Singularity containers to ensure consistent execution across different computing environments, from local machines to high-performance computing clusters.

The system provides a unified RPC interface that allows optimization algorithms to interact with benchmarks without requiring knowledge of their internal dependencies or implementation details. With support for 80 benchmarks including 18 real-world applications, Bencher offers researchers a comprehensive platform for comparing optimization algorithms under standardized conditions while maintaining flexibility for custom benchmark integration.

## Method Summary
Bencher employs a container-based architecture where each benchmark runs in an isolated environment with its own Python dependencies. The framework uses Docker and Singularity containers to encapsulate benchmark code, eliminating version conflicts between different libraries. A lightweight client with minimal dependencies communicates with benchmarks through a unified RPC interface, abstracting away the complexity of different container technologies. This design allows benchmarks to be added without modifying the optimization algorithm code, as the interface remains consistent regardless of the underlying benchmark implementation.

## Key Results
- Supports 80 benchmarks spanning continuous, categorical, and binary domains
- Includes 18 real-world applications for practical algorithm evaluation
- Eliminates dependency conflicts through container isolation
- Enables deployment on both local machines and HPC clusters

## Why This Works (Mechanism)
The framework's effectiveness stems from its architectural decoupling of benchmark execution from optimization logic. By running each benchmark in a separate container, Bencher ensures that conflicting dependencies between different benchmarks cannot interfere with each other. The RPC interface provides a standardized communication layer that abstracts the complexity of container management from both users and algorithm developers. This separation of concerns allows for independent development and maintenance of benchmarks while maintaining a consistent interface for optimization algorithms.

## Foundational Learning
- Containerization: Isolates benchmark environments to prevent dependency conflicts; quick check: verify different Python versions can run simultaneously
- RPC interface: Provides unified communication between client and benchmarks; quick check: test cross-container function calls
- Virtual environments: Maintains separate dependency trees for each benchmark; quick check: confirm isolated package installations
- Singularity containers: Enables HPC deployment without root privileges; quick check: validate container execution on cluster systems

## Architecture Onboarding
**Component Map:** Client -> RPC Interface -> Container Manager -> Benchmark Container -> Optimization Algorithm
**Critical Path:** Algorithm optimization request → RPC call → Container execution → Result return → Client processing
**Design Tradeoffs:** Container isolation vs performance overhead; RPC abstraction vs direct function call speed; unified interface vs specialized optimization
**Failure Signatures:** Container startup failures indicate dependency issues; RPC timeouts suggest network or resource constraints; benchmark execution errors point to implementation problems
**First Experiments:**
1. Run a simple continuous benchmark through the RPC interface
2. Add a new benchmark with conflicting dependencies to test isolation
3. Compare execution time with and without containerization overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Containerization may introduce performance overhead compared to direct function calls
- Requires system-level privileges for Docker/Singularity setup
- RPC interface latency could affect computationally intensive benchmarks
- Framework complexity increases with growing number of benchmarks

## Confidence
- Container-based dependency isolation: High confidence
- Modular benchmark integration capability: Medium confidence
- 80 benchmark support claim: High confidence
- Real-world application relevance: Medium confidence

## Next Checks
1. Benchmark the performance overhead introduced by RPC interface and containerization compared to direct function calls across different problem sizes
2. Test the framework's scalability by attempting to add 5-10 new benchmarks across different domains and document integration complexity
3. Verify reproducibility by running the same benchmarks across different container technologies (Docker vs Singularity) and hardware configurations to identify potential inconsistencies