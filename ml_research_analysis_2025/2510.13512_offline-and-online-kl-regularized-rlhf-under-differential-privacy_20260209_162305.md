---
ver: rpa2
title: Offline and Online KL-Regularized RLHF under Differential Privacy
arxiv_id: '2510.13512'
source_url: https://arxiv.org/abs/2510.13512
tags:
- rlhf
- policy
- function
- kl-regularized
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reinforcement learning from human feedback
  (RLHF) with KL-regularization under local differential privacy constraints on preference
  labels. The authors study both offline and online settings, where privacy is preserved
  through randomized response mechanisms on binary preference labels.
---

# Offline and Online KL-Regularized RLHF under Differential Privacy

## Quick Facts
- arXiv ID: 2510.13512
- Source URL: https://arxiv.org/abs/2510.13512
- Reference count: 40
- One-line primary result: First logarithmic regret bound for online KL-regularized RLHF with LDP, proving optimality of O(1/[(e^ε-1)²n]) suboptimality gap in offline setting

## Executive Summary
This paper addresses reinforcement learning from human feedback (RLHF) with KL-regularization under local differential privacy (LDP) constraints on preference labels. The authors study both offline and online settings, where privacy is preserved through randomized response mechanisms on binary preference labels. In the offline setting, they propose a pessimistic algorithm (PPKL-RLHF) that uses private maximum likelihood estimation with a bonus term to achieve suboptimality gap of O(1/[(e^ε-1)²n]) under single-policy concentrability, which they prove is optimal through matching lower bound. In the online setting, they design an optimistic algorithm (POKL-RLHF) that achieves logarithmic regret bound of O(d_F log(N_F·T)/(e^ε-1)²), where d_F is a variant of eluder dimension, T is total time steps, and N_F is the cardinality of reward function space.

## Method Summary
The method addresses KL-regularized RLHF under LDP by randomizing binary preference labels via the randomized response mechanism before training. For offline learning, PPKL-RLHF estimates rewards using private MLE with a pessimism bonus that subtracts uncertainty scaled by concentrability to handle distribution shift. For online learning, POKL-RLHF uses optimism-based exploration where an exploration policy selects actions proportional to uncertainty measured by a pair-eluder dimension variant. Both algorithms introduce an effective sample size penalty of (e^ε-1)² due to privacy preservation. The implementation uses Anthropic's hh-rlhf dataset with Llama-3.2-1B-Instruct backbone, training SFT baselines, privatizing preference labels with randomized response, training reward models, and optimizing policies via PPO with KL penalties.

## Key Results
- PPKL-RLHF achieves suboptimality gap of O(1/[(e^ε-1)²n]) under single-policy concentrability, matching the lower bound and proving optimality
- POKL-RLHF achieves logarithmic regret bound of O(d_F log(N_F·T)/(e^ε-1)²), the first such result for online KL-regularized RLHF
- Experiments show PPKL-RLHF achieves reasonable performance while preserving privacy, with win rates improving as ε increases
- Differential privacy introduces an effective sample size penalty of (e^ε-1)² in both settings

## Why This Works (Mechanism)

### Mechanism 1: Randomized Response Privacy Amplification
Binary preference labels y ∈ {-1, 1} are flipped with probability 1/(e^ε + 1) before transmission to the learner. The privatized probability becomes Pr(z|s,a₁,a₂) = α·σ(z·Δr*) + (1-α)·σ(-z·Δr*) where α = e^ε/(e^ε+1). This preserves the Bradley-Terry structure while adding calibrated noise. All bounds scale with (2α-1)² = [(e^ε-1)/(e^ε+1)]². If ε → 0 (maximum privacy), then α → 0.5 and (2α-1)² → 0, making bounds vacuous.

### Mechanism 2: Pessimistic Reward Estimation with Uncertainty Bonus
In offline RLHF, subtracting a carefully designed bonus term Γ_n(s,a) ∝ √(D²_F((s,a);π_ref) · log(N_F(τ)/δ)/n) from the MLE reward estimate provides conservative estimates that provably bound suboptimality under distribution shift. The pessimistic estimate r̂(s,a) = r̄(s,a) - Γ_n(s,a) ensures the learned policy doesn't overestimate reward in under-explored regions. The bonus Γ_n(s,a) captures reward uncertainty weighted by how well the reference policy covers each (s,a) pair.

### Mechanism 3: Optimism-Based Exploration via Pair Eluder Dimension
In online RLHF, an exploration policy that selects actions proportional to uncertainty (measured by a pair-eluder dimension variant) achieves logarithmic regret rather than sublinear √T regret. The exploration bonus b_t(s,a) = min{1, Γ_T · U_F(λ,s,a;D_t;π_t)} uses uncertainty quantification from the eluder dimension to guide exploration. The pair eluder dimension d_F measures how many "surprising" state-action pairs can exist before the reward function class is determined.

## Foundational Learning

- **Concept: Local Differential Privacy (LDP)**
  - Why needed here: Central to understanding why (e^ε-1)² appears throughout the bounds—it's the cost of privacy at data collection time rather than aggregation time.
  - Quick check question: Can you explain why the sample size penalty is (e^ε-1)² rather than just e^ε?

- **Concept: KL-Regularized Policy Optimization**
  - Why needed here: The Gibbs distribution π*(a|s) ∝ π_ref(a|s)·exp(β·r(s,a)) is the closed-form optimal policy; understanding this is essential for the pessimism/optimism principles to make sense.
  - Quick check question: Why does KL-regularization enable tighter bounds (1/n vs 1/√n) compared to unregularized RLHF?

- **Concept: Eluder Dimension**
  - Why needed here: This complexity measure determines exploration efficiency in the online setting; without it, you cannot interpret the regret bound or design the exploration bonus.
  - Quick check question: How does the "pair eluder dimension" differ from the standard eluder dimension used in bandits?

## Architecture Onboarding

- **Component map:**
  Raw Labels y → RR Mechanism: z=flip(y,p) → Private Dataset D̃ → Reward Class F → Private MLE/Least Squares → r̄ estimation → Offline: Pessimism: r̂ = r̄ - Γ → Gibbs Policy π̂; Online: Optimism: b_t from U_F → π²_t exploration policy

- **Critical path:**
  1. Correct implementation of RR with flip probability (1-α) = 1/(e^ε+1)
  2. Private MLE/least squares objective using privatized labels (eq. 8-9 for offline, step 7 in Algorithm 2 for online)
  3. Bonus computation using concentrability/eluder dimension (Γ_n for offline, U_F·Γ_T for online)
  4. Policy extraction via softmax with temperature β

- **Design tradeoffs:**
  - Privacy-utility: Lower ε (stronger privacy) → larger (e^ε-1)² penalty → need more samples or accept worse performance
  - Coverage vs exploration: Offline requires good π_ref coverage; online removes this but adds computational burden of optimism-based exploration
  - Assumption: The paper assumes finite/discretized F; for neural networks, τ-net approximations introduce additional τ error terms

- **Failure signatures:**
  - Win rate stuck near SFT baseline (0.53-0.55): ε too small (strong privacy) or β too large (over-regularization)
  - Regret doesn't decrease logarithmically in online setting: Eluder dimension computation incorrect, or function class misspecified
  - Value loss diverges: PPO hyperparameters need tuning; check clip range and learning rates

- **First 3 experiments:**
  1. **Offline validation on held-out preferences**: Implement PPKL-RLHF with ε ∈ {0.1, 0.5, 2.0}, measure win rate vs SFT baseline; expect monotonic improvement with ε (per Table 1)
  2. **Privacy-utility tradeoff curve**: Fix sample size n, vary ε ∈ [0.1, 10], plot suboptimality gap; should observe ~1/(e^ε-1)² scaling
  3. **Ablation on pessimism bonus**: Compare PPKL-RLHF with/without bonus term Γ_n on dataset with known distribution shift; expect bonus-free version to show reward over-optimization and worse held-out performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a computationally efficient method for online RLHF be developed that maintains logarithmic regret?
- Basis in paper: [explicit] Remark 5.3 states that while the current algorithm achieves logarithmic regret based on the eluder dimension, it is "computationally intractable when the function class is large."
- Why unresolved: Current guarantees are information-theoretic rather than computational, leaving the implementation of efficient algorithms for large function spaces unsolved.
- What evidence would resolve it: An algorithm that achieves logarithmic regret with polynomial time complexity relative to the function class size.

### Open Question 2
- Question: Can the theoretical analysis be extended to online f-regularized RLHF?
- Basis in paper: [explicit] Section 7 lists "online f-regularized RLHF" as a specific future research direction illuminated by this work.
- Why unresolved: The current theoretical framework and regret bounds are derived specifically for KL-regularization and may not hold for general f-divergences.
- What evidence would resolve it: Derivation of regret bounds or convergence rates for RLHF using general f-divergence regularizers.

### Open Question 3
- Question: How does the analysis change when modeling online KL-regularized RLHF as a Markov Decision Process (MDP)?
- Basis in paper: [explicit] Section 7 identifies "analyzing online KL-regularized RLHF from a Markov decision process perspective" as a future research direction.
- Why unresolved: The current paper adopts a contextual bandit view (prompt-response pairs), which ignores the sequential dependencies present in full MDPs.
- What evidence would resolve it: A theoretical analysis providing regret bounds for KL-regularized RLHF within a sequential decision-making (MDP) framework.

### Open Question 4
- Question: Can the performance gap between private offline RLHF (PPKL-RLHF) and non-private baselines be bridged?
- Basis in paper: [explicit] Section 6 notes that the win rate of PPKL-RLHF lags behind non-private DPO, stating "Achieving performance closer to the non-private DPO baseline remains an open direction."
- Why unresolved: Label privatization and pessimistic corrections inherently restrict the effective learning signal, degrading utility compared to non-private methods.
- What evidence would resolve it: A modified private offline algorithm achieving win rates statistically comparable to non-private DPO on standard RLHF benchmarks.

## Limitations

- Privacy parameter dependence creates a gap between theoretical pessimism and practical utility, as bounds become vacuous for ε < 0.69
- Neural network tractability is limited as D²_F and U_F are intractable for large models, with unspecified approximation methods
- Centralized privacy differs from the local DP model used, creating different privacy-utility tradeoffs than related work

## Confidence

- **High Confidence**: The pessimistic principle (PPKL-RLHF) and its suboptimality bound O(1/[(e^ε-1)²n]) are mathematically rigorous and the lower bound matching proves optimality. The randomized response mechanism is well-established in LDP literature.
- **Medium Confidence**: The online optimistic algorithm (POKL-RLHF) and its logarithmic regret bound O(d_F log(N_F·T)/(e^ε-1)²) are novel and theoretically sound, but the pair eluder dimension definition and its practical computation for neural networks introduce uncertainty.
- **Low Confidence**: The empirical evaluation's practical implementation details are sparse enough (optimizer choices, exact bonus computation for neural networks) that exact reproduction may yield different results, particularly for the pessimism bonus Γₙ(s,a).

## Next Checks

1. **Privacy-Utility Tradeoff Curve**: Fix n=38,821 (full dataset size), vary ε ∈ {0.1, 0.5, 1.0, 2.0, 5.0}, and plot suboptimality gap/held-out win rate vs ε. This should demonstrate the 1/(e^ε-1)² scaling predicted by theory and identify the ε threshold where privacy becomes too costly.

2. **Bonus Ablation Study**: Implement PPKL-RLHF with/without the pessimism bonus Γₙ(s,a) on a subset of the data with known distribution shift (e.g., filter for states where π₀ and π* likely differ). Measure reward over-optimization and held-out performance to validate the bonus's role in preventing reward model exploitation.

3. **Online Regret Validation**: Implement POKL-RLHF and track cumulative regret over time steps. Plot regret vs √T and log(T) baselines to empirically verify the logarithmic scaling, and analyze whether the pair eluder dimension computation (via τ-nets) is correctly implemented for the reward function class.