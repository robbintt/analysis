---
ver: rpa2
title: 'EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric
  Video Question Answering'
arxiv_id: '2508.10729'
source_url: https://arxiv.org/abs/2508.10729
tags:
- video
- question
- segment
- answer
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EgoCross, a new benchmark for evaluating
  cross-domain generalization of multimodal large language models (MLLMs) in egocentric
  video question answering (EgocentricQA). Existing benchmarks focus on common daily
  activities, but real-world applications require models to generalize to unfamiliar
  domains like surgery, industry, extreme sports, and animal perspective.
---

# EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering

## Quick Facts
- **arXiv ID:** 2508.10729
- **Source URL:** https://arxiv.org/abs/2508.10729
- **Reference count:** 40
- **Key outcome:** Most MLLMs struggle to generalize beyond daily-life egocentric domains, with accuracy dropping to 35-55% on specialized domains like surgery and extreme sports.

## Executive Summary
This paper introduces EgoCross, a benchmark designed to evaluate cross-domain generalization capabilities of multimodal large language models (MLLMs) in egocentric video question answering. While existing benchmarks focus on common daily activities, EgoCross addresses the critical need for models to handle unfamiliar domains including surgery, industry, extreme sports, and animal perspectives. The benchmark comprises approximately 1,000 QA pairs across 798 video clips, testing four key tasks: prediction, recognition, localization, and counting. Extensive experiments reveal that current MLLMs fail to generalize effectively beyond their training domains, with significant performance drops when moving from daily activities to specialized contexts. The authors propose reinforcement learning as a promising approach to improve cross-domain performance, showing average improvements of 22% across domains.

## Method Summary
EgoCross evaluates cross-domain egocentric video QA through a benchmark spanning four domains: surgery, industry, extreme sports, and animal perspective. The dataset includes 798 video clips with 957 QA pairs in both CloseQA (multiple-choice) and OpenQA (free-form) formats. Videos are sampled at 0.5-1 fps, and evaluation uses a combination of exact match and LLM-as-a-Judge (Qwen-Max) for semantic correctness scoring. The benchmark tests four QA tasks: prediction, recognition, localization, and counting. Pilot studies employ Qwen2.5-VL-7B with vLLM, using SFT (4 H100s, lr=1e-6, 12 epochs) and RL-GRPO (8 H100s, lr=1e-6, 16 epochs, beta=0.04) for domain adaptation. The dataset follows a 70/30 train/test split.

## Key Results
- Most existing MLLMs achieve below 55% accuracy on CloseQA and below 35% on OpenQA in cross-domain scenarios
- Performance drops 1.6× when comparing in-domain EgoSchema to EgoCross on the same question types
- RL shows the most significant improvement across all domains, with an average increase of 22% compared to baseline
- Industry domain shows RL boosting accuracy from 35.71% to 61.43%, while SFT only reaches 52.86%
- CloseQA scores are more stable than OpenQA, suggesting instruction-following challenges in free-form generation

## Why This Works (Mechanism)

### Mechanism 1: Visual-Semantic Domain Divergence
- **Claim:** Performance degradation correlates with the distance between training domains (daily activities) and target domains (surgery, industry)
- **Mechanism:** MLLMs learn priors specific to visual styles of common web data or daily egocentric datasets. When presented with unfamiliar domains, feature extractors fail to map pixel patterns to semantic concepts effectively
- **Core assumption:** Performance drop is primarily due to feature distribution shift rather than inherent complexity of reasoning tasks
- **Evidence anchors:** [abstract] "Most existing MLLMs... struggle to generalize to domains beyond daily life"; [Section 4.3] "1.6× drop in accuracy from in-domain EgoSchema to EgoCross"
- **Break condition:** If models trained exclusively on domain-specific data also fail on EgoCross, the mechanism shifts from domain shift to task complexity

### Mechanism 2: Reinforcement Learning for Cross-Domain Alignment
- **Claim:** RL improves cross-domain generalization more effectively than SFT by optimizing for outcome rewards rather than token imitation
- **Mechanism:** SFT forces models to mimic specific answers, causing overfitting to limited target domain data. RL allows models to explore reasoning paths that maximize reward signals, potentially finding robust heuristics that transfer better across visual gaps
- **Core assumption:** The reward model captures semantic correctness sufficiently to guide the policy away from hallucination
- **Evidence anchors:** [Section 4.4] "RL shows the most significant improvement across all domains (an average increase of 22%)"; [Table 3] RL boosts Industry accuracy from 35.71% to 61.43%
- **Break condition:** If RL training collapsed (reward hacking) or baseline comparison was unfair due to vLLM acceleration differences

### Mechanism 3: Metric Sensitivity to Instruction Following
- **Claim:** Gap between CloseQA and OpenQA performance reveals models possess knowledge but fail to follow strict output formatting instructions in novel contexts
- **Mechanism:** CloseQA constrains output space (A, B, C, D), helping models retrieve correct associations. OpenQA requires precise text generation without a crutch. In domain-shift scenarios, instruction-following capabilities degrade as cognitive load increases
- **Core assumption:** LLM-as-a-Judge used for OpenQA evaluation aligns perfectly with human semantic judgment
- **Evidence anchors:** [Section 4.2] "CloseQA scores tend to be more stable... while OpenQA is more sensitive"; [Section 8.1 / Supplementary] GPT-4.1 failed on Animal-Localization (0% accuracy) because it outputted frame indices instead of timestamps
- **Break condition:** If OpenQA prompts were significantly lower quality than CloseQA prompts, the gap would be an artifact of prompt engineering

## Foundational Learning

- **Concept: Domain Shift (Distribution Shift)**
  - **Why needed here:** EgoCross is explicitly designed to test this. A model trained on "cooking" sees different distribution of visual features and action verbs than one used in "surgery"
  - **Quick check question:** If a model achieves 90% on EgoSchema (Daily) but 40% on EgoCross (Surgery), is the model failing at reasoning or failing to recognize surgical tools? (Answer: Likely the latter, indicating a feature extraction gap)

- **Concept: Egocentric Video Understanding**
  - **Why needed here:** Unlike 3rd person video, egocentric video involves high camera motion (head movement), centered hands, and task-specific interactions
  - **Quick check question:** Why is "hand-object interaction" a critical sub-task in egocentric QA but less so in standard video QA? (Answer: Because the camera wearer's hands are usually the primary agents of change in the field of view)

- **Concept: GRPO (Generative Reward-based Policy Optimization)**
  - **Why needed here:** This is the specific RL technique cited as the most effective pilot solution
  - **Quick check question:** How does GRPO differ from standard Supervised Fine-Tuning? (Answer: SFT minimizes loss between predicted tokens and ground truth tokens; GRPO maximizes a reward signal based on quality of generated answer, allowing for more flexible reasoning paths)

## Architecture Onboarding

- **Component map:** Video Clip (Sampled at 0.5 fps) + Text Question -> Multimodal LLM (e.g., Qwen2.5-VL, GPT-4.1) -> JSON format `{prediction, reason}` -> Evaluator (Qwen-Max for OpenQA)
- **Critical path:** 1. Frame Sampling: 0.5 FPS extraction is the bottleneck for temporal precision. High-speed action may be undersampled here. 2. QA Generation Pipeline: Meta-annotation refinement -> LLM template expansion -> Human verification
- **Design tradeoffs:**
  - **0.5 FPS vs. High FPS:** Paper uses 0.5 fps for standardization and memory efficiency. This risks losing critical "frame-level" details for fast actions, potentially capping "Action Temporal Localization" accuracy
  - **RL vs. SFT:** RL provides better generalization (+22% avg) but is computationally expensive and requires robust reward model. SFT is faster but overfits to seen domains
- **Failure signatures:**
  - **Instruction Drift:** Model understands video but outputs frame indices ("4th image") instead of requested timestamps
  - **Domain Hallucination:** In specialized domains (Surgery), models revert to generic terms (e.g., "cutting tool") because they lack fine-grained vocabulary for specific instruments (e.g., "bipolar forceps")
  - **Metric Collapse:** Large gap between CloseQA and OpenQA indicates the model is guessing based on options rather than grounded understanding
- **First 3 experiments:**
  1. **Establish Baseline:** Run Qwen2.5-VL-7B on EgoCross CloseQA using provided 0.5 FPS sampling to reproduce ~44% accuracy baseline
  2. **Ablate Sampling Rate:** Re-evaluate "Extreme Sports" domain at 2 FPS. If accuracy for "Action Temporal Localization" increases significantly, the 0.5 FPS constraint is a limiting factor
  3. **Prompt Engineering Pilot:** Implement domain-context prompting strategy on "Surgery" domain. Compare results to zero-shot baseline to quantify value of explicit context injection

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark covers only four specialized domains, which may not fully represent real-world cross-domain scenarios
- LLM-as-a-judge evaluation methodology introduces potential bias and may not perfectly align with human judgment standards
- RL training implementation details are underspecified beyond hyperparameters, making it difficult to assess robustness of reported improvements

## Confidence
- **High Confidence:** The core finding that cross-domain generalization remains challenging for MLLMs is well-supported by consistent performance drops across multiple models and domains
- **Medium Confidence:** The efficacy of RL over SFT is supported by pilot study results, but small scale and lack of detailed implementation specifications reduce confidence
- **Low Confidence:** The hypothesis that CloseQA/OpenQA performance gaps primarily reflect instruction-following limitations rather than knowledge gaps is speculative

## Next Checks
1. **Replicate the 1.6× domain shift effect:** Test whether models achieving >90% accuracy on EgoSchema (Daily) consistently drop to <55% on EgoCross Surgery
2. **Ablate frame sampling rates:** Compare "Action Temporal Localization" accuracy on Extreme Sports at 0.5 fps versus 2 fps to quantify whether temporal undersampling artificially caps performance
3. **Independent RL implementation:** Reproduce the RL-GRPO training pipeline with detailed logging of reward curves and policy stability to verify that the 22% average improvement isn't due to hyperparameter tuning artifacts