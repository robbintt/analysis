---
ver: rpa2
title: A First-Principles Based Risk Assessment Framework and the IEEE P3396 Standard
arxiv_id: '2504.00091'
source_url: https://arxiv.org/abs/2504.00091
tags:
- information
- risk
- outputs
- risks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an information-centric risk assessment framework
  for generative AI, distinguishing outcome risks (harms from AI outputs) from process
  risks (how AI is built/operated). It categorizes AI-generated outputs into four
  types: perception-level (e.g., deepfakes), knowledge-level (factual content), decision/action
  plans, and control tokens.'
---

# A First-Principles Based Risk Assessment Framework and the IEEE P3396 Standard

## Quick Facts
- arXiv ID: 2504.00091
- Source URL: https://arxiv.org/abs/2504.00091
- Authors: Richard J. Tong; Marina Cortês; Jeanine A. DeFalco; Mark Underwood; Janusz Zalewski
- Reference count: 26
- Primary result: Information-centric risk assessment framework for generative AI, distinguishing outcome risks from process risks

## Executive Summary
This paper presents a first-principles based risk assessment framework for generative AI that distinguishes outcome risks (harms from AI outputs) from process risks (how AI is built/operated). The framework classifies AI-generated outputs into four fundamental categories—perception-level, knowledge-level, decision/action plans, and control tokens—and maps each to specific outcome risks and stakeholder responsibilities. By grounding risk analysis in information theory and cognitive science, the framework enables precise risk attribution and targeted safeguards. It aims to inform IEEE P3396 and broader AI governance with a principled approach to managing generative AI risks while enabling responsible innovation.

## Method Summary
The framework provides a theoretical classification system for generative AI outputs into four information types (perception-level, knowledge-level, decision/action, control tokens), each mapped to specific outcome risks and process risk factors. It establishes stakeholder responsibility matrices linking information types to accountable parties (developers, deployers, users, regulators). The method relies on qualitative analysis rather than quantitative metrics, using cognitive science principles (Marr's levels, Newell's knowledge level) and information theory (Shannon) as foundational concepts. No computational training or datasets are involved; the framework is conceptual and aims to guide risk assessment practices.

## Key Results
- Classifies AI outputs into four information types corresponding to cognitive processing stages
- Maps each information type to characteristic outcome risks and process risk factors
- Establishes stakeholder responsibility assignments based on information type and role
- Provides a first-principles foundation for IEEE P3396 standard development
- Enables systematic identification of harms and precise attribution of responsibility

## Why This Works (Mechanism)

### Mechanism 1: Information-Type to Risk-Category Mapping
The framework classifies outputs into four types (perception-level, knowledge-level, decision/action, control tokens), each corresponding to distinct cognitive processing stages. This enables systematic risk identification as each stage has characteristic harm patterns—perception outputs risk deception, knowledge outputs risk misinformation, decision outputs risk unsafe actions, and control tokens risk security breaches.

### Mechanism 2: Outcome Risk Priority via Process-to-Outcome Causation Chain
By focusing governance on outcome risks rather than process risks, the framework enables more precise harm prevention. Process risks are causally upstream of outcome risks but matter primarily insofar as they lead to actual harms. The framework traces each outcome risk back to its process risk factors, enabling targeted intervention at the most effective point.

### Mechanism 3: Responsibility Attribution by Stakeholder Role and Information Type
The framework maps responsibilities to stakeholder roles based on information type, closing accountability gaps. Each stakeholder has different leverage over different output types—developers control model architecture and training, deployers control usage context and constraints, users control whether to act on outputs, regulators set boundaries.

## Foundational Learning

- **Concept: Shannon Information Theory & Semantic Information (Dretske/Floridi)**
  - Why needed here: Grounds categorization in the nature of information itself—distinguishing raw signals from semantic content to propositional knowledge to operative commands
  - Quick check question: Can you explain the difference between information as a measurable quantity (Shannon) vs. information as semantic content that conveys knowledge?

- **Concept: Marr's Levels of Analysis & Newell's Knowledge Level**
  - Why needed here: The four information categories align with cognitive processing stages; understanding abstraction levels helps apply the framework independently of specific model architectures
  - Quick check question: What are Marr's three levels (computational, algorithmic, implementational), and at which level does this framework primarily operate?

- **Concept: Risk = Probability × Severity (Standard Risk Management)**
  - Why needed here: The framework adopts this as its first-principles definition; all risk analysis builds from identifying adverse events and their potential impact
  - Quick check question: For an AI that generates legal advice, what probability and severity factors would you estimate for the "misinformation" outcome risk?

## Architecture Onboarding

- **Component map:**
  AI System Output → Information Type Classifier → Outcome Risk Identifier → Process Risk Factor Tracer → Stakeholder Responsibility Mapper → Safeguard Selector

- **Critical path:**
  1. Classify: Determine which of the four information types the system produces
  2. Map risks: Identify outcome risks from Table II for each type
  3. Trace process factors: Identify which development/deployment decisions could lead to those outcomes
  4. Assign responsibilities: Use Table III to determine which stakeholder owns each mitigation
  5. Select safeguards: Choose interventions appropriate to information type

- **Design tradeoffs:**
  - Granularity vs. usability: Four categories are tractable but may miss edge cases; finer taxonomies increase precision but reduce practical adoptability
  - Outcome vs. process investment: Heavy process documentation vs. direct outcome testing
  - Universal vs. domain-specific: Standard categories enable cross-domain learning but may miss domain-unique risks

- **Failure signatures:**
  - Category ambiguity: Output doesn't fit cleanly into any type → risk of missed harm scenarios
  - Responsibility diffusion: Multiple stakeholders each claim the other should have acted → accountability gap persists
  - Process compliance without outcome improvement: All documentation complete, but harm rates unchanged

- **First 3 experiments:**
  1. Retroactive case analysis: Apply framework to 3-5 documented AI harm incidents to validate risk type identification and responsibility assignment
  2. Inter-rater reliability test: Have 3+ engineers independently classify outputs from a multimodal AI system; measure agreement on category assignment
  3. Coverage audit: For a specific AI application, enumerate all possible outputs and verify each maps to at least one category; flag any outputs that don't fit or span multiple categories ambiguously

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the four-category information taxonomy remain sufficient as AI systems evolve toward greater multimodality and autonomous agency?
- Basis in paper: [explicit] The authors state they will examine if new information categories emerge as AI technology evolves
- Why unresolved: Autonomous agents that independently execute multi-step plans may produce outputs that don't cleanly fit existing categories
- What evidence would resolve it: Empirical analysis of emerging autonomous AI systems using this framework to identify systematic categorization failures or gaps

### Open Question 2
- Question: How should risk attribution be allocated when a single AI output contains multiple information types?
- Basis in paper: [inferred] The paper acknowledges outputs could contain multiple types but provides no guidance for handling such cases
- Why unresolved: Responsibility allocation tables treat each information type separately, leaving unclear how stakeholders should prioritize competing responsibilities
- What evidence would resolve it: Case study analyses of multi-type outputs demonstrating whether the framework produces unambiguous responsibility assignments or generates conflicts

### Open Question 3
- Question: What quantitative metrics can validly measure outcome risks for each information type?
- Basis in paper: [inferred] Table III lists key risk metrics but these remain conceptual; no operationalized measurement methods are provided
- Why unresolved: The framework is principled but lacks empirical grounding in measurable outcomes
- What evidence would resolve it: Development and validation of standardized measurement instruments for each risk category with demonstrated reliability and predictive validity

### Open Question 4
- Question: Can the framework's outcome-focused approach effectively govern AI systems where process risks create harms that manifest only after deployment at scale?
- Basis in paper: [inferred] The paper argues outcome risks should be prioritized over process risks, but acknowledges process factors contribute to outcomes
- Why unresolved: Some harms may be detectable primarily through process audits rather than waiting for adverse outcomes, yet the framework de-emphasizes process
- What evidence would resolve it: Comparative analysis of outcome-only vs. combined outcome-process governance approaches across different AI application domains

## Limitations

- Framework's categorical classification introduces uncertainty when dealing with multimodal or hybrid outputs that combine multiple information types
- Absence of quantitative risk metrics or scoring thresholds means practical applicability depends heavily on user-defined scales
- Stakeholder responsibility mapping assumes clear role boundaries that may not exist in complex AI supply chains
- Focus on outcome risks may understate systemic process risks that don't manifest as discrete harm events but still require governance attention

## Confidence

**High Confidence:** The theoretical foundation drawing from Shannon information theory, cognitive science (Marr's levels, Newell's knowledge level), and standard risk management principles (risk = probability × severity)

**Medium Confidence:** The four-category classification system and its mapping to specific harm patterns, though limited empirical validation exists

**Low Confidence:** The responsibility attribution mechanism's effectiveness in real-world complex supply chains and the framework's practical utility without quantitative risk scoring or validation benchmarks

## Next Checks

1. **Cross-Framework Comparison:** Apply this framework alongside two other established AI risk assessment methodologies to the same set of AI harm cases and compare consistency of risk identification and stakeholder responsibility assignments

2. **Stakeholder Role Mapping Validation:** Survey 10+ AI practitioners across different organizational roles to identify actual responsibility boundaries in their work and compare against the framework's four-stakeholder model

3. **Ambiguity Resolution Protocol Development:** Create a test suite of 20-30 hybrid AI outputs that span multiple information categories and develop decision rules for classification in ambiguous cases, measuring inter-rater reliability