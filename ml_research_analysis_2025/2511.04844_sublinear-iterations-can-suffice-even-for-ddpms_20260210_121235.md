---
ver: rpa2
title: Sublinear iterations can suffice even for DDPMs
arxiv_id: '2511.04844'
source_url: https://arxiv.org/abs/2511.04844
tags:
- score
- diffusion
- which
- sampling
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes a randomized midpoint discretization (DDRaM)\
  \ for SDE-based diffusion models, showing it can achieve sublinear O(\u221Ad) sampling\
  \ complexity under mild smoothness assumptions. The key innovation is adapting the\
  \ \"shifted composition\" framework from log-concave sampling to handle the randomized\
  \ midpoint method, overcoming challenges posed by non-Markovian interpolations and\
  \ time-varying Lipschitz constants."
---

# Sublinear iterations can suffice even for DDPMs

## Quick Facts
- arXiv ID: 2511.04844
- Source URL: https://arxiv.org/abs/2511.04844
- Reference count: 40
- Primary result: DDRaM achieves O(√d) sampling complexity for diffusion models

## Executive Summary
This paper presents a randomized midpoint discretization method (DDRaM) for SDE-based diffusion models that achieves sublinear sampling complexity. By adapting the shifted composition framework from log-concave sampling to the diffusion model setting, the authors demonstrate that DDRaM can sample from high-dimensional distributions with only O(√d) iterations, where d is the data dimension. The method works across different diffusion processes (VP, VE, EDM) and shows both theoretical guarantees and practical improvements over standard solvers like Euler and Heun methods.

## Method Summary
DDRaM adapts the "shifted composition" framework from log-concave sampling to diffusion models by handling the randomized midpoint method's challenges. The key innovation involves managing non-Markovian interpolations and time-varying Lipschitz constants while maintaining the sublinear complexity guarantees. The method discretizes the reverse-time SDE using randomized midpoints and leverages the structure of trained diffusion models to achieve better complexity than traditional solvers. The approach is compatible with existing diffusion model architectures and requires no modifications to the underlying score networks.

## Key Results
- DDRaM achieves O(√d) sampling complexity under mild smoothness assumptions
- Outperforms standard Euler and Heun solvers in both FID and FDDINOv2 metrics
- Demonstrates robustness across VP, VE, and EDM diffusion processes
- Shows advantages for both deterministic and stochastic sampling regimes

## Why This Works (Mechanism)
DDRaM leverages the shifted composition framework by treating the diffusion sampling process as a sequence of carefully composed operators. The randomized midpoint discretization introduces controlled randomness that helps escape local structures while maintaining convergence guarantees. By adapting the time-varying Lipschitz analysis from log-concave sampling to the diffusion setting, the method can bound approximation errors more tightly than traditional deterministic discretizations.

## Foundational Learning

**Diffusion models & SDEs**: Why needed - Understanding the reverse-time SDE formulation is crucial for grasping DDRaM's discretization approach. Quick check - Can you explain how the forward and reverse SDEs relate in diffusion models?

**Shifted composition framework**: Why needed - This provides the theoretical foundation for achieving sublinear complexity. Quick check - What distinguishes shifted composition from standard composition methods?

**Polynomial decay of score functions**: Why needed - This property enables the O(√d) complexity bound. Quick check - How does polynomial decay differ from exponential decay in this context?

**Time-varying Lipschitz analysis**: Why needed - Essential for handling the non-stationary nature of diffusion processes. Quick check - Why can't standard log-concave analysis be directly applied to diffusion models?

## Architecture Onboarding

**Component map**: Score network -> Reverse SDE solver -> Sampling pipeline (DDRaM -> Image generation)

**Critical path**: DDRaM discretization operates directly on the output of pre-trained score networks, with the sampling complexity bottleneck occurring at the discretization stage rather than model inference.

**Design tradeoffs**: DDRaM trades increased per-iteration computation (randomized midpoints) for reduced total iteration count. This favors scenarios where fewer, more expensive steps are preferable to many cheap ones.

**Failure signatures**: If polynomial decay assumptions are violated, the O(√d) guarantee may not hold. Poor score network quality can amplify discretization errors beyond the theoretical bounds.

**3 first experiments**:
1. Compare FID scores of DDRaM vs Euler/Heun on standard CIFAR-10 diffusion models
2. Vary dimensionality (e.g., 64x64 to 256x256 images) to verify √d scaling
3. Test on unconditional vs conditional generation tasks to assess robustness

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical assumptions (polynomial decay, Lipschitz smoothness) may not hold for all trained models
- Complexity bounds are in total variation distance, which may not directly correlate with visual quality
- Proof technique introduces approximation errors that accumulate over iterations

## Confidence

**High confidence**: Mathematical framework for DDRaM and shifted composition connection is sound and rigorous

**Medium confidence**: Sublinear O(√d) complexity bound holds under stated assumptions and for tested diffusion processes

**Medium confidence**: Experimental improvements over baseline solvers are reproducible and meaningful

## Next Checks

1. Test DDRaM on larger-scale diffusion models (e.g., Stable Diffusion, DALL-E variants) to verify sublinear complexity scaling with dimensionality

2. Conduct ablation studies varying score network architecture and training objectives to assess robustness of polynomial decay assumption

3. Compare DDRaM against other advanced samplers (DDIM, DPM-Solver) on challenging conditional generation tasks to establish relative performance boundaries