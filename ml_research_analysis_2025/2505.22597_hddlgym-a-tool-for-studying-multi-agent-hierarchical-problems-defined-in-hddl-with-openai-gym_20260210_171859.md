---
ver: rpa2
title: 'HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in
  HDDL with OpenAI Gym'
arxiv_id: '2505.22597'
source_url: https://arxiv.org/abs/2505.22597
tags:
- agent
- planning
- hddlgym
- hddl
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HDDLGym bridges reinforcement learning and hierarchical planning
  by automatically generating OpenAI Gym environments from HDDL domains. The tool
  introduces an agent-centric extension of HDDL to support multi-agent collaboration,
  enabling seamless integration of hierarchical task networks with RL policies.
---

# HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym

## Quick Facts
- **arXiv ID:** 2505.22597
- **Source URL:** https://arxiv.org/abs/2505.22597
- **Reference count:** 1
- **Key outcome:** HDDLGym automatically generates OpenAI Gym environments from HDDL domains, enabling RL policies to guide hierarchical planning in multi-agent scenarios.

## Executive Summary
HDDLGym bridges reinforcement learning and hierarchical planning by converting HDDL domains into executable OpenAI Gym environments. The tool introduces an agent-centric extension to HDDL that explicitly models agent types and their action hierarchies, enabling multi-agent coordination. Using Transport and Overcooked domains, HDDLGym demonstrates effective planning with RL policies, achieving 80% success for two agents in Overcooked and 100% for single-agent Transport.

## Method Summary
HDDLGym parses HDDL domain and problem files into an environment dictionary during initialization, setting up the initial state, goal tasks, and agent configurations. The HDDLEnv class extends Gym's interface with standard reset() and step() functions that manage state transitions while preserving hierarchical task network structure. An agent-centric HDDL extension adds explicit agent type hierarchies and classifies primitive tasks as agent or environment actions. RL policies (default PPO) guide hierarchical decomposition by outputting probability distributions over lifted operators, which the HDDLGym Planner uses to build valid action hierarchies through iterative expansion and pruning.

## Key Results
- Overcooked domain: 80% success rate with 2 agents, 40% with 3 agents
- Transport domain: 100% success rate for 1 agent, drops to 0% with 3 agents
- Planning efficiency: Overcooked with 2 agents requires 0.374s planning time on average

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** HDDLGym converts HDDL hierarchical planning definitions into executable OpenAI Gym environments through automated parsing and state management.
- **Mechanism:** The system parses HDDL domain and problem files into an environment dictionary during initialization. This establishes the initial state (sI), goal tasks (tnI), and agent configurations. The HDDLEnv class extends Gym's interface, implementing standard reset() and step() functions that manage state transitions while preserving hierarchical task network structure.
- **Core assumption:** HDDL domains are well-formed and include complete action preconditions/effects; the paper notes domains require modification to work smoothly with HDDLGym (Section 4).
- **Evidence anchors:**
  - [abstract] "HDDLGym... automatically generates OpenAI Gym environments from HDDL domains and problems"
  - [Section 5.1] "the main parser function converts HDDL files into an environment dictionary, setting up the initial state and goals"
  - [corpus] Related work PDDLGym (Silver and Chitnis 2020) uses similar parsing approach but lacks hierarchical support; corpus confirms this is a distinct contribution.
- **Break condition:** Parsing fails if HDDL domain lacks explicit task effects or agent parameter specifications required by the agent-centric extension.

### Mechanism 2
- **Claim:** The agent-centric HDDL extension enables multi-agent coordination by explicitly modeling agent types and their action hierarchies within the planning formalism.
- **Mechanism:** The extension adds an agent type hierarchy (ta) to the standard HDDL domain tuple D = ⟨ta, L, TP, TC, M⟩. Primitive tasks (TP) are classified as agent actions (include agent parameters) or environment actions (execute automatically when preconditions met). Each agent maintains a belief B about other agents' hierarchies, enabling decentralized planning scenarios.
- **Core assumption:** Agents can be explicitly typed within the domain definition; not all IPC-HTN domains are agent-centric and thus not directly compatible (Section 6.1).
- **Evidence anchors:**
  - [Section 4] Definition 1 formally specifies the agent-centric planning domain with agent type hierarchy
  - [Section 5.2] Definition 2 defines Agent as tuple ⟨N, P, B, H, U⟩ with belief update function
  - [corpus] MA-PDDL and MA-HTN (cited in paper) explore multi-agent extensions but require different formalisms.
- **Break condition:** Multi-agent coordination fails if domain lacks collaborative operators (e.g., Transport Collab. requires explicit transfer-package action added to domain).

### Mechanism 3
- **Claim:** RL policies guide hierarchical decomposition by outputting probability distributions over lifted operators, which the planner uses to select valid action hierarchy expansions.
- **Mechanism:** The RL policy (default: PPO) takes observations (dynamic grounded predicates, goal tasks, action hierarchies) and outputs probabilities over lifted operators and objects. The HDDLGym Planner (Algorithm 1) iteratively builds action hierarchies by: (1) finding valid operators for each agent, (2) generating joint operator combinations, (3) pruning invalid combinations (conflicting preconditions/effects), and (4) selecting based on policy probabilities (deterministic or probabilistic mode).
- **Core assumption:** The observation space using dynamic grounded predicates sufficiently captures task-relevant state; this trades generalizability for scalability (Section 5.3).
- **Evidence anchors:**
  - [Section 5.5] Algorithm 1 details the centralized planner that uses policy P to guide hierarchy updates
  - [Section 6.3] Table 3 shows success rates: Transport 1-agent (100%), Overcooked 2-agent (80%), Overcooked 3-agent (40%)
  - [corpus] Hierarchical RL methods using macro-actions (Liu et al. 2017, Xiao et al. 2020) require custom environment modifications; HDDLGym automates this.
- **Break condition:** Policy guidance fails when grounded operator space becomes prohibitively large (Table 1: Overcooked 3-agent has 300,860 grounded operators), requiring lifted representations that may lose specificity.

## Foundational Learning

- **Concept: Hierarchical Task Networks (HTN) / HDDL**
  - **Why needed here:** HDDLGym's core purpose is bridging HTN-based hierarchical planning with RL. Users must understand how compound tasks decompose into primitive actions via methods.
  - **Quick check question:** Can you explain the difference between a task, a method, and a primitive action in HDDL, and how multiple methods can accomplish the same task?

- **Concept: OpenAI Gym Interface (reset, step, observation/action spaces)**
  - **Why needed here:** HDDLGym outputs standard Gym environments; understanding this interface is essential for integrating trained policies and interpreting environment feedback.
  - **Quick check question:** What does the step() function return, and how does HDDLGym extend this to handle environment actions that execute automatically?

- **Concept: Multi-Agent RL (centralized vs. decentralized planning, joint action spaces)**
  - **Why needed here:** HDDLGym supports both centralized and decentralized planning modes; users must understand how agent beliefs and joint operator combinations affect coordination.
  - **Quick check question:** In decentralized mode, how does an agent plan using only its own information and beliefs about other agents?

## Architecture Onboarding

- **Component map:** HDDL Domain/Problem Files -> Parser -> Environment Dictionary (state, goals, agents) -> HDDLEnv (Gym interface) -> Observation -> RL Policy -> Probability Distribution -> HDDLGym Planner (Algorithm 1) -> Updated Action Hierarchies -> Primitive Actions -> Environment Step -> New State + Rewards

- **Critical path:**
  1. Obtain or create valid HDDL domain/problem files (IPC-HTN or custom)
  2. Modify domain with agent-centric extension: add agent type, ensure actions have agent parameters, add task effects, include none action
  3. Initialize HDDLEnv with HDDL files and agent policies
  4. Train RL policy (PPO default) using the provided notebook
  5. Evaluate using success rate, planning time, and visualization tools

- **Design tradeoffs:**
  - **Observation space:** Dynamic grounded predicates reduce dimensionality (Table 1: Overcooked 90 vs. 937,158) but limit policy generalizability to new problem instances
  - **Action space:** Lifted operators with one-hot encoding reduce complexity but omit subtask ordering and specific object-operator links
  - **Scalability:** Success rates drop sharply with more agents (Table 3: Transport 3-agent = 0%, Overcooked 3-agent = 40%)

- **Failure signatures:**
  - 0% success rate during random exploration (Table 3) indicates domain is too complex for current HTN structure—consider simplifying methods or adding reward shaping
  - Policy fails to transfer when agent count, objects, or static conditions change due to fixed observation encoding
  - Planning timeout (NA in Table 3) suggests search space explosion—reduce problem size or prune HTN

- **First 3 experiments:**
  1. **Single-agent Transport domain:** Run the provided IPC-HTN Transport example with 1 agent to verify environment setup and observe 100% success rate baseline. Modify domain to add collaborative operators and compare planning time/steps.
  2. **Observation space ablation:** Compare policy performance using full grounded predicates vs. dynamic predicates only in the Overcooked domain. Measure generalization to unseen problem instances with different object configurations.
  3. **Multi-agent scaling test:** Train policies on Transport Collab. or Overcooked with 2 and 3 agents. Document success rate degradation and planning time increase. Test centralized vs. decentralized planning modes to identify coordination bottlenecks.

## Open Questions the Paper Calls Out

- **Question:** How can HDDLGym be extended to handle continuous or hybrid state and action spaces?
- **Basis in paper:** [explicit] "HDDLGym currently operates under certain limitations... First, it can only handle discrete state and action spaces, which restricts its application to scenarios that require continuous or hybrid spaces."
- **Why unresolved:** The current architecture relies on discrete grounded predicates and lifted operators, which do not translate directly to continuous domains.
- **What evidence would resolve it:** A modified framework demonstrating successful RL policy training in a continuous domain (e.g., robotics control) with comparable success rates to discrete benchmarks.

- **Question:** How can probabilistic or stochastic transition functions be incorporated while maintaining hierarchical planning structure?
- **Basis in paper:** [explicit] "HDDLGym assumes a deterministic transition function, meaning that action effects are predictable and do not account for probabilistic outcomes. This limits its applicability to environments where uncertainty and stochastic outcomes are common."
- **Why unresolved:** The current planner validates operators based on deterministic preconditions and effects; probabilistic outcomes would require fundamentally different planning algorithms.
- **What evidence would resolve it:** Successful training and deployment in a domain with known stochastic dynamics (e.g., stochastic Overcooked) showing robust performance under uncertainty.

- **Question:** How can RL policy generalizability be improved across HDDL problem instances with varying agents, objects, or static conditions?
- **Basis in paper:** [explicit] "This design choice may limit the generalizability of the RL policy, as it is tailored to a specific set of HDDL problem instances and may not transfer well to problems with different agents, objects, and/or static world conditions."
- **Why unresolved:** One-hot encoding over dynamic predicates ties policies to specific problem configurations; no mechanism currently exists for parameter sharing or transfer.
- **What evidence would resolve it:** A single trained policy achieving >70% success rate across multiple Transport problem variants with different numbers of packages, locations, and vehicles.

- **Question:** What modifications are needed to support competitive multi-agent interactions, agent privacy, and distributed context information?
- **Basis in paper:** [explicit] "This capability would enhance HDDLGym's ability to manage complex multi-agent dynamics beyond simple collaboration, supporting scenarios with competition, agent privacy, and distributed context information."
- **Why unresolved:** Current agent-centric extension assumes collaborative goal structures; competitive scenarios require conflicting objectives and private belief states.
- **What evidence would resolve it:** Demonstration of a zero-sum or mixed-motive game (e.g., modified Overcooked with competing teams) where agents successfully learn non-cooperative strategies.

## Limitations

- Policy generalizability is limited to specific problem instances due to fixed observation encoding over dynamic predicates
- Success rates drop sharply with more agents, indicating scalability issues (Transport 3-agent = 0%, Overcooked 3-agent = 40%)
- Only discrete state and action spaces are supported, restricting application to continuous or hybrid domains

## Confidence

- **High:** Automated conversion of HDDL domains to Gym environments via parsing; core agent-centric extension mechanism
- **Medium:** Success rate comparisons across domains and agent counts; relative performance differences are consistent
- **Low:** Generalization of learned policies to unseen problem instances; exact reproduction of training curves without specified hyperparameters

## Next Checks

1. Replicate Transport domain experiments with 1-3 agents, measuring success rate, planning time, and steps to isolate scalability bottlenecks
2. Conduct observation space ablation study (full grounded predicates vs. dynamic only) in Overcooked to quantify generalizability loss
3. Test decentralized planning mode in Transport Collab. to evaluate belief propagation and coordination under partial observability