---
ver: rpa2
title: 'Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates
  High-Quality Retinex Enhancement'
arxiv_id: '2512.08982'
source_url: https://arxiv.org/abs/2512.08982
tags:
- enhancement
- image
- low-light
- consistency
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Consist-Retinex, the first framework applying
  consistency models to Retinex-based low-light image enhancement. The key innovation
  addresses the fundamental mismatch between standard unconditional consistency training
  and conditional enhancement requirements by introducing dual-objective consistency
  loss and noise-emphasized adaptive sampling.
---

# Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement

## Quick Facts
- arXiv ID: 2512.08982
- Source URL: https://arxiv.org/abs/2512.08982
- Reference count: 40
- Introduces Consist-Retinex framework applying consistency models to Retinex-based low-light image enhancement

## Executive Summary
This paper introduces Consist-Retinex, the first framework applying consistency models to Retinex-based low-light image enhancement. The key innovation addresses the fundamental mismatch between standard unconditional consistency training and conditional enhancement requirements by introducing dual-objective consistency loss and noise-emphasized adaptive sampling. The dual-objective loss combines temporal consistency with ground-truth alignment, while the sampling strategy prioritizes large-noise regions critical for one-step conditional inference. On the VE-LOL-L dataset, Consist-Retinex achieves state-of-the-art performance with single-step sampling (PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59 compared to Diff-Retinex++), while requiring only 1/8 of the training budget relative to the 1000-step Diff-Retinex baseline. The framework enables real-time processing at ~27 FPS for 400×600 images while maintaining superior quality metrics across paired and unpaired datasets.

## Method Summary
Consist-Retinex applies consistency models to Retinex decomposition by training separate models for reflectance and illumination components. The framework uses a dual-objective loss combining temporal consistency with ground-truth alignment, trained with a noise-emphasized sampling strategy that prioritizes large-noise regions. Input images are first decomposed into reflectance and illumination via TDN, then processed by respective consistency models. The framework achieves one-step inference by mapping degraded inputs at high noise levels directly to enhanced outputs, avoiding the iterative sampling required by diffusion models.

## Key Results
- Achieves state-of-the-art PSNR of 25.51 on VE-LOL-L dataset (vs. 23.41 for Diff-Retinex++)
- Requires only 1/8 of training budget compared to 1000-step Diff-Retinex baseline
- Enables real-time processing at ~27 FPS for 400×600 images
- Demonstrates superior performance across paired (VE-LOL-L, LOL) and unpaired (DICM, VV) datasets

## Why This Works (Mechanism)

### Mechanism 1: Noise-emphasized sampling enables stable one-step conditional inference
Standard consistency training uses log-uniform sampling that allocates ~95% of iterations near σ≈0 (data manifold). Conditional inference requires mapping from σ_max where degraded input integrates with target distribution. The bimodal sampling (95% on σ∈[76,80], 5% full-spectrum) ensures the critical high-noise regime receives sufficient supervision.

### Mechanism 2: Dual-objective loss provides necessary supervision signals
L_consist enforces self-consistency along trajectories, while L_fixed anchors predictions directly to ground-truth at inference noise levels. The combination uses different sampling per term—standard sampling for consistency, noise-emphasized for alignment. GT alignment alone improves PSNR by 9.48 dB over baseline on LOL.

### Mechanism 3: Separate models for reflectance and illumination exploit Retinex decomposition
Two specialized models f_θ^R and f_θ^L operate on decomposed components with different architectures (45M params for R, 12M for L). The low-light image conditions both models; reconstruction via element-wise multiplication.

## Foundational Learning

- **Consistency Models (Self-Consistency Property)**
  - Why needed here: Core mechanism enabling one-step generation by enforcing f_θ(x_t, t) = f_θ(x_t', t') on same trajectory.
  - Quick check question: Can you explain why consistency models map any point on an ODE trajectory back to the origin in one step?

- **Retinex Theory (Image Decomposition)**
  - Why needed here: Framework decomposes I = R ⊙ L into reflectance (intrinsic scene) and illumination (lighting conditions).
  - Quick check question: Given a low-light image, which component should retain texture detail vs. global brightness?

- **EDM Preconditioning and Noise Schedule**
  - Why needed here: Paper uses EDM preconditioning with σ∈[0.002, 80]; understanding c_skip(t) and c_out(t) is essential for architecture implementation.
  - Quick check question: Why does c_skip(t) approach 1 as t→0 and 0 as t→σ_max?

## Architecture Onboarding

- **Component map:** TDN (retinex decomposition) -> f_θ^R (reflectance model) + f_θ^L (illumination model) -> Reconstruction (R̂ ⊙ L̂)

- **Critical path:**
  1. Concatenate conditioning: Input = [I_l, c_in(σ)·(x_0 + σε)]
  2. Time embedding: Sinusoidal Fourier (256 bands) -> MLP -> Adaptive GroupNorm (γ, β)
  3. Forward pass through respective model
  4. One-step inference at σ_max: no iteration

- **Design tradeoffs:**
  - Asymmetric architectures (R: 45M, L: 12M) based on task complexity assumption—reflectance requires more capacity for texture.
  - λ_fixed=0.3 vs λ_consist=1.0: ground-truth alignment is auxiliary, not primary driver.
  - Assumption: 200K iterations sufficient vs. 800K for diffusion baselines.

- **Failure signatures:**
  - Color shifts in enhanced output -> check illumination model convergence
  - Lost texture detail -> verify reflectance model receiving sufficient high-noise training
  - Training instability at early iterations -> examine EMA decay schedule µ(k)

- **First 3 experiments:**
  1. Ablate noise-emphasized sampling alone: train with log-uniform only, measure PSNR gap on VE-LOL-L.
  2. Vary τ threshold (currently 0.95): test 0.90, 0.92, 0.98 to validate high-noise concentration hypothesis.
  3. Single-model baseline: train one consistency model on I directly (no Retinex decomposition) to isolate decomposition benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-step refinement strategies further improve Consist-Retinex's quality while maintaining practical efficiency?
- Basis in paper: [explicit] "Future work may explore... investigating multi-step refinement strategies"
- Why unresolved: The paper focuses exclusively on one-step inference, demonstrating consistency models' capability for single-step conditional generation, but does not explore whether iterative refinement at inference time could improve quality metrics (PSNR, LPIPS) beyond current state-of-the-art.

### Open Question 2
- Question: Does the dual-objective consistency training framework generalize to other conditional image restoration tasks beyond low-light enhancement?
- Basis in paper: [explicit] "Future work may explore extending this framework to other conditional image restoration tasks"
- Why unresolved: The noise-emphasized sampling and ground-truth alignment loss were designed specifically for Retinex decomposition where degraded inputs bridge to enhanced outputs at high noise levels. Other restoration tasks (deblurring, super-resolution, denoising) may have different optimal noise-level operating points.

### Open Question 3
- Question: How should task-adaptive sampling schedules be designed for various degradation patterns in low-light images?
- Basis in paper: [explicit] "Future work may explore... developing task-adaptive sampling schedules for various degradation patterns"
- Why unresolved: The current bimodal sampling strategy (95% at σ > 0.95σ_max, 5% full-spectrum) uses fixed hyperparameters. Different degradation types (extreme darkness, color casts, noise-dominated scenarios) may require different emphasis ratios or threshold values (τ).

### Open Question 4
- Question: Why does Consist-Retinex underperform on no-reference metrics (NIQE) compared to PI on unpaired datasets, and can this gap be closed?
- Basis in paper: [inferred] On DICM, Consist-Retinex achieves best PI (2.932) but NIQE (3.826) lags behind Diff-Retinex++ (3.514); similar pattern on VV dataset
- Why unresolved: The paper attributes LOL vs. VE-LOL-L differences to dataset characteristics but does not explain the PI-NIQE divergence on unpaired data. This suggests potential domain shift issues or overfitting to paired training characteristics that affect naturalness statistics differently than perceptual quality.

## Limitations

- Noise-emphasized sampling assumption remains weakly validated despite strong theoretical argument
- Dual-objective loss design assumes temporal consistency alone cannot converge, but longer training might suffice
- Asymmetric architecture assumptions (45M vs 12M params) haven't been rigorously tested
- PI-NIQE divergence on unpaired datasets suggests potential domain shift issues

## Confidence

- **High confidence**: Dual-objective loss improves performance over temporal consistency alone (strong quantitative evidence)
- **Medium confidence**: Noise-emphasized sampling is necessary for one-step inference (strong theoretical argument, weak empirical validation)
- **Low confidence**: Retinex decomposition provides fundamental advantage over direct enhancement (no ablation against non-decomposed baseline)

## Next Checks

1. Ablate noise-emphasized sampling alone: train with log-uniform only, measure PSNR gap on VE-LOL-L.
2. Vary τ threshold (currently 0.95): test 0.90, 0.92, 0.98 to validate high-noise concentration hypothesis.
3. Single-model baseline: train one consistency model on I directly (no Retinex decomposition) to isolate decomposition benefit.