---
ver: rpa2
title: 'Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind
  Benchmark for Large Language Models'
arxiv_id: '2506.00334'
source_url: https://arxiv.org/abs/2506.00334
tags:
- reasoning
- emotion
- appraisal
- llms
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) reason
  about others' emotions using cognitive appraisal theory. The authors create a novel
  evaluation dataset with vignettes systematically manipulated according to cognitive
  appraisal dimensions, focusing on the Prisoner's Dilemma game show scenario.
---

# Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2506.00334
- Source URL: https://arxiv.org/abs/2506.00334
- Reference count: 9
- Key outcome: LLMs predominantly use System 1-like strategies, relying on heuristic associations rather than abstract reasoning for emotion inference tasks.

## Executive Summary
This paper investigates how Large Language Models (LLMs) reason about others' emotions using cognitive appraisal theory. The authors create a novel evaluation dataset with vignettes systematically manipulated according to cognitive appraisal dimensions, focusing on the Prisoner's Dilemma game show scenario. They assess both forward reasoning (from context to emotion) and backward reasoning (from emotion to inferred context) tasks. Results show that while LLMs can reason about emotions to some extent, they struggle to accurately associate situational outcomes and appraisals with specific emotions. Gemma 7b achieved the highest accuracy at 57.9% on the emotion classification task. The study reveals that LLMs predominantly use System 1-like strategies, relying on heuristic associations rather than abstract reasoning.

## Method Summary
The study employs a cognitive appraisal-based evaluation framework using systematically manipulated Prisoner's Dilemma vignettes. Two tasks are assessed: forward reasoning (predicting emotion from context) and backward reasoning (inferring context from emotion). The dataset includes 432 forward reasoning vignettes across three scenarios (game show, business deal, relationship) and 150 backward reasoning prompts. Four LLM architectures (Mistral-7B-Instruct-v0.3, Llama-3.1-8b-Instruct, gemma-7b-Instruct, o3-mini) are evaluated using zero-shot inference. Results are analyzed through classification accuracy, ANOVA on appraisal ratings, and emotion-outcome association heatmaps.

## Key Results
- Gemma 7b achieved the highest accuracy at 57.9% on the forward emotion classification task
- LLMs predominantly use System 1-like strategies, relying on heuristic associations rather than abstract reasoning
- Models struggle with backward reasoning, failing to accurately associate situational outcomes and appraisals with specific emotions
- Unequal processing of appraisal dimensions, with heavy weighting on fairness while largely neglecting accountability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs predominantly employ **heuristic-based, System 1-like processing** for emotion reasoning, using fast, associative pattern-matching (e.g., "unfair" → "anger") rather than slow, deliberative, context-integrating System 2 reasoning.
- Mechanism: The models rely on strong statistical correlations learned during pre-training between surface-level linguistic cues (like the word "unfair") and specific emotion labels. This allows them to bypass the more computationally difficult task of integrating multiple, nuanced contextual factors as predicted by Cognitive Appraisal Theory.
- Core assumption: The paper assumes that LLMs have learned these direct associations from their training data and that they lack an internal, generative model of emotion causality that would allow for more complex, compositional reasoning.
- Evidence anchors:
  - [abstract] The abstract states that LLMs "predominantly use System 1-like strategies, relying on heuristic associations rather than abstract reasoning."
  - [section 4] The discussion confirms this, noting that "anger ratings were consistently elevated in scenarios with perceived unfairness, regardless of outcome—a heuristic shortcut that bypasses the nuanced integration of contextual factors indicative of System 2 engagement."
  - [corpus] The related paper "Do Machines Think Emotionally?" is listed as a neighbor, suggesting this is an active area of investigation into the nature of LLM emotional processing.

### Mechanism 2
- Claim: LLMs possess **asymmetric reasoning abilities**, showing moderate success in forward reasoning (context → emotion) but significant difficulty with backward reasoning (emotion → context).
- Mechanism: Forward reasoning is partially successful because the model can map the provided context keywords to a likely emotion label. Backward reasoning requires a form of inverse inference—starting with an emotion and deducing the specific antecedent conditions—that these models are not inherently designed to perform. They lack a robust causal model that can be run in reverse.
- Core assumption: It is assumed that the models' understanding of emotions is primarily associative and forward-directed. The failure in backward reasoning suggests they do not possess a bidirectional, generative model linking emotions to their specific causes.
- Evidence anchors:
  - [abstract] The study assessed both tasks and found LLMs are "poor at associating situational outcomes and appraisals with specific emotions."
  - [section 3.2.1] The results show that in the backward reasoning task, "LLMs associate both anger and disappointment with the 'Co-De' outcome," failing to make the fine-grained distinctions required.
  - [corpus] The paper "Mechanistic Interpretability of Emotion Inference in Large Language Models" is a neighbor; its focus on "mechanistic interpretability" is the kind of approach needed to understand the internal failure of this backward inference.

### Mechanism 3
- Claim: LLMs process appraisal dimensions with **unequal priority**, heavily weighting "fairness" while largely neglecting "accountability."
- Mechanism: The internal representations or attention mechanisms of the tested LLMs have a stronger learned association between the concept of fairness (and its linguistic markers) and emotion outcomes. The concept of accountability, and its linguistic markers, appears to be a weaker signal that is not reliably integrated into the emotion reasoning process.
- Core assumption: The assumption is that this imbalance reflects the statistical distribution and co-occurrence patterns in the models' training data, where "fairness" may be a more prominent predictor of emotion than "accountability."
- Evidence anchors:
  - [section 3.1] The ANOVA results showed a statistically significant two-way interaction between outcome and fairness, but the three-way interaction with accountability was not significant, indicating accountability's minimal role.
  - [section C] The results explicitly state that "LLMs do not appear to utilize this appraisal information when reasoning about emotions," as anger ratings did not change based on accountability manipulations.
  - [corpus] No direct corpus evidence was found to support this specific imbalance; this finding appears to be a unique contribution of this paper.

## Foundational Learning

- **Cognitive Appraisal Theory**
  - Why needed here: This is the theoretical bedrock of the entire paper. The evaluation dataset is constructed by systematically manipulating vignettes along appraisal dimensions defined by this theory. Understanding that emotions are caused by an individual's *subjective evaluation* of a situation, not the situation itself, is crucial for interpreting why the LLMs' heuristic approach is insufficient.
  - Quick check question: According to Cognitive Appraisal Theory, what are the three key appraisal dimensions manipulated in the study's vignettes, and why is the theory's focus on *subjective evaluation* important for creating a valid benchmark?

- **Theory of Mind (ToM)**
  - Why needed here: The paper frames its investigation as a ToM benchmark. The task of inferring another agent's mental state—specifically their emotional state—based on context is a core test of social-cognitive reasoning. Understanding this framing is key to grasping the paper's broader goals beyond simple text classification.
  - Quick check question: How does the "forward reasoning" task in this paper serve as a test for an LLM's Theory of Mind, and what does it imply if an LLM can perform this task?

- **Dual-Process Theory (System 1 vs. System 2)**
  - Why needed here: The authors use this psychological framework to classify and explain the performance of the LLMs. They argue that the models' reliance on heuristics is evidence of "System 1" processing, while the more complex, context-sensitive reasoning required for the task would be "System 2." This distinction is central to their analysis.
  - Quick check question: Based on the paper's findings, how can you tell if an LLM is using "System 1" vs. "System 2" reasoning? Provide an example from the results that illustrates System 1 thinking.

## Architecture Onboarding

- **Component map**: Cognitive Appraisal-Based Vignette Generator -> Target LLM -> Evaluation Module
- **Critical path**: The critical evaluation path is: 1) Generate a vignette with specific manipulations (e.g., Co-De outcome, high unfairness, other-accountability). 2) Prompt the target LLM. 3) The LLM must avoid a simple "unfair → anger" heuristic (System 1) and instead integrate all three manipulated dimensions to produce the correct emotion label. 4) The evaluation module measures success based on alignment with appraisal theory predictions.
- **Design tradeoffs**: The primary tradeoff is between **experimental control and ecological validity**. The authors chose a constrained "Prisoner's Dilemma" paradigm to systematically manipulate variables, sacrificing the complexity of real-world, naturalistic conversations. A second tradeoff is using **theory-derived ground truth** over human-provided labels, which strengthens theoretical alignment but may miss nuanced human judgments.
- **Failure signatures**:
  - **Heuristic Override:** The model's output is determined almost entirely by a single strong keyword (e.g., "unfair") and ignores other variables like the game outcome. This is a sign of System 1 dominance.
  - **Backward Reasoning Conflation:** When prompted to infer a situation from an emotion, the model assigns multiple, distinct emotions (like "anger" and "disappointment") to the same outcome, failing to use appraisal dimensions to differentiate them.
  - **Dimensional Neglect:** The model's predictions show no statistically significant change when the "accountability" dimension is varied, indicating this signal is being lost or ignored by the architecture.
- **First 3 experiments**:
  1. **Reproduce the System 1 diagnostic:** Create a modified vignette where a situation is described as "unfair" but the outcome is positive (e.g., the agent wins). A System 1-reliant model will still predict high anger due to the "unfair" cue, while a more nuanced model would integrate the positive outcome.
  2. **Isolate the neglected dimension:** Generate a series of vignettes that are identical in every way except for the accountability manipulation (self vs. other). Measure if there is any shift in the predicted emotion from, for example, "guilt" to "anger." A lack of shift confirms this architectural weakness.
  3. **Test backward reasoning causality:** Provide the model with an emotion label (e.g., "guilt") and ask it to generate the most likely preceding vignette. Analyze the generated text to see if it correctly includes the necessary appraisal cues (e.g., self-accountability, goal obstructiveness). This tests if the model has a generative, bidirectional understanding of the emotion model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific training protocols or architectures shift LLMs from heuristic-based (System 1) to abstract, context-sensitive (System 2) emotion reasoning?
- Basis in paper: [explicit] The authors state that "future research may benefit from designing training protocols that encourage more abstract, rule-based inference mechanisms."
- Why unresolved: The study found that current models predominantly rely on pattern-matching heuristics (e.g., associating unfairness directly with anger) rather than integrating nuanced contextual factors.
- What evidence would resolve it: Demonstrating that models fine-tuned on explicit cognitive appraisal rules can outperform standard models in integrating multiple conflicting contextual cues.

### Open Question 2
- Question: How does LLM emotion reasoning performance generalize to unstructured, naturalistic conversations beyond controlled vignettes?
- Basis in paper: [explicit] The authors note that "real-world conversations and social interactions are often more complex" and suggest future work explore "emotion reasoning in more naturalistic conversations."
- Why unresolved: The study relied on systematically manipulated Prisoner's Dilemma vignettes, which lack the ambiguity and noise of real-world text.
- What evidence would resolve it: Evaluating these models on datasets of organic social interactions (e.g., Reddit threads or spoken dialogue) annotated for cognitive appraisals.

### Open Question 3
- Question: How does incorporating a broader range of cognitive appraisal dimensions affect the accuracy of LLM emotion reasoning?
- Basis in paper: [explicit] The authors acknowledge that a "comprehensive investigation... may require considering a broader range of appraisal dimensions" beyond the three tested (fairness, accountability, goal conduciveness).
- Why unresolved: It is unclear if models fail on specific emotions because key appraisal dimensions (e.g., novelty or control) were omitted from the experimental design.
- What evidence would resolve it: A follow-up study systematically adding dimensions like novelty or certainty to the vignettes to measure performance changes on complex emotions.

## Limitations

- **Dataset Representativeness**: The study's use of controlled Prisoner's Dilemma vignettes may not fully capture the complexity and nuance of real-world emotional reasoning scenarios.
- **Ground Truth Validity**: The paper relies on theory-derived ground truth rather than human judgments for emotion-appraisal mappings, potentially creating a mismatch with human-like reasoning.
- **Model Generalization**: Results are based on four specific LLM architectures tested under zero-shot conditions and may not generalize to other model families or sizes.

## Confidence

- **High Confidence**: The observation that LLMs predominantly use heuristic-based, System 1-like processing for emotion reasoning
- **Medium Confidence**: The finding that LLMs struggle with backward reasoning (emotion to context)
- **Medium Confidence**: The claim about unequal priority processing of appraisal dimensions (heavily weighting fairness while neglecting accountability)

## Next Checks

1. **Real-World Scenario Validation**: Test the same models on naturalistic conversational datasets where emotional reasoning occurs in context, comparing performance against the controlled vignette results to assess ecological validity.

2. **Human Baseline Comparison**: Conduct human participant studies using identical vignettes to establish human performance benchmarks, then compare LLM performance against human reasoning patterns to determine if the theory-grounded ground truth aligns with actual human emotion reasoning.

3. **Cross-Model Architecture Analysis**: Test additional LLM architectures (including those specifically trained for emotional intelligence) and conduct ablation studies to determine whether the observed System 1 dominance is a fundamental limitation of current architectures or can be mitigated through specific training approaches.