---
ver: rpa2
title: Towards an Evaluation Framework for Explainable Artificial Intelligence Systems
  for Health and Well-being
arxiv_id: '2504.08552'
source_url: https://arxiv.org/abs/2504.08552
tags:
- systems
- system
- https
- framework
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XAIHealth, a structured evaluation framework
  designed for explainable AI (XAI) systems in healthcare and well-being applications.
  The framework adapts a multidisciplinary base approach to focus on sequential phases
  of machine-centred analysis, human-centred assessment, and legal-ethical compliance.
---

# Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being

## Quick Facts
- arXiv ID: 2504.08552
- Source URL: https://arxiv.org/abs/2504.08552
- Authors: Esperança Amengual-Alcover; Antoni Jaume-i-Capó; Miquel Miró-Nicolau; Gabriel Moyà-Alcover; Antonia Paniza-Fullera
- Reference count: 19
- One-line primary result: Introduces XAIHealth, a 3-phase evaluation framework for XAI systems in healthcare, validated on pneumonia detection with high robustness (LLE=0.082) but low user trust (F1=0.0952).

## Executive Summary
This paper presents XAIHealth, a structured evaluation framework designed specifically for explainable AI systems in healthcare and well-being applications. The framework adapts a multidisciplinary approach to focus on three sequential phases: pre-evaluation (legal/ethical/data compliance), machine-centred analysis (robustness and fidelity), and human-centred assessment (trust and usability). The framework aims to ensure AI systems are not only technically sound but also interpretable, trustworthy, and compliant with regulations like GDPR and the EU AI Act.

The framework is demonstrated through a case study applying GradCAM explanations to a ResNet18 model for COVID-19 pneumonia detection from chest X-rays. While the system achieved acceptable robustness metrics (Local Lipschitz Estimate of 0.082), it failed to achieve adequate user trust scores (F1-score of 0.0952), highlighting the critical gap between technical performance and human acceptance in medical AI systems.

## Method Summary
The XAIHealth framework evaluates XAI systems through three sequential phases. Phase 0 (Pre-Evaluation) verifies legal/ethical compliance and data quality, including GDPR anonymization and bias checks. Phase 1 (Machine-Centred) assesses explanation robustness using Local Lipschitz Estimate (LLE) metrics, with a threshold of approximately 0.1. Phase 2 (Human-Centred) evaluates user trust through behavioral metrics using confusion matrices that measure reliance decisions against model correctness. The framework was applied to a ResNet18 CNN trained on 2048 chest X-ray images from Hospital Universitari Son Espases, using GradCAM for explanations. The evaluation measured machine-centred robustness (LLE=0.082) and human-centred trust metrics (Precision=0.0625, Recall=0.2022, F1=0.0952).

## Key Results
- Successfully achieved machine-centred robustness with mean Local Lipschitz Estimate of 0.082 (below the 0.1 threshold).
- Failed to achieve adequate human-centred trust, with F1-score of 0.0952 indicating poor alignment between user decisions and model correctness.
- Demonstrated the framework's ability to identify phase-specific failures, requiring return to Phase 0 for interface redesign.
- Showed the practical application of integrating ALTAI guidelines with technical evaluation metrics.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential evaluation phases prevent compounding errors where human-centered metrics are rendered invalid by unstable underlying explanations.
- **Mechanism:** The XAIHealth framework enforces a strict dependency chain: Pre-Evaluation (legal/data) → Machine-Centred (robustness/fidelity) → Human-Centred (trust). If the explanation fidelity is low, measuring user trust is meaningless because the user is trusting a fabrication.
- **Core assumption:** Explanation stability is a prerequisite for meaningful human trust measurement.
- **Evidence anchors:**
  - [abstract] The framework is "organized into three phases" including "pre-evaluation" and "machine-centred analysis."
  - [section 4] "These evaluation aspects cannot be assessed simultaneously due to their interdependencies... if an explanation lacks fidelity... the user's trust in that explanation becomes irrelevant."
  - [corpus] "Before the Clinic" supports the need for bridging "explainable AI theory" with "governance requirements" before deployment.
- **Break condition:** If the Local Lipschitz Estimate (LLE) or robustness score exceeds the failure threshold in Phase 1, the system must not proceed to human testing.

### Mechanism 2
- **Claim:** Behavioral trust metrics (reliance on system advice) provide objective failure detection where subjective "confidence" surveys fail.
- **Mechanism:** The framework utilizes a confusion-matrix-based approach to trust (True/False Trust Positives/Negatives). By measuring if the user followed the AI's advice and if that advice was correct, the system distinguishes between "blind trust" (following wrong advice) and "appropriate reliance."
- **Core assumption:** User behavior (reliance) is a more reliable proxy for effective XAI deployment than user attitude (stated confidence).
- **Evidence anchors:**
  - [abstract] Highlights "evaluating trust using behavioral metrics" with specific low precision/recall results.
  - [section 3.2] Mentions that trust is often measured via scales (subjective), but behavioral measures (Scharowski et al. 2022) and reliance frequency (Lai and Tan 2019) are preferred objective methods.
  - [corpus] "Speculative Model Risk in Healthcare AI" emphasizes the risk of "low-barrier development" and the need to surface harms, aligning with the need for objective behavioral checks over assumed safety.
- **Break condition:** If users exhibit high "False Trust" (following incorrect predictions) or low "True Trust" (ignoring correct predictions), the interface is failed and must return to Phase 0/1.

### Mechanism 3
- **Claim:** Mapping legal requirements (GDPR/AI Act) to technical phases operationalizes compliance as a functional requirement rather than an afterthought.
- **Mechanism:** The framework integrates the ALTAI (Assessment List for Trustworthy AI) guidelines directly into the evaluation flow. For example, "Privacy" is a gate for Phase 0, while "Human Agency" is a gate for Phase 2. This forces the system to prove technical robustness to satisfy the legal requirement for safety.
- **Core assumption:** High-risk AI categorization (EU AI Act) requires evidence of technical safety (LLE) to demonstrate legal compliance.
- **Evidence anchors:**
  - [section 4.1] "Legal and ethical considerations are cross-cutting elements... We propose using the ALTAI guidelines."
  - [section 4.3.1] Explicitly links the Technical Robustness ALTAI requirement to passing the LLE metric test.
  - [corpus] "RHealthTwin" highlights similar needs for "responsible" twins to address "hallucination, bias, [and] lack of transparency."
- **Break condition:** Failure to satisfy specific ALTAI requirements (e.g., lack of anonymity in data) halts the entire evaluation process regardless of model performance.

## Foundational Learning

- **Concept: Local Lipschitz Estimate (LLE)**
  - **Why needed here:** This is the primary metric used in the paper's "Machine-Centred" phase to quantify explanation robustness (stability against minor input perturbations).
  - **Quick check question:** Can you explain why an explanation that changes drastically for a slightly brightened image is considered "untrustworthy" even if the prediction remains correct?

- **Concept: Confusion Matrix for Trust**
  - **Why needed here:** The paper re-purposes the standard classification confusion matrix to measure *user trust* against *model correctness*, resulting in metrics like "Trust Precision."
  - **Quick check question:** If a user ignores a correct AI prediction, does this represent a False Positive or a False Negative in the context of the trust metrics described?

- **Concept: ALTAI Guidelines (Assessment List for Trustworthy AI)**
  - **Why needed here:** The framework uses these 7 requirements (e.g., Human Agency, Technical Robustness) as the "rubric" for the Pre-evaluation and Phase gates.
  - **Quick check question:** Which ALTAI requirement specifically mandates that users be informed they are interacting with an AI system?

## Architecture Onboarding

- **Component map:**
  - Phase 0 (Pre-Eval): Data Loader (Anonymized) + Black-box Model (e.g., ResNet18) + XAI Generator (e.g., GradCAM)
  - Phase 1 (Machine-Centred): Perturbation Engine (to test input variance) + Robustness Calculator (LLE Metric)
  - Phase 2 (Human-Centred): Explanation Interface + User Action Logger (recording reliance/acceptance of advice)

- **Critical path:**
  1. **Phase 0:** Verify Data Anonymization & Bias (Pass/Fail)
  2. **Phase 1:** Calculate Local Lipschitz Estimate (LLE). Threshold: Mean LLE ≈ 0.1 (Case study achieved 0.082)
  3. **Phase 2:** User Study. Calculate Trust F1-Score. Threshold: Must significantly exceed random chance (Case study failed at 0.0952)

- **Design tradeoffs:**
  - **Metric Validity vs. Availability:** The paper excludes "unvalidated post-hoc fidelity metrics" in favor of robustness (LLE), trading off direct fidelity measurement for metric reliability
  - **Subjective vs. Objective Trust:** The framework prioritizes behavioral (objective) reliance over attitudinal (subjective) surveys, which may miss emotional friction not reflected in user actions

- **Failure signatures:**
  - **High LLE (>0.1):** Explanations are unstable/sensitive to noise; do not show to humans
  - **High "False Trust" Rate:** Users are following the AI blindly even when it is wrong (Automation Bias)
  - **Low "Trust Recall":** Users are rejecting correct AI advice (mistrust/undertrust)

- **First 3 experiments:**
  1. **Perturbation Stress Test:** Run the Phase 1 LLE calculation on a held-out test set to ensure explanation stability is below the 0.1 threshold before building a UI
  2. **ALTAI Audit:** Run a "Pre-Evaluation" checklist on the dataset to confirm GDPR/anonymization status specifically for "Special Personal Data" (health data)
  3. **Pilot Trust Study:** Build a minimal interface showing predictions and heatmaps. Measure the "Trust F1-Score" on 5 users to see if the explanation actually improves decision-making or causes confusion

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the XAIHealth framework effective when applied to AI systems outside of medical imaging or health contexts?
- **Basis in paper:** [explicit] The conclusion states future work aims to "apply the framework to additional case studies" and hypothesizes it applies to "any AI system that has a significant impact on individuals."
- **Why unresolved:** The framework has only been validated through a single case study involving COVID-19 pneumonia detection from chest X-rays.
- **Evidence:** Successful application and evaluation of the framework in diverse high-stakes domains, such as finance or autonomous driving.

### Open Question 2
- **Question:** What specific protocols and metrics are required to operationalize the "System Operation" phase for long-term monitoring?
- **Basis in paper:** [explicit] Future work identifies the need to "elaborate on the tasks necessary during the system operation phase, specifically concerning monitoring and improvement."
- **Why unresolved:** While the paper mandates this phase for compliance with the EU AI Act, it does not define the specific tasks or evaluation cycles involved.
- **Evidence:** A formalized, repeatable checklist or protocol for post-deployment monitoring that aligns with the ALTAI guidelines.

### Open Question 3
- **Question:** How can the framework guide developers to bridge the gap between high machine-centred robustness and low human-centred trust?
- **Basis in paper:** [inferred] The case study results showed the system passed the robustness phase (LLE = 0.082) but failed the trust phase (F1-Score = 0.0952), requiring a return to Phase 0.
- **Why unresolved:** The paper identifies this failure mode but does not propose specific mechanisms within the framework to align these distinct phases.
- **Evidence:** An iterative study where specific adjustments to the explanation interface (Phase 2) are shown to improve trust metrics without compromising robustness.

## Limitations
- The framework's validity rests heavily on the assumed stability of explanation fidelity as a prerequisite for trust measurement, but this correlation is not empirically validated across different XAI methods.
- The extremely low trust F1-score (0.0952) in the case study suggests either GradCAM is particularly unsuited for this task or the framework itself may need refinement for medical contexts.
- The integration of ALTAI guidelines with technical metrics is conceptually sound but lacks empirical validation for complex cases like differential privacy or data minimization requirements.

## Confidence
- **High Confidence:** The sequential phase dependency mechanism is well-supported by the paper's explicit statement that interdependent metrics cannot be assessed simultaneously.
- **Medium Confidence:** The behavioral trust metrics show promising results in distinguishing blind trust from appropriate reliance, but the case study's extremely low trust F1-score suggests framework limitations.
- **Low Confidence:** The integration of ALTAI guidelines with technical metrics is conceptually sound but lacks empirical validation and does not address complex GDPR compliance scenarios.

## Next Checks
1. **Cross-XAI Method Validation:** Apply the full framework to a different XAI method (e.g., LIME or SHAP) on the same pneumonia detection task to determine if trust failures are method-specific or framework limitations.
2. **ALTAI Technical Mapping Audit:** Conduct a formal mapping exercise between each ALTAI requirement and the corresponding technical metric in the framework, identifying any gaps or misalignments.
3. **User Expectation Calibration Study:** Before trust measurement, conduct preliminary interviews with radiologists to establish baseline expectations for AI explanations in clinical decision-making, then assess whether the framework accounts for these professional standards.