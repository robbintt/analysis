---
ver: rpa2
title: Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation
arxiv_id: '2507.19788'
source_url: https://arxiv.org/abs/2507.19788
tags:
- morl
- multi-objective
- inventory
- solutions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a generalised multi-objective, multi-echelon
  supply chain optimisation model with non-stationary markets based on a Markov decision
  process, incorporating economic, environmental, and social considerations. The model
  is evaluated using a multi-objective reinforcement learning (RL) method, benchmarked
  against an originally single-objective RL algorithm modified with weighted sum using
  predefined weights, and a multi-objective evolutionary algorithm (MOEA)-based approach.
---

# Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation

## Quick Facts
- **arXiv ID:** 2507.19788
- **Source URL:** https://arxiv.org/abs/2507.19788
- **Reference count:** 33
- **One-line primary result:** MORL/D with shared buffer achieves up to 75% higher hypervolume than NSGA-II and 11x denser solutions than weighted-sum PPO

## Executive Summary
This paper develops a multi-objective, multi-echelon supply chain optimization model using reinforcement learning. The approach handles non-stationary markets while balancing economic, environmental, and social objectives. The model determines production and delivery quantities across supply chain routes to achieve near-optimal trade-offs between competing objectives. The primary method, MORL/D (Multi-Objective Reinforcement Learning with Decomposition), demonstrates superior performance compared to modified single-objective RL and MOEA-based approaches, particularly in complex settings.

## Method Summary
The method models the supply chain as a Multi-Objective Markov Decision Process (MOMDP) and uses MORL/D with MOSAC (Multi-Objective Soft Actor-Critic) as the learning algorithm. The approach decomposes the objective space into scalar subproblems using dynamic weight vectors adapted via Pareto Simulated Annealing (PSA), rather than static weights. A shared experience buffer allows knowledge transfer among policies. The model is evaluated on three network complexities (Simple, Moderate, Complex) using a custom simulator, with performance measured by hypervolume, sparsity, expected utility metric, and average Hausdorff distance.

## Key Results
- MORL/D with shared buffer provides the most balanced trade-off between optimality, diversity, and density across all network complexities
- In complex settings, MORL/D achieves up to 75% higher hypervolume than the MOEA-based method (NSGA-II)
- MORL/D generates solutions approximately eleven times denser than those produced by the modified single-objective RL method (weighted-sum PPO)
- The approach ensures stable production and inventory levels while minimizing demand loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic weight vector decomposition using PSA achieves higher density and diversity than static weighted-sum approaches
- **Mechanism:** MORL/D decomposes the objective space into subproblems, each optimized by an agent. PSA adapts weights during training, shifting focus toward under-explored regions when solutions are dominated by neighbors
- **Core assumption:** The optimization landscape is non-convex and static scalarisation fails to capture non-dominated solutions in concave regions
- **Evidence anchors:** Abstract confirms balanced trade-off; Section 4.2.2 describes PSA weight adjustment based on evaluations and proximity
- **Break condition:** If action space dimensionality drops significantly, decomposition overhead may not justify marginal density gains

### Mechanism 2
- **Claim:** Centralized shared experience buffer improves sample efficiency and robustness through cross-policy knowledge transfer
- **Mechanism:** Shared Buffer stores state-action-reward tuples from all subproblems, allowing policies to benefit from experiences generated by neighboring policies
- **Core assumption:** Transitions relevant to one scalarised objective are partially relevant to neighboring objectives in weight space
- **Evidence anchors:** Abstract mentions knowledge transfer among policies; Section 5.1.2 shows SB augments hypervolume and EUM values while decreasing sparsity
- **Break condition:** If objectives are orthogonal to the point where transitions for one are toxic for another, sharing buffers could introduce harmful gradient interference

### Mechanism 3
- **Claim:** Sequential MDP formulation with MOSAC handles non-stationarity and high-dimensionality better than evolutionary algorithms
- **Mechanism:** MOMDP formulation learns a policy mapping states to actions, reducing combinatorial explosion by time-decoupling decisions
- **Core assumption:** System dynamics are learnable and not purely chaotic
- **Evidence anchors:** Section 3.2 explains sequential decision-making reduces effective dimensionality; Section 5.2.1 notes NSGA-II difficulties with constraints and decision variables
- **Break condition:** If environment state transitions are extremely sparse or non-Markovian, MDP formulation fails

## Foundational Learning

- **Concept: Multi-Objective Markov Decision Process (MOMDP)**
  - **Why needed here:** Unlike standard RL which maximizes a scalar, this system optimizes a vector of rewards (Profit, Emission, Inequality). You must understand that "optimal" is not a single point but a set of non-dominated policies (Pareto Front)
  - **Quick check question:** Can you explain why a policy that maximizes profit might be considered "optimal" even if it has poor service levels in this framework?

- **Concept: Scalarisation Functions**
  - **Why needed here:** The gradient descent in the neural network requires a scalar loss. You must understand how a vector reward is converted to a scalar (e.g., weighted sum) and why linear scalarisation might fail for concave Pareto fronts
  - **Quick check question:** If you have weights [0.5, 0.5] for two objectives, and the true optimal lies on a concave part of the front, will a simple weighted sum of rewards likely find it?

- **Concept: Soft Actor-Critic (SAC) and Entropy**
  - **Why needed here:** The paper uses MOSAC. SAC adds an entropy term to the reward, encouraging the agent to explore diverse actions
  - **Quick check question:** Why is entropy regularization specifically useful when the solution space is large and you want to avoid premature convergence to a local minimum?

## Architecture Onboarding

- **Component map:** Environment (Messiah) -> Agent (MORL/D) -> Learner (MOSAC) -> Buffer (Shared) -> Archive
- **Critical path:** Environment Setup -> Normalization -> Weight Initialization -> Training Loop (Agent interacts → Shared Buffer → MOSAC Update → PSA Weight Adaptation → Archive Update)
- **Design tradeoffs:** PPO is computationally cheaper per run but requires multiple runs to build Pareto Front and produces sparse results. MORL/D is computationally intensive (runs once) but produces dense, robust fronts
- **Failure signatures:** PPO Instability (fluctuating EUM, high Sparsity), NSGA-II Collapse (concentrated solution clusters), Inventory Drift (uncontrolled stock accumulation)
- **First 3 experiments:** 1) Reproduce Simple SC Baseline: Run PPO vs. MORL/D on Simple network, verify PPO achieves high Hypervolume but high Sparsity while MORL/D has lower Hypervolume but lower Sparsity. 2) Ablation on Shared Buffer: Run MORL/D on Moderate SC with and without Shared Buffer, plot convergence speed to quantify sample efficiency gain. 3) Stress Test (Complex SC): Run NSGA-II vs. MORL/D on Complex network (5900 decision variables), observe NSGA-II's failure to maintain diversity vs. MORL/D's stable production levels

## Open Questions the Paper Calls Out
1. Can meta-learning frameworks be integrated with MORL/D to reduce computational cost and training time required to adapt to new supply chain tasks?
2. How does the performance of the MOMDP-based model degrade when the assumption of deterministic state transitions is relaxed to include stochastic lead times or yield rates?
3. How does the introduction of a direct penalty for unmet demand alter the trade-off preferences of the MORL/D agent, specifically regarding production stability and service level inequality?

## Limitations
- Use of proprietary simulator "Messiah" restricts full reproducibility despite detailed mathematical specification
- MOEA baseline (NSGA-II) shows significant performance degradation that may be partly due to implementation constraints (population size capped at 300 to avoid memory crashes)
- Comparison with PPO assumes static weights, though dynamic weighting strategies exist that could narrow the performance gap

## Confidence
- **High confidence:** MORL/D with shared buffer achieves superior hypervolume and density metrics compared to PPO and NSGA-II baselines
- **Medium confidence:** Sequential decision-making via MDP formulation is the key factor enabling MORL/D to handle high-dimensional action spaces better than evolutionary algorithms
- **Medium confidence:** Pareto Simulated Annealing weight adaptation is the primary driver of solution diversity

## Next Checks
1. Implement a simplified version of the supply chain simulator using the mathematical formulation to verify reward calculations and state transitions match the described environment
2. Conduct an ablation study isolating the effects of PSA weight adaptation versus shared buffer contribution to hypervolume improvements
3. Test MORL/D performance on a reduced-complexity variant of the Complex network to determine if NSGA-II's poor performance is due to the algorithm or the scale of the problem