---
ver: rpa2
title: Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging
arxiv_id: '2512.08333'
source_url: https://arxiv.org/abs/2512.08333
tags:
- finetuning
- merging
- policy
- task
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robustly finetuning generalist
  robot policies on new tasks while preserving their ability to generalize and perform
  previously learned tasks. Current finetuning methods often lead to overfitting on
  the limited demonstration data, resulting in poor performance on out-of-distribution
  variations of the target task and degradation of generalist capabilities.
---

# Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging

## Quick Facts
- arXiv ID: 2512.08333
- Source URL: https://arxiv.org/abs/2512.08333
- Reference count: 40
- Primary result: Simple weight interpolation (RETAIN) between pretrained and finetuned robot policies improves generalization and preserves generalist capabilities

## Executive Summary
This paper addresses a critical challenge in robot learning: finetuning generalist vision-language-action policies on new tasks without losing their ability to generalize or perform previously learned skills. Current finetuning approaches often overfit to limited demonstration data, resulting in poor performance on out-of-distribution variations and catastrophic forgetting of generalist capabilities. The authors propose RETAIN, a parameter merging approach that interpolates weights between the pretrained generalist policy and the finetuned policy. This simple method significantly improves both task-specific generalization to unseen variations and retention of generalist abilities, achieving up to 40% higher success rates in real-world experiments compared to standard finetuning.

## Method Summary
The core insight is that standard finetuning can cause overfitting and catastrophic forgetting, while naive interpolation between pretrained and finetuned weights might sacrifice task-specific performance. RETAIN addresses this by interpolating the parameters of the pretrained generalist policy with those of the finetuned policy using a weighted combination: θ_retain = λθ_pretrained + (1-λ)θ_finetuned. The interpolation coefficient λ is optimized to balance generalization and task performance. The method is particularly effective for handling out-of-distribution variations of target tasks and enables continual learning by preserving previously acquired skills while acquiring new ones. The approach works with both encoder-decoder and decoder-only transformer architectures, making it broadly applicable to different vision-language-action policy designs.

## Key Results
- RETAIN achieves up to 40% higher success rates than finetuned models in real-world experiments on out-of-distribution variations of target tasks
- The method preserves generalist capabilities, outperforming both pretrained and finetuned models on seen and unseen variations of tasks
- RETAIN enables effective continual learning, allowing sequential acquisition of new skills without catastrophic forgetting of previously learned abilities

## Why This Works (Mechanism)
Standard finetuning on limited demonstration data causes overfitting to the specific trajectories seen during training, leading to poor generalization on variations of the target task and degradation of the pretrained generalist capabilities. RETAIN works by interpolating between the generalist and task-specific parameters, effectively creating a "sweet spot" that retains the generalization properties of the pretrained model while incorporating task-specific knowledge from finetuning. The interpolation coefficient λ controls the trade-off between generalization and task performance. By combining the weights rather than replacing them entirely, RETAIN maintains the rich feature representations and robust behaviors learned during pretraining while adapting to the specific requirements of the new task. This prevents the model from "unlearning" its generalist capabilities while still achieving strong performance on the target task.

## Foundational Learning
- **Vision-Language-Action (VLA) Policies**: Why needed - to enable robots to understand and execute tasks from natural language instructions using visual inputs; Quick check - policy maps image+language to robot actions
- **Catastrophic Forgetting**: Why needed - understanding how neural networks lose previously learned skills during new task training; Quick check - model performs well on new task but poorly on old tasks after finetuning
- **Parameter Interpolation**: Why needed - to combine strengths of multiple trained models without full retraining; Quick check - weighted average of model parameters
- **Out-of-Distribution Generalization**: Why needed - real-world tasks have variations not seen in training; Quick check - performance on task variations not in demonstration data
- **Continual Learning**: Why needed - robots need to acquire new skills over time without losing old ones; Quick check - sequential task learning with preserved performance

## Architecture Onboarding

**Component Map**
VLA policy (pretrained) -> Finetuned VLA policy -> RETAIN (weighted interpolation) -> Final policy

**Critical Path**
Pretrained model parameters → Finetuning on target task → Weight interpolation with RETAIN → Evaluation on seen/unseen variations → Continual learning evaluation

**Design Tradeoffs**
- **Interpolation weight selection**: Static vs adaptive λ values; trade-off between generalization and task performance
- **Model complexity**: Adding interpolation layer vs full architecture modification; RETAIN maintains original architecture
- **Training efficiency**: Single interpolation step vs iterative optimization; RETAIN is computationally efficient

**Failure Signatures**
- λ too close to 1: Over-reliance on pretrained model, poor task-specific performance
- λ too close to 0: Overfitting to finetuned model, poor generalization to variations
- Inappropriate λ for task complexity: Simple tasks may need less pretraining influence

**3 First Experiments**
1. Evaluate RETAIN on block stacking task with varying λ values to find optimal interpolation coefficient
2. Test generalization to unseen block sizes and initial configurations not present in demonstration data
3. Assess continual learning by sequentially training on multiple tasks and measuring performance retention

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily limited to kitchen manipulation tasks (block stacking, drawer opening), may not generalize to more complex domains
- Limited analysis of computational overhead for maintaining and interpolating between multiple policy versions in real-time applications
- No systematic evaluation of performance with noisy or inconsistent demonstration data
- Limited exploration of RETAIN's effectiveness on tasks requiring substantial domain shifts or different robot morphologies

## Confidence

**High Confidence**: The core claim that weight interpolation between pretrained and finetuned models improves generalization is well-supported by empirical results showing 40% improvement in real-world success rates and consistent performance across task variations.

**Medium Confidence**: The assertion that RETAIN enables effective continual learning without catastrophic forgetting is reasonably supported but based on limited sequential task scenarios with only a few tasks.

**Low Confidence**: Claims about RETAIN's effectiveness on tasks beyond the tested kitchen manipulation scenarios are speculative and lack sufficient empirical validation for broader generalization.

## Next Checks
1. **Cross-domain validation**: Test RETAIN on diverse robotic tasks beyond kitchen manipulation, including mobile manipulation, assembly tasks, or different robot platforms to assess generalizability.

2. **Scaling analysis**: Evaluate RETAIN's performance with increasing numbers of sequential tasks in continual learning scenarios to determine if the approach scales effectively or shows degradation over extended use.

3. **Robustness to demonstration quality**: Systematically vary the quality and consistency of demonstration data used for finetuning to assess how sensitive RETAIN is to noisy or imperfect training data, including scenarios with conflicting demonstrations.