---
ver: rpa2
title: 'Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation
  Learning with Double Exploration'
arxiv_id: '2506.20307'
source_url: https://arxiv.org/abs/2506.20307
tags:
- learning
- policy
- imitation
- ilde
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ILDE addresses imitation learning with limited demonstrations
  by introducing a novel algorithm that combines two forms of exploration: optimistic
  policy optimization with uncertainty bonuses and curiosity-driven exploration of
  states deviating from demonstrations. The method learns a policy by minimizing an
  uncertainty-regularized discrepancy between learner and expert trajectories, incorporating
  both an imitation reward and intrinsic curiosity rewards.'
---

# Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration

## Quick Facts
- **arXiv ID**: 2506.20307
- **Source URL**: https://arxiv.org/abs/2506.20307
- **Reference count**: 40
- **Primary result**: ILDE achieves beyond-expert performance (5.33x expert) on Atari games using only 10% of demonstration data

## Executive Summary
ILDE (Imitation Learning with Double Exploration) addresses the challenge of learning from limited demonstrations by combining two forms of exploration: an optimistic uncertainty bonus and curiosity-driven intrinsic rewards. The method learns a policy by minimizing an uncertainty-regularized discrepancy between learner and expert trajectories, incorporating both imitation rewards and intrinsic curiosity rewards. Theoretically, ILDE-NPG achieves a sublinear regret bound of O(H^(8/3) T^(2/3)) in MDPs with nonlinear function approximation, representing the first such guarantee for this setting. Empirically, ILDE outperforms state-of-the-art methods like GAIL and GIRIL on Atari games and MuJoCo tasks while requiring fewer demonstrations.

## Method Summary
ILDE combines VAIL-style adversarial imitation learning with curiosity-driven exploration using a pretrained VAE on limited demonstration data. The method introduces an optimistic exploration bonus based on state entropy and trains policies using PPO to maximize a composite reward function. The approach addresses the fundamental challenge of achieving beyond-expert performance when demonstrations are sparse by encouraging the agent to explore states that deviate from the demonstration trajectories while maintaining alignment with the expert's behavior distribution.

## Key Results
- Achieves 5.33x expert performance on Atari games using only 10% of one-life demonstration data
- Outperforms state-of-the-art methods (GAIL, GIRIL) on both Atari and MuJoCo continuous control tasks
- Demonstrates superior sample efficiency, reaching expert-level performance with fewer interactions than competing methods
- Shows ablation studies confirming both exploration components (uncertainty bonus and curiosity reward) are essential for performance

## Why This Works (Mechanism)

### Mechanism 1: Optimistic Uncertainty Bonus
An exploration bonus $b(s,a)$ is added to the reward, identifying high-uncertainty state-action pairs using state entropy via k-NN estimation. This drives the agent to explore under-sampled regions of the state space, reducing value function estimation error faster than on-policy sampling alone.

### Mechanism 2: Curiosity-Driven Intrinsic Reward
A VAE pretrained on demonstrations provides an intrinsic reward proportional to prediction error of next states. States that surprise the VAE (deviating from demos) are rewarded, allowing the agent to discover potentially superior trajectories beyond the expert's path.

### Mechanism 3: Adversarial Distribution Matching
A discriminator provides an imitation reward $r_k$ that distinguishes agent trajectories from demonstrations, acting as a tether to prevent exploration signals from causing domain drift. This ensures the agent remains grounded in the state-action distribution implied by the expert while exploring.

## Foundational Learning

### Concept: Generalized Eluder Dimension
- **Why needed**: Theoretical tool used to bound complexity of function class in regret analysis for nonlinear settings
- **Quick check**: How does Eluder dimension relate to uncertainty of state-action pairs in continuous vs discrete spaces?

### Concept: Variational Autoencoders (VAEs) for Dynamics
- **Why needed**: Curiosity mechanism relies on pretrained VAE to model expert transition dynamics
- **Quick check**: Does high VAE reconstruction error indicate "novel" or simply "noisy" states? How to distinguish?

### Concept: Proximal Policy Optimization (PPO) with Variable Rewards
- **Why needed**: Practical implementation uses PPO to optimize composite reward with evolving discriminator and VAE outputs
- **Quick check**: How does PPO's clipping objective stabilize training when reward function changes significantly?

## Architecture Onboarding

### Component Map
1. Policy Network (Actor-Critic): PPO agent interacting with environment
2. Discriminator (VAIL): Outputs probability of being expert data; converted to imitation reward $r_k$
3. Curiosity Module (VAE): Pretrained on demos; outputs prediction error $L_{\tau_E}$ as intrinsic reward
4. State Entropy Estimator: Calculates k-NN distance in representation space for exploration bonus $b(s,a)$

### Critical Path
Pretrain Curiosity Module (VAE) on limited demonstrations → Initialize Discriminator and Policy → Collect trajectories → Aggregate Rewards ($r_k + \lambda L_{\tau_E} + b$) → Update Policy (PPO) → Update Discriminator

### Design Tradeoffs
- $\lambda$ Weight: High $\lambda$ prioritizes curiosity (high risk/high reward) vs. Low $\lambda$ prioritizes imitation (safe/conservative)
- Theory vs. Practice: Theoretical regret bound applies to ILDE-NPG, but practical implementation uses PPO and State Entropy approximations

### Failure Signatures
- Performance Collapse: Agent scores 0 or collapses to repetitive loop due to trivial curiosity rewards or overly strict discriminator
- Worse-than-Expert: Performance stalls below expert level when exploration bonus drives agent off-manifold

### First 3 Experiments
1. VAE Sanity Check: Verify VAE reconstruction error is low on demo data and high on random noise
2. Ablation on $\lambda$: Run ILDE on "BeamRider" with $\lambda \in \{2, 10, 20\}$ to observe curiosity weight sensitivity
3. Component Ablation: Run ILDE w/o $b$ (bonus) and ILDE w/o $r_k$ (imitation) to confirm both components are essential

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical regret guarantees established for ILDE-NPG be rigorously extended to the practical PPO-based implementation used in experiments? The conclusion states the analysis cannot be directly applied to the practical version implemented in experiments, representing a significant gap between theory and practice.

### Open Question 2
How does ILDE perform in environments where the ground-truth reward function is inversely correlated with state novelty, effectively penalizing curiosity? The paper identifies this as a limitation since the method assumes alignment between the target task and seeking novelty.

### Open Question 3
To what extent does ILDE actually extrapolate the expert's intention versus simply exploiting environmental rewards that happen to align with "seeking novelty"? The limitation section notes the beyond-expert performance may result from environmental reward alignment rather than true intention extrapolation.

## Limitations
- Theoretical-practical gap: O(3/2) regret bound applies to idealized ILDE-NPG algorithm, not the practical PPO implementation
- Demonstration sparsity sensitivity: Method relies on discriminator and VAE being trainable from limited data (10% of one-life trajectories), but minimum threshold is not quantified
- Curiosity reward stability: VAE-based curiosity signal depends on pretrained dynamics model's generalization, which may mislead exploration if demonstration data is not representative

## Confidence

### High Confidence
- Empirical claim of beyond-expert performance on Atari and MuJoCo tasks (Table 1, 2)
- Ablation studies showing necessity of both exploration components (Table 3, 7, 8)

### Medium Confidence
- Claim of "sample efficiency" improvement over GAIL and GIRIL requires careful interpretation
- Theoretical regret bound is mathematically sound but applies to different algorithm (ILDE-NPG) than empirically evaluated

## Next Checks

1. **Component Sensitivity Analysis**: Replicate λ ablation study (Table 8) on "BeamRider" to quantify how curiosity weight affects trade-off between imitation and exploration

2. **Demonstration Coverage Impact**: Systematically vary demonstration size (5%, 10%, 20% of one-life data) on fixed game to identify minimum data threshold where ILDE outperforms standard IL methods

3. **Generalization Test**: Evaluate ILDE on novel task "Seaquest" not used in paper to verify beyond-expert performance is not task-specific and VAE curiosity signal generalizes to new dynamics