---
ver: rpa2
title: Differentially Private In-context Learning via Sampling Few-shot Mixed with
  Zero-shot Outputs
arxiv_id: '2501.19287'
source_url: https://arxiv.org/abs/2501.19287
tags:
- dps-mozo
- output
- privacy
- zero-shot
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy leakage in large language model (LLM)
  in-context learning (ICL) when using sensitive private datasets for demonstrations.
  It introduces DPS-MOZO, a differentially private decoding framework that generates
  private text by sampling from the product of mixed one-shot and zero-shot output
  distributions, achieving privacy without additive noise.
---

# Differentially Private In-context Learning via Sampling Few-shot Mixed with Zero-shot Outputs

## Quick Facts
- **arXiv ID**: 2501.19287
- **Source URL**: https://arxiv.org/abs/2501.19287
- **Reference count**: 40
- **Primary result**: DPS-MOZO achieves ε=2 differential privacy with only 0.3% ROUGE-L F1 score decrease on SAMSum compared to non-private ICL

## Executive Summary
This paper addresses privacy leakage in large language model (LLM) in-context learning (ICL) when using sensitive private datasets for demonstrations. It introduces DPS-MOZO, a differentially private decoding framework that generates private text by sampling from the product of mixed one-shot and zero-shot output distributions, achieving privacy without additive noise. The method reduces information leakage by bounding the divergence between mixed and zero-shot distributions, and applies to both online (DPS-MOZO+On) and offline (DPS-MOZO+Off) ICL settings. Experimental results on summarization, data-to-text, and text simplification tasks with LLaMA-3 3B and Gemma-2 2B show DPS-MOZO achieves strong privacy with minimal utility loss.

## Method Summary
DPS-MOZO is a differentially private decoding framework for ICL that generates private text by sampling from mixed output distributions. The method subsamples n_shots=4 demonstrations without replacement, computes one-shot and zero-shot output distributions for each, truncates to top-100 tokens from zero-shot, and mixes via p_θ,λ = σ(λ·logit_one-shot + (1-λ)·logit_zero-shot). For each demonstration, λ is optimized via bisection to satisfy D_α(p_θ,λ || p_θ) ≤ βα Renyi divergence constraint. The final token is sampled from the product of these mixed distributions. Privacy is achieved through subsampling amplification and composition, with RDP accounting converting to (ε,δ)-DP.

## Key Results
- DPS-MOZO achieves ε=2 differential privacy with only 0.3% ROUGE-L F1 score decrease on SAMSum compared to non-private ICL
- The method maintains strong utility across three tasks: summarization (SAMSum), data-to-text (E2E), and text simplification (WikiLarge)
- Ablation studies show DPS-MOZO is robust to hyperparameters and outperforms standard DP baselines
- Membership inference attacks demonstrate strong empirical privacy against leakage

## Why This Works (Mechanism)
The core insight is that sampling from a distribution that is close to the zero-shot output distribution provides privacy, as the zero-shot distribution contains no information about the private demonstrations. By mixing the one-shot and zero-shot distributions and bounding the Renyi divergence between them, DPS-MOZO ensures that the output distribution is close enough to zero-shot to provide privacy guarantees while still incorporating useful information from the demonstrations.

## Foundational Learning

**Renyi Differential Privacy (RDP)**: A generalization of differential privacy that provides tighter composition bounds for Gaussian-like mechanisms. Needed for analyzing the privacy of sampling from mixed distributions. Quick check: Verify that RDP implies standard (ε,δ)-DP via the conversion theorem.

**Subsampling Amplification**: The principle that privacy loss is reduced when applying a mechanism to a random subset of the dataset. Needed to justify the privacy guarantees when subsampling demonstrations. Quick check: Confirm that q=n_shots/|D| amplification factor is correctly applied in the privacy accounting.

**Bisection Optimization**: A root-finding method for solving λ optimization to satisfy Renyi divergence constraints. Needed for finding the maximum λ that provides privacy for each demonstration. Quick check: Verify convergence of λ optimization across multiple token generations.

## Architecture Onboarding

**Component Map**: LLM -> Subsampling -> Logit Computation -> Truncation -> Mixing -> Sampling

**Critical Path**: For each token generation: (1) subsample demonstrations, (2) compute logits for each demonstration and zero-shot, (3) truncate to top-100, (4) optimize λ for each demonstration via bisection, (5) sample from product distribution

**Design Tradeoffs**: Mixing with zero-shot provides strong privacy but may reduce utility compared to using only one-shot; truncation to top-100 tokens balances privacy and diversity but may miss rare but relevant words

**Failure Signatures**: λ optimization getting stuck at boundaries indicates Renyi divergence computation errors or overly restrictive β; utility far below reported suggests incorrect truncation or mixing implementation

**First Experiments**: (1) Verify that DPS-MOZO generates coherent text on a simple prompt without demonstrations; (2) Confirm that Renyi divergence constraint is satisfied for sampled λ values; (3) Test privacy accounting by running multiple generations and verifying ε remains below target

## Open Questions the Paper Calls Out

**Open Question 1**: Can DPS-MOZO be effectively adapted for closed-source LLMs (e.g., GPT-4) using logit bias extraction techniques? The authors note that closed-source APIs typically restrict full logit vectors, but logit bias extraction might be possible.

**Open Question 2**: How does the privacy-utility tradeoff change if the adversary gains access to the full mixed output distribution rather than just the sampled tokens? The privacy guarantee relies on sampling stochasticity, and releasing the full distribution might leak more information.

**Open Question 3**: How can the utility degradation in DPS-MOZO+On be mitigated when scaling to a large number of shots (n_shots > 4)? Ablation studies show performance degrades with more shots due to decreased privacy amplification by subsampling.

## Limitations

- The method requires solving an optimization problem for each token generation, which could impact latency in production settings
- Reliance on truncation to top-100 tokens from zero-shot distributions may limit diversity in certain generation scenarios
- Privacy analysis assumes the sampling-without-replacement procedure accurately reflects subsampling amplification, but edge cases with small datasets could violate theoretical assumptions

## Confidence

- **High confidence**: Core theoretical framework connecting mixed-distribution sampling to Renyi differential privacy; experimental methodology and metrics reporting
- **Medium confidence**: Implementation details of bisection optimization for λ selection; exact prompt formatting templates
- **Low confidence**: Scalability analysis for larger models/datasets; robustness to prompt engineering variations

## Next Checks

1. Implement the DPS-MOZO decoding procedure on a small subset of SAMSum (e.g., 50 samples) and verify that generated sequences maintain semantic coherence while respecting the Renyi divergence constraint

2. Conduct ablation study varying β parameter to quantify the privacy-utility tradeoff curve empirically across all three tasks

3. Test membership inference attack implementation using the released DPS-MOZO outputs to confirm that private data points are statistically indistinguishable from non-members