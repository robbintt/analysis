---
ver: rpa2
title: 'SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction'
arxiv_id: '2506.02082'
source_url: https://arxiv.org/abs/2506.02082
tags:
- speech
- data
- arxiv
- features
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SALF-MOS, a novel small-sized, end-to-end model
  for predicting Mean Opinion Scores (MOS) of speech synthesis and voice conversion
  models without requiring human evaluations. The method uses latent feature compression
  and stacking of down-sampled convolutional features to achieve high accuracy in
  MOS prediction.
---

# SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction

## Quick Facts
- arXiv ID: 2506.02082
- Source URL: https://arxiv.org/abs/2506.02082
- Authors: Saurabh Agrawal; Raj Gohil; Gopal Kumar Agrawal; Vikram C M; Kushal Verma
- Reference count: 29
- Primary result: Novel small-sized, end-to-end MOS prediction model achieving state-of-the-art performance with only 1,574 parameters

## Executive Summary
This paper introduces SALF-MOS, a compact and speaker-agnostic model for predicting Mean Opinion Scores (MOS) of speech synthesis and voice conversion systems. The model uses frozen wav2vec features and a shallow architecture with hierarchical downsampling to extract quality-relevant latent representations. SALF-MOS outperforms existing models on multiple datasets while requiring minimal computational resources and avoiding the need for fine-tuning SSL models or using domain/listener IDs.

## Method Summary
SALF-MOS processes frozen wav2vec features through a U-Net-inspired encoder with four Double Convolution blocks (Conv1D-BN-ReLU) and three downsampling layers. At each depth, latent features are extracted via linear projections and stacked before final MOS prediction. The model trains with L1 loss and SGD optimizer, achieving high accuracy with only 1,574 parameters. It operates without fine-tuning SSL models or using speaker/listener IDs, making it highly generalizable.

## Key Results
- Outperforms state-of-the-art models on BVCC (MSE 0.144, LCC 0.944), VCC2018 (MSE 0.336, LCC 0.774), SOMOS (MSE 0.15, LCC 0.779), and TMHINTQI (MSE 0.383, LCC 0.795)
- Achieves speaker-agnostic operation without fine-tuning SSL models or using domain/listener IDs
- Requires only 1,574 parameters, significantly smaller than existing approaches
- Ablation shows wav2vec features outperform MFCC, LFCC, and x-vector with 3-4× lower MSE

## Why This Works (Mechanism)

### Mechanism 1
- Hierarchical downsampling with feature extraction at multiple depths enables robust MOS prediction with minimal parameters
- Sequential Double Convolutions followed by downsampling extract multi-resolution quality signals, with early layers capturing fine-grained details and deeper layers encoding abstract quality patterns
- Quality-relevant information exists at multiple temporal resolutions in wav2vec features, extractable via compressed encoding

### Mechanism 2
- Frozen wav2vec features provide speaker-agnostic quality representations while dramatically reducing model complexity
- Pre-trained wav2vec encodes acoustic properties independent of speaker identity, allowing the small SALF architecture to map patterns to quality scores without overfitting
- Wav2vec representations contain quality-relevant information extractable via linear+convolutional transforms without domain adaptation

### Mechanism 3
- Model depth of 4 provides optimal balance - deeper models underfit due to insufficient parameter count for complex decision boundaries
- With only 1,574 parameters, increasing depth spreads parameters too thinly across layers
- MOS prediction requires sufficient representational capacity per layer; shallow-but-wide outperforms deep-and-narrow for this parameter budget

## Foundational Learning

- **U-Net architecture principles**
  - Why needed: SALF-MOS adapts U-Net's encoder pattern for feature compression; understanding this helps grasp why multi-scale extraction aids quality assessment
  - Quick check: If you removed all downsampling layers and kept only one Double Convolution block, would the model still capture temporal quality patterns? Why or why not?

- **Self-supervised speech representations (wav2vec)**
  - Why needed: The entire approach depends on wav2vec encoding quality-relevant information; understanding what wav2vec learns clarifies why it works for MOS without fine-tuning
  - Quick check: Wav2vec was trained for speech recognition, not quality assessment. What assumption does SALF-MOS make about the relationship between ASR-relevant and quality-relevant features?

- **Correlation metrics (LCC, SRCC, KTAU)**
  - Why needed: The paper reports multiple correlation coefficients; understanding when they disagree helps diagnose model behavior on different error types
  - Quick check: If a model has high SRCC but low LCC, what does this indicate about its prediction errors?

## Architecture Onboarding

- Component map: Audio (16kHz) -> wav2vec features -> DoubleConv1 -> LFE1 -> Downsample -> DoubleConv2 -> LFE2 -> Downsample -> DoubleConv3 -> LFE3 -> Downsample -> DoubleConv4 -> LFE4 -> Stack[LFE1,LFE2,LFE3,LFE4] -> Linear -> MOS

- Critical path: 16kHz audio passes through frozen wav2vec, then through 4× Double Convolution blocks with 3× stride-2 pooling between them, linear latent feature extraction at each depth, feature stacking, and final linear MOS prediction

- Design tradeoffs: Frozen vs. fine-tuned wav2vec enables small model (1,574 params) but may limit adaptation to novel distortions; depth 4 vs. deeper shows deeper = underfitting with this parameter budget; single SSL vs. ensemble sacrifices accuracy gains from multi-model fusion but gains simplicity and speed

- Failure signatures: High MSE on datasets with non-Gaussian MOS distributions; underfitting if depth increased without parameter budget increase; poor generalization when switching from wav2vec to MFCC/LFCC (MSE degrades 3-4×)

- First 3 experiments:
  1. Baseline replication on BVCC: Train SALF-MOS from scratch using wav2vec features, target MSE ≤ 0.15, LCC ≥ 0.94
  2. Ablation by feature type: Replace wav2vec with MFCC, LFCC, and x-vector to validate SSL features provide unique quality information
  3. Cross-dataset zero-shot evaluation: Train on BVCC only, test on VCC2018/SOMOS without adaptation to quantify generalization gap

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Speaker-agnostic generalization beyond the four evaluation datasets (all English or limited languages) remains untested
- The optimal depth-4 architecture lacks external validation across different datasets and parameter budgets
- Model robustness to diverse distortion types beyond synthesis/conversion artifacts is unclear

## Confidence
- **High**: Speaker-agnostic operation and small parameter count (1,574 params) - directly verifiable from architecture
- **Medium**: MSE/LCC performance metrics - results consistent across multiple datasets but lack external replication
- **Low**: U-Net-inspired architecture is optimal for MOS prediction - novel adaptation untested against alternative architectures

## Next Checks
1. Cross-linguistic evaluation: Test SALF-MOS on non-English datasets (e.g., Mandarin, Japanese) to verify speaker-agnostic generalization beyond training languages
2. Distortion robustness test: Evaluate on quality-degraded speech (codec artifacts, noise, reverberation) to assess if frozen wav2vec features remain informative for diverse degradation types
3. Architecture ablation study: Compare SALF-MOS against a simpler CNN baseline (single DoubleConv + LFE) to quantify whether multi-scale feature stacking provides meaningful gains beyond basic feature extraction