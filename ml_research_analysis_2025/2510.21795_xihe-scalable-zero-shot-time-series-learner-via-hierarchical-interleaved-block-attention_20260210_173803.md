---
ver: rpa2
title: 'Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved
  Block Attention'
arxiv_id: '2510.21795'
source_url: https://arxiv.org/abs/2510.21795
tags:
- series
- time
- short
- xihe
- hiba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Xihe introduces a novel Hierarchical Interleaved Block Attention
  (HIBA) mechanism that hierarchically partitions time series into blocks of varying
  sizes and alternates intra- and inter-block attention to capture multi-scale dependencies.
  This architecture effectively addresses the challenge of modeling diverse temporal
  patterns across different domains and sampling frequencies, which existing Transformer-based
  models struggle with due to their fixed token size.
---

# Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention

## Quick Facts
- **arXiv ID:** 2510.21795
- **Source URL:** https://arxiv.org/abs/2510.21795
- **Reference count:** 40
- **Primary result:** Xihe-max (1.5B parameters) establishes new state-of-the-art zero-shot performance on GIFT-Eval benchmark, surpassing previous best results by substantial margins.

## Executive Summary
Xihe introduces a novel Hierarchical Interleaved Block Attention (HIBA) mechanism that hierarchically partitions time series into blocks of varying sizes and alternates intra- and inter-block attention to capture multi-scale dependencies. This architecture effectively addresses the challenge of modeling diverse temporal patterns across different domains and sampling frequencies, which existing Transformer-based models struggle with due to their fixed token size. Evaluated on the GIFT-Eval benchmark, Xihe-tiny (9.5M parameters) surpasses most contemporary TSFMs, demonstrating remarkable parameter efficiency. Xihe-max (1.5B parameters) establishes new state-of-the-art zero-shot performance, surpassing previous best results by substantial margins. The models follow clear scaling laws, with performance improving monotonically as parameter count increases from 9.5M to 1.5B. Xihe also achieves exceptionally high inference throughput, with Xihe-lite and Xihe-tiny delivering both strong forecasting accuracy and high efficiency suitable for practical deployment.

## Method Summary
Xihe uses Hierarchical Interleaved Block Attention (HIBA) to address time series forecasting through a novel attention mechanism that partitions sequences into blocks of varying sizes (3, 7, 21) across layers. The architecture employs alternating intra-block non-causal attention for local pattern capture and inter-block causal attention for global dependencies, reducing computational complexity while maintaining temporal integrity. The model is trained on 325 billion time points from LOTSA, Chronos subsets, and synthetic data, optimized with Adam (LR 1e-4, cosine decay) to predict quantiles {0.1,...,0.9} for horizons {96, 768}. The input pipeline uses patch size 8 with InstanceNorm, and the model architecture scales from 9.5M to 1.5B parameters while maintaining high inference throughput on NVIDIA A100 GPUs.

## Key Results
- Xihe-tiny (9.5M parameters) surpasses most contemporary TSFMs on GIFT-Eval benchmark despite its small size
- Xihe-max (1.5B parameters) establishes new state-of-the-art zero-shot performance, outperforming previous best results by substantial margins
- Xihe models follow clear scaling laws, with performance improving monotonically as parameter count increases from 9.5M to 1.5B
- Xihe-lite and Xihe-tiny achieve exceptionally high inference throughput while maintaining strong forecasting accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical block structure enables capture of multi-scale temporal dependencies that fixed-patch approaches miss.
- Mechanism: Varying block sizes (B=3,7,21) across layers creates receptive fields at different temporal scales. Small blocks (B=3) capture fine-grained local patterns; large blocks (B=21) aggregate broader trends. Information flows hierarchically as representations pass through layers with different block sizes.
- Core assumption: Time series exhibit scale-varying dependencies (local cycles vs. global trends) that single-scale tokenization cannot simultaneously represent well.
- Evidence anchors:
  - [abstract] "HIBA... hierarchically partitions time series into blocks of varying sizes and alternates intra- and inter-block attention to capture multi-scale dependencies."
  - [section 3.2] "By assigning different block sizes B to different HIBA blocks, intra- and inter-block attention can capture local and global information at multiple scales."
  - [corpus] Weak direct corpus evidence; related work (Kairos) notes heterogeneous information density in time series but doesn't validate HIBA specifically.
- Break condition: If target domains exhibit primarily single-scale dependencies (e.g., uniform sampling with consistent seasonality), hierarchical overhead may not justify added complexity.

### Mechanism 2
- Claim: Alternating intra- and inter-block attention enables efficient local-global information exchange without full-sequence attention cost.
- Mechanism: Intra-block attention (non-causal MSA) performs dense attention within each block, enabling bidirectional local fusion. Inter-block attention (causal MSA) attends across block representatives, capturing long-range dependencies with O(n/B) complexity per head. The interleaving propagates local refinements to global context and vice versa.
- Core assumption: Sparse cross-block attention with dense within-block attention is sufficient to approximate full attention for time series patterns.
- Evidence anchors:
  - [section 3.2] "Intra-block attention facilitates local information exchange, and inter-block attention operates across blocks to capture global temporal pattern interaction."
  - [section 4.4] Ablation shows replacing HIBA with standard attention increases MASE from 0.718 to 0.736 and CRPS from 0.497 to 0.507.
  - [corpus] No corpus papers directly validate sparse hierarchical attention for TSFMs.
- Break condition: If sequences require dense pairwise interactions across all positions (e.g., highly irregular sampling with global correlations), block-based sparsity may underfit.

### Mechanism 3
- Claim: Non-causal intra-block attention with causal inter-block attention balances local feature richness with temporal integrity.
- Mechanism: Non-causal attention within blocks permits future context for local pattern completion (e.g., identifying a weekly cycle mid-week). Causal inter-block attention ensures predictions only depend on past blocks at the global level. The paper treats potential leakage within blocks as auxiliary supervision, with the final patch (h_L_N) guaranteed leakage-free.
- Core assumption: Local bidirectional context improves representation quality without undermining autoregressive validity when global causality is enforced.
- Evidence anchors:
  - [section 3.2] "In intra-block attention, a non-causal multi-head self-attention (MSA_non-causal) is applied... in inter-block attention... MSA_causal."
  - [section 4.4] Ablation shows using causal attention in intra-block increases MASE from 0.718 to 0.721 and CRPS from 0.497 to 0.502.
  - [corpus] No corpus papers examine causal vs. non-causal attention trade-offs in TSFMs.
- Break condition: If downstream tasks strictly forbid any future information access (e.g., real-time control), non-causal intra-block design requires modification.

## Foundational Learning

- Concept: Multi-head self-attention (query/key/value projections, scaled dot-product)
  - Why needed here: HIBA layers use MSA with modified attention masks; understanding baseline attention is prerequisite.
  - Quick check question: Can you explain why attention complexity is O(n²) and how block partitioning reduces this?

- Concept: Time series components (trend, seasonality, noise; sampling frequency effects)
  - Why needed here: Paper's core motivation is multi-scale dependencies tied to domain-specific temporal characteristics.
  - Quick check question: Given a daily dataset with weekly and annual patterns, what time scales would you expect a model to capture?

- Concept: Quantile regression and probabilistic forecasting
  - Why needed here: Xihe outputs quantile predictions (q∈{0.1,...,0.9}) and uses quantile loss (Eq. 11).
  - Quick check question: Why use quantile loss instead of MSE for uncertainty estimation?

## Architecture Onboarding

- Component map:
  Input: InstanceNorm → Patching (P=8) → Concat with mask → MLP embedding
  Backbone: L HIBA blocks (each = RMSNorm + Intra-MSA_non-causal + FFN + RMSNorm + Inter-MSA_causal + FFN)
  Output: K prediction heads (horizons {96, 768}), each producing 9 quantiles

- Critical path:
  1. Implement patch tokenization with padding and binary mask (Eq. 3-4)
  2. Implement block partitioning with configurable B (Eq. 5)
  3. Implement intra-block non-causal MSA and inter-block causal MSA (Eq. 6-9)
  4. Implement multi-head quantile prediction and loss (Eq. 10-12)

- Design tradeoffs:
  - Smaller patch size (P=8) vs. larger: Smaller patches give finer granularity but increase sequence length; paper chose 8 to leverage HIBA's multi-scale capacity.
  - Block sizes (3,7,21): Smaller blocks capture finer local patterns; larger blocks aggregate more global context. Ablation shows uniform B=3 underperforms hierarchical.
  - Non-causal intra-block: Improves local fusion but introduces potential leakage; paper mitigates by ensuring final patch is leakage-free.

- Failure signatures:
  - Performance degrades on high-frequency data: Check if block sizes are too large relative to local pattern scale.
  - Training instability with quantile loss: Ensure proper weighting across quantiles; check for crossing quantile predictions.
  - Poor zero-shot transfer: Verify data quality weighting and augmentation are applied; check pre-training data diversity.
  - Inference slower than expected: Profile inter-block attention; ensure causal masking is optimized.

- First 3 experiments:
  1. Replicate Xihe-tiny on a subset of GIFT-Eval datasets to validate implementation correctness; compare MASE/CRPS against reported values.
  2. Ablate block sizes: Train with uniform B=3 vs. hierarchical (3,7,21) to confirm hierarchical benefit on your target domains.
  3. Test inference throughput: Measure samples/sec for Xihe-tiny vs. Xihe-lite on your hardware; compare against paper's Figure 4a to identify implementation bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the HIBA architecture be effectively extended to incorporate covariates and handle multivariate time series forecasting?
- Basis in paper: [explicit] The conclusion states that "Xihe [is] still limited to uni-variate time series forecasting," and lists "the incorporation of richer information (e.g., covariates...)" as future work.
- Why unresolved: The current HIBA mechanism processes independent univariate series; it lacks a mechanism for cross-variate attention or embedding external covariates into the hierarchical block structure.
- What evidence would resolve it: A modified Xihe model demonstrating zero-shot performance on multivariate benchmarks (e.g., electricity, traffic) with ablation studies showing the impact of covariate integration.

### Open Question 2
- Question: Does the hierarchical structure of HIBA transfer effectively to non-forecasting tasks such as time series classification and anomaly detection?
- Basis in paper: [explicit] The authors explicitly list "the extension to additional tasks (e.g., classification and anomaly detection)" as a direction for future work.
- Why unresolved: The multi-head prediction module and quantile loss are specifically tailored for forecasting; it is unclear if the multi-scale temporal features learned by HIBA are optimal for classification or anomaly detection without architectural modifications.
- What evidence would resolve it: Experimental results applying the Xihe encoder (frozen or fine-tuned) to standard time series classification and anomaly detection datasets, comparing performance against specialized models.

### Open Question 3
- Question: Do the observed scaling laws persist monotonically as parameter counts increase significantly beyond the current 1.5 billion parameter ceiling?
- Basis in paper: [explicit] The conclusion notes the intent to "expand Xihe to larger sizes to further push the limit of TSFMs."
- Why unresolved: While Figure 4b shows a clear trend from 9.5M to 1.5B, scaling laws in transformers often saturate or encounter optimization instabilities (e.g., trainability issues) at significantly larger scales.
- What evidence would resolve it: Training curves and benchmark evaluations for Xihe models with parameters exceeding 1.5B (e.g., 7B or 10B), demonstrating continued monotonic improvement in MASE and CRPS.

### Open Question 4
- Question: How sensitive is the model's zero-shot performance to the specific selection of hierarchical block sizes relative to the sampling frequency of the target dataset?
- Basis in paper: [inferred] Table 2 shows all Xihe model variants use fixed block sizes of (3, 7, 21), despite the introduction emphasizing that temporal dependencies "differ as the domain and sampling strategies change."
- Why unresolved: The paper doesn't ablate whether these specific block sizes are optimal for all frequencies or if they represent a compromise that limits performance on specific domains (e.g., high-frequency financial data vs. low-frequency yearly data).
- What evidence would resolve it: An ablation study varying the block size configurations B for datasets of differing frequencies to identify if frequency-adaptive block sizing yields significant gains over the fixed configuration.

## Limitations

- The paper lacks ablation studies on key architectural choices beyond basic HIBA components, particularly regarding alternative sparse attention patterns and different block size configurations
- The KernelSynth-inspired synthetic data generation is only described as "inspired by" rather than fully specified, making exact replication difficult
- The non-causal intra-block attention design introduces potential information leakage that isn't fully explored in terms of downstream task integrity

## Confidence

- **High confidence:** Xihe's state-of-the-art performance on GIFT-Eval benchmark, the clear scaling laws showing monotonic improvement with parameter count, and the demonstrated inference throughput advantages are well-supported by empirical results.
- **Medium confidence:** The effectiveness of alternating causal/non-causal attention patterns within HIBA blocks, while supported by ablation studies, lacks comparison to alternative attention architectures that could achieve similar efficiency.
- **Low confidence:** Claims about HIBA's superiority for specific temporal pattern types (e.g., multi-frequency seasonality vs. irregular sampling) are not directly tested, as the paper doesn't perform detailed pattern-level analysis on the benchmark datasets.

## Next Checks

1. Implement an ablation study comparing HIBA against alternative sparse attention mechanisms (e.g., LogSparse, local window attention) on GIFT-Eval to determine if the specific block alternation pattern provides unique benefits beyond basic sparsity.
2. Conduct error analysis on individual GIFT-Eval datasets to identify which temporal patterns (trend, seasonality, noise characteristics) show the largest performance gaps between Xihe and baseline models, validating the multi-scale dependency capture claim.
3. Test the model's behavior under strict causal constraints by modifying the intra-block attention to be causal and measuring the performance trade-off, directly evaluating the claimed benefit of the non-causal design choice.