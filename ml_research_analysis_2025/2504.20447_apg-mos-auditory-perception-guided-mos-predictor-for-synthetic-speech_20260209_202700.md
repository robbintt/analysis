---
ver: rpa2
title: 'APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech'
arxiv_id: '2504.20447'
source_url: https://arxiv.org/abs/2504.20447
tags:
- speech
- auditory
- semantic
- quality
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces APG-MOS, an auditory perception-guided MOS
  predictor for synthetic speech quality assessment. The model integrates biologically
  inspired auditory modeling with semantic analysis to improve consistency with human
  judgments.
---

# APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech

## Quick Facts
- **arXiv ID:** 2504.20447
- **Source URL:** https://arxiv.org/abs/2504.20447
- **Reference count:** 40
- **Primary result:** APG-MOS achieves system-level SRCC scores of 0.936 on BVCC and 0.921 on SOMOS datasets, outperforming existing MOS predictors while requiring fewer parameters

## Executive Summary
This paper introduces APG-MOS, an auditory perception-guided MOS predictor that combines biologically inspired auditory modeling with semantic analysis to improve synthetic speech quality assessment. The model simulates cochlear functions to encode acoustic signals into electrochemical representations, quantifies semantic-level quality degradation using residual vector quantization, and employs residual cross-attention for multimodal fusion. Experiments demonstrate superior performance over existing methods on standard benchmarks, achieving high system-level SRCC scores while maintaining parameter efficiency. The approach bridges the gap between expert knowledge-driven and data-driven speech quality assessment models.

## Method Summary
APG-MOS employs a 4-stage progressive training strategy: first training an Auditory Perception Modeling (APM) module with Gammatone filters to simulate cochlear responses, then fine-tuning Wav2vec2.0 for semantic features, extracting residual vectors from HuBERT embeddings via Residual Vector Quantization in the Semantic Distortion Modeling (SDM) module, and finally fusing auditory and semantic features through residual cross-attention. During inference, the model prunes semantic features to reduce noise while maintaining performance. The system predicts MOS scores through multimodal fusion guided by biologically inspired auditory processing.

## Key Results
- System-level SRCC scores of 0.936 on BVCC and 0.921 on SOMOS datasets
- Outperforms existing methods including MSP, WAIC, and Conformer-BERT based models
- Requires fewer parameters than competing approaches while achieving superior performance
- Ablation studies confirm the effectiveness of auditory-guided semantic fusion and progressive training strategy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard spectral features with biologically simulated cochlear inputs may improve the model's ability to detect fine-grained distortions that humans perceive.
- **Mechanism:** The APM module simulates the cochlea's mechanoelectrical transduction using Gammatone filters, half-wave rectification, and cube-root compression to create a cochleagram that retains different details than standard spectrograms.
- **Core assumption:** Biophysical signal transformations provide a richer feature space for quality assessment than Fourier-based transforms.
- **Evidence anchors:** APM+D outperforms Mel+D significantly on BVCC (0.872 vs 0.825 SRCC); related work suggests bio-mimetic architectures can replicate human cortical signatures.

### Mechanism 2
- **Claim:** Quantifying the deviation from standard semantic units (residuals) serves as a proxy for speech naturalness and quality degradation.
- **Mechanism:** The SDM module calculates residual vectors between raw HuBERT embeddings and their quantized versions, where large residuals indicate unnatural deviations or distortions.
- **Core assumption:** Pre-trained quantization codebooks define "high quality," and distance from this space correlates with "low quality."
- **Evidence anchors:** SDM+D significantly outperforms Quantized HuBERT, validating that residuals are more predictive of quality than semantic content itself.

### Mechanism 3
- **Claim:** Using auditory features to guide the weighting of semantic features via cross-attention improves fusion compared to simple concatenation.
- **Mechanism:** The JCM uses residual cross-attention where projected auditory features act as Query and Wav2vec2 semantic features act as Key/Value, forcing semantic representation to be weighted by "what the ear hears."
- **Core assumption:** A hierarchical structure (Auditory → Semantic) mimics human cognitive pathways better than parallel processing.
- **Evidence anchors:** w/o CA shows performance drop, confirming utility of attention fusion over simple concatenation.

## Foundational Learning

- **Concept: Gammatone Filters & ERB Scale**
  - **Why needed here:** Front-end replacement for Mel-spectrogram; required to debug APM module
  - **Quick check question:** How does the non-linear frequency resolution of the ERB scale differ from the Mel scale, and why does this paper argue it better simulates the cochlear base-to-apex gradient?

- **Concept: Residual Vector Quantization (RVQ)**
  - **Why needed here:** SDM module relies entirely on RVQ properties; must understand how residuals are calculated iteratively
  - **Quick check question:** Why is the residual vector from the first VQ layer considered a better predictor of quality degradation than the quantized vector itself?

- **Concept: Cross-Attention & "Query" Semantics**
  - **Why needed here:** Specific data flow where Auditory guides Semantic; flipping this changes causal logic
  - **Quick check question:** In the cross-attention block, which modality serves as the Query, and what does this imply about the model's information bottleneck?

## Architecture Onboarding

- **Component map:** Raw Audio → Gammatone Filterbank → Rectify/Compress → ECAPA-TDNN → $X_{aud}$ (Auditory); Audio → HuBERT → RVQ (Frozen) → Calculate Residuals → $X_{sem}$ (Semantic); Audio → Wav2vec 2.0 (Fine-tuned) → $X_{w2v}$ (Semantic); Project $X_{aud}$, concat with $X_{sem}$ to form $X_{uni}$ (Query); Cross-Attention($X_{uni}$, $X_{w2v}$) → Decoder

- **Critical path:** Progressive learning strategy is most critical constraint; cannot train full stack end-to-end immediately
  1. Train APM from scratch
  2. Fine-tune W2V
  3. Freeze APM, SDM, W2V
  4. Train Cross-Attention + Decoder

- **Design tradeoffs:**
  - Inference Pruning: SDM features discarded during inference, using only projected APM features as queries
  - Tradeoff: Add complexity during training to shape projection layer, remove it at inference to reduce noise and compute
  - Risk: Incorrect implementation including SDM features at inference degrades performance

- **Failure signatures:**
  - High MSE, Low SRCC: Check Ranking Loss implementation or missing Cross-Attention mask
  - Overfitting on small datasets: Verify frozen components are truly frozen during JCM training stage

- **First 3 experiments:**
  1. APM Baseline: Train only APM branch with decoder; compare against Mel-spectrogram + ECAPA-TDNN baseline
  2. Ablation on Inference Strategy: Run full model inference with and without pruning SDM features
  3. Cross-Attention vs. Concat: Replace cross-attention with simple concatenation to validate guiding mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent would integrating a dynamic model of outer hair cell active amplification improve biological fidelity and predictive accuracy?
- Basis in paper: [explicit] Current implementation uses static 3x gain factor to compensate for omitted outer hair cell mechanisms
- Why unresolved: Current model simplifies complex biological process to fixed gain, potentially missing dynamic non-linearities
- What evidence would resolve it: Comparative experiments replacing static gain with dynamic cochlear amplification model

### Open Question 2
- Question: Can SDM module be refined to generate beneficial inference features, eliminating need for progressive learning strategy?
- Basis in paper: [inferred] Authors observe SDM features introduce quantization noise during inference, leading to pruning design
- Why unresolved: Current architecture treats SDM features as training regularizer rather than inference signal
- What evidence would resolve it: Ablation studies showing denoised or re-weighted SDM features improve system-level SRCC when included during inference

### Open Question 3
- Question: How robust is the architecture when applied to languages with different phonetic structures or low-resource settings?
- Basis in paper: [inferred] Study evaluates exclusively on English datasets (BVCC and SOMOS)
- Why unresolved: While auditory module is biologically universal, semantic module relies on English-centric SSL models
- What evidence would resolve it: Cross-lingual evaluation results on tonal languages or low-resource languages using multilingual SSL backbones

## Limitations
- Major uncertainties from unspecified hyperparameters (batch size, RVQ checkpoint specifics, cross-attention depth) make exact reproduction challenging
- Inference pruning strategy is non-standard and could introduce instability if misimplemented
- Generalization to unseen distortions or lower-resource conditions remains untested
- Biophysical realism of cochlear simulation is asserted but not validated against human perceptual data

## Confidence
- **High confidence:** Superiority over existing methods on BVCC and SOMOS datasets (measured by SRCC scores and parameter efficiency)
- **Medium confidence:** Specific mechanisms of auditory-guided fusion and semantic distortion modeling improving quality assessment
- **Low confidence:** Exact implementation details required for faithful reproduction (training epochs, batch sizes, RVQ model specifics)

## Next Checks
1. Implement the full 4-stage progressive training pipeline and verify each stage converges independently before proceeding to the next
2. Conduct ablation study comparing inference with vs. without SDM feature pruning to confirm discarding semantic residuals improves performance
3. Test model's robustness by evaluating on a third, previously unseen TTS/VC dataset to assess generalization beyond BVCC and SOMOS