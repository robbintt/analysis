---
ver: rpa2
title: Attacks on fairness in Federated Learning
arxiv_id: '2311.12715'
source_url: https://arxiv.org/abs/2311.12715
tags:
- learning
- fairness
- attack
- attacks
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces attacks on fairness in Federated Learning
  (FL), where an attacker controls a small subset of clients to influence the aggregated
  model's accuracy distribution across different attributes. The core method involves
  computing malicious updates that, when aggregated with clean updates, produce a
  target update biasing the model toward specific attributes.
---

# Attacks on fairness in Federated Learning

## Quick Facts
- **arXiv ID**: 2311.12715
- **Source URL**: https://arxiv.org/abs/2311.12715
- **Reference count**: 38
- **Primary result**: An attacker controlling a single client out of 3, 10, or 30 can bias a federated model's accuracy distribution across classes, achieving 89.65% accuracy on targeted classes while dropping others to 0.06%.

## Executive Summary
This paper introduces a novel attack on fairness in Federated Learning where a malicious client controls a small subset of participants to manipulate the aggregated model's accuracy distribution across different attributes. The attack exploits the FedAvg aggregation mechanism by computing malicious updates that, when combined with clean updates, produce a target update biasing the model toward specific attributes. Experimental results on CIFAR-10 demonstrate that the attack is highly effective, creating extreme accuracy disparities between targeted and non-targeted classes. The attack can be applied to already-trained models and remains successful even when the attacker controls only a single client.

## Method Summary
The attack works by having the malicious client estimate the collective update from benign clients and then computing an update that counteracts this estimate while pushing the model toward a target direction. This is achieved by training locally on a dataset containing only the desired target attributes to generate a target update vector, then solving an optimization problem to craft a malicious update that, when aggregated with the estimated benign updates via FedAvg, produces the desired bias. The attack is tested in various client configurations (3, 10, and 30 clients with one malicious) and can be introduced at different points during training.

## Key Results
- After introducing the attack in a 3-client setup, targeted classes achieved 89.65% accuracy while other classes dropped to 0.06%
- The attack remains successful when introduced at round 80 in already-trained models
- In 10-client and 30-client setups, the malicious client still successfully biases the model toward target attributes
- No additional unlearning procedures were necessary to achieve the attack's effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Update Amplification via Aggregation Estimation
A malicious client can disproportionately influence the global model by computing updates that counteract the expected aggregate of benign updates. The attacker estimates the collective update vector from benign clients and solves an optimization problem to craft a malicious update that, when combined with these estimated benign updates via FedAvg, produces a target update vector favoring specific attributes.

### Mechanism 2: Attribute-Specific Optimization (Target Update Computation)
By training a local model exclusively on a subset of data containing the desired attributes, the attacker generates a "target update" that improves model performance on those attributes while degrading it on others. The malicious client trains locally on a dataset composed only of the target classes/attributes, creating gradients that direct the model's weights toward better representation of these attributes.

### Mechanism 3: Denial of Information via "Unlearning"
The attack can degrade performance on non-target classes not just through neglect, but potentially through active "unlearning" of their representations. This involves using techniques like gradient ascent on the loss for those classes to push the model to forget them, though the paper found this generally unnecessary in their experiments.

## Foundational Learning

**FedAvg Aggregation**
- Why needed here: The attack explicitly exploits the weighted averaging logic of FedAvg to inject bias
- Quick check question: In FedAvg, how is the contribution of a client weighted relative to others?

**Fairness in ML (Attribute-Level)**
- Why needed here: This paper defines fairness as the distribution of accuracy across different attributes/classes
- Quick check question: What is the difference between attribute-level fairness and client-level fairness in an FL context?

**Threat Models in FL**
- Why needed here: The paper assumes a specific threat model where the attacker knows model weights/architecture
- Quick check question: In this paper's threat model, what key information does the attacker NOT know about other clients?

## Architecture Onboarding

**Component map:**
Malicious Client Module -> Benign Client Simulation -> Aggregation Server (FedAvg) -> Evaluation Harness

**Critical path:**
Setup datasets (global train/test, malicious client's biased subset) -> Initialize global model -> For each round: (Benign clients train & produce u_i) AND (Malicious client estimates u_i, computes target update m, solves for v via Eq. 1) -> Server aggregates updates via FedAvg -> Evaluate per-class accuracy

**Design tradeoffs:**
- Attack Strength vs. Stealth: Larger malicious updates (v) have a stronger biasing effect but may be easier to detect by magnitude-based defenses
- Estimation Accuracy vs. Attack Cost: More accurate prediction of benign updates (u_i) improves attack efficacy but requires better representative data and more computation

**Failure signatures:**
- Low Disparity: Per-class accuracies converge to similar values, indicating negligible effect
- Global Model Collapse: All class accuracies drop to near zero, suggesting instability
- Target Class Degradation: Accuracy on target classes drops, suggesting the computed v inadvertently harmed desired representations

**First 3 experiments:**
1. Baseline Reproduction (3 Clients): Replicate the "3 clients, 1 malicious" setup from Table 1 with ResNet-50 on CIFAR-10, introduce attack at round 0, and plot per-class accuracy
2. Attack Timing Ablation: Introduce the same attack at different rounds (e.g., round 10, 50, 80) on an already-trained model, measure how quickly fairness disparity appears
3. Defense Sensitivity Test: Implement norm-clipping defense at the aggregator, run the attack and vary clipping threshold, observe at what threshold effectiveness is reduced

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can existing backdoor defenses effectively mitigate fairness attacks without compromising the model's intended utility?
- Basis in paper: The conclusion states the authors would like to investigate the effects of common backdoor defenses on their proposed attack
- Why unresolved: The authors focused on introducing the attack vector and demonstrating its success against FedAvg
- What evidence would resolve it: Empirical results showing the attack's success rate against standard defenses like FLAME, redundant gradients, or differential privacy

**Open Question 2**
- Question: How does data heterogeneity (non-i.i.d. data) across clients impact the efficacy and stability of fairness attacks?
- Basis in paper: The methodology section notes that clean datasets were i.i.d. and states "Further testing would be necessary to investigate the effect of higher data heterogeneity"
- Why unresolved: The attack relies on predicting clean updates; non-i.i.d. data introduces variance that may make it harder to accurately estimate the benign gradient direction
- What evidence would resolve it: Experimental evaluation of the attack's success rate and convergence speed using Dirichlet distributions or other standard non-i.i.d. partitioning methods

**Open Question 3**
- Question: Is the fairness attack robust in environments with low client participation rates?
- Basis in paper: The authors simplified their simulation with full participation and explicitly list "reduced client participation rate" as an area requiring "Further testing"
- Why unresolved: The attack strategy assumes the malicious client can consistently influence the aggregation; infrequent sampling might prevent maintaining the bias
- What evidence would resolve it: Simulations where clients are randomly sampled per round, testing if a single malicious client can still dominate the aggregation over time

## Limitations
- No verified code repository exists for the proposed method, preventing direct reproduction of results
- Critical hyperparameters (learning rate, batch size, local epochs, number of training rounds) are not specified in the paper
- The exact method for estimating benign client updates is only described conceptually with missing implementation details
- The paper assumes a very specific threat model and does not address more realistic scenarios where the attacker has less knowledge about other clients' data distributions

## Confidence

**High Confidence**: The core mechanism of estimating and counteracting benign updates to bias the aggregated model is theoretically sound and aligns with known federated learning vulnerabilities.

**Medium Confidence**: Experimental results showing extreme accuracy disparity are plausible given the mechanism, but cannot be verified without code.

**Low Confidence**: The practical feasibility of the attack in more realistic threat models (less knowledge of benign updates, stronger defenses) is not explored.

## Next Checks

1. **Code Reproduction Attempt**: Implement the attack based on the paper's description and test on a smaller-scale CIFAR-10 experiment (e.g., 3 clients, 10 rounds) to verify the core mechanism produces the claimed accuracy disparity.

2. **Defense Sensitivity Analysis**: Implement and test basic defenses (norm clipping, coordinate-wise median) against the attack to understand at what threshold they become effective and what residual bias remains.

3. **Attack Knowledge Ablation**: Systematically reduce the attacker's knowledge of benign client data distributions in the simulation and measure how the attack's effectiveness degrades, to understand the practical limits of the threat.