---
ver: rpa2
title: Guiding Audio Editing with Audio Language Model
arxiv_id: '2509.21625'
source_url: https://arxiv.org/abs/2509.21625
tags:
- audio
- editing
- sound
- atomic
- smartdj
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SmartDJ introduces the first framework for declarative audio editing,
  combining an Audio Language Model (ALM) with a Latent Diffusion Model (LDM). The
  ALM interprets high-level user instructions and generates a sequence of atomic editing
  operations, which the LDM executes sequentially to produce edited stereo audio.
---

# Guiding Audio Editing with Audio Language Model

## Quick Facts
- arXiv ID: 2509.21625
- Source URL: https://arxiv.org/abs/2509.21625
- Reference count: 26
- SmartDJ introduces the first framework for declarative audio editing, achieving the highest CLAP score of 0.238 among evaluated methods.

## Executive Summary
SmartDJ presents the first framework for declarative audio editing, enabling users to specify high-level instructions for stereo audio modification without detailing procedural steps. The system combines an Audio Language Model (ALM) with a Latent Diffusion Model (LDM) to interpret instructions and execute atomic editing operations sequentially. Through a scalable data synthesis pipeline, SmartDJ achieves superior perceptual quality, spatial realism, and semantic alignment compared to prior methods, as demonstrated in both quantitative metrics and user studies.

## Method Summary
SmartDJ operates through a two-stage process: first, an Audio Language Model (ALM) interprets high-level instructions and generates a sequence of atomic editing operations; second, a Latent Diffusion Model (LDM) executes these operations sequentially on stereo audio latents. The system is trained on synthetic data generated by combining audio clips with GPT-4o-generated instructions and atomic steps, then rendered via signal processing. The ALM uses Audio Flamingo 2 with CLAP encoder adapters, while the LDM employs a stereo VAE and Diffusion Transformer conditioned on text embeddings. This separation enables modular, interpretable editing that preserves unedited content across sequential operations.

## Key Results
- SmartDJ achieves the highest CLAP score of 0.238 among evaluated methods, indicating superior semantic alignment between instructions and edited audio.
- User studies show consistent preference for SmartDJ across audio quality and alignment metrics, outperforming baselines by at least 77% for single-step tasks and 80% for complex editing.
- The framework demonstrates superior perceptual quality, spatial realism, and content preservation compared to prior approaches across Fréchet Distance, Log-Spectral Distance, and spatial metrics.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Separating high-level instruction interpretation (planning) from audio manipulation (execution) enables declarative audio editing, where users specify *what* they want rather than *how* to achieve it.
- **Mechanism**: The ALM takes the original audio (encoded as CLAP embeddings) and a high-level instruction, then generates a sequence of atomic editing steps in natural language (e.g., "Remove the sound of car engine" → "Add rooster crowing at right"). These steps are executed sequentially by the LDM, which conditions on each step's text and the previous audio latent to produce edited audio. This mirrors a "designer–composer" paradigm.
- **Core assumption**: High-level instructions can be decomposed into sequential, largely independent atomic operations; the ALM can ground its planning in audio content via CLAP embeddings without direct waveform access.
- **Evidence anchors**:
  - [abstract]: "Given a high-level instruction, SmartDJ decomposes it into a sequence of atomic edit operations, such as adding, removing, or spatially relocating events. These operations are then executed by a diffusion model trained to manipulate stereo audio."
  - [section 3.2]: "This separation of planning and editing transforms audio editing from a procedural task into a declarative one."
  - [corpus]: Ming-UniAudio (arXiv:2511.05516) notes representation discrepancy prevents speech LLMs from instruction-based editing—indirectly supporting the difficulty of end-to-end approaches, but not validating the separation strategy.
- **Break condition**: If atomic operations have complex interdependencies (e.g., removing sound A alters spatial perception of sound B), or if CLAP embeddings lack sufficient detail for grounding, the decomposition may yield conflicting or suboptimal edit sequences.

### Mechanism 2
- **Claim**: A scalable synthesis pipeline using GPT-4o as a "designer" and signal processing as a "composer" can generate sufficient training data for both the ALM planner and LDM editor.
- **Mechanism**: Audio clips with text labels are sampled from public datasets (AudioCaps, VGGSound, etc.). GPT-4o generates high-level instructions and corresponding atomic edit sequences based on these labels. A rule-based composer renders each edit by adjusting volume, direction, or mixing new clips, producing paired (instruction, atomic steps, before/after audio) examples.
- **Core assumption**: Synthetically generated edit sequences and rule-based compositions are representative enough of real-world editing tasks to generalize to natural audio and user instructions.
- **Evidence anchors**:
  - [abstract]: "To support this, we design a data synthesis pipeline that produces paired examples of high-level instructions, atomic edit operations, and audios before and after each edit operation."
  - [section 3.5]: "We feed these labels into GPT-4o and prompt it to act as a sound designer... It then decomposes P into a sequence of atomic edits S... If si modifies an existing event, we update its volume level or sound direction."
  - [corpus]: No direct corpus evidence on synthesis pipelines for audio editing; SAO-Instruct (arXiv:2510.22795) addresses free-form editing but doesn't discuss data generation.
- **Break condition**: If synthetic data lacks diversity (e.g., rare acoustic environments, overlapping frequency content) or GPT-4o's instruction phrasing diverges from real user behavior, models may overfit to synthetic patterns and fail on in-the-wild tasks.

### Mechanism 3
- **Claim**: A latent diffusion model conditioned on both text embeddings and the previous audio latent can perform high-quality, spatially coherent stereo edits while preserving unedited content across sequential operations.
- **Mechanism**: A stereo VAE encodes audio into latents (128 channels, 7.5× compression). At each edit step, a noised latent is concatenated with the previous step's latent and fed to a Diffusion Transformer (DiT) conditioned on FLAN-T5 text embeddings via cross-attention. Classifier-free guidance interpolates conditional/unconditional predictions during inference.
- **Core assumption**: Concatenating the previous latent with a noised latent enables "editing" rather than full regeneration, preserving unedited content across steps.
- **Evidence anchors**:
  - [abstract]: "Experiments show SmartDJ achieves superior perceptual quality, spatial realism, and semantic alignment compared to prior methods, with the highest CLAP score of 0.238."
  - [section 3.4]: "At each editing step, we initialize a randomly noised latent... concatenated with â_{i-1} to form the input... The model is conditioned on E_text(s_i) via cross-attention layers."
  - [corpus]: REED-VAE (arXiv:2504.18989) notes iterative editing with diffusion accumulates artifacts due to repeated latent-space transitions—highlighting a known challenge this mechanism attempts to address.
- **Break condition**: If latent concatenation fails to preserve fine-grained details (e.g., phase for spatial cues) or if sequential denoising accumulates error, edited audio may drift from original content or lose spatial coherence.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - **Why needed here**: The LDM is the core editor; understanding diffusion in latent space vs. raw waveforms explains efficiency and quality tradeoffs.
  - **Quick check question**: Why operate on latents rather than waveforms directly? (Answer: Latent space is more compact and semantically structured, enabling efficient training/inference while maintaining fidelity.)

- **Concept: Audio Language Models (ALMs) and CLAP Embeddings**
  - **Why needed here**: The ALM's ability to ground language in audio depends on how audio is represented (CLAP) and injected into the LLM.
  - **Quick check question**: How does the ALM receive audio information? (Answer: Audio is encoded by a pretrained CLAP encoder into embeddings, injected via adapter layers.)

- **Concept: Stereo Audio and Spatial Cues**
  - **Why needed here**: SmartDJ targets stereo with spatial effects; understanding interaural differences explains why mono models fail immersive tasks.
  - **Quick check question**: What spatial information is lost in mono audio? (Answer: Interaural time and level differences—primary cues for sound localization.)

## Architecture Onboarding

- **Component map**:
  - Audio + Instruction → ALM (Audio Flamingo 2 with CLAP encoder) → Atomic Edit Steps → LDM (DiT + Stereo VAE) → Edited Audio

- **Critical path**: Instruction + audio → ALM (~4.8s) → atomic steps → sequential LDM edits (~2.4s/step) → final audio (~13.1s total for complex edits).

- **Design tradeoffs**:
  - Separate vs. end-to-end training: Enables human-in-the-loop editing and modularity but may miss joint optimization gains.
  - Sequential vs. one-shot editing: Fine-grained control but higher inference cost; ablation shows order has minimal impact.
  - Synthetic vs. real data: Scalable but may limit generalization; new operations require LDM retraining.

- **Failure signatures**:
  - **Content drift**: LSD increases over edit rounds if latents don't preserve unedited content.
  - **Spatial cue loss**: Phase-discarding encoders (e.g., mel-spectrograms) fail spatial tasks.
  - **ALM misplanning**: Weak audio grounding leads to inappropriate atomic steps.

- **First 3 experiments**:
  1. **Round-trip editing test**: "Add A" then "Remove A" for 5 rounds; measure LSD from original—validates content preservation.
  2. **Single-step ablation by operation**: Compare FD, FAD, CLAP, spatial metrics per atomic operation—identifies per-operation strengths.
  3. **ALM removal ablation**: Train LDM end-to-end without ALM on (instruction, final audio) pairs—isolates planning contribution.

## Open Questions the Paper Calls Out

- **End-to-end joint training**: The paper identifies implementing an end-to-end joint training strategy of ALM and LDM as a future direction, which could combine reasoning with audio editing optimization.
- **New task-specific operations**: Supporting new editing operations on the LDM currently requires full model retraining, limiting adaptability to novel user requirements.

## Limitations

- The reliance on synthetic data generated by GPT-4o and rule-based composition may limit real-world generalization, particularly for rare acoustic environments and complex interdependencies.
- Architectural details of the stereo VAE and specific CLAP encoder variant are underspecified, potentially affecting reproducibility.
- Sequential editing's cumulative error propagation is acknowledged but not quantitatively analyzed beyond basic content preservation metrics.

## Confidence

- **High confidence**: The declarative editing paradigm separation (planning vs. execution) and the sequential LDM approach for atomic operations are well-supported by experimental results and ablation studies.
- **Medium confidence**: The data synthesis pipeline's scalability and the synthetic data's representativeness for real-world generalization, given the lack of direct evaluation on non-synthetic instructions.
- **Low confidence**: The system's performance on highly complex, interdependent editing tasks where atomic operations have non-trivial interactions, as experiments focus on simpler decompositions.

## Next Checks

1. Test SmartDJ on a held-out set of real user instructions (not from the synthetic pipeline) to measure actual generalization gap.
2. Perform a systematic ablation varying the number of atomic steps per high-level instruction to quantify the sweet spot before perceptual quality degrades.
3. Evaluate spatial accuracy with phase-sensitive metrics (e.g., interaural cross-correlation) on complex multi-source scenes to validate the stereo VAE's spatial preservation capability.