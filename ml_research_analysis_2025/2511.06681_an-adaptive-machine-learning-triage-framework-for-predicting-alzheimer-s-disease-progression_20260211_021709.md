---
ver: rpa2
title: An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease
  Progression
arxiv_id: '2511.06681'
source_url: https://arxiv.org/abs/2511.06681
tags:
- triage
- advanced
- basic
- framework
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cost-accuracy dilemma in predicting Alzheimer's
  disease progression from mild cognitive impairment by proposing a two-stage machine
  learning framework that selectively obtains advanced testing features. The method
  employs a basic model for initial screening, an advanced model using costly features
  like PET scans and CSF biomarkers, and a triage model that decides when advanced
  testing is warranted based on expected diagnostic value.
---

# An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease Progression

## Quick Facts
- **arXiv ID**: 2511.06681
- **Source URL**: https://arxiv.org/abs/2511.06681
- **Reference count**: 24
- **Primary result**: Achieves AUROC of 0.929 while reducing advanced testing needs by 20% through selective escalation

## Executive Summary
This paper addresses the cost-accuracy dilemma in predicting Alzheimer's disease progression from mild cognitive impairment by proposing a two-stage machine learning framework that selectively obtains advanced testing features. The method employs a basic model for initial screening, an advanced model using costly features like PET scans and CSF biomarkers, and a triage model that decides when advanced testing is warranted based on expected diagnostic value. Applied to the ADNI dataset, the framework achieves an AUROC of 0.929 while reducing advanced testing needs by 20%, comparable to using all features (AUROC=0.915, p=0.1010). The triage decisions are interpretable through SHAP analysis, showing they align with clinical risk factors like cognitive scores and APOE4 status. This work demonstrates a practical approach to balancing diagnostic accuracy with testing costs in real-world clinical settings.

## Method Summary
The framework trains three models: a Basic Model (logistic regression) using clinical features only, an Advanced Model (logistic regression) using both clinical and costly imaging/biomarker features, and a Triage Model (SVM with RBF kernel) that predicts whether advanced testing will meaningfully improve diagnostic certainty. The triage model is trained on a binary label indicating whether the Advanced Model improves certainty by more than a threshold margin compared to the Basic Model, using 5-fold cross-validation to prevent data leakage. During inference, the Triage Model evaluates each patient and only escalates to the Advanced Model when it predicts a meaningful certainty gain. The system selects an operating threshold from a risk-coverage curve to balance cost savings against diagnostic accuracy.

## Key Results
- Achieves AUROC of 0.929 with 20% reduction in advanced testing needs
- Triage model maintains diagnostic performance comparable to full-feature approach (AUROC=0.915, p=0.1010)
- SHAP analysis reveals triage decisions align with clinical risk factors (cognitive scores, APOE4 status)
- Risk-coverage curve identifies inflection point at 19% coverage with 8.0% error rate

## Why This Works (Mechanism)

### Mechanism 1: Certainty-Gain Deferral (Selective Escalation)
The triage model identifies patients for whom basic clinical data provides insufficient diagnostic confidence, selectively deferring to expensive advanced testing only when the "value of information" is high. The Triage Model learns to recognize patterns in basic features that suggest the Advanced Model would significantly increase certainty. The framework trains a Triage Model to predict a binary label indicating whether certainty gain exceeds a threshold. Core assumption: certainty gain observed retrospectively correlates with actual diagnostic value. Break condition: if basic features cannot predict incremental value of advanced features, triage model fails.

### Mechanism 2: Risk-Coverage Calibration
The system can explicitly trade off "coverage" (percentage of patients handled cheaply) against "risk" (error rate of non-escalated group) by tuning an escalation threshold. The framework generates a risk-coverage curve and selects an operating point to isolate a low-risk subset for Basic Model handling. Core assumption: significant subset exists where Basic Model certainty is already high enough that advanced testing provides marginal utility. Break condition: if cost of False Negative for non-escalated patient is extremely high, calibration may require lower coverage threshold, negating cost savings.

### Mechanism 3: Post-hoc Interpretability via Feature Attribution
SHAP analysis provides clinically coherent explanations for resource allocation, fostering trust in triage decisions. The Triage Model uses basic features, and SHAP values expose which features pushed escalation score up or down. Core assumption: feature importance derived from triage decision aligns with clinical reasoning regarding diagnostic ambiguity. Break condition: if SHAP highlights features clinicians view as spurious, explanation may increase confusion rather than trust.

## Foundational Learning

- **Concept: Cascaded Machine Learning Systems**
  - Why needed here: This is not a single model but a cascade (Basic -> Triage -> Advanced). Understanding how to manage data flow is critical.
  - Quick check question: In this framework, does the Advanced Model ever see a patient rejected by the Triage Model during inference? (Answer: No).

- **Concept: Value of Information (VOI)**
  - Why needed here: The Triage Model predicts the utility of acquiring more data, reframing the problem from "What is the diagnosis?" to "Will a new test change our confidence?"
  - Quick check question: How does the paper define "certainty" mathematically? (Answer: c = |p - 0.5|).

- **Concept: Data Leakage in Label Generation**
  - Why needed here: The Triage Model requires ground-truth labels that depend on Basic/Advanced model outputs. Using the same data to train predictors and generate labels causes overfitting.
  - Quick check question: How did authors generate labels z for Triage Model to prevent leakage? (Answer: Using 5-fold cross-validation predictions).

## Architecture Onboarding

- **Component map**: Input (Basic Features) -> Basic Model -> Triage Model -> Conditional Branch -> (Advanced Features) -> Advanced Model -> Final Risk Output

- **Critical path**: Training phase is critical for correctness. Train Basic/Advanced models, generate "Certainty Gain" labels using Cross-Validation (Crucial: do not use training set predictions), train Triage Model on Basic Features using labels.

- **Design tradeoffs**: 
  - Threshold τ: Low τ = escalate more often (higher cost, lower risk). High τ = escalate less (lower cost, higher risk of missing ambiguous cases).
  - Margin δ: Defines what counts as "meaningful" gain in certainty. Higher δ creates stricter Triage Model that escalates less frequently.

- **Failure signatures**:
  - Triage Collapse: Triage Model learns to always predict "escalate" or "never escalate". Check distribution of training labels z.
  - Leakage Overfit: Triage Model reports 99% accuracy on training data but fails on test data. Check if CV was used for label generation.
  - Calibration Drift: High error rate in "non-escalated" group. Check Risk-Coverage curve; τ may be too aggressive.

- **First 3 experiments**:
  1. Baseline Validation: Replicate AUROC of Basic (0.791) and Advanced (0.915) models on provided test split to verify data integrity.
  2. Triage Label Generation: Implement logic z = 1[|p_a-0.5| - |p_b-0.5| > 0.2] using 5-fold CV. Verify distribution of z is not heavily imbalanced.
  3. Cost-AUROC Curve: Sweep threshold τ from 0 to 1 and plot system-wide AUROC against "Percentage of Patients Escalated" to find optimal operating point.

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning (RL) effectively optimize a multi-step diagnostic pathway when distinct advanced modalities (e.g., CSF vs. PET) are treated as separate actions rather than a single combined block? The current framework treats all advanced features as a single, monolithic escalation step, masking potential cost savings of ordering only specific tests. Evidence needed: RL agent simulation on ADNI demonstrating higher utility (accuracy per dollar) by selecting specific tests compared to current binary triage model.

### Open Question 2
Does incorporating longitudinal data (time-series analysis) improve the actionability and timeliness of the triage decision compared to the static, baseline-only approach? The current model relies on static snapshots, potentially missing rapid decliners who appear stable at baseline but progress quickly. Evidence needed: Comparative study showing time-series models trigger escalations earlier (e.g., 6 months prior) with equal or higher precision than static model.

### Open Question 3
Do the SHAP-based explanations for the Triage Model improve clinician trust and adoption rates in real-world clinical workflows? While authors provide example explanations, they do not validate if clinicians actually find them persuasive or useful for "clinician-in-the-loop" decision-making. Evidence needed: Results from user study where clinicians interact with system, reporting changes in confidence levels or corrections based on provided feature attributions.

## Limitations
- Triage Model's effectiveness depends on strength of correlation between basic features and certainty gain, which may vary across datasets
- Framework's cost-effectiveness depends on relative costs of Basic versus Advanced testing, which vary by healthcare system and are not explicitly modeled
- While framework reduces advanced testing needs by 20%, absolute cost savings depend on testing availability and reimbursement policies not addressed

## Confidence

- **High Confidence**: Basic and advanced model architectures (logistic regression) are well-established for AD prediction, and reported AUROC values (0.915 for full model) align with literature benchmarks
- **Medium Confidence**: Triage mechanism's effectiveness relies on strength of correlation between basic features and certainty gain, which is demonstrated but could vary across datasets
- **Medium Confidence**: Interpretability claims via SHAP are supported by examples, but clinical validity of explanations across diverse populations remains to be fully established

## Next Checks

1. **Label Generation Validation**: Verify that 5-fold CV approach for generating triage labels prevents data leakage and produces balanced distribution of escalation decisions
2. **Threshold Sensitivity Analysis**: Systematically vary escalation threshold τ to quantify trade-off between cost reduction and diagnostic accuracy across different operating points
3. **External Dataset Validation**: Test framework on independent MCI cohort (e.g., AIBL, OASIS) to assess generalizability and confirm triage model's effectiveness beyond ADNI