---
ver: rpa2
title: 'SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory
  Graph'
arxiv_id: '2510.20022'
source_url: https://arxiv.org/abs/2510.20022
tags:
- salt
- advantage
- grpo
- agents
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALT addresses the challenge of credit assignment in long-horizon
  LLM agents trained via group-based RL by providing step-level advantages derived
  from outcome rewards. It constructs a trajectory graph to distinguish shared and
  distinct steps across multiple rollouts, then refines advantages by averaging those
  from shared steps and preserving trajectory-specific advantages for distinct ones.
---

# SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph

## Quick Facts
- arXiv ID: 2510.20022
- Source URL: https://arxiv.org/abs/2510.20022
- Reference count: 23
- Primary result: SALT improves step-level credit assignment in long-horizon LLM agents, achieving 4.2pp gain on WebShop 1.5B and 4.7-6.7pp gains on AppWorld tasks when integrated with group-based RL.

## Executive Summary
SALT addresses the challenge of credit assignment in long-horizon LLM agents trained via group-based RL by providing step-level advantages derived from outcome rewards. It constructs a trajectory graph to distinguish shared and distinct steps across multiple rollouts, then refines advantages by averaging those from shared steps and preserving trajectory-specific advantages for distinct ones. Experiments on ALFWorld, WebShop, and AppWorld with various model sizes show consistent improvements when SALT is integrated with GRPO or RLOO.

## Method Summary
SALT constructs a trajectory graph from G rollouts by merging steps with identical (state history, action, next state) tuples across trajectories and preserving distinct steps. State is defined as the last h observation-action pairs to capture context. For merged edges, advantages are averaged across trajectories; for divergent edges, original trajectory-level advantages are preserved. The refined step-level advantages are then used in the standard GRPO/RLOO policy update. The method requires no additional supervision and introduces negligible computational overhead.

## Key Results
- WebShop: RLOO+SALT improves 1.5B model success rate by 4.2pp
- AppWorld: GRPO+SALT boosts Test-N TGC by 4.7pp and Test-C TGC by 6.7pp
- Performance gains scale with group size and are optimal at h=2-3 history length

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Graph Disambiguates Neutral vs Decisive Steps
Constructing a directed acyclic graph from multiple rollouts enables identification of steps that are causally neutral (appear in both success/failure paths) versus decisively good or bad. Apply merge operation when (starting_state, action, resulting_state) are identical across trajectories—these shared edges receive averaged advantages. Apply diverge operation when any component differs—these distinct edges retain their trajectory-specific advantages.

### Mechanism 2: Gradient Conflict Reduction via Advantage Averaging
Averaging advantages for merged steps reduces gradient conflicts caused by inconsistent trajectory-level reward assignment to identical behaviors. For merged edge sets E_i = {a_1, a_2, ..., a_{n_i}} with advantages {Â_1, ..., Â_{n_i}}, reassign all to the group mean: Â'_1 = Â'_2 = ... = Â'_{n_i} = (1/n_i)∑_{j=1}^{n_i} Â_j.

### Mechanism 3: History-Conditioned State Matching Captures Contextual Semantics
Defining state as a window of h prior observation-action pairs enables context-aware merging that respects sequential dependencies. State s_t = {a_{t-h+1}, o_{t-h+1}, ..., a_t, o_t} rather than just o_t. Larger h enforces stricter matching (fewer merges); smaller h permits more flexible matching (more merges).

## Foundational Learning

**Concept: Group-based RL (GRPO/RLOO)**
Why needed: SALT is architected as a plug-in for group-based algorithms; understanding their trajectory-level advantage computation clarifies what SALT modifies.
Quick check: How does GRPO estimate advantages without a critic model? (Answer: By normalizing trajectory rewards against the group mean and standard deviation.)

**Concept: Credit Assignment Problem**
Why needed: The fundamental issue SALT addresses is that sparse outcome rewards cannot distinguish good from bad intermediate actions in long horizons.
Quick check: In a 15-step WebShop task where only the final reward is observed, what does vanilla GRPO assign to each step? (Answer: The same trajectory-level advantage to all 15 steps regardless of individual quality.)

**Concept: Clipped Surrogate Objective (PPO-style)**
Why needed: SALT produces refined step-level advantages that are consumed by the standard clipped objective; understanding this clarifies where SALT plugs in.
Quick check: What role does the advantage term Â_t play in the PPO/GRPO update? (Answer: It weights how much to increase or decrease the probability of each action.)

## Architecture Onboarding

**Component map:**
Rollout G trajectories → Compute trajectory-level advantages (GRPO baseline) → Graph Constructor: Build DAG via merge/diverge on (state, action, next_state) → Advantage Refiner: For merged edges → average; for divergent → preserve → Policy Update: Feed step-level advantages to clipped objective

**Critical path:**
The graph construction and advantage refinement occur between reward computation and policy update—no changes to rollout or reward logic.

**Design tradeoffs:**
- History length h: Low (h=1) → aggressive merging may introduce noise; High (h=5) → insufficient merging, reverts to trajectory-level. Paper finds h=2–3 optimal.
- Group size G: Small (G=4) → limited diversity, SALT may hurt; Large (G≥8) → richer graph structure, SALT helps more.
- Matching strategy: Exact string matching for discrete environments (ALFWorld, WebShop); embedding similarity (>0.8 cosine) for continuous text spaces (AppWorld).

**Failure signatures:**
- Performance at h=1 drops below baseline → over-merging is corrupting advantage signals.
- SALT underperforms at G=4 but improves at G≥8 → insufficient trajectory diversity for meaningful graph.
- Merge rate →0% over training → degenerating to trajectory-level (h may be too strict).

**First 3 experiments:**
1. Sanity check: Replicate GRPO+SALT vs GRPO on ALFWorld with Qwen-1.5B, h=3, G=8; expect +3–5pp improvement on overall success rate.
2. Hyperparameter sweep: Vary h∈{1,2,3,4,5} while holding all else constant; should observe inverted-U with peak at h=2–3 and degradation at extremes.
3. Group size ablation: Test G∈{4,8,12,16}; expect SALT benefit to scale with group size as graph connectivity improves.

## Open Questions the Paper Calls Out

**Open Question 1**
Can SALT be effectively applied to continuous textual environments without relying on external embedding models or clustering mechanisms?
Basis: The Limitations section notes that for environments like AppWorld with continuous textual spaces, "additional components, such as embedding models or clustering mechanisms, are necessary to meaningfully construct the trajectory graph."
Unresolved because: The current implementation relies on exact matching or pre-defined embeddings for graph construction, which may not scale or generalize to unstructured text without these extra components.

**Open Question 2**
How does SALT interact with more advanced group-based policy optimization algorithms like DAPO or GSPO?
Basis: The authors state in the Limitations section: "Its compatibility with more advanced group-based policy optimization methods (e.g., DAPO, GSPO) has not yet been explored."
Unresolved because: SALT has only been validated as a plug-in for RLOO and GRPO; its behavior with algorithms that have different advantage normalization or regularization techniques remains unknown.

**Open Question 3**
Why does SALT degrade performance for RLOO on the 7B WebShop model, and can this negative scaling interaction be corrected?
Basis: Table 1 shows RLOO+SALT success rate drops from 76.8% to 75.2% on WebShop with the 7B model, while improving the 1.5B model; the text suggests SALT "may interact differently with larger-model optimization dynamics."
Unresolved because: The paper identifies the performance drop but does not investigate the specific optimization conflicts causing larger models to fail while smaller ones succeed.

## Limitations

- Performance degrades with small group sizes (G=4) due to insufficient trajectory diversity for meaningful merge patterns
- Narrow optimal range for history length h (2-3), with h=1 causing over-merging noise and h=5 causing under-merging
- For continuous textual environments like AppWorld, requires external embedding models or clustering mechanisms
- Assumes identical state-action-next_state triples across trajectories represent semantically neutral steps, which may not hold in all contexts

## Confidence

- **High confidence**: The core mechanism of trajectory graph construction and advantage averaging for merged edges is well-specified and theoretically sound.
- **Medium confidence**: The history-conditioned state matching provides contextual awareness, but the optimal h range is narrow and task-dependent.
- **Low confidence**: The claim that identical steps appearing across trajectories are "neutral or required" assumes uniform semantic meaning across contexts, which may not hold in all environments.

## Next Checks

1. **Gradient conflict analysis**: Measure the variance of advantages assigned to identical steps across trajectories before and after SALT application. If SALT reduces this variance as claimed, we should see a statistically significant decrease in advantage disagreement for merged steps.

2. **Context dependency test**: Design a controlled experiment where the same observation-action pair appears in semantically different contexts (e.g., early vs late in task completion). Verify that SALT's history-conditioned matching correctly distinguishes these cases while trajectory-level GRPO does not.

3. **Resource efficiency evaluation**: Measure the computational overhead of SALT relative to vanilla GRPO at different group sizes (G=4,8,16) and history lengths (h=1,2,3,4,5). Quantify the trade-off between performance improvement and additional computation required for graph construction and advantage refinement.