---
ver: rpa2
title: 'OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs'
arxiv_id: '2511.13147'
source_url: https://arxiv.org/abs/2511.13147
tags:
- fine-tuning
- sefp
- quantization
- bit-widths
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OTARo enables once fine-tuning of LLMs to support all quantization
  precisions, addressing the limitation of conventional quantization that prevents
  precision switching across bit-widths. The method introduces Shared Exponent Floating
  Point (SEFP), which eliminates scaling factors and allows flexible precision switching
  through simple mantissa truncation.
---

# OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs

## Quick Facts
- arXiv ID: 2511.13147
- Source URL: https://arxiv.org/abs/2511.13147
- Reference count: 11
- Key outcome: Enables once fine-tuning of LLMs to support all quantization precisions via SEFP, achieving 64.09% accuracy at E5M8 and 60.67% at E5M3 while reducing memory by 69% and improving throughput by 2.45×

## Executive Summary
OTARo introduces a novel approach for on-device LLM deployment by enabling a single model to support multiple quantization bit-widths at inference time. The method employs Shared Exponent Floating Point (SEFP) quantization, which eliminates scaling factors and allows bit-width switching through simple mantissa truncation. To achieve bit-width robustness, OTARo combines a Bit-Width Path Search (BPS) for optimal sampling and Low-Precision Asynchronous Accumulation (LAA) to mitigate gradient oscillations during low-precision training.

## Method Summary
OTARo fine-tunes LLMs using SEFP quantization where parameters are grouped (default 64) and share a single exponent (the maximum in the group). Individual mantissas are right-shifted to align with this shared exponent, and lower precision is achieved by truncating mantissa bits from the right. During training, BPS dynamically selects bit-widths using an exploration-exploitation score, while LAA accumulates gradients over N=10 batches for ultra-low precisions to reduce quantization noise. The method uses SGD optimizer with learning rate 1e-5 and STE for gradient approximation.

## Key Results
- Achieves 64.09% average accuracy across benchmarks at E5M8 and 60.67% at E5M3
- Reduces memory consumption by 69% compared to FP16
- Improves decoding throughput by 2.45×
- Outperforms baselines across all tested precisions (E5M8 to E5M3)

## Why This Works (Mechanism)

### Mechanism 1: Shared Exponent Floating Point (SEFP) Enables Cross-Precision Compatibility
SEFP eliminates bit-width-specific scaling factors by having parameter groups share one exponent. Lower precision is achieved by truncating mantissa bits from the right—no rescaling required. This enables a single model to support multiple bit-widths at inference time through simple truncation.

### Mechanism 2: Gradient Commonality Enables Unified Multi-Precision Learning
Cosine similarity analysis shows gradients at different bit-widths share directional structure, with higher bit-widths showing stronger alignment (e.g., E5M5-to-E5M8 similarity ≈ 0.97 vs. E5M5-to-E5M3 ≈ 0.72). This shared subspace allows updates computed at one precision to transfer to others.

### Mechanism 3: Asynchronous Accumulation Dampens Quantization Noise
Accumulating gradients over N batches reduces the relative influence of quantization-induced perturbations by approximately 1/√N. Since SEFP quantization introduces perturbations with E[Y] ≈ 0, accumulated gradients improve signal-to-noise ratio as the signal grows linearly while perturbation magnitude grows as √N.

## Foundational Learning

- **Floating-Point Representation (sign, exponent, mantissa)**: Essential for understanding SEFP's bit allocation (E5M4 = 5 exponent + 4 mantissa + 1 sign = 10 bits total). Quick check: Given FP16 values [2.5, 0.125, 1.0], what is their shared exponent and how are mantissas adjusted?

- **Straight-Through Estimator (STE)**: Needed because mantissa truncation is non-differentiable. STE passes gradients through as if quantization were identity (∂Q/∂ω = 1), enabling backpropagation. Quick check: Why does STE approximate ∂Q/∂ω = 1 rather than 0, and what bias does this introduce?

- **Gradient Accumulation Basics**: OTARo accumulates gradients over N=10 batches before updating for low-precision regimes. Quick check: If gradients accumulate over 10 batches with learning rate η, what is the effective update magnitude compared to single-batch updates?

## Architecture Onboarding

- **Component map**: SEFP Quantization Layer -> BPS Scheduler -> LAA Accumulator -> Training Loop

- **Critical path**: 1) Initialize FP16 weights; 2) BPS selects bit-width b* via scoring; 3) Forward pass with SEFP at b*; 4) Backward pass with STE; 5) For ultra-low b*: accumulate in LAA buffer, update if full; 6) Else: standard update

- **Design tradeoffs**: Group size (64): smaller = more exponents = higher precision but more storage; λ (5): higher = more exploration of low bit-widths; N (10): higher = smoother gradients but slower convergence

- **Failure signatures**: Perplexity spikes at specific bit-widths (check BPS under-sampling); training divergence at E5M3 (LAA may be insufficient or learning rate too high); inconsistent results across runs (set seed for reproducibility)

- **First 3 experiments**: 1) Sanity check: Fine-tune LLaMA3.2-1B on WikiText2 with SEFP at fixed E5M6; compare perplexity to FP16 baseline; 2) BPS ablation: Run with uniform sampling vs. BPS; plot perplexity across all bit-widths; 3) LAA threshold test: At E5M3, compare N∈{1, 5, 10, 20}; plot gradient norm oscillation and final perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does OTARo's Low-Precision Asynchronous Accumulation (LAA) strategy function effectively with adaptive optimizers like Adam or AdamW?
- Basis in paper: [inferred] The "Implementation Details" explicitly state the use of the "SGD optimizer," avoiding the adaptive optimizers standard for LLM fine-tuning.
- Why unresolved: The LAA strategy delays weight updates to smooth gradient oscillations. It is unclear if accumulating gradients over N batches disrupts the second-moment estimation crucial for Adam's convergence, effectively decoupling the optimizer's state from the real-time gradient flow.
- What evidence would resolve it: Comparative experiments running OTARo on LLaMA3 using AdamW, analyzing convergence curves and final perplexity against the reported SGD results.

### Open Question 2
- Question: Does the Shared Exponent Floating Point (SEFP) format cause catastrophic precision loss in parameter groups containing outlier weights?
- Basis in paper: [inferred] The paper asserts that "impacts of mantissa overflow... is negligible" [Page 2], relying on a shared exponent for a group of 64 parameters.
- Why unresolved: Modern LLMs often contain specific "outlier channels" with magnitudes significantly larger than the mean. If a single weight in a group forces a large shared exponent, the mantissas of the other 63 smaller weights would be shifted right, potentially dropping crucial low-order bits and causing unacceptable quantization noise for the majority of the group.
- What evidence would resolve it: A layer-wise error analysis measuring the bit-shift distance for non-outlier weights in groups containing known activation outliers versus groups without them.

### Open Question 3
- Question: Can the Bit-Width Path Search (BPS) generalize to Mixture-of-Experts (MoE) architectures without gradient divergence?
- Basis in paper: [inferred] Experiments are restricted to dense Transformer models (LLaMA, Qwen) [Page 6].
- Why unresolved: MoE models possess sparse activation patterns and router parameters which may exhibit different gradient behaviors. The BPS strategy relies on the assumption that "gradient direction shows higher consistency with those of the higher ones" [Page 3]; this gradient commonality might not hold for the discrete, sparse updates characteristic of MoE routing, potentially causing the path search to fail to converge.
- What evidence would resolve it: Application of OTARo to a sparse MoE model (e.g., Mixtral or DeepSeek-MoE) to verify if the BPS scoring mechanism maintains robust performance across experts.

## Limitations
- Limited validation of gradient commonality assumption across different layers and architectures
- Uncertainty about LAA effectiveness with adaptive optimizers like AdamW
- Potential precision loss in SEFP when parameter groups contain outlier weights

## Confidence
- **High Confidence**: SEFP quantization mechanism and mantissa truncation enabling cross-precision compatibility
- **Medium Confidence**: BPS scoring mechanism effectiveness
- **Low Confidence**: Gradient commonality assumption and perturbation independence in LAA

## Next Checks
1. Analyze cosine similarity matrices across all layers (not just one) for LLaMA3-8B and compare with other architectures (Mistral, Qwen) to validate the gradient commonality assumption
2. Test LAA performance across varying dataset sizes (100 samples to full Alpaca) to determine when batch gradient independence breaks down and LAA becomes ineffective
3. Systematically measure overflow frequency and impact at E5M3/E5M4 by logging exponent clipping events and quantifying their contribution to total quantization error