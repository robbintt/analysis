---
ver: rpa2
title: Mining Legal Arguments to Study Judicial Formalism
arxiv_id: '2512.11374'
source_url: https://arxiv.org/abs/2512.11374
tags:
- legal
- argument
- arguments
- formalism
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops automated methods to analyze judicial reasoning
  in Czech Supreme Courts using natural language processing. The researchers created
  the MADON dataset of 272 decisions with expert annotations of 9,183 paragraphs across
  eight argument types and formalism labels.
---

# Mining Legal Arguments to Study Judicial Formalism

## Quick Facts
- arXiv ID: 2512.11374
- Source URL: https://arxiv.org/abs/2512.11374
- Reference count: 39
- Primary result: Automated NLP pipeline achieves 83.2% macro-F1 for detecting formalism in Czech Supreme Court decisions

## Executive Summary
This study develops automated methods to analyze judicial reasoning in Czech Supreme Courts using natural language processing. The researchers created the MADON dataset of 272 decisions with expert annotations of 9,183 paragraphs across eight argument types and formalism labels. Using continued pretraining of transformer models on Czech legal texts, they achieved strong performance: 82.6% macro-F1 for detecting argumentative paragraphs, 77.5% macro-F1 for classifying argument types, and 83.2% macro-F1 for classifying decisions as formalistic/non-formalistic. A three-stage pipeline combining ModernBERT, Llama 3.1, and feature-based machine learning provided both high performance and explainability while reducing computational costs. Empirical analysis challenged prevailing narratives about Central and Eastern European formalism, showing that Czech courts rarely rely on text-based arguments and instead frequently use case law and purposive reasoning. The methodology is easily replicable across jurisdictions.

## Method Summary
The methodology employs a three-stage pipeline for legal argument mining: Stage 1 uses ModernBERT-large-Czech-Legal to filter non-argumentative paragraphs from court decisions; Stage 2 applies Llama 3.1 8B Base with asymmetric loss to classify 8 argument types per paragraph; Stage 3 uses an MLP to predict document-level formalism from argument frequencies and document features. The approach leverages continued pretraining on 300k Czech court decisions to adapt transformers to legal domain language, addresses severe class imbalance with asymmetric loss, and provides explainable outputs by aligning with legal theoretical frameworks. The MADON dataset contains 272 decisions with 9,183 paragraphs annotated across 8 argument types and formalism labels.

## Key Results
- Three-stage pipeline achieves 83.2% macro-F1 for detecting formalism, slightly outperforming end-to-end models
- Asymmetric loss consistently improves macro-F1 for multi-label argument classification (ModernBERT: 62.2%→67.3%, Llama 3.1: 62.9%→71.6%)
- Empirical analysis shows Czech courts rarely use text-based arguments, contradicting prevailing CEE formalism narratives
- Filtering 85% of text as non-argumentative has negligible impact on formalism classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining on domain-specific legal texts improves transformer performance for non-English legal NLP tasks.
- Mechanism: The paper continued-pretrained ModernBERT and Llama 3.1 on a corpus of 300k Czech court decisions, adapting the models to Czech legal language, terminology, and document structure before fine-tuning on the MADON dataset.
- Core assumption: Domain-specific language patterns differ sufficiently from general pretraining data that adaptation provides measurable gains.
- Evidence anchors:
  - [abstract] "Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining"
  - [section 5.2.3] "ModernBERT-large-Czech-Legal achieves 82.2% macro-F1 [for Task 3], the best among end-to-end models" compared to 73.8% for the base model
  - [corpus] Related work on legal argument mining (Habernal et al. 2024, Zhang et al. 2022) demonstrates transformer adaptation is standard practice, though corpus lacks direct Czech legal NLP comparisons
- Break condition: CPT helped ModernBERT but slightly hurt Llama 3.1 on some tasks—gains are architecture-dependent, not universal.

### Mechanism 2
- Claim: Decomposing classification into a multi-stage pipeline aligned with legal theory improves explainability and maintains (or slightly improves) performance while reducing compute costs.
- Mechanism: The three-stage pipeline filters non-argumentative text with ModernBERT (Stage 1), classifies argument types with Llama 3.1 (Stage 2), and predicts formalism from extracted argument features with an MLP (Stage 3). This mirrors how legal scholars analyze decisions.
- Core assumption: Legal formalism is primarily determined by argument type distribution rather than other textual features, making argument-level features sufficient proxies.
- Evidence anchors:
  - [abstract] "A three-stage pipeline combining ModernBERT, Llama 3.1, and feature-based machine learning provided both high performance and explainability while reducing computational costs"
  - [section 5.3.3] "The three stage pipeline yields 82.8% macro F1 and slightly outperforms the best end-to-end model" and "filtering as much as 85% of the text...has negligible impact on performance"
  - [corpus] "Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs" similarly decomposes legal reasoning into structured components, supporting staged approaches
- Break condition: The pipeline depends on reasonably accurate argument detection; if Stage 1 filters too aggressively, downstream stages receive insufficient signal.

### Mechanism 3
- Claim: Asymmetric loss functions effectively address severe class imbalance in multi-label legal argument classification.
- Mechanism: Standard binary cross-entropy loss is dominated by the majority negative class when positive instances are rare. Asymmetric loss upweights minority class contributions, improving detection of infrequent argument types (e.g., Linguistic Interpretation appeared only 108 times across 9,183 paragraphs).
- Core assumption: Macro-averaged metrics (treating each class equally) are the appropriate evaluation criterion rather than accuracy dominated by the majority class.
- Evidence anchors:
  - [abstract] "experiment with methods to address dataset imbalance including asymmetric loss and class weighting"
  - [section 5.3.2] "replacing Binary Cross-Entropy with Asymmetric Loss consistently boosts macro-average performance...ModernBERT (Base: 62.2%→67.3%; CPT: 62.9%→71.6%)"
  - [corpus] No direct corpus evidence on asymmetric loss for legal NLP; this is a domain-agnostic technique applied to legal data
- Break condition: Asymmetric loss improves macro-F1 but may increase false positives for rare classes—evaluate whether precision or recall is more important for your use case.

## Foundational Learning

- Concept: **Multi-label classification with macro-averaged F1**
  - Why needed here: Each paragraph can contain multiple argument types simultaneously (e.g., both Case Law and Historical Interpretation). Standard accuracy is misleading when 87% of paragraphs contain no arguments.
  - Quick check question: If a model predicts "no argument" for every paragraph, would accuracy be high? Would macro-F1 be high? (Answer: accuracy ~87%, macro-F1 ~50%—macro-F1 reveals the problem.)

- Concept: **Continued pretraining vs. fine-tuning**
  - Why needed here: The paper distinguishes between adapting a model to a new domain (continued pretraining on unlabeled legal text) and training for a specific task (fine-tuning on labeled MADON data). These serve different purposes.
  - Quick check question: If you only have 272 labeled documents but 300k unlabeled legal documents, which technique leverages the unlabeled data? (Answer: Continued pretraining.)

- Concept: **Class imbalance in legal NLP**
  - Why needed here: Legal datasets typically have heavy-tailed distributions—some argument types appear hundreds of times, others dozens. Standard loss functions optimize for majority classes.
  - Quick check question: Why might a model achieve 95% accuracy but fail to detect the argument types you care about? (Answer: It's predicting the majority class—"no argument" or the most frequent type—consistently.)

## Architecture Onboarding

- Component map:
  Raw court decision text -> [Stage 1] ModernBERT-large-Czech-Legal -> Binary: argumentative vs. non-argumentative paragraphs -> [Stage 2] Llama 3.1 8B Base + Asymmetric Loss -> Multi-label: 8 argument types per paragraph -> [Stage 3] MLP (2 hidden layers: 20, 50 neurons) -> Features: argument frequencies, document length, argument count -> Binary: formalistic vs. non-formalistic

- Critical path: Stage 1 filtering is the computational bottleneck reducer—if argument detection fails, the entire pipeline degrades. Verify Stage 1 achieves >80% macro-F1 on argument presence before proceeding.

- Design tradeoffs:
  - ModernBERT (395M params) vs. Llama 3.1 (8B params): Encoder is faster and sufficient for binary filtering; decoder better captures complex multi-label dependencies.
  - End-to-end vs. pipeline: End-to-end is simpler but less explainable; pipeline adds complexity but provides interpretable intermediate outputs.
  - PEFT (LoRA) vs. full fine-tuning: PEFT failed dramatically in this paper (79.5%→49.7% F1 drop for Task 1)—full fine-tuning was necessary.

- Failure signatures:
  - Low macro-F1 but high accuracy: Model predicting majority class; try asymmetric loss or class weighting.
  - PEFT/LoRA underperforming base model: Legal argument detection may require full weight updates, not just adapter layers.
  - Stage 3 MLP underperforming end-to-end: Check feature extraction—argument type predictions from Stage 2 may be too noisy.

- First 3 experiments:
  1. Replicate Task 1 (argument presence detection) with ModernBERT-base vs. ModernBERT-CPT to validate CPT gains on your hardware.
  2. Ablate Stage 1 filtering: run Stage 2 on full documents vs. filtered paragraphs to measure compute savings vs. performance delta.
  3. Test asymmetric loss vs. BCE on your most imbalanced class to quantify the improvement before committing to the full pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the empirical findings regarding the "Tale of Two Courts" hold when the models are applied to the full corpus of 230,000 Czech Supreme Court decisions?
- **Basis in paper:** [explicit] The authors state in Section 6.5 that they "plan to employ the trained models to the full corpus of 230,000 apex-court decisions to validate and refine our empirical results."
- **Why unresolved:** The current study is restricted to the MADON dataset of 272 decisions, which limits the statistical power and generalizability of the temporal trends observed between the Supreme Court and Supreme Administrative Court.
- **What evidence would resolve it:** Running the three-stage pipeline on the full 230k corpus and analyzing whether the divergence in formalism levels after 2011 persists in the larger dataset.

### Open Question 2
- **Question:** Can automated legal argument mining be utilized to predict case outcomes based on the detected reasoning patterns?
- **Basis in paper:** [explicit] Section 6.5 notes that "LAM could support related tasks... including predicting outcomes from reasoning patterns," but concludes: "Whether it is indeed so, remains open."
- **Why unresolved:** The current study focused on detecting arguments and classifying judicial philosophy (formalism) but did not test the correlation between specific argument types and the success or failure of legal claims.
- **What evidence would resolve it:** A study correlating the frequency or presence of specific argument types (e.g., teleological interpretation) with case dispositions (e.g., reversal rates or win/loss ratios).

### Open Question 3
- **Question:** Are the reasoning patterns found in Czech courts specific to the country, or do they represent broader trends across Central and Eastern European (CEE) jurisdictions?
- **Basis in paper:** [explicit] Section 6.5 suggests that "extending our methodology and pipelines to other CEE jurisdictions would better test the existing claims about CEE region."
- **Why unresolved:** The study challenges CEE formalism narratives using only Czech data; it remains unknown if the low reliance on textual arguments is unique to Czechia or shared by neighboring legal systems.
- **What evidence would resolve it:** Replicating the annotation and classification pipeline on Supreme Court decisions from other CEE countries (e.g., Poland, Hungary) and comparing the argument distributions.

## Limitations
- Dataset representativeness: MADON contains only 272 decisions, limiting statistical power for empirical conclusions about judicial formalism
- Cross-jurisdictional generalizability: Methodology demonstrated only on Czech legal texts; effectiveness on other jurisdictions untested
- Architecture dependencies: CPT benefits vary by transformer architecture (helped ModernBERT, hurt Llama 3.1 on some tasks)

## Confidence
- **High Confidence**: Core technical methodology (continued pretraining, asymmetric loss, three-stage pipeline) with clear performance metrics and reproducible code
- **Medium Confidence**: Empirical findings about Czech judicial formalism challenging CEE narratives, but based on relatively small sample size
- **Low Confidence**: Claims about methodology being "easily replicable across jurisdictions" remain aspirational without empirical validation

## Next Checks
1. Validate the formalism findings by applying the methodology to a larger sample of Czech Supreme Court decisions (500+ documents) to assess whether observed patterns hold with increased statistical power
2. Apply the three-stage pipeline methodology to legal texts from a different jurisdiction (e.g., German or French courts) to empirically test claims about generalizability
3. Systematically compare asymmetric loss against other class imbalance techniques (focal loss, class weighting, oversampling) on the MADON dataset to confirm optimal performance