---
ver: rpa2
title: Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable
  Uncertainty Bayesian Quantification
arxiv_id: '2501.10770'
source_url: https://arxiv.org/abs/2501.10770
tags:
- bayesian
- uncertainty
- deterministic
- neural
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately classifying COVID-19
  pneumonia in 3D CT scans by comparing deterministic neural networks with their Bayesian
  counterparts. The authors found that lightweight deterministic architectures achieved
  96% accuracy after hyperparameter tuning.
---

# Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable Uncertainty Bayesian Quantification

## Quick Facts
- arXiv ID: 2501.10770
- Source URL: https://arxiv.org/abs/2501.10770
- Reference count: 22
- Primary result: Bayesian neural networks with Multiplicative Normalizing Flow achieved 96% accuracy while providing calibrated uncertainty estimates for COVID-19 pneumonia detection

## Executive Summary
This study compares deterministic and Bayesian neural networks for 3D COVID-19 pneumonia classification in CT scans. The researchers found that lightweight deterministic architectures achieved 96% accuracy after hyperparameter tuning. Their Bayesian counterparts using Multiplicative Normalizing Flow (MNF) techniques maintained similar performance while providing calibrated uncertainty estimates. The research also introduced 3D visualization using SHAP values to explain model predictions, demonstrating that combining explainability with uncertainty quantification can enhance clinical decision-making in medical image analysis.

## Method Summary
The study used the MosMedData dataset with 1110 3D CT scans (128×128×64 resolution) across five categories. A deterministic baseline model with three 3D CNN blocks (128 filters each) plus batch normalization and max pooling achieved 96% accuracy using HU window W4 (-1000 to 0) preprocessing. The Bayesian variant replaced standard layers with MNF-based convolutional and dense layers, trained for 300 epochs with ELBO loss. The system included 3D SHAP explainability and reliability diagrams for calibration assessment, with a decision threshold optimized to 0.4 for improved calibration.

## Key Results
- Lightweight deterministic architecture achieved 96% accuracy on binary COVID-19 pneumonia classification
- Bayesian MNF counterpart maintained similar accuracy while providing calibrated uncertainty estimates
- SHAP-based 3D visualization highlighted regions in lungs most influential for pneumonia predictions
- Data augmentation unexpectedly degraded performance by ~20 points, contrary to typical expectations

## Why This Works (Mechanism)

### Mechanism 1
Bayesian neural networks with Multiplicative Normalizing Flows provide calibrated uncertainty estimates while maintaining classification performance. MNF enriches the variational posterior distribution by introducing auxiliary latent variables and applying invertible normalizing flow transformations, producing more flexible posterior approximations than mean-field Gaussians. This is particularly effective for 3D convolutions handling spatial dimensions and filters. Core assumption: The true posterior over network weights is complex and multi-modal. Evidence: Bayesian counterpart maintained similar performance with calibrated uncertainty estimates. Break condition: If training data is extremely limited or MNF training becomes unstable.

### Mechanism 2
SHAP values provide spatial interpretability by attributing prediction contributions to specific voxel regions in 3D CT scans. SHAP decomposes model output into feature contributions using Shapley values from cooperative game theory, computing each voxel's contribution to the final probability. This highlights regions like ground-glass opacities that drive pneumonia predictions. Core assumption: Model decisions can be meaningfully decomposed into additive feature contributions. Evidence: 3D-visualization approach based on SHAP values highlighted regions most influential in reaching conclusions. Break condition: With many classes, SHAP explanations become complex and harder to interpret clinically.

### Mechanism 3
Expected Calibration Error (ECE) and reliability diagrams validate that uncertainty estimates reflect true prediction confidence. Predictions are binned by confidence score; calibration requires accuracy within each bin to match average confidence. ECE = Σ |Bm|/n × |acc(Bm) - conf(Bm)| quantifies miscalibration. Well-calibrated models have predicted probabilities matching observed frequencies. Core assumption: Calibration on validation data generalizes to clinical deployment. Evidence: DNN W4 V1 Uncertainty V4 3C with Sigmoid layer and threshold of 0.4 showed excellent calibration characteristics. Break condition: Domain shift between validation and deployment data can decalibrate estimates.

## Foundational Learning

- Concept: **Variational Inference & ELBO**
  - Why needed here: BNNs require approximating intractable posteriors p(w|D). Understanding KL divergence minimization and ELBO maximization is essential for debugging MNF training.
  - Quick check question: Can you explain why maximizing ELBO is equivalent to minimizing KL(q(w)||p(w|D))?

- Concept: **3D Convolutional Architectures**
  - Why needed here: The core architecture uses 3D CNNs with spatial dimensions (128×128×64). Understanding how 3D kernels differ from 2D is critical for implementing MNF layers.
  - Quick check question: How does parameter sharing in 3D convolutions differ from 2D, and what does this imply for weight distributions in BNNs?

- Concept: **Uncertainty Decomposition (Epistemic vs. Aleatoric)**
  - Why needed here: BNNs primarily capture epistemic uncertainty (model uncertainty reducible with data). Clinical decisions require distinguishing this from aleatoric noise.
  - Quick check question: If a BNN shows high uncertainty on a CT scan from a new hospital, is this epistemic or aleatoric, and what action should follow?

## Architecture Onboarding

- Component map: Input: 3D CT (128×128×64) → HU Window preprocessing [-1000, 0] → 3× [Conv3D + BatchNorm + MaxPool3D] (128 filters each) → GlobalMaxPool3D → Dense(256) → Dense(2) → Output: Class probabilities + uncertainty intervals via Monte Carlo sampling

- Critical path: HU window selection (W4: -1000 to 0 HU) determines input quality → Hyperparameter tuning (learning rate 0.001, dropout 0.2, threshold 0.4) drives performance → MNF layer configuration controls uncertainty quality

- Design tradeoffs: Data augmentation (volumentations-3D) reduced accuracy by ~20% in experiments—trade robustness for performance; Lightweight architecture outperformed ResNet18/34, EfficientNet, DenseNet—simpler models better for limited medical data; MNF increases computational cost vs. Flipout or MC Dropout but provides richer posteriors

- Failure signatures: Wide confidence intervals on incorrect predictions indicate model uncertainty but not correctness; Deterministic model bias toward class 1 (pneumonia) while Bayesian shows uncertainty signals class imbalance; ECE increasing after threshold changes indicates calibration sensitivity

- First 3 experiments: 1) Replicate deterministic baseline: Train DNN with W4 window, validate 96% accuracy, plot reliability diagram with ECE; 2) Convert to BNN with MNF: Replace Conv3D/Dense layers, train with ELBO loss, compare metrics and calibration; 3) Uncertainty analysis on holdout: Apply model to CT-1/CT-4 scans (unseen classes), analyze confidence intervals and SHAP explanations for failure modes

## Open Questions the Paper Calls Out

### Open Question 1
How can 3D SHAP explainability be effectively scaled to multi-class classification scenarios without overwhelming clinical users? The current study focused primarily on distinguishing between presence and absence of pneumonia, leaving the visualization of more complex, multi-class distinctions unaddressed. A user study with radiologists evaluating SHAP visualizations on the full 5-class MosMed dataset would validate interpretability.

### Open Question 2
What specific factors caused data augmentation to degrade model performance by 20 points? The authors acknowledge the drop but do not isolate which specific transformations introduced the noise or over-regularization. An ablation study identifying which specific augmentation techniques cause the performance collapse in this 3D context would resolve this.

### Open Question 3
Can the computational overhead of MNF-based Bayesian Neural Networks be reduced for real-time clinical deployment? The study demonstrates the feasibility of the method but highlights that training is challenging and time-consuming, leaving the optimization for low-resource environments as an open challenge. Benchmarking the MNF model's inference latency against deterministic baselines on standard clinical hardware would prove feasibility.

## Limitations
- Exact 3D MNF implementation details are not publicly available, making precise replication uncertain
- Data augmentation unexpectedly degraded performance by ~20 points, contrary to typical expectations
- Calibration improvements are dataset-specific and may not generalize across different CT scan sources

## Confidence

- **High confidence**: Deterministic baseline architecture achieves 96% accuracy with HU window W4 preprocessing and threshold optimization
- **Medium confidence**: Bayesian MNF provides calibrated uncertainty estimates while maintaining classification performance, based on theoretical foundations and related work
- **Low confidence**: Exact 3D MNF implementation details and SHAP visualization methodology without code access

## Next Checks

1. Replicate deterministic baseline performance on MosMedData with W4 window preprocessing and threshold optimization to verify 96% accuracy claim
2. Implement MNF layers using Louizos & Welling (2017) formulation extended to 3D convolutions, compare calibration metrics (ECE) against deterministic baseline
3. Test uncertainty estimates on CT-1/CT-4 holdout scans (unseen classes) to validate whether Bayesian model uncertainty captures domain shift beyond the training distribution