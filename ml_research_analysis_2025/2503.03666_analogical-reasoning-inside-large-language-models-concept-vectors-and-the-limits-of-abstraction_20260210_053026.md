---
ver: rpa2
title: 'Analogical Reasoning Inside Large Language Models: Concept Vectors and the
  Limits of Abstraction'
arxiv_id: '2503.03666'
source_url: https://arxiv.org/abs/2503.03666
tags:
- concepts
- representations
- prompt
- task
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) encode
  invariant conceptual representations. The authors compare Function Vectors (FVs),
  derived via activation patching, with Concept Vectors (CVs), identified using Representational
  Similarity Analysis (RSA).
---

# Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction

## Quick Facts
- **arXiv ID:** 2503.03666
- **Source URL:** https://arxiv.org/abs/2503.03666
- **Reference count:** 5
- **Primary result:** Large Language Models encode verbal concepts (e.g., antonym, categorical) in invariant Concept Vectors but lack reusable abstractions for relational concepts like "previous" and "next," limiting analogical reasoning.

## Executive Summary
This paper investigates whether Large Language Models encode invariant conceptual representations by comparing Function Vectors (FVs) derived from activation patching with Concept Vectors (CVs) identified using Representational Similarity Analysis (RSA). The authors find that FVs are not invariant to low-level changes like switching from open-ended to multiple-choice formats, instead encoding detailed task attributes. In contrast, CVs for verbal concepts are more invariant and function as feature detectors independent of final outputs. Causal interventions show CVs can guide model behavior more portably than FVs across distribution shifts, but only for concepts with established invariant representations. For abstract concepts like "previous" and "next," no invariant linear representations emerge, correlating with poor generalization in tasks like letter-string analogies.

## Method Summary
The authors extract attention head outputs from Llama-3.1 8B and 70B models during prompt processing, then compute FVs via activation patching (identifying causal contributors) and CVs via RSA (identifying invariant representations). FVs are summed from top N heads by Causal Indirect Effect (CIE), while CVs are summed from top 3 heads by Φconcept scores. The methodology tests invariance by examining clustering patterns in representational similarity matrices and evaluates causal interventions by adding scaled vectors to hidden states at target layers.

## Key Results
- FVs cluster by format (open-ended vs. multiple-choice) rather than concept, showing low invariance to low-level transformations
- CVs for verbal concepts (antonym, categorical) are more invariant, functioning as feature detectors that can decouple from final outputs
- Causal interventions show CVs guide behavior more portably than FVs across distribution shifts, but only for concepts with established invariant representations
- For abstract concepts like "previous" and "next," no invariant linear representations emerge, correlating with poor generalization

## Why This Works (Mechanism)

### Mechanism 1: Concept Vectors as Feature Detectors
- **Claim:** CVs encode verbal concepts (e.g., "antonym") in early-to-mid layer attention heads, functioning as feature detectors that may not propagate to correct outputs
- **Mechanism:** RSA identifies attention heads whose activation patterns correlate with concept attributes. Summing top-3 heads' outputs yields a CV that clusters by concept across formats. The CV forms correctly even when the model outputs incorrectly—suggesting detection and generation can decouple
- **Core assumption:** Concepts are encoded linearly in attention head outputs (Linear Representation Hypothesis)
- **Evidence anchors:** [abstract] "These CVs function as feature detectors that operate independently of the final output—meaning that a model may form a correct internal representation yet still produce an incorrect output." [Section 4.2] Figure 6 shows CVs encoding correct concepts during incorrect predictions; N=168 correct vs N=132 incorrect activations analyzed
- **Break condition:** If concepts require nonlinear encoding (as Engels et al. 2024 suggests), RSA on linear outputs will miss them

### Mechanism 2: Representational Invariance Scales with In-Context Examples
- **Claim:** CV invariance to low-level transformations increases as ICL prompt contains more training examples
- **Mechanism:** With few examples (N=1–2), representations cluster by surface features (format, language). As N increases to 5, within-concept similarity rises and cross-concept similarity falls, indicating the model "distills" the latent concept from varied instances
- **Core assumption:** The model aggregates statistical regularities across exemplars to abstract a shared concept
- **Evidence anchors:** [Section 4.1, Figure 8] CV similarity structure shifts from low-level clustering at N=1–2 to invariant clustering at N=5. [Section 4.2, Figure 9] Φconcept (concept alignment score) grows with accuracy as N increases to 5, then plateaus
- **Break condition:** If exemplars are inconsistent or contradictory, invariance may not emerge regardless of N

### Mechanism 3: Causal Intervention Portability via CVs
- **Claim:** CVs guide model behavior more portably than FVs across distribution shifts, but require context and scaling
- **Mechanism:** Extract CV from one concept manifestation (e.g., Antonym EN), add 10× scaled CV to hidden states at the layer where concept-encoding heads peak (layer 14 for 8B, layer 31 for 70B). CVs from FR maintain performance; CVs from MC format (similarity 0.7 vs. 0.8) drop to near-baseline. FVs outperform in zero-shot but fail on out-of-distribution transfers
- **Core assumption:** CVs capture "purer" latent content; FVs embed task-specific details that hinder portability
- **Evidence anchors:** [Section 4.3, Figure 7] CVs from Antonym EN and FR boost AmbiguousICL accuracy similarly; MC-derived vectors fail. FVs win zero-shot (50–58%) but CVs win on cross-distribution transfer. [Section 3.1] FVs cluster by format (MC vs. open-ended) rather than concept, explaining low portability
- **Break condition:** If target task differs fundamentally in response type (e.g., letter vs. word), even CVs with similarity 0.8 may fail

## Foundational Learning

- **Representational Similarity Analysis (RSA):**
  - Why needed: RSA detects whether neural activation patterns align with task attributes (concept, language, format) without requiring causal intervention
  - Quick check question: Given activations from 50 prompts across 3 formats, would you expect high Φconcept if the model encodes "antonym" invariantly?

- **Activation Patching (Causal Indirect Effect):**
  - Why needed: Patching identifies which components causally affect outputs, but the paper shows it misses latent components that don't directly drive predictions
  - Quick check question: If corrupting ICL exemplars and patching clean activations yields high AIE for 100 heads, does that guarantee those heads encode the abstract concept?

- **Linear Representation Hypothesis:**
  - Why needed: The entire CV methodology assumes concepts correspond to linear directions in activation space
  - Quick check question: If "previous" is encoded nonlinearly across heads, would RSA with cosine similarity detect it?

## Architecture Onboarding

- **Component map:** Input → Embedding → Layers 1–L (each: attention heads + MLP) → Final token hidden state → Output logits
- **Critical path:** 1) Collect ICL prompts spanning concept + low-level variations (language, format) 2) Extract attention head outputs for each prompt at last token position 3) Compute RSM per head; correlate with design matrices for each task attribute 4) Select top heads by Φconcept; sum their mean activations → CV 5) For intervention: scale CV (10×), add to hidden state at target layer
- **Design tradeoffs:** CVs: Fewer heads (3 vs. 100), more invariant, require context, weaker zero-shot. FVs: Many heads, stronger zero-shot, encode format/task-type details, less portable. RSA vs. Patching: RSA finds latent representations; patching finds causal drivers—they may not overlap
- **Failure signatures:** FVs clustering by format (MC vs. open-ended) → not concept-invariant. CVs with similarity <0.75 to target task → intervention fails. No heads with Φconcept >0.2 → no linear concept representation (as seen for "previous"/"next"). High task accuracy but low Φconcept → model uses memorization/heuristics, not abstraction
- **First 3 experiments:** 1) Replicate CV extraction for "antonym" across EN/FR/MC formats on Llama-3.1-8B; verify RSM clustering matches Figure 2. 2) Run AmbiguousICL intervention with CVs extracted from each format; confirm FR transfer succeeds (similarity 0.8) while MC fails (similarity 0.7). 3) Extract CVs for "previous" and "next" across Item-in-List and Abstract-Letter tasks; verify Φconcept stays below 0.2, confirming no invariant representation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do LLMs encode abstract concepts like "previous" and "next" using nonlinear representations that linear methods fail to detect?
- **Basis in paper:** [explicit] The authors list their exclusive focus on linear representations as a key limitation, noting "Our LLMs might still encode 'Next' and 'Previous' nonlinearly but our methods fail to capture it."
- **Why unresolved:** The study relied on linear analyses (RSA on attention head outputs), which cannot identify representations that are nonlinearly encoded
- **What evidence would resolve it:** Applying nonlinear probing methods (e.g., MLP-based probes or nonlinear RSA) to the same layers to identify invariant abstract representations

### Open Question 2
- **Question:** How can LLMs be trained or fine-tuned to develop reusable, invariant abstract representations for relational concepts?
- **Basis in paper:** [explicit] The authors state that investigating "how to develop [abstract knowledge]" is "critical for achieving human-level artificial reasoning systems."
- **Why unresolved:** The study analyzed existing model capabilities without exploring training interventions that might induce these missing abstractions
- **What evidence would resolve it:** Training models on datasets explicitly designed to enforce invariance (e.g., varied "next" tasks) and re-evaluating the emergence of invariant CVs

### Open Question 3
- **Question:** Do invariant Concept Vectors for verbal and abstract concepts emerge in non-Llama architectures?
- **Basis in paper:** [explicit] The authors note their "conclusions are restricted to the LLama-3.1 8B and 70B models, leaving generalizability to other architectures untested."
- **Why unresolved:** The study focused solely on the Llama 3.1 family, leaving open the possibility that other architectures (e.g., GPT, Mistral) might rely on different representational strategies
- **What evidence would resolve it:** Replicating the RSA and CV extraction methodology on alternative model families to compare the layer-wise emergence of invariant concepts

## Limitations

- The paper's core finding that CVs capture "purer" conceptual content while FVs encode task-specific details is robust, but the mechanism for why certain concepts (like "previous/next") lack invariant representations remains unclear
- RSA's assumption that concepts correspond to linear directions in activation space is explicitly acknowledged as a limitation, yet this assumption underpins the entire CV methodology
- The causal intervention results show CVs work better for cross-distribution transfer, but the success rate (30-40% for 70B models) remains modest and the 10x scaling factor appears arbitrary

## Confidence

- **High confidence:** FVs are not invariant to low-level changes and encode task-specific details rather than abstract concepts. This is supported by clear RSM clustering patterns and multiple intervention experiments
- **Medium confidence:** CVs function as feature detectors that can decouple from final outputs. While the evidence (Figure 6) is compelling, the interpretation relies on assumptions about what constitutes "correct" internal representation
- **Medium confidence:** Representational invariance scales with in-context examples. The correlation between N and Φconcept is clear, but the plateau at N=5 suggests the mechanism may be more complex than simple exemplar aggregation

## Next Checks

1. Test whether nonlinear RSA methods (beyond cosine similarity) can detect invariant representations for "previous/next" concepts, or whether these truly lack reusable abstractions
2. Systematically vary the CV scaling factor (not just 10x) across multiple concepts to determine if the relationship between scaling and intervention success is linear or concept-dependent
3. Apply the CV methodology to other relational concepts (like "cause/effect" or "part/whole") to determine whether the pattern of invariant verbal vs. non-invariant abstract concepts generalizes beyond the current dataset