---
ver: rpa2
title: 'Tell2Reg: Establishing spatial correspondence between images by the same language
  prompts'
arxiv_id: '2502.03118'
source_url: https://arxiv.org/abs/2502.03118
tags:
- registration
- rois
- images
- prostate
- correspondence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Tell2Reg, a training-free approach for medical
  image registration that uses text prompts with pre-trained multimodal models (GroundingDINO
  and SAM) to identify corresponding regions between images. The method conceptualizes
  registration as detecting the same regions from different images using identical
  text prompts, eliminating the need for training data and expensive data curation.
---

# Tell2Reg: Establishing spatial correspondence between images by the same language prompts

## Quick Facts
- arXiv ID: 2502.03118
- Source URL: https://arxiv.org/abs/2502.03118
- Reference count: 0
- Primary result: Training-free prostate MR registration achieving Dice 0.84±0.06 and TRE 3.09±2.25 mm using text prompts with pre-trained multimodal models

## Executive Summary
Tell2Reg presents a training-free approach for medical image registration that leverages text prompts with pre-trained multimodal models (GroundingDINO and SAM) to identify corresponding regions between images. The method conceptualizes registration as detecting the same regions from different images using identical text prompts, eliminating the need for training data and expensive data curation. Tested on inter-subject prostate MR registration, Tell2Reg achieved competitive performance metrics while maintaining a training-free paradigm that offers improved generalizability and clinical potential for various registration tasks.

## Method Summary
Tell2Reg establishes spatial correspondence between medical images by using identical text prompts on pre-trained vision-language models. The pipeline begins with GroundingDINO generating bounding boxes from text prompts on both fixed and moving images, followed by size-based filtering. SAM then segments regions of interest (ROIs) using these boxes as prompts. ROI pairs are matched by maximizing cosine similarity of SAM embeddings, with prototype averaging used for feature representation. An optional dense displacement field can be computed via iterative optimization using Dice+MSE loss and L2 regularization. The approach is training-free, relying entirely on frozen foundation models without gradient updates.

## Key Results
- Achieved Dice coefficient of 0.84±0.06 on inter-subject prostate MR registration
- Target Registration Error (TRE) of 3.09±2.25 mm for centroid alignment
- Outperformed unsupervised learning-based methods while performing comparably to weakly-supervised approaches
- Demonstrated stable performance with predefined prompts, though prompt sensitivity remains an open question

## Why This Works (Mechanism)

### Mechanism 1: Semantic-to-Spatial Correspondence via Text Prompts
- Identical text prompts applied to different images yield corresponding anatomical regions, bypassing pixel-level optimization
- GroundingDINO generates bounding boxes from text prompts via contrastive vision-language alignment; these boxes condition SAM to produce segmentation masks
- Core assumption: Pre-trained multimodal models capture spatial invariances that transfer from natural images to medical domains without fine-tuning

### Mechanism 2: ROI Prototype Similarity Matching
- Correspondence refinement via embedding-space matching handles unequal ROI counts and false positives
- SAM image embeddings are masked by each ROI and averaged to create prototypes; cosine similarity matrix identifies best-matching pairs
- Core assumption: ROI-level feature aggregation produces discriminative representations for correspondence

### Mechanism 3: Training-Free Paradigm Shift
- Eliminating training data requirements enables generalization across registration tasks
- No gradient updates occur; inference directly leverages frozen foundation models
- Core assumption: Foundation model representations contain sufficient task-relevant structure without domain adaptation

## Foundational Learning

- **GroundingDINO + SAM Architecture**: Why needed here: Tell2Reg chains these models; understanding their input/output contracts is essential for debugging detection failures. Quick check question: Can you explain why GroundingDINO outputs bounding boxes while SAM outputs segmentation masks, and how they're composed?

- **Region-Based Registration Theory**: Why needed here: The paper reframes registration as region correspondence rather than dense field prediction. Quick check question: How does the limiting case where "each ROI reduces to a single pixel" relate to dense displacement fields?

- **Cosine Similarity for Correspondence**: Why needed here: Algorithm 1 Step 2 uses cosine similarity for prototype matching; understanding embedding space geometry is critical for interpreting failures. Quick check question: Why might L2 distance be inferior to cosine similarity when matching ROI prototypes across different images?

## Architecture Onboarding

- **Component map**: Text Prompt p → GroundingDINO → Bounding Boxes {B_fix, B_mov} → Size Filtering → SAM → Binary Masks {M_fix, M_mov} → SAM Encoder → Embeddings {E_fix, E_mov} → Prototype Averaging → {e_fix, e_mov} → Cosine Similarity Matrix S → argmax → Paired ROIs {(R_fix_k, R_mov_k)} → [Optional] Dense Transformation Optimization

- **Critical path**: Prompt selection → Bounding box quality → Prototype discriminability. Poor prompts cascade through entire pipeline.

- **Design tradeoffs**: Abstract/concrete nouns vs. descriptive phrases: Table 2 shows nouns outperform phrases for correspondence yield. Fixed vs. random prompts: Fixed sets provide stability; random sampling introduces variance. Training-free vs. performance ceiling: Weaker than weakly-supervised TransMorph on Dice (0.84 vs 0.85), but requires zero labels.

- **Failure signatures**: Low detection ratio: Prompt returns <20% prostate ROIs (e.g., "dog" at 19%). Mismatched correspondences: Figure 3 third group shows false positives—semantic similarity doesn't guarantee spatial correspondence. Unequal ROI counts: K_fix ≠ K_mov indicates asymmetric detection; Algorithm 1 handles via greedy matching.

- **First 3 experiments**: 1) Prompt ablation: Test 5-10 prompts on 10 image pairs; measure detection ratio and Dice. Identify which semantic categories transfer. 2) Correspondence validation: Manually annotate 20 ROI pairs; compute precision/recall of Algorithm 1 matching vs. ground truth. 3) Comparative registration: Run Tell2Reg and VoxelMorph on same test split; visualize TRE distributions to confirm reported 3.09mm vs 8.06mm improvement.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different semantic categories of text prompts (e.g., abstract nouns vs. natural image concepts) quantitatively contribute to the robustness and accuracy of spatial correspondence? The authors note performance stability with predefined prompts but emphasize uncertainty about specific contributions of different semantic classes.

- **Open Question 2**: Can fully automated prompt engineering replace the current reliance on empirically predefined, fixed text prompts without compromising registration performance? The study notes that while predefined prompts provide stability, automatic prompt refinement remains unexplored.

- **Open Question 3**: Would integrating medical-image-specific foundation models eliminate the necessity of the L2-distance-based ROI correspondence refinement step? The authors suggest that current refinement steps may be unnecessary with improved model specificity for medical domains.

## Limitations
- Performance heavily depends on prompt selection and may not generalize to other anatomical structures
- Dice score of 0.84 lags behind weakly-supervised approaches by 1-2 percentage points
- Approach's dependence on pre-trained vision-language models raises concerns about distribution shift when applied to different imaging modalities

## Confidence
- **High confidence**: Training-free paradigm (explicitly confirmed by no model updates during inference) and basic pipeline components (GroundingDINO + SAM composition is well-documented)
- **Medium confidence**: Performance metrics on prostate MR (single dataset, no external validation) and generalizability claims (only tested on one registration task)
- **Low confidence**: Claims about spatial invariance of language-prompted regions (qualitative observation without quantitative validation) and mechanism explanations for why certain prompts work better than others

## Next Checks
1. **Prompt sensitivity analysis**: Systematically vary prompts across 10+ anatomical structures to establish which semantic categories consistently yield valid correspondences
2. **Cross-domain validation**: Apply Tell2Reg to non-prostate registration tasks (e.g., brain MRI, cardiac CT) to test generalizability claims
3. **Failure mode characterization**: Create an adversarial prompt set designed to trigger false positives and analyze the pipeline's robustness to semantic ambiguity