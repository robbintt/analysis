---
ver: rpa2
title: Can Vision-Language Models Understand Construction Workers? An Exploratory
  Study
arxiv_id: '2601.10835'
source_url: https://arxiv.org/abs/2601.10835
tags:
- construction
- gpt-4o
- florence
- performance
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the ability of three vision-language models
  (GPT-4o, Florence 2, and LLaVa-1.5) to recognize construction worker actions and
  emotions from static images without domain-specific training. Using a curated dataset
  of 1,000 images across ten action and ten emotion categories, we applied standardized
  inference pipelines and assessed performance with metrics including F1-score and
  accuracy.
---

# Can Vision-Language Models Understand Construction Workers? An Exploratory Study

## Quick Facts
- **arXiv ID:** 2601.10835
- **Source URL:** https://arxiv.org/abs/2601.10835
- **Reference count:** 40
- **Primary result:** GPT-4o achieved F1-score 0.756 and accuracy 0.799 for actions, and F1-score 0.712 and accuracy 0.773 for emotions, outperforming Florence 2 and LLaVa-1.5 in zero-shot classification of construction worker behaviors from static images.

## Executive Summary
This study evaluates three vision-language models (GPT-4o, Florence 2, and LLaVa-1.5) for recognizing construction worker actions and emotions from static images without domain-specific training. Using a curated dataset of 1,000 images across ten action and ten emotion categories, the models were assessed using standardized inference pipelines and metrics including F1-score and accuracy. GPT-4o consistently outperformed the others, demonstrating that general-purpose VLMs can provide a baseline for behavior recognition in construction environments. However, all models struggled with semantically similar categories, highlighting the need for domain adaptation and additional modalities for reliable real-world deployment.

## Method Summary
The study used a curated dataset of 1,000 construction site images sourced from Unsplash and Roboflow, manually annotated with one-hot encoding across 10 action categories and 10 emotion categories. Three VLMs (GPT-4o via OpenAI API, Florence 2 and LLaVa-1.5 via Hugging Face Transformers) were evaluated using a standardized "Direct Generation" zero-shot inference prompt template. Outputs were converted to binary matrices and compared against ground truth using precision, recall, F1-score, accuracy, and confusion matrix analysis.

## Key Results
- GPT-4o achieved average F1-scores of 0.756 for actions and 0.712 for emotions, significantly outperforming Florence 2 and LLaVa-1.5.
- Visually distinct actions (e.g., "Following safety protocols" with F1 0.915) were recognized more accurately than subtle actions requiring pose inference.
- Persistent misclassification occurred between semantically similar categories (e.g., "Collaborating" vs. "Communicating," "Focused" vs. "Determined").
- LLaVa-1.5 showed near-random performance with F1-scores around 0.50 across all emotion categories.

## Why This Works (Mechanism)

### Mechanism 1: Scale-Driven Multimodal Generalization
The superior performance of GPT-4o is driven by robust alignment of visual features to general linguistic concepts during large-scale pre-training, enabling zero-shot transfer to construction domains. The model maps input images to high-dimensional feature spaces and retrieves visual concepts semantically close to the prompt, even for rare construction-specific content. This relies on the assumption that pre-training included sufficient imagery of tools, poses, and PPE. If the visual domain involves objects completely absent from pre-training (e.g., highly specialized new machinery), the semantic alignment fails, defaulting to hallucinations or generic labels.

### Mechanism 2: Visual Salience of Distinctive Anchors
Recognition accuracy is mediated by the presence of visually distinct "anchors" (e.g., hard hats, machinery cabs) rather than the nuance of the action itself. Models prioritize high-contrast or unique visual identifiers over subtle body language, leading to higher confidence in actions with strong visual markers (e.g., "Following safety protocols" implies PPE) compared to those requiring pose inference. The model may conflate specific objects (safety vests) with specific states (following protocols). If an image contains visual anchors but the action is different (e.g., carrying vs. using a drill), the model is prone to false positives due to object bias.

### Mechanism 3: Semantic Proximity Interference in Static Frames
Static image analysis creates semantic interference where models cannot distinguish between temporally related or visually similar states. Without motion vectors or audio, models rely on single-frame posture, and semantically adjacent categories (e.g., "Collaborating" vs. "Communicating") share identical visual features, causing the model to default to the more probable or linguistically frequent category. The model lacks temporal context to determine if "talking" is happening vs. "working together," leading to flattened probability distributions between similar classes. If temporal data (video) is introduced, this mechanism likely weakens, allowing motion dynamics to resolve the ambiguity between "standing" and "working."

## Foundational Learning

- **Concept: Zero-Shot Inference**
  - **Why needed here:** The study relies on the model's ability to classify actions it has never explicitly seen labeled as "construction actions" during training.
  - **Quick check question:** If you show the model a picture of a "worker holding a blueprint," can it infer "planning" without being trained on "planning" labels?

- **Concept: Confusion Matrix Interpretation**
  - **Why needed here:** Aggregate F1-scores hide specific failure modes (e.g., are we missing "tired" workers, or mislabeling them as "focused"?).
  - **Quick check question:** Does the diagonal of the matrix show strong distinct clusters, or are the predictions scattered among semantically similar neighbors?

- **Concept: Precision-Recall Trade-off**
  - **Why needed here:** In safety-critical construction environments, false negatives (missing an unsafe action) may be more costly than false positives.
  - **Quick check question:** If GPT-4o has high precision but misses 25% of actual "unsafe" acts (Recall), is it sufficient for safety monitoring?

## Architecture Onboarding

- **Component map:** Input (1,000 RGB images) -> Backbone (GPT-4o API / Florence 2 / LLaVa-1.5) -> Interface (Standardized prompt) -> Output Parser (JSON to Binary Matrix) -> Evaluator (Precision, Recall, F1, Accuracy vs Ground Truth)
- **Critical path:** The prompt engineering strategy ("Direct Generation") is the bottleneck. Unlike feature-matching classifiers, these models generate text tokens ("Yes"/"No") which must be deterministically mapped to binary labels.
- **Design tradeoffs:**
  - GPT-4o: Highest accuracy (0.79) but requires API costs and internet latency.
  - Florence 2 / LLaVa-1.5: Local deployment (privacy/latency benefits) but significantly lower F1-scores (~0.46â€“0.50), making them risky for safety-critical monitoring.
- **Failure signatures:**
  - The "Hallucination" Drift: LLaVa-1.5 predicting emotions with ~0.50 accuracy across all classes, suggesting random guessing rather than feature extraction.
  - Object-Bias False Positives: Florence 2 predicting "Heavy machinery" just because a machine is visible in the background, even if not in use.
- **First 3 experiments:**
  1. **Prompt Sensitivity Test:** Modify the prompt from "Does X appear?" to "Describe the scene" and analyze if extraction improves or degrades.
  2. **Temporal Slice Validation:** Input 3-frame sequences (if video data is available) to see if misclassification of "Communicating" vs "Collaborating" drops.
  3. **Occlusion Robustness:** Mask the faces or tools in the image to determine if the model is relying on facial emotion or context (body pose/tools) for the "Emotion" task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does incorporating temporal video data significantly improve VLM accuracy in disambiguating semantically similar construction actions compared to static images?
- **Basis in paper:** [explicit] The authors state that the static-image format is a fundamental bottleneck and that "incorporating time-series image sequences or video data could provide essential temporal context" as a major area of future work.
- **Why unresolved:** The study was restricted to single frames, preventing the assessment of dynamic behavioral evolution or temporal dependencies.
- **What evidence would resolve it:** A comparative evaluation of these VLMs on video datasets showing statistically significant improvements in distinguishing complex actions.

### Open Question 2
- **Question:** To what extent does augmenting visual inputs with non-visual modalities (e.g., physiological signals or audio) enhance the detection of subtle emotional states?
- **Basis in paper:** [explicit] The discussion notes the "absence of multimodal data... constrains the models' perceptual depth" and suggests integrating audio or physiological data is necessary to resolve ambiguities.
- **Why unresolved:** The current methodology relied solely on visual information, which limits the ability to differentiate affective states that lack strong visual anchors.
- **What evidence would resolve it:** Experiments combining VLMs with sensor data demonstrating improved F1-scores for visually ambiguous emotions like "Anxious" versus "Frustrated."

### Open Question 3
- **Question:** Can domain-specific fine-tuning effectively resolve the persistent confusion observed between semantically close categories, such as "collaborating" versus "communicating"?
- **Basis in paper:** [explicit] The authors acknowledge that the models used were not fine-tuned for construction-specific semantics and that performance "may not represent their true potential" without it.
- **Why unresolved:** This study evaluated zero-shot generalization; the impact of supervised domain adaptation on reducing specific misclassification patterns remains untested.
- **What evidence would resolve it:** Performance metrics from models fine-tuned on a labeled construction corpus showing reduced off-diagonal values in confusion matrices for overlapping classes.

## Limitations

- **Dataset Generalization:** The curated dataset of 1,000 images may not fully represent the diversity of real-world construction environments, limiting the generalizability of results.
- **Zero-Shot Constraint:** The reliance on zero-shot inference means the models are not optimized for construction-specific nuances, and domain adaptation could significantly improve performance.
- **Single Frame Analysis:** Static image analysis inherently limits the model's ability to distinguish between temporally related actions or emotions, missing the benefits of temporal context.

## Confidence

- **High Confidence:** GPT-4o's superior performance (F1-score 0.756 for actions, 0.712 for emotions) and its consistent outperformance of Florence 2 and LLaVa-1.5 across all metrics.
- **Medium Confidence:** The observation that visually distinct "anchors" (e.g., PPE, machinery) drive recognition accuracy, and the semantic proximity interference between closely related categories.
- **Low Confidence:** The exact reasons for LLaVa-1.5's significantly lower performance (F1-score ~0.50) and whether this is due to model architecture or the zero-shot constraint.

## Next Checks

1. **Dataset Expansion and Diversity:** Replicate the study with a larger, more diverse dataset of construction worker images, including varying lighting conditions, camera angles, and worker demographics, to assess the robustness of the results.
2. **Temporal Analysis:** Conduct a follow-up study using video data to evaluate if temporal context improves the model's ability to distinguish between semantically similar actions and emotions (e.g., "Communicating" vs. "Collaborating").
3. **Prompt Engineering Exploration:** Systematically vary the prompt template (e.g., "Describe the scene," "What is the worker doing?") to determine the impact of prompt phrasing on model performance and identify optimal strategies for construction-specific recognition.