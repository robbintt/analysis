---
ver: rpa2
title: Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric
  Spaces
arxiv_id: '2507.15741'
source_url: https://arxiv.org/abs/2507.15741
tags:
- regression
- prediction
- conformal
- data
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for uncertainty quantification
  in regression models defined in metric spaces. The authors develop a conformal prediction
  algorithm for homoscedastic settings that offers finite-sample coverage guarantees
  and fast convergence rates, and a local k-nearest-neighbor method for heteroscedastic
  settings that adapts to the geometry of the data without conformal calibration.
---

# Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces

## Quick Facts
- **arXiv ID:** 2507.15741
- **Source URL:** https://arxiv.org/abs/2507.15741
- **Reference count:** 40
- **Primary result:** Novel framework for uncertainty quantification in regression models with responses in general metric spaces, offering finite-sample coverage guarantees for homoscedastic data and adaptive local estimation for heteroscedastic settings.

## Executive Summary
This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces, where responses can be complex objects like probability distributions or graph Laplacians. The authors develop two complementary algorithms: a conformal prediction method for homoscedastic settings that provides finite-sample coverage guarantees, and a local k-nearest-neighbor approach for heteroscedastic settings that adapts to local data geometry. Both methods work with any regression algorithm and are scalable to large datasets, demonstrated through applications in personalized medicine including glucose monitoring and handwriting analysis.

## Method Summary
The framework operates by estimating a conditional Fréchet mean $\hat{m}(x)$ as the center of the prediction region, then constructing uncertainty sets as metric balls around this center. For homoscedastic data, a global quantile of residuals (computed as distances from responses to their centers) provides the radius, yielding finite-sample coverage guarantees via split conformal prediction. For heteroscedastic data, a local k-nearest-neighbor approach estimates the radius by computing the $(1-\alpha)$-quantile of residuals among the $k$ nearest neighbors of the query point, trading finite-sample guarantees for asymptotic consistency and better adaptation to local variance. The framework also allows decoupling the metric used for regression estimation ($d_1$) from the metric used for prediction region construction ($d_2$), enabling better model fitting and interpretable visualization.

## Key Results
- Proposed conformal algorithm offers finite-sample coverage guarantees for metric-space homoscedastic data with fast convergence rates
- Local kNN method adapts to heteroscedasticity without requiring conformal calibration, achieving superior conditional coverage in nonlinear scenarios
- Simulation studies demonstrate the methods' effectiveness on synthetic and real-world data including glucose distributions and graph Laplacians
- The approach handles complex response types (probability distributions, graphs) that standard Euclidean methods cannot process

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Split conformal prediction provides valid finite-sample prediction regions for metric-space responses if data is "metric-homoscedastic."
- **Mechanism:** Algorithm estimates conditional Fréchet mean $\hat{m}(x)$ and computes residuals as distances $d_2(Y, \hat{m}(X))$. Under homoscedasticity where $P(Y \in B(m(x), r) | X = x) = \phi(r)$ is invariant across $x$, residuals share identical distribution, allowing global quantile to serve as valid radius.
- **Core assumption:** Conditional distribution of distance from response to center is invariant across all $x$ (metric-space homoscedasticity).
- **Evidence anchors:** Proposition 2 proves $P(Y \in \hat{C}_\alpha(X; D_n)) \geq 1 - \alpha$; abstract states "finite-sample coverage guarantees."
- **Break condition:** Heteroscedastic data causes prediction regions to be too wide in low-variance areas or too narrow in high-variance areas.

### Mechanism 2
- **Claim:** Local kNN radius estimation adapts to heteroscedasticity by trading finite-sample guarantees for consistency.
- **Mechanism:** Estimates local radius $\hat{q}_{1-\alpha}(x)$ for new point $x$ by finding $(1-\alpha)$-quantile of residuals among $k$ nearest neighbors in calibration set, bypassing need for homoscedasticity assumption.
- **Core assumption:** Local averaging can approximate true conditional distribution (consistency conditions).
- **Evidence anchors:** Theorem 5 proves asymptotic consistency: $\lim n \to \infty E(\epsilon(\hat{C}_\alpha, X)|D_n) \to 0$.
- **Break condition:** High-dimensional predictor spaces cause nearest neighbors to lose locality, invalidating consistency claims.

### Mechanism 3
- **Claim:** Decoupling "loss metric" ($d_1$) from "prediction metric" ($d_2$) enables better model fitting and interpretable uncertainty visualization.
- **Mechanism:** Minimizes loss using metric $d_1$ (e.g., Wasserstein for distributions) to find center $\hat{m}$, but constructs prediction set using different metric $d_2$ (e.g., supremum norm) for geometric simplicity.
- **Core assumption:** $d_1$ suitable for regression estimation while $d_2$ suitable for defining specific uncertainty volume.
- **Evidence anchors:** Section 1.2 explicitly defines dual metrics; section 4.1 demonstrates with Euclidean vs. sup-norm.
- **Break condition:** Poor $d_2$ choice leads to prediction balls that fail to capture true distribution shape in non-convex spaces.

## Foundational Learning

- **Concept:** **Fréchet Mean**
  - **Why needed here:** Standard vector averaging is undefined for complex objects like graph Laplacians or distribution functions; the "center" $\hat{m}(x)$ must minimize expected squared distance in the metric space.
  - **Quick check question:** If $Y$ is a set of probability distributions, does $\hat{m}(x)$ output a single distribution or a vector of scalars?

- **Concept:** **Metric Space Homoscedasticity**
  - **Why needed here:** Critical distinction between Algorithm 2 (guaranteed) and Algorithm 3 (adaptive); homoscedasticity means invariance of distance distribution from mean across covariates, not constant Euclidean variance.
  - **Quick check question:** If variance of $d(Y, m(x))$ changes with $X$, is the data homoscedastic in the metric sense?

- **Concept:** **Split Conformal Inference**
  - **Why needed here:** Architecture relies on data splitting (training vs. calibration); coverage guarantees depend on exchangeability of calibration set, not training fit quality.
  - **Quick check question:** Does $P(Y \in \hat{C}(X)) \geq 1-\alpha$ hold if calibration scores aren't exchangeable (e.g., time series without correction)?

## Architecture Onboarding

- **Component map:** Fréchet Regressor -> Residualizer -> Radius Estimator (Global Quantile OR Local kNN Quantile) -> Constructor
- **Critical path:**
  1. Split data $D_n = D_{train} \cup D_{test}$
  2. Fit Regressor on $D_{train}$
  3. Calculate distances (residuals) on $D_{test}$
  4. Decision Node: Check homoscedasticity assumption (or default to heteroscedastic)
  5. Select Radius Logic (Global quantile vs. Local kNN quantile)
  6. Construct/Visualize Ball using distinct visualization metric $d_2$ if desired

- **Design tradeoffs:**
  * Algorithm 2 vs. Algorithm 3: Use Algorithm 2 for rigorous guarantees in low-data, homoscedastic settings; use Algorithm 3 for efficiency and adaptation in complex, varying-noise settings
  * Metric Choice: Informative $d_1$ (like Wasserstein) improves center accuracy but increases computational cost; simple $d_2$ speeds up prediction set computation

- **Failure signatures:**
  * Constant Widths in Heteroscedastic Data: Algorithm 2 used on heteroscedastic data; marginal coverage holds but conditional coverage fails
  * Unstable/Noisy Radii: Algorithm 3 with too small $k$ or high-dimensional covariates; neighbors are effectively random
  * Empty Prediction Sets: Discrete metric spaces without randomization strategies for tie-breaking

- **First 3 experiments:**
  1. Implement Algorithm 3 for Euclidean response space $\mathcal{Y} = \mathbb{R}^2$ using standard Euclidean mean/distance
  2. Reproduce Simulation Setting 2 (Non-linear, heteroscedastic): $X \sim U(0,5), Y = 3 + e^X + X\epsilon$
  3. Evaluate conditional coverage using GAM to estimate $P(Y \in \hat{C}_\alpha(X) | X)$ and calculate L2 error from target $1-\alpha$

## Open Questions the Paper Calls Out

- **Question:** Can the framework be extended to causal inference problems like synthetic controls or settings involving dependent data like time series?
  - **Basis in paper:** Authors explicitly state future work plans to extend methods to causal inference and dependent data
  - **Why unresolved:** Current guarantees rely on i.i.d. assumptions violated in time-series or causal structures
  - **What evidence would resolve it:** Derivation of guarantees for metric-space regression under dependence or proof for metric-based synthetic control methods

- **Question:** Can the heteroscedastic kNN algorithm be modified to provide finite-sample marginal coverage guarantees?
  - **Basis in paper:** Remark 7 notes finite-sample guarantees are not easy to prove for kNN method
  - **Why unresolved:** Standard kNN sacrifices non-asymptotic validity for statistical efficiency and local adaptation
  - **What evidence would resolve it:** Modification proving $P(Y \in \hat{C}_\alpha(X)) \geq 1 - \alpha$ for finite $n$

- **Question:** How can the framework be generalized to produce non-isotropic prediction regions adapting to local distribution shape?
  - **Basis in paper:** Authors acknowledge balls restrict level sets to isotropic regions
  - **Why unresolved:** Metric balls are symmetric; capturing complex uncertainty requires flexible level sets
  - **What evidence would resolve it:** Algorithm constructing ellipsoidal or depth-based regions while retaining coverage guarantees

## Limitations
- Framework validity critically depends on novel "metric-space homoscedasticity" assumption, which may not hold in real-world data
- kNN method lacks finite-sample coverage guarantees and suffers from curse of dimensionality in high-dimensional predictor spaces
- Computational scalability for large datasets with complex metric-space responses (graphs, distributions) remains challenging

## Confidence
- **High Confidence:** Theoretical validity of split conformal approach for metric-space homoscedastic data (Algorithm 2)
- **Medium Confidence:** Asymptotic consistency of kNN radius estimator (Algorithm 3) with uncertain finite-sample performance
- **Medium Confidence:** Benefit of decoupling $d_1$ and $d_2$ is logically sound but lacks strong external validation

## Next Checks
1. Implement formal test or visualization to empirically verify metric-space homoscedasticity assumption before applying Algorithm 2
2. Systematically evaluate kNN method's conditional coverage as predictor dimension $p$ increases ($p=10, 50, 100$) to quantify curse of dimensionality
3. Conduct controlled experiment comparing prediction region quality when using $d_1 = d_2$ versus $d_1 \neq d_2$ on simple metric-space response