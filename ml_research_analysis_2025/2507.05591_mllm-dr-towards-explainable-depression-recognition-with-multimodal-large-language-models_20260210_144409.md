---
ver: rpa2
title: 'MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large
  Language Models'
arxiv_id: '2507.05591'
source_url: https://arxiv.org/abs/2507.05591
tags:
- depression
- llms
- multimodal
- visual
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated depression diagnosis
  from interview videos, which typically lacks clear explanations of how depression
  scores are determined. The proposed method, MLlm-DR, combines a smaller language
  model with a lightweight query module to process multimodal data (text, audio, and
  visual) and generate both depression scores and evaluation rationales.
---

# MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models

## Quick Facts
- arXiv ID: 2507.05591
- Source URL: https://arxiv.org/abs/2507.05591
- Reference count: 40
- Primary result: State-of-the-art explainable depression recognition with 100% F1 on CMDC and 79% F1 on E-DAIC-WOZ

## Executive Summary
MLlm-DR addresses the challenge of automated depression diagnosis from interview videos by combining a smaller language model with a lightweight query module to process multimodal data (text, audio, and visual) and generate both depression scores and evaluation rationales. The approach uses advanced LLMs to create training data with rationales, fine-tunes a smaller model for domain-specific tasks, and introduces a lightweight query module to extract depression-related features from non-textual modalities. Experimental results show state-of-the-art performance on two benchmark datasets (CMDC and E-DAIC-WOZ), achieving 100% precision, recall, and F1-score on CMDC, and 77% precision, 81% recall, and 79% F1-score on E-DAIC-WOZ.

## Method Summary
The method involves a two-stage training pipeline: first, the LQ-former is trained to project audio and visual features into the LLM's embedding space while the LLM remains frozen; second, the LLM is fine-tuned using LoRA adapters and a Multi-Head Attention Aggregation network to jointly optimize for generating rationales and predicting depression scores. The system uses LLaMA-3-8B as the backbone, HuBERT for audio feature extraction, and OpenFace for visual feature extraction, with a lightweight query module (LQ-former) that uses learnable query vectors to summarize non-textual data into fixed-length representations compatible with the LLM.

## Key Results
- Achieves 100% precision, recall, and F1-score on CMDC dataset
- Achieves 77% precision, 81% recall, and 79% F1-score on E-DAIC-WOZ dataset
- Demonstrates strong consistency with expert evaluations (87% agreement on CMDC, 73% on E-DAIC-WOZ)

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Rationale Transfer
- **Claim:** Transferring reasoning capabilities from a large, general-purpose LLM to a smaller, domain-specific LLM via synthetic rationale generation enhances explainability without requiring prohibitive compute resources
- **Mechanism:** The authors use GPT-4o to generate "evaluation rationales" based on dialogue content and ground truth labels. This creates a supervised fine-tuning dataset that teaches a smaller model (LLaMA-3-8B) not just to predict a score, but to mimic the explicit reasoning steps of a larger model
- **Core assumption:** The rationales generated by the teacher model (GPT-4o) are logically consistent with the ground truth labels and accurately reflect the patient's clinical state
- **Evidence anchors:** Section 3.1 describes using advanced LLMs to generate evaluation rationales, section 5.6 shows case studies comparing fully trained vs. LoRA fine-tuned models, and the corpus shows weak direct evidence for this specific approach

### Mechanism 2: Compressed Multimodal Fusion
- **Claim:** Compressing non-textual cues (audio/visual) into fixed-length query vectors allows a text-only LLM to process temporal behavioral signals as "tokens"
- **Mechanism:** The Lightweight Query Module (LQ-former) uses a set of learnable vectors to perform cross-attention on high-dimensional audio/visual features. This projects variable-length, non-textual data into the LLM's static embedding space, effectively translating "sad voice" or "flat affect" into prompt prefixes the LLM can attend to
- **Core assumption:** Depression-relevant features in speech and vision can be effectively summarized by a small number of query vectors (32 in this case) without losing critical temporal nuance
- **Evidence anchors:** Abstract describes LQ-former capturing depression-related features from speech and visual data, section 3.2 details projection into LLM embedding space, and corpus provides general support from multimodal fusion literature

### Mechanism 3: Joint Optimization Consistency
- **Claim:** Joint optimization of language modeling and regression losses enforces consistency between the generated explanation and the numerical score
- **Mechanism:** The model is trained simultaneously to generate text (rationale) via cross-entropy loss and to predict a scalar value (score) via Mean Squared Error (MSE). A Multi-Head Attention Aggregation (MAA) network is used to extract a score from the LLM's hidden states
- **Core assumption:** The internal hidden states of the LLM contain a representation of depression severity that can be mapped linearly to a clinical score, and that this mapping reinforces the semantic generation of the rationale
- **Evidence anchors:** Section 3.3 introduces joint optimization combining causal language modeling and MSE loss, table 3 shows ablation study results, and corpus indicates this joint training approach is a specific feature of this method

## Foundational Learning

- **Cross-Attention and Query-based Fusion (Q-Former)**
  - **Why needed here:** The LQ-former relies on the ability of learnable "query" vectors to sift through a sequence of features (audio/visual) and extract relevant information. You must understand how Attention(Q, K, V) allows a small set of queries to summarize a large input
  - **Quick check question:** How does the LQ-former decide which parts of a 5-minute audio clip are relevant to the "depression" query vectors?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The paper fine-tunes LLaMA-3-8B using LoRA (Low-Rank Adaptation). Understanding this is critical to distinguish between the frozen pre-trained weights and the trainable adapter weights used to learn the specific depression domain
  - **Quick check question:** Which parameters are updated during the fine-tuning stage: the entire LLM or just the low-rank decomposition matrices?

- **Instruction Tuning / Prompt Engineering**
  - **Why needed here:** The system relies on specific output formats ("Evaluation Result: 0-3; Evaluation Reason: ..."). The training data is constructed by prompting GPT-4o to adhere to this structure
  - **Quick check question:** Why is few-shot or zero-shot prompting insufficient for the base LLaMA model in this specific clinical context, necessitating fine-tuning?

## Architecture Onboarding

- **Component map:** Interview Video -> (Text Transcript, Audio Features via HuBERT, Visual Features via OpenFace) -> LQ-former (Audio & Visual modules) -> LLaMA-3-8B with LoRA adapters -> (Language Modeling Head for rationale, MAA Head for regression) -> Structured output (score + rationale)

- **Critical path:**
  1. Data Prep: Use GPT-4o to generate rationale annotations for training data
  2. Stage 1 (Alignment): Train LQ-former with Frozen LLM to align audio/visual features to text space (using score prediction as the objective)
  3. Stage 2 (Joint Fine-tuning): Freeze LQ-former. Fine-tune LLM (LoRA) and MAA Head using combined L_lm and L_mse

- **Design tradeoffs:**
  - Fixed-length Queries vs. Raw Sequence: Using 32 queries in LQ-former reduces computational load but may compress long interviews (20 mins) too aggressively, leading to information loss compared to full sequence processing
  - Synthetic Data vs. Human Expert Data: Training on GPT-4o rationales is scalable but risks bias; human expert data is accurate but scarce

- **Failure signatures:**
  - Modality Disconnect: The model ignores the <AudioHere> and <VideoHere> tokens and relies solely on text (check ablation 'w/o LQ' to see if this is happening)
  - Format Drift: The model generates a coherent rationale but fails to output the numeric score in the strict "Evaluation Result: X" format, breaking the regression extraction
  - Expert Disagreement: The rationale logically follows the text but contradicts clinical consensus (e.g., missing non-verbal cues of a masked depression case)

- **First 3 experiments:**
  1. Modality Ablation: Run inference using only Text, only Audio+Visual (via LQ-former), and All Modalities to quantify the contribution of the LQ-former
  2. Rationale Validity Check: Randomly sample 50 test cases and verify if the "Evaluation Reason" text actually contains keywords from the source transcript (grounding check)
  3. Cross-Dataset Generalization: Train on CMDC (structured interviews) and test on E-DAIC-WOZ (open-ended) to verify if the LQ-former overfits to specific interview styles or captures general depression signals

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the creation of a larger-scale, fine-grained depression label set for speech and visual data improve the model's ability to generate non-textual rationales and reduce bias?
- **Basis in paper:** [explicit] The Conclusion states that constructing the training dataset "solely based on text" leads to potential bias, and explicitly lists "creating a larger-scale, fine-grained depression label set for speech and visual data" as future work
- **Why unresolved:** The current model relies on text-based rationales generated by advanced LLMs (GPT-4o) because "speech and visual data lack large-scale, fine-grained emotional labels," preventing the model from learning to explain non-verbal cues
- **What evidence would resolve it:** Training the LQ-former and LLM on a dataset with annotated emotional labels for audio/visual segments and measuring the resulting increase in performance and rationale consistency with non-verbal inputs

### Open Question 2
- **Question:** To what extent does the structure of the clinical interview (structured vs. open-ended) limit the transferability and accuracy of multimodal depression recognition models?
- **Basis in paper:** [inferred] Section 5.7 highlights that the model performed significantly better on CMDC (structured questions) than E-DAIC-WOZ (open-ended), noting that "data collection methods" play a critical role and offering "valuable insights into optimizing data collection strategies"
- **Why unresolved:** The paper observes the performance gap but does not experimentally isolate the interview format as an independent variable to determine if specific model architectures are required for open-ended dialogues
- **What evidence would resolve it:** A comparative study where the same model architecture is trained and tested on datasets differing only in interview structure (fixed questions vs. free-form conversation)

### Open Question 3
- **Question:** Can the model accurately predict depression scores and generate valid rationales in cases where a participant's verbal content contradicts their non-verbal cues (e.g., suppression)?
- **Basis in paper:** [inferred] Section 5.6 shows the model effectively captures cues in dialogue, but the limitation that rationales are trained only on text (Section 5.4) suggests the model may struggle to explain or prioritize non-verbal contradictions
- **Why unresolved:** The training dataset was constructed using advanced LLMs analyzing dialogue content, meaning the ground truth rationales are derived from text semantics rather than the integration of conflicting multimodal signals
- **What evidence would resolve it:** Testing the model on adversarial samples where participants display high-distress visual/audio features while using low-distress language, followed by an analysis of the resulting scores and reasoning

## Limitations
- The approach's generalizability to larger, more diverse populations remains untested due to reliance on relatively small datasets (78 samples for CMDC, 163 for E-DAIC-WOZ training)
- The paper doesn't provide detailed analysis of potential bias introduced by synthetic data generation using GPT-4o
- Computational efficiency claims are difficult to verify without information about training time and resource requirements for the multi-stage training pipeline

## Confidence
- **High Confidence:** The architectural design combining LQ-former with LLaMA-3-8B for multimodal depression recognition is technically sound and well-documented
- **Medium Confidence:** The reported performance metrics on the two benchmark datasets, given the absence of comparison to recent state-of-the-art methods in the results section
- **Low Confidence:** The claim of achieving "fully explainable" depression recognition, as the paper doesn't provide quantitative measures of explanation quality or user studies validating the clinical utility of the rationales

## Next Checks
1. **Generalization Test:** Train the model on CMDC and test on an entirely separate, held-out clinical dataset to verify performance doesn't degrade significantly with population shifts
2. **Explanation Quality Audit:** Conduct a blinded study where clinical experts evaluate the generated rationales against the audio/visual evidence to quantify hallucination rates and clinical relevance
3. **Computational Efficiency Benchmark:** Measure and report the actual training time and inference latency on standard hardware configurations to validate the claimed computational efficiency advantage