---
ver: rpa2
title: Prediction of Item Difficulty for Reading Comprehension Items by Creation of
  Annotated Item Repository
arxiv_id: '2502.20663'
source_url: https://arxiv.org/abs/2502.20663
tags:
- item
- difficulty
- text
- features
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting item difficulty
  for reading comprehension test items using machine learning models. The authors
  develop a repository of standardized test items from New York and Texas (grades
  3-8, 2017-2023) annotated with linguistic features of passages, test features, context
  features, and LLM-generated embeddings.
---

# Prediction of Item Difficulty for Reading Comprehension Items by Creation of Annotated Item Repository

## Quick Facts
- arXiv ID: 2502.20663
- Source URL: https://arxiv.org/abs/2502.20663
- Reference count: 5
- Primary result: Penalized regression model predicts item difficulty (RMSE 0.52, correlation 0.77) using linguistic features and LLM embeddings of reading comprehension items.

## Executive Summary
This study addresses the challenge of predicting item difficulty for reading comprehension test items using machine learning models. The authors develop a repository of standardized test items from New York and Texas (grades 3-8, 2017-2023) annotated with linguistic features of passages, test features, context features, and LLM-generated embeddings. They model item difficulty using a penalized regression approach that achieves RMSE of 0.52 (baseline 0.92) and correlation of 0.77 between true and predicted difficulty. Models using only linguistic features or only LLM embeddings perform similarly to the full model, suggesting either feature category alone may suffice. The best-performing embeddings come from ModernBERT and LlAMA models. The authors also convert raw p-values to a vertical logit scale using IRT modeling to enable cross-grade comparisons. The model could help educators and test developers predict item difficulty before field testing, potentially reducing costs and improving item quality.

## Method Summary
The authors create a repository of 1,076 reading comprehension items from New York and Texas standardized tests (grades 3-8, 2017-2023). They convert raw p-values to IRT-based easiness scores using NWEA MAP reading achievement norms for vertical scaling. Items are annotated with Coh-Metrix linguistic features (106+ metrics), test features (highlighting, text references), context features (state, grade, year), and LLM embeddings from BERT, ModernBERT, and LlAMA models. A penalized regression approach with ridge regularization and PCA-compressed embeddings predicts item difficulty, achieving RMSE of 0.52 and correlation of 0.77 between true and predicted difficulty.

## Key Results
- Penalized regression model achieves RMSE of 0.52 and correlation of 0.77 between true and predicted item difficulty
- Models using only linguistic features or only LLM embeddings perform similarly to full model (correlations 0.75-0.76)
- ModernBERT and LlAMA embeddings outperform BERT embeddings for prediction
- Converting p-values to vertical logit scale enables cross-grade difficulty comparison

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting grade-specific p-values to a vertical logit scale enables cross-grade difficulty comparison.
- **Mechanism:** The authors use publicly available NWEA MAP reading achievement norms (θg values for each grade) to transform raw p-values into IRT-based easiness scores via: bj = −θg + log(pj/(1−pj)). This accounts for the fact that a 60% correct rate in Grade 3 reflects an easier item than 60% in Grade 8.
- **Core assumption:** The NWEA scale accurately represents average reading ability growth across grades 3–8 in the tested populations.
- **Evidence anchors:**
  - [abstract] "We also convert raw p-values to a vertical logit scale using a Rasch IRT model to enable cross-grade comparisons."
  - [section 4.1] Figure 1 shows adjusted Grade 3 items as easiest on average, while Grade 8 items are hardest after transformation.
  - [corpus] Related work (SMART, arXiv:2507.05129) similarly aligns simulated students with IRT for difficulty prediction, supporting the validity of IRT-based scaling.
- **Break condition:** If the target population's ability distribution diverges significantly from NWEA norms (e.g., different country, post-pandemic shifts), the vertical scale may misalign, corrupting cross-grade comparisons.

### Mechanism 2
- **Claim:** LLM embeddings and handcrafted linguistic features capture overlapping information about item difficulty.
- **Mechanism:** ModernBERT/BERT/LLaMA embeddings encode semantic relationships between passage, question, and distractors. Separately, Coh-Metrix features quantify syntactic complexity, cohesion, and readability. The finding that either category alone performs similarly (correlation ~0.75–0.76) suggests both signal types converge on the same underlying difficulty construct.
- **Core assumption:** Pre-trained LLMs transfer sufficiently to K-12 reading comprehension items without domain-specific fine-tuning.
- **Evidence anchors:**
  - [abstract] "When models use only item linguistic features or LLM embeddings, prediction performance is similar, which suggests that only one of these feature categories may be required."
  - [section 5.2] ModernBERT embeddings alone achieve test RMSE 0.62, correlation 0.76; adding all features only marginally improves to RMSE 0.61, correlation 0.77.
  - [corpus] Ha et al. (2019) and related BEA 2024 shared task show LLM embeddings combined with linguistic features improve MCQ difficulty prediction, though gains vary by dataset.
- **Break condition:** If items require domain-specific knowledge not captured in pre-training corpora (e.g., specialized vocabulary), embeddings may underperform relative to handcrafted features.

### Mechanism 3
- **Claim:** Penalized regression with PCA-compressed embeddings prevents overfitting on small datasets while preserving predictive signal.
- **Mechanism:** Ridge regression (L2 penalty) with 5-fold CV tunes λ; PCA reduces embedding dimensions (768–4096) to components explaining 80% variance. This addresses the p >> n problem (1076 items, many more predictors).
- **Core assumption:** The relationship between features and difficulty is approximately linear; non-linear interactions are negligible.
- **Evidence anchors:**
  - [section 4.4] "Elastic net models that tune the ratio between ridge and lasso (α) always converged to the more parsimonious ridge model."
  - [section 5.1] "PCA on embeddings performs marginally better than actual embeddings; this is likely because of the small dataset size compared to the number of predictors."
  - [corpus] Corpus evidence on tree-based methods (arXiv:2504.08804) suggests non-linear approaches may offer alternatives, but direct comparison is absent.
- **Break condition:** If difficulty depends on complex feature interactions (e.g., passage length × question type), linear models may underfit; tree-based or neural approaches could then outperform.

## Foundational Learning

- **Concept: Item Response Theory (IRT) – 1PL/Rasch Model**
  - **Why needed here:** The outcome variable is IRT-derived easiness (bj), not raw p-values. Understanding the sigmoid link between ability (θ), item easiness, and response probability is essential.
  - **Quick check question:** If θg = 210 for Grade 5 and an item has p = 0.70, what is the transformed easiness bj?

- **Concept: Penalized Regression (Ridge/L2)**
  - **Why needed here:** The prediction model uses ridge regression with cross-validated λ to handle multicollinearity among 100+ linguistic features and compressed embeddings.
  - **Quick check question:** Why does L2 penalization help when predictors are highly correlated (e.g., multiple readability metrics)?

- **Concept: Sentence Embeddings from Transformers**
  - **Why needed here:** Embeddings from BERT/ModernBERT/LLaMA serve as dense vector representations of passage+question+distractors; understanding pooling strategies (last token, mean) is needed to replicate.
  - **Quick check question:** The authors extract the last hidden state corresponding to the final non-padding token—why might this capture sentence-level semantics?

## Architecture Onboarding

- **Component map:** Raw items -> p-value to IRT conversion -> text preprocessing -> Feature Extraction (Coh-Metrix, LLM embeddings) -> PCA compression -> Ridge regression -> predicted difficulty

- **Critical path:**
  1. Verify p-value to IRT conversion using correct θg values from NWEA norms
  2. Ensure embedding input format matches training (question + correct answer + wrong answers + passage)
  3. Apply same PCA transformation (fit on training data only) before inference

- **Design tradeoffs:**
  - **Embeddings only vs. full feature set:** Embeddings alone nearly match full model; trade compute for interpretability.
  - **Embedding model choice:** ModernBERT best, but LLaMA (4096-dim) more expensive; BERT sufficient for smaller datasets.
  - **With vs. without passage in embedding input:** Removing passage degrades BERT-only performance (RMSE 0.66→0.72), but full model recovers.
  - **Assumption:** Linear model sufficient; non-linear alternatives not explored in paper.

- **Failure signatures:**
  - Predictions systematically off for specific grades → vertical scale misalignment; verify θg values.
  - High train/test RMSE gap → overfitting; increase regularization or reduce feature count.
  - Embedding-only model fails on new item types → domain shift; may need fine-tuning or feature augmentation.

- **First 3 experiments:**
  1. **Reproduce baseline:** Train ridge regression on context features only (state, grade, year); confirm baseline RMSE ≈ 0.92, correlation ≈ 0.72.
  2. **Ablate feature sets:** Compare (a) text analysis features only, (b) ModernBERT embeddings only, (c) full model; expect correlations within 0.01–0.02 of each other.
  3. **Test embedding formats:** Generate embeddings with vs. without passage text for BERT; expect RMSE increase when passage excluded (0.66→0.72), recovered when text features added.

## Open Questions the Paper Calls Out
None

## Limitations
- **State-specific data:** Model trained and tested on items from only two states (NY and Texas), limiting generalizability to other regions.
- **IRT scaling assumptions:** Relies on external NWEA norms that may not perfectly align with state test populations, potentially introducing bias.
- **Linear model assumption:** The paper uses linear regression and doesn't explore non-linear approaches that might capture complex interactions between features.

## Confidence

- **Confidence in IRT scaling:** Medium. The conversion of p-values to vertical logit scale relies on external NWEA norms that may not perfectly align with state test populations. The authors acknowledge this as a limitation, noting that regional ability differences could introduce bias.

- **Confidence in model performance:** High. The paper reports consistent results across multiple validation runs (5-fold CV × 5 repeats), with clear baseline comparisons. The RMSE improvement from 0.92 to 0.52 is substantial and the correlation of 0.77 between true and predicted difficulty is well-documented.

- **Confidence in generalizability:** Low-Medium. The model is trained and tested on items from only two states (NY and Texas), raising questions about performance on items from other regions or with different item formats.

## Next Checks

1. **External validation:** Test the model on reading comprehension items from states other than NY and Texas to assess generalizability. Compare performance metrics to ensure the model generalizes beyond the training populations.

2. **Longitudinal stability:** Evaluate model performance on items from different years (e.g., pre- and post-pandemic) to detect any temporal drift in the relationship between features and difficulty.

3. **Feature importance analysis:** Conduct ablation studies to determine which specific Coh-Metrix metrics or embedding components contribute most to prediction accuracy, and whether these align with educational theory about item difficulty.