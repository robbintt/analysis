---
ver: rpa2
title: 'Large language models for behavioral modeling: A literature survey'
arxiv_id: '2509.24782'
source_url: https://arxiv.org/abs/2509.24782
tags:
- llms
- diagrams
- modeling
- software
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey identifies 14 studies on LLM-based generation of UML
  behavioral models, focusing on sequence diagrams (11 studies) and use case diagrams
  (9 studies). The majority employed GPT-based models (GPT-3.5, GPT-4, GPT-4o, GPT-4-turbo),
  with only one study evaluating open-source models (DeepSeek-Coder, CodeLlama).
---

# Large language models for behavioral modeling: A literature survey

## Quick Facts
- arXiv ID: 2509.24782
- Source URL: https://arxiv.org/abs/2509.24782
- Reference count: 32
- Primary result: 14 studies identified using LLMs for UML behavioral modeling, primarily GPT-based models generating sequence and use case diagrams

## Executive Summary
This semi-systematic literature survey examines 14 studies utilizing Large Language Models for generating UML behavioral models, specifically sequence and use case diagrams. The research reveals a heavy reliance on proprietary GPT models (13 of 14 studies) with minimal evaluation of open-source alternatives. The studies predominantly employ researcher-led or expert-based evaluation methods, highlighting a significant gap in standardized, automated assessment frameworks. The findings indicate promising performance in diagram generation but underscore critical limitations in evaluation rigor and model diversity that constrain generalizability and reproducibility.

## Method Summary
The author conducted a semi-systematic literature review using Google Scholar searches with specific Boolean queries targeting LLM applications in UML sequence and use case diagrams. The search, executed on June 29, 2025, yielded 113 total hits across two queries. After title and abstract screening to remove duplicates and irrelevant papers, 14 primary studies were identified for detailed analysis. Data extraction followed a structured template capturing task types, LLM models used, data sources, and evaluation approaches, with results aggregated in descriptive statistics.

## Key Results
- GPT-based models dominate the field with 13 of 14 studies using GPT-3.5, GPT-4, GPT-4o, or GPT-4-turbo
- Only one study evaluated open-source models (DeepSeek-Coder, CodeLlama) compared to proprietary GPT family
- Sequence diagrams were the primary focus (11 studies) compared to use case diagrams (9 studies)
- Evaluation methods relied heavily on researcher-led assessment (7 studies) rather than automated or standardized approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs map unstructured natural language requirements into structured UML behavioral syntax through probabilistic pattern matching
- **Mechanism:** The model processes requirement documents to identify actors, actions, and system boundaries, then predicts corresponding formal notation based on training data aligning code/text pairs with visual structures
- **Core assumption:** The LLM internalized syntactic rules of UML and semantic mapping between natural language verbs and diagram components during pre-training
- **Evidence anchors:** Abstract mentions LLMs extensively utilized for behavioral modeling; Table 3 identifies requirement documents and user stories as primary data sources successfully transformed into diagrams
- **Break condition:** Ambiguous requirements with unclear antecedents lead to hallucinated actors or incorrect sequence flows

### Mechanism 2
- **Claim:** LLMs generalize across behavioral modeling tasks by leveraging shared context about system functionality
- **Mechanism:** The model uses system description to construct domain mental model, identifying user goals for Use Case diagrams and ordering interactions for Sequence diagrams
- **Core assumption:** Semantic understanding for high-level scoping is compatible with granular logic required for interaction ordering
- **Evidence anchors:** Page 4 notes 9 studies focused on both diagrams, indicating same underlying model capabilities; abstract states focus on use case and sequence diagrams
- **Break condition:** Complex logic involving loops, alt-blocks, or parallel gates often fails to render correctly

### Mechanism 3
- **Claim:** Current operational viability relies on "Human-in-the-Loop" validation rather than automated correctness guarantees
- **Mechanism:** LLM generates draft diagram manually inspected by researcher or domain expert, treating model as accelerator for boilerplate generation
- **Core assumption:** Human evaluators possess consistency and domain knowledge to effectively validate generated models
- **Evidence anchors:** Table 4 shows researchers as evaluators (7 studies) and expert-based evaluation (3 studies) as dominant methods; abstract notes literature lacks expert-based evaluations
- **Break condition:** Reviewer fatigue or lack of specific domain knowledge leads to false positive acceptance of semantically incorrect diagrams

## Foundational Learning

- **Concept:** **UML Behavioral Semantics (Sequence vs. Use Case)**
  - **Why needed here:** To effectively prompt an LLM, one must distinguish between structural scope (Use Case: Actors + Goals) and interaction logic (Sequence: Messages + Time)
  - **Quick check question:** Can you identify if a prompt asking "How does the user reset the password?" implies a Use Case or a Sequence Diagram?

- **Concept:** **Prompt Engineering for Formal Syntax**
  - **Why needed here:** The paper implies models output diagrams via text-based representations (e.g., PlantUML), requiring knowledge of how to constrain output to valid syntax
  - **Quick check question:** Do you know how to instruct an LLM to output code rather than Markdown text?

- **Concept:** **Evaluation Triangulation**
  - **Why needed here:** The survey highlights major gap in evaluation rigor, requiring understanding of differences between researcher inspection, student survey, and automated rule-based checking
  - **Quick check question:** Why is a "student-centric evaluation" potentially less rigorous than an "expert-based evaluation" for validating architectural correctness?

## Architecture Onboarding

- **Component map:** Natural Language Requirements -> LLM Model -> Prompt Engineering interface -> Text-based diagram code (PlantUML/Mermaid) -> Visual graphs -> Human Expert or LLM-as-a-Judge validation
- **Critical path:** The fidelity of the Input -> Model translation, as prompts must strictly enforce specific UML syntax to avoid descriptive text output
- **Design tradeoffs:**
  - Proprietary (GPT) vs. Open Source (CodeLlama): GPT dominates for ease of use and higher performance but limits reproducibility and privacy; open source allows local hosting but requires higher setup effort
  - Researcher vs. Automated Eval: Human evaluation is slow and subjective; automated evaluation is fast but rigid and struggles with semantic nuance
- **Failure signatures:**
  - Syntax Hallucination: Model invents fake UML tags that break the renderer
  - Scope Creep in Generation: Model adds interactions not present in input requirements
  - Evaluator Bias: High "promising results" may stem from researcher-led evaluation rather than objective benchmarks
- **First 3 experiments:**
  1. Baseline Replication: Run 3 diverse system descriptions through GPT-4o and local CodeLlama to compare syntax validity rates
  2. Evaluation Contrast: Generate sequence diagrams evaluated by both LLM-as-a-Judge and rule-based parser to identify discrepancies
  3. Model Diversity Test: Test open-source models against GPT-3.5 on complex sequence logic to see if code-focused models outperform generalist models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do open-source LLMs compare to proprietary GPT-based models in generating accurate UML behavioral diagrams?
- Basis in paper: The authors explicitly state the need to "evaluate a broader range of LLMs" and note only one study assessed open-source models against the dominant GPT family
- Why unresolved: Literature heavily skewed toward GPT-3.5 and GPT-4, leaving efficacy and cost-benefit trade-offs of open-source alternatives largely unknown
- What evidence would resolve it: Comparative benchmark study evaluating proprietary and open-source models on identical behavioral modeling tasks using standardized metrics

### Open Question 2
- Question: What standardized, automated evaluation frameworks can effectively assess the quality of LLM-generated sequence and use case diagrams?
- Basis in paper: Discussion highlights "Need for robust evaluation approaches," specifically calling for "standardized rubrics... and automated consistency checks" to replace subjective human reviews
- Why unresolved: Majority of studies relied on subjective "researchers as evaluators," and field currently lacks rigorous, automated assessment methods
- What evidence would resolve it: Development and validation of automated evaluation tool showing strong correlation with human expert assessments

### Open Question 3
- Question: How does the evaluation of LLM-generated diagrams differ when conducted by domain experts versus the researchers themselves?
- Basis in paper: Abstract and conclusion explicitly identify need to "involve domain experts to evaluate the output of LLMs," as most current studies lack expert-based evaluations
- Why unresolved: Only three studies utilized expert-based evaluation, creating potential bias where model performance assessed solely by study authors
- What evidence would resolve it: Comparative analysis of evaluation scores assigned by domain experts versus researchers for same set of generated models

## Limitations
- Narrow search scope focusing exclusively on UML sequence and use case diagrams may have missed relevant papers using broader terminology
- Heavy reliance on researcher-led and expert-based evaluations introduces subjectivity that cannot be fully quantified
- Absence of automated, standardized evaluation methods means reported "promising results" cannot be objectively validated across studies

## Confidence
- **High Confidence:** Observed dominance of GPT models (13/14 studies) and prevalence of researcher-led evaluation methods are clearly supported by data extraction
- **Medium Confidence:** Characterization of LLM performance as "promising" is based on authors' interpretation of studies lacking rigorous benchmarking
- **Low Confidence:** Generalizability to other UML diagram types or non-UML behavioral modeling remains speculative due to study's narrow focus

## Next Checks
1. **Search Term Expansion:** Repeat literature search using alternative terms like "behavioral diagram generation," "activity diagram," and "state machine" to identify missed studies
2. **Automated Evaluation Implementation:** Develop and apply simple rule-based parser to validate UML syntax of generated diagrams from at least three studies, comparing results against reported researcher evaluations
3. **Open-Source Model Testing:** Conduct controlled experiments generating sequence diagrams from standardized requirement sets using GPT-4 and DeepSeek-Coder to empirically test performance differences