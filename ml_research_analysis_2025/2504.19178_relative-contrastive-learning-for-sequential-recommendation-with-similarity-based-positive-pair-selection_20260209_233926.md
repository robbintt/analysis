---
ver: rpa2
title: Relative Contrastive Learning for Sequential Recommendation with Similarity-based
  Positive Pair Selection
arxiv_id: '2504.19178'
source_url: https://arxiv.org/abs/2504.19178
tags:
- positive
- samples
- sequences
- sequence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Relative Contrastive Learning (RCL) framework
  for sequential recommendation that addresses data sparsity and noise issues by using
  both same-target sequences and similar sequences as positive samples for contrastive
  learning. The key innovation is a dual-tiered positive sample selection module that
  treats same-target sequences as strong positive samples and similar sequences as
  weak positive samples, combined with a relative contrastive learning module that
  uses a weighted loss function to ensure each sequence is represented closer to its
  strong positive samples than its weak positive samples.
---

# Relative Contrastive Learning for Sequential Recommendation with Similarity-based Positive Pair Selection

## Quick Facts
- arXiv ID: 2504.19178
- Source URL: https://arxiv.org/abs/2504.19178
- Reference count: 40
- 4.88% improvement on average compared to state-of-the-art methods

## Executive Summary
This paper proposes a Relative Contrastive Learning (RCL) framework for sequential recommendation that addresses data sparsity and noise issues by using both same-target sequences and similar sequences as positive samples for contrastive learning. The key innovation is a dual-tiered positive sample selection module that treats same-target sequences as strong positive samples and similar sequences as weak positive samples, combined with a relative contrastive learning module that uses a weighted loss function to ensure each sequence is represented closer to its strong positive samples than its weak positive samples. Experiments on six datasets show that RCL achieves 4.88% improvement on average compared to state-of-the-art methods, demonstrating its effectiveness in improving recommendation performance. The approach also shows consistent improvements when applied to different base models including both transformer-based and MLP-based architectures.

## Method Summary
RCL is a supervised contrastive learning framework for sequential recommendation that augments positive samples through similarity-based selection. It uses a dual-tiered approach where same-target sequences serve as strong positives and similar sequences (identified via offline similarity metrics like Jaccard or N-gram) serve as weak positives. The framework employs a weighted relative contrastive loss that ensures each sequence is represented closer to its strong positive samples than its weak positive samples. The method pre-computes sequence similarities offline to identify weak positive candidates, then uses a weighted loss function during training that combines the standard recommendation loss with the relative contrastive loss, where the latter is bounded to maintain the hierarchy between strong and weak positives.

## Key Results
- RCL achieves 4.88% improvement on average across six datasets compared to state-of-the-art methods
- The method consistently improves performance when applied to both transformer-based (SASRec) and MLP-based (FMLP) base models
- Ablation studies show that the weighted relative contrastive loss outperforms unweighted alternatives, and that the combined strong+weak positive approach is superior to using either alone

## Why This Works (Mechanism)

### Mechanism 1: Dual-Tiered Positive Sample Augmentation
Standard Supervised Contrastive Learning (SCL) relies on same-target sequences, but in datasets like ML-1M, 47.5% of sequences lack a same-target pair. RCL identifies "weak" positives using offline similarity metrics (e.g., Jaccard, N-gram) to ensure 100% of sequences have at least one positive sample. Sequences with high historical similarity (e.g., Jaccard > 0.7) share latent user intent, even if their immediate target items differ.

### Mechanism 2: Relative Contrastive Loss Boundary
Standard InfoNCE treats all positives equally. RCL modifies the loss to include a boundary constraint: the loss for a weak positive pair is capped by the maximum loss of the strong positive pairs ($L_{weak} \le L_{max}^{strong}$). This forces the model to prioritize strong alignment while maintaining weak proximity, treating user intent as a continuum where same-target sequences represent a "stronger" signal of identical intent than similar-history sequences.

### Mechanism 3: Similarity-Based Re-weighting
Instead of a binary 0/1 label for positive samples, the loss term integrates the pre-computed similarity score $s_{u,a}$. This transforms the objective from simple binary alignment to a regression-like alignment proportional to semantic overlap, penalizing the model more for dissimilar "positives" and less for highly similar ones.

## Foundational Learning

- **Concept:** InfoNCE / Contrastive Loss
  - **Why needed here:** RCL modifies the standard InfoNCE loss. You must understand how InfoNCE pushes negatives away and pulls positives together to grasp why a "boundary" is needed to stop weak positives from being treated as negatives.
  - **Quick check question:** In a standard InfoNCE loss, what happens to the gradient if a "positive" sample is actually dissimilar to the anchor?

- **Concept:** Sequence Similarity Metrics (Jaccard, Levenshtein, N-gram)
  - **Why needed here:** The framework relies on an offline pre-computation step to find weak positives. Understanding these metrics is required to debug the "quality" of the weak positives selected.
  - **Quick check question:** Why might Levenshtein distance (order-sensitive) perform worse than Jaccard (order-insensitive) in the ablation study (Figure 3)?

- **Concept:** Supervised Contrastive Learning (SCL) in RecSys
  - **Why needed here:** RCL is positioned as an evolution of SCL (e.g., DuoRec). You need to know that SCL uses "same-target" sequences to solve the data sparsity problem, to understand why RCL adds "similar sequences."
  - **Quick check question:** What is the specific data sparsity issue with "same-target" sampling that RCL tries to solve?

## Architecture Onboarding

- **Component map:** Offline Selector -> Data Loader -> Encoder -> Loss Head
- **Critical path:**
  1. Batch Creation: efficiently sampling weak positives from the pre-computed index without causing data loading bottlenecks
  2. Loss Calculation (Eq. 17): Computing the `max` boundary between strong and weak losses effectively

- **Design tradeoffs:**
  - Pre-compute vs. Online Calc: The paper pre-computes similarity (Section 3.3). This consumes $O(N^2)$ memory/storage but decouples similarity speed from training speed
  - Metric Choice: Figure 3 shows order-sensitive metrics (Levenshtein) underperforming. Using simpler metrics (Jaccard/TF-IDF) offers better performance/complexity tradeoff
  - Ratio $\alpha$: Figure 4 shows performance drops if $\alpha$ (weak positive pool size) is too large (>10-15%), introducing noise

- **Failure signatures:**
  - Performance Degradation with High $\alpha$: If HR/NDCG drops as $\alpha$ increases, the weak positive pool is diluting the strong signal
  - Weak < Strong Distance Violation: If t-SNE visualization or dot products show Weak Positives closer to Anchor than Strong Positives, the relative loss weight $\lambda$ or the boundary logic in Eq. 14 may be incorrect
  - Stalling Loss: If $L_{wRCL}$ collapses to 0, check if the boundary $L_{max}^u$ is dominating the weak loss term improperly

- **First 3 experiments:**
  1. Sanity Check (Ablation): Run "Strong-only" vs. "Weak-only" vs. "RCL (Combined)" on a small dataset (Beauty) to verify the implementation of the relative loss boundary (Fig 3 replication)
  2. Hyperparameter $\alpha$: Sweep the "top similar sequence ratio" $\alpha$ (e.g., 0.025 to 0.2) to find the noise/signal sweet spot for your specific dataset distribution (replicating Fig 4 trend)
  3. Visualization: Extract embeddings for a set of anchors, strong positives, weak positives, and negatives. Calculate average dot products to empirically verify the hierarchy: Strong > Weak > Negative (replicating Fig 6)

## Open Questions the Paper Calls Out

### Open Question 1
Can the sequence similarity calculation be scaled efficiently beyond heuristic constraints (e.g., restricting to specific cities or days) for extremely large industrial datasets? While the authors utilize GPU acceleration, the method retains a quadratic complexity $O(N^2)$ relative to sequence count. The proposed solution relies on external metadata constraints rather than an algorithmic approximation for the general case.

### Open Question 2
Would a learnable, task-adaptive similarity metric outperform the fixed heuristics (e.g., Jaccard, N-grams) currently used for weak positive selection? The paper does not explore if the definition of similarity can be optimized jointly with the recommendation task rather than selected via grid search over standard metrics.

### Open Question 3
How does the Relative Contrastive Learning framework interact with explicit hard negative mining strategies? It is unclear if the relative loss boundary (Eq. 14) creates gradient conflicts or synergies when specific "hard negative" samples (sequences that are dissimilar but difficult to rank) are explicitly mined.

## Limitations
- Offline pre-computation requirement scales quadratically with dataset size, creating memory constraints for large-scale deployment
- High similarity thresholds (Jaccard > 0.7) sometimes miss weak positive pairs, suggesting the similarity metric may not fully capture user intent in all cases
- Cannot adapt to rapidly changing user behavior patterns in real-time recommendation systems due to offline pre-computation

## Confidence

- **High Confidence:** The dual-tiered positive selection framework and its implementation details are well-documented and reproducible through the provided codebase
- **Medium Confidence:** The performance improvements (4.88% average gain) are well-supported by experiments, though results may vary across different dataset characteristics
- **Medium Confidence:** The theoretical justification for using weak positives as semantic signals is sound, but empirical validation of this assumption is limited to specific similarity metrics
- **Low Confidence:** The scalability analysis for very large datasets is insufficient, and the method's performance on cold-start scenarios is not explicitly addressed

## Next Checks

1. **Scalability Test:** Evaluate RCL performance and memory usage on progressively larger subsets of ML-20M to determine practical dataset size limits
2. **Cross-Domain Validation:** Test the method on non-ecommerce sequential recommendation tasks (e.g., music or video streaming) to assess generalizability beyond the evaluated domains
3. **Dynamic Update Analysis:** Implement an incremental similarity update mechanism to measure performance degradation over time in streaming data scenarios