---
ver: rpa2
title: 'Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space
  Models'
arxiv_id: '2501.02832'
source_url: https://arxiv.org/abs/2501.02832
tags:
- mamba
- speech
- audio
- performance
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Samba-ASR is the first state-of-the-art ASR model using Mamba SSMs
  as encoder and decoder, replacing transformers to address quadratic complexity in
  long audio sequences. By leveraging state-space modeling with selective recurrence,
  it efficiently captures both local and global temporal dependencies with linear
  computational scaling.
---

# Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models

## Quick Facts
- arXiv ID: 2501.02832
- Source URL: https://arxiv.org/abs/2501.02832
- Reference count: 28
- SOTA ASR model using Mamba SSMs, achieving 1.17% WER on LibriSpeech Clean

## Executive Summary
Samba-ASR is the first state-of-the-art ASR model using Mamba state-space models as both encoder and decoder, replacing transformers to address quadratic complexity in long audio sequences. By leveraging selective recurrence and input-dependent parameters, it efficiently captures both local and global temporal dependencies with linear computational scaling. Extensive evaluations on GigaSpeech, LibriSpeech Clean/Other, and SPGISpeech show an average WER of 3.65%, setting new benchmarks especially on LibriSpeech Clean (1.17%) and SPGISpeech (1.84%), while reducing inference latency and training time compared to transformer-based models.

## Method Summary
Samba-ASR replaces transformer attention with Mamba state-space models for both encoder and decoder. The architecture uses convolutional subsampling to process log-Mel spectrograms, followed by stacked Mamba blocks with a novel cross-connection mechanism between encoder and decoder. Training employs AdamW optimizer with linear learning rate decay, gradient clipping, and hardware-aware optimizations including kernel fusion and parallel scan for efficient state updates. The model achieves linear computational scaling while maintaining autoregressive generation through causal masking in the decoder.

## Key Results
- Achieves 1.17% WER on LibriSpeech Clean (SOTA)
- Sets 1.84% WER benchmark on SPGISpeech
- Average WER of 3.65% across GigaSpeech, LibriSpeech Clean/Other, and SPGISpeech
- Reduces inference latency and training time compared to transformer-based models

## Why This Works (Mechanism)

### Mechanism 1
Selective state-space dynamics enable efficient long-range dependency capture without quadratic attention costs. Mamba introduces input-dependent parameters B(xₜ) and C(xₜ) into state-space equations hₜ₊₁ = A(hₜ) + B(xₜ), yₜ = C(xₜ)hₜ, allowing selective propagation of relevant information while compressing context. This works because speech contains both local acoustic features and global linguistic dependencies that can be modeled through learned state transitions without explicit pairwise attention.

### Mechanism 2
Hardware-aware implementation with kernel fusion and parallel scan achieves genuine linear scaling during training. State updates are computed in fast SRAM rather than materializing large latent states in HBM, trading computation for memory through recomputation during backpropagation. Parallel scan enables O(log n) sequential dependency resolution by exploiting GPU memory hierarchy bottlenecks.

### Mechanism 3
Mamba-cross-connection enables targeted audio-text alignment without full cross-attention. The decoder conditions on encoded audio features through a cross-connection mechanism that allows focusing on relevant audio segments during token prediction while maintaining autoregressive causality. This works because audio-text alignment in ASR does not require computing attention weights between every decoder position and every encoder position.

## Foundational Learning

- **State-Space Models (SSMs)**: Understanding how hₜ₊₁ = A(hₜ) + B(xₜ) differs from attention-based sequence modeling is essential for debugging convergence issues. Quick check: Given a 10-second audio clip at 16kHz with 80 Mel bands, what is the approximate sequence length input to the Mamba encoder after convolutional subsampling?

- **Selective Recurrence**: The key innovation is input-dependent B and C matrices—without grasping this, you cannot understand why Mamba outperforms earlier SSMs on speech. Quick check: How do input-dependent parameters differ from LTI dynamics, and why might this matter for distinguishing phonemes in noisy speech?

- **Hardware-Aware Algorithm Design**: Mamba's efficiency claims depend on GPU-specific optimizations; understanding SRAM vs HBM tradeoffs is critical for profiling real-world throughput. Quick check: Why does recomputation during backpropagation reduce memory usage, and what is the computational cost tradeoff?

## Architecture Onboarding

- **Component map**: Raw audio → 16kHz resampling → log-Mel spectrogram → normalization → conv subsampling → Mamba encoder blocks → LayerNorm → Mamba-cross-connection → Mamba decoder blocks → LayerNorm → linear projection → softmax

- **Critical path**: Audio preprocessing quality directly impacts Mel spectrogram fidelity → Encoder Mamba blocks must capture sufficient acoustic context before cross-connection → Decoder cross-connection alignment determines transcription accuracy → Causal masking in decoder enforces autoregressive generation

- **Design tradeoffs**: Pure Mamba vs hybrid (Transformer-Mamba): Paper uses pure Mamba; Jamba and Zamba show hybrids may help with in-context reasoning—Samba-ASR does not explore this → Encoder depth vs inference latency: Deeper encoders improve accuracy but increase latency; paper does not ablate this

- **Failure signatures**: Training loss plateaus early: Check learning rate schedule and gradient clipping → High WER on spontaneous speech: May indicate insufficient training data diversity → Memory overflow on long sequences: Verify hardware-aware kernel fusion is active

- **First 3 experiments**: Baseline replication: Reproduce training on LibriSpeech Clean with paper-specified hyperparameters → Ablation: Cross-connection alternatives: Compare paper's Mamba-cross-connection against concatenation and full cross-attention → Sequence length stress test: Evaluate on synthetic long-form audio to verify linear scaling claims

## Open Questions the Paper Calls Out

- **Multilingual extension**: Can the architecture be adapted for multilingual speech recognition and translation, particularly in low-resource language scenarios? This remains untested as current evaluation is exclusively on English datasets.

- **Edge deployment**: How does accuracy and latency scale when compressed into lightweight variants for deployment on edge devices? The paper does not evaluate parameter reduction or quantization requirements for mobile/edge environments.

- **Self-supervised pre-training**: Does self-supervised pre-training on unlabeled audio data significantly improve generalization to diverse accents and noisy environments? The benefit of large-scale pre-training for Mamba blocks in ASR remains unquantified.

## Limitations

- Mamba-cross-connection mechanism details are underspecified, requiring architectural experimentation for faithful reproduction
- Model dimensions (number of blocks, hidden sizes, state dimensions) and hardware configurations are not provided
- Hardware-aware optimizations (SRAM/HBM memory management) are claimed but not independently verified

## Confidence

- **High Confidence**: SOTA WER claims on benchmark datasets are well-supported by presented evaluation results
- **Medium Confidence**: Linear computational scaling claims are supported theoretically but lack detailed implementation validation
- **Low Confidence**: Architectural details of Mamba-cross-connection mechanism are insufficient for faithful reproduction

## Next Checks

1. **Cross-connection ablation study**: Implement and compare three variants—paper's Mamba-cross-connection, simple concatenation, and full cross-attention—measuring WER and inference latency on LibriSpeech

2. **Hardware scaling verification**: Profile GPU memory usage and throughput on synthetic long-form audio (5-30 minutes) using different batch sizes, comparing against Whisper-large-v3 to verify linear scaling claims

3. **Parameter sensitivity analysis**: Systematically vary Mamba block configurations (state dimension, expand factor, number of blocks) and encoder depth to identify minimum viable configuration maintaining SOTA performance