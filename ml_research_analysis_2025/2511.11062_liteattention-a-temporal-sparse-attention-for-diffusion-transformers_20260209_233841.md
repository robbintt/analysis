---
ver: rpa2
title: 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers'
arxiv_id: '2511.11062'
source_url: https://arxiv.org/abs/2511.11062
tags:
- attention
- sparsity
- liteattention
- diffusion
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiteAttention addresses the high computational cost of attention
  in video diffusion transformers by exploiting the temporal coherence of sparsity
  patterns across denoising steps. Instead of recomputing sparse attention patterns
  at each step, LiteAttention identifies skippable tiles early and propagates these
  skip decisions forward, achieving evolutionary computation skips.
---

# LiteAttention: A Temporal Sparse Attention for Diffusion Transformers

## Quick Facts
- **arXiv ID**: 2511.11062
- **Source URL**: https://arxiv.org/abs/2511.11062
- **Reference count**: 4
- **Primary result**: Achieves up to 50% runtime reduction at 77% sparsity in video diffusion transformers

## Executive Summary
LiteAttention addresses the high computational cost of attention in video diffusion transformers by exploiting temporal coherence in sparsity patterns across denoising steps. The method identifies skippable computation tiles early and propagates these skip decisions forward through evolutionary computation skips. This approach combines the adaptivity of dynamic sparsity with the efficiency of static methods, eliminating full attention computations for marked tiles without repeated profiling. Implemented as a CUDA kernel extension to FlashAttention3, LiteAttention achieves substantial runtime improvements on production video diffusion models while preserving video quality.

## Method Summary
LiteAttention introduces temporal sparse attention for video diffusion transformers by leveraging the observation that attention sparsity patterns remain temporally coherent across denoising steps. The method works by identifying skippable tiles during early denoising steps and propagating these skip decisions forward through subsequent steps, creating an evolutionary computation skip mechanism. This approach eliminates the need to recompute sparse attention patterns at each step, combining the adaptivity of dynamic sparsity with the efficiency of static approaches. The implementation extends FlashAttention3 with a custom CUDA kernel that marks tiles as skippable based on their sparsity patterns, allowing the system to avoid full attention computations for these tiles in future steps.

## Key Results
- Achieves up to 50% runtime reduction in video diffusion transformer inference
- Maintains quality at 77% sparsity levels
- Successfully implemented as CUDA kernel extension to FlashAttention3
- Demonstrates evolutionary computation skips through temporal coherence exploitation

## Why This Works (Mechanism)
The core mechanism relies on the temporal coherence of attention sparsity patterns across denoising steps in video diffusion transformers. During the denoising process, attention patterns evolve gradually rather than changing drastically between steps. LiteAttention capitalizes on this by identifying which attention computation tiles can be skipped based on their sparsity patterns early in the process, then propagating these skip decisions forward. This creates an evolutionary computation skip where once a tile is marked as skippable, it remains skippable for subsequent denoising steps. The approach effectively amortizes the cost of sparsity pattern identification across multiple computation steps, achieving significant efficiency gains without sacrificing quality.

## Foundational Learning
- **Temporal coherence in attention patterns**: Understanding that attention sparsity patterns evolve gradually across denoising steps is crucial for justifying the skip propagation mechanism. Quick check: Verify that attention patterns show consistent sparsity across multiple consecutive denoising steps.
- **Evolutionary computation skips**: The concept of marking computations as skippable and propagating these decisions forward is fundamental to LiteAttention's efficiency gains. Quick check: Confirm that skipped computations don't accumulate error or degrade output quality over multiple steps.
- **Sparse attention mechanisms**: Knowledge of how sparse attention reduces computational complexity compared to full attention is essential background. Quick check: Compare FLOPs between full attention and sparse attention implementations.
- **Denoising diffusion transformers**: Understanding the multi-step denoising process in video diffusion models is necessary to grasp why temporal coherence matters. Quick check: Map the relationship between denoising steps and attention pattern evolution.
- **CUDA kernel optimization**: Familiarity with low-level GPU programming concepts is important for understanding the implementation details. Quick check: Verify that the CUDA kernel properly handles tile skipping without introducing memory access issues.
- **Quality preservation metrics**: Understanding how video quality is measured and maintained despite aggressive sparsity is critical. Quick check: Compare perceptual quality metrics between full attention and LiteAttention outputs.

## Architecture Onboarding

**Component Map**: Input video frames -> LiteAttention CUDA kernel -> Skippable tile identification -> Evolutionary skip propagation -> Output frames

**Critical Path**: The most critical path is the identification of skippable tiles during early denoising steps, as this decision directly impacts the efficiency gains in all subsequent steps. This involves analyzing attention patterns, marking tiles, and ensuring these marks propagate correctly through the evolutionary skip mechanism.

**Design Tradeoffs**: The primary tradeoff is between sparsity level and quality preservation. Higher sparsity (up to 77%) yields greater runtime reduction but requires careful validation that quality remains acceptable. Another tradeoff involves the overhead of skip identification versus the savings from skipping computations - the method must ensure identification overhead doesn't negate the benefits of skipping.

**Failure Signatures**: Potential failures include quality degradation when sparsity patterns change rapidly between steps, incorrect tile marking leading to visual artifacts, and overhead from skip identification outweighing computational savings. System may also fail if the CUDA kernel implementation introduces synchronization issues or memory access conflicts.

**First Experiments**:
1. Verify temporal coherence by measuring attention pattern similarity across consecutive denoising steps on diverse video content
2. Benchmark quality preservation at varying sparsity levels (50%, 60%, 70%, 77%) using perceptual metrics
3. Profile runtime overhead of skip identification versus computational savings across different video resolutions and frame rates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily conducted on synthetic video benchmarks, raising questions about real-world generalizability
- Assumes slow evolution of sparsity patterns, which may not hold for videos with rapid scene changes
- CUDA kernel implementation may limit portability across different hardware platforms
- Quality preservation claims rely on perceptual metrics without comprehensive user studies
- 77% sparsity threshold may be content-dependent and not universally applicable

## Confidence

**High confidence**: The temporal coherence observation and evolutionary computation skip mechanism are well-founded theoretically.

**Medium confidence**: The 50% runtime improvement and 77% sparsity threshold are demonstrated but may be dataset-specific.

**Medium confidence**: The quality preservation claims are supported by presented metrics but lack extensive validation.

## Next Checks

1. Evaluate LiteAttention on diverse video datasets with varying content complexity, including rapid scene transitions and high-motion sequences, to assess robustness of sparsity patterns

2. Conduct comprehensive user studies comparing perceptual quality of LiteAttention outputs versus full attention across different video categories and sparsity levels

3. Benchmark LiteAttention's performance and quality trade-offs on alternative hardware platforms and attention implementations beyond the FlashAttention3-based CUDA kernel