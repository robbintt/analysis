---
ver: rpa2
title: 'Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning
  Process'
arxiv_id: '2509.17380'
source_url: https://arxiv.org/abs/2509.17380
tags:
- causal
- reasoning
- arxiv
- rlvr
- r-ate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically analyzes the causal structures underlying\
  \ the reasoning processes of large language models (LLMs) and large reasoning models\
  \ (LRMs). By modeling the reasoning process through four key variables\u2014problem\
  \ instruction (Z), thinking process (T), reasoning steps (X), and answer (Y)\u2014\
  the authors employ treatment experiments to infer structural causal models (SCMs)."
---

# Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process

## Quick Facts
- arXiv ID: 2509.17380
- Source URL: https://arxiv.org/abs/2509.17380
- Authors: Zhizhang FU; Guangsheng Bao; Hongbo Zhang; Chenkai Hu; Yue Zhang
- Reference count: 40
- Key outcome: RLVR-trained LRMs exhibit significantly enhanced causal reasoning structures compared to standard LLMs and distilled models, with RLVR reducing spurious correlations and strengthening genuine causal patterns

## Executive Summary
This study systematically analyzes the causal structures underlying LLM and LRM reasoning processes by modeling them through four key variables and employing treatment experiments to infer structural causal models. The authors find that while LLMs generally lack ideal causal reasoning structures, LRMs trained with RLVR show significantly enhanced causal reasoning capabilities. RLVR effectively reduces spurious correlations and strengthens genuine causal patterns, leading to more robust and faithful reasoning processes. The research provides crucial insights for designing future AI systems with stronger causal foundations and highlights RLVR as a key technique for improving reasoning model reliability.

## Method Summary
The authors employ treatment experiments to infer structural causal models by intervening on specific variables (Instruction, Chain-of-Thought, Thinking) and measuring effects on the final Answer using Average Treatment Effect (ATE) and Relative ATE (R-ATE). They classify reasoning structures into four SCM types based on significant causal edges. The method uses McNemar's test for assessing significance. Models are evaluated on mathematical reasoning tasks (3-digit multiplication, 9-digit addition, MATH500, GSM8K) and logical reasoning (ProofWriter, FOLIO, LOGIQA), including a Noop variant with added irrelevant conditions. For RLVR analysis, they fine-tune Qwen2.5-3B variants using GRPO on DeepScaleR dataset across multiple checkpoints.

## Key Results
- RLVR-trained LRMs exhibit significantly enhanced causal reasoning structures (Type I SCMs) compared to standard LLMs and distilled models
- RLVR effectively reduces spurious correlations and strengthens genuine causal patterns in reasoning processes
- Distillation improves task accuracy but often fails to transfer causal reasoning structure, instead learning spurious correlations
- RLVR shows improved robustness on Noop variants where irrelevant numbers are added to prompts

## Why This Works (Mechanism)

### Mechanism 1: Intervention-Based Identification of Reasoning Types
The method uses Average Treatment Effect (ATE) and Relative ATE (R-ATE) to determine causal edges. If intervening on Chain-of-Thought significantly changes the Answer, the model possesses a causal chain structure (X → Y). If the Answer changes significantly when biasing the Instruction but not when scrambling the CoT, the model is merely "explaining" a pre-determined answer (Common Cause structure). The reasoning process is modeled as a Directed Acyclic Graph of four distinct variables (Z, T, X, Y).

### Mechanism 2: RLVR Suppresses Spurious Correlations via Verifiable Reward
RLVR optimizes for final answer correctness, incentivizing the model to ignore non-causal features and strengthen the causal path (X → Y) to ensure robust reward achievement. This reduces the Z → Y edge in the SCM by decoupling the Answer from spurious correlations in the instruction.

### Mechanism 3: Distillation Transfers Accuracy at the Expense of Causality
Supervised Fine-Tuning trains the model to maximize likelihood of teacher's tokens without enforcing that the student must use reasoning to derive the answer. This often results in Type II/III structures where the student learns to predict the answer based on statistical patterns (Z → Y) rather than reasoning logic (X → Y).

## Foundational Learning

- **Structural Causal Models (SCMs) & Intervention (do-calculus)**: Understanding the difference between observing P(Y|X) and intervening P(Y|do(X)) is crucial for interpreting experimental results. Quick check: If I change the text of a reasoning step (Intervention) and the answer stays the same, does the CoT cause the Answer?

- **Spurious Correlations vs. Causal Features**: The core finding distinguishes between LLMs/SFT relying on spurious correlations (e.g., "If the problem mentions 'apples', the answer is usually positive") versus RLVR learning genuine causality. Quick check: Why would a model perform well on a benchmark but fail on a "Noop" version where irrelevant numbers are added to the prompt?

- **Average Treatment Effect (ATE)**: This quantitative metric determines the "strength" of a causal edge in the paper. Quick check: How does R-ATE (Relative ATE) normalize the effect size to allow comparison between models of different scales?

## Architecture Onboarding

- **Component map**: Instruction (Z) -> Thinking (T) -> CoT/Reasoning (X) -> Answer (Y)

- **Critical path**:
  1. Generate baseline responses (Z, T, X, Y)
  2. **Intervention**: Generate counterfactual responses (e.g., Z, T, X_random, Y')
  3. **Comparison**: Compare Y and Y' using McNemar's test to determine statistical significance of edge X → Y
  4. **Classification**: Assign SCM Type based on which edges are significant

- **Design tradeoffs**:
  - **Accuracy vs. Faithfulness**: Distillation yields high accuracy but low faithfulness (Type II/III). RLVR may slightly lower peak in-distribution accuracy but improves faithfulness (Type I)
  - **Evaluation Cost**: Calculating R-ATE requires multiple inference passes per sample (default + interventions), increasing evaluation compute

- **Failure signatures**:
  - **Type II (Common Cause)**: The model produces a correct reasoning trace for a wrong answer, or a wrong trace for a correct answer (unfaithful)
  - **Type III (Full Connection)**: The model is easily misled by irrelevant additions to the instruction (high Δ% on Math500-Noop)

- **First 3 experiments**:
  1. **Baseline SCM Profiling**: Run R-ATE intervention pipeline on standard LLM vs. LRM on Math500 to confirm shift from Type III/IV to Type I
  2. **Spurious Feature Ablation**: Train small model using standard SFT vs. RLVR (GRPO), evaluate both on "Math500-Noop" to verify RLVR lowers accuracy gap (Δ%)
  3. **Distillation Analysis**: Compare SCM types of teacher model vs. distilled student to verify if student retains causal edges or collapses into statistical correlation

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid training frameworks combining distillation and RLVR simultaneously achieve high task accuracy and strong causal alignment? The study reveals a distinct trade-off: distillation maximizes accuracy but amplifies spurious features, while RLVR enhances causal robustness but may sacrifice some fitting ability. Experiments demonstrating that hybrid-trained models can achieve high accuracy on Math500 while maintaining low performance gap on Math500-Noop and high proportion of Type-I SCMs would resolve this.

### Open Question 2
How do advanced reasoning architectures like Tree-of-thought or Graph-of-thought alter the identified causal structures compared to basic chain-of-thought? The causal dynamics of non-sequential or branching reasoning processes remain unexplored. Application of the intervention-based SCM analysis framework to models utilizing Tree-of-thought or Graph-of-thought prompting would derive their specific SCM type distributions.

### Open Question 3
Does explicitly incorporating causal signals into the optimization objective yield better causal alignment than the implicit improvements provided by RLVR? RLVR relies on verifiable rewards which implicitly strengthen causality, but it's unknown if direct causal optimization is more effective. Comparative training experiments where one group uses standard RLVR and another uses modified objective with explicit causal regularization would resolve this.

## Limitations
- The intervention method assumes text substitutions constitute valid causal interventions, which may not hold if models detect out-of-distribution prompts
- The SCM framework reduces complex reasoning to a four-variable DAG, potentially oversimplifying real model behavior
- Training details for RLVR (GRPO hyperparameters, reward shaping) are underspecified, making exact reproduction difficult

## Confidence
- **High confidence**: RLVR produces more Type-I (causal chain) SCM structures than SFT/distillation, correlating with improved robustness on Noop variants
- **Medium confidence**: The specific quantitative thresholds for R-ATE and exact proportions of SCM types across model families
- **Medium confidence**: RLVR suppresses spurious correlations more effectively than RLHF, though mechanism is plausible
- **Low confidence**: Generalizability of SCM types beyond mathematical reasoning to other domains like code or natural language tasks

## Next Checks
1. **Intervention validity check**: Run same R-ATE analysis with different perturbation strengths (minimal vs. maximal CoT scrambling) to verify causal edge detection is robust to intervention magnitude
2. **Cross-domain SCM profiling**: Apply intervention pipeline to non-mathematical reasoning dataset (e.g., commonsense reasoning or code generation) to test whether RLVR consistently produces Type-I structures across domains
3. **Training ablation study**: Train matched models using RLVR with varying KL penalty strengths and reward shaping functions to isolate whether verifiable rewards specifically drive causal structure improvements