---
ver: rpa2
title: 'Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought
  Success Before Completion'
arxiv_id: '2505.24362'
source_url: https://arxiv.org/abs/2505.24362
tags:
- reasoning
- dataset
- representations
- answer
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether LLMs can predict the success of\
  \ Chain-of-Thought (CoT) reasoning before completing the process. Using probing\
  \ classifiers on internal LLM representations, the authors show that CoT success\
  \ can be predicted with 60\u201376.4% accuracy even before generating a single token,\
  \ outperforming a BERT baseline that relies on generated text alone."
---

# Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion

## Quick Facts
- arXiv ID: 2505.24362
- Source URL: https://arxiv.org/abs/2505.24362
- Reference count: 20
- Primary result: Probing LLM representations before CoT generation predicts success with 60-76.4% accuracy, outperforming BERT baselines

## Executive Summary
This paper investigates whether LLMs can predict the success of Chain-of-Thought (CoT) reasoning before completing the process. Using probing classifiers on internal LLM representations, the authors show that CoT success can be predicted with 60–76.4% accuracy even before generating a single token, outperforming a BERT baseline that relies on generated text alone. Later reasoning steps do not always improve predictions, indicating that key information is encoded early. Initial zero-shot experiments show that truncating CoT still improves over no CoT, though full reasoning performs best. These findings suggest that LLM representations capture intermediate reasoning knowledge, enabling potential early stopping strategies for more efficient CoT.

## Method Summary
The authors construct balanced datasets from three math reasoning tasks (AQuA, Olympiad, Cn-K12) by running Llama-3.1-8B at temperature=0 with "Let's think step by step" prompting. For each sample, they extract hidden states from all 32 layers at the last token position before any CoT generation begins. A 3-layer MLP probe (256→128→64 units) is trained per layer to predict binary correctness. The approach is extended temporally by concatenating partial CoT generations at 10% intervals with the original prompt. SVCCA analysis validates representational similarity between early and late steps. A BERT baseline using generated text serves as black-box comparison.

## Key Results
- Probe accuracy: 60.0% on AQuA, 65.3% on Cn-K12, 76.4% on Olympiad before any CoT tokens generated
- Best layers: 11-14, 16-17 consistently outperform other layers across datasets
- BERT baseline accuracy: 55.8% on AQuA, 60.2% on Cn-K12, 72.8% on Olympiad (lower than probe)
- Early stopping: Truncating CoT at 10% improves over no CoT (57% on Olympiad) but underperforms full reasoning (69%)

## Why This Works (Mechanism)

### Mechanism 1
Internal LLM representations encode predictive information about CoT success before generation begins. During the initial forward pass on the prompt, hidden states at specific layers already contain compressed information about whether subsequent reasoning will succeed. This is detected by training a probing classifier on these representations to predict final answer correctness.

### Mechanism 2
Middle-to-late layers (approximately layers 11-17 in Llama-3.1-8B) contain the most predictive information about CoT success. Information about reasoning capacity is not uniformly distributed across layers. Middle layers aggregate task-relevant computations that are neither too input-bound (early layers) nor too output-bound (final layers), making them optimal for extracting success predictions.

### Mechanism 3
When early and late representations are similar (high SVCCA correlation), additional reasoning steps provide diminishing predictive value, suggesting the model has "converged" on its answer trajectory early. SVCCA measures representational similarity across time steps. High early-to-late similarity indicates that the model's internal state stabilizes early, meaning the reasoning trajectory is largely determined.

## Foundational Learning

### Probing classifiers
- **Why needed here**: The entire methodology depends on training a lightweight neural network to read out information from frozen LLM representations. Without understanding probing, the approach appears circular.
- **Quick check question**: Can you explain why training a classifier on hidden states is evidence that the model "knows" something, versus the classifier just learning to predict from noise?

### SVCCA (Singular Vector Canonical Correlation Analysis)
- **Why needed here**: The paper uses SVCCA to validate that probing results reflect genuine representational similarity, not just classifier overfitting. This is the complementary analysis that strengthens causal claims.
- **Quick check question**: If two representations have an SVCCA score of 0.95, what does that mean about the information they encode?

### Zero-shot CoT prompting
- **Why needed here**: The experiments use "Let's think step by step" style prompting without task-specific examples. Understanding this baseline is necessary to interpret why the model might encode success information before reasoning begins.
- **Quick check question**: Why would a model have any information about whether it will succeed at a task before it has generated any reasoning steps?

## Architecture Onboarding

### Component map
Dataset construction -> Representation extraction -> Probe training -> Temporal extension -> SVCCA analysis

### Critical path
Representation extraction → Layer-wise probe training → Identify best-performing layers → Run temporal experiments → Cross-validate with SVCCA. The layer identification step gates all downstream experiments.

### Design tradeoffs
- White-box vs. black-box: Method requires internal hidden states, incompatible with closed APIs. BERT baseline serves as black-box comparison.
- Temperature=0: Ensures deterministic outputs for clean labeling, but may not reflect real-world usage with sampling.
- Class balancing: 50/50 split enables clear interpretation above random baseline (50%), but discards natural success rate information.

### Failure signatures
- Probe accuracy near 50%: Representations contain no predictive signal, or probe is undertrained.
- BERT matching or exceeding probe: Success is predictable from text cues alone; no internal computation advantage.
- SVCCA scores flat across all time steps: No representational convergence; early stopping will be unreliable.

### First 3 experiments
1. Replicate layer-wise probe on a single dataset: Train probes on each of Llama-3.1-8B's 32 layers for AQuA. Identify if layers 11-17 show peak accuracy as reported.
2. Ablate the BERT baseline: Remove the prompt and train BERT on CoT text only vs. full prompt. Test whether linguistic cues in the question vs. the reasoning chain drive baseline performance.
3. Perturb early stopping threshold: Instead of 10% intervals, try stopping at token positions 5, 10, 15 for short CoT chains. Measure whether the correlation between SVCCA similarity and early stopping success holds at finer granularity.

## Open Questions the Paper Calls Out
- Can the intermediate representations that predict CoT success also be decoded to reveal the specific final answer content, rather than just binary correctness?
- Can reinforcement learning (RL) or supervised fine-tuning methods effectively utilize the probing classifier's confidence scores to trigger early stopping without sacrificing accuracy?
- Does the pre-generation predictability of CoT success generalize to non-mathematical reasoning tasks, such as commonsense or symbolic reasoning?

## Limitations
- The findings may not transfer to other reasoning domains like commonsense reasoning, code generation, or scientific reasoning
- The paper shows that representations predict success but does not establish why certain layers contain this information
- The method requires access to internal hidden states, making it incompatible with closed API access to LLMs

## Confidence
**High confidence**:
- Internal representations encode predictive information about CoT success before generation begins
- Middle-to-late layers (11-17) contain the most predictive information about CoT success
- When early and late representations are similar (high SVCCA), additional reasoning steps provide diminishing predictive value

**Medium confidence**:
- Truncated CoT can improve over no CoT in zero-shot settings
- BERT baseline relying on generated text alone performs worse than probe-based methods
- Layer 14 and 16 consistently perform well across all three datasets

**Low confidence**:
- The specific MLP probe architecture (256→128→64 units) is optimal for this task
- Five epochs of training is sufficient for probe convergence across all datasets
- Temperature=0 generation accurately reflects real-world reasoning behavior

## Next Checks
1. Cross-domain generalization test: Apply trained probes from math datasets to commonsense reasoning (HellaSwag) or strategy games (MiniGrid) to measure accuracy drop and adaptation needs.
2. Layer attribution ablation study: Systematically disable layers 11-17 in Llama-3.1-8B and measure impact on both CoT success and probe prediction accuracy versus random layer ablation.
3. Real-time early stopping benchmark: Implement online early stopping using probe predictions at 10% intervals, comparing accuracy/time saved against static truncation and full CoT completion.