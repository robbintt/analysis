---
ver: rpa2
title: Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19
  Fact-Checking
arxiv_id: '2512.00007'
source_url: https://arxiv.org/abs/2512.00007
tags:
- covid-19
- fact-checking
- safe
- cannabis
- long-form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study developed SAFE, a multi-agent system that combines
  large language models with retrieval-augmented generation to improve long-form COVID-19
  fact-checking. The system uses a two-stage approach: an agent extracts claims from
  lengthy articles, then LOTR-RAG retrieves scientific evidence from a 130,000-document
  corpus to verify them.'
---

# Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking

## Quick Facts
- arXiv ID: 2512.00007
- Source URL: https://arxiv.org/abs/2512.00007
- Reference count: 40
- Developed SAFE, a multi-agent system that improves long-form COVID-19 fact-checking using claim extraction and LOTR-RAG retrieval

## Executive Summary
This study introduces SAFE, a multi-agent system that combines large language models with retrieval-augmented generation to address long-form COVID-19 fact-checking. The system employs a two-stage approach: first extracting discrete claims from lengthy articles through segmentation, then verifying these claims using LOTR-RAG to retrieve scientific evidence from a 130,000-document corpus. Tested on 50 fake news articles containing 246 claims, SAFE significantly outperformed baseline LLMs in consistency and subjective quality metrics, demonstrating robust improvements in accuracy and explainability for COVID-19 misinformation detection.

## Method Summary
SAFE processes long COVID-19 articles (2-17 pages) by first chunking them into 2,000-token segments with 200-token overlap, then using GPT-4o-mini to extract claims from each segment. The LOTR-RAG retrieval system combines two embedding models (OpenAI text-embedding-3-small and PubMedBERT) to query a vector store of 126,984 COVID-19 papers. A verification agent then uses the retrieved evidence to assess each claim's validity, with performance evaluated using RAGAS consistency metrics and subjective Likert scores.

## Key Results
- SAFE achieved consistency score of 0.629 versus 0.279 for baseline LLMs
- LOTR-RAG core design outperformed its Self-RAG-enhanced variant
- System processed 50 fake news articles (246 claims) with high accuracy
- Subjective quality metrics showed significant improvements in usefulness, clearness, and authenticity

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Segmenting long-form text into overlapping chunks preserves semantic continuity and mitigates the "lost in the middle" phenomenon where LLMs typically ignore central context.

**Mechanism:** A preprocessing agent divides long articles (2–17 pages) into 2,000-token chunks with a 200-token overlap. A specialized LLM agent then extracts discrete claims from these manageable segments rather than processing the entire document at once.

**Core assumption:** The 200-token overlap is sufficient to maintain context across boundaries, ensuring that claims spanning paragraphs are not severed.

**Evidence anchors:**
- [abstract]: "SAFE includes two agents—one for claim extraction and another for claim verification... tested on 50 fake news articles (2–17 pages)."
- [section]: "The process begins by dividing long fake news into overlapping chunks... The first LLM agent extracts COVID-19 claims from each segment."
- [corpus]: "FinVet" and "RAMA" similarly utilize multi-agent frameworks to decompose complex tasks, supporting the efficacy of agentic preprocessing.

**Break condition:** If a complex narrative relies on premises established in the introduction and a conclusion in the appendix, the chunking window may fail to capture the dependency.

### Mechanism 2
**Claim:** Using a merging retriever with two distinct embedding models (general vs. domain-specific) retrieves more comprehensive evidence than single-retriever RAG, reducing hallucinations.

**Mechanism:** The system employs LOTR-RAG to query a vector store using both `text-embedding-3-small` (general purpose) and `pubmedbert-base-embeddings` (domain-specific). It filters and reorders results from two independent indices to prioritize high-relevance scientific evidence.

**Core assumption:** Domain-specific embeddings (PubMedBERT) capture semantic nuances in medical literature that general models miss, without introducing excessive noise.

**Evidence anchors:**
- [abstract]: "LOTR-RAG retrieves scientific evidence from a 130,000-document corpus... LOTR-RAG core design proved more effective than its Self-RAG-enhanced variant."
- [section]: "LOTR-RAG employs a merging retriever that integrates two distinct embedding models... enabling filtration and reordering across two independently built vector indices."
- [corpus]: "TrumorGPT" emphasizes the utility of specialized retrieval structures for health domains, aligning with the finding that domain-aware retrieval aids fact-checking.

**Break condition:** If the medical corpus is outdated or lacks coverage on emerging variants, the domain-specific retriever may return null results or low-relevance abstracts, forcing the LLM to rely on parametric memory.

### Mechanism 3
**Claim:** Direct evidence grounding via LOTR-RAG is more effective for fact-checking than adding a self-reflective query rewriting loop (Self-RAG), which can inadvertently dilute specific claim keywords.

**Mechanism:** While Self-RAG attempts to rewrite queries to improve retrieval, this study found it reduced performance. The winning mechanism relies on accurate initial retrieval using the raw claim, preserving specific keywords (e.g., "5G") that a rewriter might generalize into meaninglessness.

**Core assumption:** The initial claim extraction is precise enough that rewriting introduces more noise than signal.

**Evidence anchors:**
- [abstract]: "The LOTR-RAG core design proved more effective than its Self-RAG-enhanced variant."
- [section]: "The LLM rewriter may transform '5G is the origin of the virus' to 'the human-made origin...', overlooking a key point of the original claim."
- [corpus]: Corpus papers on RAG (e.g., "SRAG: Structured Retrieval") focus on query optimization, but this paper provides counter-evidence for the fact-checking domain specifically.

**Break condition:** If the user input contains significant typos or ambiguous phrasing, the lack of a rewriting/refinement step (since SRAG was discarded) might result in failed retrieval.

## Foundational Learning

- **Concept:** "Lost in the Middle" Phenomenon
  - **Why needed here:** This architectural constraint of LLMs necessitates the chunking strategy; understanding it prevents naive attempts to stuff entire PDFs into a context window.
  - **Quick check question:** Why does SAFE split a 17-page article into overlapping 2,000-token segments instead of summarizing it first?

- **Concept:** Ensemble Retrieval (LOTR)
  - **Why needed here:** Explains why the system uses two different embedding models rather than a single vector store.
  - **Quick check question:** How does combining `text-embedding-3-small` and `pubmedbert` improve retrieval for "COVID-19 vaccine efficacy" compared to using just one?

- **Concept:** Agentic Decomposition
  - **Why needed here:** The system relies on specialized agents (Claim Extractor vs. Verifier) rather than a monolithic prompt.
  - **Quick check question:** What is the role of the first agent, and why can't the verification agent handle text ingestion directly?

## Architecture Onboarding

- **Component map:** Ingestion: PDF/Text -> Chunker (2k tokens, 200 overlap) -> Agent 1 (Extraction): LLM (GPT-4o-mini) -> List of Claims -> Retriever (LOTR): Dual Embedding (OpenAI + PubMedBERT) -> Vector Store (Qdrant) -> Agent 2 (Verification): LLM + Retrieved Context -> Fact-Check Verdict

- **Critical path:** The quality of **Agent 1's claim extraction** determines the success of the entire pipeline. If a claim is missed here, it is never verified.

- **Design tradeoffs:**
  - **Cost vs. Accuracy:** The study suggests that `GPT-4o-mini` is sufficient for extraction, but `GPT-4o` may be needed if complex reasoning is required.
  - **Speed vs. Fidelity:** Self-RAG was removed because the iterative rewriting loop increased latency and reduced accuracy.
  - **Corpus Scope:** Using abstracts instead of full texts reduces computational cost but may miss specific methodological details.

- **Failure signatures:**
  - **Hallucinated Sources:** If the LLM cites papers not in the retrieval set, the RAG integration is broken.
  - **Keyword Dilution:** If "5G" claims are verified as "virus origin" claims without mentioning 5G, a rewriting module (SRAG) is likely active and should be disabled.
  - **Missing Middle Claims:** If claims from page 5 of a 10-page document are consistently missed, check the chunk overlap configuration.

- **First 3 experiments:**
  1. **A/B Test Chunking:** Run extraction on 10 long articles with 0 overlap vs. 200 overlap to quantify "lost in the middle" recovery.
  2. **Retrieval Precision:** Compare single-model (OpenAI embedding) vs. LOTR retrieval on 20 specific scientific claims to measure evidence relevance.
  3. **Rewrite Impact:** Manually inspect 5 "conspiracy" claims processed by Self-RAG to verify if key entities (e.g., "Graphene Oxide") are being removed by the rewriter.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can self-reflective mechanisms be reliably integrated into long-form fact-checking agents without causing the loss of critical claim details during query rewriting?
- **Basis in paper:** [explicit] The authors note that adding Self-RAG (SRAG) reduced performance because the rewriter compromised critical information (e.g., changing "5G" to "human-made origin"), and they call for future work to explore more reliable integration methods.
- **Why unresolved:** The study tested a standard Self-RAG approach and found it counterproductive; the specific architectural changes needed to preserve semantic nuance while enabling self-reflection were not identified.
- **What evidence would resolve it:** A comparative study of constrained rewriting algorithms or reflection mechanisms that demonstrate higher factual consistency scores than the baseline LOTR-RAG.

### Open Question 2
- **Question:** Can the SAFE system be effectively adapted to extract and verify claims embedded in non-textual elements like images and graphs?
- **Basis in paper:** [explicit] The limitations section states the LLM agent struggles with non-textual elements, and the future work section explicitly proposes addressing the complexity of multimodal content.
- **Why unresolved:** The current system relies entirely on text extraction and text-based RAG; it lacks modules for optical character recognition (OCR) or visual feature analysis.
- **What evidence would resolve it:** An evaluation of a multimodal SAFE variant on a dataset of fake news articles containing informational charts and photographs, measuring claim extraction recall.

### Open Question 3
- **Question:** Does the SAFE architecture generalize to other health domains, such as cancer or nutrition, without significant loss in consistency?
- **Basis in paper:** [explicit] The authors state the system "holds potential for extension to other domains like cancer, HIV/AIDS and nutrition."
- **Why unresolved:** The system was tested exclusively on a COVID-19 specific corpus (130k documents) and evaluation set; it is unclear if the retrieval density and claim structures in other fields are similar enough for the current architecture to succeed.
- **What evidence would resolve it:** Benchmarks of the SAFE system operating on domain-specific corpora (e.g., oncology) compared to its performance on the COVID-19 dataset.

## Limitations
- System struggles with non-textual elements like images and graphs embedded in articles
- Performance may not generalize beyond COVID-19 domain or to emerging variants outside 2020-2024 corpus
- PDF extraction quality issues required manual reformatting for 20% of articles

## Confidence
- **High Confidence:** Chunking strategy and LOTR-RAG retrieval mechanisms are well-supported by methodology and results
- **Medium Confidence:** Self-RAG degradation finding is supported but based on relative comparisons
- **Low Confidence:** Universal superiority claims for LOTR-RAG require more extensive head-to-head comparisons

## Next Checks
1. **Chunking Boundary Test:** Re-run claim extraction on 10 articles with 0-token overlap vs. 200-token overlap to quantify "lost in the middle" recovery rates
2. **Corpus Coverage Audit:** Verify that the 126,984-paper corpus includes preprints and papers addressing recent variants to ensure retrieval comprehensiveness
3. **Domain Transfer Test:** Apply SAFE to non-COVID fact-checking tasks (e.g., political misinformation) to assess architectural generalizability beyond the trained domain