---
ver: rpa2
title: ACE and Diverse Generalization via Selective Disagreement
arxiv_id: '2509.07955'
source_url: https://arxiv.org/abs/2509.07955
tags:
- loss
- rate
- distribution
- learning
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning models that generalize
  under complete spurious correlations, where ground-truth and spurious features are
  perfectly correlated in training data but diverge in target distributions. The authors
  propose ACE (Algorithm for Concept Extrapolation), which trains an ensemble of classifiers
  to confidently disagree on instances where concepts are most likely to differ.
---

# ACE and Diverse Generalization via Selective Disagreement

## Quick Facts
- **arXiv ID:** 2509.07955
- **Source URL:** https://arxiv.org/abs/2509.07955
- **Reference count:** 40
- **Primary result:** ACE outperforms existing methods on complete-spurious correlation benchmarks across image and language datasets, achieving higher accuracy while being more configurable.

## Executive Summary
This paper addresses the problem of learning models that generalize under complete spurious correlations, where ground-truth and spurious features are perfectly correlated in training data but diverge in target distributions. The authors propose ACE (Algorithm for Concept Extrapolation), which trains an ensemble of classifiers to confidently disagree on instances where concepts are most likely to differ. The method uses a self-training approach with a top-K pseudo-label loss that encourages selective, confident disagreement among ensemble members. ACE outperforms existing methods (DivDis and D-BAT) on complete-spurious correlation benchmarks across image and language datasets, achieving higher accuracy while being more configurable. The method also demonstrates competitive performance on measurement tampering detection without requiring untrusted measurements. A key advantage is ACE's principled unsupervised model selection via validation loss, avoiding reliance on oracle target data.

## Method Summary
ACE trains an ensemble of classifiers to extrapolate concepts learned from source data to target distributions where spurious correlations break down. The method uses a shared backbone with multiple independent classification heads, each trained on labeled source data with standard cross-entropy loss. For unlabeled target data, ACE implements a top-K disagreement loss that identifies instances where ensemble members most strongly disagree and applies pseudo-labels to reinforce this divergence. The approach amplifies weak initial differences in ensemble predictions through self-training, pushing decision boundaries toward low-density regions of the input space. Unlike methods that assume zero mutual information between concepts, ACE functions as a proper scoring rule when the user-specified mix rate lower bound is valid.

## Key Results
- ACE outperforms DivDis and D-BAT on complete-spurious correlation benchmarks, achieving higher accuracy while being more configurable
- The method demonstrates competitive performance on measurement tampering detection without requiring untrusted measurements
- ACE achieves principled unsupervised model selection via validation loss, avoiding reliance on oracle target data

## Why This Works (Mechanism)

### Mechanism 1: Selective Disagreement via Top-K Bootstrapping
The method resolves underspecification by forcing an ensemble to diverge specifically on unlabeled target data where initial model randomness suggests a high probability of concept disagreement. It calculates the probability of "disagreement groups" and selects top-k instances with highest disagreement probability, applying pseudo-label loss to reinforce this divergence. This amplifies weak initial differences into distinct, semantically meaningful decision boundaries. The mechanism fails if initial decision boundaries are perfectly aligned or if the target distribution has no natural disagreement opportunities.

### Mechanism 2: Low-Density Separation (Entropy Penalty)
By enforcing confident pseudo-labels on selected target instances, the mechanism implicitly pushes decision boundaries into low-density regions of the input space. The loss function applies negative log-likelihood to high-confidence predictions, forcing the model to increase margins around disagreed points and move boundaries away from high-density data clusters. This assumes valid human concepts align with low-density regions. The mechanism fails if the target distribution is uniformly dense or if valid concepts require boundaries through high-density clusters.

### Mechanism 3: Proper Scoring via Mix Rate Lower Bounds
Unlike methods forcing specific correlation structures, ACE functions as a proper scoring rule provided the user-defined lower bound on disagreement rate is valid. The loss allows the model to find the true distribution if the assumed lower bound is less than or equal to the true mix rate. This avoids penalizing the model for failing to disagree on the rest of the data, preventing imposition of incorrect statistical assumptions. The mechanism fails if the assumed lower bound significantly exceeds the true mix rate, forcing the model to "hallucinate" disagreements.

## Foundational Learning

- **Concept: Underspecification & Spurious Correlation**
  - **Why needed here:** Standard models fail when training features are perfectly correlated with labels but decouple in testing. ACE is designed specifically for this "complete correlation" regime where ERM fails silently.
  - **Quick check question:** If a model achieves 100% training accuracy by learning a shortcut (e.g., background), will standard regularization (dropout, weight decay) likely fix the OOD generalization? (Answer: No, usually requires data augmentation or architectural changes like ACE).

- **Concept: Self-Training & Pseudo-Labeling**
  - **Why needed here:** ACE is a semi-supervised approach that uses model predictions on unlabeled target data as ground truth to shape the decision boundary.
  - **Quick check question:** What is the primary risk of naive pseudo-labeling? (Answer: Confirmation bias/reinforcing incorrect predictions). How does ACE mitigate this? (Answer: By using ensemble disagreement rather than single-model confidence to select labels).

- **Concept: Ensemble Diversity**
  - **Why needed here:** The goal is not a single robust model, but a diverse set of hypotheses consistent with training data.
  - **Quick check question:** Why is "orthogonality" or "independence" potentially a bad objective for concept diversity? (Answer: It might force the model to use noise or artificial splits rather than semantic differences).

## Architecture Onboarding

- **Component map:** Input -> Shared Backbone -> Ensemble Heads (F=2) -> Source Loss + Target Top-K Disagreement Loss
- **Critical path:** 1) Train heads on source data until convergence 2) Calculate probability for each disagreement group on target batch 3) Select top-k indices per group 4) Apply binary mask to select these indices 5) Calculate Top-K loss using group labels as targets 6) Backpropagate
- **Design tradeoffs:** High vs. low mix rate forces more/less aggressive diversity; using separate group losses (balanced) vs. aggregated top-k (imbalanced) for skewed data
- **Failure signatures:** Collapse (heads converge to identical predictions), Hallucination (disagree on noise rather than concepts), Validation Loss Divergence (sudden spike indicates too-high lower bound)
- **First 3 experiments:** 1) Sanity Check: Replicate "Gaussian Blob 2D Grid" experiment to visualize decision boundaries rotating apart 2) Ablation on r: Run sweep on CIFAR-MNIST to see performance degradation when r is set too low or high 3) Model Selection Test: Infer correct r using "Validation Loss Density" heuristic and verify loss curve behavior

## Open Questions the Paper Calls Out

- **Can the ad-hoc 20th percentile threshold for mix rate lower bound selection generalize effectively across diverse datasets and model architectures?**
  - Basis: "Future work should evaluate how well thresholds generalize to other datasets and models."
  - Why unresolved: The threshold was selected using oracle information and has only been tested on four image datasets in this study.

- **Can combining ACE with domain adaptation techniques improve performance when hypotheses disagree on instances where ground-truth concepts agree?**
  - Basis: "Future work might make use of techniques in domain adaptation... for example penalizing the distance between the source and target distributions."
  - Why unresolved: ACE currently relies solely on disagreement and may learn to disagree on unrelated features.

- **What are the theoretical guarantees on convergence and expected error for ACE under various mix rate and distribution conditions?**
  - Basis: "We are excited about... theoretical work analyzing expected error, convergence, and optimal solutions for ACE under various conditions."
  - Why unresolved: The paper provides only empirical validation without formal analysis of when and why ACE converges to semantically meaningful concepts.

## Limitations
- Performance heavily depends on correctly estimating the mix rate lower bound, which may be challenging in real-world settings without ground-truth correlation information
- The approach focuses on binary classification tasks in the paper, and scalability to multi-class problems with multiple spurious correlations remains unexplored
- ACE assumes the ensemble's initial random weights provide sufficient signal for bootstrap amplification, which may fail when initial decision boundaries are already aligned

## Confidence

| Claim | Confidence |
|-------|------------|
| Core mechanism of selective disagreement via top-k pseudo-labeling outperforms existing methods | High |
| ACE functions as a proper scoring rule under valid mix rate bounds | Medium |
| Unsupervised model selection via validation loss is reliable | Medium |

## Next Checks
1. **Mix Rate Sensitivity Analysis:** Conduct systematic experiments varying the mix rate lower bound on datasets with known correlation structures to quantify performance degradation when bounds are set too high or too low.
2. **Multi-Class Extension Test:** Implement and evaluate ACE on multi-class classification problems with multiple spurious correlations to assess scalability beyond binary tasks.
3. **Real-World Application:** Apply ACE to a practical domain where spurious correlations are known to exist (e.g., medical imaging with demographic confounders) and validate whether the method successfully identifies and mitigates these correlations without explicit supervision.