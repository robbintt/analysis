---
ver: rpa2
title: 'CHILL at SemEval-2025 Task 2: You Can''t Just Throw Entities and Hope -- Make
  Your LLM to Get Them Right'
arxiv_id: '2506.13070'
source_url: https://arxiv.org/abs/2506.13070
tags:
- translation
- entity
- language
- feedback
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate named entity translation
  in machine translation by combining Retrieval-Augmented Generation (RAG) with self-refinement
  techniques. The approach retrieves entity information from Wikidata and uses it
  to prompt a large language model (LLM), then iteratively refines translations based
  on feedback evaluating entity accuracy and overall translation quality.
---

# CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right

## Quick Facts
- arXiv ID: 2506.13070
- Source URL: https://arxiv.org/abs/2506.13070
- Reference count: 7
- Primary result: Achieved harmonic mean scores of up to 94.23 on COMET and M-ETA metrics, compared to baseline GPT-4o scores around 56-62

## Executive Summary
This paper addresses the challenge of accurate named entity translation in machine translation by combining Retrieval-Augmented Generation (RAG) with self-refinement techniques. The approach retrieves entity information from Wikidata and uses it to prompt a large language model (LLM), then iteratively refines translations based on feedback evaluating entity accuracy and overall translation quality. The system significantly improves performance across 10 language pairs, achieving harmonic mean scores of up to 94.23 on COMET and M-ETA metrics, compared to baseline GPT-4o scores around 56-62. Self-refinement further enhances results by 0.19-1.66 percentage points. Case studies demonstrate the effectiveness of the feedback mechanism in correcting both entity-specific and general translation errors.

## Method Summary
The approach combines RAG with iterative self-refinement for entity-aware machine translation. First, the system retrieves entity labels and descriptions from Wikidata using gold Wikidata IDs provided in the dataset. This information is incorporated into prompts for GPT-4o to generate initial translations. Then, the same LLM evaluates translations on two criteria: entity correctness (5 points) and overall translation quality (5 points). Based on this feedback, the model refines its translations, incorporating the history of previous attempts. This process continues for up to 2 iterations or until a perfect score is achieved. The method avoids fine-tuning by using few-shot prompting and focuses on improving entity translation accuracy while maintaining overall translation quality.

## Key Results
- RAG alone improved performance by 40+ percentage points across all language pairs compared to baseline GPT-4o
- Self-refinement added an additional 0.19-1.66 percentage points of improvement
- Achieved harmonic mean scores of up to 94.23 on COMET and M-ETA metrics
- Korean showed the largest improvement: 49.28 → 91.94 (+42.66 points)
- Self-refinement particularly helped when entity labels were literally translated (e.g., "White Army, Black Baron" → correct Korean transcreation)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Grounded Entity Injection
Providing explicit entity labels and descriptions from Wikidata during prompting substantially improves entity translation accuracy over baseline LLM performance. The system retrieves target-language labels and descriptions using gold Wikidata IDs via REST API, then embeds this information directly into the translation prompt. The LLM conditions on this grounded knowledge rather than relying on parametric memory, reducing hallucination of entity names.

### Mechanism 2: Iterative Self-Refinement with Dual-Criteria Feedback
Post-hoc self-evaluation and refinement cycles incrementally improve both entity accuracy and overall translation quality. After initial translation, the same LLM generates structured feedback scoring entity correctness (5 points) and translation quality (5 points). The model then revises the translation given its own feedback and history. This continues until perfect score (10/10) or iteration limit.

### Mechanism 3: Entity-Description Contextualization
Including entity descriptions alongside labels helps disambiguate entity types and improves context-appropriate translation. Descriptions provide semantic context that helps the model distinguish entity types and select appropriate translation strategies beyond mere label substitution.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for grounding entity translations in external knowledge. Without understanding RAG, the dramatic improvement from baseline to +RAG (e.g., Korean: 49.28 → 91.94) is opaque.
  - Quick check question: Given a source text "When was the Louvre built?" with Wikidata ID Q180636, what specific information should the RAG system retrieve before translation?

- **Concept: Self-Refine / Iterative Refinement**
  - Why needed here: The modest but consistent gains (0.19-1.66%) come from this feedback loop. Understanding why gains are smaller than RAG gains helps set realistic expectations.
  - Quick check question: Why might self-refinement yield diminishing returns after 2-3 iterations? What costs increase with each iteration?

- **Concept: Entity Transcreation vs. Transliteration**
  - Why needed here: The paper explicitly notes that entities often require cultural adaptation (e.g., "Night at the Museum" → "박물관이 살아있다" meaning "The Museum is Alive" in Korean), not literal translation.
  - Quick check question: For the entity "White Army, Black Baron" (a song), why would a literal translation fail, and what information would guide the correct transcreation?

## Architecture Onboarding

- Component map: Entity Retriever -> Translation Generator -> Feedback Generator -> Refinement Generator -> Iteration Controller
- Critical path: Entity retrieval (1 LLM call) -> Initial translation (1 LLM call) -> [Feedback (1 LLM call) -> Refinement (1 LLM call)] × iterations. With max 2 iterations, worst case is 5 LLM calls per input.
- Design tradeoffs:
  - Oracle IDs vs. Entity Linking: Current system uses gold Wikidata IDs; real deployment requires entity linking first, adding error propagation risk
  - Iteration depth vs. Cost: Paper limits to 2 iterations due to budget; more iterations might help but with diminishing returns and linear cost increase
  - Same-model evaluation vs. Separate evaluator: Using same GPT-4o for translation and evaluation is simpler but may have blind spots; a dedicated critic model might catch different errors
- Failure signatures:
  - RAG failure: Wikidata ID missing or wrong -> entity translated literally/hallucinated (baseline-level performance)
  - Self-refine failure: Model gives itself 10/10 while entity is still wrong -> early termination without improvement
  - Description misleading: Wikidata description ambiguous/wrong -> wrong entity sense selected
  - Cascading errors: Feedback misidentifies problem -> refinement introduces new errors
- First 3 experiments:
  1. Ablate entity descriptions: Run +RAG with labels only vs. labels + descriptions to measure description contribution
  2. Stress-test self-evaluation: Manually annotate cases where model rates itself 10/10 but entity is incorrect; measure false-positive rate in self-assessment
  3. Remove oracle assumption: Implement entity linking from raw text (without gold IDs) to measure performance degradation in realistic deployment conditions

## Open Questions the Paper Calls Out

### Open Question 1
Can effective entity-aware translation be achieved without relying on gold Wikidata IDs, and what would be the performance gap compared to the oracle-entity approach? The conclusion states: "Looking ahead, future work could explore incorporating entity retrieval methods without using gold entity." This is unresolved because the current system relies entirely on oracle Wikidata IDs provided by task organizers, which are unavailable in real-world applications.

### Open Question 2
What is the optimal number of self-refinement iterations, and at what point do diminishing returns or degradation set in? The authors state: "We set the maximum number of trials to 2 due to budget constraints," leaving the iteration ceiling unexplored. Only 1-2 refinement cycles were tested; it is unknown whether further iterations would improve results, plateau, or introduce error accumulation.

### Open Question 3
Does the RAG + self-refine approach generalize to smaller or open-source LLMs, or is it dependent on GPT-4o's specific capabilities? All experiments use GPT-4o exclusively, with no comparison to other models. The method relies on the LLM's ability to self-evaluate and correct its own outputs, which smaller models may struggle with.

### Open Question 4
What factors beyond surface-level label similarity determine entity translation accuracy, particularly for non-Latin scripts? The correlation analysis found low correlation between Levenshtein distance and M-ETA for Latin-script languages, concluding "other factors likely play more significant roles" without identifying them. The analysis excluded non-Latin scripts, and no investigation into alternative predictors was conducted.

## Limitations
- The system relies on gold Wikidata IDs being provided at inference time, which is unrealistic for most real-world deployment scenarios where entity linking from raw text would be necessary first
- The paper does not empirically validate whether entity descriptions provide meaningful improvement beyond labels alone
- No ablation studies measure the false-positive rate in the LLM's self-evaluation (where it might give itself a perfect score despite entity errors)

## Confidence

**High confidence**: The RAG mechanism significantly improves entity translation accuracy when gold entity IDs are available. The performance gains across all 10 language pairs are substantial and consistent, with improvements of 40+ percentage points over baseline. The retrieval process from Wikidata is straightforward and well-documented.

**Medium confidence**: The iterative self-refinement approach provides consistent but smaller improvements (0.19-1.66 percentage points). While the mechanism is sound, the paper does not validate the reliability of the LLM's self-evaluation or measure whether these gains justify the additional computational cost.

**Low confidence**: Claims about entity-description contextualization improving translation quality lack empirical support. No ablation studies compare performance with labels only versus labels plus descriptions. The paper asserts this benefit but provides no direct evidence.

## Next Checks

1. **Description ablation study**: Run the full pipeline with entity labels only (no descriptions) versus labels plus descriptions to measure the marginal utility of descriptions. Compare COMET and M-ETA scores to determine if descriptions provide statistically significant improvement.

2. **Self-evaluation reliability test**: Manually annotate 100 translations where the model gave itself a perfect 10/10 score. Identify cases where entity accuracy is actually incorrect to measure the false-positive rate in self-assessment. This will quantify how often the system terminates prematurely due to overconfidence.

3. **End-to-end entity linking evaluation**: Replace the oracle Wikidata IDs with an entity linking system that extracts entities from raw text and maps them to Wikidata. Measure performance degradation across all 10 languages to understand the real-world deployment gap between this controlled setup and production scenarios.