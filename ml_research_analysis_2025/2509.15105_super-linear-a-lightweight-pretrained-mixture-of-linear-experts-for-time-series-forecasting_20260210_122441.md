---
ver: rpa2
title: 'Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time
  Series Forecasting'
arxiv_id: '2509.15105'
source_url: https://arxiv.org/abs/2509.15105
tags:
- forecasting
- time
- super-linear
- series
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Super-Linear is a lightweight, frequency-specialized mixture-of-experts
  model for time series forecasting. It combines simple linear experts, each trained
  on a distinct frequency regime, with a spectral gating mechanism that dynamically
  selects relevant experts based on input characteristics.
---

# Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.15105
- Source URL: https://arxiv.org/abs/2509.15105
- Reference count: 40
- Lightweight MoE achieving competitive performance with only 2.5M parameters

## Executive Summary
Super-Linear introduces a novel lightweight mixture-of-experts architecture for time series forecasting that replaces deep neural networks with simple linear experts specialized for distinct frequency regimes. The model uses a spectral gating mechanism based on FFT to dynamically route inputs to the most relevant frequency-specific linear experts. Despite its minimal parameter count and computational efficiency, Super-Linear achieves competitive or superior performance compared to state-of-the-art transformer-based models across both zero-shot and full-shot forecasting benchmarks, while being orders of magnitude faster in inference.

## Method Summary
Super-Linear employs a two-stage training approach where linear experts are first trained independently on frequency-specific data resampled to match predefined fundamental frequencies. A spectral gating mechanism then routes inputs based on their normalized periodogram, selecting the top-K most relevant experts. The architecture combines frozen frequency specialists with lightweight trainable complementary experts, using instance normalization (RevIN) before and after expert application. This design enables efficient, interpretable forecasting while maintaining competitive accuracy.

## Key Results
- On GIFT-Eval, reduces MASE by 16% over TTM-R2
- On LTSF datasets, delivers up to 26.2% lower MSE than models like Chronos and TimesFM
- Achieves inference speeds orders of magnitude faster than transformer-based alternatives

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Specialized Linear Decomposition
The model trains linear experts independently on data resampled to specific fundamental frequencies, forcing weights to specialize in particular lag structures rather than averaging across competing seasonalities. This reduces "frequency confusion" where competing seasonalities interfere in monolithic models.

### Mechanism 2: Spectral Gating as a Router
A normalized periodogram computed via FFT serves as sufficient statistics for routing, bypassing learned temporal embeddings. The router activates only Top-K experts whose frequency specializations align with input spectral energy, leveraging the assumption that dominant frequencies determine optimal forecasting strategy.

### Mechanism 3: Two-Stage Decoupled Optimization
Decoupling expert specialization from routing adaptation prevents "average expert" collapse. Stage 1 trains experts in isolation on frequency-aligned data, then Stage 2 freezes these and trains only the router and complementary experts on multi-frequency data, ensuring spectral identity retention.

## Foundational Learning

- **Spectral Analysis (Periodogram/FFT)**: The router processes spectral density, not raw time series. Understanding FFT conversion to frequency bins is essential for debugging expert gating behavior.
  - *Quick check*: For a time series with strong trend but no seasonality, what would the periodogram look like, and which experts would likely activate?

- **Sparse Mixture of Experts (MoE)**: Super-Linear relies on "Top-K" sparse gating. Understanding dense vs. sparse routing trade-offs is essential for tuning inference cost vs. accuracy.
  - *Quick check*: What happens to gradient flow if Top-K is set too low (e.g., 1) during training?

- **Linear Regression as Time Series Forecasting**: Experts are Linear Layers performing weighted sums of past lags (autoregression). Understanding this limitation is crucial for recognizing modeling constraints.
  - *Quick check*: Can a linear layer model multiplicative trend (e.g., exponential growth) without preprocessing like RevIN?

## Architecture Onboarding

- **Component map**: Input Layer → Gating Network (FFT → Normalization → Linear Projection → Softmax → Top-K) → Expert Pool (Frequency Experts + Complementary Experts + Heuristic Experts) → Aggregation (Weighted sum) → RevIN (before and after)
- **Critical path**: The Gating Logic. Incorrect frequency identification leads to wrong lag correlations and phase-shifted or noisy forecasts.
- **Design tradeoffs**: Interpretability vs. Expressiveness (fixed experts enable visualization but may fail on chaotic data), Speed vs. Adaptability (tiny model but relies on lossy resampling for irregular rates)
- **Failure signatures**: High Router Entropy (flat weights suggest input spectrum mismatch), Reliance on Naive/Mean Experts (indicates frequency experts failing), Resampling Artifacts (aliasing from artificial frequencies)
- **First 3 experiments**: 1) Synthetic Frequency Isolation: Generate sine waves at specific frequencies, verify matching expert assignment >90%. 2) Ablation on "Complementary" Layers: Compare full-shot benchmark with N_c=0 vs N_c=12. 3) Inference Latency Stress Test: Compare Super-Linear against TimeMoE on batch of 1024 sequences.

## Open Questions the Paper Calls Out

- Can integration of nonlinear or hybrid experts improve performance on highly chaotic or complex time series?
- How can the model handle input lookbacks exceeding fixed 512-step training length without information loss?
- Can the set of frequency experts be learned or adapted dynamically rather than remaining fixed to predefined natural frequencies?
- How robust is the theoretical error bound when periodicity assumption between input and output is violated?

## Limitations

- Fixed frequency experts may fail on non-stationary series with shifting dominant frequencies
- Two-stage training creates rigid architecture where frozen experts may be suboptimal for complex multi-frequency interactions
- Reliance on FFT-based spectral analysis assumes linear, stationary signal components, potentially missing non-linear interactions

## Confidence

**High Confidence**: Computational efficiency claims (2.5M parameters, orders of magnitude faster) are well-supported; two-stage training effectiveness demonstrated through ablation; frequency-specialization concept is theoretically sound.

**Medium Confidence**: Generalization to zero-shot scenarios beyond GIFT-Eval is promising but not extensively validated; interpretability benefits are demonstrated but not systematically quantified; performance comparisons are comprehensive but may miss architectural variations.

**Low Confidence**: Behavior on highly irregular sampling rates or non-linear, non-stationary time series is not tested; long-term forecasting capabilities beyond tested horizons are unevaluated; impact of different Top-K configurations is mentioned but not systematically explored.

## Next Checks

1. **Spectral Robustness Test**: Generate synthetic time series with time-varying frequencies and measure Super-Linear's accuracy compared to static-frequency baselines.

2. **Irregular Sampling Evaluation**: Create benchmark datasets with highly irregular sampling intervals and test Super-Linear's performance with and without preprocessing.

3. **Long-Horizon Scaling Analysis**: Evaluate Super-Linear's performance on forecasting horizons 5-10 times longer than reported, comparing error accumulation rates against transformer-based models.