---
ver: rpa2
title: A cybersecurity AI agent selection and decision support framework
arxiv_id: '2510.01751'
source_url: https://arxiv.org/abs/2510.01751
tags:
- agent
- agents
- cybersecurity
- https
- nist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a structured framework for aligning diverse\
  \ AI agent architectures with the NIST Cybersecurity Framework (CSF) 2.0. By mapping\
  \ AI agent properties\u2014such as autonomy, adaptive learning, and reactivity\u2014\
  to specific cybersecurity tasks, the framework provides a systematic approach for\
  \ selecting and deploying AI solutions to address contemporary cyber threats."
---

# A cybersecurity AI agent selection and decision support framework

## Quick Facts
- arXiv ID: 2510.01751
- Source URL: https://arxiv.org/abs/2510.01751
- Authors: Masike Malatji
- Reference count: 40
- One-line primary result: A conceptual framework aligning AI agent architectures with NIST CSF 2.0 functions to guide deployment and improve cybersecurity operations.

## Executive Summary
This paper introduces a structured framework for aligning diverse AI agent architectures with the NIST Cybersecurity Framework (CSF) 2.0. By mapping AI agent properties—such as autonomy, adaptive learning, and reactivity—to specific cybersecurity tasks, the framework provides a systematic approach for selecting and deploying AI solutions to address contemporary cyber threats. It outlines graduated levels of autonomy to accommodate organizations at varying cybersecurity maturity levels, enabling a transition from assisted to fully autonomous intelligence. Through conceptual validation, the framework demonstrates how tailored AI agent deployments can enhance situational awareness, accelerate response times, and improve long-term resilience via adaptive risk management. Ultimately, it bridges the gap between theoretical AI constructs and operational cybersecurity demands, establishing a foundation for robust, empirically validated multi-agent systems that adhere to industry standards.

## Method Summary
The framework decomposes the NIST CSF 2.0 core functions (Govern, Identify, Protect, Detect, Respond, Recover) into granular subcategories and maps these to specific AI agent properties (autonomy, adaptive learning, reactivity). It defines a taxonomy of agent architectures (Reactive, Cognitive, Hybrid, Learning) and their suitability for different tasks, then proposes graduated autonomy levels (Assisted, Augmented, Autonomous) aligned with organizational maturity. The AI Agent Taxonomy and Decision Framework (AIATDF) provides a systematic process for selecting appropriate agent architectures and deployment modes based on task requirements and organizational capabilities.

## Key Results
- Framework provides systematic alignment between AI agent properties and NIST CSF 2.0 subcategories
- Graduated autonomy model accommodates organizations at different cybersecurity maturity levels
- Mapping demonstrates how tailored AI agent deployments enhance situational awareness and response times
- Framework bridges gap between theoretical AI constructs and operational cybersecurity demands

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning specific agent architectures with NIST CSF 2.0 subcategories may optimize task performance compared to generic "AI" deployments.
- **Mechanism:** The framework decomposes high-level NIST functions into specific tasks and maps these to agent properties—reactive agents handle high-speed triggers while learning agents handle adaptive thresholding—ensuring the agent's intrinsic limitations do not conflict with task requirements.
- **Core assumption:** NIST CSF 2.0 subcategories provide sufficiently distinct operational requirements that a "one-size-fits-all" agent architecture would result in measurable inefficiencies or security gaps.
- **Evidence anchors:** Abstract states framework "links essential AI agent properties such as autonomy, adaptive learning, and real-time responsiveness to each subcategory's security requirements." Table 6 shows reactive agents rely on stimulus-response mechanisms while cognitive agents handle higher-level functions like planning.
- **Break condition:** If decomposition reveals a single, general-purpose "Hybrid" agent outperforms specialized agents across all subcategories.

### Mechanism 2
- **Claim:** A graduated autonomy model reduces deployment failure risk by aligning agent capabilities with organizational maturity.
- **Mechanism:** The framework utilizes a Capability Maturity Model structure. By initially deploying agents in "Assisted" mode (human-in-the-loop), organizations validate the agent's decision-making logic before scaling to "Autonomous" modes where the agent acts independently.
- **Core assumption:** Organizations lack immediate trust or infrastructure to validate fully autonomous agents and require a structured "ramp-up" period.
- **Evidence anchors:** Abstract notes framework "outlines graduated levels of autonomy (assisted, augmented, and fully autonomous) to accommodate organisations at varying stages of cybersecurity maturity." Fig. 3 shows progression from manual to assisted, augmented, and fully autonomous.
- **Break condition:** If cognitive load of supervising "Assisted" agents exceeds manual operation workload, causing automation fatigue.

### Mechanism 3
- **Claim:** Integrating the "Govern" function as a distinct, agent-driven capability ensures cybersecurity strategy aligns with organizational risk appetite.
- **Mechanism:** The mechanism separates operational execution from strategic oversight. It assigns "Cognitive" or "Deliberative" agents to the Govern function to handle policy enforcement and compliance tracking, preventing reactive operational agents from acting without strategic constraints.
- **Core assumption:** Cognitive agents can effectively encode complex, qualitative organizational policies and risk appetites into computable logic.
- **Evidence anchors:** Abstract mentions aligning "diverse AI agent architectures... with the comprehensive... NIST Cybersecurity Framework (CSF) 2.0." Table 6 shows Cognitive agents for Govern handle policy-driven decision-making and compliance management.
- **Break condition:** If cognitive agent cannot dynamically interpret changing legal/ethical constraints, leading to rigid or non-compliant governance decisions.

## Foundational Learning

- **Concept:** **NIST CSF 2.0 Core Functions**
  - **Why needed here:** The entire mapping matrix relies on understanding the 6 functions (Govern, Identify, Protect, Detect, Respond, Recover). Without this, you cannot map agent properties.
  - **Quick check question:** Which function focuses on "establishing organizational commitment" and "monitoring risk management strategy" rather than active threat blocking? (Answer: Govern).

- **Concept:** **Agent Autonomy Spectrum (Assisted vs. Autonomous)**
  - **Why needed here:** The framework's deployment logic depends on distinguishing between "Human-in-the-loop" (Assisted) and "Human-out-of-the-loop" (Autonomous).
  - **Quick check question:** A system that blocks an IP address based on a learned anomaly model without asking a human is operating at which autonomy level? (Answer: Autonomous).

- **Concept:** **Agent Architectures (Reactive vs. Cognitive)**
  - **Why needed here:** Selecting the wrong architecture for a task is a primary failure mode the framework prevents.
  - **Quick check question:** An agent that responds instantly to a stimulus but lacks "memory" of past events to inform its decision is likely which type? (Answer: Reactive).

## Architecture Onboarding

- **Component map:** Organizational Risk Profile & NIST CSF 2.0 Subcategories -> AIATDF: 1) Decomposition -> 2) Matrix -> 3) CMM -> 4) Define metrics -> 5) Deploy -> 6) Refine -> Deployment Blueprint

- **Critical path:**
  1. Start with Detect or Protect functions where success metrics are clear
  2. Use Mapping Matrix (Table 6) to select a "Hybrid Agent" for balancing reactivity and deliberation
  3. Deploy in Assisted Intelligence mode to build trust in the SOC

- **Design tradeoffs:**
  - Reactive vs. Learning: Reactive agents offer speed but lack adaptability; learning agents offer adaptability but risk instability
  - Assisted vs. Autonomous: Assisted mode ensures safety but limits scalability; autonomous mode maximizes throughput but increases risk

- **Failure signatures:**
  - Strategic Misalignment: Reactive agent deployed for "Govern" creates rigid, non-adaptive policy enforcement
  - Trust Gap: Autonomous agent for "Incident Mitigation" without prior "Assisted" phase leads to SOC rejection
  - Complexity Overload: MAS for low-maturity organization where simple automation would suffice

- **First 3 experiments:**
  1. "Detect" Pilot: Deploy Reactive Agent for continuous monitoring. Metric: Reduction in alert triage time vs. baseline
  2. "Govern" Simulation: Run Cognitive Agent against compliance documents. Metric: Accuracy in flagging policy violations
  3. "Respond" Escalation: Test Hybrid Agent in "Assisted" mode for incident containment. Metric: Measure human approval time bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AIATDF perform in live Security Operations Centers regarding response times and error rates?
- Basis: Section 7.2 states empirical validation is necessary to establish applicability.
- Why unresolved: Study is limited to conceptual validation; no live data exists.
- What evidence would resolve it: Pilot deployment metrics comparing AIATDF-guided vs. unguided operations.

### Open Question 2
- Question: To what extent do organizational culture and budget constraints hinder the framework's adoption?
- Basis: Section 7.2 calls for integrating socio-technical factors into deployment methodologies.
- Why unresolved: Framework currently focuses on technical properties without modeling these constraints.
- What evidence would resolve it: Field studies correlating organizational readiness assessments with deployment success rates.

### Open Question 3
- Question: Can the framework's static taxonomy accurately map to dynamic, self-evolving hybrid agents?
- Basis: Section 6.3 notes real-world agents exhibit fluidity, deviating from rigid categories.
- Why unresolved: Mapping matrix assumes distinct architectural boundaries.
- What evidence would resolve it: Longitudinal studies tracking agent evolution within the framework's structure.

### Open Question 4
- Question: What are the quantitative trade-offs between agent autonomy levels and specific NIST CSF 2.0 outcomes?
- Basis: Section 6.3 notes critical questions regarding practical performance (e.g., false positives) remain unanswered.
- Why unresolved: Conceptual validation did not measure operational metrics.
- What evidence would resolve it: Quantitative data on false positive/negative rates across different autonomy modes.

## Limitations

- Framework remains largely conceptual with minimal direct empirical evidence demonstrating performance improvements
- Graduated autonomy model lacks quantitative thresholds for when organizations should advance between levels
- Governance complexity assumption that cognitive agents can effectively encode and dynamically interpret complex organizational policies remains unproven

## Confidence

**High Confidence:** NIST CSF 2.0 framework itself is well-established with extensive industry adoption. The decomposition of functions into subcategories follows established cybersecurity practices and aligns with existing documentation.

**Medium Confidence:** Conceptual mapping between agent architectures and task requirements follows logical principles. Reactive agents are indeed better suited for high-frequency, rule-based tasks while cognitive agents excel at strategic, policy-driven functions. However, this remains theoretical without empirical validation.

**Low Confidence:** Specific assertion that this framework will reduce deployment failures and accelerate adoption is primarily based on theoretical alignment rather than demonstrated outcomes. Framework's effectiveness in real-world cybersecurity operations has not been established.

## Next Checks

1. **Controlled Environment Test:** Deploy the framework in a simulated SOC environment with known threat scenarios. Compare response times and accuracy between agents selected via this framework versus traditional methods. Focus on Detect function's continuous monitoring task to validate Reactive-to-Hybrid agent progression.

2. **Autonomy Transition Study:** Implement the graduated autonomy model with three participating organizations at different cybersecurity maturity levels. Measure time-to-value for each autonomy level, trust indicators from SOC personnel, and performance metrics when transitioning between levels. This will establish quantitative thresholds for autonomy advancement.

3. **Governance Logic Validation:** Create test suite of evolving compliance requirements and policy changes. Deploy cognitive agents to handle governance tasks and measure accuracy in flagging violations compared to human auditors. Assess agents' ability to handle conflicting requirements and adapt to new regulations without manual reprogramming.