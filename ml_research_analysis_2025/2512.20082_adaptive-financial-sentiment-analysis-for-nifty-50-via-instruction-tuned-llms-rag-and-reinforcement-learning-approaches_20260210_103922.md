---
ver: rpa2
title: Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs
  , RAG and Reinforcement Learning Approaches
arxiv_id: '2512.20082'
source_url: https://arxiv.org/abs/2512.20082
tags:
- sentiment
- financial
- market
- source
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an adaptive framework for financial sentiment
  analysis that integrates instruction-tuned large language models (LLMs) with real-world
  market feedback. It fine-tunes LLaMA 3.2 3B on the SentiFin dataset for Indian financial
  headlines and employs a retrieval-augmented generation (RAG) pipeline to incorporate
  multi-source news context.
---

# Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches

## Quick Facts
- arXiv ID: 2512.20082
- Source URL: https://arxiv.org/abs/2512.20082
- Reference count: 27
- Primary result: Combines instruction-tuned LLaMA 3.2 3B, RAG, and RL to improve NIFTY 50 sentiment classification (up to 0.6650 accuracy, 0.5746 F1).

## Executive Summary
This paper presents an adaptive framework for financial sentiment analysis of NIFTY 50 headlines, integrating instruction-tuned large language models with retrieval-augmented generation (RAG) and reinforcement learning (RL). The system fine-tunes LLaMA 3.2 3B on a domain-specific dataset, uses RAG to provide contextual grounding from multi-source news, and employs a PPO agent to dynamically adjust source credibility based on market feedback. Experiments show improved classification accuracy and F1-score compared to static baselines, demonstrating the value of combining domain alignment, contextual retrieval, and adaptive weighting for robust financial sentiment modeling.

## Method Summary
The framework fine-tunes LLaMA 3.2 3B via QLoRA on the SentiFin dataset for financial sentiment, using instruction-formatted prompts. A RAG pipeline retrieves relevant context from 8,000 Indian financial headlines using sentence embeddings and cosine similarity within a 3-day window. Source credibility is updated via a gradient-style rule or optimized by a PPO agent using rewards based on alignment between predicted sentiment and next-day stock returns. The system is evaluated on NIFTY 50 headlines, with labels derived from price movements relative to rolling mean and standard deviation.

## Key Results
- Accuracy reaches up to 0.6650, surpassing baseline models.
- Weighted F1-score improves to 0.5746, outperforming static RAG methods.
- RL-based source weighting slightly outperforms direct feedback rules, though gains are marginal.
- Contextual retrieval helps accuracy but may bias toward majority class if not carefully tuned.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Alignment via Instruction Tuning
General-purpose LLMs struggle with financial semantics; instruction tuning aligns outputs with domain sentiment definitions. The authors fine-tune LLaMA 3.2 3B using QLoRA and the SentiFin dataset, formatting inputs as instruction-response pairs to map financial syntax to sentiment labels. Core assumption: SentiFin labels accurately reflect financial sentiment and the model can capture these nuances. Break condition: label noise or overfitting to SentiFin phrasing degrades performance on unseen NIFTY 50 headlines.

### Mechanism 2: Contextual Grounding via Retrieval-Augmented Generation (RAG)
Headlines are often context-poor; augmenting with multi-source historical news improves accuracy. The RAG pipeline retrieves relevant context from a corpus using `all-MiniLM-L6-v2` embeddings and cosine similarity, concatenating results with the headline before LLM inference. Core assumption: relevant context exists within a 3-day window and is captured by cosine similarity. Break condition: irrelevant or contradictory context confuses the LLM, lowering accuracy.

### Mechanism 3: Adaptive Source Credibility via Reinforcement Learning (PPO)
Static trust in news sources is inefficient; an RL agent optimizes source weights based on market feedback. A PPO agent treats source weighting as a policy decision, rewarded (+1) if sentiment aligns with next-day returns, penalized (-1) otherwise. Core assumption: past alignment is predictive of future alignment and stable enough for RL exploitation. Break condition: market efficiency changes or sparse rewards cause the agent to fail to converge or learn suboptimal weights.

## Foundational Learning

- **Concept: Quantized Low-Rank Adaptation (QLoRA)**
  - Why needed here: Fine-tuning a 3B parameter model is computationally expensive; QLoRA reduces memory usage by freezing pre-trained weights and training only small adapter layers.
  - Quick check question: Can you explain how LoRA reduces the number of trainable parameters without changing the model's output dimension?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: PPO provides a stable method for updating the RL agent's policy, preventing drastic changes due to single outlier rewards in noisy stock market environments.
  - Quick check question: What is the role of the "clipping" coefficient in the PPO objective function?

- **Concept: Cosine Similarity in Vector Space**
  - Why needed here: This is the retrieval metric used to find relevant news; it measures the angle between query and document embeddings, ensuring semantic similarity.
  - Quick check question: Why is cosine similarity often preferred over Euclidean distance for high-dimensional text embeddings?

## Architecture Onboarding

- **Component map:** Input Layer (headline + date) -> Retrieval Layer (embeddings + vector DB + dynamic source weighting) -> Inference Layer (fine-tuned LLaMA 3.2 3B) -> Feedback Loop (next-day return -> reward -> PPO update).
- **Critical path:** PPO Agent weights determine which documents are retrieved; if these weights are poor or unstable, the RAG pipeline retrieves sub-optimal context, and the LLM's prediction is based on flawed information. The alignment between actual return and prediction is the critical failure point.
- **Design tradeoffs:**
  - Complexity vs. Gain: PPO adds complexity for only marginal performance gains (~0.5-1% F1-score).
  - Context Window: 3-day window balances context and noise; wider windows risk outdated information.
  - Labeling Strategy: Labels from price deviations avoid human bias but assume price movement equates to linguistic sentiment.
- **Failure signatures:**
  - High accuracy but low F1: model predicts majority "Neutral" class (neutral bias).
  - PPO instability: weights oscillate wildly (learning rate or hyperparameters too high).
  - RAG adding noise: performance drops when adding RAG (retrieval threshold too low).
- **First 3 experiments:**
  1. Run fine-tuned LLaMA 3.2 without RAG on test set to isolate raw model capability.
  2. Implement RAG pipeline with equal source weights to determine if context retrieval helps before adding dynamic weighting.
  3. Implement rule-based weight update first; deploy PPO only if simpler method is unstable or insufficient.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would incorporating fundamental indicators (P/E ratios, earnings, debt levels) and peer-stock correlations improve sentiment-return alignment beyond marginal gains from price context alone? Basis: price context improved accuracy but not F1-scores; fundamental factors may explain non-sentiment-driven movements.

- **Open Question 2:** Would extending the return observation window beyond next-day returns (e.g., 3-day, 5-day, 10-day) capture delayed market reactions missed by the current labeling strategy? Basis: next-day labeling assumes immediate efficiency; no experiments tested alternative time horizons.

- **Open Question 3:** Can the PPO-learned source weighting policy generalize to other emerging markets (e.g., BSE Sensex, KOSPI, Bovespa), or do credibility patterns remain market-specific? Basis: framework tested only on NIFTY 50; source list is India-centric; learned weights deviated significantly from manual assignments.

## Limitations
- Experimental setup limited to NIFTY 50 and SentiFin dataset, restricting generalizability.
- Assumes stationarity in news source reliability and market dynamics, not validated over longer horizons.
- RL component adds complexity for only marginal performance gains, raising practical deployment concerns.
- Performance sensitive to quality and alignment of headline-return pairs; noise could significantly degrade results.

## Confidence
- **High Confidence:** Effectiveness of instruction tuning for aligning general-purpose LLMs to domain-specific sentiment tasks, supported by literature and empirical results.
- **Medium Confidence:** Integration of RAG for contextual grounding and use of market-return feedback are sound in principle but implementation details and performance improvements are less certain.
- **Low Confidence:** Value proposition of RL-based adaptive source weighting is most uncertain due to marginal improvement over static methods and sensitivity to reward signal and hyperparameter stability.

## Next Checks
1. Validate model robustness to noise and temporal drift by injecting synthetic noise or testing during high volatility; assess if RL agent adapts or fails.
2. Conduct ablation study comparing PPO-based adaptive weighting to static baseline and rule-based feedback over same horizon to quantify marginal benefit and cost.
3. Evaluate complete pipeline on different stock index (e.g., SENSEX) or financial text type (e.g., earnings call transcripts) to assess generalization beyond NIFTY 50 headlines.