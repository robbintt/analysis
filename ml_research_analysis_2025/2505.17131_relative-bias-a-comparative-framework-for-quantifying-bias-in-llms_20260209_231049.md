---
ver: rpa2
title: 'Relative Bias: A Comparative Framework for Quantifying Bias in LLMs'
arxiv_id: '2505.17131'
source_url: https://arxiv.org/abs/2505.17131
tags:
- bias
- arxiv
- llama
- topics
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Relative Bias framework, a novel method
  for quantifying bias in large language models (LLMs) by measuring their behavioral
  deviation from a set of baseline models within a specified domain. The framework
  uses two complementary approaches: Embedding Transformation, which captures bias
  patterns via sentence representations in embedding space, and LLM-as-a-Judge, where
  an LLM assigns comparative bias scores.'
---

# Relative Bias: A Comparative Framework for Quantifying Bias in LLMs

## Quick Facts
- arXiv ID: 2505.17131
- Source URL: https://arxiv.org/abs/2505.17131
- Reference count: 40
- Primary result: Introduces Relative Bias framework quantifying LLM bias as deviation from baseline models

## Executive Summary
This paper presents a novel framework for detecting and quantifying bias in large language models by measuring behavioral deviation from a consensus of baseline models. Unlike traditional absolute bias detection, the Relative Bias framework identifies models that behave differently from expected normative behavior within a specified domain. The approach uses two complementary methods—instruction-tuned embedding transformation and LLM-as-a-judge scoring—validated through statistical equivalence testing. Applied to politically sensitive domains, the framework successfully identifies deployment-time biases in commercial LLMs while demonstrating that open-source models may exhibit less relative bias.

## Method Summary
The framework generates domain-specific questions using GPT-4o, collects responses from target and baseline LLMs, then scores responses using two methods: (1) INSTRUCTOR embeddings with task-specific instructions and cosine distance scoring, and (2) LLM-as-a-Judge using a 10-point rubric with Gemini 2.0 Flash or GPT-4o. Statistical equivalence testing (TOST) validates whether the target model's deviation from baseline consensus exceeds an acceptable threshold. The method is designed to detect relative bias—behavioral patterns that deviate from normative model behavior—rather than absolute bias classifications.

## Key Results
- DeepSeek R1 shows significant relative bias on China-sensitive topics, with embedding deviation scores of 0.10-0.17 compared to 0.05-0.07 for other models
- Meta AI chatbot exhibits strong evasion on company-related questions with LLM-judge scores of 6.90 vs. 1.80-2.70 for other models
- Open-source Llama 4 does not show significant relative bias in any tested domain
- Strong alignment between embedding and LLM-judge methods validates the dual-method approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparative deviation from baseline model consensus reveals relative bias more reliably than absolute bias classification.
- Mechanism: The framework computes per-question distance between a target model's embeddings/scores and the aggregate of baseline models, then averages across all questions to produce a deviation score. Statistical equivalence testing (TOST) determines if this deviation exceeds an acceptable threshold derived from baseline variability.
- Core assumption: The consensus of multiple diverse LLMs serves as a reasonable proxy for normative behavior within a domain.
- Evidence anchors:
  - [abstract] "deviates from a set of baseline models within a specified domain"
  - [Section 3.1] "treating the consensus of baseline LLMs as a proxy for ground truth allows us to quantify how much a target LLM deviates from the normative model behavior"
  - [corpus] Weak direct support; related work focuses on absolute bias detection rather than comparative frameworks.
- Break condition: If all baselines share the same bias direction, relative comparisons will fail to detect it.

### Mechanism 2
- Claim: Instruction-tuned embeddings can capture domain-specific bias patterns without task-specific fine-tuning.
- Mechanism: The INSTRUCTOR embedding model accepts a task instruction (e.g., "Represent the response for detecting political censorship") alongside text input, projecting responses into an embedding space where relative bias manifests as measurable distance. Cosine distance quantifies deviation.
- Core assumption: The embedding model can generalize to unseen bias domains based on its multi-task training.
- Evidence anchors:
  - [Section 3.4.1] "INSTRUCTOR is trained on a multitask dataset (MEDI) comprising 330 tasks with diverse instructions"
  - [Section 3.4.2] "it is deterministic and reproducible, which yields consistent results given the same inputs"
  - [corpus] "Quantifying Positional Biases in Text Embedding Models" (arXiv:2412.15241) shows embedding models exhibit positional biases, suggesting careful validation is needed.
- Break condition: If the embedding model lacks sensitivity to the target bias domain, distances will not reflect meaningful differences.

### Mechanism 3
- Claim: Dual-method validation (embedding + LLM-judge) increases reliability while compensating for individual method weaknesses.
- Mechanism: Embedding transformation provides deterministic, reproducible scoring; LLM-as-a-Judge provides explainable scores with reasoning. Agreement between methods strengthens conclusions; divergence signals need for investigation.
- Core assumption: Both methods, despite different architectures, converge on similar bias assessments when bias is present.
- Core assumption: Judge LLMs (GPT-4o, Gemini) are not themselves biased on the target domain.
- Evidence anchors:
  - [abstract] "results show strong alignment between both scoring methods"
  - [Section 3.5] "recent work suggests combining multiple automated methods and aggregating their outputs to improve reliability"
  - [corpus] "Evaluating Scoring Bias in LLM-as-a-Judge" (arXiv:2506.22316) documents scoring biases in judge models, reinforcing the need for dual validation.
- Break condition: If the judge model shares the same bias as the target model, it will under-score bias.

## Foundational Learning

- Concept: Equivalence testing (TOST - Two One-Sided Tests)
  - Why needed here: Unlike standard hypothesis testing that detects any difference, TOST determines if a target falls within an acceptable equivalence margin—critical for distinguishing meaningful bias from natural model variation.
  - Quick check question: Why does the framework use TOST instead of ANOVA for comparing model bias scores?

- Concept: Instruction-tuned embeddings
  - Why needed here: The embedding method relies on INSTRUCTOR's ability to adapt its representations based on natural language instructions, eliminating per-domain fine-tuning.
  - Quick check question: How does an instruction-tuned embedding model differ from a standard sentence embedding model?

- Concept: Cosine distance in vector space
  - Why needed here: The embedding-based scoring computes cosine distance between response embeddings to quantify deviation; understanding this metric is essential for interpreting results.
  - Quick check question: Why is cosine distance preferred over Euclidean distance for comparing sentence embeddings?

## Architecture Onboarding

- Component map: Question Generator (GPT-4o) → Target Model + Baseline Models → Response Collection → [Branch A: INSTRUCTOR Embeddings → Cosine Distance Scoring] + [Branch B: Judge LLMs → Bias Score Assignment] → Statistical Testing (TOST) → Relative Bias Conclusion

- Critical path: Question quality → Response elicitation → Embedding/Judge scoring → Equivalence margin calculation → TOST validation. The equivalence margin (δ = k · σ) depends on baseline variability; misestimating this invalidates conclusions.

- Design tradeoffs:
  - Embedding method: Fast, deterministic, but less interpretable; judge-dependent sensitivity
  - LLM-judge method: Explainable outputs, but non-deterministic and sensitive to prompt design/judge bias
  - Baseline selection: More diverse baselines increase robustness but may dilute domain-specific norms
  - k parameter: Higher values reduce false positives but increase false negatives

- Failure signatures:
  - Embedding scores near zero for all models → instruction may not align with bias domain
  - Judge scores cluster at extremes → rubric may be poorly calibrated
  - Baseline models show high variance → δ margin too large to detect moderate bias
  - Identical model scores differently across deployments (e.g., DeepSeek R1 vs. DeepSeek AWS) → deployment-time filters active

- First 3 experiments:
  1. Replicate the China-sensitive topic analysis with a different judge model to validate method agreement and check for judge-specific bias.
  2. Test the framework on a synthetic bias domain where you control ground truth (e.g., artificially biased responses) to verify detection sensitivity.
  3. Vary the baseline set composition (e.g., all Western vs. mixed-origin models) on the same target to quantify how baseline selection affects relative bias conclusions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the embedding-based bias score be effectively integrated into fine-tuning pipelines as a penalty term in the loss function to actively mitigate relative bias?
- Basis in paper: [explicit] The authors state, "We leave this direction as a future work for further exploration" regarding using the speed and determinism of the embedding score for bias mitigation.
- Why unresolved: The paper focuses solely on the detection and quantification of bias rather than the mechanics of correcting it during training.
- What evidence would resolve it: Experimental results showing a reduction in relative bias scores after fine-tuning a model with the embedding-based penalty included in the objective function.

### Open Question 2
- Question: How does the geographic or cultural origin of the baseline models (e.g., using Eastern-developed LLMs vs. Western ones) alter the determination of relative bias?
- Basis in paper: [inferred] The authors note their baseline models are "mostly Western-developed" and explicitly state that "choosing different baselines (e.g., Eastern LLMs) could yield different results."
- Why unresolved: The framework defines bias as deviation from the baseline consensus, but it is unclear how much the cultural homogeneity of the baseline set influences the identification of a target model as "biased."
- What evidence would resolve it: A comparative study applying the framework to the same target model using two distinct sets of baselines (one Western, one Eastern) to measure the variance in resulting bias scores.

### Open Question 3
- Question: To what extent do the inherent biases of the LLM-judge and the embedding model affect the reliability and final output of the Relative Bias framework?
- Basis in paper: [inferred] The Limitations section notes that "the reliability of the evaluation depends on the quality of the embedding model and the LLM used as the judge," acknowledging that limitations in these components may influence results.
- Why unresolved: While the paper shows alignment between two methods, it does not isolate or quantify how the specific biases of the *evaluator* models (e.g., GPT-4o or INSTRUCTOR) might skew the bias scores of the target models.
- What evidence would resolve it: An analysis measuring the correlation between the known biases of various evaluator models and the bias scores they assign to a standardized set of target model responses.

## Limitations

- The framework assumes baseline models are unbiased, but all tested baselines may share Western cultural assumptions that normalize certain biases
- The equivalence margin calculation (δ = k · σ) uses k=2.81 derived from baseline variability, which appears arbitrary without theoretical grounding for what constitutes "acceptable" relative bias
- Judge model bias remains a concern—if GPT-4o or Gemini 2.0 Flash share cultural blindspots with the target model, they may under-detect relative bias

## Confidence

- **High**: The comparative framework design (relative vs. absolute bias) and dual-method validation approach are methodologically sound and well-supported by statistical theory.
- **Medium**: The case study findings for DeepSeek R1 and Meta AI chatbot are reproducible given the question sets, but may reflect alignment choices rather than inherent model bias.
- **Low**: Generalization claims to arbitrary bias domains without per-domain fine-tuning, given the framework's dependence on instruction-tuned embeddings and judge model cultural assumptions.

## Next Checks

1. Test the framework on a controlled synthetic dataset where ground truth bias is known (e.g., artificially manipulated responses) to verify detection sensitivity and false positive rates.
2. Conduct cross-cultural validation by running the same analysis with baseline sets from different geographic regions to quantify how baseline selection affects relative bias conclusions.
3. Perform temporal validation by testing the same models across different time periods to determine if relative bias scores are stable or reflect transient alignment decisions.