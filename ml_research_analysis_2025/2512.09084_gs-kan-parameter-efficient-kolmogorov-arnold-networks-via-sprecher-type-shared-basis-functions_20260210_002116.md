---
ver: rpa2
title: 'GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared
  Basis Functions'
arxiv_id: '2512.09084'
source_url: https://arxiv.org/abs/2512.09084
tags:
- gs-kan
- parameter
- function
- learnable
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GS-KAN, a parameter-efficient variant of Kolmogorov-Arnold
  Networks that leverages a shared learnable B-spline basis function per layer, rather
  than unique functions per edge. Inspired by Sprecher's refinement of the Kolmogorov-Arnold
  theorem, GS-KAN applies learnable linear transformations (scaling and shifting)
  to a single parent function, drastically reducing parameter count while maintaining
  the theoretical benefits of KANs.
---

# GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions

## Quick Facts
- arXiv ID: 2512.09084
- Source URL: https://arxiv.org/abs/2512.09084
- Reference count: 9
- Key outcome: GS-KAN achieves 87.03% accuracy on Fashion-MNIST with ~12.5k parameters, outperforming MLPs with 5% more parameters (86.00%)

## Executive Summary
This paper proposes GS-KAN, a parameter-efficient variant of Kolmogorov-Arnold Networks that leverages a shared learnable B-spline basis function per layer, rather than unique functions per edge. Inspired by Sprecher's refinement of the Kolmogorov-Arnold theorem, GS-KAN applies learnable linear transformations (scaling and shifting) to a single parent function, drastically reducing parameter count while maintaining the theoretical benefits of KANs. The architecture is evaluated across synthetic function approximation, tabular regression (California Housing), and high-dimensional image classification (Fashion-MNIST). GS-KAN outperforms MLPs and standard KANs on high-frequency function approximation, matches or exceeds existing KAN variants on tabular benchmarks, and achieves 87.03% accuracy on Fashion-MNIST with ~12.5k parameters—outperforming an MLP with 5% more parameters (86.00%). The method enables scalable, spline-based architectures in high-dimensional regimes where standard KANs are infeasible due to parameter explosion.

## Method Summary
GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. Each edge computes y_q = Σ λ_{p,q} · ψ_l(x_p + ε_q) where ψ_l is a shared B-spline basis for layer l. Per-edge weights λ_{p,q} and per-node shifts ε_q provide differentiation while amortizing the spline parameter cost across all edges. The architecture uses cubic B-splines with fixed uniform knots on a static domain [-G, G], decoupling spline resolution from network width to enable high-frequency approximation under strict parameter budgets.

## Key Results
- GS-KAN (Micro, 478 params, K=50) achieves MSE 1.65×10⁻⁴ vs Std-KAN (748 params, G=7) at 4.41×10⁻⁴ on synthetic function approximation
- Fashion-MNIST classification: GS-KAN [784,15,15,10] achieves 87.03% accuracy with ~12.5k parameters vs MLP [784,15,20,10] at 86.00% with 5% more parameters
- California Housing: GS-KAN (3,992 params) achieves R² 0.9069 vs Std-KAN (3,984 params) at 0.8971, both outperforming MLP baseline (0.8934)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single learnable B-spline per layer, transformed via learnable linear scalings and shifts, can substitute for unique edge-specific functions without catastrophic expressivity loss.
- Mechanism: Each edge computes `y_q = Σ λ_{p,q} · ψ_l(x_p + ε_q)` where ψ_l is a shared B-spline basis for layer l. Per-edge weights λ_{p,q} and per-node shifts ε_q provide differentiation while amortizing the spline parameter cost across all edges.
- Core assumption: The linear transformation space (scaling + translation) is sufficiently expressive to approximate the diversity that would otherwise require unique functions per edge.
- Evidence anchors:
  - [abstract] "GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer."
  - [Section III.A] Formal definition: "y_q = Σ λ_{p,q} · ψ_l(x_p + ε_q)" with distinction from MLPs explicitly noted.
  - [corpus] Hägg et al. (2025) independently pursue Sprecher-based efficiency, suggesting convergent validation of the core idea, though their formulation differs in constraint structure.
- Break condition: Tasks requiring edge-wise non-linearities with fundamentally incompatible shapes (e.g., one edge needing a highly oscillatory function while another requires near-linear) may exceed what linear transformations of a shared basis can provide.

### Mechanism 2
- Claim: Decoupling the spline resolution from network width enables high-frequency approximation under strict parameter budgets.
- Mechanism: Standard KANs tie knot count to each edge's parameters (O(C·N_in·N_out)). GS-KAN separates these: the shared basis cost C is paid once per layer, allowing deeper/wider networks with finer grids (e.g., K=50 knots in Micro regime) within the same budget.
- Core assumption: The approximation bottleneck in constrained regimes is primarily the inability to allocate sufficient grid resolution, not the lack of unique edge functions.
- Evidence anchors:
  - [Section III.C] "Parameter count ≈ N_in × N_out + C" versus Standard KAN's "N_in × N_out × C."
  - [Table I] GS-KAN (Micro, 478 params, K=50) achieves MSE 1.65×10⁻⁴ vs Std-KAN (748 params, G=7) at 4.41×10⁻⁴.
  - [corpus] Free-RBF-KAN addresses computational overhead of B-splines via RBFs, suggesting the decoupling strategy itself is not the efficiency bottleneck.
- Break condition: When parameter budgets are unconstrained, the relative advantage diminishes; Std-KAN may match or exceed GS-KAN if permitted equivalent parameter counts.

### Mechanism 3
- Claim: Fixed spline domains with batch-based optimization provide sufficient gradient signal for latent feature adaptation without explicit grid recalculation.
- Mechanism: The B-spline domain is fixed to [-G, G]. Rather than adapting grid points, the learnable λ and ε parameters shift/scale activations into the active region. Batch optimization ensures aggregate gradients remain informative even if individual samples occasionally fall outside the grid.
- Core assumption: The linear transformation parameters can "absorb" distributional shifts that would otherwise require grid adaptation.
- Evidence anchors:
  - [Section III.B] "Implicit Adaptation via Batch Optimization: For latent layers, we rely on the learnable parameters to naturally map feature distributions into the spline's active domain."
  - [Table III] Fashion-MNIST (784-dim input) achieves 87.03% accuracy despite high-dimensional latent features, suggesting the strategy scales.
  - [corpus] No direct corpus evidence on fixed-domain strategies; related works (Wav-KAN, Free-RBF-KAN) use alternative basis functions rather than domain management.
- Break condition: Highly non-stationary data distributions or aggressive learning rates may cause activations to drift outside the fixed domain faster than λ/ε can adapt, leading to sparse gradients.

## Foundational Learning

- **B-spline fundamentals (knots, coefficients, degree, local support)**
  - Why needed here: GS-KAN's shared basis is a cubic B-spline; understanding how knot count affects resolution and how coefficients parameterize shape is essential for debugging approximation failures.
  - Quick check question: If increasing knot count from K=20 to K=50, what happens to the number of learnable coefficients?

- **Kolmogorov-Arnold representation theorem vs. Universal Approximation Theorem**
  - Why needed here: GS-KAN explicitly departs from MLPs by placing learnable non-linearities on edges (pre-summation) rather than applying fixed activations post-summation; this distinction determines architecture choices.
  - Quick check question: In an MLP, when is the non-linearity applied relative to the weighted sum? In GS-KAN, where is the non-linearity applied?

- **Sprecher's refinement (shared inner function with scaling/translation)**
  - Why needed here: GS-KAN is explicitly grounded in this theorem variant; the relaxation from fixed constants to learnable parameters is the core theoretical justification.
  - Quick check question: What two operations does Sprecher's theorem apply to the shared function ψ to create edge-specific behavior?

## Architecture Onboarding

- **Component map:**
  - Input layer → [GS-KAN Layer] × depth → Output layer
  - Each GS-KAN Layer contains: (1) shared B-spline ψ_l with learnable coefficients c and fixed knots K; (2) weight matrix λ of shape (N_in, N_out); (3) shift vector ε of shape (N_out,).

- **Critical path:**
  1. Initialize B-spline coefficients c (small random values near identity-like behavior).
  2. Initialize λ weights (e.g., Xavier/Kaiming) and ε shifts (zeros or small values).
  3. Forward pass: for each node q, compute Σ λ_{p,q} · B-spline(x_p + ε_q).
  4. Backward pass: gradients flow to both λ/ε and spline coefficients c.
  5. Ensure input normalization places initial activations within [-G, G].

- **Design tradeoffs:**
  - Higher knot count (K) → better approximation of high-frequency components, but slightly slower evaluation.
  - Deeper networks → more expressive, but require careful initialization to keep latent features in the spline's active domain.
  - Wider layers → more capacity, but GS-KAN's advantage over Std-KAN is most pronounced in narrow, deep configurations under tight budgets.

- **Failure signatures:**
  - Loss plateaus early with near-zero gradients: activations likely drifted outside the spline domain; check distribution of pre-spline inputs.
  - Over-smoothed predictions on high-frequency targets: increase knot count K.
  - Instability during training: λ weights may be scaling activations too aggressively; consider gradient clipping or smaller learning rates for λ.

- **First 3 experiments:**
  1. **Sanity check:** 2D function approximation (e.g., `sin(x)·cos(y)`) with ~200-500 params; compare GS-KAN vs. MLP vs. Std-KAN on MSE and training stability. Verify GS-KAN reaches lower MSE with fewer parameters.
  2. **Domain sensitivity:** On the same 2D task, sweep knot counts K ∈ {10, 20, 50} and domain bounds G ∈ {1, 3, 5}. Identify the point of diminishing returns and any domain-mismatch failures.
  3. **Scaling test:** Fashion-MNIST classification with ~12k params. Reproduce the paper's 87%+ accuracy. Ablate: remove the shared basis (use fixed SiLU) to quantify the contribution of learnable ψ_l.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learnable knot positions improve efficiency over fixed uniform grids in GS-KAN?
- Basis in paper: [explicit] The authors propose moving from static grids to adaptive grid distributions to concentrate resolution on complex function regions.
- Why unresolved: The current architecture uses fixed domains, potentially limiting efficiency when modeling heterogeneous data distributions.
- What evidence would resolve it: Comparisons of parameter efficiency and approximation error on non-uniform synthetic functions using adaptive grids.

### Open Question 2
- Question: Do Gaussian Radial Basis Functions (RBFs) offer a superior trade-off between computational cost and expressivity compared to B-splines?
- Basis in paper: [explicit] The paper highlights the computational intensity of recursive B-spline evaluation and suggests Gaussian RBFs as a faster alternative.
- Why unresolved: It is unclear if RBFs retain the necessary local plasticity and gradient properties required for the shared basis mechanism.
- What evidence would resolve it: Benchmarks comparing training time and accuracy between B-spline and RBF implementations on tabular and image tasks.

### Open Question 3
- Question: Can specialized optimization strategies tailored to the coupled dynamics of weights and spline coefficients outperform standard Adam?
- Basis in paper: [explicit] The conclusion notes that standard optimizers were used and suggests developing initialization and schedules specifically for GS-KAN's parameter dynamics.
- Why unresolved: The interaction between learnable linear transformations and the shared spline basis may require gradient updates that standard optimizers do not provide.
- What evidence would resolve it: Improved convergence rates or final accuracy achieved by a custom learning rate schedule compared to a generic Adam baseline.

## Limitations
- The fixed-domain spline strategy lacks formal guarantees and may fail under aggressive distributional drift or high learning rates.
- The linear transformation model (scaling + translation) may break down for edge functions requiring fundamentally incompatible non-linear shapes.
- The independence assumption for ε_q shifts is asserted but not rigorously justified—correlated shifts might yield better approximations for certain tasks.

## Confidence
- **High confidence**: Parameter efficiency claims (theoretical O(C) vs O(C·N) reduction clearly stated and empirically supported across multiple architectures).
- **Medium confidence**: Synthetic function approximation results (strong performance on high-frequency targets with small parameter budgets, but limited ablation on knot count vs. architecture depth).
- **Medium confidence**: Fashion-MNIST classification (reproduced accuracy matches claims, but training dynamics and hyperparameter sensitivity are underspecified).
- **Low confidence**: Generalization to non-tabular, non-image domains (no experiments on time series, graphs, or sequences; Sprecher's theorem applicability outside these domains is untested).

## Next Checks
1. **Failure mode stress test**: Systematically evaluate GS-KAN on synthetic tasks requiring edge-specific non-linearities with incompatible shapes (e.g., one edge needs a step function, another a highly oscillatory wave). Measure MSE degradation and activation range statistics to identify domain-drift failures.

2. **Comparative KAN benchmark**: Directly compare GS-KAN against Free-RBF-KAN and PRKAN on Fashion-MNIST and California Housing using equivalent parameter budgets. Report both accuracy/MSE and training stability (loss curves, gradient norms) to establish relative efficiency.

3. **Distributional robustness probe**: Train GS-KAN on synthetic data with gradually shifting input distributions (e.g., slowly drifting mean/variance). Monitor activation histograms relative to fixed spline domain and measure accuracy degradation to quantify the fixed-domain strategy's limitations.