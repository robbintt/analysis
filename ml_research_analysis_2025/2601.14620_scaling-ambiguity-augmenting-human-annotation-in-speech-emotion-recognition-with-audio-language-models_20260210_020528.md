---
ver: rpa2
title: 'Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition
  with Audio-Language Models'
arxiv_id: '2601.14620'
source_url: https://arxiv.org/abs/2601.14620
tags:
- emotion
- annotations
- synthetic
- human
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse human annotations
  in Speech Emotion Recognition (SER), which limits modeling of the inherent ambiguity
  in human emotions. The authors propose a framework that leverages Large Audio-Language
  Models (ALMs) to generate synthetic annotations, augmenting human labels to improve
  the reliability of emotion distributional representations.
---

# Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models

## Quick Facts
- arXiv ID: 2601.14620
- Source URL: https://arxiv.org/abs/2601.14620
- Reference count: 0
- Primary result: ALM-generated synthetic annotations augment sparse human labels to improve emotion distributional representations, especially for low-ambiguity emotions

## Executive Summary
This paper addresses the challenge of sparse human annotations in Speech Emotion Recognition (SER), which limits modeling of the inherent ambiguity in human emotions. The authors propose a framework that leverages Large Audio-Language Models (ALMs) to generate synthetic annotations, augmenting human labels to improve the reliability of emotion distributional representations. Their approach, Synthetic Perceptual Proxies, generates synthetic annotations guided by detailed prompts and controlled variance mechanisms. They also introduce DiME-Aug, a distribution-aware multimodal augmentation strategy to address class imbalance. The augmented emotion distributions are used to fine-tune ALMs for emotion distribution estimation.

## Method Summary
The framework generates synthetic annotations via Gemini 2.5-Pro using varied temperature and prompts, creating 6-10 synthetic labels per utterance. These are combined with human annotations to form emotion distributions, which are then used to fine-tune Qwen2-Audio-7B-Instruct with LoRA. DiME-Aug addresses class imbalance through multimodal mixup using k-NN search. The model is trained with Jensen-Shannon Divergence loss to predict emotion distributions rather than single labels.

## Key Results
- Synthetic annotations closely approximate human distributions with 6-10 annotations per utterance (JS divergence convergence)
- Combined human-synthetic annotations improve performance for low/medium ambiguity emotions
- DiME-Aug augmentation improves minority class representation through multimodal interpolation
- Synthetic-only training underperforms compared to human-only or combined approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ALM-generated synthetic annotations can approximate human-derived emotion distributions when generated in sufficient quantity (6–10 per utterance).
- **Mechanism:** Controlled variance is introduced through (a) temperature parameter variation (0.1–1.0) and (b) strategically modified prompts that emphasize different acoustic or linguistic cues, simulating diverse annotator personas. Each synthetic label is treated as an independent sample from the underlying emotion distribution.
- **Core assumption:** ALMs encode sufficient acoustic-linguistic emotional knowledge to produce plausible, diverse perceptual judgments that approximate human annotator variability.
- **Evidence anchors:**
  - [abstract] "synthetic annotations closely approximate human distributions (with 6-10 synthetic annotations per utterance)"
  - [section 4.1] "JS divergence consistently decreases as more synthetic annotations are added... Convergence occurs with approximately six synthetic annotations for IEMOCAP and ten for MSP-Podcast"
  - [corpus] Related work (MELT, arXiv:2505.24493) shows LLM-embedded knowledge can automate emotion annotation, but does not address distributional representation.
- **Break condition:** Highly ambiguous emotions where synthetic labels exhibit higher agreement (F-Kappa 0.803 vs. 0.542 human) and lower entropy than human labels, indicating oversimplification of nuanced emotional states.

### Mechanism 2
- **Claim:** Distribution-aware multimodal augmentation (DiME-Aug) improves generalization by balancing minority classes through interpolated audio and distribution targets.
- **Mechanism:** For minority-class utterances, k-NN identifies nearest neighbors in feature space. Augmented samples combine: (a) weighted audio interpolation x_k = λx_i + (1-λ)x_j, (b) transcript inheritance from dominant utterance (based on λ > 0.5), and (c) interpolated emotion distributions P_k = λP_i + (1-λ)P_j.
- **Core assumption:** Interpolating emotion distributions between acoustically similar utterances produces valid blended emotional states suitable for training.
- **Evidence anchors:**
  - [abstract] "DiME-Aug, a distribution-aware multimodal augmentation strategy to address class imbalance"
  - [section 2.3] "A new augmented sample (x_k, T_k, P_k) is then generated through a weighted combination"
  - [corpus] No direct corpus evidence for distribution interpolation in SER; mixup-based audio augmentation (MixSpeech, ICASSP 2021) addresses ASR, not emotion distributions.
- **Break condition:** When synthetic annotations already provide strong distributional approximations, further augmentation may introduce redundancy (observed: synthetic-only on MSP-Podcast declined after augmentation).

### Mechanism 3
- **Claim:** Combined human-synthetic annotations improve downstream AER performance in low-ambiguity settings but diminish for highly ambiguous emotions.
- **Mechanism:** Human annotations (N small) and synthetic annotations (M large) are pooled equally to estimate augmented emotion distributions. Fine-tuning with Jensen-Shannon Divergence loss optimizes distributional predictions against these combined targets.
- **Core assumption:** Human and synthetic annotations are exchangeable samples from the same underlying emotion distribution; sparsity is the primary limitation, not annotation quality.
- **Evidence anchors:**
  - [abstract] "benefits diminish for highly ambiguous emotions where human disagreement is greater"
  - [section 4.2, Fig. 3] "combined annotations perform comparably to, or even surpass, human-only annotations under low and medium ambiguity, but the advantage diminishes under high ambiguity"
  - [corpus] Emotions as Ambiguity-aware Ordinal Representations (arXiv:2508.19193) addresses temporal ambiguity but not annotation augmentation.
- **Break condition:** High-entropy utterances where synthetic labels oversimplify (lower entropy than human), reducing complementarity and potentially introducing label noise.

## Foundational Learning

- **Concept:** Distributional emotion representation (probability distributions over emotion categories vs. single labels)
  - **Why needed here:** The entire framework treats emotions as distributions; understanding that each utterance has a ground-truth distribution estimated from sparse samples is foundational.
  - **Quick check question:** If 3 annotators label an utterance as [Angry, Angry, Sad], what is the emotion distribution?

- **Concept:** Jensen-Shannon Divergence (JSD) for distribution comparison
  - **Why needed here:** JSD is the primary metric for evaluating (a) synthetic-to-human distribution alignment and (b) downstream model performance.
  - **Quick check question:** Why is JSD preferred over KL divergence for comparing emotion distributions?

- **Concept:** Temperature-controlled sampling in LLMs/ALMs
  - **Why needed here:** Temperature variation is the primary mechanism for generating diverse synthetic annotations; understanding this is essential for reproducing the approach.
  - **Quick check question:** What happens to output diversity as temperature increases from 0.1 to 1.0?

## Architecture Onboarding

- **Component map:** Audio + transcript → Gemini inference (vary temperature/prompts, 6–10× per utterance) → Pool with human labels → Apply DiME-Aug to balance classes → Fine-tune Qwen2-Audio with JSD loss

- **Critical path:** Audio + transcript → Gemini inference (vary temperature/prompts, 6–10× per utterance) → Pool with human labels → Apply DiME-Aug to balance classes → Fine-tune Qwen2-Audio with JSD loss

- **Design tradeoffs:**
  - Synthetic-only: Fast, scalable; but lowest performance (JS ~0.43–0.48)
  - Human-only: Best for high-ambiguity; limited by annotation sparsity
  - Combined: Optimal for low/medium ambiguity; requires saturation analysis to determine synthetic count

- **Failure signatures:**
  - Synthetic F-Kappa >> human F-Kappa (e.g., 0.803 vs. 0.542) indicates oversimplified synthetic labels
  - Synthetic entropy << human entropy indicates insufficient diversity in generation
  - Performance degradation on high-ambiguity utterances when synthetic ratio is too high

- **First 3 experiments:**
  1. **Saturation analysis:** Plot JSD vs. number of synthetic annotations on your target dataset to identify convergence point (replicate Fig. 2).
  2. **Ambiguity-stratified evaluation:** Partition test set by annotation entropy (low/medium/high) and compare human-only vs. combined performance (replicate Fig. 3).
  3. **Ablation on DiME-Aug:** Train with/without augmentation on imbalanced subset to isolate augmentation effect from synthetic annotation effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What advanced prompting or generation strategies can enable ALMs to reliably annotate highly ambiguous emotional speech where human disagreement is substantial?
- **Basis in paper:** [explicit] The abstract and conclusion explicitly state: "highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases" and "achieving consistent reliability, particularly for highly ambiguous emotions, will require the development of more sophisticated prompting and generation strategies."
- **Why unresolved:** The current persona-based prompting with temperature variation produces synthetic annotations with lower entropy than human labels (Table 3), oversimplifying nuanced emotions and reducing complementarity when human disagreement is high.
- **What evidence would resolve it:** A new prompting strategy that yields synthetic annotation entropy matching or exceeding human levels for high-ambiguity utterances, with improved downstream AER performance in the high-ambiguity condition (Figure 3).

### Open Question 2
- **Question:** Why does synthetic-only training consistently underperform compared to human-only or combined training, and can synthetic annotations be made independently sufficient?
- **Basis in paper:** [inferred] Table 2 shows synthetic-only training yields the weakest results across both datasets, yet the paper does not investigate the root cause—whether due to distributional bias, lack of genuine perceptual diversity, or systematic model blindspots.
- **Why unresolved:** The paper demonstrates that synthetic annotations approximate human distributions (JS ~0.35) but does not explain why models trained exclusively on them fail to generalize, leaving unclear whether this is a fundamental limitation or an artifact of current generation methods.
- **What evidence would resolve it:** Ablation studies comparing synthetic annotations generated with different diversity mechanisms, or analysis of systematic prediction errors unique to synthetic-only trained models.

### Open Question 3
- **Question:** What factors determine the optimal number of synthetic annotations per utterance, and can this saturation point be predicted a priori?
- **Basis in paper:** [inferred] Figure 2 shows different saturation points for IEMOCAP (~6) versus MSP-Podcast (~10), which the authors attribute to annotation density differences, but the exact determinants and generalization to other datasets remain unexplored.
- **Why unresolved:** The paper empirically identifies saturation through curve fitting but provides no theoretical or predictive framework for determining this threshold before annotation generation, which is critical for practical deployment efficiency.
- **What evidence would resolve it:** Regression analysis identifying dataset characteristics (number of human annotators, entropy distribution, class balance) that predict saturation point, validated on held-out emotion datasets.

## Limitations

- The effectiveness of synthetic annotations is strongly modality-dependent, with no validation of ALM-based audio-only emotion analysis
- Synthetic annotations tend to oversimplify highly ambiguous emotions, showing higher inter-annotator agreement and lower entropy than humans
- The saturation point of 6-10 synthetic annotations appears dataset-dependent with no systematic method for determining optimal count
- k-NN search in DiME-Aug lacks specification of feature space and k value, making exact reproduction difficult

## Confidence

- **High confidence:** The core finding that ALM-generated synthetic annotations can approximate human emotion distributions when generated in sufficient quantity (6-10 per utterance) is well-supported by empirical evidence across two datasets.
- **Medium confidence:** The claim that synthetic-human combined annotations outperform human-only annotations for low-ambiguity emotions is supported, but the specific threshold for "low ambiguity" is not clearly defined.
- **Medium confidence:** The DiME-Aug augmentation strategy shows promise, but the lack of specificity around k-NN parameters and the observed performance degradation with excessive augmentation suggest the approach requires careful tuning.

## Next Checks

1. **Modality generalization test:** Generate synthetic annotations using audio-only input (without transcripts) through ALM-based audio emotion analysis to determine if the approach works across modalities or is transcript-dependent.

2. **Ambiguity-stratified ablation study:** Systematically vary the ratio of synthetic to human annotations (0%, 25%, 50%, 75%, 100% synthetic) on utterances stratified by annotation entropy (low/medium/high) to identify optimal mixing ratios for different ambiguity levels.

3. **Cross-dataset saturation analysis:** Apply the same ALM generation pipeline to a third emotion dataset (e.g., CREMA-D or Ravdess) to test whether the 6-10 synthetic annotation saturation point generalizes or if dataset-specific calibration is required.