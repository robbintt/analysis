---
ver: rpa2
title: Km-scale dynamical downscaling through conformalized latent diffusion models
arxiv_id: '2510.13301'
source_url: https://arxiv.org/abs/2510.13301
tags:
- downscaling
- diffusion
- prediction
- uncertainty
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of overconfident uncertainty estimates
  in generative diffusion models for km-scale dynamical downscaling of meteorological
  fields. The authors propose augmenting a residual corrective latent diffusion model
  with a conformalized quantile regression framework to produce grid-point-level prediction
  intervals with improved coverage guarantees.
---

# Km-scale dynamical downscaling through conformalized latent diffusion models

## Quick Facts
- **arXiv ID:** 2510.13301
- **Source URL:** https://arxiv.org/abs/2510.13301
- **Reference count:** 24
- **Key outcome:** Conformalized quantile regression substantially improves prediction interval coverage probability (PICP) for km-scale meteorological downscaling, increasing from 0.82 to 0.94 at 90% nominal coverage.

## Executive Summary
This paper addresses overconfident uncertainty estimates in generative diffusion models for km-scale dynamical downscaling of meteorological fields. The authors propose augmenting a residual corrective latent diffusion model with a conformalized quantile regression framework to produce grid-point-level prediction intervals with improved coverage guarantees. Experiments on ERA5 reanalysis data over Italy show that the conformalized approach (CQLDM) achieves substantially improved prediction interval coverage probability (PICP) compared to the baseline diffusion model, with PICP increasing from approximately 0.82 to 0.94 for 2-meter temperature at 90% nominal coverage. The approach also yields more stable interval scores and quantile scores across coverage levels while maintaining appropriate interval widths.

## Method Summary
The method employs a three-stage training process for km-scale dynamical downscaling. First, a deterministic U-Net learns large-scale deterministic dynamics via MSE loss. The residual (observation - deterministic prediction) is then projected into a lower-dimensional latent space and modeled by a latent diffusion model. The VAE-based latent space compression reduces the complexity of the generative task. Finally, conformalized quantile regression is applied to the diffusion model samples to produce calibrated prediction intervals. The approach uses split conformal prediction with asymmetric conformity scores computed separately for upper and lower deviations on a calibration set, ensuring finite-sample marginal validity of the resulting prediction intervals.

## Key Results
- CQLDM achieves PICP of 0.94 for 2-meter temperature at 90% nominal coverage, compared to 0.82 for baseline diffusion model
- The conformalized approach yields more stable interval scores and quantile scores across coverage levels
- Grid-wise conformal calibration provides spatially adaptive uncertainty, expanding intervals more in difficult regions like mountains and coastlines
- Prediction intervals maintain appropriate widths while achieving target coverage, avoiding both under- and over-dispersion

## Why This Works (Mechanism)
The method works through three key mechanisms. First, separating large-scale deterministic dynamics from fine-scale stochastic residuals enables more tractable learning of the conditional distribution. Second, conformalized quantile regression transforms heuristic uncertainty estimates from diffusion samples into prediction intervals with finite-sample coverage guarantees. Third, grid-wise conformal calibration provides spatially adaptive uncertainty that accounts for heterogeneous difficulty across terrain.

## Foundational Learning
- **Residual Corrective Diffusion Models**: Used to decompose downscaling into deterministic large-scale dynamics plus stochastic fine-scale residuals. Why needed: Reduces complexity of generative modeling by isolating fine-scale variability. Quick check: Verify residuals are approximately stationary and decorrelated from large-scale patterns.
- **Latent Diffusion Models**: Enable efficient modeling of high-dimensional residuals in compressed latent space. Why needed: Direct pixel-space diffusion would be computationally prohibitive for km-scale domains. Quick check: Confirm VAE reconstruction quality and latent space dimensionality reduction.
- **Conformalized Quantile Regression**: Provides finite-sample coverage guarantees for prediction intervals. Why needed: Standard diffusion models produce under-dispersive uncertainty estimates. Quick check: Validate calibration set is exchangeable with test set and check coverage across spatial domains.

## Architecture Onboarding

### Component Map
ERA5 inputs (16 km) -> Deterministic U-Net -> Residuals -> VAE Encoder -> Latent Space -> Conditioner -> Denoiser -> M Samples -> Empirical Quantiles -> Asymmetric Conformity Scores -> Calibrated Intervals -> 2 km targets

### Critical Path
Deterministic U-Net (Stage 1) -> VAE Compression (Stage 2) -> Denoiser with Conditioner (Stage 3) -> M-sample Quantile Estimation -> Split Conformal Calibration

### Design Tradeoffs
- **Grid-wise vs Spatially Correlated Conformity**: Grid-wise scoring simplifies implementation but ignores spatial dependencies, relying on backbone model to capture correlations
- **VAE Latent Compression**: Reduces computational burden but may lose fine-scale detail critical for downscaling
- **Number of Samples M**: Higher M improves quantile stability but increases computational cost linearly

### Failure Signatures
- **Under-dispersive Intervals**: PICP consistently below nominal coverage (e.g., 0.82 vs 0.90)
- **Spatial Heterogeneity Issues**: Poor coverage in complex terrain despite overall good aggregate performance
- **Temporal Drift**: Coverage degrades when calibration and test periods are separated by significant time

### 3 First Experiments
1. **Coverage Validation**: Compute PICP separately for mountainous regions, coastal zones, and flat inland areas to verify uniform improvement
2. **Ensemble Size Sensitivity**: Vary M from 10 to 1000 samples and measure impact on PICP stability and computational cost
3. **Temporal Robustness**: Perform rolling-window calibration tests to assess coverage degradation with temporal distance from calibration data

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes grid-wise independence of conformity corrections, which may not hold in regions with strong spatial gradients
- Coverage guarantees depend on exchangeability between calibration and test sets, potentially vulnerable to temporal drift
- The ablation study only compares quantile regression vs conformalized quantile regression, not other uncertainty calibration methods

## Confidence

**Confidence Level: Medium** for the core PICP improvement claim. The ablation study only tests quantile regression (QR) vs conformalized quantile regression (CQR), not other plausible baselines like ensembling or temperature scaling.

**Confidence Level: Medium** for the grid-wise independence assumption. The method explicitly notes that pixel-wise conformity scoring does not account for coverage dependencies among adjacent points.

**Confidence Level: Low** regarding the robustness of the method to temporal drift. The split-conformal approach requires exchangeability between calibration and test sets, which may be violated given strong seasonality and potential climate trends.

## Next Checks

1. **Coverage Stability Across Spatial Domains**: Compute PICP separately for mountainous regions (Alps), coastal zones, and flat inland areas to verify that conformalization improves coverage uniformly rather than just in aggregate.

2. **Temporal Coverage Validation**: Perform rolling-window calibration tests where the calibration set slides forward in time, measuring how PICP degrades with temporal distance from calibration data to assess robustness to distribution shift.

3. **Ensemble Size Sensitivity**: Systematically vary M (number of diffusion samples) from 10 to 1000 and measure the impact on PICP, interval width, and calibration time to establish minimum viable ensemble size and computational trade-offs.