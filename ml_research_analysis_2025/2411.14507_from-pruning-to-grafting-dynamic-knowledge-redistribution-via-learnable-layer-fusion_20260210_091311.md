---
ver: rpa2
title: 'From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer
  Fusion'
arxiv_id: '2411.14507'
source_url: https://arxiv.org/abs/2411.14507
tags:
- fusegpt
- fusion
- block
- blocks
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuseGPT introduces a prune-and-fuse compression paradigm for large
  language models, reframing structured pruning as iterative knowledge redistribution
  rather than simple removal. The method employs a dynamic Macro Influence metric
  to re-evaluate block redundancy as the network topology evolves and uses learnable
  low-rank fusion to adaptively graft pruned block knowledge onto surviving layers
  via lightweight local distillation.
---

# From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion

## Quick Facts
- arXiv ID: 2411.14507
- Source URL: https://arxiv.org/abs/2411.14507
- Reference count: 20
- New SOTA compression-accuracy Pareto frontier for LLMs

## Executive Summary
FuseGPT introduces a prune-and-fuse compression paradigm that reframes structured pruning as iterative knowledge redistribution rather than simple removal. The method employs a dynamic Macro Influence metric to re-evaluate block redundancy as the network topology evolves and uses learnable low-rank fusion to adaptively graft pruned block knowledge onto surviving layers via lightweight local distillation. Experimental results show FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25% sparsity, it achieves lower perplexity than prior methods at 20% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33× inference speedup with 25% memory reduction.

## Method Summary
FuseGPT implements an iterative prune-and-fuse framework where transformer blocks are removed based on their Macro Influence (MI) scores, which measure functional overlap with neighboring blocks. The knowledge from pruned blocks is transferred to survivors using learnable low-rank matrix factorization, where coefficients are optimized through local distillation. Each fusion iteration updates a window of G=7 neighboring blocks, using 1024 calibration samples and under one GPU-hour. The method is orthogonal to quantization, achieving 52.1% total compression with 4-bit GPTQ.

## Key Results
- Achieves lower perplexity at 25% sparsity than prior methods at 20% sparsity
- Improves zero-shot reasoning by up to 4.5 points across multiple benchmarks
- Delivers 1.33× inference speedup with 25% memory reduction
- Combines with 4-bit GPTQ for 52.1% total compression with negligible quality loss

## Why This Works (Mechanism)

### Mechanism 1: Macro Influence (MI) for Fusion-Aware Block Selection
- Claim: MI identifies blocks whose knowledge can be most effectively absorbed by neighbors, not just which blocks are "least important."
- Mechanism: For each block Bi, temporarily remove it and measure cosine similarity between final hidden states of original vs. pruned model. Lower MI = more absorbable. Scores are recomputed after each fusion iteration.
- Core assumption: Blocks causing minimal perturbation to the final representation manifold have functional overlap with remaining blocks.
- Evidence anchors:
  - [Section 4.2]: MI defined as 1 - cos(H_M^(N), H_(M\i)^(N-1)) over calibration data.
  - [Figure 2(b)]: Spearman correlation ρ=0.58 between pre- and post-removal rankings—demonstrates rank shift invalidates one-shot pruning.
  - [corpus]: Weak corpus support; neighbor papers on MI-PRUN (arXiv:2601.07212) explore mutual information for pruning but not fusion-aware selection.
- Break condition: If MI scores stabilize across iterations (ΔMI < threshold), the rank shift effect is negligible and iterative re-computation may be skipped.

### Mechanism 2: Learnable Low-Rank Manifold Grafting
- Claim: Learnable fusion preserves more knowledge than linear weight averaging by adapting to non-linear feature manifolds.
- Mechanism: For each linear layer, inject pruned block's weights via W_fused = W_i + (AB^T) ⊙ W_p, where A, B are low-rank factors (rank r≪d). Initialize A=0 so fusion starts from identity. Train via local KL distillation.
- Core assumption: Weight matrices of different blocks lie on distinct functional manifolds; direct averaging causes "destructive interference."
- Evidence anchors:
  - [Section 4.3]: Equations 4-6 define coefficient-based injection with low-rank parameterization.
  - [Figure 2(a)]: Pearson r=0.23 correlation between activation similarity and fusion MSE—high similarity pairs can still have high fusion error.
  - [Table 9]: Learnable fusion reduces C4 perplexity by 0.47 vs. static fusion (12.05→11.58).
  - [corpus]: No direct corpus evidence for low-rank grafting in pruning context; this appears novel.
- Break condition: If distillation loss plateaus without improvement, the fusion window or rank r may be insufficient—increase G or r.

### Mechanism 3: Iterative Prune-and-Fuse with Local Healing
- Claim: Iterative removal with localized distillation prevents cascading error accumulation.
- Mechanism: After each block fusion, run local distillation on fusion window only (G≈7 blocks) using KL(H_orig || H_fused). This aligns fused representations before the next iteration.
- Core assumption: Block importance is topology-dependent; removing one block redistributes computational burden and changes remaining blocks' criticality.
- Evidence anchors:
  - [Table 9]: Adding iterative updates yields additional 0.41 perplexity reduction (11.58→11.17).
  - [Section 3, Observation II]: Blocks ranked 25th jumped to 8th after neighbor removal.
  - [corpus]: Prompt-based Depth Pruning (arXiv:2502.04348) confirms block importance is task-dependent, supporting topology-sensitivity.
- Break condition: If validation perplexity increases after a fusion step, the block may have been misidentified—revert and re-evaluate MI.

## Foundational Learning

- **Concept: Low-Rank Matrix Factorization (LoRA-style)**
  - Why needed here: FuseGPT uses C = AB^T to parameterize fusion coefficients, requiring r≪min(d_out, d_in).
  - Quick check question: Given W ∈ R^(4096×4096) and rank r=128, how many trainable parameters does C require? (Answer: 2 × 4096 × 128 = 1,048,576 vs. 16,777,216 for full matrix.)

- **Concept: Knowledge Distillation with KL Divergence**
  - Why needed here: Local distillation aligns fused block outputs with original sequence using KL(σ(H_orig/τ) || σ(H_fused/τ)).
  - Quick check question: What happens when temperature τ→∞? (Answer: Distributions become uniform, losing fine-grained guidance.)

- **Concept: Transformer Block Structure**
  - Why needed here: Fusion applies to Q, K, V, O, Up, Down projections individually; understanding residual connections is critical for correct forward pass construction.
  - Quick check question: If you fuse attention weights but not FFN weights, what asymmetry might occur? (Answer: Attention transformations adapt but FFN remains rigid, potentially misaligning feature distributions.)

## Architecture Onboarding

- **Component map:**
  1. MI Calculator: Takes model + calibration batch → MI score per block
  2. Block Selector: argmin over MI scores
  3. Fusion Window Builder: G neighbors around selected block
  4. Low-Rank Grafting: Initialize A=0, B~U(-√(1/r), √(1/r)) per linear layer
  5. Local Distillation: Forward pass through window, compute KL, update A, B + LoRA weights
  6. Weight Folding: W_i ← W_i + (AB^T) ⊙ W_p, discard pruned block

- **Critical path:**
  MI computation → Block selection → Fusion window construction → Local distillation (T steps) → Weight folding → Repeat until target sparsity

- **Design tradeoffs:**
  - Window size G: Larger G = more parameters updated per iteration (better healing) but higher compute. Paper uses G=7.
  - Rank r: Higher r = more expressive fusion but risk of overfitting. Paper uses r=128.
  - Calibration samples: Paper uses 32 for MI, 1024 for distillation. Fewer samples may cause unstable MI rankings.

- **Failure signatures:**
  - Perplexity spike >2× baseline: Linear averaging used instead of learnable fusion (check Eq. 6 implementation).
  - MI scores all similar (low variance): Calibration data may be unrepresentative—try different samples.
  - Distillation loss diverges: Learning rate for A, B too high—paper uses 0.001 for coefficients vs. 9.65e-6 for weights.

- **First 3 experiments:**
  1. Single-block fusion sanity check: Select one block via MI, fuse to neighbors, verify perplexity change <0.5 on WikiText-2 vs. dense model.
  2. MI vs. BI (Block Influence) comparison: Run 10 pruning iterations using each metric, plot perplexity trajectory. Expect MI to show slower degradation.
  3. Fusion window ablation: Test G∈{3, 5, 7, 9} at 25% sparsity on LLaMA-2-7B. Expect G=7 to balance perplexity and GPU-hours per Table 9 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can global fusion strategies that redistribute knowledge to non-adjacent blocks outperform the current local neighborhood constraint?
- Basis in paper: [explicit] Limitations section states "our current approach constrains knowledge fusion to local, neighboring blocks...it may not be optimal if a pruned block's knowledge is more relevant to distant blocks in the network. Future work could explore more global fusion strategies."
- Why unresolved: The local fusion window (G=7) assumes functional similarity between adjacent blocks, but the paper provides no evidence that pruned knowledge is best absorbed by neighbors rather than functionally related distant blocks.
- What evidence would resolve it: ComparingFuseGPT with global fusion variants on models where block similarity analysis reveals cross-layer functional relationships, measuring perplexity and zero-shot performance.

### Open Question 2
- Question: How does the iterative prune-and-fuse pipeline scale to models with 70B+ parameters, and can the computational overhead be reduced?
- Basis in paper: [explicit] Limitations section notes "the iterative prune-and-fuse process...can be computationally intensive, especially for very large models or when targeting high sparsity levels."
- Why unresolved: Experiments only cover 7B-13B models; the 45-minute compression time for LLaMA-2-7B may grow prohibitively for frontier models.
- What evidence would resolve it: ApplyingFuseGPT to LLaMA-70B or larger, reporting compression time, memory requirements, and exploring approximations like batched MI computation or parallel fusion.

### Open Question 3
- Question: Is the fixed low-rank coefficient (r=128) optimal across different model architectures and sparsity levels?
- Basis in paper: [inferred] The paper sets r=128 without ablation; Table 9 shows each component contributes, but rank sensitivity is unexplored.
- Why unresolved: Different architectures may require different fusion capacity—the rank controls how much knowledge can be transferred from pruned blocks.
- What evidence would resolve it: Systematic ablation varying r across {32, 64, 128, 256, 512} on multiple architectures (LLaMA, Mistral, Qwen), analyzing the accuracy-efficiency trade-off.

## Limitations
- MI metric stability across different calibration dataset sizes remains unclear with notable variability (ρ=0.58)
- Claims of orthogonality to quantization based on only one experimental point (52.1% compression with 4-bit GPTQ)
- "New SOTA" claim based on comparison against only 3 prior structured pruning methods

## Confidence

- High: The learnable low-rank fusion mechanism (Mechanism 2) - supported by controlled ablation in Table 9 showing consistent perplexity reduction
- Medium: The iterative re-computation of MI scores (Mechanism 1) - supported by Spearman correlation analysis but lacks ablation showing degradation without iteration
- Low: The orthogonality claim with quantization - based on single experimental combination without systematic exploration

## Next Checks

1. Perform ablation study removing iterative MI re-computation to quantify its contribution to the 4.5-point zero-shot reasoning improvement
2. Test FuseGPT on models with different attention patterns (e.g., Mamba, RWKV) to validate the generalizability of learnable low-rank grafting beyond standard transformers
3. Conduct sensitivity analysis varying calibration sample sizes (8, 16, 32, 64) to determine the minimum required for stable MI ranking and its impact on final perplexity