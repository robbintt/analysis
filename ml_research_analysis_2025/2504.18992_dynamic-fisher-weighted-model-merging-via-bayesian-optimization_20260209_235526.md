---
ver: rpa2
title: Dynamic Fisher-weighted Model Merging via Bayesian Optimization
arxiv_id: '2504.18992'
source_url: https://arxiv.org/abs/2504.18992
tags:
- merging
- df-merge
- task
- fisher
- coefficients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for model merging in multi-task
  settings and introduces Dynamic Fisher-weighted Merging (DF-Merge), which leverages
  Bayesian optimization to dynamically adjust scaling coefficients of fine-tuned models
  while incorporating Fisher information to preserve parameter importance. The method
  aims to address the performance gap between model merging and multi-task fine-tuning
  by balancing parameter interference and optimizing coefficients in a flexible manner.
---

# Dynamic Fisher-weighted Model Merging via Bayesian Optimization

## Quick Facts
- arXiv ID: 2504.18992
- Source URL: https://arxiv.org/abs/2504.18992
- Reference count: 21
- Primary result: DF-Merge achieves 78.14% average accuracy for T5-base and 83.59% for T5-large across six diverse tasks, significantly outperforming strong baselines and narrowing the gap with multi-task fine-tuning.

## Executive Summary
This paper introduces Dynamic Fisher-weighted Merging (DF-Merge), a unified framework for merging multiple fine-tuned language models into a single multi-task model. The method combines model-wise scaling coefficients with parameter-wise Fisher importance weighting, optimized via Bayesian optimization. By dynamically estimating Fisher information at scaled models and efficiently searching the coefficient space, DF-Merge significantly outperforms traditional merging approaches while requiring minimal validation data and iterations.

## Method Summary
DF-Merge merges task-specific fine-tuned models by computing task vectors (τᵢ = θᵢ − θ_pre), then using Bayesian optimization to find optimal scaling coefficients (λᵢ) for each task vector. The merging function incorporates diagonal Fisher information computed at dynamically scaled models (θᵢ(λᵢ) = θ_pre + λᵢτᵢ). The method alternates between coefficient sampling, Fisher computation, model merging, and validation until convergence. The unified framework subsumes both averaging and Fisher merging as special cases, while the dynamic Fisher estimation captures loss landscape curvature more relevant to the merged solution.

## Key Results
- DF-Merge achieves 78.14% average accuracy for T5-base and 83.59% for T5-large across six diverse tasks
- Significantly outperforms strong baselines including averaging, task arithmetic, and static Fisher merging
- Converges to near-optimal performance within ~10 iterations using only 5% validation data
- Narrows the performance gap with multi-task fine-tuning while requiring no training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unifying model-wise scaling and parameter-wise importance produces a more general merging objective that captures benefits of both approaches.
- **Mechanism:** The general merging function f = (∑C_θi)^(-1) (∑C_θi · λiτi) + θ_pre subsumes both Averaging (C=I, λ=1/M), Task Arithmetic (C=I, variable λ), and Fisher Merging (C=diag(F̂), λ=1/M). By allowing both C and λ to vary, DF-Merge can weight important parameters while also adjusting per-model contributions.
- **Core assumption:** Parameter importance (via Fisher) and model-level contributions are complementary dimensions that orthogonally improve merging.
- **Evidence anchors:** [abstract] "We unify these seemingly distinct strategies into a more general merging framework"; [Section 3] Eq. 7-8 showing the unified formulation; ablation Table 3 showing both components contribute significantly.

### Mechanism 2
- **Claim:** Computing Fisher information at dynamically scaled models (θ_i(λ_i) = λ_iτ_i + θ_pre) captures local loss landscape curvature more relevant to the merged solution than fixed endpoints.
- **Mechanism:** Traditional Fisher Merging estimates importance at θ_i (λ=1). DF-Merge estimates diag(F̂_θi(λ_i)) at the scaled position, which the paper argues restricts movement along loss-insensitive directions relative to the interpolated model. This aligns importance weighting with where the merged model actually lands.
- **Core assumption:** The loss landscape along the interpolation path contains alternative local minima (Figure 2), and Fisher information estimated mid-path remains meaningful for guiding merging.
- **Evidence anchors:** [Section 2] Geometric analysis showing Fisher Merging minimizes ||Λ^(1/2)(Q^Tθ_i - Q^Tθ_i)||²; [Section 5.3] Figure 6 showing DF-Merge has broader high-accuracy regions than GTA.

### Mechanism 3
- **Claim:** Bayesian optimization efficiently finds near-optimal scaling coefficients for non-differentiable metrics with few iterations and minimal validation data.
- **Mechanism:** Gaussian Process surrogate models the validation accuracy as a function of λ, with acquisition functions (EI, UCB) balancing exploration-exploitation. Each iteration suggests coefficients, merges models, evaluates on validation sets, and updates the posterior.
- **Core assumption:** The validation accuracy landscape is smooth enough for GP modeling; optimal coefficients generalize from small validation sets to test performance.
- **Evidence anchors:** [Section 4.1] "Bayesian optimization is well-suited for optimizing non-differentiable metrics like accuracy"; [Section 5.2] Figure 4 showing convergence in ~10 iterations; Figure 5 showing 5% validation data suffices.

## Foundational Learning

- **Fisher Information Matrix (diagonal approximation)**
  - Why needed here: DF-Merge uses diagonal Fisher to weight parameter importance; understanding why it captures loss curvature and why diagonality is a strong assumption is essential.
  - Quick check question: Why does the diagonal approximation assume parameter independence, and what might this miss for transformer models?

- **Bayesian Optimization (GP surrogate, acquisition functions)**
  - Why needed here: The core optimization loop relies on BO; practitioners must understand exploration-exploitation tradeoffs and how EI/UCB differ.
  - Quick check question: Given 5 initial random points and a 6-dimensional coefficient space, why might BO still find good solutions quickly?

- **Task Vectors and Mode Connectivity**
  - Why needed here: The entire merging paradigm assumes linear interpolation between fine-tuned models remains in a low-loss basin; this is the theoretical foundation.
  - Quick check question: What does "linear mode connectivity" mean, and why does it require shared pretraining initialization?

## Architecture Onboarding

- **Component map:**
  Task-specific models (θ_1...θ_M) + Pretrained (θ_pre) → Task vectors: τ_i = θ_i - θ_pre → Bayesian Optimizer → Sample coefficients [λ_1...λ_M] → Scale task vectors: λ_i · τ_i → Scaled models θ_i(λ_i) → Compute diagonal Fisher F̂ at each θ_i(λ_i) (30 val samples, no labels needed for expectation) → Merge: θ* = (∑F̂_i)^(-1) (∑F̂_i · λ_i · τ_i) + θ_pre → Evaluate merged model on validation sets → Return accuracy to BO → Iterate until convergence or max iterations

- **Critical path:** Fisher computation is the computational bottleneck (requires forward/backward passes over validation samples). Optimization efficiency depends on early termination.

- **Design tradeoffs:**
  - EI vs. UCB acquisition: EI more conservative; UCB explores more aggressively (β tuning required)
  - Validation set size vs. coefficient quality: 5% sufficient per Figure 5, but may vary by task diversity
  - Diagonal vs. full Fisher: Diagonal is tractable but ignores parameter correlations; block-diagonal is a future direction mentioned in Limitations

- **Failure signatures:**
  - Coefficients collapse to uniform (λ_i → 1/M): suggests BO not exploring or validation signal too noisy
  - Single task dominates merged performance: coefficient search may have found local optimum favoring one task
  - Performance degrades with more iterations: overfitting to validation set; implement early stopping

- **First 3 experiments:**
  1. **Reproduction check:** Merge 2-3 fine-tuned T5-base models on provided datasets; verify ~10 iterations reaches within 1% of reported accuracy
  2. **Ablation on validation size:** Run DF-Merge with 5%, 10%, 20%, 50%, 100% validation data; confirm diminishing returns curve matches Figure 5
  3. **Coefficient landscape visualization:** For a 2-model merge, grid-sample λ_1, λ_2 ∈ [0, 1.5] and plot validation accuracy; compare GTA vs. DF-Merge heatmaps to validate Figure 6 patterns

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the assumption of parameter independence in Fisher information be relaxed to improve merging performance?
  - Basis in paper: [explicit] The authors note that diagonal Fisher information "implicitly supposes the model parameters are not related... A promising research direction would be relaxing this assumption, such as representing Fisher Information as a block-diagonal matrix."
  - Why unresolved: The full Fisher information matrix is computationally intractable for large PLMs ($O(d^2)$), forcing a diagonal simplification that ignores parameter correlations.
  - What evidence would resolve it: Experiments utilizing block-diagonal or Kronecker-factored approximations that demonstrate superior performance compared to the diagonal baseline without excessive computational overhead.

- **Open Question 2:** Can DF-Merge be adapted to operate effectively without labeled validation sets?
  - Basis in paper: [explicit] The authors list reliance on labeled validation sets as a limitation but suggest that "fine-tuned models could serve as the pseudo-labeler at the test time" as a feasible approach.
  - Why unresolved: The current Bayesian optimization loop relies on observed validation accuracy as its objective function to search for optimal coefficients.
  - What evidence would resolve it: A modified framework where the optimization objective maximizes consistency with pseudo-labels or minimizes unsupervised loss, achieving performance comparable to the supervised DF-Merge baseline.

- **Open Question 3:** Can DF-Merge be extended to merge models with distinct architectures or different pre-trained initializations?
  - Basis in paper: [explicit] The paper states: "We leave this direction for the future research" regarding scenarios where users "wish to fuse the distinct task expertise of models with different initializations or even across incompatible architectures."
  - Why unresolved: The method relies on linear mode connectivity and parameter alignment, which presupposes a shared initialization and architecture.
  - What evidence would resolve it: A methodology that incorporates weight permutation (e.g., git re-basin) or feature alignment techniques to enable merging across heterogeneous models.

## Limitations

- The method requires models with identical pretraining initialization, limiting applicability to multi-task settings where all tasks start from the same checkpoint
- Performance degradation occurs at very low scaling coefficients, suggesting sensitivity to initialization quality or task similarity
- The diagonal Fisher approximation ignores parameter correlations, potentially missing important optimization structure in transformer models

## Confidence

**High confidence**: The unified merging framework that incorporates both scaling coefficients and Fisher weighting provides theoretical coherence and empirical improvement over single-dimension approaches. The Bayesian optimization component effectively navigates the coefficient space with minimal iterations.

**Medium confidence**: Dynamic Fisher computation at scaled models provides meaningful improvements over static importance estimation. The claim that 5% validation data is sufficient for reliable coefficient estimation requires task-specific validation.

**Low confidence**: The method will generalize equally well to larger model families (T5-3B) or more diverse task sets without modification. The performance gap closure relative to multi-task fine-tuning may narrow or reverse with different task distributions.

## Next Checks

1. **Coefficient sensitivity analysis**: Systematically vary validation set sizes (1%, 5%, 10%, 20%, 50%) across all six tasks to quantify the relationship between validation data and merging performance, particularly for tasks with different data distributions.

2. **Initialization robustness test**: Compare DF-Merge performance when merging models from different random seeds (same architecture, different initialization) versus identical initialization to isolate the impact of the initialization requirement.

3. **Parameter correlation impact**: Implement block-diagonal Fisher estimation (e.g., per-layer blocks) versus diagonal approximation on a subset of tasks to measure the performance cost of the independence assumption and inform future scalability.