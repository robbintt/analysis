---
ver: rpa2
title: 'Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case
  Study of COVID-19 Related Tweets'
arxiv_id: '2510.18908'
source_url: https://arxiv.org/abs/2510.18908
tags:
- topic
- rephrasing
- modeling
- public
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TM-Rephrase is a model-agnostic framework that uses large language
  models to rephrase informal social media short texts into standardized, formal language
  before topic modeling. By transforming noisy, brief tweets into more coherent inputs,
  the framework improves the quality of topic modeling outputs.
---

# Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets

## Quick Facts
- **arXiv ID**: 2510.18908
- **Source URL**: https://arxiv.org/abs/2510.18908
- **Reference count**: 36
- **Primary result**: TM-Rephrase framework improves topic coherence, uniqueness, and diversity in short-text topic modeling by rephrasing informal tweets into formal language before analysis.

## Executive Summary
TM-Rephrase is a model-agnostic framework that uses large language models to rephrase informal social media short texts into standardized, formal language before topic modeling. By transforming noisy, brief tweets into more coherent inputs, the framework improves the quality of topic modeling outputs. In experiments with 25,027 COVID-19-related tweets and four topic modeling algorithms (LDA, BERTopic, FASTopic, TSCTM), TM-Rephrase consistently increased topic coherence, uniqueness, and diversity while reducing redundancy. The colloquial-to-formal rephrasing strategy yielded the greatest gains, especially for LDA. This approach enhances the interpretability of social media analytics for public health and crisis informatics, offering a practical solution to improve thematic discovery in noisy short-text data.

## Method Summary
The study applied TM-Rephrase to 25,027 COVID-19-related tweets from CDC followers, preprocessing by removing URLs, punctuation, stopwords, emojis, and applying lemmatization. Two LLM-based rephrasing strategies were used: general rephrasing and colloquial-to-formal transformation. Topic models (LDA, BERTopic, FASTopic, TSCTM) were then applied to both original and rephrased corpora. Results were evaluated using topic coherence (Cv), uniqueness (TU), redundancy (TR), and diversity (TD) metrics, comparing performance across models and rephrasing strategies.

## Key Results
- TM-Rephrase consistently increased topic coherence, uniqueness, and diversity while reducing redundancy
- Colloquial-to-formal rephrasing strategy yielded the greatest gains, especially for LDA
- TSCTM with C-to-F rephrasing achieved perfect TU and TD scores (TU=1, TD=1) and eliminated redundancy (TR=0)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rephrasing short texts improves topic modeling by enriching sparse contextual signals that algorithms rely on.
- **Mechanism:** LLM-based rephrasing expands brief, informal posts into more grammatically complete and semantically explicit sentences. This increases word co-occurrence density and provides richer lexical patterns for topic models to exploit.
- **Core assumption:** The rephrasing preserves semantic fidelity while adding useful linguistic context.
- **Evidence anchors:**
  - [abstract] "TM-Rephrase consistently increased topic coherence, uniqueness, and diversity while reducing redundancy"
  - [section V] "LDA with C-to-F rephrasing achieved the highest coherence score (Cv = 0.5004), substantially outperforming the baseline without rephrasing (Cv = 0.3094)"
  - [corpus] Weak direct evidence; neighbor papers focus on topic modeling applications but not rephrasing mechanisms.
- **Break condition:** If rephrasing introduces semantic drift or hallucinates content not present in the original, topic quality may degrade.

### Mechanism 2
- **Claim:** Colloquial-to-formal transformation provides greater gains for lexically sparse topic models than general rephrasing.
- **Mechanism:** C-to-F prompts explicitly constrain output to professional vocabulary, reducing lexical variability and mapping diverse informal expressions to consistent formal terms. Models like LDA, which rely on word co-occurrence statistics, benefit more than embedding-based models already resilient to noise.
- **Core assumption:** Formal vocabulary maps more consistently to coherent thematic clusters than informal, variable expressions.
- **Evidence anchors:**
  - [abstract] "colloquial-to-formal rephrasing strategy yielded the greatest gains, especially for LDA"
  - [section V] "C-to-F scheme yielded the highest coherence scores for both LDA and BERTopic"
  - [corpus] No direct corpus evidence comparing rephrasing strategies across topic models.
- **Break condition:** Over-formalization may strip domain-specific slang that carries thematic meaning (e.g., "long covid" as a distinct concept).

### Mechanism 3
- **Claim:** Rephrasing reduces topic redundancy by normalizing linguistic variation across documents.
- **Mechanism:** By mapping multiple informal phrasings to similar formal expressions, rephrasing reduces the surface-form variation that causes topic models to split single themes into multiple overlapping topics.
- **Core assumption:** Linguistic variation in the original corpus is noise rather than meaningful thematic distinction.
- **Evidence anchors:**
  - [section V] "TSCTM with C-to-F rephrasing achieved perfect TU and TD scores (TU=1, TD=1) and eliminated redundancy (TR=0)"
  - [section V] "rephrasing enhanced TD and TU, while simultaneously reducing TR"
  - [corpus] Weak corpus support; neighbor papers don't address redundancy reduction mechanisms.
- **Break condition:** If linguistic variation reflects genuine thematic distinctions (e.g., different community dialects expressing different concerns), normalization may obscure meaningful topics.

## Foundational Learning

- **Topic Coherence Metrics (NPMI, Cv)**
  - Why needed here: The paper evaluates all improvements using Cv coherence scores derived from NPMI; understanding how these metrics capture semantic relatedness is essential for interpreting results.
  - Quick check question: Given two topic word sets {vaccine, adverse, risk} and {vaccine, trump, cdc}, which would likely score higher on Cv coherence and why?

- **Data Sparsity in Short-Text Topic Modeling**
  - Why needed here: The entire motivation for TM-Rephrase stems from how brief texts lack sufficient word co-occurrence signals for reliable topic inference.
  - Quick check question: Why does a 25-word tweet present different statistical challenges for LDA compared to a 500-word news article?

- **Prompt Engineering for Semantic Preservation**
  - Why needed here: The framework's effectiveness depends on prompts that formalize text while constraining against semantic drift or content addition.
  - Quick check question: What specific instruction in the C-to-F prompt prevents the LLM from removing or altering named entities?

## Architecture Onboarding

- **Component map:**
  Raw tweets → Preprocessing (URL/stopword removal, lemmatization) → LLM Rephrasing (Gemini API + prompt templates) → Post-rephrase preprocessing → Topic modeling (LDA/BERTopic/FASTopic/TSCTM) → Evaluation (Cv, TU, TR, TD metrics)

- **Critical path:** The rephrasing quality is the bottleneck. If prompts allow semantic drift, all downstream topic quality gains are compromised. Validate rephrasing fidelity on a sample before full pipeline execution.

- **Design tradeoffs:**
  - General vs C-to-F rephrasing: General preserves more original voice/nuance; C-to-F maximizes coherence gains but risks losing emotional tone and context-specific terms.
  - Model selection: LDA shows largest gains but is most sensitive to input quality; transformer-based models (FASTopic, BERTopic) are more robust to noise but show smaller improvements.
  - Topic count (K=8): Fixed for this study; optimal K may vary with rephrasing strategy.

- **Failure signatures:**
  - Semantic drift: Rephrased text introduces concepts not in original (e.g., Table VII C-to-F omits "vaccine" terminology from original "Your Junk" reference)
  - Over-formalization: Emotional content or urgent tone flattened to neutral professional language
  - Inconsistent entity handling: Named entities altered despite prompt constraints
  - Coherence-diversity trade-off: C-to-F achieved perfect diversity (TD=1) for FASTopic but reduced coherence from 0.3688 to 0.3301

- **First 3 experiments:**
  1. **Fidelity audit:** Manually compare 50 original–rephrased pairs for semantic preservation; flag any hallucinations or content omissions before scaling.
  2. **Ablation by prompt:** Run both prompt variants on same 1,000-tweet sample; quantify difference in word count expansion, vocabulary shift rate, and downstream Cv scores.
  3. **Model sensitivity test:** Apply TM-Rephrase to a single topic model (start with LDA as highest-gain candidate) across 3 topic counts (K=5, 8, 12) to identify whether rephrasing benefits scale consistently or interact with granularity.

## Open Questions the Paper Calls Out

- **Question:** Does the TM-Rephrase framework generalize to noisy short-text domains other than public health, such as e-commerce reviews or financial discussions?
  - Basis in paper: [explicit] The authors state future research should "explore the framework's applicability beyond public health domains."
  - Why unresolved: The study was restricted to a single dataset of COVID-19 related tweets directed at the CDC, limiting the ability to claim universal utility across different vocabularies and linguistic noise patterns.
  - What evidence would resolve it: Experiments applying the framework to diverse short-text corpora (e.g., product reviews, stock market chatter) showing consistent improvements in topic coherence and diversity.

- **Question:** How does the choice of underlying Large Language Model impact the quality of topic modeling outputs?
  - Basis in paper: [explicit] The authors note the study "utilized one specific LLM, Google Gemini" and suggest future work should "compare the performance of various LLMs."
  - Why unresolved: It is undetermined whether the observed benefits are specific to Gemini's architecture or if they hold true for other models like GPT-4 or Llama.
  - What evidence would resolve it: A comparative analysis using identical rephrasing prompts across different state-of-the-art LLMs to measure variance in downstream topic metrics.

- **Question:** How does the rephrasing effectiveness interact with the hyperparameter $K$ (number of topics)?
  - Basis in paper: [explicit] The paper states that "the interaction between rephrasing effectiveness and model hyperparameters, such as the number of topics ($K$), warrants further investigation."
  - Why unresolved: The experiments fixed $K$ at 8 based on preliminary analysis; it remains unclear if rephrasing benefits diminish or amplify when modeling finer or coarser thematic granularities.
  - What evidence would resolve it: Ablation studies varying the number of topics ($K$) across different magnitudes (e.g., 10, 20, 50, 100) on the rephrased corpus.

- **Question:** Can adaptive rephrasing strategies optimize the trade-off between semantic fidelity and topic formality?
  - Basis in paper: [explicit] The authors propose future work "explore adaptive rephrasing strategies that adjust formality based on the input."
  - Why unresolved: The "Colloquial-to-Formal" strategy sometimes over-formalized text to the point of omitting context (e.g., losing the specific reference to vaccines in the "Your Junk" example), creating a trade-off between clarity and nuance.
  - What evidence would resolve it: Development and testing of a dynamic prompt system that assesses input noise levels and selectively applies formalization only where necessary to preserve critical semantic content.

## Limitations
- The study's effectiveness hinges on the semantic fidelity of LLM rephrasing, yet no systematic validation of rephrasing quality is reported
- The analysis uses a single dataset (COVID-19 tweets) and fixed topic count (K=8), limiting generalizability across domains and granularities
- The observed gains may partly reflect vocabulary normalization rather than genuine thematic discovery improvements

## Confidence
- **High Confidence**: The mechanism that rephrasing reduces linguistic variation and improves topic coherence is well-supported by the consistent metric improvements across all four topic models and both rephrasing strategies.
- **Medium Confidence**: The specific claim that colloquial-to-formal rephrasing yields greater gains than general rephrasing is supported but requires more systematic comparison across diverse datasets and topic models.
- **Medium Confidence**: The finding that transformer-based models (FASTopic, BERTopic) show smaller improvements than LDA is plausible but could reflect both model architecture differences and the specific dataset characteristics.

## Next Checks
1. **Semantic Fidelity Audit**: Manually evaluate 100 original-rephrased tweet pairs for semantic preservation, quantifying hallucination rate and content omissions to establish the upper bound of achievable gains.
2. **Domain Transfer Test**: Apply TM-Rephrase to short-text datasets from different domains (political discourse, product reviews, news headlines) to assess generalizability beyond COVID-19 Twitter data.
3. **Topic Count Sensitivity Analysis**: Systematically vary K from 5 to 20 topics to determine whether rephrasing benefits scale consistently or interact with topic granularity, potentially revealing optimal configurations for different use cases.