---
ver: rpa2
title: 'Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional
  Analysis'
arxiv_id: '2505.17636'
source_url: https://arxiv.org/abs/2505.17636
tags:
- safety
- benchmarks
- harm
- while
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study employed UMAP dimensionality reduction and k-means clustering
  to analyze five open-source AI safety benchmarks, identifying six primary harm categories
  with a silhouette score of 0.470. The analysis revealed distinct semantic clusters
  and coverage gaps, with GretelAI focusing on privacy concerns and WildGuardMix emphasizing
  self-harm scenarios.
---

# Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis

## Quick Facts
- arXiv ID: 2505.17636
- Source URL: https://arxiv.org/abs/2505.17636
- Reference count: 40
- Key outcome: Six primary harm categories identified (silhouette score 0.470) across five AI safety benchmarks using UMAP clustering

## Executive Summary
This study analyzes five open-source AI safety benchmarks using semantic embeddings and clustering to quantify their orthogonality and identify coverage gaps. The methodology employs UMAP dimensionality reduction and k-means clustering to reveal six primary harm categories with moderate cluster separation. The analysis exposes significant differences in prompt length distributions across benchmarks, suggesting potential confounds in data collection and harm interpretation. By providing a quantitative framework for evaluating benchmark orthogonality, the work enables targeted development of datasets to comprehensively address evolving AI harms.

## Method Summary
The methodology processes concatenated safety benchmark prompts through MiniLM embeddings, applies UMAP dimensionality reduction (2D, n_neighbors=15-30, min_dist=0.1), and performs k-means clustering with k=6. Cluster quality is evaluated using silhouette scores, with GPT-4 labeling centroids using AEGIS 2.0 taxonomy. The pipeline identifies semantic clusters, quantifies coverage gaps, and analyzes prompt length distributions to reveal potential confounds in safety assessment approaches.

## Key Results
- Identified six primary harm categories with silhouette score of 0.470
- Revealed GretelAI focusing on privacy concerns and WildGuardMix emphasizing self-harm scenarios
- Demonstrated significant prompt length differences (55-136 chars vs >700 chars) indicating distinct safety assessment approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic embeddings capture harm category distinctions sufficiently for clustering safety benchmarks.
- Mechanism: The pipeline converts prompt strings to dense vector representations via transformer-based embedding models (MiniLM/MPNet), then applies UMAP dimensionality reduction to preserve both local neighborhoods and global manifold structure, enabling k-means to partition the semantic space into coherent harm categories.
- Core assumption: The embedding model's semantic representation aligns with human-interpretable harm taxonomies—specifically, that semantically similar prompts cluster near each other in the embedding space and correspond to the same harm category.
- Evidence anchors:
  - [abstract] "distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470)"
  - [section 3.2-3.3] MiniLM provides efficient embeddings; UMAP preserves local and global structure while scaling efficiently
  - [corpus] Weak direct support—neighbor papers focus on safety evaluation frameworks but don't validate embedding-to-harm alignment
- Break condition: If silhouette scores drop below ~0.3 across configurations, or if centroid prompts from different clusters appear semantically indistinguishable to human reviewers, the embedding-to-harm mapping is unreliable.

### Mechanism 2
- Claim: Silhouette score optimization across embedding models, distance metrics, and dimensionality reduction techniques identifies the configuration that best separates harm categories.
- Mechanism: Grid search compares MiniLM vs MPNet, Euclidean vs Mahalanobis distance, and UMAP vs t-SNE. Silhouette score measures intra-cluster cohesion vs inter-cluster separation. The best configuration (MiniLM + Euclidean + UMAP, score 0.470 ± 0.024) balances semantic accuracy with computational efficiency (69.8s processing).
- Core assumption: Higher silhouette scores correspond to more meaningful semantic distinctions—that the metric captures human-relevant category separation rather than artifact-driven clustering.
- Evidence anchors:
  - [abstract] "silhouette score: 0.470"
  - [section 3.7, Fig. 4] "MiniLM with Euclidean distance and UMAP reduction achieves one of the highest silhouette scores of 0.470 ± 0.024... making it the optimal clustering configuration"
  - [corpus] No direct corpus validation of silhouette score as proxy for harm category quality
- Break condition: If different configurations achieve similar silhouette scores but produce vastly different cluster interpretations, the metric is not a reliable proxy for semantic quality.

### Mechanism 3
- Claim: Prompt length distributions reveal confounds in data collection and harm interpretation across benchmarks.
- Mechanism: Statistical comparison of prompt lengths (IQR vs z-score outlier removal, kernel density visualization) shows GretelAI and WildGuardMix use substantially longer prompts (medians >700 chars) while AEGIS, BeaverTails, and AILuminate favor concise prompts (55-136 chars). This bimodal pattern suggests different operationalizations of harm scenarios.
- Core assumption: Prompt length correlates with scenario complexity and reflects intentional design choices rather than random variation.
- Evidence anchors:
  - [abstract] "Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm"
  - [section 6, Fig. 7] "This divergence indicates differing safety assessment approaches: shorter prompts target specific vulnerabilities, while longer prompts evaluate model behavior in nuanced real-world scenarios"
  - [corpus] Related work (NOHARM, SAGE) acknowledges context-dependent safety evaluation but doesn't directly validate length-as-confound
- Break condition: If prompt length variation within a single benchmark exceeds variation between benchmarks, the confound interpretation weakens.

## Foundational Learning

- Concept: **UMAP dimensionality reduction**
  - Why needed here: Core technique for visualizing and clustering high-dimensional embeddings. Requires understanding how `n_neighbors` balances local vs global structure preservation.
  - Quick check question: If you increase `n_neighbors` from 15 to 30, would you expect tighter local clusters or better global organization?

- Concept: **Silhouette score interpretation**
  - Why needed here: Primary metric for cluster quality evaluation. Score of 0.470 indicates moderate cluster separation; understanding what constitutes acceptable scores is critical for configuration selection.
  - Quick check question: A silhouette score of 0.1 would indicate what—well-separated clusters, overlapping clusters, or incorrect cluster count?

- Concept: **Distance metrics in high-dimensional spaces**
  - Why needed here: Euclidean distance fails with correlated high-dimensional data; Mahalanobis accounts for covariance. The choice affects which prompts cluster together.
  - Quick check question: Why might Mahalanobis distance better capture semantic relationships when prompts have varying lengths?

## Architecture Onboarding

- Component map: Raw prompts → [Embedding Model: MiniLM/MPNet] → Dense vectors (768-dim) → [Dimensionality Reduction: UMAP] → 2D projection → [Clustering: K-means with k=6] → Cluster assignments → [Centroid Labeling: GPT-4 + AEGIS taxonomy] → Harm category labels → [Analysis: Silhouette scores, prompt length distributions] → Coverage gaps

- Critical path: Embedding model selection → UMAP hyperparameter tuning (`n_neighbors`, `min_dist`) → k selection (elbow + silhouette agreement) → centroid interpretation. Errors in early stages compound; embedding quality cannot be recovered by later optimization.

- Design tradeoffs:
  - 85% confidence level (α=0.15) vs 95%: Prioritizes exploratory breadth over statistical rigor, increases Type I error risk
  - Z-score vs IQR outlier removal: Z-score retains long-tail prompts (better for diverse harm coverage) but assumes normality
  - MiniLM vs MPNet: MiniLM faster (69.8s) with comparable quality; MPNet better for contextual nuance but slower

- Failure signatures:
  - Silhouette score <0.3 across all configurations → embedding model may not capture harm semantics
  - Elbow method suggests k=5, silhouette suggests k=6 with large gap → reconsider harm taxonomy granularity
  - Centroid labels inconsistent across inference runs → labeling prompt or taxonomy ambiguous

- First 3 experiments:
  1. **Reproduce optimal configuration**: Run MiniLM + Euclidean + UMAP with k=6 on sample data; verify silhouette score falls within 0.470 ± 0.024 bootstrapped CI.
  2. **Stress test distance metrics**: Compare cluster assignments using Mahalanobis vs Euclidean on prompts with extreme length variance; quantify how many prompts switch clusters.
  3. **Validate centroid labels**: Manually review 4 prompts per cluster centroid; assess whether GPT-4 labels match human judgment for at least 5/6 clusters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the identified six-cluster harm taxonomy generalize to safety benchmarks from diverse cultural contexts?
- Basis in paper: [explicit] The authors explicitly state that future work should "expand this framework to include benchmarks from more diverse cultural contexts."
- Why unresolved: The current study relied on existing benchmarks that likely contain implicit Western views of harm, potentially skewing category definitions and cluster boundaries.
- What evidence would resolve it: Replicating the clustering methodology on non-Western safety datasets to observe if the semantic structure and silhouette scores remain consistent.

### Open Question 2
- Question: To what extent does the choice of embedding model propagate bias into the semantic orthogonality of safety benchmarks?
- Basis in paper: [explicit] The limitations section lists "evaluate embedding bias propagation" as a specific avenue for future research.
- Why unresolved: While MiniLM and MPNet were compared, the study acknowledges that model selection imposes representational constraints that were not fully isolated.
- What evidence would resolve it: Ablation studies across a wider variety of embedding architectures to quantify variance in cluster assignment and centroid locations.

### Open Question 3
- Question: How do prompt-response relationships in adversarial settings alter the identified semantic clusters?
- Basis in paper: [explicit] The paper calls for future work to "explore prompt-response relationships in adversarial settings."
- Why unresolved: The current methodology clusters static prompt strings in isolation, failing to capture the dynamic interaction of model responses or jailbreak strategies.
- What evidence would resolve it: Extending the dimensional reduction framework to include response vectors and measuring changes in cluster cohesion under adversarial conditions.

## Limitations

- Validation of semantic embedding quality for harm categorization lacks direct human verification
- Findings based on text-only benchmarks limit generalizability to multimodal safety assessment
- Prompt length analysis shows confounds but relationship between length and complexity remains correlational

## Confidence

- **High Confidence**: The computational methodology (embedding → UMAP → k-means → silhouette optimization) is reproducible and the results are internally consistent. The clustering pipeline and parameter choices are well-documented.
- **Medium Confidence**: The identification of six primary harm categories and coverage gaps is supported by the data but relies on the assumption that semantic embeddings adequately capture harm distinctions. The prompt length analysis is statistically valid but interpretation as confounds requires additional validation.
- **Low Confidence**: The generalizability of findings to emerging harms and different safety contexts. The analysis is based on five existing benchmarks, which may not represent the full landscape of AI safety concerns.

## Next Checks

1. **Human Validation of Cluster Semantics**: Recruit 5-10 safety experts to independently label cluster centroids and evaluate whether the automated clustering aligns with human understanding of harm categories. Measure inter-annotator agreement and compare with GPT-4 labels.

2. **Cross-Validation with Different Embedding Models**: Replicate the analysis using alternative embedding approaches (e.g., MPNet, CLIP for multimodal) to assess whether the six-cluster structure persists. Compare silhouette scores and cluster stability across models.

3. **Temporal Validation of Coverage Gaps**: Apply the clustering framework to newly released safety benchmarks from the past 6-12 months. Quantify whether the identified coverage gaps in 2024 correspond to areas where new benchmarks have emerged, validating the framework's predictive capability for addressing evolving harms.