---
ver: rpa2
title: Provable Separations between Memorization and Generalization in Diffusion Models
arxiv_id: '2511.03202'
source_url: https://arxiv.org/abs/2511.03202
tags:
- score
- diffusion
- lemma
- network
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework to explain memorization
  in diffusion models by analyzing both statistical and architectural properties of
  score functions. From a statistical perspective, the authors show that the ground-truth
  score function does not minimize the denoising score matching loss, creating a non-vanishing
  gap that drives memorization.
---

# Provable Separations between Memorization and Generalization in Diffusion Models

## Quick Facts
- arXiv ID: 2511.03202
- Source URL: https://arxiv.org/abs/2511.03202
- Reference count: 40
- Key outcome: Provides theoretical framework explaining memorization in diffusion models through statistical estimation gaps and architectural capacity separation, validated with pruning-based mitigation

## Executive Summary
This paper presents a theoretical framework explaining why diffusion models memorize training data despite standard training objectives. The authors establish that the ground-truth score function does not minimize the empirical denoising score matching loss, creating a non-vanishing Fisher divergence gap that drives memorization. They prove that approximating the empirical score function requires network size to scale with sample size, while the ground-truth score admits a compact representation. Guided by these insights, they propose a pruning-based method that reduces memorization in diffusion transformers while maintaining generation quality, validated on both synthetic Gaussian mixture data and CIFAR-10.

## Method Summary
The authors analyze diffusion models through two lenses: statistical estimation gaps and architectural capacity requirements. They prove that the ground-truth score function incurs a non-vanishing loss gap in the empirical denoising score matching objective, quantified as Fisher divergence. They establish that approximating the empirical score (which memorizes) requires network width scaling linearly with sample size, while the ground-truth score admits compact representation. Based on these findings, they propose a pruning-based method that identifies and removes network components critical for memorization while preserving generalization. The method computes head importance via gradient-based scores, prunes the lowest-scoring components, and fine-tunes the reduced network.

## Key Results
- Proved that ground-truth score function does not minimize empirical denoising loss, creating a Fisher divergence gap that drives memorization
- Established that network width must scale linearly with sample size to approximate empirical score, while ground-truth admits compact representation
- Demonstrated that pruning-based method reduces memorization ratio in diffusion transformers while maintaining FID scores on CIFAR-10
- Validated theoretical predictions on synthetic Gaussian mixture models with varying dimensions and sample sizes

## Why This Works (Mechanism)

### Mechanism 1: Statistical Estimation Gap (Fisher Divergence)
- **Claim:** Optimizing the empirical denoising score matching loss biases models toward memorization because the ground-truth score function is not the minimizer of the empirical loss.
- **Mechanism:** The empirical score function (which reproduces training data) minimizes the training loss, whereas the ground-truth score function (which generalizes) incurs a non-vanishing error gap. This gap, defined as `Loss-Gap_t`, is equivalent to the Fisher divergence between the empirical and true distributions.
- **Core assumption:** The training data consists of finite i.i.d. samples from a sub-Gaussian mixture distribution.
- **Evidence anchors:**
  - [Abstract]: "ground-truth score function does not minimize the empirical denoising loss, creating a separation that drives memorization."
  - [Section 4]: Proposition 4.1 proves `Loss-Gap_t = Fisher(\hat{P}_t, P_t)`.
  - [Corpus]: Neighbors like "Smoothing the Score Function" reinforce that minimizing empirical scores leads to overfitting without smoothing.
- **Break condition:** The mechanism fails if the sample size $n \to \infty$ (asymptotic regime) where the empirical distribution converges to the true distribution, closing the gap.

### Mechanism 2: Architectural Capacity Separation
- **Claim:** Memorization requires significantly higher network capacity (width/parameters) than generalization because approximating the empirical score is structurally more complex than approximating the ground truth.
- **Mechanism:** The empirical score function is a Gaussian mixture of $n$ components (one per data point), requiring network width to scale linearly with sample size $n$ (Theorem 5.1). In contrast, the ground-truth score admits a compact representation independent of $n$.
- **Core assumption:** The score network class consists of feedforward ReLU networks (though the authors suggest this extends to other architectures).
- **Evidence anchors:**
  - [Abstract]: "implementing the empirical score function requires network size to scale with sample size."
  - [Section 5]: Theorem 5.1 establishes bounds showing empirical approximators scale as $W_1 = \tilde{O}(n)$ vs ground-truth $W_2 = \tilde{O}(\epsilon^{-d/2\beta})$.
- **Break condition:** The mechanism breaks if the network is under-parameterized relative to the complexity of the ground-truth score (failing to learn anything), or if the data manifold violates the Hölder smoothness assumptions.

### Mechanism 3: Regularization via Smoothness Constraints
- **Claim:** Implicit or explicit regularization (weight decay, pruning) mitigates memorization by enforcing Lipschitz continuity that the empirical score function cannot satisfy.
- **Mechanism:** The empirical score function exhibits sharp irregularities (high Lipschitz constants, $\Omega(\sigma_t^{-4})$) near data points in the small-$t$ regime. Weight decay penalizes high Lipschitz constants, making it difficult for the network to fit the irregular empirical score, thereby biasing it toward the smoother ground-truth score.
- **Core assumption:** The ground-truth data distribution is sub-Gaussian Hölder, implying a regular ground-truth score.
- **Evidence anchors:**
  - [Section 5]: Lemma 5.2 quantifies the exploding Lipschitz constant of the empirical score at small $t$.
  - [Section 6]: "Weight decay effectively control the Lipschitz continuity."
- **Break condition:** If the ground-truth distribution itself has discontinuities or sharp edges that violate smoothness assumptions, enforcing high smoothness might degrade generation quality.

## Foundational Learning

- **Concept: Denoising Score Matching (DSM)**
  - **Why needed here:** This is the core training objective. Understanding that the model predicts the gradient of the log-density (score) rather than direct pixel values is crucial to grasping why the "empirical score" leads to density collapse around training points.
  - **Quick check question:** Can you explain why predicting the score $\nabla \log p_t(x)$ is sufficient to generate samples via the reverse SDE, without explicitly knowing the normalization constant of $p_t$?

- **Concept: Fisher Divergence**
  - **Why needed here:** The paper equates the training loss gap to the Fisher divergence. One must understand that this measures the distance between probability distributions based on the difference of their score functions.
  - **Quick check question:** If two distributions have identical score functions everywhere, what does that imply about the distributions themselves?

- **Concept: Network Approximation Theory (ReLU)**
  - **Why needed here:** The theoretical proof relies on how network width and depth constrain the ability to approximate functions (specifically separating the complexity of $n$ Gaussians vs. a smooth density).
  - **Quick check question:** Why does approximating a function with $n$ distinct discontinuities (or sharp peaks) typically require network parameters to scale with $n$?

## Architecture Onboarding

- **Component map:** Score Network ($s_\theta$) -> Forward Process (noise schedule) -> Denoising Loss ($\hat{L}$)
- **Critical path:** The interaction between **Network Width** and **Time Step $t$**. The paper highlights that in the small-$t$ regime (low noise), the empirical score becomes highly irregular. If the network has sufficient width to represent this irregularity, memorization occurs.
- **Design tradeoffs:**
  - **Width vs. Sample Size ($n$):** Increasing width improves generation quality up to a point, but if width scales linearly with $n$ (or $n$ is small), it risks memorization. One must tune width relative to dataset size, not just model capacity goals.
  - **Weight Decay Strength:** Strong decay enforces smoothness (preventing memorization) but may underfit complex true distributions. Weak decay allows memorization.
- **Failure signatures:**
  - **Training Loss:** The model achieves near-zero training loss but produces outputs identical to training data.
  - **Lipschitz Explosion:** Gradient norms explode during inference at low noise levels ($t \to 0$) if the model has memorized the empirical score.
- **First 3 experiments:**
  1. **Capacity Scaling Test:** Train models with varying widths (small, medium, large) on a fixed small dataset (e.g., 5k CIFAR subset). Verify that larger widths increase the "memorization ratio" (reproduction of training data).
  2. **Loss Gap Validation:** Plot `Loss-Gap_t` over time $t$. Confirm using the paper's Theorem 4.3 prediction that the gap scales with $d\sigma_t^{-2}$ and is highest at small $t$.
  3. **Pruning for Mitigation:** Implement the "One-Shot Pruning" method (Algorithm 1) by identifying low-importance attention heads specifically using gradients from the small-$t$ regime. Compare the FID and memorization ratio against random pruning baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the statistical separation framework hold for heavy-tailed distributions?
- **Basis in paper:** [explicit] The Conclusion states the "theoretical framework does not yet extend to heavy-tailed distributions."
- **Why unresolved:** The current proofs rely on sub-Gaussian assumptions (Definition 3.2) to bound tail behaviors and score function regularity.
- **What evidence would resolve it:** A generalization of Theorem 4.3 providing lower bounds on the loss gap for distributions with polynomial tails.

### Open Question 2
- **Question:** Does the pruning strategy scale effectively to large foundation models and datasets?
- **Basis in paper:** [explicit] The authors acknowledge they "lack the computational resources to fully validate their performance on larger datasets and models."
- **Why unresolved:** Experiments were limited to CIFAR-10 and synthetic data; efficacy on high-dimensional data (e.g., Stable Diffusion) is unknown.
- **What evidence would resolve it:** Empirical validation of Algorithm 1 on large-scale diffusion transformers (e.g., DiT-XL) with datasets like ImageNet or LAION.

### Open Question 3
- **Question:** Do gradient-based optimizers implicitly bias training toward the compact ground-truth score representation?
- **Basis in paper:** [inferred] Theorem 5.1 proves such a compact network *exists*, but Section 4 notes that strong optimizers drive large networks to memorize the empirical score.
- **Why unresolved:** The theory proves approximation bounds but does not characterize the optimization trajectory required to find the generalizing solution over the memorizing one.
- **What evidence would resolve it:** Theoretical analysis of gradient dynamics showing convergence to the compact network architecture defined in Theorem 5.1.

## Limitations
- Theoretical framework relies on strong assumptions about sub-Gaussian mixture distributions and ReLU network architectures
- Pruning method requires careful tuning and validation on larger datasets remains limited
- Empirical validation focused on synthetic data and small CIFAR-10 subset, limiting generalizability

## Confidence
- **High Confidence:** The statistical estimation gap mechanism (Fisher divergence equivalence) is rigorously proven under stated assumptions
- **Medium Confidence:** The architectural capacity separation results hold for ReLU networks under Hölder smoothness assumptions, but extension to other architectures remains conjectural
- **Low Confidence:** The exact scaling relationships for network width requirements in practical architectures may vary significantly from theoretical predictions

## Next Checks
1. **Architecture Generalization Test:** Reproduce the capacity separation experiments with Transformer-based score networks (DiT) and compare width scaling requirements against theoretical predictions for ReLU networks
2. **Data Distribution Robustness:** Evaluate memorization metrics on non-smooth data distributions (e.g., mixture of uniform distributions, MNIST with sharp edges) to test the limits of the smoothness-based regularization mechanism
3. **Large-Scale Validation:** Implement the pruning method on full CIFAR-10 and ImageNet-10/100 datasets, measuring both generation quality (FID, precision/recall) and memorization ratio across different network scales (1M-100M parameters)