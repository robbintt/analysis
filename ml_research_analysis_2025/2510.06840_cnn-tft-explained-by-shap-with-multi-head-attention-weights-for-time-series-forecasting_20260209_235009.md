---
ver: rpa2
title: CNN-TFT explained by SHAP with multi-head attention weights for time series
  forecasting
arxiv_id: '2510.06840'
source_url: https://arxiv.org/abs/2510.06840
tags:
- attention
- time
- forecasting
- series
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a CNN-TFT-SHAP-MHAW model for multivariate
  time series forecasting, combining convolutional neural networks (CNNs) for local
  pattern extraction, a temporal fusion transformer (TFT) for long-range dependency
  modeling, and SHAP with multi-head attention weights (MHAW) for interpretability.
  The CNN module uses causal 1D convolutions to capture short-term temporal patterns,
  while the TFT applies multi-head self-attention to capture both short- and long-term
  dependencies.
---

# CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting

## Quick Facts
- arXiv ID: 2510.06840
- Source URL: https://arxiv.org/abs/2510.06840
- Reference count: 40
- Primary result: Proposed CNN-TFT-SHAP-MHAW model achieves MAPE of up to 2.2% on hydroelectric natural flow forecasting

## Executive Summary
This paper proposes a hybrid deep learning architecture for multivariate time series forecasting that combines convolutional neural networks (CNNs) for local pattern extraction with a temporal fusion transformer (TFT) for long-range dependency modeling. The model introduces a novel interpretability framework called SHAP-MHAW that combines SHAP values with multi-head attention weights to identify which time steps most influence predictions. Evaluated on hydroelectric natural flow data, the model outperforms established deep learning approaches at shorter forecast horizons while demonstrating robustness through statistical analysis of 50 experimental runs.

## Method Summary
The CNN-TFT-SHAP-MHAW model uses a hierarchical CNN encoder with causal 1D convolutions to extract local temporal patterns, followed by a TFT with multi-head self-attention to capture both short- and long-term dependencies. The architecture fuses CNN-extracted features with attention-derived global features through concatenation, then applies global average pooling and dense layers for final predictions. The SHAP-MHAW interpretability framework multiplies SHAP values (feature contributions) with attention weights (focus locations) to highlight truly influential time steps. Bayesian optimization determined optimal hyperparameters including 3 CNN layers and 4 attention heads.

## Key Results
- Achieved MAPE of up to 2.2% on hydroelectric natural flow forecasting task
- Outperformed well-established deep learning models at horizons 15, 30, and 60 steps
- Statistical analysis of 50 runs showed average RMSE of 521.5 and MAE of 419.5
- Demonstrated interpretability by identifying t-6 to t-1 as most influential time steps for predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal convolutions serve as a noise filter and local pattern extractor, reducing burden on attention layers
- Mechanism: Stacked 1D convolutional layers with causal padding slide kernels across input sequence, creating hierarchical features where each time step only depends on current and past inputs
- Core assumption: Important predictive signals reside in local contiguous subsequences that can be captured by shared weights before global reasoning
- Evidence anchors: Abstract states CNN distills salient local patterns from raw input sequences; Section III-A confirms causal padding preserves autoregressive structure
- Break condition: If time series is purely defined by non-local, discontinuous dependencies, inductive bias of local convolution may discard critical information

### Mechanism 2
- Claim: Multi-head attention captures long-range dependencies by computing relationships between distant time steps, bypassing CNN locality constraints
- Mechanism: CNN output is projected into Query, Key, and Value matrices; softmax of QK^T generates attention map weighting relevance of all other time steps to current one
- Core assumption: Dataset contains long-range dependencies where relevance of past event is not strictly determined by proximity
- Evidence anchors: Abstract mentions TFT with multi-head attention captures long-range temporal dependencies; Section III-B describes self-attention computing attention scores between all time steps
- Break condition: If sequence length becomes excessive, O(n²) complexity of self-attention may cause memory overflow

### Mechanism 3
- Claim: Multiplying SHAP values by attention weights yields more reliable influence map than either method alone by filtering "look-but-don't-touch" attention heads
- Mechanism: Model computes standard SHAP values (causal contribution) and extracts attention weights (where model looks); element-wise product c = s ⊙ a
- Core assumption: Attention weights alone are insufficient for explanation; SHAP values alone lack temporal focus; their intersection reveals true drivers
- Evidence anchors: Abstract states SHAP-MHAW framework reveals which time steps most influence predictions; Section III-E describes combination highlighting attended time steps with meaningful influence
- Break condition: If attention heads act as residual connections or if SHAP estimation is unstable, product signal may become noisy or misleading

## Foundational Learning

- **Concept: Causal Padding in Convolutions**
  - Why needed here: Standard convolutions look at future values, causing data leakage in forecasting; causal padding ensures output at time t only sees inputs from t and earlier
  - Quick check question: If you run a 1D convolution with kernel size 3 on a time series, does the output at index t depend on the input at index t+1?

- **Concept: Self-Attention vs. Recurrence**
  - Why needed here: Must understand why this architecture replaces/augments RNNs/LSTMs; RNNs compress history into single state vector, while Attention preserves access to entire history sequence
  - Quick check question: In sequence of length 100, how many steps does RNN need to traverse to connect step 1 to step 100? How many steps does Self-Attention need?

- **Concept: Shapley Values (SHAP)**
  - Why needed here: Paper relies on SHAP for interpretability; need to know SHAP approximates marginal contribution of feature by averaging impact of adding feature across all possible subsets
  - Quick check question: If model predicts 100, and removing Feature A drops prediction to 80 (holding others constant), what is approximate SHAP value for Feature A?

## Architecture Onboarding

- **Component map**: Input -> CNN Encoder (1D Conv layers) -> Attention Block (Q,K,V projection -> Multi-Head Attention) -> Fusion (Concatenation) -> Head (Global Average Pooling -> Dense -> Prediction) -> Explainer (SHAP-MHAW post-processor)

- **Critical path**: The Fusion layer (Concatenation) is critical junction; model fails if CNN features are too compressed or if Attention weights are uniform before they merge

- **Design tradeoffs**:
  - CNN vs. Attention Depth: Paper uses Bayesian Optimization to find 3 CNN layers and 4 heads; too many CNN layers may over-smooth signal, too few attention heads may miss diverse temporal patterns
  - Interpretability vs. Complexity: SHAP-MHAW method adds significant computational overhead for explanation but necessary for claimed "trustworthiness"

- **Failure signatures**:
  - Border Effects: Paper explicitly notes erratic values at edge of window (e.g., t-29) in attention/SHAP maps; expect instability at very start of input window
  - High Variance: Table II shows high kurtosis and skewness in errors; model prone to "bad runs" during initialization
  - Horizon Degradation: Performance degrades significantly at horizon 120 compared to 15/30; do not expect uniform accuracy across all future steps

- **First 3 experiments**:
  1. **Ablation Study**: Run model with only CNN branch and only Attention branch (zeroing out other) to verify if fusion is synergistic or if one branch dominates
  2. **Window Sensitivity**: Test window sizes (30 vs 60 vs 90) to find where attention mechanism saturates, given Bayesian optimization settled on specific layer counts
  3. **Interpretability Consistency**: Check if "t-6 to t-1" dominance observed in paper holds for your specific dataset, or if model shifts focus to older lags indicating different underlying pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CNN-TFT-SHAP-MHAW generalize to time series domains beyond hydroelectric natural flow data?
- Basis in paper: [explicit] Authors state "Future work may explore the integration of external knowledge sources" but evaluation conducted only on Tucuruí hydroelectric dataset
- Why unresolved: No experiments on datasets with different temporal patterns, seasonalities, or noise characteristics
- What evidence would resolve it: Benchmarking across diverse multivariate time series datasets with varying characteristics and domains

### Open Question 2
- Question: Why does model performance degrade relative to NBEATSx at longer forecast horizons (120 steps)?
- Basis in paper: [explicit] Table I shows NBEATSx achieved 0.92% better RMSE at horizon 120, while proposed model excelled at shorter horizons
- Why unresolved: Paper does not investigate architectural or representational limitations causing this degradation
- What evidence would resolve it: Systematic ablation studies across increasing horizons and comparison of feature fusion strategies for long-range dependencies

### Open Question 3
- Question: Is element-wise multiplication optimal method for combining SHAP values with attention weights in SHAP-MHAW?
- Basis in paper: [inferred] Algorithm 1 uses element-wise multiplication without theoretical justification or comparison to alternative strategies
- Why unresolved: No empirical validation or theoretical framework demonstrates why multiplication is appropriate versus other fusion methods
- What evidence would resolve it: Comparative evaluation of alternative combination operators and analysis of combined metric properties

### Open Question 4
- Question: What causes sporadic high-error outliers across experimental runs, and how can stability be improved?
- Basis in paper: [explicit] Section V-B notes "opportunities for improved stability and robustness" with worst RMSE reaching 1,246.5 and MAPE up to 30.7% across 50 runs
- Why unresolved: Paper acknowledges outliers but does not investigate failure modes or propose stabilization mechanisms
- What evidence would resolve it: Analysis of initialization conditions, training dynamics, and hyperparameter sensitivity during outlier runs

## Limitations
- Interpretability mechanism relies on stable attention patterns, but attention weights can be noisy or uniform, potentially undermining combined explanation signal
- Performance evaluation limited to single hydroelectric natural flow dataset, raising concerns about generalizability to other domains
- High variance in model performance (high kurtosis and skewness in error metrics) suggests sensitivity to initialization and hyperparameters

## Confidence
- **High Confidence**: Architectural design combining CNN local feature extraction with TFT global attention is well-grounded in established literature; causal convolution implementation and attention mechanism are standard
- **Medium Confidence**: SHAP-MHAW interpretability framework shows promise but lacks extensive validation across diverse datasets; specific product of SHAP values and attention weights as explanatory method needs more rigorous testing
- **Low Confidence**: Claims about model's superiority over well-established deep learning models based on limited comparisons and single dataset, making broad performance claims premature

## Next Checks
1. **Ablation Study**: Systematically disable either CNN or Attention branch to verify if fusion architecture provides genuine synergistic benefits or if one component dominates performance
2. **Cross-Domain Testing**: Evaluate model on multiple time series datasets with different characteristics (financial, traffic, energy consumption) to assess generalizability beyond hydroelectric flow data
3. **Interpretability Robustness**: Test SHAP-MHAW explanations across multiple random seeds and data subsets to verify consistency of identified influential time steps, particularly examining whether "t-6 to t-1" pattern holds across different contexts