---
ver: rpa2
title: 'RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across
  Domains'
arxiv_id: '2511.02331'
source_url: https://arxiv.org/abs/2511.02331
tags:
- instances
- rome
- learning
- domains
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoME, a domain-robust Mixture-of-Experts
  framework for cross-domain MILP solution prediction. RoME addresses the limitation
  of existing single-domain learning approaches by training a unified model across
  multiple MILP domains using a two-level distributionally robust optimization strategy.
---

# RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains

## Quick Facts
- arXiv ID: 2511.02331
- Source URL: https://arxiv.org/abs/2511.02331
- Authors: Tianle Pu; Zijie Geng; Haoyang Liu; Shixuan Liu; Jie Wang; Li Zeng; Chao Chen; Changjun Fan
- Reference count: 40
- Primary result: Achieves 67.7% average improvement over strong baselines across five MILP domains using cross-domain learning

## Executive Summary
RoME introduces a domain-robust Mixture-of-Experts framework for cross-domain MILP solution prediction. The approach addresses the limitation of existing single-domain learning methods by training a unified model across multiple MILP domains using a two-level distributionally robust optimization strategy. RoME combines a shared graph encoder, multiple expert networks for domain-specific specialization, and a task decoder with dynamic routing based on task embeddings. The framework employs intra-domain robustness through embedding perturbations and inter-domain robustness via group-level DRO to enhance generalization. Evaluated across five diverse MILP domains, RoME demonstrates significant performance gains where existing approaches struggle to generalize.

## Method Summary
RoME is a domain-robust Mixture-of-Experts framework that predicts marginal probabilities for binary variables in MILP instances to guide a trust-region search. The architecture consists of a shared graph neural network encoder that processes bipartite representations of MILP instances, extracting variable embeddings and a pooled task embedding. Multiple expert networks specialize in distinct MILP distributions, with a gating network routing instances to appropriate experts based on learned task embeddings. Task decoders aggregate expert outputs to produce final logits. The framework incorporates two levels of robustness: intra-domain robustness through isotropic perturbations on task embeddings during training, and inter-domain robustness through group-level distributionally robust optimization that minimizes worst-case loss across domains.

## Key Results
- Achieves 67.7% average improvement over strong baselines across five MILP domains
- Demonstrates significant performance gains in zero-shot settings on MIPLIB instances
- Outperforms both single-domain models and traditional predict-and-search approaches in cross-domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A mixture-of-experts (MoE) architecture with dynamic routing enables a single model to capture domain-specific combinatorial patterns while maintaining cross-domain generalization.
- Mechanism: The shared graph encoder extracts common structural representations; multiple expert networks specialize on distinct MILP distributions via a learned gating network that routes instances based on task embeddings; task decoders aggregate expert outputs. This allows instance-adaptive specialization without requiring separate models per domain.
- Core assumption: MILP domains share some underlying combinatorial patterns that can be captured by a shared encoder, while domain-specific differences can be modeled by specialized experts with appropriate routing.
- Evidence anchors:
  - [abstract] "RoME dynamically routes problem instances to specialized experts based on learned task embeddings."
  - [Section 3.2] "The MoE consists of multiple expert networks, each specializing in distinct MILP distributions. For each instance, the MoE dynamically routes it to specialized experts based on its graph embedding."
  - [corpus] Corpus shows active interest in learning-based MILP acceleration (branching policies, solution prediction, collaborative solving), but no direct evidence for MoE efficacy in MILP; assumption based on MoE success in other domains.
- Break condition: If domains are extremely dissimilar with no shared structure, the shared encoder provides little benefit; if experts collapse to similar functions, specialization fails; routing may overfit to training domains.

### Mechanism 2
- Claim: Applying isotropic perturbations to task embeddings during training stabilizes expert selection and improves robustness to domain uncertainty.
- Mechanism: Perturbed task embeddings induce different routing vectors; enforcing consistency between original and perturbed representations (via L2 loss) encourages stable outputs under domain assignment uncertainty, acting as regularization that simulates cross-domain shifts.
- Core assumption: Small perturbations in task embedding space meaningfully simulate domain shift; domains are reducible to one another in complexity-theoretic sense (NP-completeness), enabling structural overlap.
- Evidence anchors:
  - [abstract] "intra-domain robustness to enhance local robustness by introducing perturbations on task embeddings"
  - [Section 3.3] "We encourage the model to maintain output consistency under slight changes in the domain assignment... This regularization encourages the model to produce stable representations even under uncertainty in domain assignment."
  - [corpus] Corpus provides no direct evidence for this specific perturbation strategy; limited external validation.
- Break condition: If perturbation magnitude is too large, it corrupts task embeddings; if too small, regularization effect is negligible; perturbation may not correctly model actual distribution shifts.

### Mechanism 3
- Claim: Group-level distributionally robust optimization (DRO) minimizes worst-case loss across domains, mitigating global distributional shifts and improving generalization to unseen domains.
- Mechanism: Rather than uniformly averaging per-domain losses, DRO maintains a learnable weight distribution over domains, up-weighting high-loss domains during training. This balances learning across domains and focuses capacity on harder or underrepresented domains.
- Core assumption: Training domains are representative of the distribution of potential test domains; worst-case performance on training domains correlates with generalization to unseen domains.
- Evidence anchors:
  - [abstract] "inter-domain robustness to mitigate global shifts across domains"
  - [Section 3.4] "Rather than minimizing a uniform average of per-domain losses, we adopt a min-max objective that optimizes performance under the worst-case distribution over training domains."
  - [corpus] DRO techniques appear in broader ML literature (Group DRO, Wasserstein DRO referenced in Section 2.2), but corpus lacks direct validation of DRO for cross-domain MILP.
- Break condition: If one training domain is an outlier (noisy labels or extremely hard), DRO may over-emphasize it at expense of others; DRO assumes convex combinations of domain losses approximate the test distribution.

## Foundational Learning

- Concept: **Mixed-Integer Linear Programming (MILP) and the Predict-and-Search paradigm**
  - Why needed here: RoME operates within the PS framework, predicting marginal probabilities for binary variables that guide a trust-region search. Understanding how predicted solutions are refined by solvers is essential.
  - Quick check question: Can you explain why predicting marginal probabilities rather than exact solutions improves solver guidance?

- Concept: **Graph Neural Networks for MILP bipartite representations**
  - Why needed here: MILP instances are encoded as bipartite graphs (constraint-variable), processed by a GNN encoder. Understanding how GNNs propagate information across this structure is critical.
  - Quick check question: How does a half-convolution operation in a bipartite GNN differ from standard graph convolution?

- Concept: **Mixture-of-Experts and sparse routing**
  - Why needed here: RoME's core architecture is MoE with soft routing; understanding gating mechanisms, expert specialization, and diversity regularization is necessary for both implementation and debugging.
  - Quick check question: What is the role of the temperature parameter in the gating softmax, and how does it affect expert selection sharpness?

## Architecture Onboarding

- Component map:
  - Bipartite graph representation -> Shared GNN Encoder -> Variable embeddings + Task embedding
  - Task embedding -> Encoder Router -> Expert weights -> Expert networks -> Expert outputs
  - Expert outputs -> Decoder Router -> Head weights -> Task decoders -> Logits
  - Logits -> Sigmoid -> Marginal probabilities -> Trust-region search

- Critical path:
  1. Encode MILP instance → variable embeddings + task embedding
  2. Router produces expert weights α; aggregate expert outputs
  3. Decoder router produces head weights β; aggregate decoder outputs → logits
  4. Apply sigmoid → marginal probabilities
  5. Trust-region search around predicted marginals

- Design tradeoffs:
  - More experts: greater specialization capacity but higher risk of collapse and routing overfitting
  - Perturbation magnitude (r): larger r increases regularization strength but risks corrupting embeddings
  - DRO sensitivity (η): higher η aggressively reweights to hard domains, may overfit to outliers
  - Single unified model vs. per-domain models: cross-domain training improves generalization but requires diverse training data

- Failure signatures:
  - **Expert collapse**: All experts produce similar outputs; diversity loss ≈ 0; activation patterns uniform
  - **Routing overfitting**: Perfect routing on training domains but random on unseen domains
  - **DRO domination**: One domain's loss dominates; others show no improvement across epochs
  - **Perturbation instability**: Loss diverges or oscillates when perturbation applied; suggests r too large

- First 3 experiments:
  1. **Single-domain baseline**: Train MoE on one domain, compare with standard PS/ConPS to isolate MoE contribution
  2. **Routing ablation**: Visualize expert activation patterns across domains; verify distinct specialization (per Figure 6a methodology)
  3. **Perturbation sensitivity**: Sweep perturbation ratio (0.05, 0.10, 0.15, 0.20) on held-out domain; observe consistency loss vs. final gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RoME's cross-domain performance scale when training on more than 3-5 diverse MILP domains, and what is the optimal trade-off between domain diversity and computational cost?
- Basis in paper: [explicit] Section E.1 states "cross-domain solvers demonstrate strong performance, their training requires substantial diverse data and incurs relatively high computational costs. Limited by GPU memory capacity, we selected 300 samples per cross-domain dataset."
- Why unresolved: The paper only experiments with training on 3 domains and evaluating on 5 total. The relationship between number of training domains, sample complexity, and generalization performance remains unexplored.
- What evidence would resolve it: Systematic experiments varying the number and diversity of training domains (e.g., 3, 5, 10, 20 domains) with controlled computational budgets, measuring performance on held-out domains and training efficiency metrics.

### Open Question 2
- Question: Would sparse Mixture-of-Experts architectures specifically optimized for MILP problems improve RoME's efficiency and scalability compared to the current dense MoE implementation?
- Basis in paper: [explicit] Section E.1 notes "the field would benefit from more efficient sparse MoE architectures specifically optimized for large-scale MILP applications" and E.2 mentions exploring "sparse MoE variants."
- Why unresolved: RoME uses dense expert weighting, which may be computationally inefficient for large-scale problems. Sparse MoE variants have shown success in other domains but remain unexplored for MILP.
- What evidence would resolve it: Implementing sparse MoE variants (e.g., top-k routing) and comparing against dense MoE on metrics including inference speed, memory usage, and solution quality across different problem scales.

### Open Question 3
- Question: Can more sophisticated mathematical techniques for uncertainty quantification in the DRO framework further enhance robustness beyond the current perturbation-based approach?
- Basis in paper: [explicit] Section E.2 states "more sophisticated mathematical techniques could further enhance model robustness by explicitly incorporating uncertainty" beyond the current approach of "simulating cross-domain distribution shifts through perturbations in the task embedding space."
- Why unresolved: The current perturbation strategy is heuristic. More principled approaches from robust optimization theory could provide tighter generalization bounds and better worst-case guarantees.
- What evidence would resolve it: Comparative study of alternative uncertainty sets (e.g., Wasserstein balls, moment constraints) with theoretical analysis of generalization bounds and empirical evaluation on adversarial domain shifts.

### Open Question 4
- Question: How does the expert specialization pattern emerge during training, and what determines whether experts learn domain-specific versus cross-domain combinatorial patterns?
- Basis in paper: [inferred] The interpretability analysis in Section 4.4 shows clear expert specialization but doesn't explain the mechanism or training dynamics that lead to this emergence. Figure 6 shows final state but not how experts evolved.
- Why unresolved: Understanding the emergence of specialization could inform better expert initialization, routing design, and training curricula for improved generalization.
- What evidence would resolve it: Training dynamics analysis tracking expert activation patterns over epochs, ablation studies varying expert initialization strategies, and experiments with different routing temperature schedules to understand specialization emergence.

## Limitations
- Assumption that MILP domains share sufficient structural overlap for effective cross-domain generalization is critical but unverified across diverse real-world problem sets
- Performance gains on MIPLIB instances are demonstrated but limited to 5 test cases, raising questions about scalability to truly diverse real-world MILPs
- Approach depends on high-quality bipartite graph representations and may struggle with instances where constraints/variables have limited connectivity

## Confidence

- **High Confidence**: The core architecture (MoE with shared encoder + experts + routing) is technically sound and aligns with established ML practices
- **Medium Confidence**: The perturbation-based regularization mechanism has theoretical justification but lacks extensive empirical validation specific to MILP domains
- **Medium Confidence**: The DRO component for inter-domain robustness is well-established in ML literature, though its effectiveness for cross-domain MILP remains to be thoroughly validated

## Next Checks

1. **Perturbation Sensitivity Analysis**: Systematically vary the perturbation ratio r across a wider range (0.05 to 0.25) on multiple held-out domains, measuring both consistency loss and solution quality to identify optimal regularization strength
2. **Expert Diversity Quantification**: Beyond qualitative inspection of Figure 6a, implement quantitative metrics for expert specialization (e.g., mutual information between expert activations and domain labels) to verify meaningful diversity rather than superficial differentiation
3. **Out-of-Distribution Robustness**: Evaluate RoME on MILP instances from domains completely absent from training (e.g., scheduling, network design) to assess whether learned cross-domain generalization transfers beyond the training domain distribution