---
ver: rpa2
title: Mi:dm 2.0 Korea-centric Bilingual Language Models
arxiv_id: '2601.09066'
source_url: https://arxiv.org/abs/2601.09066
tags:
- data
- korean
- training
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mi:dm 2.0 is a Korea-centric bilingual LLM developed to address
  limitations of existing models in Korean language and cultural understanding. It
  introduces a robust data pipeline with quality filtering, synthetic data generation,
  and curriculum learning, along with a Korean-optimized tokenizer.
---

# Mi:dm 2.0 Korea-centric Bilingual Language Models

## Quick Facts
- arXiv ID: 2601.09066
- Source URL: https://arxiv.org/abs/2601.09066
- Reference count: 40
- Mi:dm 2.0 is a Korea-centric bilingual LLM developed to address limitations of existing models in Korean language and cultural understanding.

## Executive Summary
Mi:dm 2.0 is a Korea-centric bilingual large language model designed to overcome the limitations of existing models in understanding Korean language and culture. The model introduces a robust data pipeline with quality filtering, synthetic data generation, and curriculum learning, along with a Korean-optimized tokenizer. It includes two variants: Mi:dm 2.0 Base (11.5B parameters) and Mi:dm 2.0 Mini (2.3B parameters), trained using depth-up scaling and knowledge distillation respectively. Evaluations demonstrate state-of-the-art performance on Korean-specific benchmarks and strong results on English benchmarks, with top-tier zero-shot performance. Safety and robustness are validated through comprehensive RAI evaluations, achieving high Not Unsafe Rates and low Attack Success Rates. Mi:dm 2.0 is released under the MIT license to support research and commercial use.

## Method Summary
Mi:dm 2.0 employs a comprehensive data pipeline to enhance Korean language representation, including quality filtering, synthetic data generation, and curriculum learning. A Korean-optimized tokenizer is introduced to improve language understanding. The model lineup consists of Mi:dm 2.0 Base (11.5B parameters) and Mi:dm 2.0 Mini (2.3B parameters), trained using depth-up scaling and knowledge distillation respectively. Evaluations on Korean-specific benchmarks like KMMLU and English benchmarks demonstrate state-of-the-art performance, with strong zero-shot capabilities. Safety and robustness are validated through RAI evaluations, achieving high Not Unsafe Rates and low Attack Success Rates.

## Key Results
- State-of-the-art performance on Korean-specific benchmarks such as KMMLU.
- Strong results on English benchmarks with top-tier zero-shot performance.
- High Not Unsafe Rates and low Attack Success Rates in RAI evaluations.

## Why This Works (Mechanism)
Mi:dm 2.0's effectiveness stems from its Korea-centric approach, which addresses the limitations of existing models in understanding Korean language and culture. The model's robust data pipeline, including quality filtering, synthetic data generation, and curriculum learning, ensures high-quality and diverse training data. The Korean-optimized tokenizer enhances language understanding by better capturing the nuances of Korean text. The use of depth-up scaling and knowledge distillation allows for efficient training of both the Base and Mini variants, balancing performance and computational efficiency. Additionally, the comprehensive RAI evaluations validate the model's safety and robustness, ensuring reliable performance in real-world applications.

## Foundational Learning
- **Curriculum Learning**: Why needed - to improve model learning efficiency by presenting data in a structured order; Quick check - measure performance improvement over baseline training.
- **Synthetic Data Generation**: Why needed - to address data scarcity for Korean language; Quick check - evaluate diversity and quality of synthetic data.
- **Korean-Optimized Tokenizer**: Why needed - to enhance Korean language understanding; Quick check - compare tokenization accuracy with standard tokenizers.

## Architecture Onboarding
- **Component Map**: Data Pipeline -> Tokenizer -> Model (Base/Mini) -> Training (Depth-up Scaling/Knowledge Distillation) -> Evaluation (Benchmarks/RAI)
- **Critical Path**: Data Pipeline -> Tokenizer -> Model Training -> Evaluation
- **Design Tradeoffs**: Balancing model size (Base vs Mini) with performance and computational efficiency; prioritizing Korean language understanding over general multilingual capabilities.
- **Failure Signatures**: Poor performance on Korean-specific tasks may indicate tokenizer or data pipeline issues; safety failures may suggest gaps in RAI evaluations.
- **First Experiments**:
  1. Evaluate tokenizer performance on Korean text segmentation tasks.
  2. Test synthetic data generation quality using diversity and relevance metrics.
  3. Assess model performance on Korean and English benchmarks with varying data sizes.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed ablation studies to isolate contributions of individual components.
- Limited transparency in RAI evaluation methodologies and datasets.
- Potential biases introduced by Korea-centric focus, affecting generalizability.

## Confidence
- **Performance Claims (High)**: Supported by quantitative metrics on Korean and English benchmarks.
- **Safety and Robustness Claims (Medium)**: Favorable RAI results, but methodologies lack transparency.
- **Technical Contributions (Medium)**: Novel components introduced, but implementation details and validation are insufficient.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the Korean-optimized tokenizer, synthetic data generation, and curriculum learning to the overall performance of Mi:dm 2.0.
2. Perform independent safety and robustness evaluations using standardized datasets and methodologies to verify the RAI assessment results reported in the paper.
3. Evaluate the model's performance on multilingual and multicultural benchmarks beyond Korean and English to assess its generalizability and potential biases introduced by the Korea-centric focus.