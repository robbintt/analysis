---
ver: rpa2
title: 'SPAP: Structured Pruning via Alternating Optimization and Penalty Methods'
arxiv_id: '2505.03373'
source_url: https://arxiv.org/abs/2505.03373
tags:
- pruning
- spap
- arxiv
- sparsity
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPAP addresses structured pruning for LLMs by formulating a mixed-integer
  optimization problem and solving it via a penalty method combined with alternating
  minimization. The approach targets MLP layers, using theoretical analysis to show
  relaxation preserves optimality, then employs a penalty framework to make pruning
  decisions and an alternating minimization algorithm for efficient weight updates.
---

# SPAP: Structured Pruning via Alternating Optimization and Penalty Methods

## Quick Facts
- arXiv ID: 2505.03373
- Source URL: https://arxiv.org/abs/2505.03373
- Reference count: 9
- Primary result: Achieves 1.29× inference speedup and 23% memory reduction at 30% sparsity while maintaining lower perplexity and higher zero-shot accuracy than state-of-the-art methods

## Executive Summary
SPAP addresses structured pruning for LLMs by formulating a mixed-integer optimization problem and solving it via a penalty method combined with alternating minimization. The approach targets MLP layers with GLU architecture, using theoretical analysis to show relaxation preserves optimality, then employs a penalty framework to make pruning decisions and an alternating minimization algorithm for efficient weight updates. Experiments demonstrate SPAP achieves superior perplexity and zero-shot accuracy trade-offs compared to state-of-the-art methods across multiple model families.

## Method Summary
SPAP formulates structured pruning as a mixed-integer optimization problem, then relaxes the binary constraints while preserving optimality under specific structural conditions. It employs a penalty method that transforms hard constraints into soft penalties, iteratively increasing the penalty parameter to enforce sparsity. The alternating minimization algorithm exploits the GLU architecture's block structure, using closed-form updates for one projection while applying gradient descent to others. The method uses a composite score metric for pruning decisions that balances column norms and activation magnitudes, enabling efficient mask selection while maintaining model performance.

## Key Results
- Achieves 1.29× inference speedup and 23% memory reduction at 30% sparsity
- Maintains lower perplexity and higher zero-shot accuracy than FASP, CFSP, and FLAP across LLaMA-3/3.1/3.2 and Qwen2.5 families
- Demonstrates effectiveness across model scales from 0.5B to 7B parameters
- Shows superior performance on 6 zero-shot benchmarks including ARC, PIQA, OBQA, WinoGrande, and RTE

## Why This Works (Mechanism)

### Mechanism 1: Integer Relaxation Preserving Optimality
The relaxation of binary pruning decisions to continuous [0,1] preserves global optimality under specific constraint structures. Theorem 1 proves that any optimal solution to the relaxed problem can be transformed into an optimal binary solution through the bilinear constraint structure. The complementarity property ensures that non-zero entries in relaxed solutions correspond to zero columns in weight matrices, enabling accurate binary reconstruction.

### Mechanism 2: Progressive Penalty Enforcement
The penalty method iteratively increases the penalty parameter ρ to enforce sparsity constraints while maintaining optimization stability. By transforming hard bilinear constraints into tractable unconstrained optimization via the penalty term (ρ/2)Σᵢ sᵢ||W[:,i]||²₂, solutions progressively satisfy the original constraints as ρ increases. This approach balances constraint satisfaction with optimization tractability.

### Mechanism 3: Alternating Minimization Exploiting Closed-Form Structure
The alternating minimization algorithm combines closed-form updates for one projection with gradient updates for others, exploiting the GLU architecture's block separability. When up and gate projections are fixed, the down projection admits a least-squares solution, while the other projections require gradient methods. This decomposition enables efficient optimization compared to joint gradient descent.

## Foundational Learning

- **Mixed-Integer Programming Relaxation**: Essential for understanding why continuous relaxation preserves optimality in the pruning problem. Quick check: Why might the continuous optimum of s ∈ [0,1]^n indicate which λ indices to select when the binary constraint s ∈ {0,1}^n is relaxed?

- **Penalty Methods in Constrained Optimization**: Critical for grasping how hard constraints transform into soft penalties. Quick check: As ρ → ∞ in min f(x) + (ρ/2)g(x)², what must happen to g(x) at convergence?

- **Block Coordinate Descent**: Necessary for understanding the alternating minimization convergence properties. Quick check: If subproblem A has closed-form solution but subproblem B requires iterative optimization, why might alternating between them converge faster than joint gradient descent?

## Architecture Onboarding

- **Component map**: Calibration pass -> Penalty Module (scores + soft update) -> Closed-Form Solver (W_down) -> Gradient Updater (W_up/W_gate) -> Penalty Scheduler (ρ increase)

- **Critical path**: 1) Calibration pass collects X and Y from 128 WikiText2 samples 2) Initialize parameters s^(0), W^(0), ρ^(0) 3) Iteratively compute scores, soft update s, closed-form W update, increase ρ 4) Hard threshold s to binary, then run alternating minimization for up/gate/down projections

- **Design tradeoffs**: τ (penalty increase rate) affects enforcement speed vs. convergence risk; t (score balance) trades off column magnitude vs. activation importance; α (soft update momentum) controls mask evolution smoothness; K (iterations) balances convergence quality vs. computational cost

- **Failure signatures**: NaN perplexity in baselines indicates numerical instability in matrix inversion; perplexity spikes at moderate sparsity suggest incorrect mask selection; memory overflow during XX^T computation requires δI perturbation for stability

- **First 3 experiments**: 1) Reproduce LLaMA-3.2-1B at 20% sparsity targeting perplexity ~12.58 2) Ablate alternating minimization vs. gradient descent on Qwen2.5-0.5B 3) Sweep sparsity levels 10%-50% on single model to verify perplexity scaling

## Open Questions the Paper Calls Out

- **Question 1**: Can the mixed-integer optimization formulation be extended to structured pruning of attention heads in Grouped-Query Attention architectures? The current approach targets specifically MLP layers, and the GLU structure derivation differs from attention mechanism computation graphs.

- **Question 2**: Is the SPAP framework applicable to standard (non-GLU) MLP architectures without the structural correspondence exploited in the optimization model? The current formulation relies on specific interdependence between up, gate, and down projections that doesn't exist in standard feed-forward layers.

- **Question 3**: What is the theoretical optimality gap introduced by the heuristic composite score metric used to solve the s-subproblem? While the paper proves relaxation preserves optimality for the model, the algorithm approximates the subproblem solution heuristically to ensure speed.

## Limitations
- Theoretical guarantees rely on specific constraint structures that may not generalize to other pruning targets or activation functions
- Hyperparameter sensitivity to K, t, α, τ, ρ^(0), δ without reported sensitivity analysis
- Numerical stability concerns with matrix inversion in closed-form updates at high sparsity levels

## Confidence

**High confidence** in:
- The alternating minimization algorithm correctly implements the described optimization procedure
- Experimental results showing SPAP outperforms baselines on reported metrics and models
- The core insight that exploiting GLU block structure enables efficient closed-form updates

**Medium confidence** in:
- The relaxation theorem's practical impact on solution quality (theoretical but not empirically validated)
- The penalty method's convergence guarantees under realistic conditions
- The claim that MLP-only pruning scales effectively to larger models

**Low confidence** in:
- Generalizability to non-GLU architectures or different activation functions
- Robustness across diverse datasets and task types
- Performance in extreme sparsity regimes (>50%)

## Next Checks

1. **Ablation study on MLP-only scaling**: Test SPAP on progressively larger models (0.5B → 7B → 70B) to verify memory/compute benefits scale proportionally and closed-form update remains stable.

2. **Cross-dataset perplexity validation**: Evaluate the same pruned models on multiple datasets (C4, OpenWebText, multilingual corpora) to confirm WikiText2-specific calibration doesn't create overfitting to that domain.

3. **Activation function robustness test**: Apply SPAP to models with different activation patterns (ReLU, GeLU, SwiGLU) to validate whether theoretical relaxation guarantees extend beyond the specific GLU/Swish combination used in experiments.