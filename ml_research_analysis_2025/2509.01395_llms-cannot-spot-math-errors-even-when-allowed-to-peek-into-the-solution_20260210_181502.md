---
ver: rpa2
title: LLMs cannot spot math errors, even when allowed to peek into the solution
arxiv_id: '2509.01395'
source_url: https://arxiv.org/abs/2509.01395
tags:
- solution
- error
- step
- student
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  accurately identify the first error step in student math solutions, even when given
  the reference solution. The authors propose generating a corrected student solution
  that aligns more closely with the student's original approach, making errors easier
  to spot.
---

# LLMs cannot spot math errors, even when allowed to peek into the solution

## Quick Facts
- arXiv ID: 2509.01395
- Source URL: https://arxiv.org/abs/2509.01395
- Authors: KV Aditya Srivatsa; Kaushal Kumar Maurya; Ekaterina Kochmar
- Reference count: 34
- Key outcome: LLMs struggle to identify the first error step in student math solutions, even when given the reference solution, though performance improves when using a corrected student solution that better aligns with the original approach.

## Executive Summary
This paper investigates whether large language models can accurately identify the first error step in student math solutions, even when given the reference solution. The authors propose generating a corrected student solution that aligns more closely with the student's original approach, making errors easier to spot. Experiments on two datasets (VtG and PRM800K) show that while providing the gold solution improves performance over no reference, models still struggle. Using the corrected student solution further boosts accuracy, especially for smaller models. Notably, problem-solving ability does not strongly correlate with error localization performance, and feature analysis highlights the importance of solution alignment for effective error detection.

## Method Summary
The authors evaluate three prompting conditions for error localization: (1) problem + student solution only (w/o-S), (2) problem + student solution + gold solution (w-GS), and (3) problem + student solution + corrected student solution (w-Cor). The corrected solution is generated by aligning the gold solution more closely with the student's approach. Experiments use 6 LLMs on VtG (1,002 examples) and PRM800K (2,077 examples), measuring exact first error step localization accuracy and problem-solving ability separately.

## Key Results
- LLMs show low accuracy in identifying first error steps, with best performance at 53.6% on VtG and 42.1% on PRM800K.
- Providing gold solutions improves performance, but corrected student solutions yield further gains, especially for smaller models.
- Problem-solving accuracy does not correlate with error localization ability (phi < 0.2, p > 0.01).
- Semantic alignment between solutions is the most important feature for successful error detection (17.9% feature importance).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Corrected student solutions improve error localization by increasing step-level alignment between the reference and student trace, making the divergence point (first error) more apparent.
- **Mechanism:** The corrected solution preserves the student's reasoning style and step structure while fixing only errors. This creates a near-identical trace up to the first error step, after which the two paths diverge. The LLM can then compare aligned steps more directly, reducing cognitive load from reconciling different problem-solving approaches.
- **Core assumption:** The LLM's comparison process benefits from structural similarity; humans teachers' "offloading" benefit (noted in §1) transfers to model reasoning.
- **Evidence anchors:**
  - [abstract] "approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance"
  - [section 4.2] "Semantic Recall is the most important (17.9%), highlighting the role of alignment in successful error localization"
  - [corpus] Weak direct support; SMRC paper mentions aligning with student reasoning for correction, but not specifically for localization
- **Break condition:** If the model generating the corrected solution fails to preserve stylistic similarity (as Qwen2.5-72B-Math did with 63.3% stylistic similarity), the alignment benefit degrades and error localization can worsen below baseline.

### Mechanism 2
- **Claim:** Error detection ability is largely independent from problem-solving ability, suggesting separate underlying competencies.
- **Mechanism:** The task requires meta-reasoning—evaluating another agent's reasoning trace—rather than executing one's own. A model may generate correct solutions without being able to articulate *why* an alternative reasoning path fails at a specific step.
- **Core assumption:** Meta-reasoning and object-level reasoning engage different model capabilities or representations.
- **Evidence anchors:**
  - [section 4.2] "whether the LLM solved the problem correctly (Solved?) has low importance (1.1%)"
  - [section 4.2] "chi-squared test for independence confirms that error localization and problem-solving accuracy are weakly correlated (p > 0.01; ϕ < 0.2)"
  - [corpus] Self-Correction Bench paper finds LLMs struggle with self-correction, consistent with meta-reasoning being distinct
- **Break condition:** Models fine-tuned specifically for pedagogical tasks (e.g., LearnLM-1.5-Pro) may have partially merged these capabilities, reducing the decoupling effect.

### Mechanism 3
- **Claim:** Error type influences detection accuracy and the direction of prediction errors (overshooting vs. undershooting).
- **Mechanism:** Question-independent errors (calculation, unit conversion) are detected more uniformly, while conceptual errors (misunderstanding the question) cause models to predict errors later than they occur. Missing/extra quantity errors tend to be predicted earlier.
- **Core assumption:** Different error types create different signals in the reasoning trace that the model weights differently.
- **Evidence anchors:**
  - [section 4.3] "errors resulting from question misunderstanding are predicted much later than they occur...errors involving missing or extra variables tend to be predicted a little before they occur"
  - [section 4.3] Figure 4 shows distribution of error types vs. normalized error-step distance
  - [corpus] No direct support in neighboring papers
- **Break condition:** When error type annotations are unavailable (as in PRM800K), this mechanism cannot be leveraged explicitly.

## Foundational Learning

- **Concept: Step-level alignment (semantic recall)**
  - Why needed here: This is the strongest predictor of successful error localization. Understanding how to measure and maximize alignment between solutions is critical.
  - Quick check question: Given a student solution using variable substitution and a gold solution using direct arithmetic, how would you manually create an aligned reference?

- **Concept: Meta-reasoning vs. object-level reasoning**
  - Why needed here: Explains why high problem-solving accuracy doesn't guarantee error detection, which is non-obvious for practitioners.
  - Quick check question: Can you name a task where your ability to critique someone else's work differs from your ability to complete the same task yourself?

- **Concept: Error typology in mathematical reasoning**
  - Why needed here: Error type significantly affects where models make prediction errors. The VtG taxonomy (7 error types) provides a practical framework.
  - Quick check question: Classify: A student writes "2x = 10, therefore x = 20" — is this a calculation error, conceptual error, or something else?

## Architecture Onboarding

- **Component map:**
  ```
  [Problem + Student Solution] 
         ↓
  [Path A: Direct detection] → Error prediction (baseline)
         ↓
  [Path B: With Gold Solution] → Error prediction (improved)
         ↓
  [Path C: Corrected Student Solution Generator]
         ↓ (uses Gold + Student)
  [Aligned Reference S'] → Error prediction (best)
  ```
  The corrected solution generator (§2) is a separate prompt that takes gold and student solutions as input.

- **Critical path:** The corrected solution generation step. If this produces a low-quality correction (wrong intermediate steps, poor stylistic match), downstream error localization degrades. Manual annotation (§C.2) found most models achieve >93% correctness and >87% stylistic similarity, but Qwen2.5-72B-Math achieved only 69.6% and 63.3% respectively.

- **Design tradeoffs:**
  - Corrected solution generation adds latency (two LLM calls vs. one)
  - Quality depends on the generating model's alignment capability
  - Gold solutions from benchmarks may not always be available in production
  - Error type information (available in VtG but not PRM800K) cannot be exploited uniformly

- **Failure signatures:**
  - **Conformity bias / hallucination to match final answer:** The model introducing additional errors later in the corrected solution to force the final answer to match (see Figure 2 with Qwen2.5-72B-Math)
  - **Prediction clustering:** On VtG, 45-60% of incorrect predictions fall within ±1 step; on PRM800K, ~50% fall within ±2 steps (§C.5)
  - **Directional bias by error type:** Misunderstanding errors → late predictions; missing/extra quantity errors → early predictions

- **First 3 experiments:**
  1. **Baseline replication:** Run w/o-S (no reference), w-GS (gold solution), and w-Cor (corrected solution) conditions on a held-out subset of VtG to confirm the performance hierarchy (w-Cor > w-GS > w/o-S).
  2. **Ablation on corrected solution quality:** Manually introduce controlled errors into corrected solutions (e.g., preserve stylistic match but introduce intermediate errors) to measure sensitivity to each quality dimension.
  3. **Error type stratification:** Evaluate per-error-type accuracy to confirm that misunderstanding errors have systematically late predictions and quantify the magnitude of this bias across models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the benefits of using intermediate corrected solutions for error localization generalize to reasoning domains outside of mathematics, such as code debugging or logical inference?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that the experiments are "confined to the math domain" and it remains "unclear whether the observed challenges and benefits would generalize to other domains."
- **Why unresolved:** The study only validates the proposed method on math word problems (GSM8K and MATH datasets) and does not test on other types of reasoning tasks.
- **What evidence would resolve it:** Applying the "corrected student solution" approach to non-math datasets (e.g., code generation benchmarks) and observing if performance improvements persist.

### Open Question 2
- **Question:** Can advanced prompting strategies, such as tree-of-thought (ToT), significantly improve error localization accuracy compared to the few-shot prompting used in this study?
- **Basis in paper:** [explicit] The authors note in the Limitations section that they used a targeted prompting setup for comparability and that "Advanced prompting strategies—such as tree-of-thought prompting—have not been explored."
- **Why unresolved:** The paper restricts its methodology to specific few-shot prompts to ensure fair comparison across models, leaving other strategies untested.
- **What evidence would resolve it:** Experiments comparing the performance of ToT or self-consistency prompting against the current baselines on the VtG and PRM800K datasets.

### Open Question 3
- **Question:** Does the difficulty of error localization increase significantly for large language models when processing non-English or low-resource languages?
- **Basis in paper:** [explicit] The Limitations section acknowledges the study is "limited to English-language math problems" and hypothesizes that "difficulties would be exacerbated in languages with less extensive data representation."
- **Why unresolved:** All experiments were conducted on English-language datasets (VtG and PRM800K).
- **What evidence would resolve it:** Evaluating model performance on translated versions of the datasets in low-resource languages to measure performance degradation.

## Limitations
- **Dataset limitations:** Experiments confined to English-language math problems (GSM8K and MATH datasets), limiting generalizability to other domains and languages.
- **Quality dependency:** Performance heavily depends on the quality of corrected solution generation, which varies significantly across models (e.g., Qwen2.5-72B-Math achieved only 63.3% stylistic similarity).
- **Evaluation scope:** Task measures only first-error-step localization, not subsequent errors or the ability to distinguish primary from secondary mistakes.

## Confidence

- **High confidence:** The core finding that problem-solving ability does not correlate with error detection accuracy (phi < 0.2, p > 0.01). This is directly supported by chi-squared tests and feature importance analysis.
- **Medium confidence:** The claim that step-level alignment is the primary driver of w-Cor improvements. While semantic recall shows highest feature importance (17.9%), the causal mechanism depends on corrected solution quality, which varies significantly by model.
- **Medium confidence:** The error type analysis showing directional prediction biases. The pattern is observable in Figure 4, but the VtG dataset contains only 1,002 examples, and PRM800K lacks error type annotations, limiting generalizability.

## Next Checks

1. **Replicate the corrected solution generation step:** Run the w-Cor pipeline on a small held-out subset using multiple models to measure stylistic similarity and correctness rates. Verify that improvements are not driven solely by one high-performing model.

2. **Error type distribution audit:** Before comparing performance across datasets, compute the error type distribution in your test splits. If misunderstanding errors dominate, expect systematically later predictions; if missing/extra quantity errors dominate, expect earlier predictions.

3. **Ablation on alignment quality:** Manually perturb corrected solutions to break stylistic similarity while preserving semantic correctness. Measure the degradation in error localization accuracy to confirm alignment is the causal mechanism.