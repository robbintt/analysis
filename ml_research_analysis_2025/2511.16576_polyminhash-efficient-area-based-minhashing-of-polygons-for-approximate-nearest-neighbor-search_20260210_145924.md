---
ver: rpa2
title: 'PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest
  Neighbor Search'
arxiv_id: '2511.16576'
source_url: https://arxiv.org/abs/2511.16576
tags:
- polygons
- polygon
- minhash
- polyminhash
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents PolyMinHash, a novel system for approximate
  nearest neighbor search on polygon data. The core innovation is adapting MinHashing
  to 2D polygon geometry by using rejection sampling: for each polygon, random points
  are sampled within a global minimum bounding rectangle until one lands inside the
  polygon, and the number of attempts forms the hash value.'
---

# PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2511.16576
- Source URL: https://arxiv.org/abs/2511.16576
- Reference count: 9
- Primary result: PolyMinHash achieves up to 98% reduction in query refinement candidates while maintaining high recall (0.64-0.97) for approximate nearest neighbor search on polygon datasets.

## Executive Summary
This paper introduces PolyMinHash, a novel system for approximate nearest neighbor search on polygon data using area-based Jaccard similarity. The core innovation adapts MinHashing to 2D geometry through rejection sampling: random points are sampled within a global bounding rectangle until one lands inside the polygon, with the attempt count forming the hash value. This preserves geometric Jaccard similarity between polygons. The system significantly reduces candidates needing expensive exact geometric checks—by up to 98% compared to brute-force—while maintaining high recall. Experiments on real-world polygon datasets demonstrate effective filtering and substantial runtime speedup, enabling scalable similarity search for spatial data in geographic information systems.

## Method Summary
PolyMinHash centers all polygons to (0,0), computes a global Minimum Bounding Rectangle (MBR) enclosing all polygons, and generates m-length MinHash signatures via rejection sampling. For each polygon, the system uniformly samples points from the global MBR, counts attempts until one lands inside the polygon (after local MBR filtering and point-in-polygon testing), and uses fixed RNG seeds for consistency. Polygons are indexed in hashmaps based on their signatures, and queries retrieve candidates from matching buckets before exact Jaccard refinement. The method preserves geometric Jaccard similarity, with collision probability equal to the intersection-over-union ratio.

## Key Results
- Achieves up to 98% reduction in query refinement candidates compared to brute-force approach
- Maintains recall between 0.64-0.97 depending on hash length and dataset characteristics
- Demonstrates effective pruning efficiency across multiple real-world polygon datasets (Cemetery, Sports, Parks, Urban Areas)
- Shows significant runtime speedup while preserving area-based Jaccard similarity

## Why This Works (Mechanism)

### Mechanism 1: Rejection Sampling for Geometric Hashing
The system uses rejection sampling where random points are sampled within a global bounding box until one lands inside a polygon, generating a hash value that correlates with the polygon's relative area. Fixed RNG seeds ensure consistent point sequences across polygons, making attempt counts comparable. This approach maps continuous polygon area to discrete hash values while preserving similarity relationships.

### Mechanism 2: Area-Based Collision Probability
The probability of two polygons generating the same hash value equals their geometric Jaccard similarity. This relies on the mathematical relationship where if the first point to land in either polygon falls within their intersection area, they share the hash value. The uniform distribution of points over the Global MBR partitions the area correctly to maintain this probability relationship.

### Mechanism 3: Filter-and-Refine via Hash Length
Increasing the signature length (m) creates a stricter filter, drastically reducing the candidate set size for expensive exact geometric checks. The system uses m independent hash functions, requiring polygons to match on all values to be candidates. This filter-and-refine approach leverages the computational efficiency of hashing versus exact intersection/union calculations.

## Foundational Learning

- **Concept: MinHashing (Locality Sensitive Hashing)**
  - Why needed here: PolyMinHash adapts standard MinHash to continuous polygon areas rather than discrete sets
  - Quick check question: Can you explain why $Pr[h(A) = h(B)] = J(A,B)$ holds for standard set MinHash, and how PolyMinHash maps "area" to this probability?

- **Concept: Point-in-Polygon (PnP) Algorithms**
  - Why needed here: The efficiency of hashing depends entirely on how quickly the system can check if a random point is inside the polygon
  - Quick check question: What is the computational complexity of a standard ray-casting PnP test relative to the number of vertices in the polygon?

- **Concept: Jaccard Similarity/Index**
  - Why needed here: This is the specific distance metric being preserved, measuring intersection area over union area
  - Quick check question: If Polygon A is completely inside Polygon B, how does the Jaccard similarity behave compared to two polygons that just touch at a corner?

## Architecture Onboarding

- **Component map:** Preprocessor -> Hash Generator -> LSH Index -> Refinement Engine
- **Critical path:** The Rejection Sampling Loop (Lines 6-17, Algorithm 1) where hashing efficiency depends on polygon area relative to Global MBR
- **Design tradeoffs:**
  - Hash Length (m): Increasing m from 1 to 5 improves pruning (70% → 98%) but drops recall significantly (0.97 → 0.60)
  - Global MBR sizing: Tighter MBR improves hitting probability but requires recomputation if new data falls outside existing bounds
- **Failure signatures:**
  - High Latency on Sparse Data: Tiny polygons relative to global space cause excessive rejection sampling loop iterations
  - Recall Collapse: Setting hash length too high (m=5) results in missing true neighbors (Recall drops to ~0.60)
  - Seed Drift: Inconsistent RNG seeds across runs break index consistency
- **First 3 experiments:**
  1. Sparsity Stress Test: Measure hashing time for polygons with varying area-to-MBR ratios to validate Theorem 2
  2. Recall vs. Pruning Curve: Reproduce Figure 4(a) to find optimal operating point for specific data
  3. Refinement Cost Analysis: Benchmark exact Jaccard computation vs. hash generation costs

## Open Questions the Paper Calls Out
The paper explicitly states it does not consider scale and rotational invariance for polygons, leaving open how to extend the method to support these transformations.

## Limitations
- Core assumption that rejection sampling remains cheaper than exact refinement may break down for sparse polygons or complex vertex counts
- LSH indexing scheme details (banding strategy, hashmap structure) are underspecified, creating ambiguity in reproducing query logic
- Performance on other spatial data types (raster-based regions, multi-part polygons) remains untested

## Confidence
- **High Confidence:** Theoretical proof that $Pr[h(P)=h(Q)] = J(P,Q)$ is mathematically sound and clearly presented
- **Medium Confidence:** Practical effectiveness across diverse datasets is demonstrated, but optimal configuration parameters are dataset-specific
- **Low Confidence:** Computational complexity claims regarding hashing vs. refinement are based on implicit assumptions about polygon sparsity

## Next Checks
1. **Sparsity Stress Test:** Measure hashing time for polygons with varying area-to-MBR ratios (Sp) to validate Theorem 2's variance claims
2. **Configuration Sensitivity Analysis:** Systematically vary hash length (m) and number of hashmaps to map the recall-pruning tradeoff curve
3. **Edge Case Robustness:** Test the system with degenerate polygons (zero-area, self-intersecting) and verify point-in-polygon handling