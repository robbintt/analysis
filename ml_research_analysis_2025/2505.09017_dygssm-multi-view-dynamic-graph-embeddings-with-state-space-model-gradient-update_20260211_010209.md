---
ver: rpa2
title: 'DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient
  Update'
arxiv_id: '2505.09017'
source_url: https://arxiv.org/abs/2505.09017
tags:
- graph
- dynamic
- dygssm
- time
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of capturing both local and global
  structural information in dynamic graphs while effectively managing temporal dependencies
  during model updates. Existing methods typically focus on either local or global
  structures and often fail to consider the current snapshot's performance when updating
  parameters.
---

# DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update

## Quick Facts
- arXiv ID: 2505.09017
- Source URL: https://arxiv.org/abs/2505.09017
- Reference count: 40
- Key outcome: DyGSSM outperforms existing methods in 17/20 cases, achieving substantial improvements in accuracy, AUC, MRR, and Recall@10 on five public datasets.

## Executive Summary
This paper addresses the challenge of capturing both local and global structural information in dynamic graphs while effectively managing temporal dependencies during model updates. The proposed DyGSSM method integrates Graph Convolutional Networks (GCN) for local feature extraction and a biased random walk with Gated Recurrent Unit (GRU) for global feature extraction. These features are fused using a lightweight attention mechanism, and a State Space Model (SSM) based on the HiPPO algorithm is employed to update model parameters while considering long-term dependencies. Experiments on five public datasets show that DyGSSM outperforms existing baseline and state-of-the-art methods in 17 out of 20 cases, achieving substantial improvements in accuracy, AUC, MRR, and Recall@10.

## Method Summary
DyGSSM is a dynamic graph embedding method that captures local and global structural information while managing temporal dependencies. The method uses a two-layer GCN to extract local features from immediate neighbors, while a biased random walk captures global positional context. These features are fused via lightweight attention. A State Space Model (SSM) initialized with the HiPPO algorithm updates model parameters by maintaining a compressed history of gradients, allowing the model to preserve long-term dependencies. The update magnitude is dynamically scaled by the reciprocal of the current loss, prioritizing updates during stable learning phases. The model is trained using cross-entropy loss with Adam optimizer for 100 epochs, with early stopping after 10 epochs of no improvement.

## Key Results
- DyGSSM outperforms existing baseline and state-of-the-art methods in 17 out of 20 cases
- Substantial improvements in accuracy, AUC, MRR, and Recall@10 metrics
- Effective at capturing both local and global structural information while managing temporal dependencies

## Why This Works (Mechanism)

### Mechanism 1: Multi-View Structural Decomposition
Separating feature extraction into local message-passing and global random walks allows the model to resolve distinct structural scales before fusion. A GCN aggregates immediate neighbor features (local view), while a biased Random Walk samples distant nodes to capture positional context (global view). These are fused via lightweight attention. The core assumption is that local and global structures encode orthogonal information critical for link prediction, preventing signal dilution found in single-view methods.

### Mechanism 2: SSM-Based Gradient Memory (HiPPO)
Replacing standard meta-learning update rules with a State Space Model (SSM) initialized via HiPPO theory enables the optimizer to maintain a compressed history of gradients, managing long-term dependencies without manual window sizing. The HiPPO matrix projects gradient history into a state vector, theoretically allowing the model to "remember" critical patterns from much earlier snapshots. The core assumption is that the gradient trajectory follows a pattern approximable by polynomial projections.

### Mechanism 3: Loss-Weighted Dynamic Gating
Scaling parameter updates by the reciprocal of the current loss dynamically prioritizes "stable" learning phases and dampens updates during high-error turbulence. The update magnitude is gated by $1/L_t$, where low loss (high confidence) allows more significant SSM state influence on parameters. The core assumption is that low loss correlates with stable, generalizable graph states worthy of being "memorized" into the long-term parameter trajectory.

## Foundational Learning

- **State Space Models (SSMs) & HiPPO Theory**: Why needed: Unlike standard RNNs or Transformers, DyGSSM uses SSMs not just for sequence modeling but for optimization dynamics. You must understand how continuous-time memory is approximated via polynomial projections to grasp why this replaces "window sizes." Quick check: How does the HiPPO matrix differ from a standard recurrent weight matrix in an RNN regarding how it handles long-range history?

- **Dynamic Graph Formalization (Discrete vs. Continuous)**: Why needed: The paper claims applicability to both snapshot-based (discrete) and event-stream (continuous) graphs. Understanding how a "snapshot" is constructed or how a continuous event triggers an update is vital for data preprocessing. Quick check: Does the DyGSSM architecture require aggregating continuous events into discrete batches for the GCN, or can it process single edges event-by-event?

- **Random Walk Sampling Strategies**: Why needed: The "Global View" relies entirely on the quality of the random walk. Understanding biased vs. uniform walks is necessary to tune the "global" receptive field. Quick check: In DyGSSM's global view, how does the "biased" random walk differ from a standard DeepWalk, and what structural property is it trying to exaggerate?

## Architecture Onboarding

- **Component map**: Input Graph Snapshot $G_t$ -> Local Encoder (2-Layer GCN) -> Global Encoder (Biased Random Walk + Conv1D) -> Fusion (Lightweight Cross-Attention) -> Optimizer (SSM State + Current Gradient) -> Parameter Update for $t+1$

- **Critical path**: The flow is circular: Data -> Model -> Loss -> SSM State Update -> Model Weights Update. The SSM state is the "memory" of the optimization process itself.

- **Design tradeoffs**: Stability vs. Adaptability (SSM provides long memory but may lag on sudden shifts); Cache Efficiency vs. Freshness (RW caching speeds inference but risks stale views if topology changes faster than cache invalidation).

- **Failure signatures**: Attention Collapse (if attention heavily favors only one view, check RW sampling depth or global encoder learning rate); Gradient Vanishing in SSM (if performance degrades on very long sequences, check eigenvalues of the $\hat{K}$ matrix).

- **First 3 experiments**: 1) Ablate the Optimizer: Swap SSM-based update for standard Adam or SGD to measure HiPPO gradient mechanism contribution on DBLP. 2) Cache Sensitivity: Vary RW caching threshold to find trade-off between throughput and accuracy drop on Bitcoin-OTC. 3) View Importance: Monitor attention weights over time to validate if model shifts from local to global dependencies as graph matures.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can DyGSSM maintain its performance and parameter efficiency when extended to downstream tasks such as node classification? The Conclusion states plans to extend DyGSSM to node classification to validate its representational power on real-world dynamic graphs.

- **Open Question 2**: Can the biased Random Walk component be replaced by learned graph embeddings without sacrificing the model's ability to capture global structure? The Conclusion notes plans to explore replacing the RW process with learned graph embeddings.

- **Open Question 3**: Can parallelized mini-batch sampling and graph partitioning successfully resolve the scalability limitations of the Random Walk component on extremely large dense networks? The Conclusion states aims to optimize the RW component through parallelized mini-batch sampling, graph partitioning, and subgraph-based RW.

## Limitations

- Weak empirical grounding for HiPPO-SSM: The specific application to gradient update memory lacks direct corpus validation and ablation studies isolating the SSM contribution.
- Hyperparameter transparency: Critical architectural details (learning rate, dimensions, kernel sizes, bias parameters) are unspecified, limiting exact reproduction.
- Dataset generalization: All experiments use publicly available datasets; robustness to noisy, incomplete, or adversarial graphs is untested.

## Confidence

- **High confidence**: The multi-view structural decomposition (local GCN + global RW + attention fusion) is well-supported by the text and aligns with established multi-view learning principles.
- **Medium confidence**: The HiPPO-SSM gradient update mechanism is theoretically sound but lacks ablation evidence; performance gains may partially derive from other factors.
- **Low confidence**: The loss-weighted dynamic gating's effectiveness is asserted but not empirically isolated; reciprocal loss scaling could have unintended consequences in high-variance loss regimes.

## Next Checks

1. **Ablate the SSM Optimizer**: Replace the HiPPO-SSM gradient update with standard Adam or SGD on a dataset with long-term dependencies (e.g., DBLP) to measure the specific contribution of the SSM mechanism to performance gains.

2. **Random Walk Sensitivity**: Systematically vary random walk parameters (number of walks, path length, bias weights) and caching thresholds to quantify their impact on both accuracy and computational efficiency, especially on high-churn datasets like Bitcoin-OTC.

3. **Attention Dynamics Monitoring**: Track attention weight distributions over training epochs to verify whether the model adaptively shifts from local to global dependencies as the graph evolves, validating the multi-view hypothesis.