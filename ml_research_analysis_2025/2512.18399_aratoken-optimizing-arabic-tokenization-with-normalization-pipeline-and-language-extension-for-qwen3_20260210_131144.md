---
ver: rpa2
title: 'AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language
  Extension for Qwen3'
arxiv_id: '2512.18399'
source_url: https://arxiv.org/abs/2512.18399
tags:
- arabic
- normalization
- language
- alif
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AraToken optimizes Arabic tokenization by applying a normalization
  pipeline that unifies orthographic variations and removes optional diacritics, then
  training a SentencePiece Unigram tokenizer on Arabic corpora. Compared to unnormalized
  baselines, it achieves 18% lower fertility (1.199 vs 1.35 tokens/word) and higher
  compression efficiency.
---

# AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3

## Quick Facts
- arXiv ID: 2512.18399
- Source URL: https://arxiv.org/abs/2512.18399
- Reference count: 19
- AraToken achieves 18% lower tokenization fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines

## Executive Summary
AraToken introduces a comprehensive approach to Arabic tokenization optimization that combines language-specific normalization with efficient LLM adaptation. The method applies a normalization pipeline that unifies orthographic variations and removes optional diacritics, then trains a SentencePiece Unigram tokenizer on Arabic corpora. The authors demonstrate 18% lower tokenization fertility compared to unnormalized baselines. To integrate the tokenizer into existing models, they propose a Language Extension Pipeline (LEP) that extends Qwen3-0.6B's vocabulary with Arabic tokens, initializes embeddings using mean subtoken averaging, and selectively unfreezes transformer layers. LEP achieves rapid adaptation, reducing evaluation loss from 8.28 to 2.43 within 800 training steps using only 100K Arabic samples.

## Method Summary
The method consists of two main components: a normalization pipeline and vocabulary extension for LLMs. The normalization pipeline applies NFKC Unicode decomposition, unifies four Alif variants to bare Alif (or preserves them as Alif4), maps Arabic-Indic numerals to Western equivalents, removes Tatweel, and optionally drops diacritics. A SentencePiece Unigram tokenizer is trained on Arabic corpora with vocabulary size 150K, then pruned to 76K tokens for 99% coverage. For integration with Qwen3-0.6B, the Language Extension Pipeline extracts Arabic tokens not in the original vocabulary, extends it via HuggingFace's merge_vocabulary, and initializes new token embeddings as the mean of their constituent subtoken embeddings. Original model embeddings are frozen while the last 4 of 28 transformer layers are unfrozen for fine-tuning.

## Key Results
- 18% lower tokenization fertility: 1.199 vs 1.35 tokens/word compared to unnormalized baselines
- SentencePiece Unigram outperforms BPE and WordPiece on all Arabic-specific metrics
- LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples
- Preserving Alif variants (Alif4) achieves lower language modeling loss (2.43) than unified Alif (3.03)
- 100K Arabic samples represent less than 0.01% of typical pretraining data for adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalization reduces token fertility by collapsing orthographic variants into unified representations.
- Mechanism: The normalization pipeline applies NFKC Unicode decomposition, unifies four Alif variants to bare Alif, maps Arabic-Indic numerals to Western equivalents, removes Tatweel, and optionally drops diacritics. This reduces the effective character set the tokenizer must learn, enabling SentencePiece's Unigram algorithm to find more globally optimal segmentations across a smaller search space.
- Core assumption: Arabic orthographic variants (e.g., Hamza-above vs. Hamza-below) are semantically redundant for most downstream tasks and can be unified without information loss.
- Evidence anchors:
  - [abstract] "demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines"
  - [section 5.1, Table 3] sp_drop_norm achieves fertility 1.199 vs sp_drop at 1.311; normalization consistently reduces fertility by 8-9% across all algorithms
  - [corpus] Related work on morphologically-aware tokenization (MorphBPE, BengaliBPE) confirms that language-specific preprocessing improves subword segmentation for complex scripts, though corpus lacks direct Arabic normalization comparisons.
- Break condition: If orthographic distinctions carry grammatical or semantic information critical to the task (e.g., Quranic text where Hamza placement indicates case), aggressive normalization degrades performance. The paper's Alif4 ablation (Section 6.2) actually supports this—preserving variants achieved lower loss (2.43) than unified Alif (3.03).

### Mechanism 2
- Claim: Mean subtoken initialization anchors new Arabic token embeddings in semantically meaningful regions of the embedding space.
- Mechanism: When adding new Arabic tokens to Qwen3's vocabulary, each token's embedding is initialized as the mean of its constituent subtoken embeddings from the original tokenizer (Equation 1: e_new = 1/|S| × Σe_i). This positions new embeddings near the centroid of their character-level or subword-level components, providing a reasonable starting point for gradient descent.
- Core assumption: The original tokenizer's subtoken embeddings encode semantic information that composes meaningfully—i.e., the embedding for "كتاب" (book) should be near the average of its constituent byte-level tokens.
- Evidence anchors:
  - [section 3.3] "This initialization provides a semantically meaningful starting point, as the new token's embedding is positioned near the centroid of its subtokens in embedding space"
  - [section 5.3, Figure 7] Rapid convergence from loss 8.28 to 2.43 within 800 steps suggests effective initialization; ablation shows baseline (no adaptation) stuck at 8.28
  - [corpus] WECHSEL and FOCUS (cited in Section 2.3) demonstrate cross-lingual embedding initialization benefits, though specific mean-subtoken initialization lacks direct external validation
- Break condition: If new tokens have no compositional relationship to existing subtokens (e.g., entirely novel morphological patterns), mean initialization provides little benefit over random initialization.

### Mechanism 3
- Claim: Selective layer unfreezing enables adaptation to new tokenization while preserving existing language knowledge.
- Mechanism: Gradient masking freezes original Qwen3 token embeddings (zeroing gradients for indices below vocabulary extension threshold). Only the last 4 of 28 transformer layers (layers 24-27) are unfrozen. This constrains adaptation to higher-level representations while protecting lower-level linguistic features learned during pretraining.
- Core assumption: Higher transformer layers encode more task-specific and language-specific representations, while lower layers capture more universal linguistic patterns worth preserving.
- Evidence anchors:
  - [section 3.3, Equation 2] Gradient masking formula explicitly zeroes gradients for original vocabulary
  - [section 5.3, Table 5] "freeze all" configuration achieves loss 4.02 vs. 2.43 with last 4 layers unfrozen—51% vs 71% reduction from baseline
  - [corpus] BLOOM+1 and LLaMA Beyond English (cited) find adapters and selective training effective for language adaptation; corpus supports this pattern across languages
- Break condition: If the target language has fundamentally different morphological or syntactic structure requiring low-level representation changes, freezing lower layers prevents necessary adaptation.

## Foundational Learning

- Concept: **Subword Tokenization Algorithms (BPE vs. Unigram vs. WordPiece)**
  - Why needed here: The paper systematically compares these algorithms and explains why SentencePiece Unigram outperforms BPE/WordPiece for Arabic morphology. Understanding the difference is essential to interpret results.
  - Quick check question: BPE greedily merges the most frequent character pairs iteratively. How does the Unigram algorithm's probabilistic approach differ, and why might it better handle Arabic's multiple valid segmentations?

- Concept: **Tokenization Metrics (Fertility, Compression Ratio, OOV Rate)**
  - Why needed here: These are the intrinsic evaluation metrics used throughout Section 5.1. Practitioners must understand what "fertility 1.199" means operationally.
  - Quick check question: If a tokenizer has fertility 1.35 tokens/word and compression ratio 4.47 chars/token, how many tokens would a 10,000-character Arabic document with 2,000 words produce?

- Concept: **Catastrophic Forgetting and Transfer Learning**
  - Why needed here: LEP's design (gradient masking, selective unfreezing) directly addresses catastrophic forgetting. Without this context, the architectural choices seem arbitrary.
  - Quick check question: If you fine-tune Qwen3-0.6B on Arabic data without freezing any parameters, what behavior would you expect on original English tasks, and why?

## Architecture Onboarding

- Component map:
```
Input Arabic Text
       ↓
[NFKC Unicode Normalization]
       ↓
[Arabic Normalization Pipeline]
├── Alif variant unification (configurable: Alif4 vs unified)
├── Arabic-Indic numerals → Western numerals
├── Tatweel removal
└── Diacritics handling (drop/keep)
       ↓
[SentencePiece Unigram Tokenizer] (vocab: 150K → pruned to 76K)
       ↓
[Language Extension Pipeline]
├── Vocabulary Extension: Extract Arabic tokens not in Qwen3
├── Mean Subtoken Initialization: e_new = mean(subtoken embeddings)
├── Gradient Masking: Freeze original Qwen3 embeddings
└── Selective Unfreezing: Unfreeze layers 24-27 only
       ↓
[Qwen3-0.6B-Base with Extended Vocabulary]
       ↓
[Fine-tuning on Arabic corpus: 100K samples, 800 steps]
```

- Critical path:
  1. **Corpus preparation**: Combine Arabic Wikipedia (~1.1M articles) + ArabicText-Large (~120M tokens total)
  2. **Normalization configuration**: Choose Alif4 (preserve variants) or unified based on task requirements—Alif4 achieved lower loss (2.43 vs 3.03)
  3. **Tokenizer training**: Train SentencePiece Unigram with vocab 150K, apply frequency-based pruning to 76K (99% coverage)
  4. **Vocabulary merge**: Filter Arabic-only tokens (exclude Latin/digits/Cyrillic), add to Qwen3 vocabulary with lstrip=True
  5. **Embedding initialization**: Apply mean subtoken initialization to new tokens
  6. **Training configuration**: LR=2e-4, warmup=10%, batch=16, grad_accum=6, max_steps=800, seq_len=256

- Design tradeoffs:
  - **Normalization aggressiveness**: Alif4 preservation (loss 2.43) vs. unified Alif (loss 3.03)—orthographic fidelity vs. compression efficiency
  - **Diacritics handling**: Drop for maximum compression (fertility 1.199) vs. keep for phonetic/linguistic tasks (fertility 1.218)
  - **Vocabulary pruning**: 76K (99% coverage, fertility 1.229) vs. 42K (95% coverage, fertility 1.293)—size vs. quality
  - **Unfrozen layers**: More layers = more adaptation capacity but higher forgetting risk; paper uses 4 layers as sweet spot

- Failure signatures:
  - **OOV rate > 1%**: Normalization not applied correctly or vocabulary too aggressively pruned
  - **Loss plateaus above 5.0**: Learning rate too low (try 2e-4) or too many frozen layers
  - **Loss explodes**: Original embeddings not frozen—verify gradient masking implementation
  - **Good loss, poor downstream performance**: Over-normalization removed task-critical orthographic information
  - **Training instability**: Check that new token count is reasonable (<10% vocabulary extension typical)

- First 3 experiments:
  1. **Tokenizer algorithm comparison**: Train BPE, WordPiece, and SentencePiece Unigram on identical normalized Arabic corpus; measure fertility, compression ratio, OOV rate to validate SentencePiece advantage (replicates Table 3)
  2. **Normalization ablation**: Compare unified Alif vs. Alif4 preservation vs. no normalization on downstream language modeling loss (replicates Section 6.2 finding)
  3. **LEP configuration validation**: Train with (a) all layers frozen except embeddings, (b) last 4 layers unfrozen, (c) all layers unfrozen; measure evaluation loss and English task degradation to validate selective unfreezing strategy (replicates Table 5)

## Open Questions the Paper Calls Out

- Question: Does the Language Extension Pipeline (LEP) maintain its efficiency when scaling to larger model sizes (e.g., Qwen3-1.7B, 7B)?
  - Basis in paper: [explicit] "scaling to larger models requires validation" listed as a limitation; extending to larger models listed as future work.
  - Why unresolved: All experiments used only Qwen3-0.6B with 28 layers; unfreezing 4 layers represents ~14% of parameters, but this proportion may not transfer linearly to deeper architectures.
  - What evidence would resolve it: LEP training curves and final evaluation loss on Qwen3-1.7B and 7B variants using identical configurations.

- Question: Does improved tokenization fertility translate to measurable gains on downstream Arabic NLP benchmarks (ALUE, ArabicGLUE)?
  - Basis in paper: [explicit] Authors state "We do not evaluate on downstream tasks such as question answering or summarization" as a limitation.
  - Why unresolved: Only intrinsic metrics (fertility, compression ratio) and language modeling loss were reported; downstream task performance remains unverified.
  - What evidence would resolve it: Benchmark results comparing AraToken+LEP adapted models against baselines on tasks like sentiment analysis, NER, and QA.

- Question: Why does preserving Alif variants (Alif4) yield lower language modeling loss than aggressive unification, despite increased vocabulary complexity?
  - Basis in paper: [explicit] "preserving Alif variants (Alif4) leads to lower language modeling loss... Alif variants carry disambiguating information that aids language modeling."
  - Why unresolved: The paper reports the empirical finding but provides only speculative explanation about grammatical case and word origin signals.
  - What evidence would resolve it: Controlled ablations isolating specific Alif variants, combined with linguistic analysis of attention patterns to orthographic distinctions.

## Limitations
- Focus on Modern Standard Arabic only, excluding dialectal Arabic varieties
- Evaluation limited to intrinsic metrics and language modeling loss, without downstream task validation
- All experiments conducted on Qwen3-0.6B model; scaling to larger models requires validation
- Dataset limitations: ArabicText-Large dataset not publicly available with specified URL

## Confidence
- High: 18% lower fertility is clearly stated in abstract and supported by Table 3 data
- Medium: LEP adaptation efficiency supported by single experiment but needs scaling validation
- Low: Claims about orthographic information preservation need downstream task validation

## Next Checks
1. Replicate Table 3 tokenizer comparison using publicly available Arabic corpora to verify SentencePiece Unigram advantage
2. Implement LEP on a different Arabic model (e.g., AraBART) to validate cross-model generalization
3. Run ablation study on Alif variant preservation vs. unification on a specific downstream task (e.g., sentiment analysis) to measure practical impact