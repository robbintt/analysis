---
ver: rpa2
title: A Universal Approach to Feature Representation in Dynamic Task Assignment Problems
arxiv_id: '2507.03579'
source_url: https://arxiv.org/abs/2507.03579
tags:
- assignment
- problems
- graph
- problem
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of representing and solving
  dynamic task assignment problems with potentially infinite state and action spaces,
  a limitation of current Deep Reinforcement Learning (DRL) approaches. The core contribution
  is a graph-based feature representation called assignment graphs, which map complex
  assignment problems from marked Colored Petri Nets to graph structures that can
  be handled by Graph Neural Networks.
---

# A Universal Approach to Feature Representation in Dynamic Task Assignment Problems

## Quick Facts
- **arXiv ID**: 2507.03579
- **Source URL**: https://arxiv.org/abs/2507.03579
- **Reference count**: 24
- **Primary result**: Graph-based DRL method handles infinite state/action spaces in dynamic task assignment, outperforming vector-based approaches.

## Executive Summary
This paper addresses the challenge of representing and solving dynamic task assignment problems with potentially infinite state and action spaces, a limitation of current Deep Reinforcement Learning (DRL) approaches. The core contribution is a graph-based feature representation called assignment graphs, which map complex assignment problems from marked Colored Petri Nets to graph structures that can be handled by Graph Neural Networks. The method includes a translation algorithm from Petri Nets to assignment graphs and an adaptation of the Proximal Policy Optimization (PPO) algorithm to learn policies on these dynamic graphs. Evaluation on three archetypal problems—ranging from finite to infinite state/action spaces—demonstrates that the proposed method successfully learns close-to-optimal policies across all cases, outperforming vector-based DRL approaches and matching heuristic baselines, even in scenarios with unbounded feature values.

## Method Summary
The method translates dynamic task assignment problems modeled as Attributed Action-Evolution Petri Nets (A-E PN) into assignment graphs through an expansion algorithm and a mapping function. The expansion algorithm transforms the A-E PN into an equivalent net where each place contains at most one token, creating a dynamic graph where each node represents either a single token (with its attributes) or a transition. This assignment graph is processed by a Heterogeneous Graph Attention Network (HANConv) to produce fixed-size embeddings, which a downstream policy network uses to generate action probabilities. The Proximal Policy Optimization (PPO) algorithm is adapted to handle these dynamic graphs with variable numbers of actions by using a diagonal adjacency matrix concatenation for batched observations and an indexed softmax to ensure correct action probability assignment despite the changing action space size.

## Key Results
- The graph-based approach successfully learns close-to-optimal policies across all three problem types (finite small, finite large, and infinite state/action spaces).
- On finite large state space problems, the graph-based PPO matches the performance of greedy heuristics while standard vector-based PPO struggles to converge.
- The method handles unbounded feature values in the infinite state space problem, demonstrating generalization to unseen budget distributions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The assignment graph representation eliminates the need for fixed-size input/output vectors, enabling DRL to handle infinite state and action spaces.
- **Mechanism**: The expansion algorithm transforms a marked Colored Petri Net (where places can hold multiple tokens) into an equivalent net where each place contains at most one token. This expanded net is then mapped to a graph where each node represents either a single token (with its attributes) or a transition. Action selection becomes a node selection problem on this dynamic graph. The Heterogeneous Graph Attention Network (HANConv) processes this variable-sized graph to produce fixed-size embeddings, which a downstream policy network uses to generate action probabilities.
- **Core assumption**: The expansion process preserves the essential decision structure of the original problem, and the HANConv layers can effectively learn meaningful representations from the dynamically changing graph topology.
- **Evidence anchors**: [abstract] "provides... A graph-based feature representation of assignment problems, which we call assignment graph"; [section 4, Definition 3 & 4] Defines assignment graphs and the expansion function; [corpus] Corpus signals mention "dynamic" and "assignment" in other contexts, but no direct evidence for this specific graph-based mechanism.

### Mechanism 2
- **Claim**: By separating the modeling (Action-Evolution Petri Nets) from the learning (PPO on graphs), the method allows domain experts to model problems using a familiar formalism without performing manual feature engineering.
- **Mechanism**: The problem is first modeled using an Attributed Action-Evolution Petri Net (A-E PN). The A-E PN distinguishes between action transitions (agent decisions) and evolution transitions (environment events). An automated translation pipeline (expansion + mapping) converts the marked A-E PN into an assignment graph, which is then fed to the PPO algorithm. This decoupling isolates the modeling complexity from the learning complexity.
- **Core assumption**: The Attributed A-E PN formalism is sufficiently expressive to represent a wide range of dynamic task assignment problems, and the translation pipeline is lossless or near-lossless.
- **Evidence anchors**: [abstract] "A mapping from marked Colored Petri Nets to assignment graphs"; [section 3, Definition 1 & 2] Defines A-E PN and Attributed A-E PN; [corpus] No strong corpus evidence for this specific decoupling mechanism in similar contexts.

### Mechanism 3
- **Claim**: The adapted PPO algorithm can learn near-optimal policies on dynamic graphs with variable numbers of actions by leveraging permutation-invariant GNN layers and indexed softmax.
- **Mechanism**: The PPO implementation is modified to handle batched graph observations (using a diagonal adjacency matrix concatenation) and a variable number of actions per step. The HANConv layer provides permutation invariance, and an indexed softmax ensures correct action probability assignment despite the changing action space size. The policy network decodes embeddings for action nodes individually via an MLP, while the value network aggregates all node embeddings before decoding.
- **Core assumption**: The indexed softmax and HANConv layer correctly handle the dynamic nature of the action space, and the standard PPO optimization steps remain effective with this modified architecture.
- **Evidence anchors**: [abstract] "adaptation of the Proximal Policy Optimization (PPO) algorithm"; [section 5, Fig. 6 & text] Describes the policy/value network architecture and handling of variable actions; [corpus] Related work in dynamic MDPs (NVMDP) addresses non-stationarity, but not specifically the variable action space mechanism.

## Foundational Learning

- **Concept: Colored Petri Nets (CPN) and Action-Evolution Petri Nets (A-E PN)**
  - **Why needed here**: This is the modeling language used to define the problem. Understanding the distinction between action and evolution transitions is crucial for defining the agent's decision points.
  - **Quick check question**: Can you identify the action and evolution transitions in a simple A-E PN diagram?

- **Concept: Graph Neural Networks (GNNs), specifically Heterogeneous Graph Attention Networks (HAN)**
  - **Why needed here**: GNNs are the core component that enables learning on variable-sized graph structures. The HANConv layer is specifically used to handle the heterogeneous node types in assignment graphs.
  - **Quick check question**: How does a HAN layer aggregate information from neighbors of different types?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here**: PPO is the reinforcement learning algorithm used to train the policy. Understanding its core mechanics (advantage estimation, policy updates) is necessary to understand the adaptations made for graph-based observations.
  - **Quick check question**: What is the role of the value network in the PPO algorithm?

## Architecture Onboarding

- **Component map**: Attributed Action-Evolution Petri Net (A-E PN) -> Expansion Algorithm -> Mapping Algorithm -> Assignment Graph -> HANConv Encoder -> Policy Network Decoder (MLP + Softmax) -> Value Network Decoder (Aggregation + MLP) -> Adapted PPO -> Optimized Policy

- **Critical path**: The correctness of the Translation Layer is critical. If the expansion or mapping algorithms fail to correctly represent the A-E PN state, the GNN will learn from flawed data.

- **Design tradeoffs**:
  - **Expressiveness vs. Complexity**: The Attributed A-E PN is expressive but may lead to large graphs after expansion.
  - **Computational Cost**: The expansion step adds computational overhead.
  - **Generalization**: Using a GNN promotes permutation invariance but may lose some global structural information compared to a hand-engineered global feature vector (though the paper argues against the latter's feasibility).

- **Failure signatures**:
  - **Poor performance on large-scale problems**: The paper notes limited exploration of scalability. Large graphs may lead to memory issues or slow training.
  - **Instability during training**: The dynamic action space and use of indexed softmax could introduce instabilities not seen in standard PPO.
  - **Suboptimal policies**: If the A-E PN model omits critical state features, the learned policy will be suboptimal.

- **First 3 experiments**:
  1. **Reproduce the finite, small state space problem (Fig. 7)**: Train PPO Vector and PPO Graph. Verify that both converge to the optimal reward (2000).
  2. **Run the finite, large state space problem (Fig. 8)**: Train PPO Vector and PPO Graph. Confirm that PPO Graph matches the greedy baseline while PPO Vector struggles.
  3. **Implement the infinite state space problem (Fig. 9)**: Since PPO Vector is inapplicable, focus on PPO Graph. Vary the stochastic parameters (e.g., interarrival time, budget variance) and observe policy robustness against the greedy baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed method scale to large, real-world industrial scenarios where the number of tokens and potential bindings creates prohibitively large assignment graphs?
- **Basis in paper**: [explicit] The authors state that this work focused on "small-scale use cases" and that applying the method to "real-world problems" is a direction for future work.
- **Why unresolved**: The expansion algorithm creates a graph node for every token and transition binding, which poses a risk of combinatorial explosion in high-density environments not tested in the evaluation.
- **What evidence would resolve it**: Benchmarking results on complex, high-volume business processes showing that the method converges and maintains tractable inference times.

### Open Question 2
- **Question**: What is the computational overhead of the dynamic expansion and mapping process during online inference compared to static vector representations?
- **Basis in paper**: [inferred] The method relies on "Expansion" (Algorithm 1) and "Mapping" (Algorithm 2), which must be executed repeatedly to convert the marked Petri Net state into an assignment graph for the neural network.
- **Why unresolved**: The paper evaluates the quality of the learned policies (reward) but does not analyze the latency or computational cost of the translation layer itself.
- **What evidence would resolve it**: A comparative runtime analysis measuring the wall-clock time of the graph generation phase against standard vector observation generation.

### Open Question 3
- **Question**: Can a policy trained on assignment graphs generalize effectively to structural changes in the process, such as the introduction of new resource types or task attributes?
- **Basis in paper**: [inferred] The mapping function (Definition 5) creates node types based on the specific places in the Petri Net, suggesting the network architecture may be tied to the specific process topology used during training.
- **Why unresolved**: While GNNs handle varying graph sizes, the reliance on fixed node types ($Y$) in the HANConv layers implies that structural changes to the process model might require retraining or architectural adjustments.
- **What evidence would resolve it**: Experiments testing a trained model on modified Petri Nets containing previously unseen places or transition types to see if the policy degrades gracefully.

## Limitations
- The scalability of the expansion algorithm to large-scale industrial problems remains unproven, with the paper focusing on small-scale use cases.
- Critical PPO hyperparameters (learning rate, clip ratio, GAE lambda, etc.) are omitted, creating uncertainty about reproducibility and potential performance variance.
- The computational overhead of the expansion step for problems with frequent state changes could limit practical applicability in real-time scenarios.

## Confidence
- **High Confidence**: The core claim that graph-based representation enables DRL to handle infinite state/action spaces is well-supported by the empirical results across all three problem types, with clear performance advantages over vector-based approaches in the large and infinite state space cases.
- **Medium Confidence**: The claim that the Attributed A-E PN formalism is sufficiently expressive for a wide range of dynamic task assignment problems is reasonable given the successful demonstrations, but the paper doesn't explore edge cases or more complex dependencies that might challenge this expressiveness.
- **Medium Confidence**: The effectiveness of the adapted PPO algorithm on dynamic graphs is demonstrated, but the lack of detailed hyperparameter specifications and the potential for training instability in more complex scenarios warrant caution.

## Next Checks
1. **Scalability Test**: Implement a problem with a significantly larger state space (e.g., more resource types, longer time horizons) to evaluate the expansion algorithm's scalability and the GNN's ability to process larger graphs efficiently.
2. **Hyperparameter Sensitivity Analysis**: Systematically vary key PPO hyperparameters (learning rate, clip ratio, GAE lambda) to identify optimal settings and assess the method's robustness to hyperparameter choices.
3. **Generalization Evaluation**: Test the trained policy on problems with budget distributions or arrival patterns that differ from the training distribution to assess the agent's ability to generalize to unseen scenarios.