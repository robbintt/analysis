---
ver: rpa2
title: 'MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language
  Inference'
arxiv_id: '2510.24295'
source_url: https://arxiv.org/abs/2510.24295
tags:
- variants
- label
- scores
- problems
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERGE automatically generates minimal variants of NLI problems
  by replacing open-class words between premise and hypothesis with contextually probable
  alternatives from masked language models. These replacements preserve reasoning,
  syntax, and word overlap, ensuring high-quality, plausible variants without manual
  intervention.
---

# MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference

## Quick Facts
- arXiv ID: 2510.24295
- Source URL: https://arxiv.org/abs/2510.24295
- Authors: Mădălina Zgreabăn; Tejaswini Deoskar; Lasha Abzianidze
- Reference count: 29
- Primary result: MERGE automatically generates minimal variants of NLI problems, revealing that NLI models show 4-20% worse performance on variants, with significant drops beyond 80% accuracy thresholds.

## Executive Summary
MERGE is a methodology for automatically generating minimal variants of Natural Language Inference (NLI) problems by replacing open-class words between premise and hypothesis with contextually probable alternatives from masked language models. The approach preserves reasoning, syntax, and word overlap while requiring replacements to be plausible in both premise and hypothesis contexts. Evaluation on SNLI shows that NLI models perform significantly worse on these minimally altered variants, with performance drops becoming substantial beyond 80% accuracy thresholds. The method reveals that models struggle to generalize even to small perturbations in lexical items, particularly with noun replacements proving more challenging than verbs.

## Method Summary
MERGE generates variants by identifying shared open-class words (nouns, verbs, adjectives, adverbs) between premise and hypothesis pairs in NLI problems. For each occurrence of a shared word, masked language models (BERT, RoBERTa, ALBERT, Electra) generate candidate replacements, which are filtered for same POS, higher probability than original, and not already present in the sentences. The key innovation is intersecting suggestions across all occurrences in both sentences to ensure replacements are contextually plausible in both contexts. Seeds with at least 20 total variants are retained, and 20 variants are randomly subsampled per seed. Model performance is evaluated using both sample accuracy (individual variants) and pattern accuracy (requiring a threshold percentage of variants correct per seed problem).

## Key Results
- NLI models show 4-20% worse performance on MERGE variants compared to original problems
- Significant performance drops occur beyond 80% accuracy thresholds when using pattern accuracy evaluation
- Noun replacements are more challenging for models than verb replacements
- Variant diversity (number of unique variants) has a stronger effect on model scores than filtering criteria like probability or plausibility
- Using variants generated by corresponding MLMs does not improve model performance

## Why This Works (Mechanism)

### Mechanism 1: Cross-Context Intersection Validation
- **Claim:** Requiring replacement words to be plausible in BOTH premise and hypothesis contexts preserves the original reasoning relationship.
- **Mechanism:** For each shared open-class word w, MLMs generate candidate replacements. These candidates are intersected across all occurrences in P and H using: W_M(⟨P, H⟩, w^c_>) = ∩_{S∈{P,H}} W_M(S, w^c_>). Only words surviving this intersection become valid replacements.
- **Core assumption:** If a word fits both contexts with equal or higher probability than the original, the entailment label is preserved.
- **Evidence anchors:**
  - [abstract]: "replacements preserve reasoning, syntax, and word overlap, ensuring high-quality, plausible variants without manual intervention"
  - [section 5.4]: "suggestions being equally plausible in both the premise and the hypothesis seems to be the most important [filtering criterion]"
  - [corpus]: Limited corpus support; related NLI robustness work focuses on paraphrasing rather than intersection-based validation
- **Break condition:** When no suggestions survive the intersection (empty candidate set), that seed problem is excluded.

### Mechanism 2: Pattern Accuracy Thresholding
- **Claim:** Evaluating consistency across variant groups (rather than individual accuracy) exposes generalization failures that standard metrics miss.
- **Mechanism:** A seed problem is marked "correct" only if ≥X% of its variants are correctly classified. Models must achieve this threshold across ALL seed problems, not just on average.
- **Core assumption:** True generalization requires consistent reasoning across minimally altered versions of the same problem.
- **Evidence anchors:**
  - [abstract]: "significant drops beyond 80% accuracy thresholds"
  - [section 4, Evaluation Metrics]: "a problem ⟨P, H⟩ is correct only if at least an x threshold amount of its variants ⟨P_ik, H_ik⟩ are as well"
  - [corpus]: Abzianidze et al. (2023) cited as prior work using similar group-based evaluation
- **Break condition:** When threshold is set too low (<60%), performance matches standard accuracy and masks generalization gaps.

### Mechanism 3: Variant Diversity Over Filtering Strictness
- **Claim:** The number of unique variants affects model scores more than filtering criteria like probability thresholds.
- **Mechanism:** Datasets with more unique variants (via looser filtering or union instead of intersection) create lower model scores, suggesting lexical diversity—not plausibility—is the harder challenge.
- **Core assumption:** Models overfit to specific lexical items seen during training; diversity exposes this overfitting.
- **Evidence anchors:**
  - [section 5.4]: "the number of unique variants in a variant dataset affects models' scores more than other quality control criteria"
  - [section 5.4, Figure 8]: P∪H, Pos, and None datasets show lowest scores despite having the most variants (≈380k unique each)
  - [corpus]: No direct corpus support for this specific diversity finding
- **Break condition:** When scrambling letters (Scr condition) yields highest scores, it suggests token frequency—not diversity—may also play a role.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: This is the core task being stress-tested; you must understand entailment/contradiction/neutral relationships to interpret variant quality.
  - Quick check question: Given "A dog runs" (P) and "An animal moves" (H), what is the correct label and why?

- **Concept: Masked Language Model Token Prediction**
  - Why needed here: MERGE relies on MLM probability rankings to select replacements; understanding how context influences predictions is essential.
  - Quick check question: Why might BERT suggest different replacements for "bank" in "river bank" vs. "bank account"?

- **Concept: Distribution Shift vs. Minimal Perturbation**
  - Why needed here: MERGE specifically avoids distribution shift by preserving syntax/overlap; understanding this distinction clarifies what generalization failure is being measured.
  - Quick check question: Why is replacing "girl" with "boy" a different kind of test than replacing "girl" with "the aforementioned entity"?

## Architecture Onboarding

- **Component map:**
  SNLI Test Problems → Shared Word Extractor → MLM Ensemble (BERT, RoBERTa, ALBERT, ELECTRA) → Per-Occurrence Suggestions ← POS/Probability Filter → Cross-Context Intersector (P ∩ H validation) → Variant Generator (word substitution) → Pattern Accuracy Evaluator (threshold-based scoring)

- **Critical path:**
  1. Identify open-class words appearing in BOTH P and H
  2. For each word occurrence, get 200 MLM suggestions
  3. Filter: same POS, higher probability than original, not already in P/H
  4. Intersect suggestions across ALL occurrences in both sentences
  5. Union suggestions across multiple MLMs
  6. Keep seed problems with ≥20 variants
  7. Randomly subsample 20 variants per seed
  8. Evaluate with Pattern Accuracy at 90% threshold

- **Design tradeoffs:**
  - **Intersection vs. Union filtering:** Intersection (P∩H) yields fewer but higher-quality variants; Union (P∪H) yields more variants but lower model scores (Section 5.4)
  - **Number of MLMs:** More MLMs = more unique suggestions (Figure 9), but BART was excluded for low quality (Section 4)
  - **Accuracy threshold:** 90% threshold recommended; lower thresholds (60%) match standard accuracy and hide generalization gaps

- **Failure signatures:**
  - **Empty variant sets:** Too-strict filtering excludes seed problems (solution: relax probability requirement or use union)
  - **Label non-preservation:** Rare (<5% in manual annotation), usually from ungrammatical suggestions
  - **Subword exclusion:** Words split by tokenizers are automatically excluded—may bias against certain word types

- **First 3 experiments:**
  1. **Validate variant quality:** Manually annotate 100 random variants for fluency (1-5) and reasoning preservation (1-5); aim for F+R ≥9 in >90% of cases before scaling up.
  2. **Intersection vs. Union ablation:** Compare model PA scores on variants generated with P∩H intersection vs. P∪H union; expect 5-10% lower scores with union due to increased diversity.
  3. **Threshold sensitivity analysis:** Plot PA curves from 60-100% threshold for your target model; identify where performance drops sharply (the paper finds ~80% is the inflection point for most models).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do prompt-based Large Language Models (LLMs) exhibit greater robustness on MERGE variants compared to fine-tuned models, and does prompt engineering influence this performance?
- **Basis in paper:** [explicit] The paper states in the Limitations section: "Future studies could also consider evaluating prompt-based models, to observe if different prompt strategies might help the models perform better on variants."
- **Why unresolved:** The current study exclusively evaluates fine-tuned models (e.g., BERT, RoBERTa, DeBERTa) and excludes prompt-based models, leaving the generalization capabilities of instruction-tuned or few-shot LLMs on minimal variants untested.
- **What evidence would resolve it:** Application of the MERGE methodology to prompt-based LLMs (e.g., GPT-4, Llama) using varied prompting strategies (e.g., Chain-of-Thought) to measure performance drops relative to seed problems.

### Open Question 2
- **Question:** Is the MERGE methodology effective for testing generalization in Natural Language Understanding (NLU) tasks outside of Natural Language Inference, such as Reading Comprehension?
- **Basis in paper:** [explicit] The conclusion states: "this strategy can be extended to test generalizability in other NLI benchmarks and NLU tasks (e.g. reading comprehension), especially as many can be framed as NLI... which we are planning to do in the future."
- **Why unresolved:** The paper validates the method only on SNLI; it is unknown if the replacement of open-class words preserves the underlying "reasoning" or necessary context in tasks requiring complex retrieval or synthesis, such as Reading Comprehension.
- **What evidence would resolve it:** Implementation of the MERGE replacement strategy on datasets like SQuAD or MultiRC, followed by an evaluation of whether model performance drops similarly to the NLI setting.

### Open Question 3
- **Question:** Does single-occurrence masking introduce bias in the generated variants, and would simultaneous multi-occurrence masking improve the quality of reasoning-preserving replacements?
- **Basis in paper:** [explicit] The Limitations section notes: "suggestions are obtained by masking one occurrence of a word at a time. However, when one occurrence is masked, the other ones are still part of the sentence, which might bias the suggestions of models."
- **Why unresolved:** The current methodology replaces words by masking them individually while the original word remains present in other positions. It is unclear if this context biases the Masked Language Model (MLM) toward synonyms or if it degrades the contextual fit compared to masking all instances simultaneously.
- **What evidence would resolve it:** A comparative analysis of variant quality between the current single-mask approach and a simultaneous multi-mask approach (if feasible for the model architectures).

### Open Question 4
- **Question:** Why does increased lexical diversity (and even noise) in variants appear to stabilize or improve model performance at high accuracy thresholds compared to strict plausibility filtering?
- **Basis in paper:** [inferred] Section 5.4 notes that "variant diversity (number of unique variants) has a stronger effect on scores than filtering criteria like probability or plausibility," and that "scrambled" variants ended up with the highest scores on very high thresholds.
- **Why unresolved:** The authors observe that strict filtering for probability or plausibility (Pos, Prob) yielded lower scores than "None" or "Scrambled" variants at high thresholds. The mechanism driving this counter-intuitive result—whether it is artifact reduction or noise regularization—is not identified.
- **What evidence would resolve it:** A targeted ablation study analyzing the specific lexical features of scrambled vs. filtered variants to determine if strict filtering inadvertently amplifies specific heuristics or biases that the models rely on.

## Limitations

- **Intersection validation reliability:** Limited to manual annotation of 100 variants; the assumption that semantic preservation follows from syntactic plausibility needs systematic validation across diverse semantic phenomena.
- **Threshold selection arbitrariness:** The 90% accuracy threshold lacks principled justification and may be dataset-specific rather than universal.
- **Vocabulary and tokenization bias:** Automatic exclusion of subword-split words may systematically bias the dataset against morphologically complex words or proper nouns.

## Confidence

- **High confidence:** The core observation that MERGE generates minimal variants preserving syntax and reasoning is well-supported by manual annotation (F+R ≥ 9 in >90% of cases). The finding that noun replacements are more challenging than verb replacements has clear empirical support.
- **Medium confidence:** The claim that variant diversity affects scores more than filtering criteria is supported by the P∪H vs P∩H comparison, but the effect size and causal mechanism need further validation. The assertion that "most models generalize poorly" to minimally altered problems is suggestive but based on a limited model set.
- **Low confidence:** The mechanism claim that cross-context intersection validation preserves reasoning relationships lacks systematic empirical validation beyond manual annotation. The threshold inflection point at ~80% accuracy may be dataset-specific rather than a universal property.

## Next Checks

1. **Semantic preservation stress test:** Systematically annotate 500 variants across different semantic phenomena (quantifiers, negations, temporal relations) to quantify whether intersection validation actually preserves reasoning beyond syntactic plausibility. Compare against a control set using union filtering to measure the specific contribution of intersection validation.

2. **Threshold sensitivity across datasets:** Evaluate MERGE's Pattern Accuracy threshold sensitivity on multiple NLI datasets (MNLI, ANLI, BreakingNLI) to determine whether the 80% inflection point is universal or dataset-specific. This would validate whether the observed generalization failures reflect fundamental model limitations or dataset artifacts.

3. **Architecture generalization gap analysis:** Test MERGE variants on a broader range of architectures including smaller models (BERT-tiny, ALBERT-base), specialized NLI models (e.g., ESIM, Decomp-Ent), and models trained with different objectives (e.g., contrastive learning). This would determine whether the generalization failures are architecture-agnostic or specific to the models tested.