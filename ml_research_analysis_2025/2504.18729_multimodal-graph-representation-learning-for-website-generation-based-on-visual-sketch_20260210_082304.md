---
ver: rpa2
title: Multimodal graph representation learning for website generation based on visual
  sketch
arxiv_id: '2504.18729'
source_url: https://arxiv.org/abs/2504.18729
tags:
- visual
- graph
- code
- https
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the Design2Code problem, aiming to automatically
  convert webpage designs into functional HTML code. The core method introduces a
  multimodal graph representation learning framework that integrates visual information
  from webpage screenshots with structural relationships extracted via OCR and segmentation
  models.
---

# Multimodal graph representation learning for website generation based on visual sketch

## Quick Facts
- arXiv ID: 2504.18729
- Source URL: https://arxiv.org/abs/2504.18729
- Reference count: 18
- Method converts webpage screenshots into functional HTML using multimodal graph learning, outperforming existing techniques

## Executive Summary
This paper addresses the Design2Code problem by introducing a multimodal graph representation learning framework that integrates visual and textual webpage components. The approach constructs a graph where nodes represent text and visual elements, with edges capturing spatial and semantic relationships. By leveraging a graph-enhanced vision-language model, the method achieves superior performance in layout fidelity, content accuracy, and visual coherence compared to existing techniques, demonstrating the potential of multimodal graph learning for automated design-to-code conversion.

## Method Summary
The method extracts webpage components using PaddleOCR for text and SAM for visual elements, constructing a multimodal graph where nodes are CLIP-embedded components and edges connect related elements based on spatial overlap (IoU > 80%) or full text connectivity. A Graph Convolutional Network encodes the graph structure, while a pre-trained vision encoder processes the full screenshot. Both representations condition a frozen language model through interleaved Gated Cross-Attention blocks, with only the GCN, resampler, and GCA layers trained from scratch.

## Key Results
- Achieves state-of-the-art performance on both WebSight and Design2Code benchmarks
- Demonstrates significant improvements across Block-Match, Text, Position, Color, and CLIP Score metrics
- Outperforms existing techniques in layout fidelity, content accuracy, and visual coherence

## Why This Works (Mechanism)
The multimodal graph effectively captures both the visual appearance and spatial relationships between webpage components, providing rich structural context that traditional vision-only or text-only approaches miss. By encoding these relationships through graph neural networks and conditioning the language model on both visual and structural features, the model generates more accurate and contextually appropriate HTML code that better preserves the original design intent.

## Foundational Learning
- **Graph Convolutional Networks (GCNs)**: Process graph-structured data by propagating information between connected nodes; needed for encoding spatial relationships between webpage components; quick check: verify message passing updates node features correctly
- **Vision-Language Models**: Jointly process visual and textual information; needed for understanding the semantic relationship between webpage components and their HTML representations; quick check: confirm cross-modal attention attends to relevant regions
- **CLIP Embeddings**: Encode visual and textual components into a shared semantic space; needed for creating meaningful node features that capture component semantics; quick check: verify similar components have similar embeddings
- **PaddleOCR**: Extracts text from images; needed for identifying and processing textual webpage elements; quick check: ensure text extraction accuracy exceeds 95%
- **Segment Anything Model (SAM)**: Segments visual elements from images; needed for isolating non-text components for graph construction; quick check: verify segmentation precision with IoU > 0.8 for connected components

## Architecture Onboarding

**Component Map:** PaddleOCR/SAM -> Graph Construction -> GCN Encoder -> Perceiver Resampler -> Gated Cross-Attention -> Frozen LM -> HTML Output

**Critical Path:** Component extraction → graph construction → GCN encoding → vision encoding → GCA conditioning → language model prediction

**Design Tradeoffs:** The approach balances complexity and performance by freezing large pre-trained vision and language models while training only the graph encoder and conditioning layers, reducing computational cost while maintaining strong performance through transfer learning.

**Failure Signatures:** Poor graph construction (incorrect edge rules) leads to failure in capturing spatial relationships; component extraction errors (incomplete text masking or segmentation noise) introduce irrelevant nodes; insufficient GCN depth fails to propagate relationship information effectively.

**Three First Experiments:**
1. Train with only text-to-text edges (fully connected) to assess the importance of visual relationships
2. Train with only visual-to-visual edges to evaluate visual component understanding in isolation
3. Train with random edge connections to establish baseline performance without meaningful structure

## Open Questions the Paper Calls Out
None

## Limitations
- Missing specific hyperparameter values (GCN layers, learning rate, batch size) prevents exact reproduction
- No named versions for critical pre-trained models (vision encoder, base LM) limits implementation consistency
- Gated Cross-Attention implementation details are underspecified, requiring engineering assumptions

## Confidence
**High Confidence:** Problem formulation, multimodal graph construction methodology, and experimental results showing improvement over baselines are clearly presented and well-supported.

**Medium Confidence:** Architectural contributions (GCN encoding, GCA conditioning) are described with sufficient conceptual detail, though exact implementation specifications are missing.

**Low Confidence:** Complete reproduction requires significant engineering effort to fill in missing hyperparameter details and exact model specifications; reported performance improvements cannot be independently verified without access to implementation code.

## Next Checks
1. Implement a small-scale hyperparameter sweep (varying GCN layers 2-4, learning rates 1e-4 to 1e-5) on a 1k subset of training data to identify optimal configurations
2. Create controlled ablation experiments removing different edge types (text-to-text, visual-to-visual, text-to-visual) to quantify their individual contributions
3. Compute pairwise correlations between all evaluation metrics (Block-Match, Text, Position, Color, CLIP Score, BLEU variants) on validation set to determine if they measure distinct quality aspects or are redundant proxies