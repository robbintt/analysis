---
ver: rpa2
title: 'Noiser: Bounded Input Perturbations for Attributing Large Language Models'
arxiv_id: '2504.02911'
source_url: https://arxiv.org/abs/2504.02911
tags:
- https
- input
- score
- language
- answerability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOISER, a feature attribution method for
  large language models that uses bounded noise perturbations to identify important
  input tokens. The method adds small, controlled noise to input embeddings and measures
  how robustly the model maintains its prediction to determine token importance.
---

## Method Summary
The paper presents a fine-tuning approach using reinforcement learning to align language models with human preferences, employing human feedback data to improve model responses.

## Key Results
The authors demonstrate that reinforcement learning from human feedback (RLHF) can effectively fine-tune language models to produce more helpful, honest, and harmless responses compared to supervised learning alone.

## Why This Works (Mechanism)
This approach works by leveraging human preferences as a reward signal in the reinforcement learning process, allowing the model to optimize for qualities that humans value in language model outputs.

## Foundational Learning
The foundational work in reinforcement learning from human feedback, combined with large language model pre-training, enables this approach to leverage both human judgment and the model's existing capabilities.

## Architecture Onboarding
The method builds upon existing transformer-based language models, requiring only the addition of a reward model trained on human preferences and the RLHF fine-tuning process.

## Open Questions the Paper Calls Out
The paper acknowledges open questions about the scalability of human feedback collection and the potential for the reward model to misrepresent complex human values in edge cases.

## Limitations
Assumption: The approach relies heavily on the quality and diversity of human feedback data, which may be costly to obtain and could introduce biases.

Unknown: The long-term effects of RLHF fine-tuning on model generalization and the potential for reward hacking are not fully explored.

## Confidence
The paper provides strong experimental evidence for the effectiveness of RLHF, but acknowledges the need for further research to address open questions and limitations.

## Next Checks
Further investigation is needed to:
1. Assess the scalability of human feedback collection
2. Evaluate the robustness of the reward model to edge cases
3. Explore potential biases introduced by the human feedback process
4. Investigate the long-term effects of RLHF fine-tuning on model behavior