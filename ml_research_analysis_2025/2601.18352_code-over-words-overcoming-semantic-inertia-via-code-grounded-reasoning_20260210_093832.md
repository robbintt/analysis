---
ver: rpa2
title: 'Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning'
arxiv_id: '2601.18352'
source_url: https://arxiv.org/abs/2601.18352
tags:
- rules
- reasoning
- semantic
- language
- grid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning

## Quick Facts
- arXiv ID: 2601.18352
- Source URL: https://arxiv.org/abs/2601.18352
- Authors: Manjie Xu; Isabella Yin; Xinyi Tu; Chi Zhang; Yixin Zhu
- Reference count: 40
- Key outcome: Code-grounded reasoning reverses inverse scaling and enables effective prior inhibition in LLMs

## Executive Summary
This paper addresses semantic inertia in LLMs, where models struggle to suppress pre-trained associations when reasoning about mutable physical laws. The authors demonstrate that larger models exhibit inverse scaling on anti-commonsense tasks when using natural language prompts—they perform worse than smaller models because they default to familiar physics. They propose representing dynamic rules as executable Python code rather than descriptive text, which reverses this trend by enforcing a separation between variable names and their affordances. The Code-Grounded Vistas (LCV) framework fine-tunes models on counterfactual rule pairs to force logical grounding over visual semantics.

## Method Summary
The approach trains an LLM to generate executable Python transition functions from game states and mutable rule text blocks. The model is fine-tuned on paired counterfactual examples where the same visual state has contradictory rules (e.g., "WALL IS STOP" vs "WALL IS PASS"), forcing it to suppress visual priors and attend to logical constraints. At inference, the generated code serves as a transition oracle for a classical planner. The training uses contrastive disentanglement with LoRA fine-tuning on Qwen2.5-7B-Instruct, targeting complete `next_state()` and `check_win()` function generation.

## Key Results
- Natural language prompting exhibits inverse scaling: larger models perform worse on anti-commonsense tasks due to stronger semantic priors
- Code grounding inverts this pattern, enabling scale to become beneficial (Llama-3-70B achieves +0.29 ΔP improvement)
- Contrastive counterfactual alignment (LCD) fine-tuning significantly improves performance on dynamic rule-change tasks
- The amortized approach achieves single-pass inference compared to iterative search methods

## Why This Works (Mechanism)

### Mechanism 1: Inverse Scaling of Semantic Inertia in Natural Language
Natural language encoding entangles descriptive semantics with logical rules. Larger models, having stronger distributional priors, "hallucinate" familiar physics even when explicit rules state otherwise. The model defaults to pattern matching rather than logical inference.

### Mechanism 2: Code-Grounded Decoupling of Semantics and Logic
Code syntax enforces separation between variable names and affordances. By treating rules as variable assignments, the model shifts from semantic pattern matching to syntactic logic execution, bypassing semantic interference present in natural language.

### Mechanism 3: Counterfactual Contrastive Alignment (LCD)
Training on paired counterfactual examples (identical visuals, contradictory rules) forces the model to ground reasoning in logic rather than visual priors. Gradients from visual features oscillate and cancel out, forcing the model to attend exclusively to differentiating rule-text.

## Foundational Learning

- **Semantic Inertia / Prior Bias**: LLMs are "clumsy" at unlearning or inhibiting strong statistical correlations (e.g., "Lava = Danger") in favor of new context. Quick check: If I tell a standard LLM "Apples are blue and poisonous," will it consistently describe an apple as dangerous in a subsequent logic puzzle involving apples?

- **Inverse Scaling**: Model performance can degrade with scale on specific "anti-commonsense" tasks. Quick check: Why would a 70B model fail a logic puzzle that a 1B model solves?

- **Amortized Theory Induction**: LCV learns to instantly output the "theory" (physics engine code) in a single forward pass, distinct from inference-time search. Quick check: What is the trade-off between inference-time search (e.g., TheoryCoder) and LCV's amortized approach?

## Architecture Onboarding

- **Component map**: Environment (Baba Is You) -> Paired-Rule Intervention Controller -> Amortized Theory Inducer (LCV Model) -> Heuristic Search Planner
- **Critical path**: The Counterfactual Contrastive Alignment during fine-tuning (Section 5.2) is most critical. Without paired contrastive samples, the model fails to inhibit semantic priors.
- **Design tradeoffs**: LCV requires upfront training cost but offers O(1) inference; TheoryCoder requires no training but has O(N) inference. Code enforces logical decoupling but requires structured state representation; NL is flexible but prone to semantic entanglement.
- **Failure signatures**: Semantic Hallucination (code re-implements default physics despite explicit rules), Visual Overfitting (ignores text-block rules entirely)
- **First 3 experiments**: 1) Probing Inverse Scaling on Tier 2 tasks with NL vs Code prompts, 2) Ablation on training objective (standard SFT vs LCD), 3) Inference latency benchmark comparing LCV vs TheoryCoder

## Open Questions the Paper Calls Out
None

## Limitations
- The code-over-words mechanism is primarily supported by internal experimental results rather than external validation
- Effectiveness depends heavily on quality and diversity of counterfactual pairs, which remains underspecified
- The approach may not generalize beyond the Baba Is You domain to broader reasoning tasks

## Confidence
- **High confidence**: Observation of inverse scaling in natural language prompting for anti-commonsense tasks
- **Medium confidence**: Theoretical mechanism of code-grounded decoupling of semantics and logic
- **Medium confidence**: Effectiveness of contrastive counterfactual alignment (LCD) for suppressing visual priors
- **Low confidence**: Claim that this approach generalizes beyond Baba Is You domain

## Next Checks
1. **Cross-Domain Generalization Test**: Apply the code-grounded reasoning approach to a non-Baba Is You environment (e.g., Sokoban or different rule-changing puzzle game) to verify mechanism generalizes beyond specific training domain.

2. **Ablation on Rule Complexity**: Systematically vary complexity and number of simultaneous rules to determine whether code generation approach maintains advantage over natural language prompting as task difficulty increases.

3. **Human Evaluation of Generated Code**: Conduct human studies where experts evaluate whether generated Python transition functions correctly implement specified rules or inadvertently encode semantic priors, providing qualitative validation of code-over-words mechanism.