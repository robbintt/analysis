---
ver: rpa2
title: A Survey of Dimension Estimation Methods
arxiv_id: '2507.13887'
source_url: https://arxiv.org/abs/2507.13887
tags:
- dimension
- estimators
- datasets
- nonlinear
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of dimension estimation
  methods, categorizing them into tangential, parametric, and topological/metric families
  based on the geometric information they exploit. It evaluates 17 estimators across
  benchmark datasets with varying dimensions (1-70), codimensions (0-72), and geometries
  (flat, curved, nonlinear), while investigating sensitivity to hyperparameters, sample
  size, noise, and curvature.
---

# A Survey of Dimension Estimation Methods

## Quick Facts
- **arXiv ID**: 2507.13887
- **Source URL**: https://arxiv.org/abs/2507.13887
- **Reference count**: 40
- **Primary result**: Comprehensive benchmark of 17 dimension estimators on synthetic datasets with dimensions 1-70

## Executive Summary
This survey systematically evaluates dimension estimation methods across three families: tangential (PCA-based), parametric (distance/statistics-based), and topological/metric approaches. Testing 17 estimators on 24 benchmark datasets, the study finds no single estimator performs consistently well across all scenarios. The Expected Simplex Skewness (ESS) estimator shows strong overall performance, while local estimators like lPCA are highly sensitive to neighborhood size. The study reveals that high-dimensional datasets consistently cause underestimation due to boundary effects, and curvature impacts estimators differently—lPCA overestimates on negatively curved regions while parametric methods remain robust.

## Method Summary
The study benchmarks 17 dimension estimators using synthetic manifolds from scikit-dimension and custom geometries. Estimators are tested across sample sizes (625-5000 points), dimensions (1-70), and geometries (flat, curved, nonlinear). Performance is measured by comparing estimated dimension to ground truth across 20 runs. Hyperparameter ranges are fixed (e.g., neighborhood sizes k ∈ {10, 20, 40, 80}) rather than optimized. The augmented scikit-dimension fork includes additional estimators (GeoMLE, GRIDE, CDim) not in standard releases.

## Key Results
- No single estimator performs consistently well across all datasets
- ESS shows strong performance across most evaluation criteria
- Hyperparameter choice critically affects results, particularly for local estimators
- High-dimensional datasets consistently cause underestimation due to boundary effects
- Curvature impacts estimators differently—lPCA overestimates on negative curvature while MLE remains robust
- Parametric methods based on radial distances are generally more robust to curvature than tangential methods

## Why This Works (Mechanism)

### Mechanism 1
If data locally approximates a flat affine subspace, the decay of eigenvalues from a local covariance matrix can be thresholded to count dimensions.
**Tangential Estimation (lPCA)**: A neighborhood is constructed (via k-NN or ε-ball), covariance matrix computed, and dimension estimated by counting eigenvalues significantly larger than zero using thresholding strategies.
**Core assumption**: The manifold is locally linear within the chosen neighborhood scale.
**Evidence anchors**: Section 2.1.1 describes identifying tangent space via PCA eigenvalues; Section 3.4.3 shows thresholding method choice dramatically shifts estimates on nonlinear datasets.
**Break condition**: High curvature or non-linearity within neighborhood (Section 3.6 shows negative curvature causes lPCA to overestimate).

### Mechanism 2
If points are sampled from a uniform probability distribution on a manifold, the ratio of distances to nearest neighbors scales deterministically with dimension.
**Parametric Estimation (MLE/TwoNN)**: Models local point process as Poisson process where ball volume grows as r^d. Dimension inferred via Maximum Likelihood Estimation or linear regression on log-survival function.
**Core assumption**: Local sampling density is approximately uniform and neighborhood is small enough to ignore boundary effects.
**Evidence anchors**: Section 2.2.2 derives MLE estimator from Poisson point process rate; explains TwoNN relies on CDF of ratio ρ₂,₁ where log(1-F(ρ))/log(ρ) = d.
**Break condition**: Non-uniform density or boundary effects where theoretical ball is cut off (Section 2.6.1 notes high dimensions cause boundary concentration).

### Mechanism 3
The total length of a geometric graph (like Minimum Spanning Tree) scales with sample size at a rate determined by intrinsic dimension.
**Topological/Metric Estimation (PH0/MST)**: Uses Beardwood–Halton–Hammersley theorem. Constructs graph on data, computes α-weight, and infers dimension via power law E_α ∝ n^(1-α/d).
**Core assumption**: Sample size is sufficiently large to enter asymptotic regime where growth rate stabilizes.
**Evidence anchors**: Section 2.3.1 describes how slope of log(E_α) vs log(n) yields dimension; Remark 2.6 warns high dimensions make slope m close to 1, causing small errors to result in massive errors.
**Break condition**: Insufficient sample size or high intrinsic dimension where slope is too close to 1 to distinguish from noise.

## Foundational Learning

**Concept: Boundary Effects & Concentration of Measure**
- **Why needed here**: Primary cause of underestimation in high-dimensional datasets; explains why volume concentrates near surface in high dimensions
- **Quick check question**: Why does a uniform distribution in a high-dimensional ball appear to most estimators as a lower-dimensional set concentrated near the boundary?

**Concept: Hyperparameter Sensitivity (Throttling)**
- **Why needed here**: Unlike many ML algorithms, best hyperparameter here depends on geometric scale of manifold
- **Quick check question**: If k=5 in lPCA, why is it impossible to estimate a dimension of 6?

**Concept: Aggregation Strategies (Harmonic Mean)**
- **Why needed here**: Local estimators produce distribution of estimates; choice of aggregation significantly impacts robustness
- **Quick check question**: Why does the paper suggest Harmonic Mean is theoretically superior for aggregating local MLE estimates compared to arithmetic mean?

## Architecture Onboarding

**Component map**: Data Interface -> Neighborhood Engine -> Geometric Engine -> Aggregator
- *Data Interface*: Accepts point cloud X ⊂ R^D
- *Neighborhood Engine*: Constructs local views (k-NN or ε-balls)
- *Geometric Engine*:
  - Tangential: Eigen-decomposition (lPCA)
  - Parametric: Distance statistics & Likelihood functions (MLE, TwoNN, ESS)
  - Topological: Graph construction & Subsampling (PH0, KNN)
- *Aggregator*: Combines pointwise estimates (Mean, Median, Harmonic Mean)

**Critical path**: Hyperparameter Selection (k). The paper explicitly warns that overfitting is frequent when tuning hyperparameters to specific benchmark datasets. There is no "universal" k; it depends on curvature and sample density.

**Design tradeoffs**:
- Local vs. Global: Global estimators (PH0, KNN) need fewer points per region but suffer high variance in high dimensions; local estimators (lPCA, MLE) have lower variance but require high sampling density
- Robustness vs. Accuracy: Parametric methods (MLE, ESS) are robust to curvature but sensitive to density variations/noise; Tangential methods (lPCA) are intuitive but highly sensitive to curvature

**Failure signatures**:
- Underestimation: Common in high dimensions (>20) due to boundary effects and finite sample limits
- Overestimation: Occurs in lPCA with negative curvature or when Gaussian noise creates "ghost" dimensions
- Throttling: Output values stuck at or near parameter k (for lPCA) or low integers, indicating neighborhood is too small

**First 3 experiments**:
1. Baseline Sanity Check: Run ESS and lPCA on M10a Cubic dataset; check if lPCA outputs exactly 10 and if ESS is close
2. Sensitivity Sweep: Generate M6 Nonlinear dataset; run lPCA while sweeping k from 10 to 100; observe if estimate drifts or throttles
3. Curvature Stress Test: Compare lPCA vs MLE on dataset with significant negative curvature; verify if lPCA overestimates while MLE remains robust

## Open Questions the Paper Calls Out

**Open Question 1**: Can adaptive methods be developed to automate hyperparameter selection using intrinsic features of the input data?
- Basis: Authors state investigating adaptive methods automating hyperparameter choice using input data features would be valuable
- Why unresolved: Current methods rely heavily on user-specified hyperparameters sensitive to geometry and density
- What evidence would resolve it: Algorithm dynamically selecting k or other parameters based on local geometry/density metrics showing improved accuracy over fixed-parameter baselines

**Open Question 2**: To what extent does manifold curvature systematically impact the bias of different dimension estimation families?
- Basis: Conclusion notes limited results on curvature role justify systematic approach to determining curvature's impact
- Why unresolved: Study found parametric methods are generally robust to curvature while tangential estimators overestimate on negative curvature, but general systematic theory is lacking
- What evidence would resolve it: Theoretical analysis or empirical study mapping specific Gaussian curvature values against estimation error for major estimator families

**Open Question 3**: How can dimension estimators be improved to handle non-linear manifolds with intrinsic dimensions greater than six?
- Basis: Authors identify estimator performance on non-linear manifolds and high dimensional manifolds as area requiring major progress
- Why unresolved: Current estimators tend to underestimate dimensions in high-dimensional spaces due to boundary effects and concentration of measure phenomena
- What evidence would resolve it: Novel estimator or correction factor maintaining accuracy on benchmark Nonlinear datasets when d > 6

## Limitations

- Empirical conclusions rely heavily on synthetic benchmark datasets that may not reflect real-world data assumptions
- Hyperparameter tuning was performed within fixed ranges without optimization procedures
- Study cannot quantify impact of real-world data violations (non-uniform sampling, irregular manifold structure)

## Confidence

**High Confidence**: Claims about ESS estimator performance across datasets, fundamental mechanisms of lPCA and MLE estimators, and finding that no single estimator dominates all scenarios

**Medium Confidence**: Claims about specific hyperparameter sensitivities, comparative robustness of parametric vs. tangential methods, and characterization of boundary effects in high dimensions

**Low Confidence**: Claims about aggregation strategy superiority and specific quantitative thresholds for estimator failures in various scenarios

## Next Checks

1. **Real-world dataset validation**: Apply top-performing estimators (ESS, MLE) to at least three real-world datasets with known intrinsic dimensions to verify synthetic benchmark performance translates to practical applications

2. **Hyperparameter optimization study**: Implement systematic hyperparameter search (grid or Bayesian optimization) for most sensitive estimators (lPCA, MLE) to determine if better settings exist beyond fixed ranges tested

3. **Noise sensitivity characterization**: Systematically vary noise levels and types (Gaussian, uniform, outliers) across all estimators to create comprehensive noise-robustness ranking addressing finding that noise severely affects most estimators when ambient dimension greatly exceeds intrinsic dimension