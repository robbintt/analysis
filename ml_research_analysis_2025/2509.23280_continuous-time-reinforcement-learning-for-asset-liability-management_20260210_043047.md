---
ver: rpa2
title: Continuous-Time Reinforcement Learning for Asset-Liability Management
arxiv_id: '2509.23280'
source_url: https://arxiv.org/abs/2509.23280
tags:
- learning
- continuous-time
- exploration
- policy
- surplus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continuous-time reinforcement learning
  approach to asset-liability management (ALM) that formulates the problem as a linear-quadratic
  control with both interim and terminal objectives. The method employs a model-free,
  entropy-regularized soft actor-critic algorithm with adaptive actor exploration
  and scheduled critic exploration to achieve effective exploration-exploitation balance.
---

# Continuous-Time Reinforcement Learning for Asset-Liability Management

## Quick Facts
- **arXiv ID:** 2509.23280
- **Source URL:** https://arxiv.org/abs/2509.23280
- **Reference count:** 40
- **Key outcome:** Model-free continuous-time RL outperforms traditional financial strategies and state-of-the-art RL algorithms across 200 randomized market scenarios.

## Executive Summary
This paper introduces a continuous-time reinforcement learning approach to asset-liability management (ALM) that formulates the problem as a linear-quadratic control with both interim and terminal objectives. The method employs a model-free, entropy-regularized soft actor-critic algorithm with adaptive actor exploration and scheduled critic exploration to achieve effective exploration-exploitation balance. Evaluated across 200 randomized market scenarios, the algorithm outperforms two enhanced traditional financial strategies, a model-based continuous-time RL method, and three state-of-the-art RL algorithms (SAC, PPO, DDPG), achieving statistically significant higher average rewards. The outperformance stems from directly learning the optimal ALM strategy without learning the environment, rather than from complex neural networks or parameter estimation.

## Method Summary
The method formulates ALM as a continuous-time linear-quadratic control problem where the agent minimizes surplus deviation (assets minus liabilities minus target) using a control variable. The algorithm uses a model-free, entropy-regularized soft actor-critic approach that directly parameterizes the value function as a quadratic form and the policy as a Gaussian distribution, avoiding neural networks and parameter estimation of market dynamics. The algorithm updates three parameter sets: critic weights, policy mean, and policy variance, using stochastic gradients derived from continuous-time policy gradients. Key innovations include adaptive actor exploration (dynamically updating policy variance) and scheduled critic exploration (decaying temperature parameter over episodes).

## Key Results
- Achieves statistically significant higher average rewards than two enhanced traditional financial strategies across 200 randomized market scenarios
- Outperforms a model-based continuous-time RL method by directly learning the optimal strategy without environment parameter estimation
- Beats three state-of-the-art RL algorithms (SAC, PPO, DDPG) while using simpler function approximation (quadratic/linear) instead of neural networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Formulating ALM as a continuous-time LQ control problem with running costs reduces time-inconsistency risks inherent in traditional Mean-Variance approaches.
- **Mechanism:** The algorithm minimizes a quadratic cost of "surplus deviation" continuously over [0, T] rather than solely optimizing for a terminal state, forcing the policy to maintain stability throughout the investment horizon.
- **Core assumption:** Financial dynamics can be modeled by a linear SDE where drift and diffusion are linear in state and control.
- **Evidence anchors:** Abstract mentions "linear-quadratic (LQ) formulation... incorporating both interim and terminal objectives." Section 2.1 discusses MV limitations and introduces running cost. Related continuous-time LQ control papers support structural validity.
- **Break condition:** If market exhibits jump-diffusion or non-linear dynamics not captured by linear SDE, LQ optimal solution may no longer be optimal or stable.

### Mechanism 2
- **Claim:** Model-free approach outperforms model-based plugins because it avoids compounding estimation errors from unknown market parameters.
- **Mechanism:** Instead of estimating environment parameters (A, B, C, D), the algorithm parameterizes value function (quadratic) and policy (Gaussian) based on theoretical structure, updating parameters directly using stochastic gradients.
- **Core assumption:** Optimal value function is quadratic in state deviation and optimal policy is linear (Gaussian), allowing simple function approximation without large neural networks.
- **Evidence anchors:** Abstract states outperformance stems from "directly learning the optimal ALM strategy without learning the environment." Section 3.1 describes parameterization. Internal empirical results provide primary evidence.
- **Break condition:** If "indefinite" nature of control weight leads to numerical instability in gradient updates, policy may diverge.

### Mechanism 3
- **Claim:** Decoupling exploration into adaptive actor exploration (variance) and scheduled critic exploration (temperature) stabilizes convergence trajectory better than standard fixed or single-variable exploration.
- **Mechanism:** Actor policy variance φ₂ is updated dynamically via gradient descent based on data, allowing agent to adjust noise levels to specific market state. Critic temperature γ follows deterministic decay schedule (γₙ = cᵧ / bₙ), systematically shifting from exploration to exploitation over episodes.
- **Core assumption:** Learning rates aₙ and decay schedule bₙ satisfy standard stochastic approximation conditions (∑aₙ = ∞, ∑aₙ² < ∞).
- **Evidence anchors:** Section 3.4 & 3.5 defines update rules for φ₂ and schedule for γ. Theorem 1 proves almost sure convergence under these update rules. Related work provides theoretical grounding.
- **Break condition:** If projection bounds (Uθ, U₁, U₂) are set too tight, algorithm may clip valid policy updates, preventing convergence to optimal strategy.

## Foundational Learning

- **Concept:** Stochastic Differential Equations (SDEs) & Brownian Motion
  - **Why needed here:** State dynamics (surplus deviation) evolve continuously over time with random market fluctuations. Understanding dW(t) is essential for grasping source of uncertainty.
  - **Quick check question:** How does control u affect drift vs. volatility of surplus deviation in Equation (1)?

- **Concept:** Entropy-Regularized Reinforcement Learning
  - **Why needed here:** Algorithm is "soft" actor-critic, maximizing expected reward PLUS entropy. Crucial for adaptive exploration mechanism.
  - **Quick check question:** What happens to policy if temperature parameter γ is set to zero?

- **Concept:** Temporal Difference (TD) Learning in Continuous Time
  - **Why needed here:** Critic update relies on difference between infinitesimal change in value dJ and instantaneous reward, integrated over time.
  - **Quick check question:** Why does method discretize only for numerical integration (Eq 16) rather than formulating problem as discrete MDP from start?

## Architecture Onboarding

- **Component map:** State (x) → Policy (Actor: Gaussian π(u|x) with mean φ₁x and variance φ₂) → Value (Critic: Quadratic J(t,x) = -½k₁(t)x² + k₃(t)) → Updaters (θ, φ₁, φ₂)

- **Critical path:**
  1. Initialize θ, φ₁, φ₂
  2. **Trajectory Rollout:** Simulate SDE using discretization Δt to generate path {x(tₖ)}
  3. **Critic Update:** Compute TD error integrals to update θ (Eq 16)
  4. **Actor Update:** Update φ₁ (mean) and φ₂ (variance) using policy gradients (Eq 17, 18)
  5. **Schedule Step:** Decay critic temperature γ

- **Design tradeoffs:**
  - **Analytical Parameterization vs. Neural Networks:** Uses simple quadratic/linear parameterizations derived from theory rather than deep nets, reducing overfitting and sample complexity but assuming LQ model is correct "physics" of market
  - **Model-Free vs. Plugin:** Avoiding parameter estimation (A,B,C,D) improves robustness to noise but may require more samples to converge than oracle with perfect knowledge

- **Failure signatures:**
  - **Exploding Gradients:** If φ₂ (variance) decays too close to zero, inverse φ₂⁻¹ in gradient term causes numerical instability (mitigated by projection ε)
  - **Oscillation:** If learning rate aₙ is too high relative to exploration schedule, policy mean φ₁ may oscillate around optimal φ*
  - **Suboptimal Convergence:** If scheduled decay bₙ is too fast, agent may commit to policy before critic is accurate (premature exploitation)

- **First 3 experiments:**
  1. **Sanity Check (Oracle Comparison):** Run algorithm in simulated environment where A,B,C,D are known. Compare learned φ₁ to analytical optimal φ* (Eq 3) to verify convergence
  2. **Ablation on Exploration:** Run three variants: (a) Fixed γ and φ₂, (b) Adaptive φ₂ only, (c) Scheduled γ only. Compare final rewards against full dual-exploration method
  3. **Discretization Sensitivity:** Test performance while varying Δt (e.g., 0.001 vs 0.1). Verify paper's claim that continuous-time formulation handles small time steps better than discrete-time MDP baselines (SAC/PPO)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed continuous-time RL framework maintain its convergence guarantees and performance in high-dimensional state and action spaces (e.g., multi-asset portfolios)?
- **Basis in paper:** Formulation restricts state x(t) and control u(t) to ℝ (scalar), and theoretical proofs rely on parameter projections for stability in this single-dimensional setting
- **Why unresolved:** Paper doesn't test multi-dimensional settings, and linear parameterization may face curse of dimensionality or instability without neural network approximation
- **What evidence would resolve it:** Empirical results from simulations where x(t) ∈ ℝⁿ and u(t) ∈ ℝᵐ for n, m > 1, showing convergence comparable to scalar case

### Open Question 2
- **Question:** Does algorithm remain robust in "more complex" market environments that violate LQ assumptions, such as those with stochastic volatility or market jumps?
- **Basis in paper:** Conclusion states "Future research will focus on... evaluating its performance in more complex and dynamic market environments"
- **Why unresolved:** Current method leverages specific structural properties of LQ formulation (linear drift, quadratic value) for parameterization, which may not hold or approximate well in non-LQ dynamics
- **What evidence would resolve it:** Simulations using non-LQ dynamics (e.g., Heston model or jump-diffusion processes) demonstrating learned policy outperforms traditional benchmarks without modifying algorithm's core structural assumptions

### Open Question 3
- **Question:** How does scheduled critic exploration mechanism (γₙ) interact with stability of TD learning process over very long horizons?
- **Basis in paper:** Introduces scheduled temperature γₙ = cᵧ / bₙ (Eq 15) to balance exploration/exploitation, but relies on discretization of Δt = 0.01 for stability
- **Why unresolved:** While Theorem 1 proves almost sure convergence, sensitivity of this convergence to interaction between diminishing temperature γₙ and discretization step size Δt in continuous time is not fully explored for extended horizons
- **What evidence would resolve it:** Sensitivity analysis showing convergence behavior across various time step sizes (Δt) and temperature schedules (bₙ) over longer time horizons T

## Limitations

- Performance claims rely heavily on simulation with randomized parameters rather than real-world market data validation
- LQ framework assumes market dynamics can be captured by linear SDEs, which may not hold during market stress or regime shifts
- Several hyperparameters (time horizon T, initial state, penalty coefficients Q and H) are not explicitly specified, creating ambiguity for reproduction

## Confidence

- **High confidence:** LQ control formulation with running costs effectively addresses time-inconsistency (Mechanism 1). Convergence proof for exploration schedules (Mechanism 3) appears mathematically sound.
- **Medium confidence:** Model-free approach's advantage over plugin methods (Mechanism 2) is primarily supported by internal empirical results rather than strong external validation. Empirical outperformance vs. baselines is convincing but depends on simulation assumptions.
- **Low confidence:** Claim that performance "stems not from parameter estimation but from directly learning the optimal ALM strategy" is difficult to verify without access to oracle baselines with perfect market knowledge.

## Next Checks

1. Implement the algorithm in a fixed-parameter environment where analytical optimal solutions are known, and verify that learned parameters converge to theoretical optima
2. Test algorithm robustness by evaluating performance across extreme market scenarios (high volatility, regime shifts) not represented in the uniform random parameter distribution
3. Compare sample efficiency and convergence speed against a model-based oracle that knows true market parameters, to quantify the claimed advantage of model-free learning