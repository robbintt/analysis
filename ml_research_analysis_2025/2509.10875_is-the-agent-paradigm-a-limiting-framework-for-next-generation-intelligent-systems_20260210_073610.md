---
ver: rpa2
title: Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent
  Systems?
arxiv_id: '2509.10875'
source_url: https://arxiv.org/abs/2509.10875
tags:
- systems
- agents
- intelligence
- agent
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the dominant agent-centric paradigm in AI,
  arguing it is a limiting framework for developing next-generation intelligent systems.
  Through a systematic literature review and quantitative analysis of a 98-concept
  knowledge graph, the authors reveal a structural crisis: critiques of the agent
  paradigm, particularly those centered on anthropocentric biases, are now more influential
  than the foundational agent concept itself.'
---

# Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?

## Quick Facts
- arXiv ID: 2509.10875
- Source URL: https://arxiv.org/abs/2509.10875
- Authors: Jesse Gardner; Vladimir A. Baulin
- Reference count: 0
- Primary result: The agent-centric paradigm in AI may be a limiting framework; critiques of anthropocentrism now exceed foundational agent concepts in influence, suggesting need for system-level approaches.

## Executive Summary
This paper challenges the dominant agent-centric paradigm in AI, arguing it is a limiting framework for developing next-generation intelligent systems. Through a systematic literature review and quantitative analysis of a 98-concept knowledge graph, the authors reveal a structural crisis: critiques of the agent paradigm, particularly those centered on anthropocentric biases, are now more influential than the foundational agent concept itself. The analysis demonstrates a persistent gap between high-level theoretical/critical debate and practical implementation. As an alternative, the paper proposes a shift towards "agential systems" and systemic intelligence, emphasizing world models, continuous interaction, and materiality.

## Method Summary
The study constructs a knowledge graph from systematic literature review, analyzing 98 concepts across six categories: Theoretical Concept, Architecture/Model, Entity/System, Method/Technique, Application/Domain, and Critique/Challenge. The authors employ network analysis metrics including PageRank centrality, Shannon entropy for interdisciplinarity, and temporal dynamics across three periods. They generate an "Atlas of Opportunity" heatmap identifying missing links between concept categories as potential research frontiers. The analysis reveals that critiques of anthropocentrism now exceed foundational agent concepts in influence, indicating accumulated conceptual debt in the field.

## Key Results
- Critiques of anthropocentrism now exceed foundational agent concepts in influence according to network analysis
- Persistent gap exists between high-level theoretical/critical debate and practical implementation
- LLM-based "agentic" systems achieve behavioral plausibility through statistical pattern completion rather than genuine goal-directed autonomy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent paradigm introduces persistent conceptual ambiguities that constrain AI research directions.
- Mechanism: Agent definitions rely on unmeasurable folk-psychology concepts (autonomy, intentionality, beliefs) that resist operationalization. These inherited anthropocentric framings bias research toward replicating human cognition rather than discovering alternative intelligence architectures. The paper's network analysis shows critiques of anthropocentrism now exceed the foundational agent concept in influence—indicating accumulated conceptual debt.
- Core assumption: Conceptual clarity is prerequisite to engineering progress; ambiguous constructs yield misaligned research programs.
- Evidence anchors: [abstract] "persistent conceptual ambiguities and inherent anthropocentric biases may represent a limiting framework"; [Section II.A] "systematic review reveals a persistent lack of consensus and consistency in these definitions... The objective measurement of such internal states... poses particular difficulties for empirical verification"

### Mechanism 2
- Claim: LLM-based "agentic" systems achieve behavioral plausibility through statistical pattern completion, not genuine goal-directed autonomy.
- Mechanism: LLMs operate as tensor transformation machines performing interpolation within learned embedding spaces. Their apparent planning/reasoning emerges from pattern recognition over vast training corpora—not from internal goal representations or world models in the agent-theoretic sense. This creates an "agentic facade": behavior that satisfies user intuitions while lacking the functional architecture attributed to it.
- Core assumption: Genuine agency requires internal goal representations and world models that causally drive behavior, not merely behavioral outputs that correlate with goal-achievement patterns.
- Evidence anchors: [abstract] "much of what is termed 'Agentic AI' is sophisticated mimicry rather than genuine agency"; [Section II.C] "their impressive performance may represent sophisticated 'algorithmic mimicry' or merely be a 'simulacra' of agency, rather than genuine, robust agency derived from vast implicit 'world models'"

### Mechanism 3
- Claim: System-level intelligence can emerge from distributed sub-optimal components without centralized agent architectures.
- Mechanism: Complex adaptive systems (biological collectives, physical substrates) demonstrate that collective intelligence arises from local interaction rules among individually bounded-rational units. The system-level dynamics—not individual optimality—produce adaptive behavior. Material/embodied computing exploits this by letting physics perform computation directly.
- Core assumption: Intelligence is not monolithic or necessarily localized; emergent system-level properties can exceed component-level capabilities.
- Evidence anchors: [abstract] "propose a shift in focus towards frameworks grounded in system-level dynamics, world modeling, and material intelligence"; [Section III.B] "Their true power often emerges when these agents operate en masse, creating complex adaptive systems where sophisticated collective intelligence arises from simple, local interaction rules"

## Foundational Learning

- **Concept: Bounded Rationality**
  - Why needed here: The paper argues that optimal individual agents may be unnecessary—or even counterproductive—for system-level intelligence. Understanding computational/information constraints on decision-making is prerequisite to evaluating sub-optimal component design.
  - Quick check question: Can you explain why a chess-playing system with limited lookahead might outperform exhaustive search in time-constrained real-world scenarios?

- **Concept: Emergence in Complex Systems**
  - Why needed here: The proposed "agential systems" alternative depends on intelligence emerging from system-level dynamics rather than engineered into components. Without this conceptual foundation, the shift from agents to systems appears merely semantic.
  - Quick check question: Can you describe a system where global behavior cannot be predicted from component specifications alone?

- **Concept: World Models vs. Reactive Policies**
  - Why needed here: The paper distinguishes systems with internal predictive models from those executing learned input-output mappings. This distinction underlies the critique of LLM-based agents as mimicry rather than genuine reasoning systems.
  - Quick check question: What's the functional difference between a system that predicts environment dynamics and one that learns successful action patterns?

## Architecture Onboarding

- **Component map:**
  - Traditional agent stack: Perception module → Internal model → Planner → Action selector (centralized, bounded)
  - Agential system stack: Distributed sensors/actuators ↔ Local interaction rules ↔ Emergent coordination (decentralized, porous boundaries)
  - Material computing stack: Physical substrate → Intrinsic dynamics → Functional behavior (no explicit software layer)

- **Critical path:**
  1. Identify task domain where agent boundaries are ambiguous (e.g., swarm robotics, distributed sensing)
  2. Specify local interaction rules rather than global policies
  3. Measure system-level behavior emergence vs. design intent
  4. Iterate on interaction rules based on systemic outcomes

- **Design tradeoffs:**
  - Interpretability vs. emergence: System-level approaches sacrifice component-level explainability for adaptive robustness
  - Engineering control vs. self-organization: Pre-specifying behavior limits emergence; ceding control enables novelty
  - Verification complexity: Proving properties of emergent systems exceeds formal methods for bounded agents

- **Failure signatures:**
  - System locks into suboptimal stable configurations (local minima in collective behavior space)
  - Emergent behavior violates safety constraints unpredictably
  - No clear attribution path when system fails (accountability dissolution)
  - Anthropomorphic projections persist despite non-agent design (users impute agency incorrectly)

- **First 3 experiments:**
  1. Comparative benchmark: Match bounded-rational multi-agent system against optimal single-agent baseline on tasks requiring environmental adaptation. Measure performance gap vs. computational cost ratio.
  2. Mimicry detection: Test LLM-based "agents" on goal structures systematically excluded from training distribution. Quantify generalization failure modes that distinguish pattern completion from reasoning.
  3. Material substrate test: Implement identical computational task in software vs. physical reservoir computing substrate. Measure energy efficiency and robustness to perturbation differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reinforcement learning algorithms be designed to be robust to Goodhart's Law (where a measure becomes a target and ceases to be a good measure)?
- Basis in paper: [explicit] The "Atlas of Opportunity" identifies the link between Method/Technique (RL) and Critique/Challenge (Goodhart's Law) as a primary research frontier with high untapped potential.
- Why unresolved: Current agents optimize narrow, gameable proxies (e.g., "dust collected") rather than holistic goals, leading to misaligned or inefficient behavior.
- What evidence would resolve it: Development of algorithms that learn complex reward functions based on holistic assessments (e.g., general room cleanliness) rather than easily manipulated metrics.

### Open Question 2
- Question: Do LLM-based social simulations model genuine human decision-making or merely reproduce statistical patterns (simulacra) from training data?
- Basis in paper: [explicit] The paper highlights the link between Application/Domain (Social Simulation) and Critique/Challenge (Simulacra of Agency) as a major gap in Section V.D.
- Why unresolved: It is unclear if LLM agents possess causal understanding or simply mimic correlations found in their training sets.
- What evidence would resolve it: Validation studies placing LLM-agents in novel scenarios not represented in training data to test if behavior remains coherent and human-like.

### Open Question 3
- Question: Can formal methods be used to verify if a neural architecture's dynamics are computationally equivalent to specific inferential algorithms (e.g., Active Inference)?
- Basis in paper: [explicit] The authors propose connecting Method/Technique (Modal Logic) with Critique/Challenge (As-If Fallacy) to move beyond metaphorical descriptions of AI.
- Why unresolved: Theories often describe systems "as if" they perform inference without mechanistic proof, risking the map-territory fallacy.
- What evidence would resolve it: Rigorous proofs demonstrating that specific neural dynamics are mathematically equivalent to the inferential algorithms posited by theoretical frameworks.

## Limitations

- The central claims about the agent paradigm being a limiting framework rest on conceptual critiques rather than empirical falsification of agent architectures
- The quantitative analysis reveals shifts in academic discourse patterns but cannot definitively establish causal relationships between paradigm structure and research outcomes
- The proposed "agential systems" alternative remains largely theoretical, with limited concrete engineering guidance for practical implementation

## Confidence

- **High Confidence**: The observation that critiques of anthropocentrism now exceed foundational agent concepts in influence (network analysis finding)
- **Medium Confidence**: The claim that LLM-based agents represent "sophisticated mimicry" rather than genuine agency (supported by mechanism but lacks direct experimental validation)
- **Low Confidence**: The assertion that system-level approaches will necessarily outperform agent architectures for general intelligence (largely speculative, minimal empirical evidence)

## Next Checks

1. **Empirical Agency Test**: Design a benchmark suite where LLM-based agents and explicitly goal-directed agent architectures must generalize to novel task structures. Measure systematic generalization failures versus architectural differences to distinguish pattern completion from reasoning.

2. **System vs. Agent Comparison**: Implement identical computational tasks using both centralized optimal agent designs and distributed sub-optimal component systems. Quantify the performance gap versus computational resource ratios across multiple domains requiring environmental adaptation.

3. **Anthropomorphism Detection**: Conduct user studies where participants interact with systems designed to be explicitly non-agentic (material computing, emergent coordination) versus traditional agent architectures. Measure residual anthropomorphic projections and their impact on system adoption and evaluation.