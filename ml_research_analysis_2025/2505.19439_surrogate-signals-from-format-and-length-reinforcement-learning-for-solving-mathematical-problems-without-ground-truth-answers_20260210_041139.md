---
ver: rpa2
title: 'Surrogate Signals from Format and Length: Reinforcement Learning for Solving
  Mathematical Problems without Ground Truth Answers'
arxiv_id: '2505.19439'
source_url: https://arxiv.org/abs/2505.19439
tags:
- reward
- length
- training
- reasoning
- format
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of reinforcement learning for\
  \ mathematical problem solving without access to ground truth answers, which are\
  \ costly to obtain. The authors propose using surrogate signals\u2014specifically\
  \ format correctness and response length\u2014as rewards in reinforcement learning."
---

# Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers

## Quick Facts
- arXiv ID: 2505.19439
- Source URL: https://arxiv.org/abs/2505.19439
- Reference count: 22
- Key outcome: 40.0% accuracy on AIME2024 using format and length surrogate signals in GRPO with a 7B base model

## Executive Summary
This paper addresses the challenge of reinforcement learning for mathematical problem solving without access to ground truth answers, which are costly to obtain. The authors propose using surrogate signals—specifically format correctness and response length—as rewards in reinforcement learning. Format correctness ensures structured and clear solutions, while length rewards penalize overly long or short responses to encourage concise, effective reasoning. The method, implemented via GRPO, achieves 40.0% accuracy on AIME2024 with a 7B base model and generalizes across different model sizes. The findings suggest that RL primarily activates latent reasoning capabilities already embedded in pre-trained models, rather than imparting new knowledge, enabling lightweight, label-efficient strategies for reasoning-intensive tasks.

## Method Summary
The method employs GRPO (Group Relative Policy Optimization) with two surrogate rewards: format correctness and length-based rewards. Format reward checks if responses contain a \boxed{} with mathematically valid expressions validated by SymPy. Length reward uses a piecewise function that peaks at 50% of maximum length to discourage both truncation and verbosity. The combined reward is applied only if format is correct. Training uses 8 responses per prompt with group-relative advantages, and evaluation uses greedy decoding. The approach requires strong base models with high pass@K but low pass@1 performance.

## Key Results
- 40.0% accuracy on AIME2024 test set with Qwen2.5-Math-7B base model
- Format-only rewards achieve 85% of total performance gains in first ~15 optimization steps
- Pass@64 rates remain nearly identical across base (63.3%), correctness-RL (73.3%), format-only (66.7%), and format-length (66.7%) models
- Format-length rewards achieve up to 20.2 points improvement over base models on AIME

## Why This Works (Mechanism)

### Mechanism 1: Format Reward as Early-Stage Structural Aligner
- Claim: Format-only rewards achieve comparable early gains to ground-truth rewards, accounting for ~85% of total improvement
- Mechanism: Binary format reward (1 if \boxed{} contains valid math via SymPy, 0 otherwise) guides the model to learn solution structure before content refinement; GRPO's group-relative advantages amplify this signal
- Core assumption: Base model already possesses latent mathematical knowledge that proper formatting unlocks
- Evidence anchors:
  - [abstract]: "early training is dominated by format learning, where structural feedback alone accounts for most performance gains"
  - [Section 3.2]: "Within the first ~15 optimization steps, models rapidly converge toward structured and concise solutions, yielding over 85% of the total performance improvements"
  - [corpus]: "Decoupling Task-Solving and Output Formatting in LLM Generation" supports formatting as distinct learnable component
- Break condition: Format-only plateaus after ~15 steps—cannot guide content refinement beyond structural compliance

### Mechanism 2: Length Reward Balances Reasoning Depth vs Efficiency
- Claim: Piecewise length reward (rise-then-fall curve) prevents truncation while encouraging comprehensive reasoning
- Mechanism: R_l peaks at x=p (default 0.5 of max length), penalizing both under-exploration (too short) and verbosity (too long); combined R_fl = R_f + R_l only if format correct
- Core assumption: Moderate-length responses correlate with better reasoning; extreme lengths indicate problems
- Evidence anchors:
  - [abstract]: "Incorporating length-based rewards further refines outputs by discouraging overly long or short responses"
  - [Section 4.3]: Linear length rewards "led to a rapid surge in response length... causing a 52.9% truncation rate by step 54"
  - [corpus]: CoLD paper identifies "pervasive length bias in existing PRMs"
- Break condition: Linear length rewards cause truncation; p=1.0 (no penalty) → 52.9% truncation; p=0.1 → limited exploration, accuracy decline

### Mechanism 3: RL Activates Latent Knowledge Rather Than Learning New
- Claim: RL gains arise from surfacing pre-existing knowledge, not acquiring new capabilities
- Mechanism: GRPO with surrogate signals shifts probability mass from incorrect to correct reasoning paths already present in base model
- Core assumption: Strong base models have high pass@K but low pass@1—knowledge exists but retrieval is inefficient
- Evidence anchors:
  - [abstract]: "RL primarily activates reasoning capabilities already embedded in pre-trained models"
  - [Section 4.1/Table 7]: pass@64 nearly identical across base (63.3% AIME), correctness-RL (73.3%), format-only (66.7%), format-length (66.7%)
  - [corpus]: Weak direct evidence on this specific activation hypothesis
- Break condition: Limited gains if base model lacks latent capability—method requires strong pre-trained foundation

## Foundational Learning

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: Core algorithm eliminating value function via group-normalized advantages
  - Quick check question: Can you explain how GRPO computes A_t = (R(o_i) - mean) / std without a value network?

- Concept: **Reward Engineering & Reward Hacking**
  - Why needed here: Designing length rewards requires anticipating gaming behaviors (repetition, truncation)
  - Quick check question: Why would a model generate repetitive content if rewarded for length?

- Concept: **Pass@K Evaluation**
  - Why needed here: Distinguishes knowledge (pass@64) from reliable expression (pass@1); validates activation hypothesis
  - Quick check question: If pass@64 is 94% but pass@1 is 50%, what does this suggest about training strategy?

## Architecture Onboarding

- Component map:
  - Format validator: Binary check for \boxed{} + SymPy mathematical validity
  - Length reward: Piecewise function R_l(x) with parameter p∈(0,1), x=L/L_max
  - Combined reward: R_fl = R_f + R_l if format correct; min(0, R_f + R_l) otherwise
  - GRPO trainer: G=8 responses per prompt, group-relative advantage normalization
  - Eval pipeline: vLLM greedy decoding, MARIO_EVAL extraction, temperature=0 for eval

- Critical path:
  1. Verify base model pass@N capability (prerequisite)
  2. Implement format validation (regex + SymPy)
  3. Implement length reward with configurable p
  4. Configure on-policy GRPO (train_batch_size = ppo_mini_batch_size = 128)
  5. Monitor: accuracy, response length, truncation rate, repetition

- Design tradeoffs:
  - **p parameter**: 0.5 default; p→1.0 risks truncation; p→0 risks under-exploration
  - **Format-only vs Format-Length**: Format-only simpler but plateaus; Format-Length requires tuning but sustains gains
  - **Clipping**: Authors test with clipping disabled—gains persist, confirming mechanism isn't clipping artifact

- Failure signatures:
  - Truncation >30%: Lower p or add hard length cap
  - Plateau before step 20: Likely format-only; add length signal
  - Repetition spike: Reward hacking; add repetition penalty (longest repeated substring ratio)
  - No improvement: Check base pass@N; weak base = limited gains

- First 3 experiments:
  1. **Capability check**: Measure base model pass@1 and pass@64 on MATH500/AIME to confirm latent ability
  2. **Format-only ablation**: Run 30 steps, verify early alignment with correctness baseline (~15 steps)
  3. **p-sweep**: Test p∈{0.4, 0.5, 0.6, 0.8}; monitor truncation rate, accuracy curve, final avg score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively do format and length surrogate signals transfer to complex reasoning domains beyond mathematical problem solving, such as scientific hypothesis testing or advanced programming challenges?
- Basis in paper: [explicit] Limitations section: "The evaluation of format and length as surrogate signals was predominantly focused on mathematical problem-solving, leaving open the question of their effectiveness in other complex reasoning domains, such as scientific hypothesis testing or advanced programming challenges."
- Why unresolved: The paper's experiments were confined to mathematical reasoning datasets (MATH, AIME, AMC); no evaluation was conducted on non-mathematical reasoning tasks beyond a brief OOD test (Table 5) that showed mixed results.
- What evidence would resolve it: Systematic experiments applying format-length rewards to diverse reasoning domains (code generation, scientific reasoning, logical inference) with appropriate domain-specific format definitions.

### Open Question 2
- Question: What is the minimum level of latent mathematical competence required in a base model for format-length RL to yield meaningful performance gains?
- Basis in paper: [explicit] Limitations section: "our method only works if the chosen base model already has strong latent potential on the target task; If the base model is not powerful, we expect this approach to yield limited gains."
- Why unresolved: Experiments showed varying gains across models (DeepSeek-Math-7B-Base: +13.4 points vs Qwen-Math-7B: +20.2 points), but the threshold of "sufficient latent capability" was not quantified or systematically investigated.
- What evidence would resolve it: Controlled experiments with base models at varying levels of pre-training mathematical competence, measuring the correlation between initial pass@N performance and format-length RL gains.

### Open Question 3
- Question: What are the precise mechanisms by which format-length surrogate signals approximate ground-truth reward optimization trajectories in GRPO?
- Basis in paper: [inferred] Section 4.1 asks "how can RL without explicit answer supervision match the effectiveness of traditional ground truth-based methods?" and attributes success to activating latent knowledge, but the mechanistic explanation remains at the conceptual level.
- Why unresolved: While the paper shows comparable pass@64 rates (Table 7) suggesting RL optimizes knowledge retrieval rather than acquisition, the specific learning dynamics and representation changes induced by surrogate vs. ground-truth rewards were not analyzed.
- What evidence would resolve it: Probing experiments comparing internal representations, attention patterns, and gradient trajectories between format-length and correctness-based GRPO training across training steps.

## Limitations
- Method requires strong base models with high pass@K capability; weak models yield limited gains
- Format reward depends on mathematical syntax validity, limiting generalization to non-mathematical domains
- Length reward parameter p requires empirical tuning and may not generalize across all problem types

## Confidence

**High confidence**: Format reward achieves early structural alignment (85% of gains in first 15 steps), length reward prevents truncation when properly parameterized, overall accuracy improvements on MATH500/AIME2024 are reproducible with specified methodology.

**Medium confidence**: The activation hypothesis (RL surfacing vs creating knowledge) is supported but not definitively proven; length reward parameter p=0.5 generalizes across model scales but optimal tuning remains empirical.

**Low confidence**: Claims about decoupling task-solving from formatting require stronger validation across diverse mathematical domains; the specific mechanism by which format rewards enable content refinement beyond structure is not fully characterized.

## Next Checks

1. **Base capability validation**: Measure pass@1 and pass@64 on MATH500 and AIME2024 for multiple base models (7B, 13B, 34B) before any RL training to confirm the pass@K ≫ pass@1 pattern that enables surrogate signal effectiveness.

2. **Format-only ablation across domains**: Run format-only GRPO training on non-mathematical reasoning tasks (e.g., logical deduction, code generation) to test whether early structural gains (85% within 15 steps) generalize beyond mathematical syntax.

3. **Parameter sensitivity sweep**: Systematically vary p∈{0.3, 0.5, 0.7, 0.9} across model sizes and problem types, measuring not just accuracy but also response length distribution, truncation rates, and repetition patterns to characterize the full behavioral space of the length reward mechanism.