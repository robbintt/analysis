---
ver: rpa2
title: An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource
  Management
arxiv_id: '2501.12991'
source_url: https://arxiv.org/abs/2501.12991
tags:
- marl
- offline
- online
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an offline multi-agent reinforcement learning
  (MARL) framework for radio resource management (RRM) in wireless networks. The framework
  addresses challenges in online MARL, such as safety concerns, high signaling overhead,
  and computational complexity, by using pre-collected offline datasets for training.
---

# An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource Management

## Quick Facts
- arXiv ID: 2501.12991
- Source URL: https://arxiv.org/abs/2501.12991
- Reference count: 40
- Primary result: Offline MARL with CQL achieves over 15% improvement in weighted sum/tail rate metrics compared to baseline approaches

## Executive Summary
This paper introduces an offline multi-agent reinforcement learning (MARL) framework for radio resource management (RRM) in wireless networks. The framework addresses challenges in online MARL, such as safety concerns, high signaling overhead, and computational complexity, by using pre-collected offline datasets for training. The authors propose three offline MARL variants—centralized training (C-MARL-CQL), independent training (I-MARL-CQL), and centralized training with decentralized execution (CTDE-MARL-CQL)—based on conservative Q-learning (CQL) to optimize scheduling policies for multiple access points (APs) while maximizing both sum and tail rates. The CTDE-MARL-CQL variant effectively balances computational efficiency and performance, achieving over 15% improvement in a weighted combination of sum and tail rates compared to baseline approaches.

## Method Summary
The framework formulates RRM as a cooperative multi-agent Markov game where each AP acts as an agent. The authors implement three offline MARL variants using CQL: centralized training with full joint state access, independent training with separate Q-functions, and CTDE where agents train cooperatively but execute independently. The CTDE approach uses value decomposition, summing individual Q-functions during training while maintaining decentralized execution. The reward function incorporates proportional fairness weighting to jointly optimize sum and tail rates without explicit percentile calculation. The framework is evaluated in a multi-AP multi-user scenario with synthetic datasets collected from various behavioral policies.

## Key Results
- CTDE-MARL-CQL achieves over 15% improvement in weighted combination of sum and tail rates compared to baseline approaches
- CQL regularization enables stable offline policy learning by preventing Q-value overestimation on out-of-distribution actions
- CTDE architecture achieves 98% of centralized performance while maintaining decentralized execution capability
- Dataset quality significantly impacts performance, with SAC-collected data outperforming DQN and random-walk datasets

## Why This Works (Mechanism)

### Mechanism 1: CQL Conservative Regularization
Conservative Q-learning enables stable offline policy learning by penalizing out-of-distribution actions that would otherwise cause Q-value overestimation. CQL adds a KL-divergence regularization term to the standard SAC loss, penalizing the difference between `log(Σ exp(Q(s,a)))` and `Q(s,a)`, effectively pushing learned policies toward actions present in the offline dataset. The framework requires the offline dataset to contain sufficient coverage of near-optimal trajectories, as behavioral policy quality directly determines achievable performance ceiling.

### Mechanism 2: CTDE Value Decomposition
The CTDE architecture balances centralized coordination during training with scalable decentralized execution. During training, individual agent Q-functions `Q̃ᵢ(oᵢ, aᵢ)` are summed to approximate global Q_tot(s), enabling credit assignment across agents without requiring full joint state observation at execution. Each agent independently selects actions via `argmax Q̃ᵢ(oᵢ, aᵢ)` during deployment. This approach requires the value decomposition approximation `Q_tot ≈ Σ Q̃ᵢ` to sufficiently capture inter-agent dependencies for cooperative tasks.

### Mechanism 3: Proportional Fairness Reward
Proportional fairness weighting in the reward function enables joint optimization of sum-rate and tail-rate objectives without explicit percentile calculation. The recursive weighting factor `wⱼ(t) = 1/C̃ⱼ(t)` increases priority for historically underserved users. The reward `r = Σ wⱼ^λ · Cⱼ` with λ∈[0,1] trades off throughput versus fairness, where λ=0.8 in experiments favors sum-rate while maintaining tail-rate improvement. This mechanism assumes user rate histories within an episode sufficiently predict future scheduling needs.

## Foundational Learning

- **Markov Decision Processes and Partial Observability**: The paper formulates RRM as a PO-MDP where each AP only observes local SINR and weights for its top-N users. Understanding state vs. observation distinction is critical for implementing correct input dimensions. Quick check: Can you explain why the state space size is 2×N×I while each agent's observation is only 2×N?

- **Actor-Critic Architecture with Entropy Regularization (SAC)**: The offline algorithms build on SAC rather than DQN. SAC maintains separate policy (actor) and Q-function (critic) networks with entropy bonus for exploration. Understanding the dual-loss structure is required before adding CQL regularization. Quick check: What is the purpose of the `log π(a|s)` term in the policy improvement loss (Equation 18)?

- **Offline RL Distributional Shift Problem**: Without understanding why standard RL fails on static datasets (Q-value overestimation on unseen actions), the motivation for CQL's conservative penalty term remains opaque. Quick check: Why does training an off-policy algorithm like SAC on a fixed dataset typically fail, even though SAC normally uses replay buffers?

## Architecture Onboarding

- **Component map**: Environment wrapper -> Dataset collector -> Agent networks (critic Qᵢ, policy πᵢ) -> CQL loss module -> CTDE aggregator
- **Critical path**: Verify dataset quality first—SAC-collected data outperforms DQN or baseline mixtures; implement CQL regularization term correctly (α=1.0 used); start with centralized C-MARL-CQL to establish performance upper bound before implementing CTDE
- **Design tradeoffs**: Centralized vs. CTDE vs. Independent—Centralized achieves best 5-percentile rate but requires joint state; CTDE offers 98% of centralized Rscore with decentralized execution; Independent fails to coordinate; Dataset size vs. convergence stability—<20K samples cause saddle-point convergence; >320K provides no additional gain; SAC vs. DQN backbone—SAC sacrifices sum-rate for better tail-rate and overall Rscore
- **Failure signatures**: Rscore dropping to ~1.0 (TDM baseline level) indicates insufficient dataset size or poor behavioral policy coverage; high sum-rate but near-zero 5-percentile rate suggests λ parameter too low or DQN backbone prioritizing greedy throughput; training instability with oscillating losses indicates α hyperparameter too large, over-regularizing toward behavioral policy
- **First 3 experiments**: 1) Dataset quality ablation—Train CTDE-MARL-CQL on datasets from SAC, DQN, and random-walk policies to validate data quality extraction; 2) Training paradigm comparison—Run all three variants (C-MARL, I-MARL, CTDE) on identical SAC-collected dataset to confirm CTDE achieves within 5% of centralized Rscore; 3) Lambda sensitivity analysis—Sweep λ∈{0.5, 0.6, 0.7, 0.8, 0.9} to characterize sum-rate vs. tail-rate tradeoff frontier

## Open Questions the Paper Calls Out
None identified in provided content.

## Limitations
- Framework performance critically depends on dataset quality, with significant degradation when behavioral policies don't adequately explore high-reward regions
- CTDE value decomposition assumption may break down in highly coupled scenarios where individual greedy actions harm global objectives
- PF-based reward mechanism's effectiveness relies on bounded user mobility patterns, with no validation under highly dynamic user arrivals/departures
- λ parameter requires careful tuning per deployment scenario, with no clear methodology for setting this value without extensive experimentation

## Confidence

- **High confidence**: CQL regularization mechanism and its role in preventing Q-value overestimation on out-of-distribution actions is well-established in offline RL literature
- **Medium confidence**: PF-based reward formulation's ability to jointly optimize sum and tail rates without explicit percentile calculation appears effective but lacks extensive ablation studies across diverse user mobility patterns
- **Low confidence**: Paper doesn't adequately address how framework handles non-stationary environments where user distributions change significantly between dataset collection and deployment

## Next Checks

1. **Dataset quality threshold validation**: Systematically determine minimum dataset size and behavioral policy quality required to achieve stable convergence across all three MARL variants
2. **Dynamic user environment testing**: Evaluate framework performance under highly dynamic user mobility patterns where historical rate averaging becomes stale, testing limits of PF-based reward mechanism
3. **Inter-agent coupling stress test**: Design scenarios with highly coupled agent objectives to quantify CTDE performance degradation relative to centralized approaches, identifying coupling thresholds where value decomposition fails