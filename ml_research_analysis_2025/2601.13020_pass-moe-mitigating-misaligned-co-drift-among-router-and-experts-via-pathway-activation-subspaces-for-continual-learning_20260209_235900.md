---
ver: rpa2
title: 'PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway
  Activation Subspaces for Continual Learning'
arxiv_id: '2601.13020'
source_url: https://arxiv.org/abs/2601.13020
tags:
- continual
- expert
- rank
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses catastrophic forgetting in continual instruction\
  \ tuning of multimodal large language models (MLLMs) by introducing the concept\
  \ of Pathway Activation Subspace (PASs) and a PASs-based MoE-LoRA method. The core\
  \ method idea involves using PASs\u2014a LoRA-induced subspace that reflects which\
  \ low-rank pathway directions an input activates in each expert\u2014to guide both\
  \ routing and parameter stabilization."
---

# PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning

## Quick Facts
- arXiv ID: 2601.13020
- Source URL: https://arxiv.org/abs/2601.13020
- Reference count: 16
- Key outcome: 5.1% improvement in final Average Performance (AP) and 2.15% reduction in Backward Transfer (BWT) on MLLM-CTBench

## Executive Summary
This paper introduces PASs-MoE, a method to address catastrophic forgetting in continual instruction tuning of multimodal large language models (MLLMs). The approach leverages Pathway Activation Subspaces (PASs) to guide both routing and parameter stabilization, eliminating the need for an independent router. PASs-guided Reweighting (PASs-RW) modulates expert contributions based on input activation within each expert's pathway, while PASs-aware Rank Stabilization (PASs-RS) selectively stabilizes historically important rank directions. Experiments on MLLM-CTBench show consistent improvements over conventional continual learning baselines and MoE-LoRA variants.

## Method Summary
PASs-MoE introduces the concept of Pathway Activation Subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert. PASs-guided Reweighting (PASs-RW) modulates expert contributions based on input activation within each expert's pathway, eliminating the need for an independent router. PASs-aware Rank Stabilization (PASs-RS) selectively stabilizes historically important rank directions to reduce forgetting. This approach consistently outperforms conventional continual learning baselines and MoE-LoRA variants, achieving a 5.1% improvement in final Average Performance (AP) and a 2.15% reduction in Backward Transfer (BWT), without increasing model capacity.

## Key Results
- 5.1% improvement in final Average Performance (AP) on MLLM-CTBench
- 2.15% reduction in Backward Transfer (BWT) on MLLM-CTBench
- Outperforms conventional continual learning baselines and MoE-LoRA variants

## Why This Works (Mechanism)
PASs-MoE works by using Pathway Activation Subspaces (PASs) to guide both routing and parameter stabilization. PASs-guided Reweighting (PASs-RW) modulates expert contributions based on input activation within each expert's pathway, eliminating the need for an independent router. PASs-aware Rank Stabilization (PASs-RS) selectively stabilizes historically important rank directions to reduce forgetting. This approach mitigates misaligned co-drift among router and experts, addressing catastrophic forgetting in continual instruction tuning of MLLMs.

## Foundational Learning
- **Pathway Activation Subspace (PASs)**: A LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert. *Why needed*: To guide both routing and parameter stabilization without an independent router. *Quick check*: Ensure PASs effectively capture input-specific activation patterns.
- **Catastrophic Forgetting**: The phenomenon where a model forgets previously learned tasks when trained on new tasks. *Why needed*: To address the core challenge in continual learning. *Quick check*: Verify that PASs-MoE reduces forgetting compared to baselines.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that inserts low-rank matrices into pre-trained models. *Why needed*: To enable efficient adaptation of MLLMs for continual learning. *Quick check*: Confirm that PASs-MoE maintains efficiency gains from LoRA.

## Architecture Onboarding
- **Component Map**: Input -> PASs-RW -> PASs-RS -> Output
- **Critical Path**: Input is processed through PASs-RW for expert contribution modulation, then through PASs-RS for rank stabilization, resulting in the final output.
- **Design Tradeoffs**: Eliminates the need for an independent router, reducing complexity, but requires maintaining and updating PASs across tasks.
- **Failure Signatures**: Potential overfitting to specific tasks, increased computational overhead, or failure to generalize to new tasks.
- **First Experiments**:
  1. Evaluate PASs-MoE on a broader set of continual learning benchmarks to assess generalizability.
  2. Conduct an ablation study isolating the contributions of PASs-RW and PASs-RS.
  3. Analyze the computational overhead introduced by PASs-MoE.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on empirical validation without theoretical grounding for why PASs effectively mitigate misaligned co-drift.
- Absence of comparisons against task-incremental or domain-incremental baselines.
- Lack of exploration of computational overhead for very large or rapidly changing task sequences.

## Confidence
- **High confidence**: Experimental setup and reported improvements on MLLM-CTBench.
- **Medium confidence**: General applicability of PASs-MoE to other continual learning scenarios.
- **Low confidence**: Scalability of PASs-MoE for very large or rapidly changing task sequences.

## Next Checks
1. Evaluate PASs-MoE on a broader set of continual learning benchmarks, including task- and domain-incremental settings, to assess generalizability.
2. Conduct an ablation study isolating the contributions of PASs-RW and PASs-RS to quantify their individual impact on mitigating forgetting.
3. Analyze the computational overhead introduced by PASs-MoE, particularly in terms of memory and inference time, to ensure practical viability.