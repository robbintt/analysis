---
ver: rpa2
title: Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning
arxiv_id: '2501.14315'
source_url: https://arxiv.org/abs/2501.14315
tags:
- training
- data
- task
- performance
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in LLM fine-tuning
  by showing that training on LLM-generated data (Self-Output, Rephrase) reduces degradation
  on non-target tasks compared to ground truth data. The key insight is that LLM-generated
  responses have lower token-level perplexity, leading to less weight update and better
  preservation of general capabilities.
---

# Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning

## Quick Facts
- arXiv ID: 2501.14315
- Source URL: https://arxiv.org/abs/2501.14315
- Authors: Chao-Chung Wu; Zhi Rui Tam; Chieh-Yen Lin; Yun-Nung Chen; Shao-Hua Sun; Hung-yi Lee
- Reference count: 40
- Primary result: STM reduces catastrophic forgetting to near zero while maintaining target task improvement by masking high-perplexity tokens

## Executive Summary
This paper addresses catastrophic forgetting in LLM fine-tuning by showing that training on LLM-generated data (Self-Output, Rephrase) reduces degradation on non-target tasks compared to ground truth data. The key insight is that LLM-generated responses have lower token-level perplexity, leading to less weight update and better preservation of general capabilities. The proposed Selective Token Masking (STM) method masks high-perplexity tokens in ground truth training data, achieving comparable non-target task performance preservation. Experiments across multiple model families and scales show STM reduces BWT (backward transfer) to near zero while maintaining target task improvement. The optimal masking threshold is around 20-24% of high-perplexity tokens. STM also generalizes to different fine-tuning techniques (LoRA, DoRA, full-weight fine-tuning) and performs better than regularization approaches at similar weight update magnitudes.

## Method Summary
The method computes token-level perplexity using the pre-fine-tuning model and masks tokens exceeding a threshold τ≈2.5 (filtering ~20-24% of tokens). During fine-tuning, masked tokens are excluded from the loss calculation. This reduces weight updates on high-perplexity tokens that would otherwise cause interference with non-target task capabilities. The approach is compared against baseline fine-tuning, synthetic data generation (Self-Output), and regularization methods. STM is evaluated on target tasks MBPP and MATH with non-target evaluation on GSM8K, ARC-Challenge, and BIRD across models including Gemma 2 IT 2B, Llama 3 8B Instruct, Mistral 7B Instruct, Gemma 2 IT 9B, and OLMo 2 7B Instruct.

## Key Results
- STM achieves BWT≈0% while maintaining target task improvement, comparable to synthetic data generation methods
- Optimal masking threshold τ=2.5 filters 23.8% of tokens and minimizes forgetting
- STM produces the smallest L2 norm of weight changes (0.45-0.55) compared to ground truth (5.69-17.75)
- STM generalizes across fine-tuning methods (LoRA, DoRA, full-weight) and model families
- STM outperforms regularization methods at similar weight update magnitudes

## Why This Works (Mechanism)

### Mechanism 1
Low-perplexity training data induces smaller weight updates, preserving pre-trained knowledge. Token-level perplexity correlates with gradient magnitude during backpropagation. Low-perplexity tokens produce small loss values and thus small parameter updates. High-perplexity tokens drive larger weight changes that can overwrite existing representations. Evidence: STM produces smallest L2 norm of ΔW (0.45-0.55) compared to Ground Truth (5.69-17.75); STM and Self-Output converge with much lower training loss than Ground Truth.

### Mechanism 2
High-perplexity tokens in ground truth data are a primary driver of non-target task degradation. Ground truth responses contain "perplexity spikes" reflecting natural human variation. These unexpected tokens force the model to adjust parameters along more dimensions (higher effective rank in LoRA updates), increasing interference with unrelated capabilities stored in shared parameters. Evidence: Ground Truth exhibits frequent perplexity spikes (6-8) while Self-Output rarely exceeds 2; variance in first 20 tokens: Ground Truth 2.04, Self-Output 0.04.

### Mechanism 3
Selective Token Masking (STM) at 20-24% threshold approximates the benefits of synthetic data generation without its computational cost. By pre-computing token perplexities with the instruction-tuned model and masking tokens above threshold τ≈2.5, STM removes the subset of tokens most responsible for weight perturbations while retaining task-relevant content. Evidence: STM achieves similar BWT to Self-Output "in a streamlined single-stage workflow... avoiding the need to train or maintain any additional models."

## Foundational Learning

- **Token-level perplexity**: Measuring individual token probabilities under the model. Why needed: STM requires computing perplexity per token to identify masking candidates; sequence-level perplexity averages obscure the distribution. Quick check: Given a token with probability 0.1 under the model, what is its perplexity contribution? (Answer: exp(−log(0.1)) = 10)

- **Catastrophic forgetting / Backward Transfer (BWT)**: Measuring degradation on non-target tasks after fine-tuning. Why needed: The paper measures success via BWT reduction; understanding this metric is essential for interpreting results. Quick check: If a model improves 10% on task A but degrades 5% on task B after fine-tuning on A, what is the BWT for the two-task setting? (Answer: −5%, the average degradation on non-target tasks)

- **LoRA weight decomposition (ΔW = BA)**: Understanding low-rank updates in fine-tuning. Why needed: The paper analyzes weight update magnitude via L2 norm of ΔW; understanding LoRA's low-rank structure clarifies why small updates matter. Quick check: Why might low-rank updates be more easily reversible or less interfering than full-rank updates? (Answer: Low-rank updates modify a constrained subspace, potentially leaving more of the original representation intact.)

## Architecture Onboarding

- **Component map**: Perplexity Calculator -> Threshold Selector -> Token Masker -> Fine-tuning Loop -> Evaluator

- **Critical path**:
  1. Load pre-trained instruction-tuned model M
  2. For each training sample, compute token perplexities using M
  3. Mask tokens where perplexity > τ (or top-k% highest)
  4. Fine-tune M on masked data
  5. Evaluate on target and non-target benchmarks

- **Design tradeoffs**:
  - **Threshold τ**: Lower values mask more tokens (better BWT, risk of lower TI); higher values mask fewer (better TI, worse BWT). Paper finds τ≈2.5 optimal.
  - **Perplexity model**: Using the same model for perplexity and fine-tuning outperforms cross-scale filtering.
  - **Masking strategy**: Replace with `[MASK]` vs. exclude from loss; paper uses exclusion (tokens contribute zero to loss).

- **Failure signatures**:
  - **Over-masking (>30%)**: TI drops significantly; model learns insufficient task-specific content.
  - **Under-masking (<10%)**: BWT remains high (similar to baseline fine-tuning).
  - **Cross-scale perplexity**: Using larger model for perplexity calculation can mask wrong tokens (larger models assign lower perplexity to tokens smaller models find challenging).
  - **Random masking**: Random 25% masking produces BWT of −8.6% vs. +0.4% for high-perplexity masking, confirming high-perplexity tokens specifically drive forgetting.

- **First 3 experiments**:
  1. **Threshold sweep on held-out validation**: Run STM with τ∈{1.5, 2.5, 10, 25, 1000} on a small target task subset; plot BWT vs. TI to find the operating point nearest BWT≈0 with positive TI. Verify 20-24% masking rate aligns with τ≈2.5.
  2. **Baseline comparison**: Compare (a) baseline fine-tuning on ground truth, (b) Self-Output training (if feasible), (c) STM on ground truth. Confirm STM achieves BWT within 2-3% of Self-Output with comparable TI.
  3. **Cross-dataset generalization**: Train STM on one domain (e.g., MBPP code) and evaluate non-target tasks across different domains (math, reasoning, knowledge). Verify BWT remains near zero across diverse non-target tasks.

## Open Questions the Paper Calls Out
- Does STM generalize to specialized domains (e.g., medicine, finance) and models significantly larger than 10B parameters? (The authors list lack of experiments on models larger than 10B and specific domains as explicit limitations.)
- Can an adaptive or learnable masking mechanism outperform the fixed perplexity threshold used in current STM implementations? (Section 9 lists "a smarter masking mechanism for perplexity threshold decision" as a limitation.)
- How does the robustness of STM change in curriculum learning scenarios where task difficulty scales progressively? (The authors explicitly state they "did not consider how perplexity changes when task difficulty scales up.")

## Limitations
- Optimal threshold τ=2.5 and 20-24% masking rate may not generalize beyond tested datasets and model families
- High-perplexity tokens may contain critical task-specific information rather than being purely detrimental
- Direct comparison between STM and Self-Output is complicated by different computational budgets

## Confidence
- **High Confidence**: STM achieves near-zero BWT on non-target tasks when properly tuned; high-perplexity tokens correlate with larger weight updates; perplexity masking mechanism works across different fine-tuning methods
- **Medium Confidence**: τ=2.5 is the optimal threshold across all settings; STM generalizes equally well to different model scales and architectures; 20-24% masking rate represents an ideal balance point
- **Low Confidence**: STM's superiority over regularization methods at equal L2 norms implies the mechanism is exclusively about perplexity, not weight magnitude; cross-scale perplexity filtering will consistently underperform; STM will generalize to domains with highly specialized vocabulary without parameter adjustment

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically sweep τ from 1.0 to 5.0 on held-out validation data for multiple target tasks, measuring both TI and BWT. Verify that the 20-24% masking rate consistently corresponds to optimal BWT≈0 across different domains.

2. **Masked Token Content Analysis**: Extract and analyze the actual tokens masked at τ=2.5 across multiple datasets. Classify masked tokens by part-of-speech and semantic role (task-critical vs. filler). Test whether targeted masking of specific token categories (e.g., only nouns, only adjectives) achieves similar BWT preservation.

3. **Cross-Domain Generalization**: Apply STM to fine-tune models on datasets from vastly different domains (e.g., medical Q&A, legal document classification, creative writing) and evaluate non-target task performance on completely unrelated benchmarks. Measure whether BWT remains near zero when source and target domains have minimal overlap.