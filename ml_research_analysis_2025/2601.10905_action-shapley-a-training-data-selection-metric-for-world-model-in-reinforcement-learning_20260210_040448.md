---
ver: rpa2
title: 'Action Shapley: A Training Data Selection Metric for World Model in Reinforcement
  Learning'
arxiv_id: '2601.10905'
source_url: https://arxiv.org/abs/2601.10905
tags:
- data
- shapley
- action
- training
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Action Shapley, a metric for selecting high-quality
  training data for world models in reinforcement learning. The method addresses the
  challenge of data selection when direct interactions with the real environment are
  costly or impractical.
---

# Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.10905
- **Source URL:** https://arxiv.org/abs/2601.10905
- **Reference count:** 20
- **Primary result:** Action Shapley outperforms ad-hoc data selection methods in five RL case studies, achieving over 80% reduction in computation time.

## Executive Summary
This paper introduces Action Shapley, a metric for selecting high-quality training data for world models in reinforcement learning. The method addresses the challenge of data selection when direct interactions with the real environment are costly or impractical. Action Shapley measures the marginal contribution of each training data point to the overall performance of the RL agent, inspired by Shapley values from cooperative game theory. To address computational complexity, the authors propose a randomized dynamic algorithm that significantly reduces computation time (over 80% improvement). The metric is validated across five real-world case studies including VM right-sizing, load balancing, database tuning, Kubernetes management, and data center cooling. The results show that the Action Shapley-based training data selection policy consistently outperforms ad-hoc selection methods, leading to better-performing RL agents with fewer training data points.

## Method Summary
Action Shapley calculates the marginal contribution of each training data point to the RL agent's cumulative reward using a modified Shapley value approach. The method employs a randomized dynamic algorithm that traverses the power set of training data in descending order of cardinality, using failure memoization to identify a cut-off cardinality where further computation can be terminated. The algorithm wraps the entire training and evaluation pipeline to compute the marginal value of data points, allowing for efficient selection of high-contribution data. The selected data is used to train a world model (RBF network with autoencoder pre-training) which is then used by an RL agent (SAC-PID or PPO-PID) to optimize policies. The method is validated on five case studies involving time-series data from infrastructure management tasks.

## Key Results
- Action Shapley-based training data selection policy consistently outperforms ad-hoc selection methods across five case studies
- The randomized dynamic algorithm achieves over 80% reduction in computation time compared to naive Shapley value calculation
- The method successfully identifies "indispensable" data points that are critical for agent learning and "dispensable" points that can be excluded without performance degradation
- Performance improvements were demonstrated in VM right-sizing, load balancing, database tuning, Kubernetes management, and data center cooling applications

## Why This Works (Mechanism)

### Mechanism 1
Data selection quality is improved by evaluating the marginal contribution of individual data points to the cumulative reward of the RL agent, rather than relying on data volume or random selection. This approach adapts Shapley values from cooperative game theory to RL data valuation by calculating a value $\phi_k$ for a training data point $k$ by averaging its marginal utility across all possible subsets of the training data. This isolates the specific causal impact of a data point on the final policy performance. The core assumption is that the valuation function (cumulative reward) is a reliable proxy for the "utility" of the world model, and the relationships between data points and rewards adhere to properties of linearity and symmetry required for Shapley values.

### Mechanism 2
The exponential computational cost of calculating exact Shapley values can be mitigated using a randomized dynamic algorithm that exploits a "cut-off cardinality." The algorithm traverses the power set of training data in descending order of cardinality and monitors a "failure memoization" counter. If the RL agent fails to learn on a subset a specific number of times, the algorithm identifies a cut-off cardinality and terminates, avoiding the traversal of smaller, insufficient subsets. The core assumption is that there exists a minimum dataset size required for the RL agent to succeed, and failure to learn is a consistent signal of data insufficiency rather than algorithmic noise.

### Mechanism 3
A training dataset curated to maximize the average Action Shapley value produces a higher-performing agent than ad-hoc or "use-all" baselines. The policy selects the top data points with the highest individual Action Shapley values, filtering out low-contribution or detrimental data while retaining critical points. This selection is based on the assumption that data points that are individually high-value remain high-value when aggregated, and excluding low-value points does not remove critical context needed for world model stability.

## Foundational Learning

### Concept: Model-Based Reinforcement Learning (MBRL)
**Why needed here:** Action Shapley specifically targets the World Model in MBRL. You must understand that the agent learns by simulating the environment inside this model, making the quality of the training data for the world model the critical bottleneck.
**Quick check question:** How does a world model differ from a Q-function in standard RL?

### Concept: Shapley Values (Game Theory)
**Why needed here:** The entire metric is built on this concept. You need to understand it as a method for "fairly" distributing a total payoff (reward) among players (data points) based on their marginal contributions in different coalitions.
**Quick check question:** Why does the Shapley value require averaging over all permutations of the data set?

### Concept: The "Deadly Triad" in RL
**Why needed here:** The paper explicitly mentions this as a source of instability in RL. Understanding that combining function approximation, bootstrapping, and off-policy learning can diverge helps explain why high-quality data selection is vital.
**Quick check question:** Which three components make up the "deadly triad," and why does data sparsity exacerbate the risk of divergence?

## Architecture Onboarding

### Component map:
Data Store -> Action Shapley Calculator -> World Model Trainer -> RL Policy Optimizer -> Evaluator

### Critical path:
The Action Shapley Calculator is the bottleneck. It wraps the entire training and evaluation pipeline in a loop to compute the marginal value of data points.

### Design tradeoffs:
Accuracy vs. Compute: Setting the error bound $\epsilon$ in Algorithm 1. A lower $\epsilon$ reduces computation time but might prune the search space too aggressively, missing optimal data combinations. Indispensability vs. Noise: The system treats "RL failure" as a signal for cut-off. If the underlying RL algorithm is unstable, the Shapley calculation might conflate "bad data" with "unlucky initialization."

### Failure signatures:
"Indispensable" Flag: If a data point is flagged as "Indispensable," the system implies the agent cannot learn at all without it. If too many points are indispensable, the selection metric loses its discriminatory power. Negative Shapley Values: Data points with negative values actively degrade performance and must be excluded from the training set.

### First 3 experiments:
1. Sanity Check (Synthetic): Create a dataset with a known "golden" subset. Verify Action Shapley selects that subset over random noise.
2. Ablation on $\epsilon$: Run Algorithm 1 on one case study with varying $\epsilon$. Plot the "Computational Efficiency Improvement" vs. the "Cumulative Reward" of the resulting agent to find the optimal balance.
3. Baseline Comparison: Replicate the "Use-All" vs. "Action Shapley" experiment for a new target environment to verify generalization beyond the 5 provided case studies.

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational efficiency of the randomized dynamic algorithm scale to datasets with significantly higher cardinality than the 5â€“15 points tested? The authors claim an 80% improvement, but the empirical validation is restricted to small datasets where $n$ is between 5 and 15. The algorithm's complexity relies on a "cut-off cardinality" ($\theta$) and parameter $\epsilon$. It is unclear if the early termination conditions hold effectively for large-scale datasets common in deep RL.

### Open Question 2
How robust is the Action Shapley metric to variance in the valuation function $U$ caused by the intrinsic instability of RL training? The paper acknowledges the "deadly triad" and RL stability problems but defines Action Shapley based on a single valuation function (cumulative reward), which can be noisy. If the underlying RL agent produces high-variance rewards for the same data subset, the Shapley value calculation may become unreliable or inconsistent.

### Open Question 3
Does the data selection policy remain effective when applied to more complex, non-linear world model architectures like Transformers or deep ensemble models? The authors claim the metric is "agnostic," but implementation details and results rely exclusively on Radial Basis Function (RBF) networks. Complex function approximators have different sample efficiencies and interpolation behaviors compared to RBFs, potentially altering the "marginal contribution" of specific data points.

## Limitations
- The core evaluation relies on five proprietary case studies with undisclosed industrial data, limiting generalization to other domains
- Despite the proposed randomized algorithm, Action Shapley remains expensive for large datasets
- The RBF network with autoencoder world model may not generalize to non-time-series or high-dimensional state spaces common in other RL domains

## Confidence

### High Confidence:
- The theoretical foundation connecting Shapley values to data selection is sound, drawing from established cooperative game theory
- The randomized algorithm's complexity reduction is mathematically justified (Proposition 1)

### Medium Confidence:
- Empirical results across five case studies demonstrate consistent performance improvements over baselines
- However, the proprietary nature of data and lack of ablation studies on hyperparameter sensitivity limit reproducibility

### Low Confidence:
- The claim that Action Shapley generalizes to non-time-series RL problems lacks validation
- The method's robustness to reward function design and environmental stochasticity remains unexplored

## Next Checks
1. **Synthetic Dataset Validation:** Create controlled synthetic environments where ground truth optimal data subsets are known. Verify Action Shapley correctly identifies these subsets versus random selection, establishing baseline correctness.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary $\epsilon$ (failure memoization threshold) and PID controller gains across the five case studies. Quantify the tradeoff between computational efficiency improvement and agent performance degradation.

3. **Cross-Domain Transfer Test:** Apply Action Shapley to a standard RL benchmark (e.g., MuJoCo continuous control tasks) with publicly available datasets. Compare performance against domain-specific selection methods to assess generalization capability.