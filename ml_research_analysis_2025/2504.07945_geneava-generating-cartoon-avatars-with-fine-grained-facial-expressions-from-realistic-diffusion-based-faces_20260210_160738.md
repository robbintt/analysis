---
ver: rpa2
title: 'GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from
  Realistic Diffusion-based Faces'
arxiv_id: '2504.07945'
source_url: https://arxiv.org/abs/2504.07945
tags:
- facial
- expression
- cartoon
- images
- expressions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel framework, GenEAva, for generating
  expressive cartoon avatars with fine-grained facial expressions. They fine-tune
  a state-of-the-art text-to-image diffusion model on 135 facial expression categories
  to generate high-quality realistic faces.
---

# GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces

## Quick Facts
- arXiv ID: 2504.07945
- Source URL: https://arxiv.org/abs/2504.07945
- Authors: Hao Yu; Rupayan Mallick; Margrit Betke; Sarah Adel Bargal
- Reference count: 40
- Key outcome: A novel framework generating expressive cartoon avatars from fine-grained facial expressions, outperforming SDXL across multiple evaluation metrics.

## Executive Summary
GenEAva introduces a framework for generating expressive cartoon avatars with fine-grained facial expressions by fine-tuning a text-to-image diffusion model on 135 facial expression categories. The method generates high-quality realistic faces which are then stylized into cartoon avatars. The resulting GenEAva 1.0 dataset contains 13,230 avatars with balanced demographic distributions and demonstrates superior performance over SDXL across multiple evaluation metrics. User studies confirm high satisfaction with both expression preservation (96%) and identity preservation (93%).

## Method Summary
The GenEAva framework operates in two main stages: first, fine-tuning a state-of-the-art text-to-image diffusion model on 135 facial expression categories to generate realistic faces; second, applying a stylization model to convert these realistic faces into cartoon avatars. The framework introduces GenEAva 1.0, a novel dataset containing 13,230 cartoon avatars with fine-grained facial expressions and balanced gender, racial, and age distributions. The approach demonstrates superior performance over baseline models through comprehensive evaluation using CLIP, DINO, LPIPS metrics, and user studies confirming effective identity and expression preservation.

## Key Results
- Fine-tuned model outperforms SDXL across all evaluation metrics (CLIP, DINO, LPIPS, and expression error)
- No identity memorization detected through quantitative analysis and user studies
- Stylization method preserves identity and expressions with 96% approval for expression preservation and 93% for identity preservation

## Why This Works (Mechanism)
The framework leverages fine-grained expression control through specialized training on 135 facial expression categories, enabling precise control over avatar expressions. The two-stage approach (realistic generation followed by stylization) allows each component to specialize in its task, resulting in high-quality outputs. The balanced dataset ensures diverse and representative avatar generation across demographic groups.

## Foundational Learning

**Facial Expression Recognition**: Understanding 135 fine-grained expression categories is crucial for generating nuanced avatars. Quick check: Verify the expression taxonomy covers the intended range of emotions and micro-expressions.

**Diffusion Models**: The text-to-image diffusion model forms the backbone for generating realistic faces. Quick check: Ensure the model architecture supports fine-tuning on specialized expression categories.

**Style Transfer**: Converting realistic faces to cartoon avatars while preserving identity requires effective stylization techniques. Quick check: Validate that the stylization preserves key facial features and expressions.

## Architecture Onboarding

**Component Map**: Text Prompt -> Fine-tuned Diffusion Model -> Realistic Face -> Stylization Model -> Cartoon Avatar

**Critical Path**: The pipeline follows a sequential process where expression-controlled realistic face generation must succeed before stylization can produce the final cartoon avatar.

**Design Tradeoffs**: The two-stage approach trades computational efficiency for quality and specialization, allowing each component to excel at its specific task rather than forcing a single model to handle both realistic generation and stylization.

**Failure Signatures**: Common failure modes include loss of expression detail during stylization, identity distortion when converting to cartoon style, and generation of unrealistic facial features due to insufficient fine-tuning data.

**First Experiments**:
1. Generate a test set of realistic faces with specific expression prompts to verify fine-tuning effectiveness
2. Apply stylization to baseline faces to establish baseline cartoon avatar quality
3. Evaluate expression preservation by comparing input expressions to stylized outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on effectiveness of fine-tuning and stylization, lacking extensive analysis of novel avatar generation beyond predefined expressions
- Dataset diversity claims lack detailed methodology for ensuring balanced gender, racial, and age distributions
- Limited elaboration on methods used to verify absence of identity memorization from fine-tuning data

## Confidence
High: Claims about model superiority and stylization effectiveness are well-supported by quantitative metrics and user studies
Medium: Dataset diversity claims require more detailed validation methodology
Medium: Identity memorization absence claims need more rigorous verification methods

## Next Checks
1. Conduct extensive diversity analysis to verify balanced gender, racial, and age distributions in GenEAva 1.0 dataset
2. Study model's ability to generate novel and diverse avatars beyond the 135 predefined facial expressions
3. Implement additional identity detection tests to ensure fine-tuned model doesn't replicate specific identities from training data