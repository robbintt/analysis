---
ver: rpa2
title: Structure-Preserving Nonlinear Sufficient Dimension Reduction for Tensors
arxiv_id: '2512.20057'
source_url: https://arxiv.org/abs/2512.20057
tags:
- tensor
- linear
- dimension
- nonlinear
- reduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two nonlinear sufficient dimension reduction
  (SDR) methods for tensor-valued predictors that preserve tensor structure and substantially
  reduce dimensionality. The proposed Tucker-form and CP-form NTSDR methods extend
  linear tensor SDR by incorporating nonlinear representations through reproducing
  kernel Hilbert spaces defined on tensor singular vectors.
---

# Structure-Preserving Nonlinear Sufficient Dimension Reduction for Tensors

## Quick Facts
- **arXiv ID:** 2512.20057
- **Source URL:** https://arxiv.org/abs/2512.20057
- **Reference count:** 9
- **Primary result:** Introduces nonlinear sufficient dimension reduction methods (Tucker-form and CP-form NTSDR) for tensor-valued predictors that preserve tensor structure and outperform existing methods in simulations and image quality assessment

## Executive Summary
This paper proposes two nonlinear sufficient dimension reduction (SDR) methods for tensor-valued predictors: Tucker-form NTSDR and CP-form NTSDR. These methods extend linear tensor SDR by incorporating nonlinear representations through reproducing kernel Hilbert spaces defined on tensor singular vectors. The methods preserve tensor structure while achieving substantial dimensionality reduction, maintaining model parsimony and interpretability of tensor modes.

The authors establish theoretical guarantees including Fisher consistency at the population level and consistency and convergence rates for the CP-form method. Empirical results show NTSDR methods consistently outperform existing approaches (GSIR and KSIR) across four simulation settings with distance correlations between 0.609-0.876 compared to 0.313-0.797. In a real application to image quality assessment, CP-NTSDR achieves Pearson correlations of 0.407-0.935 with ground-truth DMOS values, outperforming GSIR on 24 of 30 images.

## Method Summary
The proposed methods extend linear tensor SDR to nonlinear settings by incorporating kernel-based representations. Tucker-form NTSDR generalizes dimension folding to the nonlinear case, while CP-form NTSDR extends linear tensor SDR frameworks. Both methods use reproducing kernel Hilbert spaces defined on tensor singular vectors to capture nonlinear relationships. Sample-level implementation uses iterative least-squares steps for Tucker-form and singular value decompositions for CP-form, enabling practical computation while preserving the tensor structure and achieving model parsimony.

## Key Results
- NTSDR methods achieve distance correlations of 0.609-0.876 in simulations compared to 0.313-0.797 for GSIR and KSIR
- Methods successfully recover true sufficient predictors with distance correlations typically above 0.8
- In image quality assessment, CP-NTSDR achieves Pearson correlations of 0.407-0.935 with ground-truth DMOS, outperforming GSIR on 24 of 30 images

## Why This Works (Mechanism)
The methods work by extending linear tensor SDR frameworks to nonlinear settings through kernel representations on tensor singular vectors. By defining reproducing kernel Hilbert spaces on these singular vectors, the methods can capture complex nonlinear relationships while preserving the tensor structure. The Tucker-form approach generalizes dimension folding to nonlinear cases, while the CP-form approach extends linear tensor SDR to handle nonlinear dependencies. This combination allows for substantial dimensionality reduction while maintaining interpretability of tensor modes and achieving model parsimony.

## Foundational Learning

1. **Sufficient Dimension Reduction (SDR)**
   - *Why needed:* Reduces predictor dimensionality while preserving information about the response variable
   - *Quick check:* Can be verified through sufficient dimension reduction property: E(Y|X) = E(Y|B'X) where B contains sufficient predictors

2. **Tensor Singular Value Decomposition (t-SVD)**
   - *Why needed:* Decomposes tensors into orthogonal components for dimension reduction
   - *Quick check:* t-SVD factorization should satisfy X = U*S*V^H where U, V are orthogonal and S is diagonal

3. **Reproducing Kernel Hilbert Spaces (RKHS)**
   - *Why needed:* Enables nonlinear function representation while preserving tensor structure
   - *Quick check:* Kernel trick should allow inner products in feature space without explicit mapping

4. **Fisher Consistency**
   - *Why needed:* Guarantees population-level correctness of the dimension reduction
   - *Quick check:* At population level, the method should recover the true sufficient predictors

## Architecture Onboarding

**Component map:** Data → Tensor SVD → Kernel RKHS → NTSDR Transformation → Reduced Dimension Predictors

**Critical path:** Tensor decomposition → kernel mapping → nonlinear SDR estimation → sufficient predictor extraction

**Design tradeoffs:** Linear SDR preserves tensor structure with lower computational cost but cannot capture nonlinear relationships; NTSDR methods capture nonlinear relationships while preserving structure but require kernel parameter selection and higher computational cost

**Failure signatures:** Poor kernel choice leading to underfitting or overfitting; tensor decomposition instability with high-dimensional or noisy data; convergence issues in iterative optimization

**3 first experiments:**
1. Verify tensor decomposition recovers known factors on synthetic tensor data
2. Test kernel choice sensitivity by comparing different kernels on simple nonlinear relationships
3. Validate sufficient dimension reduction property by checking E(Y|X) ≈ E(Y|B'X) on simulated data

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation study limited to four settings with small sample sizes (n=300 or 500) and modest tensor dimensions (10x10x3)
- Real-data application based on single image quality assessment dataset with only 30 images
- Comparison limited to GSIR and KSIR methods without benchmarking against other nonlinear SDR approaches
- Computational complexity for high-dimensional tensors not thoroughly discussed

## Confidence

| Claim | Confidence |
|-------|------------|
| Mathematical formulation and theoretical guarantees | High |
| Simulation results showing improved performance | Medium |
| Real-data application results | Medium |

## Next Checks

1. Test NTSDR methods on larger tensor dimensions (e.g., 100x100x10) and larger sample sizes (n > 1000) to evaluate scalability and performance in realistic settings

2. Compare NTSDR methods against other state-of-the-art nonlinear SDR approaches (kernel-based methods, deep learning-based SDR) on both simulated and real tensor datasets

3. Conduct sensitivity analyses to assess impact of different kernel choices, tuning parameters, and initialization strategies on stability and reproducibility of NTSDR results