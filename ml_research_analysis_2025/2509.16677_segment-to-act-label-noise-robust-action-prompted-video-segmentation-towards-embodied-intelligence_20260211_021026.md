---
ver: rpa2
title: 'Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards
  Embodied Intelligence'
arxiv_id: '2509.16677'
source_url: https://arxiv.org/abs/2509.16677
tags:
- noise
- object
- video
- segmentation
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ActiSeg-NL, the first benchmark for studying\
  \ label-noise robustness in action-based video object segmentation. The benchmark\
  \ simulates two types of noise\u2014textual prompt noise (category flips and noun\
  \ substitutions) and mask annotation noise (boundary perturbations)\u2014and evaluates\
  \ six label-noise learning strategies across clean, single-noise, and mixed conditions."
---

# Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence

## Quick Facts
- **arXiv ID:** 2509.16677
- **Source URL:** https://arxiv.org/abs/2509.16677
- **Reference count:** 36
- **Key outcome:** This work introduces ActiSeg-NL, the first benchmark for studying label-noise robustness in action-based video object segmentation. The benchmark simulates two types of noise—textual prompt noise (category flips and noun substitutions) and mask annotation noise (boundary perturbations)—and evaluates six label-noise learning strategies across clean, single-noise, and mixed conditions. Results show that noise significantly degrades performance, especially from masks, with distinct trade-offs: some methods preserve foregrounds under text noise (Co-teaching), others improve overlap (APL), and pixel-wise robust losses (GCE, SCE) achieve balanced performance in mixed conditions. A proposed Parallel Mask Head Mechanism (PMHM) further mitigates boundary noise. The study highlights the need for multimodal, partitioned metrics and boundary-focused robustness for embodied perception.

## Executive Summary
This paper tackles label-noise robustness in action-based video object segmentation (ActionVOS), a critical component for embodied perception in robotics. The authors introduce ActiSeg-NL, a benchmark simulating two types of noise—textual prompt noise (category flips, noun substitutions) and mask annotation noise (boundary perturbations)—and evaluate six label-noise learning strategies. Results reveal that text noise primarily impacts object identity, while mask noise degrades boundary quality, with pixel-wise robust losses (GCE, SCE) and Co-teaching providing complementary strengths under mixed conditions. A novel Parallel Mask Head Mechanism (PMHM) is also proposed to specifically address boundary noise. The study emphasizes the need for multimodal, partitioned metrics and boundary-focused robustness for real-world robotics applications.

## Method Summary
The authors simulate label noise in action-based video object segmentation (ActionVOS) by corrupting both textual prompts and segmentation masks. Textual noise is introduced via category flips and noun substitutions at varying rates (20%, 40%, 60%), while mask noise is simulated using morphological dilation with kernel sizes (9, 15, 21) to perturb boundaries. Six label-noise learning strategies are evaluated: sample selection (Co-teaching), robust losses (GCE, SCE, APL), and loss correction (ELR, NPN). Additionally, a Parallel Mask Head Mechanism (PMHM) is proposed to mitigate boundary noise by enforcing consistency between main and auxiliary segmentation heads on uncertain pixels. All methods are trained on VISOR and evaluated on a clean 294-clip ActionVOS benchmark using partitioned metrics (p-mIoU, n-mIoU, etc.) to separately assess foreground and background performance.

## Key Results
- Mask annotation noise (dilation) degrades p-mIoU by up to 18% and increases n-mIoU by 5-6%, with p-mIoU and p-cIoU decreasing as kernel size grows.
- Textual prompt noise (category flips) reduces p-mIoU by up to 9% and increases n-mIoU by 2-3%, with errors concentrated at object boundaries.
- Under mixed noise, pixel-wise robust losses (GCE, SCE, APL) achieve balanced performance, while Co-teaching preserves foregrounds but loses background precision.
- PMHM effectively mitigates mask noise, improving gIoU by 3-4% over baseline, but offers limited benefit under severe text noise.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Co-teaching preserves foreground regions under text prompt noise by filtering instance-level grounding errors.
- **Mechanism:** Two networks exchange small-loss samples at each epoch. Samples with loss rank ≤ 0.95 are retained; each network supervises its peer using only these "clean" samples, progressively excluding mismatched prompts.
- **Core assumption:** Text noise (category flips, noun substitutions) corrupts at the *instance level*—entire prompts are wrong—so sample selection can isolate clean examples.
- **Evidence anchors:**
  - [abstract] "some methods preserve foregrounds under text noise (Co-teaching)"
  - [Section IV.A] "Co-teaching suppresses heavy prompt mismatch by peer selection of small loss samples between two networks"
  - [Table IIIa-c] Co-teaching maintains highest p-mIoU (63.2→60.2) across Name-20/40/60
- **Break condition:** Fails under dense pixel-level mask corruption where many edge pixels are systematically wrong—sample filtering cannot exclude localized boundary errors.

### Mechanism 2
- **Claim:** Pixel-wise robust losses (GCE, SCE, APL) achieve balanced performance under mixed noise by dampening per-pixel label errors without discarding entire samples.
- **Mechanism:** GCE interpolates between CE and MAE to reduce outlier impact; SCE balances forward and reverse CE for partial corruption; APL strengthens confident decisions while passively penalizing ambiguous pixels.
- **Core assumption:** Boundary dilation creates *dense, localized* noise that cannot be filtered at sample level but can be down-weighted pixel-wise.
- **Evidence anchors:**
  - [abstract] "pixel-wise robust losses (GCE, SCE) achieve balanced performance in mixed conditions"
  - [Section V.E] "losses that damp per-pixel label noise (GCE, SCE, APL)...tend to rank higher on gIoU" under mixed noise
  - [Table V] GCE achieves highest gIoU (61.1) and p-mIoU (38.7) under Name-60 + Mask-21
- **Break condition:** If foreground and background are corrupted asymmetrically, symmetric losses may under-correct; tuning α, β, q per-dataset is often required.

### Mechanism 3
- **Claim:** PMHM mitigates mask annotation noise by enforcing prediction consistency between main and auxiliary heads on uncertain boundary pixels.
- **Mechanism:** A lightweight auxiliary head shares decoder features but receives perturbations (dropout, freezing). Uncertain pixels U_t are identified near decision boundaries (|p - 0.5| < τ_m) or high gradients (∥∇p∥₂ > τ_e). Symmetric KL divergence aligns both heads on U_t only; auxiliary head is discarded at inference.
- **Core assumption:** Noisy boundary supervision creates disagreement between diverse decoders; consistency on uncertain regions regularizes toward cleaner boundaries.
- **Evidence anchors:**
  - [abstract] "A proposed Parallel Mask Head Mechanism (PMHM) further mitigates boundary noise"
  - [Section IV.B] Equations 5-7 define head consistency, layer consistency, and total loss
  - [Table III d-f] PMHM achieves highest gIoU (67.8, 64.6, 61.7) across all kernel sizes under mask-only noise
- **Break condition:** Under severe text noise, prompt mis-grounding dominates; PMHM's boundary-focused regularization provides limited benefit (Table V shows PMHM gIoU 59.9 < baseline 60.5 under mixed noise).

## Foundational Learning

- **Concept:** Referring Video Object Segmentation (R-VOS)
  - **Why needed here:** ActionVOS extends R-VOS by conditioning segmentation on action narrations. Understanding R-VOS grounding (text→mask) is prerequisite.
  - **Quick check question:** Can you explain how a text prompt "left hand holding cup" maps to pixel masks across video frames?

- **Concept:** Label Noise Learning (sample selection vs. robust loss)
  - **Why needed here:** The benchmark adapts six strategies; distinguishing instance-level filtering (Co-teaching) from pixel-wise losses (GCE/SCE/APL) is critical for selecting the right method per noise type.
  - **Quick check question:** Why does sample selection fail when boundary pixels are systematically corrupted?

- **Concept:** Morphological dilation for boundary perturbation
  - **Why needed here:** Mask noise is simulated via separate-dilate-combine; kernel size (9, 15, 21) directly controls noise severity and mimics crowdsourced annotation blur.
  - **Quick check question:** What happens to overlap metrics when dilation kernel increases? (Answer: p-mIoU/p-cIoU decrease, n-cIoU rebounds at highest levels per Table II.)

## Architecture Onboarding

- **Component map:** ResNet-101 visual encoder -> RoBERTa text encoder (frozen) -> Shared decoder features -> Main Seg Head + [Aux Seg Head (PMHM, training only)] -> Losses (hard mask + head consistency + layer consistency)
- **Critical path:**
  1. Initialize from Ref-YouTube-VOS checkpoint
  2. For PMHM: add auxiliary head, set τ_m=0.20, τ_e=0.85, λ_head=λ_layer=0.1
  3. For ELR/NPN: disable spatial transforms (preserve per-pixel alignment for history tracking)
  4. Train 6 epochs (10 for Co-teaching); evaluate on clean 294-clip benchmark
- **Design tradeoffs:**
  - **Co-teaching:** Best foreground preservation under text noise; requires 2× models, 10 epochs, gradient accumulation (128×)—high memory/time cost
  - **PMHM:** +1 lightweight head; no inference overhead; targets mask noise only
  - **GCE/SCE:** Simple loss swap; balanced but requires hyperparameter (q=0.7, α/β) tuning
- **Failure signatures:**
  - **Boundary leakage:** Mask noise causes thickened edges, shape jitter (Fig. 5) → grasp-pose drift in robotics
  - **Identity substitution:** Text flips cause wrong-object segmentation (e.g., "banana" instead of "pepper")
  - **Foreground-background trade-off:** Methods boosting p-mIoU often increase n-mIoU (e.g., ELR at Name-60: p-mIoU 48.5, but n-mIoU 9.8 is best)
- **First 3 experiments:**
  1. **Baseline sensitivity:** Train standard ActionVOS on Name-20/40/60 and Mask-k=9/15/21; plot p-mIoU vs. n-mIoU to confirm asymmetric degradation
  2. **Single-noise strategy comparison:** Run all 6 methods + PMHM on Name-40 and Mask-15; verify Co-teaching wins text-only, PMHM wins mask-only (per Table III)
  3. **Mixed-noise validation:** Combine Name-60 + Mask-21; confirm GCE/SCE outrank Co-teaching on gIoU (per Table V), demonstrating non-transitive robustness

## Open Questions the Paper Calls Out
- **Open Question 1:** Can vision-language foundation models resolve the foreground-background trade-off observed in current noise-robust strategies? (The authors suggest future work will integrate foundation models to drive boundary-focused objectives.)
- **Open Question 2:** How do the identified failure modes quantitatively impact task success in closed-loop robotic manipulation? (The authors call for systematic validation in closed-loop robotic systems to verify if segmentation robustness translates to physical stability.)
- **Open Question 3:** Does training on ActiSeg-NL's synthetic noise generalize to natural, crowd-sourced annotation errors? (The authors note that simulated noise follows structured rules, whereas real-world noise may contain semantic or structural artifacts not captured by these perturbations.)

## Limitations
- Label noise generation focuses on synthetic flips/dilations, which may not fully capture real-world prompt errors (e.g., stuttering, synonym ambiguity) or mask noise (crowd-sourced blur, annotation drift).
- Evaluation metrics assume static object identity; temporally inconsistent masks across frames (e.g., occlusion) are not explicitly modeled.
- Results are based on VISOR; transferability to other video segmentation datasets (DAVIS, YouTube-VOS) under similar noise conditions remains untested.

## Confidence
- **High:** Clean vs. noise degradation trends; effectiveness of GCE/SCE under mixed noise; PMHM’s boundary noise mitigation (supported by ablation in Table III).
- **Medium:** Superiority of Co-teaching under text noise (requires exact noise distribution replication); trade-off claims (p-mIoU vs. n-mIoU) are metric-dependent.
- **Low:** Claims about real-world robotics performance (e.g., grasp-pose accuracy) are extrapolated from segmentation metrics without empirical validation.

## Next Checks
1. **Noise Realism Test:** Apply the benchmark to a small subset of VISOR with manually annotated "real" prompt/mask errors (e.g., human re-labeling) and compare against synthetic noise performance.
2. **Cross-Dataset Robustness:** Evaluate GCE/SCE/APL on DAVIS-17 under VISOR-trained noise models to test domain adaptation.
3. **Temporal Consistency Audit:** Track object identity consistency across frames under noise; measure identity switches and their impact on downstream metrics.