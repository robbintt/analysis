---
ver: rpa2
title: 'DreamAudio: Customized Text-to-Audio Generation with Diffusion Models'
arxiv_id: '2509.06027'
source_url: https://arxiv.org/abs/2509.06027
tags:
- audio
- generation
- reference
- customized
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DreamAudio, a novel system for customized
  text-to-audio generation (CTTA) that enables users to generate audio samples with
  specific, user-provided sound events. The core innovation is a multi-reference customization
  (MRC) structure that extracts and fuses audio features from reference concepts using
  a latent diffusion model with rectified flow matching.
---

# DreamAudio: Customized Text-to-Audio Generation with Diffusion Models

## Quick Facts
- **arXiv ID**: 2509.06027
- **Source URL**: https://arxiv.org/abs/2509.06027
- **Reference count**: 40
- **Key outcome**: DreamAudio achieves state-of-the-art results on customized text-to-audio generation with CLAP A scores up to 87.7 and FAD scores as low as 0.46, while maintaining competitive performance on general text-to-audio tasks.

## Executive Summary
DreamAudio introduces a novel system for customized text-to-audio generation (CTTA) that enables users to generate audio samples with specific, user-provided sound events. The core innovation is a multi-reference customization (MRC) structure that extracts and fuses audio features from reference concepts using a latent diffusion model with rectified flow matching. The system generates high-quality, customized audio clips that align well with input text prompts and reference audio events. DreamAudio significantly outperforms existing methods on customized tasks, achieving state-of-the-art results while also delivering competitive performance on general text-to-audio generation. The work establishes a new benchmark dataset for CTTA and demonstrates strong adaptability to varying numbers of reference concepts.

## Method Summary
DreamAudio is a latent diffusion model that generates audio conditioned on text prompts and reference audio-caption pairs. The system uses rectified flow matching (RFM) for stable training and a multi-reference customization (MRC) structure for feature extraction and fusion. The MRC architecture employs two parallel encoder paths with independent weights: one for text conditioning and another for reference feature extraction. These features are fused through cross-attention mechanisms during the decoding process. The model operates on compressed VAE latents and uses BigVGAN for waveform synthesis. Training incorporates data augmentation through content masking and dropping to prevent over-reliance on reference features while maintaining customization capability.

## Key Results
- Achieves CLAP A scores up to 87.7 and FAD scores as low as 0.46 on customized text-to-audio generation tasks
- Outperforms existing methods on CTTA benchmarks by 5-10% in CLAP A scores
- Maintains competitive performance on general text-to-audio generation with FAD scores around 1.92
- Successfully handles up to 3 reference concepts without architectural modification

## Why This Works (Mechanism)

### Mechanism 1: Rectified Flow Matching
Rectified Flow Matching provides faster and more stable training than traditional DDPM-based diffusion by replacing discrete denoising steps with continuous flow interpolation. The model learns a constant velocity field that smoothly interpolates between Gaussian noise and target data, enabling synthesis in fewer than 50 integration steps. This schedule-free training objective eliminates the need for noise schedule tuning and provides consistent supervision across all flow positions.

### Mechanism 2: Multi-Reference Customization Structure
The MRC structure enables tuning-free customization by independently encoding reference features and fusing them during generation. Two parallel encoder groups process noisy latents with text embeddings and reference audio latents with reference captions separately. Features from each encoder scale are fed as external conditions to corresponding decoder blocks through cross-attention, allowing the model to inject reference-specific acoustic characteristics without modifying base generator weights.

### Mechanism 3: Data Augmentation with Masking and Dropping
Data augmentation through content masking and dropping prevents over-reliance on reference features while maintaining customization capability. The model randomly masks 10% of customized audio content and drops 40% of reference concepts entirely during training. This forces the generator to learn partial dependency on references, improving generalization when references are incomplete or novel combinations are required.

## Foundational Learning

- **Concept: Latent Diffusion Models**
  - Why needed here: DreamAudio operates on compressed VAE latents rather than raw spectrograms, reducing computational cost and enabling efficient RFM training.
  - Quick check question: Can you explain why latent diffusion is preferred over pixel-space diffusion for audio generation?

- **Concept: Cross-Attention for Conditioning**
  - Why needed here: MRC uses separate cross-attention blocks for prompt embeddings and reference caption embeddings, enabling multi-modal conditioning without feature interference.
  - Quick check question: How does cross-attention differ from concatenation-based conditioning, and why might it be better for combining text and audio features?

- **Concept: Classifier-Free Guidance (CFG)**
  - Why needed here: The paper uses CFG with scale 2.0 during inference to balance adherence to text prompts and reference concepts against generation diversity.
  - Quick check question: What happens to output quality if CFG scale is set too high (e.g., >5.0) or too low (e.g., 1.0)?

## Architecture Onboarding

- **Component map**: Text prompt → Flan-T5 → C embedding → Feature encoding encoder → Decoder → VAE decoder → BigVGAN → Waveform
- **Critical path**: Reference audio → VAE encoder → R embedding → Reference extraction encoder → Scale-wise features → Decoder cross-attention → VAE decoder → BigVGAN → Waveform

- **Design tradeoffs**:
  - **Dual vs. shared encoder weights**: DreamAudio uses independent weights for reference extraction and generation encoding, improving CLAP A by ~0.4 points but increasing parameters from 760M to 815M.
  - **RFM vs. DDPM**: RFM achieves faster inference (<50 steps vs. hundreds) but DDPM shows slightly better semantic alignment in some comparisons.
  - **Fixed vs. variable reference count**: Default architecture supports K=3; extending to K=4 requires fine-tuning an alignment layer (50K steps for competitive FAD).

- **Failure signatures**:
  - **Over-reliance on references**: If masking/dropping is disabled during training, CLAP A increases but general TTA FAD degrades from 1.92 to >4.0.
  - **Reference conflicts**: With 4+ references without fine-tuning, FAD jumps to 14.31, indicating architectural capacity limits.
  - **Semantic drift**: Excessive Customized-Concatenation training data causes the model to prioritize reference features over text prompts.

- **First 3 experiments**:
  1. **Validate RFM convergence**: Train a small DreamAudio model (n_hidden=64) on AudioCaps-General for 100K steps with both DDPM and RFM objectives. Compare FAD/KL/CLAP on 100 held-out samples to verify RFM stability claims.
  2. **MRC ablation with single reference**: Train DreamAudio-SEncoder (shared encoder weights) vs. full DreamAudio on Customized-Overlay with K=1. Measure CLAP A difference to quantify the benefit of dual encoder paths for simple customization.
  3. **Masking sensitivity sweep**: Train three DreamAudio variants with (masking%, dropping%) = (5%,20%), (10%,40%), (20%,60%) on a 10K subset of Customized-Concatenation. Plot CLAP A vs. AudioCaps FAD to identify the Pareto frontier between customization and generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MRC structure be adapted to integrate visual modalities like images or video as conditional inputs?
- Basis in paper: Section VII (Conclusion and Future Works) explicitly lists plans to "extend the model to the integration of additional modalities such as images or video."
- Why unresolved: The current architecture encodes only audio (VAE) and text (Flan-T5), lacking the encoders and cross-attention mechanisms necessary to process visual data into the latent flow.
- What evidence would resolve it: A modified DreamAudio framework that successfully generates audio conditioned on image or video prompts alongside text.

### Open Question 2
- Question: Can the requirement for reference audio-caption pairs be removed to allow for audio-only customization?
- Basis in paper: Section VI-E (Limitation) identifies the "Fixed Reference Format" as a barrier, noting that "preparing suitable captions for referenced audio samples can lead to extra workloads."
- Why unresolved: The Multi-Reference Customization (MRC) structure currently relies on cross-attention between specific audio features and their text embeddings; removing one input requires architectural re-engineering.
- What evidence would resolve it: Successful customization using un-captioned audio references that achieves comparable CLAP A and FAD scores to the paired approach.

### Open Question 3
- Question: Does replacing artificial concatenation/overlay data with high-quality real-world datasets resolve the issue of unnatural generated audio?
- Basis in paper: Section VI-E (Limitation) states that due to artificial training data, "audio generated by the proposed model can sometimes sound unnatural."
- Why unresolved: It is unclear if the model's limitations are due to the synthetic nature of the data or inherent architectural constraints in the diffusion process.
- What evidence would resolve it: Subjective evaluations (OVL scores) showing statistically significant improvements in naturalness when the model is retrained on a curated, non-synthetic dataset.

## Limitations
- Architectural scaling limitations beyond 3 reference concepts without fine-tuning
- Performance degradation when handling more than 3 reference concepts without architectural modification
- Evaluation primarily focused on controlled datasets rather than diverse real-world audio generation tasks

## Confidence

- **High Confidence**: The basic mechanism of MRC structure for dual-path feature extraction and fusion (CLAP A scores of 87.7 with 0.46 FAD) is well-supported by ablation studies and quantitative metrics.
- **Medium Confidence**: The claim that RFM provides superior training stability and speed compared to DDPM is supported by the paper's results but lacks comprehensive external validation across different audio generation tasks.
- **Low Confidence**: The system's ability to handle complex real-world scenarios with more than 3 reference concepts or highly conflicting acoustic features remains largely untested, as evidenced by the significant performance drop when scaling beyond the default configuration.

## Next Checks

1. **Scaling Reference Count Validation**: Test DreamAudio with 5-10 reference concepts on a subset of the CTTA benchmark to quantify the architectural limitations and determine if additional fusion mechanisms (weighted averaging, attention-based selection) can improve performance.

2. **Cross-Domain Generalization Study**: Evaluate DreamAudio on audio generation tasks outside the CTTA benchmark, such as music generation with genre/style references or environmental sound synthesis with complex acoustic scenes, to assess real-world applicability.

3. **RFM vs DDPM Head-to-Head Comparison**: Conduct a systematic comparison of RFM and DDPM across multiple audio datasets (AudioCaps, MusicCaps, environmental sound datasets) with identical model architectures, training schedules, and evaluation metrics to verify the claimed training stability and speed advantages.