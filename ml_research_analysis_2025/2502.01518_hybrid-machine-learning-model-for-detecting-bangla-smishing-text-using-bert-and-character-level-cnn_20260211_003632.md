---
ver: rpa2
title: Hybrid Machine Learning Model for Detecting Bangla Smishing Text Using BERT
  and Character-Level CNN
arxiv_id: '2502.01518'
source_url: https://arxiv.org/abs/2502.01518
tags:
- smishing
- detection
- bert
- text
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid machine learning model combining
  BERT and CNN for Bangla smishing text detection. The model addresses multi-class
  classification by distinguishing between Normal, Promotional, and Smishing SMS,
  outperforming traditional binary classifiers.
---

# Hybrid Machine Learning Model for Detecting Bangla Smishing Text Using BERT and Character-Level CNN

## Quick Facts
- arXiv ID: 2502.01518
- Source URL: https://arxiv.org/abs/2502.01518
- Reference count: 36
- Primary result: 98.47% accuracy on multi-class Bangla SMS classification (Normal, Promotional, Smishing)

## Executive Summary
This study presents a hybrid machine learning model combining BERT's contextual embeddings with CNN's character-level features for detecting Bangla smishing text. The model addresses the multi-class classification challenge of distinguishing between Normal, Promotional, and Smishing SMS messages, outperforming traditional binary classifiers. Using a combination of semantic understanding from BERT and morphological pattern recognition from character-level CNN with attention mechanisms, the approach achieves high precision and recall across all categories, particularly excelling in Smishing detection. The methodology demonstrates significant improvements over baseline models and provides a robust foundation for mobile security applications against smishing attacks.

## Method Summary
The methodology employs a dual tokenization approach using bert-base-multilingual-cased for word-level processing (padded to 128 tokens) and a custom character-level tokenizer for Bangla Unicode text (padded to 256 characters). The hybrid model concatenates BERT embeddings with CNN character-level features, enhanced by an attention mechanism, before passing through a fully connected layer with 3-class softmax output. The model is trained on 2,287 Bangla SMS samples using AdamW optimizer with learning rate 2e-5, cross-entropy loss, and 3 epochs. Five-fold cross-validation is used for evaluation, with the dataset containing 924 Normal, 914 Smishing, and 449 Promotional messages.

## Key Results
- Achieved 98.47% overall accuracy across Normal, Promotional, and Smishing categories
- High precision and recall in Smishing detection, with strong performance across all categories
- Demonstrated superiority over traditional machine learning algorithms (SVM, Random Forest) in multi-class classification

## Why This Works (Mechanism)

### Mechanism 1
BERT's contextual embeddings capture semantic distinctions between Smishing, Promotional, and Normal SMS categories through bidirectional context encoding. The bert-base-multilingual-cased tokenizer converts Bangla text into subword units with attention-masking, differentiating message intent by capturing word relationships within SMS content. This mechanism assumes Bangla smishing messages exhibit distinguishable semantic patterns at the word-sequence level.

### Mechanism 2
Character-level CNN captures morphological patterns and local character sequences that word-level models miss. Custom character tokenizer encodes Bangla Unicode into integer sequences (padded to 256 characters), with convolutional filters detecting local patterns including deliberate misspellings, character substitutions, or morphological variants common in smishing. This mechanism assumes character-level irregularities in smishing messages serve as discriminative signals.

### Mechanism 3
Attention mechanism focuses classification on the most discriminative text segments by computing weighted importance over feature dimensions after BERT-CNN feature fusion. This selectively amplifies signals from tokens most indicative of smishing while dampening irrelevant content, assuming smishing detection depends more heavily on specific text segments than uniform analysis.

## Foundational Learning

- **Concept: Subword Tokenization (BPE/WordPiece)**
  - Why needed here: BERT's multilingual tokenizer uses subword units to handle Bangla's agglutinative morphology and out-of-vocabulary words
  - Quick check question: Given the Bangla word "প্রমোশনাল" (promotional), would BERT tokenize it as one unit or multiple subword pieces?

- **Concept: Multi-class vs. Binary Classification with Cross-Entropy**
  - Why needed here: The model distinguishes three classes using categorical cross-entropy, explaining why Promotional SMS shows lower recall than Normal
  - Quick check question: If the model outputs probabilities [0.33, 0.33, 0.34] for a Promotional SMS, what is the cross-entropy loss?

- **Concept: Feature Fusion Strategies in Hybrid Architectures**
  - Why needed here: The model concatenates BERT embeddings (word-level, 128 tokens) with CNN outputs (character-level, 256 chars)
  - Quick check question: If BERT outputs 768-dimensional vectors and CNN outputs 256-dimensional vectors, what is the concatenated feature dimension?

## Architecture Onboarding

- **Component map:**
  Raw Bangla SMS → Text Cleaning → BERT Tokenizer → bert-base-multilingual-cased → Contextual Embeddings
  │
  └──→ Character Tokenizer → Char Encoding (256) → CNN Layers → Concatenation
                                                   │
                                                   ↓
                                               Attention Layer
                                                   │
                                                   ↓
                                               Fully Connected
                                                   │
                                                   ↓
                                          3-Class Softmax Output

- **Critical path:** Classification performance hinges on BERT pathway quality (semantic signal) + CNN pathway quality (character signal) + effective attention weighting. The weakest link appears to be Promotional vs. Smishing discrimination.

- **Design tradeoffs:**
  - BERT fine-tuning vs. frozen embeddings: Paper fine-tunes BERT, improving accuracy but increasing compute and overfitting risk
  - Character sequence length (256) vs. memory: Longer sequences capture more detail but increase CNN memory/computation
  - Three-class vs. binary: Multi-class enables Promotional detection but introduces harder boundaries
  - AdamW learning rate (2×10⁻⁵): Conservative for BERT fine-tuning stability

- **Failure signatures:**
  - High validation loss with low training loss: Overfitting on small dataset (2,287 samples)
  - Promotional → Smishing confusion: Attention may attend to urgency words present in both
  - <UNK> token spikes: Character vocabulary missing key Bangla characters or diacritics
  - Class imbalance effects: Promotional (449) has lowest support; expect higher variance in its metrics

- **First 3 experiments:**
  1. Ablation study: Disable CNN pathway and disable BERT pathway to quantify each component's contribution
  2. Attention visualization on errors: Extract attention weights from misclassified Promotional→Smishing samples
  3. Cross-validation class balance check: Run stratified 5-fold CV with explicit tracking of per-class metrics

## Open Questions the Paper Calls Out

1. Can the hybrid CNN-BERT architecture be optimized for real-time latency constraints in mobile security applications?
2. Would integrating alternative deep learning architectures improve robustness over the current CNN component?
3. Can the model's distinction between Promotional and Smishing classes be improved through data augmentation?
4. How does the model generalize to adversarial attacks or obfuscated text (e.g., intentional misspellings)?

## Limitations
- Dataset size and generalizability concerns due to small sample size (2,287 total samples)
- Architecture specification gaps including exact CNN architecture and attention mechanism details
- Cross-lingual generalization not evaluated for other languages or Bangla dialects
- Real-world deployment considerations not addressed (computational efficiency, adversarial attacks)

## Confidence

**High confidence**: The hybrid architecture combining BERT and CNN is technically sound and follows established patterns in NLP. The 98.47% accuracy is internally consistent with reported metrics.

**Medium confidence**: Mechanism explanations for why each component contributes are plausible but not empirically validated through ablation studies.

**Low confidence**: Assertions about maintaining high accuracy in production environments and attention mechanism reliability across all classes equally.

## Next Checks
1. Perform ablation study by systematically disabling each pathway (BERT-only, CNN-only, attention layer) to quantify individual contributions
2. Execute stratified 5-fold cross-validation with explicit tracking of per-class metrics and variance
3. Extract and visualize attention weight distributions for samples misclassified between Promotional and Smishing categories