---
ver: rpa2
title: LLM Performance for Code Generation on Noisy Tasks
arxiv_id: '2505.23598'
source_url: https://arxiv.org/abs/2505.23598
tags:
- performance
- llms
- tasks
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how well large language models can solve
  tasks when the text is severely obfuscated, even to the point of being unintelligible
  to humans. The study uses three datasets (LeetCode, NewLeetCode, and MATH) and applies
  noise methods like typos, deletions, and truncation to create increasingly obfuscated
  versions of each task.
---

# LLM Performance for Code Generation on Noisy Tasks

## Quick Facts
- **arXiv ID**: 2505.23598
- **Source URL**: https://arxiv.org/abs/2505.23598
- **Reference count**: 38
- **Primary result**: All tested models could solve heavily obfuscated tasks if the task structure matched training data, suggesting memorization rather than robust reasoning.

## Executive Summary
This study investigates how well large language models (LLMs) can solve tasks when input text is severely obfuscated through typos, deletions, and truncation. Using LeetCode, NewLeetCode, and MATH datasets, the authors find that models like Claude 3.5 Haiku, DeepSeek V3, and others can still produce correct solutions on heavily obfuscated problems—if the task structure is similar to their training data. This suggests models rely on pattern matching rather than genuine reasoning. The authors introduce "eager pattern matching" to describe this behavior, which poses risks for both benchmarking and real-world deployment.

## Method Summary
The authors evaluated five LLMs on three datasets, progressively applying noise methods (typos, deletions, truncations) to obfuscate task descriptions. Performance was measured as accuracy across increasing noise levels. They compared older (likely contaminated) and newer datasets to observe differences in performance decay. Adversarial examples were also used to probe model behavior. The key hypothesis was that slower performance decay under obfuscation indicates dataset contamination.

## Key Results
- All models could solve heavily obfuscated tasks when the task structure matched training data, even missing crucial details.
- The older LeetCode dataset showed high accuracy under severe obfuscation, suggesting contamination from prior training exposure.
- Newer LeetCode and MATH datasets exhibited sharp performance drops with increasing obfuscation, indicating reliance on genuine reasoning.
- "Eager pattern matching" behavior—matching to similar tasks from training data rather than solving the obfuscated input—was widespread.

## Why This Works (Mechanism)
Assumption: The mechanism likely involves pattern matching over reasoning, as models perform well on tasks with structures similar to their training data even when key details are obfuscated. This suggests the models retrieve memorized solutions rather than engaging in genuine problem-solving.

## Foundational Learning
- **Pattern matching vs. reasoning**: Why needed: To understand if models are generalizing or just retrieving memorized solutions; Quick check: Compare performance on clean vs. obfuscated versions of new vs. old datasets.
- **Dataset contamination**: Why needed: To assess whether high performance under obfuscation reflects memorization; Quick check: Measure performance decay under increasing noise.
- **Adversarial examples in LLM evaluation**: Why needed: To test robustness of model behavior under misleading or noisy inputs; Quick check: Use adversarial inputs to probe for eager pattern matching.

## Architecture Onboarding
- **Component map**: Task description -> Noise obfuscation -> LLM inference -> Output solution -> Accuracy evaluation
- **Critical path**: Noise application and evaluation design are central; model inference is standard.
- **Design tradeoffs**: Balance between realistic noise types and task interpretability; risk of overfitting in older datasets.
- **Failure signatures**: High accuracy on heavily obfuscated old tasks but not new ones indicates memorization.
- **First experiments**: 1) Replicate with non-contaminated training data; 2) Expand noise types; 3) Apply to non-coding domains.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly list open questions, though the findings raise several implicit ones about the nature of LLM reasoning and the reliability of benchmarks.

## Limitations
- Contamination detection relies on the assumption that newer datasets are less contaminated, which may not always hold.
- "Eager pattern matching" is not quantitatively validated; no explicit metrics distinguish pattern matching from reasoning.
- Results are limited to coding and math; generalizability to other domains is untested.

## Confidence
- **High**: Models exhibit significantly better performance on older, likely contaminated datasets under obfuscation.
- **Medium**: "Eager pattern matching" is the dominant behavior under heavy obfuscation.
- **Low**: The proposed contamination detection method reliably distinguishes between memorization and generalization across all settings.

## Next Checks
1. Conduct ablation studies with models trained on non-contaminated subsets to isolate memorization effects.
2. Expand evaluation to non-coding domains (e.g., natural language QA) to test generalizability of eager pattern matching.
3. Perform human evaluation to assess semantic correctness of model outputs on obfuscated tasks.