---
ver: rpa2
title: 'Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for
  DeFi User Transaction Intent Mining'
arxiv_id: '2511.15456'
source_url: https://arxiv.org/abs/2511.15456
tags:
- intent
- transaction
- data
- defi
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inferring user intents behind
  complex DeFi transactions, which is difficult due to opaque blockchain logs, multifaceted
  interactions, and external influences. To tackle this, the authors propose TIM,
  a multi-agent LLM framework that dynamically coordinates domain experts to analyze
  transactions from multiple perspectives using both on-chain and off-chain data.
---

# Keep Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining

## Quick Facts
- arXiv ID: 2511.15456
- Source URL: https://arxiv.org/abs/2511.15456
- Reference count: 40
- Key outcome: Multi-agent LLM framework TIM achieves 0.75 F1-micro for DeFi intent mining, outperforming baselines (ML 0.49-0.62, single LLMs 0.30, single agents 0.41).

## Executive Summary
This paper tackles the challenge of inferring user intents behind complex DeFi transactions, which are difficult to interpret due to opaque blockchain logs, multifaceted interactions, and external influences. The authors propose TIM, a multi-agent LLM framework that dynamically coordinates domain experts to analyze transactions from multiple perspectives using both on-chain and off-chain data. A Meta-Level Planner decomposes tasks, Question Solvers retrieve and process data, and a Cognitive Evaluator ensures result validity by filtering hallucinations. TIM demonstrates strong performance with 0.75 F1-micro, providing explainable and reliable intent mining for DeFi applications.

## Method Summary
TIM is a four-component autonomous framework: (1) A Meta-Level Planner analyzes transactions and dynamically spawns Perspective-Specific Domain Experts to decompose the analysis into specialized sub-tasks; (2) Question Solvers use tool-augmented retrieval (blockchain nodes, web search) to gather multimodal data and reason about it using a reflective ReAct paradigm; (3) A Cognitive Evaluator acts as a final gatekeeper, validating analysis reports based on verifiability of facts and relevance to intent; and (4) The system produces a multi-label intent report. The framework uses Grok-2 with CRIPSE-style prompts and operates with temperature settings of 0.5 for planning/creative tasks and 0.0 for execution/evaluation tasks.

## Key Results
- TIM achieves 0.75 F1-micro on expert-annotated DeFi transaction dataset (600 samples, 21 intent categories).
- Ablation study confirms each component's contribution: removing MP drops F1 to 0.63, removing DE drops it to 0.44, removing CE reduces precision from 0.72 to 0.45.
- TIM outperforms traditional ML models (0.49-0.62 F1) and single LLM/agent baselines (0.30-0.41 F1).
- The framework successfully handles complex, multi-intent transactions while maintaining high precision through hallucination filtering.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition via Specialized Agents
- **Claim:** If complex DeFi transactions are decomposed into specialized sub-tasks managed by distinct agents, the system achieves higher accuracy than monolithic models because it reduces the semantic search space for each step.
- **Mechanism:** A Meta-Level Planner analyzes the transaction and spawns Perspective-Specific Domain Experts. Each DE decomposes its perspective into questions for Question Solvers, enforcing a "divide-and-conquer" strategy.
- **Core assumption:** The intent of a transaction is composed of distinct, analyzable perspectives (e.g., Smart Contract vs. Market Dynamics) that do not require simultaneous, tightly-coupled reasoning to be accurate.
- **Evidence anchors:** Abstract states MP "dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks." Section 3.4 discusses dynamic planning. Table 2 ablation shows removing MP drops F1 from 0.75 to 0.63.
- **Break condition:** Performance degrades to single-agent levels if the MP fails to generate diverse perspectives or if the DEs generate redundant/irrelevant questions.

### Mechanism 2: Reflective Multimodal Live Data Retrieval
- **Claim:** Providing agents with access to live on-chain and off-chain data, coupled with a critique step, reduces hallucinations by grounding reasoning in verifiable external facts.
- **Mechanism:** Question Solvers use tools to fetch data (Blockchain nodes, Web3Research, Web Search) and employ reflective design: after fetching, they "critique the data" to determine if they need to switch sources or fetch more, rather than immediately summarizing.
- **Core assumption:** The relevant context for a transaction intent often lies off-chain (social sentiment, market news) or in linked on-chain events, and cannot be inferred from the transaction hex log alone.
- **Evidence anchors:** Section 3.3 states "we use prompts to help understand and critique the data, aiding them in optimizing their retrieval methods." Abstract mentions "Question Solvers handle the tasks with multi-modal on/off-chain data."
- **Break condition:** System hallucinates or fails if retrieval tools return stale/incorrect data, or if context window optimization discards critical "weak signals."

### Mechanism 3: Dual-Dimensional Cognitive Evaluation
- **Claim:** If a dedicated evaluator agent filters results based on explicit evidence links and intent relevance, precision increases by pruning speculative or hallucinated intents.
- **Mechanism:** The Cognitive Evaluator acts as a final gatekeeper, scoring analysis reports on "Verifiability of facts" (traceability) and "Relevance to intent." It excludes intents that lack grounded evidence.
- **Core assumption:** LLMs can reliably assess the "verifiability" of their own (or other agents') outputs if instructed to look for evidence chains.
- **Evidence anchors:** Abstract states "Cognitive Evaluator ensures result validity by filtering hallucinations." Section 3.7 defines verifiability as "whether the statements... can be validated by objective and traceable evidence." Table 2 shows removing CE increases Recall (0.78) but drops Precision (0.45).
- **Break condition:** System becomes overly conservative (low Recall) if CE is too strict on "verifiability" for complex/implicit intents, or fails to catch hallucinations if cited "evidence" is itself hallucinated.

## Foundational Learning

- **Concept: DeFi Transaction Anatomy (Logs, Traces, ABIs)**
  - **Why needed here:** The framework relies on parsing raw hex data. You cannot define "Perspectives" or "Questions" if you don't distinguish between a function signature and an event log.
  - **Quick check question:** Can you identify the difference between a transaction `input` data and a `log` event in an Ethereum receipt?

- **Concept: ReAct (Reason + Act) Pattern**
  - **Why needed here:** The Question Solvers operate using this paradigm. Understanding the loop of "Thought -> Action -> Observation" is essential for debugging why an agent retrieved specific data.
  - **Quick check question:** If a Question Solver retrieves a contract ABI, what is the next logical step in the ReAct loop before it outputs a final answer?

- **Concept: Prompt Engineering for Role-Playing (Personas)**
  - **Why needed here:** The system creates "Domain Experts" dynamically. You need to understand how to construct system prompts that constrain an LLM to a specific domain (e.g., "You are a Smart Contract Auditor").
  - **Quick check question:** How does the prompt for a "Market Dynamics" expert differ in token focus from a "Smart Contract Analysis" expert?

## Architecture Onboarding

- **Component map:** Input (Raw Transaction Hash) -> Meta-Level Planner -> Domain Experts -> Question Solvers (with Tools) -> Cognitive Evaluator -> Output (Final Intent Report)

- **Critical path:**
  1. MP deriving the correct perspectives (Critical for coverage)
  2. QS successfully retrieving and parsing multimodal data (Critical for grounding)
  3. CE filtering based on evidence (Critical for precision)

- **Design tradeoffs:**
  - **Cost vs. Accuracy:** Paper notes 10x cost reduction using "chunk-and-summarize." High accuracy requires full context, but practical deployment requires aggressive context optimization.
  - **Recall vs. Precision:** CE improves precision (0.45 -> 0.72) but slightly lowers recall (0.78 -> 0.78) compared to "w/o CE" baseline. Tuning CE's strictness trades missed intents for reliability.

- **Failure signatures:**
  - **High Recall/Low Precision on Intents A1/A14:** System over-predicts "Spot Trading" or "Permission Management" because these are foundational operations.
  - **Low F1 on A11 (Airdrops):** System fails to infer implicit intents (participating in airdrop) that don't have explicit function names, requiring deeper off-chain context which may be missing or ambiguous.
  - **Temperature Misconfiguration:** Setting "Executive" tasks (QS/CE) to high temperature (>0.5) significantly degrades performance.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run transactions through "Single LLM" setup vs. full TIM framework to verify performance gap (should see ~0.30 vs ~0.75 F1).
  2. **Ablation of the Evaluator:** Run pipeline with Cognitive Evaluator disabled. Check if "Spot Trading" false positives increase significantly (Precision drop).
  3. **Temperature Sensitivity:** Run pipeline with Temperature=0.8 for all agents vs. Temperature=0.5 (Creative) / 0.0 (Executive). Verify if JSON output structure breaks or if fact-retrieval accuracy drops.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain-specific fine-tuning of the base LLMs significantly improve recall and precision for semantically ambiguous intents (e.g., "Participating in Airdrops" or "Hedging") where the current RAG-based approach struggles?
- **Basis in paper:** [explicit] Authors note in Section 4.2 that performance on complex intents dependent on external factors is lower, stating, "This may require fine-tuning of LLMs to enhance understanding of DeFi domain-specific knowledge."
- **Why unresolved:** Current implementation relies on general-purpose models (Grok, GPT-4) enhanced with live data retrieval, but has not tested specialized model weights for the DeFi domain.
- **What evidence would resolve it:** A comparative study benchmarking current TIM framework against a version utilizing LLMs fine-tuned on DeFi smart contract code and whitepapers.

### Open Question 2
- **Question:** To what extent does the "text-only" unification strategy for multimodal data result in the loss of critical structural signals needed to detect complex or malicious contract interactions?
- **Basis in paper:** [inferred] Section 3.3 states data is "unified into a pure text format" by removing machine-generated content like Merkle trees, which may discard structural patterns relevant to security analysis.
- **Why unresolved:** While this simplification aids LLM processing, paper does not quantify if this filtering removes features necessary for distinguishing sophisticated attacks from legitimate complex transactions.
- **What evidence would resolve it:** An error analysis of false negatives involving structural data fields (like Merkle proofs or raw bytecode) that were stripped during the unification process.

### Open Question 3
- **Question:** How does the latency of the sequential, multi-agent workflow (Planner to Evaluator) impact the feasibility of deploying TIM for real-time risk interception?
- **Basis in paper:** [inferred] Framework involves deep chain of dependencies (Meta-Level Planner -> Domain Experts -> Question Solvers -> Cognitive Evaluator) and multiple live data retrievals, which inherently introduces processing delays.
- **Why unresolved:** Paper focuses on accuracy and cost (reduced to <$0.10) but does not report time-to-inference, which is critical for "dark forest" security applications.
- **What evidence would resolve it:** Measurement of average end-to-end latency per transaction under varying network congestion and data retrieval loads.

### Open Question 4
- **Question:** Is the LLM-based Cognitive Evaluator robust enough to detect "hallucinations" in Domain Expert reports when those hallucinations are logically consistent but factually incorrect?
- **Basis in paper:** [inferred] Section 3.7 relies on an LLM agent to verify "verifiability of facts," yet LLMs are known to struggle with distinguishing plausible reasoning from factual truth.
- **Why unresolved:** Paper demonstrates improved performance over baselines, but autonomous "LLM-checking-LLM" system may still propagate subtle, coherent errors that pass semantic relevance checks.
- **What evidence would resolve it:** A manual audit of the Cognitive Evaluator's "Pass" decisions to identify instances where convincing but false reasoning was incorrectly validated.

## Limitations
- **Data access bottlenecks:** System requires access to specific live data tools (Blockchain nodes, Web3Research, Web Search) whose integration details are not fully specified.
- **Generalization uncertainty:** Performance is validated on specific protocols (Uniswap, Aave, Compound) and tokens (WETH, WBTC, USDT, USDC, DAI), with unclear extrapolation to novel DeFi ecosystems.
- **Prompt engineering opacity:** Critical performance depends on specific prompt templates for four agent roles and CRIPSE-style prompting structure, which are not fully disclosed.

## Confidence
- **Hierarchical Task Decomposition via Specialized Agents:** High - Ablation study provides strong empirical evidence (F1 drops from 0.75 to 0.63 without MP, to 0.44 without DE).
- **Reflective Multimodal Live Data Retrieval:** Medium - Described mechanism and theoretical benefits are clear, but lacks quantitative evidence comparing hallucination reduction against non-reflective baseline.
- **Dual-Dimensional Cognitive Evaluation:** High - Ablation study directly validates this claim (removing CE increases Recall to 0.78 but drops Precision to 0.45 from 0.72).

## Next Checks
1. **Ablation of the Meta-Level Planner's Diversity:** Modify MP to generate only a single, generic perspective instead of dynamic multi-perspective planning. Run on held-out test set and measure F1-micro score. Significant drop towards "w/o MP" baseline of 0.63 would confirm MP's diversity is a core driver of performance gain.

2. **Stress Test the Cognitive Evaluator's Verifiability Threshold:** Create synthetic dataset of 50 transactions (25 with clear verifiable evidence, 25 requiring complex inference from indirect signals). Run full pipeline and analyze Precision/Recall for each group. Large gap would validate CE's role and reveal limitations with implicit intents.

3. **Temperature Sensitivity on Fact-Retrieval Tasks:** Run Question Solvers with three temperature settings (0.0, 0.5, 1.0) on 20 transactions requiring complex on-chain data retrieval. Measure accuracy of retrieved data and final intent classification F1. Sharp decline from T=0.0 to T=1.0 would confirm high temperature on "Executive" tasks leads to fact-retrieval errors and hallucinations.