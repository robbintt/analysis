---
ver: rpa2
title: 'Toward Autonomous UI Exploration: The UIExplorer Benchmark'
arxiv_id: '2506.17779'
source_url: https://arxiv.org/abs/2506.17779
tags:
- exploration
- action
- agents
- agent
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UIExplore-Bench is the first benchmark focused on evaluating autonomous
  agents' UI exploration capabilities, distinct from task-specific performance. The
  benchmark evaluates agents in both Structured (DOM access) and Screen (visual-only)
  modes across three GitLab environment levels.
---

# Toward Autonomous UI Exploration: The UIExplorer Benchmark

## Quick Facts
- arXiv ID: 2506.17779
- Source URL: https://arxiv.org/abs/2506.17779
- Authors: Andrei Cristian Nica; Akshaya Vishnu Kudlu Shanbhogue; Harshil Shah; Aleix Cambray; Tudor Berariu; Lucas Maystre; David Barber
- Reference count: 38
- Primary result: First benchmark focused on autonomous UI exploration, introducing hUFO metric and UIExplore-AlGo algorithm

## Executive Summary
UIExplore-Bench is the first benchmark dedicated to evaluating autonomous agents' UI exploration capabilities, distinct from task-specific performance. The benchmark evaluates agents in both Structured (DOM access) and Screen (visual-only) modes across three GitLab environment levels. It introduces the human-normalized UI-Functionalities Observed (hUFO) metric to quantify exploration effectiveness, alongside UI-Functionalities Tested (UFT) for efficiency measurement.

The proposed UIExplore-AlGo algorithm achieves the highest mean hUFO scores (77.2% in Structured, 59.0% in Screen mode at 2,000 steps), outperforming baselines including BFS/DFS, random agents, and GUI-Bee. The algorithm uses hierarchical exploration with macro-actions and novelty-based prioritization inspired by Go-Explore. Results show substantial performance gaps between current agents and human experts (up to 22% in Structured, 40% in Screen), highlighting room for improvement. The benchmark includes a publicly released exploration dataset to enable downstream research on experience-driven task completion and metric learning.

## Method Summary
The benchmark evaluates autonomous UI exploration on GitLab sandbox environments with three difficulty levels (Abundant/Moderate/Sparse) in both Structured (DOM access) and Screen (visual-only) observation modes. The primary metric is hUFO@T, measuring human-normalized UI functionalities discovered within T steps. UIExplore-AlGo uses a hierarchical approach with GPT-4o proposing macro-actions (high-level exploration goals) that are executed by Claude-3.5-Sonnet through sequences of atomic UI interactions. The algorithm maintains a knowledge graph of visited states and uses textual novelty scoring via cosine similarity over embedded state descriptions to prioritize exploration.

## Key Results
- UIExplore-AlGo achieves highest mean hUFO scores: 77.2% in Structured mode, 59.0% in Screen mode at 2,000 steps
- Substantial performance gaps between agents and human experts: up to 22% in Structured, 40% in Screen mode
- Ablation studies show significant performance drops when removing novelty scoring (-10%) or LLM importance ranking
- BFS outperforms UIExplore-AlGo in first 500 steps in Abundant level due to rapid shallow exploration
- One seed failed to create repository across three attempts, degrading to baseline performance

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Macro-Action Decomposition
Hierarchical macro-action decomposition enables more efficient long-horizon exploration than atomic action selection alone. GPT-4o proposes high-level macro-actions (e.g., "create project") that are executed by Claude through sequences of atomic UI interactions (up to NA=6 in Structured, NA=12 in Screen mode). This decomposition allows the agent to reason about exploration at the level of functional regions rather than individual clicks. The approach provides efficiency gains, with reducing atomic-action budget producing only modest declines (-5%).

### Mechanism 2: Text-Based Novelty Scoring
Text-based novelty scoring provides more robust exploration signals than raw visual or DOM-based novelty in dynamic UI environments. Instead of using screenshots or DOM snapshots directly, the algorithm prompts GPT-4o to generate natural language descriptions of UI states. These descriptions serve as keys in a novelty table, computed via cosine similarity against embeddings of previously visited states. This abstracts away visual drift (e.g., theme changes) while preserving functional distinctions.

### Mechanism 3: Tri-Objective Macro-Action Prioritization
Tri-objective macro-action prioritization (novelty + trajectory dissimilarity + LLM importance) prevents agents from getting stuck in redundant or low-value UI regions. Macro-actions are scored on predicted novelty of endpoint, dissimilarity from past trajectories, and GPT-4o's importance prior. The agent first filters to top novel actions, then selects based on LLM ranking. This mixture biases the agent toward cumulative, long-horizon payoff instead of one-step curiosity.

## Foundational Learning

- **Go-Explore algorithm paradigm**: UIExplore-AlGo is explicitly "Go-Explore-inspired." Understanding cellular automata exploration, archive-based novelty storage, and the "first return, then explore" principle is essential. Quick check: Can you explain why Go-Explore separates exploration into a "return to frontier" phase followed by "explore from frontier"?

- **Intrinsic motivation and novelty-based exploration in RL**: The algorithm builds on count-based methods and curiosity-driven intrinsic rewards. Understanding prediction error as an exploration bonus clarifies why textual novelty is used. Quick check: How does count-based exploration differ from prediction-error-based curiosity, and which approach does textual novelty more closely resemble?

- **Hierarchical reinforcement learning / options framework**: Macro-actions are essentially "options" in the RL senseâ€”temporally extended actions with initiation conditions and termination conditions. Quick check: What is the relationship between the macro-action horizon NM and the options framework's termination condition?

## Architecture Onboarding

- **Component map**: State Describer (GPT-4o) -> Knowledge Graph G -> Novelty Scorer -> Frontier Selector -> Act Agent (Claude) -> UI State
- **Critical path**: 1) Observe UI state (DOM tree or screenshot) 2) GPT-4o generates state description + macro-action candidates 3) Novelty scorer filters to top-N novel actions 4) Select highest-ranked macro-action 5) Claude executes up to NA atomic actions 6) Record state transition in knowledge graph G 7) After NM macro-actions, select frontier state and goto()
- **Design tradeoffs**: Hybrid approach uses GPT-4o for high-level planning (expensive but better reasoning) and Claude for execution (cheaper, good at coordinate prediction). Setting NM=1 performs similarly to heuristics. Too low NA forces macro-intent to spill across multiple selections; too high wastes steps.
- **Failure signatures**: Seed variance causes macro-action completion failures. Early BFS plateau occurs in Abundant level. DFS help-section trap degrades quickly due to GitLab's extensive help documentation.
- **First 3 experiments**: 1) Reproduce baseline comparison on Screen-Moderate setting with 3 seeds each 2) Ablate novelty vs. importance to isolate contributions 3) Test generalization to unseen UI by evaluating on different web application

## Open Questions the Paper Calls Out

- **Can RL effectively train agents to maximize exploration coverage without handcrafted novelty heuristics?** The paper explicitly calls for research into training agents with RL to "learn a better exploration policy, without handcrafted novelty heuristics." The current leading agent relies on Go-Explore-inspired novelty scorer and GPT-4o ranking rather than learned policies.

- **Can self-supervised metrics accurately predict exploration progress in Screen mode without access to privileged DOM information?** Section 7 invites the design of "self-supervised metrics... not reliant on privileged structural information" to measure progress at test time. The current efficiency metric is restricted to Structured mode.

- **Do exploration strategies developed on the GitLab sandbox generalize to applications with different UI paradigms?** Section 8 notes the benchmark currently targets a single application and suggests future work should probe generalization to more subtle elements. The paper evaluates performance on three levels of the same GitLab environment.

## Limitations

- **Macro-action execution reliability**: Failed macro-actions degrade performance to baseline levels, and the feedback mechanism for correcting incomplete macros appears insufficient
- **Novelty scoring generalizability**: The approach depends on LLMs' ability to capture functionally relevant differences, which is not evaluated across diverse UI domains
- **Knowledge graph persistence**: The schema and storage format for the knowledge graph G are unspecified, making it unclear whether exploration experiences persist across sessions

## Confidence

- **High confidence**: Benchmark design (hUFO/UFT metrics), baseline comparisons, and overall performance trends across environment levels
- **Medium confidence**: The mechanism of hierarchical macro-action decomposition and its efficiency gains
- **Low confidence**: Claims about text-based novelty scoring being superior to visual/DOM methods due to lack of comparative experiments

## Next Checks

1. **Execute macro-action failure analysis**: Log and categorize all macro-action failures across seeds, measuring their frequency, types, and impact on overall hUFO scores
2. **Compare novelty scoring methods**: Implement and evaluate UIExplore-AlGo using alternative novelty metrics (pixel similarity, DOM tree edit distance) alongside the text-based approach
3. **Cross-domain generalization test**: Train UIExplore-AlGo on GitLab Sparse level, then evaluate on a different web application (e.g., simplified e-commerce checkout flow) to measure hUFO@2000 and analyze transfer effectiveness