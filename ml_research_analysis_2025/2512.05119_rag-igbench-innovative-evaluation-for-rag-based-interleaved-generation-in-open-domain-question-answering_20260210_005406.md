---
ver: rpa2
title: 'RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in
  Open-domain Question Answering'
arxiv_id: '2512.05119'
source_url: https://arxiv.org/abs/2512.05119
tags:
- wang
- evaluation
- arxiv
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAG-IGBench, a comprehensive benchmark for
  evaluating interleaved image-text generation in open-domain question answering.
  The benchmark addresses challenges in multimodal content evaluation by introducing
  innovative metrics for text quality (ROUGE scores), image quality (edit distance
  and Kendall correlation), and image-text coherence (CLIP-score and semantic alignment).
---

# RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering

## Quick Facts
- **arXiv ID**: 2512.05119
- **Source URL**: https://arxiv.org/abs/2512.05119
- **Reference count**: 40
- **Key outcome**: Introduces comprehensive benchmark for evaluating interleaved image-text generation in open-domain QA with 6,057 curated samples

## Executive Summary
This paper presents RAG-IGBench, a novel benchmark designed to evaluate interleaved image-text generation in open-domain question answering systems. The benchmark addresses the critical need for comprehensive evaluation frameworks that can assess both the quality of generated text and images, as well as their semantic coherence. By curating 6,057 samples from social platforms and developing innovative metrics including ROUGE scores for text, edit distance and Kendall correlation for images, and CLIP-score for image-text coherence, the authors provide a robust framework for evaluating multimodal content generation.

## Method Summary
The benchmark employs a multi-metric evaluation approach that simultaneously assesses text quality, image quality, and image-text coherence. For text evaluation, ROUGE scores measure generation quality against reference texts. Image quality is evaluated using edit distance to compare generated images with ground truth, along with Kendall correlation for assessing visual similarity. Image-text coherence is measured using CLIP-score to evaluate semantic alignment between generated multimodal outputs. The dataset consists of 6,057 curated samples drawn from social platforms, featuring complex multimodal queries that require both visual and textual responses. Extensive experiments with state-of-the-art MLLMs demonstrate strong correlation between automated metrics and human assessments.

## Key Results
- Benchmark demonstrates strong correlation between automated metrics and human assessments
- Models fine-tuned on RAG-IGBench show improved performance across multiple evaluation tasks
- Dataset of 6,057 curated samples provides substantial coverage of complex multimodal queries

## Why This Works (Mechanism)
The benchmark works by addressing the fundamental challenge of evaluating multimodal content through a comprehensive, multi-faceted approach. By combining multiple metrics that target different aspects of quality - textual coherence, visual fidelity, and semantic alignment - the framework captures the complexity of interleaved generation tasks. The use of CLIP-score for image-text coherence is particularly effective because it leverages pre-trained vision-language models to assess semantic relationships, which is more sophisticated than simple keyword matching. The strong correlation with human assessments validates that these automated metrics effectively capture what humans consider important in multimodal outputs.

## Foundational Learning
- **Multimodal Evaluation Metrics**: Why needed - Traditional text-only evaluation metrics are insufficient for assessing interleaved image-text generation; Quick check - Verify that multiple metrics (text, image, coherence) are employed rather than relying on single-score approaches
- **Semantic Alignment Assessment**: Why needed - Ensuring generated images and text are semantically coherent is critical for quality multimodal responses; Quick check - Confirm use of vision-language models like CLIP for coherence evaluation
- **Dataset Curation from Social Platforms**: Why needed - Real-world data from social platforms provides diverse, complex query scenarios that better represent practical use cases; Quick check - Verify sample diversity and complexity levels in the dataset
- **Correlation with Human Assessments**: Why needed - Automated metrics must align with human judgment to be practically useful; Quick check - Review correlation analysis between automated scores and human evaluations

## Architecture Onboarding
**Component Map**: Data Curation -> Metric Development -> Model Evaluation -> Fine-tuning Pipeline
**Critical Path**: Curated Samples → Multi-metric Evaluation → Model Fine-tuning → Performance Validation
**Design Tradeoffs**: Comprehensive evaluation vs. computational complexity; multiple specialized metrics vs. unified scoring approach
**Failure Signatures**: Poor text quality metrics indicate generation issues; low image-text coherence scores suggest semantic misalignment; inconsistent human-automated correlation reveals metric limitations
**First Experiments**: 1) Baseline evaluation of SOTA models without fine-tuning, 2) Fine-tuning experiments on selected models, 3) Correlation analysis between automated metrics and human assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Claims primarily supported by internal validation with state-of-the-art MLLMs, lacking broader model landscape representation
- Limited information about diversity and representativeness of social platform data sources affecting generalizability
- Reliance on specific metrics may not capture all aspects of multimodal quality, particularly for specialized domains or languages

## Confidence
- **High Confidence**: Dataset construction methodology and basic evaluation framework are well-documented and technically sound
- **Medium Confidence**: Reported improvements in model performance after fine-tuning are promising but need more extensive validation across diverse architectures
- **Medium Confidence**: Correlation between automated metrics and human assessments is reported but needs more detailed analysis of agreement levels and potential biases

## Next Checks
1. Conduct cross-validation studies using diverse model architectures beyond the state-of-the-art MLLMs initially tested to assess benchmark generalizability
2. Perform detailed error analysis on social platform data sources to evaluate representativeness and potential biases across different domains and user demographics
3. Implement additional evaluation metrics or user studies to validate the effectiveness of the current metric suite in capturing nuanced aspects of multimodal quality, particularly for specialized use cases