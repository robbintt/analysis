---
ver: rpa2
title: 'Learning to Discover: A Generalized Framework for Raga Identification without
  Forgetting'
arxiv_id: '2601.18766'
source_url: https://arxiv.org/abs/2601.18766
tags:
- raga
- ragas
- classes
- learning
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of Raga identification in Indian
  Art Music where models must recognize known Ragas while also discovering previously
  unseen ones without degrading performance on known classes. The authors adapt the
  Generalized Category Discovery (GCD) framework, combining supervised and unsupervised
  contrastive learning objectives to jointly train on both labeled and unlabeled data.
---

# Learning to Discover: A Generalized Framework for Raga Identification without Forgetting

## Quick Facts
- arXiv ID: 2601.18766
- Source URL: https://arxiv.org/abs/2601.18766
- Reference count: 23
- Primary result: GCD framework achieves 77.49% clustering accuracy on all classes (vs 57.96% baseline) on PIM dataset while preventing catastrophic forgetting

## Executive Summary
This paper addresses Raga identification in Indian Art Music by adapting Generalized Category Discovery (GCD) to enable simultaneous recognition of known Ragas and discovery of previously unseen ones without degrading performance on known classes. The authors propose a unified framework combining supervised and unsupervised contrastive learning objectives, trained jointly on labeled and unlabeled data. By leveraging source-recording identity as a proxy for positive pairs and using a frozen feature extractor with a trainable transformer encoder, the method successfully mitigates catastrophic forgetting while improving clustering accuracy on both old and new Raga classes. Experiments on PIM and Saraga datasets demonstrate significant improvements over baseline Novel Class Discovery approaches.

## Method Summary
The framework consists of a CNN-LSTM feature extractor trained once on labeled data and frozen, followed by a transformer encoder that learns refined representations through joint supervised and unsupervised contrastive losses. The supervised contrastive loss operates on labeled data to maintain old-class representations, while the unsupervised contrastive loss uses source-recording identity to construct positive pairs from unlabeled data. The encoder is trained with a weighted combination of these losses (parameterized by λ), and final clustering is performed using K-Means. Hard negative mining (50 least similar samples) and source-based positive pairs (5 random samples from same source) are used to improve learning efficiency.

## Key Results
- M2 model achieves 77.49% clustering accuracy on all classes (vs 57.96% baseline) on PIM dataset
- Old class accuracy: 91.16% (M2) vs 79.24% (baseline) on PIM, demonstrating forgetting prevention
- New class accuracy: 84.68% (M2) vs 79.34% (baseline) on PIM
- Superior performance over MERT and CultureMERT pre-trained transformers across all metrics
- Maintains strong performance on known Raga classes while discovering new ones

## Why This Works (Mechanism)

### Mechanism 1: Joint Supervised-Unsupervised Contrastive Learning
Combining supervised and unsupervised contrastive objectives enables simultaneous retention of known-class representations and discovery of novel categories. Supervised contrastive loss on labeled data pulls same-class embeddings together while pushing different-class embeddings apart. Unsupervised contrastive loss extends this structure to unlabeled data using source-recording identity as proxy positive pairs. The weighted combination (λ parameter) balances these objectives during encoder training.

### Mechanism 2: Source-Audio Identity as Proxy Label for Positive Pairs
Metadata linking clips to their source recording provides reliable positive pair construction for unsupervised contrastive learning. Each IAM concert recording corresponds to a single Raga; therefore, 30-second clips derived from the same source file share the same (unknown) Raga label. The model samples 5 clips from the same source as positives and 50 least-similar clips as hard negatives.

### Mechanism 3: Frozen Feature Extractor with Trainable Transformer Encoder
Decoupling feature extraction from representation refinement allows domain-specific features to remain stable while the encoder adapts to the discovery task. CNN-LSTM trained once on labeled data via cross-entropy, then frozen. Transformer encoder receives frozen embeddings and learns to reorganize them via self-attention and contrastive objectives, producing refined clusters without altering base features.

## Foundational Learning

- **Concept: Contrastive Representation Learning**
  - Why needed here: Core training objective for both supervised class discrimination and unsupervised cluster formation.
  - Quick check question: Given an anchor embedding z_i, how would you construct positive and negative sets differently for supervised vs. unsupervised contrastive loss?

- **Concept: Catastrophic Forgetting in Continual/Open-World Learning**
  - Why needed here: The central failure mode this paper addresses; understanding why baseline NCD degrades on known classes is essential.
  - Quick check question: Why does training exclusively on unlabeled new-class data distort representations learned for old classes?

- **Concept: Hard Negative Mining**
  - Why needed here: The paper selects the 50 least-similar embeddings as negatives to focus learning on challenging boundaries.
  - Quick check question: What is the risk of using all negatives versus only hard negatives in contrastive learning for fine-grained categories like similar Ragas?

## Architecture Onboarding

- **Component map:**
  Audio clip (30s) → CNN-LSTM Feature Extractor fθ (FROZEN) → Embedding h_i → Transformer Encoder E(·) (TRAINABLE) → Refined embedding z_i → K-Means / Cosine Similarity → Cluster assignment

- **Critical path:**
  1. Train fθ on labeled dataset D_l using cross-entropy (once; freeze thereafter)
  2. Extract embeddings h_i for all clips in D_l ∪ D_u
  3. Initialize transformer encoder E(·)
  4. Train E(·) with joint loss: L_cl = (1-λ)L_scl + λL_u, where L_scl is supervised contrastive loss on D_l, L_u is unsupervised contrastive loss on D_l ∪ D_u
  5. Extract final embeddings Z and apply K-Means with k = total expected classes

- **Design tradeoffs:**
  - λ weighting: Higher λ emphasizes discovery; lower λ emphasizes retention
  - Clustering method: K-Means produces more stable clusters; cosine similarity thresholding is slightly worse but threshold-tunable
  - Feature extractor choice: Domain-trained CNN-LSTM outperforms general music transformers (MERT, CultureMERT) on this task

- **Failure signatures:**
  - Catastrophic forgetting: Old-class ACC drops below feature-extractor baseline (~90%)
  - Over-separated but meaningless clusters: High Silhouette Score with low ACC/NMI/ARI
  - Persistent confusion between similar Ragas: Bhopali ↔ Shuddha-Kalyan confusion is musically expected

- **First 3 experiments:**
  1. Baseline reproduction: Implement NCD-only training and verify catastrophic forgetting on Old classes (expect ~10-15% ACC drop)
  2. Ablation on loss components: Compare M1 (unsupervised contrastive only) vs. M2 (joint supervised + unsupervised)
  3. λ sensitivity analysis: Sweep λ ∈ {0.3, 0.5, 0.7} and plot Old vs. New class ACC to identify optimal tradeoff

## Open Questions the Paper Calls Out

- **Unknown 1:** How can the framework be extended to scenarios where the number of Raga classes (K) is unknown, given that current K-Means clustering requires K as input?
- **Unknown 2:** Can multimodal inputs (video, symbolic notation) effectively disambiguate tonally similar Ragas that consistently confuse the audio-only model?
- **Unknown 3:** How does the framework scale to larger music archives with significantly more Raga classes and greater class imbalance?
- **Unknown 4:** Can self-supervised or continual learning strategies reduce reliance on labeled data while preserving the balance between known and novel class performance?

## Limitations

- Assumes concert recordings contain exactly one Raga throughout, which may not hold for all performances
- Architectural details of CNN-LSTM feature extractor and transformer encoder are unspecified, limiting precise replication
- Optimal λ weighting for balancing supervised and unsupervised losses is not reported
- Relies on source-audio identity as proxy for positive pairs, assuming consistent recording quality and no cross-fades
- Performance on smallest Raga classes (e.g., 0.06% representation) may be unstable

## Confidence

- **High Confidence:** The fundamental mechanism of joint supervised-unsupervised contrastive learning preventing catastrophic forgetting is well-supported by experimental results
- **Medium Confidence:** The source-audio identity assumption for positive pair construction is plausible given domain knowledge but lacks direct empirical validation
- **Medium Confidence:** The superiority of domain-trained CNN-LSTM over general music transformers is demonstrated, but architectural differences make direct comparison difficult

## Next Checks

1. **Assumption validation:** Analyze a sample of source recordings to verify they contain single Ragas throughout, checking for cross-fades, medleys, or multi-Raga performances
2. **Hyperparameter sensitivity:** Systematically vary λ from 0.3 to 0.7 and plot the tradeoff curve between Old class retention and New class discovery performance
3. **Architecture ablation:** Replace the CNN-LSTM feature extractor with the best-performing general music transformer (MERT or CultureMERT) to isolate the contribution of domain-specific features versus the GCD framework itself