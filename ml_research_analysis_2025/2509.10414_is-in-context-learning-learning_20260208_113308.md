---
ver: rpa2
title: Is In-Context Learning Learning?
arxiv_id: '2509.10414'
source_url: https://arxiv.org/abs/2509.10414
tags:
- learning
- prompt
- average
- prompts
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: In-context learning (ICL) in autoregressive large language models
  was examined to determine whether it constitutes genuine learning or merely pattern
  matching. The study performed large-scale empirical analysis on four models, nine
  formal language tasks, and varied prompting strategies including chain-of-thought,
  automated prompt optimization, and direct encoding.
---

# Is In-Context Learning Learning?

## Quick Facts
- arXiv ID: 2509.10414
- Source URL: https://arxiv.org/abs/2509.10414
- Authors: Adrian de Wynter
- Reference count: 40
- Primary result: Large-scale empirical analysis shows ICL performs learning as accuracy improves with more exemplars, but exhibits limited robustness to distributional shifts and overfocuses on statistical prompt features rather than underlying data relations.

## Executive Summary
This paper investigates whether in-context learning (ICL) in autoregressive large language models constitutes genuine learning or merely pattern matching. Through extensive experiments on four models and nine formal language tasks with varied prompting strategies, the study finds that ICL does exhibit learning behavior as performance improves with more exemplars and converges across strategies. However, the results reveal significant limitations: ICL shows poor robustness to distributional shifts, is sensitive to task complexity, and accuracy varies substantially between formally similar tasks. The analysis suggests that ICL overfocuses on statistical features in the prompt rather than learning the underlying data relations, limiting its generalizability beyond observed training distributions.

## Method Summary
The study evaluates ICL as a learning paradigm using synthetic datasets generated by probabilistic automata, with training sets drawn from distribution P and test sets from distributions Q at varying distances δ. Nine formal language tasks were tested across seven prompting strategies (modus ponens, description, chain-of-thought, automated prompt optimization, direct encoding, word salad, salad-of-thought) with shot counts from 0 to 100. Four LLMs (GPT-4 Turbo, GPT-4o, Mixtral 8x7B, Phi-3.5 MoE) were evaluated using temperature=0 and max_tokens=3 (or 1024 for CoT). Accuracy was measured against baselines (decision trees, kNN, MLP) and analyzed through slope fitting to quantify learning trends and robustness to distributional shifts.

## Key Results
- ICL performs learning: accuracy improves with more exemplars and converges across prompting strategies in the limit
- ICL exhibits poor distributional robustness: negative slopes (-0.6 to -1.4) indicate accuracy collapse as test distributions shift from training distributions
- ICL overfocuses on statistical features: word salad prompts match description prompts in the limit, and random label baselines sometimes outperform unshuffled baselines under distributional shifts

## Why This Works (Mechanism)

### Mechanism 1: Pattern Deduction from Prompt Regularities
ICL learns by extracting statistical regularities from exemplars rather than understanding task semantics. As exemplars increase, the model identifies recurring patterns in input-output mappings via next-token prediction, with the system prompt's contribution vanishing in the limit. This works when the exemplar distribution P is representative of the test distribution Q, but fails when distributions differ (OOD brittleness with δ → 0.85 causing accuracy collapse).

### Mechanism 2: Ad-hoc Encoding via Autoregression
ICL encodes observations temporarily through prompt context rather than permanent weight updates, creating a task-specific but fragile representation. The autoregressive paradigm conditions on prompt components recursively without changing frozen weights, simulating memory but lacking explicit state management. This insufficiently simulates stack-based computation for PDA-level tasks, resulting in performance gaps (94% for FSA vs 61-73% for PDA tasks).

### Mechanism 3: Statistical Feature Overfocus
ICL overfits to spurious statistical features in the observed training distribution rather than learning underlying data relations. The model prioritizes surface-level regularities (token patterns, positional frequencies) over relational structure, manifesting as sensitivity to label imbalance, exemplar ordering, and lexical features. This explains why word salad prompts match natural language prompts in the limit and why OOD brittleness is severe when exemplars are fully randomized.

## Foundational Learning

- Concept: **PAC Learning Framework**
  - Why needed here: The paper reframes Valiant's PAC learning to evaluate whether ICL constitutes genuine learning (bounded error on Q ≠ P). Understanding Eq. 1-2 is essential to interpret the robustness results.
  - Quick check question: Can you explain why a learner achieving 95% accuracy on distribution P might still fail on distribution Q in PAC terms?

- Concept: **Chomsky Hierarchy and Automata Theory**
  - Why needed here: Tasks are classified by the automaton required (FSA vs. PDA), and performance differences reveal autoregression's computational limits. Stack and Reversal (PDA tasks) underperform vs. Pattern Matching (FSA).
  - Quick check question: Why would a task requiring stack memory (PDA) be harder for an autoregressive model than a finite-state task (FSA)?

- Concept: **Distributional Shift (OOD)**
  - Why needed here: The paper's core robustness measure is accuracy degradation as ||P - Q||∞ = δ increases. Interpreting the negative slopes in Table 2 requires understanding why train-test distribution mismatch breaks pattern deduction.
  - Quick check question: If a model trained on short strings (δ=0) is tested on strings 7× longer (δ=0.85 for Reversal), what type of distributional shift is occurring?

## Architecture Onboarding

- Component map: System prompt (p), exemplars (π(eᵢ)), query (π̃(xₖ)) → feed into autoregressive decoder → accuracy/slope computation
- Critical path: Define task with automaton G and alphabet Σ → Generate training set (2000 samples from P) and test sets (5 × 2000 samples with δ ∈ {0, 0.2, 0.45, 0.65, 0.85}) → Select prompting strategy and shot count (0, 2, 5, 10, 20, 50, 100) → Call LLM with temperature=0, parse single-token output → Compute accuracy and fit OLS slopes over shots (learning trend) and δ (robustness)
- Design tradeoffs: Shots vs. cost (peak at 50-100 shots increases API costs 10-50×); Prompt strategy vs. OOD robustness (CoT/APO highest peak but most brittle); Natural language vs. formal encoding (word salad matches description in the limit)
- Failure signatures: Parsing errors (2-shot modus ponens highest error rates); Out-of-token errors (complex tasks exceed context windows); Negative shot slopes (CoT shows decreasing accuracy with more exemplars under alternate distributions)
- First 3 experiments: 1) Baseline convergence test: Run target task with modus ponens at 0, 10, 50, 100 shots on ID data; confirm positive slope and narrowing variance. 2) OOD robustness probe: Fix shot count at best-performing level; test across δ ∈ {0, 0.45, 0.85} with CoT and modus ponens; identify δ threshold where accuracy drops >15%. 3) Prompt strategy ablation: At fixed shot count and δ=0, compare Description, CoT, and Word Salad; if Word Salad matches Description within σ, task is solvable via statistical features alone.

## Open Questions the Paper Calls Out

### Open Question 1
Do models with "baked-in" chain-of-thought (reasoning models) act as robust Pushdown Automata (PDAs) and generalize to context-sensitive languages? The study was restricted to standard autoregressive models with external CoT prompts and did not evaluate models with internal, immutable reasoning chains. Empirical evaluation of reasoning models on the paper's formal language tasks, specifically testing for generalization on context-sensitive languages beyond the FSA/PDA tasks used here, would resolve this question.

### Open Question 2
To what extent does tokenization, specifically BPE, impair ICL performance on non-decision problems like arithmetic? The authors note that tokenization could explain the failure in the Vending Machine (Sum) task, and their finding regarding data features applies "only to decision problems." Comparing ICL performance on arithmetic tasks using character-level tokenization versus standard sub-word tokenization would isolate the impact of input representation.

### Open Question 3
Can the "ad-hoc encoding" mechanism of autoregression be replaced or augmented to improve robustness to distributional shifts? The study identifies autoregression's "ad-hoc encoding" as the root cause of brittleness but stops short of proposing or testing architectural alternatives. Designing architectural variants that explicitly encode feature relations rather than relying on prompt statistics, and testing their OOD performance against the baselines established in this paper, would resolve this question.

## Limitations

- The study uses synthetic formal languages that may not generalize to real-world natural language tasks, raising questions about ecological validity.
- The paper doesn't investigate whether more sophisticated prompting strategies could mitigate OOD brittleness, potentially underestimating ICL's capabilities.
- The study assumes temperature=0 for all experiments, which may not reflect practical usage scenarios where stochasticity could improve robustness.

## Confidence

**High** for computational limitations claims: The performance gap between FSA and PDA tasks (94% vs 61-73%) is well-documented and directly attributable to autoregressive models' inability to simulate stack-based computation.

**Medium** for distributional robustness findings: While the paper systematically demonstrates ICL's brittleness to OOD shifts, the synthetic nature of the automata-based tasks may not fully capture real-world distributional complexity.

**Low** for claims about ICL constituting "learning" in the human sense: The paper demonstrates that ICL exhibits learning-like behavior, but this may be pattern matching rather than genuine understanding.

## Next Checks

1. **Real-world distributional shift validation**: Replicate the δ-slope analysis using natural language tasks with controlled semantic shifts (e.g., same task on different domains or writing styles) to test whether synthetic task findings generalize.

2. **Prompt strategy innovation test**: Implement and test advanced prompting techniques (chain-of-verification, self-consistency sampling, or learned prompt optimizers) to determine if negative δ-slopes can be substantially improved beyond the seven strategies examined.

3. **Memory mechanism ablation**: Design experiments comparing ICL performance on tasks requiring different memory structures (stack, queue, working memory) to more precisely map autoregressive models' computational limitations and identify which tasks benefit most from alternative architectures.