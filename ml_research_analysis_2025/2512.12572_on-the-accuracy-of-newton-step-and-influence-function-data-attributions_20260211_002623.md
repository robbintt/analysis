---
ver: rpa2
title: On the Accuracy of Newton Step and Influence Function Data Attributions
arxiv_id: '2512.12572'
source_url: https://arxiv.org/abs/2512.12572
tags:
- lemma
- bound
- theorem
- data
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the accuracy of data attribution methods, specifically
  Influence Functions (IF) and single Newton Steps (NS), for convex learning problems.
  The authors address the limitations of existing analyses, which rely on unrealistic
  global strong convexity assumptions and poor scaling with problem dimension and
  number of samples removed.
---

# On the Accuracy of Newton Step and Influence Function Data Attributions

## Quick Facts
- arXiv ID: 2512.12572
- Source URL: https://arxiv.org/abs/2512.12572
- Authors: Ittai Rubinstein; Samuel B. Hopkins
- Reference count: 40
- One-line primary result: Proves tight Θ(kd/n²) error for Newton Step and Θ((k+d)√(kd)/n²) for IF in logistic regression with Gaussian features

## Executive Summary
This paper analyzes the accuracy of data attribution methods, specifically Influence Functions (IF) and single Newton Steps (NS), for convex learning problems. The authors address the limitations of existing analyses, which rely on unrealistic global strong convexity assumptions and poor scaling with problem dimension and number of samples removed. The core method introduces a new analysis that assumes only local strong convexity and Lipschitzness of the Hessian in a small neighborhood of the Newton step, allowing for tighter bounds without requiring strong convexity over the entire optimization domain.

## Method Summary
The paper provides a theoretical analysis of data attribution accuracy for convex empirical risk minimization problems, specifically logistic regression with Gaussian features. The method involves comparing three approaches: the true parameter after retraining (ground truth), Newton Step approximation (using the Hessian after removal), and Influence Function approximation (using the original Hessian). The analysis assumes logistic regression with Gaussian features, regime where n ≫ k², d², and characterizes the expected L2 error between true retrain θ_T and approximations (NS, IF).

## Key Results
- NS data attribution achieves Θ(kd/n²) expected error for average-case sample removals
- IF error compared to NS is Θ((k+d)√(kd)/n²), explaining why NS typically outperforms IF in practice
- The analysis relies on local strong convexity and Lipschitzness assumptions rather than global strong convexity
- Results apply to both random and adversarial drop-sets, providing insights into scaling behavior

## Why This Works (Mechanism)

### Mechanism 1: Local Strong Convexity in Newton Step Neighborhood
- Claim: NS data attribution achieves tight error bounds by requiring strong convexity only in a small ellipsoidal neighborhood around the first Newton step, rather than globally.
- Mechanism: The proof shows that if the true solution θ̂_T lies within a Σ-ball of radius r around θ̂^NS_T where the Hessian has bounded inverse spectrum (C_op), and the Hessian changes slowly along the Newton direction (C_h), then the NS error is bounded by C_h × C_op.
- Core assumption: The Hessian inverse operator norm is bounded within the local neighborhood: ||Σ^(1/2) H_θ^(-1) Σ^(1/2)||_op ≤ C_op for all θ in the ball B around θ̂^NS_T.
- Evidence anchors: [abstract]: "introducing a new analysis that assumes only local strong convexity and Lipschitzness of the Hessian in a small neighborhood of the Newton step"
- Break condition: If the Newton step moves far from θ̂ (large gradient), the local neighborhood may not contain θ̂_T, violating Condition 6 (C_h × C_op < r).

### Mechanism 2: Quadratic Approximation Error from Third-Order Curvature
- Claim: The NS approximation error scales as Θ(kd/n²) for average-case removals because the gradient after one Newton step has norm Ω(n||Δ||²_2) where Δ is the Newton step.
- Mechanism: The third derivative tensor T_θ along the Newton path creates a residual gradient after one step. For logistic regression with Gaussian features, the expected third derivative satisfies E[T_θ(v,v,θ)] = -Θ(1)||v||²_2||θ||²_2, creating a consistent negative contribution to the gradient at θ̂^NS_T that cannot be eliminated by a single Newton iteration.
- Core assumption: The third-order derivative tensor has bounded operator norm O(n) with very high probability for all θ on the Newton path.
- Evidence anchors: [section 3.3, Lemma C.3]: Proves ||g_{θ̂^NS_T}||_2 ≥ Ω(n||Δ||²_2) when step is small
- Break condition: If ||Δ||_2 is large (> 1/polylog(n)), the quadratic approximation degrades and higher-order terms dominate.

### Mechanism 3: Influence Function Underestimates Removal Effects in High Dimensions
- Claim: IF error relative to NS is Θ((k+d)√(kd)/n²), dominating NS error when d >> k, because IF uses the wrong Hessian.
- Mechanism: IF uses H^(-1) at the original model θ̂, while NS uses H^(-1)_w from the weighted loss after removal. The error decomposes as (H^(-1)_w - H^(-1)) × g, where the Hessian difference captures the curvature change from dropping samples.
- Core assumption: The dropped samples' gradients do not "miss" the important eigendirections of the Hessian change (formalized through decoupling arguments in Lemma D.4).
- Evidence anchors: [section 1.2, Theorem 1.2]: Explicit error bounds showing IF error exceeds NS when d >> k
- Break condition: If k >> d, the IF and NS errors become comparable since Hessian change affects fewer eigendirections.

## Foundational Learning

- Concept: **Self-Concordant Analysis for Logistic Regression**
  - Why needed here: The paper uses self-concordance theory (citing [Bac10, HM24]) to prove that θ̂_T must lie within the neighborhood B around θ̂^NS_T, by showing that moving away from θ̂^NS_T increases the inner product with the gradient.
  - Quick check question: Can you explain why strong convexity in a local neighborhood suffices to prove global convergence for self-concordant functions?

- Concept: **Empirical Risk Minimization with Convex Losses**
  - Why needed here: The entire analysis assumes the loss L = Σ ℓ_i(θ) is convex, with gradients g and Hessians H, enabling use of convex optimization guarantees and Newton's method convergence theory.
  - Quick check question: For logistic regression, why does the Hessian H_θ = Σ β_i x_i x_i^T become ill-conditioned as ||θ|| → ∞?

- Concept: **Hessian Concentration for Sub-Gaussian Features**
  - Why needed here: The asymptotic analysis relies on uniform concentration of the empirical Hessian to its expectation (Lemma G.9), showing that ||H^(-1)||_op = O(1/n) with high probability for well-behaved logistic regression.
  - Quick check question: How does the sample complexity n ≥ d(log d)^O(1) ensure uniform convergence of the Hessian over the bounded parameter region?

## Architecture Onboarding

- Component map:
  - Data Attribution Query: Input (drop set T, size k) → Output (estimate of θ̂_T - θ̂)
  - NS Estimator: θ̂^NS_T = θ̂ - H_T^(-1) × g_T (requires computing Hessian inverse after removal)
  - IF Estimator: θ̂^IF_T = θ̂ - H^(-1) × g_T (uses original Hessian, additive over samples)
  - DRIF Estimator (proposed): θ̂^DRIF_T = θ̂ + (n/(n-k)) × Σ_{i∈T} H_{\{i\}}^(-1) × g_i (additive, nearly NS-accurate)

- Critical path:
  1. Train model to convergence on full dataset → get θ̂, g, H
  2. For IF: compute H^(-1) × g_T in O(d³ + kd²) or use iterative methods
  3. For NS: recompute Hessian after removal (requires second pass over data) → compute H_T^(-1) × g_T
  4. For DRIF: precompute H_{\{i\}}^(-1) for each sample (O(nd³) naive) → sum contributions from T

- Design tradeoffs:
  - **IF vs NS**: IF is O(k) faster (additive) but less accurate when d >> k; NS requires Hessian recomputation
  - **DRIF compromise**: Nearly NS-accurate (error Õ(kd/n²)) with additive structure, but requires precomputing n inverse Hessians
  - **Regularization**: Adding λ||θ||² creates global strong convexity but makes bounds scale as 1/λ³, rendering them too loose for practical λ = O(d/n)

- Failure signatures:
  - **Large Newton step**: If ||Δ||_2 = ||H^(-1)g||_2 > 1/polylog(n), the local analysis breaks down; consider multiple Newton iterations or line search
  - **Adversarial drop sets**: Worst-case error grows as Θ(k²/n²) rather than Θ(kd/n²); IF/NS may both fail
  - **Insufficient samples**: When n < d² or n < k², concentration bounds fail and error bounds may not hold

- First 3 experiments:
  1. **Scaling validation on synthetic data**: Generate logistic regression with n ∈ {10⁴, 10⁵}, d ∈ {100, 1000}, k ∈ {1, 10, 100}. Measure ||θ̂_T - θ̂^NS_T|| and ||θ̂^NS_T - θ̂^IF_T||; verify Θ(kd/n²) and Θ((k+d)√(kd)/n²) scaling.
  2. **NS vs IF comparison on real data**: Use last-layer logistic regression on CIFAR/MNIST features. Plot correlation between NS and IF estimates vs ground truth retrain; measure accuracy gap as function of d/n ratio.
  3. **DRIF ablation**: Compare DRIF vs RIF vs NS for random and adversarial drop sets (e.g., dropping high-gradient samples). Verify DRIF achieves NS-like accuracy for random drops while maintaining additivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the error scaling laws for Influence Functions (IF) and Newton Step (NS) methods in the non-asymptotic regime where the number of samples $n$ is not significantly larger than the dimension $d^2$ or the removal set size $k^2$?
- Basis in paper: [explicit] The paper states that Theorem 1.2 requires the large-$n$ regime ($n \gg k^2, d^2$) and explicitly identifies "characterizing the scaling rates allowing $n \geq d$ is an interesting direction for future work."
- Why unresolved: The current proof techniques rely on concentration bounds and assumptions (like small Newton step size) that break down or become too loose when $n$ is not dominant over $d^2$.
- What evidence would resolve it: A theoretical extension of Theorem 1.2 providing tight upper and lower bounds for the under-parametrized or critically sampled regimes, or empirical simulations demonstrating scaling behavior when $n \approx d^2$.

### Open Question 2
- Question: Can the guarantees for NS and IF accuracy be rigorously extended to non-convex optimization landscapes, such as those found in deep neural networks?
- Basis in paper: [inferred] The paper notes that while data attribution for neural nets often reduces to convex problems (e.g., via TRAK), "our theorems do not directly speak to the effectiveness of IF methods applied to neural nets."
- Why unresolved: The proofs rely heavily on local strong convexity (Assumption 4) and properties of the logistic loss, which do not hold globally in non-convex settings.
- What evidence would resolve it: A formal analysis of NS/IF accuracy in non-convex settings assuming local convexity near the minimizer, or bounds specific to the Neural Tangent Kernel (NTK) regime.

### Open Question 3
- Question: Does the Newton Step method maintain superior directional accuracy over Influence Functions, matching its superior norm accuracy?
- Basis in paper: [explicit] The authors note in the limitations: "our analysis focuses on the norms of the estimation errors, even though we may often also be interested in the direction of these errors."
- Why unresolved: The current bounds use $\ell_2$ norms; the analysis of the angle between the predicted update vector $\hat{\theta}_T^{\text{NS}}$ and the true update $\hat{\theta}_T$ requires different geometric tools.
- What evidence would resolve it: A theorem bounding the cosine similarity or angular error between the estimated and true parameter shifts, or empirical plots showing directional error scaling with $n, d, k$.

## Limitations

- The analysis is limited to convex ERM problems and does not directly apply to non-convex deep learning models
- The theoretical bounds rely on idealized assumptions (Gaussian features, exact logistic regression) that may not hold in practical scenarios
- The computational complexity of exact Newton steps remains prohibitive for large-scale applications, limiting practical applicability

## Confidence

- **High Confidence**: Local strong convexity analysis and its implications for Newton step error bounds
- **Medium Confidence**: Scaling relationships between NS and IF errors (Θ(kd/n²) vs Θ((k+d)√(kd))/n²) for Gaussian features
- **Low Confidence**: Extension to non-convex models and real-world datasets with complex feature distributions

## Next Checks

1. **Scaling Verification**: Implement synthetic experiments to empirically validate the predicted Θ(kd/n²) and Θ((k+d)√(kd))/n² scaling laws across different (n, d, k) regimes.

2. **Adversarial Drop Set Analysis**: Systematically test the worst-case error bounds by constructing adversarial drop sets (e.g., high-gradient samples) and measuring deviation from theoretical predictions.

3. **Regularization Impact Study**: Compare error bounds and empirical performance across different regularization strengths (λ) to understand the trade-off between numerical stability and theoretical looseness mentioned in the paper.