---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Mental Illnesses in
  Arabic Context
arxiv_id: '2501.06859'
source_url: https://arxiv.org/abs/2501.06859
tags:
- arabic
- datasets
- performance
- dataset
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluated 8 large language models on
  Arabic mental health datasets, examining prompt design, language configuration,
  and few-shot prompting effects. Results showed that prompt engineering significantly
  influenced performance, with structured prompts outperforming less structured ones
  by 14.5% on multi-class datasets.
---

# A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context

## Quick Facts
- **arXiv ID**: 2501.06859
- **Source URL**: https://arxiv.org/abs/2501.06859
- **Reference count**: 40
- **Primary result**: This study comprehensively evaluated 8 large language models on Arabic mental health datasets, examining prompt design, language configuration, and few-shot prompting effects.

## Executive Summary
This study presents a comprehensive evaluation of eight large language models (LLMs) on mental health diagnostic tasks in Arabic, examining how prompt engineering, language configuration, and few-shot learning affect performance. The research systematically compares zero-shot and few-shot prompting across binary classification, multi-class classification, severity prediction, and MCQ knowledge evaluation tasks using both native Arabic and English-translated datasets. Results demonstrate that structured prompts significantly outperform less structured ones, Phi-3.5 MoE excels in balanced accuracy while Mistral NeMo shows superior performance in severity prediction, and few-shot prompting consistently improves accuracy across models.

The study reveals important nuances in multilingual performance, finding that English configuration generally outperforms Arabic by 6% on average, though Arabic can perform better under specific conditions when maintaining Arabic prompts with translated datasets. These findings have practical implications for developing culturally sensitive LLM-based mental health tools for Arabic-speaking populations, highlighting the importance of prompt optimization and the potential of few-shot learning to enhance diagnostic accuracy in resource-constrained settings.

## Method Summary
The study evaluated eight LLMs (GPT-4o, GPT-4o Mini, Llama-3.1-70B, Phi-3.5 MoE, Mistral Large 2, Mistral NeMo, Gemma2-9B, and Command R+) on mental health diagnostic tasks using zero-shot and few-shot prompting approaches. Native Arabic datasets (DCAT, MCD, ARADEPSU, CAIRODEP, AMI, MDE) and English datasets translated to Arabic via Google Translate (Dreaddit, DEPSEVERITY, SAD, DEPTWEET, SDCNL, RED SAM, MedMCQA) were used across four task types: binary classification (depressed/normal), multi-class classification (depression/anxiety/suicidality), severity prediction (0-N scale), and MCQ knowledge evaluation. Two prompt templates were tested: ZS-1 (less structured with "act as psychologist" instruction) and ZS-2 (structured without role). Performance was measured using Balanced Accuracy and Mean Absolute Error, with max 1000 examples sampled per dataset.

## Key Results
- Prompt engineering significantly influenced performance, with structured prompts (ZS-2) outperforming less structured ones (ZS-1) by 14.5% on multi-class datasets
- Phi-3.5 MoE excelled in balanced accuracy for binary classification tasks, while Mistral NeMo showed superior performance in mean absolute error for severity prediction tasks
- Few-shot prompting consistently improved performance, with GPT-4o Mini achieving a 1.58x accuracy boost on multi-class classification
- English configuration outperformed Arabic by 6% on average, though translating Arabic datasets to English while maintaining Arabic prompts showed Arabic could perform better (17.3% difference on binary tasks)

## Why This Works (Mechanism)
The performance differences observed can be attributed to how model architecture interacts with prompt structure and language configuration. Structured prompts (ZS-2) provide clearer task boundaries and expected output formats, reducing ambiguity in complex classification tasks. The 14.5% improvement in multi-class datasets suggests that models benefit from explicit format constraints when handling nuanced mental health categories. Few-shot prompting leverages in-context learning capabilities, allowing models to extract patterns from demonstration examples and apply them to new cases. The multilingual performance gap likely stems from training data distribution, where English-dominant models show better generalization in English configuration, while Arabic prompts may reduce translation artifacts when paired with translated datasets.

## Foundational Learning
1. **Zero-shot prompting** - Providing task instructions without examples to evaluate model generalization capabilities; needed to establish baseline performance across models without adaptation
2. **Few-shot prompting** - Including 1-3 examples per class in prompts to demonstrate task format; needed to assess how demonstration examples improve model accuracy on specialized domains
3. **Prompt engineering** - Systematic modification of prompt structure, role instructions, and formatting to optimize model responses; needed to identify optimal configurations for mental health diagnostics
4. **Multilingual evaluation** - Testing models across Arabic and English configurations to assess language-specific performance differences; needed to understand cultural and linguistic factors in mental health assessment
5. **Balanced Accuracy metric** - Averaging recall across all classes to account for class imbalance in mental health datasets; needed to fairly evaluate models on skewed psychiatric data
6. **Mean Absolute Error normalization** - Scaling prediction errors by severity scale range to enable cross-dataset comparison; needed to quantify severity prediction accuracy across different rating scales

## Architecture Onboarding

**Component map**: Datasets (Arabic/English) -> Prompt Templates (ZS-1/ZS-2/Few-shot) -> API Calls -> Response Parsing -> Metrics Calculation -> Model Ranking

**Critical path**: Dataset sampling → Prompt template generation → API query execution → Response parsing → Metric computation → Performance comparison

**Design tradeoffs**: Zero-shot vs. few-shot prompting (simplicity vs. accuracy), Arabic vs. English configuration (cultural relevance vs. model performance), structured vs. unstructured prompts (format adherence vs. flexibility)

**Failure signatures**: High invalid response rates (>90%) on multi-class tasks with ZS-1 prompts indicate models ignoring format constraints; parsing failures occur when models output explanations instead of single-word answers

**First experiments**:
1. Test ZS-1 and ZS-2 prompts on binary classification task to establish baseline performance difference
2. Implement few-shot prompting with 1 example per class to measure accuracy improvement
3. Compare English vs. Arabic configuration on translated datasets to validate language performance differences

## Open Questions the Paper Calls Out
The study acknowledges several critical open questions that emerged from the research findings. First, how do these prompt engineering approaches generalize to clinical datasets beyond the publicly available ones used in this study? Second, what are the long-term performance implications of using translated datasets versus native language data in mental health applications? Third, how might these findings translate to other low-resource languages where similar translation approaches would be necessary? The authors suggest these questions warrant further investigation to establish the robustness and clinical applicability of their findings.

## Limitations
- Exclusive focus on zero-shot and few-shot prompting without exploring fine-tuning approaches
- Reliance on publicly available datasets that may introduce sampling bias and limit generalizability to clinical settings
- Translation of English datasets to Arabic using Google Translate, introducing potential quality variations that could affect cross-linguistic comparisons

## Confidence
- Prompt engineering effects (14.5% performance difference): High
- Multilingual analysis showing English outperforming Arabic by 6% on average: Medium
- Specific model performance rankings and rankings: Medium

## Next Checks
1. Conduct sensitivity analysis on prompt template variations to establish the robustness of the 14.5% performance difference between structured and less structured prompts
2. Implement cross-validation on the dataset sampling process to quantify the variance in model performance rankings
3. Replicate the English-Arabic performance comparison using professionally translated datasets to validate the 6% average difference and the specific finding that Arabic can outperform English (17.3% difference) under certain conditions