---
ver: rpa2
title: Emergent morpho-phonological representations in self-supervised speech models
arxiv_id: '2509.22973'
source_url: https://arxiv.org/abs/2509.22973
tags:
- word
- noun
- figure
- verb
- wav2vec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-supervised speech models (S3Ms) can recognize spoken words
  with high accuracy, but the underlying linguistic representations remain unclear.
  This paper investigates whether S3Ms rely on distinct phonological, morphological,
  or lexical representations for word recognition.
---

# Emergent morpho-phonological representations in self-supervised speech models

## Quick Facts
- arXiv ID: 2509.22973
- Source URL: https://arxiv.org/abs/2509.22973
- Reference count: 35
- Key outcome: Word-optimized subspaces in S3Ms encode distributional phonological constraints rather than explicit morphological categories

## Executive Summary
Self-supervised speech models can recognize spoken words with high accuracy, but the underlying linguistic representations remain unclear. This paper investigates whether S3Ms rely on distinct phonological, morphological, or lexical representations for word recognition. The authors develop a word-optimized probe that isolates the subspace of S3M representations used for word contrast, then test whether these representations encode morphological distinctions, phonological differences, lexical contrasts, or distributional phonological patterns. The probe successfully links inflected forms to their base forms through a global linear geometry, performing well above chance across multiple experiments. However, this geometry does not directly track morphological or phonological units. Instead, it captures the distributional constraints governing word-final sounds [z], [s], and [Iz] in English, which determine how noun plurals and verb inflections surface.

## Method Summary
The authors train linear probes on Wav2Vec2 activations to isolate word-optimal representations. Each probe is a 32-dimensional linear projection trained with contrastive hinge loss to discriminate word identities. Frame-level probe outputs are mean-pooled within word boundaries to create fixed-length word embeddings. The authors then evaluate these embeddings using analogy tasks (b - a + c) to test systematic relationships between base and inflected forms. Multiple experiments examine sensitivity to morphological categories, allomorphic variation, and phonological distributional patterns using stimuli from LibriSpeech.

## Key Results
- Word probes are less sensitive to allomorphic and morphological distinctions than Wav2Vec2 baseline
- Forced-choice experiments show word probes prefer phonologically consistent items (obeying voicing-based allomorphy) over morphologically related items
- The linear geometry linking inflected forms to base words performs above chance across noun/verb categories and allomorph types

## Why This Works (Mechanism)

### Mechanism 1
Optimizing a linear probe on Wav2Vec2 activations for word discrimination yields a subspace that encodes distributional phonological constraints rather than explicit morphological categories. A linear projection z(t) = W_z · x(t) is trained with a contrastive hinge loss that pulls same-word frames together and pushes different-word frames apart. This objective implicitly privileges subspaces where distributionally similar phonological alternates are treated as functionally equivalent for word identity, collapsing morphological and allomorphic distinctions while preserving the abstract voicing-based pattern.

### Mechanism 2
Self-supervised pre-training on raw audio encodes fine-grained phonetic and allomorphic information that is progressively discarded when optimizing for word-level tasks. Wav2Vec2 learns frame-level representations that capture sub-phonemic detail and allomorphic alternations. The word probe, trained on top, reduces sensitivity to these distinctions because they are not necessary for discriminating word types; instead, it retains higher-level distributional regularities that generalize across allomorphs and morphological categories.

### Mechanism 3
Vector arithmetic in the word-optimal subspace supports systematic analogy completion across morphological categories, indicating a global linear geometry for inflectional relationships. The difference vectors between base and inflected forms cluster along a consistent direction in principal component space. Adding this vector to a base form of a different word yields embeddings close to the correct inflected form, even across noun/verb categories and allomorph types.

## Foundational Learning

- Concept: Contrastive learning objectives (hinge loss with positive/negative pairs)
  - Why needed here: The word probe is trained with a contrastive loss that maximizes within-word similarity and between-word distance; understanding this objective is essential to interpret what the probe learns to preserve or discard.
  - Quick check question: Given frames t, t+ from the same word and t− from a different word, what does the hinge loss L(t) = max(0, m + cos(z_t, z_{t+}) − cos(z_t, z_{t−})) optimize?

- Concept: Vector analogy arithmetic (Mikolov et al. 2013 style)
  - Why needed here: The paper's core evaluation uses b − a + c ≈ d to test whether the model encodes systematic relationships between base and inflected forms.
  - Quick check question: If "king − man + woman ≈ queen" in word embedding space, what does "shirts − shirt + cheese ≈ ?" test in this paper's acoustic word embedding space?

- Concept: English voicing-based allomorphy (distributional constraints on /-z/)
  - Why needed here: The paper tests whether the model has learned the abstract rule that /-z/ surfaces as [z] after voiced non-sibilants, [s] after voiceless non-sibilants, and [ɪz] after sibilants, independent of morphological category.
  - Quick check question: Why do "dogs" [dɔgz] and "cats" [kæts] both reflect the same underlying morpheme /-z/, and how does this paper test whether the model knows this?

## Architecture Onboarding

- Component map:
  Wav2Vec2 Base (12-layer Transformer) -> Linear probe (768→32 dim) -> Mean-pooled word embeddings -> Analogy evaluator (vector arithmetic + nearest-neighbor)

- Critical path:
  1. Pre-trained Wav2Vec2 is frozen; word probe is trained on 100h of word-aligned LibriSpeech
  2. Probe training uses contrastive hinge loss with margin m, optimized via AdamW with early stopping
  3. At inference, frame-level probe outputs are mean-pooled to word embeddings
  4. Analogy tasks compute difference vectors and evaluate via nearest-neighbor rank of the target word

- Design tradeoffs:
  - 32-dim probe output vs. 768-dim Wav2Vec: Lower dimensionality may compress information and affect what is preserved/discarded
  - Word-level pooling vs. phoneme-level pooling: Word pooling may erase temporal dynamics; paper shows results hold under phoneme-level pooling
  - Linear geometry assumption: The analogy method assumes inflectional relationships are linear translations; nonlinear relationships would not be captured

- Failure signatures:
  - Analogy rank close to random (94,306): Probe failed to learn word-discriminative subspace
  - Large performance gap between within-category and cross-category analogies: Probe has learned category-specific rather than abstract representations
  - Sensitivity to allomorph matches in the word probe: Objective may not have converged, or pooling strategy is biased

- First 3 experiments:
  1. Reproduce the word probe training at layer 8 on LibriSpeech train-clean-100 with the hyperparameters in Table 9; validate mean average precision on word classification
  2. Run the within-vs-cross morphological category analogy experiment (Section 4.1) on the trained probe and compare the interaction effect size to Wav2Vec baseline; confirm reduced morphology sensitivity
  3. Run the forced-choice phonological consistency experiment (Section 4.4) on the 35 triples in Table 10; verify that the word probe prefers the consistent item above chance

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed linear geometry linking inflected forms to base words generalize to other English morphological phenomena with dual phonological-morphological patterning (e.g., word-final "-er" in both comparative and agentive forms)? The current study only examines noun plurals and third-person verb inflections ending in [z], [s], or [ɪz].

### Open Question 2
Are the distributional sensitivity patterns found in word-optimal S3M representations specific to English phonological rules, or do they generalize across languages? The study uses only LibriSpeech English data; it remains unclear whether the pattern reflects language-specific learning or a more general representational strategy.

### Open Question 3
Do the findings depend on the assumption of linear translation relationships between word embeddings, or would nonlinear geometric relationships reveal different sensitivities to morphology and phonology? All analyses assume difference vectors represent morphological/phonological transformations linearly, which may oversimplify the embedding geometry.

### Open Question 4
Do these findings extend beyond Wav2Vec2 to other self-supervised speech model architectures and training objectives? Different S3M architectures (e.g., HuBERT, WavLM) and training objectives may encode linguistic information differently.

## Limitations
- The linear probe methodology may not capture the full complexity of how S3Ms represent linguistic structure; nonlinear probes might reveal different patterns
- The forced-choice experiments rely on manually constructed stimuli (35 triples) that may not comprehensively sample the space of morpho-phonological alternations
- The distributional pattern interpretation assumes English voicing-based allomorphy is the dominant constraint, but other phonological regularities could also shape the subspace geometry

## Confidence
- **High confidence**: The empirical finding that word probes are less sensitive to allomorphic and morphological distinctions than Wav2Vec2 baseline
- **Medium confidence**: The interpretation that distributional phonological constraints, rather than morphological categories, drive the word probe's performance
- **Low confidence**: The specific claim about linear geometry organizing inflectional relationships

## Next Checks
1. **Probe dimensionality ablation**: Train word probes with varying output dimensions (e.g., 16, 64, 128) and measure how analogy performance and phonological sensitivity change
2. **Cross-linguistic replication**: Apply the same methodology to a language with different morpho-phonological patterns (e.g., Turkish with vowel harmony, or Mandarin with tonal morphology)
3. **Nonlinear probe comparison**: Train a small MLP (one hidden layer) as an alternative to the linear probe and compare its sensitivity to morphological vs. phonological distinctions