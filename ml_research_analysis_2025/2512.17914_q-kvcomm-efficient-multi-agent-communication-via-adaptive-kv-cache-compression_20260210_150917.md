---
ver: rpa2
title: 'Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression'
arxiv_id: '2512.17914'
source_url: https://arxiv.org/abs/2512.17914
tags:
- compression
- quantization
- across
- information
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Q-KVComm tackles the inefficiency of multi-agent LLM systems that
  waste bandwidth by transmitting raw text instead of leveraging internal semantic
  representations. The paper introduces a protocol for direct transmission of compressed
  key-value (KV) cache representations between LLM agents.
---

# Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression

## Quick Facts
- arXiv ID: 2512.17914
- Source URL: https://arxiv.org/abs/2512.17914
- Reference count: 26
- Primary result: 5-6× compression ratio with >0.70 contextual relevance and >0.92 coherence across three QA datasets

## Executive Summary
Q-KVComm introduces a protocol for efficient multi-agent communication by transmitting compressed key-value (KV) cache representations instead of raw text between LLM agents. The system employs adaptive layer-wise quantization with variable bit-widths (4-8 bits) based on sensitivity profiling, hybrid information extraction to preserve critical facts across domains, and heterogeneous model calibration for cross-architecture communication. Experimental results demonstrate 5-6× compression ratios while maintaining semantic fidelity, achieving bandwidth savings of 1.86-5.23 GB and contextual relevance scores above 0.70.

## Method Summary
Q-KVComm transmits compressed KV cache representations between LLM agents using a four-stage pipeline: layer selection combining attention-based importance with Gaussian positional prior, information extraction using YAKE and NER patterns, adaptive layer-wise quantization with sensitivity-based bit allocation (30% layers at 8-bit, 40% at 6-bit, 30% at 4-bit), and cross-model calibration via z-score normalization. The protocol achieves 5-6× compression while maintaining semantic fidelity through variable bit-width allocation and heterogeneous model calibration.

## Key Results
- 5-6× compression ratios achieved across SQuAD, HotpotQA, and NarrativeQA datasets
- Contextual relevance scores above 0.70 and coherence scores exceeding 0.92 maintained
- Bandwidth savings of 1.86-5.23 GB demonstrated
- Robust performance across model sizes (1.1B-1.5B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Layer-wise Quantization
Allocating variable bit-widths (4-8 bits) based on per-layer sensitivity profiling preserves semantic fidelity under compression better than uniform quantization. During calibration, each layer is quantized at minimum bit-width and reconstruction error is measured via L2 norm between original and quantized-then-dequantized KV tensors. Layers ranked by sensitivity receive differential bit allocation: top 30% → 8-bit, middle 40% → 6-bit, bottom 30% → 4-bit. Asymmetric per-tensor quantization uses scale = (max - min) / (2^b - 1) with bit-packing for transmission.

### Mechanism 2: Hybrid Layer Selection via Attention-Gaussian Scoring
Combining attention-based importance with a Gaussian positional prior centered on middle layers identifies semantically critical layers more reliably than either signal alone. Attention importance S_a^l averages attention weights across heads and positions. Gaussian prior P^l = exp(-(l-μ)²/2σ²) with μ = γ_μL centers on middle layers. Combined score S_l = αS_a^l + (1-α)P^l ranks layers; top fraction selected for transmission.

### Mechanism 3: Cross-Model Calibration via Distributional Alignment
Zero-shot calibration using first/second-order statistics enables KV cache transfer between heterogeneous architectures without fine-tuning. Compute sender mean μ_s and std σ_s per layer over calibration data; similarly for receiver. Transform: KV_calibrated = (KV_s - μ_s)/σ_s × σ_r + μ_r. For dimension mismatches, use scalar statistics (averaged across dimensions).

## Foundational Learning

- **Concept: KV Cache in Transformers**
  - Why needed here: Q-KVComm transmits compressed KV caches; understanding that K/V store attention keys/values from prior tokens enables efficient autoregressive decoding by avoiding recomputation.
  - Quick check question: What information does the KV cache encode that enables it to substitute for reprocessing raw text?

- **Concept: Quantization (Asymmetric Per-Tensor)**
  - Why needed here: Core compression mechanism; understanding scale/zero-point computation and bit-packing explains how 4-8 bits approximate FP16 values.
  - Quick check question: Given tensor T with min=-2.5, max=5.0, what scale and zero-point for 4-bit quantization?

- **Concept: Distributional Alignment / Calibration**
  - Why needed here: Enables heterogeneous model communication; understanding that standardization followed by rescaling aligns statistical moments across models.
  - Quick check question: Why might scalar (dimension-averaged) statistics work when per-dimension statistics are incompatible?

## Architecture Onboarding

- **Component map:**
  Sender Agent → Layer Selection (attention + Gaussian scoring) → Info Extraction (YAKE/NER/patterns) → Quantization (sensitivity-based bit allocation) → Bit-Packing → Calibration Transform → Transmission
  Receiver Agent → Deserialization → Dequantization → Inverse Calibration → KV Integration (concatenate along sequence dimension) + Fact Summary Prepension

- **Critical path:** Sensitivity profiling (offline calibration) → determines bit allocation → enables quantization. If profiling is skipped or uses unrepresentative data, compression degrades semantic fidelity.

- **Design tradeoffs:**
  - 4-bit: Maximum compression (6.93×), higher latency due to quantization overhead; best for bandwidth-constrained scenarios
  - 8-bit: Best throughput (37% faster than 6-bit), lower compression (5.06×); best for latency-sensitive applications
  - 6-bit: Avoided by authors—neither compression nor speed advantages over alternatives
  - Layer selection ratio 0.7: Balances transmission size vs. semantic coverage; may need adjustment for task complexity

- **Failure signatures:**
  - Low contextual relevance (<0.4) on complex tasks: May indicate layer selection ratio too aggressive or calibration data mismatch
  - Higher latency at 6-bit than 4-bit or 8-bit: Expected due to quantization pipeline overhead without proportional gains
  - Coherence drop in multi-hop reasoning: HotpotQA shows lower scores; may require higher bit-width for reasoning chains
  - Cross-architecture degradation >5%: Calibration statistics may be insufficient or sender/receiver representations fundamentally misaligned

- **First 3 experiments:**
  1. Sensitivity profiling on calibration dataset: Measure E_l for each layer, verify top-30% sensitivity aligns with semantic importance (spot-check reconstruction error vs. downstream task performance).
  2. Layer selection ablation: Compare attention-only vs. Gaussian-only vs. hybrid (α=0.5) on SQuAD; validate that hybrid achieves more consistent compression ratios across datasets.
  3. Cross-model calibration test: Send KV cache from TinyLlama to Qwen2.5 with/without calibration; measure coherence and relevance degradation to quantify calibration benefit.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does Q-KVComm's compression ratio and semantic fidelity scale favorably to larger language models (7B+ parameters), or do fundamental limits emerge at greater scale?
  - Basis: The conclusion explicitly calls for extending Q-KVComm to larger models (7B+ parameters) as a promising direction, noting uncertainty about whether compression ratios will scale.
  - Why unresolved: All experiments were conducted on compact models (1.1B-1.5B parameters); the relationship between model scale and KV cache compressibility remains unexplored.

- **Open Question 2:** Do KV cache representations leak sensitive information about model internals or training data when shared between agents?
  - Basis: The conclusion states: "the security implications of sharing internal representations deserve investigation, as KV caches may leak information about model internals or training data."
  - Why unresolved: No security analysis was conducted; the paper focuses entirely on efficiency and semantic preservation without evaluating potential information disclosure.

- **Open Question 3:** Can streaming protocols for incremental KV cache updates enable efficient continuous communication in long-running multi-agent dialogues?
  - Basis: The conclusion identifies "streaming protocols for incremental KV cache updates" as future work for "continuous communication in long-running multi-agent dialogues without retransmitting entire contexts."
  - Why unresolved: The current protocol transmits entire compressed KV caches; no mechanism exists for partial or differential updates when context evolves.

## Limitations

- Evaluation limited to three QA datasets with relatively narrow complexity ranges, leaving open questions about performance on diverse task types including code generation, long-form reasoning, or multilingual contexts.
- Cross-architecture calibration lacks empirical validation showing failure cases or upper bounds on performance degradation when sender-receiver representations are fundamentally misaligned.
- Absence of ablation studies comparing hybrid attention-Gaussian layer selection against either component alone, making it difficult to validate whether the combination provides additive benefits.

## Confidence

- **High Confidence:** Layer-wise sensitivity-based quantization mechanism and its theoretical grounding in preserving semantically important information.
- **Medium Confidence:** Hybrid attention-Gaussian layer selection scoring function, as the paper provides the mathematical formulation but lacks comparative ablation studies.
- **Low Confidence:** Cross-model calibration effectiveness in worst-case scenarios (severe architectural mismatches) and the robustness of the approach across diverse task domains.

## Next Checks

1. **Ablation Study on Layer Selection:** Implement and compare attention-only, Gaussian-only, and hybrid layer selection with varying α values on the same three datasets to quantify the marginal benefit of combining both signals.

2. **Cross-Architecture Failure Analysis:** Systematically test calibration performance across increasingly dissimilar model pairs (e.g., decoder-only vs. encoder-decoder, different embedding dimensionalities) to identify breaking points and establish calibration failure bounds.

3. **Task Diversity Stress Test:** Evaluate Q-KVComm on non-QA tasks including code generation (HumanEval), mathematical reasoning (GSM8K), and multilingual tasks to assess generalization limits and identify task types where semantic fidelity degrades most significantly under compression.