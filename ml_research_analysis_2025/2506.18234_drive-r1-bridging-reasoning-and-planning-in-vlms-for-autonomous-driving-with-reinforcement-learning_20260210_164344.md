---
ver: rpa2
title: 'Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with
  Reinforcement Learning'
arxiv_id: '2506.18234'
source_url: https://arxiv.org/abs/2506.18234
tags:
- reasoning
- planning
- trajectory
- driving
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Drive-R1 addresses the challenge of bridging scenario-level reasoning
  and trajectory planning in autonomous driving. The method proposes a domain-specific
  vision-language model that combines supervised fine-tuning with reinforcement learning
  to align textual reasoning with numerical trajectory outputs.
---

# Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.18234
- Source URL: https://arxiv.org/abs/2506.18234
- Reference count: 40
- Primary result: L2 error of 0.31 meters and collision rate of 0.09 on nuScenes, outperforming end-to-end and vision-language baselines

## Executive Summary
Drive-R1 addresses the critical challenge of aligning textual reasoning with numerical trajectory planning in autonomous driving by developing a domain-specific vision-language model trained through supervised fine-tuning and reinforcement learning. The method introduces a two-stage supervised fine-tuning approach with adaptive chain-of-thought supervision and a Group Relative Policy Optimization (GRPO) framework with composite rewards to ensure reasoning traces align with trajectory outputs. Experiments on nuScenes and DriveLM-nuScenes benchmarks demonstrate state-of-the-art performance with an average L2 error of 0.31 meters and collision rate of 0.09, validating the effectiveness of the reasoning-planning alignment strategy.

## Method Summary
Drive-R1 employs a two-stage supervised fine-tuning process followed by reinforcement learning. The first stage adapts a base vision-language model (InternVL2-4B) to autonomous driving using 3 million domain-specific question-answer pairs. The second stage applies adaptive chain-of-thought supervision across five reasoning domains, using a difficulty proxy to assign short or long CoT based on scenario complexity. The model is then fine-tuned using Group Relative Policy Optimization with a composite reward function that includes trajectory accuracy, meta-action correctness, repetition penalty, and format compliance. The input consists of multi-view images, ego-states, historical trajectory, and mission goals, with outputs structured as reasoning traces followed by trajectory waypoints.

## Key Results
- Achieves 0.31m average L2 error on nuScenes, outperforming baselines including GPT-4o and InternVL2
- Reduces collision rate to 0.09 through meta-action reward and repetition penalty in GRPO
- Demonstrates 0.37m L2 error with 0.10 collision rate when RL is applied to domain-aligned models versus 0.59m L2 and 0.20 collision on base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage supervised fine-tuning mitigates shortcut learning where models over-rely on historical trajectory patterns instead of visual inputs.
- Mechanism: The first SFT stage on 3M domain-specific AD samples familiarizes the base VLM (InternVL2-4B) with driving scenarios. The second SFT stage on RP-CoT data enforces step-by-step reasoning from visual observations to trajectory outputs, explicitly breaking the dependency on textual history priors by requiring grounded intermediate reasoning.
- Core assumption: The base VLM has sufficient capacity to learn domain-specific visual grounding when forced through structured CoT supervision.
- Evidence anchors:
  - [abstract]: "VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs"
  - [Page 5, Table 1]: Models trained without CoT supervision perform better when visual inputs are removed (DS-WI: 0.56 L2 vs DS with images: 0.45 L2), demonstrating visual under-utilization
  - [corpus]: AlphaDrive (arXiv:2503.07608) similarly applies RL to VLMs in AD, suggesting domain adaptation is a recognized prerequisite
- Break condition: If the base model capacity is too limited (as suggested in Page 6 regarding "limited representation capacity of small-scale models"), CoT supervision may introduce noise rather than improve grounding.

### Mechanism 2
- Claim: Adaptive CoT length based on scenario complexity prevents reasoning misalignment that degrades trajectory precision.
- Mechanism: Rather than applying uniform CoT supervision, the method uses a proxy model to assess planning difficulty. Simple scenarios receive short CoT (minimal deliberation), while complex multi-agent scenes receive long CoT (rich step-by-step reasoning). This prevents "overthinking" noise injection in simple cases and provides sufficient reasoning depth in complex ones.
- Core assumption: A model trained to predict trajectories without CoT can reliably serve as a difficulty proxy for scenario classification.
- Evidence anchors:
  - [Page 5-6]: "applying long CoT supervision during the SFT stage lead to a decline in performance compared to directly supervising the final trajectory output"
  - [Page 6, Table 1]: Combining short and long CoT achieves 0.41 L2 error versus 0.56 (long only) or 0.45 (short only, implicit from DS baseline)
  - [corpus]: No direct corpus evidence on adaptive CoT length strategies in AD; this appears novel
- Break condition: If the difficulty classifier mischaracterizes scenarios, complex scenes may receive insufficient reasoning while simple scenes accumulate unnecessary noise.

### Mechanism 3
- Claim: Group Relative Policy Optimization (GRPO) with composite rewards aligns textual reasoning quality with numerical trajectory accuracy.
- Mechanism: GRPO samples multiple candidate outputs per input, computes group-relative advantages, and updates policy to prefer candidates with higher rewards. The composite reward—trajectory accuracy (outcome-level), meta-action correctness (process-level), repetition penalty, and format compliance—creates complementary signals that reinforce reasoning-planning alignment. Process-oriented meta-action rewards and outcome-oriented trajectory rewards are positively correlated, enabling mutual reinforcement.
- Core assumption: The model has sufficient domain alignment before RL; otherwise, reinforcement signals cause unstable updates.
- Evidence anchors:
  - [Page 7, Section 3.3.2]: "our analysis shows that the result-oriented trajectory reward and the process-oriented meta-action reward are positively correlated"
  - [Page 9, Table 3]: Adding meta-action reward and repetition penalty reduces collision rate from 0.18 to 0.10 while L2 remains stable at 0.37
  - [Page 9, Table 1]: RL on domain-specific model with both CoT types achieves best performance (0.37 L2, 0.10 collision), whereas RL on base model shows minimal improvement
  - [corpus]: COVLM-RL (arXiv:2512.09349) also applies RL for VLM reasoning in AD, suggesting this is an emerging direction
- Break condition: If domain alignment is insufficient (e.g., applying RL directly to base model), policy updates become unstable. Excessive rollouts (24) cause training collapse.

## Foundational Learning

- Concept: **Vision-Language Model (VLM) grounding problem**
  - Why needed here: The paper's central challenge is that VLMs can achieve competitive planning metrics without using visual inputs—relying on textual shortcuts instead. Understanding why multimodal models sometimes ignore visual modalities is prerequisite to appreciating the two-stage SFT solution.
  - Quick check question: Can you explain why a model might predict accurate trajectories from text alone, and why this is problematic for real-world deployment?

- Concept: **Chain-of-Thought (CoT) supervision tradeoffs**
  - Why needed here: The paper finds that indiscriminate CoT supervision degrades performance. Understanding that reasoning traces can propagate hallucinations to numerical outputs—and that complexity must match scenario difficulty—is essential for the fast-and-slow thinking strategy.
  - Quick check question: Why might forcing elaborate reasoning on simple scenarios hurt rather than help planning accuracy?

- Concept: **Group Relative Policy Optimization (GRPO) intuition**
  - Why needed here: Unlike standard RL that optimizes absolute rewards, GRPO compares multiple candidates within a group. This relative comparison is suited for motion planning where multiple plausible trajectories exist. Understanding why relative optimization helps generalization is key to the RL phase.
  - Quick check question: In a scenario with three reasonable trajectory candidates, how does comparing them against each other differ from scoring each independently?

## Architecture Onboarding

- Component map:
  - Input Layer: Multi-view images (6 views), ego-states (velocity, acceleration, steering), historical trajectory (2s), mission goal
  - Vision Encoder + LLM Backbone: InternVL2-4B adapted via two-stage SFT
  - RP-CoT Module: Five-domain reasoning (traffic knowledge, element recognition, graph generation, attribute comprehension, decision/planning)
  - Output Layer: Structured `<think reasoning> <trajectory>` format with 6 waypoints over 3 seconds
  - RL Optimizer: GRPO with composite reward (trajectory, meta-action, format, repetition)
  - Training Pipeline: Stage 1 SFT (3M samples) → Stage 2 SFT (4K RP-CoT, adaptive length) → RL (GRPO, 6 rollouts)

- Critical path: Image inputs → Vision encoder → Domain-adapted VLM → CoT reasoning generation → Trajectory decoding → Reward computation → Policy update (if RL phase). The paper emphasizes that visual grounding must be enforced at Stage 2 SFT; RL alone cannot recover from weak grounding.

- Design tradeoffs:
  - **Short vs. Long CoT**: Use difficulty proxy model to assign; incorrect assignment causes noise (overthinking) or insufficient reasoning
  - **Rollout count**: 6 rollouts stable; 12 acceptable; 24 causes training collapse for small models
  - **Reward weights**: Meta-action reward critical for collision reduction; trajectory reward alone insufficient for safety metrics
  - **Model scale**: 4B parameter model shows capacity limits—complex CoT may exceed representation capacity

- Failure signatures:
  - **Visual grounding loss**: Model performs equally well or better without images (check by ablating visual input)
  - **CoT-Planning misalignment**: Long CoT supervision causes L2 degradation (Page 6)
  - **RL instability**: Loss spikes or policy degradation when applied to insufficiently domain-aligned models or with excessive rollouts
  - **Over-reliance on history**: Shortcut learning manifests as minimal performance gap between with/without image conditions

- First 3 experiments:
  1. **Visual grounding ablation**: Train baseline model on trajectory-only task (no CoT), then evaluate with vs. without images. Expect <0.05 L2 difference if shortcut learning is present. (Replicates Table 1 finding)
  2. **CoT length sensitivity**: Train three variants—short CoT only, long CoT only, adaptive mix—on held-out validation split. Measure both L2 error and collision rate to confirm adaptive strategy advantage. (Validates Page 6 claim)
  3. **Reward component contribution**: Train with trajectory+format rewards only, then incrementally add meta-action and repetition penalties. Monitor collision rate reduction specifically. (Replicates Table 3 progression from 0.18 → 0.14 → 0.10 collision)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stability of the reinforcement learning phase be ensured when scaling the number of rollouts beyond the observed limits?
- Basis in paper: [explicit] Page 9 notes that training "becomes unstable beyond a certain number of steps under the 24-rollout setting," limiting the use of higher sample diversity.
- Why unresolved: The paper does not identify the specific mechanism causing the policy collapse during Group Relative Policy Optimization (GRPO) at higher rollout counts.
- What evidence would resolve it: Ablation studies analyzing gradient norms or KL divergence stability across different rollout configurations, or the introduction of stabilization techniques that allow successful training with >12 rollouts.

### Open Question 2
- Question: Does the proposed reasoning-planning alignment transfer to closed-loop driving simulations, or is the performance gain specific to open-loop metrics?
- Basis in paper: [inferred] The paper evaluates performance using L2 distance and collision rates on the nuScenes validation set, which are open-loop metrics that do not account for compounding errors in dynamic control.
- Why unresolved: The model is trained and tested on ground-truth history; it is unclear if the visual grounding and reasoning remain robust when the ego-vehicle must execute the predicted trajectory in a feedback loop.
- What evidence would resolve it: Evaluation results in a closed-loop simulator (e.g., CARLA) measuring success rates and ride comfort over extended routes.

### Open Question 3
- Question: Can the "fast-and-slow" thinking strategy be automated to allow the model to self-determine reasoning complexity without pre-categorization?
- Basis in paper: [explicit] Page 6 describes using a separate proxy model to assign scenes as "short" or "long" CoT based on planning metrics, implying a reliance on external annotation.
- Why unresolved: Relying on a proxy model for data curation limits scalability and may not capture nuances of novel, edge-case scenarios not present in the training distribution.
- What evidence would resolve it: A unified model architecture that dynamically adjusts reasoning depth (CoT length) based on internal uncertainty estimates without external supervision.

## Limitations
- The adaptive CoT strategy relies on a proxy model for difficulty classification that is not independently validated
- The GRPO implementation details and stability thresholds are opaque, limiting reproducibility across hardware configurations
- The 4B parameter model shows capacity constraints with complex CoT supervision, suggesting scalability issues for larger reasoning tasks

## Confidence
- **High confidence**: The two-stage SFT methodology effectively addresses visual shortcut learning, as evidenced by the ablation showing degraded performance without images and the improvement from adaptive CoT length
- **Medium confidence**: The GRPO with composite rewards provides stable policy updates and improves safety metrics (collision rate), though the exact mechanism for why 6 rollouts are stable while 24 cause collapse requires more investigation
- **Low confidence**: The difficulty proxy model for CoT length assignment is validated only implicitly through downstream performance, without independent accuracy assessment of the proxy predictions

## Next Checks
1. **Visual grounding ablation study**: Systematically remove image inputs at test time to verify that the model genuinely relies on visual observations rather than textual history patterns
2. **Proxy model validation**: Evaluate the difficulty classification accuracy of the proxy model used for CoT length assignment, ensuring it correctly identifies complex scenarios requiring long CoT
3. **Rollout sensitivity analysis**: Test GRPO stability across a wider range of rollout counts (N=3, 8, 12, 18) to establish the precise stability threshold and determine if the 6-rollout configuration is optimal or merely sufficient