---
ver: rpa2
title: Perceptrons and localization of attention's mean-field landscape
arxiv_id: '2601.21366'
source_url: https://arxiv.org/abs/2601.21366
tags:
- theorem
- perceptron
- gradient
- page
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the mean-field limit of transformer self-attention
  as a Wasserstein gradient flow on the unit sphere, where time corresponds to layers,
  particles to token embeddings, and layer normalization is idealized. The authors
  show that the perceptron block fundamentally reshapes the landscape: for ReLU perceptrons
  in dimension two, all stationary measures are atomic with finite support, and for
  higher dimensions this discreteness holds generically.'
---

# Perceptrons and localization of attention's mean-field landscape

## Quick Facts
- arXiv ID: 2601.21366
- Source URL: https://arxiv.org/abs/2601.21366
- Reference count: 14
- Primary result: Mean-field limit of transformer self-attention with perceptron block forms Wasserstein gradient flow on sphere; stationary measures are atomic with finite support, cluster count scales as √β.

## Executive Summary
This paper analyzes the mean-field limit of transformer self-attention coupled with a perceptron (MLP) block as a Wasserstein gradient flow on the unit sphere. The authors prove that the perceptron block fundamentally reshapes the landscape, causing stationary measures to be atomic (finite support) rather than continuous. For ReLU perceptrons in dimension two, all stationary measures are atomic, and this discreteness holds generically in higher dimensions. The dynamics lead to cluster formation where masses scale with inverse temperature β while cluster counts scale as √β.

## Method Summary
The authors study the mean-field limit of self-attention coupled with a perceptron block as a Wasserstein gradient flow on S^(d-1), where time corresponds to layers and particles to token embeddings. They discretize the dynamics using Explicit Euler with step size Δt=0.1, initializing 1000 particles uniformly on the sphere with inverse temperature β∈{0.01,...,50} and perceptron weights sampled from standard normal distribution. Convergence is defined by cluster count stability over 5 snapshots and max velocity below 10^(-4). Cluster analysis uses distance thresholds based on β.

## Key Results
- ReLU perceptrons in dimension two produce stationary measures that are atomic with finite support
- For higher dimensions, atomicity holds generically for typical weight configurations
- In descent regime, cluster masses are bounded away from collapse, scaling as 1/β
- Number of heavy atoms scales as √β
- Simulations visualize emergence of singular stationary measures and confirm scaling laws

## Why This Works (Mechanism)
The perceptron block fundamentally alters the landscape from continuous to atomic stationary measures through its interaction with the self-attention mechanism. The gradient flow structure on the Wasserstein space, combined with the perceptron's nonlinearity (ReLU/GeLU), creates energy wells that trap particles into discrete clusters. The competition between repulsive self-attention and attractive perceptron potential leads to singular measures concentrated on finite supports.

## Foundational Learning
- **Wasserstein gradient flows**: Gradient descent in probability measure space; needed to analyze continuous-time limit of layer updates.
  - Quick check: Verify that velocity field equals negative Wasserstein gradient of energy functional.
- **Mean-field limit**: N→infinity limit where particle interactions become continuous density; needed to transition from discrete tokens to measure-valued dynamics.
  - Quick check: Confirm that empirical measure converges to solution of continuity equation.
- **Singular measures**: Measures concentrated on sets of measure zero (atoms); needed to characterize final clustered states.
  - Quick check: Test whether empirical distribution shows Dirac deltas at convergence.
- **Projection operators**: Mathematical tool to restrict dynamics to manifold (sphere); needed to maintain normalization constraint.
  - Quick check: Verify particles remain on unit sphere throughout simulation.
- **Stationary measures**: Probability measures invariant under the dynamics; needed to characterize final states.
  - Quick check: Confirm velocity field vanishes at convergence.
- **Energy landscape analysis**: Study of potential functions governing dynamics; needed to prove atomicity results.
  - Quick check: Plot energy decay during simulation to verify gradient flow behavior.

## Architecture Onboarding

### Component Map
Particles → Attention Kernel → Perceptron Drift → Projected Velocity → Particle Update → Stationary Measure

### Critical Path
Initialization → Gradient Flow Evolution → Cluster Formation → Convergence → Atomic Stationary Measure

### Design Tradeoffs
- **Continuous vs discrete dynamics**: Mean-field approximation trades exact token tracking for analytical tractability
- **Gradient vs general drift**: Assuming gradient field enables energy landscape analysis but excludes general perceptron weights
- **Explicit vs implicit integration**: Explicit Euler enables direct implementation but may require small step sizes for stability

### Failure Signatures
- **Metastability**: System gets stuck in local basin rather than reaching global equilibrium
- **Numerical instability**: Exponential terms cause exploding gradients at high β
- **Constraint violation**: Particles drift off unit sphere due to projection errors

### First Experiments
1. Run simulation for d=2 with ReLU perceptron and β=10, verify final distribution shows discrete clusters
2. Plot cluster count vs √β across range β∈{0.1,1,10,50} to confirm scaling law
3. Test convergence criterion by running simulation to t=200 and checking if cluster count changes

## Open Questions the Paper Calls Out

### Open Question 1
Do the atomicity and localization results persist when the perceptron drift does not derive from a scalar potential (i.e., for general weights outside the symmetry assumed in Eq. 2.6)?
- Basis in paper: Section 2.4 states the drift is a gradient field only under specific weight symmetries, noting that more general weights could be interpreted as preconditioners.
- Why unresolved: The theoretical framework relies on the Wasserstein gradient flow structure of an explicit energy functional E_β,ϑ, which exists only if the drift is a gradient.
- What evidence would resolve it: An extension of Theorems 3.1 and 3.3 to the general drift dynamics (2.5) without assuming the existence of the potential v_ϑ.

### Open Question 2
Can the unique global minimizer μ* of the energy E_β,ϑ be explicitly characterized in terms of its support and mass distribution?
- Basis in paper: Theorem 3.7(i) provides explicit characterizations for global maximizers, while part (ii) only guarantees the existence and uniqueness of the minimizer without describing its structure.
- Why unresolved: The minimizer results from a competition between the repulsive self-attention energy and the perceptron potential, likely leading to a more complex configuration than the single-point maximizers.
- What evidence would resolve it: Derivation of explicit conditions for the support of μ* in specific cases (e.g., diagonal or collinear weights) or a proof that μ* is purely atomic with computable masses.

### Open Question 3
How does the breaking of symmetry in the attention kernel (e.g., using non-symmetric Q^T K) affect the singularity and atomicity of the stationary measures?
- Basis in paper: The analysis in Section 2.4 and Remark 3.4 focuses on isotropic or symmetric invertible B, explicitly setting aside non-symmetric cases.
- Why unresolved: Non-symmetric kernels generally induce dynamics that are not gradient flows, making the standard variational tools used to prove singularity inapplicable.
- What evidence would resolve it: A stability analysis showing that atomicity holds for perturbations of symmetric kernels, or a counterexample showing non-atomic stationary states for non-symmetric weights.

## Limitations
- Perceptron weight initialization lacks specific random seed, preventing exact replication of cluster counts
- Projection operator numerical stability details missing, potentially causing constraint violations
- Convergence criterion is heuristic and may identify metastable states rather than true equilibrium
- Analysis assumes gradient drift structure, excluding general perceptron weight configurations
- High-dimensional atomicity claims rely on generic weight assumptions not fully validated

## Confidence
- **High confidence**: The proof that ReLU perceptrons induce atomic stationary measures for d=2 and generically for d≥2 (Theorem 3.3)
- **Medium confidence**: The scaling law for cluster counts (√β) and the bound on cluster masses (1/β) are derived analytically but rely on generic weight assumptions not fully validated numerically
- **Low confidence**: The exact cluster counts in simulations (e.g., Figure 9) are not reproducible without the specific weight initialization

## Next Checks
1. Replicate the scaling law numerically: Run the simulation with multiple random weight initializations (seeds 1-10) for β∈{0.1,1,10,50} and plot cluster counts vs √β. Check if the slope is consistent across seeds.
2. Test the convergence criterion: Run simulations for d=2 with ReLU perceptrons until t=200 (instead of stopping at the velocity threshold). Verify whether the cluster count remains stable or if additional merging occurs.
3. Validate the metastable state hypothesis: For high β (e.g., 50), run the simulation from two different initializations (uniform vs. pre-clustered). Compare final cluster counts and masses to test if the system consistently reaches the same attractor.