---
ver: rpa2
title: Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks
  Through Entity Anonymization
arxiv_id: '2511.11946'
source_url: https://arxiv.org/abs/2511.11946
tags:
- knowledge
- anonymized
- dialogue
- llms
- anonymization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of large language models (LLMs)
  relying on their internal knowledge rather than the provided knowledge graphs during
  dialogue generation, which can lead to inappropriate responses. The authors introduce
  LLM-KAT, an evaluation procedure to measure knowledge attachment, and propose entity
  anonymization to encourage LLMs to better leverage external knowledge.
---

# Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization

## Quick Facts
- arXiv ID: 2511.11946
- Source URL: https://arxiv.org/abs/2511.11946
- Reference count: 32
- Key outcome: Entity anonymization improves LLMs' attachment to external knowledge in dialogue generation without significantly affecting response quality

## Executive Summary
This paper addresses the challenge of large language models (LLMs) relying on their internal knowledge rather than provided knowledge graphs during dialogue generation, which can lead to inappropriate responses. The authors introduce LLM-KAT, an evaluation procedure to measure knowledge attachment, and propose entity anonymization to encourage LLMs to better leverage external knowledge. By replacing named entities with typed identifiers (e.g., "Person1", "Film2"), the approach forces LLMs to rely on the given context rather than their internal knowledge. Experiments on the OpenDialKG dataset demonstrate that anonymization improves LLMs' attachment to external knowledge, with models showing increased F1 scores in contextual alignment without significantly affecting response quality.

## Method Summary
The method involves zero-shot inference using entity anonymization and LLM-KAT evaluation. Entity anonymization replaces named entities in dialogue history and knowledge graph triplets with typed identifiers (e.g., "Person1") using an LLM (QwQ-32B). This forces LLMs to rely on provided context rather than internal knowledge. LLM-KAT transforms the evaluation into a QA-style span extraction task, prompting an LLM to extract entities that complete triplets from generated responses. The approach uses OpenDialKG dataset with Normal and Anonymized variants, measuring attachment via F1 scores and response quality via UniEval metrics.

## Key Results
- Anonymization improves LLMs' attachment to external knowledge, with increased F1 scores in contextual alignment
- Larger models (32B) maintain higher response quality under anonymization compared to smaller models (7B)
- LLM-KAT provides more reliable measurement of knowledge attachment than prior BERT-based methods, with 79% correct identification of synthetic unanswerable contexts

## Why This Works (Mechanism)

### Mechanism 1: Entity Anonymization Disables Parametric Shortcuts
- Claim: Replacing named entities with typed identifiers prevents LLMs from retrieving associated parametric knowledge, forcing reliance on provided context.
- Mechanism: When "Robert Downey Jr." becomes "[P1]", the model cannot activate pre-trained associations and must derive entity attributes solely from the anonymized knowledge triplets provided in-context.
- Core assumption: LLMs primarily retrieve entity-specific knowledge through pattern matching on entity names rather than structural reasoning.
- Evidence anchors:
  - [abstract]: "By anonymizing entities in the dialogue history and knowledge graph, the approach forces LLMs to rely on the given context rather than their internal knowledge."
  - [Page 2]: "In this way, the entities in conversation would not match the internal knowledge of LLM, forcing the reasoning of response generation to be attached to the given knowledge."

### Mechanism 2: LLM-KAT Quantifies Attachment via Structured Extraction
- Claim: Converting attachment evaluation into a QA-style span extraction task provides more reliable measurement than prior BERT-based methods.
- Mechanism: Given triplet (e1, r, X), the evaluator prompts an LLM to extract X from generated responses. SQuAD-style F1 scoring captures both precision and recall of knowledge utilization.
- Core assumption: The extracting LLM can reliably identify entity spans and correctly judge when context is insufficient.
- Evidence anchors:
  - [Page 2]: "We transform the evaluation into Question Answering (QA) format and prompt a strong LLM to extract the answers from the generated response."
  - [Page 4, Table 3]: LLM-KAT flags 79% of synthetic unanswerable contexts correctly vs. 100% false positive rate for KQA.

### Mechanism 3: Model Scale Moderates Anonymization-Quality Tradeoff
- Claim: Larger models maintain response naturalness and coherence better under anonymization than smaller models.
- Mechanism: Larger models (32B) show smaller quality drops (<2% naturalness decrease) while gaining similar attachment improvements as 7B models, which suffer larger quality degradation (>3%).
- Core assumption: Larger models develop more robust reasoning circuits that can operate on abstracted representations without fluent entity names.
- Evidence anchors:
  - [Page 5, Table 5]: Quality gap narrows with scale: DeepSeek-7B shows 3.47% naturalness drop vs. 1.93% for 32B.
  - [Page 5]: "Larger LLMs maintain high response quality regardless of anonymization."

## Foundational Learning

- Concept: **Knowledge Graph Triplets (e1, r, e2)**
  - Why needed here: External knowledge is formatted as subject-predicate-object triplets. Understanding this canonical form is essential for parsing the knowledge input and the evaluation extraction task.
  - Quick check question: Given triplet ([Film1], directed_by, [Person2]), what entity type should fill the missing object?

- Concept: **Retrieval-Augmented Generation (RAG) with Graphs**
  - Why needed here: This work addresses a core RAG failure mode—models ignoring retrieved context. Understanding baseline RAG helps contextualize why "detachment" is problematic.
  - Quick check question: In standard RAG, what are two failure modes when an LLM receives retrieved documents?

- Concept: **F1 Score for Span Extraction**
  - Why needed here: LLM-KAT uses SQuAD-style F1 to measure how well generated responses contain the correct entities. Understanding token-level F1 (harmonic mean of precision/recall) is needed to interpret Table 1 results.
  - Quick check question: If a response contains "Tom Hanks" but the ground truth is "Thomas Hanks," how would F1 penalize this vs. exact match?

## Architecture Onboarding

- Component map:
  Input Data → [Anonymizer LLM] → Anonymized {dialogue + triplets}
                                      ↓
                              [Generator LLM] → Generated Response
                                      ↓
                              [Evaluator LLM] → F1 Attachment Score

- Critical path: The anonymization quality directly determines the intervention's success. Poor anonymization (missed aliases like "RDJ" for "Robert Downey Jr.") allows parametric leakage.

- Design tradeoffs:
  - **Anonymizer strength vs. cost**: QwQ-32B provides "perfect" anonymization but is computationally expensive. Weaker anonymizers may require partial anonymization strategies.
  - **Attachment vs. fluency**: Anonymization improves F1 scores but may reduce naturalness by 2-3% for smaller models.
  - **Evaluation robustness vs. complexity**: LLM-KAT is more reliable than KQA but requires a strong LLM evaluator, adding inference cost.

- Failure signatures:
  - **False attachment improvement**: If anonymization accidentally preserves entity clues (e.g., "Person_Tom_1"), gains may be artifactual.
  - **Quality collapse**: If anonymization is over-aggressive (anonymizing common nouns), responses may become incoherent.
  - **Evaluator hallucination**: LLM-KAT may extract entities not actually present if the evaluator LLM is not carefully prompted.

- First 3 experiments:
  1. **Baseline reproduction**: Run generator LLM on Normal OpenDialKG with default prompt. Compute LLM-KAT F1 and UniEval coherence to establish baseline.
  2. **Anonymization ablation**: Compare full anonymization vs. partial (50%) vs. none on a 1K-turn subset. Plot attachment F1 vs. naturalness drop to find operating point.
  3. **Scale sensitivity test**: Run the same anonymized input through 7B, 14B, and 32B variants of the same model family (e.g., DeepSeek-R1). Verify that quality gap narrows with scale as claimed in Table 5.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach's effectiveness depends on the anonymizer reliably detecting and replacing all entity aliases
- Real-world error rates in anonymization and evaluation remain unquantified
- The approach may not transfer to domains requiring different entity types or reasoning patterns beyond the movie/TV series dataset

## Confidence

**High confidence**: Observed F1 improvements under anonymization and basic efficacy of LLM-KAT over KQA for contextual alignment measurement. The quantitative results are reproducible and internally consistent.

**Medium confidence**: Causal mechanism (entity anonymization disabling parametric shortcuts) and generalizability of scale-based quality tradeoffs. While evidence is strong from OpenDialKG, the mechanistic explanation assumes LLMs primarily rely on surface entity matching.

**Low confidence**: Long-term robustness of the approach without further validation on diverse datasets and knowledge domains. The paper focuses on a single movie/TV series dataset.

## Next Checks

1. **Robustness to entity leakage**: Systematically test anonymization on entity aliases and synonyms (e.g., "Iron Man" vs. "Tony Stark") to quantify false attachment improvements from missed replacements.

2. **Evaluator bias characterization**: Run LLM-KAT on a held-out validation set with known ground truth entities to measure precision/recall of the evaluator itself, and test whether prompt variations affect extracted spans.

3. **Cross-domain generalizability**: Apply the full pipeline (anonymization + LLM-KAT evaluation) to a non-movie knowledge graph (e.g., scientific literature or business domains) to verify the mechanism and tradeoffs hold beyond the original dataset.