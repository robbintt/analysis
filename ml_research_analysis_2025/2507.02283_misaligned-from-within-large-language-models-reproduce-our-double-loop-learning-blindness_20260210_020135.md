---
ver: rpa2
title: 'Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning
  Blindness'
arxiv_id: '2507.02283'
source_url: https://arxiv.org/abs/2507.02283
tags:
- action
- learning
- theory
- chatgpt
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical gap in AI alignment research by
  showing how Large Language Models inherit and amplify human cognitive blind spots
  related to organizational learning. Using action science theory, the authors demonstrate
  through a case study that ChatGPT's advice, while professionally sound on the surface,
  systematically reinforces Model 1 defensive reasoning patterns that inhibit double-loop
  learning.
---

# Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness

## Quick Facts
- arXiv ID: 2507.02283
- Source URL: https://arxiv.org/abs/2507.02283
- Authors: Tim Rogers; Ben Teehankee
- Reference count: 0
- Primary result: LLMs inherit and amplify human cognitive blind spots related to organizational learning, reproducing Model 1 defensive reasoning that inhibits double-loop learning.

## Executive Summary
This paper identifies a critical gap in AI alignment research by showing how Large Language Models inherit and amplify human cognitive blind spots related to organizational learning. Using action science theory, the authors demonstrate through a case study that ChatGPT's advice, while professionally sound on the surface, systematically reinforces Model 1 defensive reasoning patterns that inhibit double-loop learning. The AI's recommendations consistently bypassed testing underlying assumptions about workplace dynamics, potentially entrenching anti-learning practices while appearing authoritative. The research suggests that LLMs trained on human-generated text absorb these counterproductive reasoning patterns, creating a specific alignment problem where systems mirror human behavior but also replicate our cognitive limitations.

## Method Summary
The study used a qualitative case study approach with a single organizational scenario involving a manager (Alison) seeking advice about a subordinate (Ms X) and role boundary issues. The authors presented this case to ChatGPT 4o, requesting interpretation and action design, then followed up with a transcript of failed implementation. The same test was replicated with Deepseek V3, Grok 3, and Claude Opus 4. Analysis focused on whether LLM advice exhibited Model 1 patterns (treating assumptions as facts, advocating without inquiry, bypassing underlying issues) versus Model 2 patterns (testing assumptions, inviting genuine inquiry, exploring deeper causal dynamics).

## Key Results
- ChatGPT's advice reproduced Model 1 defensive reasoning patterns despite espousing Model 2 values
- LLM recommendations consistently treated Alison's assumptions as facts rather than hypotheses to test
- The AI's responses bypassed opportunities for double-loop learning by focusing on single-loop adjustments
- Model 1 patterns appear to permeate LLM development stages including training data and alignment processes

## Why This Works (Mechanism)

### Mechanism 1
LLMs absorb Model 1 defensive reasoning patterns from human training data because these patterns are ubiquitous in human discourse. Training corpora contain predominantly Model 1 interpersonal reasoning (unilateral control, advocacy without inquiry, avoidance of testing assumptions). With little to no exposure to Model 2 dialogue patterns, LLMs learn to generate plausible professional advice that reproduces these defensive routines. Core assumption: Model 1 is near-universal in human organizational interactions.

### Mechanism 2
Alignment processes (RLHF, Constitutional AI) fail to correct Model 1 absorption because human raters and principle-authors themselves operate from Model 1 theories-in-use. Human evaluators reward responses that appear professional and diplomatic—qualities that Model 1 advice exhibits. Since raters are blind to their own Model 1 patterns, they cannot identify or penalize Model 1 reasoning in LLM outputs. Core assumption: Human annotators involved in alignment share the general population's Model 1 theories-in-use.

### Mechanism 3
LLMs' optimization for plausibility and social appropriateness reinforces Model 1 responses because Model 2 inquiry can appear confrontational or norm-violating from a Model 1 perspective. Model 2 strategies (surfacing contradictions, testing assumptions directly) often feel uncomfortable or "argumentative" to Model 1 perceivers. LLMs rewarded for helpful, diplomatic outputs avoid such moves, defaulting to Model 1-congruent suggestions. Core assumption: Model 2 responses are perceived as less socially desirable by typical users/evaluators operating from Model 1.

## Foundational Learning

- **Model 1 vs. Model 2 theories-in-use**: Why needed: The entire diagnosis depends on distinguishing defensive reasoning (Model 1: unilateral control, advocacy without inquiry, minimizing negative feelings) from learning-oriented reasoning (Model 2: valid information, free choice, internal commitment, joint inquiry). Quick check: Can you identify whether a given piece of advice advocates a position without inviting genuine inquiry into it?

- **Espoused theory vs. theory-in-use**: Why needed: LLMs may espouse Model 2 values ("involve others in decisions") while their actual advice enacts Model 1 patterns (involving others only in implementation, not in determining whether the decision itself is correct). Quick check: When an LLM recommends "collaboration," does it mean joint inquiry into assumptions or buy-in for a pre-determined outcome?

- **Double-loop learning**: Why needed: The paper's core claim is that LLMs block double-loop learning (questioning underlying assumptions, norms, and objectives) while facilitating single-loop learning (adjusting actions within fixed assumptions). Quick check: Does a given intervention address only "how do I achieve my goal?" or also "why is this my goal, and what assumptions underlie it?"

## Architecture Onboarding

- **Component map**: Training corpus → Model 1 pattern absorption → Alignment layer (RLHF/Constitutional AI) → Inference behavior → User interaction

- **Critical path**: Training data → Pre-training → Alignment (RLHF/Constitutional AI) → Deployment. Each stage contains Model 1 patterns; intervention must target multiple stages.

- **Design tradeoffs**: Plausibility vs. depth (optimizing for helpful, diplomatic responses may systematically exclude Model 2 inquiry patterns that feel uncomfortable); Automation vs. augmentation (full Model 2 automation may require persistent state and strategic reasoning capabilities LLMs currently lack); Detection vs. generation (detecting Model 1 patterns in outputs may be easier than generating Model 2 responses reliably).

- **Failure signatures**: Advice that treats manager's framing as fact without testing; recommendations to "involve" subordinates only in implementation, not in problem definition; bypassing expressed concerns about autonomy/professional judgment rather than exploring them; surface-level "collaborative" language that maintains unilateral control.

- **First 3 experiments**:
  1. **Diagnostic probe**: Present organizational scenarios to multiple LLMs; code responses for Model 1 vs. Model 2 strategies using established theory-of-action coding schemes.
  2. **Alignment intervention**: Fine-tune or prompt-engineer with explicit Model 2 principles; test whether outputs shift toward genuine inquiry or merely espouse Model 2 values while enacting Model 1.
  3. **Evaluator blindness test**: Present Model 1 and Model 2 advice variants to human raters; test whether raters prefer Model 1 advice and whether this preference correlates with raters' own Model 1 theories-in-use.

## Open Questions the Paper Calls Out

- **Can LLMs be trained to incorporate Model 2 reasoning capabilities through fine-tuning, constitutional AI, or other alignment methods?** The paper presents competing arguments—optimistic factors (no emotional defensiveness, existing espoused Model 2 values) versus pessimistic barriers (Model 1 permeates training data, RLHF processes, and strategic reasoning limitations)—without empirical resolution.

- **Can Constitutional AI approaches successfully encode Model 2 values into LLM behavior?** Only speculative parallels are drawn between Constitutional AI and theory of action learning; no empirical testing has been conducted.

- **Does the reproduction of Model 1 reasoning generalize across diverse organizational contexts and problem types?** The study examined a single HR case study with one manager-subordinate conflict scenario; unknown whether findings apply to strategic decisions, cross-cultural teams, or routine operations.

- **Can AI systems effectively teach Model 2 capabilities to human practitioners at scale?** The paper demonstrates AI reproducing Model 1 but provides no evidence for AI-facilitated Model 2 learning in humans.

## Limitations

- The core claim rests on a single qualitative case study with one LLM, though replicated with other models, lacking systematic quantitative validation across diverse organizational scenarios.
- The Model 1 vs. Model 2 framework represents a specific epistemological lens that may not capture all dimensions of organizational learning dynamics.
- Claims about alignment process blindness remain speculative without direct evidence about human evaluator reasoning patterns during RLHF or constitutional AI development.

## Confidence

- **High confidence**: LLMs can generate plausible professional advice that bypasses double-loop learning opportunities when given organizational scenarios.
- **Medium confidence**: The proposed mechanism (Model 1 pattern absorption from training data + alignment process blindness) explains observed behavior, but requires empirical validation beyond the single case study.
- **Low confidence**: Specific claims about the prevalence of Model 1 patterns in training corpora or alignment datasets, as these remain untested assumptions.

## Next Checks

1. **Systematic diagnostic study**: Code responses from multiple LLMs across 50+ organizational scenarios using established theory-of-action coding schemes to measure Model 1 vs. Model 2 pattern prevalence.

2. **Alignment evaluator study**: Survey or experimentally test human evaluators (RLHF raters, constitutional AI principle authors) for their own Model 1 theories-in-use using validated instruments, then test whether these correlate with their alignment preferences.

3. **Intervention effectiveness trial**: Test whether fine-tuning or prompt engineering with explicit Model 2 principles produces genuine double-loop learning behavior versus merely espousing Model 2 values while enacting Model 1 patterns.