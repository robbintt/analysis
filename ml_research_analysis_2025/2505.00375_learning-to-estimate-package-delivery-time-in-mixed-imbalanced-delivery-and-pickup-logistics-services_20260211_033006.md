---
ver: rpa2
title: Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and
  Pickup Logistics Services
arxiv_id: '2505.00375'
source_url: https://arxiv.org/abs/2505.00375
tags:
- delivery
- time
- package
- pickup
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of estimating package delivery
  time in mixed logistics scenarios where couriers handle both deliveries and pickups,
  with deliveries far outnumbering pickups. The proposed TransPDT model uses a Transformer-based
  multi-task learning approach, incorporating route prediction as an auxiliary task
  to enhance time prediction.
---

# Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services

## Quick Facts
- **arXiv ID**: 2505.00375
- **Source URL**: https://arxiv.org/abs/2505.00375
- **Reference count**: 40
- **One-line primary result**: Proposes TransPDT, a Transformer-based multi-task model that outperforms baselines by 3.54% in RMSE and 18.04% in MAPE for delivery time prediction.

## Executive Summary
This paper addresses the challenge of estimating package delivery time in mixed logistics scenarios where couriers handle both deliveries and pickups, with deliveries far outnumbering pickups. The proposed TransPDT model uses a Transformer-based multi-task learning approach, incorporating route prediction as an auxiliary task to enhance time prediction. It captures spatio-temporal dependencies of historical and pending packages, learns pickup patterns from imbalanced data using pattern memory, and integrates courier spatial mobility preferences. Experiments on real-world datasets from JD Logistics show TransPDT outperforms baselines by 3.54% in RMSE and 18.04% in MAPE for delivery time prediction. The system is deployed internally to track over 2000 couriers, achieving a 0.68% higher daily delivery timely rate compared to non-deployed stations.

## Method Summary
TransPDT uses a dual-branch Transformer encoder to process historical and pending package embeddings, with a pattern memory module for learning pickup patterns via attention. An LSTM-Attention decoder generates route probabilities, refined by prior spatial mobility matrices. The model jointly optimizes delivery time prediction (main task) and route prediction (auxiliary task) using a learnable parameter to balance losses. It handles data imbalance through pattern memory and integrates courier mobility and spatial distance priors to improve route plausibility.

## Key Results
- TransPDT outperforms baselines by 3.54% in RMSE and 18.04% in MAPE for delivery time prediction.
- Achieves 6.25% and 3.55% improvement in LMD and Hit-Rate@3 for route prediction.
- Deployed internally at JD Logistics, tracking over 2000 couriers and achieving a 0.68% higher daily delivery timely rate.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating route prediction as an auxiliary task improves delivery time estimation accuracy in mixed logistics.
- **Mechanism:** A shared LSTM-Attention model predicts the probability of visiting each pending package at each step. The learned route probabilities are then refined using prior spatial mobility matrices (courier transitions and spatial distances) before the final time prediction is made. The main time prediction loss and the auxiliary route classification loss are optimized jointly, forcing the model to learn representations useful for both tasks.
- **Core assumption:** The sequence in which a courier visits packages is highly predictive of the total time, and learning to predict this sequence forces the model to internalize the constraints and preferences that govern courier decision-making.
- **Evidence anchors:**
  - [abstract] "...set the route prediction as an auxiliary task of delivery time prediction, and incorporate the prior courier spatial movement regularities in prediction."
  - [section 4.1] "It first utilizes the attention-based LSTM to predict the intermediate probability of each package... It then integrates the prior knowledge of spatial relation and movement regularity... to subsequently generate the courier’s future package route as the auxiliary task and corresponding delivery time as the main task."
- **Break condition:** If route and time are truly independent in a specific new domain, the shared representations could introduce harmful noise, degrading performance.

### Mechanism 2
- **Claim:** A "pattern memory" with attention allows the model to learn the influence of rare pickup tasks on delivery routes.
- **Mechanism:** To handle the class imbalance (few pickups vs. many deliveries), a learnable memory matrix M stores embeddings of pickup patterns. The model computes a similarity score between the current package context and these memory patterns via attention, retrieves a weighted context vector from memory, and concatenates it to the package representation. This allows the model to generalize pickup influence from limited examples.
- **Core assumption:** The influence of pickups on delivery behavior follows a finite set of recurring patterns that can be captured as vector representations and shared across different contexts via attention.
- **Evidence anchors:**
  - [abstract] "...design the pattern memory to learn the patterns of pickup in the imbalanced dataset via attention mechanism."
  - [section 4.3] "The network adopts the pattern memory which serves as learnable key-value pairs, to learn and store the patterns of limited pickup packages... We use the output of the last module as the query... to calculate the similarity score."
- **Break condition:** This mechanism assumes patterns are transferable. If pickup influence is highly idiosyncratic to individual couriers or locations without commonalities, the shared memory will fail to retrieve relevant information.

### Mechanism 3
- **Claim:** Integrating prior spatial mobility knowledge and distance matrices refines the probabilistic route prediction.
- **Mechanism:** The model pre-calculates a Courier Mobility Matrix (transition counts) and a Spatial Distance Matrix. After the LSTM-Attention component generates an initial probability for the next package, these matrices are used to re-weight the probabilities. This step injects hard constraints (e.g., avoid long detours) and learned preferences (e.g., historical flow) into the route prediction before it is finalized.
- **Core assumption:** Historical movement frequencies and spatial distances are strong proxies for a courier's future routing preferences and decisions.
- **Evidence anchors:**
  - [abstract] "...incorporate the prior courier spatial movement regularities in prediction."
  - [section 4.4.2] "...we first divide a day into 12 time slots... Then we construct two original location-pair matrices... M'C (Courier Mobility Matrix) and M'D (Spatial Distance Matrix)... used to weight the previous intermediate probability."
- **Break condition:** The mechanism relies on historical regularity. It will break in environments with significant disruption (e.g., new road closures, traffic incidents, major route changes) where historical patterns are no longer predictive.

## Foundational Learning

### Concept: Multi-Task Learning with Auxiliary Loss
- **Why needed here:** The primary task is time prediction, but the model must also understand route sequence. Jointly optimizing for both via a combined loss forces the model to learn shared representations that capture the underlying structure of the courier's problem.
- **Quick check question:** Can you explain how the gradients from the route prediction loss would influence the weights in the shared LSTM layer during backpropagation?

### Concept: Class Imbalance & Attention-Based Retrieval
- **Why needed here:** Standard models struggle to learn from minority classes (pickups). The pattern memory uses attention to selectively retrieve learned patterns, acting as an external knowledge base to amplify the signal from rare but important pickup events.
- **Quick check question:** How does the attention mechanism in the pickup module help the model focus on relevant historical pickup patterns for a new, rare request?

### Concept: Prior Knowledge Integration (Hybrid AI)
- **Why needed here:** Purely data-driven models might predict routes that violate spatial constraints or historical preferences. Integrating pre-computed matrices of distance and mobility acts as a constraint and a prior, guiding the model toward more plausible predictions.
- **Quick check question:** What are the trade-offs of using a fixed historical mobility matrix versus a dynamic, real-time one?

## Architecture Onboarding

### Component map:
Input Embedding -> Dual Transformer Encoders -> Pattern Memory + Attention -> LSTM-Attention Decoder -> Matrix Refinement -> Final Prediction

### Critical path:
Input Embedding -> Transformer Encoders -> Memory Augmented Context -> LSTM-Attention -> Matrix Refinement -> Final Prediction. The pickup memory and mobility matrices are side-inputs that augment the core sequential prediction.

### Design tradeoffs:
- **Fixed vs. Dynamic Priors:** The paper uses pre-computed matrices. A dynamic approach would be more adaptive but far more complex and computationally expensive.
- **Memory Size:** The number of patterns in memory is a hyperparameter. Too few misses nuance; too many risks noise and overfitting.
- **Auxiliary Loss Weighting:** The paper uses a learnable parameter (`α`). A fixed weight would be simpler to implement but might fail to balance the tasks optimally as training progresses.

### Failure signatures:
- **Overfitting to Regular Patterns:** If the mobility matrix is too dominant, the model will fail to predict route deviations caused by urgent or unusual pickups.
- **Memory Collapse:** If the attention mechanism collapses to a single pattern, the model loses its ability to handle diverse pickup scenarios.
- **Ignoring Rare Signals:** If the auxiliary loss weight is too low or the memory isn't trained well, the model might still effectively ignore the minority pickup patterns.

### First 3 experiments:
1. **Ablate the Pattern Memory:** Disable the pickup pattern memory module and retrain. This quantifies the performance gain specifically attributable to handling pickup imbalance.
2. **Tune Memory Size:** Run a sweep on the `L_m` parameter (e.g., 10, 20, 40) to find the optimal memory capacity for capturing pickup patterns without overfitting.
3. **Analyze Matrix Weighting Impact:** Visualize route predictions with and without the mobility/distance matrix refinement on a test set. Compare the "plausibility" of the generated routes against ground truth.

## Open Questions the Paper Calls Out
1. **Question:** How can specific neural modules be designed to explicitly process and weigh time constraints (e.g., remaining time) rather than treating them as simple concatenated features?
2. **Question:** What architectures can effectively model the complex interaction between couriers' historical behaviors and future package sets beyond simple vector concatenation?
3. **Question:** Is the fixed-size pattern memory mechanism robust to drastic shifts in the delivery-to-pickup ratio (e.g., during holiday return seasons)?

## Limitations
- The mechanism by which pattern memory learns and generalizes pickup influence is not empirically validated; the assumption of transferable pickup patterns is asserted but not tested.
- The exact integration of spatial/mobility matrices into route prediction is underspecified, particularly how matrices are dynamically indexed and applied.
- Claims about outperformance rely on relative metrics (3.54% RMSE, 18.04% MAPE) without absolute baselines or comparisons to simpler models.
- The real-world deployment claim (0.68% higher timely rate) lacks details on control conditions and measurement methodology.

## Confidence
- **High confidence**: Core architecture design (dual Transformer encoders, LSTM-Attention decoder) and joint loss formulation are well-specified and technically sound.
- **Medium confidence**: Pattern memory and mobility matrix integration mechanisms are described but lack empirical validation of their effectiveness.
- **Low confidence**: Claims about real-world deployment impact and generalizability to other logistics domains are insufficiently supported.

## Next Checks
1. **Ablate pattern memory**: Disable pickup pattern memory and retrain to quantify its specific contribution to handling class imbalance.
2. **Visualize route plausibility**: Compare route predictions with and without mobility matrix weighting against ground truth to assess if priors improve realism.
3. **Probe pickup influence**: Analyze model attention weights on pickup patterns during rare pickup events to verify the memory retrieves relevant, context-specific information.