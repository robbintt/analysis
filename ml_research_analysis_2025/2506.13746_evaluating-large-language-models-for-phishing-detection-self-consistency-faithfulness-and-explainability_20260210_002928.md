---
ver: rpa2
title: Evaluating Large Language Models for Phishing Detection, Self-Consistency,
  Faithfulness, and Explainability
arxiv_id: '2506.13746'
source_url: https://arxiv.org/abs/2506.13746
tags:
- phishing
- llama
- emails
- classification
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates fine-tuned LLMs for phishing detection using
  three approaches: binary classification, contrastive learning, and direct preference
  optimization. BERT achieved the highest accuracy (98.89%) and lowest loss with binary
  classification, while Llama 7B and Llama 8B also performed well.'
---

# Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability

## Quick Facts
- **arXiv ID:** 2506.13746
- **Source URL:** https://arxiv.org/abs/2506.13746
- **Authors:** Shova Kuikel; Aritran Piplai; Palvi Aggarwal
- **Reference count:** 29
- **Key outcome:** BERT achieved 98.89% accuracy with binary classification; Llama models showed high explanation consistency but low phishing accuracy, revealing a trade-off between prediction accuracy and explanation faithfulness.

## Executive Summary
This paper evaluates fine-tuned large language models for phishing detection using three approaches: binary classification, contrastive learning, and direct preference optimization. BERT achieved the highest accuracy (98.89%) and lowest loss with binary classification, while Llama 7B and Llama 8B also performed well. Wizard 7B underperformed across all approaches. For explainability, Llama models showed higher consistency (CC-SHAP scores > 0.95) but lower phishing detection accuracy (30–40%), while Wizard 7B had lower consistency but higher accuracy (80%). The findings reveal a trade-off between prediction accuracy and explanation consistency, highlighting the need for LLMs that balance reliability and nuanced reasoning in phishing detection.

## Method Summary
The study fine-tuned three LLM families (BERT, Llama, Wizard) using LoRA adapters on query and value attention matrices with a binary sequence classification approach. Models were trained on balanced Enron (legitimate) and Nazario (phishing) datasets using cross-entropy loss. Evaluation included classification accuracy and CC-SHAP, a metric measuring token-level contribution consistency between predictions and explanations through perturbation-based SHAP scoring and cosine similarity comparison.

## Key Results
- BERT achieved 98.89% validation accuracy with binary classification, outperforming other approaches and models.
- Llama models exhibited high CC-SHAP scores (>0.95) but low phishing detection accuracy (30–40%), indicating strong explanation consistency but poor decision-making.
- Wizard-7B demonstrated higher phishing accuracy (80%) but significantly lower CC-SHAP scores (~0.12), suggesting less faithful explanations relative to predictions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary sequence classification with LoRA fine-tuning produces contextualized embeddings that effectively distinguish phishing from legitimate emails.
- Mechanism: LoRA freezes original model weights while adding trainable sub-modules targeting query and value attention matrices. The model learns domain-specific patterns (sender, subject, body features) through cross-entropy minimization on labeled examples without full parameter updates.
- Core assumption: Pre-trained linguistic knowledge transfers to phishing-specific distinctions with minimal parameter updates.
- Evidence anchors:
  - [abstract] "Binary classification proved most effective overall, with BERT achieving 98.55% validation accuracy"
  - [section 3] "By keeping only query and value as target modules for LoRA, we fine-tuned our three different large language models"
  - [corpus] EXPLICATE paper confirms fine-tuned transformers for phishing detection but uses post-hoc LIME explanations rather than integrated alignment metrics
- Break condition: If domain shift between pre-training corpora and phishing emails is too large, LoRA adaptations may insufficiently capture deceptive linguistic patterns.

### Mechanism 2
- Claim: CC-SHAP quantifies prediction-explanation alignment by measuring token-level contribution consistency between classification decisions and generated explanations.
- Mechanism: Perturbation-based masking replaces tokens with padding, computing marginal probability shifts via Monte Carlo sampling. Normalized SHAP vectors for prediction and explanation are compared using cosine similarity—high scores indicate the explanation faithfully reflects tokens influencing the decision.
- Core assumption: Tokens contributing to classification probability should overlap with tokens cited in natural language explanations if reasoning is faithful.
- Evidence anchors:
  - [section 5] "CC-SHAP = 1 − cosine-dist(ϕ(p)norm, ϕ(e)norm)" — explicit formula for alignment measurement
  - [section 6.2] "Llama models exhibit stronger prediction-explanation token alignment with higher CC-SHAP scores despite lacking reliable decision-making accuracy"
  - [corpus] NeuroFaith paper evaluates LLM self-explanation faithfulness via internal representation alignment, addressing similar concerns about whether explanations reflect actual reasoning
- Break condition: If explanations are generated post-hoc through a separate process than classification, CC-SHAP may measure surface-level correlation rather than causal faithfulness.

### Mechanism 3
- Claim: Architecture-specific priors create a trade-off between classification accuracy and explanation-prediction consistency.
- Mechanism: Llama models prioritize semantic urgency cues ("bank", "card", "security") yielding consistent explanations but poor phishing discrimination. Wizard-7B, fine-tuned for instruction-following, prioritizes action verbs ("participate", "subscribe") achieving higher accuracy but lower token alignment—suggesting different internal decision processes.
- Core assumption: CC-SHAP scores reflect genuine reasoning alignment rather than linguistic pattern matching in explanations.
- Evidence anchors:
  - [section 6.2] "Llama 7B and Llama 8B model exhibited high CC-SHAP scores... 0.9659 ± 0.030 for phishing... However, their phishing accuracy remained low 40 and 30"
  - [section 6.2] "Wizard 7B demonstrated a lower CC-SHAP score for phishing 0.123 ± 0.0906... yet attained a higher phishing accuracy of 80"
  - [corpus] Weak direct corpus evidence for this specific accuracy-consistency trade-off; Trustworthiness Calibration Framework paper addresses calibration but not CC-SHAP alignment
- Break condition: If the 40-email evaluation set (20 phishing, 20 ham) is too small, observed trade-offs may not generalize.

## Foundational Learning

- **SHAP (SHapley Additive exPlanations)**:
  - Why needed here: Core to CC-SHAP metric; measures per-token contribution to model predictions through coalitional game theory
  - Quick check question: If masking token "urgent" shifts phishing probability from 0.7 to 0.4, what is its approximate SHAP contribution?

- **LoRA (Low-Rank Adaptation)**:
  - Why needed here: Enables fine-tuning 7B+ parameter models efficiently by updating only low-rank decomposition matrices rather than full weights
  - Quick check question: Why might freezing original weights while training only query/value matrices preserve pre-trained knowledge better than full fine-tuning?

- **Cosine Similarity for Vector Alignment**:
  - Why needed here: CC-SHAP uses cosine distance to compare normalized SHAP vectors between prediction and explanation
  - Quick check question: Two vectors with cosine similarity of 0.95—does this indicate they point in similar directions, have similar magnitudes, or both?

## Architecture Onboarding

- **Component map**: Enron (ham) + Nazario (phishing) → BeautifulSoup HTML cleaning → deduplication → balanced sampling (2500 each) → LoRA fine-tuning on query/value matrices → binary classification/DPO/contrastive learning → classification accuracy + CC-SHAP evaluation

- **Critical path**: 1. Binary sequence classification with LoRA (most effective per Table 2) 2. Target modules: query and value attention matrices only 3. Combine sender + subject + body as input sequence

- **Design tradeoffs**:
  - BERT: Highest accuracy (98.55%), no generative explanations—use for pure classification
  - Llama models: High explanation consistency (CC-SHAP >0.95) but poor phishing detection (30-40%)—use when explainability trumps accuracy
  - Wizard-7B: Best LLM accuracy (80%) but inconsistent explanations (CC-SHAP ~0.12)—use for detection with caution on trust

- **Failure signatures**:
  - High validation loss (>1.0) with DPO fine-tuning: preference optimization unsuitable for binary phishing tasks
  - CC-SHAP >0.95 with accuracy <50%: model generates plausible explanations unrelated to correct classification
  - 100% ham accuracy with <50% phishing accuracy: potential false-negative bias toward "legitimate" predictions

- **First 3 experiments**:
  1. Replicate binary classification with BERT on balanced Nazario/Enron subset—verify 98%+ validation accuracy
  2. Implement CC-SHAP pipeline: perturbation masking → Monte Carlo SHAP sampling → cosine similarity between prediction and explanation vectors
  3. Ablation: fine-tune Llama-2-7B with LoRA on all four attention matrices vs. query/value only—compare accuracy and CC-SHAP to isolate target module contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely small evaluation set (20 phishing, 20 ham) for CC-SHAP analysis raises concerns about statistical significance and generalizability of observed accuracy-explanation trade-off.
- Lack of ablation studies isolating impact of LoRA fine-tuning strategies versus base model differences on performance gaps.
- Absence of external validation on datasets beyond Enron and Nazario limits assessment of real-world robustness and domain shift issues.

## Confidence
- **Binary classification with LoRA achieving 98.55% accuracy:** High
- **CC-SHAP as a reliable measure of explanation faithfulness:** Medium
- **Architecture-specific trade-off between accuracy and explanation consistency:** Low

## Next Checks
1. **Statistical validation of CC-SHAP results:** Replicate the explanation-consistency analysis on a 10x larger balanced evaluation set (200+ phishing, 200+ ham) to establish confidence intervals and test whether the accuracy-explanation trade-off persists across sufficient samples.

2. **Cross-dataset generalization test:** Evaluate the three best-performing fine-tuned models (BERT, Llama-7B, Wizard-7B) on at least two additional phishing datasets (e.g., PhishTank, combined phishing corpora) to verify whether the observed accuracy-consistency trade-off holds beyond the Enron/Nazario domain.

3. **Ablation study of LoRA fine-tuning targets:** Systematically compare fine-tuning all four attention matrices versus query/value only across all three model families, measuring both classification accuracy and CC-SHAP scores to isolate whether the observed trade-offs stem from architectural priors or fine-tuning strategy differences.