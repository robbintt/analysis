---
ver: rpa2
title: Can Bayesian Neural Networks Make Confident Predictions?
arxiv_id: '2501.11773'
source_url: https://arxiv.org/abs/2501.11773
tags:
- predictive
- distribution
- posterior
- networks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Bayesian neural networks (BNNs)
  can make confident predictions by analyzing their posterior predictive distributions.
  The authors introduce a discretized prior on inner layer weights, which allows exact
  characterization of the posterior predictive as a Gaussian mixture.
---

# Can Bayesian Neural Networks Make Confident Predictions?

## Quick Facts
- **arXiv ID:** 2501.11773
- **Source URL:** https://arxiv.org/abs/2501.11773
- **Authors:** Katharine Fisher; Youssef Marzouk
- **Reference count:** 40
- **Primary result:** Under discretized priors on inner layer weights, Bayesian neural networks exhibit multimodal posterior predictive distributions with non-contracting variance in proportional asymptotic regimes, raising questions about their ability to produce confident predictions.

## Executive Summary
This paper investigates whether Bayesian neural networks (BNNs) can make confident predictions by analyzing their posterior predictive distributions. The authors introduce a discretized prior on inner layer weights, which enables exact characterization of the posterior predictive as a Gaussian mixture. This analytical approach reveals that distinct parameter realizations with identical training error often map to distinct modes in the posterior predictive distribution, leading to multimodal predictions. The analysis demonstrates that unimodal approximations (like Laplace or variational inference) systematically underestimate predictive uncertainty by collapsing these multiple modes. Most significantly, the authors show that the posterior predictive distribution does not contract as network and training set sizes grow proportionally, suggesting BNNs struggle to "forget their priors" in overparameterized regimes.

## Method Summary
The authors construct a discrete prior on inner layer weights Θ^(j) while maintaining a continuous Gaussian prior on final layer weights w. By placing P(Θ = Θ^(j)) = ρ_j over a finite set J of inner layer parameter realizations, they can exactly characterize the posterior predictive as a weighted sum of J Gaussian components. Each component's parameters follow from Bayesian linear regression given fixed Θ^(j), and mixture weights derive from each candidate's marginal likelihood via Bayes' rule. The authors construct optimal parameter candidates with high marginal likelihood through unitary rotations of feature representations, sampling the preimage of ReLU, and sampling column spaces—all preserving training fit while varying predictions elsewhere.

## Key Results
- Distinct parameter realizations with identical training error map to distinct modes in the posterior predictive distribution, causing multimodal predictions
- Unimodal posterior approximations are overconfident as they underestimate true predictive uncertainty by collapsing multiple modes
- The posterior predictive distribution does not contract as network and training set sizes grow proportionally, suggesting BNNs cannot "forget their priors" in overparameterized regimes
- A prior putting mass on constructed optimal parameters produces multimodal predictive distributions even when n=p, contrasting with Gaussian prior results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A discretized prior on inner layer weights enables exact analytical characterization of the posterior predictive distribution as a Gaussian mixture.
- **Mechanism:** By placing a discrete prior P(Θ = Θ^(j)) = ρ_j over a finite set J of inner layer parameter realizations while keeping final layer weights w continuous Gaussian, the posterior predictive becomes a weighted sum of J Gaussian components. Each component's mean and variance follow from Bayesian linear regression given a fixed Θ^(j), and mixture weights derive from each candidate's marginal likelihood via Bayes' rule.
- **Core assumption:** The discrete prior adequately covers the relevant regions of parameter space; finite J does not systematically exclude high-likelihood regions that would qualitatively change the predictive distribution.
- **Evidence anchors:** [abstract] "We demonstrate that under a discretized prior for the inner layer weights, we can exactly characterize the posterior predictive distribution as a Gaussian mixture." [Section 2, Eq. 5-8] Full derivation of mixture representation with explicit formulas for component densities and weights.

### Mechanism 2
- **Claim:** Distinct parameter realizations with identical training error can map to distinct modes in the posterior predictive distribution.
- **Mechanism:** Equivalence classes [Θ]_L exist where different Θ values produce identical marginal likelihood L(Θ) but generate different feature representations X_L, leading to different predictive distributions at test points. The authors construct such candidates via unitary rotations of X_L, sampling the preimage of ReLU, and sampling column spaces—all preserving training fit but varying predictions elsewhere.
- **Core assumption:** The activation function (ReLU) and network architecture permit non-trivial parameter symmetries beyond permutation/scale invariance.
- **Evidence anchors:** [Section 4] "We may now consider an equivalence class [Θ]_L of network parameters Θ that map to X*_L... It is also possible to construct {Θ^(j)} that map to the same training error without relying on permutation and scale invariance." [Figure 2] Visual demonstration of multiple distinct predictive modes arising from constructed candidates with identical L(Θ).

### Mechanism 3
- **Claim:** In proportional asymptotic regimes (n, p, d → ∞ with fixed ratios), the posterior predictive distribution does not contract, suggesting BNNs cannot "forget their priors" given finite data.
- **Mechanism:** When the prior places mass on constructed "optimal" parameters from equivalence class [Θ]_L, multiple high-likelihood candidates persist even as training data grows proportionally with model capacity. Predictive variance remains influenced by prior weight distribution across modes rather than concentrating on a single data-determined solution.
- **Core assumption:** Overparameterization fundamentally changes the relationship between data and posterior concentration compared to classical statistical settings.
- **Evidence anchors:** [Section 4, Figure 2] "The posterior predictive distribution does not contract as n, d, and p increase proportionally. For a given ratio p/n, we see that variance increases as the network and training set sizes scale proportionally." [Section 6] Key implications explicitly state non-contraction as central finding.

## Foundational Learning

- **Concept: Bayesian posterior predictive distribution**
  - Why needed here: The entire paper analyzes properties of π(ỹ | X₁, Y, x̃₁)—understanding how this differs from the posterior on parameters is essential.
  - Quick check question: Given a posterior p(θ|D) and likelihood p(y|x,θ), write the integral for the posterior predictive. Can you explain why a multimodal posterior might or might not produce multimodal predictions?

- **Concept: Marginal likelihood (evidence) and its role in model comparison**
  - Why needed here: Mixture weights in the predictive depend entirely on L(Θ^(j)) values; understanding why high marginal likelihood correlates with low training error is critical.
  - Quick check question: Why might two models with identical training error have different marginal likelihoods? What role does the prior play?

- **Concept: Convergence/contraction of posterior distributions**
  - Why needed here: The paper's central claim is that BNNs fail to achieve posterior contraction in overparameterized regimes—this is a technical concept with precise meaning.
  - Quick check question: In classical Bayesian statistics, what conditions guarantee posterior consistency? Why might overparameterization violate these?

## Architecture Onboarding

- **Component map:**
  Input x₁ ∈ ℝ^d → [Hidden layers with Θ = {Θ_ℓ, b_ℓ}] ← DISCRETE PRIOR (J candidates) → Features X_L ∈ ℝ^{d_L × n} → [Final layer w ∈ ℝ^p] ← CONTINUOUS GAUSSIAN PRIOR N(0, p^{-1}I) → Prediction ŷ = w^T x_L

- **Critical path:**
  1. Generate or construct J candidates Θ^(j) for inner layer parameters
  2. For each candidate, compute feature representation X_L^(j) and marginal likelihood L(Θ^(j)) via Eq. 8
  3. Normalize to obtain mixture weights P(Θ^(j)|X₁,Y) via Eq. 7
  4. For any test input x̃₁, compute component parameters (μ_j, σ²_j) via Eq. 6 for each candidate
  5. Form weighted Gaussian mixture for full predictive distribution

- **Design tradeoffs:**
  - **J (number of candidates):** Larger J improves coverage but increases compute; Figure 10 shows qualitative patterns stabilize at moderate J (~2000) for the multimodality question
  - **Prior on Θ:** Uniform vs. informed by optimal construction (Section 4) changes findings dramatically—Gaussian discretization misses structure that optimal construction reveals
  - **Noise variance γ²:** Acts as regularization; smaller γ² strengthens double descent effects at n=p (Figure 11)

- **Failure signatures:**
  - **Unimodal approximations (Laplace, VI):** Will systematically underestimate uncertainty when true predictive has multiple modes
  - **Single MAP/MLE estimate:** Captures only one trajectory in equivalence class, missing predictive variance from other high-likelihood regions
  - **Overconfident predictions:** Characterized by predictive variance smaller than true posterior predictive; occurs when approximation collapses mixture to single component

- **First 3 experiments:**
  1. **Reproduce Figure 1 heatmaps:** Vary n/d and p/d ratios with d=100, J=200,000, γ²=0.01. Count significant modes (weight > 10⁻⁶). Verify phase transition near n=p and expansion of unimodal region with increasing d.
  2. **Construct optimal parameters per Section 4:** For given (X₁, Y), build Θ candidates via rotation + column space sampling + ReLU preimage sampling. Compare resulting predictive modes to those from Gaussian-sampled Θ. Test n=p case where Gaussian prior falsely suggests unimodality.
  3. **Test contraction hypothesis:** Fix ratios n/d=0.7 and p/n, vary absolute scale (d ∈ {100, 200, 300}). Measure predictive variance as function of d. Confirm that variance does not decrease (may increase) under optimal parameter construction, contrary to Gaussian prior case.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can overparameterized Bayesian neural networks "forget their priors" sufficiently to produce confident predictions whose posterior predictive distributions contract around the truth as network and training set sizes grow proportionally?
- **Basis in paper:** [explicit] The paper states this behavior "raises the question of whether overparameterized BNNs can produce 'confident predictions'" and Section 3 asks "whether overparameterized BNNs can successfully 'forget their priors' to learn from data."
- **Why unresolved:** The paper demonstrates that with constructed optimal parameters, the posterior predictive distribution does not contract as n, p, and d increase proportionally (n=p regime), but does not establish theoretical conditions under which contraction could occur.
- **What evidence would resolve it:** Theoretical characterization of minimum growth rates required for network size relative to training set size to guarantee predictive contraction, or proof that such contraction is impossible under specified conditions.

### Open Question 2
- **Question:** What are the minimum rates at which network size must grow relative to training set size such that the posterior predictive distribution contracts?
- **Basis in paper:** [explicit] The authors state in Section 6: "Future work will...establish minimum rates at which network size must grow with respect to training set size such that the predictive distribution does not contract."
- **Why unresolved:** Current experiments only examine proportional scaling regimes (fixed ratios n/d and p/d) and do not establish theoretical bounds on required growth rates.
- **What evidence would resolve it:** Theoretical analysis establishing necessary and/or sufficient conditions on network-to-data scaling ratios for posterior predictive contraction.

### Open Question 3
- **Question:** Do the predictive distributions produced by partially stochastic networks (which treat only a subset of parameters as Bayesian) diverge significantly from the full Bayesian predictive distribution?
- **Basis in paper:** [explicit] Section 5.3 states: "A future application of our use of discrete priors to access predictive distributions may be to determine whether the distributions produced by partially stochastic networks diverge significantly from the full Bayesian predictive distribution."
- **Why unresolved:** The discrete prior methodology introduced in this paper has not yet been applied to analyze partially stochastic architectures.
- **What evidence would resolve it:** Application of the discrete prior framework to partially stochastic networks, comparing their predictive distributions to those obtained from full Bayesian treatment across multiple network sizes and data regimes.

### Open Question 4
- **Question:** How can implicit regularization and structural properties that make SGD-trained networks successful be formally captured within the Bayesian inference framework?
- **Basis in paper:** [explicit] Section 5.4 states: "Future work on Bayesian inference in overparameterized models might investigate how to formally capture such structural information" referring to "implicit regularization" and "self-induced regularization."
- **Why unresolved:** Current Bayesian approaches do not account for the structural qualities that enable successful generalization in standard neural network training.
- **What evidence would resolve it:** Development of prior specifications or inference methods that incorporate structural regularization properties, demonstrating improved generalization and calibrated uncertainty compared to standard BNN approaches.

## Limitations

- The analysis focuses on two-layer ReLU networks, limiting generalizability to deeper networks or different activation functions
- The constructed "optimal" parameters depend on specific symmetries of the ReLU activation and may not capture all sources of uncertainty in practical applications
- The discrete prior framework, while enabling analytical tractability, may not fully represent continuous BNN posteriors in real-world scenarios

## Confidence

**High confidence:** The analytical framework for Gaussian mixture posterior predictive is mathematically sound and reproducible. The mechanism linking parameter equivalence classes to predictive modes is rigorously established.

**Medium confidence:** The claim about non-contraction in proportional asymptotic regimes is supported by theoretical construction and limited numerical experiments, but lacks comprehensive empirical validation across diverse architectures and data regimes.

**Low confidence:** The practical implications for real-world BNN applications are not fully explored, particularly regarding how these findings translate to common approximate inference methods like VI or Laplace approximation.

## Next Checks

1. **Numerical validation:** Implement the full pipeline to reproduce Figure 1 heatmaps and verify phase transitions. Vary J systematically to assess sensitivity to candidate coverage.

2. **Architecture generalization:** Extend the analysis to three-layer networks with batch normalization or other architectures to test robustness of multimodality findings beyond the two-layer case.

3. **Approximate inference comparison:** Compare predictive distributions from Laplace approximation and VI to the exact Gaussian mixture on the same synthetic problems to quantify under-confidence in practical settings.