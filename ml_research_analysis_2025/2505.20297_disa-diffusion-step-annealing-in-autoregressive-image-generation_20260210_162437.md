---
ver: rpa2
title: 'DiSA: Diffusion Step Annealing in Autoregressive Image Generation'
arxiv_id: '2505.20297'
source_url: https://arxiv.org/abs/2505.20297
tags:
- diffusion
- generation
- steps
- tokens
- disa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in autoregressive
  models that use diffusion sampling (e.g., MAR, FlowAR, xAR, Harmon), where generating
  each token requires tens to hundreds of denoising steps, causing high inference
  latency. The authors observe that as more tokens are generated, subsequent tokens
  follow more constrained distributions and can be sampled with fewer diffusion steps.
---

# DiSA: Diffusion Step Annealing in Autoregressive Image Generation

## Quick Facts
- arXiv ID: 2505.20297
- Source URL: https://arxiv.org/abs/2505.20297
- Reference count: 40
- Key outcome: DiSA achieves 5-10× speedup for MAR and Harmon, and 1.4-2.5× for FlowAR and xAR, while maintaining generation quality.

## Executive Summary
This paper addresses the efficiency bottleneck in autoregressive models that use diffusion sampling (e.g., MAR, FlowAR, xAR, Harmon), where generating each token requires tens to hundreds of denoising steps, causing high inference latency. The authors observe that as more tokens are generated, subsequent tokens follow more constrained distributions and can be sampled with fewer diffusion steps. Based on this, they propose Diffusion Step Annealing (DiSA), a training-free method that gradually reduces the number of diffusion steps during generation—for example, from 50 steps early on to 5 steps later. Experiments show that DiSA achieves 5-10× speedup for MAR and Harmon, and 1.4-2.5× for FlowAR and xAR, while maintaining generation quality.

## Method Summary
DiSA is a training-free sampling strategy that reduces diffusion steps during autoregressive generation. It uses a linear scheduler to decrease diffusion steps $T(k)$ from $T_{early}$ (e.g., 50) to $T_{late}$ (e.g., 5) as the autoregressive step $k$ increases. The formula is $T(k) = T_{early} + (T_{late} - T_{early}) \times k/K$. The method is validated on MAR, FlowAR, xAR, and Harmon models using ImageNet 256×256 and GenEval datasets, measuring FID, IS, and inference time.

## Key Results
- MAR-L with DiSA runs 5.1× faster with nearly unchanged FID (1.77 vs 1.78)
- xAR-L improves FID from 1.28 to 1.23 while achieving 1.5× speedup
- DiSA achieves 5-10× speedup for MAR and Harmon, and 1.4-2.5× for FlowAR and xAR
- Asymmetric step sensitivity: reducing early steps harms quality more than reducing late steps

## Why This Works (Mechanism)

### Mechanism 1: Constrained Distribution Hypothesis
As more tokens are generated, subsequent token distributions become increasingly constrained by conditioning context, reducing the need for dense diffusion sampling. Early tokens require modeling broad, multimodal distributions, while later tokens complete already-determined structures, collapsing the distribution to narrower regions.

### Mechanism 2: Asymmetric Step Sensitivity
Reducing diffusion steps in early generation stages degrades quality significantly, while equivalent reductions in late stages have marginal impact. Early diffusion steps must explore high-entropy distribution space requiring fine-grained denoising, while late-stage sampling operates in low-entropy regions where coarse discretization suffices.

### Mechanism 3: Trajectory Straightness Enables Coarse Discretization
Denoising paths become progressively straighter as generation advances, permitting larger step sizes without divergence. Rectified Flow theory shows straight trajectories can be simulated with coarse time discretization. The paper measures straightness via cosine similarity between score direction and straight-line vector, finding it increases in later stages.

## Foundational Learning

- **Autoregressive Token Prediction**
  - Why needed: DiSA operates on AR models that generate tokens sequentially via $p(x_i | x_1, \ldots, x_{i-1})$; understanding how context conditions each step is essential
  - Quick check: Given tokens [dog_body, dog_head, grass], would you expect $p(\text{tail} | \text{context})$ to have higher or lower entropy than $p(\text{first\_token})$?

- **Diffusion Sampling as Iterative Denoising**
  - Why needed: The method directly manipulates diffusion step counts; you must understand that fewer steps = coarser discretization of the reverse SDE/ODE
  - Quick check: If a diffusion model uses 100 steps by default, what happens to sample quality if you naively reduce to 5 steps without any compensation?

- **Flow Matching and Rectified Flow Basics**
  - Why needed: FlowAR uses flow matching; xAR uses Euler/Euler-Maruyama samplers; understanding trajectory straightness requires knowing why straight paths enable fewer steps
  - Quick check: Why does a straight-line trajectory from noise to data require fewer integration steps than a curved trajectory?

## Architecture Onboarding

- **Component map**: Backbone $f$ -> Diffusion/Flow Head $\epsilon_\theta$ or $v_\theta$ -> Scheduler (DiSA) -> Tokenizer/VAE

- **Critical path**:
  1. Sample initial noise token for position $i$
  2. Compute $z_i = f(x_1, \ldots, x_{i-1})$
  3. Query DiSA scheduler for $T(k)$ steps
  4. Run $T(k)$ denoising iterations via diffusion/flow head
  5. Output clean token $x_i$, append to sequence
  6. Repeat until all tokens generated

- **Design tradeoffs**:
  - Scheduler choice: Linear provides smooth transition; two-stage is simplest but abrupt; cosine decays slowly then rapidly
  - $T_{early}$ vs $T_{late}$: Higher $T_{early}$ preserves quality but limits speedup; lower $T_{late}$ increases speedup but risks artifacts
  - Time offset: MAR requires starting from $t=950$ instead of $t=999$ due to head instability at late timesteps

- **Failure signatures**:
  - FID spikes dramatically when $T_{early}$ reduced (e.g., MAR-L fails to generate meaningful images at 10 steps)
  - Late-stage color/artifact inconsistencies if $T_{late}$ too low without time offset
  - No speedup gained if scheduler not actually reducing steps

- **First 3 experiments**:
  1. Ablation on $T_{early}$ vs $T_{late}$: Fix one, vary the other, plot FID to confirm asymmetric sensitivity exists
  2. Scheduler comparison: Implement all three schedulers, measure FID/IS at equivalent average step counts
  3. Straightness validation: Compute straightness metric at early/mid/late generation stages to verify the core hypothesis holds

## Open Questions the Paper Calls Out

- **Distribution complexity**: Early generation stages may require more Gaussian components in Mixture Models than later stages. The paper speculates about this but only validates DiSA on diffusion-based heads, not GMM-based heads like GIVT.

- **Method combination**: Combining DiSA with feature-caching techniques like LazyMar could achieve cumulative speedups. The paper notes these methods are orthogonal but doesn't validate their combination.

- **Optimal scheduling**: Whether there exists an optimal, learnable scheduler for diffusion step annealing that outperforms heuristic linear and cosine schedules remains open. The paper uses manually selected schedulers without exploring data-driven approaches.

## Limitations

- **Architecture specificity**: The asymmetric step sensitivity appears strongest in MAR and Harmon but smaller in FlowAR and xAR, suggesting limited generalizability across architectures.

- **Distribution constraint assumptions**: The assumption that entropy monotonically decreases with generation progress may not hold for tasks requiring diversity maintenance like inpainting or conditional generation with ambiguous completion.

- **Time offset necessity**: MAR requires starting diffusion at t=950 instead of t=999 when using low steps, indicating head instability and raising questions about generalizability to other diffusion-based AR models.

## Confidence

**High Confidence**: The core observation that step reduction works asymmetrically is well-supported by Figure 4 experiments across multiple architectures. Quantitative claims about MAR-L appear reproducible.

**Medium Confidence**: The mechanism explanations (constrained distributions, trajectory straightness) are plausible but primarily correlational. The asymmetric sensitivity could also stem from other factors like attention patterns.

**Low Confidence**: Claims about DiSA being "complementary to existing methods" lack quantitative validation. The paper doesn't test combinations with acceleration techniques, so we can't confirm additive benefits.

## Next Checks

1. **Distribution entropy validation**: Measure the conditional entropy H(x_i | x_1, ..., x_{i-1}) at different generation stages using the trained diffusion head's predicted variance to verify entropy decreases monotonically and correlates with step reduction tolerance.

2. **Cross-architecture generalization**: Implement DiSA on a diffusion-based AR model not in the paper (e.g., a vanilla DDPM-based AR generator) to test whether asymmetric step sensitivity holds and whether time offsets are needed.

3. **Method combination study**: Apply DiSA to xAR with adversarial distillation enabled to measure whether DiSA provides additional speedup beyond what distillation already achieves, confirming the "complementary" claim quantitatively.