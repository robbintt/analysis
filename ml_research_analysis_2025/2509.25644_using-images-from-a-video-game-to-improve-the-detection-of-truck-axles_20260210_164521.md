---
ver: rpa2
title: Using Images from a Video Game to Improve the Detection of Truck Axles
arxiv_id: '2509.25644'
source_url: https://arxiv.org/abs/2509.25644
tags:
- data
- images
- neural
- networks
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether synthetic images from the video
  game Euro Truck Simulator 2 can effectively train convolutional neural networks
  (CNNs) to detect real-life truck axles. Three datasets were created: real-world
  truck images, synthetic truck images from the game, and a mixed dataset combining
  both.'
---

# Using Images from a Video Game to Improve the Detection of Truck Axles

## Quick Facts
- arXiv ID: 2509.25644
- Source URL: https://arxiv.org/abs/2509.25644
- Reference count: 40
- Primary result: Synthetic images from Euro Truck Simulator 2 achieved up to 99% mAP for truck axle detection, matching or exceeding real-world training data.

## Executive Summary
This study investigated whether synthetic images from the video game Euro Truck Simulator 2 can effectively train convolutional neural networks to detect real-life truck axles. Three datasets were created: real-world truck images, synthetic truck images from the game, and a mixed dataset combining both. These datasets were used to train nine YOLO-based models across 27 different configurations. Models were evaluated using recall, precision, F1-score, and mean Average Precision (mAP) on a fixed test set of real-world images containing 119 truck axles. Results showed that synthetic images improved or matched the performance of real-world-only training, with mAP scores reaching up to 99%. The Mann-Whitney U test confirmed statistical equivalence between synthetic and real-world training, validating the use of video game imagery as a cost-effective, safe, and scalable alternative for training CNNs in transportation engineering applications.

## Method Summary
The study created three datasets: 346 real-world truck images from Brazilian highways, 326 synthetic truck images extracted from Euro Truck Simulator 2, and a mixed dataset combining 175 real and 175 synthetic images. Nine YOLO variants (YOLOv3-tiny, v3, v3-spp, v8n, v8l, v8x, v11n, v11l, v11x) were trained using pre-trained COCO weights on these datasets, resulting in 27 model configurations. Training was implemented using TensorFlow/Keras on a GeForce GTX 1080, with each model running for approximately 4 hours. Models were evaluated on a fixed test set of 36 real-world images containing 119 axles, using metrics including Recall, Precision, F1-score, and mean Average Precision (mAP). Statistical significance was verified using the Mann-Whitney U test.

## Key Results
- Synthetic-only training achieved mAP scores up to 99%, matching or exceeding real-world-only training performance.
- Mixed datasets consistently improved or matched performance compared to real-only training, with larger YOLO models (v8x, v11x) achieving the highest scores.
- Statistical analysis confirmed no significant difference between synthetic and real-world training performance (Mann-Whitney U test).
- YOLOv3-tiny performed poorly on real-world training (mAP 0.03%) but improved significantly when trained on mixed data.

## Why This Works (Mechanism)

### Mechanism 1
Synthetic imagery from high-fidelity video games may serve as a functional proxy for real-world data when detecting structural vehicle components (axles). CNNs extract hierarchical features (edges, textures, shapes) from input images. If the game's 3D rendering engine produces visual gradients and geometries for axles that sufficiently overlap with the feature space of real photographs, the model generalizes from the synthetic domain to the real domain. The core assumption is that the "reality gap" (visual differences between rendered and real pixels) does not disrupt the learning of the specific structural features required for axle detection. Evidence shows synthetic images proved reliable for training data, and video games are increasingly used as a source of data producing realistic 3D models. Break condition: If game textures or lighting models lack sufficient realism, the model will learn artifacts specific to the game engine that fail to trigger on real footage.

### Mechanism 2
Mixing synthetic and real images improves model robustness by expanding the "viewpoint diversity" of the training distribution. The real-world dataset suffered from a specific bias (all images were side-view, perpendicular to the road). The synthetic dataset introduced varied axle angles. By combining them, the decision boundary of the CNN becomes less sensitive to specific camera orientations, reducing overfitting to a single viewpoint. The core assumption is that performance gain is driven by geometric variability rather than purely texture realism. Evidence shows all trucks on the real-world database were from a side viewpoint while synthetic images have a more comprehensive range of axle positions. Using a mixed database meant better or at least comparable results due to the variability of examples. Break condition: If synthetic viewpoints do not cover the distribution found in the deployment environment, performance will degrade due to distribution mismatch.

### Mechanism 3
Larger, deeper YOLO architectures act as more effective feature extractors for cross-domain transfer than "tiny" or compressed variants. Deeper networks (e.g., YOLOv8x) possess higher capacity to learn abstract, disentangled representations of "axle-ness" that are invariant to superficial differences between game renders and real photos. Smaller models (YOLOv3-tiny) lack the capacity to bridge this domain gap. The core assumption is that model capacity is the bottleneck for domain adaptation, not the fundamental similarity of the data. Evidence shows YOLOv3-tiny performed poorly on real-world training (mAP 0.03%) vs. YOLOv8x (mAP 97.60%), and smaller models are capable of running on portable devices but by a diminishing factor in accuracy. Break condition: If the training dataset is too small, large models may overfit the synthetic data rather than learning generalized features.

## Foundational Learning

- **Concept: Sim-to-Real Transfer**
  - Why needed: This is the core premise of the paper. One must understand that neural networks do not "see" objects like humans do; they match statistical patterns. If the game patterns match reality closely enough, the transfer works.
  - Quick check: Does the simulation engine accurately render the lighting and geometry of the specific object class (axles) under the conditions where the model will be deployed?

- **Concept: Statistical Significance (Mann-Whitney U Test)**
  - Why needed: The authors rely on this test to prove that synthetic data is not just "working," but is statistically equivalent to real data.
  - Quick check: Are the performance differences between Model A and Model B likely due to random chance, or is there a genuine statistical equivalence in their distributions?

- **Concept: Model Capacity vs. Dataset Scale**
  - Why needed: The paper shows that tiny models fail while large models succeed on the same data. Understanding the balance between model complexity and available training samples is critical.
  - Quick check: Is the model large enough to capture the visual complexity of the object, or is it too simple to distinguish the object from background noise?

## Architecture Onboarding

- **Component map:** Input (Real, Synthetic, Mixed datasets) -> Backbone (YOLO variants with COCO pre-trained weights) -> Head (Bounding box regression for axle localization) -> Evaluation (mAP and Mann-Whitney U test)
- **Critical path:** The most sensitive step is Dataset Curation. As noted in the paper, the real dataset had a "side viewpoint" bias. The improvement came not just from "more data," but from the synthetic data's ability to correct this angular bias.
- **Design tradeoffs:**
  - Tiny Models (YOLOv3-tiny/v8n): Fast, low resource usage, but fragile to domain shifts (failed to detect axles when trained only on real data).
  - Large Models (YOLOv8x/v11x): Computationally expensive (longer training/inference), but robust enough to handle the "mixed" data and achieve 99% mAP.
- **Failure signatures:**
  - High False Positives: YOLOv3-tiny produced 153 false positives on 119 axles, indicating the model is latching onto non-axle wheel components or background textures.
  - Viewpoint Overfitting: A model that works on side-view trucks but fails on angled trucks indicates a lack of rotational variance in the training set.
- **First 3 experiments:**
  1. Baseline Real: Train a standard YOLOv8 on only the 346 real images to establish a performance floor (expected: ~90-92% mAP based on paper).
  2. Domain Shift Test: Train the same model on only synthetic images and test on real images. If this fails (mAP < 50%), the simulation fidelity is insufficient.
  3. Mixed Ablation: Combine 50% real and 50% synthetic data. Verify if mAP increases or stabilizes (indicating successful regularization/diversification).

## Open Questions the Paper Calls Out

### Open Question 1
Does synthetic data retain its performance advantage when the real-world training data matches the synthetic set's diversity in viewing angles and lighting conditions? The authors note that real-world images were limited to a "side viewpoint," whereas the synthetic database provided a "more comprehensive range of axle positions," making it unclear if the synthetic data's success was due to inherent realism or simply greater variety. This remains unresolved because the study did not control for the variability gap between the homogeneous real-world training set and the heterogeneous synthetic set. An experiment where real-world training data is curated to match the angular diversity of the synthetic set, followed by comparative evaluation, would resolve this question.

### Open Question 2
Can these findings be replicated using video games with lower graphical fidelity or different rendering engines? The study relies exclusively on Euro Truck Simulator 2, which features high-fidelity 3D models, leaving the importance of graphical realism on model transferability untested. This remains unresolved because the specific contribution of the game's visual realism to the high mAP scores (99%) remains isolated to a single engine. Training identical architectures on synthetic data from lower-fidelity simulators and comparing the resulting transfer learning performance would resolve this question.

### Open Question 3
Why do lightweight models (e.g., YOLOv3-tiny) fail to generalize from synthetic-only data compared to larger models? While larger models achieved >98% mAP with synthetic data, YOLOv3-tiny collapsed to 1.78% mAP, suggesting the "reality gap" impacts smaller architectures disproportionately. This remains unresolved because the paper reports the performance drop but does not analyze whether this is due to insufficient feature extraction depth or a lack of domain adaptation layers. An ablation study analyzing feature map activations in lightweight versus large models when processing synthetic versus real inputs would resolve this question.

## Limitations
- The study relies on a small test set (36 images with 119 axles), making statistical metrics highly sensitive to individual prediction errors and limiting generalizability to other datasets or truck models.
- The Euro Truck Simulator 2 synthetic dataset may not capture the full diversity of real-world trucking scenarios (e.g., lighting conditions, dirt/mud on axles, camera angles not represented in the game).
- No ablation study was performed to isolate the effect of viewpoint diversity from other factors like texture fidelity or model architecture.

## Confidence
- High Confidence: The core finding that synthetic data can match real data performance for axle detection is well-supported by statistical testing (Mann-Whitney U) and multiple model evaluations.
- Medium Confidence: The claim that synthetic data improves robustness through viewpoint diversity is plausible given the methodology, but not definitively proven without controlled ablation experiments.
- Low Confidence: The assertion that YOLOv8x is universally superior for this task across all potential datasets and truck types is overstated, as the results are dataset-specific.

## Next Checks
1. **Dataset Expansion:** Test the synthetic-to-real transfer on a larger, more diverse real-world dataset (different truck models, lighting, weather conditions) to assess generalization.
2. **Ablation on Viewpoint:** Train models using only synthetic images with side-view angles (to match the real dataset bias) and compare performance to mixed data to isolate the effect of viewpoint diversity.
3. **Model Capacity Scaling:** Train a tiny model (YOLOv3-tiny) on a much larger synthetic dataset (e.g., 1000+ images) to determine if increased data volume can compensate for limited model capacity in domain transfer tasks.