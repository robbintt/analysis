---
ver: rpa2
title: Entropy is all you need for Inter-Seed Cross-Play in Hanabi
arxiv_id: '2511.22581'
source_url: https://arxiv.org/abs/2511.22581
tags:
- entropy
- policies
- coefficient
- which
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper shows that standard IPPO with an increased entropy coefficient
  of 0.05, rather than the typical 0.01, achieves new state-of-the-art inter-seed
  cross-play performance in Hanabi. This beats previous specialized algorithms by
  a significant margin.
---

# Entropy is all you need for Inter-Seed Cross-Play in Hanabi

## Quick Facts
- arXiv ID: 2511.22581
- Source URL: https://arxiv.org/abs/2511.22581
- Authors: Johannes Forkel; Jakob Foerster
- Reference count: 40
- Key result: Standard IPPO with entropy coefficient 0.05 achieves state-of-the-art inter-seed cross-play in Hanabi, outperforming specialized algorithms.

## Executive Summary
This paper demonstrates that standard IPPO with increased entropy regularization (α=0.05) achieves superior inter-seed cross-play performance in Hanabi compared to previous specialized algorithms. The authors show that entropy regularization prevents symmetry-breaking across random seeds, causing independently trained agents to converge to mutually compatible joint policies. Additionally, using RNNs instead of feed-forward networks and setting λGAE≈0.9 further improve cross-play scores. While entropy helps prevent symmetry-breaking and improves cross-play, it doesn't always lead to optimal symmetric policies in all Dec-POMDPs.

## Method Summary
The method involves standard IPPO with three key modifications: (1) increased entropy coefficient α=0.05, (2) 2-layer LSTM actor-critic architecture with 512 hidden units, and (3) λGAE=0.9. The approach uses public-private observation architecture optionally and employs weight sharing across agents. The entropy regularization creates a more concave optimization landscape that encourages symmetric convergence across different random seeds.

## Key Results
- Inter-seed cross-play score: 3.18 ± 0.12 (versus previous state-of-the-art of 2.83 ± 0.16)
- Entropy coefficient α=0.05 prevents symmetry-breaking across seeds
- RNN architectures substantially outperform feed-forward networks for inter-seed XP
- λGAE=0.9 reduces the SP/XP gap by mitigating asymmetric critic bias

## Why This Works (Mechanism)

### Mechanism 1: Entropy Regularization Enforces Symmetric Convergence
Increasing the entropy coefficient from 0.01 to 0.05 causes independently seeded training runs to converge to mutually compatible joint policies. Entropy is strictly concave on the probability simplex, and increasing α makes the optimization landscape more concave, creating a threshold above which all random seeds converge to the same limiting policy rather than breaking symmetries in different ways.

### Mechanism 2: Recurrent Architectures Enable Signal Interpretation
RNN-based actor-critic networks achieve substantially higher inter-seed XP than feed-forward alternatives because RNNs maintain action-observation histories, allowing agents to infer what information partners intend to convey through their actions—a capability feed-forward networks lack with limited memory.

### Mechanism 3: GAE Lambda Controls Asymmetric Critic Bias
Setting λGAE≈0.9 reduces the SP/XP gap by mitigating asymmetric bias in advantage estimates. Low λGAE yields low-variance but high-bias advantage estimates. If the critic learns asymmetrically biased value functions, the actor will break symmetries accordingly. Higher λGAE reduces this bias at acceptable variance cost.

## Foundational Learning

- **Zero-Shot Coordination (ZSC)**: Why needed here: The paper's central metric is inter-seed cross-play—pairing agents from different training seeds without retraining. Quick check: Why might two agents trained identically except for random seed fail to coordinate?
- **Symmetry Breaking in Dec-POMDPs**: Why needed here: Standard IPPO breaks game symmetries in arbitrary, seed-dependent ways, creating incompatible conventions. Quick check: In a game where both players choosing A or both choosing B yield equal rewards, why might independently trained policies fail to coordinate?
- **Entropy-Regularized Policy Gradient**: Why needed here: The key intervention modifies the standard PPO objective by increasing the entropy bonus coefficient. Quick check: How does adding an entropy term to the policy gradient objective affect exploration versus exploitation?

## Architecture Onboarding

- **Component map**: IPPO pipeline with three modifications: entropy coefficient=0.05 → symmetric convergence → XP improvement; RNN architecture → temporal inference → XP improvement; λGAE=0.9 → reduced bias → smaller SP/XP gap
- **Critical path**: Entropy coefficient → symmetric convergence → XP improvement
- **Design tradeoffs**: Higher entropy slightly reduces SP (~0.1 points) but improves XP dramatically (~5-7 points vs. α=0.01). RNNs slower than FF but essential. λGAE=1.0 collapses performance.
- **Failure signatures**: XP << SP with low entropy; poor XP with FF architecture regardless of entropy; SP/XP gap persists with λGAE < 0.8
- **First 3 experiments**:
  1. Entropy sweep (0.01–0.10) measuring SP and XP across 4 seeds each to find the α where SP ≈ XP
  2. Architecture comparison: LSTM vs. FF vs. Public-Private LSTM at optimal entropy
  3. λGAE sweep (0.0–1.0) at α=0.05 to verify SP/XP gap closes around 0.9

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Findings may not generalize beyond Hanabi to other Dec-POMDPs
- Entropy coefficient of 0.05 is heuristic and may require tuning for different games
- Mechanism explaining why RNNs enable temporal inference lacks mechanistic detail
- λGAE analysis has limited empirical validation of symmetry-breaking connection

## Confidence
- **High Confidence**: Entropy coefficient of 0.05 improving inter-seed cross-play in Hanabi
- **Medium Confidence**: RNN architectures enabling better temporal inference for partner action interpretation
- **Medium Confidence**: λGAE=0.9 reducing SP/XP gap through critic bias mitigation
- **Low Confidence**: Generalization of these findings to other Dec-POMDPs beyond Hanabi

## Next Checks
1. Apply the α=0.05, LSTM, λGAE=0.9 configuration to at least two other Dec-POMDP benchmarks to verify generalization beyond Hanabi.
2. Design experiments isolating temporal inference capability to confirm that understanding partner intent drives XP improvements.
3. Conduct a finer-grained entropy sweep across more seeds to precisely characterize threshold behavior and verify the "more concave" optimization landscape claim.