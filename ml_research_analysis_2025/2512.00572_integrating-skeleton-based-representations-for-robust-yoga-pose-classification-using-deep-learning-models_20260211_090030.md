---
ver: rpa2
title: Integrating Skeleton Based Representations for Robust Yoga Pose Classification
  Using Deep Learning Models
arxiv_id: '2512.00572'
source_url: https://arxiv.org/abs/2512.00572
tags:
- pose
- yoga
- poses
- dataset
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a curated dataset, \"Yoga-16,\" and systematically\
  \ evaluates deep learning architectures for yoga pose classification using skeleton-based\
  \ representations. The research addresses limitations in existing datasets and benchmarks\
  \ three deep learning models\u2014VGG16, ResNet50, and Xception\u2014on three input\
  \ modalities: direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton\
  \ images."
---

# Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models

## Quick Facts
- arXiv ID: 2512.00572
- Source URL: https://arxiv.org/abs/2512.00572
- Reference count: 40
- Primary result: VGG16 with MediaPipe skeleton input achieves 96.09% accuracy on 16-class yoga pose classification

## Executive Summary
This study addresses the limitations of existing yoga pose datasets by introducing a curated "Yoga-16" dataset with 1,280 images across 16 yoga poses. The research systematically evaluates three deep learning architectures (VGG16, ResNet50, Xception) on three input modalities: raw images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images. The results demonstrate that skeleton-based representations significantly outperform raw image inputs, with VGG16 achieving the highest accuracy of 96.09% using MediaPipe Pose skeleton input. The study also provides Grad-CAM visualizations to explain model decisions and confirms generalizability through cross-validation analysis.

## Method Summary
The authors created a curated Yoga-16 dataset with 1,280 images (896 train, 256 test, 128 validation) across 16 yoga poses, each with approximately 80 images. They systematically compared three CNN architectures (VGG16, ResNet50, Xception) trained from scratch on three input modalities: direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images. VGG16 was configured with 541,712 trainable parameters, using Adam optimizer (lr=0.001), batch size 32, 50 epochs, categorical cross-entropy loss, early stopping (patience=10), and learning rate reduction. Skeleton images were generated using MediaPipe's BlazePose (33 keypoints) and YOLOv8 Pose estimation models, rendered as consistent black-and-white line drawings.

## Key Results
- VGG16 with MediaPipe skeleton input achieved 96.09% accuracy, significantly outperforming other combinations
- Skeleton-based representations consistently outperformed raw image inputs across all three CNN architectures
- ResNet50 and Xception performed well but did not surpass VGG16 on this skeleton dataset
- MediaPipe Pose estimation produced more accurate classifications (96.09%) than YOLOv8 Pose (91.41%) for skeleton generation

## Why This Works (Mechanism)
The success stems from transforming yoga pose classification from a complex visual recognition task into a simpler skeletal pattern recognition problem. By extracting 33 keypoints per pose using MediaPipe or YOLOv8 and rendering them as consistent skeleton images, the CNN architectures can focus on the essential geometric relationships between body parts rather than being distracted by clothing, background, lighting, and other visual noise present in raw images. The skeleton representation standardizes the input format, making the classification task more about recognizing spatial configurations than holistic image understanding.

## Foundational Learning
- **Pose Estimation (Keypoint Detection)**: Transforming raw RGB images into skeleton-based representations by detecting human joints. Why needed: To convert yoga images into standardized skeleton format for CNN input. Quick check: Can you list the 33 x,y,z keypoints output by MediaPipe BlazePose for a yoga pose?
- **Convolutional Neural Networks (CNNs) for Classification**: Using CNN architectures to classify skeleton images into yoga poses. Why needed: Core classification mechanism for transforming skeleton features into pose predictions. Quick check: How does max-pooling affect feature map dimensions and why is this useful before classification?
- **Training from Scratch vs. Transfer Learning**: Choosing to train CNNs from scratch rather than using ImageNet weights. Why needed: ImageNet features don't align with skeletal patterns, making pre-trained weights ineffective. Quick check: What are two problems with using ImageNet weights for synthetic black-and-white skeleton images?

## Architecture Onboarding
- **Component map**: Raw Image -> Pose Estimator (MediaPipe/YOLOv8) -> Skeleton Image Renderer -> CNN Backbone (VGG16/ResNet50/Xception) -> Global Average Pooling -> Dense Layer -> Softmax -> Predicted Pose Class
- **Critical path**: 1) Data Preprocessing: Ensure robust pose estimation and consistent skeleton rendering. 2) Model Training: Train CNN from scratch with specified hyperparameters. 3) Evaluation: Use K-fold cross-validation to ensure generalizability on small dataset.
- **Design tradeoffs**: VGG16 simplicity and parameter count favored skeleton patterns; MediaPipe keypoint consistency superior to YOLOv8; training from scratch avoids domain mismatch but requires sufficient data.
- **Failure signatures**: Similar poses with overlapping skeletal structures cause confusion; inconsistent pose estimation leads to downstream failures; overfitting risk on small dataset.
- **First 3 experiments**: 1) Baseline with Raw Images: Train all three CNNs on direct images. 2) Skeleton Ablation: Compare VGG16 performance on MediaPipe vs YOLOv8 skeletons. 3) Generalization Test: Evaluate best model on separate YouTube video test set without retraining.

## Open Questions the Paper Calls Out
- **Multi-modal input representations**: Can combining skeleton inputs with other modalities resolve misclassification of poses with subtle or overlapping skeletal features? The paper lists this as future work since current single-modality skeleton inputs struggle with ambiguity in similar poses.
- **Real-time AR/VR optimization**: How can the skeleton-based VGG16 architecture be optimized for real-time inference within AR/VR environments? While achieving high accuracy, computational efficiency and latency on resource-constrained hardware were not evaluated.
- **Specialized skeleton pre-training**: Does pre-training on a large-scale generic skeleton dataset provide better initialization than training from scratch for yoga classification? The authors trained from scratch because ImageNet weights don't align with skeletal patterns, suggesting a gap in transfer learning strategies.

## Limitations
- Small curated dataset (Yoga-16: 1,280 images) limits real-world generalization despite cross-validation and separate test set
- Synthetic skeleton rendering rather than raw keypoint sequences may not capture all pose nuances
- Claims about model generalizability beyond Yoga-16 remain untested on diverse external datasets

## Confidence
- **High**: VGG16 with MediaPipe skeleton input achieves 96.09% accuracy on Yoga-16 test set
- **Medium**: Skeleton-based representations outperform raw images across all three CNN architectures
- **Low**: Claims about model generalizability beyond Yoga-16 without external validation on diverse datasets

## Next Checks
1. Replicate the full pipeline (pose estimation → skeleton rendering → CNN classification) on an independent yoga pose dataset to test generalizability
2. Conduct ablation on skeleton rendering parameters (line thickness, color, keypoint markers) to quantify their impact on classification accuracy
3. Compare Grad-CAM visualizations quantitatively against other explanation methods (e.g., SHAP) to assess robustness of interpretability claims