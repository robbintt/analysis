---
ver: rpa2
title: 'Scaling Crowdsourced Election Monitoring: Construction and Evaluation of Classification
  Models for Multilingual and Cross-Domain Classification Settings'
arxiv_id: '2503.03582'
source_url: https://arxiv.org/abs/2503.03582
tags:
- election
- reports
- classification
- crowdsourced
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of scaling crowdsourced election
  monitoring by advancing automated classification of election reports to multilingual
  and cross-domain settings. A two-step classification approach is proposed: first
  identifying informative reports, then categorizing them into information types.'
---

# Scaling Crowdsourced Election Monitoring: Construction and Evaluation of Classification Models for Multilingual and Cross-Domain Classification Settings

## Quick Facts
- arXiv ID: 2503.03582
- Source URL: https://arxiv.org/abs/2503.03582
- Reference count: 23
- Primary result: 77% F1 for informativeness detection, 75% F1 for information type classification; 59% zero-shot, 63% few-shot cross-domain transfer

## Executive Summary
This study advances automated classification of crowdsourced election reports for multilingual and cross-domain settings. The authors propose a two-step classification pipeline: first identifying informative reports, then categorizing them into information types. Experiments use multilingual transformer models (XLM-RoBERTa, mDeBERTav3) and SBERT embeddings, augmented with linguistically motivated features including context, temporal information, and sentiment. The approach achieves strong performance (77% F1 for informativeness, 75% F1 for information type classification) and demonstrates promising cross-domain transfer potential (59% zero-shot, 63% few-shot). However, a performance bias favoring English over Swahili reports is observed, highlighting the need for balanced language representation in training data.

## Method Summary
The study employs a two-step classification approach for crowdsourced election reports from Kenya (2017, 2022) and Nigeria (2023). Step 1 uses XLM-RoBERTa with linguistically motivated features (context from 3 prior reports, temporal encoding, sentiment) to filter informative from non-informative reports. Step 2 classifies informative reports into 5 information types using SBERT embeddings with the same features and logistic regression. Cross-domain experiments test transfer from Kenya to Nigeria data using zero-shot and few-shot approaches. Models are evaluated using macro F1-score with stratified 70/10/20 splits averaged over 3 runs.

## Key Results
- Achieved 77% F1-score for binary informativeness detection using XLM-RoBERTa with linguistically motivated features
- Achieved 75% F1-score for 5-way information type classification using SBERT embeddings with logistic regression
- Demonstrated cross-domain transfer potential with 59% F1-score in zero-shot and 63% F1-score in few-shot settings from Kenya to Nigeria
- Observed 7 percentage-point performance gap favoring English (78% F1) over Swahili (71% F1) reports

## Why This Works (Mechanism)

### Mechanism 1
- A two-step classification pipeline improves signal-to-noise handling by first filtering non-informative reports, reducing class imbalance and noise before complex multi-class classification
- Core assumption: Non-informative reports share distinct linguistic patterns separable from all informative categories collectively
- Break condition: If the informativeness boundary is highly ambiguous or domain-specific, the first-stage filter may systematically exclude needed report types

### Mechanism 2
- Augmenting transformer embeddings with linguistically motivated features (context, temporal signals, sentiment) yields measurable performance gains
- Core assumption: Election reports exhibit predictable temporal and contextual patterns that complement semantic text representations
- Break condition: If temporal patterns differ substantially across electoral contexts, engineered temporal features may mislead rather than help

### Mechanism 3
- Pre-trained models transfer to new domains more effectively when fine-tuned with minimal target-domain data than training from scratch
- Core assumption: Electoral reports across countries share sufficient semantic structure for cross-domain transfer despite different political contexts
- Break condition: If target domain has fundamentally different information type taxonomy or reporting conventions, zero-shot transfer degrades substantially

## Foundational Learning

- **Multilingual Transformer Architectures (XLM-RoBERTa, mDeBERTaV3)**
  - Why needed: Reports contain English, Swahili, and Sheng' code-switched text; models must handle multilingual input without separate language-specific pipelines
  - Quick check: Can you explain why XLM-R's shared vocabulary across 100 languages enables cross-lingual transfer compared to monolingual BERT?

- **Sentence Embeddings vs. Fine-tuned Transformers**
  - Why needed: SBERT embeddings + Logistic Regression achieved comparable results to fine-tuned XLM-R (75% vs. 72% F1), offering simpler deployment for resource-constrained organizations
  - Quick check: What is the trade-off between using frozen SBERT embeddings with a linear classifier versus fine-tuning the full transformer?

- **Zero-shot and Few-shot Transfer Learning**
  - Why needed: Real-world deployments often lack labeled data at election onset; models must generalize or adapt quickly with minimal supervision
  - Quick check: Why does fine-tuning a pre-trained model with 10 target-domain examples outperform training a model from scratch on those same examples?

## Architecture Onboarding

- **Component map**: Raw election report text + metadata → Preprocessing (URL/mention cleaning, emoji conversion) → Feature Engineering (context aggregation, temporal encoding, sentiment scores) → Stage 1 Model (Binary informativeness classifier) → Stage 2 Model (Multi-class information type classifier) → Cross-Domain Adapter (Few-shot fine-tuning)

- **Critical path**: Data ingestion and deduplication → Stage 1 inference (filter to informative reports) → Stage 2 inference (classify into 5 information types) → Language-specific performance monitoring

- **Design tradeoffs**:
  - SBERT + LR simpler to deploy but may underperform on nuanced categories; XLM-R captures more context but requires GPU for fine-tuning
  - XLM-R achieves best F1 via higher precision; election monitoring may prefer higher recall to avoid missing critical reports
  - Zero-shot enables immediate deployment (59% F1); few-shot with 105 labels improves to 63% but requires annotation effort

- **Failure signatures**:
  - 7 percentage-point gap between English (78% F1) and Swahili (71% F1) indicates training data imbalance
  - Keywords like "hurt" or "win" trigger misclassification due to spurious correlations
  - "Positive Events" overlaps conceptually with all other categories (20% of errors)
  - Reports with images/videos misclassified when text alone is insufficient

- **First 3 experiments**:
  1. Train SVM with TF-IDF on Kenya dataset to establish baseline (~70% F1 for information type classification)
  2. Test XLM-R with each feature (context, temporal, sentiment) individually, then combined, to quantify contribution to 3-5% F1 improvement
  3. Apply Kenya-trained model to Nigerian data in zero-shot setting to confirm ~59% F1 before few-shot fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Performance bias favoring English over Swahili reports (7 percentage-point gap) could undermine real-world deployment in multilingual contexts
- Reliance on single-source datasets (Uchaguzi for Kenya, Uzabe for Nigeria) without external validation limits generalizability
- Two-step classification approach adds architectural complexity and potential error propagation from first stage to second

## Confidence
- **High Confidence**: Multilingual transformers augmented with linguistically motivated features improve election report classification (supported by controlled experiments and consistent performance gains)
- **Medium Confidence**: Cross-domain transfer learning effectiveness (based on limited target-domain data from one country)
- **Low Confidence**: Real-world deployment feasibility without addressing English-Swahili performance disparity and without validating against truly unseen election events

## Next Checks
1. **Language Balance Audit**: Stratify performance metrics by language across all models to quantify and visualize the English-Swahili gap; test whether oversampling or data augmentation for Swahili reports reduces the bias

2. **Cross-Country Transfer Validation**: Apply the Kenya-trained models to election monitoring data from a third country (e.g., Ghana or Uganda) without any fine-tuning to assess true zero-shot generalization capability

3. **Temporal Pattern Stability Test**: Validate whether engineered temporal features (cyclic hour encoding, days from election) maintain predictive power across different electoral systems or introduce spurious correlations