---
ver: rpa2
title: 'Aligned Query Expansion: Efficient Query Expansion for Information Retrieval
  through LLM Alignment'
arxiv_id: '2507.11042'
source_url: https://arxiv.org/abs/2507.11042
tags:
- query
- retrieval
- expansion
- expansions
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of vocabulary mismatch in information
  retrieval by improving query expansion using large language models. The authors
  propose Aligned Query Expansion (AQE), a method that directly optimizes query generation
  for retrieval effectiveness through fine-tuning with Direct Preference Optimization
  (DPO) and Rejection Sampling Fine-Tuning (RSFT), eliminating the need for expensive
  filtering steps.
---

# Aligned Query Expansion: Efficient Query Expansion for Information Retrieval through LLM Alignment

## Quick Facts
- **arXiv ID**: 2507.11042
- **Source URL**: https://arxiv.org/abs/2507.11042
- **Reference count**: 35
- **Primary result**: Achieves up to 17.8% improvement in top-1 retrieval accuracy while reducing computational memory by 71.1% and time by 69.5% compared to filtering-based approaches

## Executive Summary
This paper addresses vocabulary mismatch in information retrieval by proposing Aligned Query Expansion (AQE), a method that uses large language model alignment techniques to optimize query expansion for retrieval effectiveness. AQE eliminates expensive filtering steps by directly fine-tuning LLMs with retrieval feedback through Direct Preference Optimization (DPO) and Rejection Sampling Fine-Tuning (RSFT). The method generates candidate expansions, ranks them by the BM25 rank of the true document, and uses this signal to align the model to prefer expansions that improve retrieval performance. Empirical results demonstrate significant improvements in retrieval accuracy while achieving substantial computational efficiency gains.

## Method Summary
AQE follows a three-stage pipeline: first, it generates 50 candidate query expansions per query using zero-shot LLM generation with a simple prompt. Second, it ranks these expansions by retrieving documents with BM25 and recording the rank of the ground-truth document. Third, it aligns the model using either RSFT (fine-tuning on only the best-ranked expansions) or DPO (contrastive fine-tuning on best/worst pairs), or a combination of both. The aligned model is then used at inference time with single greedy decoding, eliminating the need for filtering. Training uses AdamW optimizer with learning rate 5e-5, batch size 16, and one epoch.

## Key Results
- AQE achieves up to 17.8% improvement in top-1 retrieval accuracy compared to baselines
- Computational memory reduced by 71.1% and time by 69.5% compared to filtering-based approaches
- RSFT+DPO combination provides the best results overall, outperforming individual alignment methods
- Strong generalization demonstrated across both in-domain (Natural Questions, TriviaQA) and out-of-domain (WebQA, Entity Questions) settings

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Effectiveness-Grounded Alignment
Fine-tuning LLMs with retrieval feedback directly improves expansion quality for downstream retrieval tasks. AQE uses BM25 rank of the true document as a proxy for expansion quality, creating preference pairs from best/worst expansions for alignment. This assumes BM25 rank is a valid signal for expansion quality.

### Mechanism 2: Direct Preference Optimization for Query Expansion
DPO improves expansion quality by contrastively increasing likelihood of high-performing expansions while decreasing likelihood of low-performing ones. For each query, DPO constructs pairs (e_best, e_worst) based on retrieval rank, encouraging higher probability for e_best relative to e_worst with KL-regularization to the reference policy.

### Mechanism 3: Rejection Sampling Fine-Tuning (RSFT) as High-Quality Data Curation
RSFT selects only e_best for each query and fine-tunes the LLM via maximum likelihood on this curated set, teaching the model what "good" expansions look like. This assumes best-ranked expansions are sufficiently diverse and representative of optimal expansion behavior.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed: AQE's core alignment technique; understanding the contrastive loss and implicit reward formulation is essential
  - Quick check: Given two responses y_w and y_l, how does DPO update the policy without explicitly training a reward model?

- **Concept**: Query Expansion in Information Retrieval
  - Why needed: The problem AQE solves; vocabulary mismatch is the motivating failure mode
  - Quick check: Why does adding synonyms/related terms to a query help BM25 but risk "over-expansion"?

- **Concept**: Sparse Retrieval (BM25)
  - Why needed: BM25 is the retrieval metric used to rank expansions during alignment
  - Quick check: If BM25 rank is used as the alignment signal, what failure modes could arise when the ground-truth document is not in the corpus?

## Architecture Onboarding

- **Component map**: LLM generation -> BM25 ranking -> RSFT/DPO alignment -> Greedy inference
- **Critical path**: Training data → zero-shot generation → BM25 ranking → (e_best, e_worst) extraction → RSFT and/or DPO fine-tuning → greedy inference
- **Design tradeoffs**: RSFT alone is simpler but may reduce diversity; DPO adds contrastive signal but requires careful β tuning. Using BM25 rank assumes ground-truth labels are correct; single-shot greedy inference is fast but removes filtering safety net
- **Failure signatures**: OOD generalization drops if training domain is narrow; expansion diversity collapse (cosine similarity → 1.0) causes overfitting; high β in DPO shows minimal improvement
- **First 3 experiments**: 1) Replicate Table 1 with only RSFT vs. only DPO vs. RSFT+DPO; 2) Ablate n (number of expansions) to test if 50 is necessary; 3) Evaluate OOD transfer (train on TriviaQA, test on EntityQuestions)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations
- Dependence on BM25 rank of ground-truth documents as sole alignment signal assumes perfect relevance labels and ignores cases where relevant documents are absent from corpus
- Computational efficiency comparisons only against filtering-based approaches, not other non-filtering baselines
- Focus exclusively on sparse retrieval (BM25) without testing dense retrieval compatibility or hybrid approaches

## Confidence
- **High confidence**: Computational efficiency improvements (directly measured memory and time reduction metrics)
- **Medium confidence**: Retrieval effectiveness improvements (significant gains shown but dependent on BM25 rank proxy validity)
- **Medium confidence**: Generalization across domains (OOD results show improvement but only tested on two out-of-domain datasets)

## Next Checks
1. **Label noise robustness test**: Systematically inject noise into relevance labels (10%, 20%, 30% incorrect rankings) and measure degradation in AQE performance versus baselines
2. **Dense retrieval compatibility**: Replace BM25 with a dense retriever (DPR or Contriever) in the alignment pipeline and measure whether effectiveness gains transfer
3. **Corpus completeness analysis**: Create controlled scenarios where ground-truth documents are partially or completely missing from the corpus, then measure how AQE alignment behaves when the proxy signal becomes unreliable