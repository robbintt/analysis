---
ver: rpa2
title: 'MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation'
arxiv_id: '2506.20737'
source_url: https://arxiv.org/abs/2506.20737
tags:
- information
- agents
- private
- privacy
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAGPIE, a novel benchmark for evaluating
  contextual privacy preservation in multi-agent LLM systems. Unlike prior benchmarks
  that focus on single-turn interactions, MAGPIE includes 158 real-world high-stakes
  scenarios across 15 domains where privacy-sensitive information is central to task
  completion.
---

# MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation

## Quick Facts
- arXiv ID: 2506.20737
- Source URL: https://arxiv.org/abs/2506.20737
- Authors: Gurusha Juneja; Alon Albalak; Wenyue Hua; William Yang Wang
- Reference count: 8
- Key outcome: Current SOTA LLMs misclassify private data as shareable in 25.2-43.6% of cases and leak private information in up to 59.9% of multi-turn conversations despite explicit privacy instructions.

## Executive Summary
This paper introduces MAGPIE, a novel benchmark for evaluating contextual privacy preservation in multi-agent LLM systems. Unlike prior benchmarks that focus on single-turn interactions, MAGPIE includes 158 real-world high-stakes scenarios across 15 domains where privacy-sensitive information is central to task completion. The evaluation reveals that current SOTA LLMs (GPT-4o, Claude-3.7-Sonnet, Llama-70B, Mistral-123B) misclassify private data as shareable in 25.2-43.6% of cases, leak private information in up to 59.9% of multi-turn conversations despite explicit privacy instructions, and fail to complete tasks in 71% of scenarios. The results demonstrate that current models lack robust contextual privacy understanding and cannot balance privacy preservation with collaborative task-solving.

## Method Summary
MAGPIE evaluates multi-agent LLM systems through three settings: single-turn classification of private vs public information, explicit/implicit instruction following under probing, and multi-turn passive/active collaboration. The benchmark uses 158 tasks across 16 domains, each defined as a tuple containing agents, task, deliverable, constraints, public/private information, penalties, and utilities. Multi-turn conversations run for up to 10 rounds or until consensus. Leakage is detected through exact-match of private information values in dialogue. The evaluation measures misclassification rate, leakage rate, consensus rate, and task success rate across GPT-4o, Claude-3.7-Sonnet, Llama-3-70B, and Mistral-Large-123B models.

## Key Results
- Current SOTA LLMs misclassify private data as shareable in 25.2-43.6% of cases (vs 10.2% human baseline)
- In multi-turn conversations, models disclose private information in 59.9% and 50.5% of cases despite explicit privacy instructions
- Multi-agent systems fail to complete tasks in 71% of scenarios, with task success negatively correlated with leakage
- Passive collaboration shows lower leakage (16-28.8%) than active collaboration (50.5-60.0%)

## Why This Works (Mechanism)

### Mechanism 1: Contextual Privacy Misclassification
- **Claim:** Current LLMs lack intrinsic understanding of context-dependent privacy norms, leading to systematic misclassification of sensitive information as shareable.
- **Mechanism:** Models trained on general corpora do not internalize domain-specific privacy boundaries (e.g., revealing team overtime constraints in GPU negotiations) because privacy norms are contextually situated, not universally defined. The misclassification rate (25.2-43.6% across models vs. 10.2% human baseline) indicates failure to distinguish information that is technically discussable from information that is strategically harmful to disclose.
- **Core assumption:** Privacy norms require reasoning about downstream consequences and stakeholder motives, not just surface-level sensitivity detection.
- **Evidence anchors:**
  - [abstract] "Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2% and 43.6% of the time."
  - [Section 5.1] Single-agent classification experiment showing misclassification rates across all tested models substantially exceeding human baseline.
  - [corpus] Related work (ConfAIde, PrivacyLens) confirms prior benchmarks focused on trivial single-turn scenarios where private information could be easily excluded.

### Mechanism 2: Instruction Adherence Degradation Over Multi-turn Interactions
- **Claim:** Privacy instructions that are followed in single-turn settings degrade significantly during multi-turn collaborative dialogue.
- **Mechanism:** In active collaboration settings, agents must respond to queries, maintain conversation coherence, and pursue task goals. The cognitive load of balancing these objectives causes privacy constraints to receive lower priority in generation. Leakage rates increase from 5.7-31.4% (explicit instruction, single-turn) to 50.5-59.9% (active collaboration, multi-turn), suggesting that sustained dialogue introduces opportunities for privacy drift.
- **Core assumption:** Models optimize locally per-turn rather than maintaining global privacy constraints across conversation history.
- **Evidence anchors:**
  - [abstract] "In multi-turn conversations, these models disclose private information in 59.9% and 50.5% of cases even under explicit privacy instructions."
  - [Section 5.3] Passive collaboration shows lower leakage (16-28.8%) than active collaboration (50.5-60.0%), indicating that active querying triggers more disclosure.
  - [corpus] Terrarium and GAMA papers address multi-agent safety but do not provide benchmark evaluations of privacy degradation over turns.

### Mechanism 3: Privacy-Utility Tradeoff Failure
- **Claim:** Current models cannot balance privacy preservation against task completion, evidenced by strong negative correlation between low leakage and task success.
- **Mechanism:** The benchmark scenarios are designed such that complete privacy preservation impedes task resolution, requiring strategic disclosure of low-penalty information to achieve high-utility outcomes. Models fail to navigate this tradeoff: tasks with ≤10% leakage achieve only 6.3% success, while maximum success (~67%) occurs at higher leakage levels. This suggests models either over-protect (failing tasks) or under-protect (leaking excessively) without finding optimal equilibrium.
- **Core assumption:** Strategic information sharing requires modeling others' knowledge states and reasoning about acceptable disclosure boundaries—capabilities not demonstrated by current models.
- **Evidence anchors:**
  - [abstract] "multi-agent systems fail to complete tasks in 71% of scenarios"
  - [Section 5.4] Figure 7 shows negative correlation between leakage and success; 51% consensus rate and 29.7% success rate averaged across models.
  - [Section 3.1] Dataset formulation includes granular penalties (ρ) and utilities (U) specifically to enable reinforcement learning on privacy-utility tradeoffs.

## Foundational Learning

- **Concept: Contextual Integrity Theory (Nissenbaum)**
  - **Why needed here:** The paper operationalizes this theory—privacy norms depend on context (healthcare vs. corporate), sender-receiver relationships, and information type. Understanding this frames why "project deadline" is private in cross-team negotiations but shareable within-team.
  - **Quick check question:** Can you explain why sharing patient data with a specialist is appropriate but sharing the same data with an insurer for marketing purposes violates contextual integrity?

- **Concept: Theory of Mind (ToM) in Multi-agent Settings**
  - **Why needed here:** Privacy preservation requires modeling what other agents know, want, and might do with disclosed information. The paper explicitly links ToM deficits to privacy failures (agents cannot infer how disclosures might be misused).
  - **Quick check question:** If Agent A discloses "we can complete with 55 GPUs with overtime," what must Agent A infer about Agent B's potential use of this information to classify it as a privacy violation?

- **Concept: Multi-turn Dialogue State Tracking**
  - **Why needed here:** Privacy violations often emerge incrementally across turns—each individual disclosure may be harmless, but collectively they expose private data. The penalty framework applies to information inferable from full conversation history, not single statements.
  - **Quick check question:** How would you detect that information disclosed across turns 3, 5, and 8 collectively reveals a private constraint that no single turn explicitly stated?

## Architecture Onboarding

- **Component map:**
  Task Generator -> Agent Profiles -> Conversation Manager -> Leakage Detector -> Success Evaluator

- **Critical path:**
  1. Load task tuple ⟨N, T, D, C, I, P, ρ, U⟩
  2. Initialize agents with system prompts (see Appendix A.4/A.5)
  3. Run multi-turn conversation (max 10 rounds or until consensus)
  4. Scan full conversation history for verbatim private information matches
  5. Evaluate deliverable against constraints using LLM-judge

- **Design tradeoffs:**
  - Leakage detection is conservative (exact match only)—inference-based detection would show higher leakage rates
  - 10-round limit may truncate complex negotiations; real-world scenarios might require more turns
  - Group chat setting (all agents see all messages) simplifies architecture but differs from bilateral negotiation

- **Failure signatures:**
  - High misclassification in single-turn classification task indicates fundamental privacy understanding gap
  - Leakage rate increases with conversation rounds (passive < active collaboration)
  - Consensus without success (agents agree on invalid solutions) indicates constraint reasoning failure
  - Models with better instruction following (GPT-4o) show lower explicit-setting leakage but still fail in multi-turn active collaboration

- **First 3 experiments:**
  1. **Baseline replication:** Run classification experiment on 25 sampled tasks with your target model to establish misclassification baseline. Compare to reported rates (GPT-4o: 25.2%, Claude: 43.6%).
  2. **Ablation on instruction explicitness:** Compare leakage rates between explicit instruction (private info labeled with penalties) vs. implicit instruction (generic "some info is private" warning) to measure instruction-following vs. privacy-reasoning contributions.
  3. **Turn-level leakage analysis:** Track leakage at each conversation turn (1-10) to identify when privacy degradation occurs and whether it correlates with specific interaction patterns (questioning, consensus-seeking, utility-optimizing statements).

## Open Questions the Paper Calls Out

- **Question:** Does enforcing strict privacy preservation inevitably compromise the agents' capacity to achieve their assigned task objectives?
  - **Basis in paper:** [explicit] Introduction explicitly lists this as a critical research question: "(3) Most importantly, does enforcing strict privacy preservation inevitably compromise the agents' capacity to achieve their assigned task objectives?"
  - **Why unresolved:** While the paper establishes a negative correlation between low leakage and task success in current SOTA models (Fig 7), it does not determine if this trade-off is fundamental to contextual privacy or merely a limitation of current model alignment.
  - **What evidence would resolve it:** The development of an agent architecture or alignment technique that achieves high task success rates (e.g., >90%) on MAGPIE while maintaining near-zero privacy leakage.

- **Question:** Can reinforcement learning (RL) training utilizing the specific penalty-reward system provided in MAGPIE align agents to better balance utility with data protection?
  - **Basis in paper:** [explicit] Conclusion suggests future work should focus on "Training agents using the penalty-reward system from our dataset to practice resisting privacy leaks in simulated scenarios, balancing task completion with data protection."
  - **Why unresolved:** The current study evaluates off-the-shelf SOTA models using prompting strategies; it does not assess whether models can learn adaptive privacy-preserving policies through the dataset's granular reward signals.
  - **What evidence would resolve it:** Empirical results showing that agents fine-tuned via RL on MAGPIE's utility-privacy trade-offs significantly outperform prompted baselines in both consensus rates and leakage reduction.

- **Question:** To what extent are agents vulnerable to implicit information leakage (logical inference of private data) versus the verbatim leakage evaluated in the paper?
  - **Basis in paper:** [inferred] Section 5.3 states that leakage is flagged only if the exact value appears verbatim, noting this is a "weaker condition than the being able to infer information from the conversation."
  - **Why unresolved:** The paper provides a lower bound on failure rates. It is unclear if current models are inadvertently revealing private parameters through combinations of harmless-looking statements that allow an adversary to infer the private data.
  - **What evidence would resolve it:** An evaluation employing an inference-detection mechanism (e.g., a judge LLM or human annotator) to detect private data reconstruction from non-verbatim multi-turn dialogue.

## Limitations

- The evaluation relies on exact-match leakage detection, which underestimates privacy violations from inference
- The 10-round conversation limit may truncate complex negotiations where privacy degradation occurs after the cutoff
- Human evaluation only addresses the single-turn classification task, not multi-turn collaborative performance where most privacy failures occur

## Confidence

- Single-turn classification results: **High** - Direct measurement with clear ground truth, consistent across models
- Multi-turn leakage rates: **Medium** - Exact-match detection underestimates true leakage; 10-round limit may truncate real-world scenarios
- Task success metrics: **Medium** - LLM-as-judge evaluation introduces judgment variability; 29.7% average success suggests either genuine model limitations or overly stringent constraint satisfaction

## Next Checks

1. Implement inference-based leakage detection using semantic similarity to test whether exact-match rates underestimate true privacy violations by 20-40%
2. Extend conversation length to 20 rounds on 25 tasks to determine if privacy degradation plateaus or continues increasing
3. Conduct human evaluation of 50 multi-turn conversations to assess LLM-as-judge reliability for consensus and task success determination