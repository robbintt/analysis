---
ver: rpa2
title: 'WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts'
arxiv_id: '2506.15594'
source_url: https://arxiv.org/abs/2506.15594
tags:
- charts
- tables
- chart
- table
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WikiMixQA is a multimodal benchmark for evaluating long-context
  document understanding. It contains 1,000 multiple-choice questions derived from
  4,000 Wikipedia pages, requiring models to reason over tables and charts.
---

# WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts

## Quick Facts
- arXiv ID: 2506.15594
- Source URL: https://arxiv.org/abs/2506.15594
- Reference count: 19
- Models achieve ~70% accuracy in oracle setting but drop to ~23-55% in wikidoc setting, with open-source models performing near random at ~27%

## Executive Summary
WikiMixQA is a multimodal benchmark containing 1,000 multiple-choice questions derived from 4,000 Wikipedia pages, requiring models to reason over tables and charts. The benchmark emphasizes complex reasoning by combining information across modalities—table-table, chart-chart, or table-chart pairs. We evaluate 12 state-of-the-art vision-language models under three settings: blind (no context), oracle (exact relevant data provided), and wikidoc (full Wikipedia page snapshots). Results show that proprietary models achieve ~70% accuracy in the oracle setting but drop significantly when retrieval from long documents is required, with only GPT-4-o exceeding 50% accuracy. Open-source models perform considerably worse, with a maximum accuracy of 27%. These findings highlight the challenges of long-context multimodal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.

## Method Summary
The benchmark construction pipeline involves collecting Wikipedia documents with tables and charts, generating textual descriptions for both modalities, using cross-encoder similarity scoring to pair semantically related modalities, generating questions via GPT-4-turbo, validating with automated models and human annotators, and evaluating 12 VLLMs across three settings (blind, oracle, wikidoc). The wikidoc setting uses image snapshots of full Wikipedia pages to test long-context visual document understanding.

## Key Results
- Proprietary models achieve ~70% accuracy in oracle setting when provided with exact relevant context
- GPT-4-o exceeds 50% accuracy in wikidoc setting, while other proprietary models drop to 23-55%
- Open-source models perform near random (27%) even with oracle context, suggesting fundamental reasoning limitations
- Economy topic consistently shows lowest performance across all models due to complex visual interpretation requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically-constrained modality pairing creates questions that require genuine cross-modal synthesis rather than single-source lookup.
- Mechanism: The pipeline generates textual descriptions for tables (via Llama-3-8B-Instruct) and charts (via GPT-4-turbo), then computes pairwise similarity using the BAAI/bge-reranker-v2-m3 cross-encoder. Only pairs scoring between macro-mean and 0.9 similarity are retained, filtering both irrelevant pairs and near-duplicates that would yield trivial questions.
- Core assumption: Cross-encoder similarity scores correlate with the potential for meaningful reasoning questions that span both modalities.
- Evidence anchors:
  - [Section 2.2]: "We used the cross-encoder model BAAI/bge-reranker-v2-m3... which directly outputs a similarity score for two inputs instead of generating embeddings."
  - [Section 2.3]: "We calculated the macro mean similarity score for each pair type across topics and retained pairs with scores between the macro mean and 0.9."
  - [Corpus]: VDocRAG and DMAP papers similarly emphasize structural/semantic document understanding but use different retrieval approaches; cross-encoder filtering appears unique to this benchmark construction.
- Break condition: If questions can be answered from one modality alone (human annotators flagged this in validity checks), the pairing mechanism failed to enforce true cross-modal dependency.

### Mechanism 2
- Claim: Multi-stage validation (automated + human) reduces invalid questions while maintaining diversity through strategic sampling.
- Mechanism: After GPT-4-turbo generates candidates, InternVL2-Llama3-76B validates whether sufficient information exists and whether the proposed answer is correct. Human annotators then verify validity and answer correctness. Crucially, 1,063 pairs from "rejected" pool were randomly sampled for annotation, recovering 405 additional valid questions beyond the 595 "correct" labels.
- Core assumption: The InternVL2 filter catches clear failures but may reject valid challenging questions; random sampling from rejects recovers diversity.
- Evidence anchors:
  - [Section 2.4]: "This included 938 pairs positively evaluated by the InternVL2 model and 1,063 pairs randomly sampled from the remaining dataset."
  - [Section 2.5]: "Approximately 515 of the 1000 pairs were validated as Correct by our AI evaluator... underscoring the value of sampling from initially rejected questions."
  - [Corpus]: Weak comparative evidence—corpus papers don't detail multi-stage validation pipelines for benchmark construction.
- Break condition: If human accuracy on final dataset significantly exceeds model performance by >50% gap on same modalities, validation succeeded; if humans also struggle with validity, the filtering failed.

### Mechanism 3
- Claim: Three-tier evaluation (blind/oracle/wikidoc) isolates retrieval failures from reasoning failures in long-context multimodal settings.
- Mechanism: The blind setting tests world knowledge (should fail—questions require context). The oracle setting tests cross-modal reasoning with exact context provided. The wikidoc setting tests retrieval + reasoning over full Wikipedia pages. The performance drop from oracle (~70%) to wikidoc (~23-55%) reveals retrieval as the bottleneck for most models.
- Core assumption: Performance differences between settings can be attributed to the specific capability being tested rather than prompt format or context length artifacts.
- Evidence anchors:
  - [Section 3, Evaluation Setup]: Defines three settings explicitly.
  - [Table 1]: GPT-4o drops from 71.42% (oracle) to 55.24% (wikidoc); Gemini-2.0-pro drops from 69.53% to 23.47%.
  - [Corpus]: VDocRAG and MMTBENCH papers similarly isolate retrieval vs. reasoning but use different evaluation frameworks.
- Break condition: If models with strong oracle performance show near-random wikidoc performance (as with Gemini models), retrieval is the clear bottleneck. If oracle itself is near-random (as with open-source models), reasoning is the fundamental limitation.

## Foundational Learning

- Concept: Cross-encoder vs. bi-encoder retrieval architectures
  - Why needed here: The dataset construction uses cross-encoder (BAAI/bge-reranker-v2-m3) for modality pair selection, trading speed for accuracy. Understanding this distinction is critical for interpreting why certain modality pairs were selected and for potential pipeline modifications.
  - Quick check question: Given 10 tables and 5 charts in a document, how many pairwise similarity computations would a cross-encoder require? Would a bi-encoder be more efficient?

- Concept: Vision-language model context handling (image tokens vs. text tokens)
  - Why needed here: The wikidoc setting provides Wikipedia page snapshots as images split into 768-pixel segments. Models must process multiple images and locate relevant information—a fundamentally different challenge than text-only long-context.
  - Quick check question: Why does the paper exclude some open-source models from wikidoc evaluation due to "limited context length" when images, not text, are the primary input?

- Concept: Multi-hop reasoning over structured vs. unstructured modalities
  - Why needed here: Questions require synthesizing information from tables (structured HTML) and charts (visual representations). The reasoning patterns differ—tables support direct lookup and comparison; charts require visual decoding of trends, values, and relationships.
  - Quick check question: Table 3 shows Gemini-2.0-pro excels at "2 Tables" questions (77.43%) but struggles with "2 Charts" (54.65%). What does this suggest about model capabilities vs. training data?

## Architecture Onboarding

- Component map:
  - Document Collection: WTabHTML Wikipedia dump → filter (≥3 tables) → filter (≥1 chart via ViT/DINOv2) → categorize into 7 topics → 7,258 documents
  - Modality Selection: Llama-3-8B generates table descriptions + GPT-4-turbo generates chart descriptions → cross-encoder similarity scoring → filter pairs by score range
  - Question Generation: GPT-4-turbo generates 3 question types per pair (individual modalities + combined) → InternVL2-76B validates → human annotation (validity + correctness)
  - Evaluation Pipeline: Three settings (blind/oracle/wikidoc) → MCQ accuracy measurement across 12 VLLMs

- Critical path:
  1. Chart identification is the bottleneck—ViT classifier produces false positives, requiring GPT-3.5-turbo filename analysis as secondary filter.
  2. Human curation is rate-limiting—3 annotators, majority voting, but only 1,000 final questions from 3,528 candidates.
  3. Long-context evaluation (wikidoc) is computationally expensive—excluded open-source models due to context length limitations.

- Design tradeoffs:
  - Cross-encoder vs. bi-encoder: Chose accuracy over speed for pair selection (acceptable since documents have limited modalities).
  - Automated vs. human validation: InternVL2 pre-filtering reduces human workload but may reject valid challenging questions (mitigated by random sampling).
  - Image-based vs. text-based wikidoc: Chose image snapshots to test visual document understanding, but acknowledges limitation—missing textual representations may underestimate model capabilities.

- Failure signatures:
  - Open-source models at ~27% oracle accuracy (near-random 25% baseline) suggests fundamental cross-modal reasoning limitation, not just retrieval failure.
  - Gemini models' dramatic oracle-to-wikidoc drop (69% → 23%) indicates retrieval bottleneck despite strong reasoning.
  - Economy topic consistently lower across all models—likely due to bar/line charts requiring both visual interpretation and comparative analysis.

- First 3 experiments:
  1. **Baseline replication**: Run GPT-4o and one open-source model (e.g., InternVL2.5-78B) on oracle setting with same prompts to establish reproducibility of reported ~70% vs. ~27% gap.
  2. **Retrieval ablation**: For wikidoc setting, provide models with ground-truth page segment containing relevant modalities vs. full page. Quantify how much of the performance drop is retrieval vs. context-length handling.
  3. **Modality-specific error analysis**: On oracle setting, categorize errors by question type (table-table, chart-chart, table-chart) and error pattern (numerical extraction, comparison failure, synthesis failure). This identifies whether the 17% human-model gap is concentrated in specific reasoning patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does augmenting visual snapshots with underlying textual representations (HTML) significantly improve VLLM accuracy in long-context settings?
- **Basis in paper:** [explicit] The authors state a limitation is that evaluation relied on image-based inputs "without incorporating the textual representations of these pages."
- **Why unresolved:** The current study isolates visual processing; the potential synergy or conflict with text-stream inputs in this specific benchmark remains untested.
- **What evidence would resolve it:** A comparative evaluation on WikiMixQA using the raw HTML/text data versus the image-only snapshots.

### Open Question 2
- **Question:** What specific architectural constraints cause open-source models to perform near random levels (~27%) even when provided with direct relevant context (Oracle setting)?
- **Basis in paper:** [inferred] While closed-source models achieved ~70% in the Oracle setting, open-source models capped at 27%, suggesting fundamental reasoning or visual encoding failures distinct from the long-context retrieval issue.
- **Why unresolved:** The paper identifies the gap but does not ablate whether this failure stems from visual encoding of charts/tables or the reasoning layer itself.
- **What evidence would resolve it:** An error analysis of open-source model outputs specifically categorizing failures into visual extraction errors versus cross-modal synthesis errors.

### Open Question 3
- **Question:** To what extent does the semantic similarity filtering of table-chart pairs limit the ability of this benchmark to evaluate "multi-hop" reasoning across disparate information?
- **Basis in paper:** [explicit] The authors note the dataset "does not yet support more complex multi-hop reasoning" because questions rely on semantically similar pairs, and future work requires releasing the full dataset to address this.
- **Why unresolved:** The current pipeline enforces semantic similarity, potentially simplifying the retrieval/synthesis step compared to real-world documents with unrelated or conflicting modalities.
- **What evidence would resolve it:** Performance comparison of models on a modified dataset version containing randomly paired or semantically distant modalities.

## Limitations

- The cross-encoder similarity filtering mechanism may not perfectly capture genuine cross-modal reasoning requirements, as some questions may still be solvable from single modalities despite pairing.
- The image-based wikidoc evaluation introduces ambiguity about whether performance drops stem from retrieval failures or the computational burden of processing multiple image segments versus text.
- The benchmark currently lacks support for complex multi-hop reasoning across semantically distant modalities due to the enforced similarity filtering.

## Confidence

- **High confidence** in oracle setting results: These controlled conditions isolate reasoning capability and show consistent, interpretable gaps between proprietary (~70%) and open-source (~27%) models.
- **Medium confidence** in wikidoc setting results: The performance drop reveals real challenges, but the image-based format versus text-based retrieval creates ambiguity about whether retrieval or modality processing is the bottleneck.
- **Medium confidence** in the pairing mechanism: The cross-encoder filtering appears sound, but without explicit measurement of whether questions truly require both modalities, some may still be solvable from single sources.

## Next Checks

1. **Controlled retrieval ablation**: For 50 randomly selected wikidoc questions, provide models with either (a) ground-truth page segment containing relevant modalities or (b) full page snapshot. Measure the performance difference to quantify how much of the oracle-to-wikidoc drop stems from retrieval versus context-length handling.

2. **Cross-modal dependency validation**: Have human annotators independently rate 100 questions on a 3-point scale (1=can be answered from one modality, 2=benefits from both, 3=requires both). Compare these ratings with the question generation pipeline's intent to verify that the pairing mechanism successfully enforces genuine cross-modal reasoning.

3. **Text-based wikidoc replication**: Re-run wikidoc evaluation for GPT-4o using extracted text from Wikipedia pages (same content as images) rather than image snapshots. This isolates whether the performance drop is due to retrieval challenges versus the computational burden of processing multiple image segments in long-context.