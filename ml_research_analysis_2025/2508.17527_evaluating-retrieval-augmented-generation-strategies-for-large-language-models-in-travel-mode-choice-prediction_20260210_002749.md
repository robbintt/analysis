---
ver: rpa2
title: Evaluating Retrieval-Augmented Generation Strategies for Large Language Models
  in Travel Mode Choice Prediction
arxiv_id: '2508.17527'
source_url: https://arxiv.org/abs/2508.17527
tags:
- travel
- retrieval
- mode
- choice
- trip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the application of large language models\
  \ (LLMs) with retrieval-augmented generation (RAG) for travel mode choice prediction.\
  \ It compares four RAG strategies\u2014basic RAG, balanced retrieval, cross-encoder\
  \ re-ranking, and their combination\u2014across three LLM architectures (GPT-4o,\
  \ o4-mini, o3) using 2023 Puget Sound travel survey data."
---

# Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction

## Quick Facts
- arXiv ID: 2508.17527
- Source URL: https://arxiv.org/abs/2508.17527
- Authors: Yiming Xu; Junfeng Jiao
- Reference count: 9
- Primary result: GPT-4o with balanced retrieval and cross-encoder re-ranking achieved 80.8% accuracy, outperforming traditional baselines

## Executive Summary
This study evaluates how retrieval-augmented generation (RAG) can improve large language model (LLM) predictions for travel mode choice. Using 2023 Puget Sound travel survey data, the researchers tested four RAG strategies across three LLM architectures (GPT-4o, o4-mini, o3). Results show that RAG significantly improves predictive accuracy for most models, with GPT-4o achieving the highest accuracy of 80.8% when using balanced retrieval combined with cross-encoder re-ranking. The study reveals that model capability strongly influences RAG effectiveness—while o3 achieved strong zero-shot performance, basic RAG strategies slightly reduced its accuracy, indicating advanced models require highly optimized retrieval pipelines.

## Method Summary
The researchers applied RAG to travel mode choice prediction by converting survey data into natural language prompts and using vector similarity search to retrieve relevant examples from training data. They tested four RAG strategies: basic top-k retrieval, balanced retrieval (top-k per class), cross-encoder re-ranking (bi-encoder recall + cross-encoder refinement), and their combination. Three LLM architectures were evaluated: GPT-4o, o4-mini, and o3. The dataset included 2,847 Seattle-area trips from the 2023 Puget Sound Regional Household Travel Survey, split 80/20 for training and testing. Trip duration was excluded to prevent information leakage, and predictions were evaluated using accuracy, weighted F1, precision, and recall metrics.

## Key Results
- GPT-4o with balanced retrieval and cross-encoder re-ranking achieved highest accuracy of 80.8%
- RAG significantly improved GPT-4o and o4-mini performance over zero-shot baselines
- o3 achieved strong zero-shot performance (78.3%) but showed degradation with basic RAG strategies
- RAG effectiveness varied dramatically by model architecture, requiring strategy alignment with model capability
- All RAG strategies outperformed traditional statistical and machine learning baselines

## Why This Works (Mechanism)

### Mechanism 1: Empirical Grounding Through Retrieved Examples
RAG improves LLM predictions by grounding reasoning in contextually similar past trips from training data. When a query trip is embedded and matched against the vector store, the retriever surfaces k nearest neighbors (similar trips with known outcomes). These examples are concatenated into the prompt, providing the LLM with concrete precedents that constrain its prediction space and reduce hallucination. This grounding assumes semantic similarity in embedding space correlates with decision-relevance for mode choice.

### Mechanism 2: Class-Balanced Retrieval Counters Majority Bias
Partitioning retrieval by class label prevents over-representation of dominant modes in the prompt context. Instead of retrieving top-k globally (which over-represents "Drive" at 44.6% prevalence), balanced retrieval partitions the corpus by mode and retrieves kj examples per class. This ensures the LLM sees precedents from minority classes (Transit 13.3%, Bike 5.8%), reducing prediction bias toward the majority. The mechanism assumes the LLM's prediction distribution is influenced by the class distribution of in-context examples.

### Mechanism 3: Cross-Encoder Re-ranking Improves Relevance Precision
Two-stage retrieval (bi-encoder recall + cross-encoder ranking) surfaces more decision-relevant examples than similarity alone. The bi-encoder retrieves K'=20 candidates efficiently. The cross-encoder then jointly encodes (query, document) pairs, attending to both simultaneously to produce contextualized relevance scores. This deeper semantic alignment selects the top K=4 most relevant documents. The mechanism assumes cross-encoder joint attention captures query-document relevance signals that independent embeddings miss.

## Foundational Learning

- **Concept: Embedding-based similarity search**
  - **Why needed here:** The entire RAG framework depends on converting trip descriptions to vectors and retrieving neighbors via cosine similarity. Without understanding embedding spaces, you cannot diagnose why retrieval fails.
  - **Quick check question:** Given two trip descriptions with identical distance and demographics but different trip purposes, would you expect high or low cosine similarity? Why might this be problematic?

- **Concept: Cross-encoder vs. bi-encoder architectures**
  - **Why needed here:** The re-ranking strategy explicitly trades off bi-encoder speed for cross-encoder precision. Understanding this distinction is essential for debugging retrieval quality vs. latency.
  - **Quick check question:** A bi-encoder pre-computes document embeddings once; a cross-encoder recomputes for every query-document pair. What does this imply about scalability and when to use each?

- **Concept: Zero-shot reasoning in LLMs**
  - **Why needed here:** The o3 model achieved 78.3% accuracy without any retrieved examples, and basic RAG degraded its performance. Understanding zero-shot baselines is critical for determining whether RAG adds value.
  - **Quick check question:** If a model already achieves 85% zero-shot accuracy on a task, what must be true about retrieved examples for RAG to improve rather than harm performance?

## Architecture Onboarding

- **Component map:** Data serialization layer -> Embedding layer -> Vector store -> Retrieval module -> Generation module
- **Critical path:** Serialize test trip → embed query → retrieve candidates → construct prompt → LLM inference → parse mode prediction
- **Design tradeoffs:**
  - Basic vs. Balanced retrieval: Basic preserves semantic relevance but risks majority bias; Balanced ensures class diversity but may surface less-similar examples
  - Re-ranking vs. no re-ranking: Re-ranking improves precision but adds latency and cost; essential for high-reasoning models (o3), less critical for others
  - Model selection: o3 has highest zero-shot but requires optimized RAG; GPT-4o benefits most from augmentation; o4-mini offers cost efficiency with moderate gains
- **Failure signatures:**
  - RAG degrades zero-shot performance: Model has strong intrinsic reasoning; retrieved examples are low-relevance noise → switch to re-ranking or skip RAG
  - Precision high but recall low: Over-constrained retrieval (e.g., aggressive re-ranking) misses relevant examples → expand candidate pool K'
  - Majority class bias in predictions: Basic retrieval on imbalanced data → switch to balanced retrieval
  - Poor generalization to new regions: Training data distribution differs from target → use broader retrieval (Basic RAG) rather than refined strategies
- **First 3 experiments:**
  1. Establish zero-shot baseline for your chosen LLM: Prompt the model with serialized trip descriptions and no retrieved examples. Record accuracy. If baseline exceeds 75%, RAG must be high-quality to help.
  2. Compare Basic RAG vs. Balanced Retrieval: Using the same k=4, test whether balanced retrieval improves F1 on minority classes (Transit, Bike) without sacrificing overall accuracy. Check if majority class bias decreases.
  3. Add cross-encoder re-ranking: Retrieve K'=20 candidates with balanced retrieval, re-rank with cross-encoder, select top K=4. Compare accuracy, F1, and latency against Basic and Balanced-only. If accuracy increases for high-reasoning models but not others, confirm model-strategy alignment hypothesis.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the internal Chain-of-Thought (CoT) reasoning process of LLMs change when utilizing high-precision RAG versus basic retrieval? The authors state they "did not directly analyze the models' internal Chain-of-Thought processes" and suggest future research employ methods to analyze step-by-step reasoning.

- **Open Question 2:** Do the optimal strategies for aligning retrieval with model capability (e.g., advanced models requiring high-precision retrieval) generalize to diverse cultural and geographic travel contexts? The conclusion notes future research should "validate these findings across more diverse datasets" beyond the Puget Sound and US-based NHTS data used.

- **Open Question 3:** How do variations in chunking strategies, embedding models, or graph-based retrieval architectures impact the predictive accuracy of travel mode choice models? The authors acknowledge their "RAG pipelines were limited to a specific set of components" and that "other variations... could yield different results."

## Limitations

- Model-strategy dependence is not fully understood—why o3 degrades with basic RAG while GPT-4o improves remains unexplained beyond performance metrics
- Geographic homogeneity limits external validity—findings based solely on Seattle metropolitan area may not generalize to different urban contexts
- Feature exclusion (trip duration) creates potentially unrealistic evaluation scenario where key contextual information is withheld

## Confidence

- **High Confidence:** RAG improves LLM performance for GPT-4o and o4-mini when retrieval quality is optimized through balanced retrieval and cross-encoder re-ranking
- **Medium Confidence:** The mechanism that RAG grounds LLM reasoning through empirically similar examples is supported by design and results, but specific semantic relevance criteria remain underspecified
- **Low Confidence:** The hypothesis that o3's intrinsic reasoning capability makes it sensitive to retrieval noise is plausible but not rigorously tested—the study observes degradation but doesn't systematically vary retrieval quality

## Next Checks

1. **Cross-encoder sensitivity analysis:** Systematically vary the number of candidates (K') retrieved for cross-encoder re-ranking and measure accuracy/latency tradeoffs across all three LLM models to quantify the optimal balance point.

2. **Geographic generalization test:** Apply the best-performing RAG strategy (GPT-4o + balanced + re-ranking) to travel data from a different metropolitan region (e.g., Portland or Atlanta) and measure performance degradation to assess external validity.

3. **Feature ablation study:** Re-run experiments with trip duration included in the prompt (as a controlled variable) to quantify the information leakage concern and understand how much this feature contributes to baseline performance differences.