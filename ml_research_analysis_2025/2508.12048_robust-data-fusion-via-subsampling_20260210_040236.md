---
ver: rpa2
title: Robust Data Fusion via Subsampling
arxiv_id: '2508.12048'
source_url: https://arxiv.org/abs/2508.12048
tags:
- data
- have
- where
- transfer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of transfer learning when the
  target data is limited but the external data is large and contaminated with outliers.
  The authors propose a robust transfer learning framework that combines subsampling
  techniques with mean-shift models to handle outliers deviating from the underlying
  true model due to arbitrary mean shifts.
---

# Robust Data Fusion via Subsampling

## Quick Facts
- arXiv ID: 2508.12048
- Source URL: https://arxiv.org/abs/2508.12048
- Reference count: 10
- Primary result: A robust transfer learning framework combining subsampling with mean-shift models to handle outliers in large external datasets when target data is limited

## Executive Summary
This paper addresses the challenge of transfer learning when target data is scarce but external data is abundant yet contaminated with outliers. The authors propose a robust framework that leverages subsampling techniques to create more reliable estimators by carefully selecting source data points that minimize both bias and variance. The approach is particularly relevant for scenarios like airplane hard landing prediction, where rare airplane types can benefit from data fusion with more common types while maintaining robustness against contamination.

## Method Summary
The core method combines two subsampling strategies: leverage-based random sampling to minimize variances and target-guided selection to reduce biases by choosing source data points that share similar underlying models with the target data. The authors develop a mean-shift contamination model where outliers deviate from the true model by arbitrary mean shifts. They provide non-asymptotic error bounds demonstrating that their subsampled estimators can outperform those using full contaminated data, establishing theoretical guarantees under specific distributional assumptions about model error distributions.

## Key Results
- Theoretical error bounds show subsampled estimators can outperform full-data estimators under contamination
- Extensive simulations demonstrate superior performance across various contamination scenarios
- Real-world application to airplane hard landing prediction shows improved estimation efficiency for rare airplane types

## Why This Works (Mechanism)
The method works by strategically selecting informative samples that balance bias-variance trade-offs in the presence of contamination. Leverage-based sampling identifies high-influence points that contribute most to estimation accuracy, while target-guided selection ensures selected source data shares similar characteristics with the target domain. This dual approach creates robust estimators that are less sensitive to outliers while maintaining the benefits of transfer learning from large external datasets.

## Foundational Learning

### Mean-shift contamination models
- **Why needed**: Provides a tractable mathematical framework for modeling outliers that deviate from true models by arbitrary mean shifts
- **Quick check**: Verify that contamination follows the assumed additive mean-shift structure in validation data

### Subsampling theory
- **Why needed**: Enables efficient data reduction while maintaining statistical guarantees for estimator quality
- **Quick check**: Confirm that subsampling rates achieve theoretical variance reduction bounds

### Transfer learning bias-variance trade-off
- **Why needed**: Critical for balancing domain adaptation benefits against contamination risks
- **Quick check**: Monitor bias and variance components separately during validation

## Architecture Onboarding

**Component map**: Target data → Target-guided selection → Subsampled target data; External data → Leverage-based sampling → Subsampled external data → Fusion → Final estimator

**Critical path**: Data preprocessing → Subsampling (both strategies) → Model estimation → Performance evaluation

**Design tradeoffs**: Computational efficiency vs. statistical accuracy; Subsampling rate vs. contamination robustness; Bias reduction vs. variance control

**Failure signatures**: High bias when target-guided selection is too restrictive; High variance when leverage sampling is insufficient; Degraded performance when contamination violates mean-shift assumptions

**3 first experiments**:
1. Compare subsampled vs. full-data estimators under controlled contamination levels
2. Evaluate sensitivity to sampling rate choices across different signal-to-noise ratios
3. Test performance when contamination follows alternative structures (e.g., cluster-based outliers)

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework primarily validated under mean-shift contamination assumptions
- Computational complexity not thoroughly analyzed for large-scale applications
- Limited evaluation of high-dimensional settings (d > 100)

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical error bounds | High |
| Practical effectiveness in simulations | Medium |
| Real-world applicability across domains | Medium |

## Next Checks
1. Evaluate the method's performance on high-dimensional datasets (d > 100) to test scalability beyond the theoretical assumptions
2. Test the robustness of the approach under various contamination models beyond mean-shift, including cluster-based and structured outliers
3. Implement a computational efficiency analysis comparing the proposed subsampling methods against standard robust estimation techniques on datasets with millions of samples