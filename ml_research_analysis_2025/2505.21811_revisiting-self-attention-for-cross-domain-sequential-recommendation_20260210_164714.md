---
ver: rpa2
title: Revisiting Self-attention for Cross-domain Sequential Recommendation
arxiv_id: '2505.21811'
source_url: https://arxiv.org/abs/2505.21811
tags:
- cross-domain
- autocdsr
- recommendation
- knowledge
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-domain sequential recommendation
  (CDSR), where user behavior sequences from multiple domains are used to predict
  future interactions. The core issue is negative transfer, where irrelevant or noisy
  cross-domain information degrades recommendation performance.
---

# Revisiting Self-attention for Cross-domain Sequential Recommendation

## Quick Facts
- **arXiv ID:** 2505.21811
- **Source URL:** https://arxiv.org/abs/2505.21811
- **Reference count:** 40
- **Primary result:** AutoCDSR improves Recall@10 by 9.8% and 16.0% and NDCG@10 by 12.0% and 16.7% for SASRec and BERT4Rec respectively

## Executive Summary
This paper addresses cross-domain sequential recommendation (CDSR), where user behavior sequences from multiple domains are used to predict future interactions. The core challenge is negative transfer, where irrelevant cross-domain information degrades performance. The proposed AutoCDSR method automates knowledge transfer by optimizing self-attention in transformer models through a multi-objective approach that balances recommendation accuracy with dynamic minimization of cross-domain attention scores. This ensures cross-domain interactions occur only when beneficial.

## Method Summary
AutoCDSR formulates CDSR as a multi-objective optimization problem that simultaneously optimizes recommendation performance while dynamically minimizing cross-domain attention scores. Instead of adding domain-specific components, it uses Multiple Gradient Descent Algorithm (MGDA) with Frank-Wolfe iterations to find optimal trade-offs between recommendation loss and cross-domain attention loss. AutoCDSR+ further enhances this with information bottleneck tokens that channel cross-domain communication through dedicated tokens, preventing direct item-level conflicts. The method introduces minimal computational overhead and enables simple models to perform comparably to state-of-the-art CDSR methods while being 4× faster.

## Key Results
- AutoCDSR significantly improves Recall@10 by 9.8% and 16.0% and NDCG@10 by 12.0% and 16.7% for SASRec and BERT4Rec respectively
- The method introduces minimal computational overhead and enables simple models to perform comparably to state-of-the-art CDSR methods while being 4× faster
- AutoCDSR+ provides additional performance gains with slight extra cost, though it is sensitive to domain label quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic minimization of cross-domain attention scores acts as an automated filter for negative transfer.
- **Mechanism:** The paper proposes that attention scores between items of different domains ($a_{cd}$) quantify knowledge transfer. By treating the minimization of these scores as an auxiliary objective, the model is forced to rely on single-domain information by default. Cross-domain attention is permitted only when it provides a gradient signal that reduces the primary recommendation loss ($L_{rec}$).
- **Core assumption:** The magnitude of cross-domain attention scores correlates directly with the volume of knowledge transfer; reducing these scores effectively filters "noisy" or "conflicting" information without losing beneficial cross-domain signals.
- **Evidence anchors:**
  - [abstract] "optimize the recommendation task while dynamically minimizing the cross-domain attention scores. This ensures cross-domain interactions occur only when beneficial."
  - [section 4.2] Figure 2 shows that negative transfer occurs when cross-domain scores are high but single-domain knowledge is sufficient; AutoCDSR reduces scores specifically in these cases (-17%).
- **Break condition:** If beneficial cross-domain knowledge requires *high* attention weights to be encoded effectively, this regularization might excessively dampen positive transfer.

### Mechanism 2
- **Claim:** Preference-aware Pareto optimization solves the "static weight" problem inherent in multi-task learning for CDSR.
- **Mechanism:** Instead of using a fixed hyperparameter ($\alpha$) to balance the recommendation loss and attention loss, the method uses a Multiple Gradient Descent Algorithm (MGDA). It computes gradients for both tasks and finds a descent direction that optimizes the primary task while satisfying the auxiliary task, effectively navigating the Pareto front dynamically per batch/sequence.
- **Core assumption:** The optimal trade-off between recommendation accuracy and cross-domain regularization varies significantly across different user sequences and cannot be captured by a global static weight.
- **Evidence anchors:**
  - [section 4.3] "manually tuning $\alpha$ usually does not lead to the optimal performance... calling for an automatic tuning of $\alpha$ in a more fine-grained manner."
  - [section 5.3] Figure 4 shows that performance is sensitive to the weight selection, and no single weight is optimal across domains, validating the need for dynamic optimization.
- **Break condition:** If the loss landscapes are highly conflicting or non-convex, the gradient descent might converge to a local minimum that sacrifices too much recommendation accuracy for the sake of regularization.

### Mechanism 3
- **Claim:** Information Bottleneck (IB) tokens structure knowledge transfer to prevent direct item-level conflicts.
- **Mechanism:** AutoCDSR+ introduces dedicated tokens that act as "hubs." Items within a domain attend to these IB tokens, and the tokens communicate across domains. This prevents direct attention between potentially conflicting items in different domains (e.g., a specific "Book" item attending to an irrelevant "Toy" item), forcing transfer to occur through a compressed, abstract representation.
- **Core assumption:** Compressing cross-domain communication through a small set of tokens (bottleneck) forces the model to distill only the most relevant transferable features, filtering out fine-grained noise.
- **Evidence anchors:**
  - [abstract] "AutoCDSR+ further enhances this with information bottleneck tokens that channel cross-domain communication through dedicated tokens."
  - [section 4.3.2] "This structured approach prevents direct information flow across domains, ensuring that knowledge transfer is both selective and efficient."
- **Break condition:** If the number of IB tokens is too low, the capacity for transfer may be insufficient; if domain labels are noisy, the rigid structure of IB tokens may propagate errors more aggressively than the soft regularization of AutoCDSR.

## Foundational Learning

- **Concept:** **Pareto Optimality in Multi-Task Learning (MTL)**
  - **Why needed here:** The core of AutoCDSR is not just adding a loss term, but dynamically solving a multi-objective problem. You must understand why "scalarization" (fixed weighted sum of losses) fails to solve the CDSR problem effectively across varied data distributions.
  - **Quick check question:** Why does finding a single static weight $\alpha$ to balance $L_{rec}$ and $L_{cd-attn}$ fail in this context?

- **Concept:** **Self-Attention Mechanics & Masking**
  - **Why needed here:** The method modifies the transformer's attention mechanism directly. Understanding how attention scores are calculated (Query/Key) and how they influence the final representation (Value) is required to grasp why minimizing the score ($a_{cd}$) effectively turns off information flow.
  - **Quick check question:** In the context of AutoCDSR, does minimizing the cross-domain attention score remove the interaction entirely, or just weigh it down?

- **Concept:** **Negative Transfer**
  - **Why needed here:** This is the primary failure mode the paper addresses. You need to understand that adding more data (cross-domain) can *decrease* performance if the domains are irrelevant or conflicting, which is counter-intuitive to standard "more data is better" thinking.
  - **Quick check question:** According to Table 1, what specific evidence suggests that self-attention can both help and hurt performance when trained on cross-domain sequences?

## Architecture Onboarding

- **Component map:** Standard Transformer Encoder -> Attention Matrix Extraction -> Cross-Domain Score Calculation -> Multi-Objective Loss Calculation -> Pareto Gradient Optimization -> Parameter Update

- **Critical path:**
  1.  **Forward Pass:** Calculate standard attention matrix $A$.
  2.  **Metric Extraction:** Calculate $a_{cd}$ (sum of attention weights where $domain(item_i) \neq domain(item_j)$).
  3.  **Loss Calc:** Compute $L_{rec}$ (Cross Entropy) and $L_{cd-attn} = a_{cd}$.
  4.  **Backward Pass (The "Magic"):** Instead of `loss.backward()`, intercept gradients $\nabla L_{rec}$ and $\nabla L_{cd-attn}$. Use the Frank-Wolfe iteration (Eq. 12) to find the optimal update direction that favors $L_{rec}$.

- **Design tradeoffs:**
  - **AutoCDSR vs. AutoCDSR+:** AutoCDSR is "plug-and-play" and robust to domain noise (soft constraint). AutoCDSR+ provides structured transfer and potentially higher performance (Table 2) but introduces sensitivity to domain label correctness (Section 5.5) and slight implementation complexity.
  - **Iterations ($K$):** The paper sets iterations to 100 for Pareto optimization. Lower $K$ increases speed but might yield sub-optimal gradient directions (Table 7).

- **Failure signatures:**
  - **Collapse to Single-Domain:** If the preference for $L_{rec}$ is too strong or the auxiliary task is too hard, the model might learn to set *all* cross-domain scores to zero, effectively ignoring cross-domain data.
  - **Domain Label Sensitivity (AutoCDSR+):** If domain labels are incorrect, AutoCDSR+ forces the model to communicate through wrong channels, degrading performance significantly compared to the soft AutoCDSR.

- **First 3 experiments:**
  1.  **Validation of Proxy:** Replicate Figure 2. Train a baseline BERT4Rec on cross-domain data and plot the attention scores for "Both Correct" vs "Single-Domain Correct / Cross-Domain Wrong". This confirms $a_{cd}$ is a valid proxy for negative transfer.
  2.  **Pareto vs. Static Weight:** Replicate Figure 4. Compare AutoCDSR (dynamic) against a grid search of fixed $\alpha$ weights to demonstrate that no single weight is optimal.
  3.  **Plug-and-Play Test:** Apply AutoCDSR to a minimal SASRec implementation on the provided dataset. Verify that convergence time is roughly $1.09\times$ the baseline (Table 4) and check if Recall@10 improves by the stated margins (approx 9-16%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the robustness of structured cross-domain transfer methods (like AutoCDSR+) be improved to handle noisy or inaccurate domain labels?
- **Basis in paper:** [explicit] Appendix A.5 explicitly states that AutoCDSR+ is sensitive to domain labeling noise, and Section 5.5 notes that its performance drops when domains are mislabeled.
- **Why unresolved:** The authors identify this sensitivity as a limitation but do not propose a mechanism to mitigate it, suggesting only to use AutoCDSR when domain information is unreliable.
- **What evidence would resolve it:** Demonstrating a modification to the Information Bottleneck token training process that maintains performance advantages even under synthetic domain label corruption.

### Open Question 2
- **Question:** Do advanced aggregation strategies for Information Bottleneck (IB) tokens provide significant gains over the simple element-wise summation used in AutoCDSR+?
- **Basis in paper:** [explicit] Section 4.3.2 and Footnote 6 state that while a simple element-wise summation is used to combine IB tokens, "Advanced combination strategies can be helpful but out-of-scope for this research."
- **Why unresolved:** It is unclear if the simple summation is a performance bottleneck that limits the complex knowledge exchange the authors aim to facilitate.
- **What evidence would resolve it:** A comparative analysis of AutoCDSR+ using attention-based or gating mechanisms for IB token combination versus the baseline summation.

### Open Question 3
- **Question:** Does the preference-aware Pareto optimization scale effectively to scenarios with a significantly larger number of distinct domains (e.g., >10)?
- **Basis in paper:** [inferred] The experiments are limited to a maximum of 5 domains (Amazon-Review), and the Pareto optimization involves iterative refinement (Frank-Wolfe) which may face convergence or efficiency challenges as the solution space expands.
- **Why unresolved:** The computational overhead and optimization stability of the preference-aware Pareto method have only been validated on low-domain-count datasets.
- **What evidence would resolve it:** Benchmarking AutoCDSR on synthetic or industrial datasets with high domain cardinality to measure convergence speed and recommendation accuracy.

## Limitations
- The information bottleneck approach in AutoCDSR+ introduces additional complexity and sensitivity to domain label quality, potentially limiting its robustness in real-world scenarios
- The scalability of the preference-aware Pareto optimization to scenarios with significantly larger numbers of distinct domains (>10) has not been validated
- The method's performance on very large-scale industrial datasets beyond the tested public benchmarks requires further verification

## Confidence

**High Confidence:** The core concept of using cross-domain attention scores as a proxy for knowledge transfer is well-supported by empirical evidence in Figure 2.

**Medium Confidence:** The preference-aware Pareto optimization framework shows promise in addressing the limitations of fixed-weight multi-task learning.

**Low Confidence:** The scalability claims regarding the 4× speedup compared to state-of-the-art CDSR methods require further validation on larger-scale industrial datasets.

## Next Checks

1. **Gradient Direction Validation:** Implement the Frank-Wolfe optimization and visualize the trajectory of the combined gradient direction over training iterations. Compare this against a baseline with fixed weight scalarization to confirm that the dynamic approach consistently finds better trade-offs.

2. **IB Token Sensitivity Analysis:** Systematically vary the number of information bottleneck tokens (e.g., 1, 2, 4, 8) and measure the impact on both performance and training stability. This will quantify the sensitivity to this architectural choice.

3. **Domain Label Robustness Test:** Intentionally corrupt a subset of domain labels (e.g., 5%, 10%, 20%) and measure the degradation in AutoCDSR+ performance compared to the soft AutoCDSR. This will quantify the method's robustness to label noise.