---
ver: rpa2
title: Arbitrary Entropy Policy Optimization Breaks The Exploration Bottleneck of
  Reinforcement Learning
arxiv_id: '2510.08141'
source_url: https://arxiv.org/abs/2510.08141
tags:
- entropy
- aepo
- policy
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Arbitrary Entropy Policy Optimization (AEPO),
  a novel reinforcement learning framework designed to address the entropy collapse
  problem in LLM reasoning. By reformulating entropy regularization as a policy-gradient
  optimization problem, AEPO uses temperature-adjusted REINFORCE regularization to
  achieve stable and controllable entropy regulation without introducing optimization
  bias.
---

# Arbitrary Entropy Policy Optimization Breaks The Exploration Bottleneck of Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.08141
- Source URL: https://arxiv.org/abs/2510.08141
- Reference count: 23
- Primary result: AEPO consistently outperforms GRPO and entropy-regularized baselines on mathematical reasoning benchmarks, achieving average score improvements of 1.87-2.27 points and even surpassing base models on pass@1024

## Executive Summary
AEPO addresses the entropy collapse problem in LLM reasoning by reformulating entropy regularization as a policy-gradient optimization problem. It uses temperature-adjusted REINFORCE regularization to achieve stable and controllable entropy regulation without introducing optimization bias. This enables effective exploration beyond the base model's knowledge frontier, with experimental results showing AEPO outperforms GRPO and entropy-regularized baselines on mathematical reasoning benchmarks.

## Method Summary
AEPO adds REINFORCE regularization on temperature-adjusted samples to the standard GRPO objective. It monitors policy entropy at each step and samples from either a higher-temperature distribution (T_high=1.2) or lower-temperature distribution (T_low=0.8) based on whether entropy falls below or exceeds a target level. The REINFORCE term uses binary rewards to filter out negative samples, forming a unidirectional gradient from positive samples. This approach maintains entropy around a target value while avoiding the optimization bias of traditional entropy bonuses.

## Key Results
- AEPO achieves average score improvements of 1.87-2.27 points across different model sizes compared to GRPO and entropy-regularized baselines
- Notably surpasses base model on pass@1024, providing direct evidence that RL can expand reasoning capabilities rather than merely sharpening existing knowledge
- Ablation studies confirm both temperature adjustment and REINFORCE-based regularization are necessary for effective entropy control and performance gains
- Demonstrates non-monotonic relationship between entropy and performance, with moderate entropy levels yielding optimal reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
AEPO reformulates entropy regularization as a policy-gradient optimization problem by adding a REINFORCE regularization term to the GRPO objective. This term samples from a temperature-adjusted distribution (π_old^T) rather than the current policy, using REINFORCE (raw reward signal) which filters out negative samples and forms a unidirectional gradient from positive samples. This works specifically for binary rewards (R=1 for correct, 0 otherwise) in RLVR settings.

### Mechanism 2
Temperature-adjusted sampling provides a bidirectional lever to steer policy entropy towards a target level. AEPO monitors policy entropy at each step: if entropy falls below the target (H(π_old) < H̄), it samples from a higher-temperature distribution (T_high) to encourage exploration; if entropy is too high, it samples from a lower-temperature distribution (T_low) to promote stability. This creates a feedback loop that maintains entropy around the target.

### Mechanism 3
A controlled, moderate entropy level is intrinsically linked to better reasoning performance. By solving entropy collapse, AEPO enables more sustained and effective exploration of the reasoning space, allowing the model to discover and reinforce diverse, valid reasoning strategies. This leads to improved performance on metrics like pass@1 and pass@k, while excessive entropy can harm performance by dispersing optimization.

## Foundational Learning

- **Policy Gradient (REINFORCE) and GRPO**: AEPO modifies GRPO by adding REINFORCE regularization. GRPO uses group-relative advantages while REINFORCE uses raw rewards. Understanding this difference is essential to see what AEPO adds and why.

- **Entropy in Reinforcement Learning**: Entropy measures the randomness or diversity of the policy's actions. AEPO specifically addresses "entropy collapse," where entropy approaches zero, indicating the policy becomes deterministic and exploration ceases.

- **Temperature in Softmax Policies**: Temperature T controls the sharpness of the softmax distribution: T > 1 flattens the distribution (higher entropy) and T < 1 sharpens it (lower entropy). AEPO's core control mechanism is temperature-adjusted sampling.

## Architecture Onboarding

- **Component map**: Entropy monitor -> Temperature selector (T_low/T_high) -> Temperature-adjusted sampler -> REINFORCE loss computation -> GRPO loss combination -> Backpropagation

- **Critical path**: Monitor Entropy of old policy π_old → Select Temperature (T_high if entropy is low, T_low if high) → Sample from π_old at that Temperature → Compute REINFORCE loss (positive samples only) → Add to GRPO loss and perform backpropagation

- **Design tradeoffs**: Setting hyperparameters H̄ (target entropy), T_high, T_low, and α (regularization strength). Too low H̄ leads to collapse; too high H̄ causes instability. Computational overhead of additional sampling and loss calculation must be considered.

- **Failure signatures**:
  - Entropy Collapse/Explosion: Feedback loop broken or hyperparameters mismatched
  - No Performance Gain: Target entropy in suboptimal regime or task doesn't benefit from exploration
  - Unstable Training: α too high, causing regularization to dominate GRPO objective

- **First 3 experiments**:
  1. Entropy Ablation (H̄ Sweep): Run AEPO with range of target entropy values on MATH, plot pass@1 and pass@k against final entropy
  2. Component Ablation: Compare AEPO (full) vs AEPO without temperature adjustment vs AEPO without REINFORCE filtering
  3. Baselines Comparison: Compare AEPO against GRPO and entropy-regularized baseline on AIME, AMC, GSM8K benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Is entropy a sufficient proxy for exploration, or does AEPO's success stem from alternative mechanisms beyond pure exploration? The paper states it remains unclear whether entropy is a sufficient proxy for exploration and whether exploration itself consistently improves training outcomes.

### Open Question 2
What theoretical principles determine the optimal entropy level for a given task, given the observed non-monotonic entropy-performance relationship? The paper empirically observes the inverted-U relationship but provides no theory for predicting optimal H values or explaining why different benchmarks respond differently.

### Open Question 3
Can AEPO's target-distribution framework successfully control non-entropy behaviors such as response length or reasoning conciseness? The paper proposes this generalization but validates only entropy control on mathematical reasoning tasks.

### Open Question 4
Does AEPO scale effectively to substantially larger models (70B+ parameters) and maintain its entropy-control properties? Experiments are limited to Qwen2.5-7B, Qwen2.5-Math-7B, and Qwen3-4B models with no larger-scale validation provided.

## Limitations
- The regularization coefficient α is not specified in the paper, making reproduction essentially impossible without additional tuning
- The relationship between temperature and entropy is assumed monotonic but not empirically validated across different model sizes and datasets
- Binary reward assumption is fundamental to AEPO's mechanism but may not generalize to more complex reward structures

## Confidence

**High Confidence**: The empirical demonstration that AEPO improves pass@1 and pass@k scores over GRPO and entropy-regularized baselines on mathematical reasoning benchmarks. The ablation study results showing both temperature adjustment and REINFORCE regularization are necessary are also highly reliable.

**Medium Confidence**: The theoretical mechanism that temperature-adjusted sampling with REINFORCE regularization provides stable entropy control without optimization bias. While the math is sound, the practical implementation details (particularly α) are underspecified.

**Low Confidence**: The claim that AEPO demonstrates RL can expand reasoning capabilities beyond the base model's knowledge frontier. This conclusion relies heavily on pass@1024 results but the underlying assumption that this represents true capability expansion rather than memorization is not rigorously validated.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary α across several orders of magnitude to identify the optimal range and determine how sensitive AEPO's performance is to this critical parameter.

2. **Temperature-Entropy Relationship Validation**: Conduct controlled experiments varying temperature (T_low, T_high) while monitoring actual entropy changes to empirically verify the assumed monotonic relationship across different model sizes and reasoning tasks.

3. **Generalization Beyond Binary Rewards**: Test AEPO on tasks with non-binary or continuous rewards to evaluate whether the REINFORCE filtering mechanism breaks down and identify potential modifications needed for broader applicability.