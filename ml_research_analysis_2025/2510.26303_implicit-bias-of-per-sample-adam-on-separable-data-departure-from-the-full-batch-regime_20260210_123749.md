---
ver: rpa2
title: 'Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch
  Regime'
arxiv_id: '2510.26303'
source_url: https://arxiv.org/abs/2510.26303
tags:
- adam
- data
- bias
- max-margin
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how the mini-batch regime changes Adam's implicit\
  \ bias in linear classification on separable data. The key insight is that full-batch\
  \ Adam converges to the \u2113\u221E-max-margin solution, but mini-batch Adam (batch\
  \ size 1) can converge to the \u21132-max-margin solution or other data-dependent\
  \ solutions depending on the dataset structure."
---

# Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime

## Quick Facts
- **arXiv ID**: 2510.26303
- **Source URL**: https://arxiv.org/abs/2510.26303
- **Reference count**: 40
- **Key outcome**: Full-batch Adam converges to ℓ∞-max-margin solution, but mini-batch Adam (batch size 1) can converge to ℓ2-max-margin solution or other data-dependent solutions depending on dataset structure.

## Executive Summary
This paper studies how the mini-batch regime fundamentally alters Adam's implicit bias in linear classification on separable data. The key finding is that while full-batch Adam maintains its ℓ∞-max-margin convergence (similar to SignGD), incremental Adam (batch size 1) exhibits dramatically different behavior. The authors develop a proxy algorithm to approximate incremental Adam in the high β₂ regime and characterize its limit direction through a dual fixed-point formulation. They show that Adam's coordinate adaptivity, which drives the ℓ∞-bias, can be eliminated on structured datasets like Generalized Rademacher data, causing convergence to ℓ2-max-margin instead. For general datasets, the limit direction depends on data geometry and can differ from both ℓ2 and ℓ∞ solutions.

## Method Summary
The paper analyzes linear binary classification on separable data using logistic/exponential loss. The primary methods include Det-Adam (full-batch), Inc-Adam (incremental/batch size 1), and a Proxy algorithm that approximates Inc-Adam when β₂→1. The authors generate synthetic datasets including Gaussian, Generalized Rademacher (GR), and shifted-diagonal data. They track cosine similarity of weight directions with ℓ2/ℓ∞-max-margin solutions and use CVXPY to compute reference margin solutions. The key theoretical contribution is characterizing Inc-Adam's limit direction via a dual fixed-point formulation that solves a parametric optimization problem.

## Key Results
- Full-batch Adam converges to ℓ∞-max-margin solution (similar to SignGD)
- Incremental Adam (batch size 1) on GR data provably converges to ℓ2-max-margin solution
- For general datasets in high β₂ regime, Inc-Adam's limit direction is characterized by a data-dependent dual fixed-point formulation
- Signum maintains ℓ∞-max-margin bias regardless of batch size when momentum β is close to 1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mini-batch Adam departs from SignGD-like dynamics, behaving as weighted normalized Gradient Descent.
- **Mechanism**: Full-batch Adam's denominator aligns with squared gradient, leaving only sign updates. In incremental regime, the denominator accumulates individual sample gradients cyclically, decoupling coordinate-wise scaling from current gradient direction.
- **Core assumption**: Dataset is linearly separable with specific learning rate decay schedules.
- **Evidence anchors**: Abstract shows ℓ2 vs ℓ∞ convergence differences; Section 2/Prop 2.5 shows Inc-Adam epoch updates approximate weighted sum of gradients; "Rich and Simple" paper supports optimizer divergence.
- **Break condition**: If batch size increases to N (full-batch), mechanism reverts to standard SignGD approximation.

### Mechanism 2
- **Claim**: On GR data, Inc-Adam converges to ℓ2-max-margin because coordinate adaptivity is nullified.
- **Mechanism**: GR data has constant |x_i[k]| across coordinates for each sample. This uniformity causes Adam's adaptive denominator to reduce to scalar scaling, making the optimizer behave like normalized GD.
- **Core assumption**: Data satisfies |x_i[k]| = |x_i[l]| for all coordinates.
- **Evidence anchors**: Abstract mentions GR data convergence; Section 3/Theorem 3.3 proves ℓ2 limit for GR; "Spectral Descent" paper provides context on geometric biases.
- **Break condition**: If data lacks per-sample magnitude symmetry (e.g., Gaussian), coordinate adaptivity persists.

### Mechanism 3
- **Claim**: For general datasets with high β₂, Inc-Adam's implicit bias is characterized by dual fixed-point formulation.
- **Mechanism**: As β₂→1, second moment history approximates uniform average. The limit direction solves parametric problem P_Adam(c) where c equals dual variables (fixed point T(c)=c).
- **Core assumption**: β₂ very close to 1 and limit directions exist.
- **Evidence anchors**: Abstract mentions proxy algorithm and dual formulation; Section 4/Theorem 4.8 establishes fixed point existence; Limited corpus evidence for this specific formulation.
- **Break condition**: Fixed-point approximation degrades if β₂ significantly less than 1.

## Foundational Learning

- **Concept**: Max-Margin Classifiers (ℓ2 vs ℓ∞)
  - **Why needed here**: Paper frames implicit bias around which margin optimizer maximizes - ℓ2 (Euclidean) vs ℓ∞ (Chebyshev) distance to decision boundary.
  - **Quick check question**: Does full-batch Adam favor direction maximizing minimum Euclidean distance (ℓ2) or minimum Chebyshev distance (ℓ∞)?

- **Concept**: Coordinate-wise Adaptivity
  - **Why needed here**: Adam's defining feature is per-coordinate division by root-mean-square of historical gradients. Paper investigates when this mechanism breaks down.
  - **Quick check question**: If all coordinates of gradient vector have same magnitude, does coordinate-wise scaling still change update direction?

- **Concept**: Incremental vs. Stochastic Gradient Descent
  - **Why needed here**: Paper analyzes "Incremental Adam" (cyclic processing, batch size 1) as tractable proxy for general stochastic mini-batch training.
  - **Quick check question**: In incremental setting (batch size 1), does optimizer see average gradient of dataset at every step?

## Architecture Onboarding

- **Component map**: Det-Adam -> Inc-Adam -> AdamProxy -> Fixed-Point Iterator
- **Critical path**: 1) Run Inc-Adam on separable data, 2) Approximate epoch updates using Proxy if β₂≈1, 3) Solve parametric problem P_Adam(c) and iterate until dual variables match parameters
- **Design tradeoffs**:
  - Analysis vs. Reality: Relies heavily on β₂→1 limit and batch size 1; practical Adam (β₂=0.999) approximates this well, but smaller β₂ or larger batches drift toward ℓ∞ regime
  - Signum (Alg 4): Guarantees ℓ∞ bias regardless of batch size if momentum β close to 1
- **Failure signatures**:
  - Drift to ℓ∞: If Inc-Adam converges to ℓ∞-margin when expecting ℓ2 on non-GR data, check batch size or β₂ values
  - Proxy Divergence: Fixed-Point Iterator fails to match empirical Inc-Adam if β₁≫0 not handled or β₂ not near 1
- **First 3 experiments**:
  1. GR Data Verification: Train Inc-Adam on GR data and plot cosine similarity with ℓ2 vs ℓ∞ solutions to verify ℓ2 convergence
  2. Gaussian Data Fixed-Point: Train Inc-Adam on Gaussian data and compare final direction against Fixed-Point Iterator output
  3. Signum Robustness: Train Signum with varying batch sizes (b=1, 5, N) to demonstrate consistent ℓ∞ solution regardless of batch size

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does implicit bias of mini-batch Adam vary with batch sizes between 1 and N, and can we characterize this transition theoretically?
- **Basis**: Appendix A.1 presents empirical evidence that increasing batch size pushes limit direction toward ℓ∞-max-margin, but no formal characterization
- **Why unresolved**: Paper's theoretical framework analyzes batch size 1 as extreme case; extending to intermediate sizes requires tracking multiple gradient accumulations
- **What evidence would resolve it**: Theoretical characterization of limit direction dependence on batch size b ∈ [1, N], validated by experiments across multiple batch sizes

### Open Question 2
- **Question**: Can fixed-point characterization be extended to finite β₂ values (e.g., β₂=0.9 or 0.95 commonly used in practice)?
- **Basis**: Appendix A.2 states "notable direction for future work is to generalize methodology for arbitrary β₂ values" and explains KKT-based approach fails for finite β₂
- **Why unresolved**: For finite β₂, denominator involves β₂^(i,j) weights creating coupled system not reducible to standard optimization KKT conditions
- **What evidence would resolve it**: Either generalized fixed-point formulation for finite β₂, or proof such characterization is fundamentally impossible

### Open Question 3
- **Question**: Does loss of ℓ∞-adaptivity in mini-batch Adam extend to neural networks, and does this explain why Adam's advantage over SGD diminishes with smaller batches?
- **Basis**: Section 7 states "important direction for future work is to investigate whether this loss of ℓ∞-adaptivity extends beyond linear models"
- **Why unresolved**: Paper's analysis restricted to linear classification on separable data; extending to neural networks requires different mathematical tools
- **What evidence would resolve it**: Experiments on deep networks showing batch-size dependence of Adam's implicit bias, ideally with theoretical analysis for simple neural network architectures

## Limitations
- Analysis relies heavily on proxy approximation and fixed-point formulation assuming β₂→1
- Focus restricted to separable data with exponential loss; behavior on non-separable data or other loss functions unexplored
- Theoretical tractability prioritized over direct empirical validation of all claims, particularly for general dataset case

## Confidence

- **High Confidence**: Full-batch Adam converges to ℓ∞-max-margin (Prop 2.4), Signum maintains ℓ∞-max-margin bias regardless of batch size (Thm 5.1)
- **Medium Confidence**: Inc-Adam converges to ℓ2-max-margin on GR data (Thm 3.3), fixed-point formulation characterizes general dataset behavior (Thm 4.8)
- **Medium Confidence**: Proxy algorithm accurately approximates Inc-Adam in β₂→1 regime (Prop 4.1)

## Next Checks
1. **Fixed-Point Validation**: Implement Algorithm 3 (Fixed-Point Iterator) for Gaussian data and compare predicted limit direction against empirical Inc-Adam results to verify dual formulation accuracy

2. **β₂ Sensitivity Analysis**: Run Inc-Adam on both GR and Gaussian data with varying β₂ values (0.9, 0.95, 0.99, 0.999) to empirically validate threshold where proxy approximation breaks down

3. **Batch Size Robustness**: Train Inc-Adam on non-GR data with batch sizes b=2, 5, 10 to quantify transition from ℓ2- to ℓ∞-max-margin solutions as batch size increases