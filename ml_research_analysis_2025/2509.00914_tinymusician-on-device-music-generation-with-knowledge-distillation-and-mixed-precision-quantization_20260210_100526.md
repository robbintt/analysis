---
ver: rpa2
title: 'TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed
  Precision Quantization'
arxiv_id: '2509.00914'
source_url: https://arxiv.org/abs/2509.00914
tags:
- music
- arxiv
- generation
- quantization
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TinyMusician, a lightweight music generation
  model distilled from MusicGen-Small, addressing the challenge of deploying large
  transformer-based models on edge devices with limited computational resources. The
  proposed framework integrates two key innovations: (1) Stage-mixed Bidirectional
  and Skewed KL-Divergence for knowledge distillation, which stabilizes training and
  improves generalization by dynamically balancing forward and backward KL-divergence
  terms during different training stages; and (2) Adaptive Mixed-Precision Quantization,
  which applies different quantization formats (Int8, Float16, Float32) to different
  model components to optimize both efficiency and audio fidelity.'
---

# TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization

## Quick Facts
- **arXiv ID**: 2509.00914
- **Source URL**: https://arxiv.org/abs/2509.00914
- **Authors**: Hainan Wang; Mehdi Hosseinzadeh; Reza Rawassizadeh
- **Reference count**: 13
- **Primary result**: First mobile-deployable music generation model achieving 93% of baseline performance with 55% size reduction

## Executive Summary
This paper introduces TinyMusician, a lightweight music generation model distilled from MusicGen-Small, addressing the challenge of deploying large transformer-based models on edge devices with limited computational resources. The proposed framework integrates two key innovations: (1) Stage-mixed Bidirectional and Skewed KL-Divergence for knowledge distillation, which stabilizes training and improves generalization by dynamically balancing forward and backward KL-divergence terms during different training stages; and (2) Adaptive Mixed-Precision Quantization, which applies different quantization formats (Int8, Float16, Float32) to different model components to optimize both efficiency and audio fidelity. Experimental results demonstrate that TinyMusician retains 93% of the MusicGen-Small performance while achieving 55% reduction in model size. When deployed on an iPhone 16 Pro, TinyMusician achieves strong text-audio alignment (CLAPscore of 0.373) and near-baseline fidelity (FADscore of 7.05), outperforming state-of-the-art models in the trade-off between compactness and perceptual quality.

## Method Summary
TinyMusician employs a two-stage optimization approach. First, stage-mixed bidirectional KL-divergence knowledge distillation transfers knowledge from the larger MusicGen-Small teacher to the smaller student model. This involves a time-dependent weight function α(t) that transitions from forward KL divergence (teacher→student) in early training to reverse KL divergence (student→teacher) after a threshold τstep, with a smoothing parameter λ to prevent abrupt transitions. Second, adaptive mixed-precision quantization assigns different bit-widths to different model components based on their sensitivity: the text encoder uses Int8 for efficiency, the MusicGen-decoder uses Float16 for autoregressive stability, and the Encodec-decoder uses Float32 for audio reconstruction fidelity. This component-adaptive approach preserves generation quality better than uniform quantization while achieving significant model size reduction.

## Key Results
- TinyMusician achieves 93% of MusicGen-Small performance with 55% reduction in model size
- Deployed on iPhone 16 Pro, TinyMusician achieves CLAPscore of 0.373 and FADscore of 7.05
- Mixed-precision quantization alone improves text-audio alignment by 16% (CLAP 0.303→0.352) while slightly degrading audio fidelity by 9.6% (FAD 6.49→7.11)
- Stage-mixed bidirectional KD outperforms unidirectional and static bidirectional methods in convergence stability and generalization

## Why This Works (Mechanism)

### Mechanism 1
Stage-mixed bidirectional KL divergence improves knowledge transfer fidelity over unidirectional or static bidirectional methods by using a time-dependent weight function α(t) that switches between forward KL (teacher→student) in early training and reverse KL (student→teacher) after threshold τstep. Mixed distributions Sλ and Tλ smooth optimization and prevent overfitting to teacher-specific patterns. This works because music generation benefits from learning global structural patterns first, then refining local temporal details (rhythm, melody).

### Mechanism 2
Component-adaptive mixed-precision quantization preserves generation quality better than uniform quantization by applying different bit-widths based on sensitivity: Text-Encoder→Int8 (efficiency), MusicGen-Decoder→Float16 (autoregressive stability), Encodec-Decoder→Float32 (audio reconstruction fidelity). This works because the audio decoder is most sensitive to quantization error while the text encoder is most robust.

### Mechanism 3
Quantization acts as implicit regularization, improving text-audio alignment (CLAP) but degrading audio fidelity (FAD) by constraining precision and reducing model capacity. This prevents overfitting to audio details while enforcing stronger conditioning on text prompts. This works because overfitting in full-precision models manifests as high-fidelity but weakly aligned outputs.

## Foundational Learning

- **Concept**: KL Divergence directions (forward vs reverse)
  - Why needed here: Bidirectional KL is core to the loss function; understanding DKL(P||Q) vs DKL(Q||P) is essential for debugging training dynamics.
  - Quick check question: Which direction penalizes missing modes in the teacher distribution?

- **Concept**: Post-training quantization (PTQ) vs quantization-aware training (QAT)
  - Why needed here: TinyMusician uses PTQ; knowing why avoids unnecessary retraining overhead.
  - Quick check question: Why might QAT outperform PTQ for autoregressive decoders?

- **Concept**: Autoregressive token generation with cross-attention conditioning
  - Why needed here: MusicGen-Decoder generates tokens sequentially conditioned on text embeddings; causal masking and CFG are critical for quality.
  - Quick check question: What happens if future tokens leak into the attention window during training?

## Architecture Onboarding

- **Component map**: Text Prompt → [T5 Text-Encoder (Int8)] → Text Embeddings → [MusicGen-Decoder (Float16)] → Audio Tokens → [Encodec-Decoder (Float32)] → Waveform Output

- **Critical path**: MusicGen-Decoder autoregressive loop dominates inference latency (26.54s total); cross-attention and CFG are called per-token

- **Design tradeoffs**: Smaller model size vs inference speed: Mixed-precision reduces size 55% but latency increases 2.6× vs baseline (26.54s vs 10s)—likely due to unoptimized quantized kernels; FAD vs CLAP: Quantization improves alignment but hurts fidelity; KD alone has opposite effect

- **Failure signatures**: Abrupt rhythm/melody shifts → decoder quantization too aggressive (try Float32 for decoder); Weak text adherence → check CFG scale or text-encoder precision; Training loss spikes after τstep → reduce λ or increase τstep

- **First 3 experiments**:
  1. Baseline validation: Deploy original MusicGen-Small (no KD, no quantization) and measure FAD, CLAP, latency, memory on your target device to confirm paper's baseline (FAD 6.49, CLAP 0.303)
  2. Ablation sweep: Test KD-only, quantization-only, and KD+quantization configurations independently to reproduce Table 3 tradeoffs and validate the regularization hypothesis
  3. Precision sensitivity: Systematically vary decoder precision (Float32→Float16→Int8) while holding other components constant to identify the cliff where FAD degrades >10%

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture constraints and component interdependence may not generalize across different teacher-student pairs
- Evaluation scope limited to iPhone 16 Pro, may not translate to other mobile platforms
- Data and generalization concerns due to limited training methodology details

## Confidence
- **High confidence**: Mixed-precision quantization reduces model size while maintaining reasonable quality; autoregressive decoding is primary inference bottleneck
- **Medium confidence**: Stage-mixed bidirectional KL divergence mechanism and convergence curves, but lacks ablation studies for different τstep values
- **Low confidence**: Implicit regularization hypothesis lacks theoretical justification and empirical validation

## Next Checks
1. **Component sensitivity ablation**: Systematically test each quantization precision independently while measuring FAD and CLAP to identify precise failure thresholds and validate sensitivity assumptions
2. **Cross-device performance verification**: Deploy TinyMusician on at least three different mobile platforms and measure qualitative differences in audio output quality
3. **Training dynamics stress test**: Reproduce stage-mixed KD training while varying τstep and λ to identify robustness boundaries and test generalization on held-out musical styles