---
ver: rpa2
title: 'ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?'
arxiv_id: '2509.19070'
source_url: https://arxiv.org/abs/2509.19070
tags:
- color
- vlms
- visual
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ColorBlindnessEval, a benchmark for evaluating
  Vision-Language Models (VLMs) on visually adversarial tasks inspired by the Ishihara
  color blindness test. The dataset contains 500 Ishihara-like images with numbers
  0-99 embedded in complex color patterns, designed to challenge VLM robustness in
  recognizing numerical information under adversarial visual conditions.
---

# ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?

## Quick Facts
- arXiv ID: 2509.19070
- Source URL: https://arxiv.org/abs/2509.19070
- Reference count: 16
- Models evaluated on adversarial color recognition tasks perform significantly worse than humans

## Executive Summary
This paper introduces ColorBlindnessEval, a benchmark for evaluating Vision-Language Models (VLMs) on visually adversarial tasks inspired by the Ishihara color blindness test. The dataset contains 500 Ishihara-like images with numbers 0-99 embedded in complex color patterns, designed to challenge VLM robustness in recognizing numerical information under adversarial visual conditions. Results show that while VLMs perform well on clear images, their accuracy drops significantly when processing adversarial backgrounds, highlighting prevalent hallucination issues. The study finds no strong correlation between model scale and performance, and VLMs perform better on color sets with high foreground-background contrast.

## Method Summary
The benchmark evaluates nine VLMs using 500 Ishihara-style images generated through a 3-stage pipeline: creating black-on-white reference images, Monte Carlo circle packing using modified IshiharaMC, and assigning foreground/background colors based on reference positions. Models are tested with Yes/No and open-ended prompts across four conditions (correct number asked, incorrect number asked, open identification, and clear foreground-only controls). Human participants complete a similar evaluation with 20 images for baseline comparison.

## Key Results
- VLMs show accuracy drops of ~40% on adversarial images compared to ~8% for humans
- GPT-4o-mini achieves the highest overall accuracy among evaluated models
- No strong correlation between model scale and performance on this task
- VLMs perform better on color sets with high foreground-background contrast (ColorSet1) and worst on low-contrast sets (ColorSet3)
- Few-shot in-context learning actually degrades performance rather than improving it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial color backgrounds disrupt VLM feature extraction for numerical patterns, causing hallucinations.
- Mechanism: VLMs process images through vision encoders that aggregate spatial-color features. When foreground and background colors share similar luminance or hue distributions (as in Ishihara plates), models fail to segment the figure-ground boundary, defaulting to pattern completion from parametric memory rather than grounded visual evidence.
- Core assumption: VLMs rely more on learned statistical correlations than on low-level spatial-color discrimination.
- Evidence anchors:
  - [abstract] "Our experiments reveal limitations in the models' ability to interpret numbers in adversarial contexts, highlighting prevalent hallucination issues."
  - [section 4.2] "Humans showed a slight performance drop of approximately 8%, while VLM performance decreased substantially."
  - [corpus] Hydra paper corroborates adversarial robustness and hallucination as linked failure modes in VLMs.

### Mechanism 2
- Claim: High foreground-background luminance contrast improves VLM recognition accuracy.
- Mechanism: When foreground and background colors occupy distinct regions in color space (particularly luminance channel), vision encoders can more readily form separable feature representations, enabling the language decoder to correctly bind visual tokens to numerical concepts.
- Core assumption: VLM vision encoders retain sufficient low-level color-luminance sensitivity when contrast is sufficiently large.
- Evidence anchors:
  - [section 4.4] "VLMs tend to perform better on ColorSet1 and worst on ColorSet3. This suggests that the models are more effective at handling samples with large foreground-background contrasts."
  - [section D.1] ColorSet1 uses high-contrast pairs: Background (106,124,115) vs Foreground (245,97,60).
  - [corpus] ColorBench paper similarly identifies color perception robustness as underexplored in VLMs.

### Mechanism 3
- Claim: Few-shot in-context learning degrades rather than improves performance on this task.
- Mechanism: Adding exemplar images to the prompt increases token context length and introduces competing visual patterns, which may cause attention dilution or interference with the target image's feature binding, leading to more conservative or confused responses.
- Core assumption: VLM attention mechanisms do not efficiently isolate relevant visual features when multiple similar images compete for attention weights.
- Evidence anchors:
  - [section B.2] "Few-shot learning did not lead to performance improvements; in fact, it had a negative impact across various models and prompting methods compared to the zero-shot setting."
  - [corpus] No direct corpus corroboration found for few-shot degradation on adversarial color tasks—this finding is paper-specific.

## Foundational Learning

- Concept: **Figure-ground segmentation in computer vision**
  - Why needed here: Ishihara plates are explicitly designed to challenge figure-ground separation via color confusion. Understanding this perceptual principle clarifies why VLMs fail where humans succeed.
  - Quick check question: Given an image with red-orange dots forming "42" on an orange-pink background, would a model without explicit segmentation be expected to reliably extract "42"? Why or why not?

- Concept: **Hallucination in multimodal models**
  - Why needed here: The paper frames accuracy drops under adversarial conditions as hallucination—outputs disconnected from visual input. This connects benchmark results to broader VLM reliability concerns.
  - Quick check question: If a VLM outputs "17" when viewing an Ishihara plate containing "74", is this necessarily a hallucination, or could it be a reasonable perceptual error? What additional evidence would distinguish these?

- Concept: **Contrast sensitivity and color space representation**
  - Why needed here: ColorSet performance variance suggests VLMs have uneven sensitivity across color dimensions. Understanding LAB/RGB representations helps diagnose which color axes are most problematic.
  - Quick check question: In RGB space, which pair has higher perceptual contrast: (100,100,100) vs (150,150,150), or (200,50,50) vs (50,200,200)? How might this affect Ishihara-style recognition?

## Architecture Onboarding

- Component map:
  Vision Encoder (CLIP ViT) -> Projection Layer -> Language Model Backbone

- Critical path:
  1. Image → Vision Encoder → Patch embeddings
  2. Patch embeddings → Projection → Visual tokens
  3. Visual tokens + Prompt tokens → LLM → Text output
  - Failure most likely at step 1-2 transition where figure-ground ambiguity is encoded into undifferentiated tokens.

- Design tradeoffs:
  - Higher-resolution encoders may improve fine-grained discrimination but increase compute cost.
  - Stronger visual-language alignment may reduce hallucinations but could overfit to common distributions.
  - Architectures with explicit segmentation heads (not in evaluated models) might improve robustness.

- Failure signatures:
  - **"Always No" bias**: Models like Claude3-Haiku (Y*/N=0.008, Y/N*=0.998) refuse to commit, suggesting feature extraction fails entirely.
  - **Contrast-dependent accuracy**: Sharp drops on ColorSet3 indicate luminance-hue confusion thresholds.
  - **Font sensitivity**: 27% accuracy drop (97.2%→70.4%) when switching Arial→DejaVuSans suggests spatial distribution of colored dots matters.

- First 3 experiments:
  1. **Ablate contrast levels systematically**: Generate Ishihara plates with controlled luminance deltas (ΔL=10,20,40,80 in LAB space) to map contrast-accuracy curves for each model.
  2. **Visualize attention maps**: Extract attention weights from vision encoder patches on adversarial vs clear images to confirm whether foreground regions receive suppressed attention.
  3. **Test intermediate representations**: Probe vision encoder outputs with linear classifiers for "contains digit X" to determine whether figure-ground information is present but lost during projection.

## Open Questions the Paper Calls Out
None

## Limitations
- Ishihara-like generation pipeline relies on modified Monte Carlo sampling without full hyperparameter specification, making exact replication challenging
- Few-shot learning results showing performance degradation lack external corroboration from the corpus
- Evaluation focuses exclusively on numerical recognition, limiting generalizability to broader visual reasoning tasks

## Confidence
- **High confidence**: VLM performance degradation on adversarial color backgrounds (supported by controlled human baseline and multiple model comparisons)
- **Medium confidence**: No correlation between model scale and performance (based on 9 models but limited size range in certain families)
- **Medium confidence**: Contrast-dependent accuracy patterns (observed across color sets but not systematically varied)
- **Low confidence**: Few-shot learning degradation effect (paper-specific finding without corpus support)

## Next Checks
1. **Systematic contrast ablation**: Generate Ishihara plates with controlled LAB-space luminance differences (ΔL=10,20,40,80) to map precise contrast-accuracy curves for each model and verify the ColorSet performance ordering is truly contrast-driven.

2. **Attention visualization**: Extract and visualize attention weights from vision encoder patches on adversarial vs clear images to confirm whether foreground regions receive suppressed attention when background colors are similar, providing mechanistic evidence for the figure-ground failure hypothesis.

3. **Intermediate representation probing**: Test vision encoder outputs with linear classifiers for "contains digit X" on both adversarial and clear images to determine whether figure-ground information is present in early representations but lost during projection to language space.