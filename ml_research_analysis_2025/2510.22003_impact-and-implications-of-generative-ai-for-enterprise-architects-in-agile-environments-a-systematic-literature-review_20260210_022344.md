---
ver: rpa2
title: 'Impact and Implications of Generative AI for Enterprise Architects in Agile
  Environments: A Systematic Literature Review'
arxiv_id: '2510.22003'
source_url: https://arxiv.org/abs/2510.22003
tags:
- genai
- agile
- https
- enterprise
- architects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review examined the impact of generative
  AI on enterprise architects in agile environments, analyzing 33 studies from 1,697
  records. The research found that GenAI consistently supports design ideation, rapid
  artifact creation, and architectural decision-making while introducing risks including
  opacity, bias, and privacy concerns.
---

# Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review

## Quick Facts
- arXiv ID: 2510.22003
- Source URL: https://arxiv.org/abs/2510.22003
- Reference count: 40
- Primary result: Systematic review of 33 studies showing GenAI accelerates EA artifact creation while requiring architects to shift from creators to curators, with critical governance needs

## Executive Summary
This systematic literature review examines how generative AI impacts enterprise architects working in agile environments, analyzing 33 studies from an initial pool of 1,697 records. The research reveals that GenAI consistently supports design ideation, rapid artifact creation, and architectural decision-making while introducing significant risks around opacity, bias, and privacy concerns. The findings indicate a fundamental shift in the architect's role from primary creator to curator and validator of AI-generated outputs, requiring new skills like prompt engineering and model evaluation. Success depends heavily on organizational readiness, governance maturity, and the development of dynamic frameworks to ensure responsible AI integration while maintaining architectural integrity.

## Method Summary
The study follows established systematic literature review protocols (Kitchenham and PRISMA) to examine GenAI's impact on enterprise architects in agile environments. The methodology involved defining five research questions, conducting database searches across IEEE Xplore and Scopus yielding 1,697 records, applying inclusion/exclusion criteria through multi-stage screening to select 33 studies, and performing thematic data extraction and synthesis. The review maps findings against technical characteristics, use cases, role impacts, adoption factors, and governance requirements, providing a comprehensive evidence-based analysis of current GenAI adoption patterns and challenges in enterprise architecture contexts.

## Key Results
- GenAI accelerates architectural documentation and code generation, reducing manual effort in early design phases
- Architects increasingly serve as curators and validators of AI outputs rather than primary creators of design artifacts
- Successful adoption requires dynamic governance frameworks and organizational readiness to manage risks including bias, opacity, and privacy concerns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GenAI compresses the cycle time for architectural ideation and artifact generation, contingent on output quality.
- **Mechanism:** By automating synthesis of code, models, and documentation, GenAI reduces manual effort in early design phases, enabling rapid exploration of multiple design paths and trade-offs rather than sequential development.
- **Core assumption:** Time saved in generation exceeds time required to fix contextually incorrect outputs or hallucinations.
- **Evidence anchors:**
  - Abstract: "GenAI most consistently supports (i) design ideation and trade-off exploration; (ii) rapid creation and refinement of artifacts."
  - Section: "LLMs can accelerate creation of architectural artifacts (documentation, diagrams, code), allowing enterprise and solution architects to complete tasks more quickly."
  - Corpus: Neighbors such as *A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension* support GenAI's capability to reshape engineering tasks.

### Mechanism 2
- **Claim:** GenAI drives a shift in professional identity from "architect-as-creator" to "architect-as-curator."
- **Mechanism:** As AI tools assume content generation roles (diagrams, code), human architects pivot to validating, selecting, and integrating these artifacts, moving value from production to high-level oversight and contextual alignment.
- **Core assumption:** Architects possess professional oversight and domain expertise to detect subtle errors (bias, logical flaws) that AI misses.
- **Evidence anchors:**
  - Abstract: "The review suggests a shift from 'architect-as-creator' to 'architect-as-curator,' with GenAI augmenting rather than replacing traditional competencies."
  - Section: "Architects increasingly operate as curators and validators of GenAI outputs, rather than primary creators of design artifacts."
  - Corpus: *Between Policy and Practice: GenAI Adoption in Agile Software Development Teams* reinforces contextual nature of adoption.

### Mechanism 3
- **Claim:** Adaptive governance functions as necessary control layer to mitigate "black box" risks of GenAI.
- **Mechanism:** Dynamic governance frameworks (policies, audit trails) ensure AI-generated architectures comply with privacy and regulatory standards before deployment, addressing explainability limitations.
- **Core assumption:** Governance mechanisms can evolve fast enough to keep pace with iterative nature of agile development and GenAI updates.
- **Evidence anchors:**
  - Abstract: "Organizational readiness and adaptive governance are critical for successful integration."
  - Section: "To support GenAI's potential while mitigating associated risks, organizations should adopt dynamic governance frameworks that evolve with model performance."
  - Corpus: *Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption* strongly supports EA governance and sustainable AI use.

## Foundational Learning

- **Concept: Prompt Engineering**
  - **Why needed here:** The review identifies this as an "emerging skill" required to effectively guide the model toward valid architectural outputs rather than generic or incorrect ones.
  - **Quick check question:** Can you structure a prompt that constrains the AI to generate a solution compatible with a specific technology stack (e.g., "Design a serverless function using AWS Lambda")?

- **Concept: Hallucination vs. Contextual Error**
  - **Why needed here:** The paper flags "contextually incorrect outputs" as major risk. Architects must distinguish between syntactically correct but architecturally unfeasible suggestions.
  - **Quick check question:** If an AI suggests a database schema that looks valid but violates your company's data residency requirements, is this a model failure or a governance failure?

- **Concept: Agile Release Trains (SAFe)**
  - **Why needed here:** The review is set in "large-scale agile environments." Understanding how architecture fits into "Agile Release Trains" is necessary to know when to inject GenAI support.
  - **Quick check question:** In an agile sprint, at what point does the "architect-as-curator" review the GenAI artifactsâ€”during the sprint demo or before the code commit?

## Architecture Onboarding

- **Component map:** Architect (Prompt Engineer) + Enterprise Context (Strategic Goals, Constraints) -> GenAI Model (LLM) + Knowledge Retrieval -> Dynamic Policy Engine + Audit Logs -> Architectural Artifacts (Code, Diagrams) + Validation Status

- **Critical path:**
  1. Define Scope: Identify specific architectural phase (Ideation vs. Artifact Creation)
  2. Construct Prompt: Embed constraints and context to guide the model
  3. Generate: AI produces initial drafts or trade-off options
  4. Curate/Validate: Architect reviews for bias, hallucinations, and compliance
  5. Govern: Log decision and output for traceability

- **Design tradeoffs:**
  - Speed vs. Trust: Accepting rapid "black box" suggestions vs. slower, validated outputs
  - Innovation vs. Standardization: GenAI may suggest novel patterns that violate established enterprise standards (TOGAF/Zachman)

- **Failure signatures:**
  - The "Black Box" Drift: Inability to explain why an architectural decision was made because AI generated it without logging rationale
  - Social Loafing: Team accepts AI code/architecture without review, leading to accumulated technical debt
  - Context Overflow: Model ignores critical enterprise constraints because input prompt exceeded context window

- **First 3 experiments:**
  1. Drafting Artifacts: Use GenAI to convert user stories into draft UML class diagram. Measure time saved versus manual creation and check for "contextually incorrect" relationships.
  2. Trade-off Analysis: Ask model to compare two architectural patterns (e.g., Microservices vs. Modular Monolith) for specific project constraint. Evaluate if AI missed any "quality attributes" (scalability, maintainability) human would catch.
  3. Compliance Check: Feed generated architectural snippet to governance checklist to see if it flags privacy risks model missed. This tests "Governance Maturity" factor.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the longitudinal impacts of GenAI adoption on architectural roles, inter-team collaboration, and overall enterprise agility?
- **Basis in paper:** [explicit] Authors explicitly state that "Future work should pursue empirical studies on the longitudinal effects of GenAI on architectural roles, inter-team collaboration, and enterprise agility."
- **Why unresolved:** Current body of literature is described as "nascent," "conceptual," and "exploratory," lacking long-term empirical data to establish causal claims or track evolution of these dynamics over time.
- **What evidence would resolve it:** Multi-site case studies, ethnographic fieldwork, or controlled experiments conducted over extended periods to observe how roles and collaboration patterns stabilize or shift post-adoption.

### Open Question 2
- **Question:** What concrete principles and patterns can effectively guide architects in responsible and efficient use of GenAI?
- **Basis in paper:** [explicit] Conclusion identifies specific research agenda item: "A concrete agenda is to develop principles and patterns that guide architects' effective and responsible use of GenAI."
- **Why unresolved:** While paper maps risks (e.g., opacity, social loafing) and enablers (e.g., governance maturity), it notes lack of standardized implementation frameworks or vetted design patterns for human-AI collaboration in this context.
- **What evidence would resolve it:** Design science research or action research that proposes, applies, and validates specific governance patterns and operational principles in real-world enterprise settings.

### Open Question 3
- **Question:** How does GenAI impact specific responsibilities and required competencies of distinct architectural sub-roles (e.g., solution vs. enterprise architect)?
- **Basis in paper:** [inferred] Authors note limitation that term "architects" was used broadly (enterprise, domain, solution, business, IT), which "may blur role-specific nuances," and that "transferability is uncertain."
- **Why unresolved:** SLR aggregates findings across diverse roles, making it unclear if shift to "architect-as-curator" applies uniformly or if technical vs. strategic roles diverge in their adoption needs and risks.
- **What evidence would resolve it:** Comparative empirical studies focusing on individual architect sub-roles to isolate specific skill shifts, productivity metrics, and risk profiles for each role type.

## Limitations
- Findings constrained by scope of published literature available as of early 2025, potentially missing emerging practices or gray literature
- Review methodology depends on quality and representativeness of 33 included studies, which may overrepresent positive use cases
- Analysis of governance frameworks based on theoretical models rather than empirical implementation data, creating uncertainty about real-world effectiveness

## Confidence
- **High Confidence:** Identification of skill shifts (architect-as-curator) and need for dynamic governance are well-supported across multiple studies; mechanisms describing time compression through automated artifact generation are consistently documented
- **Medium Confidence:** Specific impact on enterprise architecture documentation and decision-making processes relies heavily on extrapolation from software development studies rather than EA-specific research; proposed governance framework maturity model lacks empirical validation
- **Low Confidence:** Exact quantification of time savings and productivity gains remains speculative, as most studies report qualitative benefits rather than measured outcomes; long-term sustainability of AI-driven architectural decisions is not addressed

## Next Checks
1. **Implementation Audit:** Select three organizations with documented GenAI adoption in EA processes and measure actual time savings versus reported benefits, focusing on curation versus creation time ratio
2. **Governance Effectiveness Test:** Deploy proposed dynamic governance framework in controlled environment with pre- and post-implementation architectural compliance rates to validate effectiveness against static approaches
3. **Skill Gap Analysis:** Conduct competency assessments of practicing architects before and after GenAI integration to empirically verify emergence of prompt engineering and model evaluation as critical skills