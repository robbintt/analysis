---
ver: rpa2
title: Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition
  Learning
arxiv_id: '2503.01859'
source_url: https://arxiv.org/abs/2503.01859
tags:
- medical
- question
- documents
- learning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a Retrieval-Augmented Generation (RAG) pipeline\
  \ for generating educational content for Poland\u2019s State Specialization Examination\
  \ (PES) in medicine. The system integrates verified medical documents with LLM-generated\
  \ explanations, tailored for spaced repetition learning."
---

# Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning

## Quick Facts
- arXiv ID: 2503.01859
- Source URL: https://arxiv.org/abs/2503.01859
- Reference count: 8
- Primary result: RAG pipeline increases relevant documents retrieved from 4.59 to 6.83 out of 10 for medical exam explanations

## Executive Summary
This paper presents a Retrieval-Augmented Generation (RAG) pipeline for generating educational content for Poland's State Specialization Examination (PES) in medicine. The system integrates verified medical documents with LLM-generated explanations, tailored for spaced repetition learning. By employing a Query Rephraser, a refined retrieval system, and an advanced reranker, the pipeline significantly improved document relevance, increasing the number of relevant documents retrieved from 4.59 to 6.83 out of 10. Rigorous evaluation by medical annotators demonstrated high quality in document credibility, accuracy, logic, completeness, and readability. The system offers a scalable and cost-effective approach to producing high-quality, individualized medical learning resources.

## Method Summary
The pipeline processes medical exam questions through three key stages: (1) a Query Rephraser LLM converts multi-sentence exam questions into targeted search queries, (2) a refined retrieval system using Apache SOLR with synonym expansion retrieves top 200 candidates, and (3) a cross-encoder reranker evaluates full-paragraph context to select top 10 documents. GPT-4o then generates educational explanations using these documents. The system is designed for batch generation prioritizing accuracy over efficiency, with human annotator validation before deployment.

## Key Results
- Document relevance increased from 4.59 ± 3.18 to 6.83 ± 2.70 relevant documents per question (10-document set)
- Human annotators rated generated content highly across 7 quality dimensions (Credibility, Accuracy, Logic, Completeness, Conciseness, Readability, Prioritization)
- Optimal document count determined at 10 documents per query—fewer reduced performance, more offered no improvement
- System shows good generalization across medical specialties, though narrow specialties showed lower relevance scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query rephrasing improves retrieval relevance for multi-sentence exam questions by converting them into targeted search queries.
- Mechanism: An LLM-based Query Rephraser processes exam questions (which contain multiple sentences, answer choices, and topics) and generates concise, search-optimized queries that better match the retrieval system's keyword-based architecture.
- Core assumption: Short, targeted queries retrieve more relevant documents than raw multi-sentence exam questions.
- Evidence anchors:
  - [abstract] "By employing a Query Rephraser, a refined retrieval system, and an advanced reranker, our modified RAG solution promotes accuracy more than efficiency."
  - [Section 6.1] "Directly inputting the full exam question into the search engine resulted in suboptimal retrieval performance."
  - [corpus] LECTOR paper demonstrates LLM-enhanced concept extraction for spaced learning, supporting query optimization approaches in educational RAG.
- Break condition: If exam questions are already concise single-clause queries, rephrasing may not improve retrieval.

### Mechanism 2
- Claim: Expanding reranker scope and using full-paragraph context increases document relevance scores.
- Mechanism: The refined reranker considers up to 200 candidate documents (vs. production 100-200) and uses full paragraphs (~500 words) instead of snippets (~140 characters), combined with a more powerful cross-encoder model (polish-reranker-large-ranknet).
- Core assumption: More context and a stronger reranker yield better relevance judgments, computationally expensive but acceptable for offline batch generation.
- Evidence anchors:
  - [Section 6.2] "These measures significantly improved the relevance of the retrieved documents."
  - [Table 2] Total relevant docs increased from 4.59 ± 3.18 (Base RAG) to 6.83 ± 2.70 (+Refined Reranker).
  - [corpus] "Pursuing Best Industrial Practices for RAG in the Medical Domain" notes no consensus on best reranking practices—this paper provides empirical data for the accuracy-over-efficiency tradeoff.
- Break condition: If computational budget requires real-time retrieval, full-paragraph reranking becomes impractical.

### Mechanism 3
- Claim: Supplying exactly 10 retrieved documents to the LLM optimizes explanation quality without diminishing returns.
- Mechanism: The LLM (GPT-4o) generates explanations justifying correct answers using retrieved documents. Pilot studies found that fewer than 10 documents reduced performance, while more than 10 offered no substantial improvement.
- Core assumption: The optimal document count generalizes across medical specialties and question types.
- Evidence anchors:
  - [Section 6.3] "We observed that supplying exactly 10 documents yielded optimal results—fewer led to a significant performance drop, while more offered no substantial improvement."
  - [Section 7.1.2] Annotators classified documents as completely relevant, partially relevant, or irrelevant; total relevant docs correlated most strongly with overall commentary quality.
  - [corpus] Related medical RAG papers lack explicit document count optimization data; this provides a testable baseline.
- Break condition: If document corpus density varies significantly across domains (e.g., rare specialties with sparse sources), optimal count may differ.

## Foundational Learning

- Concept: Cross-encoder vs. Bi-encoder retrieval
  - Why needed here: The paper assumes readers understand why cross-encoders are slower but more accurate than bi-encoders for reranking.
  - Quick check question: Can you explain why a cross-encoder examines query-document pairs jointly while a bi-encoder embeds them separately?

- Concept: Spaced repetition algorithms (SM-2, SM-17)
  - Why needed here: The system integrates RAG outputs with SuperMemo's spaced repetition scheduling; understanding forgetting curves is essential for grasping the full pipeline value.
  - Quick check question: How does a spaced repetition algorithm decide when to schedule the next review of a flashcard?

- Concept: Hallucination in LLMs
  - Why needed here: The paper explicitly states RAG is used to minimize hallucinations by grounding outputs in verified medical sources.
  - Quick check question: What is an LLM hallucination, and why is it particularly problematic in medical education?

## Architecture Onboarding

- Component map: Query Rephraser -> SOLR retrieval -> Refined Reranker -> GPT-4o generator -> SuperMemo platform

- Critical path:
  1. Exam question + answers + correct answer → Query Rephraser
  2. Rephrased query → SOLR retrieval (top 200 candidates)
  3. Candidates → Refined Reranker (full paragraph context)
  4. Top 10 documents + question → GPT-4o prompt
  5. Generated comment + source links → SuperMemo course item

- Design tradeoffs:
  - Accuracy vs. efficiency: System prioritizes retrieval quality over speed (batch generation, not real-time)
  - Coverage vs. depth: 10 documents balance information completeness against LLM context limits
  - Automation vs. verification: Pipeline requires human annotator validation before deployment

- Failure signatures:
  - Low document relevance (< 5/10 relevant docs) → vague, over-generalized comments with irrelevant citations
  - Query rephraser over-simplification → loss of critical medical nuance
  - Narrow specialty questions → fewer relevant documents, lower credibility scores (observed in validation: 6.11 ± 2.91 relevant docs vs. 6.83 in development)

- First 3 experiments:
  1. A/B test retrieval quality: Run identical 40-question evaluation set through Base RAG vs. Base + Query Rephraser vs. Base + Query Rephraser + Refined Reranker; measure total relevant docs (/10) as primary metric.
  2. Document count sensitivity analysis: Test 5, 10, 15 documents per query; evaluate commentary completeness/depth and conciseness scores to validate the 10-document optimum.
  3. Cross-specialty generalization check: Evaluate commentary quality across narrow specialties (e.g., dermatology, nephrology) vs. core specialties (internal medicine, pediatrics); compare credibility and relevant doc counts to identify corpus coverage gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the pipeline be optimized to maintain high document relevance and commentary credibility in narrow medical specialties characterized by sparse source material?
- Basis in paper: [explicit] The authors observe that during validation, "The total number of relevant documents and credibility scores were noticeably lower... possibly due to limited relevant content in narrow medical fields."
- Why unresolved: The study identifies a performance drop in narrow fields but does not propose or test specific retrieval or generation adaptations (such as query expansion or fallback mechanisms) to mitigate the scarcity of verified documents.
- What evidence would resolve it: A follow-up evaluation showing that modified retrieval strategies (e.g., semantic search widening) or generation constraints yield comparable relevance scores between core and narrow specialties.

### Open Question 2
- Question: Can the evaluation guidelines be refined to improve the inter-annotator agreement (TIAA) for the "Credibility" metric?
- Basis in paper: [explicit] The validation results show a Total Inter-Annotator Agreement (TIAA) of only 38% for Credibility, leading the authors to conclude there is a "need for more precise evaluation guidelines."
- Why unresolved: The low agreement suggests that annotators struggle to consistently distinguish between model hallucination, internal knowledge, and correct source attribution under the current framework.
- What evidence would resolve it: A new annotation round using revised guidelines that demonstrates a statistically significant increase in TIAA for Credibility, bringing it closer to the levels seen in Logic or Readability.

### Open Question 3
- Question: Does the integration of RAG-based explanations into a spaced repetition system result in superior long-term knowledge retention compared to traditional static expert annotations?
- Basis in paper: [inferred] The paper states the system is designed to "enhance knowledge retention" and "minimize cognitive overload," and that it "will soon be deployed," but the presented evaluation relies on immediate output quality by medical annotators rather than longitudinal user learning data.
- Why unresolved: While the content quality is validated, the pedagogical efficacy of these specific RAG-generated comments in improving actual exam scores or retention curves remains unproven.
- What evidence would resolve it: A randomized controlled trial (A/B test) with medical students comparing retention rates and exam performance between groups using the RAG-based system versus those using standard question banks.

## Limitations

- Pipeline effectiveness depends critically on proprietary Medico PZWL knowledge base, which is not publicly available
- Study demonstrates success primarily in Polish medical education context, with generalizability to other languages and domains requiring validation
- Evaluation relies on relatively small sample (80 questions from 20 test papers) evaluated by medical annotators from a single institution

## Confidence

- High confidence: Query rephrasing improves retrieval relevance for multi-sentence exam questions
- Medium confidence: Refined reranker with full-paragraph context increases document relevance
- Medium confidence: 10-document optimum generalizes across medical specialties
- Low confidence: System scalability to other languages and medical domains

## Next Checks

1. Cross-specialty generalization test: Evaluate the pipeline across all medical specialties in the PES examination, comparing document relevance scores and commentary quality metrics between core specialties (high corpus density) and narrow specialties (low corpus density).

2. Prompt template replication study: Implement the query rephrasing and comment generation modules using publicly available medical question-answer pairs and medical literature repositories, measuring whether similar document relevance improvements can be achieved without the proprietary knowledge base.

3. Real-time vs. batch generation comparison: Test a modified pipeline that optimizes for efficiency (e.g., top 50 candidates, snippets instead of full paragraphs) against the accuracy-optimized version, measuring the tradeoff between document relevance scores and generation latency for potential deployment scenarios.