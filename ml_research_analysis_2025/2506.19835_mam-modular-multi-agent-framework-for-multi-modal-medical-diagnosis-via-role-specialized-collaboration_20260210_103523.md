---
ver: rpa2
title: 'MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized
  Collaboration'
arxiv_id: '2506.19835'
source_url: https://arxiv.org/abs/2506.19835
tags:
- medical
- answer
- llms
- specialist
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The MAM framework introduces a modular multi-agent system for multi-modal
  medical diagnosis, assigning specialized roles (General Practitioner, Specialist
  Team, Radiologist, Medical Assistant, Director) to LLM-based agents to emulate collaborative
  human medical teams. This design enables efficient knowledge updates, flexible integration
  of existing models, and improved diagnostic performance compared to unified multimodal
  medical LLMs.
---

# MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration

## Quick Facts
- arXiv ID: 2506.19835
- Source URL: https://arxiv.org/abs/2506.19835
- Reference count: 34
- Primary result: Modular multi-agent framework achieves 18%–365% accuracy gains across medical diagnosis tasks by assigning specialized roles to LLM agents

## Executive Summary
MAM introduces a modular multi-agent system for multi-modal medical diagnosis, where LLM-based agents assume specialized medical roles (General Practitioner, Specialist Team, Radiologist, Medical Assistant, Director) to emulate collaborative human medical teams. This design enables efficient knowledge updates, flexible integration of existing models, and improved diagnostic performance compared to unified multimodal medical LLMs. Extensive experiments on diverse medical datasets (text, image, audio, video) show MAM consistently outperforms modality-specific LLMs with performance gains ranging from 18% to 365%. The modular architecture allows granular knowledge updates without global retraining, and ablation studies confirm the value of each component.

## Method Summary
MAM is an inference-only multi-agent orchestration framework that decomposes medical diagnosis into specialized roles. The General Practitioner classifies cases and routes them to appropriate specialists, while the Specialist Team (3+ domain-specific agents), Radiologist (for imaging), and Medical Assistant (for retrieval) work in parallel. The Director synthesizes reports, manages voting loops (up to 3 rounds), and outputs final diagnoses. The framework uses no training, instead leveraging base LLMs per modality: Medichat-Llama3-8B (text), HuatuoGPT-Vision-7B (image), Qwen-Audio-Chat (audio), and VideoLLaMA2-7B (video). The Medical Assistant performs retrieval via Google API and summarizes findings, while the Director coordinates discussion and consensus voting among specialists.

## Key Results
- MAM achieves 18%–365% accuracy gains over unified multimodal medical LLMs across 10 diverse medical datasets
- Role-specialized prompting improves diagnostic accuracy by 5.0%–38.5% compared to direct prompting (e.g., PubMedQA: 48.5% → 87.0%)
- Iterative discussion with consensus voting yields gains of 5.0%–11.0% (e.g., PubMedQA: 69.5% → 77.0%)
- The modular architecture allows knowledge updates without retraining and supports flexible integration of existing models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Role-specialized prompting improves diagnostic accuracy by activating domain-relevant reasoning patterns in LLMs.
- **Mechanism:** When an LLM is assigned a specific medical role (e.g., "You are a pulmonologist"), the prompt conditions the model's output distribution toward specialist vocabulary, reasoning frameworks, and diagnostic heuristics associated with that role. This narrows the hypothesis space compared to undifferentiated prompting.
- **Core assumption:** LLMs encode sufficient domain knowledge; the bottleneck is retrieval and application, not knowledge absence.
- **Evidence anchors:**
  - [abstract] "MAM decomposes the medical diagnostic process into specialized roles... each embodied by an LLM-based agent"
  - [section] Table 1: "Assigned Roles" outperforms "Direct" by 5.0%–38.5% across all datasets (e.g., PubMedQA: 48.5 → 87.0)
  - [corpus] MedChat and KERAP confirm role-based prompting gains in medical tasks; corpus evidence is moderate but consistent.
- **Break condition:** If base LLM lacks sufficient medical pretraining, role prompting provides limited gains—garbage in, amplified garbage out.

### Mechanism 2
- **Claim:** Iterative discussion with consensus voting reduces diagnostic errors through perspective diversity and error-correction dynamics.
- **Mechanism:** Multiple specialist agents generate independent diagnostic opinions (O_si). The Director synthesizes these into a report, which specialists vote on. Disagreement triggers re-discussion. This creates a noise-averaging effect where individual agent errors are more likely to be caught and corrected through peer review.
- **Core assumption:** Agents have non-identical error patterns; a majority or consensus mechanism can surface the correct answer.
- **Evidence anchors:**
  - [section] Algorithm 1: "while No Consensus do... Director Dir synthesizes a report Rp... Specialist Team S reviews report Rp... votes vi ∈ {0, 1}"
  - [section] Table 7: Adding "+Discussion" yields gains (e.g., PubMedQA: 69.5 → 77.0, Brain Tumor: 92.4 → 97.0)
  - [corpus] MEDDxAgent and MedCoAct use similar multi-round collaboration; evidence supports consensus-based refinement.
- **Break condition:** If agents share systematic biases (e.g., all trained on same skewed data), discussion amplifies rather than corrects errors.

### Mechanism 3
- **Claim:** External retrieval grounds diagnoses in verifiable medical knowledge, reducing hallucination.
- **Mechanism:** The Medical Assistant decomposes the case into sub-problems, retrieves relevant documents via search API, and summarizes findings. This retrieved context (I_s) is injected into specialist reasoning, providing factual anchoring that constrains LLM generation to evidence-based conclusions.
- **Core assumption:** Retrieval system returns relevant documents; LLM can effectively integrate retrieved context.
- **Evidence anchors:**
  - [section] "The Medical Assistant agent plays a crucial role in information management... processing medical data to facilitate retrieval"
  - [section] Table 9: When retrieval contains the correct answer, "Answer Correct" rates are 46.2%–58.8%, substantially above non-retrieval baselines
  - [corpus] RAD framework explicitly targets retrieval-augmented clinical diagnosis; retrieval is a validated pattern.
- **Break condition:** Low retrieval recall (12.1%–34.0% in Table 9) means the mechanism fails when relevant documents are not retrieved.

## Foundational Learning

- **Concept: Role Prompting / Persona Conditioning**
  - **Why needed here:** MAM relies entirely on role-specific prompts to activate specialist behaviors. Without understanding how prompts shape LLM output distributions, you cannot debug or improve role design.
  - **Quick check question:** Can you explain why adding "You are a radiologist" to a prompt changes an LLM's token probabilities for medical terms?

- **Concept: Multi-Agent Orchestration Patterns (Debate, Voting, Consensus)**
  - **Why needed here:** The Director agent coordinates discussion and voting; understanding failure modes (deadlocks, cascading errors) is essential for tuning iteration limits and vote thresholds.
  - **Quick check question:** What happens to consensus time if all agents are highly correlated in their outputs?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The Medical Assistant implements a RAG pipeline. Retrieval quality directly caps diagnostic accuracy—you must understand recall vs. precision tradeoffs.
  - **Quick check question:** If retrieval recall is 30%, what is the maximum possible accuracy ceiling for the full system?

## Architecture Onboarding

- **Component map:** Multimodal input → GP (classification & referral) → Specialist Team + Radiologist (opinion generation) → Medical Assistant (retrieval & summarization) → Director (synthesis → report → voting) → if no consensus, loop to Specialist Team → Director outputs final diagnosis

- **Critical path:**
  1. Multimodal input → GP (classification & referral)
  2. Specialist Team + Radiologist (independent opinion generation)
  3. Medical Assistant (parallel retrieval & summarization)
  4. Director (synthesis → report → voting)
  5. If no consensus, loop to step 2 (max 3 rounds)
  6. Director outputs final diagnosis

- **Design tradeoffs:**
  - **Role count:** 3 roles optimal; 5+ roles degrade performance due to redundancy and coordination overhead (Figure 3).
  - **Discussion rounds:** 2–3 rounds beneficial; more rounds introduce noise or over-fitting (Figure 2).
  - **Retrieval vs. speed:** Retrieval improves accuracy but adds latency; skip for time-critical cases where baseline confidence is high.
  - **Model selection:** Framework supports swapping base LLMs; stronger base models amplify framework gains but increase cost.

- **Failure signatures:**
  - **Low consistency + low accuracy:** Baseline model weak; consider upgrading base LLM before tuning framework.
  - **High consensus rounds, low accuracy:** Agents correlated; diversify specialist prompts or use different base models.
  - **Retrieval present but no accuracy gain:** Summarization loses critical information; improve Medical Assistant prompt or chunking strategy.
  - **Consensus deadlock (max rounds hit):** Voting threshold too strict; relax to majority vote instead of unanimity.

- **First 3 experiments:**
  1. **Ablation by component:** Run Direct → +Roles → +Discussion → +Retrieval on a held-out subset; identify which component drives gains for your target modality.
  2. **Role count sweep:** Test 1, 3, 5 specialist roles on 3 datasets; verify inverted U-shape and find optimal granularity for your domain.
  3. **Retrieval quality audit:** Measure recall on 50 cases; if <25%, improve query formulation or switch retrieval corpus before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MAM framework perform when deployed in real-world clinical settings compared to standardized public datasets?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "the absence of real-world clinical validation" is a key constraint due to the "resource allocation and human expertise required."
- Why unresolved: Public datasets may not reflect the noise, complexity, or data variability found in actual hospital workflows and Electronic Health Records (EHRs).
- Evidence: Results from a prospective study or pilot program within a clinical environment, measuring diagnostic accuracy and latency against human expert benchmarks.

### Open Question 2
- Question: Can the retrieval recall be improved beyond the current 12%–34% range without introducing noise that confuses the diagnostic agents?
- Basis in paper: [explicit] Section 5.7 notes that retrieval recall "varies across datasets" and "significant improvement is needed," while the Conclusion lists "integrate advanced knowledge retrieval" as future work.
- Why unresolved: The current reliance on Google API results in low recall for specific datasets like NIH (12.1%), creating a knowledge bottleneck for the Medical Assistant.
- Evidence: Integration with a specialized medical knowledge base or enhanced query formulation that demonstrably increases the "Recall" metric in Table 9 while maintaining high "Answer Correct" rates.

### Open Question 3
- Question: How can the number of agent roles be dynamically optimized to prevent the performance degradation observed with excessive specialization?
- Basis in paper: [inferred] Section 5.6 reports an "inverted U-shape" in performance, where increasing the number of roles from 3 to 5 led to a decrease, likely due to redundancy or overhead.
- Why unresolved: The optimal number of agents likely varies based on case complexity, but the current framework appears to rely on a static configuration that balances general cases but may over-complicate simple ones.
- Evidence: An adaptive mechanism that adjusts the number of active specialist agents based on the General Practitioner's initial triage, showing consistent performance improvements across varied case difficulties.

## Limitations

- The framework's accuracy gains rely heavily on the quality of the base LLM and the effectiveness of retrieval; without strong medical pretraining, role prompting and discussion provide limited benefit.
- The modular design assumes that agents can work independently and that disagreement is productive—this may fail if agents share systematic biases or if the retrieval corpus is incomplete or noisy.
- The discussion mechanism, while promising, is not rigorously evaluated for pathological cases like persistent disagreement or collusion, and there is a risk of over-engineering.

## Confidence

- **High confidence**: The modular architecture enables knowledge updates without retraining, and ablation studies confirm the value of each component (roles, discussion, retrieval).
- **Medium confidence**: Performance gains (18%–365%) are robust across diverse modalities, but are sensitive to base model quality and retrieval effectiveness.
- **Low confidence**: The discussion mechanism's robustness to systematic bias or disagreement deadlocks is not thoroughly validated; scalability to rare diseases or ambiguous cases is uncertain.

## Next Checks

1. **Base Model Dependency**: Run the full pipeline with a weaker base LLM (e.g., general-purpose model) and compare accuracy to the specialized models used in the paper; assess whether gains persist without strong medical pretraining.
2. **Retrieval Quality Impact**: Measure retrieval recall on a held-out set and correlate with final diagnostic accuracy; test alternative retrieval APIs or corpora to quantify the ceiling effect.
3. **Discussion Robustness**: Introduce systematic bias into one agent and run the consensus loop; observe whether discussion amplifies or corrects the bias, and test the system's behavior under persistent disagreement or collusion.