---
ver: rpa2
title: Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice
  QA
arxiv_id: '2509.25941'
source_url: https://arxiv.org/abs/2509.25941
tags:
- reasoning
- cots
- answer
- correct
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to explicitly model the solvability of multiple-choice
  questions to improve chain-of-thought reasoning in large language models. The key
  insight is that unsolvable questions often produce spurious chains of thought, leading
  to false positives, and there is an intermediate regime where learning is most effective.
---

# Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA

## Quick Facts
- arXiv ID: 2509.25941
- Source URL: https://arxiv.org/abs/2509.25941
- Reference count: 40
- Primary result: Modeling question solvability improves process-correctness of chain-of-thought reasoning while reducing hallucinations

## Executive Summary
This paper addresses the problem of spurious chain-of-thought reasoning in LLMs for multiple-choice QA. The key insight is that unsolvable questions produce false-positive answer-correct CoTs, where the final answer happens to be correct but the reasoning process is flawed. To address this, the authors propose explicitly modeling the solvability of each question using a Beta posterior approach, then incorporating solvability estimates into both outcome-supervised reward models and reinforcement learning. Experiments show these modifications consistently yield higher rates of process-correct reasoning and, in RL, improved answer accuracy.

## Method Summary
The approach consists of three main components: (1) estimating question solvability by sampling G CoTs per question and computing the probability that true performance exceeds random guessing using a Beta posterior survival function; (2) modifying outcome-supervised reward models to down-weight loss from unsolvable questions by using solvability-weighted labels; and (3) adjusting reinforcement learning with group-relative advantage by multiplying the advantage by p_solvable. The solvability estimator uses G=32 samples for math datasets and G=8 for multimodal datasets. The ORM uses a 2-layer feed-forward network on last hidden states with BCE loss, while the RL implementation modifies DrGRPO advantage calculation.

## Key Results
- Solvability estimation using Beta posterior identifies questions likely to produce spurious reasoning chains
- Down-weighting loss from unsolvable questions reduces false-positive CoTs in outcome-supervised reward models
- Reinforcement learning with solvability-adjusted advantage focuses learning on the intermediate-difficulty regime with highest learning potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Solvability estimation using Beta posterior identifies questions likely to produce spurious reasoning chains
- Mechanism: Sample G CoTs per question, compute observed success rate μ_observed, then calculate p_solvable as the survival function P(μ_true > μ_random) of the Beta posterior Beta(1+Gμ, 1+G(1-μ)). This survival function transitions sharply from ~0 to ~1 as answer-correct samples increase, with inflection determined by number of choices |c|.
- Core assumption: CoT correctness follows a Bernoulli distribution per question; uniform Beta(1,1) prior is reasonable; random guessing baseline μ_random = 1/|c| accurately separates solvable from unsolvable.
- Evidence anchors:
  - [abstract] "By estimating the solvability of each question, we uncover an intermediate regime where learning is most effective."
  - [section 2, Eq. 5] "We compute the probability that a question is solvable for the model as the survival function of the Beta distribution."
- Break condition: If G is too small (<8), the Beta posterior is under-informed and p_solvable estimates become unreliable; if questions have ambiguous answer formats (non-MCQA), μ_random cannot be cleanly defined.

### Mechanism 2
- Claim: Down-weighting loss from unsolvable questions reduces false-positive CoTs in outcome-supervised reward models
- Mechanism: Replace binary label z_ij ∈ {0,1} with solvability-weighted label z_ij = p_solvable(q_i) if answer is correct, else 0. This reduces gradient contribution from likely spurious answer-correct CoTs, training the ORM to prefer process-correct candidates.
- Core assumption: Answer-correct CoTs from low-solvability questions are systematically process-incorrect; the reward model can learn to distinguish reasoning quality from outcome alone given reweighted supervision.
- Evidence anchors:
  - [abstract] "when questions are effectively unsolvable for a model, spurious chains of thought (CoTs) are more likely to appear, leading to false positives."
  - [section 4, Eq. 12] Explicit label formulation incorporating solvability into BCE loss.
- Break condition: If process-correct CoTs are also rare for solvable questions (e.g., complex multi-step reasoning), the signal may be too sparse; if p_solvable estimates are miscalibrated, down-weighting could suppress valid learning signal.

### Mechanism 3
- Claim: Reinforcement learning with solvability-adjusted advantage focuses learning on the intermediate-difficulty regime with highest learning potential
- Mechanism: Define learning potential LP = p_novel × p_solvable, where p_novel = (incorrect fraction) captures informational gain and p_solvable bounds it by solvability. Adjust DrGRPO advantage: A_MCQ-DrGRPO = p_solvable × A_DrGRPO, down-weighting gradients from unsolvable questions.
- Core assumption: Learning follows a trade-off—too easy provides no new information; too hard provides noisy/usable signal. The product form captures this peak at intermediate difficulty.
- Evidence anchors:
  - [abstract] "intermediate regime where learning is most effective"
  - [section 5.1, Eq. 15] "LP(q_i, o_ij) = p_novel(q_i) × p_solvable(q_i)" aligns with empirical accuracy improvements in Figure 3.
- Break condition: If model capability changes during training (distribution shift), pre-computed p_solvable estimates become stale; if tasks require exploration beyond current capability, down-weighting unsolvable questions may prevent discovery.

## Foundational Learning

- Concept: Beta distribution as conjugate prior for Bernoulli parameters
  - Why needed here: Core to estimating solvability with uncertainty; the posterior Beta(α, β) directly gives interpretable probability that true performance exceeds random guessing.
  - Quick check question: Given 3 successes out of 10 trials, what is P(p > 0.25) under Beta(1,1) prior?

- Concept: Outcome-supervised reward models (ORM) vs. process reward models (PRM)
  - Why needed here: Paper modifies ORM training; understanding the distinction clarifies why outcome-based supervision can still improve process-correctness via reweighting.
  - Quick check question: What label would a PRM use for an answer-correct but process-incorrect CoT?

- Concept: Group-relative policy optimization (GRPO/DrGRPO) advantage estimation
  - Why needed here: Paper modifies advantage calculation; must understand how baseline subtraction and normalization affect gradient magnitude per sample.
  - Quick check question: In a group of 32 samples with 2 correct answers, what is the DrGRPO advantage of a correct sample?

## Architecture Onboarding

- Component map: Solvability estimator -> MCQ-ORM -> MCQ-DrGRPO
- Critical path:
  1. Offline: Sample G CoTs per training question, compute and cache p_solvable
  2. ORM training: Train MCQ-ORM with reweighted labels on hidden states
  3. RL training: Run PPO with MCQ-DrGRPO advantage, using cached p_solvable at each step
- Design tradeoffs:
  - More samples G → sharper solvability estimates but higher compute (paper uses G=32 for math, G=8 for multimodal)
  - Training on unsolvable questions with down-weighted advantage → may reduce exploration but improves reliability
  - Using hidden states vs. raw text for ORM → paper uses hidden states for efficiency; raw text may capture more semantic detail
- Failure signatures:
  - MCQ-ORM underperforms baseline on answer-accuracy (Table 8): solvability weighting sacrifices answer selection for process selection
  - p_solvable ≈ 0 for all questions in a dataset: model is underpowered or questions are misformatted
  - RL training divergence: if KL penalty is too weak and advantage scaling is aggressive, policy may collapse
- First 3 experiments:
  1. Validate solvability estimation: For a held-out set, sample G=32 CoTs, compute p_solvable, and manually verify that low-p_solvable questions produce process-incorrect CoTs (replicate Figure 1 right).
  2. Ablate G: Train MCQ-ORM and MCQ-DrGRPO with G ∈ {4, 8, 16, 32} and plot process-accuracy vs. G to find compute-quality sweet spot.
  3. Cross-domain transfer: Train on one MCQA dataset (e.g., AQuA) and evaluate p_solvable calibration on another (e.g., MedMCQA) to test generalization of solvability estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can solvability modeling be effectively extended to open-ended question answering formats where answer matching is ambiguous?
- Basis in paper: [inferred] The paper explicitly restricts to MCQA because "matching of open-ended answers can be ambiguous," but many real-world reasoning tasks require open-ended responses.
- Why unresolved: The solvability estimation relies on clean binary correctness signals from exact answer matching, which becomes noisy or unavailable for open-ended responses.
- What evidence would resolve it: Experiments applying solvability-weighted objectives to open-ended reasoning benchmarks (e.g., free-form math explanations, code generation) with appropriate answer extraction or semantic matching techniques.

### Open Question 2
- Question: How does the interaction between dynamic solvability estimation and model improvement affect training stability and convergence?
- Basis in paper: [inferred] Solvability is estimated from samples of the current model, which itself is being updated during RL training—creating a moving target for what counts as "solvable."
- Why unresolved: The paper uses fixed solvability estimates from the base model, but does not analyze how re-estimating solvability during training would affect the learning dynamics.
- What evidence would resolve it: Ablation studies comparing fixed vs. periodically updated solvability estimates during RL training, measuring training curves and final performance.

### Open Question 3
- Question: Can solvability estimation be achieved with fewer samples to reduce computational overhead while maintaining effectiveness?
- Basis in paper: [inferred] The method requires sampling 32 CoTs per question to reliably distinguish solvable from unsolvable questions; Figure 1 shows clearer distinction with more samples.
- Why unresolved: The computational cost scales linearly with samples, making the approach expensive for large-scale training.
- What evidence would resolve it: Experiments varying G (samples per question) systematically below 32, analyzing the trade-off between computational cost and performance gains.

### Open Question 4
- Question: How would solvability modeling combine with process reward models (PRMs) that provide step-level feedback?
- Basis in paper: [inferred] The paper modifies outcome-based reward models but notes related work on PRMs that use "implicit step-level feedback derived from final answer correctness."
- Why unresolved: PRMs already aim to improve process correctness; the interaction between solvability-weighting and step-level process rewards is unexplored.
- What evidence would resolve it: Experiments integrating solvability weighting into PRM training objectives and comparing to standard PRM baselines.

## Limitations
- The method requires sampling 32 CoTs per question, creating significant computational overhead that scales linearly with dataset size
- The solvability estimation assumes uniform random guessing (1/|c|) as the baseline, which may not capture the actual difficulty distribution of answer choices
- Down-weighting unsolvable questions in RL may reduce exploration and potentially create a capability ceiling for difficult but solvable questions

## Confidence
- **High confidence**: The core observation that unsolvable questions produce spurious CoTs leading to false positives is well-supported by Figure 1 and consistent with the broader literature on reward hacking in LLMs
- **Medium confidence**: The Beta posterior approach for solvability estimation is mathematically sound, but its calibration across diverse question types and model capabilities requires further validation
- **Medium confidence**: The effectiveness of solvability-weighted training (both ORM and RL) is demonstrated empirically, but the specific functional form (survival function weighting, LP = p_novel × p_solvable) lacks theoretical justification beyond empirical fit

## Next Checks
1. **Cross-dataset solvability calibration**: Train p_solvable estimators on one MCQA dataset (e.g., AQuA) and evaluate their predictive power on held-out datasets (e.g., MedMCQA) to test generalization across domains and question types.

2. **Sample efficiency analysis**: Systematically vary G (number of CoT samples per question) from 4 to 64 and measure the trade-off between p_solvable estimation accuracy and computational cost, identifying the optimal G for different model scales.

3. **Ablation on solvability thresholds**: Instead of continuous weighting, implement a threshold-based approach where questions with p_solvable < θ are excluded entirely from training, and compare performance across different threshold values to understand the robustness of the solvability modeling approach.