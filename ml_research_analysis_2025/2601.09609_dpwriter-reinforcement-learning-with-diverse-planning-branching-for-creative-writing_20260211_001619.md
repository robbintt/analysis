---
ver: rpa2
title: 'DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative
  Writing'
arxiv_id: '2601.09609'
source_url: https://arxiv.org/abs/2601.09609
tags:
- diversity
- arxiv
- response
- diverse
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPWriter, a reinforcement learning framework
  designed to enhance the diversity of large language models in creative writing tasks.
  The key innovation is a Diverse Planning Branching method that strategically branches
  generation at the planning stage, combined with a group-aware diversity reward that
  encourages distinct response trajectories.
---

# DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing

## Quick Facts
- **arXiv ID:** 2601.09609
- **Source URL:** https://arxiv.org/abs/2601.09609
- **Reference count:** 40
- **Primary result:** Introduces DPWriter, an RL framework that enhances LLM output diversity in creative writing via semi-structured planning branching, achieving 15% increase in embedding-based diversity while maintaining or improving generation quality.

## Executive Summary
DPWriter addresses the challenge of improving LLM output diversity in creative writing without sacrificing quality. The method introduces a novel Diverse Planning Branching (DPB) approach that strategically branches generation at the planning stage, combined with a group-aware diversity reward. By structuring generation around a semi-structured long Chain-of-Thought reasoning process, DPWriter guides models to explore multiple divergent paths. Experiments on WritingBench, Creative Writing v3, and ArenaHard v2.0 show significant improvements in output diversity (15% increase in embedding-based diversity) while maintaining or improving generation quality, outperforming several strong baselines.

## Method Summary
DPWriter implements a three-stage generation process: plan → CoT reasoning → response. The method constructs a 43K semi-structured CoT dataset using GPT-4.1 to generate 5-aspect plans (goal, info, structure, language, presentation) wrapped in special tokens, followed by plan-consistent CoT revisions. A cold-start SFT model is trained on this data to learn the planning format. RL training uses GRPO with Diverse Planning Branching: at each planning segment, the model samples K=32 continuations and selects G most diverse candidates using n-gram metrics. The combined reward function is r = (1-λ)·r_qua + λ·r_qua·r_div with λ=0.6 and quality threshold τ=10, using Skywork-Reward-V2-Llama-3.1-8B for quality assessment.

## Key Results
- DPWriter achieves 15% increase in embedding-based diversity on WritingBench compared to baselines
- Maintains or improves generation quality across all benchmarks while significantly increasing diversity
- Ablation studies show DPB alone improves Emb diversity from 10.05 to 10.45, and combined with diversity reward reaches 10.45 (vs 9.19 without)
- Outperforms strong baselines including standard GRPO, Guided Generation, and various prompting strategies

## Why This Works (Mechanism)

### Mechanism 1: Semi-Structured Planning Scaffold
Introducing explicit multi-aspect planning before reasoning provides controllable branching points that guide diverse exploration trajectories. The generation process is decomposed into plan → CoT reasoning → response, with plans structured into 5 delimited segments using special tokens. This creates explicit decision points where divergence can be strategically introduced and measured.

### Mechanism 2: Diverse Planning Branching (DPB)
Branching at structured planning segments with explicit diversity selection yields more controllable and semantically diverse outputs than token-level entropy-based branching. For each planning segment, K continuations are generated from each candidate, then G candidates are selected maximizing a diversity metric. This creates a beam-like search where diversity—not just probability—determines which branches survive.

### Mechanism 3: Group-Aware Diversity Contribution Reward
Rewarding responses proportionally to their unique n-gram contribution within a group encourages exploration of distinct content regions while quality-thresholding prevents degenerate diverse outputs. The diversity reward counts unique n-grams in each response absent from the group, normalized by length, and is combined multiplicatively with quality reward.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** DPWriter builds on GRPO, which estimates advantages within a group of rollouts rather than using a critic model. Understanding how GRPO normalizes rewards within groups is essential for seeing why group-aware diversity rewards integrate naturally.
  - **Quick check question:** Given rewards {r_1, r_2, r_3} for three rollouts, can you compute the GRPO advantage for each using A_i = (r_i - r̄) / σ_r?

- **Concept: Semi-Structured vs. Unstructured Chain-of-Thought**
  - **Why needed here:** The paper's core innovation is adding explicit planning structure to CoT. Understanding the tradeoff—unstructured CoT offers flexibility but less control; structured planning offers control points but risks rigidity—is critical for diagnosing failures.
  - **Quick check question:** In the semi-structured paradigm, what are the three generation stages and their conditioning dependencies? (Answer: p∼M(·|q), c∼M(·|q,p), y∼M(·|q,p,c))

- **Concept: Diversity Metrics (N-gram vs. Embedding-based)**
  - **Why needed here:** DPB uses both n-gram and embedding-based diversity to select diverse branches. Each has blindspots: n-grams miss semantic equivalence; embeddings may conflate surface diversity with semantic novelty.
  - **Quick check question:** If two planning candidates are "Write a dark, atmospheric mystery" and "Compose a gloomy, moody suspense story," would n-gram diversity capture their semantic similarity? Would embedding diversity?

## Architecture Onboarding

- **Component map:**
Input: instruction q
  ↓
[SFT Cold-Start Model] → generates plan segments p_1...p_S, CoT c, response y
  ↓ (during RL only)
[Diverse Planning Branching] → at each segment s: sample K continuations, select G by D(·)
  ↓
[Reward Computation] → quality reward r_qua from Skywork-Reward-V2-Llama-3.1-8B
                      → diversity reward r_div from unique n-gram count in group
                      → combined: r = (1-λ)·r_qua + λ·r_qua·r_div (if r_qua > τ)
  ↓
[GRPO Update] → advantage normalization within group, policy gradient with clipping

- **Critical path:**
1. Construct 43K semi-structured CoT dataset using GPT-4.1 prompts (multi-aspect planning, plan-consistent CoT revision)
2. SFT cold-start on this data to learn planning format
3. Filter to 10K harder samples for RL (where SFT model underperforms reward model threshold)
4. RL training with DPB (K=32 branch factor, G=8 group size, 5 epochs)

- **Design tradeoffs:**
- **Branch factor K vs. compute:** Higher K explores more but costs O(K×G) generations per segment. Paper uses K=32; ablation shows diversity gains continue to K=128.
- **N-gram vs. embedding diversity:** N-gram is faster; embedding captures semantics. Paper uses n-gram for DPB, embedding for final evaluation. Table 3 shows n-gram slightly outperforms embedding for branching.
- **λ weighting:** Set to 0.6 per appendix. Higher λ prioritizes diversity but risks quality degradation if threshold τ is too permissive.

- **Failure signatures:**
- **Plan-CoT inconsistency:** If reasoning ignores the plan, check SFT data quality—GPT-4.1 revision prompts may not enforce alignment strongly enough.
- **Collapse to similar plans:** If all branches converge despite DPB, the base model may be overfitted; inspect entropy of plan segment tokens.
- **Quality-diversity tradeoff surfaces:** If diversity gains come at quality cost, λ may be too high or τ too low; verify r_qua distribution before/after RL.

- **First 3 experiments:**
1. **Sanity check SFT quality:** Generate plans and responses for 50 held-out prompts; manually verify plan-reasoning-response coherence and that plans meaningfully vary across samples.
2. **Ablate branching alone:** Run RL with DPB but λ=0 (no diversity reward); measure whether branching alone improves Emb/EAD on WritingBench. Expect ~0.4 Emb gain per table 3.
3. **Calibrate λ and τ:** Sweep λ∈{0.3, 0.6, 0.9} and τ∈{5, 10, 15} on a validation split; plot quality score vs. Emb diversity to find the Pareto frontier. Paper claims τ=10, λ=0.6 works, but this may be task-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increased output diversity directly enhance other creative dimensions, such as novelty or surprise, beyond mere variance?
- **Basis in paper:** [explicit] "besides quality improvement, whether diversity can benefit other aspects like creativity remains a more open question" (Section 9).
- **Why unresolved:** The paper measures diversity using N-gram and embedding distances, which quantify difference but do not necessarily capture higher-order creative attributes like originality or thematic depth.
- **What evidence would resolve it:** A study correlating DPWriter's diversity metrics with human or model-based evaluations of "creativity" (e.g., using the Torrance Tests of Creative Thinking adapted for text).

### Open Question 2
- **Question:** Does the computational overhead of Diverse Planning Branching (DPB) limit its scalability to extremely large language models?
- **Basis in paper:** [explicit] The authors state the method "may introduce additional computational overhead, potentially limiting scalability for extremely large models or datasets" (Section 9).
- **Why unresolved:** Experiments were restricted to 3B and 4B parameter models; the latency and memory costs of branching and maintaining multiple candidate plans may grow non-linearly with model size.
- **What evidence would resolve it:** Profiling the training throughput and GPU memory consumption of DPWriter when applied to a 70B+ parameter backbone compared to standard GRPO.

### Open Question 3
- **Question:** Is the observed "seesaw" between diversity and quality a solvable optimization problem or a fundamental trade-off in RL alignment?
- **Basis in paper:** [explicit] "the seesaw between these two aspects may not be fully resolved, and further research is needed" (Section 9).
- **Why unresolved:** While DPWriter mitigates the collapse, the reward formulation r = (1-λ)r_qua + λ r_qua r_div still requires balancing weights, suggesting the model cannot maximize both objectives simultaneously without compromise.
- **What evidence would resolve it:** Identifying a theoretical guarantee or empirical setting where increasing the diversity weight λ does not induce any variance in the quality reward r_qua.

## Limitations
- **Limited diversity metric validation:** The paper relies on n-gram and embedding-based diversity metrics that may not fully capture semantic novelty, and the quality-diversity tradeoff management via multiplicative coupling may not generalize across creative domains.
- **Single reward model dependency:** Quality assessment depends on Skywork-Reward-V2-Llama-3.1-8B without cross-validation with alternative quality estimators, potentially making improvements artifacts of reward model alignment.
- **Dataset construction opacity:** The 43K semi-structured CoT dataset is synthesized using GPT-4.1, but the quality and diversity of synthetic plans are not independently validated, potentially constraining downstream RL performance.

## Confidence
- **High confidence:** The experimental design (controlled ablations, multiple baselines, diverse benchmarks) is rigorous and reproducible. The improvement in diversity metrics (15% Emb gain, consistent EAD increases) is well-documented with statistical significance.
- **Medium confidence:** The mechanism by which planning-level diversity propagates to final response diversity is plausible but not fully validated. The paper shows correlation (diverse plans → diverse responses) but does not rule out alternative explanations.
- **Low confidence:** Claims about the generality of the λ=0.6 weighting and τ=10 threshold are weakly supported. These hyperparameters are tuned on WritingBench but may require domain-specific adjustment for other creative writing tasks.

## Next Checks
1. **Validate planning-reasoning alignment:** Generate 100 held-out samples with DPWriter; manually annotate whether the final response faithfully follows the initial plan and reasoning chain. Quantify plan-reasoning consistency scores to ensure the scaffold is not being ignored.
2. **Cross-validate with alternative reward models:** Re-run RL with a different quality estimator (e.g., GPT-4.1-based pairwise comparison) to confirm that observed diversity gains are not artifacts of the specific Skywork reward model used.
3. **Test hyperparameter robustness:** Perform a grid search over λ∈{0.3, 0.6, 0.9} and τ∈{5, 10, 15} on a validation split; plot quality vs. diversity to identify the Pareto frontier and assess whether the claimed settings are optimal or merely locally good.