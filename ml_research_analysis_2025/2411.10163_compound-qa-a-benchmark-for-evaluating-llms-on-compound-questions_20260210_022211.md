---
ver: rpa2
title: 'Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions'
arxiv_id: '2411.10163'
source_url: https://arxiv.org/abs/2411.10163
tags:
- questions
- compound
- arxiv
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Compound-QA, a benchmark for evaluating large
  language models on compound questions that contain multiple interrelated sub-questions.
  The authors propose a data synthesis framework (CQ-Syn) that generates compound
  questions from existing QA datasets, categorizes them into five types (Factual-Statement,
  Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, Evaluation-and-Suggestion),
  and evaluates models across understanding, reasoning, and knowledge dimensions.
---

# Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions

## Quick Facts
- arXiv ID: 2411.10163
- Source URL: https://arxiv.org/abs/2411.10163
- Reference count: 0
- Primary result: Compound question performance lags non-compound performance across nine LLMs

## Executive Summary
This paper introduces Compound-QA, a benchmark for evaluating large language models on compound questions containing multiple interrelated sub-questions. The authors develop a data synthesis framework (CQ-Syn) that generates compound questions from existing QA datasets and categorize them into five types: Factual-Statement, Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion. Testing nine open-source LLMs shows their performance on compound questions is notably lower than on non-compound questions, with Qwen3 achieving the best overall performance (up to 90.5% win rate on Evaluation-and-Suggestion). The study finds that sub-questions positioned at sequence boundaries perform better than those in the middle, and that supervised fine-tuning with instruction data augmented by compound questions substantially improves model performance while maintaining general capabilities.

## Method Summary
The Compound-QA benchmark employs a three-step data synthesis framework: Question Design (LLM generates compound questions using type-specific prompts from source QA datasets), Question Verification (keyword filtering plus LLM-based quality checks), and Reference Generation (proprietary LLM produces reference answers, human-verified for accuracy). The benchmark consists of 1,500 samples across five compound question types and three capability dimensions (understanding, reasoning, knowledge). Evaluation uses GPT-4o-mini as an automatic evaluator with a three-dimensional rubric (Comprehensiveness, Correctness, Diversity) and position-bias mitigation through swapped-order averaging. Model improvement is achieved through LoRA fine-tuning on a 70/30 train/test split of compound-question instruction data, with validation on general benchmarks to ensure capability preservation.

## Key Results
- Compound question performance significantly lags non-compound performance across all nine tested LLMs
- Qwen3 achieves highest overall performance with up to 90.5% win rate on Evaluation-and-Suggestion questions
- Sub-questions positioned at sequence boundaries (beginning/end) consistently outperform middle-position sub-questions
- LoRA fine-tuning with compound-question instruction data substantially improves performance while maintaining general capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-questions positioned at sequence boundaries receive more effective attention and produce higher-quality responses than those in the middle.
- Mechanism: Transformer attention patterns exhibit a U-shaped effectiveness curve—initial tokens anchor context while terminal tokens benefit from recency in the residual stream. Middle positions compete for attention with both earlier context and later tokens, reducing the model's ability to fully address mid-sequence sub-questions.
- Core assumption: Attention bottlenecks compound when multiple questions require independent reasoning within a single forward pass.
- Evidence anchors:
  - Table 3 shows LLaMA and InternLM achieve highest win rates for sub-questions at first (1XX) and last (XX1) positions, with middle positions (X1X) underperforming consistently.
  - Page 4 states: "This is consistent with previous research findings that LLMs exhibit weaker performance when handling information located in the middle of long contexts [28]" (citing "Lost in the Middle").
  - No direct corpus reinforcement for this specific compound-question context; the mechanism extrapolates from prior long-context findings.

### Mechanism 2
- Claim: Explicit decomposition of compound questions into sequential sub-questions, followed by individual responses and synthesis, reduces omission and confusion errors.
- Mechanism: By forcing the model to first enumerate sub-questions (externalizing the decomposition), each sub-question receives dedicated attention and generation capacity. The synthesis step then integrates partial answers, reducing interference between sub-questions and making omissions detectable.
- Core assumption: The model's failure on compound questions stems partly from implicit load—holding multiple reasoning chains simultaneously without explicit scaffolding.
- Evidence anchors:
  - Section 3.2.4 describes "Decomposition strategy (Decom-S)" which "explicitly instructs the model to break down a compound question into multiple sub-questions, address each sequentially, and synthesize the individual responses into a complete answer."
  - Figure 4 shows Decom-S improves performance over baseline, though LoRA fine-tuning achieves higher scores.
  - Related work on multi-task inference and sequential instruction following (cited as [17], [20]) supports decomposition as a strategy.

### Mechanism 3
- Claim: Supervised fine-tuning on compound-question instruction data improves multi-question handling without degrading general capabilities.
- Mechanism: Fine-tuning adapts the model's prior distribution toward recognizing compound-question patterns—learning to allocate attention across sub-questions and structure responses comprehensively. LoRA's low-rank adaptation preserves base model weights, limiting catastrophic forgetting while enabling task-specific refinement.
- Core assumption: The base model already possesses sufficient knowledge and reasoning capacity; the gap is in instruction-following patterns for compound structures specifically.
- Evidence anchors:
  - Figure 4 shows "LoRA fine-tuning achieves the highest scores on most dimensions, demonstrating its effectiveness for compound question answering."
  - Table 4 shows fine-tuned models maintain or slightly improve performance on MMLU, GSM8K, and TruthfulQA, with no significant degradation.

## Foundational Learning

- **Concept: Transformer attention and positional bias**
  - Why needed here: Understanding why sub-questions at sequence boundaries perform better requires grasping how self-attention distributes computational focus across token positions.
  - Quick check question: Given a 512-token input, which positions would you expect a standard Transformer to attend to most strongly, and why might this differ for models trained with special positional objectives?

- **Concept: Instruction tuning and catastrophic forgetting**
  - Why needed here: The paper's LoRA fine-tuning approach depends on understanding how supervised fine-tuning shifts model behavior while preserving prior knowledge.
  - Quick check question: If you fine-tune a general-purpose LLM on a narrow task, what are two mechanisms that could cause performance degradation on unrelated tasks, and how does LoRA specifically address one of them?

- **Concept: Multi-hop reasoning vs. multi-question handling**
  - Why needed here: Compound questions are distinct from multi-hop reasoning; the former involves multiple (potentially independent) sub-questions in one query, while the latter involves chained reasoning steps.
  - Quick check question: In a compound question asking "What is X? How does Y affect X? Which is better, X or Z?", identify which parts require multi-hop reasoning versus independent retrieval.

## Architecture Onboarding

- **Component map:**
  - CQ-Syn Framework: Question Design -> Question Verification -> Reference Generation
  - Compound-QA Benchmark: 5 types × 3 dimensions × 100 samples = 1,500 total
  - Evaluation Framework: Comprehensiveness/Correctness/Diversity → GPT-4o-mini evaluation → Position-bias mitigation

- **Critical path:**
  1. Select appropriate question type and source dataset for target evaluation dimension
  2. Generate compound questions via CQ-Syn, ensuring type-specific logical relationships are present
  3. Apply filtering (keyword + LLM) and human verification for quality assurance
  4. Evaluate target model using multi-dimensional scoring with position-bias mitigation
  5. For improvement: apply LoRA fine-tuning on 70/30 train-test split, validate on both Compound-QA and general benchmarks

- **Design tradeoffs:**
  - Synthetic vs. organic compound questions: CQ-Syn generates questions from existing datasets, ensuring coverage but potentially missing natural user phrasing patterns.
  - Automatic vs. human evaluation: GPT-4o-mini enables scalable evaluation but introduces model-specific biases (84% human agreement).
  - LoRA vs. full fine-tuning: LoRA preserves general capabilities (Table 4) but may achieve lower ceiling performance on Compound-QA specifically.

- **Failure signatures:**
  - Omission: Model fails to address all sub-questions (low Comprehensiveness score)
  - Confusion: Model conflates related sub-questions, producing blended or incomplete responses
  - Off-topic: Model generates content that doesn't follow the required logical reasoning chain
  - Positional degradation: Middle-position sub-questions systematically underperform relative to first/last positions

- **First 3 experiments:**
  1. Baseline evaluation: Run target model on Compound-QA across all 5 types and 3 dimensions
  2. Decomposition ablation: Test Decom-S prompting strategy on held-out subset vs. baseline
  3. LoRA fine-tuning with generalization check: Fine-tune on 70% training split, evaluate on remaining 30% plus MMLU/GSM8K/TruthfulQA

## Open Questions the Paper Calls Out
- How can compound question evaluation be extended to multimodal applications where sub-questions may involve text, images, audio, or video inputs?
- Do the observed position effects generalize to all five compound question types beyond Factual-Statement?
- Can targeted mitigation strategies be developed for the three specific error types identified?

## Limitations
- Synthetic nature of generated questions may not reflect natural human query patterns despite human verification
- Evaluation relies on GPT-4o-mini with 84% human agreement, suggesting potential systematic errors in 16% of cases
- Five compound question type categorization may not capture all real-world variations

## Confidence
**High Confidence:** Compound question performance lags non-compound performance (nine-model comparison, full benchmark coverage)
**Medium Confidence:** Positional attention mechanism and decomposition strategy effectiveness (supported by consistent patterns but relies on extrapolation)
**Low Confidence:** Five compound question type categorization captures all variations, automatic evaluation bias mitigation

## Next Checks
1. Conduct ablation studies comparing sub-question performance at different sequence positions while controlling for complexity, using attention visualization tools to directly observe the claimed U-shaped attention distribution.
2. Create a small corpus of naturally occurring compound questions from real user queries and compare model performance on these versus synthetically generated questions of identical type and complexity.
3. Test decomposition prompting with varying levels of granularity (from no decomposition to full enumeration) on a subset of compound questions to identify the optimal balance between explicit structure and natural reasoning flow.