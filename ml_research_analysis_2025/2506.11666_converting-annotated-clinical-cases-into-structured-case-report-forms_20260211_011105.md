---
ver: rpa2
title: Converting Annotated Clinical Cases into Structured Case Report Forms
arxiv_id: '2506.11666'
source_url: https://arxiv.org/abs/2506.11666
tags:
- clinical
- diagnosis
- items
- cases
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a methodology to convert annotated clinical
  case datasets into structured Case Report Forms (CRFs), addressing the scarcity
  of CRF datasets needed for automated CRF filling systems. The approach clusters
  clinical cases by semantic similarity, generates group-specific CRFs, and populates
  them using existing annotations.
---

# Converting Annotated Clinical Cases into Structured Case Report Forms

## Quick Facts
- arXiv ID: 2506.11666
- Source URL: https://arxiv.org/abs/2506.11666
- Reference count: 24
- Primary result: GPT-4o achieves 59.7% F1 in Italian and 67.3% in English for CRF slot-filling

## Executive Summary
This paper addresses the scarcity of structured Case Report Form (CRF) datasets needed for automated CRF filling systems. The authors propose a methodology to convert annotated clinical case datasets into structured CRFs by clustering cases by semantic similarity, generating group-specific CRFs, and populating them using existing annotations. Applied to the E3C dataset in English and Italian, the approach produces a new multilingual CRF slot-filling dataset. Experiments with various Large Language Models show that GPT-4o outperforms open-source models, but even state-of-the-art LLMs struggle with this task, achieving only 59.7-67.3% F1.

## Method Summary
The methodology clusters clinical cases by semantic similarity using a graph-based approach (s = 3d + e, where d = diagnosis similarity, e = entity overlap ratio) with UMLS augmentation for terminology normalization. Louvain algorithm identifies communities as CRF groups. Group-specific CRFs are generated from existing annotations through section-specific mapping rules, with semi-automated manual revision. The resulting CRFs are populated with gold-standard values extracted from annotations. Zero-shot inference with instruct LLMs evaluates performance on three sub-tasks: diagnosis classification, clinical history classification, and exam result extraction.

## Key Results
- GPT-4o achieves 59.7% micro F1 (Italian) and 67.3% (English) for CRF slot-filling
- Performance scales with model size, showing ~20 Macro F1 point improvement when scaling from 7/8B to 70/72B models
- Exams task is most challenging (max 56.2% F1 even for GPT-4o), while diagnosis performs best (up to 83.5% F1)
- Italian consistently lags English by ~7.5 F1 points, suggesting language-specific capacity gaps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Grouping clinical cases by semantic similarity produces coherent CRFs that balance specificity and generalizability
- **Mechanism**: A graph-based representation connects clinical notes via weighted edges (s = 3d + e, where d = diagnosis similarity, e = entity overlap ratio). UMLS augmentation normalizes terminology variants before similarity computation. Louvain algorithm then identifies communities as CRF groups
- **Core assumption**: Diagnosis is the primary organizing dimension for clinical data collection; cases sharing diagnoses require similar CRF items
- **Evidence anchors**: 
  - [abstract] "The approach clusters clinical cases by semantic similarity, generates group-specific CRFs"
  - [section 3.1] "We prioritized diagnosis as the key clustering dimension since CRF items are typically guided by the specific condition being studied"
- **Break condition**: When cases lack explicit diagnoses or when diagnosis similarity poorly predicts shared clinical items

### Mechanism 2
- **Claim**: Existing IE annotations can be systematically converted into structured CRF fields through section-specific mapping rules
- **Mechanism**: E3C annotations (clinical entities, RMLs, PERTAINS_TO relations) map to CRF sections: entities → clinical history (with polarity/modality/permanence attributes), RMLs → exams, extracted diagnoses → diagnosis section. UMLS normalization collapses equivalent terms during item consolidation
- **Core assumption**: The source dataset's annotation schema adequately covers clinically relevant CRF fields for the target use case
- **Evidence anchors**:
  - [abstract] "populates them using existing annotations"
  - [section 3.2] "Clinical history items can be generated using annotations such as symptom, sign, clinical entity, disease, condition, procedure"
- **Break condition**: When source annotations lack key sections (E3C had no treatment annotations, so treatment section was excluded)

### Mechanism 3
- **Claim**: LLMs can extract structured CRF values via zero-shot prompting, with performance scaling by model size and varying by task complexity
- **Mechanism**: Decoder-only LLMs receive clinical case + CRF item + task-specific guidelines. Tasks range from binary classification (diagnosis: yes/no/NA) to multi-class classification (history: 12 valid values) to open generation (exams: free-text results)
- **Core assumption**: LLM pre-training includes sufficient clinical knowledge for medical terminology comprehension and extraction
- **Evidence anchors**:
  - [abstract] "GPT-4o achieves 59.7% F1 in Italian and 67.3% in English, outperforming open-source models"
  - [section 7] "we observe an average improvement of around 20 Macro F1 points when scaling from small (7/8B) to large (70/72B) models"
- **Break condition**: When tasks require nuanced extraction without constrained outputs (exams: max 56.2% F1) or when target language has less pre-training data

## Foundational Learning

- **Concept: Case Report Forms (CRFs)**
  - Why needed here: CRFs are structured clinical research tools with predefined items; understanding their inherent sparsity (~90% unfilled per patient) explains why clustering is necessary
  - Quick check question: Why would a single universal CRF for all clinical cases be problematic?

- **Concept: Slot Filling Task Hierarchy**
  - Why needed here: CRF filling decomposes into sub-tasks with increasing complexity (classification → nuanced classification → generation), directly affecting model performance
  - Quick check question: What makes the "exams" sub-task harder than "diagnosis"?

- **Concept: UMLS Metathesaurus for Semantic Normalization**
  - Why needed here: Terminology normalization enables matching across variant expressions ("malignant tumor" vs. "cancer") during both clustering and item consolidation
  - Quick check question: How does UMLS augmentation improve clustering coherence?

## Architecture Onboarding

- **Component map**: Diagnosis Extraction -> Similarity Graph -> Clustering -> CRF Generator -> Manual Revision -> LLM Filling
- **Critical path**: 1. Extract diagnoses (prerequisite for similarity) 2. Build graph, cluster cases 3. Generate group-specific CRFs from annotations 4. Populate gold-standard CRFs 5. Evaluate LLMs on held-out test split
- **Design tradeoffs**: Per-case vs. clustered vs. universal CRF (paper chose clustering), automatic vs. manual revision (semi-automated with human gatekeeping), cross-split CRF construction (CRFs built from full corpus but only training notes used for learning)
- **Failure signatures**: Extremely low precision on history (Llama 8B: 7.2%) → over-generation, exams consistently weakest (47–56% F1 even for GPT-4o) → extraction difficulty, Italian lagging English by 7.5 F1 points → language-specific capacity gaps
- **First 3 experiments**: 1. Replicate baseline (pattern matching) on simplified binary history task to validate dataset integrity 2. Compare GPT-4o vs. Qwen-72B on English, analyzing per-section F1 to isolate hardest sub-tasks 3. Ablate UMLS augmentation in similarity computation; manually review cluster coherence to quantify impact

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed clustering and generation methodology generalize to denser, longitudinal clinical corpora like MIMIC-IV or i2b2 without significant manual re-tuning? The authors state the methodology "has been experimented only on the E3C corpus" and suggest that "additional insights may derive from different available datasets."

- **Open Question 2**: To what extent does the simplified three-value schema for clinical history impact model performance compared to a schema that fully captures nuanced attributes like chronicity? The authors note they "made a few simplifications" assuming a three-value schema, while acknowledging that "in reality the possible values should be extended to cover cases of chronicity."

- **Open Question 3**: What modeling strategies are required to improve extraction performance on the "Exams" sub-task, which requires generative answers without a predefined set of valid responses? The results show that the "Exams" task is the most challenging, and the authors highlight that this section is particularly difficult because it requires "extraction and interpretation of numerical and textual values."

## Limitations

- Performance directly tied to E3C's annotation coverage; missing treatment data required dataset-specific adaptation
- Italian consistently underperforms English by ~7.5 F1 points, suggesting insufficient multilingual pre-training data for clinical terminology
- CRF filling requires precise extraction with strict formatting, yet LLMs trained on generative tasks show ~60% F1 even at 70B parameters

## Confidence

- **High confidence**: Clustering methodology produces coherent CRF groups (Louvain algorithm well-established, similarity computation reproducible with provided weights)
- **Medium confidence**: LLM filling performance claims (dependent on exact prompt formatting and model versions not fully specified)
- **Low confidence**: Generalization to non-E3C datasets (method tailored to specific annotation schema and clinical note structure)

## Next Checks

1. **Ablation study**: Remove UMLS augmentation from similarity computation and manually evaluate cluster coherence to quantify terminology normalization impact
2. **Cross-dataset transfer**: Apply methodology to a different annotated clinical corpus (e.g., MIMIC-CXR) to test schema independence
3. **Prompt sensitivity analysis**: Systematically vary prompt templates and formatting rules to identify performance bottlenecks in LLM extraction