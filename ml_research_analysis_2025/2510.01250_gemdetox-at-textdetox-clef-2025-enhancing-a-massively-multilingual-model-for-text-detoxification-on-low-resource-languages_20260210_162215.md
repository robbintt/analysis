---
ver: rpa2
title: 'GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model
  for Text Detoxification on Low-resource Languages'
arxiv_id: '2510.01250'
source_url: https://arxiv.org/abs/2510.01250
tags:
- languages
- detoxification
- data
- language
- toxic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work describes the GemDetox system for multilingual text detoxification,
  developed for the PAN 2025 challenge. The approach builds on a 12B-parameter Gemma-3
  multilingual transformer fine-tuned with parameter-efficient LoRA and enhanced with
  Chain-of-Thought prompting and few-shot examples.
---

# GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages

## Quick Facts
- arXiv ID: 2510.01250
- Source URL: https://arxiv.org/abs/2510.01250
- Reference count: 40
- Primary result: Ranked 1st on TextDetox PAN 2025 with joint score up to 0.802 on French, excelling across 15 typologically diverse languages

## Executive Summary
GemDetox is a multilingual text detoxification system developed for the PAN 2025 challenge, achieving state-of-the-art performance across 15 typologically diverse languages. The approach builds on a 12B-parameter Gemma-3 multilingual transformer fine-tuned with parameter-efficient LoRA and enhanced with Chain-of-Thought prompting and few-shot examples. Training combines human-authored parallel pairs, machine-translated extensions for six unseen languages, and model-generated synthetic pairs filtered by semantic overlap. At inference, inputs are enriched with semantic neighbors and toxic-span annotations. Evaluated across language tiers, the system achieves strong performance, ranking first on high- and low-resource languages with a joint score up to 0.802 on French.

## Method Summary
GemDetox fine-tunes Gemma-3-12B-Instruct with LoRA (rank=16, alpha=16, ~65M trainable params) in three phases: (1) all 15 languages, (2) low-resource languages only at lower LR, (3) synthetic pairs filtered by Jaccard similarity. The system uses Chain-of-Thought prompting with 4-step reasoning and prepends 3 semantically retrieved few-shot examples. Inputs are enriched with LaBSE neighbor retrieval and rule-based toxic-span annotation. Inference samples 3 candidates and selects the highest joint score (STA × SIM × FL). Training data includes ParaDetox-9 (3,600 human pairs), ParaDetox-MT6 (21,600 MT pairs), and synthetic pairs from Multilingual Toxicity Dataset.

## Key Results
- Ranked 1st in automatic evaluation across 15 languages with joint score up to 0.802 on French
- Language resource tier is strongest predictor of performance (η² = 0.667, p < 0.01)
- Phase 3 synthetic data augmentation significantly improves low-resource languages: Tatar 0.43→0.56, Hinglish 0.40→0.51
- Ablations show +0.081 joint score increase from few-shot examples and +0.088 from basic CoT prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) prompting improves detoxification quality by decomposing the rewriting task into explicit reasoning steps
- Mechanism: A 4-step instructional prompt guides the model to (i) identify toxic element(s), (ii) retrieve semantic content, (iii) rewrite using neutral words, (iv) verify non-toxicity—forcing explicit intermediate reasoning before generation
- Core assumption: Detoxification benefits from sequential reasoning that surfaces toxicity identification before paraphrase generation
- Evidence anchors:
  - [abstract] "Ablations show +0.081 joint score increase from few-shot examples and +0.088 from basic CoT prompting"
  - [section 5] "By implementing Chain-of-Thought, J climbs to 0.650 / 0.592 (Δ +0.088 / +0.078 over baseline)"
  - [corpus] No direct corpus validation of CoT for detoxification; related SynthDetoxM paper (FMR=0.53) focuses on data generation, not prompting
- Break condition: When implicit toxicity (sarcasm, coded hate speech) dominates explicit lexical toxicity—paper explicitly notes this is out of scope

### Mechanism 2
- Claim: Few-shot conditioning with semantically retrieved examples provides in-language detoxification patterns
- Mechanism: LaBSE embeddings retrieve the top-3 most similar toxic sentences from training data; these are prepended to the prompt, giving the model language-aware demonstrations before processing the target
- Core assumption: Semantic similarity in multilingual embedding space correlates with transferable detoxification strategies
- Evidence anchors:
  - [abstract] "Ablations show +0.081 joint score increase from few-shot examples"
  - [section 3.3] "the prompt prepends the semantically closest k examples (three in our experiments) drawn from the same language, retrieved from the datasets described in 3.1"
  - [corpus] SynthDetoxM paper confirms LLMs can few-shot annotate detoxification data, indirectly supporting few-shot utility
- Break condition: Low-resource languages where LaBSE embeddings are unreliable; paper acknowledges "LaBSE-based retrieval...can be unreliable for very low-resource languages"

### Mechanism 3
- Claim: Tiered multi-phase training with synthetic data augmentation closes performance gaps for low-resource languages
- Mechanism: Three-phase LoRA fine-tuning—Phase 1: all 15 languages; Phase 2: only unseen languages at lower LR; Phase 3: synthetic pairs (model-generated, Jaccard-filtered)—specializes adapters while preserving high-resource performance
- Core assumption: Machine translation preserves detoxification patterns; semantic overlap thresholds filter out low-quality synthetic pairs
- Evidence anchors:
  - [abstract] "Training combines human-authored parallel pairs, machine-translated extensions for six unseen languages, and model-generated synthetic pairs filtered by semantic overlap"
  - [section 5/Figure 2] "Tatar 0.43→0.56; Hinglish 0.40→0.51" showing largest gains for low-resource languages after synthetic data phases
  - [corpus] SynthDetoxM (FMR=0.53) directly supports synthetic parallel detoxification data generation
- Break condition: When synthetic data introduces semantic bias or stylistic overfitting; reviewers noted this concern

## Foundational Learning

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: 12B-parameter model must fit in <24GB GPU memory while adapting to 15 languages; LoRA trains only 0.55% of parameters (~65M)
  - Quick check question: Can you explain why freezing base weights W₀ and learning low-rank updates ΔW = BA preserves multilingual capabilities while adapting task behavior?

- Concept: **Multilingual Embedding Spaces (LaBSE)**
  - Why needed here: Semantic neighbor retrieval and content preservation metrics require language-agnostic sentence representations across typologically diverse languages (Tatar to Amharic)
  - Quick check question: How would you verify that LaBSE embeddings capture meaning similarity between a Hinglish toxic sentence and its Hindi counterpart?

- Concept: **Style Transfer Evaluation Triad (STA × SIM × FL)**
  - Why needed here: Detoxification is a multi-objective problem—removing toxicity while preserving meaning and fluency; the joint score multiplies (not averages) these to penalize any single failure mode
  - Quick check question: Why does multiplying Style Transfer Accuracy, Content Preservation, and Fluency create a stricter constraint than averaging them?

## Architecture Onboarding

- Component map: Input sentence → rule-based toxic-span detection + LaBSE embedding → LaBSE neighbor retrieval (3 neighbors) → construct prompt [CoT system message + 3 few-shot examples + user input] → LoRA-adapted Gemma-3 forward pass → sample 3 candidates → compute joint score → return highest-scoring

- Critical path:
  1. Input sentence → rule-based toxic-span detection + LaBSE embedding
  2. Retrieve 3 semantic neighbors from training corpus
  3. Construct prompt: [CoT system message] + [3 few-shot examples] + [user input with language tag]
  4. Forward pass through LoRA-adapted Gemma-3 (only adapter weights updated during training)
  5. Sample 3 candidate outputs → compute joint score against gold reference → return highest-scoring

- Design tradeoffs:
  - 4-bit quantization enables single-GPU training but may degrade low-resource language quality
  - Jaccard threshold 0.80-0.90: higher preserves semantics but filters out more synthetic data, reducing diversity
  - Phase 2 learning rate (5×10⁻⁶) prevents catastrophic forgetting but slows adaptation for unseen languages
  - JSON output format: reliable parsing but adds ~10-15 overhead tokens per generation

- Failure signatures:
  - **Compound toxicity**: Multiple toxic elements in one sentence → only one addressed (6.7% in Hinglish; see Appendix D examples)
  - **Nonsensical generation**: ~30% of Amharic outputs ungrammatical despite parallel data availability
  - **Dialectal gap**: Regional profanity (e.g., Argentine "vale pico") not in lexicon → passes through unchanged
  - **Over-detoxification**: Hebrew political criticism stripped along with explicit insults

- First 3 experiments:
  1. **Ablate CoT**: Train identical model with direct instruction only (no 4-step reasoning) → measure Δ joint score; expect -0.088 based on paper
  2. **Vary few-shot k**: Test k ∈ {0, 1, 3, 5} on dev set across language tiers → identify optimal retrieval count and interaction with resource level
  3. **Synthetic data ratio sweep**: Train with 0%, 25%, 50%, 100% synthetic pairs for low-resource languages → isolate augmentation effect on Tatar/Hinglish/Amharic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can morphological typology and genetic affiliation effects on detoxification performance be disentangled through broader language sampling and mixed-effects modeling?
- Basis in paper: [explicit] "broader language sampling and mixed-effects modeling are necessary to disentangle their actual contributions more clearly."
- Why unresolved: ANOVA shows both factors explain substantial variance (η² = 0.654 and 0.592) but are confounded in the 15-language sample; genetic affiliation and typology correlate across current languages
- What evidence would resolve it: Study with 40+ languages stratified to separate typological from genetic effects, analyzed via mixed-effects models controlling for both factors simultaneously

### Open Question 2
- Question: Can preference-based optimization (GRPO, DPO) improve multilingual detoxification over standard SFT fine-tuning?
- Basis in paper: [explicit] "Future research could explore integrating such approaches with multilingual LLMs to fine-tune systems more effectively in line with human preferences."
- Why unresolved: Current system uses only LoRA SFT; preference optimization techniques remain untested for multilingual detoxification despite their promise for directly optimizing non-toxicity while preserving meaning
- What evidence would resolve it: Comparative experiments applying DPO/GRPO to the same multilingual detoxification task, measuring joint scores and human preference alignment across all 15 languages

### Open Question 3
- Question: What dimensions of text style do human-calibrated LLM judgments emphasize that differ from standard automatic metrics?
- Basis in paper: [explicit] "human-calibrated LLM judgments emphasize different dimensions of text style, and a future investigation of such differences will certainly yield useful insights."
- Why unresolved: Model ranked first in automatic evaluation but third under LLM-as-a-Judge protocol; the specific stylistic dimensions causing this discrepancy are unidentified
- What evidence would resolve it: Systematic correlation analysis between automatic metrics vs. LLM-as-a-Judge scores, paired with human annotation of style dimensions (register, naturalness, tone shift subtlety)

### Open Question 4
- Question: Can detoxification systems be made robust to creative obfuscation (masked profanities, emoji-based strategies) in low-resource languages?
- Basis in paper: [explicit] "detoxification systems should incorporate dynamic lexicon updating and character-substitution detection" for creative obfuscation and emergent slang
- Why unresolved: Current system relies on rule-based toxic-span detection and LaBSE retrieval, which fail on character-level obfuscation; qualitative analysis already shows missed toxicity in code-switched Hinglish
- What evidence would resolve it: Evaluation on adversarially obfuscated inputs across low-resource languages, comparing baseline vs. systems augmented with character-substitution detection and dynamic lexicon updating

## Limitations

- Language Coverage Bias: Performance on low-resource languages (Tatar, Hinglish, Amharic) remains notably weaker than high-resource languages, with Amharic showing ~30% nonsensical outputs despite parallel data availability
- Synthetic Data Quality: Jaccard-based filtering (0.80-0.90 threshold) for synthetic pairs lacks semantic validation beyond lexical overlap, with acknowledged concerns about introducing bias or overfitting
- Toxicity Detection Limitations: Rule-based toxic-span annotation has documented blind spots for compound toxicity (6.7% failure rate in Hinglish) and dialectal variations, with sarcasm and coded hate speech explicitly out of scope

## Confidence

**High Confidence (Strong evidence, well-validated claims)**:
- Gemma-3-12B with LoRA fine-tuning achieves state-of-the-art performance on the PAN 2025 challenge
- Language resource tier (high vs. low-resource) is the strongest predictor of detoxification performance (η² = 0.667, p < 0.01)
- Phase 3 synthetic data augmentation significantly improves low-resource language performance (Tatar 0.43→0.56, Hinglish 0.40→0.51)

**Medium Confidence (Supported by ablation studies, but limited external validation)**:
- Chain-of-Thought prompting provides +0.088 joint score improvement through explicit reasoning decomposition
- Few-shot examples with semantic retrieval provide +0.081 improvement by conditioning on language-aware detoxification patterns
- The three-phase training strategy effectively balances high-resource preservation with low-resource specialization

**Low Confidence (Mechanism claims lack direct empirical support)**:
- Semantic neighbor enrichment meaningfully improves content preservation (no ablation provided)
- Jaccard similarity threshold of 0.80-0.90 optimally balances semantic preservation with data diversity
- LaBSE embeddings reliably capture cross-lingual detoxification strategy transferability (only indirect support from SynthDetoxM)

## Next Checks

1. **Ablate Semantic Neighbor Enrichment**: Train an identical model without LaBSE neighbor retrieval and few-shot example enrichment to isolate their contribution beyond CoT prompting. Compare performance across resource tiers to determine if semantic enrichment provides independent benefits or merely compounds few-shot effects.

2. **Synthetic Data Quality Audit**: Manually annotate 100 synthetic pairs from Phase 3 (50 accepted by Jaccard filter, 50 rejected) for grammaticality, semantic equivalence, and stylistic consistency. Compare error rates between accepted and rejected pairs to validate whether Jaccard filtering effectively removes low-quality synthetic data.

3. **Compound Toxicity Stress Test**: Construct a benchmark of 200 sentences containing multiple independent toxic elements (insults, threats, profanity) across 5 languages spanning high-to-low resource tiers. Evaluate whether the system addresses all toxic elements or exhibits the 6.7% compound toxicity failure rate observed in Hinglish, and whether this varies systematically with language resource level.