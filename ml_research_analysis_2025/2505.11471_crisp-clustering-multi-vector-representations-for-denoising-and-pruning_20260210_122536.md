---
ver: rpa2
title: 'CRISP: Clustering Multi-Vector Representations for Denoising and Pruning'
arxiv_id: '2505.11471'
source_url: https://arxiv.org/abs/2505.11471
tags:
- multi-vector
- retrieval
- query
- clustering
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of multi-vector
  models like ColBERT, which use token-level embeddings but incur high storage and
  runtime costs. The authors propose CRISP, a training method that integrates clustering
  into the end-to-end learning process to produce inherently clusterable representations.
---

# CRISP: Clustering Multi-Vector Representations for Denoising and Pruning

## Quick Facts
- **arXiv ID:** 2505.11471
- **Source URL:** https://arxiv.org/abs/2505.11471
- **Reference count:** 40
- **Primary result:** CRISP integrates K-means clustering into training to produce inherently clusterable multi-vector embeddings, achieving up to 11x compression with only a 3.6% drop in NDCG@10 on BEIR.

## Executive Summary
CRISP is a training method that addresses the computational inefficiency of multi-vector retrieval models by learning inherently clusterable representations through end-to-end integration of K-means clustering. Unlike post-hoc clustering of frozen embeddings, CRISP jointly optimizes the embedding and clustering objectives, producing centroids that serve as compact document/query representations. The method not only achieves significant compression but also acts as a denoising mechanism by grouping low-information tokens separately. Experiments on BEIR benchmarks show CRISP outperforms both fixed selection heuristics and post-hoc clustering approaches, achieving better compression-quality trade-offs and enabling query compression.

## Method Summary
CRISP integrates K-means clustering directly into the training loop of a dual-encoder multi-vector retrieval model. During forward pass, token embeddings from the Gemma 2B backbone are clustered using K-means, and the Chamfer similarity loss is computed on the resulting centroids rather than individual tokens. This forces the model to learn embeddings that naturally form meaningful clusters. The approach supports both fixed and relative cluster sizing, with variants like C8x32 (8 query clusters, 32 document clusters) achieving 2.9x compression while improving NDCG@10 by 0.4%. Training uses standard contrastive objectives with in-batch and hard negatives over 20,000 steps.

## Key Results
- CRISP achieves up to 11x reduction in document vectors with only a 3.6% drop in NDCG@10 on BEIR.
- C8x32 variant improves NDCG@10 by 0.4% while reducing vectors by 2.9x, outperforming unpruned models.
- CRISP significantly outperforms post-hoc clustering and fixed selection heuristics across BEIR tasks.
- The learned clustering acts as a denoising mechanism, improving performance on noisy datasets like ArguAna.

## Why This Works (Mechanism)

### Mechanism 1: End-to-End Learned Clusterability
Integrating clustering into end-to-end training produces inherently more clusterable representations than post-hoc clustering. K-means is applied to token embeddings during training, with the loss computed on centroids, forcing the model to learn embeddings that group cleanly by semantic similarity. This couples representation learning with the clustering objective, avoiding sub-optimal clusters formed on frozen embeddings.

### Mechanism 2: Denoising Through Semantic Clustering
Clustering semantically similar token vectors into centroids can improve retrieval quality by filtering noisy information. K-means groups low-information tokens (e.g., stopwords, punctuation) into distinct clusters. During Chamfer similarity, these low-content clusters show no strong preference for any document, effectively reducing their contribution to the final relevance score.

### Mechanism 3: Superior Compression-Quality Trade-off
A learned clustering approach achieves a superior compression-quality trade-off compared to fixed heuristics or post-hoc methods. Instead of pruning tokens by position or at a fixed rate, CRISP dynamically groups tokens by semantic similarity, allowing flexible capacity allocation where dense semantic regions are captured by individual centroids.

## Foundational Learning

- **Multi-Vector Representations (ColBERT/MaxSim)**: Why needed here? CRISP is fundamentally a method to compress multi-vector models. Without understanding that these models represent a query/document as a set of token-level embeddings and use Chamfer Similarity scoring, the goal and mechanism of CRISP are unintelligible. **Quick check:** Given a query with embeddings $Q$ and a document with embeddings $D$, what is the formula for their Chamfer Similarity?

- **K-Means Clustering**: Why needed here? The core of CRISP's compression technique is applying K-means to token embeddings and using the resulting centroids as new representations. A basic grasp of how K-means partitions data and defines a centroid is required. **Quick check:** If you have 100 token embeddings and apply K-means with k=10, how many vectors will represent that text after clustering?

- **Training with a Differentiable Proxy**: Why needed here? K-means is not a differentiable operation. Understanding how CRISP achieves end-to-end learning requires understanding that the model is trained with the loss computed on the output of the clustering step. **Quick check:** In CRISP's training loop, is the clustering operation itself modified by gradients, or is the gradient computed based on the centroids produced by the clustering?

## Architecture Onboarding

- **Component map**: Text input -> Gemma 2B dual-encoder -> Token embeddings -> K-means clustering -> Centroids -> Chamfer similarity loss
- **Critical path**: The forward pass flows from text input through the dual-encoder to token embeddings. The critical step is K-means clustering, which transforms these embeddings into centroids. The loss is computed on these centroids, and gradients are backpropagated to encoder weights.
- **Design tradeoffs**: Fixed vs. Relative Clustering uses constant vs. adaptive cluster counts; Compression vs. Quality involves choosing smaller k for higher compression but increased quality risk.
- **Failure signatures**: Poor hyperparameter tuning (critical for learning rate and L2 normalization); Loss of nuance from overly aggressive compression; Missing instruction prefixes causing quality drops.
- **First 3 experiments**:
  1. Replicate baseline comparison: Fine-tune baseline multi-vector model and CRISP model (e.g., C8x32) on a small dataset, comparing performance and storage size.
  2. Ablate clustering in training: Train a model as standard multi-vector model, then apply K-means post-hoc. Compare to CRISP-trained model to measure end-to-end benefit.
  3. Probe the denoising hypothesis: Train CRISP model on a noisy dataset (e.g., ArguAna) and compare performance against baseline. Inspect clusters to see if low-information tokens are grouped into separate, low-impact clusters.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can CRISP be extended to adaptively learn the optimal number of clusters per query/document rather than fixing k in advance?
- **Basis in paper**: The authors state in limitations: "CRISP fixes the maximum number of clusters k to be used during training in advance. This prevents the model from learning the optimal number of clusters... We leave the exploration of methods which adapt the number of clusters... to future work."
- **Why unresolved**: Current CRISP variants use predetermined cluster counts (e.g., C4x8, C8x32), which may be suboptimal for inputs of varying length or complexity.
- **What evidence would resolve it**: A modified CRISP with learned/dynamic k selection achieving comparable compression-quality trade-offs without manual tuning of cluster counts per dataset.

### Open Question 2
- **Question**: How do alternative clustering mechanisms (hierarchical, spectral, density-based) compare to K-means within the CRISP training framework?
- **Basis in paper**: The authors explicitly defer this: "We leave the exploration of... alternative clustering mechanisms, to future work."
- **Why unresolved**: Only K-means is evaluated; other algorithms may yield different clustering properties or better handle semantic structure.
- **What evidence would resolve it**: Systematic comparison of multiple clustering algorithms trained end-to-end under identical conditions, reporting compression-quality Pareto curves.

### Open Question 3
- **Question**: Can CRISP be combined with existing quantization techniques (e.g., centroid-based quantization in ColBERTv2) for additive compression gains?
- **Basis in paper**: Related work discusses quantization as a separate efficiency strategy, but CRISP experiments use full-precision embeddings without quantization.
- **Why unresolved**: It is unclear whether learned clusterability and quantization are orthogonal or interact negatively.
- **What evidence would resolve it**: Experiments applying both CRISP and quantization jointly, measuring storage, latency, and quality trade-offs.

## Limitations
- CRISP fixes the maximum number of clusters k in advance, preventing the model from learning the optimal number of clusters per input.
- The K-means clustering step during training is non-differentiable, and the paper does not fully specify how gradients are propagated.
- Evaluation focuses on BEIR tasks; generalizability to all retrieval tasks is not directly tested.

## Confidence
- **High Confidence**: CRISP achieves superior compression-quality trade-offs compared to post-hoc clustering and fixed heuristics (e.g., C8x32 improving NDCG@10 by 0.4% while reducing vectors by 2.9x).
- **Medium Confidence**: The denoising hypothesis is supported by qualitative inspection of cluster behavior on ArguAna, but more rigorous quantitative validation is needed.
- **Medium Confidence**: The claim that CRISP learns "inherently clusterable" representations is logically sound given the training procedure, but a direct ablation study comparing end-to-end vs. post-hoc clustering on frozen embeddings is not provided.

## Next Checks
1. **Replicate the Learned vs. Post-Hoc Clustering Ablation**: Train a model to convergence as a standard multi-vector model, then apply K-means clustering post-hoc. Compare its performance on BEIR to a CRISP-trained model with the same number of clusters to isolate the benefit of end-to-end learned clusterability.
2. **Stress-Test the Denoising Hypothesis**: Conduct a controlled experiment where a CRISP model and a baseline are trained on a dataset with artificially injected noise (e.g., random tokens). Measure not only the retrieval quality difference but also the distribution of cluster assignments for the noisy vs. clean tokens.
3. **Validate the K-Means Implementation**: The non-differentiability of K-means is a potential failure point. Implement and test a "CRISP" model where the clustering step is removed during training (i.e., use all token embeddings) to confirm that the specific design of CRISP, not just longer training, is responsible for the performance gains.