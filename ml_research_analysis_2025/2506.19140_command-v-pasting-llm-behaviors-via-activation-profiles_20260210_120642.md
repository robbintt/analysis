---
ver: rpa2
title: 'Command-V: Pasting LLM Behaviors via Activation Profiles'
arxiv_id: '2506.19140'
source_url: https://arxiv.org/abs/2506.19140
tags:
- e-06
- e-05
- arxiv
- e-08
- e-07
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Command-V, a method for transferring behaviors
  between large language models without backpropagation or training data. Command-V
  profiles layer activations on shared prompts, derives linear converters between
  corresponding layers, and applies donor adapter interventions in the recipient's
  activation space.
---

# Command-V: Pasting LLM Behaviors via Activation Profiles

## Quick Facts
- **arXiv ID**: 2506.19140
- **Source URL**: https://arxiv.org/abs/2506.19140
- **Reference count**: 40
- **Key outcome**: Command-V transfers behaviors between LLMs without backpropagation or training data by profiling activation spaces and applying linear converters. In three case studies, it matches or exceeds direct finetuning while using orders of magnitude less compute.

## Executive Summary
Command-V introduces a novel method for transferring behaviors between large language models without backpropagation or original training data. By profiling layer activations on shared prompts, deriving linear converters between corresponding layers, and applying donor adapter interventions in the recipient's activation space, Command-V achieves comparable or superior results to direct finetuning across three case studies: safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning. The method uses orders of magnitude less compute than traditional finetuning while demonstrating effective cross-model behavior transfer.

## Method Summary
Command-V works by first extracting activation profiles from donor and recipient models on shared prompts (typically 1030 LIMA prompts), then training low-rank DiReFT adapters on the donor model for the target behavior. Linear converters between corresponding layers are computed via Moore-Penrose pseudoinverse. During inference, recipient activations are converted to donor space, the intervention applied, and results converted back and added to the recipient's residual stream. The method transfers DiReFT adapters (rank=8, every other layer) and matches layers via proportional depth heuristic.

## Key Results
- Safety-refusal enhancement: Transferred refusal adapter from Llama3.2-3B to Llama3.1-8B reduced ASR from 36.85% to 20.40%, outperforming direct training (28.55%)
- Jailbreak facilitation: Transferred jailbreak adapter from Llama3.1-8B to Llama3.2-3B increased ASR from 36.85% to 44.75%, exceeding direct training results
- Chain-of-thought reasoning: Transferred CoT adapter from Qwen2.5-7B to Llama3.2-3B increased reasoning output length from 8.57 to 16.78 words

## Why This Works (Mechanism)

### Mechanism 1: Linear Cross-Model Representation Alignment
- Claim: Corresponding layers across different transformer architectures encode semantically similar information in linearly mappable subspaces.
- Mechanism: By collecting activation matrices X ∈ ℝ^(N×dR) and Y ∈ ℝ^(N×dD) from shared prompts, pseudoinverse converters (CR→D = X†Y, CD→R = Y†X) minimize reconstruction error between representation spaces.
- Core assumption: The last-token activations capture task-relevant semantic structure that generalizes across model families.
- Evidence anchors:
  - [abstract]: "Command-V profiles layer activations on shared prompts, derives linear converters between corresponding layers"
  - [Section 3.2]: "The converter directly maps between representation spaces using linear transformations computed by solving the least squares problem"
  - [corpus]: Related work "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences" (arXiv:2510.13900) supports that finetuning creates interpretable activation biases, though cross-family transfer evidence is limited.
- Break condition: Highly dissimilar architectures (e.g., different activation functions like GELU-tanh vs SiLU) show higher cycle MSE—Gemma-2-2B exhibits the worst transfer properties in Figure 5.

### Mechanism 2: Depth-Proportional Layer Correspondence
- Claim: Functional layer roles scale approximately linearly with relative depth across transformer models.
- Mechanism: Layer mapping uses lR = ⌊α·lD⌋ where α = |LR|/|LD|, matching early layers to early layers proportionally.
- Core assumption: Transformer semantic processing follows a depth-ordered pipeline (syntax → semantics → task-specific), as suggested by prior interpretability work (Tenney et al., 2019a).
- Evidence anchors:
  - [Section 3.2]: "Layer-specific functionalities tend to scale linearly with depth, meaning the second layer of a shallow network tends to correspond to the fourth layer of a network twice as deep"
  - [Appendix A.2]: MSE-minimizing layer matching strategies favor early layers but perform poorly downstream, contradicting semantic-layer hypotheses
  - [corpus]: No direct corpus evidence validates this heuristic; it remains an unproven assumption.
- Break condition: Alternative layer-matching strategies (MSE minimization) produce inconsistent results—Figure 6 shows later layers are inherently harder to approximate.

### Mechanism 3: Residual Adapter Transfer via Intervention Space Projection
- Claim: A low-rank intervention ΔI learned in one model's activation space can be applied in another model's space via the converter transformation.
- Mechanism: The transfer equation h^lR_intervened = h^lR + CD→R(ΔI^lD(CR→D(h^lR))) projects recipient activations to donor space, applies the intervention, and projects back.
- Core assumption: The converter approximately preserves the intervention's functional effect—i.e., CD→R(ΔI^lD(CR→D(h))) ≈ ΔI^lR(h) for equivalent h.
- Evidence anchors:
  - [Section 3.2]: Equation (6) formally defines the three-step intervention transfer process
  - [Section 4.2]: Transferred refusal adapter from Llama3.2-3B to Llama3.1-8B reduced ASR from 36.85% to 20.40%, outperforming direct training (28.55%)
  - [corpus]: "Neural Organ Transplantation" (arXiv:2601.13580) explores checkpoint-based layer reuse but uses weight-space transfer, not activation-space mapping.
- Break condition: Tokenizer mismatch and approximation error cause degraded output when transferring formatting behaviors requiring exact token generation (Section 4.4).

## Foundational Learning

- Concept: **Residual Stream Architecture in Transformers**
  - Why needed here: Command-V operates on layer-output activations in the residual stream; understanding how information flows through skip connections clarifies why interventions at specific layers affect behavior.
  - Quick check question: Can you explain why adding a vector to the residual stream at layer 15 would affect the final output, given skip connections?

- Concept: **Parameter-Efficient Finetuning (PEFT) / Representation Finetuning (ReFT)**
  - Why needed here: The method transfers DiReFT adapters—low-rank interventions on hidden states. Understanding LoRA/reFT helps you grasp what's being transferred.
  - Quick check question: If a ReFT adapter has rank 8 operating on a 4096-dim activation, how many parameters does it add per layer?

- Concept: **Moore-Penrose Pseudoinverse and Least-Squares Solutions**
  - Why needed here: Converters are derived via pseudoinverse (X†Y) to solve the overdetermined least-squares mapping between activation spaces.
  - Quick check question: Given X ∈ ℝ^(100×3072) and Y ∈ ℝ^(100×4096), what are the dimensions of CR→D = X†Y?

## Architecture Onboarding

- Component map:
  1. **Activation Profiling Module**: Passes profiling prompts (N=1030 from LIMA) through both models, extracts last-token activations per layer → stores A^lD ∈ ℝ^(N×dD), A^lR ∈ ℝ^(N×dR)
  2. **Layer Matcher**: Computes depth ratio α and maps donor layers to recipient layers
  3. **Converter Derivation**: For each matched layer pair, computes CR→D and CD→R via pseudoinverse (no backprop, CPU-efficient)
  4. **Inference Hook**: During recipient forward pass, intercepts activations at matched layers, applies converted intervention, adds to residual stream

- Critical path: Profiling prompts → activation extraction → pseudoinverse computation → converter matrices → inference-time hooks

- Design tradeoffs:
  - Linear vs non-linear converters: Paper uses linear (fast, no training); non-linear could improve fit but adds complexity
  - Layer matching strategy: Depth-proportional heuristic vs MSE-minimizing (latter performs worse despite lower reconstruction loss)
  - Profiling dataset: Task-agnostic (LIMA) vs task-specific; paper shows LIMA works for jailbreak/refusal/CoT, but may not generalize to all domains

- Failure signatures:
  - Output collapse: Recipient generates incoherent content, especially smaller models or cross-family transfers
  - Overrefusal: Benign queries resembling jailbreaks get refused (Table 1: 14.48% overrefusal for Llama 3.2 3B with ⌘V)
  - Format degradation: Models fail to follow JSON output requirements after CoT adapter transfer (up to 28.8% non-compliant)
  - Language switching: Qwen models sometimes output in wrong language post-transfer

- First 3 experiments:
  1. **Sanity check**: Profile Llama3.1-8B and Llama3.2-3B on 100 LIMA prompts; compute converters and verify cycle-consistency MSE is < 10^-4 (Figure 5 baseline)
  2. **Same-family transfer**: Train refusal adapter on Llama3.2-3B using WildJailbreak subset (1000 examples), transfer to Llama3.1-8B, measure ASR reduction on held-out adversarial prompts
  3. **Cross-family boundary test**: Attempt transfer from Qwen2.5-7B to Gemma-2-2B; expect high failure rate based on Figure 5 showing Gemma as outlier with highest forward/cycle MSE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we predict the transferability of specific behaviors between arbitrary donor-recipient model pairs prior to application?
- **Basis in paper:** [explicit] The authors state that "predicting good model pairs and task performance transferability remains an open question."
- **Why unresolved:** Empirical results show high variance; transfer often fails or degrades utility when architectures diverge significantly (e.g., Gemma-2-2B resisting jailbreak transfers), but the specific geometrical or architectural indicators for success are unidentified.
- **What evidence would resolve it:** A theoretical framework or metric derived from activation profiles that correlates with downstream task performance, allowing users to forecast transfer success rates without running full inference.

### Open Question 2
- **Question:** Do non-linear transformation functions improve the fidelity of behavior transfer compared to the current linear least-squares approach?
- **Basis in paper:** [explicit] The discussion notes that "more intricate converters beyond the least-squares method we use may improve performance. One could explore non-linear maps between the representation spaces..."
- **Why unresolved:** The paper relies exclusively on linear pseudoinverse solutions to bridge activation spaces, which may fail to capture complex topological differences between models.
- **What evidence would resolve it:** Experiments demonstrating that trained non-linear converters (e.g., small MLPs) reduce the performance gap between the donor model and the recipient model on complex tasks like Chain-of-Thought reasoning.

### Open Question 3
- **Question:** Can multiple specialized adapters be composed within a single recipient model to build generalist capabilities without interference?
- **Basis in paper:** [explicit] The authors suggest the method "motivates work on task composition" and hypothesizes that "training small specialist adapters could lead to cheap ways to build high-performing generalists."
- **Why unresolved:** The paper evaluates behaviors in isolation; it remains unclear if applying multiple distinct activation interventions (e.g., safety plus reasoning) simultaneously results in superposition or destructive interference.
- **What evidence would resolve it:** A demonstration of a single recipient model successfully integrating three or more transferred behaviors (e.g., refusal, CoT, and specific formatting) with performance comparable to individual transfers.

## Limitations

- Cross-family generalization uncertainty: The paper shows successful same-family transfers but cross-family results vary significantly, with Gemma-2-2B exhibiting notably poor transfer properties.
- Adapter transfer fidelity uncertainty: While linear converters minimize reconstruction error between activation spaces, there's no guarantee this preserves the functional intent of the donor adapter.
- Evaluation scope limitation: The three case studies focus on safety-related behaviors (refusal, jailbreak, CoT reasoning), representing a narrow slice of potential behaviors.

## Confidence

- **High confidence**: Same-family transfer effectiveness for safety behaviors, computational efficiency advantage over direct finetuning
- **Medium confidence**: Cross-family transfer success (variable), generalizability to non-safety behaviors
- **Low confidence**: Layer matching heuristic validity across all architecture types, pseudoinverse converters preserving semantic intent

## Next Checks

1. **Layer matching ablation study**: Systematically compare depth-proportional matching against MSE-minimizing layer matching across multiple behavior types and model pairs. Measure both reconstruction MSE and downstream task performance to validate whether lower MSE correlates with better transfer.

2. **Non-linear converter experiment**: Implement and test non-linear converter models (e.g., small MLPs) between activation spaces. Compare transfer effectiveness and computational overhead against the linear pseudoinverse approach to determine if linear approximation is sufficient.

3. **Cross-architecture stress test**: Design a controlled experiment transferring a simple, well-defined behavior (e.g., JSON formatting or specific stylistic pattern) across diverse architectures including GELU, SiLU, and other activation functions. This would reveal whether fundamental architectural differences limit the method's applicability.