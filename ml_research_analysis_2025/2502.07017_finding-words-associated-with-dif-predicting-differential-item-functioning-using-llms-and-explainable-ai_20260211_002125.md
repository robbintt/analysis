---
ver: rpa2
title: 'Finding Words Associated with DIF: Predicting Differential Item Functioning
  using LLMs and Explainable AI'
arxiv_id: '2502.07017'
source_url: https://arxiv.org/abs/2502.07017
tags:
- item
- group
- items
- were
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies explainable AI (XAI) methods to large language
  models (LLMs) to identify words associated with differential item functioning (DIF).
  The authors fine-tune encoder-based Transformer LLMs to predict DIF from item text,
  then use SHAP to attribute token-level importance.
---

# Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI

## Quick Facts
- arXiv ID: 2502.07017
- Source URL: https://arxiv.org/abs/2502.07017
- Reference count: 21
- Primary result: LLMs with SHAP attribution identify words linked to DIF; R² 0.04-0.32 across 8 focal/ref groups, often reflecting minor sub-domains.

## Executive Summary
This paper proposes a novel method to predict and interpret differential item functioning (DIF) using large language models (LLMs) combined with explainable AI (XAI). By fine-tuning encoder-based Transformer LLMs on item text to predict DIF, then applying SHAP for token-level attribution, the authors identify specific words associated with DIF. The approach provides interpretable insights into why items function differently across groups, moving beyond traditional statistical detection methods. The findings suggest many DIF-associated words reflect minor test sub-domains rather than construct-irrelevant bias, offering a tool for screening item text during development or interpreting traditional DIF results.

## Method Summary
The authors fine-tune encoder-based Transformer LLMs to predict DIF from item text. After training, SHAP (SHapley Additive exPlanations) is applied to attribute importance to individual tokens, identifying which words are most associated with DIF. The study examines eight focal/reference group pairs, reporting prediction R² values ranging from 0.04 to 0.32. The method is tested on a single operational assessment, with qualitative inspection of high-importance tokens suggesting many DIF cases relate to minor sub-domains rather than construct-irrelevant bias.

## Key Results
- Prediction R² ranged from 0.04 to 0.32 across eight focal/reference group pairs.
- SHAP-identified words often reflect minor test sub-domains rather than construct-irrelevant bias.
- The approach offers a screening tool for item development and a means to interpret traditional DIF results by highlighting key words.

## Why This Works (Mechanism)
The method works by leveraging the semantic understanding of LLMs to capture contextual and linguistic features in item text that correlate with DIF. SHAP then provides a principled, model-agnostic way to attribute token-level importance, making the model's predictions interpretable. This combination allows researchers to move beyond detecting DIF statistically to understanding which specific words or phrases contribute to group differences.

## Foundational Learning
- **Differential Item Functioning (DIF)**: The phenomenon where test items function differently for different groups, even when ability is held constant. Needed to understand the problem being addressed.
- **Large Language Models (LLMs)**: Neural networks trained on vast text corpora to understand and generate language. Quick check: Can the model process and extract meaning from item text?
- **SHAP (SHapley Additive exPlanations)**: An XAI method that attributes feature importance based on game theory principles. Quick check: Are SHAP attributions stable and meaningful for token-level interpretation?
- **Encoder-based Transformers**: A type of LLM architecture that processes input text and produces contextual embeddings. Quick check: Does the model capture relevant linguistic cues for DIF prediction?

## Architecture Onboarding
- **Component map**: Item text -> LLM fine-tuning -> DIF prediction -> SHAP attribution -> Token importance scores
- **Critical path**: Fine-tune LLM on labeled DIF data → Apply SHAP to trained model → Interpret high-importance tokens
- **Design tradeoffs**: Trade off between model complexity (for accurate DIF prediction) and interpretability (via SHAP); balance between token-level detail and item-level DIF patterns.
- **Failure signatures**: Low R² values suggest item-level or contextual factors not captured by text; unstable SHAP attributions indicate model sensitivity or overfitting.
- **First experiments**: (1) Evaluate model performance on held-out test items; (2) Compare SHAP attributions to expert-coded item content; (3) Test model on at least two additional assessments with known DIF patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- R² values (0.04-0.32) indicate substantial variability and suggest many DIF cases involve item-level or contextual factors not captured by token-level text features.
- The attribution of DIF to minor sub-domains is interpretive and requires systematic validation against independent content analyses.
- The absence of stability checks for SHAP attributions and lack of head-to-head comparisons to traditional DIF methods limits confidence in the tool's incremental value.

## Confidence
- **High**: The technical pipeline (fine-tuning LLMs → SHAP attribution) is correctly implemented and reproducible.
- **Medium**: The finding that many DIF-associated words reflect minor sub-domains is plausible but requires broader validation.
- **Low**: Claims about replacing or supplementing traditional DIF methods in operational settings are premature without comparative studies.

## Next Checks
1. Apply the model to at least two additional, independently developed assessments with known DIF patterns to test external validity.
2. Compare SHAP-identified words against expert-coded item content analyses to assess whether attributions align with theoretically expected sources of DIF.
3. Evaluate the stability of token-level SHAP attributions across multiple training runs and alternative XAI techniques (e.g., LIME, Integrated Gradients).