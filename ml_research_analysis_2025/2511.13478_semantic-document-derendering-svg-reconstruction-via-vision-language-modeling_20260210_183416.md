---
ver: rpa2
title: 'Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling'
arxiv_id: '2511.13478'
source_url: https://arxiv.org/abs/2511.13478
tags:
- image
- raster
- text
- slide
- assets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SliDer is a VLM-based framework for semantic document derendering,
  transforming raster slides into editable SVG representations. It leverages vision-language
  models to detect and extract attributes of individual image and text elements, then
  iteratively refines its predictions to improve reconstruction fidelity.
---

# Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling

## Quick Facts
- **arXiv ID:** 2511.13478
- **Source URL:** https://arxiv.org/abs/2511.13478
- **Reference count:** 40
- **Primary result:** VLM-based framework SliDer achieves 0.069 LPIPS for SVG reconstruction, outperforming zero-shot VLMs (0.118) and geometric vectorization (0.169).

## Executive Summary
SliDer is a Vision-Language Model (VLM)-based framework for converting raster slide images into structured, editable SVG representations. It leverages fine-tuned VLMs to detect and extract attributes of text and image elements, then iteratively refines predictions for improved fidelity. The approach uses a novel Slide2SVG dataset of real scientific presentations and achieves state-of-the-art reconstruction quality. Human evaluators strongly preferred SliDer over zero-shot baselines, confirming superior visual fidelity and editability.

## Method Summary
SliDer fine-tunes a VLM (Gemini-1.5-Flash or Gemma 3 12B) on the Slide2SVG dataset using LoRA adapters. The model takes a raster slide plus auxiliary contexts (skeleton, YOLO-guided partial template, or initial predictions) and outputs SVG code. Training uses 20K augmented samples with 3 contexts, and inference includes optional iterative refinement. Post-processing cleans backgrounds via TELEA inpainting and crops image assets. The pipeline requires training a YOLOv8 detector for spatial priors and relies on precise SVG coordinate normalization.

## Key Results
- Achieves 0.069 LPIPS reconstruction quality, outperforming zero-shot VLMs (0.118) and geometric vectorization (0.169).
- Human evaluators favored SliDer over the strongest zero-shot baseline in 82.9% of pairwise comparisons.
- Uses YOLO spatial priors to improve mIoU from 81.90% to 89.71% and OCR accuracy from 73.73% to 92.40%.

## Why This Works (Mechanism)

### Mechanism 1
If a Vision-Language Model (VLM) is fine-tuned to output hierarchical SVG code rather than natural language descriptions, it can semantically decompose a raster image into editable assets (text, images) better than primitive-based geometric methods.
- **Core assumption:** The VLM's pre-trained visual encoder has sufficient resolution and attention mechanisms to distinguish proximal elements (e.g., a caption near an image) without conflating them.
- **Evidence anchors:**
  - [abstract] SliDer detects and extracts attributes from individual image and text elements... outperforming geometric vectorization (0.169 LPIPS).
  - [section 4.1] The VLM takes... a raster slide... generating SVG code that simultaneously predicts spatial placement... and content styling.
  - [corpus] Related papers like *SlideAgent* focus on generation from text; SliDer inverts this process (image-to-structure), confirming the feasibility of VLM-driven structural mapping.
- **Break condition:** The mechanism fails if the raster input has severe occlusions or overlapping transparent layers where the visual encoder cannot resolve distinct object boundaries.

### Mechanism 2
If an external object detector (YOLO) provides bounding box priors to the VLM, the model’s spatial accuracy and convergence speed improve significantly compared to generating coordinates from scratch.
- **Core assumption:** The external detector is robust to the specific domain (scientific slides) and its bounding box errors are smaller than the VLM's native spatial hallucinations.
- **Evidence anchors:**
  - [section 4.1] "Partial Template: ...providing the layout, this context focuses the model’s task on learning to infer stylistic properties."
  - [section 6.4] "Comparing the model with priors... versus without, the mIoU jumps from 81.90% to 89.71%."
  - [corpus] *Weak/missing:* The corpus neighbors do not detail YOLO integration for SVG, suggesting this specific spatial scaffolding is a novel contribution of this architecture.
- **Break condition:** The mechanism degrades if the object detector hallucinates boxes or misses small text elements, as the VLM may trust the priors blindly or struggle to correct the layout.

### Mechanism 3
If the model is trained to refine an existing (potentially flawed) SVG draft using the original raster as context, it can self-correct structural and stylistic errors via an iterative inference loop.
- **Core assumption:** The VLM can attend to both the "current state" (draft SVG code) and the "goal state" (raster pixels) simultaneously to compute the error signal implicitly.
- **Evidence anchors:**
  - [abstract] "Crucially, the model iteratively refines its predictions during inference... generating SVG code that more faithfully reconstructs the original."
  - [section 6.4] "One step of iterative refinement provides a consistent boost... LPIPS dropping from 0.072 to 0.069."
  - [corpus] *Weak/missing:* Iterative self-correction is not a dominant theme in the retrieved corpus (mostly single-pass generation), highlighting this as a specific design choice for derendering fidelity.
- **Break condition:** The loop may fail to converge or introduce new artifacts if the model learns to "smooth" details rather than sharpen them, or if the error signal is too subtle for the VLM.

## Foundational Learning

- **Concept: Scalable Vector Graphics (SVG) DOM Structure**
  - **Why needed here:** The model outputs code, not pixels. Understanding the hierarchical structure (groups `<g>`, images `<image>`, and text containers `<foreignObject>`) is required to interpret the output and debug formatting errors.
  - **Quick check question:** Can you identify where font styling attributes live in an SVG snippet containing a `<foreignObject>`?

- **Concept: Object Detection & Bounding Box IoU**
  - **Why needed here:** The architecture relies on YOLO priors for spatial layout. Understanding Intersection over Union (IoU) is essential to evaluate if the "Partial Template" is actually helping or hindering the VLM.
  - **Quick check question:** If a YOLO box covers 90% of a text area but extends 50% into an image, is that a good prior for the VLM?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The paper mentions fine-tuning large models (Gemini/Gemma) for the task. Understanding how LoRA adapters work is critical for training the system without retraining the full VLM backbone.
  - **Quick check question:** If the fine-tuned model hallucinates text, does the error likely reside in the frozen VLM weights or the LoRA adapter weights?

## Architecture Onboarding

- **Component map:** Raster Image + Prompt + Auxiliary Context → YOLOv8 (spatial prior) → VLM (Gemini/Gemma) fine-tuned via LoRA → Iterative Inference Loop → TELEA Inpainting + Cropping → SVG Output
- **Critical path:** The dataset creation pipeline is the most fragile link. The PDF-to-Figma-to-SVG conversion, followed by coordinate normalization, defines the ground truth. If the ground truth SVGs are messy (e.g., absolute vs. relative coordinates), the VLM will learn unstable mapping.
- **Design tradeoffs:**
  - **Partial vs. Skeleton Template:** Partial (YOLO-guided) yields better spatial accuracy (mIoU 89% vs 81%) but introduces dependency on an external detector. Skeleton is purely end-to-end but struggles with layout.
  - **Iterative Refinement:** Increases inference cost (multiple VLM calls) for marginal perceptual gain (LPIPS 0.072 → 0.069). Best reserved for "High Quality" mode.
- **Failure signatures:**
  - **Semantic Drift:** The VLM OCRs text correctly but places it in the wrong text box (swapping title and caption).
  - **Inpainting Artifacts:** If bounding boxes are slightly too large, the TELEA algorithm erodes parts of the background or text while trying to isolate assets.
  - **Token Limit:** Complex slides with many assets may exceed the output token limit of the VLM, resulting in truncated SVG files.
- **First 3 experiments:**
  1. **Spatial Prior Ablation:** Run inference with only "Skeleton" vs. "YOLO Partial" templates to measure the delta in mIoU and OCR accuracy on the test set.
  2. **Refinement Loop Analysis:** Compare 1-step vs. 2-step iterative refinement on a subset of "hard" slides (dense text) to check for convergence or error amplification.
  3. **End-to-End Editability Test:** Take 5 derendered SVGs, edit the text content manually in a code editor (e.g., change "Method" to "Approach"), and re-render to verify that layout stability is preserved.

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the specific trade-offs between derendering quality and the computational cost of multi-step iterative refinement?
  - **Basis in paper:** [explicit] The authors state that "Further investigation into the trade-offs between derendering quality and the computational cost of multi-step iterative refinement also presents a valuable research direction."
  - **Why unresolved:** Due to the high computational cost of generating training data, the current models are trained with only a single refinement step, leaving the potential benefits of deeper refinement unexplored.
  - **What evidence would resolve it:** A comprehensive ablation study benchmarking performance gains (e.g., LPIPS reduction) against inference latency and token consumption across varying numbers of refinement steps.

- **Open Question 2:** Can SliDer be extended to effectively handle primitive vector shapes (e.g., rectangles, paths) in addition to text and images?
  - **Basis in paper:** [explicit] The Appendix notes that the "current implementation of SliDer exclusively handles text and image elements" and suggests expanding the dataset to include "primitive vector shapes."
  - **Why unresolved:** The Slide2SVG dataset currently limits elements to text and images, meaning the model's ability to generate code for other geometric primitives remains untested.
  - **What evidence would resolve it:** Expanding the Slide2SVG dataset to include labeled vector shapes and demonstrating that the fine-tuned VLM can accurately predict the attributes and code for these primitives.

- **Open Question 3:** Can the framework generalize to denser, more complex document layouts such as scientific posters or infographics?
  - **Basis in paper:** [explicit] The Conclusion lists "improving performance on layouts with higher content density" and expanding to "posters and infographics" as opportunities for future work.
  - **Why unresolved:** The current evaluation is restricted to slide documents; while challenging, they may not represent the density or structural complexity found in large-format scientific posters.
  - **What evidence would resolve it:** Evaluating the model's reconstruction fidelity (using mIoU and OCR accuracy) on a curated dataset of high-density posters or infographics without architecture changes.

## Limitations
- The framework is limited to text and image elements; primitive vector shapes are not yet supported.
- Training data creation is computationally expensive, restricting model training to single-step refinement.
- The approach may struggle with layouts that have high content density or complex overlapping elements.

## Confidence
- **High:** LPIPS performance claims (0.069 vs. 0.118 vs. 0.169) and human preference (82.9%) are directly supported by Table 1 and 2.
- **Medium:** The efficacy of iterative refinement is supported by ablation results (0.072 → 0.069 LPIPS) but depends on unstated inference parameters.
- **Low:** The robustness of YOLO spatial priors across diverse slide layouts is implied but not extensively validated beyond mIoU improvements.

## Next Checks
1. **Spatial Prior Ablation:** Run inference with only "Skeleton" vs. "YOLO Partial" templates to measure the delta in mIoU and OCR accuracy on the test set.
2. **Refinement Loop Analysis:** Compare 1-step vs. 2-step iterative refinement on a subset of "hard" slides (dense text) to check for convergence or error amplification.
3. **End-to-End Editability Test:** Take 5 derendered SVGs, edit the text content manually in a code editor (e.g., change "Method" to "Approach"), and re-render to verify that layout stability is preserved.