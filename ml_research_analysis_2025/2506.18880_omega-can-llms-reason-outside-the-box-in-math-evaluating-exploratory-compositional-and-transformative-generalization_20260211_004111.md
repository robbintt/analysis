---
ver: rpa2
title: 'OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional,
  and Transformative Generalization'
arxiv_id: '2506.18880'
source_url: https://arxiv.org/abs/2506.18880
tags:
- problems
- problem
- reasoning
- generalization
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OMEGA, a benchmark for evaluating three
  types of mathematical generalization in LLMs: exploratory (scaling complexity),
  compositional (combining skills), and transformative (novel strategies). Using templated
  problem generators across six domains, OMEGA enables controlled experiments with
  matched training-test pairs.'
---

# OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization

## Quick Facts
- arXiv ID: 2506.18880
- Source URL: https://arxiv.org/abs/2506.18880
- Reference count: 40
- Primary result: Current LLMs struggle with genuine mathematical creativity despite strong pattern-matching abilities

## Executive Summary
OMEGA introduces a benchmark evaluating three types of mathematical generalization in LLMs: exploratory (scaling complexity), compositional (combining skills), and transformative (novel strategies). Using templated problem generators across six domains, OMEGA enables controlled experiments with matched training-test pairs. Experiments show that while RL improves exploratory generalization, performance gains are limited for compositional and transformative tasks. State-of-the-art models degrade sharply as problem complexity increases, with CoT traces revealing error spirals and inefficient verification. Complexity analysis confirms that reasoning failures are not due to context limits but reflect fundamental reasoning limitations.

## Method Summary
OMEGA uses templated problem generators with explicit complexity measures to create controlled training-test pairs. The framework trains models on low-complexity instances and tests on higher-complexity variants to isolate generalization capabilities. GRPO fine-tuning is applied to 1,000 training problems per setting, with evaluations on in-distribution, exploratory, compositional, and transformative test sets. The benchmark spans arithmetic, algebra, combinatorics, number theory, geometry, and logic/puzzles, with problems calibrated to AIME difficulty levels.

## Key Results
- RL improves exploratory generalization but shows limited gains for compositional and transformative tasks
- State-of-the-art models degrade sharply as problem complexity increases, approaching zero accuracy at highest levels
- CoT traces reveal error spirals where early mistakes cascade into increasingly flawed conclusions
- Error analysis shows ~38% of incorrect responses involve "correct → incorrect" reasoning shifts

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Controlled Training-Test Pairs Enable Isolated Generalization Analysis
By constraining training data to low-complexity instances and testing on higher complexity, OMEGA isolates whether models extend known algorithms or merely memorize fixed-complexity solutions. This controlled approach enables causal attribution of reasoning failures to generalization limits rather than memorization.

### Mechanism 2: RL Reinforces In-Distribution Patterns but Does Not Induce Novel Strategy Discovery
RL optimizes for reward signals on training distribution, reinforcing successful patterns but providing no gradient toward strategies not represented in the base model's initial policy. This explains why RL improves exploratory generalization but fails on transformative tasks requiring qualitatively new approaches.

### Mechanism 3: Autoregressive Error Compounding Creates Reasoning Spirals
Transformers generate tokens autoregressively; early errors condition later reasoning, creating feedback loops where incorrect intermediate results cascade into increasingly flawed conclusions. This explains why incorrect responses consume more tokens than correct ones.

## Foundational Learning

- **Boden's Creativity Typology**: OMEGA's three generalization axes are directly inspired by Boden's framework distinguishing between extending known patterns vs. combining them vs. inventing new approaches. Quick check: Can you explain why "solving a dodecagon problem after training on octagons" is exploratory rather than transformative generalization?

- **Out-of-Distribution (OOD) Generalization**: The entire benchmark evaluates generalization beyond training distribution, requiring understanding of distribution shift and why ID performance doesn't guarantee OOD transfer. Quick check: If a model achieves 90% on training complexity levels 1-2 but 10% on test level 5, what does this reveal about its learned strategy?

- **GRPO (Group Relative Policy Optimization)**: All RL experiments use GRPO; understanding how reward signals shape policy is necessary to interpret why certain generalization types improve while others plateau. Quick check: Why might maximizing reward on isolated skill A and skill B separately fail to improve performance on tasks requiring A⊕B composition?

## Architecture Onboarding

- **Component map**: Templated generators (40 total) → Complexity calibration (5 levels per template) → Verification layer (symbolic/numerical/graphical solvers) → Training-test pair constructor → GRPO fine-tuning → Evaluation on ID/exploratory/compositional/transformative sets

- **Critical path**: 1) Define problem template τ with parameter space Θτ and complexity measure δ 2) Generate training instances with δ ≤ δ₀ 3) Construct test pairs across generalization types 4) Run GRPO fine-tuning on training set 5) Evaluate on held-out sets

- **Design tradeoffs**: Synthetic vs. natural problems (controlled experiments vs. real diversity), single-scope templates vs. multi-step problems (isolated attribution vs. authentic problem-solving), complexity measure fidelity (domain-specific metrics vs. imperfect calibration)

- **Failure signatures**: Exploratory plateau (accuracy gains diminish at level 5+), compositional gap (isolated skills >69% but composed tasks <10%), transformative collapse (OOD transformative accuracy near 0% after RL), error spiral indicator (incorrect responses consume more tokens, ~38% "correct → incorrect" shifts)

- **First 3 experiments**: 1) Reproduce complexity degradation curve across five complexity levels 2) Ablate compositional skill pairing to isolate semantic alignment effects 3) Trace error spiral timing by annotating incorrect CoT traces

## Open Questions the Paper Calls Out

1. What is the relationship between a model's prior exposure to specific mathematical domains and the effectiveness of RL fine-tuning on those domains?

2. Why do models that master individual skills in isolation fail to compose them, despite having learned the underlying components?

3. Can meta-reasoning controllers that detect strategy stalls and actively search for alternative solutions improve transformative generalization?

4. Do the observed reasoning failures—error spirals, overthinking, and heuristic preference—persist across different model architectures beyond transformer-based LLMs?

## Limitations

- The templated approach may not capture real mathematical reasoning diversity and assumes solution strategies scale monotonically with complexity
- RL experiments rely on domain-specific verification functions that may not be available for broader mathematical domains
- The framework may underrepresent authentic problem-solving that requires multiple simultaneous strategies

## Confidence

- **High Confidence**: Complexity degradation curves and error spiral analysis are well-supported by controlled experiments
- **Medium Confidence**: RL improvement claims are supported but specific attribution to GRPO vs. other factors remains unclear
- **Low Confidence**: Transformative generalization results depend heavily on template design choices for "strategy shift" problems

## Next Checks

1. Have domain experts independently construct transformative test cases for 3 templates to verify "strategy shifts" represent genuine mathematical creativity

2. Implement post-hoc error detection mechanism to identify first reasoning errors and measure whether targeted interventions improve accuracy

3. Test models on authentic competition problems matching OMEGA complexity profiles to assess ecological validity of synthetic benchmark findings