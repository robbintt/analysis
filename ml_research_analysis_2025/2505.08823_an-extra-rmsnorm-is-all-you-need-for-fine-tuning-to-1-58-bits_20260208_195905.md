---
ver: rpa2
title: An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits
arxiv_id: '2505.08823'
source_url: https://arxiv.org/abs/2505.08823
tags:
- quantization
- rmsnorm
- quantized
- ternary
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that inserting RMS normalization layers
  before each quantized linear projection and using a gradual layer-wise quantization
  schedule allows stable fine-tuning of full-precision language model checkpoints
  into ternary (2-bit) LLMs. The approach avoids complex knowledge distillation pipelines
  and instead optimizes cross-entropy directly.
---

# An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits

## Quick Facts
- arXiv ID: 2505.08823
- Source URL: https://arxiv.org/abs/2505.08823
- Reference count: 11
- One-line primary result: Inserting RMSNorm layers before quantized projections + gradual layer-wise quantization schedule enables stable 1.58-bit LLM fine-tuning with accuracy on par with full-precision baselines.

## Executive Summary
This paper introduces a method for fine-tuning large language models to ternary (1.58-bit) weights with accuracy comparable to full-precision baselines. The approach inserts RMS normalization layers before each quantized linear projection and uses a gradual layer-wise quantization schedule. Experiments on Qwen-1.5B and Llama3-8B show that this method achieves lower cross-entropy loss and comparable accuracy on mathematical reasoning benchmarks (AIME-2024, MATH-500) relative to full-precision baselines. The memory efficiency enables large models (e.g., 70B parameters) to fit on single commodity GPUs, making ultra-low-bit inference practical.

## Method Summary
The method replaces all dense layers with BitLinear modules using straight-through estimation (STE) for ternary quantization, inserts RMSNorm layers immediately before each linear projection, removes bias terms, and employs a gradual quantization schedule via a lambda parameter that ramps from 0 to 1 during training. The approach optimizes cross-entropy directly without knowledge distillation, using the interpolation W_q = W + lambda * (weight_quant(W) - W).detach() to smoothly transition from full-precision to ternary weights. Activations remain at higher precision during training.

## Key Results
- RMSNorm insertion before quantized projections stabilizes training, preventing loss spikes that occur with abrupt quantization
- Direct cross-entropy optimization matches or exceeds knowledge distillation performance when RMSNorm is present
- Ternary fine-tuning achieves lower cross-entropy and comparable accuracy on AIME-2024 and MATH-500 versus full-precision baselines
- Memory efficiency enables 70B-parameter models to fit on single commodity GPUs

## Why This Works (Mechanism)

### Mechanism 1: Input Distribution Stabilization via Pre-Projection RMSNorm
Ternary weights (-1, 0, +1) have severely limited expressivity. Without normalization, upstream scale variations propagate as inconsistent outputs. RMSNorm re-scales inputs to unit variance per token, constraining the dynamic range that quantized weights must accommodate. If upstream layers produce activations with near-zero variance, RMSNorm's division by sqrt(mean(x²) + ε) could amplify noise.

### Mechanism 2: Gradual Lambda Scheduling for Quantization Transition
Ramping the quantization strength (lambda) from 0 to 1 over training reduces initial loss spikes and allows the model to adapt progressively to ternary constraints. The two-phase schedule (ramping to 1 by mid-training) gives the model time to adjust before committing to hard quantization. If learning rate is too high during the lambda ramp, weight magnitudes may oscillate, causing unstable quantization boundaries.

### Mechanism 3: Straight-Through Estimation Preserves Gradient Flow
STE allows backpropagation through non-differentiable quantization by treating the quantization function as identity for gradients, enabling gradient descent despite discrete forward-pass weights. The identity approximation is sufficiently accurate for ternary quantization; accumulated gradient error does not prevent convergence. If weight distributions become bimodal at quantization boundaries, STE gradients may point in conflicting directions.

## Foundational Learning

- **RMSNorm (Root Mean Square Layer Normalization)**: Why needed here: The paper's central modification. Unlike LayerNorm, RMSNorm does not subtract the mean—simpler computation and more stable when zero is an explicit quantization target. Quick check: Given input `[2.0, -2.0]`, compute RMSNorm output (ε=1e-6, no learned scale). Is the result zero-centered?
- **Straight-Through Estimator (STE)**: Why needed here: Enables gradient-based training of quantized weights. Without STE, the quantization step has zero gradient almost everywhere. Quick check: If `quantize(w) = round(w)` and `w = 0.6`, what gradient does STE return for the quantization operation? What should the true gradient be?
- **Ternary Quantization Scaling**: Why needed here: Ternary values {-1, 0, +1} alone lose magnitude information. A per-tensor or per-channel scale factor (often mean absolute value) preserves representational capacity. Quick check: Why is scaling by average absolute value preferred over max absolute value for ternary quantization? What happens to sparsity under each scheme?

## Architecture Onboarding

- **Component map**: Input Token → [Embedding] → For each Transformer Block: → [Pre-Attention RMSNorm] → [Q,K,V Projections with BitLinear] → [Attention] → [Pre-FFN RMSNorm] → [BitLinear W1] → [SwiGLU Activation] → [Pre-W2 RMSNorm] → [BitLinear W2] → [Residual Add] → [Output RMSNorm] → [LM Head]
- **Critical path**: 1. Replace all `nn.Linear` with `BitLinear` (weights stored in FP16/FP32, quantized in forward) 2. Insert RMSNorm before each BitLinear input (including Q/K/V, up/down projections) 3. Remove bias terms from all linear layers (per BitNet design) 4. Implement lambda schedule in training loop, passing current lambda to BitLinear
- **Design tradeoffs**: Extra RMSNorm adds ~0.3% parameters per layer but enables stable ternary training. Lambda scheduling requires mid-training checkpoint alignment; abrupt quantization at t=0 is simpler but unstable. Direct cross-entropy vs. KD: Direct training is simpler and matches KD performance when RMSNorm is present, per Figure 3
- **Failure signatures**: Training loss spikes >2× baseline early in training → Lambda ramp too aggressive or RMSNorm missing. Loss plateaus higher than baseline → Check that STE `.detach()` is correctly implemented (not double-detaching). Weight distributions not concentrating at {-1, 0, +1} → Scale factor computation may be incorrect; verify `abs(mean(abs(W)))`
- **First 3 experiments**: 1. **Ablation: Remove all extra RMSNorm layers.** Train Qwen-1.5B with BitLinear but no pre-projection normalization. Expect: divergence or 15-30% higher final loss. 2. **Lambda schedule sweep.** Compare (a) immediate quantization (lambda=1 from step 0), (b) linear ramp to 1 at final step, (c) two-phase ramp to 1 at 50% progress. Measure final cross-entropy and training stability. 3. **Scale factor sensitivity.** Compare scaling by (a) mean absolute value per tensor, (b) max absolute value, (c) learned per-channel scales. Track weight histogram concentration and downstream accuracy on MATH-500.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the extra-RMSNorm method scale effectively to models larger than 8B parameters (e.g., 70B+) without requiring knowledge distillation? Experiments only validated Qwen-1.5B and Llama3-8B; the 70B claim rests on memory benefits, not accuracy.
- **Open Question 2**: Can full activation quantization (e.g., 8-bit) be added without degrading the accuracy gains observed with weight-only ternary quantization? The reported parity with full-precision baselines applies only to ternary weights, not quantized activations.
- **Open Question 3**: Would channel-wise normalization (e.g., Dynamic Tanh) provide further stability or accuracy improvements over the current per-layer RMSNorm approach? The paper does not compare per-layer vs. per-channel normalization schemes.

## Limitations
- The central claim that RMSNorm alone enables stable ternary fine-tuning lacks direct ablation comparisons against all possible normalization schemes
- The scaling formula for ternary weights (inverse mean absolute value) is presented as effective but not rigorously compared against alternatives like max absolute value or learned scales
- Claims about deployment on "single commodity GPUs" for 70B-parameter models are not quantified; memory usage tables or profiling data are missing

## Confidence
- **High confidence**: The empirical results showing lower cross-entropy and comparable benchmark accuracy versus baselines are well-supported by the experiments on Qwen-1.5B and Llama3-8B
- **Medium confidence**: The mechanism explanations (RMSNorm stabilizing input distributions, lambda scheduling preventing loss spikes, STE enabling gradient flow) are plausible and consistent with quantization literature, but direct mechanistic validation is absent
- **Low confidence**: Claims about deployment on "single commodity GPUs" for 70B-parameter models are not quantified; memory usage tables or profiling data are missing

## Next Checks
1. **Ablation Study**: Systematically compare RMSNorm insertion against no normalization and LayerNorm in the same ternary fine-tuning setup. Measure final cross-entropy and training stability to isolate the specific benefit of RMSNorm.
2. **Scale Factor Sensitivity**: Test ternary quantization with alternative scaling strategies: (a) max absolute value, (b) learned per-channel scales, (c) no scaling (pure {-1,0,+1}). Evaluate impact on weight histogram concentration and downstream accuracy.
3. **Memory Profiling**: Quantify peak VRAM usage for 70B-parameter ternary models during inference on a representative commodity GPU (e.g., RTX 4090). Compare against full-precision baselines and verify the claimed memory efficiency.