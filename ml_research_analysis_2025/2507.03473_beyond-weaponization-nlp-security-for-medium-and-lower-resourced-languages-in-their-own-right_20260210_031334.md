---
ver: rpa2
title: 'Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages
  in Their Own Right'
arxiv_id: '2507.03473'
source_url: https://arxiv.org/abs/2507.03473
tags:
- latn
- languages
- adversarial
- language
- lrls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the security of language models (LMs) for
  medium- and lower-resourced languages (MRLs and LRLs) by extending adversarial attacks
  to 70 languages. It evaluates monolingual and multilingual LMs using a multilingual
  version of TextFooler and a round-trip machine translation attack, finding that
  monolingual models are often too small to ensure security, and while multilingual
  models offer some security benefits, they do not guarantee improved security.
---

# Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right

## Quick Facts
- **arXiv ID:** 2507.03473
- **Source URL:** https://arxiv.org/abs/2507.03473
- **Reference count:** 40
- **Primary result:** Monolingual models are often too small for robust security, and multilingual models offer inconsistent security gains across 70 languages.

## Executive Summary
This study addresses the critical security gap in language models (LMs) for medium- and lower-resourced languages (MRLs and LRLs) by systematically evaluating their robustness against adversarial attacks. Using a multilingual TextFooler attack and round-trip machine translation, the research finds that monolingual models are frequently too small to ensure security, while multilingual models provide inconsistent protection. The findings highlight that MRLs and LRLs represent significant security vulnerabilities that require urgent attention from the broader NLP community.

## Method Summary
The research evaluates security robustness of monolingual and multilingual LMs for 70 languages (29 LRLs, 33 MRLs, 8 HRLs) using two topic classification datasets (Taxi1500, SIB200). Classifiers are finetuned on eight models (Goldfish variants, mBERT, XLM-R, Glot500) with standard hyperparameters. Two black-box attacks are applied to correctly classified samples: a multilingual TextFooler variant using FastText embeddings (157 languages) with relaxed linguistic constraints, and a round-trip MT attack pivoting through Zulu using NLLB-200. Attack Success Rate (ASR) is calculated as 1 - accuracy on adversarial samples.

## Key Results
- Monolingual models are frequently too small to ensure robust security against adversarial attacks.
- Multilingual models provide inconsistent security gains and do not guarantee improved robustness across all languages.
- Medium- and lower-resourced languages represent significant security vulnerabilities requiring community attention.

## Why This Works (Mechanism)
The study works by systematically extending adversarial attack methodologies to 70 languages, revealing security gaps that emerge when linguistic resources are scarce. By applying the same attack algorithms across languages with varying resource availability, it quantifies how model size and multilingual training impact robustness. The multilingual TextFooler attack, modified to work without extensive linguistic resources like POS taggers, generates adversarial examples by substituting synonyms based on cosine similarity thresholds. The round-trip MT attack creates perturbations through translation cycles, exploiting potential vulnerabilities in cross-lingual mappings.

## Foundational Learning
- **Adversarial attack in NLP:** Deliberately crafting inputs to fool models while maintaining semantic meaning. Needed to understand the threat model being evaluated. Quick check: Does the attack preserve the original label according to human evaluation?
- **Attack Success Rate (ASR):** Metric measuring model vulnerability, calculated as 1 - accuracy on adversarial samples. Needed to quantify robustness across different language-resource scenarios. Quick check: Is ASR calculated only on samples that were originally classified correctly?
- **Multilingual embeddings:** Cross-lingual vector representations enabling synonym lookup across languages. Needed for the multilingual TextFooler to function without language-specific resources. Quick check: Does FastText cover all 70 target languages?
- **Round-trip machine translation:** Translating text to an intermediate language and back to create perturbations. Needed as an alternative attack method requiring minimal linguistic resources. Quick check: Is the pivot language (Zulu) appropriate for all language pairs?
- **Encoder-only vs. decoder models:** Architectural distinction affecting how models process and generate text. Needed to understand the scope and limitations of the evaluated models. Quick check: Are the evaluated models primarily encoder-only architectures?

## Architecture Onboarding
- **Component map:** Datasets (Taxi1500, SIB200) -> Classifiers (8 models) -> Attacks (TextFooler, RT-MT) -> Evaluation (ASR calculation)
- **Critical path:** Data loading → Model finetuning → Attack generation → ASR computation → Analysis
- **Design tradeoffs:** Removed linguistic constraints (stop-word filtering, POS tagging) to enable multilingual attacks at the cost of sample quality; used Zulu as pivot language for RT-MT for consistency across language pairs
- **Failure signatures:** Low-quality adversarial samples due to missing linguistic resources; model coverage mismatches where certain languages lack support in some architectures
- **First experiments:**
  1. Verify FastText embedding coverage for a representative subset of target languages
  2. Test synonym substitution quality for 3-5 languages to assess degradation from removed constraints
  3. Confirm RT-MT attack functionality with Zulu pivot for 2-3 language pairs

## Open Questions the Paper Calls Out
- How do larger decoder-based language models and parameter-efficient finetuning techniques impact security for lower-resourced languages compared to encoder-only models?
- What adversarial defense mechanisms are most effective at securing LMs for medium and lower-resourced languages without extensive language-specific resources?
- Can the "stealthiness" and grammaticality of multilingual adversarial attacks be improved for resource-scarce languages to match English attack quality?

## Limitations
- Removed linguistic constraints (stop-word filtering, POS tagging) from TextFooler, potentially degrading adversarial sample quality
- Reliance on Zulu as pivot language in RT-MT may not be optimal for all language pairs
- Focus on encoder-only models limits generalizability to modern larger LLMs and parameter-efficient finetuning methods

## Confidence
- **Central claims about model size and multilingual training impact:** High
- **Generalizability of TextFooler modifications across all 70 languages:** Medium
- **Optimality of Zulu pivot in RT-MT attack:** Medium

## Next Checks
1. Verify the specific FastText model version used and test synonym substitution quality for a subset of MRL/LRL languages to assess potential degradation from removed linguistic constraints.
2. Reproduce ASR calculations on Taxi1500 and SIB200 for a representative subset of 5-10 languages using both attack methods to confirm the reported trends.
3. Test alternative pivot languages in the RT-MT attack to determine if Zulu consistently produces comparable adversarial examples across language families.