---
ver: rpa2
title: '"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals
  with Visual Impairments'
arxiv_id: '2505.04488'
source_url: https://arxiv.org/abs/2505.04488
tags:
- tasks
- videollms
- user
- impaired
- visually
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic evaluation of real-time
  VideoLLMs for assisting visually impaired individuals. The authors conduct a user
  survey to design the VisAssistDaily benchmark, evaluate three popular VideoLLMs
  (GPT-4o, Zhipu, VITA-1.5), and perform a user study.
---

# "I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments

## Quick Facts
- **arXiv ID:** 2505.04488
- **Source URL:** https://arxiv.org/abs/2505.04488
- **Reference count:** 40
- **Primary result:** GPT-4o achieves highest task success rate among evaluated VideoLLMs for visually impaired assistance, with SafeVid fine-tuning improving VITA-1.5 risk detection from 25% to 76%.

## Executive Summary
This paper introduces the first systematic evaluation of real-time VideoLLMs for assisting visually impaired individuals. The authors conduct a user survey to design the VisAssistDaily benchmark, evaluate three popular VideoLLMs (GPT-4o, Zhipu, VITA-1.5), and perform a user study. GPT-4o achieves the highest task success rate, while VITA-1.5 shows the lowest performance. To address the challenge of proactive hazard perception, the authors propose SafeVid, an environment-awareness dataset, and fine-tune VITA-1.5, improving risk recognition accuracy from 25.00% to 76.00%. The study highlights the potential and challenges of VideoLLMs in assistive technology.

## Method Summary
The authors designed VisAssistDaily benchmark through user surveys, evaluating three VideoLLMs on 9 tasks across Basic Skills, Home Life, and Social Life in English and Chinese. They collected 2,205 video clips with 4-level risk annotations to create SafeVid dataset, then fine-tuned VITA-1.5 on 1,204 clips for 2 epochs using batch size 4, learning rate 1e-5, and cosine scheduler on 6 Nvidia L20 GPUs. Evaluation metrics included Task Success Rate (TSR), Average Prompt Cost (APC), Average Response Latency (ARL), and Language Consistency (LC).

## Key Results
- GPT-4o achieved highest TSR (88.33%) across all tasks, while VITA-1.5 scored lowest (52.50%)
- SafeVid fine-tuning improved VITA-1.5 risk recognition accuracy from 25.00% to 76.00%
- Users preferred GPT-4o for accuracy but reported privacy concerns with voice interaction in public
- Common failure modes included spatial inversion (left/right confusion) and context drift in navigation tasks

## Why This Works (Mechanism)

### Mechanism 1: Risk-Aware Instruction Tuning
The base VideoLLM optimizes for general semantic correlation. By introducing SafeVid with explicit severity labels (e.g., "High Risk" for stairs/obstacles), the model's decision boundary shifts to prioritize safety-critical tokens over neutral descriptions. The visual representations of "risk" in training data must generalize effectively to variable real-world conditions.

### Mechanism 2: Unified Multimodal Semantic Grounding
The model maintains a rolling context window where visual tokens align with audio intent, enabling spatial queries static models cannot handle. It ingests video streams and attends to recent visual frames when users ask location-based questions, using temporal consistency to ground objects.

### Mechanism 3: Human-Centric Evaluation Constraints
Utility is gated by "interaction efficiency"—the number of prompts required to complete a task (APC). Users have limited cognitive bandwidth for device interaction while navigating. A model requiring 5 prompts creates higher cognitive load than one needing 2, even if both succeed.

## Foundational Learning

**Concept:** Online vs. Offline Video Understanding
- **Why needed here:** The paper distinguishes between static image analysis and "continuous perception." Understanding the model processes streams (online) rather than files (offline) is critical for diagnosing latency and context-drift issues.
- **Quick check question:** Does the model process video as a single batch file or as a stream of frame-wise queries?

**Concept:** Hallucination in Multimodal Models
- **Why needed here:** The paper explicitly notes "hallucination" (describing non-existent objects) as a failure mode. This is a foundational risk where language priors overpower visual evidence.
- **Quick check question:** When the model describes a "black chair" that isn't there, is it failing to see or is it "imagination" based on typical room priors?

**Concept:** Spatial Reasoning & Localization
- **Why needed here:** The benchmark tests "Orientation Skills" and "Cane Techniques." Distinguishing between classification ("I see a cane") and localization ("The cane is 2 meters to your left") is vital for safety-critical applications.
- **Quick check question:** Can the model output relative coordinates (e.g., "3 o'clock, 2 meters") or only semantic descriptions (e.g., "to your right")?

## Architecture Onboarding

**Component map:** Mobile Camera -> Visual Encoder -> LLM Backbone with Multimodal Projector -> Text-to-Speech -> User Audio -> User Microphone -> ASR -> LLM Backbone

**Critical path:** The latency loop. The chain of [Camera Capture] -> [Encoding] -> [Inference] -> [Audio Response] must remain under the user's reaction time threshold (measured as Average Response Latency). If ARL exceeds ~2.5s, the user may over-walk the hazard.

**Design tradeoffs:**
- **Accuracy vs. Proactivity:** Base models (GPT-4o) are accurate but reactive (user must ask). Fine-tuned models (VITA-1.5 + SafeVid) are proactive (alert without asking) but may lack general reasoning breadth.
- **Proprietary vs. Local:** Cloud models (GPT-4o, Zhipu) offer high TSR but require internet and raise privacy concerns. Open models (VITA) allow local deployment but currently suffer from lower accuracy and language consistency.

**Failure signatures:**
- **Spatial Inversion:** Model correctly identifies an object but flips left/right (Error Case 1, Section 6.1)
- **Context Drift:** Model describes environment at top of stairs when user is at bottom (Error Case 2, Section 6.1)
- **Language Reversion:** Model responds in Chinese to English prompt (VITA-1.5, Section 6.1)

**First 3 experiments:**
1. **SafeVid Validation:** Reproduce fine-tuning of VITA-1.5 on SafeVid dataset to verify 25% → 76% risk detection claim
2. **Noise Injection Test:** Test Zhipu/GPT-4o pipeline with synthesized street noise to quantify ASR degradation
3. **Spatial Stress Test:** Run "Cane Techniques" benchmark specifically to measure left/right accuracy in object localization

## Open Questions the Paper Calls Out

### Open Question 1
How can VideoLLMs be enhanced to enable robust proactive hazard perception across diverse, unseen environmental risks beyond SafeVid scenarios? The SafeVid dataset contains only 1,204 clips from limited locations, while real-world hazards are far more diverse and unpredictable.

### Open Question 2
What interaction modalities can effectively address privacy concerns for visually impaired users without compromising real-time situational awareness? Participants reported psychological pressure to be eavesdropped when speaking personal information in public places.

### Open Question 3
How can VideoLLMs maintain reliable performance in low-light and noisy environments where both visual and audio inputs are degraded? The authors identify that current VideoLLMs are significantly limited in dim light or night scenes and noisy environments.

### Open Question 4
What security mechanisms are necessary to protect visually impaired users from adversarial manipulation of VideoLLM-based assistive systems? The authors report that Zhipu could be successfully jailbroken using FC-Attack.

## Limitations

- Proprietary nature of GPT-4o and Zhipu limits reproducibility of core findings
- VisAssistDaily and SafeVid datasets are not publicly available, preventing independent verification
- User study sample size (5 participants) is small, raising concerns about statistical significance
- Generalization of SafeVid fine-tuning results to unseen environments remains uncertain

## Confidence

- **High Confidence:** Task success rate hierarchy (GPT-4o > Zhipu > VITA-1.5) and SafeVid fine-tuning improvement from 25% to 76% accuracy
- **Medium Confidence:** User preference for GPT-4o and reported privacy concerns with voice interaction
- **Low Confidence:** Claim that fine-tuned VITA-1.5 can reliably provide "proactive" hazard alerts in diverse real-world settings

## Next Checks

1. **Dataset Release Request:** Verify claims by requesting access to VisAssistDaily and SafeVid datasets for independent reproduction of TSR, APC, and ARL metrics across all three models

2. **Real-World Deployment Test:** Conduct a longitudinal field study with 15+ visually impaired participants across varied environments (urban, suburban, indoor) to validate model performance under operational conditions

3. **Failure Mode Analysis:** Systematically document and quantify spatial reasoning errors (left/right confusion, stair mislocalization) and ASR failures in noise to establish baseline error rates and their impact on user safety