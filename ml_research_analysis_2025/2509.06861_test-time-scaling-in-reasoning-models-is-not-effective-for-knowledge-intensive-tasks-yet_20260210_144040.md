---
ver: rpa2
title: Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive
  Tasks Yet
arxiv_id: '2509.06861'
source_url: https://arxiv.org/abs/2509.06861
tags:
- answer
- reasoning
- thinking
- test-time
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether test-time scaling\u2014generating\
  \ longer reasoning chains before answering\u2014improves performance on knowledge-intensive\
  \ tasks. The authors evaluate 14 reasoning models on two benchmarks requiring factual\
  \ recall."
---

# Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet

## Quick Facts
- arXiv ID: 2509.06861
- Source URL: https://arxiv.org/abs/2509.06861
- Reference count: 40
- This paper investigates whether test-time scaling—generating longer reasoning chains before answering—improves performance on knowledge-intensive tasks.

## Executive Summary
This paper investigates whether test-time scaling—generating longer reasoning chains before answering—improves performance on knowledge-intensive tasks. The authors evaluate 14 reasoning models on two benchmarks requiring factual recall. They find that increasing test-time computation does not consistently improve accuracy and often increases hallucinations. Analysis reveals that reduced hallucinations typically result from abstention rather than improved knowledge retrieval, while increased hallucinations stem from attempting previously unanswered questions. Extended reasoning can induce confirmation bias, leading to overconfident hallucinations. The authors provide an information-theoretic explanation: compute-only test-time scaling is a post-processing of a fixed model and cannot create new information about ground-truth answers beyond what is already encoded, explaining its limited effectiveness for knowledge-intensive tasks.

## Method Summary
The study evaluates 14 reasoning models across three test-time scaling paradigms: reasoning effort (low/medium/high) for GPT-5, o3-mini, o4-mini, gpt-oss-20b, Grok-3 mini; thinking budget (256/512/1024/2048 tokens) for Gemini 2.5 Flash/Pro, Claude Sonnet 4; and budget forcing (appending "Wait" 0/2/4/8/12 times) for DeepSeek-R1-Distill and Qwen3 models. Models are tested on SimpleQA (800 randomly sampled questions) and FRAMES (824 multi-hop reasoning questions) without external knowledge access. A gpt-4o-mini grader classifies responses as correct/incorrect/not attempted, and metrics include accuracy, hallucination ratio, and not-attempted rates.

## Key Results
- Test-time scaling does not consistently improve accuracy on knowledge-intensive tasks and often increases hallucinations.
- When hallucinations decrease with extended reasoning, this typically reflects increased abstention rather than improved knowledge retrieval.
- Extended reasoning can induce confirmation bias, where models fabricate supporting details to justify initial hypotheses.
- Information-theoretic analysis shows compute-only test-time scaling cannot increase mutual information between outputs and ground-truth answers beyond what's encoded in trained parameters.

## Why This Works (Mechanism)

### Mechanism 1: Abstention-Driven Hallucination Reduction (Not Knowledge Improvement)
- Claim: When test-time scaling reduces hallucinations, it primarily reflects increased abstention rather than improved factual retrieval.
- Mechanism: Extended reasoning increases the model's uncertainty awareness, causing it to refuse answering questions it previously would have guessed on. This lowers the hallucination ratio by shrinking the denominator of attempted answers.
- Core assumption: Models have latent uncertainty signals that longer reasoning can surface.
- Evidence anchors:
  - [abstract] "reduced hallucinations typically result from abstention rather than improved knowledge retrieval"
  - [section 4.2] For Grok-3 mini, 93.1% of reduced hallucinations were "not attempted"; for DS-R1-Distill-Qwen-14B, 88.8%
- Break condition: If a model's training penalizes abstention heavily, extended reasoning may not increase refusal rates, and this mechanism weakens.

### Mechanism 2: Confirmation Bias Amplification Through Extended Reasoning
- Claim: Longer reasoning chains can induce confirmation bias, where models fabricate supporting details to justify initial hypotheses.
- Mechanism: As reasoning progresses, tentative claims ("maybe 2005") transform into confident assertions ("I am fairly sure it's 2005") through self-generated pseudo-evidence. Incorrect answers appear earlier and more frequently in traces as thinking increases.
- Core assumption: Autoregressive generation can reinforce its own errors without external grounding.
- Evidence anchors:
  - [abstract] "extended reasoning can induce confirmation bias, leading to overconfident hallucinations"
  - [section 4.3] In gpt-oss-20b, normalized position of first incorrect answer appearance dropped from 45.7% (low effort) to 17.2% (high effort) on SimpleQA
- Break condition: Models explicitly trained to doubt intermediate conclusions or verify against retrieved evidence may resist this pattern.

### Mechanism 3: Information-Theoretic Ceiling for Compute-Only Scaling
- Claim: Compute-only test-time scaling cannot increase mutual information between model outputs and ground-truth answers beyond what is encoded in trained parameters.
- Mechanism: Test-time computation is a post-processing operation on a fixed model. By the data-processing inequality, I(A; output | prompt) ≤ I(A; model | prompt). For arbitrary facts not seen during training, I(A; model | prompt) ≈ 0, creating an error floor near random guessing.
- Core assumption: Knowledge-intensive questions contain "arbitrary facts" where answers are not inferable from prompt structure alone.
- Evidence anchors:
  - [abstract] "compute-only test-time scaling is a post-processing of a fixed trained model and therefore cannot increase information about the ground-truth answer beyond what is already encoded"
  - [section 5.3] Theorem 1 formalizes I(A; ̂R|c) ≤ I(A; ̂p|c) via Markov chain A → ̂p → Tk → ̂R
- Break condition: If test-time scaling includes external information sources (retrieval, tools), the Markov chain includes oracle O, and the ceiling can be raised.

## Foundational Learning

- Concept: Sequential vs. Parallel Test-Time Scaling
  - Why needed here: The paper focuses on sequential (chain-of-thought) scaling, but understanding the distinction clarifies what mechanisms apply where.
  - Quick check question: If you sample 10 independent reasoning paths and take a majority vote, which paradigm is this?

- Concept: Hallucination vs. Abstention Trade-off
  - Why needed here: The key finding is that hallucination rate changes are often willingness-to-answer changes in disguise.
  - Quick check question: A model's hallucination ratio drops from 30% to 20%. Before concluding it "knows more," what else should you check?

- Concept: Data-Processing Inequality (Information Theory)
  - Why needed here: The theoretical argument hinges on this principle—that post-processing cannot create new information.
  - Quick check question: If a fixed model M has I(M; ground_truth) = 0.3 bits, can any compute-only procedure produce output R with I(R; ground_truth) = 0.5 bits?

## Architecture Onboarding

- Component map:
  - Reasoning effort parameter (GPT-5, o-series): Adjusts thinking duration via API setting
  - Thinking budget tokens (Gemini, Claude): Explicit token limit for reasoning
  - Budget forcing (R1-Distill, Qwen3): Appends "Wait" prompts to extend reasoning
  - Grader model (gpt-4o-mini): Classifies responses as correct/incorrect/not attempted

- Critical path: Prompt → TTS setting → Model generates reasoning → Model produces answer → Grader evaluates → Metrics computed (accuracy, hallucination ratio, attempt rate)

- Design tradeoffs:
  - Abstention-encouraging prompts reduce hallucinations but may lower accuracy on questions the model could answer
  - Higher reasoning effort increases compute cost 8-15x with inconsistent accuracy gains
  - Budget forcing is model-agnostic but can produce unnatural reasoning traces

- Failure signatures:
  - Hallucination ratio increases with thinking budget (observed in GPT-5-mini, o3-mini, gpt-oss-20b, Gemini 2.5 Flash)
  - Incorrect answers appear repeatedly throughout reasoning trace with fabricated justifications
  - Confidence inflation: "maybe X" → "I recall X" → "I am fairly sure X" within same trace

- First 3 experiments:
  1. Baseline characterization: Run your model at multiple reasoning levels on a held-out knowledge benchmark. Track accuracy, hallucination ratio, and attempt rate separately.
  2. Transition analysis: For questions where outcomes change between reasoning levels, classify whether changes stem from new attempts (previously abstained) or new abstentions (previously answered).
  3. Trace inspection: Sample 20 cases where hallucinations increased with reasoning. Annotate where the incorrect answer first appears and whether fabricated supporting details follow.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can test-time scaling combined with external information sources (e.g., retrieval, tools) overcome the information ceiling for knowledge-intensive tasks?
- Basis in paper: [explicit] Appendix G.3 states that if a test-time scaling method "additionally queries an external oracle O... then the ceiling I(A; (p̂, O)|c) can be strictly larger than I(A; p̂|c)," explaining why "tool-augmented methods can improve knowledge-intensive tasks."
- Why unresolved: The study only evaluates compute-only test-time scaling without external access; the theoretical extension to retrieval-augmented settings is sketched but not empirically tested.
- What evidence would resolve it: A controlled comparison of compute-only vs. retrieval-augmented test-time scaling on the same knowledge benchmarks, measuring whether external access breaks the observed accuracy ceiling.

### Open Question 2
- Question: What characterizes the "compute-limited" regime where test-time scaling can improve knowledge-intensive performance?
- Basis in paper: [explicit] Section 5.3 Discussion: "We do not claim that test-time scaling can never help: in settings where the model contains substantial signal about A but baseline decoding fails to extract it reliably, i.e., a compute-limited regime, test-time scaling can still improve performance."
- Why unresolved: The paper demonstrates an information-limited regime but does not empirically characterize when compute-limitation applies or how to detect it.
- What evidence would resolve it: Identification of task/model conditions where I(A; p̂|c) is substantial but baseline accuracy is low, yet test-time scaling yields gains.

### Open Question 3
- Question: How can test-time scaling methods be modified to reduce confirmation bias and overconfident hallucinations?
- Basis in paper: [explicit] Sections 4.3 and 6 identify confirmation bias as a mechanism driving increased hallucinations: "the model recalls or even makes up information to reinforce its initial belief... leading to overconfident hallucinations."
- Why unresolved: The paper diagnoses the problem but does not propose or evaluate interventions to mitigate bias during extended reasoning.
- What evidence would resolve it: Evaluation of reasoning interventions (e.g., self-correction prompts, uncertainty calibration) that reduce fabricated supporting details while preserving beneficial reasoning.

## Limitations

- The study focuses exclusively on knowledge-intensive tasks requiring factual recall, which may not generalize to reasoning tasks where answers can be deduced from prompt structure.
- Several key models (GPT-5, o4-mini, Claude Sonnet 4, Grok-3 mini) were evaluated under undocumented API settings, making exact replication impossible without model access.
- The grader reliability is a critical limitation, as no human validation was reported for the hallucination/abstention classifications that drive the core analysis.

## Confidence

- High Confidence: The finding that test-time scaling often reduces hallucinations through abstention rather than improved knowledge retrieval. This is supported by clear numerical evidence (93.1% of Grok-3 mini's hallucination reduction came from not attempting previously answered questions) and the mechanism is straightforward.
- Medium Confidence: The confirmation bias mechanism where extended reasoning reinforces initial errors. While the positional evidence is compelling (incorrect answers appearing earlier and more frequently with higher reasoning effort), the broader literature on this phenomenon in LLMs is limited, and alternative explanations weren't fully controlled.
- Medium Confidence: The information-theoretic ceiling argument. The formal proof is sound within its stated assumptions (no external information sources), but the practical implications depend on how "arbitrary facts" are defined and whether this theoretical limit meaningfully constrains real-world performance.

## Next Checks

1. Cross-task validation: Test the same models on reasoning tasks where answers can be deduced from prompt structure (e.g., mathematical word problems or logical puzzles) to determine if the information-theoretic ceiling applies differently when the answer space is constrained rather than arbitrary facts.

2. Human evaluation of grader classifications: For a random sample of 100 questions where models changed their behavior between reasoning levels, have human annotators independently classify whether changes represent genuine knowledge improvement, abstention shifts, or confirmation bias, comparing against the gpt-4o-mini grader.

3. External knowledge access test: Evaluate a subset of models with retrieval augmentation (e.g., RAG or tool use) at different reasoning levels to test whether the information-theoretic ceiling can be raised when test-time scaling includes information-gathering rather than pure computation.