---
ver: rpa2
title: Evaluating Large Language Models on Urdu Idiom Translation
arxiv_id: '2510.17460'
source_url: https://arxiv.org/abs/2510.17460
tags:
- urdu
- translation
- prompt
- idiomatic
- roman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the first evaluation datasets for Urdu-to-English
  idiomatic translation, covering both Native Urdu and Roman Urdu scripts with gold-standard
  English equivalents. The authors evaluate open-source LLMs and NMT systems using
  automatic metrics including BLEU, BERTScore, COMET, and XCOMET.
---

# Evaluating Large Language Models on Urdu Idiom Translation

## Quick Facts
- arXiv ID: 2510.17460
- Source URL: https://arxiv.org/abs/2510.17460
- Authors: Muhammad Farmal Khan; Mousumi Akter
- Reference count: 6
- Primary result: Introduces first evaluation datasets for Urdu-to-English idiomatic translation

## Executive Summary
This study presents the first evaluation datasets for Urdu-to-English idiomatic translation, covering both Native Urdu and Roman Urdu scripts with gold-standard English equivalents. The authors evaluate open-source LLMs and NMT systems using automatic metrics including BLEU, BERTScore, COMET, and XCOMET. They find that prompt engineering significantly improves idiomatic translation quality, with Cultural and Paraphrase prompts outperforming Literal prompts, especially for larger models like GPT-OSS-20B. Native Urdu script consistently yields more accurate translations than Roman Urdu due to spelling inconsistencies in the latter. GPT-OSS-20B demonstrates superior performance across semantic metrics, establishing a benchmark for low-resource idiomatic translation in Urdu.

## Method Summary
The researchers created evaluation datasets containing 200 Urdu idiom pairs with gold-standard English equivalents, covering both Native Urdu and Roman Urdu scripts. They evaluated multiple open-source LLMs and NMT systems using automatic metrics (BLEU, BERTScore, COMET, XCOMET). The study tested different prompt engineering approaches (Literal, Cultural, Paraphrase) to improve idiomatic translation quality. Models were compared across writing systems to assess performance differences between Native Urdu and Roman Urdu inputs.

## Key Results
- Prompt engineering significantly improves idiomatic translation compared to direct translation
- Cultural and Paraphrase prompts outperform Literal prompts, particularly for larger models like GPT-OSS-20B
- Native Urdu script produces more accurate idiomatic translations than Roman Urdu due to spelling inconsistencies
- GPT-OSS-20B consistently achieves highest performance across semantic metrics

## Why This Works (Mechanism)
The study demonstrates that large language models can effectively capture idiomatic meaning when properly prompted, leveraging their instruction-tuned capabilities. The superior performance of Native Urdu over Roman Urdu reflects the models' better handling of standardized script with consistent spelling patterns. Prompt engineering works by providing contextual guidance that helps models understand the figurative rather than literal meaning of idioms. Larger models show better performance due to their enhanced capacity for semantic understanding and cultural nuance.

## Foundational Learning
- Idiomatic translation complexity: Figurative language requires understanding cultural context beyond literal word meaning - needed to explain why standard translation approaches fail; quick check: compare literal vs. idiomatic English equivalents
- Script consistency impact: Standardized writing systems improve model performance through reduced ambiguity - needed to justify Native Urdu superiority; quick check: analyze spelling variations in Roman Urdu corpus
- Prompt engineering effectiveness: Contextual guidance helps models identify intended meaning - needed to explain performance improvements; quick check: compare metric scores across prompt types

## Architecture Onboarding
- Component map: Idiom pairs (input) -> Language model (processing) -> Automatic metrics (evaluation) -> Performance analysis
- Critical path: Data preparation → Model inference → Metric calculation → Analysis
- Design tradeoffs: Dataset size vs. coverage (200 idioms provides focused evaluation but limited diversity)
- Failure signatures: Low metric scores indicate literal translation approaches, Roman Urdu input errors, or insufficient model capacity
- First experiments:
  1. Test prompt engineering on a small subset of idioms to validate approach
  2. Compare Native Urdu vs. Roman Urdu performance on identical idiom sets
  3. Evaluate different model sizes on the same idiom pairs to establish scaling patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small dataset (200 idiom pairs) may limit generalizability to broader idiomatic expressions
- Focus exclusively on open-source models, excluding potentially different results from proprietary models
- Reliance on automatic metrics may not fully capture semantic accuracy where cultural nuance and context are critical

## Confidence
- High confidence: Native Urdu script outperforms Roman Urdu due to inconsistent spelling
- High confidence: Prompt engineering improves idiomatic translation quality
- Medium confidence: GPT-OSS-20B consistently outperforms other models
- Medium confidence: Cultural and Paraphrase prompts are most effective

## Next Checks
1. Conduct human evaluation study with native Urdu speakers to validate semantic accuracy of automatic metric results
2. Expand dataset to include 500-1000 idiom pairs covering diverse categories to test generalizability
3. Test additional open-source models (Llama-3, Mistral) and include one proprietary model for comparative analysis