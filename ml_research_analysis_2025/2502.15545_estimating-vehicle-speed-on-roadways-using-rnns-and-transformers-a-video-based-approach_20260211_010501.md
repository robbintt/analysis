---
ver: rpa2
title: 'Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based
  Approach'
arxiv_id: '2502.15545'
source_url: https://arxiv.org/abs/2502.15545
tags:
- speed
- vehicle
- data
- estimation
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This project addresses the challenge of vehicle speed estimation
  using video data by leveraging advanced machine learning models including LSTM,
  GRU, and Transformers. Traditional speed estimation methods are constrained by high
  costs and limited coverage, while video-based approaches offer non-intrusive, scalable
  solutions.
---

# Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach

## Quick Facts
- arXiv ID: 2502.15545
- Source URL: https://arxiv.org/abs/2502.15545
- Reference count: 7
- Key result: LSTM achieves 94.25% accuracy (RMSE 3.96) on VS13 dataset

## Executive Summary
This paper addresses vehicle speed estimation from video data using advanced machine learning models including LSTM, GRU, and Transformers. Traditional speed estimation methods are constrained by high costs and limited coverage, while video-based approaches offer non-intrusive, scalable solutions. The study demonstrates that gated recurrent networks (LSTM/GRU) outperform basic RNNs due to their ability to manage long-term dependencies, while Transformers provide exceptional adaptability through self-attention mechanisms. Results show significant performance improvements with increased sequence length, highlighting the importance of contextual information in dynamic environments.

## Method Summary
The approach involves extracting bounding box coordinates from video frames, computing frame-to-frame changes (center displacement, dimension changes), and normalizing features to zero mean and unit variance. Two main architectures are implemented: (1) RNN path with Conv embedding → LSTM/GRU → Attention → Conv processing → FC layer → Speed output, and (2) Transformer path with 1D Conv embedding → Positional encoding → Transformer encoder (self-attention + feedforward) → FC layers → Speed output. The models are trained on VS13 (400 HD videos, 13 vehicle models, speeds 30-105 km/h) and I24-3D datasets (16-17 cameras, 4K/30fps, 877K+ 3D bounding boxes).

## Key Results
- LSTM achieves 94.25% accuracy (RMSE 3.96) on VS13 dataset
- GRU achieves 92.66% accuracy (RMSE 4.47) on VS13 dataset
- Transformer achieves 90.90% accuracy (RMSE 5.48) on VS13 dataset
- Increasing sequence length consistently improves model accuracy across all architectures

## Why This Works (Mechanism)

### Mechanism 1: Gated Temporal Memory (LSTM/GRU)
LSTM and GRU models achieve higher accuracy than basic RNNs due to advanced gating mechanisms that manage long-term dependencies. LSTM uses forget, input, and output gates; GRU uses update and reset gates. These gates selectively retain relevant temporal information (e.g., sustained velocity trends) while discarding noise from frame-to-frame variations. Vehicle speed estimation requires capturing consistent movement patterns across multiple frames, not just adjacent-frame differences.

### Mechanism 2: Context Accumulation via Sequence Length
Increasing input sequence length consistently improves model accuracy by providing more contextual information. Longer sequences allow models to observe acceleration/deceleration patterns, reducing ambiguity in instantaneous speed estimates derived from bounding box displacements. Vehicle motion exhibits temporal continuity that extends beyond immediate adjacent frames.

### Mechanism 3: Self-Attention for Selective Frame Prioritization
Transformer self-attention mechanisms enable the model to focus on the most informative frames, improving robustness across varied conditions. Self-attention computes weighted relationships across all positions simultaneously, assigning higher weights to frames showing significant positional changes (indicative of speed-relevant motion). Not all frames contribute equally to speed estimation; frames with minimal movement or occlusion should be downweighted.

## Foundational Learning

- **Concept: LSTM/GRU Gating Mechanisms**
  - Why needed here: Understanding how gates control information flow explains why LSTM/GRU outperform basic RNNs on temporal tasks
  - Quick check question: If a vehicle stops at a red light mid-sequence, which gate in LSTM would prevent the prior velocity from corrupting the stationary-state representation?

- **Concept: Positional Encoding**
  - Why needed here: Transformers process sequences in parallel without inherent order; positional encodings inject temporal information critical for video frame sequences
  - Quick check question: Without positional encoding, how would a Transformer distinguish frame 1 from frame 10 in a 20-frame input?

- **Concept: Attention Weighting**
  - Why needed here: Both RNN+Attention and Transformer architectures use attention to prioritize informative frames over redundant or noisy ones
  - Quick check question: Would attention weights be higher for frames showing a vehicle at constant highway speed or frames capturing sudden braking?

## Architecture Onboarding

- **Component map:**
  Bounding box extraction → Feature normalization → Embedding layer → Temporal model (LSTM/GRU or Transformer) → Fully connected output → MSE loss

- **Critical path:**
  Bounding box extraction → Feature normalization → Embedding layer → Temporal model (LSTM/GRU or Transformer) → Fully connected output → MSE loss

- **Design tradeoffs:**
  - LSTM vs. GRU: LSTM (3 gates) offers more expressive power; GRU (2 gates) trains faster with comparable performance
  - RNN+Attention vs. Transformer: RNNs process sequentially (simpler debugging); Transformers enable parallel processing and adaptive attention but require positional encoding and more data
  - Sequence length: Longer sequences improve accuracy (per paper findings) but increase memory/compute costs
  - Assumption: Transformers may underperform on smaller datasets (VS13) due to limited training data relative to model capacity

- **Failure signatures:**
  - Basic RNN underperforming by 2-3% vs. LSTM/GRU → gradient vanishing on longer sequences
  - Transformer accuracy lower than LSTM on small datasets → insufficient training data for attention learning
  - High RMSE on I24-3D vs. VS13 → greater scene complexity, multi-camera calibration issues, or annotation noise
  - Uniform attention weights → features insufficiently discriminative or learning rate too high

- **First 3 experiments:**
  1. Train RNN, LSTM, GRU, and Transformer on VS13 with fixed sequence length (e.g., 20 frames); expect LSTM (94.25%) > GRU (92.66%) > RNN (91.64%), Transformer (90.90%) per reported results
  2. Test LSTM on VS13 with sequence lengths [5, 10, 20, 30]; expect monotonic accuracy improvement as length increases
  3. Train Transformer on VS13 with and without positional encoding; expect performance degradation without positional encoding, confirming temporal order importance

## Open Questions the Paper Calls Out
1. Would hybrid models combining LSTM/GRU's sequential processing with Transformer's self-attention mechanisms outperform individual approaches for vehicle speed estimation?
2. How well do these models maintain accuracy under adverse weather, varying lighting, and extreme traffic conditions compared to controlled dataset scenarios?
3. Can these models be optimized to maintain high accuracy while achieving the low latency required for real-time deployment on resource-constrained edge devices?
4. What is the relative contribution of different input features (position changes vs. bounding box size changes) to speed estimation accuracy?

## Limitations
- Exact hyperparameter configurations for LSTM, GRU, and Transformer models remain unspecified
- No information on dataset split ratios or specific preprocessing steps beyond bounding box normalization
- Limited discussion of failure modes beyond basic performance metrics
- Comparison with non-neural baselines (e.g., optical flow, classical tracking) is absent

## Confidence

- High confidence in the superiority of LSTM/GRU over basic RNNs (supported by gating mechanism literature)
- Medium confidence in Transformer performance claims (no direct comparisons provided with other sequence models)
- Medium confidence in sequence length effects (empirical observation, but mechanism not fully explained)
- Low confidence in generalization across datasets (VS13 and I24-3D have different characteristics and collection methods)

## Next Checks
1. **Architecture Ablation Test:** Train basic RNN, LSTM, GRU, and Transformer models on VS13 with identical hyperparameters (except model-specific components) to verify reported performance hierarchy
2. **Attention Mechanism Analysis:** Visualize attention weights across frames for both RNN+Attention and Transformer models to confirm selective frame prioritization mechanism
3. **Sequence Length Boundary Test:** Conduct systematic experiments across sequence lengths [5, 10, 20, 30, 40, 50] on VS13 to identify the point of diminishing returns and validate the monotonic improvement claim