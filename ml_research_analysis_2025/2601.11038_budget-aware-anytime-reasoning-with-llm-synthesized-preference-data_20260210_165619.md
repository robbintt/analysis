---
ver: rpa2
title: Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data
arxiv_id: '2601.11038'
source_url: https://arxiv.org/abs/2601.11038
tags:
- reasoning
- anytime
- budget
- preference
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies budget-aware anytime reasoning in LLMs, where
  models must deliver high-quality solutions under strict token or latency constraints.
  The authors introduce an evaluation framework featuring the Anytime Index to measure
  solution quality across varying reasoning lengths, capturing how efficiently models
  convert tokens into better outputs.
---

# Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data

## Quick Facts
- arXiv ID: 2601.11038
- Source URL: https://arxiv.org/abs/2601.11038
- Reference count: 24
- Models can improve their reasoning efficiency under token constraints by learning from self-generated preference data

## Executive Summary
This paper addresses the challenge of budget-aware anytime reasoning in large language models, where models must deliver high-quality solutions under strict token or latency constraints. The authors introduce the Anytime Index to measure solution quality across varying reasoning lengths, capturing how efficiently models convert tokens into better outputs. They propose an inference-time self-improvement method using LLM-synthesized preference data, where models learn from contrastive examples of their own reasoning to guide future inference. Experiments on NaturalPlan (trip planning), AIME (math), and GPQA (scientific QA) show that Preference Data Prompting consistently improves both intermediate and final solution quality, outperforming baselines like Chain-of-Thought and LEAP.

## Method Summary
The method involves sampling multiple reasoning traces from a base model, truncating them at fixed token budgets, and evaluating their quality. Preference pairs are constructed by contrasting high-quality traces against lower-quality ones of the same length. These pairs are then used as in-context examples during inference to guide the model toward more efficient reasoning patterns. The approach works without human supervision by having the model generate and evaluate its own reasoning traces.

## Key Results
- Preference Data Prompting consistently improves both intermediate and final solution quality across NaturalPlan, AIME, and GPQA tasks
- The method achieves higher Anytime Index scores, indicating better quality-per-token efficiency compared to Chain-of-Thought and LEAP baselines
- Reasoning-specialized models like Grok-3 show the strongest improvements when using PDP
- Cross-domain testing shows promise, with NaturalPlan-generated preferences improving AIME performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Anytime Index quantifies reasoning efficiency by measuring how quickly solution quality improves with token count
- **Mechanism:** Computes normalized area under the curve of solution quality across increasing token budgets, rewarding early arrival at high-quality solutions
- **Core assumption:** Solution quality at discrete token checkpoints proxies internal reasoning state
- **Evidence anchors:** [abstract] "We introduce... the Anytime Index, a metric that quantifies how effectively solution quality improves as reasoning tokens increase."
- **Break condition:** Loses discriminative power if token-quality relationship is non-linear or checkpoints misalign with reasoning boundaries

### Mechanism 2
- **Claim:** Fixed-budget preference pairs isolate reasoning density as the learning signal
- **Mechanism:** Contrasts same-length traces with different quality scores, forcing model to optimize information density rather than length
- **Core assumption:** Model can distinguish semantic correctness within fixed token counts through self-evaluation
- **Evidence anchors:** [Section 3.1] "By fixing the budget, we isolate differences in reasoning quality, not length"
- **Break condition:** Fails if quality strictly correlates with length or scoring metrics are noisy

### Mechanism 3
- **Claim:** In-context learning from contrastive pairs steers reasoning trajectories
- **Mechanism:** Injects synthesized preference examples into prompt to bias next-token predictions toward higher-utility reasoning
- **Core assumption:** Model can generalize efficiency patterns from provided examples to new queries
- **Evidence anchors:** [abstract] "...inference-time self-improvement method using LLM-synthesized preference data"
- **Break condition:** Performance degrades if context window is exhausted or model exhibits primacy bias

## Foundational Learning

- **Concept: Anytime Algorithms**
  - **Why needed here:** Grounds evaluation in classic AI theory where algorithm value lies in performance profile over time
  - **Quick check question:** Does the model improve solution quality monotonically as token budget increases?

- **Concept: Constraint Satisfaction Rate (CSR)**
  - **Why needed here:** Allows partial credit for intermediate solutions in NaturalPlan tasks where EM is too brittle
  - **Quick check question:** If model satisfies 8/10 constraints at 200 tokens and 10/10 at 800 tokens, does framework capture this trajectory?

- **Concept: Contrastive In-Context Learning**
  - **Why needed here:** Core method relies on showing "what to do" vs. "what not to do" rather than just positive examples
  - **Quick check question:** Why might showing a "bad" reasoning trace be more effective for teaching efficiency than only showing a "good" one?

## Architecture Onboarding

- **Component map:** Data Sampler -> Budget Truncator -> Evaluator -> Ranker -> Prompt Constructor -> LLM Inference
- **Critical path:** The Evaluator - if constraint checker or accuracy metric is flawed, preference pairs contain label noise
- **Design tradeoffs:**
  - Sampling width N=64 balances compute cost against finding optimal reasoning paths
  - Omitting intermediate solutions from prompt saves context space but trades explicit guidance for pattern guidance
- **Failure signatures:**
  - Non-monotonic reasoning where longer traces reduce quality due to drift
  - Metric saturation when tasks are too easy, making Anytime Index uninformative
- **First 3 experiments:**
  1. Sanity Check: Run evaluation pipeline on base model to establish baseline Anytime Index and verify performance increases with tokens
  2. Ablation: Compare full PDP against PDP+ (only positive examples) to isolate value of rejected traces
  3. Cross-Domain Validation: Test NaturalPlan-generated preference pairs on AIME/GPQA to assess transferability of reasoning efficiency

## Open Questions the Paper Calls Out

- **Open Question 1:** Can preference-driven fine-tuning methods (e.g., DPO) internalize anytime reasoning behavior more effectively than inference-time PDP?
  - **Basis:** [Section 6] Authors suggest training integration could further improve budget-aware reasoning
  - **What evidence would resolve it:** Compare DPO/RL-finetuned models against inference-time PDP on Anytime Index and task metrics

- **Open Question 2:** How does the framework generalize to open-ended domains like code generation where intermediate quality is harder to quantify?
  - **Basis:** [Section 6] Authors note future work should validate on broader tasks including code generation
  - **What evidence would resolve it:** Extend framework to code tasks with test-case pass rates or human evaluations as intermediate metrics

- **Open Question 3:** What mechanisms cause performance degradation with increased reasoning tokens in some models, and can this be mitigated?
  - **Basis:** [Appendix F] Authors observe GPT-4o's non-monotonic performance due to drift and self-revision
  - **What evidence would resolve it:** Ablations varying self-revision frequency and testing stabilizing interventions

- **Open Question 4:** How does PDP compare to other prompting strategies (e.g., Tree-of-Thoughts, Self-Consistency) under token budget constraints?
  - **Basis:** [Section 6] Authors acknowledge not exhaustively benchmarking against other strategies
  - **What evidence would resolve it:** Head-to-head evaluation of PDP against ToT and Self-Consistency using Anytime Index

## Limitations
- The method requires extensive sampling (N=64 traces) per input, making it computationally expensive
- Performance depends heavily on the quality of the scoring metric, which may be noisy or incomplete
- Cross-domain generalization claims need more systematic validation beyond single examples
- Some models show non-monotonic performance where longer reasoning degrades quality

## Confidence
- **High Confidence:** Anytime Index metric design and preference data construction pipeline
- **Medium Confidence:** Specific quality improvements for individual model-dataset combinations
- **Low Confidence:** Universal effectiveness across all reasoning tasks and model types

## Next Checks
1. **Cross-Dataset Transferability Test:** Systematically evaluate whether preference pairs from one task domain improve structurally different domains
2. **Failure Mode Analysis:** Test method on tasks requiring knowledge outside model's training cutoff to distinguish "wrong reasoning" from "insufficient reasoning"
3. **Prompt Length Sensitivity:** Vary number of preference pairs in prompt to identify optimal tradeoff between context usage and performance improvement