---
ver: rpa2
title: Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors
arxiv_id: '2506.05479'
source_url: https://arxiv.org/abs/2506.05479
tags:
- algorithm
- cost
- time
- step
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper considers a novel learning-augmented setting for Metrical
  Task Systems (MTS) where an algorithm can query only one of multiple heuristics
  at each time step, and the cost of a heuristic can only be estimated if it was also
  queried in the previous step. This setting introduces a bandit-feedback challenge
  that prevents direct application of classical full-feedback learning methods.
---

# Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors

## Quick Facts
- arXiv ID: 2506.05479
- Source URL: https://arxiv.org/abs/2506.05479
- Reference count: 37
- Primary result: Achieves O(OPT^2/3) regret for MTS with m-delayed bandit access to multiple heuristics

## Executive Summary
This paper introduces a novel learning-augmented setting for Metrical Task Systems (MTS) where algorithms can only query one of multiple heuristics per step, and can only observe a heuristic's cost if it was queried in the previous step. This m-delayed bandit access creates a challenging bandit-feedback problem that prevents direct application of classical full-feedback learning methods. The authors propose an algorithm that alternates between exploration and exploitation phases, using MTS-specific rounding techniques to bound switching costs. They prove both upper and lower bounds of Θ(OPT^2/3), establishing the tightness of their result.

## Method Summary
The algorithm maintains a distribution over ℓ heuristics using HEDGE or SHARE as an internal learner. It alternates between exploration (with probability ε) and exploitation. During exploration, it queries a random heuristic, skips m steps to bootstrap its state, then observes the cost and updates the distribution. During exploitation, it samples from the current distribution and follows the suggested state, using greedy steps when the heuristic's state is unknown. The rounding procedure uses Earth Mover Distance to bound switching costs between consecutive distributions. The doubling trick adapts parameters without knowing OPT in advance.

## Key Results
- Achieves O(OPT^2/3) regret with respect to the best static heuristic
- Proves matching lower bound of Ω(OPT^2/3), establishing tightness
- Extends approach to settings with k switching heuristics, achieving O(k^1/3 OPT^2/3) regret
- First tight bounds for this challenging MTS learning-augmented setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating exploration-exploitation with m-step bootstrapping enables bandit-style learning under delayed state observability
- Mechanism: With probability ε, sample heuristic uniformly, wait m steps to bootstrap its state, observe cost at step t+m, update weights via HEDGE/SHARE. During exploitation, sample from current weight distribution and follow suggested state. Greedy fallback when states unknown.
- Core assumption: Cost functions ct are fully observable
- Break condition: If ε too high relative to OPT^(-1/3), switching overhead dominates; if too low, insufficient signal accumulates

### Mechanism 2
- Claim: MTS-style rounding via Earth Mover Distance bounds switching costs while converting fractional expert weights to discrete state selections
- Mechanism: Maintain distributions xt ∈ Δℓ over heuristics. Use rounding procedure (Algorithm 4) that transfers probability mass according to EMD, ensuring expected movement cost equals EMD(pt-1, pt) rather than potentially larger distances from independent resampling
- Core assumption: Metric diameter D is bounded and known
- Break condition: If learning rate η is too aggressive, ||xt-1 - xt||1 becomes large, causing high switching overhead

### Mechanism 3
- Claim: Improper (greedy) steps during knowledge gaps prevent infinite costs while maintaining regret bounds
- Mechanism: During m steps before/after exploration (when exploited heuristic's state unknown), take greedy steps: st := argmin_s(d(bt, s) + ct(s)) where bt is last known state from most recent successful query
- Core assumption: Cost functions ct are observable; states with ct(s) = 0 exist at each time step
- Break condition: If greedy steps are not properly bounded, they can accumulate costs that cannot be charged to any heuristic's observed loss

## Foundational Learning

- Concept: **Metrical Task Systems (MTS)**
  - Why needed here: Underlying online optimization framework where decisions incur both service costs ct(st) and movement costs d(st-1, st)
  - Quick check question: Can you explain why worst-case competitive ratio is 2n-1 (deterministic) or Θ(log²n) (randomized), and why this motivates learning-augmented approaches?

- Concept: **Bandit Learning against Memory-Bounded Adversaries**
  - Why needed here: m-delayed bandit access directly maps to this setting where losses depend on last m actions; standard bandit estimators are biased
  - Quick check question: Why does the classic block-based approach (Arora et al.) fail in MTS when the adversary can place costs strategically at block boundaries?

- Concept: **HEDGE and SHARE Online Learning Algorithms**
  - Why needed here: Provide internal full-feedback learner with stability property (||xt-1 - xt||1 ≤ η · loss) essential for bounding switching costs
  - Quick check question: What is the difference between HEDGE (static best expert) and SHARE (tracking best expert with k switches), and when would you choose each?

## Architecture Onboarding

- Component map: Input -> Algorithm 1: Learning Dynamics -> Algorithm 2: MTS Solution Production -> Output
- Critical path: Exploration trigger → m-step bootstrap → cost observation → weight update → rounding → state selection. Regret bound O(OPT^2/3) hinges on balancing exploration frequency ε ∝ OPT^(-1/3), learning rate γ ∝ OPT^(-1/3), and switching overhead via stability property
- Design tradeoffs: Higher ε gives more signal but more exploration cost and switching overhead; higher m requires longer bootstrap phases; higher k allows dynamic benchmark but increases regret by k^(1/3) factor
- Failure signatures: Regret scales as O(T^2/3) rather than O(OPT^2/3): likely not properly estimating OPT; infinite cost incurred: greedy fallback not triggered when heuristic state unknown; competitive ratio > (1+ε) even for large OPT: exploration rate or learning rate misconfigured
- First 3 experiments:
  1. Synthetic caching instance with ℓ=3 heuristics (LRU, LFU, FIFO), m=2, varying OPT by scaling request sequence length. Verify regret scales as OPT^2/3
  2. Adversarial block-boundary attack: Construct instance where costs spike only at exploration phase boundaries. Confirm algorithm still achieves bounded regret (unlike naive block-based approaches)
  3. Parameter sensitivity: Grid search over ε, γ with fixed OPT. Confirm optimal setting matches theoretical ε ∝ (Dℓ ln ℓ)^(1/3) · OPT^(-1/3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the m^(2/3) dependence in the upper bound regret tight for all m ≥ 2, or can it be improved?
- Basis in paper: The paper proves a lower bound based on a construction with m=2, but the upper bound in Theorem 1.1 scales with m^(2/3). Section 4.1 discusses optimality of other parameters but only establishes the lower bound for the specific case of m=2
- Why unresolved: The provided lower bound construction does not scale with arbitrary bootstrapping times m, leaving the optimal dependence on the memory parameter undetermined
- What evidence would resolve it: A lower bound construction that scales as Ω(m^(2/3)) or an improved algorithm that achieves regret with a smaller dependence on m

### Open Question 2
- Question: Can the O(OPT^2/3) regret guarantee be extended to adaptive adversaries?
- Basis in paper: The analysis explicitly assumes an "oblivious" adversary that fixes the input instance before seeing the algorithm's random bits
- Why unresolved: Adaptive adversaries can react to the algorithm's exploration choices, potentially invalidating the stability analysis used to bound the switching costs in Lemma 3.7
- What evidence would resolve it: A modification of the algorithm or analysis that bounds regret even when cost functions ct depend on the algorithm's previous states, or a counter-example showing worse performance

### Open Question 3
- Question: Can the logarithmic gap between the upper and lower bounds be closed?
- Basis in paper: The paper establishes a lower bound of Ω̃(OPT^2/3) in Theorem 1.3, whereas the upper bound is O(OPT^2/3), leaving a logarithmic factor difference (specifically regarding ln ℓ) between the two results
- Why unresolved: The specific constants and logarithmic factors arising from the HEDGE algorithm and the lower bound construction do not match exactly
- What evidence would resolve it: An algorithm that removes the ln ℓ dependence from the leading term or a refined lower bound that explicitly includes it

## Limitations
- The algorithm requires knowledge of OPT to set exploration rate ε and learning rate η, though the doubling trick mitigates this
- Implementation of Earth Mover Distance rounding requires solving a coupling problem that may be computationally expensive for large ℓ
- The greedy fallback step assumes cost functions ct are observable (full input access), which may not hold in all online settings

## Confidence
- **High Confidence**: Regret bound proofs, lower bound construction, and core algorithmic mechanisms (exploration-exploitation alternation, EMD-based rounding, greedy fallback)
- **Medium Confidence**: Parameter tuning and hyperparameter choices (ε, η)
- **Low Confidence**: Practical implementation details for complex metric spaces and the greedy step computation in continuous domains

## Next Checks
1. Implement the doubling trick mechanism to verify adaptive parameter tuning achieves O(OPT^2/3) regret without prior knowledge of OPT
2. Test on a concrete MTS instance (e.g., caching with LRU/LFU/FIFO heuristics) to verify both theoretical bounds and practical performance
3. Evaluate sensitivity to the exploration parameter ε across different OPT regimes to confirm the theoretical scaling relationship