---
ver: rpa2
title: A Binary Classification Social Network Dataset for Graph Machine Learning
arxiv_id: '2503.02397'
source_url: https://arxiv.org/abs/2503.02397
tags:
- learning
- graph
- dataset
- machine
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiSND, a novel binary classification social
  network dataset for graph machine learning. It addresses the gap in available benchmark
  datasets by providing a real-world dataset with tabular and graph formats (node-only,
  undirected, directed).
---

# A Binary Classification Social Network Dataset for Graph Machine Learning

## Quick Facts
- **arXiv ID:** 2503.02397
- **Source URL:** https://arxiv.org/abs/2503.02397
- **Reference count:** 36
- **Primary result:** Introduced BiSND, a binary classification social network dataset with F1-scores ranging from 67.66 to 70.15

## Executive Summary
This paper introduces BiSND, a novel binary classification social network dataset specifically designed for graph machine learning research. The dataset addresses a critical gap in available benchmarks by providing a real-world dataset that supports multiple graph representations (node-only, undirected, directed) alongside tabular formats. The dataset consists of 12,788 Twitter users with 19 metadata features and 430 mention-based edges, making it highly sparse with 80.13% isolated nodes. The authors validate the dataset using diverse classifiers including traditional ML, deep neural networks, graph neural networks, and graph contrastive learning methods, establishing robust baseline performance metrics.

## Method Summary
The paper presents BiSND, a binary classification dataset created from Twitter user metadata and mention relationships. The dataset includes 12,788 nodes with 19 features per node and 430 directed edges representing user mentions. The authors employ multiple preprocessing steps including feature normalization to [0,1] scale and construct three graph variants: node-only (no edges), undirected, and directed. They evaluate the dataset using traditional ML classifiers (Random Forest, XGBoost, KNN, MLP), graph neural networks (GCN), and graph contrastive learning methods (DAENS). The evaluation protocol involves 20 execution runs with averaging to ensure result stability.

## Key Results
- BiSND achieves F1-scores ranging from 67.66 to 70.15 across different classification methods
- Random Forest (tabular) achieves 68.73% F1-score, demonstrating strong baseline performance
- Graph Contrastive Learning (DAENS 2D2D) achieves the highest score of 70.15% F1-score
- Directed graph variant outperforms undirected (67.25% vs 66.47% F1) and node-only (66.92% F1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-supervised Graph Contrastive Learning (GCL) extracts robust representations from sparse social structures better than supervised Graph Neural Networks (GNNs).
- **Mechanism:** The dataset has a very low Node Degree (0.01) and high sparsity (80% isolated nodes). Supervised GCNs struggle to propagate labels effectively across disconnected or weakly connected components. GCL methods (specifically DAENS) utilize heterogeneous data augmentation to generate contrasting views, forcing the model to learn intrinsic node features and structural invariances rather than relying on dense neighbor aggregation which is absent here.
- **Core assumption:** The performance gain is not solely due to the specific DAENS architecture but stems from the contrastive pre-training objective being more suited to low-density graphs than standard convolution.
- **Evidence anchors:** [table i] (BiSND ND=0.01, Isolated Nodes=80.13%), [section iv.f] (DAENS variants outperform GCN and MLP)

### Mechanism 2
- **Claim:** Directionality in social interactions provides a critical signal for binary classification that undirected graphs obfuscate.
- **Mechanism:** In BiSND, edges represent "mentions." A directed edge (User A mentions User B) implies an active engagement or status check. Preserving this direction (Directed Graph) allows the model to distinguish between the mentioner and the mentioned, which correlates with the "existing" vs. "deleted" status. Converting to an undirected graph treats the relationship symmetrically, diluting the causal signal of who initiated the interaction.
- **Core assumption:** The "mention" relationship is not purely symmetric in its information content regarding user existence.
- **Evidence anchors:** [section iv.e] (Directed variant shows best performance among GNN types, F1 67.25 vs 66.47 Undirected)

### Mechanism 3
- **Claim:** Tree-based ensemble methods act as a strong baseline by capturing non-linear feature interactions within the limited 19-feature metadata space.
- **Mechanism:** The tabular dataset contains only 19 features. Deep Neural Networks (MLP) often require vast data to learn weights for high-dimensional spaces or complex hierarchies, risking overfitting on small feature sets. Tree-based methods (Random Forest, XGBoost) explicitly model feature thresholds and interactions (e.g., "high follower count" AND "recent activity") without requiring backpropagation, making them highly efficient for this low-dimensional tabular context.
- **Core assumption:** The 19 metadata features contain sufficient predictive power on their own, reducing the necessity of structural (graph) data for high accuracy.
- **Evidence anchors:** [section iii.b] (F=19 identified features), [table vi] (Random Forest Entropy: 68.73 F1 vs MLP: 66.83 F1)

## Foundational Learning

- **Concept:** Graph Contrastive Learning (GCL)
  - **Why needed here:** This is the SOTA method identified in the paper (DAENS) for handling the dataset's extreme sparsity. Unlike supervised learning, it learns by contrasting augmented views of the graph, requiring no labels during training.
  - **Quick check question:** Can you explain why creating two different "views" of the same sparse graph and maximizing their agreement helps the model learn better than just feeding the raw graph into a GCN?

- **Concept:** Graph Sparsity and Isolated Nodes
  - **Why needed here:** BiSND has 80.13% isolated nodes (no edges). Standard GNN assumptions—that nodes classify similarly to their neighbors—break down here.
  - **Quick check question:** How does a GNN predict the class of a node that has zero neighbors (an isolated node), and why might a tabular classifier be superior in this specific scenario?

- **Concept:** Heterogeneous Data Augmentation
  - **Why needed here:** The winning model (DAENS) relies on this. It involves altering node features or edges differently based on node types or properties to create the contrastive views mentioned above.
  - **Quick check question:** If you randomly drop 50% of edges in a graph that already has very few edges (BiSND), what is the risk to the model's learning capability?

## Architecture Onboarding

- **Component map:** Raw Twitter Metadata (19 features) + Mention Edges -> Normalization (0-1 scale) -> Graph Construction (Adjacency Matrix A) -> Three branches: Tabular (X), Undirected G, Directed G -> Model Layer: Decision Tree / Random Forest / XGBoost (on X) / MLP (on X) / GCN (on G) / GCL (DAENS/GRACE on G) -> Binary Probability Score (Existing vs. Deleted)

- **Critical path:** The **Edge Construction Logic**. The paper defines edges based on mentions (`ui` mentions `uj`). This must be strictly implemented as a directed graph to replicate the 70.15% F1 score; converting to undirected drops performance.

- **Design tradeoffs:**
  - **Complexity vs. Accuracy:** GCL (DAENS 2D2D) achieves the highest accuracy (70.15%) but takes ~715s to run. Random Forest achieves 68.73% in 3.22s.
  - *Guidance:* If latency is a concern, use Random Forest. If maximizing ROC-AUC is the goal, use GCL.

- **Failure signatures:**
  - **GCN Underperformance:** If your GCN model performs significantly worse than the MLP or Tree models, check the graph loading pipeline—it likely failed to load the edges, forcing the GCN to rely solely on node features (acting like an MLP).
  - **Low Recall on Deleted Users:** The paper notes a class imbalance or difficulty in identifying deleted users (Recall often lower than Precision in Table VI). If Recall is unacceptably low (<70%), adjust the classification threshold or class weights.

- **First 3 experiments:**
  1. **Baseline Replication:** Run Random Forest (Entropy, Depth 9) on the tabular data to verify F1 ~68.7%. This validates data integrity.
  2. **Topology Ablation:** Train a GCN on "Node-only" vs. "Directed" graphs. Confirm that "Directed" outperforms "Node-only" (approx 67.25 vs 66.92) to prove the edges contain signal.
  3. **SOTA Verification:** Run DAENS 2D2D. If F1 is not ~70.15, check the augmentation parameters, as the sparsity requires careful handling to avoid dropping the few existing edges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel graph architectures or contrastive learning strategies be developed to improve the BiSND classification F1-score beyond the current 67-71% ceiling?
- Basis in paper: [explicit] The authors conclude that the experimental results indicate "promising avenues for future enhancements" in classification outcomes.
- Why unresolved: The current state-of-the-art methods (DAENS) achieve a maximum F1-score of 70.15%, leaving significant room for improvement in predictive accuracy.
- What evidence would resolve it: A study applying new Graph Contrastive Learning (GCL) techniques to BiSND that yields statistically significant F1-score gains over the reported baselines.

### Open Question 2
- Question: Does the extreme sparsity of the BiSND graph (high percentage of isolated nodes) limit the effectiveness of standard message-passing neural networks?
- Basis in paper: [inferred] Table I shows BiSND has a node degree of 0.01 and 80.13% "IN" (likely isolated nodes/non-zeros), whereas standard GNNs assume meaningful node connectivity for message passing.
- Why unresolved: While the paper verifies the dataset is robust, it does not analyze if the lack of edges creates a bottleneck for GNNs compared to tabular methods.
- What evidence would resolve it: An ablation study analyzing the performance gap of GNNs on the connected subset of nodes versus the isolated nodes in the dataset.

### Open Question 3
- Question: To what extent does the graph structure provide additive discriminative power over the 19 metadata features, given the small performance gap between tabular and graph methods?
- Basis in paper: [inferred] Results show Random Forest (tabular) achieved 68.73% F1-score, while the best Graph Contrastive Learning method achieved only 70.15%.
- Why unresolved: The marginal improvement suggests the graph topology may offer limited additional signal compared to the metadata features.
- What evidence would resolve it: A comparative analysis where models are trained on features only, structure only, and combined data to quantify the specific contribution of the graph edges.

## Limitations
- The dataset's extreme sparsity (80.13% isolated nodes) may limit generalizability to denser social networks
- The focus on binary classification restricts applicability to multi-class social network problems
- The paper does not explore hyperparameter optimization for baseline methods, potentially leaving performance gains unrealized

## Confidence
- **High Confidence:** Claims about dataset creation and basic statistical properties (sparsity, node degree, class distribution)
- **Medium Confidence:** Claims about classifier performance rankings, as these depend on implementation details not fully specified in the paper
- **Low Confidence:** Claims about mechanism explanations (why certain methods work better), as these require deeper theoretical justification beyond empirical results

## Next Checks
1. **Dataset Access Verification:** Obtain the actual BiSND dataset and verify the 12,788 node count, 19 features, and 430 edge statistics reported in the paper
2. **Reproducibility Benchmark:** Implement the Random Forest baseline with specified parameters (entropy criterion, depth 9) and verify it achieves the reported ~68.7% F1-score
3. **Graph Structure Impact:** Systematically compare the three graph variants (node-only, undirected, directed) using the same GCN architecture to validate the directional signal claim