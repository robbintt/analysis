---
ver: rpa2
title: 'Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time
  Series: Application in Scalable and Interpretable SEP Event Prediction'
arxiv_id: '2601.08999'
source_url: https://arxiv.org/abs/2601.08999
tags:
- counterfactual
- explanations
- data
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating interpretable and
  physically plausible counterfactual explanations for solar energetic particle (SEP)
  event prediction using large-scale multivariate time series (MVTS) data. The authors
  propose a Physics-Guided Counterfactual Explanation framework that extends the DiCE
  method by incorporating domain-specific physical constraints such as flux channel
  ordering, range bounds, and temporal consistency.
---

# Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction

## Quick Facts
- **arXiv ID:** 2601.08999
- **Source URL:** https://arxiv.org/abs/2601.08999
- **Reference count:** 21
- **Primary result:** PGCE achieves >80% DTW reduction, higher sparsity, ~50% runtime improvement vs DiCE baselines for SEP event prediction

## Executive Summary
This work addresses the challenge of generating interpretable and physically plausible counterfactual explanations for solar energetic particle (SEP) event prediction using large-scale multivariate time series (MVTS) data. The authors propose a Physics-Guided Counterfactual Explanation framework that extends the DiCE method by incorporating domain-specific physical constraints such as flux channel ordering, range bounds, and temporal consistency. The framework ensures that generated counterfactual explanations remain scientifically valid and actionable for space weather experts. Experimental results on GOES-based SEP forecasting datasets show that the proposed method achieves over 80% reduction in Dynamic Time Warping distance, higher sparsity, and nearly 50% runtime improvement compared to DiCE baselines. Additionally, the framework includes a reconstruction step that maps counterfactual perturbations back into physically consistent time series, enhancing interpretability and enabling actionable insights for scientific decision-making.

## Method Summary
The method trains a Random Forest classifier on GSEP dataset features (P3, P5, P7 proton flux channels) extracted via sliding windows (6-360 min mean flux). A custom `ConstrainedDiceGenetic` explainer extends DiCE with physics penalties enforcing flux channel ordering (P3 > P5 > P7), range bounds, and temporal consistency. Genetic optimization searches for counterfactuals while respecting these constraints. A reconstruction algorithm maps abstract feature-level counterfactuals back to continuous time series via mean-adjusted offsets. Hyperparameters are tuned via grid search optimizing F1 score, with proximity_weight=10, sparsity_weight=0.1, diversity_weight=1, ordering_penalty=10.

## Key Results
- >80% reduction in Dynamic Time Warping distance between original and counterfactual time series
- Higher sparsity in feature modifications compared to baseline DiCE
- Nearly 50% runtime improvement in counterfactual generation
- Maintained counterfactual fidelity (prediction flipping) at 1.0 precision/recall/F1

## Why This Works (Mechanism)

### Mechanism 1
Physics-guided penalty terms in the optimization objective constrain counterfactual candidates to remain within physically plausible regions of the search space. The objective function adds three penalty terms—ordering penalty (P_order) enforces monotonic flux relationships across energy channels (P3 > P5 > P7), range penalty (P_range) bounds values to empirical min-max distributions, and temporal smoothness (δ tolerance) penalizes abrupt inter-window discontinuities. These penalties are weighted by λ coefficients and minimized alongside proximity and validity losses during genetic optimization. Core assumption: The underlying physical constraints (ordering, ranges, smoothness) correctly characterize the true feasible region of SEP events; violations indicate scientifically invalid scenarios rather than model limitations.

### Mechanism 2
Window-aware encoding reduces search dimensionality while preserving temporal context, enabling scalable optimization on high-frequency MVTS data. Raw time series are aggregated into W windows via sliding-window mean extraction per channel. This produces an m×W feature matrix (channels × windows) that is vectorized for DiCE compatibility. Genetic operators (tournament selection, blend crossover, constrained mutation) operate on this reduced representation, with constraints enforced per-window to maintain intra-window ordering and inter-window continuity. Core assumption: Mean flux values per window capture sufficient discriminative information for classification and counterfactual reasoning; fine-grained temporal patterns within windows are less critical.

### Mechanism 3
Local-global reconstruction maps abstract counterfactual feature vectors back to continuous, temporally consistent time series for domain interpretation. For each window slice, the original segment is adjusted by computing Δ = (counterfactual mean) - (original segment mean). Overlapping adjustments are accumulated and averaged via division by a counter array, producing smooth offsets that are added to the original signal. This yields a reconstructed time series Y_pert that satisfies counterfactual feature values while preserving temporal structure. Core assumption: Linear offset addition to raw signals preserves physical plausibility; the relationship between window-level features and raw values is approximately invertible via mean-matching.

## Foundational Learning

- **Counterfactual Explanations**: Why needed here: Core framework for generating "what-if" scenarios that flip model predictions; requires understanding validity, proximity, sparsity, and diversity tradeoffs. Quick check question: Given a non-SEP instance, what minimal feature changes would cause the classifier to predict SEP while remaining physically plausible?

- **Dynamic Time Warping (DTW)**: Why needed here: Primary proximity metric for comparing original and counterfactual time series; accounts for temporal misalignment unlike Euclidean distance. Quick check question: Why is DTW preferred over L1/L2 distance when comparing two time series with potential phase shifts?

- **Genetic Algorithm Optimization**: Why needed here: DiCE backend for counterfactual search; operates via selection, crossover, and mutation without requiring gradient information (suitable for tree-based models). Quick check question: How does tournament selection balance exploitation (good candidates) vs exploration (diversity) in counterfactual generation?

## Architecture Onboarding

- **Component map**: Raw GOES data -> sliding-window feature extraction -> vectorized input x -> ConstrainedDiceGenetic explainer (physics penalties) -> counterfactual x' -> local-global reconstruction -> reconstructed time series Y_pert

- **Critical path**: 1) Extract window-level features from raw GOES proton flux data 2) Define physics constraints: ordering (P3 > P5 > P7), range bounds (min/max per channel/window), temporal tolerance δ 3) Initialize ConstrainedDiceGenetic with penalty weights (λ₁, λ₂, λ₃) 4) Run genetic optimization to generate counterfactual x′ 5) Apply Algorithm 2 reconstruction to produce Y_pert 6) Validate fidelity (classifier must predict flipped label on x′)

- **Design tradeoffs**: Proximity vs Diversity: Higher proximity weight reduces DTW distance but may limit exploration of alternative scenarios (DiCE achieves higher diversity, PGCE achieves higher proximity); Sparsity vs Physics Compliance: Strong ordering penalties (λ₂ = 10) may require modifying more features to maintain physical plausibility; Runtime vs Population Size: Larger genetic populations improve solution quality but increase computation; reported ~50% runtime reduction vs standard DiCE

- **Failure signatures**: Fidelity < 1.0: Counterfactual fails to flip prediction; increase validity weight λ₁ or population iterations; Ordering violations: Counterfactual shows x₁[wj] < x₂[wj]; increase ordering penalty λ₂; Reconstruction artifacts: Step discontinuities at window boundaries; decrease δ tolerance or increase overlap averaging; Runtime explosion: Dimensionality too high (small windows, many channels); reduce W or increase window size

- **First 3 experiments**: 1) Hyperparameter sweep: Grid search over proximity_weight ∈ {0.1, 1, 10}, sparsity_weight ∈ {0.1, 1, 10}, diversity_weight ∈ {0, 5, 10}, ordering_penalty ∈ {1, 10, 100}; evaluate DTW, sparsity, diversity; identify optimal config (reported: proximity=10, sparsity=0.1, diversity=1, ordering=10) 2) Baseline comparison: Generate counterfactuals for same test instances using standard DiCE vs PGCE; measure DTW, sparsity, diversity, runtime; expect PGCE to show lower DTW, higher sparsity, lower diversity, faster runtime 3) Fidelity validation: Pass all generated counterfactuals through pretrained Random Forest; verify classification matches intended target label; report precision/recall/F1 (target: 1.0 for all metrics)

## Open Questions the Paper Calls Out
1. Can physics-guided counterfactual explanations be effectively integrated with deep learning forecasting architectures while preserving interpretability? (explicit)
2. How does the physics-guided counterfactual framework perform under real-time streaming data conditions and distributed computation? (explicit)
3. How robust is the physics-guided framework to data quality issues such as missing values, sensor noise, and erroneous measurements? (inferred)
4. Can physics-guided counterfactual explanations generalize to other multivariate time series domains beyond solar physics? (inferred)

## Limitations
- Framework evaluated only on SEP forecasting with specific physical constraints (flux ordering, ranges) that may not transfer to other domains
- No systematic evaluation of robustness to missing values, sensor noise, or data quality degradation
- Integration with deep learning architectures not demonstrated; current implementation optimized for tree-based models

## Confidence
- Method Description: High
- Claims: High
- Reproducibility: Medium (key hyperparameters and implementation details unspecified)

## Next Checks
1. Verify ordering penalty calculation correctly rejects counterfactuals violating P3 > P5 > P7 constraint
2. Test reconstruction algorithm with overlapping windows to confirm temporal continuity
3. Validate counterfactual fidelity by passing all generated examples through Random Forest classifier