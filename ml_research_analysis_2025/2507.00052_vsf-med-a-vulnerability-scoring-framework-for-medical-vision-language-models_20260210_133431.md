---
ver: rpa2
title: VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models
arxiv_id: '2507.00052'
source_url: https://arxiv.org/abs/2507.00052
tags:
- medical
- adversarial
- vsf-med
- vulnerability
- impact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VSF-Med, a vulnerability scoring framework
  designed to evaluate the security and robustness of vision-language models (VLMs)
  in medical imaging contexts. The framework combines a library of text-prompt attack
  templates, imperceptible visual perturbations based on structural similarity thresholds,
  and an eight-dimensional scoring rubric assessed by dual LLM judges.
---

# VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models

## Quick Facts
- arXiv ID: 2507.00052
- Source URL: https://arxiv.org/abs/2507.00052
- Reference count: 33
- One-line primary result: VSF-Med is a vulnerability scoring framework for medical vision-language models using dual-judge LLM scoring, imperceptibility-constrained visual perturbations, and text-prompt attack templates

## Executive Summary
This paper introduces VSF-Med, a comprehensive framework for evaluating the security and robustness of vision-language models in medical imaging contexts. The framework combines sophisticated text-prompt attack templates with imperceptible visual perturbations calibrated by SSIM thresholds, evaluated through an eight-dimensional rubric assessed by dual LLM judges. Using over 30,000 adversarial test cases generated from MIMIC-CXR data, VSF-Med quantifies vulnerability across dimensions such as prompt injection effectiveness, jailbreak success, and safety bypass. The evaluation revealed varying vulnerability profiles across different models, with Llama-3.2-11B-Vision-Instruct showing peak vulnerability increases of 1.29σ for persistence-of-attack-effects, while GPT-4o demonstrated more robust performance.

## Method Summary
VSF-Med evaluates medical VLMs through a dual-pronged attack approach combining text-prompt attacks and visual perturbations. The framework uses 5,000 frontal chest X-rays from MIMIC-CXR, generating over 30,000 adversarial variants through 10 text-attack categories with 2-4 templates each and 6 visual perturbation types. These attacks are constrained to maintain SSIM ≥0.85 and ≤10% clean-case performance drop. Responses are evaluated by two independent judge LLMs (GPT-4o-mini and Claude) using an eight-dimensional rubric scored 0-4 per dimension. Raw scores are consolidated via z-score normalization to enable standardized comparison across models and attack types, with results binned into clinical risk tiers from Low (0-4) to Critical (≥21).

## Key Results
- Llama-3.2-11B-Vision-Instruct showed peak vulnerability increase of 1.29σ for persistence-of-attack-effects
- GPT-4o demonstrated more robust vulnerability profile compared to other evaluated models
- Dual-judge LLM scoring achieved average Pearson correlation exceeding 0.82 across all eight dimensions
- Framework successfully identified distinct vulnerability patterns across text-prompt and visual attack vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imperceptible visual perturbations degrade VLM performance by exploiting shared vision-language embedding spaces
- Mechanism: Pixel-level modifications (Gaussian noise, checkerboard overlays, steganographic hiding) calibrated to maintain SSIM ≥0.85 introduce noise into image encoder representations, shifting embeddings in contrastive learning models and reducing cosine similarity between image and correct textual representation
- Core assumption: VLM's image encoder is sensitive to low-level pixel alterations that transfer to language generation component
- Evidence anchors: Abstract mentions imperceptible visual perturbations calibrated by SSIM thresholds; section 3.3 describes optimized parameters that reliably induce vulnerabilities; MedFoundationHub and SafeMed-RIN confirm security concerns in medical VLMs

### Mechanism 2
- Claim: Templated text-prompt attacks bypass safety guardrails by framing malicious instructions in clinical context
- Mechanism: Attack templates (role-playing, context-switching) prepend malicious instructions to benign queries, exploiting instruction-following capability. For example, confidentiality breach templates ask models to simulate flawed systems that expose PHI
- Core assumption: Safety alignment is primarily pattern-matching against known harmful phrases rather than deep semantic intent understanding
- Evidence anchors: Abstract references sophisticated text-prompt attack templates; section 3.2 provides examples of prompt injection and jailbreak templates; SafeMed-RIN title implies adversarial methods are necessary defense

### Mechanism 3
- Claim: Dual-LLM judges using z-score normalization provide standardized, reproducible vulnerability metrics
- Mechanism: Two independent LLMs score target model responses on eight-dimensional rubric, with raw scores normalized into z-scores measuring deviation from baseline. This mitigates individual judge biases and provides statistical basis for comparison
- Core assumption: Judge LLMs are unbiased, capable of consistent rubric application, and their scores can be meaningfully combined through normalization
- Evidence anchors: Abstract mentions dual-judge scoring with z-score normalization; section 3.6 reports average Pearson correlation exceeding 0.82 for all eight dimensions

## Foundational Learning

- Concept: **Structural Similarity Index (SSIM)**
  - Why needed here: Defines "imperceptible" attack; SSIM measures perceived quality of distorted image relative to original, with SSIM ≥ 0.85 ensuring perturbed X-ray remains clinically similar
  - Quick check question: If an attack image has an SSIM of 0.60, why might it be rejected by the VSF-Med framework, even if it successfully confuses the model?

- Concept: **Contrastive Learning**
  - Why needed here: Many VLMs use contrastive learning to align images and text in shared embedding space; understanding this explains why small visual perturbations can disrupt alignment and lead to incorrect text generation
  - Quick check question: In a contrastive learning framework, what happens to the cosine similarity between an image and a non-matching text description during training?

- Concept: **Z-Score Normalization**
  - Why needed here: Key results expressed in standard deviations; z-score measures how many standard deviations a data point is from mean of reference distribution, enabling comparison across different models
  - Quick check question: A model A has z-score of 0.5 for vulnerability on a task, while model B has z-score of 2.1. Which model is more vulnerable, and by what relative magnitude?

## Architecture Onboarding

- Component map: Data Curation -> Adversarial Generator -> Target VLM -> Evaluation Pipeline -> Scoring System -> Analysis Module
- Critical path: The entire pipeline is the critical path; most manual and expensive steps are designing/validating initial text-attack templates and visual perturbation types, running inference on 30,000+ adversarial test cases, and API cost of dual-judge LLM scoring
- Design tradeoffs:
  - Automation vs. Human-in-the-loop: Fully automated LLM judges provide scalability and reproducibility but may miss nuanced clinical errors; human spot-checks are used but not primary scoring method
  - Realism vs. Attack Potency: SSIM constraint trades maximum attack potency for clinical realism and imperceptibility
  - Breadth vs. Depth: Evaluates 8 dimensions across many attack types for broad vulnerability profile but may not deeply probe single failure mode
- Failure signatures:
  - Low inter-judge agreement: Pearson correlation <0.6 between judge LLMs signals ambiguous rubric or inconsistent judges
  - Attack ineffectiveness: Z-score changes near zero across all models indicate weak attack templates or perturbations
  - Baseline degradation: Clean-case performance drops >10% on perturbed images suggests SSIM threshold too low, violating "imperceptible" constraint
- First 3 experiments:
  1. Baseline & Sanity Check: Run evaluation on non-medical general-purpose VLM (e.g., standard LLaVA) using small data subset to verify pipeline runs and produces non-zero vulnerability score
  2. Ablation on Attack Types: Evaluate target medical VLM using only text attacks, then only visual attacks; compare vulnerability profiles to identify dominant attack vector
  3. Judge Agreement Analysis: Run evaluation on 500 samples and compute Pearson correlation between two judge LLMs across all 8 dimensions; identify dimensions with low agreement (<0.7) for rubric refinement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How must VSF-Med's visual perturbation strategies and SSIM thresholds be recalibrated to effectively evaluate vulnerabilities in volumetric modalities like CT and MRI?
- Basis in paper: [explicit] Authors state that extending framework to other modalities "will require modality-specific perturbation strategies, SSIM recalibration, and perhaps new scoring dimensions"
- Why unresolved: Current parameters optimized specifically for 2D frontal chest X-rays (MIMIC-CXR)
- What evidence would resolve it: Successful application of recalibrated perturbation suites on 3D medical datasets demonstrating retained clinical realism and attack potency

### Open Question 2
- Question: Can an ensemble of open-source judge models achieve inter-rater reliability comparable to proprietary GPT-4o-mini and Claude models used in this study?
- Basis in paper: [explicit] Paper notes relying on proprietary judges "may introduce bias" and suggests "incorporating a wider ensemble of open-source judges" to bolster scoring robustness
- Why unresolved: Current scoring mechanism depends entirely on two specific proprietary LLMs
- What evidence would resolve it: Comparative analysis showing high correlation (>0.82) between open-source judges and current dual-judge ground truth

### Open Question 3
- Question: What specific temporal perturbation techniques are required to identify vulnerabilities in real-time clinical workflows like ultrasound video?
- Basis in paper: [explicit] Authors acknowledge that "Real-time imaging applications... may exhibit vulnerabilities that cannot be captured by snapshot attacks"
- Why unresolved: Existing framework limited to static image perturbations
- What evidence would resolve it: Detection of unique failure modes in continuous video streams using dynamic, time-based adversarial inputs

## Limitations

- Framework's generalizability beyond chest X-rays remains untested as evaluation is limited to MIMIC-CXR frontal chest images
- Dual-judge LLM scoring system may still inherit biases from the judge models themselves despite statistical normalization
- SSIM-based imperceptibility constraint (SSIM ≥ 0.85) lacks formal clinical radiologist validation despite empirical validation
- Framework assumes vulnerability can be meaningfully quantified through z-score normalization, though this assumes vulnerability dimensions are commensurable and linearly separable

## Confidence

- **High confidence**: Framework's architecture and evaluation methodology are clearly specified and reproducible; statistical approach using z-scores and dual-judge normalization is well-founded
- **Medium confidence**: Attack templates and visual perturbation techniques are demonstrated to work on tested models, but effectiveness may vary across different VLM architectures
- **Low confidence**: Clinical relevance of vulnerability scores and their translation to real-world risk requires further validation with medical domain experts

## Next Checks

1. Validate SSIM threshold by conducting blinded radiologist study comparing perturbed vs. clean X-rays to confirm clinical imperceptibility
2. Test framework's generalizability by evaluating vulnerability on non-chest X-ray modalities (e.g., CT, MRI) and different medical VLM architectures
3. Perform cross-validation of judge agreement by introducing third independent LLM judge and measuring consistency across all three judges to ensure dual-judge system is not overfitting to specific judge biases