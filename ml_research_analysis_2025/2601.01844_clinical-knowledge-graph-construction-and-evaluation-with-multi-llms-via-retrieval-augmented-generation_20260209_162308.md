---
ver: rpa2
title: Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented
  Generation
arxiv_id: '2601.01844'
source_url: https://arxiv.org/abs/2601.01844
tags:
- clinical
- semantic
- knowledge
- graph
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of constructing clinically accurate
  knowledge graphs from unstructured oncology narratives. It proposes a multi-agent
  LLM pipeline integrating Gemini 2.0 Flash for schema-guided EAV extraction, GPT-4o
  for contextual refinement, and Grok 3 for validation, using ontology alignment and
  iterative refinement.
---

# Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2601.01844
- Source URL: https://arxiv.org/abs/2601.01844
- Authors: Udiptaman Das; Krishnasai B. Atmakuri; Duy Ho; Chi Lee; Yugyung Lee
- Reference count: 24
- Constructs clinically accurate knowledge graphs from unstructured oncology narratives using multi-agent LLM pipeline with RAG

## Executive Summary
This study addresses the challenge of constructing clinically accurate knowledge graphs from unstructured oncology narratives. It proposes a multi-agent LLM pipeline integrating Gemini 2.0 Flash for schema-guided EAV extraction, GPT-4o for contextual refinement, and Grok 3 for validation, using ontology alignment and iterative refinement. Applied to 40 oncology reports (PDAC and BRCA), the method achieved over 97% entity and attribute correctness with minimal hallucination rates, and produced SPARQL-compatible, ontology-grounded graphs. The approach outperformed baseline methods in precision, semantic coherence, and clinical relevance, enabling scalable and explainable clinical KG construction directly from free text without requiring gold-standard labels.

## Method Summary
The method employs a five-stage pipeline: (1) FHIR-guided EAV extraction using Gemini 2.0 Flash with entropy-based confidence scoring, (2) ontology mapping to SNOMED CT/LOINC/RxNorm/GO/ICD via lexical/semantic similarity, (3) relation discovery using Gemini and GPT-4o with plausibility scoring, (4) RDF/RDFS/OWL encoding with SWRL rules, and (5) multi-LLM trust validation using a composite function T(τ) = λ1R(τ) + λ2C(τ) + λ3J(τ). The approach processes 40 oncology reports from the CORAL dataset, achieving high correctness rates through consensus-based hallucination suppression and ontology grounding.

## Key Results
- Entity correctness >99%, attribute correctness >97% across PDAC and BRCA reports
- Hallucination rates: Gemini 0.18%, Grok 0.04%, GPT-4o 0.92%
- SPARQL compatibility >96% with minimal domain/range inconsistencies
- Ontology unmapped rates: PDAC 0.69%, BRCA 0.85%

## Why This Works (Mechanism)

### Mechanism 1: Multi-LLM Consensus for Hallucination Suppression
Staggering three LLMs across extraction, refinement, and adversarial validation reduces hallucination rates compared to single-model pipelines. Gemini 2.0 Flash generates schema-constrained EAV candidates; GPT-4o performs contextual plausibility scoring; Grok 3 applies counterfactual perturbation to flag contradictions. Triples require agreement from ≥2 models to pass. This assumes hallucination patterns are not systematically correlated across architectures.

### Mechanism 2: Entropy-Based Confidence Flagging
Token-level entropy H(vj) computed during value extraction predicts extraction uncertainty and enables targeted re-validation. For each extracted value vj with sub-token distribution P(vj), compute H(vj) = −Σpt·log(pt). Values exceeding threshold δ are flagged for multi-model filtering rather than automatic acceptance. This assumes high entropy correlates with factual errors rather than legitimate clinical uncertainty.

### Mechanism 3: Ontology Alignment as External Grounding
Mapping extracted entities/attributes to SNOMED CT, LOINC, RxNorm, GO, ICD before graph encoding provides external semantic validation and reduces domain drift. Raw terms are scored via Score(ci, oj) = α·simlex + β·simsem; mapped concepts are typed as OWL classes with domain/range constraints. This assumes biomedical ontologies cover clinical oncology terminology sufficiently.

## Foundational Learning

- **Entity-Attribute-Value (EAV) Modeling**: The entire extraction stage outputs (entity, attribute, value) triples like (Procedure, performed_by, SurgicalOncologist) as atomic knowledge units. Why needed: Core representation format for clinical knowledge extraction. Quick check: Given "CA 19-9 elevated at 142 U/mL," can you identify entity, attribute, and value?

- **RDF/RDFS/OWL Semantic Web Stack**: Stage 4 encodes all validated triples as RDF with RDFS domain/range constraints and OWL subclass axioms; outputs must be SPARQL-queryable. Why needed: Enables semantic reasoning and standardized graph querying. Quick check: What is the difference between rdf:type and rdfs:subClassOf?

- **Retrieval-Augmented Generation (RAG)**: The KG-RAG strategy retrieves ontology concepts and evidence during extraction rather than relying purely on parametric LLM knowledge. Why needed: Provides external grounding for factual accuracy. Quick check: How does RAG differ from standard few-shot prompting for factual grounding?

## Architecture Onboarding

- **Component map**: EAV Extraction (Gemini 2.0 Flash) -> Entropy Flagging -> Ontology Mapping (Gemini) -> GPT-4o Plausibility Scoring -> Grok 3 Adversarial Filter -> RDF Encoding -> Trust Filtering (threshold δT=0.65)

- **Critical path**: EAV Extraction → Entropy Flagging → Ontology Mapping → GPT-4o Plausibility → Grok 3 Adversarial Filter → RDF Encoding → Trust Filtering (threshold δT=0.65)

- **Design tradeoffs**: Gemini prioritizes precision (99.4% entity correctness) but limited inference; GPT-4o excels at implicit/cross-sentence relations (3.2 cross-sentence inferences vs 1.0 for Gemini) but higher hallucination (0.92%). PDAC achieves higher correctness (73.22%) with lower coverage (29.74%); BRCA shows inverse pattern. Multi-model consensus adds latency but reduces hallucination by ~5x vs single-model baselines.

- **Failure signatures**: High-entropy values (H > δ) requiring manual review; low consensus (1/3 model agreement) on relation triples; domain/range violations caught during OWL validation (BRCA: 41, PDAC: 25 inconsistencies); unmapped terms >1% indicating schema drift.

- **First 3 experiments**: (1) Single-model baseline comparison: Run EAV extraction using only Gemini 2.0 Flash without GPT-4o/Grok validation; measure hallucination rate vs multi-agent pipeline on 5 held-out reports. (2) Entropy threshold sensitivity: Vary δ across {0.3, 0.5, 0.7, 0.9}; plot precision-recall curve for flagged vs accepted triples. (3) Ontology coverage audit: On 10 PDAC and 10 BRCA reports, manually annotate unmapped terms; classify as (a) true errors, (b) novel biomarkers, or (c) institutional shorthand.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the integration of structured EHR data and imaging modalities impact the semantic coherence and completeness of the constructed knowledge graphs? The authors state the framework "operates solely on unstructured clinical text, omitting other critical modalities such as imaging... and structured EHR data." This requires new alignment mechanisms to fuse structured codes and visual data into the RDF/OWL schema.

- **Open Question 2**: Does the high entity correctness and low hallucination rate generalize to multi-institutional datasets with varying narrative styles? The authors note that the "evaluation, based on 40 oncology reports (PDAC and BRCA) from the CORAL dataset, also limits generalizability across broader clinical settings." Different institutions may exhibit distinct documentation artifacts affecting prompt tuning effectiveness.

- **Open Question 3**: Can the computational overhead of the multi-agent validation pipeline be reduced to support real-time clinical decision support systems (CDSS)? The paper lists "real-time CDSS integration" as a future direction and admits the system is "not yet integrated with live clinical decision support systems." The sequential consensus process may introduce latency incompatible with real-time clinical workflows.

## Limitations
- Prompt engineering details are not fully specified, making exact replication difficult
- Cross-model hallucination correlation is assumed but not empirically tested
- Entropy threshold calibration lacks systematic validation across clinical domains
- Ontology coverage adequacy for emerging oncology biomarkers not evaluated

## Confidence
- Entity/attribute extraction correctness (>97%): High - supported by multiple model outputs and external validation
- Hallucination suppression mechanism: Medium - theoretical soundness demonstrated but correlation patterns untested
- Ontology alignment effectiveness: Medium - coverage metrics provided but unmapped term classification not detailed
- Cross-sentence relation inference capability: Low-Medium - GPT-4o performance superior but methodology underspecified

## Next Checks
1. Conduct ablation study comparing hallucination rates between single-model and multi-agent pipelines on identical clinical reports
2. Perform ontology coverage audit to classify unmapped terms as true errors vs. novel/valid oncology concepts
3. Test entropy-based confidence scoring across multiple clinical domains to validate generalizability beyond oncology