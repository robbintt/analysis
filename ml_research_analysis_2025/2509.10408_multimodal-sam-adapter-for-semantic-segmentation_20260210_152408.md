---
ver: rpa2
title: Multimodal SAM-adapter for Semantic Segmentation
arxiv_id: '2509.10408'
source_url: https://arxiv.org/abs/2509.10408
tags:
- multimodal
- segmentation
- features
- fusion
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MM SAM-adapter, a novel framework that extends
  the Segment Anything Model (SAM) for multimodal semantic segmentation. The approach
  employs an adapter network that fuses features from auxiliary modalities (e.g.,
  LiDAR, infrared) with SAM's RGB features, enabling selective incorporation of multimodal
  information only when beneficial.
---

# Multimodal SAM-adapter for Semantic Segmentation

## Quick Facts
- **arXiv ID**: 2509.10408
- **Source URL**: https://arxiv.org/abs/2509.10408
- **Reference count**: 40
- **Primary result**: State-of-the-art multimodal semantic segmentation on DeLiVER, FMB, and MUSES benchmarks using an asymmetric adapter architecture.

## Executive Summary
This paper introduces MM SAM-adapter, a framework that extends the Segment Anything Model (SAM) for multimodal semantic segmentation by fusing auxiliary modalities with RGB through an adapter network. The approach employs asymmetric architecture that prioritizes SAM's RGB knowledge while selectively incorporating multimodal information only when beneficial. Evaluated across three benchmarks, the method achieves state-of-the-art performance by maintaining strong results in both "RGB-easy" and "RGB-hard" conditions, demonstrating robust scene understanding when RGB information alone is insufficient.

## Method Summary
The MM SAM-adapter framework uses SAM's ViT-L encoder as the main branch with a lighter ConvNeXt-S encoder for the auxiliary branch. A Road-Fusion module fuses RGB and auxiliary features before they are injected into SAM via cross-attention mechanisms. The adapter iteratively refines features through bi-directional flow between branches, with a SegFormer decoder producing final segmentation. The asymmetric design prevents performance degradation in easy RGB scenarios while improving challenging conditions where auxiliary modalities provide critical information.

## Key Results
- Achieves state-of-the-art mIoU on DeLiVER (57.14), FMB (53.22), and MUSES (66.41) benchmarks
- Maintains high RGB-easy performance (57.75 mIoU) while improving RGB-hard conditions through multimodal fusion
- Asymmetric architecture outperforms symmetric alternatives, validating the design choice to prioritize SAM's RGB knowledge
- Demonstrates consistent improvements over competing methods across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Feature Injection
The model prioritizes foundational RGB knowledge by using a heavy SAM encoder (ViT-L) as main branch and lighter ConvNeXt-S encoder for adapter. This parameter imbalance forces reliance on SAM's robust RGB priors unless adapter provides strong corrective signal. Symmetric models perform worse (55.49 vs 57.75 mIoU), underscoring importance of asymmetric structure.

### Mechanism 2: Fused Selective Gating
Adapter receives pre-fused RGB and auxiliary features rather than just auxiliary, enabling selective utilization of auxiliary modality only when beneficial. In RGB-easy cases, fusion relies on high-quality RGB features, minimizing noisy auxiliary signal. In RGB-hard cases, fusion emphasizes auxiliary modality. RGB & LiDAR adapter maintains high RGB-easy mIoU (57.75) vs LiDAR-only adapter (55.49).

### Mechanism 3: Bi-Directional Knowledge Refinement
Injector (SAM queries Adapter) contaminates SAM features with multimodal spatial priors. Extractor (Adapter queries SAM) retrieves refined SAM features back into adapter stream. This bi-directional flow ensures adapter's multimodal tokens are constantly updated with SAM's high-level semantic context through MSDA attention.

## Foundational Learning

- **Vision Transformers (ViT) & SAM Architecture**: Understanding patch embedding, positional embeddings, and global self-attention is crucial for troubleshooting feature injection points. *Quick check*: Can you explain how a standard ViT processes image into tokens and where Block boundaries are in SAM?

- **Side-Tuning / Adapter Networks**: The paper uses side-network (Adapter) rather than fine-tuning backbone or adding LoRA. Understanding parallel execution and attention interaction is crucial. *Quick check*: How does information flow differ between LoRA adapter and ViT-adapter side-network?

- **Cross-Attention Mechanics**: Injector and Extractor rely on cross-attention (MSDA). One stream queries the other. Need to know Query vs Key/Value to debug feature alignment. *Quick check*: In Injector, does SAM backbone query Adapter or Adapter query SAM backbone?

## Architecture Onboarding

- **Component map**: Data -> ConvNext Encoders -> Fusion Module -> Injector (Cross-Attn) -> SAM Block -> Extractor (Cross-Attn) -> Loop N=4 times -> Decoder
- **Critical path**: RGB Image + Auxiliary Map -> ConvNeXt-S Encoders -> Road-Fusion -> Injector -> SAM Block -> Extractor -> Loop 4 times -> SegFormer Decoder
- **Design tradeoffs**: Do NOT balance backbone sizes (keep Adapter lighter); Do NOT feed raw Auxiliary data directly to Adapter (fuse with RGB first); Fine-tuning SAM works better than freezing
- **Failure signatures**: Performance drop on RGB-Easy (likely asymmetric fusion or LiDAR-only adapter); Catastrophic Forgetting (too high SAM backbone LR or large adapter weights initialization)
- **First 3 experiments**: 1) Run "SAM + SH" (No adapter) baseline; 2) Implement "SAM + Adapter" with only RGB input to verify adapter mechanism; 3) Add Fusion Module with target auxiliary modality and compare mIoU on RGB-easy vs RGB-hard splits

## Open Questions the Paper Calls Out

- **Extension to Multiple Modalities**: How can MM SAM-adapter framework be extended to simultaneously process more than two input modalities? The Road-Fusion module constrains current architecture to binary fusion, requiring innovative fusion module design for 3+ modalities.

- **Generalization to Other Tasks**: Can adapter-based adaptation strategy be effectively generalized to other dense prediction tasks such as panoptic segmentation? The current focus on semantic segmentation leaves panoptic segmentation as promising future research.

- **Performance in Uniformly Degraded RGB**: Does asymmetric architecture limit performance in environments where auxiliary modality is consistently more informative than RGB? The design prioritizes RGB priors but may cap performance when RGB is uniformly low-quality across entire dataset.

## Limitations

- Architectural choices lack comprehensive ablation validation, particularly the specific 4:1 parameter ratio and Road-Fusion module's isolated contribution
- "RGB-hard" subset partitioning methodology is described but not standardized, raising reproducibility concerns across different datasets
- Bi-directional knowledge refinement contribution is theoretically underspecified with no ablation isolating extractor's impact

## Confidence

- **High Confidence**: Asymmetric architecture design choice and empirical validation (mIoU drop when symmetric) - directly supported by Table 9 results
- **Medium Confidence**: Fused selective gating mechanism's effectiveness - Table 9 supports fused inputs outperform auxiliary-only, but specific gating behavior lacks direct analysis
- **Low Confidence**: Bi-directional knowledge refinement contribution - paper claims alignment benefit but provides no ablation isolating extractor's impact

## Next Checks

1. **Ablation of Fusion Module**: Compare fused adapter (RGB + LiDAR input) against LiDAR-only adapter and simple addition fusion to quantify Road-Fusion's specific contribution to RGB-easy performance maintenance

2. **Symmetric Architecture Scaling**: Implement version with balanced parameter counts between SAM and auxiliary branches to validate whether 4:1 ratio is optimal or if other ratios yield better performance

3. **Extractor Isolation Test**: Remove extractor component while keeping injector, then measure performance degradation to confirm bi-directional flow's necessity for multimodal alignment