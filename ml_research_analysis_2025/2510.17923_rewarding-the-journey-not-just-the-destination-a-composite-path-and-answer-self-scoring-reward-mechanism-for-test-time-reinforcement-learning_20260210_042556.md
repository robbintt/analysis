---
ver: rpa2
title: 'Rewarding the Journey, Not Just the Destination: A Composite Path and Answer
  Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning'
arxiv_id: '2510.17923'
source_url: https://arxiv.org/abs/2510.17923
tags:
- arxiv
- reasoning
- reward
- learning
- compass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scalability challenge in reinforcement
  learning for large language models (LLMs) by proposing a method to enable self-improvement
  on unlabeled data without external supervision. The core contribution is COMPASS,
  a novel reward mechanism that combines two complementary components: the Dual-Calibration
  Answer Reward (DCAR), which establishes trustworthy pseudo-labels through confidence
  and credibility calibration, and the Decisive Path Reward (DPR), which optimizes
  reasoning process quality beyond outcome supervision.'
---

# Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2510.17923
- **Source URL:** https://arxiv.org/abs/2510.17923
- **Authors:** Jingyu Xing; Chenwei Tang; Xinyu Liu; Deng Xiong; Shudong Huang; Wei Ju; Jiancheng Lv; Ziyue Qiao
- **Reference count:** 5
- **Primary result:** COMPASS achieves up to 15.8% improvement on GPQA and 12.3% on AIME using Qwen2.5-Math-1.5B

## Executive Summary
This paper addresses the scalability challenge in reinforcement learning for large language models by proposing COMPASS, a novel reward mechanism that enables self-improvement on unlabeled data without external supervision. The core innovation combines Dual-Calibration Answer Reward (DCAR) for trustworthy pseudo-label generation through confidence and credibility calibration, with Decisive Path Reward (DPR) for optimizing reasoning process quality beyond outcome supervision. Extensive experiments demonstrate COMPASS's effectiveness across diverse reasoning tasks (AIME, AMC, MATH, GPQA) and model architectures, achieving significant performance gains over baseline methods like TTRL.

## Method Summary
COMPASS uses GRPO (Group Relative Policy Optimization) with a composite reward consisting of DCAR and DPR. DCAR constructs pseudo-labels via confidence-calibrated self-consistency, where confidence is defined as the exponential of the negative standard deviation of token probability differences. It weighs votes by confidence and scales rewards by credibility (ratio of general to elite confidence). DPR provides entropy-weighted decisiveness rewards, prioritizing confident decisions at high-uncertainty reasoning junctures. The method requires 64 rollouts per prompt for pseudo-label generation and 32 for training, using AdamW optimizer with cosine learning rate schedule.

## Key Results
- COMPASS achieves up to 15.8% improvement on GPQA and 12.3% on AIME using Qwen2.5-Math-1.5B
- DPR adds 2-5% performance gains over DCAR alone on MATH and GPQA datasets
- COMPASS consistently scales with model size, showing increasing gains from 1.5B to 7B parameters
- Method shows 11.1% improvement on MATH-500 and 12.3% on AIME for Qwen2.5-Math-1.5B

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Calibrated Consensus
If token-level decisiveness correlates with answer correctness, weighting votes by this signal improves pseudo-label accuracy over raw majority voting. The model calculates $PD_{topk}$ (probability difference between top-1 and top-2 tokens) for each generated token, aggregates this into a confidence score $c(\hat{y}_i) = \exp(-\text{std}_t PD_{topk}(x_t))$, and selects consensus answers based on the sum of confidence weights rather than raw counts.

### Mechanism 2: Credibility-Gated Reward Scaling
If a consensus answer lacks support from the model's "most confident" reasoning paths, the reward signal should be suppressed to prevent reinforcing potentially false agreements. COMPASS compares the confidence of the group supporting the consensus ($C_{General}$) against the single most confident response among all samples ($C_{Elite}$), scaling the final answer reward by the ratio $S_{cred} = C_{General} / C_{Elite}$.

### Mechanism 3: Entropy-Weighted Decisiveness (DPR)
Reinforcing decisive token selection at moments of high uncertainty (entropy) improves the reasoning process, provided the model has sufficient foundational knowledge. DPR computes per-token rewards that weight the "decisiveness" ($d_t$) by the entropy ($w_t \propto e^{h_t}$) at that step, prioritizing confident decisions at branching points over obvious ones.

## Foundational Learning

- **Concept: Self-Consistency (Majority Voting)**
  - **Why needed here:** COMPASS modifies the TTRL baseline that relies on majority voting to generate pseudo-labels
  - **Quick check question:** Can you explain why majority voting might fail if the model has a systematic bias?

- **Concept: Token Logits and Entropy**
  - **Why needed here:** The core innovation uses specific statistics of the output distribution (top-1 vs. top-2 probability gap and entropy) rather than just the final text
  - **Quick check question:** Does a low entropy distribution always indicate the answer is factually correct, or just that the model is internally consistent?

- **Concept: Process Reward Models (PRM) vs. Outcome Reward Models (ORM)**
  - **Why needed here:** COMPASS introduces DPR as a dense, process-based reward contrasting with the sparse, outcome-based DCAR
  - **Quick check question:** Why is a dense reward signal (per token/step) generally easier for optimization than a sparse reward (final answer only)?

## Architecture Onboarding

- **Component map:** Rollout -> DCAR Module -> DPR Module -> Optimizer
- **Critical path:** The computation of $PD_{topk}$ (Equation 1) is critical; errors here cascade into both the pseudo-label selection (DCAR) and the process reward (DPR)
- **Design tradeoffs:**
  - Sampling cost: The method requires large $N$ (e.g., 64 samples initially) to establish reliable consensus, increasing inference cost during training
  - Model Scale: The DPR mechanism assumes the model's uncertainty is meaningful. For very small or weak models, this signal appears to be noise, leading to performance degradation (as seen in the LLaMA-1B ablation)
- **Failure signatures:**
  - Degradation on Hard Tasks: If training loss diverges or performance drops on complex reasoning tasks (like AIME for 1B models), verify if DPR is reinforcing high-entropy noise
  - Reward Hacking: If the "Elite" confidence is consistently higher for wrong answers, the credibility scaling in DCAR may excessively penalize the correct consensus
- **First 3 experiments:**
  1. Sanity Check (Correlation): Reproduce Figure 2 on your target model. Verify that $PD_{topk}$ actually correlates with correctness before enabling the full mechanism
  2. Component Ablation: Run COMPASS vs. "COMPASS w/o DPR" vs. "COMPASS w/o Credibility" on a subset of MATH-500 to isolate gains
  3. Scaling Test: Compare performance gains on 1.5B vs. 7B parameters to ensure the model is sufficiently strong for the entropy-weighting logic to function as intended

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text. However, the limitations section raises implicit questions about the method's applicability to models with insufficient foundational knowledge, its effectiveness in continuous lifelong learning settings, and its performance on creative or subjective tasks where high confidence is not strictly synonymous with correctness.

## Limitations

- GRPO hyperparameters are not specified, making exact reproduction challenging without extensive hyperparameter search
- The method requires sufficiently large models to function properly, as evidenced by failure on LLaMA-3.2-1B for AIME
- Substantial computational overhead from 64 rollouts for pseudo-label generation and 32 for training
- Potential reward hacking if high-confidence paths consistently produce incorrect answers

## Confidence

- **High Confidence**: The core DCAR mechanism is well-specified and supported by correlation analysis showing 11.1% improvement on MATH-500
- **Medium Confidence**: DPR's theoretical justification is sound but practical impact is modest (2-5% gains), requiring more empirical validation
- **Low Confidence**: Scalability claims are undermined by documented failure on 1B models, requiring qualification about minimum effective model size

## Next Checks

1. **Correlation Verification**: Reproduce Figure 2's PD_topk vs. correctness analysis on your target model before implementing full COMPASS. If the correlation is weak or absent, the core DCAR assumption fails and the method won't work.

2. **Component Ablation Study**: Run COMPASS vs. COMPASS w/o DPR vs. COMPASS w/o Credibility on a subset of MATH-500 to isolate each component's contribution. This will verify whether DPR's 2-5% gain is consistent across datasets and model scales.

3. **Scale Breakpoint Analysis**: Systematically test COMPASS across 1.5B, 7B, and 13B models on AIME to identify the minimum effective model size. Document the performance degradation pattern to establish clear operational boundaries for the method.