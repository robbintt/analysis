---
ver: rpa2
title: 'Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control
  in LLMs'
arxiv_id: '2601.18483'
source_url: https://arxiv.org/abs/2601.18483
tags:
- concept
- level
- generation
- control
- single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be controlled for single stylistic concepts
  (e.g., humor, persuasiveness) through prompting, but performance drops when two
  concepts are controlled simultaneously. Experiments with 7B-14B models across three
  tasks (argument, story, and structured text generation) showed strong single-concept
  control (Spearman correlations up to 0.98) but significant degradation in dual-concept
  settings (correlations dropping by 0.2-0.5).
---

# Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs

## Quick Facts
- arXiv ID: 2601.18483
- Source URL: https://arxiv.org/abs/2601.18483
- Reference count: 35
- Large language models can be controlled for single stylistic concepts (e.g., humor, persuasiveness) through prompting, but performance drops when two concepts are controlled simultaneously.

## Executive Summary
This paper evaluates fine-grained single- and dual-concept control in large language models across six stylistic concepts (humor, persuasiveness, clarity, politeness, assertiveness, formality) at five discrete levels. Experiments with 7B-14B models across three tasks (argument, story, and structured text generation) show strong single-concept control (Spearman correlations up to 0.98) but significant degradation in dual-concept settings (correlations dropping by 0.2-0.5). Even for conceptually orthogonal pairs like humor-persuasiveness or clarity-politeness, introducing a secondary concept reduces controllability, indicating models struggle with compositionality. This limitation persists across multiple model families and tasks, suggesting a fundamental challenge in multi-attribute fine-grained control.

## Method Summary
The paper evaluates fine-grained control of six stylistic concepts at five discrete levels (0-4) using structured prompts with explicit level encoding. Models: Llama 3.2-11B, Gemma 3-12B, Qwen3-14B. Judge: GPT-4.1 via bidirectional pairwise comparisons. Datasets: 75 samples each from Persuasion (argument generation), ROCStories (story generation), and GEM (structured text generation). Spearman correlation (ρ) measures monotonic alignment between intended levels and judge rankings, averaged across samples (ρ̄). Fisher z-transformed correlations enable statistical testing.

## Key Results
- Single-concept control achieves strong correlations (ρ=0.76-0.98) across models for humor and persuasiveness
- Dual-concept control shows significant degradation (correlations drop by 0.2-0.5) even for orthogonal concept pairs
- Task context strongly modulates controllability—narrative generation allows more flexible style variation than argumentative contexts
- Larger models (Qwen-14B) show less degradation than smaller models (Llama-11B) in dual-concept settings

## Why This Works (Mechanism)

### Mechanism 1
- Prompt-based control achieves strong single-concept calibration through explicit level encoding
- Structured prompts encoding desired levels (0-4) create interpretable conditioning signals that models can map to graduated stylistic outputs
- Models have internalized sufficient stylistic variation during pre-training to express graduated concept levels when explicitly instructed
- Strong single-concept correlations (ρ=0.76-0.98) across models for humor and persuasiveness support this

### Mechanism 2
- Dual-concept control degrades due to representational entanglement rather than linguistic incompatibility
- When models receive multiple concept specifications, interference emerges at the activation level—concepts that should be orthographically independent compete for shared representational resources
- Stylistic concepts are not independently controllable in model activation space even when linguistically separable
- Clarity-politeness pair shows clarity dropping from ρ=0.45 (single) to ρ=-0.01 (dual) in story generation for Llama

### Mechanism 3
- Task context modulates controllability by constraining stylistic flexibility
- Narrative generation affords broader stylistic latitude than argumentative or structured text tasks, which impose semantic constraints that conflict with stylistic manipulation
- Task-internal coherence requirements compete with stylistic instructions for model capacity
- Clarity control in arguments: ρ=-0.02 vs stories: ρ=0.45 (single-concept) demonstrates task-dependent modulation

## Foundational Learning

### Concept: Spearman rank correlation
- Why needed here: Core metric for measuring monotonic alignment between intended concept levels (0-4) and empirical judge rankings
- Quick check question: If model outputs levels [3,1,4,0,2] when prompted for [0,1,2,3,4], what is the Spearman correlation? (Answer: 1.0—perfect monotonic alignment)

### Concept: LLM-as-judge pairwise evaluation
- Why needed here: Human evaluation is costly; this paper uses GPT-4.1 for pairwise comparisons to derive rankings, validated against human judgments (14/18 agreement)
- Quick check question: Why use pairwise comparisons rather than direct scoring? (Answer: Reduces position bias and calibration drift; listwise ranking has strong position bias)

### Concept: Fisher z-transformation
- Why needed here: Spearman correlations are bounded [-1,1]; averaging them directly is problematic. Fisher transform enables valid statistical testing
- Quick check question: A correlation of ρ=0.9 transforms to z≈1.47. What advantage does this provide for aggregation? (Answer: Unbounded scale, approximately normal distribution for statistical tests)

## Architecture Onboarding

### Component map:
Prompt Generator → Target Model (7-14B) → Outputs (5 levels per concept) → LLM Judge (GPT-4.1) ← Pairwise Comparisons ←┘ → Empirical Rankings → Spearman ρ → Aggregation (Fisher z)

### Critical path:
1. Prompt design (structured templates with explicit level encoding)
2. Generation with controlled randomness (fixed seed, deterministic decoding recommended)
3. Judge calibration (bidirectional pairwise comparisons to eliminate position bias)
4. Correlation computation (per-sample Spearman, then Fisher aggregate)

### Design tradeoffs:
- Pairwise vs listwise judging: Pairwise requires O(n²) comparisons but eliminates position bias; listwise is O(n) but shows severe bias
- Fixed vs random secondary concept: Fixed enables controlled experiments; random tests disentanglement robustness
- Model scale vs interpretability: Larger models maintain better dual-concept control but are harder to analyze

### Failure signatures:
- Near-zero or negative correlations: Model cannot reliably distinguish concept levels
- High variance (±0.4-0.5): Inconsistent control across samples
- Asymmetric interference: Concept A controlled well with B fixed, but B degrades with A fixed

### First 3 experiments:
1. **Baseline single-concept**: Run all 6 concepts at 5 levels across 3 tasks; verify strong correlations (ρ>0.7) before proceeding
2. **Dual-concept with fixed secondary**: Hold secondary at level 2; sweep primary from 0-4; measure correlation drop vs single-concept baseline
3. **Randomized secondary**: Sample secondary level uniformly for each primary level; test whether primary control is robust to uncorrelated secondary variation

## Open Questions the Paper Calls Out

### Open Question 1
- Does scaling to larger model sizes (e.g., 70B+ parameters) improve multi-concept compositional control, or does the interference effect persist?
- Only 7B-14B models were tested; it remains unknown whether this is a fundamental architectural limitation or a scale issue
- Evidence would require applying the same evaluation framework to models in the 70B-200B parameter range

### Open Question 2
- Can representation engineering or activation steering methods achieve better multi-concept control than naive prompting?
- The paper demonstrates prompting limitations but does not test whether methods operating on internal activations might disentangle concept representations more effectively
- Evidence would require evaluating methods like Contrastive Activation Addition (CAA) or SteerLM on the same dual-concept framework

### Open Question 3
- Does the specific intensity level of the secondary concept (e.g., level 0 vs. level 4) differentially affect primary concept controllability?
- The paper uses "fixed" and "random" secondary levels but reports aggregated results, not analyzing whether certain secondary levels cause more interference than others
- Evidence would require fine-grained analysis showing Spearman correlations for each fixed secondary level (0-4) separately

## Limitations
- The observed entanglement in dual-concept control may be specific to the chosen concepts and prompting methodology rather than a fundamental architectural limitation
- Judge reliability, while validated against human judgments, still represents a black-box evaluation that may introduce systematic biases
- Task-specific constraints are discussed but not quantitatively isolated—it's unclear whether degradation stems from stylistic interference or from task-level semantic constraints

## Confidence

### High confidence:
- Single-concept control effectiveness (ρ=0.76-0.98) across multiple models and tasks

### Medium confidence:
- The entanglement hypothesis for dual-concept failure (empirical pattern is clear but mechanism could involve task interference)

### Low confidence:
- Claims about architectural scale effects (Qwen-14B showing less degradation) based on limited model comparisons

## Next Checks

1. **Concept universality test**: Repeat dual-concept experiments with 3-4 new concept pairs (e.g., creativity-technical accuracy, formality-ambiguity) to determine if entanglement is concept-specific or general

2. **Scale extrapolation analysis**: Test the smallest effective model size for reliable dual-concept control by interpolating between 7B and 14B scales with additional intermediate models

3. **Task constraint isolation**: Design a synthetic task where semantic constraints are minimal but stylistic variation is maximal, to separate task interference from true concept entanglement