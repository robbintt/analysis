---
ver: rpa2
title: Hierarchical Federated Unlearning for Large Language Models
arxiv_id: '2510.17895'
source_url: https://arxiv.org/abs/2510.17895
tags:
- unlearning
- merging
- data
- retention
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of decentralized, heterogeneous
  LLM unlearning in federated settings where different parties hold unlearn and retain
  data asymmetrically. The authors propose a hierarchical federated unlearning framework
  (FULM) that decouples unlearning and retention into separate LoRA adapters and merges
  them using a two-step process: intra-cluster voting-based merging for near-iid tasks
  and inter-cluster summation for heterogeneous domains.'
---

# Hierarchical Federated Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2510.17895
- Source URL: https://arxiv.org/abs/2510.17895
- Reference count: 40
- This paper proposes a hierarchical federated unlearning framework (FULM) that decouples unlearning and retention into separate LoRA adapters and merges them using a two-step process: intra-cluster voting-based merging for near-iid tasks and inter-cluster summation for heterogeneous domains.

## Executive Summary
This paper addresses the challenge of decentralized, heterogeneous LLM unlearning in federated settings where different parties hold unlearn and retain data asymmetrically. The authors propose a hierarchical federated unlearning framework (FULM) that decouples unlearning and retention into separate LoRA adapters and merges them using a two-step process: intra-cluster voting-based merging for near-iid tasks and inter-cluster summation for heterogeneous domains. Experiments on WMDP, TOFU, and MUSE benchmarks show that FULM achieves superior balance between forgetting (e.g., 51.84% in TOFU) and retention (e.g., 70.62% utility) compared to baselines like SUM, AVG, TIES, and KNOT, particularly in heterogeneous scenarios. The method scales to dynamic unlearning requests while preserving model utility, offering a privacy-preserving solution for LLM knowledge governance.

## Method Summary
The paper proposes FULM, a hierarchical federated unlearning framework that uses LoRA adapters for efficient, privacy-preserving unlearning in decentralized settings. The method decouples unlearning and retention into separate adapters, clusters them by cosine similarity, and applies TIES-based intra-cluster merging and summation-based inter-cluster aggregation. This two-step merging strategy effectively balances forgetting efficacy and model utility, particularly in heterogeneous scenarios where traditional approaches struggle with conflicting objectives.

## Key Results
- FULM achieves 51.84% forgetting on TOFU benchmark while maintaining 70.62% utility, outperforming baselines.
- On WMDP, FULM reduces interference in heterogeneous scenarios compared to simple summation, achieving better balance between forget accuracy and retention.
- The hierarchical merging strategy shows significant improvements over baselines (SUM, AVG, TIES, KNOT) particularly in heterogeneous settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separately training task-specific LoRA adapters for unlearning and retention tasks, then merging them, yields a better balance than optimizing a joint objective.
- Mechanism: A "split" phase isolates the conflicting optimization objectives. An unlearning adapter (∇θu) is trained on the private forget set (Du) using methods like gradient ascent, while a retention adapter (∇θr) is trained on the retain set (Dr). These adapters are then merged in a "merge" phase using task arithmetic (e.g., summation). This prevents the gradients from the retain set from interfering with and reducing the efficacy of the unlearning process.
- Core assumption: The knowledge to be removed occupies a small subspace of the model, which can be effectively captured and manipulated by low-rank adapters without requiring full model fine-tuning.
- Evidence anchors:
  - [abstract] "Our method decouples unlearning and retention via task-specific adapter learning... to mitigate conflicting objectives."
  - [section 4.1] Describes the "split phase where unlearning and retention are handled independently, and merge phase where the resulting adapters are aggregated."
  - [corpus] Weak/None. The corpus confirms the use of LoRA in federated settings (e.g., "WinFLoRA"), but does not provide direct evidence for the decoupling mechanism's superiority over joint training.
- Break condition: Fails if the unlearning task requires modifying a very large number of parameters that cannot be represented in a low-rank form, or if the forget and retain sets are so deeply entangled that separating their optimization is impossible.

### Mechanism 2
- Claim: A two-step hierarchical merging strategy based on task vector similarity effectively mitigates both intra-cluster (similar tasks) and inter-cluster (dissimilar tasks) interference.
- Mechanism: The server first computes cosine similarity between all task vectors (LoRA adapters). Vectors with high similarity (>ξ) are grouped into a cluster (Ck). The method assumes vectors in a cluster have aligned objectives. For intra-cluster merging, it uses a voting-based method like TIES to select dominant parameter signs and average updates, preventing the over-amplification of updates common in simple summation. For inter-cluster merging, it treats the merged clusters as distinct domains and aggregates them using direct summation (task arithmetic), preserving the unique information from each domain.
- Core assumption: Task adapters trained on near-IID data or with aligned objectives will exhibit high positive cosine similarity, while those with orthogonal data or conflicting objectives will be near-zero or negative.
- Evidence anchors:
  - [abstract] "By clustering task adapters based on parameter similarity and applying voting-based intra-cluster merging with summation-based inter-cluster aggregation..."
  - [section 4.2, Figure 2] Shows the cosine similarity matrix, demonstrating that unlearning adapters from the same domain (Bio1, Bio2) have high similarity, while unlearning and retention adapters on similar data have negative similarity.
  - [corpus] Weak/None. Related work like "Oblivionis" discusses federated unlearning but does not detail this specific hierarchical merging logic based on parameter similarity.
- Break condition: Fails if the relationship between data distribution/objective and parameter similarity breaks down (e.g., unrelated tasks have high cosine similarity by chance), leading to incorrect clustering and subsequent destructive interference during merging.

### Mechanism 3
- Claim: A one-round federated learning framework enables unlearning when the client (requester) and server (model owner) have asymmetric access to the forget and retain data.
- Mechanism: Clients (k) with private unlearning data (Duk) train local unlearning adapters (∇θuk) and send them to the server. The server, which cannot access Duk, aggregates these with a server-side retention adapter (∇θrserver) trained on its own pretraining data. This avoids the need for either party to share their sensitive raw data. The server updates the global model as θ' = θ + ∇Θmerged + ∇θrserver.
- Core assumption: A lightweight subset of the server's pretraining data is a sufficient proxy for general knowledge retention.
- Evidence anchors:
  - [abstract] "...decentralized, sensitive data with asymmetric access... propose a federated unlearning approach... privacy preserving."
  - [section 4] Problem Setting explicitly defines the framework where clients have Duk but not Dr, and the server has access to pretraining data but not Duk.
  - [corpus] Weak/None. Corpus papers (e.g., "ToFU", "BadFU") discuss federated unlearning frameworks in general terms but do not provide evidence for this specific one-round, split-and-merge architecture.
- Break condition: Fails if the server's proxy retention data does not cover the full scope of knowledge needed for utility, leading to "over-forgetting" in areas not represented in the proxy set.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: This is the core adapter technology used for efficient fine-tuning. Understanding that LoRA adds low-rank matrices (W' = W + BA) instead of modifying full weights is essential, as all merging and similarity calculations operate on these adapter parameters.
  - Quick check question: Why can't we just average the full model weights instead of the LoRA adapters in this framework?

- Concept: **Task Arithmetic**
  - Why needed here: The "merge" phase is built on this concept. The method treats a fine-tuned adapter as a "task vector" which can be added to or subtracted from a model to edit its capabilities. This is the underlying logic for the summation-based inter-cluster merging.
  - Quick check question: How does the paper combine the merged unlearning clusters with the original model?

- Concept: **TIES-Merging**
  - Why needed here: This is the specific voting-based technique selected for intra-cluster merging. The paper relies on its ability to trim negligible weights and resolve sign conflicts to handle interference among similar unlearning tasks.
  - Quick check question: Why does the paper choose TIES for merging similar task vectors instead of simple arithmetic summation?

## Architecture Onboarding

- Component map: Client Node (trains Unlearning Adapter) -> Server Node (hosts Base LLM, Retention Adapter) -> Aggregation Module (clusters by cosine similarity) -> Global Model Updater (applies merged adapter)
- Critical path:
  1. Client receives base model θ.
  2. Client trains ∇θu locally on Du.
  3. Client sends ∇θu to server.
  4. Server clusters ∇θu and ∇θr by cosine similarity.
  5. Server merges adapters within each cluster using TIES.
  6. Server sums the merged cluster vectors.
  7. Server adds the final vector to θ to produce θ'.
- Design tradeoffs:
  - One-shot vs. Iterative: The paper uses a one-shot FL setting for efficiency. A tradeoff is that iterative communication could potentially lead to better-converged models at the cost of bandwidth and latency.
  - TIES vs. SUM: TIES is used for similar tasks to avoid over-amplifying updates, while SUM is used for dissimilar clusters to ensure all unique domain information is preserved. The tradeoff is the added complexity of a hierarchical system versus a single merging strategy.
  - LoRA Rank: A higher rank (r=16) captures more knowledge but increases adapter size and computation. A lower rank is faster but might fail to capture complex unlearning targets.
- Failure signatures:
  - Ineffective Unlearning (Forget Score High): May indicate that the intra-cluster merging method is too conservative, or that the unlearning adapters were not trained sufficiently locally.
  - Catastrophic Utility Loss (Utility Score Low): May signal that the summation in inter-cluster merging is too aggressive, or that the server's retention adapter is not a sufficient proxy for general knowledge. Check if SUM merge is being used where TIES should be (e.g., in a heterogeneous cluster).
  - High Interference (Overall Low): Could result from incorrect clustering. If the similarity threshold ξ is poorly set, tasks with conflicting objectives might be merged using TIES, leading to destructive interference.
- First 3 experiments:
  1. Reproduce Merging Ablation (Table 4/5): Take the pre-trained Zephyr-7B model and 3 WMDP-Bio adapters. Compare merging them with simple summation vs. TIES. This will validate the core claim that TIES is superior for intra-cluster merging.
  2. Test Similarity Hypothesis (Figure 2): Train two new adapters on distinct datasets (e.g., one on Harry Potter, one on a biology textbook) and compute their cosine similarity. This tests the core assumption that unrelated domains produce orthogonal vectors.
  3. End-to-End Federated Run (Algorithm 1): Simulate a single round with 2 clients and 1 server. Have Client 1 train an unlearning adapter on a small subset of data (e.g., a "forget author" set), and the server train a retention adapter. Execute the full clustering and merging pipeline and evaluate the resulting model's Forget and Utility scores on held-out sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FULM perform in a continuous, multi-round federated learning setting rather than the current one-shot merging approach?
- Basis in paper: [explicit] The authors state, "In contrast, we employ a one-shot FL setting and omit frequent parameter exchanges," despite noting in the abstract that practical needs are often "continuous."
- Why unresolved: Multi-round settings introduce client drift and iterative optimization dynamics not present in one-shot aggregation.
- What evidence would resolve it: Experiments evaluating FULM across multiple communication rounds to measure convergence stability and forgetting retention over time.

### Open Question 2
- Question: Does the hierarchical merging strategy maintain its efficacy on significantly larger LLMs (e.g., 70B+ parameters) where parameter interference scales differently?
- Basis in paper: [inferred] The experimental setup explicitly restricts evaluation to the "Zephyr-7B model," leaving scalability to larger model sizes unstated.
- Why unresolved: Parameter interference and clustering dynamics may change fundamentally in higher-dimensional embedding spaces found in frontier models.
- What evidence would resolve it: Benchmarks running FULM on larger model families (e.g., Llama-3-70B) comparing merge interference levels against the 7B baseline.

### Open Question 3
- Question: Is cosine similarity sufficient for clustering adapters with overlapping but conflicting semantic knowledge, or would learned metrics provide better separation?
- Basis in paper: [explicit] The method states, "Without losing generality, we use cosine similarity as the task adapter correlation metric."
- Why unresolved: Cosine similarity relies on linear parameter geometry, which might fail to capture complex, non-linear dependencies between diverse unlearning tasks.
- What evidence would resolve it: Comparative analysis against learned similarity functions or functional space metrics for clustering accuracy and merging performance.

## Limitations
- Critical hyperparameters including the similarity threshold ξ for clustering adapters and training configuration for unlearning and retention adapters are underspecified.
- The exact method for computing cosine similarity across LoRA low-rank matrices is not detailed.
- The one-round federated learning setup's ability to handle highly dynamic or adversarial unlearning requests is not extensively tested.

## Confidence
- **High Confidence**: The core framework of decoupling unlearning and retention into separate LoRA adapters and merging them is well-defined and supported by experimental results.
- **Medium Confidence**: The effectiveness of the two-step hierarchical merging strategy (TIES for intra-cluster, SUM for inter-cluster) is demonstrated, but the paper does not provide strong evidence that the clustering is always accurate or that the chosen methods are optimal for all scenarios.
- **Low Confidence**: The assumption that a proxy retention set is sufficient for general knowledge preservation is not rigorously validated.

## Next Checks
1. Reproduce the WMDP intra-cluster ablation (Table 4): Train three WMDP-Bio adapters and merge them using both simple summation and TIES. Compare forget accuracy and utility to validate the superiority of TIES for similar tasks.
2. Test the cosine similarity clustering hypothesis: Train adapters on two completely unrelated datasets (e.g., one on medical text, one on legal text) and compute their cosine similarity. Verify that they are near-zero or negative, supporting the clustering mechanism.
3. Simulate a heterogeneous federated round: Create a minimal federated setup with 2 clients and 1 server. Have each client train an unlearning adapter on a small, distinct forget set, and the server train a retention adapter. Run the full clustering and merging pipeline and evaluate the model's forget and utility scores to confirm the method's effectiveness in practice.