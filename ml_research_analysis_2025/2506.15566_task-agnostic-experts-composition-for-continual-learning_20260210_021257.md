---
ver: rpa2
title: Task-Agnostic Experts Composition for Continual Learning
arxiv_id: '2506.15566'
source_url: https://arxiv.org/abs/2506.15566
tags:
- composition
- experts
- expert
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the need for more efficient and sustainable\
  \ AI frameworks by leveraging compositionality in neural networks. The authors propose\
  \ an expert composition method where multiple small, pretrained ResNet-18 models\u2014\
  each specialized in recognizing a subset of object classes\u2014are combined in\
  \ a zero-shot manner to tackle compositional image classification tasks."
---

# Task-Agnostic Experts Composition for Continual Learning

## Quick Facts
- arXiv ID: 2506.15566
- Source URL: https://arxiv.org/abs/2506.15566
- Reference count: 15
- Primary result: Expert composition achieves 58% accuracy on CGQA, nearly doubling state-of-the-art continual learning baselines

## Executive Summary
This work proposes a zero-shot expert composition method for compositional image classification that avoids additional training while achieving state-of-the-art performance. The approach trains multiple small ResNet-18 models, each specialized in recognizing a subset of object classes, then combines them to tackle compositional tasks without fine-tuning. When tested on the CGQA benchmark, the method achieves 58% accuracy, nearly doubling the performance of continual learning baselines like ER (26%), while requiring no gradient updates during composition. The framework demonstrates significant efficiency gains and robustness to distribution shifts.

## Method Summary
The method trains 7 ResNet-18 experts, each on 3 unique object classes plus a generic "other" class from the GQA dataset. Images are cropped using bounding boxes and resized to 98×98 pixels. During composition, input images are divided into quadrants, each processed independently by all experts via argmax selection, then predictions are concatenated. The zero-shot approach requires no fine-tuning, though optional fine-tuning with a linear classifier yields lower accuracy (18.82%-42.82%). The framework is evaluated on the CGQA benchmark, a synthetic 2×2 grid dataset containing 100 compositional labels with 100 test instances each.

## Key Results
- Expert composition achieves 58% accuracy on CGQA, nearly doubling state-of-the-art continual learning baselines (ER at 26%)
- Zero-shot composition significantly outperforms fine-tuned approaches (18.82%-42.82% accuracy)
- Method demonstrates robustness to data distribution shifts without requiring additional training
- Each expert achieves ~88% accuracy on specialized classes during training

## Why This Works (Mechanism)

### Mechanism 1: Out-of-Distribution Detection via "Other" Class Training
Adding an explicit "other" class during expert training enables models to recognize when inputs fall outside their specialization, improving composition reliability. Each expert learns 3 specific classes plus a generic "other" category that absorbs all remaining classes, creating a soft gating mechanism where experts naturally suppress predictions on out-of-scope inputs.

### Mechanism 2: Zero-Shot Concatenative Composition
Composing expert predictions via simple concatenation (p_comp = p_1 ∪ p_2) outperforms learned composition heads because it avoids overfitting to sparse training data. Each quadrant is independently classified by all experts via argmax, then final predictions are concatenated without gradient updates.

### Mechanism 3: Spatial Decomposition Matching Task Structure
The CGQA benchmark's 2×2 grid structure with object crops naturally decomposes into independent classification subproblems, aligning with the experts' training distribution. The pre-processing during expert training matches the quadrant structure in CGQA test images, minimizing domain shift.

## Foundational Learning

- **Concept: Modular/Specialist Neural Networks**
  - Why needed: The approach relies on training independent expert modules rather than one monolithic model
  - Quick check: Can you explain why training seven 3-class classifiers might yield different representations than one 21-class classifier?

- **Concept: Zero-Shot vs. Few-Shot Learning**
  - Why needed: The paper demonstrates that zero-shot composition outperforms few-shot fine-tuning—a counterintuitive result
  - Quick check: Why might adding a trained classifier head on top of frozen features perform worse than direct argmax concatenation?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed: The paper positions itself against continual learning baselines that suffer from forgetting
  - Quick check: What would happen to a fine-tuned ResNet-18 trained sequentially on CGQA's 100 combinations without replay or regularization?

## Architecture Onboarding

- **Component map**: Input Image (2×2 grid) → Quadrant Extraction (4 crops) → Expert Parallel Inference (7 × ResNet-18) → Per-Quadrant Argmax Selection → Prediction Concatenation [obj_1, obj_2]

- **Critical path**: Expert training: 7 ResNet-18 models, each on ~38K images (3 classes + "other"), ~13 min/model on Tesla T4. Inference pipeline: No training required—forward pass through all experts, select highest logit per quadrant. Optional fine-tuning path: Freeze experts, train linear classifier on concatenated features using few-shot samples.

- **Design tradeoffs**: Model size vs. specialization (smaller experts enable efficient training but may lack capacity); number of experts vs. coverage (7 experts × 3 classes = 21 classes); zero-shot vs. fine-tuning (zero-shot avoids overfitting but cannot adapt to distribution shifts).

- **Failure signatures**: Low per-expert accuracy (<70%) indicates training data quality issues; "other" class dominates predictions suggests expert specialization has collapsed; fine-tuning outperforming zero-shot indicates task may require relational reasoning.

- **First 3 experiments**: 1) Expert validation: Train one expert on 3 classes + "other"; verify >85% accuracy. 2) Single-quadrant composition test: Test experts on individual CGQA quadrants; target >80% accuracy per quadrant. 3) Zero-shot baseline: Run full pipeline on CGQA test set; expect 55-60% accuracy.

## Open Questions the Paper Calls Out
- How can the Experts Composition framework be adapted for realistic, non-synthetic environments where object boundaries are not pre-defined grid quadrants?
- Why does few-shot fine-tuning degrade performance relative to zero-shot composition, and can alternative adapter strategies restore effectiveness?
- Can functional expert modules be efficiently extracted or pruned from large pretrained foundation models to create the required ensemble without training from scratch?

## Limitations
- Highly constrained synthetic nature of CGQA (2×2 grid structure with pre-cropped objects) doesn't reflect real-world complexity
- Limited ablation studies on expert architecture choices, class partitioning strategies, or "other" class sampling ratios
- Fine-tuning results suggest limited adaptability when zero-shot composition fails, with no exploration of intermediate approaches

## Confidence

- **High confidence**: Zero-shot composition mechanism works as described for CGQA synthetic task; accuracy difference between zero-shot and fine-tuned approaches is robust
- **Medium confidence**: "Other" class training meaningfully improves out-of-distribution detection; requires empirical validation
- **Low confidence**: Generalization to real-world compositional tasks; synthetic nature of CGQA limits practical applicability claims

## Next Checks
1. Apply method to a more realistic compositional dataset (e.g., Visual Genome with scene graphs) where objects aren't pre-cropped and may overlap
2. Measure "other" class prediction rates during composition to verify experts are genuinely detecting out-of-scope inputs
3. Test whether zero-shot approach can handle compositional tasks requiring object relationships (e.g., "red ball on green table" vs. just "red ball" and "green table" separately)