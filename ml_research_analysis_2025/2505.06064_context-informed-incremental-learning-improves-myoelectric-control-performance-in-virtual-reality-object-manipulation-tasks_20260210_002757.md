---
ver: rpa2
title: Context Informed Incremental Learning Improves Myoelectric Control Performance
  in Virtual Reality Object Manipulation Tasks
arxiv_id: '2505.06064'
source_url: https://arxiv.org/abs/2505.06064
tags:
- ciil
- control
- systems
- online
- trial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates a myoelectric control system using Context
  Informed Incremental Learning (CIIL) to improve gesture recognition during realistic
  object manipulation in VR. Nine participants without limb differences performed
  a task moving virtual objects with EMG-based control.
---

# Context Informed Incremental Learning Improves Myoelectric Control Performance in Virtual Reality Object Manipulation Tasks

## Quick Facts
- arXiv ID: 2505.06064
- Source URL: https://arxiv.org/abs/2505.06064
- Reference count: 25
- Participants without limb differences performed VR object manipulation using EMG control, with CIIL improving usability metrics despite lowering offline accuracy

## Executive Summary
This paper evaluates a myoelectric control system using Context Informed Incremental Learning (CIIL) to improve gesture recognition during realistic object manipulation in VR. Nine participants performed a task moving virtual objects with EMG-based control, comparing standard non-adaptive control against CIIL-enabled real-time adaptation using contextual cues. CIIL reduced perceived workload by 7.1% and increased task efficiency, allowing completion of 5.78 vs 5.00 objects in 31.26 vs 60.59 seconds per object. Despite lowering offline classification accuracy by 5.8%, CIIL improved real-world usability by adapting the classifier to natural user behavior, bridging the gap between screen-guided training and goal-directed tasks.

## Method Summary
The study used a SiFiBand 8-channel EMG armband at 2000 Hz with 20-450 Hz bandpass filtering, processing 200ms windows with 50ms stride into TDPSD features (6 features × 8 channels = 48-dim input). A 1D CNN classifier (Conv1D 32→Conv1D 32→Dense 256→Dense 8) was trained for 15 epochs on screen-guided data. CIIL implemented every 2 seconds: windows accumulated, pseudo-labels generated from environmental context (hand-object proximity <10cm with valid grasp matching prediction), label spreading with α=0.2 applied, and model fine-tuned for 1 epoch. Nine participants without limb differences completed virtual object manipulation tasks in VR, comparing non-adaptive (NA) control against CIIL adaptation.

## Key Results
- CIIL reduced NASA-TLX perceived workload by 7.1% compared to non-adaptive control
- Task efficiency improved: 5.78 objects completed vs 5.00 with CIIL (31.26s vs 60.59s per object)
- Offline classification accuracy decreased by 5.8% with CIIL, but real-world usability metrics improved significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Context-derived pseudo-labels enable classifier adaptation without explicit user labeling.
- **Mechanism:** The VR environment tracks spatial relationships (hand-to-object proximity, grasp validity). When a user's hand enters an object's grasping radius and the classifier predicts a valid grip for that object, the system assigns a pseudo-label matching the prediction. Invalid predictions receive weighted pseudo-labels across all valid grasp options. These pseudo-labels drive incremental model updates every 2 seconds.
- **Core assumption:** Object-gesture constraints in the environment reliably indicate when predictions are correct.
- **Evidence anchors:**
  - [abstract] "continuously adapt the classifier using contextual cues"
  - [section II.D.3] "When the hand entered the object's grasping radius (10 cm), the classifier's prediction was compared with the set of grasps associated with that object"
  - [corpus] Campbell et al. (2024) in references demonstrates CIIL feasibility in 2D games, but VR object manipulation remains novel.
- **Break condition:** If environmental constraints are ambiguous (multiple valid grasps with similar probabilities) or if objects permit unintended grasps, pseudo-label quality degrades and adaptation introduces noise.

### Mechanism 2
- **Claim:** Label spreading with differential sample weighting prevents catastrophic forgetting while allowing adaptation.
- **Mechanism:** Positive-context samples receive high contribution weights (resistant to relabeling); negative-context samples receive low weights (easily relabeled). This semi-supervised approach smooths label assignments across the feature space, preserving confident predictions while allowing correction of uncertain ones.
- **Core assumption:** The α=0.2 parameter appropriately balances supervised vs. unsupervised influence; the feature space geometry reflects gesture similarity.
- **Evidence anchors:**
  - [section II.D.3] "treating positive suitability samples as supervised (high contribution, resistant to relabeling) and negative suitability samples as unsupervised (low contribution, easily relabeling), with α = 0.2"
  - [section IV] Discussion notes that imbalanced adaptation datasets can cause "forgetting" of less prevalent classes (HR accuracy dropped from 91% to 84%).
  - [corpus] "Detecting Domain Shifts in Myoelectric Activations" addresses stream learning challenges but does not directly validate label spreading for EMG.
- **Break condition:** If certain gesture classes rarely occur during task performance (e.g., Hand Resting, Wrist Extension not mapped to objects), their representations drift without corrective supervision.

### Mechanism 3
- **Claim:** Real-time adaptation improves usability by aligning classifier boundaries with natural, goal-directed muscle activations.
- **Mechanism:** During goal-directed tasks, users produce muscle activations that differ from screen-guided training (different intensity, co-contraction patterns, temporal dynamics). CIIL adapts to these natural patterns rather than forcing users to reproduce training-like contractions. This reduces cognitive effort because users don't need to consciously "perform for the classifier."
- **Core assumption:** Users' natural gesture patterns are more repeatable and sustainable than deliberate performed training gestures.
- **Evidence anchors:**
  - [abstract] "This decline is largely due to goal-directed behaviors that are not captured in static, offline scenarios"
  - [section IV] "the CIIL framework allows the classifier to evolve alongside the user's natural gesture patterns... bridging the gap between SGT-generated EMG and goal-oriented behavior"
  - [corpus] Campbell et al. [8] previously established that SGT does not capture goal-oriented behaviors.
- **Break condition:** If users exhibit high within-session gesture variability (fatigue, changing strategy), adaptation may chase a moving target rather than converge.

## Foundational Learning

- **Concept: Semi-supervised learning with pseudo-labels**
  - **Why needed here:** CIIL generates training labels from environmental context rather than explicit user annotation; understanding how pseudo-labels propagate through label spreading is essential for debugging adaptation behavior.
  - **Quick check question:** If 70% of pseudo-labels during a session are derived from negative-context windows (weighted across multiple classes), how would this affect classifier confidence over time?

- **Concept: Domain shift in biosignals**
  - **Why needed here:** EMG signals change between calibration and use due to electrode shift, sweat, fatigue, and task context; CIIL is explicitly designed to address this.
  - **Quick check question:** Why would a classifier trained on isometric, screen-guided contractions fail during dynamic object manipulation even if the same gesture is performed?

- **Concept: Windowing and latency in real-time EMG**
  - **Why needed here:** The system uses 200ms windows with 50ms stride; understanding this tradeoff is critical for interpreting why certain gestures are misclassified during rapid movements.
  - **Quick check question:** If a user transitions between gestures in 150ms, how many classification windows will contain mixed signals, and what happens to pseudo-label quality during transitions?

## Architecture Onboarding

- **Component map:** SiFiBand (8-channel EMG, 2000 Hz) -> 200ms/50ms windowing -> TDPSD feature extraction (48-dim) -> 1D CNN classifier (Conv1D 32→Conv1D 32→Dense 256→Dense 8) -> VR control output, parallel to Context engine (Unity, hand-object proximity tracking) -> Pseudo-label generation -> Label spreading (α=0.2) -> 1-epoch fine-tuning every 2s

- **Critical path:** Offline calibration (3 reps × 8 gestures, 15-epoch training) -> Online task with parallel streams: (a) classification → control, (b) context → pseudo-labels → adaptation. The adaptation path must not block the control path; updates are applied asynchronously.

- **Design tradeoffs:**
  - **Window size (200ms) vs. latency:** Shorter windows reduce delay but decrease feature reliability; 200ms/50ms is a compromise from prior literature.
  - **Adaptation frequency (2s) vs. stability:** More frequent updates respond faster but risk overfitting to transient noise; 2s provides sufficient sample accumulation.
  - **Label spreading α=0.2:** Lower values give more weight to unsupervised propagation (risk: drift), higher values anchor to pseudo-labels (risk: lock in errors). This value is not rigorously tuned.

- **Failure signatures:**
  - **Class collapse:** Accuracy on unmapped gestures (HR, WE) degrades over session—they receive no positive-context supervision.
  - **Feedback loop errors:** If pseudo-labels are systematically wrong (e.g., object permits unintended grasp), classifier reinforces errors.
  - **High variance in online metrics:** Large SD in objects completed (5.00 ± 1.89 NA, 5.78 ± 0.42 CIIL) suggests user-specific factors dominate; expect heterogeneous results.

- **First 3 experiments:**
  1. **Reproduce the NA vs. CIIL comparison with a single participant.** Log all pseudo-labels and context flags. Verify that positive-context samples have higher prediction confidence than negative-context samples.
  2. **Ablate label spreading.** Run CIIL with α=1.0 (no spreading) and α=0.0 (full propagation). Compare offline accuracy degradation and online task metrics to quantify spreading's contribution.
  3. **Stress-test class imbalance.** Design a task where only 2 of 8 gestures are used for 5 minutes, then test all 8 gestures. Measure which classes are "forgotten" and whether label spreading accelerates or mitigates this effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can alternative pseudo-labeling strategies prevent the degradation of underrepresented classes during the CIIL adaptation process?
- **Basis in paper:** [explicit] The authors note that the current relabeling strategy using label spreading "risks gradually eroding less prevalent classes" and suggest future work explore "balanced sampling or regularization techniques."
- **Why unresolved:** The study observed a drop in accuracy for the Hand Resting (HR) class (91% to 84%) after the CIIL trial, indicating that the adaptation dataset imbalance causes the system to "forget" gestures not frequently used during the task.
- **What evidence would resolve it:** A comparative study implementing class-balanced sampling or specific regularization penalties during incremental learning, demonstrating preserved accuracy for minority classes compared to the standard label spreading approach.

### Open Question 2
- **Question:** Does the CIIL-adapted classifier retain its improvements and stability across multiple sessions or days of use?
- **Basis in paper:** [explicit] The authors state that "long-term, cross-session studies are needed to verify the stability of CIIL-learned models across sessions."
- **Why unresolved:** The current study was limited to a single session (approx. 1 hour), so it is unknown if the model adaptations persist effectively over time or if they introduce instability (catastrophic forgetting) in subsequent sessions.
- **What evidence would resolve it:** A longitudinal study measuring the retention of classification accuracy and usability metrics (NASA-TLX) across multiple separate sessions spaced over days or weeks.

### Open Question 3
- **Question:** Does incorporating multimodal sensor data, such as IMU-based limb orientation, enhance the robustness of CIIL frameworks?
- **Basis in paper:** [explicit] The discussion suggests that "leveraging multimodality could further improve CIIL" by addressing limb orientation, which is identified as a key confounding factor in EMG signal reliability.
- **Why unresolved:** The current system relies solely on EMG signals, which vary significantly with arm orientation; fusing additional data streams might correct for these variations during real-time adaptation.
- **What evidence would resolve it:** A comparison of standard EMG-only CIIL against a multimodal CIIL system (EMG + IMU) while users perform tasks in varied arm positions to isolate the reduction in signal variability.

## Limitations
- The study only tested participants without limb differences, limiting generalizability to amputee populations
- No control condition with ground truth feedback to distinguish adaptation quality from feedback design effects
- High variability in online metrics (5.00 ± 1.89 vs 5.78 ± 0.42 objects completed) suggests heterogeneous user experiences not fully explored

## Confidence

- **High confidence**: CIIL reduces perceived workload (NASA-TLX -7.1%) and improves task completion metrics (objects completed, time per object). The experimental design is rigorous and the effect sizes are substantial.
- **Medium confidence**: The mechanism explanation (context-informed pseudo-labels enabling adaptation) is plausible but not directly validated. The paper shows correlation between context and performance but does not prove that pseudo-label quality drives the improvement.
- **Low confidence**: The explanation for offline accuracy decline (5.8%) is speculative. The paper asserts this reflects natural gesture patterns vs. screen-guided training but provides no direct evidence comparing these activation patterns.

## Next Checks

1. **Ablate context detection**: Run the same experiment with CIIL disabled but keep all other aspects (feedback, timing). Compare task performance to both the original NA condition and CIIL to isolate the effect of real-time adaptation versus feedback design.

2. **Log pseudo-label quality**: During CIIL sessions, record the confidence scores of predictions that receive positive vs. negative context labels. Analyze whether positive-context samples consistently have higher confidence and whether pseudo-label accuracy correlates with task performance improvements.

3. **Test class stability**: After each CIIL session, evaluate offline accuracy on all 8 gesture classes. Measure which classes degrade (likely unmapped gestures like HR, WE) and whether this degradation correlates with their frequency of use during the task. This would validate the "class forgetting" mechanism.