---
ver: rpa2
title: LLM-Assisted Pseudo-Relevance Feedback
arxiv_id: '2601.11238'
source_url: https://arxiv.org/abs/2601.11238
tags:
- query
- retrieval
- documents
- information
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses vocabulary mismatch in information retrieval
  by proposing a novel LLM-assisted pseudo-relevance feedback method. The core idea
  is to use a large language model to filter the top-k retrieved documents before
  applying RM3 query expansion, thereby reducing topic drift caused by noisy documents.
---

# LLM-Assisted Pseudo-Relevance Feedback

## Quick Facts
- arXiv ID: 2601.11238
- Source URL: https://arxiv.org/abs/2601.11238
- Reference count: 19
- Primary result: LLM filtering of top-k documents before RM3 query expansion consistently improves retrieval effectiveness by reducing topic drift, with best results when incorporating TREC topic narratives into prompts.

## Executive Summary
This paper addresses vocabulary mismatch in information retrieval by proposing a novel LLM-assisted pseudo-relevance feedback method. The core idea is to use a large language model to filter the top-k retrieved documents before applying RM3 query expansion, thereby reducing topic drift caused by noisy documents. The LLM judges each document's relevance to the query, and only documents classified as relevant are used for RM3 estimation. Experiments on multiple datasets (AP88-89, ROBUST04, MSMARCO DL-19/20, WT10G) show consistent improvements over standard RM3 and strong baselines. The method achieves statistically significant gains in both AP@1000 and NDCG@100, with the best results when incorporating the TREC topic narrative into the LLM prompt. This approach effectively leverages LLM semantic judgment while remaining grounded in corpus evidence, avoiding the hallucinations typical of generative methods.

## Method Summary
The method filters top-k pseudo-relevant documents using an LLM before applying RM3 query expansion. The process starts with standard first-pass retrieval using Query Likelihood with Dirichlet smoothing. The LLM then judges each document in the top-k ranking for relevance to the query, producing binary judgments. Only documents classified as relevant are retained for RM3 estimation, creating a filtered set D^LLM_k. The relevance model P(t|R) is computed over this filtered set, optionally weighted by the LLM's probability for the "true" token. This relevance model is interpolated with the original query model using parameter λ to create the expanded query, which is then used for second-pass retrieval. The approach can use either fine-tuned MonoT5 for relevance or zero-shot instruction-tuned LLMs like Llama 3.1-8B-Instruct, with optional inclusion of TREC topic narratives in prompts for improved filtering accuracy.

## Key Results
- LLMF + RM3 consistently improves over standard RM3 and strong baselines across AP88-89, ROBUST04, MSMARCO DL-19/20, and WT10G datasets
- Including TREC topic narratives in prompts yields 10-23% improvements in AP@1000 on AP88-89 and ROBUST04
- MonoT5F (fine-tuned for relevance) outperforms generic LLMs like Llama 3.1-8B-Instruct on the filtering task
- The approach achieves statistically significant gains (p < 0.05) in both AP@1000 and NDCG@100 metrics

## Why This Works (Mechanism)

### Mechanism 1: LLM-based Document Filtering Reduces Topic Drift
- **Claim:** Filtering pseudo-relevant documents with LLM relevance judgments before RM3 estimation improves retrieval effectiveness by reducing noise in the feedback set.
- **Mechanism:** Standard PRF assumes all top-k documents are relevant, but noisy documents introduce irrelevant terms that dilute the expanded query. The LLM filters out documents classified as non-relevant, so RM3 estimates the relevance model from a cleaner set D^LLM_k. This preserves corpus-grounded expansion terms while removing tangential content.
- **Core assumption:** LLMs can accurately judge query-document relevance in a zero-shot or instruction-tuned setting without fine-tuning on the target corpus.
- **Evidence anchors:**
  - [abstract]: "the LLM judges the documents in the initial top-k ranking, and RM3 is computed only over those accepted as relevant. This simple intervention improves over blind PRF and a strong baseline across several datasets and metrics."
  - [section 2.1]: "The key advantage of this approach is that LLM judgement capabilities improve the quality of the pseudo-relevance assumption without introducing external content."
  - [corpus]: Related work (Generalized Pseudo-Relevance Feedback, LLM-VPRF) confirms PRF noise sensitivity is a known problem; this paper provides empirical mitigation via filtering rather than generative approaches.
- **Break condition:** If the LLM systematically misclassifies relevant documents as irrelevant (false negatives), the feedback set becomes too sparse for reliable RM3 estimation.

### Mechanism 2: Probability-Weighted Relevance Model
- **Claim:** Substituting the LLM's next-token probability for "true" into the RM3 relevance model estimation further improves effectiveness over binary filtering alone.
- **Mechanism:** Instead of treating all LLM-accepted documents equally, the method weights each document's contribution by P_LLM(next token='true'|q, d). This replaces the query-likelihood factor in Equation 2, allowing the LLM's confidence to modulate term extraction.
- **Core assumption:** The LLM's token probability correlates with actual relevance degree, not just binary classification confidence.
- **Evidence anchors:**
  - [section 2.2]: "we replace that factor by P_LLM(next token='true'|q, d)"
  - [table 2]: MonoT5F + RM3 w/prob shows additional gains over MonoT5F alone on AP88-89 (0.3072 vs 0.2975 AP@1000) and WT10G (0.3680 vs 0.3651 NDCG@100).
  - [corpus]: Related work on vector PRF (LLM-VPRF) explores similar weighting strategies in dense retrieval; this paper applies the principle to sparse lexical expansion.
- **Break condition:** If token probabilities are poorly calibrated (e.g., uniformly high across documents), weighting provides no discriminative benefit and may introduce noise.

### Mechanism 3: Narrative-Enhanced Prompting Improves Filtering
- **Claim:** Including the TREC topic narrative in the LLM prompt substantially improves filtering accuracy and reduces damaged queries.
- **Mechanism:** Short title queries create ambiguous relevance boundaries. The narrative provides explicit relevance criteria, reducing LLM classification errors. This improves the quality of D^LLM_k without changing the RM3 computation itself.
- **Core assumption:** Narratives accurately capture user intent and the LLM can follow narrative instructions as relevance guidelines.
- **Evidence anchors:**
  - [section "Prompt ablation with Llama"]: "including the narrative consistently improves the LLM-filtered variants over their title-only counterparts on AP88–89 and ROBUST04 for both AP@1000 and NDCG@100"
  - [table 3]: LLMF + RM3 w/prob with narrative achieves 0.3540 AP@1000 on AP88-89 (↑20% over title-only).
  - [corpus]: Weak direct evidence—neighbor papers do not systematically compare prompt designs for PRF filtering tasks.
- **Break condition:** In production settings where detailed narratives are unavailable (e.g., web search), this mechanism cannot be directly applied.

## Foundational Learning

- **Concept: Relevance-Based Language Models (RM3)**
  - **Why needed here:** RM3 is the core PRF algorithm being modified. Understanding how it estimates P(t|R) from top-k documents and interpolates with the original query is essential to see why noisy documents cause topic drift.
  - **Quick check question:** If 3 of 10 top-ranked documents are irrelevant, how does RM3's term weighting propagate that error into the expanded query?

- **Concept: Topic Drift in Query Expansion**
  - **Why needed here:** The paper frames LLM filtering as a solution to topic drift. Understanding that expanded queries can shift away from user intent when noisy terms dominate helps contextualize the filtering motivation.
  - **Quick check question:** What is the failure mode when PRF extracts terms from a tangentially-related but non-relevant document?

- **Concept: LLM as a Zero-Shot Relevance Classifier**
  - **Why needed here:** The method relies on instruction-tuned LLMs (MonoT5, Llama 3.1-8B-Instruct) producing reliable binary relevance judgments without task-specific fine-tuning.
  - **Quick check question:** Why might a generative LLM's relevance judgment differ in reliability from its generation capabilities?

## Architecture Onboarding

- **Component map:** First-pass retrieval -> Candidate selection (top-k) -> LLM filtering stage -> Filtered set construction -> RM3 estimation -> Query interpolation -> Second-pass retrieval
- **Critical path:** The LLM filtering stage (step 3) is the novel insertion point. Latency here directly impacts total query time. RM3 parameters (k, e, λ) must be tuned separately for filtered vs. unfiltered configurations.
- **Design tradeoffs:**
  - **MonoT5 vs. Generic LLM:** MonoT5 (fine-tuned for relevance) outperforms Llama 3.1-8B-Instruct but requires task-specific training data. Generic LLMs are more portable but less accurate.
  - **Binary vs. Probability weighting:** Probability weighting adds complexity but shows mixed results—improves some datasets, neutral on others.
  - **Narrative inclusion:** Improves effectiveness but unavailable in many production contexts.
- **Failure signatures:**
  - **Over-aggressive filtering:** If |D^LLM_k| ≈ 0, RM3 has insufficient evidence → expansion fails or defaults to original query.
  - **LLM hallucination in judgment:** LLM may classify based on surface patterns rather than semantic relevance.
  - **Domain mismatch:** MonoT5 trained on MSMARCO underperforms on out-of-domain collections (observed in DL-20 results).
- **First 3 experiments:**
  1. **Baseline replication:** Run QLD + RM3 on AP88-89 and ROBUST04 with paper's parameter ranges (k ∈ {5,10,25,50,75,100}, e ∈ {5-30}, λ ∈ {0.1-0.9}) to verify RM3 baseline numbers.
  2. **Filtering ablation:** Compare MonoT5F vs. LLMF (Llama) on same collections, measuring |D^LLM_k| size to understand filtering aggression.
  3. **Narrative sensitivity:** On collections with available narratives (AP88-89, ROBUST04, WT10G), run title-only vs. title+narrative prompts and measure both effectiveness gains and Robustness Index (queries improved vs. damaged).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the computational overhead of the LLM-based filtering stage, and how does it trade off against effectiveness gains?
- **Basis in paper:** [explicit] Authors state "We believe that a more detailed analysis of this overhead and its trade-offs with effectiveness is an interesting direction for future work."
- **Why unresolved:** The paper acknowledges the overhead is "limited to a small subset of top-ranked documents" but provides no latency, throughput, or cost measurements.
- **What evidence would resolve it:** Systematic measurement of query latency, GPU/CPU costs, and tokens processed per query across different k values and LLM sizes.

### Open Question 2
- **Question:** How do the PRF parameters (k, e, λ) interact with LLM-based filtering to affect overall performance?
- **Basis in paper:** [explicit] Authors list "performing a more detailed analysis of the effects of the PRF parameters in the overall performance" as future work.
- **Why unresolved:** Parameters were tuned separately per method and dataset, but the paper does not analyze how optimal settings shift when filtering is applied or how sensitive results are to parameter choices.
- **What evidence would resolve it:** Ablation studies varying each parameter systematically, with analysis of performance surfaces and sensitivity across filtering and non-filtering conditions.

### Open Question 3
- **Question:** Can the approach maintain effectiveness without access to topic narratives, which are unavailable in many real-world scenarios?
- **Basis in paper:** [inferred] The prompt ablation shows that including narratives yields large gains (10–23% improvement), and DL-20 was excluded because narratives were unavailable.
- **Why unresolved:** Real user queries lack narrative guidance; the paper doesn't test alternative strategies (e.g., query rewriting, synthetic narratives) when narratives are absent.
- **What evidence would resolve it:** Experiments on datasets without narratives, testing proxy strategies such as LLM-generated relevance definitions or few-shot prompting with example narratives.

### Open Question 4
- **Question:** Do reasoning-augmented LLMs or stronger models further improve filtering quality over the tested MonoT5 and Llama 3.1-8B-Instruct?
- **Basis in paper:** [explicit] Authors state they "would also like to explore the use of stronger LLM models and reasoning-powered alternatives."
- **Why unresolved:** Only two models were tested; MonoT5 is fine-tuned for relevance while the generic LLM is not, leaving unclear whether scaling or reasoning capabilities yield additional gains.
- **What evidence would resolve it:** Experiments with larger models (e.g., 70B+), reasoning-augmented architectures, and systematic comparison of zero-shot vs. fine-tuned variants on the filtering task.

## Limitations

- **Narrative dependency:** Best results require TREC topic narratives, which are unavailable in most real-world search scenarios, limiting practical deployment.
- **Efficiency unknown:** The paper provides no measurements of query latency or computational overhead introduced by the LLM filtering stage.
- **Domain generalization:** MonoT5 fine-tuned on MSMARCO underperforms on out-of-domain collections, suggesting limited cross-domain effectiveness.

## Confidence

- **Core filtering mechanism:** High confidence—robust across datasets, metrics, and LLM types
- **Probability weighting improvements:** Medium confidence—gains observed but inconsistent across datasets
- **Narrative prompting benefits:** Medium confidence—strong on evaluated datasets but limited by availability

## Next Checks

1. **Domain robustness test:** Apply LLMF + RM3 to a domain-shifted collection (e.g., biomedical or legal) where MonoT5 was not trained, measuring degradation relative to in-domain performance.
2. **Efficiency benchmarking:** Measure end-to-end query latency (including LLM inference) and compute per-query costs, comparing to vanilla RM3 on the same hardware.
3. **Narrative-absent scenario:** Evaluate title-only prompting on all collections, measuring performance drop and analyzing which query types are most affected by narrative removal.