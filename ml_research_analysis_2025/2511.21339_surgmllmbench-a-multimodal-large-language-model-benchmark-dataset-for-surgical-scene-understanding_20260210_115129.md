---
ver: rpa2
title: 'SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical
  Scene Understanding'
arxiv_id: '2511.21339'
source_url: https://arxiv.org/abs/2511.21339
tags:
- surgical
- dataset
- surgmllmbench
- multimodal
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurgMLLMBench is a multimodal large language model benchmark for
  surgical scene understanding that unifies pixel-level instrument segmentation and
  structured visual question answering across laparoscopic, robot-assisted, and micro-surgical
  domains. It addresses the limitation of existing surgical datasets that lack pixel-level
  annotations and consistent taxonomies, by introducing the newly collected MAVIS
  dataset alongside five existing datasets under a unified annotation schema.
---

# SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding

## Quick Facts
- arXiv ID: 2511.21339
- Source URL: https://arxiv.org/abs/2511.21339
- Authors: Tae-Min Choi; Tae Kyeong Jeong; Garam Kim; Jaemin Lee; Yeongyoon Koh; In Cheul Choi; Jae-Ho Chung; Jong Woong Park; Juyoun Park
- Reference count: 32
- Primary result: Unified benchmark achieving cross-domain generalization for surgical scene understanding through template-based VQA and pixel-level segmentation

## Executive Summary
SurgMLLMBench is a multimodal large language model benchmark that unifies pixel-level instrument segmentation and structured visual question answering across laparoscopic, robot-assisted, and micro-surgical domains. The benchmark addresses the limitation of existing surgical datasets that lack pixel-level annotations and consistent taxonomies by introducing the newly collected MAVIS dataset alongside five existing datasets under a unified annotation schema. Experiments with OMG-LLaVA and LLaVA show that a single model trained on SurgMLLMBench achieves competitive performance across domains and generalizes effectively to unseen datasets, with accuracy improvements of up to 10.67 percentage points on stage recognition in the withheld MAVIS dataset.

## Method Summary
SurgMLLMBench integrates six heterogeneous surgical datasets (Cholec80, EndoVis2018, AutoLaparo, GraSP, MISAW, MAVIS) under a unified COCO-style metadata schema with standardized fields including video ID, frame ID, stage, phase, step, instrument action, and segmentation masks. A template-based VQA generation approach creates consistent prompts across five query types (workflow, count, type, action, source) using ground-truth labels rather than generative LLM methods. The benchmark is evaluated using OMG-LLaVA and LLaVA models with a two-stage training paradigm: pre-training for alignment followed by instruction tuning with LoRA on the LLM component. The MAVIS dataset is withheld from training to test cross-domain generalization.

## Key Results
- Single model trained on SurgMLLMBench achieves competitive performance across laparoscopic, robot-assisted, and micro-surgical domains
- Instruction tuning on SurgMLLMBench yields accuracy improvements of up to 10.67 percentage points on stage recognition in withheld MAVIS dataset
- Visualization results demonstrate enhanced pixel-level segmentation quality and cross-domain workflow reasoning compared to dataset-specific baselines
- OMG-LLaVA shows improved segmentation mIoU and reduced false detections while better capturing thin structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified taxonomy and annotation schema enable cross-domain generalization
- Mechanism: Six heterogeneous surgical datasets are mapped to a COCO-style metadata schema with standardized fields, creating a single supervision signal spanning temporal workflow and spatial layout
- Core assumption: Label semantics across datasets can be meaningfully aligned without losing task-relevant nuance
- Evidence anchors: [abstract] "integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy"
- Break condition: If source datasets have fundamentally incompatible task definitions, unified training will introduce label noise rather than generalization

### Mechanism 2
- Claim: Template-based VQA generation provides more consistent supervision than generative LLM-based prompt creation
- Mechanism: Fixed prompt templates are uniformly sampled across five query types and instantiated with ground-truth labels, avoiding linguistic variability
- Core assumption: Surgical tasks require deterministic, high-precision supervision where phrasing consistency matters more than linguistic diversity
- Evidence anchors: [abstract] "A template-based VQA generation approach ensures consistent, high-precision prompt creation across five query types"
- Break condition: If downstream applications require open-ended conversational flexibility, rigid templates may limit model expressiveness

### Mechanism 3
- Claim: Joint training on pixel-level segmentation and VQA improves visual grounding and workflow reasoning
- Mechanism: OMG-LLaVA combines a segmentation encoder with a vision-language decoder, enabling simultaneous mask generation and natural-language responses
- Core assumption: Pixel-level grounding provides a stronger training signal for scene understanding than bounding-box or text-only supervision
- Evidence anchors: [abstract] "Experiments with OMG-LLaVA and LLaVA show that a single model trained on SurgMLLMBench achieves competitive performance across domains"
- Break condition: If the segmentation decoder and LLM are not well-aligned during instruction tuning, pixel-level supervision may degrade text reasoning

## Foundational Learning

- **Visual Question Answering (VQA)**
  - Why needed here: SurgMLLMBench evaluates models on five VQA query types; understanding how VQA datasets are constructed and evaluated is essential
  - Quick check question: Can you explain the difference between open-ended and multiple-choice VQA evaluation metrics?

- **Instance Segmentation (COCO Format)**
  - Why needed here: Pixel-level instrument masks follow COCO polygon format; model I/O and evaluation (mIoU) depend on this structure
  - Quick check question: How is mean Intersection-over-Union (mIoU) computed across multiple instrument classes?

- **Vision-Language Model Instruction Tuning**
  - Why needed here: OMG-LLaVA and LLaVA use a two-stage paradigm (pre-training for alignment, instruction tuning for task adaptation); LoRA fine-tuning is applied to the LLM
  - Quick check question: What is the difference between freezing vs. LoRA-adapting an LLM during instruction tuning?

## Architecture Onboarding

- **Component map**: Vision encoder (OMG perceptual model) → Visual projector → LLM backbone → Text decoder (VQA output); OMG-decoder (segmentation head) runs in parallel to produce pixel-level masks grounded in text queries; COCO-style annotation store provides frame-level metadata

- **Critical path**: Load unified SurgMLLMBench dataset with standardized annotations → Run pre-training (frozen vision + LLM, train projectors only; use provided checkpoint) → Instruction tuning on SurgMLLMBench (LoRA on LLM, unfreeze OMG-decoder) → Evaluate on withheld MAVIS to test cross-domain generalization

- **Design tradeoffs**: Template-based VQA ensures consistency but limits linguistic diversity; withholding MAVIS from training tests generalization but reduces available micro-surgical data; OMG-LLaVA adds segmentation capability but shows performance degradation on some transfer tasks compared to text-only LLaVA

- **Failure signatures**: Overfitting to dataset-specific visual styles (spurious masks, missed instruments on new datasets); cross-dataset label conflation (e.g., predicting "RETRACTION" instead of "PULL" due to inconsistent action taxonomies); OMG-decoder failing to transfer when visual gaps between datasets are large

- **First 3 experiments**: Replicate LLaVA baseline on SurgMLLMBench (excluding MAVIS); report phase/step/action/count accuracy on each dataset; train OMG-LLaVA with same split; compare segmentation mIoU and VQA accuracy against dataset-specific fine-tuning; fine-tune the SurgMLLMBench-checkpoint on MAVIS; measure improvement vs. training from scratch to quantify transfer benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of additional modalities—such as kinematics, audio, and depth data—affect the temporal reasoning capabilities of multimodal LLMs trained on SurgMLLMBench?
- Basis in paper: [explicit] The conclusion states, "Future work will extend temporal and multimodal coverage (e.g., kinematics, audio, depth)... and assess real-time reliability"
- Why unresolved: The current benchmark focuses primarily on visual and textual modalities, lacking the sensorimotor data required to assess temporal reasoning in dynamic surgical environments fully
- What evidence would resolve it: A modified version of the benchmark incorporating these modalities, followed by experiments showing improved performance on temporal tasks like workflow anticipation or action prediction

### Open Question 2
- Question: What specific mechanisms cause pixel-level decoding components (as in OMG-LLaVA) to suffer negative transfer when generalizing to unseen datasets, compared to text-vision models like LLaVA?
- Basis in paper: [inferred] Section 4.3 notes that while LLaVA benefits from SurgMLLMBench initialization on the unseen MAVIS dataset, OMG-LLaVA experiences performance degradation, likely due to "visual gaps between datasets limiting the transferability of its pixel-level decoder"
- Why unresolved: The paper identifies the phenomenon but does not isolate the architectural or data-distribution causes responsible for this negative transfer
- What evidence would resolve it: Ablation studies analyzing feature alignment between the encoder and decoder across domains, or experiments utilizing domain adaptation techniques to close the visual gap for the segmentation head

### Open Question 3
- Question: How can surgical action taxonomies be unified or normalized to prevent models from penalizing contextually valid predictions that utilize semantically similar labels (e.g., "Retraction" vs. "Pull")?
- Basis in paper: [inferred] Section 4.3 highlights that the model often predicts actions like "RETRACTION" when the ground truth is "PULL" (or "IDLE" vs. "STILL"), resulting in low numerical accuracy despite the prediction being contextually valid due to inconsistent naming conventions across source datasets
- Why unresolved: The current evaluation relies on strict text matching, and the unification process has not fully resolved semantic overlaps where different datasets use distinct terms for identical actions
- What evidence would resolve it: The introduction of a soft-matching evaluation metric or a mapping layer that groups semantically equivalent actions across datasets, resulting in higher accuracy scores for valid inferences

### Open Question 4
- Question: What evaluation frameworks are required to assess the real-time reliability, uncertainty, and safety of these models for actual clinical deployment?
- Basis in paper: [explicit] The conclusion explicitly lists "assess[ing] real-time reliability, uncertainty, and safety for clinical deployment" as a direction for future work
- Why unresolved: The current benchmark evaluates static frame-level accuracy and segmentation mIoU, which are insufficient metrics for verifying the safety and reliability required in an intraoperative setting
- What evidence would resolve it: The development of new metrics for uncertainty calibration (e.g., Expected Calibration Error) and latency benchmarks that measure model performance under real-time constraints

## Limitations
- Unified taxonomy mapping may introduce label noise where semantic conflicts exist across heterogeneous datasets
- Template-based VQA generation may limit linguistic diversity needed for real-world conversational flexibility
- OMG-LLaVA performance degradation on MAVIS transfer suggests potential misalignment between pixel-level and text reasoning pathways
- Paper lacks specification of pre-training dataset composition and LoRA hyperparameters, creating reproducibility gaps

## Confidence

- Cross-domain generalization through unified taxonomy: **Medium** - The mechanism is sound but label conflicts and semantic ambiguities across datasets create uncertainty about the quality of unification
- Template-based VQA superiority over generative approaches: **High** - Direct empirical comparison and clear mechanism, though real-world applicability remains untested
- Joint segmentation-VQA training improving grounding: **Medium** - Visualizations show improvements but OMG-LLaVA transfer performance is inconsistent across datasets

## Next Checks

1. **Label alignment audit**: Manually inspect 100 random samples from each dataset to quantify semantic conflicts in mapped labels (e.g., count instances where action taxonomies cannot be consistently unified)

2. **Template flexibility stress test**: Generate 100 diverse linguistic variants for each query type using an LLM, then evaluate whether SurgMLLMBench-trained models maintain performance across phrasing variations

3. **Pixel-text alignment ablation**: Train three variants - (a) segmentation-only, (b) VQA-only, (c) joint - then measure cross-dataset generalization and inspect attention maps to verify whether joint training genuinely improves visual grounding or merely adds capacity