---
ver: rpa2
title: Attention Is All You Need for KV Cache in Diffusion LLMs
arxiv_id: '2510.14973'
source_url: https://arxiv.org/abs/2510.14973
tags:
- tokens
- arxiv
- cache
- diffusion
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the high computational cost of decoding in
  diffusion large language models (DLMs) by identifying redundancy in recomputing
  key-value (KV) caches for all tokens and layers at every denoising step. The authors
  propose Elastic-Cache, a training-free, architecture-agnostic method that adaptively
  decides when and where to refresh KV caches using two strategies: (1) an attention-aware
  drift test on the most-attended token to trigger refreshes only when KV states change
  significantly, and (2) a depth-aware refresh schedule that recomputes from a chosen
  layer onward while reusing shallow-layer caches and block-caching distant MASK tokens.'
---

# Attention Is All You Need for KV Cache in Diffusion LLMs

## Quick Facts
- **arXiv ID:** 2510.14973
- **Source URL:** https://arxiv.org/abs/2510.14973
- **Reference count:** 40
- **Key outcome:** Introduces Elastic-Cache, a training-free, architecture-agnostic method that reduces KV cache recomputation in diffusion LLM decoding via selective refresh based on attention drift and depth-aware scheduling, achieving up to 45.1× speedup on long sequences with minimal quality loss.

## Executive Summary
This paper addresses the high computational cost of decoding in diffusion large language models (DLMs) by identifying redundancy in recomputing key-value (KV) caches for all tokens and layers at every denoising step. The authors propose Elastic-Cache, a training-free, architecture-agnostic method that adaptively decides when and where to refresh KV caches using two strategies: (1) an attention-aware drift test on the most-attended token to trigger refreshes only when KV states change significantly, and (2) a depth-aware refresh schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and block-caching distant MASK tokens. This approach reduces redundant computation and accelerates decoding with minimal loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: 8.7× on GSM8K (256 tokens), up to 45.1× on longer sequences, and 6.8× higher throughput than existing confidence-based approaches while preserving accuracy.

## Method Summary
Elastic-Cache targets inefficiency in diffusion LLM decoding, where KV caches are recomputed for every token and layer at each denoising step. The method introduces two complementary strategies. First, an attention-aware drift test monitors the most-attended token at each layer; if its attention weights change significantly, a KV cache refresh is triggered for that layer. Second, a depth-aware refresh schedule recomputes KV states from a chosen layer onward, reusing caches from shallower layers and applying block-caching to distant MASK tokens. This allows selective recomputation only when and where the attention distribution has meaningfully drifted, avoiding unnecessary work and enabling significant computational savings without retraining.

## Key Results
- Achieves 8.7× speedup on GSM8K with 256 tokens and up to 45.1× on longer sequences.
- Outperforms confidence-based baselines by 6.8× in throughput while preserving accuracy.
- Maintains generation quality across mathematical reasoning and code generation tasks on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V.

## Why This Works (Mechanism)
Elastic-Cache exploits the observation that diffusion LLM decoding involves redundant KV cache recomputations at every step, even when attention patterns remain stable. By monitoring attention drift on the most-attended token per layer and selectively refreshing only when significant change is detected, the method avoids unnecessary recomputation. Depth-aware scheduling further reduces work by reusing shallow-layer caches and applying block-caching for distant tokens. This adaptive, layer- and step-wise approach targets the primary source of inefficiency in DLM decoding.

## Foundational Learning

- **KV Cache**: Stores intermediate attention results to avoid recomputing token representations at every decoding step; essential for efficient autoregressive generation.
  - *Why needed:* Reduces computational cost by reusing past computations across decoding steps.
  - *Quick check:* Verify that cache updates only occur when input or state changes.

- **Attention Drift**: Refers to changes in the distribution of attention weights over time or steps; a key signal for when cached values become stale.
  - *Why needed:* Identifies when cached KV values are no longer representative, prompting refresh.
  - *Quick check:* Measure attention weight variance between consecutive steps.

- **Block-Caching**: A strategy to cache and reuse KV states for tokens far from the current position, leveraging sparsity in attention patterns.
  - *Why needed:* Reduces redundant computation for distant tokens with predictable or infrequent attention changes.
  - *Quick check:* Confirm that distant token updates are infrequent or predictable.

## Architecture Onboarding

- **Component map**: Input tokens → Attention layers (shallow to deep) → Attention-aware drift test → Depth-aware refresh schedule → KV cache updates → Output tokens.
- **Critical path**: Token input → Attention computation → Drift detection → Conditional KV refresh → Next decoding step.
- **Design tradeoffs**: Selective refresh vs. full recomputation (speed vs. staleness risk); block-caching vs. per-token caching (efficiency vs. memory use).
- **Failure signatures**: Excessive drift detection triggers (false positives), stale KV caches leading to quality drop, memory overhead from auxiliary state.
- **First experiments**:
  1. Benchmark drift test sensitivity and false positive rate on held-out data.
  2. Profile memory and compute overhead of block-caching strategy.
  3. Validate generation quality preservation on a diverse set of tasks and model sizes.

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus on synthetic benchmarks and controlled tasks; real-world deployment and production-scale scenarios are not evaluated.
- The approach is validated only on diffusion LLMs; generalization to other architectures is claimed but not empirically demonstrated.
- No quantification of computational or memory overhead introduced by the drift test or auxiliary state management.

## Confidence
- **High confidence:** Identification of KV cache redundancy and internal consistency of reported speedups.
- **Medium confidence:** Effectiveness of proposed strategies within tested models and tasks.
- **Low confidence:** Claims about architecture-agnosticism and robustness in diverse deployment settings.

## Next Checks
1. Benchmark Elastic-Cache on a production-grade serving stack with varied task types, sequence lengths, and concurrent request patterns to measure actual throughput and latency gains.
2. Test the method on non-diffusion architectures (e.g., standard autoregressive LLMs) and across a broader range of model sizes and attention variants.
3. Quantify the memory and compute overhead introduced by drift testing and auxiliary state management, and provide ablations to isolate the impact of each proposed strategy.