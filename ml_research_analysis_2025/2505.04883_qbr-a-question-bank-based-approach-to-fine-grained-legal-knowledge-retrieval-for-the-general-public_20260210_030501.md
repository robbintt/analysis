---
ver: rpa2
title: 'QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval
  for the General Public'
arxiv_id: '2505.04883'
source_url: https://arxiv.org/abs/2505.04883
tags:
- legal
- retrieval
- document
- user
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of legal knowledge retrieval
  for the general public, who often struggle with technical legal documents. The proposed
  QBR method uses a Question Bank (QB) to bridge this knowledge gap by deriving training
  samples that enhance the embedding of knowledge units within documents, enabling
  fine-grained knowledge retrieval.
---

# QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public

## Quick Facts
- **arXiv ID:** 2505.04883
- **Source URL:** https://arxiv.org/abs/2505.04883
- **Reference count:** 9
- **Primary result:** QBR achieves 54% Recall@1 for document retrieval vs 25% for BM25, and 83.7% accuracy for scope identification vs 48.7% for SBERT

## Executive Summary
QBR addresses the challenge of legal knowledge retrieval for the general public who struggle with technical legal documents. The method uses a Question Bank (QB) to bridge this knowledge gap by deriving training samples that enhance embedding of knowledge units within documents, enabling fine-grained knowledge retrieval. QBR's two-step process involves document retrieval followed by scope identification within retrieved documents. Experiments show QBR significantly outperforms traditional methods, achieving 54% Recall@1 for document retrieval (vs 25% for BM25) and 83.7% accuracy for scope identification (vs 48.7% for SBERT). The method also demonstrates improved efficiency and explainability by returning question-answer pairs instead of full documents.

## Method Summary
QBR employs a two-stage retrieval process: first identifying relevant documents, then pinpointing specific answer scopes within those documents. The method uses a Question Bank containing question-document-scope triples. For document retrieval, user queries are matched against QB question-scope pairs rather than directly against documents. For scope identification, contrastive learning fine-tunes embeddings using positive pairs (questions matched with their answering scopes) and negative pairs (questions matched with non-answering scopes from the same document). GPT-augmentation generates synthetic user inputs for "hard-to-distinguish" scopes to improve disambiguation.

## Key Results
- **Document retrieval:** QBR achieves 54% Recall@1 vs 25% for BM25 and 45% for MPNet
- **Scope identification:** QBR achieves 83.7% accuracy vs 48.7% for SBERT, with MRRs of 0.910 vs 0.706
- **QB effectiveness:** Even 10K-question QB achieves ~72% accuracy vs ~50% baseline; 38K QB achieves 83.7%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Matching user input against question-scope pairs from a Question Bank (QB) improves document retrieval accuracy compared to direct document matching.
- **Mechanism:** Instead of computing similarity between user input `u` and full documents `d`, QBR computes cosine similarity between `T(u)` and `T(q; s)` for all question-scope pairs `(q, s)` in the QB. The document containing the best-matching scope is returned.
- **Core assumption:** Users naturally express queries as questions, and pre-generated questions in the QB better capture semantic intent than keyword-based document matching.
- **Evidence anchors:** [abstract]: "QBR significantly outperforms traditional methods, achieving a Recall@1 of 0.54 compared to 0.25 for BM25"; [Page 5, Table 2]: QBR Recall@1 = 0.5400 vs MPNet = 0.4500 vs BM25 = 0.2540

### Mechanism 2
- **Claim:** Contrastive learning (CL) using QB-derived training examples significantly improves fine-grained scope disambiguation within documents.
- **Mechanism:** For each document `d`, the QB provides question sets `Rs` for each scope `s`. Positive examples `E+(s) = {(q, s) | q ∈ Rs}` pair questions with their answering scopes. Negative examples `E-(s) = {(q, s') | q ∈ Rs, s' ∈ Sd \ {s}}` pair the same questions with non-answering scopes in the same document.
- **Core assumption:** Questions associated with each scope provide sufficient discriminative signal to separate semantically similar scopes within the same document.
- **Evidence anchors:** [Page 4]: "We fine-tune the embedding function T() with the objective of pulling the embedding vectors T(q), T(s) of positive examples (q, s) ∈ E+(s) closer and pushing those of negative examples in E-(s) farther."; [Page 6, Table 3]: QBR scope identification accuracy = 0.837 vs MPNet = 0.500

### Mechanism 3
- **Claim:** GPT-augmented synthetic user inputs further improve CL-based scope disambiguation by simulating realistic layperson query patterns.
- **Mechanism:** For the two most similar (hardest-to-disambiguate) scopes in each document, ChatGPT generates synthetic user inputs `cuq` based on questions `q` with the prompt: "Given the following context, provide a realistic real-life scenario that a person who knows nothing about legal knowledge might encounter."
- **Core assumption:** GPT-generated scenarios adequately mimic how non-experts describe legal situations, providing diverse training signal for disambiguation.
- **Evidence anchors:** [Page 4]: "We augment the CL training data by employing ChatGPT to generate user input that mimics novice users."; [Page 6, Table 3]: QBR without GPT-augmentation (QBR¬GPT) drops accuracy from 0.837 to 0.646

## Foundational Learning

- **Contrastive Learning (InfoNCE loss):**
  - Why needed here: Core to scope disambiguation; understanding how positive/negative pair construction and temperature-scaled softmax loss shapes embedding space is essential.
  - Quick check question: Given two scopes in the same document, can you construct valid positive and negative training pairs from associated questions?

- **Dense Retrieval / Embedding-based IR:**
  - Why needed here: QBR relies on transformer embeddings (`T` and `T'`) and cosine similarity for all matching operations.
  - Quick check question: How does cosine similarity in embedding space differ from lexical matching (e.g., BM25) in handling paraphrased queries?

- **Transformer-based Sentence Embeddings (MPNet/BERT):**
  - Why needed here: MPNet provides the base embedding function `T`; understanding pre-trained sentence encoders is prerequisite to fine-tuning with CL.
  - Quick check question: What does MPNet learn during pre-training, and why is it suitable for semantic similarity tasks?

## Architecture Onboarding

- **Component map:** Question Bank -> Base Embedding Function T -> CL-Adjusted Embedding Function T' -> Vector Database -> GPT-Augmentation Module

- **Critical path:**
  1. **QB Construction:** Generate/collect questions and scope annotations for each document
  2. **CL Training:** Construct positive/negative examples from QB; optionally augment with GPT-generated scenarios; fine-tune T → T'
  3. **Inference:** Given user input u, compute similarity against all (q, s) pairs → identify best document d* → re-rank scopes within d* using T' → return top (q, s*) pairs

- **Design tradeoffs:**
  - **QB size vs. quality:** Larger QB improves recall but requires more storage and computation; paper shows even 10K questions achieve ~0.72 accuracy vs ~0.50 baseline
  - **Human vs. machine questions:** HCQs slightly outperform MGQs; combined QB performs best (complementary coverage)
  - **CL training cost:** Fine-tuning requires constructing (q, s) pairs per document; GPT-augmentation adds ~12K synthetic examples but significantly boosts accuracy

- **Failure signatures:**
  - **Low scope identification accuracy:** May indicate insufficient QB coverage or semantically indistinguishable scopes; check QB size and scope granularity
  - **Poor document retrieval recall:** May indicate embedding function mismatch or QB questions not covering query intent; verify domain alignment
  - **Slow query latency:** May indicate inefficient vector indexing; ensure approximate nearest neighbor (ANN) indexing is used

- **First 3 experiments:**
  1. **Baseline comparison:** Measure Recall@1, Recall@5, MRRd for document retrieval against BM25 and MPNet using the provided test set
  2. **Ablation on QB size:** Evaluate QBR performance with progressively smaller QB subsets (e.g., 5K, 10K, 20K questions) to quantify coverage effects
  3. **Ablation on GPT-augmentation:** Train QBR with and without GPT-augmented examples; compare scope identification accuracy and MRRs to isolate augmentation contribution

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- **Limited generalizability:** QBR is evaluated only on legal knowledge, with effectiveness on other domains unclear
- **GPT-augmentation dependency:** Significant performance gains (Scope Acc 83.7% → 64.6% without GPT) suggest potential brittleness if synthetic data quality degrades
- **Resource intensity:** Two-stage inference with re-ranking may not scale efficiently for large document collections despite vector indexing

## Confidence
- **High confidence:** Document retrieval improvements (54% Recall@1 vs 25% BM25) with clear mechanism and consistent results
- **Medium confidence:** Scope identification improvements (83.7% vs 48.7%) given GPT-augmentation's outsized contribution and lack of robustness testing
- **Medium confidence:** Explainability benefits (returning QA pairs vs full documents) though user study evidence is limited

## Next Checks
1. **Cross-domain validation:** Test QBR on non-legal domains (e.g., medical, technical support) to verify QB-based matching generalizes beyond legal knowledge
2. **Robustness to QB sparsity:** Evaluate performance with progressively smaller QBs (5K, 10K questions) to determine minimum viable coverage and identify breaking points
3. **Real-user query evaluation:** Conduct user studies with actual layperson queries (not synthetic) to validate that QBR handles natural language variations and confirms explainability benefits in practice