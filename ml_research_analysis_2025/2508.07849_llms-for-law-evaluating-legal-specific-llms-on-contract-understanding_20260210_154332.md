---
ver: rpa2
title: 'LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding'
arxiv_id: '2508.07849'
source_url: https://arxiv.org/abs/2508.07849
tags:
- llms
- legal
- legal-specific
- contract
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of 10 legal-specific
  large language models (LLMs) against 7 general-purpose LLMs across three contract
  understanding tasks. The study aims to address the gap in legal NLP research where
  legal-specific models are often excluded from evaluations despite their potential
  advantages.
---

# LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding

## Quick Facts
- arXiv ID: 2508.07849
- Source URL: https://arxiv.org/abs/2508.07849
- Reference count: 20
- Primary result: Legal-specific LLMs (Legal-BERT, Contracts-BERT) achieve SOTA performance on contract understanding tasks despite having 69% fewer parameters than general-purpose models

## Executive Summary
This paper presents a comprehensive evaluation of 10 legal-specific large language models (LLMs) against 7 general-purpose LLMs across three contract understanding tasks. The study addresses the gap in legal NLP research where legal-specific models are often excluded from evaluations despite their potential advantages. Results show that legal-specific models, particularly Legal-BERT and Contracts-BERT, consistently outperform general-purpose models on tasks requiring nuanced legal understanding, despite having 69% fewer parameters. These models achieve new state-of-the-art performances on two of three tasks. The evaluation identifies Legal-BERT, Contracts-BERT, CaseLaw-BERT, and LexLM as strong baselines for contract understanding tasks. The study also highlights limitations in recent legal-specific LLMs due to limited pretraining on diverse contract data, suggesting future models should incorporate more representative contract datasets.

## Method Summary
The study evaluates 17 LLMs (10 legal-specific, 7 general-purpose) on three contract understanding tasks: UNFAIR-ToS (terms of service clause classification), LEDGAR (prospectus provision classification), and LEXDEMOD (lease clause classification). All models use encoder-only architectures and are fine-tuned with supervised learning using standard hyperparameters (learning rate 3e-5 for base models, 1e-5 for large models, batch size 8, max sequence length 128). Performance is measured using macro-F1 and micro-F1 metrics, with evaluation across five random seeds for most tasks. The study uses Hugging Face's training framework with early stopping based on development set performance.

## Key Results
- Legal-specific models outperform general-purpose models on all three contract understanding tasks
- Legal-BERT and Contracts-BERT establish new SOTAs on two of three tasks despite having 69% fewer parameters
- PoL-BERT catastrophically fails on LEXDEMOD (µ-F1: 41.35) despite strong performance elsewhere
- Recent legal-specific LLMs underperform older models due to limited contract pretraining data diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining can compensate for significantly fewer parameters on specialized legal tasks.
- Mechanism: Legal-specific models encode legal terminology, document structure, and semantic patterns during pretraining, reducing the learning burden during fine-tuning. This allows 110M parameter models to outperform 355M general-purpose models on tasks requiring nuanced legal understanding.
- Core assumption: The performance gains stem from domain knowledge transfer rather than architectural differences, as all evaluated legal models share BERT/RoBERTa base architectures.
- Evidence anchors:
  - [abstract] "Legal-BERT and Contracts-BERT establish new SOTAs on two of the three tasks, despite having 69% fewer parameters than the best-performing general-purpose LLM."
  - [section 4/SRQ1] "Contracts-BERT and Legal-BERT (110M) outperform RoBERTa-large on UNFAIR-ToS and LEXDEMOD, respectively, despite having 69% fewer parameters."
  - [corpus] Related work on legal LLM benchmarks consistently shows domain adaptation improves specialized task performance, supporting transfer mechanism.
- Break condition: If legal-specific pretraining corpora lack diversity in contract types, domain knowledge becomes too narrow, limiting generalization across contract understanding tasks.

### Mechanism 2
- Claim: Corpus composition quality trumps corpus quantity for contract-specific tasks.
- Mechanism: Models pre-trained on focused contract corpora (e.g., Contracts-BERT on 76K US contracts) can outperform models trained on larger but more heterogeneous legal corpora because the learned representations better align with contract-specific language patterns and clause structures.
- Core assumption: Contract language has distinct distributional properties from legislation, case law, and other legal genres that benefit from dedicated pretraining.
- Evidence anchors:
  - [section 4/SRQ3] "Older legal-specific base-variant LLMs, such as LegalBERT and ContractsBERT, which are pre-trained on just 354k and 76k legal documents respectively, still outperform many recent base-variant legal-specific models."
  - [section 4/SRQ3] "A key limitation of recent legal-specific LLMs is that they are pre-trained on few, or no, diverse contract documents compared to other legal texts like legislation and court cases."
  - [corpus] Weak direct corpus evidence on corpus composition effects specifically for contracts; related legal reasoning papers don't isolate this variable.
- Break condition: When target tasks require broader legal reasoning beyond contract interpretation (e.g., statutory analysis, precedent chains), focused contract corpora become insufficient.

### Mechanism 3
- Claim: Supervised fine-tuning with legal-specific initialization provides stronger baselines than prompting general-purpose LLMs for contract classification.
- Mechanism: Fine-tuning adapts all model parameters to task-specific label distributions and legal semantics, while prompting relies on frozen representations that may not capture nuanced distinctions (e.g., distinguishing "unfair" from "fair" contractual terms under EU consumer law).
- Core assumption: The labeled training data is sufficiently representative of test distributions, and legal-specific pretraining provides initialization that is closer to the optimal fine-tuned weights.
- Evidence anchors:
  - [section 4/Table 4] GPT-3.5-Turbo zero-shot achieves µ-F1 of 41.4 on UNFAIR-ToS vs. 96.2 for fine-tuned Contracts-BERT; few-shot improves to 64.7 but still lags significantly.
  - [section 3] The study focuses on encoder-based models with supervised fine-tuning; decoder-based models excluded due to metric compatibility issues.
  - [corpus] Related papers on legal LLM evaluation suggest chain-of-thought prompting narrows but doesn't close the gap with fine-tuned models on complex legal tasks.
- Break condition: For tasks with limited labeled data or rapidly evolving legal standards, the cost of fine-tuning may outweigh benefits, and prompting with retrieval augmentation becomes competitive.

## Foundational Learning

- Concept: **Encoder-only vs. Decoder-only architectures for classification**
  - Why needed here: All evaluated legal-specific LLMs use encoder-only architectures (BERT-family), which are optimized for producing fixed representations for classification, unlike decoder models designed for text generation.
  - Quick check question: Can you explain why masked language modeling (BERT pretraining) produces representations better suited for classification than causal language modeling (GPT pretraining)?

- Concept: **Multi-label vs. Multi-class classification in legal contexts**
  - Why needed here: UNFAIR-ToS and LEXDEMOD are multi-label tasks (one clause can have multiple labels), while LEDGAR is multi-class (one label per provision). Different evaluation metrics and loss functions apply.
  - Quick check question: Why does macro-F1 matter more than micro-F1 when evaluating on imbalanced legal datasets with rare but important categories?

- Concept: **Transfer learning in specialized domains**
  - Why needed here: Legal-specific models start from general-purpose checkpoints (BERT-base-uncased) and undergo continued pretraining on legal corpora before task-specific fine-tuning—a two-stage transfer process.
  - Quick check question: What could go wrong if you fine-tune a legal-specific model on a contract dataset with a very different jurisdiction (e.g., training on US contracts, testing on EU consumer contracts)?

## Architecture Onboarding

- Component map:
  - **Pretrained encoders**: 10 legal-specific (Legal-BERT, Contracts-BERT, CaseLaw-BERT, InLegalBERT variants, LexLM, Legal-XLM-R, PoL-BERT) + 7 general-purpose (BERT, RoBERTa variants, DeBERTa, Longformer, BigBird)
  - **Classification heads**: Task-specific linear layers added on top of [CLS] token representation
  - **Fine-tuning pipeline**: Hugging Face models → task-specific dataloaders → supervised training with early stopping

- Critical path:
  1. Select appropriate legal-specific baseline based on contract type and jurisdiction (see Table 3 for pretraining corpora)
  2. Configure sequence length based on document statistics (LEDGAR requires truncation handling—many provisions exceed 512 tokens per Figure 1)
  3. Set learning rate: 3e-5 for base models, 1e-5 for large models (PoL-BERT)
  4. Train with early stopping (patience=3) on development set

- Design tradeoffs:
  - **Truncation vs. long-context models**: LEDGAR paragraphs often exceed 512 tokens; this study truncates to 128 for efficiency with ~0.1-0.4% performance drop. Alternative: use Longformer/BigBird at higher computational cost.
  - **Mixed precision**: Disabled (fp16=False) for training stability despite longer training times.
  - **Seed selection**: Report best of 5 seeds for UNFAIR-ToS/LEDGAR, average of best 3 for LEXDEMOD—introduces variance in reported results.

- Failure signatures:
  - PoL-BERT (340M parameters) catastrophically fails on LEXDEMOD (µ-F1: 41.35) despite strong performance elsewhere—likely jurisdiction/corpus mismatch (Pile-of-Law emphasizes US legal documents; LEXDEMOD contains lease clauses)
  - CustomInLawBERT underperforms on UNFAIR-ToS (m-F1: 79.9 vs. 83.4 for Contracts-BERT)—Indian legal pretraining may not transfer well to EU consumer protection concepts

- First 3 experiments:
  1. **Baseline replication**: Fine-tune Legal-BERT and Contracts-BERT on UNFAIR-ToS using provided hyperparameters (lr=3e-5, batch_size=8, max_seq_length=128). Verify you can reproduce µ-F1 scores within 1% of reported values.
  2. **Corpus ablation**: Test whether a general-purpose BERT with added legal vocabulary (but no continued pretraining) narrows the gap with Legal-BERT—isolates vocabulary effects from representation learning effects.
  3. **Cross-task transfer**: Fine-tune Contracts-BERT (pretrained on US contracts) on LEXDEMOD (lease contracts) vs. UNFAIR-ToS (ToS contracts). Compare performance deltas to understand contract-type sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do decoder-based legal-specific LLMs (e.g., AdaptLLM, SaulLM-7B) compare to encoder-based legal-specific models on contract understanding tasks?
- Basis in paper: [explicit] The authors state: "Decoder-only models like AdaptLLM and SaulLM-7B are emerging but custom metrics are not well-supported by the TRL library... We leave their benchmarking for future work."
- Why unresolved: The study excluded decoder-only models due to technical limitations with custom metric support in the TRL library, and these models remain scarce.
- What evidence would resolve it: A comprehensive benchmark comparing decoder-based legal LLMs against the identified strong baselines (Legal-BERT, Contracts-BERT, CaseLaw-BERT, LexLM) on all three contract tasks.

### Open Question 2
- Question: Does pretraining on more diverse contract documents improve performance of recent legal-specific LLMs on contract understanding tasks?
- Basis in paper: [explicit] The authors conclude: "A key limitation of recent legal-specific LLMs is that they are pre-trained on few, or no, diverse contract documents compared to other legal texts... future legal-specific LLMs should incorporate a more diverse and representative set of contract documents."
- Why unresolved: Older models (Legal-BERT, Contracts-BERT) trained on fewer documents outperformed newer models trained on larger corpora lacking contract diversity.
- What evidence would resolve it: Comparing existing models against new legal-specific LLMs pretrained with augmented contract corpora across the three benchmark tasks.

### Open Question 3
- Question: How do legal-specific LLMs perform on contract understanding tasks in languages other than English?
- Basis in paper: [explicit] The authors note: "The limited availability of contract benchmark datasets in languages other than English poses a challenge for multilingual extension... leaving evaluation on non-English data for future work."
- Why unresolved: The study focused exclusively on English-language contract tasks due to dataset availability constraints.
- What evidence would resolve it: Evaluating legal-specific multilingual models like Legal-XLM-R on contract understanding benchmarks in multiple languages once such datasets become available.

## Limitations

- The study focuses exclusively on encoder-based models with supervised fine-tuning, excluding decoder-based and encoder-decoder models that might perform differently under zero/few-shot prompting paradigms.
- Limited diversity in evaluated contract types (US contracts, terms of service, and lease agreements) may not represent the full spectrum of legal contract understanding tasks.
- The catastrophic failure of PoL-BERT on LEXDEMOD suggests jurisdiction-specific pretraining may not transfer well across legal domains, but this finding is based on a single data point.

## Confidence

- **High Confidence**: Legal-specific models consistently outperform general-purpose models on the evaluated contract tasks, with Legal-BERT and Contracts-BERT establishing new SOTAs. This finding is robust across multiple datasets and evaluation metrics.
- **Medium Confidence**: The mechanism explaining why fewer parameters can achieve superior performance (domain-specific pretraining encoding legal semantics) is plausible but not definitively proven, as the study does not control for all architectural variables.
- **Low Confidence**: The claim that corpus composition quality trumps quantity for contract-specific tasks is weakly supported, with only indirect evidence from comparing model performance to pretraining corpus sizes.

## Next Checks

1. **Cross-Jurisdiction Transfer Learning**: Fine-tune a US-contract-pretrained model (e.g., Contracts-BERT) on European contract datasets and measure performance degradation compared to European-pretrained models. This would validate whether corpus composition effects are jurisdiction-dependent.

2. **Architecture-Agnostic Legal Knowledge Transfer**: Train a RoBERTa model with legal vocabulary injection (but no continued pretraining) and compare performance to Legal-BERT on all three tasks. This would isolate the contribution of domain-specific pretraining from architectural advantages.

3. **Long-Context Performance Evaluation**: Re-evaluate LEDGAR using Longformer/BigBird architectures without truncation and compare to truncated BERT-family models. This would determine whether sequence length limitations artificially constrained performance on longer contract provisions.