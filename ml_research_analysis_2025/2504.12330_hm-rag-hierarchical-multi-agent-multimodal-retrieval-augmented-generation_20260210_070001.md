---
ver: rpa2
title: 'HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation'
arxiv_id: '2504.12330'
source_url: https://arxiv.org/abs/2504.12330
tags:
- retrieval
- arxiv
- multimodal
- agent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HM-RAG, a novel hierarchical multi-agent multimodal
  retrieval-augmented generation framework designed to address the limitations of
  conventional single-agent RAG systems in resolving complex queries across heterogeneous
  data ecosystems. The framework introduces a three-tiered architecture with specialized
  agents: a Decomposition Agent that dissects complex queries into contextually coherent
  sub-tasks via semantic-aware query rewriting, Multi-source Retrieval Agents that
  conduct parallel, modality-specific retrieval across vector, graph, and web-based
  databases, and a Decision Agent that integrates multi-source answers through consistency
  voting and expert model refinement.'
---

# HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2504.12330
- Source URL: https://arxiv.org/abs/2504.12330
- Reference count: 40
- Key outcome: HM-RAG achieves 12.95% improvement in answer accuracy over baseline RAG systems on ScienceQA and CrisisMMD benchmarks

## Executive Summary
This paper presents HM-RAG, a hierarchical multi-agent multimodal retrieval-augmented generation framework designed to address the limitations of conventional single-agent RAG systems in resolving complex queries across heterogeneous data ecosystems. The framework introduces a three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks via semantic-aware query rewriting, Multi-source Retrieval Agents that conduct parallel, modality-specific retrieval across vector, graph, and web-based databases, and a Decision Agent that integrates multi-source answers through consistency voting and expert model refinement. HM-RAG achieves state-of-the-art results in zero-shot settings on both ScienceQA and CrisisMMD benchmarks, establishing a new standard for complex multimodal reasoning in RAG systems.

## Method Summary
HM-RAG employs a three-tier hierarchical architecture consisting of Decomposition, Multi-source Retrieval, and Decision Agents. The Decomposition Agent first classifies queries as single or multi-intent, rewriting complex queries into 2-3 logically connected sub-questions. The Multi-source Retrieval Agents then execute parallel retrieval across three modalities: vector-based semantic matching, graph-based structural reasoning, and web-based real-time knowledge acquisition. The Decision Agent synthesizes results using consistency voting metrics (ROUGE-L/BLEU) and escalates to an Expert Model (GPT-4o) when conflicts arise. The framework supports seamless integration of new data modalities while maintaining strict data governance.

## Key Results
- 12.95% improvement in answer accuracy over baseline RAG systems on ScienceQA and CrisisMMD benchmarks
- 3.56% boost in question classification accuracy compared to single-agent approaches
- State-of-the-art results in zero-shot settings on both ScienceQA and CrisisMMD datasets
- Ablation studies show Decision Agent removal causes 10.82% accuracy drop, confirming its critical role

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Query Decomposition via Schema-Guided Rewriting
- **Claim:** Complex, multi-intent queries overwhelm single-agent retrieval systems; decomposition improves precision.
- **Mechanism:** LLM classifies query as single/multi-intent, then rewrites multi-intent queries into 2-3 logically connected sub-questions while retaining keywords.
- **Core assumption:** LLM has sufficient semantic understanding to disentangle compound queries without losing original intent.
- **Evidence anchors:** Abstract states Decomposition Agent dissects queries via semantic-aware rewriting; Section 3.2 describes structured prompt for query decomposition.
- **Break condition:** If decomposition strips away critical nuance or creates independently unanswerable sub-questions, final synthesis fails.

### Mechanism 2: Complementary Evidence Retrieval (Vector vs. Graph vs. Web)
- **Claim:** Single modality retrieval creates blind spots; parallel vector, graph, and web retrieval mitigates modality isolation.
- **Mechanism:** Parallel dispatch to Vector Agent (semantic matching via Cosine similarity), Graph Agent (LightRAG for structural reasoning), and Web Agent (Google Serper API for real-time knowledge).
- **Core assumption:** Parallel retrieval latency is acceptable and Decision Agent can resolve conflicts between heterogeneous sources.
- **Evidence anchors:** Abstract describes parallel, modality-specific retrieval using plug-and-play modules; Section 3.3 details decoupling retrieval into specialized agents.
- **Break condition:** If Knowledge Graph is sparse or Web returns noisy results, evidence degrades rather than enhances final answer.

### Mechanism 3: Consistency Voting and Expert Refinement
- **Claim:** Aggregating answers via consistency metrics and refining discrepancies with Expert Model reduces hallucinations.
- **Mechanism:** Decision Agent calculates pairwise similarity (ROUGE-L/BLEU) between candidate answers; if consistent, synthesizes via lightweight LLM; if conflicting, invokes Expert Model (GPT-4o) for deep reasoning.
- **Core assumption:** High lexical overlap correlates with factual correctness and Expert Model can arbitrate truth when sources disagree.
- **Evidence anchors:** Abstract describes consistency voting and Expert Model Refinement; Section 4.3 explains expert refinement for high-level thinking when retrieval agents fail.
- **Break condition:** If all agents return plausible but incorrect answers, voting reinforces the hallucination.

## Foundational Learning

- **Concept: Multimodal Knowledge Graphs (MMKG)**
  - **Why needed here:** Framework converts images and text into unified graph structure to enable structural reasoning beyond vector similarity.
  - **Quick check question:** Can you explain how an image is converted into an entity or relation in a graph database via BLIP-2 captioning?

- **Concept: Dense Retrieval vs. Graph Traversal**
  - **Why needed here:** HM-RAG contrasts vector-based (semantic/fine-grained) vs. graph-based (relational/multi-hop) retrieval; understanding trade-offs is essential for debugging.
  - **Quick check question:** If user asks "What is the capital of France?", which agent is most efficient? If they ask "How is CEO of Company X related to 2020 scandal?", which agent is critical?

- **Concept: ROUGE-L and BLEU Metrics**
  - **Why needed here:** These metrics serve as voting mechanism for Decision Agent to determine if answers agree before synthesis.
  - **Quick check question:** Why might ROUGE-L be a poor metric for determining semantic equivalence of answers using different synonyms?

## Architecture Onboarding

- **Component map:** BLIP-2 (Image → Text) → LightRAG (Text → Graph DB + Vector DB) → Decomposition Agent (LLM) → Router → Parallel Vector Agent, Graph Agent, Web Agent → Decision Agent (Calculate Similarity → Vote or Escalate to Expert LLM)

- **Critical path:** Decision Agent is single point of failure; ablation studies show 10.82% accuracy drop when removed, more than any single retrieval agent.

- **Design tradeoffs:**
  - Latency vs. Robustness: Parallel three retrieval methods plus potential Expert Model invocation increases latency significantly vs. simple vector lookup
  - Modularity: Plug-and-play design allows swapping Web Agent for private database but requires maintaining three distinct indexing pipelines

- **Failure signatures:**
  - Consensus on Error: If Vector and Graph agents retrieve similar wrong contexts, voting treats answer as "High Confidence" and skips Expert Refinement
  - Decomposition Drift: Generated sub-queries might drift from original user intent, leading to correct answers to wrong questions

- **First 3 experiments:**
  1. Ablation on Retrieval: Run eval set using only Vector Agent vs. only Graph Agent to establish baselines and identify query types requiring hybrid approach
  2. Decision Stress Test: Feed adversarial examples where Vector and Web results conflict to verify Expert Refinement logic triggers correctly
  3. Decomposition Audit: Log input prompts vs. generated sub-queries for "SOC" category (highest variance in results) to check for query-rewriting errors

## Open Questions the Paper Calls Out
None

## Limitations
- 12.95% accuracy improvement is benchmark-specific and may not generalize to domains outside ScienceQA and CrisisMMD
- Expert Model reliance on GPT-4o creates cost and privacy concerns, with no evaluation of smaller domain-specific models
- System latency is not reported, though parallel execution of three retrieval methods plus Expert Model suggests significant overhead

## Confidence
- **High Confidence:** Hierarchical architecture design and decomposition mechanism are well-supported by literature and align with established RAG limitations
- **Medium Confidence:** 12.95% improvement claim is benchmark-specific; external validation across diverse domains would strengthen generalizability
- **Low Confidence:** Consistency voting mechanism's reliance on ROUGE-L/BLEU as truth arbiters is theoretically weak—high lexical overlap doesn't guarantee factual correctness

## Next Checks
1. Domain Transfer Test: Evaluate HM-RAG on medical or legal QA benchmark to assess performance outside ScienceQA/CrisisMMD scope
2. Expert Model Substitution: Replace GPT-4o with smaller, domain-specific LLM in Expert Refinement role and measure accuracy/quality trade-offs
3. Latency Benchmarking: Measure end-to-end query response times across three retrieval modalities and compare against baseline single-agent RAG systems