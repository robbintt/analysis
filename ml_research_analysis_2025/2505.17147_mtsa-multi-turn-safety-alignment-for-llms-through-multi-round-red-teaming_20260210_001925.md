---
ver: rpa2
title: 'MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming'
arxiv_id: '2505.17147'
source_url: https://arxiv.org/abs/2505.17147
tags:
- attack
- uni00000013
- safety
- red-team
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for improving the security of large
  language models (LLMs) in multi-round interactions. The framework addresses the
  challenge of defending against jailbreak attacks that hide malicious intentions
  across multiple dialogue rounds.
---

# MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming

## Quick Facts
- arXiv ID: 2505.17147
- Source URL: https://arxiv.org/abs/2505.17147
- Reference count: 31
- Primary result: A framework that improves LLM security in multi-round dialogues through adversarial red-teaming and multi-turn reinforcement learning, achieving state-of-the-art attack success rates while maintaining safety performance.

## Executive Summary
This paper proposes MTSA, a framework for improving the security of large language models (LLMs) in multi-round interactions by defending against jailbreak attacks that hide malicious intentions across multiple dialogue rounds. The framework consists of two stages: thought-guided attack learning, where a red-team model learns to generate adversarial prompts, and adversarial iterative optimization, where the red-team and target models continuously improve through interaction. The approach introduces a multi-turn reinforcement learning algorithm based on future rewards to enhance safety alignment. Experimental results show that the red-team model achieves state-of-the-art attack success rates, while the target model significantly improves its safety performance on both single- and multi-round benchmarks without losing generality or causing over-rejection.

## Method Summary
MTSA employs a two-stage approach: first, a red-team model is fine-tuned on a "Think-before-Attack" dataset to generate adversarial prompts with explicit reasoning steps; second, the red-team and target models engage in iterative adversarial optimization using a multi-turn reinforcement learning algorithm based on future rewards. The framework uses trajectory sampling to estimate Q-values from final state rewards, allowing credit assignment across multiple turns. This process alternates between updating the target model to resist attacks and updating the red-team model to generate more effective attacks, creating a self-improving security alignment loop.

## Key Results
- Red-team model achieves state-of-the-art attack success rates on multi-round benchmarks
- Target model significantly improves safety performance on both single- and multi-round benchmarks
- The framework maintains generality and does not cause over-rejection of benign prompts
- Thought-guided strategy improves attack diversity and toxicity compared to direct generation
- Future-reward trajectory sampling effectively handles credit assignment in multi-turn attacks

## Why This Works (Mechanism)

### Mechanism 1: Thought-Guided Strategy Expansion
The framework uses explicit reasoning steps ("thoughts") in the attack generation pipeline to increase diversity and toxicity of adversarial prompts. By fine-tuning the red-team model on a "Think-before-Attack" dataset, the model learns to evaluate the current state space and plan multi-step strategies (e.g., Role Play, Decomposition) rather than simply reacting. This conditions the policy to generate context-aware attacks. The ablation study shows removing the thought process significantly reduces toxicity.

### Mechanism 2: Future-Reward Trajectory Sampling
The framework optimizes policy based on the final state of a dialogue trajectory (future reward) rather than per-turn rewards, aligning early-turn safety better. Instead of training a separate critic model to predict value, the framework samples independent trajectories from the current state to the terminal state and uses the reward of the terminal state as the Q-value for the current turn. This allows backpropagation of the "harm" signal from final successful attacks back to seemingly benign initial turns.

### Mechanism 3: Adversarial Co-Evolution via Iterative Resampling
Alternating optimization of the attacker and defender creates a self-improving loop that exposes vulnerabilities that static datasets miss. The red-team attacks the current target; the resulting failure cases are used to update the target (defense). Simultaneously, the red-team updates its policy to attack the new version of the target. This creates a "moving target" dynamic that continuously improves both models' security capabilities.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) in Dialogue**
  - **Why needed here:** The paper models multi-turn dialogue as a state sequence where history is the state and response is the action. Understanding state transitions is required to grasp why "future rewards" are needed to solve the credit assignment problem (why punish turn 1 for a turn 5 error?).
  - **Quick check question:** Can you explain why a standard single-turn DPO loss fails to penalize the early "setup" questions in a multi-turn attack?

- **Concept: Direct Preference Optimization (DPO) and RLHF**
  - **Why needed here:** The paper modifies standard DPO, which usually optimizes $log \pi(y_w|x) / \pi(y_l|x)$. The paper changes this to optimize implicit rewards based on future states rather than just the current response pair.
  - **Quick check question:** How does the paper modify the standard DPO loss to incorporate "future rewards" instead of immediate human preferences?

- **Concept: Automated Red Teaming (ART)**
  - **Why needed here:** The framework relies on a "Red-team Model" to generate attacks. Understanding the difference between gradient-based attacks (white-box) and generation-based attacks (black-box, used here) is essential to evaluate the results.
  - **Quick check question:** What are the four attack strategies (categories) the paper uses to guide the red-team model's reasoning process?

## Architecture Onboarding

- **Component map:** Red-Team Model -> Trajectory Sampler -> Reward Models -> Target Model -> ArmoRM/Safety Classifier
- **Critical path:** 1. Seed Data: Create "Think-before-Attack" dataset (400 examples) 2. Warm-up: SFT the Red-Team on this data 3. Loop (T=1 to 3): Red-Team attacks Target (max 5 turns) → Sample trajectories to estimate future rewards → Construct preference pairs → Update Target Model using Eq. 4 → Update Red-Team using Eq. 5
- **Design tradeoffs:** Cost vs. Robustness: Trajectory sampling reduces training cost by 34% vs. critic-based methods but increases data generation cost by 17% due to re-sampling. Diversity vs. Toxicity: Top-k data selection (k=100) balances diversity and toxicity; lower k increases toxicity but hurts diversity.
- **Failure signatures:** Over-rejection (Target model refusing benign prompts), Red-Team Collapse (Attack diversity drops), Covariate Shift (Model performs well on benchmarks but fails in real multi-turn chats).
- **First 3 experiments:** 1. Verify the "Thought" Ablation: Train a red-team model with thought tokens stripped and compare ASR on AdvBench. 2. Test Credit Assignment: Run future reward target model against PAIR attack vs. single-turn DPO model. 3. Generality Check: Run aligned target model on AlpacaEval/MT-Bench to ensure safety training didn't degrade capability.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can a dynamic safety evaluation framework be developed to better assess LLM safety compared to static benchmarks? The authors note current evaluations are "static and cannot be well used to evaluate safety in dynamic environments" and building such a framework is future work.
- **Open Question 2:** How can the quality and diversity of jailbreak attacks be improved beyond the limitations of the initial manual seed dataset? The authors note red team dialogue data still rely on manual templates, which cannot exceed the limitations of the coverage and quality of the initial dataset.
- **Open Question 3:** How can the red-team model's planning capabilities be extended to maintain effectiveness in dialogues exceeding 5 rounds? The authors note the model experiences a drop in attack performance when rounds exceed 5 because it is unable to efficiently plan an attack strategy for too many rounds.

## Limitations

- The framework relies on synthetic attack data and assumes the red-team model can generalize from a small seed dataset (400 synthesized examples) to diverse real-world attacks.
- The use of a fixed horizon (5 turns) may not capture longer, more sophisticated jailbreak dialogues.
- Evaluation focuses on benchmark datasets, which may not fully represent the complexity of real user interactions.
- The thought-guided initialization relies on manual templates, capping the potential diversity of generated attacks regardless of iterative optimization.

## Confidence

- **High Confidence:** The ablation study results showing the importance of the thought-guided strategy and the improvement in safety metrics from T1 to T3 are well-supported by experimental data.
- **Medium Confidence:** The claim that future-reward trajectory sampling improves credit assignment is plausible given theoretical motivation, but the paper does not provide direct comparison against a single-turn baseline on multi-turn attacks.
- **Medium Confidence:** The assertion that the iterative adversarial process leads to robust generalization is supported by the lack of over-rejection on XSTest, but the test set size is relatively small.

## Next Checks

1. **Thought Mechanism Ablation:** Re-run the red-team training with the thought-guided strategy removed and measure the impact on Attack Success Rate across all attack types in AdvBench.
2. **Credit Assignment Test:** Compare the safety performance of the target model trained with future rewards against a model trained with standard single-turn DPO when subjected to a multi-turn attack that relies on early-turn setup (e.g., a PAIR-style attack).
3. **Long-Horizon Robustness:** Evaluate the framework's performance on dialogue trajectories longer than 5 turns to assess the impact of horizon length on both attack success and defense efficacy.