---
ver: rpa2
title: Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small
  Models
arxiv_id: '2601.20687'
source_url: https://arxiv.org/abs/2601.20687
tags:
- on-premise
- learning
- distillation
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning small, on-premise
  language models without relying on expensive human feedback or reward model training.
  The proposed method, Positive-Unlabeled Reinforcement Learning Distillation (PU-RLD),
  enables local preference optimization by querying a black-box teacher model once
  per prompt to obtain a high-quality anchor response.
---

# Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models

## Quick Facts
- arXiv ID: 2601.20687
- Source URL: https://arxiv.org/abs/2601.20687
- Reference count: 40
- Primary result: PU-RLD enables on-premise small models to achieve GPT-4o-level alignment performance without human feedback or reward models

## Executive Summary
This paper introduces Positive-Unlabeled Reinforcement Learning Distillation (PU-RLD), a method for aligning small on-premise language models using only a black-box teacher model. The approach queries the teacher once per prompt to obtain a high-quality anchor response, then uses the student model's self-evaluation capability to induce local preference signals from multiple sampled candidates. Theoretical analysis shows the induced supervision is order-consistent and concentrates on near-optimal candidates. Experiments on creative writing, math reasoning, and vision-language tasks demonstrate consistent performance improvements over strong baselines, achieving higher win rates against GPT-4o under both raw and length-controlled metrics.

## Method Summary
The method operates in two stages: (1) supervised fine-tuning (SFT) on teacher-generated responses, followed by (2) LDL-GRPO optimization. For each prompt, the teacher is queried once to obtain an anchor response. The student then samples K candidates and performs anchor-conditioned self-ranking using its own scoring function. The resulting margins between candidate and anchor scores are converted into a soft preference distribution via a sigmoid gating function. Optimization minimizes KL divergence between this induced distribution and the student's normalized policy, with regularization toward the SFT checkpoint. This approach avoids expensive human feedback and external reward models while maintaining stable training dynamics.

## Key Results
- Consistent win-rate improvements over SFT baseline across all tested tasks
- Achieves higher performance than DPO and GRPO baselines in anchor-conditioned settings
- Maintains effectiveness in both unimodal (creative writing, math reasoning) and multimodal (vision-language understanding) tasks
- Demonstrates theoretical order-consistency and concentration on near-optimal candidates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single black-box teacher response can serve as a high-confidence "positive anchor," reducing the need for expensive human labeling or exhaustive teacher scoring.
- Mechanism: The method applies Positive-Unlabeled (PU) learning logic. The teacher response $a$ is treated as a known positive sample, while the student's locally sampled candidates $U(x)$ are treated as "unlabeled" data (containing a mix of good and bad responses). The student compares $U(x)$ against $a$ to infer relative quality.
- Core assumption: The teacher-generated anchor is consistently superior to the average student-generated candidate, providing a reliable calibration baseline for the student's self-evaluation.
- Evidence anchors:
  - [abstract] "...query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking..."
  - [section 3.3] "We therefore query the black-box teacher once to obtain an anchor response a, which serves as a high-confidence positive seed..."
  - [corpus] Related work like *Preference Distillation via Value based Reinforcement Learning* suggests utilizing teachers for preference distillation, but this method specifically isolates the "positive" signal to reduce query complexity.
- Break condition: If the teacher generates a low-quality or hallucinated anchor, the "positive" reference point is corrupted, potentially reinforcing errors during the student's self-ranking.

### Mechanism 2
- Claim: An on-premise student model can induce dense preference signals by evaluating its own candidates against the anchor using its latent capabilities, bypassing the need for an external reward model.
- Mechanism: Anchor-conditioned self-evaluation. The student uses a scoring function $g_\theta(x, a, y_k)$ to assign utility scores to its candidates. These scores are calibrated against the anchor's self-score ($s^*$) to create a margin $r_k$. This margin is converted into a soft positivity confidence via a sigmoid function, resulting in a listwise preference distribution $D_x$.
- Core assumption: The student model (post-SFT) has sufficient latent knowledge to perform a valid relative comparison ("Is candidate Y better than anchor A?") even if it cannot generate the perfect answer itself.
- Evidence anchors:
  - [abstract] "...uses anchor-conditioned self-ranking to induce local preference signals."
  - [section 3.3] "The anchor a serves as an in-context reference... enabling the student to self-evaluate its sampled candidates without external supervision."
  - [corpus] *Capturing Nuanced Preferences* highlights the difficulty of modeling preferences in small models; this mechanism relies on the student's self-evaluation capability to capture those nuances.
- Break condition: If the student's self-evaluation is miscalibrated (e.g., bias toward verbosity), the induced preference distribution $D_x$ will reinforce negative behaviors (reward hacking) rather than alignment.

### Mechanism 3
- Claim: Optimizing a group-relative policy using an induced label distribution (LDL-GRPO) provides more stable training than pairwise preference optimization.
- Mechanism: Label Distribution Learning (LDL). Instead of hard pairwise constraints (A > B), the method minimizes the KL divergence between the student's normalized policy distribution $q_\theta$ and the induced soft label distribution $D_x$. Theoretical analysis suggests $D_x$ is order-consistent and concentrates on near-optimal candidates.
- Core assumption: The theoretical bounds ($r_{max} - E[r_k] \leq \tau \log K$) hold in practice, ensuring the optimization targets high-quality candidates within the sampled group.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that the induced preference supervision is order-consistent and concentrates on near-optimal candidates..."
  - [section 3.4] "Theorem 3.1 (Order consistency)... guarantees that the PU gating $\sigma(\gamma r)$ does not distort rankings."
  - [corpus] *Alignment as Distribution Learning* supports the premise that alignment can be effectively viewed as distribution learning rather than purely point-wise or pairwise optimization.
- Break condition: If the temperature $\tau$ is set too high, the soft label distribution $D_x$ becomes uniform, providing weak supervision; if too low, it becomes over-confident, potentially magnifying noise from the self-evaluation step.

## Foundational Learning

- Concept: **Positive-Unlabeled (PU) Learning**
  - Why needed here: The core data paradigm. You must understand that the student is learning to distinguish "positive" traits (from the teacher) within a pool of "unlabeled" (and potentially noisy) self-generated data.
  - Quick check question: If 50% of the student's "unlabeled" candidates are actually better than the teacher's anchor, how does the assumption of the anchor as a "positive seed" affect the risk estimation?

- Concept: **Label Distribution Learning (LDL)**
  - Why needed here: This is the optimization strategy replacing standard hard labels. Instead of "Response A is better," the target is a probability distribution over all candidates reflecting their relative quality.
  - Quick check question: Why is minimizing the KL divergence between two distributions (student policy vs. induced labels) more robust to noise than maximizing the margin of a single binary pair?

- Concept: **Knowledge Distillation (Black-box)**
  - Why needed here: The system relies on a "teacher" that is only accessible via text generation (no gradients/logits). The distillation happens through the relative ordering induced by the teacher's outputs.
  - Quick check question: How does the "one-query-per-prompt" constraint change the distillation architecture compared to methods requiring full logit access?

## Architecture Onboarding

- Component map:
  - Teacher API (Black-box): Inputs prompt $x$, outputs anchor $a$. (O(N) calls).
  - Student Policy ($p_\theta$): The on-premise model being optimized. Initialized from SFT.
  - Sampler: Generates $K$ candidates $U(x)$ from Student Policy.
  - Self-Evaluator ($g_\theta$): The Student Policy itself acting as a judge to score candidates against the anchor.
  - LDL-GRPO Loss: Computes KL divergence between the induced distribution $D_x$ and the student's normalized likelihood.

- Critical path:
  1. Prompt $x$ enters system.
  2. Query Teacher → Anchor $a$.
  3. Sample Student → Candidates $\{y_1 \dots y_K\}$.
  4. Score locally: $s_k = g_\theta(x, a, y_k)$ and $s^* = g_\theta(x, a, a)$.
  5. Normalize to create target distribution $D_x$.
  6. Update Student $\theta$ via LDL-GRPO loss.

- Design tradeoffs:
  - **Sampling Budget ($K$):** Higher $K$ improves the chances of finding near-optimal candidates (Theorem 3.2) but increases on-premise compute/latency.
  - **Temperature ($\tau$):** Controls the "sharpness" of the preference distribution. Lower $\tau$ forces the student to focus only on the top-ranked candidates, which might be risky if self-evaluation is noisy.
  - **Anchor vs. Judge:** The architecture strictly avoids using the teacher as a judge (scoring K responses) to save cost, relying on the student to do the heavy lifting of evaluation.

- Failure signatures:
  - **Collapsed Preferences:** Loss goes to zero but performance degrades. Check if $D_x$ has become uniform (temperature too high) or if the student is exploiting the self-evaluator (reward hacking).
  - **Anchor Misalignment:** Student learns to mimic the specific length/style of the anchor rather than the quality, especially if the anchor is noisy.
  - **Stalling:** LDL-GRPO loss plateaus early. Check if the student's sampling diversity has collapsed (entropy too low).

- First 3 experiments:
  1. **Sanity Check (SFT vs. LDL-GRPO):** Verify that the LDL-GRPO step provides a win-rate lift over the Stage I SFT checkpoint on a held-out validation set (e.g., WritingPrompts).
  2. **Ablation on Group Size ($K$):** Run experiments with $K=2, 4, 8$ to visualize the trade-off between compute cost and alignment performance (validating Theorem 3.2).
  3. **Robustness Test (Noisy Anchor):** Manually replace a percentage of teacher anchors with low-quality text to see how robust the self-ranking mechanism is to corrupted positive seeds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more robust self-evaluation mechanisms be developed to improve preference signal quality when the student model is relatively weak or when tasks require fine-grained judgment?
- Basis in paper: [explicit] Appendix D states: "the quality of the induced preference signal depends on the student's anchor-conditioned self-evaluation, which may be imperfect when the student is still relatively weak or when the task requires fine-grained judgment... exploring more robust self-evaluation mechanisms remains an interesting direction."
- Why unresolved: The current PU formulation and label-distribution learning help mitigate noise but do not fully address the fundamental limitation that weak students may produce unreliable self-evaluation scores.
- What evidence would resolve it: Experiments showing improved performance with alternative self-evaluation mechanisms (e.g., ensemble scoring, curriculum-based evaluation, or auxiliary verification modules) specifically for weaker student models or fine-grained tasks.

### Open Question 2
- Question: Can the required number of sampled candidates (K) per prompt be reduced without compromising training stability or final alignment quality?
- Basis in paper: [explicit] Appendix D states: "our current implementation requires sampling multiple candidates per prompt to construct group-level supervision, which introduces additional computation compared to single-response updates. Improving the efficiency of candidate generation and reuse, or reducing the required group size without hurting stability, is left for future work."
- Why unresolved: The theoretical bound (Theorem 3.2) suggests concentration depends on K, but the practical trade-off between group size and efficiency remains unexplored.
- What evidence would resolve it: Systematic ablation studies varying K (e.g., K=2,4,8,16) with analysis of both computational cost and alignment quality metrics.

### Open Question 3
- Question: How does the method perform when applied to models smaller than 7B-8B parameters, where self-evaluation capability may be fundamentally limited?
- Basis in paper: [inferred] The experiments only evaluate 7B-8B student backbones (Qwen2.5-7B, LLaMA3-8B, LLaVA-7B, Qwen2.5-VL-7B). The limitation discussion notes self-evaluation may be imperfect for weak students, but the threshold at which this becomes problematic is unknown.
- Why unresolved: Smaller models may lack sufficient capacity for reliable self-evaluation, potentially breaking the core assumption of the anchor-conditioned ranking mechanism.
- What evidence would resolve it: Experiments on smaller backbone sizes (e.g., 1B-3B parameters) with comparison of self-evaluation accuracy against external judges.

## Limitations

- The method's effectiveness depends heavily on the quality of the teacher's single anchor response, with no mechanism to detect or reject poor anchors
- The induced preference signal quality is fundamentally limited by the student model's self-evaluation capability, which may be unreliable for weak students or fine-grained tasks
- The requirement to sample multiple candidates per prompt increases on-premise computation compared to single-response update methods

## Confidence

**High Confidence** (supported by clear theoretical and experimental evidence):
- The two-stage pipeline structure (SFT + LDL-GRPO) is technically sound and well-implemented
- The LDL-GRPO optimization framework is theoretically justified and produces stable training dynamics
- Experimental results showing consistent win-rate improvements over SFT baselines are robust across multiple tasks and model scales

**Medium Confidence** (supported by evidence but with notable limitations):
- The PU learning assumption that teacher anchors are consistently superior to student candidates holds in practice
- The self-evaluation mechanism can reliably induce order-consistent preference distributions
- The method generalizes effectively from unimodal to multimodal tasks without architectural modifications

**Low Confidence** (theoretical claims not fully validated empirically):
- The theoretical concentration bounds (Theorem 3.2) translate to practical sample efficiency gains
- The method maintains robustness when teacher anchors have high variance in quality
- The approach scales effectively to significantly larger model sizes or more complex task domains

## Next Checks

1. **Anchor Quality Sensitivity Analysis**: Systematically vary anchor quality by replacing a percentage of teacher anchors with random or low-quality text, then measure the impact on student performance and self-evaluation calibration. This validates whether the PU assumption breaks down when anchors are noisy.

2. **Self-Evaluation Calibration Test**: Evaluate the student's self-scoring function on a held-out set of prompts where human preference labels exist. Measure correlation between self-evaluation margins and actual preference rankings to quantify the reliability of the induced preference distribution $D_x$.

3. **Long-Horizon Task Performance**: Test the method on tasks requiring extended reasoning or multi-step planning (e.g., complex coding tasks, scientific reasoning) to evaluate whether the preference induction mechanism scales to tasks where quality differences are more nuanced than single-response generation.