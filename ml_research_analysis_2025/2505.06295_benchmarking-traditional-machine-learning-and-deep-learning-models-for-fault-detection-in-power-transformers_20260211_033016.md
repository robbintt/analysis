---
ver: rpa2
title: Benchmarking Traditional Machine Learning and Deep Learning Models for Fault
  Detection in Power Transformers
arxiv_id: '2505.06295'
source_url: https://arxiv.org/abs/2505.06295
tags:
- learning
- fault
- data
- transformer
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study benchmarks traditional machine learning and deep learning\
  \ models for fault detection in power transformers using a 10-month dataset of condition-monitored\
  \ sensor readings. Five conventional ML classifiers\u2014Random Forest, SVM, KNN,\
  \ XGBoost, and ANN\u2014were compared with four deep learning models\u2014LSTM,\
  \ GRU, 1D-CNN, and TabNet\u2014after preprocessing with scaling and SMOTE for class\
  \ balance."
---

# Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers

## Quick Facts
- arXiv ID: 2505.06295
- Source URL: https://arxiv.org/abs/2505.06295
- Reference count: 8
- Primary result: Random Forest achieved highest ML accuracy at 86.82%, while 1D-CNN reached 86.30% in DL models

## Executive Summary
This study benchmarks traditional machine learning and deep learning models for fault detection in power transformers using a 10-month dataset of condition-monitored sensor readings. Five conventional ML classifiers—Random Forest, SVM, KNN, XGBoost, and ANN—were compared with four deep learning models—LSTM, GRU, 1D-CNN, and TabNet—after preprocessing with scaling and SMOTE for class balance. Results show that both ML and DL models performed comparably, with Random Forest achieving the highest ML accuracy of 86.82% and 1D-CNN the top DL accuracy of 86.30%, both closely matching on precision, recall, and F1-score. The study demonstrates that conventional ML models can match deep learning performance on this structured, tabular fault detection task.

## Method Summary
The study collected IoT sensor data from power transformers over 10 months (June 2019–April 2020) at 15-minute intervals, capturing voltage, current, and temperature readings. Binary classification was performed where "Faulty" instances were labeled when any of four alarm sensors (WTI, OTIA, OTIT, MOGA) were active. Data preprocessing included dropping non-numeric columns, feature scaling with StandardScaler, and SMOTE oversampling for class balance. Models were trained on 80% of the data and tested on 20%, with performance evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- Random Forest achieved highest ML accuracy at 86.82% with precision 80.42% and recall 97.80%
- 1D-CNN attained top DL accuracy of 86.30%, closely matching RF performance
- Both ML and DL models showed similar performance patterns across all evaluation metrics
- SMOTE oversampling effectively addressed class imbalance while maintaining realistic performance estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ensemble tree methods can match or exceed deep learning performance on structured, tabular sensor data for fault detection.
- **Mechanism:** Random Forest aggregates predictions from multiple decision trees trained on bootstrapped samples, reducing variance through majority voting. Each tree learns axis-aligned decision boundaries on subsets of features.
- **Core assumption:** Fault signatures in transformer telemetry can be captured through combinations of individual sensor thresholds without requiring explicit temporal modeling.
- **Evidence anchors:**
  - [abstract] "The RF model achieved the highest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%."
  - [section 7.1] "Random Forest achieved the highest performance... Its ability to handle complex feature interactions and its robustness to overfitting contributed to its superior performance."
  - [corpus] Limited direct corpus support for this specific ML-DL comparison on tabular data; neighbor papers focus on different domains (nanosatellites, HPC systems).
- **Break condition:** When faults manifest primarily through long-range temporal dependencies that require modeling sequences across many timesteps, tree-based methods treating each instance independently may underperform.

### Mechanism 2
- **Claim:** 1D-CNNs effectively extract local temporal patterns from sensor sequences for fault classification.
- **Mechanism:** Convolutional filters slide across input sequences to detect local patterns—sudden spikes, periodic fluctuations, or trend changes—automatically learning discriminative features without manual engineering.
- **Core assumption:** Fault signatures appear as localizable patterns in the sensor time series rather than requiring full sequence context.
- **Evidence anchors:**
  - [section 6.3] "By applying convolutional filters across input sequences, 1D-CNNs automatically learn discriminative features related to sudden changes or periodic fluctuations."
  - [section 7.2] "1D-CNN emerged as the best-performing model... Its convolutional layers effectively captured spatial patterns in the data."
  - [corpus] Weak corpus validation; neighbor papers on fault detection use different approaches (LLMs for 5G networks, statistical signatures for approximate DNNs).
- **Break condition:** When fault detection requires understanding relationships across widely separated timesteps or when the receptive field of convolutional filters is insufficient to capture the fault pattern extent.

### Mechanism 3
- **Claim:** SMOTE-based oversampling enables effective training on imbalanced fault datasets by creating synthetic minority examples.
- **Mechanism:** SMOTE generates synthetic samples by interpolating between existing minority class instances and their k-nearest neighbors, expanding the decision boundary region for the minority class.
- **Core assumption:** The linear interpolation between real fault examples produces plausible fault instances that help the model generalize rather than memorize noise.
- **Evidence anchors:**
  - [section 5] "To address class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, generating a balanced dataset for training and testing."
  - [section 8.1] "Although SMOTE was used to address class imbalance, synthetic oversampling may introduce noise into the dataset."
  - [corpus] No direct corpus validation for SMOTE effectiveness in transformer fault detection specifically.
- **Break condition:** When minority class samples are too few or too scattered, interpolation may create unrealistic synthetic samples that degrade model generalization.

## Foundational Learning

- **Concept: Feature scaling (StandardScaler)**
  - Why needed here: Sensor readings span different magnitudes—voltage (kV range), current (amps), temperature (°C). Without normalization, distance-based models (KNN, SVM) and neural networks would be dominated by high-magnitude features.
  - Quick check question: If you removed scaling, which model types would likely show the largest performance drop?

- **Concept: Binary classification metrics for imbalanced data**
  - Why needed here: Accuracy alone is misleading when faults are rare. The study reports precision (80.42% for RF) and recall (97.80% for RF), revealing that models prioritize catching faults (high recall) over avoiding false alarms.
  - Quick check question: Why might a model with 95% accuracy still be dangerous for transformer fault detection?

- **Concept: Train-test split with class balance preservation**
  - Why needed here: The 80/20 split ensures evaluation on held-out data. SMOTE is applied to training data; testing on original distribution ensures realistic performance estimates.
  - Quick check question: What would happen to your evaluation if you applied SMOTE before the train-test split?

## Architecture Onboarding

- **Component map:** Raw sensor data (17 features) → Drop non-numeric columns → StandardScaler → SMOTE (training set only) → Model training → Evaluation on original test distribution

- **Critical path:** The fault label construction (Section 3.2) is foundational—if WTI, OTIA, OTIT, or MOGA sensors signal faults, the instance is labeled "Faulty." Dropping these columns after label creation prevents target leakage.

- **Design tradeoffs:**
  - **Random Forest vs. 1D-CNN:** RF offers interpretability (feature importance) and faster training; 1D-CNN may better capture temporal patterns but requires sequence reshaping and longer training.
  - **Recall vs. Precision:** Current models optimize for high recall (97.80% for RF), accepting more false positives to avoid missing faults. Adjust decision threshold if false alarms are costly.
  - **SMOTE vs. cost-sensitive learning:** SMOTE balances classes synthetically; the paper notes ADASYN or cost-sensitive approaches as alternatives.

- **Failure signatures:**
  - Low recall (<80%): Model misses actual faults—check if training data has sufficient fault examples after SMOTE.
  - Large train-test performance gap: Possible overfitting or SMOTE applied incorrectly before split.
  - TabNet underperforming (83.93% accuracy): May require more epochs or different sparsity settings for this dataset size.

- **First 3 experiments:**
  1. **Baseline replication:** Train Random Forest and 1D-CNN on the provided dataset with documented hyperparameters; verify you achieve ~86% accuracy before exploring modifications.
  2. **Ablation on SMOTE:** Compare model performance with SMOTE vs. without SMOTE vs. class-weighted loss to quantify the benefit of synthetic oversampling.
  3. **Threshold tuning for operational constraints:** Vary the decision threshold on RF probability outputs to trace precision-recall tradeoff; select threshold based on acceptable false alarm rate for your deployment context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating domain-specific feature engineering improve model performance and interpretability compared to the automated scaling and SMOTE approach used here?
- Basis in paper: [explicit] Section 8.1 states the study "did not incorporate domain-specific knowledge into feature engineering," relying instead on automated techniques.
- Why unresolved: It is unknown if physics-based features (e.g., thermal degradation rates) would boost the 86.82% accuracy cap or provide better explainability than the current generic inputs.
- What evidence would resolve it: Benchmarking the current models against versions trained on datasets enriched with expert-crafted features derived from transformer fault mechanisms.

### Open Question 2
- Question: Can alternative techniques for handling class imbalance, such as cost-sensitive learning or ADASYN, yield higher precision and recall than the SMOTE method employed?
- Basis in paper: [explicit] Section 8.1 notes that "synthetic oversampling may introduce noise" and explicitly suggests exploring "adaptive synthetic sampling (ADASYN) or cost-sensitive learning."
- Why unresolved: The current performance metrics may be artificially limited or noisy due to the specific limitations of SMOTE, which synthesizes examples in feature space where real faults may not exist.
- What evidence would resolve it: A comparative study showing F1-scores and ROC curves for the same architectures trained with cost-sensitive loss functions instead of SMOTE.

### Open Question 3
- Question: Would spatiotemporal modeling or Graph Neural Networks (GNNs) provide superior fault detection accuracy by utilizing geographic transformer locations and raw time-series data?
- Basis in paper: [explicit] Section 8.2 proposes "Temporal and Spatial Analysis" as future work, suggesting techniques like "spatiotemporal modeling or graph neural networks (GNNs)."
- Why unresolved: The current study focused on tabular snapshots; it is unclear if capturing the spatial relationships between transformers or long-term temporal dependencies would significantly outperform the Random Forest baseline.
- What evidence would resolve it: Accuracy results from GNN or LSTM models trained on the continuous time-series and location data of the transformer network.

## Limitations
- Small dataset (10 months) may not capture full spectrum of fault types and operating conditions
- Absence of hyperparameter optimization potentially underrepresents both ML and DL model capabilities
- SMOTE oversampling may introduce synthetic noise affecting model generalization for minority fault class

## Confidence
- **High Confidence:** Random Forest achieving 86.82% accuracy is reproducible given dataset and methodology
- **Medium Confidence:** 1D-CNN effectively capturing local temporal patterns requires validation across different datasets
- **Low Confidence:** Claim that ML and DL models performed "similarly" across all metrics needs operational context analysis

## Next Checks
1. **Hyperparameter Optimization Study:** Conduct systematic grid searches for all models (RF depth/n_estimators, CNN filter sizes, LSTM units) to determine if reported accuracies represent true performance ceilings or optimization gaps.
2. **Temporal Validation Protocol:** Implement walk-forward validation instead of random train-test split to ensure models generalize across seasonal operating patterns and prevent temporal leakage in fault detection.
3. **Interpretability Comparison:** Generate feature importance rankings from RF and activation visualizations from 1D-CNN to identify whether models detect the same fault signatures or rely on different sensor combinations.