---
ver: rpa2
title: Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected
  Kronecker-Factored Parameterization
arxiv_id: '2505.05017'
source_url: https://arxiv.org/abs/2505.05017
tags:
- influence
- training
- pre-training
- multi-stage
- ek-fac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of attributing predictions of
  fine-tuned large language models (LLMs) to their pre-training data, enabling training
  data attribution for multi-stage training paradigms like pre-training followed by
  full-parameter fine-tuning. The core method idea is to extend influence functions
  to the multi-stage setting by reformulating the fine-tuning objective with an additional
  proximity constraint that connects the pre-trained and fine-tuned model parameters
  in the parameter space.
---

# Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization

## Quick Facts
- arXiv ID: 2505.05017
- Source URL: https://arxiv.org/abs/2505.05017
- Reference count: 37
- Primary result: Multi-stage influence function with EK-FAC achieves 257× speedup over LiSSA and Spearman ρ=0.612 for training data attribution in fine-tuned LLMs

## Executive Summary
This paper introduces a scalable method for attributing predictions of fine-tuned large language models to their pre-training data. The approach extends influence functions to multi-stage training by reformulating the fine-tuning objective with a proximity constraint connecting pre-trained and fine-tuned parameters. To handle billion-parameter models, the authors employ eigenvalue-corrected Kronecker-factored parameterization to efficiently approximate inverse Hessian-vector products, and use semantic-similarity-based heuristics to select candidate training examples. The method demonstrates superior scalability compared to existing approaches while maintaining reasonable accuracy in retrieving relevant knowledge sources from pre-training data.

## Method Summary
The paper proposes a multi-stage influence function that computes the influence of pre-training examples on downstream predictions in models trained through pre-training followed by full-parameter fine-tuning. The method reformulates the fine-tuning objective with an L2 proximity constraint to enable closed-form influence computation across training stages. For scalability, it employs eigenvalue-corrected Kronecker-factored (EK-FAC) approximation to efficiently compute inverse Hessian-vector products for billion-parameter models. The approach uses semantic similarity to select a small subset of candidate training examples for influence computation, focusing on MLP and MHA linear weights while excluding embeddings, unembeddings, and layer normalization parameters.

## Key Results
- EK-FAC achieves Spearman ρ=0.612 vs. Conjugate Gradient ground truth with 257× speedup over LiSSA for pair-wise computation
- Multi-stage influence function outperforms single-stage and gradient-similarity baselines in fact-tracing benchmark
- MLP parameters contribute 75% of analyzed parameters but ~85.5% of total influence signal, enabling practical compute reduction
- Case studies on dolly-v2-3b demonstrate ability to identify relevant pre-training documents for factual knowledge attribution and bias attribution tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuned LLM predictions can be attributed to pre-training data via a reformulated influence function that connects parameter spaces across training stages.
- **Mechanism:** The method adds an L2 proximity constraint ||θ - θ_pt||² to the fine-tuning objective, reformulating full-parameter fine-tuning as regularized optimization near the pre-trained weights. This enables a closed-form multi-stage influence expression that chains two inverse Hessian operations.
- **Core assumption:** Fine-tuned parameters remain geometrically close to pre-trained parameters in L2 distance (validated: ≤8% relative distance).
- **Evidence anchors:** [abstract] reformulating with proximity constraint; [Section 4.1] derives multi-stage IF from perturbation analysis; [corpus] extends Chen et al. 2020 work.
- **Break condition:** Fine-tuning causing large parameter shifts (>10-20% relative L2 distance) weakens the proximity assumption.

### Mechanism 2
- **Claim:** Eigenvalue-corrected Kronecker-factored (EK-FAC) approximation enables tractable inverse Hessian-vector products for billion-parameter models.
- **Mechanism:** EK-FAC approximates the Generalized Gauss-Newton matrix as block-diagonal, factors each block as Kronecker product, and corrects eigenvalues with empirical variances for better curvature approximation.
- **Core assumption:** GGN block-diagonality is acceptable; input/output gradient independence for Kronecker factorization; eigenvalue correction captures sufficient residual curvature.
- **Evidence anchors:** [Section 3.3] full EK-FAC derivation; [Table 1] Spearman ρ=0.612 with 257× speedup; [corpus] Better Hessians Matter (arXiv 2509.23437) supports curvature approximation importance.
- **Break condition:** If curvature information is critical for specific query types or highly correlated layers violate independence assumptions.

### Mechanism 3
- **Claim:** MLP parameters contribute disproportionately more to influence estimation than MHA parameters.
- **Mechanism:** Empirical finding showing MLP parameters account for 75% of analyzed parameters but ~85.5% of total influence signal.
- **Core assumption:** Factual associations are primarily encoded in MLP weights rather than attention patterns.
- **Evidence anchors:** [Section 5.1] ablation shows modest degradation (Spearman 0.612 → 0.523) when removing MHA; [Section 6] MLP dominance suggests practical trade-off.
- **Break condition:** Tasks heavily reliant on relational reasoning or long-range dependencies may show different MLP vs. MHA influence ratios.

## Foundational Learning

- **Concept: Influence Functions (Koh & Liang 2017)**
  - **Why needed here:** The entire method builds on classical IF theory; understanding I(z, z_q) = ∇m^T H^{-1} ∇ℓ is essential.
  - **Quick check question:** If you upweight a training example by ε, how does the loss on a test point change? (Answer: approximately ε × I(z, z_q))

- **Concept: Generalized Gauss-Newton (GGN) Matrix**
  - **Why needed here:** EK-FAC approximates the GGN, not the full Hessian; understanding why GGN is positive semi-definite and tractable is critical.
  - **Quick check question:** Why is GGN preferable to Hessian for influence computation in neural networks with cross-entropy loss?

- **Concept: Kronecker Product Properties**
  - **Why needed here:** EK-FAC relies on (A ⊗ B)^{-1} = A^{-1} ⊗ B^{-1} to avoid O(p³) inversion.
  - **Quick check question:** If A is d×d and B is p×p, what's the complexity of inverting A ⊗ B directly vs. inverting A and B separately?

## Architecture Onboarding

- **Component map:** EK-FAC Factor Precomputation -> Candidate Selection Module -> Multi-Stage IF Computation -> Influence Scoring
- **Critical path:** 1) Fit EK-FAC factors for pre-trained and fine-tuned models 2) Build embedding index over pre-training corpus 3) For each query: retrieve candidates → compute gradients → apply EK-FAC preconditioning → score influence
- **Design tradeoffs:**
  - Damping term: Smaller values (10⁻⁸ to 10⁻⁶) yield better fact-tracing accuracy but risk numerical instability
  - MLP-only vs. full model: ~30% compute reduction with ~15% influence accuracy drop
  - Candidate set size: Smaller sets are faster but may miss true influential examples
- **Failure signatures:**
  - Spearman correlation near zero: Check damping term, verify EK-FAC factor convergence, ensure correct model gradients
  - Retrieved documents irrelevant: Candidate selection issue; verify embedding model quality
  - OOM during EK-FAC factor fitting: Reduce batch size or use gradient checkpointing
  - Influence scores all similar: Hessian approximation too coarse; try increasing EK-FAC fitting iterations
- **First 3 experiments:**
  1. **Sanity check on small model:** Train 1M-parameter GPT-NeoX on Penn Treebank, compute single-stage IF with EK-FAC, verify Spearman correlation against CG baseline matches paper (ρ ≈ 0.6)
  2. **Damping sweep:** On BLOOM-560m → BLOOMZ-560m fact-tracing benchmark, sweep λ ∈ {10⁻⁸, 10⁻⁶, 10⁻⁴} and plot MRR/Recall@10 to reproduce Figure 3 curves
  3. **MLP-only ablation:** Repeat experiment 2 with only MLP parameters; verify influence quality degrades modestly (Spearman 0.61 → 0.52) while compute time drops proportionally

## Open Questions the Paper Calls Out
None

## Limitations
- Proximity assumption lacks systematic validation across diverse fine-tuning scenarios and extreme fine-tuning schedules
- Block-diagonal EK-FAC approximation ignores layer-wise correlations that may be important for certain tasks
- Empirical finding that MLP parameters dominate influence estimation is task-dependent and may not generalize to attention-heavy reasoning tasks

## Confidence
- **High confidence:** EK-FAC approximation achieves stated speedups and moderate correlation with ground truth (ρ≈0.6); MLP-only mode provides reasonable trade-off
- **Medium confidence:** Multi-stage influence function improves fact-tracing performance over single-stage baselines; proximity assumption holds for tested models
- **Low confidence:** Generalization of MLP dominance to non-factual tasks; robustness to extreme fine-tuning scenarios

## Next Checks
1. Test proximity assumption violation: Fine-tune BLOOM-560m with aggressive learning rates or long schedules, measure parameter shift, and evaluate multi-stage IF quality degradation
2. Evaluate on attention-heavy task: Apply method to a relational reasoning dataset (e.g., HellaSwag) to test whether MLP dominance holds for attention-dependent tasks
3. Compare to full Hessian: For a small model (≤10M parameters), compute exact multi-stage IF using CG and compare to EK-FAC approximation quality and influence ranking correlation