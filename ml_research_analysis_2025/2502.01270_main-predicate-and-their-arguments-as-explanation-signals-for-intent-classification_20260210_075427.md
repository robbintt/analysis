---
ver: rpa2
title: Main Predicate and Their Arguments as Explanation Signals For Intent Classification
arxiv_id: '2502.01270'
source_url: https://arxiv.org/abs/2502.01270
tags:
- intent
- explanation
- classification
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using main predicates and their arguments as
  explanation signals for intent classification in chatbots. The authors automatically
  annotate text samples from ATIS and SNIPS datasets with word-level explanations
  using dependency relations.
---

# Main Predicate and Their Arguments as Explanation Signals For Intent Classification

## Quick Facts
- **arXiv ID:** 2502.01270
- **Source URL:** https://arxiv.org/abs/2502.01270
- **Reference count:** 11
- **Primary result:** Silver standard explanation signals (main predicates + arguments) improve plausibility Token F1 by 3-4% in intent classification

## Executive Summary
This paper proposes using main predicates and their arguments as explanation signals for intent classification in chatbots. The authors automatically annotate text samples from ATIS and SNIPS datasets with word-level explanations using dependency relations. They evaluate the reasoning of deep learning models on this new benchmark dataset and find that guiding models to focus on explanation signals during training improves plausibility Token F1 score by 3-4%. The proposed method provides a novel way to create silver standard explanations for intent classification, which is highly relevant for dialogue agents and chatbots.

## Method Summary
The method creates silver standard explanation signals by extracting main predicates and their arguments from dependency parse trees. The algorithm identifies the main predicate (typically the main verb) and collects children with specific relations: direct object (dobj), nominal subject (nsubj), open clausal complement (xcomp), and oblique nominal (obl). For obl with multiple candidates, it filters for common nouns. The approach recursively includes compound children (excluding proper nouns) and removes stopwords. Models are trained with a joint loss combining cross-entropy classification loss and an attribution prior loss that aligns model attributions with silver rationale tokens using Integrated Gradients.

## Key Results
- Guiding models to focus on explanation signals during training improves plausibility Token F1 score by 3-4%
- Joint training improves plausibility metrics while maintaining classification accuracy
- LIME explanations show better faithfulness than gradient-based methods in post-hoc evaluation

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Based Silver Annotation
The algorithm traverses dependency parse trees to identify the main predicate (often the main verb), then collects children with specific relations: direct object (dobj), nominal subject (nsubj), open clausal complement (xcomp), and oblique nominal (obl). For obl with multiple candidates, it filters for common nouns. It recursively includes compound children (excluding proper nouns) and removes stopwords. This captures the action (verb) and domain objects/arguments, based on the linguistic hypothesis that main predicate and arguments jointly form a sufficient explanation of the intent label.

### Mechanism 2: Attribution Prior Regularization
Joint training uses attribution prior loss to align model gradients with silver rationale tokens. Compute attribution scores using Integrated Gradients (average across all output classes per token). Define prior loss L_prior = Σ_i (a_i - t_i)^2 where t_i ∈ {0,1} are silver labels. Combine with cross-entropy: L_joint = L_CE + λ·L_prior. This penalizes the model when its gradient-based attribution deviates from silver spans, encouraging focus on rationale tokens during training.

### Mechanism 3: Post-hoc Explanation Alignment
Models are evaluated using post-hoc saliency maps generated by LIME and Integrated Gradients. Plausibility is measured via Token F1 and IOU F1 comparing top-k attributions to silver spans. Faithfulness is measured via Comprehensiveness (probability drop when rationale removed) and Sufficiency (probability using only rationale). Joint training improves plausibility, with LIME showing better faithfulness than gradient-based methods.

## Foundational Learning

- **Concept: Dependency Parsing and Predicate–Argument Structure**
  - Why needed here: The silver annotation algorithm relies on identifying the main predicate and its syntactic arguments (dobj, nsubj, xcomp, obl) via dependency trees.
  - Quick check question: Given a sentence, can you identify the root predicate and list its children with dobj, nsubj, xcomp, obl relations?

- **Concept: Integrated Gradients and Attribution Priors**
  - Why needed here: Joint training uses gradient-based attribution as a prior to guide model focus; understanding how attributions are computed and averaged is essential.
  - Quick check question: Explain how Integrated Gradients assigns attribution to each token and why averaging over all output classes is used in the prior loss.

- **Concept: Plausibility vs Faithfulness in Explainability**
  - Why needed here: Evaluation uses Token F1/IOU F1 (plausibility) and Comprehensiveness/Sufficiency (faithfulness); understanding their definitions and trade-offs is critical.
  - Quick check question: Define Comprehensiveness and Sufficiency and describe a scenario where improving plausibility might not improve faithfulness.

## Architecture Onboarding

- **Component map:** Data pipeline: ATIS/SNIPS text → dependency parsing (Stanford CoreNLP with OpenIE) → silver annotation algorithm → binary label vectors t_i per token → Model: CNN/LSTM/BERT/RoBERTa/GPT-2 with joint loss (L_CE + λ·L_prior) → Evaluation: Post-hoc explainers (LIME, Integrated Gradients) → metrics (Token F1, IOU F1, Comprehensiveness, Sufficiency)

- **Critical path:** 1. Parse utterances with CoreNLP, extract dependency trees. 2. Run silver annotation algorithm to label explanation tokens. 3. Fine-tune models with joint loss (λ tuned per model/dataset). 4. Compute post-hoc attributions and evaluate plausibility/faithfulness.

- **Design tradeoffs:** Silver vs human annotation: Silver annotations are cheap and scalable but may miss edge cases; human eval shows ~96–98% accuracy but confusion exists among fine-grained intents. Gradient vs perturbation explanations: Integrated Gradients is differentiable (suitable for joint training); LIME has better faithfulness but is not used in loss. Model size vs explainability: Larger models (BERT, RoBERTa) achieve higher classification accuracy and plausibility but show smaller gains from joint training; smaller models (CNN, LSTM) benefit more from attribution priors.

- **Failure signatures:** Low plausibility despite high accuracy: Model focuses on frequent words (e.g., "airport" instead of "aircraft"); indicates spurious correlations from class imbalance. Conflicting fine-grained intents: Confusion between atis-airline vs atis-flight, or SearchCreativeWork vs PlayMusic; silver rationale may not fully disambiguate. Poor faithfulness after joint training: Comprehensiveness/Sufficiency may degrade if λ is too high, over-constraining the model.

- **First 3 experiments:** 1. Validate silver annotation quality: Run human evaluation on a balanced subset (200 samples per dataset) and compute Fleiss' Kappa and accuracy vs gold intents. 2. Joint vs baseline training: Fine-tune BERT and LSTM with and without attribution prior (λ=0 vs tuned λ), compare Token F1 and classification accuracy on ATIS/SNIPS. 3. Compare explainers: Compute LIME and Integrated Gradients attributions for all models; evaluate plausibility (Token F1, IOU F1) and faithfulness (Comprehensiveness, Sufficiency) to confirm paper's finding that LIME has better faithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the main predicate and argument extraction method generalize to morphologically rich or low-resource languages to create multilingual explanation benchmarks?
- **Basis in paper:** [explicit] The authors state in the future work section that it will be "interesting to apply the concept of main predicate and arguments to other languages."
- **Why unresolved:** The current technique relies on dependency relations and POS tags which may not transfer effectively to languages with structural differences, as noted in the limitations.
- **Evidence to resolve it:** Performance metrics (Token F1, IOU F1) on a dataset of intent classification queries translated into typologically diverse languages (e.g., German, Chinese).

### Open Question 2
- **Question:** Does training on explanation signals significantly improve intent classification accuracy in zero-shot or few-shot learning scenarios compared to standard fine-tuning?
- **Basis in paper:** [explicit] The conclusion suggests future work could attempt to "improve the model's performance under scarce data scenarios by training it on explanation signals."
- **Why unresolved:** While the paper observes improvements for classes with fewer samples, it does not isolate or specifically benchmark performance in strict low-data or zero-shot settings.
- **Evidence to resolve it:** A comparative study of model accuracy on classes with extremely limited training data (e.g., <5 samples) with and without the attribution prior loss.

### Open Question 3
- **Question:** Does incorporating ground truth intent labels during the silver annotation phase yield higher-quality explanation signals than the current dependency-parsing-only approach?
- **Basis in paper:** [inferred] The limitations section notes the method does not leverage ground truth intent to generate explanations, potentially missing cases where original slots provide hints.
- **Why unresolved:** The current algorithm relies solely on syntactic structure (dependency trees) without semantic guidance from the intent label, potentially lowering annotation quality.
- **Evidence to resolve it:** Human evaluation scores (Inter-Annotator Agreement) comparing current silver annotations against annotations generated using an intent-aware algorithm.

## Limitations
- The silver annotation algorithm relies on dependency parsing without incorporating ground truth intent labels, potentially missing cases where original slots provide hints
- The linguistic hypothesis that main predicates and arguments are sufficient explanations may not hold for all intent types, especially fine-grained distinctions
- The method has not been validated on morphologically rich or low-resource languages, limiting generalizability

## Confidence

- **High Confidence:** The technical implementation of silver annotation via dependency parsing is sound and reproducible. The joint training framework with attribution prior loss is well-specified with clear equations and hyperparameters.
- **Medium Confidence:** The core claim that guiding models to focus on explanation signals improves plausibility by 3-4% is supported by experimental results, but the external validity of the predicate-argument hypothesis remains untested beyond the two datasets.
- **Low Confidence:** The assumption that post-hoc attribution methods (LIME, Integrated Gradients) provide faithful explanations for complex transformer models, and that improvements in plausibility transfer to other explanation types, requires further validation.

## Next Checks

1. **External Dataset Validation:** Apply the silver annotation algorithm and joint training framework to a third intent classification dataset (e.g., CLINC150 or banking77) to test generalizability of the predicate-argument hypothesis and plausibility gains.

2. **Human Evaluation of Rationale Quality:** Conduct human evaluation on a balanced subset of ATIS/SNIPS test examples to measure accuracy and inter-annotator agreement between silver rationales and human-identified explanation signals.

3. **Explanation Type Robustness:** Evaluate whether plausibility improvements transfer to alternative explanation methods (e.g., counterfactual explanations, concept-based explanations) beyond token-level saliency, and test faithfulness across different model architectures and sizes.