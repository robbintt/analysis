---
ver: rpa2
title: A margin-based replacement for cross-entropy loss
arxiv_id: '2501.12191'
source_url: https://arxiv.org/abs/2501.12191
tags:
- loss
- learning
- performance
- training
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes high error margin (HEM) loss as a general-purpose
  replacement for cross-entropy (CE) loss in deep neural network classification. The
  key idea is to modify multi-class margin loss by focusing on reducing only the largest
  errors during training, rather than averaging all errors.
---

# A margin-based replacement for cross-entropy loss

## Quick Facts
- arXiv ID: 2501.12191
- Source URL: https://arxiv.org/abs/2501.12191
- Reference count: 10
- This paper proposes high error margin (HEM) loss as a general-purpose replacement for cross-entropy (CE) loss in deep neural network classification, achieving superior performance on unknown class rejection, adversarial robustness, imbalanced data, continual learning, and semantic segmentation while maintaining comparable clean accuracy.

## Executive Summary
This paper introduces High Error Margin (HEM) loss as a general-purpose replacement for cross-entropy loss in deep neural networks. HEM modifies multi-class margin loss by focusing on reducing only the largest errors during training, rather than averaging all errors. This addresses key issues with CE loss including overconfidence, catastrophic forgetting, and poor performance on imbalanced data. The authors demonstrate HEM's effectiveness across 18+ architectures and 8 datasets, showing it outperforms CE on unknown class rejection, adversarial robustness, and continual learning while maintaining comparable clean accuracy.

## Method Summary
HEM loss computes per-sample errors by calculating the margin between correct and incorrect class logits, setting errors below the batch mean to zero, and averaging only the remaining non-zero errors. This focuses learning on the most challenging samples while preventing weight updates on well-classified examples. For imbalanced data, HEM+ uses class-specific margins inversely proportional to class frequency. The mean computation is detached from the gradient graph to prevent training instability. The method requires lower initial learning rates than CE (typically 0.05 vs 0.1) and uses margin scaling of μ = √(2000 / total_training_samples) for balanced datasets.

## Key Results
- HEM achieves superior unknown class rejection with AUROC improvements of 10-15% over CE across multiple OOD datasets
- On CIFAR10/100, HEM shows 74.95% accuracy vs 70.13% for standard multi-class margin loss, comparable to CE accuracy
- HEM+ significantly improves performance on imbalanced data and semantic segmentation tasks compared to both CE and HEM
- The method provides better adversarial robustness (higher DAR scores) and reduces catastrophic forgetting in continual learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** HEM loss produces better-calibrated confidence estimates, enabling superior unknown class rejection.
**Mechanism:** Unlike cross-entropy, which continues producing non-zero loss and gradients even for correctly classified samples, HEM loss becomes zero once the correct-class logit exceeds all other logits by the margin μ. This prevents the network from learning to produce uniformly high confidence on all inputs.
**Core assumption:** Overconfidence in CE-trained networks is caused by the loss's property of continuing to push logits toward larger magnitudes even after correct classification.
**Evidence anchors:**
- [abstract] "This addresses issues with CE loss such as overconfidence and catastrophic forgetting."
- [section 2.1] "On each presentation of a correctly classified example the non-zero CE loss will cause the outputs to become ever more extreme... Hence, it is to be expected that such a DNN will produce high confidence for all samples, including ones not seen during training."
- [section 5.1.2] Figure 4 shows CE loss produces MSP values clustered near 1.0 for both known and unknown samples, while HEM produces wider distribution.

### Mechanism 2
**Claim:** HEM loss reduces catastrophic forgetting and improves performance on imbalanced data by stopping weight updates once samples are correctly classified.
**Mechanism:** CE loss continues updating weights on majority-class samples even after they're learned, potentially overwriting representations needed for minority classes or previously learned tasks. HEM's zero-loss property for well-classified samples prevents this overwriting.
**Core assumption:** Weight interference from continued training on already-learned samples is a primary driver of both class imbalance problems and catastrophic forgetting.
**Evidence anchors:**
- [section 2.1] "this will cause weights to be over-written. We hypothesised that this might be one reason why learning with imbalanced training data is difficult"
- [section 5.3] "In 11 of the 16 conditions tested, considerably better performance was obtained using HEM loss rather than CE loss" for continual learning.

### Mechanism 3
**Claim:** HEM's error combination method sustains learning pressure on remaining difficult samples where standard margin losses fail.
**Mechanism:** Standard multi-class margin (MM) loss averages all errors, causing premature convergence when many errors are zero. HEM: (1) sets errors below the mean to zero, focusing on largest errors; (2) takes mean of only non-zero values, keeping loss magnitude meaningful even with few remaining errors.
**Core assumption:** The failure of standard MM loss on DNNs is due to loss magnitude collapsing before all samples are learned, not fundamental issues with margin-based optimization.
**Evidence anchors:**
- [section 2.2] "Averaging the errors (across logits and samples in the batch) causes MM loss to become close to zero prior to all samples being correctly classified"
- [section 4.1] "by taking the mean of only the above-zero values, the loss will remain large even when there are few non-zero errors in each sample"
- [appendix B.2] Table 3 shows ablation: MM achieves 70.13% on CIFAR100, HEM achieves 74.95%.

## Foundational Learning

- **Concept: Margin-based classification losses**
  - **Why needed here:** HEM is fundamentally a margin loss variant. Understanding that margin losses define a "safety gap" between correct and incorrect class scores (rather than maximizing probability) is essential to grasp why HEM behaves differently from CE.
  - **Quick check question:** Given logits [2.0, 0.5, 0.3] for a 3-class problem with class 1 correct and margin μ=1.0, what is the multi-class margin loss? (Answer: zero, since 2.0 - 0.5 = 1.5 > 1.0)

- **Concept: Cross-entropy loss saturation behavior**
  - **Why needed here:** CE loss never reaches zero for finite logits—it continues pushing toward infinite confidence. This contrasts with margin losses and explains the overconfidence problem HEM addresses.
  - **Quick check question:** Why doesn't CE loss equal zero even when softmax probability for the correct class is 0.99? (Answer: -log(0.99) ≈ 0.01, and only approaches 0 as probability approaches 1.0, which requires infinite logits)

- **Concept: Out-of-distribution detection via confidence scoring**
  - **Why needed here:** HEM's practical value is largely evaluated through unknown class rejection using MSP or MLS confidence scores. Understanding that lower/more varied confidence on known samples enables better OOD detection is critical.
  - **Quick check question:** If a network outputs MSP=0.99 for both in-distribution and out-of-distribution samples, can it perform OOD detection? (Answer: No—separation requires different confidence distributions)

## Architecture Onboarding

- **Component map:**
  Input → [Backbone: Any standard architecture] → Logits (y ∈ R^n) → [HEM Loss Computation] → Loss value

- **Critical path:**
  1. Margin selection: Use μ = √(2000 / total_training_samples) for balanced data
  2. For imbalanced data (HEM+): μ_i = √(2000 / (n × s_i)) where s_i is samples per class i
  3. Detach the mean computation from gradient graph—critical for training stability
  4. Use lower initial learning rate than CE (paper suggests 0.05 vs 0.1 for CIFAR)

- **Design tradeoffs:**
  - **HEM vs CE:** HEM sacrifices ~1% clean accuracy for large gains in OOD detection, adversarial robustness, and continual learning
  - **HEM vs specialized losses:** HEM is general-purpose; specialized losses may outperform on their target task but fail elsewhere
  - **HEM vs HEM+:** HEM+ requires class frequency information; use HEM+ only when training data is imbalanced

- **Failure signatures:**
  1. Loss doesn't decrease early in training: Likely learning rate too high; HEM shows larger early-training fluctuations than CE
  2. Clean accuracy significantly below CE (>3% gap): Check margin value; too large a margin can hurt accuracy, especially with few training samples
  3. Poor OOD detection despite HEM: Verify using MLS (max logit) rather than MSP if margin is large—large margins inflate MSP values

- **First 3 experiments:**
  1. **Baseline validation on CIFAR10 with ResNet18:** Train identical architectures with CE and HEM using paper's hyperparameters. Verify clean accuracy within 1% and AUROC improvement >5%.
  2. **OOD detection stress test:** Train on CIFAR10, evaluate OOD detection on SVHN, CIFAR100, and Textures datasets using both MSP and MLS confidence scores. HEM should show clear separation between known/unknown confidence distributions.
  3. **Learning rate sensitivity:** Train with initial lr ∈ {0.05, 0.1, 0.2} on a validation split. Paper indicates HEM benefits from lower initial lr (0.05 improved all metrics vs 0.1 on WRN22-10/CIFAR10).

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can adaptive heuristics or learnable parameters for setting margins improve HEM+ performance on imbalanced datasets?
**Basis in paper:** [explicit] The authors state: "Future work might usefully explore alternative heuristics for setting the margins or ways of learning margins for different tasks."
**Why unresolved:** The current HEM+ uses a static inverse-frequency heuristic for margins which performs well, but it is unclear if this is optimal compared to dynamic or learned margin strategies.
**What evidence would resolve it:** Benchmark results comparing HEM+ against versions with learned margins on long-tailed recognition tasks.

### Open Question 2
**Question:** Can training efficiency be improved by exploiting the HEM property of producing zero gradients for well-classified samples?
**Basis in paper:** [explicit] Appendix B.3 suggests: "Future work might explore if this property could be exploited to reduce training time by avoiding repeatedly presenting to the network training samples that have been learnt."
**Why unresolved:** While standard training re-evaluates all samples, skipping those with zero loss might save computation, but the impact on convergence stability and final accuracy is unknown.
**What evidence would resolve it:** Measurements of wall-clock time and final accuracy for a modified training loop that discards or reduces the frequency of zero-loss samples.

### Open Question 3
**Question:** Can specialized data augmentation or regularization terms applied to "saturated" samples improve generalization?
**Basis in paper:** [explicit] Appendix B.3 asks: "future work might explore if such training samples could be augmented to improve generalisation, or if regularisation terms could be added to HEM loss to improve the representations that are learnt."
**Why unresolved:** Samples that generate zero loss cease weight updates; it is unknown if applying stronger augmentation specifically to these samples would further improve the learned features.
**What evidence would resolve it:** Experiments applying targeted augmentations to zero-loss samples and measuring the resulting change in robustness and accuracy metrics.

## Limitations

- The specific margin scaling formula (μ = √(2000 / N)) was tuned on CIFAR and may not generalize optimally to all dataset sizes and class counts
- HEM shows larger early-training fluctuations than CE and benefits from lower initial learning rates, suggesting sensitivity to optimizer configuration
- For large margins, MSP confidence scores can become inflated, requiring careful selection between MSP and MLS depending on margin size

## Confidence

**High confidence:** HEM's superior performance on unknown class rejection (OOD detection) is well-supported by multiple datasets and visualization of confidence distributions.

**Medium confidence:** Claims about HEM's benefits for imbalanced data and continual learning are supported by ablation studies and comparisons, but relative advantage compared to specialized methods is not thoroughly benchmarked.

**Low confidence:** The assertion that HEM is a "general-purpose replacement" for CE is overstated given the 1-2% accuracy trade-off on clean data.

## Next Checks

1. **Cross-dataset margin scaling validation:** Systematically test the margin formula μ = √(2000 / N) across datasets spanning three orders of magnitude in training samples (MNIST, CIFAR, ImageNet) to verify the √N scaling remains optimal.

2. **Early-stopping comparison study:** Compare HEM's forgetting prevention against explicit regularization methods (EWC, L2 regularization) and replay buffers in continual learning benchmarks to isolate the specific advantage of the margin-based approach.

3. **Confidence calibration benchmark:** Beyond AUROC, conduct proper calibration analysis (ECE, NLL) comparing HEM vs CE on both in-distribution and OOD data to verify that HEM's confidence estimates are not just separable but also well-calibrated.