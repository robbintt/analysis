---
ver: rpa2
title: 'FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form
  Question Answering'
arxiv_id: '2510.06426'
source_url: https://arxiv.org/abs/2510.06426
tags:
- financial
- llama-3
- question
- knowledge
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FinLFQA, a benchmark designed to evaluate
  large language models'' ability to generate long-form answers to complex financial
  questions with reliable attributions. Unlike prior work that focuses on simple evidence
  retrieval, FinLFQA evaluates three critical attribution types: supporting evidence
  from financial reports, intermediate numerical reasoning steps, and domain-specific
  financial knowledge.'
---

# FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering

## Quick Facts
- **arXiv ID**: 2510.06426
- **Source URL**: https://arxiv.org/abs/2510.06426
- **Authors**: Yitao Long; Tiansheng Hu; Yilun Zhao; Arman Cohan; Chen Zhao
- **Reference count**: 37
- **Primary result**: Introduces FinLFQA benchmark with 1,008 expert-annotated instances for evaluating LLMs' long-form financial QA with attributions across evidence, code, and knowledge dimensions

## Executive Summary
This paper introduces FinLFQA, a benchmark designed to evaluate large language models' ability to generate long-form answers to complex financial questions with reliable attributions. Unlike prior work that focuses on simple evidence retrieval, FinLFQA evaluates three critical attribution types: supporting evidence from financial reports, intermediate numerical reasoning steps, and domain-specific financial knowledge. The benchmark includes 1,008 expert-annotated instances and provides an automatic evaluation framework covering answer quality and attribution quality. Experiments on eight LLMs across three attribution-generation paradigms show that fine-grained metrics are essential for distinguishing model capabilities, end-to-end generation performs comparably to post-hoc approaches, and iterative refinement only improves when guided by external feedback. GPT-4o achieves the highest overall performance, while several open-source models demonstrate competitive results, particularly in attribution and reasoning tasks.

## Method Summary
The authors created FinLFQA by collecting 1,008 expert-annotated financial question-answer pairs across two domains: quarterly reports and ESG analysis. Each instance requires analyzing two company financial reports to generate answers with attributions. The benchmark evaluates three attribution types: (1) evidence paragraphs from reports, (2) executable Python code for numerical reasoning, and (3) domain knowledge concepts. The evaluation framework uses both LLM-as-judge (GPT-4o) for semantic quality and numerical F1 scores for code execution, with human validation on 50 samples to establish correlation metrics. Three generation paradigms are tested: post-hoc (generate answer then evidence), end-to-end (generate answer with evidence simultaneously), and iterative refinement (generate, receive feedback, regenerate).

## Key Results
- Fine-grained attribution taxonomy (evidence, code, knowledge) effectively distinguishes model capabilities that coarse metrics miss
- End-to-end generation performs comparably to post-hoc approaches with better code-answer consistency
- Iterative refinement only improves when guided by external feedback, not self-feedback alone
- GPT-4o achieves highest overall performance; open-source models show competitive results particularly in attribution tasks
- Models below 7B parameters struggle significantly with attribution quality and numerical reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained attribution taxonomy captures financial reasoning complexity better than single-type citation.
- Mechanism: Decomposing attribution into three channels—(1) paragraph-level evidence retrieval, (2) executable Python code for numerical reasoning, and (3) domain knowledge references—allows independent evaluation of retrieval, calculation, and conceptual grounding.
- Core assumption: Financial QA requires distinct verification paths for factual claims (evidence), computational claims (code), and conceptual claims (knowledge).
- Evidence anchors:
  - [abstract] "FINLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process."
  - [section 4.3] "We design a set of fine-grained metrics to assess the key dimensions... Evidence... Code... Professional Knowledge."
  - [corpus] Related paper "LogicScore" (arXiv 2601.15050) similarly critiques "attribution myopia" in existing AQA evaluation.

### Mechanism 2
- Claim: End-to-end generation achieves comparable performance to post-hoc attribution in financial LFQA.
- Mechanism: Frontier LLMs can simultaneously generate answer clauses and their attributions without quality degradation. End-to-end improves code-answer consistency because numerical reasoning steps are computed alongside the answer rather than retrofitted to a pre-generated response.
- Core assumption: Model capacity is sufficient to maintain reasoning coherence while producing structured output in a single pass.
- Evidence anchors:
  - [abstract] "end-to-end generation performs comparably to post-hoc approaches"
  - [section 5.3] "post-hoc and end-to-end generation perform similarly... In post-hoc generation, numerical reasoning steps are inferred from a pre-generated answer, leading to inconsistencies and lower execution success rates. In contrast, end-to-end generation ensures that answers and reasoning steps are aligned."
  - [corpus] No direct corpus contradiction found.

### Mechanism 3
- Claim: Iterative self-refinement without external feedback does not improve financial LFQA performance; gains require external corrective signals.
- Mechanism: Self-feedback provides no new information—the model cannot generate corrective signals it doesn't already possess. Improvement occurs only when: (1) code execution provides explicit error feedback, or (2) a stronger/domain-specialized model provides structured critique.
- Core assumption: Reasoning tasks require external ground-truth signals for correction; self-supervised refinement is circular.
- Evidence anchors:
  - [abstract] "iterative refinement only improves when guided by external feedback"
  - [section 5.4] "Self-feedback alone is insufficient... External feedback requires sufficient model capacity... Domain-specific feedback improves outcomes—Llama-3.1-8B significantly improves when guided by Fino1-8B... fine-tuned on financial data."
  - [corpus] Related paper "Decomposing and Revising What Language Models Generate" (arXiv 2509.00765) similarly finds generated questions for retrieval are "often irrelevant and incomplete."

## Foundational Learning

- **Concept: Attribution types in high-stakes domains**
  - Why needed here: Financial QA cannot rely on text-only citation; numerical derivation and domain knowledge require separate verification paths.
  - Quick check question: Can you identify which attribution type (evidence, code, knowledge) is missing if a model states "Company X's debt-to-equity ratio improved" without showing the calculation?

- **Concept: Execution-grounded verification**
  - Why needed here: Generated code provides executable proof of numerical reasoning; static text claims cannot be verified without re-running calculations.
  - Quick check question: If a model claims "Revenue grew 15.3%" and provides Python code, what must you verify beyond the code executing successfully?

- **Concept: Feedback capacity threshold**
  - Why needed here: Smaller models cannot effectively utilize external feedback even when available—capacity gates refinement utility.
  - Quick check question: A 3B parameter model receives correct feedback from a 70B model but shows no improvement. What is the likely bottleneck?

## Architecture Onboarding

- **Component map**: Input reports → Query + knowledge sampling → LLM generation (post-hoc/end-to-end/iterative) → Structured output (clauses + attributions) → Three parallel evaluators (evidence matcher, code executor, knowledge scorer) → LLM-as-judge evaluation

- **Critical path**: Report preprocessing (table serialization using `|` column separators) → Prompt construction (include reports, question, knowledge list) → Generation (post-hoc OR end-to-end OR iterative) → Code extraction and execution (for numerical validation) → Attribution evaluation against ground-truth indices

- **Design tradeoffs**:
  - Post-hoc vs End-to-end: Post-hoc allows independent answer/attribution optimization; end-to-end reduces latency and improves code-answer consistency but requires stronger models.
  - Self-feedback vs External feedback: Self-feedback is cheap but ineffective for reasoning; external feedback requires stronger/domain-specific models and increases cost.
  - Coarse vs Fine-grained evaluation: ROUGE/BERTScore fail to distinguish models (all ~87-88 BERTScore); fine-grained metrics expose numerical reasoning gaps.

- **Failure signatures**:
  - Evidence attribution errors (25%): Redundant/invalid citations, missing supporting paragraphs
  - Execution errors (22%): Missing function arguments, absent return statements, indentation issues
  - Numerical extraction errors (20%): Unit mismatches (table headers), rounding errors
  - Knowledge validation errors (15%): Irrelevant financial concepts cited
  - Capacity threshold: Models <3B show near-random attribution performance (Evidence F1 <2%)

- **First 3 experiments**:
  1. **Baseline establishment**: Run GPT-4o and Qwen2.5-72B on 50 dev samples in post-hoc mode; verify evaluation pipeline reproduces paper's LLM-as-judge scores (target: GPT-4o ~13.6-13.7, Qwen ~13.0).
  2. **Paradigm comparison**: For a single open-source model (Llama-3.3-70B), compare post-hoc vs end-to-end vs iterative-refinement on 100 samples; hypothesis: end-to-end ≈ post-hoc, iterative self-feedback shows no gain.
  3. **Feedback source ablation**: Using Llama-3.2-3B as base, test three feedback sources (self, Llama-3.3-70B, domain-specific Fino1-8B if available); hypothesis: stronger/domain-specific feedback improves, self-feedback degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can performance gains from external feedback be sustained when scaling iterative refinement to financial analyses involving more than two companies?
- Basis in paper: [explicit] The authors state in the Limitations section: "Expanding the dataset to include more companies is a promising direction for future work, and we plan to pursue this as models advance and show improved performance in the current setting."
- Why unresolved: The current benchmark is limited to two-company comparisons, and it remains unclear whether the observed benefits of domain-specific external feedback (e.g., from Fino1-8B) generalize to more complex multi-company scenarios with larger context windows and increased reasoning complexity.
- What evidence would resolve it: Extending FinLFQA to include 3-5 companies per question and measuring whether external feedback mechanisms maintain their effectiveness in longer contexts.

### Open Question 2
- Question: What architectural or training modifications would enable smaller models (e.g., 1-3B parameters) to effectively utilize external feedback for iterative refinement?
- Basis in paper: [inferred] The paper finds that "Llama-3.2-3B shows improvement when receiving feedback from the stronger Llama-3.3-70B model, whereas the smaller Llama-3.2-1B fails to benefit," indicating capacity constraints in feedback utilization.
- Why unresolved: The paper demonstrates that model capacity affects feedback utilization but does not investigate whether fine-tuning, intermediate training, or architectural changes could help smaller models interpret and apply external guidance.
- What evidence would resolve it: Experiments fine-tuning smaller models on feedback interpretation tasks or modifying their architectures to include dedicated feedback processing modules.

### Open Question 3
- Question: How can code generation for financial calculations be improved to reduce the 78% execution failure rate observed across models?
- Basis in paper: [inferred] The error analysis reveals that code execution success rates remain low (29.8% for GPT-4o in iterative settings), with 46% of failures due to missing function arguments, 20% due to missing return statements, and 16% due to indentation errors.
- Why unresolved: The paper identifies the problem but does not propose solutions for improving executable code generation in financial contexts where precise numerical reasoning is critical.
- What evidence would resolve it: Implementing code verification steps, fine-tuning on financial calculation code, or integrating symbolic computation modules, then measuring execution success improvements.

## Limitations

- Evaluation pipeline relies on GPT-4o as automatic evaluator with validation on only 50 samples across two financial subdomains
- Capacity threshold identification based on limited model size sampling (1B, 3B, 8B, 70B+) without systematic scaling studies
- Knowledge attribution evaluation depends on sampled knowledge bases that may not represent full financial domain coverage

## Confidence

**High confidence**: The tripartite attribution taxonomy effectively distinguishes model capabilities; evaluation methodology for evidence and code attributions is straightforward and reproducible; finding that self-feedback alone is insufficient is well-supported.

**Medium confidence**: End-to-end generation performs comparably to post-hoc approaches; iterative refinement only improves with external feedback. Both findings are empirically supported but mechanisms are not fully explored, and results may depend on model architecture, task complexity, or evaluation criteria.

## Next Checks

1. **Cross-domain evaluation stability**: Apply FinLFQA evaluation framework to financial questions from additional domains (risk assessment, M&A analysis, annual reports) using the same 50-sample human validation protocol. Verify GPT-4o evaluator correlations remain above 0.8 across all domains.

2. **Capacity threshold mapping**: Systematically evaluate models at 2B, 4B, 7B, and 13B parameter scales on FinLFQA attribution tasks. Identify precise parameter threshold where evidence citation F1 scores drop below 0.5 and code execution success rates fall below 50%.

3. **Knowledge base sensitivity analysis**: Re-run knowledge attribution evaluations using three different knowledge base sampling strategies: (a) fixed 50 concepts, (b) variable 25-100 concepts based on report complexity, (c) domain-specific knowledge bases. Assess whether model rankings change across sampling strategies.