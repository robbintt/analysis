---
ver: rpa2
title: Robust Causal Discovery under Imperfect Structural Constraints
arxiv_id: '2511.06790'
source_url: https://arxiv.org/abs/2511.06790
tags:
- uni00000013
- uni00000003
- uni00000016
- uni00000011
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust causal discovery from
  observational data when prior knowledge contains errors. Existing methods degrade
  substantially with imperfect structural constraints because they rely on inflexible
  thresholding strategies that conflict with data distributions.
---

# Robust Causal Discovery under Imperfect Structural Constraints

## Quick Facts
- arXiv ID: 2511.06790
- Source URL: https://arxiv.org/abs/2511.06790
- Reference count: 27
- One-line primary result: RoaDs achieves average F1-score improvement of 4.4% and 17.0% decrease in SHD compared to GOLEM-EV on causal discovery with imperfect constraints

## Executive Summary
This paper addresses the challenge of causal discovery when prior structural constraints contain errors. Existing methods degrade substantially with imperfect constraints because they rely on inflexible thresholding strategies. The authors propose RoaDs (Robust Causal Discovery under imperfect structural constraints), which harmonizes knowledge and data through Prior Alignment and Conflict Resolution. The method uses a surrogate model to dynamically assess constraint credibility and employs multi-task learning with multi-gradient descent to balance competing objectives.

## Method Summary
RoaDs consists of two main components: Prior Alignment and Conflict Resolution. Prior Alignment uses a surrogate model (e.g., Random Forest) to estimate relationship strengths for constrained edges, dynamically weighting constraints based on observed data correlations. Conflict Resolution employs multi-task learning with multi-gradient descent to balance data-driven and knowledge-driven objectives, avoiding the pitfalls of weighted sum optimization. The method includes a warm-up stage that optimizes the data-driven objective alone before reconciling it with potentially flawed priors. Theoretical guarantees establish that under consistent constraints, the surrogate model successfully recovers true edges while rejecting false positives.

## Key Results
- Achieves average F1-score improvement of 4.4% compared to GOLEM-EV baseline
- Reduces Structural Hamming Distance (SHD) by 17.0% compared to GOLEM-EV
- Maintains stable performance even at high error rates in prior constraints (up to 70% error)

## Why This Works (Mechanism)

### Mechanism 1
The system mitigates the negative impact of erroneous constraints by dynamically re-weighting them based on observed data correlations. Prior Alignment employs a surrogate model to estimate relationship strength for constrained edges. If a constraint claims an edge exists but the surrogate model finds near-zero correlation, the constraint weight is reduced. Core assumption: the Consistent Constraints assumption holds - dependencies in flawed constraints do not perfectly mask true causal edges or create spurious conditional dependencies. Break condition: if the surrogate model is mis-specified, alignment will fail.

### Mechanism 2
The framework avoids the pitfalls of "weighted sum" optimization by treating data-fit and prior-fit as competing objectives to find a balanced trade-off. Conflict Resolution uses a Multi-Task Learning approach optimized via Multi-Gradient Descent Algorithm (MGDA). Instead of minimizing L_data + λL_prior, it finds a gradient update direction that minimizes both losses simultaneously, finding a Pareto stationary point rather than blindly following one. Core assumption: a Pareto stationary point exists that represents a valid DAG structure satisfying acyclicity. Break condition: if objectives are strictly contradictory (e.g., prior insists on a cycle), optimization may stall.

### Mechanism 3
Normalizing gradients by loss magnitude and norm prevents the "easier" knowledge-driven task from dominating update steps. The paper notes that the knowledge-driven objective requires only sparse modifications and is "easier to optimize" than the data-driven objective. Without normalization, optimization would bias towards satisfying the prior while ignoring data reconstruction. Core assumption: gradient magnitude correlates with task difficulty. Break condition: if loss values drop to near zero for the "harder" task but remain high for the "easier" task, normalization might amplify noise.

## Foundational Learning

- **Concept: Structural Equation Models (SEMs)** - Why needed: To select correct surrogate model for Prior Alignment (e.g., using Lasso/Linear Regression for linear SEMs vs. Random Forest for non-linear SEMs). Quick check: Is the data generation process assumed to be linear, or does it involve complex non-linear interactions?

- **Concept: DAG Acyclicity Constraints (h(W))** - Why needed: The core causal discovery loop must enforce that the learned graph is a Directed Acyclic Graph. Quick check: How does the penalty parameter ρ change during training to enforce the constraint?

- **Concept: Pareto Stationarity** - Why needed: To understand the output of the MGDA solver—it does not find a "perfect" graph, but rather a graph where you cannot improve data-fit without hurting prior-fit. Quick check: If κ_t (convergence scalar) is negative, what does that imply about the current solution's relation to the Pareto front?

## Architecture Onboarding

- **Component map:** Observational Data X + Constraint Matrix B_c -> Surrogate Model -> Refined Weights W_c -> Data Model + MGDA Solver -> Learned Graph W

- **Critical path:** The warm-up stage (lines 2-4 in Alg 1) is critical. The model must first minimize L_data alone to find a reasonable starting point before attempting to reconcile it with potentially wrong prior.

- **Design tradeoffs:**
  - Surrogate Choice: Parametric (Linear) is faster but fails on non-linear data. Non-parametric (Random Forest) is robust but slower.
  - Thresholding: The paper uses a threshold (e.g., 0.3) to binarize the final graph. Tuning this affects Precision/Recall trade-offs significantly.

- **Failure signatures:**
  - High SHD + Dense Graph: Likely indicates surrogate model failed to align priors, and MGDA is forcing edges to satisfy flawed B_c.
  - Trivial Solution (Empty Graph): Acyclicity penalty ρ might be too high, or MGDA failing to find valid descent direction.

- **First 3 experiments:**
  1. Sanity Check (Linear EV): Run on Linear Gaussian data with 30% flawed constraints. Ensure Prior Alignment reduces SHD compared to GOLEM-EV.
  2. Ablation (Non-linear): Remove Prior Alignment component and check if F1-score drops by ~20%.
  3. Stress Test (Constraint Error Rate): Increase error rate p_b from 0.1 to 0.7. Verify RoaDs maintains stable SHD while baselines degrade.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can RoaDs framework be adapted to incorporate specific decision-maker preferences for the Pareto front rather than identifying a random stationary point? The conclusion states this work uses MGDA to randomly identify the solution on Pareto front, which may not align with decision-maker's specific preferences.

- **Open Question 2:** Can RoaDs be extended to robustly integrate interventional data in addition to observational data? The conclusion lists extending RoaDs to incorporate interventional data as a specific direction for future work.

- **Open Question 3:** How does the Prior Alignment component fail or degrade when the "consistent constraint" assumption is violated? The paper theoretically proves alignment works under Assumption 1 but admits verifying consistency is often intractable.

## Limitations
- Computational overhead from MGDA and multi-model training makes it slower than single-objective baselines
- Sensitivity to hyperparameter tuning, particularly surrogate threshold τ and normalization strategy
- Reliance on accurate surrogate model specification—mis-specification can undermine Prior Alignment

## Confidence
- Core claims about mechanism effectiveness (F1-score improvement, SHD reduction): High
- Claims about theoretical guarantees under real-world imperfect constraints: Medium (depends on consistent constraints assumption holding approximately)
- Computational scalability claims: Medium (not extensively validated beyond moderate graph sizes)

## Next Checks
1. **Robustness to Surrogate Mis-specification:** Test RoaDs with intentionally mis-specified surrogates (e.g., linear regressor on highly non-linear data) to quantify performance degradation.

2. **Scalability Analysis:** Evaluate RoaDs on larger graphs (n_v > 100) to assess computational feasibility and whether performance gains persist with increased model complexity.

3. **Real-World Constraint Error Patterns:** Apply RoaDs to datasets with structured constraint errors (e.g., systematic biases) rather than random noise to test robustness in practical scenarios.