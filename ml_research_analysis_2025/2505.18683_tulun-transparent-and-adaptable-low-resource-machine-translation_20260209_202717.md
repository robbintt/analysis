---
ver: rpa2
title: 'TULUN: Transparent and Adaptable Low-resource Machine Translation'
arxiv_id: '2505.18683'
source_url: https://arxiv.org/abs/2505.18683
tags:
- translation
- system
- machine
- low-resource
- post-editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TULUN, a user-friendly platform that combines
  neural machine translation with LLM-based post-editing guided by glossaries and
  translation memories. This approach addresses the challenge of domain-specific translation
  accuracy for low-resource languages without requiring technical expertise or model
  fine-tuning.
---

# TULUN: Transparent and Adaptable Low-resource Machine Translation

## Quick Facts
- arXiv ID: 2505.18683
- Source URL: https://arxiv.org/abs/2505.18683
- Reference count: 34
- TULUN achieves 16.90-22.41 ChrF++ point improvements over baseline MT systems for Tetun medical and Bislama disaster relief translations

## Executive Summary
TULUN addresses the challenge of domain-specific translation accuracy for low-resource languages without requiring technical expertise or model fine-tuning. The platform combines neural machine translation with LLM-based post-editing guided by glossaries and translation memories through a user-friendly web interface. Users can configure MT systems, LLMs, and terminology resources, enabling collaborative human-machine translation with immediate feedback loops. The system demonstrates significant improvements over baseline MT systems across multiple low-resource language pairs while maintaining excellent usability scores.

## Method Summary
TULUN employs a two-stage translation pipeline where an NMT model produces an initial translation, followed by LLM-based automated post-editing (APE) that uses retrieved glossary entries and translation memories as in-context examples. The system retrieves glossary terms via {1,2}-gram overlap and translation memories using BM25, then formats these with the NMT output into a few-shot prompt for the LLM. Built on Django with Docker deployment, TULUN supports multiple MT backends (Google Translate API or HuggingFace models) and LLM providers via LiteLLM, with a web interface for translation, glossary/TM management, and evaluation.

## Key Results
- TULUN achieves 16.90-22.41 ChrF++ point improvements over baseline MT systems for Tetun medical and Bislama disaster relief translations
- Shows average 2.8 ChrF point improvement over NLLB-54B across six low-resource languages on FLORES
- Demonstrates excellent usability with System Usability Scale score of 81.25 and 5/5 usefulness rating

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining neural machine translation with LLM-based automated post-editing (APE) improves translation accuracy over standalone NMT or LLM systems for low-resource, domain-specific translation.
- **Mechanism:** NMT produces a draft translation; an LLM then post-edits it using retrieved glossary entries and similar past translations as in-context examples. This leverages NMT's general translation capability while the LLM performs targeted correction of terminology and domain-specific phrasing.
- **Core assumption:** The LLM can effectively identify and correct terminology mismatches and domain inconsistencies when given explicit reference materials, without introducing new errors.
- **Evidence anchors:**
  - [abstract] "combining neural MT with large language model (LLM)-based post-editing guided by existing glossaries and translation memories"
  - [section 4.1.1, Table 1] MADLAD + Gemini achieves 47.91 avg ChrF++ vs. 24.37 for MADLAD alone (+22.41 points)
  - [corpus] Raunak et al. (2023, cited in paper) find MT + LLM APE outperforms either in isolation for high-resource pairs; this paper extends evidence to low-resource settings
- **Break condition:** If the LLM lacks sufficient exposure to the target language (extremely low-resource with rare scripts), it may hallucinate or misapply glossary entries, as observed with Rundi (-1.34 ChrF++ points in Table 2).

### Mechanism 2
- **Claim:** Retrieving relevant glossary entries and translation memories via lexical matching and injecting them into the LLM prompt enables terminology-aware adaptation at inference time.
- **Mechanism:** Glossary retrieval uses {1,2}-gram overlap between input tokens and glossary entries; translation memory retrieval uses BM25 to find the top-N similar source sentences. These are formatted as few-shot examples in the LLM prompt.
- **Core assumption:** BM25 and n-gram overlap reliably surface terminology and phrasing patterns relevant to the current input, and the LLM can follow the provided examples to apply corrections.
- **Evidence anchors:**
  - [section 3.2] "relevant glossary entries are retrieved using {1,2}-gram overlap... Relevant translation memories are the top N... BM25 matches"
  - [section 2] cites Bouthors et al. (2024) for BM25 effectiveness in retrieval-augmented MT
  - [corpus] Evidence is indirect; corpus neighbors focus on data augmentation rather than retrieval-augmented APE specifically
- **Break condition:** If glossary entries are incorrect or retrieval surfaces irrelevant memories, the LLM may introduce errors. The paper notes prompt tuning and glossary curation are needed (section 5, Rundi analysis).

### Mechanism 3
- **Claim:** User-editable glossaries and translation memories create a feedback loop where human corrections systematically improve future translations without model retraining.
- **Mechanism:** Users view how glossary/TM entries informed the translation, then add or correct entries via the UI. The system immediately incorporates updates for subsequent translations.
- **Core assumption:** Users possess domain expertise and will accurately curate resources; the UI makes this process low-friction enough to sustain.
- **Evidence anchors:**
  - [abstract] "enables users to easily create, edit, and leverage terminology resources, fostering a collaborative human-machine translation process"
  - [section 4.1.2] Usability score of 81.25 (SUS); 5/5 rating for overall usefulness
  - [corpus] No direct corpus evidence for this specific feedback-loop mechanism
- **Break condition:** If users lack domain expertise or curation burden is too high, resource quality stagnates or degrades.

## Foundational Learning

- **Concept: Neural Machine Translation (NMT)**
  - Why needed here: TULUN's first stage relies on an NMT model; understanding its baseline behavior helps diagnose when post-editing is most impactful.
  - Quick check question: Can you explain why an NMT model might produce fluent but terminologically incorrect output for a low-resource language?

- **Concept: In-Context Learning / Few-Shot Prompting**
  - Why needed here: The LLM post-editing stage uses glossary entries and translation memories as few-shot examples to guide corrections.
  - Quick check question: How does providing example input-output pairs in a prompt affect an LLM's behavior on a new input?

- **Concept: BM25 Retrieval**
  - Why needed here: Translation memories are retrieved via BM25; understanding term frequency and inverse document frequency helps debug retrieval quality.
  - Quick check question: Why might BM25 outperform dense retrieval for very low-resource languages with limited training data for embedders?

## Architecture Onboarding

- **Component map:**
  - Django backend with models for glossary and translation memory
  - Translator class: Google Translate API or HuggingFace translation pipeline
  - LLM post-editing layer via LiteLLM (supports 100+ providers)
  - Retrieval: spaCy tokenization + n-gram matching for glossary; Tantivy BM25 for TM
  - Frontend: web UI with translation view, glossary/TM management, evaluation mode
  - Deployment: Docker + Docker Compose

- **Critical path:**
  1. User enters source text in Translation View
  2. NMT model translates → displays draft
  3. System retrieves matching glossary entries (n-gram overlap) and top-N TM entries (BM25)
  4. LLM receives prompt with: glossary entries, TM examples, NMT output → returns post-edited translation
  5. User reviews highlighted changes, can add/correct glossary or TM entries

- **Design tradeoffs:**
  - Accuracy vs. latency: two-stage pipeline (NMT + LLM) improves quality but adds latency
  - Configurability vs. simplicity: admin can swap MT models, LLMs, and prompts, but this requires some MT/LLM knowledge
  - Cloud vs. offline: currently requires cloud LLM; future work targets lightweight local models

- **Failure signatures:**
  - Negative ChrF++ delta (e.g., Rundi: -1.34): LLM misapplies glossary entries → requires prompt tuning or glossary cleanup
  - Hallucinations in NMT output not corrected by LLM: suggests insufficient TM coverage or LLM unfamiliarity with target language
  - Poor retrieval relevance: n-gram or BM25 returns irrelevant entries → may need threshold tuning or alternative retrieval

- **First 3 experiments:**
  1. Baseline comparison: Run TULUN vs. NMT-only vs. LLM-only on 10-20 sentences from your domain; compute ChrF++ against references if available
  2. Ablation: Disable glossary retrieval, then disable TM retrieval, to isolate contribution of each component
  3. Prompt iteration: Test alternative system prompts (e.g., with/without chain-of-thought, varying few-shot count) on a held-out evaluation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized small models be fine-tuned to perform terminology-aware post-editing effectively without relying on cloud-based LLMs?
- Basis in paper: [explicit] Section 6 explicitly lists "Offline Deployment Option" as future work, proposing to explore "specialized small models fine-tuned specifically for the post-editing task" to serve users with limited connectivity.
- Why unresolved: The current system relies on large, proprietary models (Gemini 2.0), which may be inaccessible to the target demographic of small organizations in low-resource settings due to infrastructure or cost barriers.
- What evidence would resolve it: A comparative evaluation of local, fine-tuned small models against the current Gemini-based baseline on the Tetun and Bislama datasets, measuring both translation accuracy (ChrF++) and latency.

### Open Question 2
- Question: Can language-specific prompts be automatically optimized to prevent the negative transfer seen in languages like Rundi?
- Basis in paper: [explicit] In Section 6, the authors suggest future work should explore "automatically generating language-specific prompts using techniques like DSPy’s MIPRO" and optimizing based on error patterns.
- Why unresolved: The manual prompt design yielded a significant performance drop for Rundi (-1.34 ChrF++ points), indicating that the static prompt strategy is not robust across all language pairs and requires adaptive tuning.
- What evidence would resolve it: Implementing an automated prompt optimizer (e.g., DSPy) and comparing the resulting translation scores against the static prompt baseline, specifically targeting the underperforming languages in the FLORES benchmark.

### Open Question 3
- Question: Does the retrieval method of using {1,2}-gram overlap for glossaries limit performance for low-resource languages with complex morphology?
- Basis in paper: [inferred] Section 3.2 describes the retrieval mechanism as simple {1,2}-gram overlap. Section 5 notes that for "extremely low-resource languages with complex morphology... this assumption might not hold," potentially leading to higher hallucination rates.
- Why unresolved: The paper does not analyze if the simple string-matching retrieval fails to surface relevant glossary entries for morphologically rich languages, thereby depriving the LLM of necessary context.
- What evidence would resolve it: An ablation study replacing the n-gram overlap retrieval with a semantic or morphological matcher for complex languages, measuring the change in glossary hit-rates and translation accuracy.

## Limitations
- Reliance on cloud-based LLMs (Gemini 2.0 Flash) introduces ongoing costs and latency, though authors plan to address this
- Evaluation based on limited set of languages (6 FLORES-200 languages plus Tetun and Bislama) raises generalizability questions
- Usability evaluation based on small user study (n=4) without detailed demographic information

## Confidence

- **High Confidence**: The core claim that combining NMT with LLM-based APE improves over standalone NMT or LLM systems for low-resource languages, supported by substantial ChrF++ improvements (16.90-22.41 points) across multiple languages and domains.
- **Medium Confidence**: The usability claims (SUS score of 81.25, 5/5 usefulness rating) given the small user study size and limited participant information.
- **Medium Confidence**: The retrieval mechanism's effectiveness, as the paper relies on established methods (BM25, n-gram overlap) but doesn't provide extensive retrieval quality analysis or ablation studies.

## Next Checks

1. **Retrieval Quality Analysis**: Evaluate the precision and recall of the glossary and TM retrieval components separately by manually annotating a sample of retrieved entries for relevance, particularly for morphologically rich or script-different languages.
2. **Scalability Testing**: Test the system with 3-4 additional low-resource language pairs (e.g., from different language families or scripts) to assess generalizability beyond the evaluated languages.
3. **Cost-Benefit Analysis**: Measure the actual latency overhead and approximate cost per translation when using cloud LLM post-editing versus standalone NMT, to quantify the tradeoff between quality improvement and resource consumption.