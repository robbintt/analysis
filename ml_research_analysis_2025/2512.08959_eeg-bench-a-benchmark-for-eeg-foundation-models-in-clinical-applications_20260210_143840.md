---
ver: rpa2
title: 'EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications'
arxiv_id: '2512.08959'
source_url: https://arxiv.org/abs/2512.08959
tags:
- dataset
- datasets
- clinical
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EEG-Bench provides a standardized framework for evaluating EEG
  foundation models in clinical applications, integrating 14 publicly available datasets
  across 11 diverse diagnostic tasks. It features minimal preprocessing, consistent
  evaluation protocols, and supports both classical ML baselines and modern foundation
  models.
---

# EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications

## Quick Facts
- arXiv ID: 2512.08959
- Source URL: https://arxiv.org/abs/2512.08959
- Reference count: 40
- Key outcome: EEG-Bench provides a standardized framework for evaluating EEG foundation models across 11 clinical diagnostic tasks using 14 datasets

## Executive Summary
EEG-Bench establishes a unified benchmark for evaluating foundation models in clinical EEG applications, addressing the fragmentation of previous research through standardized protocols and reproducible evaluation. The benchmark spans 11 diagnostic tasks including abnormal EEG detection, epilepsy classification, Parkinson's disease, and obsessive-compulsive disorder across 14 publicly available datasets. Experiments show foundation models like LaBraM achieving strong performance on tasks such as abnormal EEG detection (0.838 balanced accuracy) while revealing limitations in low-data regimes and highly imbalanced scenarios. Classical models like LDA surprisingly outperformed all foundation models on mild traumatic brain injury classification, highlighting the continued relevance of simpler approaches in certain clinical contexts.

## Method Summary
The benchmark evaluates both foundation models (LaBraM, Neuro-GPT, BENDR) and classical baselines (LDA, SVM) on 11 clinical diagnostic tasks using 14 publicly available EEG datasets. All tasks employ strict cross-subject evaluation to ensure generalization beyond individual patients, with recordings resampled to 200 Hz and filtered (0.1-75 Hz bandpass, notch). Foundation models are fine-tuned end-to-end (LaBraM) or using encoder-only approaches (Neuro-GPT), while classical models extract handcrafted features using the Brainfeatures toolbox. Long clinical recordings are segmented into 4-60 second chunks, encoded separately, and averaged before classification. The benchmark reports Balanced Accuracy as the primary metric with Weighted F1 as secondary, using 5-fold cross-validation with different random seeds.

## Key Results
- Foundation models excel in large-data regimes: LaBraM achieved 0.838 balanced accuracy on abnormal EEG detection with ~3,000 samples
- Classical models dominate low-data scenarios: LDA outperformed all foundation models on mTBI classification (177 samples, 0.813 accuracy)
- Class imbalance severely impacts foundation models: LaBraM achieved only 0.565 accuracy on epilepsy detection despite strong performance elsewhere
- Channel flexibility matters: LaBraM accommodates heterogeneous channel layouts while BENDR/Neuro-GPT require fixed channel sets with zero-padding

## Why This Works (Mechanism)

### Mechanism 1: Cross-Subject Generalization Enforced by Evaluation Design
- Claim: The benchmark's strict cross-subject evaluation protocol ensures that models cannot rely on patient-specific patterns and must learn representations that generalize across individuals.
- Mechanism: All 11 tasks use predefined subject splits where training and test sets contain entirely different patients. This forces models to capture condition-related neural signatures rather than memorizing individual recording idiosyncrasies.
- Core assumption: Cross-subject generalization is a valid proxy for real-world clinical deployment where models encounter new patients.
- Evidence anchors:
  - [abstract] "benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets"
  - [section] "All tasks are evaluated in a strict cross-subject setting to ensure generalization beyond individual patients"
  - [corpus] Limited direct corpus evidence on this specific mechanism; neighboring papers focus on model architecture rather than evaluation protocols

### Mechanism 2: Task-Dependent Model Capacity Requirements
- Claim: Foundation models excel when sufficient training data exists and task-relevant patterns are distributed across channels, while classical models with strong inductive biases outperform in low-data regimes.
- Mechanism: LaBraM achieved 0.838 balanced accuracy on abnormal EEG detection (large dataset: ~3,000 samples) but only 0.565 on epilepsy detection (highly imbalanced). Conversely, LDA achieved 0.813 on mTBI classification with only 177 samples—outperforming all foundation models.
- Core assumption: The observed performance patterns stem from model capacity/data interactions rather than preprocessing differences or hyperparameter tuning quality.
- Evidence anchors:
  - [section] "In the mild traumatic brain injury (mTBI) task, a simple classical method — LDA — surprisingly outperformed all foundation models"
  - [section] "Given the highly imbalanced nature of the epilepsy dataset, this suggests that LaBraM may be more sensitive to label imbalance"
  - [corpus] No corpus papers directly validate this mechanism for EEG; it remains an observation from this benchmark

### Mechanism 3: Embedding Aggregation for Long-Duration Recordings
- Claim: Segmenting long clinical recordings into chunks, encoding separately, and averaging embeddings enables foundation models (designed for 4–60s inputs) to process hour-long recordings while preserving global context.
- Mechanism: Clinical recordings span minutes to hours, exceeding model input limits. The benchmark divides recordings into non-overlapping chunks, passes each through the encoder, and averages the resulting embeddings before classification.
- Core assumption: Averaging chunk embeddings preserves task-relevant information without introducing position-dependent biases.
- Evidence anchors:
  - [section] "This allows scalable inference on long-duration EEG while retaining global context through embedding aggregation"
  - [section] "Since clinical recordings often span minutes to hours, exceeding the input limits of these models (4–60 s), we segment each recording into non-overlapping chunks, encode them separately, and average embeddings before classification"
  - [corpus] No corpus evidence directly evaluates this aggregation strategy

## Foundational Learning

- **Concept: EEG Foundation Model Pre-training Objectives**
  - Why needed here: Understanding why BENDR, Neuro-GPT, and LaBraM differ in their channel flexibility and performance requires knowing their pre-training approaches. BENDR and Neuro-GPT require fixed channel sets (necessitating zero-padding), while LaBraM accommodates heterogeneous layouts.
  - Quick check question: Can you explain why a model pre-trained on a specific electrode montage might fail when tested on a dataset with no overlapping channels?

- **Concept: Class Imbalance in Clinical EEG**
  - Why needed here: The epilepsy detection task showed LaBraM achieving only 0.565 accuracy despite strong performance elsewhere, attributed to class imbalance. Understanding how imbalance affects different loss functions and model architectures is essential for interpreting these results.
  - Quick check question: Why might a high-capacity transformer be more sensitive to class imbalance than an LDA classifier with balanced accuracy as a metric?

- **Concept: Handcrafted EEG Features vs Learned Representations**
  - Why needed here: The benchmark compares Brainfeatures-extracted features (spectral power, entropy, fractal dimension, wavelet transforms) against end-to-end learned representations. Knowing what these features capture helps explain where classical models succeed.
  - Quick check question: What frequency bands and complexity metrics does the Brainfeatures toolbox extract, and why might these be sufficient for some diagnostic tasks?

## Architecture Onboarding

- **Component map**: eeg_bench/datasets/ -> BaseClinicalDataset/BaseBCIDataset -> _download/load_data methods
- **Critical path**:
  1. Set up conda environment via `environment.yml`
  2. Configure local storage paths in `eeg_bench/config.json`
  3. For TUAB/TUEP/TUAR: Accept NEDC data use agreement before automatic download will work
  4. Run `python benchmark_console.py --all` or target specific model/task combinations
  5. To add a new model: implement `AbstractModel` interface with `fit()` and `predict()` methods

- **Design tradeoffs**: Minimal preprocessing (resampling to 200 Hz, bandpass 0.1–75 Hz) preserves signal fidelity but may retain more noise than aggressive pipelines
- **Failure signatures**:
  - Foundation models achieving ~random performance (e.g., BENDR/Neuro-GPT on sleep stages at 0.166–0.169) likely indicates channel mismatch where no pretraining channels overlap with dataset channels
  - Large standard deviations across seeds (e.g., LaBraM on mTBI: 0.740 ± 0.173) suggest high sensitivity to initialization or training instability in low-data regimes
  - Classical models outperforming foundation models by >10% signals potential overfitting or inadequate regularization in the deep models

- **First 3 experiments**:
  1. Reproduce the abnormal EEG detection results to validate environment setup—this task has the largest dataset and showed the clearest foundation model advantage (LaBraM: 0.838)
  2. Test a new foundation model on the OCD and mTBI tasks to understand whether the classical model advantage in low-data regimes is robust across model architectures
  3. Add a custom dataset by inheriting from `BaseClinicalDataset`, implementing `_download` and `load_data`, and verifying it integrates with existing task pipelines

## Open Questions the Paper Calls Out

- **Open Question 1**: How do state-of-the-art specialized models for specific clinical tasks compare against general-purpose foundation models when evaluated within the unified EEG-Bench framework?
  - Basis in paper: [explicit] The authors state in the Limitations section that the current model suite is limited and that "many specialized models have been developed for individual tasks (e.g., seizure detection...), and integrating these will provide a more complete view."

- **Open Question 2**: To what extent is LaBraM's suboptimal performance on epilepsy detection attributable to sensitivity to label imbalance versus architectural overfitting?
  - Basis in paper: [inferred] The paper notes LaBraM underperformed on epilepsy detection (0.565 accuracy) while BENDR achieved 0.740. The authors hypothesize this is due to LaBraM being "more sensitive to label imbalance or more prone to overfitting in such contexts," but do not isolate the cause.

- **Open Question 3**: How does the strategy for handling missing or incompatible input channels (e.g., zero-padding vs. arbitrary assignment) impact the transfer learning capabilities of foundation models on datasets like CHB-MIT and Sleep-Telemetry?
  - Basis in paper: [inferred] The paper notes that for datasets with no common channels (CHB-MIT, Sleep-Telemetry), they "assign the dataset channels to arbitrary... input channels" or "ignore the model-channels." It observes that models like Neuro-GPT failed to learn on these tasks, potentially because they are "not capable of using previously unseen channels in a meaningful way."

## Limitations

- The current model suite is limited, excluding specialized architectures developed for individual clinical tasks that may outperform general foundation models
- Performance degradation on highly imbalanced tasks suggests foundation models may lack adequate regularization or require specialized training protocols for clinical EEG
- Channel handling strategies for heterogeneous datasets (zero-padding vs. arbitrary assignment) may introduce artifacts that affect model performance, particularly for models requiring fixed channel configurations

## Confidence

- **High confidence**: Cross-subject evaluation design effectively prevents patient-specific memorization and forces generalization learning
- **Medium confidence**: Classical models' superior performance in low-data regimes reflects genuine capacity limitations rather than optimization gaps
- **Low confidence**: Embedding averaging preserves sufficient diagnostic information for long-duration clinical recordings

## Next Checks

1. **Reproduce the mTBI results** where LDA (0.813) outperformed all foundation models (LaBraM: 0.740 ± 0.173) to verify whether the classical advantage persists with optimized foundation model hyperparameters
2. **Test alternative imbalance mitigation strategies** on the epilepsy dataset, such as weighted loss functions or oversampling, to determine if LaBraM's poor performance stems from architectural limitations or training protocol issues
3. **Validate the embedding averaging approach** by comparing it against temporal attention mechanisms on tasks where timing is critical (e.g., seizure detection) to assess information preservation tradeoffs