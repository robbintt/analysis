---
ver: rpa2
title: Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural
  Integration
arxiv_id: '2504.12773'
source_url: https://arxiv.org/abs/2504.12773
tags:
- reasoning
- symbolic
- language
- geometric
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoGen, a pipeline that enhances multimodal
  large language models (MLLMs) for geometry problem solving (GPS) by integrating
  symbolic reasoning. GeoGen automatically generates step-wise reasoning paths for
  geometry diagrams, producing high-quality question-answer pairs using precise symbolic
  reasoning.
---

# Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration

## Quick Facts
- arXiv ID: 2504.12773
- Source URL: https://arxiv.org/abs/2504.12773
- Reference count: 40
- GeoGen pipeline achieves 63.9% completion accuracy and 74.0% choice accuracy on MathVista-mini-GPS using Qwen2.5-VL-7B.

## Executive Summary
This paper addresses the challenge of geometric problem solving (GPS) for multimodal large language models (MLLMs), which struggle with complex reasoning despite advances in visual question answering. The authors propose GeoGen, a symbolic-neural integration pipeline that generates high-quality training data by combining a symbolic geometry solver with natural language translation. By constructing two datasets (GeoExpand and GeoSynth) and training a bridging model (GeoLogic) to interface with symbolic verification during inference, the approach significantly improves MLLM accuracy on geometry benchmarks while reducing hallucinations.

## Method Summary
The GeoGen pipeline generates synthetic geometry problems by randomly sampling geometric predicates, calculating coordinates, and rendering diagrams using OpenCV. A symbolic solver (FormalGeo with 210 theorems) performs forward search and traceback to produce formal reasoning paths, which are translated to natural language via an LLM. The system constructs two datasets: GeoExpand (45K samples from real data) and GeoSynth (62K synthetic samples). A bridging model, GeoLogic, translates natural language reasoning steps into formal representations for symbolic verification during inference. The main MLLM (Qwen2.5-VL) is fine-tuned on the combined datasets and uses step-level tree search with symbolic verification to solve new problems.

## Key Results
- GeoGen-SFT-7B achieves 63.9% completion accuracy and 74.0% choice accuracy on MathVista-mini-GPS, outperforming existing MLLM-based geometry solvers.
- Adding GeoSynth synthetic data improves long-term generalization compared to dataset expansion alone (T2 vs T3 in experiments).
- The GeoLogic translation model achieves 94.16% accuracy in converting natural language steps to formal representations.
- Search width tuning shows performance peaks then drops at high width, indicating verification robustness issues.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating chain-of-thought (CoT) training data via symbolic engines improves MLLM reasoning accuracy by providing logically guaranteed solution paths, which reduces the learning of hallucinated inference patterns.
- **Mechanism:** The GeoGen pipeline utilizes a symbolic system (FormalGeo) to perform forward search and traceback, producing formal reasoning paths (sequences of theorems and literals). These paths are mathematically verified as valid before being translated into natural language. By training on this data, the MLLM learns to mimic reasoning steps that are structurally sound, rather than learning from inconsistent or error-prone human/GPT-generated text.
- **Core assumption:** The translation from formal logic to natural language preserves the logical dependencies without introducing ambiguity that confuses the MLLM.
- **Evidence anchors:**
  - [Abstract] "...GeoGen... produces large-scale, high-quality question-answer pairs. [Using] precise symbolic reasoning..."
  - [Section 3.1.2] "...symbolic system... generates reasoning paths with guaranteed correctness."
  - [Corpus] The presence of papers like *NeSyGeo* (arXiv 2505.17121) in the corpus supports the general trend of neuro-symbolic data generation for reasoning, though specific validation for this specific pipeline is limited to the paper's internal benchmarks.
- **Break condition:** If the symbolic solver's theorem set is too narrow (limited to 210 theorems as noted in Section 4.1.2), the MLLM may overfit to specific proof templates and fail on problems requiring novel or complex geometric rules outside the solver's definition.

### Mechanism 2
- **Claim:** Using a translation model (GeoLogic) to interface with a symbolic verifier during inference reduces hallucinations by constraining the MLLM's output generation to visually grounded logic.
- **Mechanism:** The system employs a step-level tree search. The MLLM generates $K$ candidate reasoning steps. GeoLogic translates these steps into formal representations. A symbolic verifier checks these representations against the diagram's formal state. Steps that fail verification (e.g., claiming a relationship that doesn't exist) are pruned, forcing the model to select a valid continuation.
- **Core assumption:** GeoLogic can reliably translate the potentially noisy natural language of the MLLM into the strict syntax required by the symbolic solver.
- **Evidence anchors:**
  - [Abstract] "...GeoLogic... enables symbolic tools to help verifying MLLM outputs, making the reasoning process more rigorous and alleviating hallucinations."
  - [Section 5] "By further integrating GeoLogic... the model generates more accurate and coherent reasoning steps... mitigates hallucinations."
  - [Corpus] *Pi-GPS* (arXiv 2503.05543) supports the importance of diagrammatic information in disambiguating text, providing context for why visual grounding works. Direct evidence for the GeoLogic translation mechanism is specific to this paper.
- **Break condition:** If GeoLogic's translation accuracy drops (e.g., on complex linguistic phrasing), the symbolic verifier will reject correct reasoning steps or accept incorrect ones, breaking the search process.

### Mechanism 3
- **Claim:** Scaling synthetic diagram generation via predicate sampling expands the MLLM's geometric "vocabulary" and improves generalization compared to augmenting existing datasets alone.
- **Mechanism:** The "Plotter" module samples random combinations of geometric predicates (e.g., "Parallelogram", "IsBisector") to synthesize novel diagrams (GeoSynth) rather than just perturbing existing ones. This forces the model to learn fundamental geometric relationships (predicates) rather than memorizing specific dataset artifacts.
- **Core assumption:** The random sampling of predicates results in solvable and pedagogically useful diagrams, rather than degenerate or nonsensical geometric figures.
- **Evidence anchors:**
  - [Section 3.1.1] "Plotter starts by randomly sampling a set of predicates... [we] explored 3,159 unique predicate combinations."
  - [Section 4.3] Shows that adding GeoSynth (T3) yields better long-term generalization than T2 (GeoExpand only), particularly in later epochs.
  - [Corpus] Weak direct support in the provided corpus signals; neighboring papers focus more on evaluation (GeoSense) or specific reasoning techniques rather than predicate-based synthesis.
- **Break condition:** If the visual rendering (OpenCV) produces diagrams that are visually ambiguous (e.g., overlapping labels) or if the predicate combinations are too simple, the model fails to learn robust visual reasoning.

## Foundational Learning

- **Concept: Formal Language vs. Natural Language in Geometry**
  - **Why needed here:** The core of GeoGen is translating between these two. The symbolic system requires strict "Formal Language" (e.g., `IsMidpointOfLine(P, AB)`), while the MLLM produces "Natural Language" (e.g., "Point P is the middle of line AB"). Understanding this mapping is essential for designing the GeoLogic model.
  - **Quick check question:** Can you identify the formal predicate for the statement "Angle A is 90 degrees"?

- **Concept: Supervised Fine-Tuning (SFT) Objective**
  - **Why needed here:** The paper relies on a specific training loss (Equation 1) to align the visual input and text output. Understanding that the model is trained to maximize the likelihood of the next token given the image and previous text helps explain why "guaranteed correct" data is valuable.
  - **Quick check question:** In the SFT loss equation, what does `x_image` represent and why is the vision encoder frozen?

- **Concept: Tree Search with Verifiers**
  - **Why needed here:** The inference mechanism is not a single pass; it is a search process. You must understand that the MLLM proposes options (branches) and the symbolic system prunes them (verification).
  - **Quick check question:** In Algorithm 1, what happens to the candidate steps in set `C` if the verification results `V` are all False?

## Architecture Onboarding

- **Component map:**
  - **GeoGen (Data Pipeline):**
    - *Plotter:* Samples predicates $\to$ Calculates coordinates $\to$ Renders Diagram (OpenCV).
    - *Target Finder:* FormalGeo Solver $\to$ Forward Search $\to$ Traceback $\to$ Reasoning Path.
    - *Translator:* Qwen2.5-32B (External) $\to$ Natural Language Solution.
  - **GeoLogic (Bridging Model):** A fine-tuned LLM (Qwen2.5-3B) that maps Natural Language Steps $\to$ Formal Triples (Condition, Theorem, Conclusion).
  - **Inference Engine:** MLLM (Qwen2.5-VL) $\to$ Candidate Generation $\to$ GeoLogic $\to$ Symbolic Verifier.

- **Critical path:**
  1.  **Data Synthesis:** Run Plotter to generate 129k diagrams $\to$ Filter by theorem sequence diversity $\to$ Result: GeoSynth (62k samples).
  2.  **Training:** Train MLLM on GeoExpand + GeoSynth.
  3.  **Bridging:** Train GeoLogic on pairs of (Natural Language Step, Formal Logic Step).
  4.  **Inference:** MLLM generates candidates $\to$ GeoLogic translates $\to$ Verifier checks. If valid, append to history; else, re-sample.

- **Design tradeoffs:**
  - **Strict vs. Fast Verification:** *Strict* checks full theorem logic but is brittle to naming/ordering mismatches. *Fast* only checks if visual elements exist (necessary condition) but allows logical leaps. The paper implies "Fast" is more robust engineering-wise, though logically weaker.
  - **Search Width:** Increasing width $K$ helps find valid paths but introduces noise. Paper results show accuracy peaks then drops if width is too high (Figure 4), likely because random selection among many "valid-but-incorrect" paths degrades performance.

- **Failure signatures:**
  - **Visual Hallucination:** The model describes a relationship (e.g., "AB is parallel to CD") that is textually plausible but visually false. (Mitigation: Fast mode verification).
  - **Syntax Mismatch:** GeoLogic outputs a formal string that doesn't match the symbolic solver's required syntax, causing the verifier to crash or reject valid steps.
  - **Overfitting to Theorems:** The model learns to apply specific theorems (e.g., Pythagorean theorem) even when not applicable, simply because they appeared frequently in the synthesized data.

- **First 3 experiments:**
  1.  **GeoLogic Accuracy Test:** Before inference, validate the GeoLogic model on a held-out set of natural-to-formal translations. The paper reports 94.16% accuracy; ensure your replication meets $>90\%$ or the verifier will fail too often.
  2.  **Ablation on Data Source:** Train three models: (A) Geo170K only, (B) Geo170K + GeoExpand, (C) Geo170K + GeoExpand + GeoSynth. Verify that (C) provides the "generalization" boost described in Section 4.3.
  3.  **Search Width Tuning:** On a small validation set (e.g., Geometry3K subset), vary the sampling width $K \in \{1, 5, 10, 16\}$. Confirm the performance curve matches Figure 4 (improvement then degradation) to calibrate the inference budget.

## Open Questions the Paper Calls Out
- **Question:** How can symbolic verification be improved to validate the sufficiency of reasoning steps rather than just necessary conditions?
  - **Basis in paper:** [inferred] Page 8 notes that increasing search width lowers accuracy because verification currently only checks necessary conditions, failing to guarantee logical correctness.
  - **Why unresolved:** The current "fast mode" prevents hallucination of objects but allows logically invalid conclusions to pass verification.
  - **What evidence would resolve it:** An algorithm that maintains accuracy gains as search width increases by filtering for sufficiency.

- **Question:** How can this symbolic-neural integration be effectively applied to reinforcement learning (RL) scenarios?
  - **Basis in paper:** [explicit] Page 8 explicitly lists "explore their application in reinforcement learning scenarios" as a future direction.
  - **Why unresolved:** The current methodology relies solely on Supervised Fine-Tuning (SFT); the potential for symbolic systems to act as reward signals in RL is untested.
  - **What evidence would resolve it:** A training framework where symbolic verification provides rewards, leading to improved policy optimization.

- **Question:** What constitutes a "more effective interaction mechanism" between MLLMs and symbolic tools beyond the current GeoLogic bridging model?
  - **Basis in paper:** [explicit] Page 8 mentions the plan to "develop more effective interaction mechanisms between MLLMs and symbolic tools."
  - **Why unresolved:** GeoLogic currently functions as a one-way translator for verification; multi-turn or bi-directional interactions are not defined.
  - **What evidence would resolve it:** A new architecture demonstrating tighter feedback loops or error correction capabilities between the MLLM and solver.

## Limitations
- **Data Generation Constraints:** The symbolic solver's limited theorem set (210 theorems) may lead to overfitting to specific proof templates and fail on problems requiring novel geometric rules.
- **Verification Tradeoffs:** The symbolic verification process faces a fundamental tradeoff between rigor (Strict mode) and robustness (Fast mode), with neither fully solving the hallucination problem.
- **Generalization Uncertainty:** The assumption that randomly sampling geometric predicates produces pedagogically useful diagrams lacks validation, raising questions about whether the model learns fundamental relationships or memorizes artifacts.

## Confidence
- **High Confidence:** The basic architecture of integrating symbolic verification with neural generation is sound and the reported performance improvements on benchmarks are plausible given the methodology.
- **Medium Confidence:** The specific implementation details of the symbolic solver (FormalGeo) and the GeoLogic translation model are sufficient for the reported results, but their generalizability to more complex geometric domains remains uncertain.
- **Low Confidence:** The long-term generalization benefits claimed from synthetic data augmentation (GeoSynth) over dataset expansion alone (GeoExpand) are based on internal benchmarks and may not hold across different problem distributions.

## Next Checks
1. **GeoLogic Translation Robustness Test:** Evaluate the GeoLogic model on a held-out test set of natural language reasoning steps from diverse sources (not just the training data). Measure both translation accuracy and the symbolic verifier's acceptance rate to identify whether translation errors or verifier brittleness is the primary failure mode.

2. **Predicate Sampling Validation:** Systematically analyze a random sample of generated diagrams from the GeoSynth dataset to verify that randomly sampled predicates produce visually coherent, solvable, and pedagogically useful geometric figures. Check for visual ambiguity, degenerate cases, and whether the diagrams span the intended diversity of geometric concepts.

3. **Theorem Coverage Stress Test:** Design a set of geometry problems that require applying theorems outside the 210-theorem FormalGeo system. Evaluate whether the model overfits to specific proof templates by checking if it attempts to apply learned theorems inappropriately when faced with novel problem structures.