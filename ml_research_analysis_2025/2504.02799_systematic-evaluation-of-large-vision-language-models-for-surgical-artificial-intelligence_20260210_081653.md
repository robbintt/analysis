---
ver: rpa2
title: Systematic Evaluation of Large Vision-Language Models for Surgical Artificial
  Intelligence
arxiv_id: '2504.02799'
source_url: https://arxiv.org/abs/2504.02799
tags:
- frame
- train
- surgical
- image
- gallbladder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates 11 large vision-language models
  (VLMs) across 17 surgical AI tasks spanning 13 datasets, including laparoscopic,
  robotic, and open procedures. Proprietary models like GPT-4o and Gemini demonstrated
  strong performance in surgical scene comprehension and progression understanding,
  with GPT-4o achieving up to 0.52 F1 score on error recognition and 0.33 F1 on phase
  recognition.
---

# Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence

## Quick Facts
- **arXiv ID:** 2504.02799
- **Source URL:** https://arxiv.org/abs/2504.02799
- **Reference count:** 40
- **One-line primary result:** VLMs show promise for surgical workflow optimization and report generation, with GPT-4o achieving up to 0.52 F1 on error recognition, but struggle with spatial reasoning tasks (gesture recognition F1 < 0.1).

## Executive Summary
This study presents a comprehensive evaluation of 11 large vision-language models across 17 surgical AI tasks spanning 13 datasets. The research systematically compares proprietary models (GPT-4o, Gemini 1.5 Pro), open-source VLMs (Qwen2-VL, PaliGemma, LLaVA-NeXT), and a surgery-specific model (SurgVLP) on tasks including tool recognition, phase detection, skill assessment, and critical view of safety evaluation. Results demonstrate that while VLMs excel at surgical scene comprehension and can achieve competitive or superior performance compared to task-specific models in few-shot settings, they consistently struggle with spatial and temporal reasoning tasks. The study highlights VLMs' potential for workflow optimization and report generation while identifying spatial reasoning as a key limitation for surgical field augmentation.

## Method Summary
The study evaluates 11 VLMs on 17 surgical AI tasks across 13 datasets through zero-shot and few-shot (in-context learning) inference only, without any model training. Researchers implemented a comprehensive evaluation pipeline using specific prompt templates for every model-task pair, with detailed subsampling rates for test sets provided in Table S2. For video tasks, frames were uniformly sampled (e.g., 32 frames for 30-second clips), and JSON parsing was used to extract model outputs. The primary metric was F1 score, with mAP for detection tasks and mIoU for segmentation. A custom F1-max thresholding approach was applied to contrastive models to optimize binary classification performance. The evaluation included both public datasets (Cholec80, HeiChole, Endoscapes, JIGSAWS) and two proprietary datasets (Intermountain, SST), though results from the private datasets cannot be fully reproduced.

## Key Results
- GPT-4o achieved the highest performance across multiple tasks, with up to 0.52 F1 score on error recognition and 0.33 F1 on phase recognition
- In-context learning improved performance up to 3-fold for certain tasks compared to zero-shot inference
- SurgVLP excelled at domain-specific tasks, achieving up to 0.46 F1 score on critical view of safety assessment
- All models struggled with spatial and temporal reasoning tasks, with gesture recognition F1 scores remaining below 0.1
- 5-shot VLM results approached or exceeded task-specific models trained on thousands of labeled images in several domains

## Why This Works (Mechanism)
The performance of VLMs on surgical tasks stems from their ability to process both visual and textual information through unified multimodal embeddings. The study demonstrates that VLMs leverage their pre-trained knowledge of surgical contexts to perform surprisingly well on domain-specific tasks without task-specific fine-tuning. In-context learning enables VLMs to adapt to surgical terminology and task requirements through carefully crafted prompts, though this adaptation is limited by context window constraints and the models' inherent spatial reasoning capabilities.

## Foundational Learning
**Surgical Computer Vision Tasks:** Why needed: To understand the specific challenges of surgical video analysis beyond general image classification. Quick check: Can you list at least three distinct types of surgical AI tasks (e.g., phase recognition, tool detection, skill assessment)?
**In-Context Learning:** Why needed: The study relies heavily on few-shot performance through prompt engineering rather than model fine-tuning. Quick check: Can you explain the difference between zero-shot and few-shot inference in VLMs?
**Vision-Language Model Architecture:** Why needed: Understanding how VLMs process multimodal inputs is crucial for interpreting performance differences. Quick check: Can you describe the basic architecture of a VLM (visual encoder + language model)?
**Threshold Optimization for Contrastive Models:** Why needed: The study uses F1-max thresholding, which is critical for binary classification tasks. Quick check: Can you explain why threshold optimization might be necessary for CLIP-based models?

## Architecture Onboarding

**Component Map:** Data Preprocessing -> VLM Inference (with prompts) -> Output Parsing -> Metric Calculation -> Threshold Optimization
**Critical Path:** Dataset subsampling → Prompt template selection → Model inference → JSON output parsing → F1 calculation (with F1-max for contrastive models)
**Design Tradeoffs:** Zero-shot evaluation enables broad comparison but may underestimate model potential; proprietary dataset inclusion limits reproducibility; threshold optimization provides upper bounds but may not reflect real-world deployment
**Failure Signatures:** API context limit errors (token overflow for video tasks), JSON parsing failures (hallucinated or malformed outputs), performance drops with increased spatial reasoning complexity
**Three First Experiments:**
1. Validate phase recognition performance on Cholec80 using the specified prompt and sampling rate
2. Test tool presence detection on EndoVis17 with few-shot examples to confirm 3-fold improvement
3. Evaluate gesture recognition task to reproduce the sub-0.1 F1 score performance limitation

## Open Questions the Paper Calls Out
- What architectural modifications could improve VLMs' spatial and temporal reasoning capabilities for surgical applications?
- How can the performance gap between threshold-optimized "upper bound" results and real-world deployment scenarios be bridged?
- Would fine-tuning VLMs on surgical datasets significantly outperform the few-shot results achieved in this study?
- How do VLMs compare to specialized surgical models when trained with comparable data volumes and task-specific objectives?

## Limitations
- Private dataset dependency prevents full reproducibility of results for CVS assessment, disease severity, and error detection tasks
- Spatial reasoning limitations identified but underlying causes and potential solutions remain speculative
- F1-max thresholding methodology produces "upper bound" performance that may not reflect real-world deployment scenarios

## Confidence
- **High confidence:** Zero-shot and few-shot performance comparisons between VLMs and task-specific models on public datasets
- **Medium confidence:** Domain-specific model advantages demonstrated but limited by single surgical VLM benchmarked
- **Low confidence:** Spatial reasoning limitations conclusively identified but path to improvement remains unclear

## Next Checks
1. Reproduce core results on public datasets by validating F1 scores for phase recognition (Cholec80) and tool presence (EndoVis17) using specified prompts and few-shot sampling rates
2. Conduct threshold sensitivity analysis to quantify performance variation with threshold selection for contrastive models and assess gap between "upper bound" F1-max and real-world deployment
3. Perform spatial reasoning ablation study to test whether performance improves with explicit spatial reasoning prompts or by combining VLMs with traditional computer vision components for gesture and trajectory analysis