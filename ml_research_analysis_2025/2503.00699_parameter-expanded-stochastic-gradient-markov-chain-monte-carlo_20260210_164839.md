---
ver: rpa2
title: Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo
arxiv_id: '2503.00699'
source_url: https://arxiv.org/abs/2503.00699
tags:
- sghmc
- posterior
- px-sghmc
- conference
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limited sample diversity in Stochastic Gradient
  Markov Chain Monte Carlo (SGMCMC) methods for Bayesian Neural Networks, which affects
  uncertainty estimation and model performance. The authors propose Parameter Expanded
  SGMCMC (PX-SGMC), a simple yet effective approach that reparameterizes each weight
  matrix in the neural network as a product of multiple matrices.
---

# Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo

## Quick Facts
- **arXiv ID:** 2503.00699
- **Source URL:** https://arxiv.org/abs/2503.00699
- **Reference count:** 40
- **Primary result:** Parameter Expanded SGMCMC (PX-SGMC) improves sample diversity in Bayesian Neural Networks through matrix reparameterization, achieving better uncertainty estimation and OOD robustness without increasing inference cost.

## Executive Summary
This paper addresses limited sample diversity in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) methods for Bayesian Neural Networks, which affects uncertainty estimation and model performance. The authors propose Parameter Expanded SGMCMC (PX-SGMC), a simple yet effective approach that reparameterizes each weight matrix in the neural network as a product of multiple matrices. This reparameterization introduces implicit preconditioning on gradient updates, allowing better exploration of the posterior parameter space without increasing inference cost. Extensive experiments on CIFAR-10 and other datasets demonstrate that PX-SGHMC outperforms standard SGMCMC methods and deep ensembles in terms of classification error, negative log-likelihood, and out-of-distribution robustness.

## Method Summary
PX-SGMC reparameterizes each weight matrix W as a product W = P·V·Q, where P and Q are expanded matrices and V is the base matrix. This introduces implicit preconditioning on gradient updates through the structure of the expanded matrices, improving posterior exploration. The method maintains the same computational efficiency during inference since the expanded matrices are reassembled into W before deployment. During training, separate friction parameters are used for expanded matrices (γ=1) versus base matrices (γ=100) to balance exploration and stability. The approach is implemented within the SGHMC framework and tested on ResNet20 architectures with FRN normalization and Swish activation.

## Key Results
- PX-SGHMC outperforms standard SGMCMC methods and deep ensembles on CIFAR-10 in ERR, NLL, and OOD detection metrics
- Sample diversity from PX-SGHMC matches or exceeds Hamiltonian Monte Carlo while maintaining SGMCMC computational efficiency
- Parameter expansion depth directly correlates with exploration distance and singular value dynamics in weight matrices
- The method achieves superior uncertainty estimation with minimal training overhead (40% memory increase for c=d=1)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Preconditioning via Matrix Factorization
Decomposing weight matrices into products of matrices induces implicit preconditioning on gradient updates, improving posterior exploration. When weight matrix W is reparameterized as W = PVQ, gradient updates to P, V, Q individually produce a combined update to W that is effectively preconditioned by a matrix derived from current parameter values. This preconditioning amplifies updates along certain directions in parameter space.

### Mechanism 2: Implicit Adaptive Step-Size Scaling
Parameter expansion achieves exploration benefits similar to larger step sizes while maintaining convergence stability. The preconditioner scales updates along different eigendirections proportionally to their eigenvalues, rather than uniformly scaling all directions. This allows larger effective step sizes along well-conditioned directions without the instability that arises from simply increasing ε.

### Mechanism 3: Singular Value Dynamics Amplify Exploration
Deeper expansion increases maximum singular values and decreases minimum singular values, which enlarges the theoretical exploration bound. The expansion depth appears in the exploration bound. Empirically, as expansion depth increases, maximum singular values grow while minimum shrink, and consecutive-sample distances increase.

## Foundational Learning

- **Concept:** SGMCMC fundamentals (SGLD, SGHMC update rules)
  - **Why needed here:** PX-SGMCMC is built on top of existing SGHMC dynamics; understanding the base sampler is prerequisite to understanding what the reparameterization modifies.
  - **Quick check question:** Can you write out the SGLD and SGHMC update equations and explain the role of the friction term γ in SGHMC?

- **Concept:** Bayesian Neural Networks and posterior sampling via Langevin dynamics
  - **Why needed here:** The method targets the posterior distribution p(θ|D); understanding BMA and why sample diversity matters is essential context.
  - **Quick check question:** Why does low sample diversity hurt uncertainty estimation in Bayesian model averaging?

- **Concept:** Preconditioning in optimization (conceptual)
  - **Why needed here:** The paper's mechanism hinges on understanding how preconditioning changes gradient directions and magnitudes.
  - **Quick check question:** How does a preconditioner differ from simply multiplying the learning rate by a constant?

## Architecture Onboarding

- **Component map:** Expanded matrices P^(l)₁:c (left expansion), V^(l) (base matrix), Q^(l)₁:d (right expansion) per layer -> Standard SGMCMC sampler SGHMC with position θ containing all expanded parameters and momentum r -> Inference-time reassembly W = P₁:c V Q₁:d computed once after sampling completes

- **Critical path:**
  1. Initialize expanded matrices (P, Q as identity or small random; V with standard initialization)
  2. Run SGHMC updates on all expanded parameters jointly
  3. Set friction γ for P, Q parameters lower than for V (paper uses γ_P = γ_Q = 1, γ_V = 100)
  4. After sampling, reassemble W = PVQ for inference

- **Design tradeoffs:**
  - **Expansion depth (c, d):** Deeper expansion → better exploration but more parameters during training
  - **Memory overhead:** ~40% increase for c = d = 1 on standard layers; lower-rank variants available
  - **Inference cost:** Zero overhead—matrices reassembled before deployment

- **Failure signatures:**
  - Singular values collapsing to near-zero (monitor via σ_max and σ_min per layer)
  - Training instability if friction for P, Q is set too high
  - Degraded performance if expansion depth is too shallow (c = d = 0 reverts to standard SGHMC)

- **First 3 experiments:**
  1. Implement HMC on 2D mixture of 25 Gaussians with SP vs EP to confirm mode-hopping behavior before scaling up.
  2. Run SGHMC vs PX-SGHMC with (c=0, d=0), (c=1, d=0), (c=1, d=1), (c=2, d=2) on CIFAR-10 validation split, measuring ERR, NLL, and AMB.
  3. Compare consecutive-sample Euclidean distances and ensemble ambiguity between SGHMC and PX-SGHMC to verify the exploration improvement mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal depth (c+d+1) of parameter expansion for different neural network architectures and posterior geometries? The authors observe that "performance monotonically improved as we increased the depth of EP, while converging to a certain fixed level in the end" but do not provide principled guidance on depth selection.

### Open Question 2
Can the theoretical analysis in Theorem 3.2 be strengthened to provide tighter, lower-bound guarantees on exploration rather than just upper bounds on sample distances? The authors note "the bound in Theorem 3.2 is not necessarily the maximum distance between consecutive samples, its dependency on the depth (c+d+1) of our EP suggests that the preconditioning induced by EP may improve the exploration."

### Open Question 3
Can the reparameterization design be optimized to minimize training memory and computation overheads while preserving or enhancing diversity gains? The authors state "While EP does not increase inference costs, it does require additional training resources in terms of memory and computation. In future work, we aim to optimize the reparameterization design to minimize these computational overheads while further enhancing diversity."

## Limitations
- Theoretical preconditioning mechanism assumes gradient flow rather than stochastic Langevin dynamics, requiring additional assumptions for SGMCMC extension
- Singular value dynamics enabling exploration are observed empirically but lack theoretical guarantees for nonlinear networks
- Superiority over adaptive preconditioning methods (Fisher information) is not directly compared in the paper

## Confidence
- **High:** Empirical improvements in ERR/NLL on CIFAR-10 and OOD detection are clearly demonstrated
- **Medium:** Mechanism linking expansion depth to exploration distance is supported by experiments but relies on unproven assumptions about gradient preconditioning in stochastic settings
- **Medium:** Claims about parameter efficiency relative to deep ensembles are valid but assume ensembles are trained independently without knowledge transfer

## Next Checks
1. Prove that Lemma 3.1's preconditioning matrix remains valid under stochastic gradient noise, or show empirically that noise preserves the directional amplification effect
2. Compare PX-SGHMC against SGMCMC with explicit Fisher-preconditioned gradients to isolate the architectural effect from the preconditioning effect
3. Track σ_max/σ_min ratios across training for both standard and expanded parameterizations to verify that singular value separation correlates with exploration improvements without numerical instability