---
ver: rpa2
title: 'The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation
  Learning in Pretrained Language Models'
arxiv_id: '2511.14365'
source_url: https://arxiv.org/abs/2511.14365
tags:
- smiles
- tokens
- chemistry
- pretraining
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the tokenization bottleneck in chemistry representation
  learning, where standard tokenizers fragment chemical notations like SMILES into
  semantically uninformative sub-tokens. The authors propose a principled methodology
  that extends a pretrained LLM's vocabulary with chemically salient tokens, derived
  from a data-driven analysis of SMILES strings, and performs continued pretraining
  on a curated chemistry-domain corpus.
---

# The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models

## Quick Facts
- arXiv ID: 2511.14365
- Source URL: https://arxiv.org/abs/2511.14365
- Reference count: 9
- Key outcome: Vocabulary extension reduces SMILES tokenization from median 41 to 10 tokens, improving LLM performance on chemistry tasks.

## Executive Summary
Standard LLM tokenizers fragment chemical notations like SMILES into semantically uninformative sub-tokens, hampering chemistry representation learning. This paper proposes extending a pretrained LLM's vocabulary with chemically salient tokens and performing continued pretraining on a chemistry-domain corpus. The approach unifies natural language and molecular structure representations within a single model, significantly improving performance across chemistry tasks while reducing SMILES tokenization length.

## Method Summary
The authors extend Llama3-8B's vocabulary with 17,795 new tokens (1,000 text OOV + 16,795 SMILES substructures) derived from a data-driven analysis of SMILES strings. They initialize new embeddings as the mean of existing embeddings and perform continued pretraining for 1 epoch on a curated 16.37B token blend of chemistry-domain data. The training uses a cosine learning rate schedule (max 3e-4, min 3e-6, warmup 200 steps) with global batch size 16 on 32 H100s. Evaluation on SMILESMolInstruct benchmark shows consistent improvements across five task categories.

## Key Results
- SMILES tokenization reduced from median 41 to 10 tokens per string
- Forward synthesis: Morgan FPS improved from 0.44 to 0.84
- Retrosynthesis: Morgan FPS improved from 0.40 to 0.69
- BBBP property prediction: F1 score improved from 0.79 to 0.88

## Why This Works (Mechanism)
Standard tokenizers treat SMILES strings as arbitrary text, breaking chemical structures into meaningless fragments. By extending the vocabulary with chemically meaningful substructures (functional groups, aromatic rings), the model learns to represent these as single tokens. This enables the LLM to capture chemical semantics directly, improving both generation quality and downstream task performance. The continued pretraining phase allows the model to integrate these new representations with existing language understanding.

## Foundational Learning
- **SMILES notation**: Text-based representation of molecular structures using atoms, bonds, and branching. Why needed: Understanding the target chemical representation format. Quick check: Can you parse a simple SMILES string like "CCO" into its constituent atoms?
- **Tokenization bottlenecks**: Standard tokenizers fragment domain-specific notations into semantically meaningless pieces. Why needed: Core problem being solved. Quick check: What is the median tokenization length for SMILES before and after extension?
- **Vocabulary extension**: Process of adding new tokens to an existing tokenizer. Why needed: Key technical approach. Quick check: How many new tokens were added and how were they selected?
- **Continued pretraining (CPT)**: Further training a pretrained model on domain-specific data. Why needed: Method to integrate new vocabulary. Quick check: What was the training duration and learning rate schedule?
- **Morgan fingerprints**: Circular molecular descriptors used for similarity comparison. Why needed: Evaluation metric for synthesis tasks. Quick check: What does a higher Morgan FPS score indicate?

## Architecture Onboarding

**Component map:** SMILES extraction -> Vocabulary extension -> Tokenizer update -> Continued pretraining -> Evaluation

**Critical path:** The vocabulary extension and continued pretraining phases are critical. Without proper vocabulary extension, continued pretraining cannot effectively learn chemical representations.

**Design tradeoffs:** The authors chose frequency-based merging (threshold=3) for simplicity over more sophisticated methods. They also opted for mean initialization of new embeddings rather than training from scratch.

**Failure signatures:** High invalid SMILES rate indicates vocabulary extension failed to merge common substructures properly. Catastrophic forgetting on general text suggests FineWeb replay is insufficient.

**First experiments:**
1. Verify vocabulary extension reduces SMILES tokenization length from 41 to ~10 tokens
2. Check initial model performance on a simple chemistry task (e.g., molecule captioning) before and after CPT
3. Measure general text perplexity to detect catastrophic forgetting

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results are based on a single architecture (Llama3-8B) and one benchmark (SMILESMolInstruct)
- Underperformance on continuous property regression tasks (LIPO, ESOL) suggests uneven domain adaptation
- Exact data blend sampling schedule is ambiguous, complicating exact replication
- Vocabulary expansion method involves heuristic merging thresholds that may not generalize

## Confidence
- **High confidence**: Vocabulary extension reduces SMILES tokenization length and improves generation validity
- **Medium confidence**: Performance gains on forward/retrosynthesis and BBBP classification tasks
- **Medium confidence**: Unified natural language and chemistry representation within single model

## Next Checks
1. Verify tokenizer fertility: median tokens per SMILES should be â‰¤15 for successful vocabulary extension
2. Monitor general-domain perplexity during CPT to detect catastrophic forgetting
3. Compare performance on classification vs regression tasks to understand domain adaptation limits