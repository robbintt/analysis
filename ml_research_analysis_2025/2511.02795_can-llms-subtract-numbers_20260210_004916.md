---
ver: rpa2
title: Can LLMs subtract numbers?
arxiv_id: '2511.02795'
source_url: https://arxiv.org/abs/2511.02795
tags:
- llms
- subtraction
- accuracy
- sign
- addition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of subtraction capabilities
  in large language models (LLMs), highlighting that subtraction accuracy lags significantly
  behind addition despite being a fundamental arithmetic operation. The authors evaluate
  eight pretrained LLMs from four families on single-token and multi-token subtraction
  problems, revealing that LLMs struggle particularly when the result is negative
  (a < b), often producing the correct magnitude but omitting the negative sign.
---

# Can LLMs subtract numbers?
## Quick Facts
- arXiv ID: 2511.02795
- Source URL: https://arxiv.org/abs/2511.02795
- Authors: Mayank Jobanputra; Nils Philipp Walter; Maitrey Mehta; Blerta Veseli; Evan Parker Kelly Chapple; Yifan Wang; Sneha Chetani; Ellie Pavlick; Antonio Vergari; Vera Demberg
- Reference count: 17
- Key outcome: LLMs struggle with subtraction accuracy, particularly for negative results, despite encoding sign information internally

## Executive Summary
This paper presents a systematic study of subtraction capabilities in large language models (LLMs), highlighting that subtraction accuracy lags significantly behind addition despite being a fundamental arithmetic operation. The authors evaluate eight pretrained LLMs from four families on single-token and multi-token subtraction problems, revealing that LLMs struggle particularly when the result is negative (a < b), often producing the correct magnitude but omitting the negative sign. Probing analyses demonstrate that LLMs internally encode whether results should be negative, but this information fails to transfer to generated outputs. While few-shot prompting yields modest gains for some models, instruction-tuned LLMs achieve near-perfect accuracy on negative results.

## Method Summary
The study evaluates subtraction vs addition performance across 8 pretrained LLMs from 4 families (Gemma-2, Qwen3, OLMo-2, Llama-3), focusing on cases where a < b (negative results). Synthetic datasets are generated with operands sampled from tokenizer-specific single-token ranges, balanced across a>b and a<b conditions. Greedy decoding (temperature=0) is used for zero-shot inference across multiple prompt variants. Accuracy metrics include exact match and magnitude-only match (ignoring sign), stratified by operand ordering. Linear probes are trained on final-layer hidden states to verify internal sign encoding.

## Key Results
- Subtraction accuracy lags behind addition across all tested LLMs
- LLMs struggle particularly when results are negative (a < b), often producing correct magnitude but omitting negative sign
- Internal probing reveals LLMs encode sign information with near-100% accuracy, but this fails to transfer to generated outputs
- Instruction-tuned models achieve near-perfect accuracy on negative results compared to 0-64% for pretrained variants
- Few-shot prompting yields inconsistent improvements across model families

## Why This Works (Mechanism)

### Mechanism 1: Representation-to-Generation Disconnect
- Claim: Pretrained LLMs encode correct sign information in hidden states but fail to transfer this to the output layer during decoding.
- Mechanism: Linear probes trained on final-layer activations can predict whether results should be positive or negative with 99-100% accuracy, indicating the information exists internally. However, the generation process does not consistently access or act on this representation when producing tokens.
- Core assumption: The probe's high accuracy implies the model "knows" the sign rather than the probe learning an orthogonal classification.
- Evidence anchors: Probes achieved "100% on Gemma-2 9B and Qwen3-8B, and above 99% on Llama-3.1-8B" on sign classification, while generation accuracy for a<b cases was often below 5%.

### Mechanism 2: Magnitude Computation Is Partially Independent of Sign Generation
- Claim: LLMs can compute the correct numerical magnitude even when failing to produce the correct sign, suggesting partially separate processing streams.
- Mechanism: When ignoring the negative sign, accuracy for a<b cases jumps dramatically (e.g., OLMo-2-13B from ~4% to ~96%). This indicates the magnitude computation pathway functions while the sign assignment pathway fails.
- Core assumption: The model processes magnitude and sign through at least partially separable internal mechanisms rather than computing the full signed result in one unified process.
- Evidence anchors: Accuracy increases substantially under "ignoring '-' sign" metric across all tested LLMs (e.g., Gemma-2-27B: 3% → 34%; OLMo-2-13B: 4% → 96%).

### Mechanism 3: Instruction-Tuning Data Exposure Remediates Negative-Result Generation
- Claim: Instruction-tuning on datasets containing negative subtraction results enables near-perfect sign generation, suggesting the failure is primarily a training coverage issue rather than architectural limitation.
- Mechanism: Pretrained models lack sufficient exposure to explicit negative-result subtraction during pretraining. Instruction-tuning datasets (MATH, GSM8k, Tülu 3 for OLMo-2) contain such cases, providing the necessary signal for the model to learn the output mapping.
- Core assumption: The improvement stems primarily from data exposure rather than other aspects of instruction-tuning (e.g., format alignment, RLHF).
- Evidence anchors: Instruction-tuned variants achieve 88-100% accuracy on a<b cases versus 0.2-64% for pretrained variants.

## Foundational Learning

- Concept: Non-commutative operations (order-dependence)
  - Why needed here: Subtraction differs from addition in that a−b ≠ b−a. Understanding this asymmetry is essential for interpreting why operand order creates dramatically different accuracy regimes (a>b vs. a<b).
  - Quick check question: Given 7−3 and 3−7, would you expect similar error patterns? Why or why not?

- Concept: Probing classifiers
  - Why needed here: The paper uses linear probes to demonstrate that sign information exists in hidden states. Understanding what probes can and cannot reveal about internal representations is critical for interpreting these results.
  - Quick check question: If a probe achieves 100% accuracy on a classification task using hidden states, does this guarantee the model "uses" this information during generation? What are the limitations?

- Concept: Zero-shot vs. few-shot vs. instruction-tuned evaluation
  - Why needed here: The paper shows dramatically different results across these three regimes. Understanding what each evaluation mode tests helps interpret whether failures reflect capability gaps or training coverage gaps.
  - Quick check question: If a model fails at zero-shot subtraction but succeeds with few-shot examples, what does this suggest about the nature of the failure?

## Architecture Onboarding

- Component map:
  - Pretrained base models: Gemma-2 (9B, 27B), Llama-3 (8B, 70B), OLMo-2 (13B, 32B), Qwen3 (8B, 14B)
  - Instruction-tuned variants: Corresponding fine-tuned versions of each family
  - Tokenization layer: Each model has different single-token numeric ranges
  - Probing infrastructure: Linear classifiers trained on final-layer activations for binary sign prediction

- Critical path:
  1. Generate balanced subtraction datasets within tokenizer ranges
  2. Run zero-shot inference with greedy decoding across multiple prompt variants
  3. Extract numerical answers and compute accuracy with and without sign consideration
  4. Train linear probes on hidden states to verify internal sign encoding
  5. Compare pretrained vs. instruction-tuned vs. few-shot conditions

- Design tradeoffs:
  - Single-token vs. multi-token operands: Paper focuses primarily on single-token for controlled analysis
  - Prompt standardization: Five prompt variants reduce spurious correlation risk but add complexity
  - Pretrained-only evaluation: Excludes closed-source models since their pretrained versions are unavailable

- Failure signatures:
  - "Correct magnitude, wrong sign": Model outputs |a−b| instead of a−b when a<b
  - Asymmetric operand handling: Near-perfect performance on a>b, near-zero on a<b for same model
  - Few-shot inconsistency: Some models improve with examples, others show no gain or degradation

- First 3 experiments:
  1. Replicate the sign-omission pattern: Test a pretrained LLM on 100 a>b and 100 a<b single-token subtraction problems
  2. Probe internal representations: Extract final-layer hidden states for both operand orderings, train a linear probe to predict sign
  3. Test instruction-tuning effect systematically: Compare pretrained and instruction-tuned variants on identical a<b test sets

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Architectural generalizability: Study focuses exclusively on autoregressive transformer-based LLMs, limiting applicability to other architectures
- Probing interpretation ambiguity: High probe accuracy doesn't definitively prove the model "knows" the sign in a meaningful computational sense
- Data distribution constraints: All experiments use single-token operands within tokenizer-specific ranges, may not generalize to multi-digit arithmetic

## Confidence
- High Confidence: Subtraction accuracy lags behind addition across all tested LLMs; magnitude computation remains functional even when sign generation fails; instruction-tuned models show dramatic improvement on negative results
- Medium Confidence: The representation-to-generation disconnect explains the sign-omission failure mode
- Low Confidence: The claim that instruction-tuning primarily works by exposing models to negative subtraction examples

## Next Checks
1. **Layer-wise Probing Analysis**: Train linear probes on hidden states from multiple layers to determine whether sign information emerges early in the network or only appears in final representations

2. **Targeted Intervention Experiment**: Implement a simple post-hoc fix that explicitly checks the sign prediction from the probe and prepends a negative sign when appropriate

3. **Zero-Shot Alternative Prompting**: Test whether radically different prompting strategies (e.g., chain-of-thought, step-by-step decomposition, or explicit sign rules) can eliminate the sign-omission pattern in pretrained models