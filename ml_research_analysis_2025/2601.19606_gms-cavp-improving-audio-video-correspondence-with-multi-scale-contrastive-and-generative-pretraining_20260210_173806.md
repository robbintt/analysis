---
ver: rpa2
title: 'GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive
  and Generative Pretraining'
arxiv_id: '2601.19606'
source_url: https://arxiv.org/abs/2601.19606
tags:
- audio
- multi-scale
- generation
- video
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GMS-CAVP, a framework that combines multi-scale
  contrastive and generative pretraining to improve audio-video correspondence. The
  key idea is to use hierarchical spatial-temporal alignment and diffusion-based generation
  to capture fine-grained dependencies and enable modality translation between video
  and audio.
---

# GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining

## Quick Facts
- arXiv ID: 2601.19606
- Source URL: https://arxiv.org/abs/2601.19606
- Reference count: 0
- Primary result: State-of-the-art video-to-audio generation (KLD 1.63, FAD 0.75, Align Acc 95.87) and retrieval (R@1 28.90, R@5 43.70, R@10 57.90)

## Executive Summary
GMS-CAVP introduces a unified framework that combines multi-scale contrastive and generative pretraining to improve audio-video correspondence. The key innovation is using hierarchical spatial-temporal alignment and diffusion-based generation to capture fine-grained dependencies and enable modality translation between video and audio. Experiments on VGGSound, AudioSet, and Panda70M demonstrate state-of-the-art performance in both video-to-audio generation and retrieval tasks, outperforming prior methods like Diff-Foley and V ATT.

## Method Summary
GMS-CAVP employs Multi-scale Spatial-temporal Alignment (MSA) with hierarchical contrastive learning and Multi-scale Spatial-temporal Diffusion (MSD) for conditioned generation. The method uses pretrained video and audio encoders to extract features, which are then decomposed into L scales via temporal pyramidal pooling and multi-resolution convolutions. MSA applies InfoNCE contrastive loss at each scale independently, while MSD uses a hierarchical diffusion decoder to progressively denoise audio representations conditioned on multi-scale video features. The unified discriminative-generative formulation facilitates deeper cross-modal understanding and enables both retrieval and synthesis tasks.

## Key Results
- Video-to-audio generation: KLD 1.63, FAD 0.75, Align Acc 95.87 (outperforms Diff-Foley and V ATT)
- Video-audio retrieval: R@1 28.90, R@5 43.70, R@10 57.90 on VGGSound
- Combined training shows synergistic gains: MSA+MSD achieves substantially better results than either component alone

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale Spatial-temporal Alignment (MSA)
Hierarchical contrastive learning captures fine-to-coarse audio-video correspondences that single-scale approaches miss. The method decomposes video and audio features into L scales via temporal pyramidal pooling and multi-resolution convolutions, applying InfoNCE contrastive loss at each scale independently with adaptive temporal attention weighting to emphasize salient moments.

### Mechanism 2: Multi-scale Spatial-temporal Diffusion (MSD)
Diffusion-based generation conditioned on hierarchical video features bridges the modality translation gap that pure contrastive learning cannot. The hierarchical diffusion decoder progressively denoises audio representations while conditioning on multi-scale video features at each timestep, enabling high-fidelity audio synthesis from video inputs.

### Mechanism 3: Unified Discriminative-Generative Training
Joint optimization of contrastive and generative objectives creates mutually reinforcing representations for both retrieval and generation. The combined loss L_total = L_MSA + L_MSD ensures that representations optimal for discrimination also provide good conditioning for generation, creating a unified framework for cross-modal understanding.

## Foundational Learning

- **InfoNCE Contrastive Loss**: Core objective for aligning video-audio pairs at each scale. Without understanding positive/negative pair construction, the multi-scale extension is opaque. Quick check: Given a batch of N video-audio pairs, can you explain which pairs are positives and how the loss maximizes their similarity relative to all other combinations?

- **Denoising Diffusion Probabilistic Models**: MSD builds on standard diffusion but adds hierarchical video conditioning. Understanding forward/reverse processes is prerequisite. Quick check: Can you sketch the forward noise schedule and reverse denoising process, and explain where conditioning features are injected?

- **Feature Pyramid / Multi-Scale Representations**: The paper's core contribution assumes you understand why multi-scale helps capture hierarchical structure. Quick check: How does temporal pyramidal pooling differ from simply resizing inputs, and what inductive bias does it introduce?

## Architecture Onboarding

- **Component map**: Video V → f_v (video encoder) → F_v → Multi-scale decomposition → {F_v^1, ..., F_v^L}; Audio A → f_a (audio encoder) → F_a → Multi-scale decomposition → {F_a^1, ..., F_a^L}; [MSA Path] Each (F_v^l, F_a^l) → InfoNCE → L_MSA (sum over scales); [MSD Path] F_v^multi → Diffusion decoder → p_θ(A|V) → L_MSD

- **Critical path**: 1) Load pre-trained video/audio encoders (likely ViT-based for video, audio CNN/Transformer for audio based on related works); 2) Add multi-scale decomposition heads (temporal pooling + convolutions); 3) Train MSA first until convergence; 4) Add MSD diffusion decoder, train jointly

- **Design tradeoffs**: Number of scales L (more scales capture finer granularity but increase memory/compute); Diffusion steps T (more steps improve quality but slow inference); Dataset combination (VGGS