---
ver: rpa2
title: Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress
  Disorders
arxiv_id: '2510.20239'
source_url: https://arxiv.org/abs/2510.20239
tags:
- ptsd
- depression
- fusion
- text
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tri-modal fusion framework for joint, severity-aware
  diagnosis of depression and PTSD. It combines sentence-level transformer embeddings
  from interview text, log-Mel acoustic statistics with deltas, and facial descriptors
  from OpenFace to predict graded severity for PHQ-8 depression (5 classes) and PTSD
  (3 classes).
---

# Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders

## Quick Facts
- **arXiv ID:** 2510.20239
- **Source URL:** https://arxiv.org/abs/2510.20239
- **Reference count:** 40
- **Primary result:** Multi-task severity-aware diagnosis of depression and PTSD using tri-modal fusion outperforms unimodal baselines in accuracy, weighted F1, and decision-curve utility.

## Executive Summary
This paper presents a tri-modal fusion framework for joint severity-aware diagnosis of depression and PTSD from clinical interviews. The approach combines text embeddings from MPNet, log-Mel acoustic features with deltas, and facial descriptors from OpenFace, using standardized fusion and calibrated late fusion classification. The model predicts graded severity for PHQ-8 depression (5 classes) and PTSD (3 classes), achieving strong performance while demonstrating robustness under missing modalities. Ablation analysis reveals text contributes most to depression severity, while audio and facial cues are critical for PTSD differentiation.

## Method Summary
The framework extracts sentence-level transformer embeddings from interview text (768-D), log-Mel acoustic statistics with deltas (256-D), and facial descriptors from OpenFace (512-D), concatenates them into a 1,536-D vector after standardization, and uses XGBoost with `multi:softprob` for calibrated probability outputs. Separate classification heads handle PHQ-8 depression (5 classes) and PTSD (3 classes), with class imbalance addressed via inverse-frequency weighting. The system is trained and evaluated using stratified 5-fold cross-validation at the participant level.

## Key Results
- Tri-modal fusion achieves higher accuracy and weighted F1 than unimodal baselines for both depression and PTSD severity classification
- Text modality contributes most to depression severity detection, while audio and facial cues are critical for PTSD differentiation
- Late fusion provides robustness under missing or degraded modalities while maintaining diagnostic accuracy
- Errors cluster between adjacent severity classes, with extreme classes reliably detected
- Decision-curve analysis shows improved net benefit compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Late fusion with calibrated probabilities provides robustness under missing or degraded modalities while maintaining diagnostic accuracy.
- **Mechanism:** Each modality is encoded independently, standardized on training data, then concatenated into a 1,536-D vector. XGBoost with `multi:softprob` outputs calibrated class probabilities per disorder. If one stream is unreliable, others can dominate the decision without retraining.
- **Core assumption:** Modalities provide partially redundant diagnostic information; when one fails, others compensate.
- **Evidence anchors:**
  - [abstract] "The fused model matches the strongest unimodal baseline on accuracy and weighted F1, while improving decision curve utility and robustness under noisy or missing modalities."
  - [section III.E] "Late fusion offers two practical advantages: (i) it tolerates heterogeneous noise/missingness—if one stream is unreliable for a participant, the others can dominate the decision; (ii) it produces well-behaved probabilities."
- **Break condition:** If modalities are highly correlated (near-redundant), fusion gains diminish; if one modality dominates completely, removing it will collapse performance.

### Mechanism 2
- **Claim:** Text embeddings carry the strongest signal for depression severity; audio and facial cues are disproportionately important for PTSD differentiation.
- **Mechanism:** Sentence-transformer embeddings (MPNet) capture linguistic markers like negative affect, self-referential pronouns, and reduced lexical diversity—known depression correlates. PTSD, characterized by arousal-driven symptoms, relies more on prosodic cues (hesitation, jitter from log-Mel deltas) and facial expressivity (OpenFace AUs, gaze patterns).
- **Core assumption:** Linguistic vs. paralinguistic markers differentially map to disorder-specific symptom profiles.
- **Evidence anchors:**
  - [abstract] "Ablations show text contributes most to depression severity, audio–facial cues are critical for PTSD, whereas attributions align with linguistic and behavioral markers."
  - [section IV.F] "Removing TEXT causes the only material degradation across tasks (DEP: 0.852→0.830; PTSD: 0.854→0.819)"; audio/face removal yields changes within CI bounds but supports clinical robustness.
- **Break condition:** If text quality is severely degraded (ASR errors, short responses), audio/face must compensate; if PTSD presentations are alexithymic or muted, audio/face signal weakens.

### Mechanism 3
- **Claim:** Multi-task severity prediction with shared fused representation enables cross-disorder generalization while preserving disorder-specific decision boundaries.
- **Mechanism:** The same 1,536-D fused vector feeds two separate XGBoost heads (5-way PHQ-8, 3-way PTSD). Shared representation captures comorbid symptom overlap (e.g., anhedonia), while task-specific heads learn distinct severity mappings.
- **Core assumption:** Depression and PTSD share latent behavioral features that benefit from joint modeling; their severity gradients are not fully independent.
- **Evidence anchors:**
  - [abstract] "Our unified tri modal affective severity framework [...] to output graded severities for diagnosing both depression (PHQ-8; 5 classes) and PTSD (3 classes)."
  - [section III.G] "We train separate heads for PHQ-8 (5-way) and PTSD (3-way), enabling disorder-specific decision boundaries while sharing the same fused representation."
- **Break condition:** If disorders have orthogonal symptom structures in a population, shared representation may add noise; single-task models would then outperform multi-task.

## Foundational Learning

- **Concept: Log-Mel spectrograms with deltas**
  - Why needed here: Audio features summarize prosody and energy dynamics; deltas capture temporal change (jitter, hesitation) critical for PTSD arousal markers.
  - Quick check question: Can you explain why mean+std pooling over time preserves prosodic patterns while discarding frame-level noise?

- **Concept: Late fusion vs. early fusion trade-offs**
  - Why needed here: Late fusion tolerates missing modalities at inference; early fusion requires all streams present and aligned.
  - Quick check question: What happens to prediction confidence if video frames are dropped during inference in a late-fusion vs. early-fusion system?

- **Concept: Calibrated probabilities and decision-curve analysis**
  - Why needed here: Clinical thresholds (e.g., PHQ-8 ≥10 for moderate depression) require well-calibrated probabilities, not just rank ordering.
  - Quick check question: If a model outputs 0.7 probability for "severe depression," what does calibration guarantee about the true positive rate at that threshold?

## Architecture Onboarding

- **Component map:**
  ```
  [Interview data] → [Preprocessing: remove interviewer turns, VAD]
      ↓
  [Text: MPNet sentence embeddings → 768-D mean-pooled]
  [Audio: log-Mel stats + deltas → 256-D]
  [Face: OpenFace AUs/gaze/pose stats → 512-D]
      ↓
  [StandardScaler per fold] → [Concatenate → 1,536-D]
      ↓
  [XGBoost multi:softprob] → [Depression head: 5 classes] / [PTSD head: 3 classes]
      ↓
  [Calibrated probabilities] → [SHAP attributions]
  ```

- **Critical path:**
  1. **Preprocessing correctness:** Filter to patient speech only; incorrect filtering contaminates embeddings with interviewer language.
  2. **Standardization without leakage:** Fit `StandardScaler` on training fold only, apply to validation—leakage inflates metrics.
  3. **Stratified participant-level splits:** Never split by utterance/session; must split by participant to avoid data leakage.

- **Design tradeoffs:**
  - Late fusion over end-to-end transformers: sacrifices some temporal modeling for robustness and reproducibility.
  - Statistical pooling over sequence models: reduces overfitting on limited clinical data but loses fine-grained dynamics.
  - XGBoost over neural classifiers: provides native feature attributions (SHAP) and handles class imbalance via weighted loss.

- **Failure signatures:**
  - Middle-severity confusion (mild vs. moderate): expected; label noise and symptom overlap make adjacent classes hard to separate.
  - Face-only baseline near random (~40% accuracy): indicates facial features alone are insufficient; multimodal fusion required.
  - Large gap between training and validation accuracy: suggests overfitting—reduce tree depth or increase regularization.

- **First 3 experiments:**
  1. **Unimodal baseline sweep:** Run text-only, audio-only, face-only with identical cross-validation. Verify text dominates depression, audio/face contribute to PTSD. Check CI overlap.
  2. **Missing-modality robustness test:** Zero out one modality at inference (e.g., set face vector to zeros) and measure accuracy drop. Confirm graceful degradation per Figure 7.
  3. **Calibration audit:** Generate reliability diagrams for both heads. If ECE > 0.1, apply temperature scaling or Platt scaling and re-evaluate decision-curve net benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can ordinal regression objectives (e.g., cumulative link losses) significantly reduce the confusion observed between adjacent severity tiers compared to standard cross-entropy?
- **Basis in paper:** [explicit] Section V.A notes that the label structure is ordinal and motivates "ordinal objectives... to reduce mid-tier confusions" found in the current results.
- **Why unresolved:** The current system uses a multi-class classifier (`multi:softprob`) which treats severity levels as independent categories, failing to enforce the inherent order of PHQ-8 or PTSD scales.
- **What evidence would resolve it:** A comparative study showing reduced errors between adjacent classes (e.g., Mild vs. Moderate) using ordinal losses versus the baseline on the DAIC datasets.

### Open Question 2
- **Question:** Do architectures with disorder-conditional gating or modality-aware attention outperform the current static late-fusion approach?
- **Basis in paper:** [explicit] Section V.A suggests that "future architectures should explicitly encode disorder–modality priors (e.g., modality-aware gating...)" because ablation shows text dominates depression while audio/face dominate PTSD.
- **Why unresolved:** The current standardized fusion treats modalities as interchangeable inputs, potentially diluting the specific signal-to-noise ratio required for each distinct disorder.
- **What evidence would resolve it:** Ablation studies on DAIC-WOZ/E-DAIC comparing the performance of dynamic gating mechanisms against the static concatenation used in the paper.

### Open Question 3
- **Question:** How does the calibrated severity framework generalize to external sites with diverse demographics, recording equipment, and interview protocols?
- **Basis in paper:** [inferred] Section V.A lists "cross-site, cross-microphone, and demographically stratified evaluation" as a priority, while Section II.C highlights that current datasets are "limited in scope" and lack diversity.
- **Why unresolved:** The model is validated exclusively on DAIC-derived corpora, which share similar interview structures and populations, leaving robustness to domain shift untested.
- **What evidence would resolve it:** Evaluation of the pre-trained model on unseen, multi-centric clinical datasets (e.g., different hospitals or cultural contexts) without retraining.

## Limitations

- **Population generalizability:** Findings are based on a single pooled DAIC-WOZ/E-DAIC dataset without explicit stratification by trauma type, cultural background, or language.
- **Calibration validation:** While calibration is mentioned, reliability diagrams or Expected Calibration Error (ECE) metrics are not reported, leaving calibration claims unverified.
- **Missing-modality testing:** The claim of robustness under missing modalities is conceptually justified but lacks direct experimental validation through systematic ablation.

## Confidence

- **High confidence:** Core architecture and feature extraction pipeline are well-specified and reproducible. Reported improvements in accuracy, weighted F1, and decision-curve net benefit over baselines are directly supported by ablation experiments.
- **Medium confidence:** Claims about modality-specific contributions (text for depression, audio/face for PTSD) are supported by ablation results and align with clinical knowledge, but would benefit from population-stratified analysis to confirm robustness.
- **Low confidence:** Claim of robust performance under missing or degraded modalities is only conceptually justified, not empirically tested. Lack of explicit calibration metrics means calibration claims are inferred rather than demonstrated.

## Next Checks

1. **Population-stratified ablation:** Re-run the modality ablation analysis stratified by trauma type (e.g., accident vs. assault) and demographic subgroups to confirm that text dominates depression severity and audio/face drives PTSD detection across diverse populations.
2. **Missing-modality inference test:** At inference, systematically zero out one modality (text, audio, or face) for each test fold and measure accuracy/weighted F1 drops for both PHQ-8 and PTSD. Confirm graceful degradation per the paper's claim of robustness.
3. **Calibration audit:** Generate reliability diagrams and compute Expected Calibration Error (ECE) for both depression and PTSD heads. If ECE > 0.1, apply temperature scaling and re-evaluate decision-curve net benefit to ensure clinical thresholds are well-calibrated.