---
ver: rpa2
title: Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response
  Generation in LLMs
arxiv_id: '2507.16951'
source_url: https://arxiv.org/abs/2507.16951
tags:
- salu
- unanswerable
- questions
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliable response generation
  in conversational information retrieval systems, specifically focusing on handling
  unanswerable questions to prevent hallucination and misinformation. The authors
  propose SALU (Self-Aware LLM for Unanswerability), a novel approach that integrates
  unanswerability detection directly within the LLM's generative process using a multi-task
  learning framework for both QA and abstention generation.
---

# Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs

## Quick Facts
- arXiv ID: 2507.16951
- Source URL: https://arxiv.org/abs/2507.16951
- Reference count: 32
- Key outcome: SALU achieves 90.8% overall accuracy with 1.3% hallucination rate by integrating unanswerability detection into LLM generative process

## Executive Summary
This paper addresses the critical challenge of reliable response generation in conversational information retrieval systems by preventing hallucination on unanswerable questions. The authors propose SALU, a novel approach that integrates unanswerability detection directly within the LLM's generative process using multi-task learning for both QA and abstention generation. Crucially, SALU incorporates confidence-score-guided reinforcement learning with human feedback that explicitly penalizes hallucinated responses and rewards appropriate abstentions. Experiments on a custom Chinese C-IR_Answerability dataset demonstrate SALU consistently outperforms strong baselines, achieving higher accuracy for correctly answering or abstaining from questions while dramatically reducing hallucination to 1.3% compared to 8.9% without RLHF.

## Method Summary
SALU employs a two-phase training approach. First, a multi-task supervised fine-tuning phase jointly trains the LLM on both standard question answering and explicit abstention generation for unanswerable queries using a combined loss function. Second, a confidence-score-guided reinforcement learning with human feedback (RLHF) phase optimizes the model using PPO, where rewards are augmented based on the model's confidence scores - boosting rewards for confident correct abstentions while heavily penalizing confident hallucinations. This end-to-end integration eliminates the inconsistency between separate detection and generation modules, allowing the model to develop intrinsic self-awareness of its knowledge boundaries.

## Key Results
- SALU achieves 90.8% overall accuracy compared to 84.7% for hybrid baseline systems
- Unanswerability detection F1 improves to 0.931 from 0.901 in hybrid approaches
- Hallucination rate drops dramatically from 8.9% to 1.3% through RLHF
- Optimal performance achieved at 50% unanswerable ratio in training data

## Why This Works (Mechanism)

### Mechanism 1
Joint training on answerable and unanswerable examples creates internal discriminative signals that outperform external classifiers. The multi-task loss forces the model to learn when to generate answers versus abstention phrases from the same parameter space, eliminating disconnect between detection and generation. Core assumption: the model can recognize information absence in context through supervised exposure to negative examples. Evidence shows multi-granular approaches are promising but external classifier integration remains inconsistent. Break condition: if training data is heavily imbalanced toward answerable questions (>70% answerable), discriminative signal degrades.

### Mechanism 2
Confidence-score-guided RLHF explicitly shapes the model's uncertainty calibration, penalizing confident hallucinations more severely. The reward function is augmented with confidence score computed from token log-probabilities. For unanswerable questions, correct abstention receives boosted reward when confidence is high; hallucinated answers receive amplified penalties proportional to misplaced confidence. Core assumption: token-level log-probabilities correlate meaningfully with epistemic uncertainty. Evidence suggests internal representations encode answerability signals. Break condition: if confidence scores become decoupled from actual correctness, reward shaping backfires.

### Mechanism 3
End-to-end integration eliminates inference-time inconsistency between detection and generation modules. During inference, SALU generates either an answer or the predefined abstention phrase in a single forward pass, removing failure mode where separate classifier and generative model disagree. Core assumption: generative distribution implicitly encodes answerability decision boundary that can be reliably thresholded via exact phrase matching. Evidence shows hybrid approach achieves 0.901 unanswerability F1 vs. SALU's 0.931. Break condition: if abstention phrase appears in legitimate answer contexts or model learns near-miss variations that bypass exact matching.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: RLHF phase is core to SALU's hallucination reduction (1.3% vs. 8.9% without RLHF). Understanding reward model training, PPO optimization, and KL-divergence regularization is essential.
  - Quick check question: Can you explain why PPO uses a clipping parameter ε and KL penalty term γ in the objective function?

- **Concept: Multi-Task Learning with Loss Weighting**
  - Why needed here: SALU's supervised phase jointly optimizes LQA and LNA with weights α and β. Table IV shows performance is sensitive to task balance—50% NA ratio is optimal.
  - Quick check question: What happens to Answerable QA F1 if you weight the abstention task too heavily (β >> α)?

- **Concept: Uncertainty Calibration in Neural Networks**
  - Why needed here: Confidence-score mechanism assumes log-probabilities reflect genuine uncertainty. Without calibration, high-confidence hallucinations would be over-penalized or under-penalized incorrectly.
  - Quick check question: Would temperature scaling before computing confidence scores S(Y|X) improve or degrade the reward shaping mechanism?

## Architecture Onboarding

- **Component map:** Base LLM (M) -> Reward Model (R) -> Value Function (V) -> Confidence Scorer -> Input Formatter

- **Critical path:** 1. SFT Phase: Multi-task training on balanced QA + NA examples → produces initial policy πold 2. Reward Model Training: Human preference data → reward model R 3. RLHF Phase: PPO optimization with confidence-augmented rewards → final policy πθ 4. Inference: Single forward pass → check if output == RNA → abstain or return answer

- **Design tradeoffs:** Latency vs. reliability: SALU achieves 485ms inference vs. 530ms for hybrid while improving accuracy; Abstention conservativeness vs. answer coverage: SALU's failures are primarily over-abstention, not hallucination—a safer but potentially less helpful tradeoff; Data balance: 50% NA ratio maximizes overall accuracy but slight QA F1 degradation.

- **Failure signatures:** Subtle unanswerability: Questions that appear answerable superficially but lack specifics—SALU may still hallucinate; Over-abstention: Highly implicit answers trigger incorrect abstention; Phrase collision: If RNA tokens appear in valid answers, false abstentions occur; Confidence miscalibration: If S(Y|X) doesn't reflect true uncertainty, reward shaping degrades.

- **First 3 experiments:** 1. Ablation: SFT-only vs. SFT+RLHF. Replicate Table V on your domain—measure hallucination rate reduction. 2. NA ratio sweep. Replicate Table IV on your data distribution. Optimal balance may differ by domain. 3. Confidence threshold analysis. Plot hallucination rate vs. abstention rate at different confidence score thresholds.

## Open Questions the Paper Calls Out

- **Can the SALU framework be adapted to generate context-aware, dynamic abstention responses rather than a fixed phrase?** The Conclusion states, "Future work will explore dynamic abstention phrases," acknowledging the current limitation of using a predefined response ($R_{NA}$). Evidence would require modification of training objective to allow variable abstention generation, evaluated through human ratings of conversational naturalness.

- **Does the unanswerability detection capability learned via RLHF transfer effectively to cross-lingual contexts without extensive retraining?** Authors explicitly list "adaptation to cross-lingual contexts" as a direction for future work. Evidence would require zero-shot or few-shot evaluation of the model's accuracy on standard English unanswerable QA benchmarks (e.g., SQuAD 2.0).

- **How can the trade-off between unanswerability detection and the quality of answerable generation be further minimized?** Table IV shows increasing unanswerable training examples lowers Answerable QA F1 score (from 0.845 to 0.820), suggesting "catastrophic forgetting" of generation skills. Evidence would require architectural changes or regularization techniques that maintain Answerable QA F1 stability even as negative example ratio increases.

## Limitations

- The confidence-score-guided RLHF mechanism assumes token-level log-probabilities meaningfully reflect epistemic uncertainty, but calibration across different question types or domains isn't validated.
- Performance sensitivity to data distribution (imbalanced NA ratios degrade performance) isn't fully characterized beyond the 20% vs. 50% comparison.
- Human evaluation methodology lacks key details about rater selection, inter-rater reliability, and specific evaluation criteria, making reliability claims difficult to assess.

## Confidence

- **High confidence:** Overall architecture design (multi-task SFT + RLHF) is technically sound; quantitative results showing improved accuracy (0.908 vs. 0.847) and reduced hallucination (1.3% vs. 8.9%) are well-supported.
- **Medium confidence:** Confidence-score-guided reward shaping effectiveness relies on unproven assumptions about log-probability calibration; ablation showing RLHF's contribution doesn't isolate whether confidence scoring specifically matters.
- **Low confidence:** Generalizability claims beyond Chinese C-IR dataset are weakly supported; optimal 50% NA ratio may not transfer to datasets with different answerability distributions.

## Next Checks

1. **Confidence calibration validation:** Plot calibration curves (expected vs. actual accuracy) at different confidence thresholds across answerable and unanswerable question types to validate whether confidence-score mechanism reflects epistemic uncertainty.

2. **Cross-domain robustness test:** Apply SALU to different question-answering domain (e.g., medical or technical QA) with different NA ratio distribution to measure whether optimal α/β weights and 50% NA ratio assumption still hold.

3. **Phrase collision analysis:** Systematically test whether abstention phrase RNA appears in legitimate answer contexts within dataset; measure false abstention rate and evaluate whether probabilistic abstention mechanism would be more robust than exact phrase matching.