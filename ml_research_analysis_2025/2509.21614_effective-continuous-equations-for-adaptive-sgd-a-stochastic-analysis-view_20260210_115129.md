---
ver: rpa2
title: 'Effective continuous equations for adaptive SGD: a stochastic analysis view'
arxiv_id: '2509.21614'
source_url: https://arxiv.org/abs/2509.21614
tags:
- theorem
- dynamics
- stochastic
- continuous
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives effective continuous stochastic differential
  equations (SDEs) for adaptive stochastic gradient descent (SGD) methods, specifically
  RMSprop and Adam, in the small learning rate regime using the stochastic modified
  equations framework. The authors identify that sampling-induced noise in adaptive
  methods manifests in the limit as independent Brownian motions driving both the
  parameter and gradient second momentum evolutions, revealing an additional layer
  of stochasticity not present in vanilla SGD.
---

# Effective continuous equations for adaptive SGD: a stochastic analysis view

## Quick Facts
- **arXiv ID**: 2509.21614
- **Source URL**: https://arxiv.org/abs/2509.21614
- **Reference count**: 40
- **Primary result**: Derives effective continuous stochastic differential equations (SDEs) for RMSprop and Adam in the small learning rate regime using stochastic modified equations framework.

## Executive Summary
This paper develops continuous stochastic differential equation (SDE) approximations for adaptive stochastic gradient descent methods (RMSprop and Adam) by applying the stochastic modified equations framework in the small learning rate regime. The authors derive order-1 and order-2 continuous approximations that capture the inherent stochasticity in adaptive methods, revealing that the sampling-induced noise manifests as independent Brownian motions driving both parameter evolution and gradient second-moment updates. The key insight is that these equations reveal an additional layer of stochasticity beyond vanilla SGD, arising from fluctuations in the adaptive learning rate mechanism itself.

## Method Summary
The authors employ stochastic modified equations (SMEs) to derive continuous approximations of discrete adaptive optimizers. Starting from the discrete update rules of RMSprop and Adam, they perform Taylor expansions and stochastic calculus to obtain continuous SDEs. The analysis identifies scaling rules between learning rate and hyperparameters that determine the limiting dynamics, distinguishing between "ballistic" and "batch-equivalent" regimes. The order-2 continuous equations include additional stochastic terms capturing fluctuations in the adaptive preconditioner, while the order-1 equations represent deterministic approximations. Numerical validation is performed on a synthetic quadratic problem with Gaussian data, comparing discrete optimizer trajectories against simulated continuous dynamics using Euler-Maruyama integration.

## Key Results
- Order-2 continuous approximations significantly improve accuracy over order-1 approximations, particularly in noise-driven regimes
- Adaptive methods exhibit two distinct types of fluctuations: gradient estimation noise and fluctuations in the quadratic variation process affecting the adaptive learning rate
- The scaling rules between learning rate and hyperparameters determine which limiting dynamics emerge in the continuous approximation
- Numerical experiments validate theoretical predictions, showing second-order approximations maintain low error throughout dynamics while first-order deterministic dynamics struggle

## Why This Works (Mechanism)
The stochastic modified equations framework captures the continuous-time limit of discrete adaptive optimizers by accounting for the accumulation of stochastic gradients over time. In the small learning rate regime, the discrete updates can be approximated by continuous processes where the accumulated gradients form a Brownian motion. For adaptive methods, the key mechanism is that the adaptive preconditioner itself evolves stochastically, driven by an independent Brownian motion. This creates a coupling between the parameter dynamics and the preconditioner dynamics that is not present in vanilla SGD. The order-2 approximation captures the second-order effects of this coupling, including the stochastic fluctuations in the quadratic variation process that governs the adaptive learning rate.

## Foundational Learning
- **Stochastic Modified Equations (SMEs)**: Continuous-time approximations of discrete optimization algorithms using stochastic calculus; needed to analyze the limiting behavior of adaptive methods in the small learning rate regime
- **Quick check**: Verify that the continuous SDE reduces to the discrete update when integrated with Euler-Maruyama method
- **Brownian Motion in Optimization**: Random process representing accumulated gradient noise; needed to model the stochastic nature of gradient estimation
- **Quick check**: Confirm that the variance of the Brownian increment matches the gradient noise covariance
- **Quadratic Variation**: Measure of accumulated variance in stochastic processes; needed to capture fluctuations in the adaptive preconditioner
- **Quick check**: Verify that the quadratic variation term correctly scales with the learning rate and hyperparameters
- **Order of Approximation**: Different levels of accuracy in continuous approximations; needed to balance computational tractability with accuracy
- **Quick check**: Compare weak error between order-1 and order-2 approximations across different timescales
- **Scaling Rules**: Relationships between learning rate and hyperparameters determining limiting dynamics; needed to identify which continuous dynamics emerge in different regimes
- **Quick check**: Verify that the continuous dynamics match the discrete optimizer's behavior under the specified scaling rules

## Architecture Onboarding

**Component Map**: Discrete Adaptive Optimizer -> Taylor Expansion -> Stochastic Calculus -> Continuous SDE -> Numerical Integration -> Validation

**Critical Path**: Discrete update rule → Taylor expansion → Stochastic calculus derivation → Continuous SDE formulation → Order-1 and order-2 approximations → Scaling rule identification → Numerical validation

**Design Tradeoffs**: Order-1 approximations are computationally simpler but miss crucial stochastic effects; order-2 approximations capture more accurate dynamics but require more complex numerical integration. The choice between "ballistic" and "batch-equivalent" scaling rules affects which limiting dynamics emerge.

**Failure Signatures**: Divergence in numerical integration when regularization fails; high variance in weak error estimates indicating insufficient Monte Carlo samples; mismatch between discrete and continuous dynamics suggesting incorrect scaling rules or initialization.

**First Experiments**:
1. Implement and compare order-1 vs order-2 continuous dynamics for RMSprop on the quadratic problem
2. Vary the scaling rule (ballistic vs batch-equivalent) and observe which dynamics emerge
3. Test the sensitivity of the continuous approximation to the regularization threshold c=τ

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis restricted to quadratic objective functions, limiting generalizability to complex non-convex landscapes
- Regularization of the preconditioner in continuous SDEs is a mathematical artifact not present in discrete algorithms
- Assumes small learning rate regime where continuous approximations are valid
- The specific form of the Hessian matrix H is not fully specified, affecting reproducibility of numerical results

## Confidence
- **High confidence**: Theoretical derivation of scaling rules and order-2 SME framework
- **Medium confidence**: Numerical validation on synthetic quadratic problem
- **Low confidence**: Generalizability to non-quadratic or highly non-convex landscapes

## Next Checks
1. Reproduce with varied H: Generate multiple instances of the Hessian H with different eigenvalue spectra and verify qualitative improvement of order-2 over order-1 approximations
2. Test on non-quadratic objective: Apply SME framework to a simple non-quadratic objective to assess robustness of order-2 approximation
3. Study regularization impact: Perform sensitivity analysis on regularization function φ(x) to quantify its effect on continuous dynamics compared to discrete adaptive method