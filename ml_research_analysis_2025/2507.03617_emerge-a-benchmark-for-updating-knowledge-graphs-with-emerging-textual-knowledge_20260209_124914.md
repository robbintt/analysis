---
ver: rpa2
title: 'EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge'
arxiv_id: '2507.03617'
source_url: https://arxiv.org/abs/2507.03617
tags:
- knowledge
- triples
- entities
- text
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EMERGE is a new benchmark for evaluating how well models can update
  knowledge graphs based on new information from text. It connects emerging textual
  knowledge from Wikipedia to specific changes in Wikidata snapshots over time, supporting
  five update operations: detecting existing triples, adding new ones between known
  entities, adding ones with new entities, linking new entities to existing ones,
  and deprecating outdated triples.'
---

# EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge

## Quick Facts
- arXiv ID: 2507.03617
- Source URL: https://arxiv.org/abs/2507.03617
- Reference count: 40
- A benchmark connecting Wikipedia text to Wikidata edits for evaluating KG update operations

## Executive Summary
EMERGE addresses the challenge of updating knowledge graphs based on new information from text by creating a benchmark that links emerging textual knowledge from Wikipedia to specific changes in Wikidata snapshots over time. The benchmark supports five update operations: detecting existing triples, adding new ones between known entities, adding ones with new entities, linking new entities to existing ones, and deprecating outdated triples. Experiments with two state-of-the-art IE models reveal that while they can extract triples involving existing entities, they struggle significantly with incorporating emerging entities or deprecating outdated information, highlighting a critical gap in current IE capabilities for dynamic KG updating.

## Method Summary
The method constructs EMERGE by extracting weekly deltas from Wikidata snapshots (2019-2025) and aligning them to Wikipedia passages created in corresponding time windows. The pipeline uses distant supervision for initial alignment, followed by LLM-based curation (Llama-3.1-8B) to filter misaligned pairs. The benchmark includes 376K passages paired with 1.25M edits across 10 snapshots. Evaluation uses Recall for closed IE models and a Completeness Score (cosine similarity ≥0.9) for generative models, addressing the open-world nature of KG updating where extracted triples may not yet exist in ground truth.

## Key Results
- State-of-the-art IE models show high performance on detecting existing triples (X-Triples) but near-zero on emerging entity integration (EE-Triples) and triple deprecation (D-Triples)
- The benchmark reveals a fundamental gap in current IE models' ability to handle dynamic KG updates
- EMERGE provides the first comprehensive evaluation framework for text-driven KG updating with temporal awareness

## Why This Works (Mechanism)

### Mechanism 1: State-Conditional Operation Taxonomy
The paper defines five TKGU operations (X-Triples, E-Triples, EE-Triples, EE-KG-Triples, D-Triples) that force models to differentiate between existing and emerging knowledge based on the current KG snapshot. This taxonomy enables precise evaluation of whether models can identify novel updates versus common knowledge. The assumption is that KG state at time t is sufficient for determining operation validity. Break condition occurs when KG snapshots contain errors or are incomplete, making the E-Triples/EE-Triples distinction noisy.

### Mechanism 2: Time-Aligned Delta Curation Pipeline
High-quality alignment is achieved by coupling distant supervision (hyperlinks) with LLM-based verification using Llama-3.1-8B. This pipeline filters pairs where text doesn't semantically support KG edits, removing ~30% of initial alignments. The core assumption is that Wikipedia and Wikidata timestamps sufficiently correlate to identify emerging knowledge. Break condition occurs when temporal windows are misaligned, causing valid context to be missed or alignment to become too noisy.

### Mechanism 3: Completeness Metric for Open-World Evaluation
Standard Precision/Recall is insufficient because valid triples may not yet exist in ground truth KG. The Completeness Score (cosine similarity) provides semantic matching credit for valid extractions even without exact ID grounding. The assumption is that semantic embeddings adequately proxy factual equivalence. Break condition occurs when embeddings conflate distinct relations or when models hallucinate plausible but false facts.

## Foundational Learning

- **Knowledge Graph Snapshots (G_t)**: Required to determine if a triple is emerging or existing. Without a fixed reference snapshot, the benchmark cannot distinguish between novel updates and common knowledge. Quick check: Given a KG snapshot from 2020 and text from 2021, how would you classify a triple present in both?

- **Distant Supervision**: Used for initial text-triple alignment by assuming sentences containing entity hyperlinks describe relations involving those entities. Quick check: Why might distant supervision introduce noise when aligning text passages to KG triples?

- **Closed vs. Open Information Extraction**: Closed IE (ReLiK) is restricted to known entities while Open IE (EDC) can generate new entities. This distinction explains performance gaps on EE-Triples. Quick check: Which paradigm is better suited for discovering new entities not yet in the KG?

## Architecture Onboarding

- **Component map**: Text Passage (d_t') + Reference KG Snapshot (G_t) -> Alignment Module -> Curation Module (LLM filter) -> Inference Model (ReLiK/EDC) -> Set of Operations and Triples -> Evaluator (Completeness Score/Recall)

- **Critical path**: The Curation Module is critical for dataset quality. If filtering fails, benchmark contains misaligned pairs invalidating evaluations.

- **Design tradeoffs**: ReLiK (span-based) offers high speed but cannot handle EE-Triples or D-Triples. EDC+ (LLM-based) handles generative extraction and deprecation but suffers from precision/grounding issues and computational overhead.

- **Failure signatures**: Low Completeness Scores indicate embedding or grounding issues. High X-Triple / Low E-Triple performance suggests mere extraction of common knowledge. Zero D-Triple detection reveals inability to recognize outdated information.

- **First 3 experiments**: 1) Baseline Retrieval: Compare ReLiK performance with/without G_t context on X-Triples/E-Triples. 2) Generative Deprecation: Test EDC+ specifically on D-Triples to analyze prompt sensitivity. 3) Delta Window Analysis: Vary time window Δ to measure impact on EE-Triples performance.

## Open Questions the Paper Calls Out

1. **Leveraging KG Structure and Temporal Dynamics**: How can IE models effectively use KG internal structure and temporal patterns to identify emerging knowledge? Current models lack access to KG content and structure, presenting a promising research direction.

2. **Temporal Discrepancy Impact**: To what extent does the lag between Wikipedia knowledge appearance and Wikidata integration affect TKGU evaluation? The paper acknowledges this temporal misalignment and plans to investigate it more thoroughly.

3. **Lightweight Extractive Models**: Can efficient, non-generative models be developed to handle emerging entity integration and triple deprecation? The current performance gap suggests a research opportunity for such models.

## Limitations

- **Temporal Alignment Uncertainty**: Knowledge often appears in Wikipedia before Wikidata, potentially creating alignment errors that could artificially affect model performance.
- **LLM-Based Curation Dependency**: Dataset quality heavily relies on LLM filtering without systematic validation of potential biases or false removals.
- **Evaluation Metric Sensitivity**: The 0.9 cosine similarity threshold is arbitrary, with no sensitivity analysis showing how performance varies across different values.

## Confidence

**High Confidence**: The core claim that existing IE models struggle with emerging entities and deprecation is well-supported by experimental results showing near-zero performance on EE-Triples and D-Triples.

**Medium Confidence**: The dataset construction methodology is detailed but relies on pipeline steps that introduce potential noise sources not fully quantified.

**Low Confidence**: The assertion that EMERGE comprehensively covers all necessary KG update operations, given acknowledged limitations in capturing temporal dynamics and lack of explicit quality validation.

## Next Checks

1. **Temporal Gap Analysis**: Measure model performance on EE-Triples and D-Triples as a function of time gap between Wikipedia text creation and Wikidata snapshot to quantify temporal misalignment impact.

2. **Curation Quality Validation**: Implement human evaluation on a random sample of 100 curated text-triple pairs to assess false positive/negative rates in LLM-based filtering.

3. **Threshold Sensitivity Testing**: Evaluate model rankings and absolute performance across Completeness Score thresholds (0.8, 0.85, 0.9, 0.95) to determine evaluation methodology robustness.