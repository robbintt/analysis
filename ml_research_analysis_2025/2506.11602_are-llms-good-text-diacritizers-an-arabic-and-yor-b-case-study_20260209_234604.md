---
ver: rpa2
title: "Are LLMs Good Text Diacritizers? An Arabic and Yor\xF9b\xE1 Case Study"
arxiv_id: '2506.11602'
source_url: https://arxiv.org/abs/2506.11602
tags:
- zhang
- diacritization
- wang
- llms
- yoruba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of large language models
  (LLMs) for text diacritization in Arabic and Yoruba, two languages where diacritics
  are essential for meaning. The authors introduce a novel multilingual dataset, MultiDiac,
  specifically designed to evaluate diacritization by including diverse samples with
  varying linguistic ambiguity to minimize overlap with LLM pre-training data.
---

# Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study

## Quick Facts
- arXiv ID: 2506.11602
- Source URL: https://arxiv.org/abs/2506.11602
- Authors: Hawau Olamide Toyin; Samar M. Magdy; Hanan Aldarmaki
- Reference count: 9
- Many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yorùbá, with Grok-3 achieving the strongest performance on Arabic and GPT-4o excelling in Yorùbá.

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively perform text diacritization for Arabic and Yorùbá, two languages where diacritics are crucial for meaning. The authors introduce MultiDiac, a novel multilingual dataset designed to evaluate diacritization with diverse samples and minimal pre-training overlap. They evaluate 14 LLMs against 6 specialized diacritization models and fine-tune four small open-source models using LoRA for Yorùbá. Results show that many LLMs outperform specialized models without any fine-tuning, with Grok-3 and GPT-4o achieving the best results for Arabic and Yorùbá respectively. Arabic diacritization performance is consistently stronger across both open and closed models. Fine-tuning notably improved Yorùbá performance and reduced hallucination rates in smaller models, demonstrating the benefits of task adaptation in low-resource settings.

## Method Summary
The study evaluates 14 LLMs and 6 specialized diacritization models on Arabic and Yorùbá text diacritization using the MultiDiac dataset. The dataset contains 562 Yorùbá training samples, 41 dev samples, and 101 test samples, plus 106 Arabic test samples. Models are evaluated using Character Error Rate (CER), Word Error Rate (WER), and hallucination WER (WER computed after removing diacritics from predictions). LoRA fine-tuning is applied to four small open-source models for Yorùbá with rank=16, alpha=32, and dropout=0.05, targeting projection and feedforward layers. Zero-shot evaluation uses simple prompts: "Add diacritics and accent marks to this Yorùbá text" or "Add diacritics to this Arabic text."

## Key Results
- Many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yorùbá without any fine-tuning
- Grok-3 achieves the best overall performance for Arabic, while GPT-4o excels in Yorùbá diacritization
- Arabic diacritization performance is consistently stronger across both open and closed models compared to Yorùbá
- Fine-tuning with LoRA substantially reduces hallucination rates and improves accuracy for low-resource languages like Yorùbá
- Smaller models suffer from hallucinations, generating content that alters the underlying undiacritized text, but fine-tuning can help reduce this issue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large general-purpose LLMs can outperform specialized diacritization models through contextual reasoning, without task-specific training.
- Mechanism: Models with extensive multilingual pre-training internalize orthographic patterns and context-dependent disambiguation rules. When presented with undiacritized text, they leverage surrounding context to select the appropriate diacritized form (e.g., distinguishing "Ajé" /wealth/ from "Àjé" /witch/ based on sentence semantics).
- Core assumption: The contextual representations learned during pre-training transfer to fine-grained orthographic tasks without explicit supervision.
- Evidence anchors:
  - [abstract] "many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba"
  - [section 5] "Grok-3 achieves the best overall performance... Large multilingual LMs outperform both specialized and language-specific models"
  - [corpus] Related work on Arabic diacritization (Sadeed, Proper Noun Diacritization) confirms ongoing interest in both specialized and LLM-based approaches, but limited direct comparison studies exist.
- Break condition: Performance degrades when models lack sufficient pre-training exposure to the target language (e.g., LLaMA-3.2-1B "often generates output in other language characters" for Yorùbá).

### Mechanism 2
- Claim: Smaller LLMs exhibit higher hallucination rates, generating content that alters the underlying undiacritized text.
- Mechanism: Lower-capacity models lack robust cross-lingual alignment, leading to translation, transliteration, or unrelated generation instead of faithful diacritization. This manifests as high WER even after diacritics are stripped.
- Core assumption: Hallucination correlates with model capacity and language exposure during pre-training.
- Evidence anchors:
  - [abstract] "smaller models suffer from hallucinations"
  - [section 6] "LLaMA-3.2-1B and Qwen2.5-7B demonstrate severe hallucination, generating highly inaccurate outputs with extremely high error rates"
  - [corpus] No direct corpus evidence on hallucination in diacritization; related work focuses on accuracy metrics rather than faithfulness.
- Break condition: Hallucination is not uniform—Arabic-centric small models (ALLam-7b) hallucinate less than general-purpose small models, suggesting language-specific training mitigates the issue.

### Mechanism 3
- Claim: LoRA fine-tuning on small datasets substantially reduces hallucination and improves diacritization accuracy for low-resource languages.
- Mechanism: Parameter-efficient adaptation focuses gradient updates on task-relevant projections, steering the model toward faithful diacritization without full fine-tuning. The small training set (562 Yorùbá samples) appears sufficient to shift behavior.
- Core assumption: The task-specific signal generalizes from limited examples when model capacity is appropriately constrained.
- Evidence anchors:
  - [abstract] "Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates"
  - [section 6] "LLaMA-3.2-1B showed the most dramatic reduction in hallucinations, with WER dropping from 729.90 to 45.77"
  - [corpus] Corpus does not provide comparative LoRA studies for diacritization; evidence is paper-internal.
- Break condition: Fine-tuning effects are inconsistent across architectures—Phi-4 WER increased slightly post fine-tuning, suggesting hyperparameter sensitivity.

## Foundational Learning

- **Character Error Rate (CER) vs. Word Error Rate (WER)**
  - Why needed here: CER captures fine-grained diacritic accuracy (single-character differences), while WER reflects semantic impact (entire words affected). Hallucination uses WER on stripped text to measure faithfulness.
  - Quick check question: If a model outputs "Àjé" instead of "Ajé" (one diacritic difference), does CER or WER increase more?

- **Diacritization Ambiguity**
  - Why needed here: Many base words have multiple valid diacritizations with different meanings. Models must use context to select correctly—this is why the MultiDiac dataset includes "contextually rich sentences."
  - Quick check question: Given "Aje wa" without diacritics, what two meanings are possible in Yorùbá?

- **Hallucination in Generative Models**
  - Why needed here: Unlike specialized encoder-decoder models (T5-based diacritizers), autoregressive LLMs may generate content unrelated to input. This is measured by computing WER after removing all diacritics.
  - Quick check question: A model receives "ktb" and outputs "the book was written"—is this correct diacritization or hallucination?

## Architecture Onboarding

- **Component map:**
  - Input: Undiacritized text (Arabic or Yorùbá)
  - Prompt template: "Add diacritics to this [language] text" (+ "Don't number the texts or add extra characters" for chat interfaces)
  - Model: LLM (decoder-only) or specialized encoder-decoder (T5-based)
  - Output: Diacritized text
  - Evaluation: CER/WER against gold reference; hallucination WER on stripped text

- **Critical path:**
  1. Prompt engineering to prevent formatting artifacts
  2. Output normalization (strip numbering, extra whitespace)
  3. Strip diacritics from both prediction and reference for hallucination scoring
  4. Compute edit distances at character and word levels

- **Design tradeoffs:**
  - Large closed-source models (Grok, GPT-4o): Best accuracy, minimal hallucination, but no local deployment, cost per inference
  - Small open-source models + LoRA: Deployable, lower cost, but require fine-tuning data and still underperform large models
  - Specialized models (CATT, T5-based): Efficient, no hallucination by design, but lower accuracy and language-specific

- **Failure signatures:**
  - Model outputs translation/transliteration instead of diacritization (Jais WER 171.25 for Arabic)
  - Model generates wrong script entirely (LLaMA-3.2-1B for Yorùbá)
  - Diacritics correct but underlying characters altered (hallucination WER > 10%)

- **First 3 experiments:**
  1. Benchmark a large multilingual LLM (GPT-4o or equivalent) on MultiDiac test sets for both languages to establish upper bound.
  2. Fine-tune a small open-source model (LLaMA-3.2-1B or Gemma-7B) with LoRA using the 562-sample Yorùbá training set; measure CER/WER reduction vs. zero-shot.
  3. Implement hallucination detection by computing WER on diacritic-stripped outputs; compare across model sizes to validate correlation between capacity and faithfulness.

## Open Questions the Paper Calls Out
- Can LLMs be effectively utilized for data augmentation to improve the performance of specialized diacritization models? The authors state in the Limitations that their results should motivate improvements in specialized models, "potentially using LLMs for data augmentation."
- How do the efficiency and latency of specialized models compare to general LLMs when deployed in pipeline systems? The authors note that specialized models "can be more efficient and thus more suitable as a pre-processing step in pipeline systems," contrasting with the resource intensity of LLMs.
- Can advanced fine-tuning techniques beyond LoRA fully resolve the hallucination issues observed in small language models for low-resource languages? While LoRA fine-tuning reduced hallucinations, the authors report that smaller models "still suffer from hallucinations" and specialized T5 models for Yorùbá exhibited "severe hallucinations."

## Limitations
- Evaluation datasets are relatively small (562 Yorùbá training samples, 101 test samples; 106 Arabic test samples), raising questions about robustness across broader linguistic variation.
- Hallucination detection via WER on stripped text doesn't capture semantic drift—a model could output grammatically correct but contextually inappropriate diacritization that passes this test.
- The LoRA fine-tuning benefits are shown only for Yorùbá, with limited analysis of why certain architectures (like Phi-4) show minimal improvement.
- The Arabic performance advantage across models suggests possible dataset bias toward Arabic orthographic patterns in the test set construction.

## Confidence
- **High confidence**: LLMs outperform specialized diacritization models for both languages in zero-shot settings. This claim is well-supported by direct quantitative comparisons showing CER/WER improvements across multiple models and languages.
- **Medium confidence**: Fine-tuning with LoRA substantially reduces hallucination and improves accuracy for low-resource languages. While the Yorùbá results are compelling, the evidence is limited to one language and the inconsistent effects across architectures suggest hyperparameters may significantly impact outcomes.
- **Low confidence**: The contextual reasoning mechanism underlying LLM success in diacritization is well-understood. The paper assumes that multilingual pre-training transfers to orthographic tasks, but doesn't provide ablation studies or probe representations to confirm this mechanism.

## Next Checks
1. **Dataset bias validation**: Evaluate the same LLMs on a held-out Arabic subset with balanced ambiguous diacritization cases versus unambiguous ones. If Arabic performance advantage disappears for ambiguous cases, it suggests the MultiDiac test set construction may favor Arabic orthographic patterns.

2. **Fine-tuning hyperparameter sensitivity**: Systematically vary LoRA rank (8, 16, 32), learning rate (1e-4, 2e-4, 5e-4), and epochs (1-5) on Yorùbá validation set to identify optimal configurations and assess whether current settings are near-optimal or could yield substantially better results.

3. **Hallucination mechanism probing**: For models showing high hallucination rates, conduct targeted experiments where inputs are deliberately ambiguous base words. Analyze whether hallucinations correlate with specific linguistic patterns to distinguish between true hallucination and reasonable ambiguity resolution.