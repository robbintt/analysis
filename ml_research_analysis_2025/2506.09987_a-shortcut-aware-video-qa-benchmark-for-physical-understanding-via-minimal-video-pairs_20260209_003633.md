---
ver: rpa2
title: A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal
  Video Pairs
arxiv_id: '2506.09987'
source_url: https://arxiv.org/abs/2506.09987
tags:
- video
- answer
- arxiv
- question
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MVP, a shortcut-aware video QA benchmark\
  \ designed to assess physical world understanding in video-language models. The\
  \ benchmark addresses score inflation issues in existing datasets by using minimal-change\
  \ video pairs\u2014visually similar videos with identical questions but opposing\
  \ answers."
---

# A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs

## Quick Facts
- arXiv ID: 2506.09987
- Source URL: https://arxiv.org/abs/2506.09987
- Reference count: 40
- Best open-source video-language model achieves only 40.2% accuracy, highlighting significant gap in physical understanding capabilities compared to humans (92.9%)

## Executive Summary
This paper introduces MVP, a shortcut-aware video QA benchmark designed to assess physical world understanding in video-language models. The benchmark addresses score inflation issues in existing datasets by using minimal-change video pairs—visually similar videos with identical questions but opposing answers. To achieve correct answers, models must process both videos in a pair independently, penalizing reliance on superficial visual or textual cues. MVP comprises 55K high-quality examples from diverse sources, including egocentric videos, robotic interactions, and cognitive science benchmarks. Human performance on MVP is 92.9%, while the best open-source video-language model achieves only 40.2%, highlighting a significant gap in physical understanding capabilities compared to humans. The findings suggest current models still struggle with seemingly simple physical reasoning tasks, despite strong performance on standard benchmarks.

## Method Summary
MVP is constructed through automated mining of existing video datasets. The process involves filtering 9 diverse datasets, formatting them as question-answer pairs, mining minimal-change video pairs using ViCLIP embedding similarity and symbolic metadata rules, and applying single-frame filtering with an ensemble of 5 VLMs. The benchmark comprises 54,828 examples covering 10 physical understanding categories, including intuitive physics, state reasoning, and object permanence. Pairs are filtered to ensure visual similarity (0.9 threshold for Language Table) and answer opposition through entailment detection.

## Key Results
- Human performance on MVP is 92.9%, while the best open-source video-language model achieves only 40.2%
- Models relying on shortcuts score near-random (25%) on MVP's binary-choice minimal-pair format
- Single-frame filtering further drops accuracy 2.2% (27.3% → 25.1%), with larger drops in temporal reasoning categories
- Intuitive physics and state reasoning show near-zero performance across models despite being fundamental physical concepts

## Why This Works (Mechanism)

### Mechanism 1: Minimal-Change Pair Scoring Penalizes Shortcut Reliance
- **Claim:** Requiring models to correctly answer both videos in a minimally-different pair with opposing answers exposes superficial cue reliance.
- **Mechanism:** When two videos share visual/textual cues but require different answers, models relying on those cues produce identical outputs for both, scoring zero. Only models that process distinguishing features succeed.
- **Core assumption:** True physical understanding requires sensitivity to fine-grained differences that superficial cues cannot resolve.
- **Evidence anchors:**
  - [abstract] "models that solely rely on visual or textual biases would achieve below random performance"
  - [Table 5] Random video pairing yields 45.4% accuracy; minimal-change pairing drops this to 27.3% (18.1 point reduction)
  - [corpus] CausalVQA (arXiv:2506.09943) similarly probes causal understanding via counterfactual questioning
- **Break condition:** If models develop shortcuts that distinguish paired videos without genuine understanding (e.g., exploiting compression artifacts from different video sources), the metric becomes gameable.

### Mechanism 2: Multi-Model Ensemble Filtering Removes Single-Frame-Solvable Examples
- **Claim:** Filtering examples solvable by any randomly-sampled frame encourages genuine temporal processing.
- **Mechanism:** Five VLMs attempt to answer each question given a single frame. If ≥4/5 succeed on ≥30% of frames in both paired videos, the pair is discarded—ensuring remaining examples require temporal integration.
- **Core assumption:** Tasks solvable from single frames don't test spatiotemporal understanding.
- **Evidence anchors:**
  - [Section 3] "If at least 4 out of 5 models in the ensemble predict the correct answer... the minimal-change pair is then discarded if 30% of the frames in both videos are deemed solvable"
  - [Table 5] Single-frame filtering further drops accuracy 2.2% (27.3% → 25.1%), with larger drops in temporal reasoning categories
  - [corpus] Weak corpus evidence—related benchmarks don't systematically filter single-frame bias
- **Break condition:** If key-frame identification itself requires temporal reasoning (as authors acknowledge in Section 8), filtering with random frames may retain examples where temporal reasoning is needed to find the informative frame.

### Mechanism 3: Visual Similarity Mining Creates High-Conflict Test Cases
- **Claim:** ViCLIP embedding similarity combined with symbolic metadata rules efficiently mines challenging minimal pairs from existing datasets.
- **Mechanism:** Candidate pairs are ranked by cosine similarity in the ViCLIP embedding space after filtering via metadata (shared objects, antonymous actions, mutually exclusive answers via entailment detection).
- **Core assumption:** Embedding similarity correlates with perceptual similarity; symbolic rules ensure answer contradiction.
- **Evidence anchors:**
  - [Section 3] "we rank video pairs by their cosine similarity in the ViCLIP video embedding space"
  - [Appendix B.2] Detailed pairing rules per dataset (e.g., Something-Something v2: 82/174 action types have well-defined antonyms)
  - [corpus] TimeBlind (arXiv:2602.00288) addresses compositional spatio-temporal understanding but uses different methodology
- **Break condition:** Embedding similarity may not capture semantic similarity for physics-relevant features (object permanence violations vs. plausible motion may embed similarly).

## Foundational Learning

- **Concept: Contrast Sets / Minimal Pairs**
  - **Why needed here:** MVP adapts the minimal-pair paradigm from NLP (Winograd Schema, WSC) and image-VLM evaluation (Winoground) to video QA.
  - **Quick check question:** If a model scores 50% on a binary-choice minimal-pair benchmark where random is 25%, is it relying on shortcuts?

- **Concept: Language Bias in VQA**
  - **Why needed here:** Table 1 shows Llama3-70B achieves 78% on Action Antonym without seeing the video—priors like "books fall like rocks" are sufficient.
  - **Quick check question:** Why might a language-only model outperform random on a visual QA task?

- **Concept: Single-Frame / Bag-of-Frames Bias**
  - **Why needed here:** Prior work (Buch et al. 2022, Cores et al. 2024) shows video models often ignore temporal ordering; MVP's filtering addresses this.
  - **Quick check question:** If shuffling video frames doesn't change a model's accuracy, what does this imply about its temporal reasoning?

## Architecture Onboarding

- **Component map:** 9 datasets → Manual categorical filtering → QA formatting → Pair mining → Single-frame filtering → MVP (54,828 examples)
- **Critical path:** Metadata grouping → Entailment detection → ViCLIP similarity ranking → Top-k selection
- **Design tradeoffs:**
  - Scalability vs. precision: Automated mining enables 55K examples but introduces noise (authors note ~small subset ambiguous/too easy)
  - Evaluation format: Multiple-choice (A/B) enables automated scoring but prevents chain-of-thought reasoning that might improve performance
  - Video sources: Diverse (egocentric, robotic, synthetic physics) but limits cultural/domain knowledge tasks to reduce language bias—sacrifices some realism
- **Failure signatures:**
  - Models scoring ~25% (random) likely relying entirely on shortcuts
  - High performance on one category but near-random on others suggests dataset-specific overfitting, not general physical understanding
  - Intuitive physics near 0% across models (Table 4) indicates this subcategory may need investigation for annotation quality
- **First 3 experiments:**
  1. Establish baselines: Run single-frame and language-only models on MVP to verify shortcut mitigation (target: <30% accuracy)
  2. Category breakdown: Evaluate which MVP subcategories show above-random performance for your model—prioritize those for improvement
  3. Ablate filtering: Compare model performance with/without single-frame filtering to confirm the filtering step affects your architecture (if no difference, your model may not leverage temporal information effectively)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does allowing for free-form Chain-of-Thought (CoT) reasoning significantly improve model performance on the MVP benchmark compared to the forced single-letter output format?
- Basis in paper: [explicit] The authors state in the Limitations section: "it is possible that forcing the model to output a single letter without any room for free-form reasoning (CoT) limits its performance."
- Why unresolved: The evaluation protocol defined in the paper explicitly forces models to output a single letter (A or B) via specific prompts and regex extraction, preventing the analysis of reasoning steps that might correct initial visual misinterpretations.
- What evidence would resolve it: A comparative evaluation of state-of-the-art VideoLLMs on MVP using a CoT prompting strategy (allowing the model to "think" before answering) versus the standard direct prompting method used in the paper.

### Open Question 2
- Question: To what extent does the density of frame sampling (frame rate) affect a model's ability to resolve the minimal differences in video pairs?
- Basis in paper: [inferred] In Appendix F ("Behind the Scenes"), the authors note that "Frame rate plays a big role for solving many examples in MVP" and that "many models may not have access to this short span in principle as they represent a video as 16 uniformly sampled frames."
- Why unresolved: The paper evaluates models using standard configurations, which often rely on uniform sampling of a small number of frames (e.g., 16). The specific sensitivity of the "minimal-change" detection capability to temporal resolution is not isolated or tested in the main results.
- What evidence would resolve it: An ablation study measuring model performance on MVP while systematically varying the number of sampled frames (e.g., 8 vs. 32 vs. 64 frames) and the sampling strategy (uniform vs. scene-aware).

### Open Question 3
- Question: What specific training data compositions or learning criteria are required to bridge the gap between current VideoLLM performance (~40%) and human performance (~93%) on physical understanding tasks?
- Basis in paper: [explicit] The Discussion section concludes that current models are far from human performance and "calling for more research in this direction to develop better training data for world modelling, as well as novel learning criteria and model architectures."
- Why unresolved: While the paper identifies the performance gap and the failure of current models, it does not propose or validate specific training methodologies (beyond standard pre-training) that specifically target the intuitive physics and state reasoning capabilities tested in MVP.
- What evidence would resolve it: Training new VideoLLM variants on datasets specifically curated for physical interactions (e.g., simulated robotics or causal video data) and measuring the resulting performance delta on the MVP benchmark.

### Open Question 4
- Question: Does high performance on the MVP benchmark correlate with improved success rates in real-world embodied AI tasks, such as robotic manipulation?
- Basis in paper: [inferred] The Introduction frames the benchmark around "abilities essential for an agent to interact within the physical world" and includes "robotic interaction data," but the evaluation remains strictly passive video QA. The assumption that video QA performance translates to agentic capability is unstated.
- Why unresolved: The paper validates physical *understanding* via visual question answering but does not test if this understanding translates to physical *interaction* or control, leaving the link between the benchmark and robotics applications unproven.
- What evidence would resolve it: A correlation analysis comparing the MVP scores of various vision-language-policy models against their success rates on a standardized set of real-world or simulated robotic manipulation tasks.

## Limitations
- Automated mining process introduces potential noise with some examples possibly ambiguous or too easy
- 0.9 visual similarity threshold for Language Table pairs may be overly restrictive
- Binary-choice format prevents chain-of-thought reasoning that might improve model performance

## Confidence
- **High confidence:** The general framework of using minimal-change pairs to expose shortcut reliance is well-established in related work (Winoground, CausalVQA). The 18.1 point accuracy drop from random to minimal-change pairing strongly supports the shortcut detection mechanism.
- **Medium confidence:** The effectiveness of single-frame filtering in requiring genuine temporal reasoning is supported by the 2.2 point accuracy reduction, though the relationship between random frame sampling and temporal understanding warrants further validation.
- **Medium confidence:** The embedding similarity mining approach appears sound methodologically, but the correlation between ViCLIP similarity and semantic similarity for physics-relevant features needs empirical verification.

## Next Checks
1. Filter Ablation Study: Run MVP with and without the single-frame filtering step to quantify its impact on your specific model architecture. If no significant difference is observed, this suggests your model doesn't leverage temporal information effectively or that the filtering threshold needs adjustment.

2. Pair Quality Analysis: Manually examine 50 randomly selected minimal-change pairs to verify the entailment detection and similarity ranking are producing high-quality test cases. Document any systematic failures in the mining pipeline.

3. Category-Specific Investigation: For categories where your model performs near-random (particularly intuitive physics), conduct a detailed error analysis to determine whether failures stem from annotation quality issues, inherent task difficulty, or model limitations in physical reasoning.