---
ver: rpa2
title: Improving Video Question Answering through query-based frame selection
arxiv_id: '2601.07459'
source_url: https://arxiv.org/abs/2601.07459
tags:
- video
- frames
- selection
- frame
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of uniform frame sampling
  in video question answering (VideoQA) by introducing a query-based frame selection
  method using submodular mutual information (SMI) functions. The approach leverages
  the diminishing returns property of submodular functions to select frames that are
  both diverse and relevant to the given query, improving the quality of visual context
  for large vision-language models.
---

# Improving Video Question Answering through query-based frame selection

## Quick Facts
- **arXiv ID:** 2601.07459
- **Source URL:** https://arxiv.org/abs/2601.07459
- **Reference count:** 38
- **Primary result:** Query-based frame selection using submodular mutual information (SMI) functions improves VideoQA accuracy by up to 4% over uniform sampling on MVBench dataset

## Executive Summary
This paper addresses the inefficiency of uniform frame sampling in video question answering (VideoQA) by introducing a query-based frame selection method using submodular mutual information (SMI) functions. The approach leverages the diminishing returns property of submodular functions to select frames that are both diverse and relevant to the given query, improving the quality of visual context for large vision-language models. Experiments on the MVBench dataset with two VLMs—Video-LLaVA and LLaVA-NeXT—demonstrate up to 4% accuracy improvement over uniform sampling. The method is training-free, model-agnostic, and particularly effective for tasks that benefit from query-specific frame selection, though its performance depends on the encoder's ability to capture relevant features.

## Method Summary
The method extracts candidate frames from input videos, encodes them with CLIP ViT-B/32 along with the query using the corresponding text encoder, then applies submodular mutual information functions (FLMI or GCMI) via the SUBMODLIB library to select a subset of frames that maximizes relevance to the query while maintaining diversity. The selected frames are then passed to a VLM (Video-LLaVA-7B or LLaVA-NeXT-Video) for question answering. The approach uses greedy selection with theoretical optimality guarantees and requires no additional training, making it compatible with any VLM.

## Key Results
- Up to 4% accuracy improvement over uniform sampling baseline on MVBench dataset
- GCMI (graph cut mutual information) achieves 37.10% accuracy vs FLMI (facility location mutual information) at 35.13% on Video-LLaVA with 4 frames
- Performance gains are task-dependent, with spatial reasoning tasks benefiting more than temporal reasoning tasks
- Method is training-free and model-agnostic, compatible with any VLM architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Query-based frame selection using SMI functions improves VideoQA accuracy by selecting frames that are both relevant to the question and non-redundant.
- **Mechanism:** SMI functions measure mutual information between query embeddings and frame embeddings, optimizing for both relevance (similarity to query) and diversity (diminishing returns property prevents selecting similar frames). The greedy selection provides near-optimal subsets with theoretical guarantees.
- **Core assumption:** The visual encoder can capture query-relevant features in its embeddings.
- **Evidence anchors:**
  - [abstract] "By replacing uniform frame sampling with query-based selection, our method ensures that the chosen frames provide complementary and essential visual information"
  - [section 3.2] "The SMI function quantifies the mutual information between these two sets, enabling the selection of frames from B that best align with the content of A while maintaining diversity and coverage"
  - [corpus] Related work on moment sampling confirms query-aware selection improves long-form VideoQA, though corpus lacks direct SMI comparisons.
- **Break condition:** When encoder embeddings fail to capture task-relevant features (e.g., temporal dependencies with CLIP), the method underperforms uniform sampling.

### Mechanism 2
- **Claim:** GCMI prioritizes query relevance while FLMI balances relevance with diversity, leading to task-dependent performance differences.
- **Mechanism:** GCMI uses graph cut formulation with λ=1, maximizing direct query-frame similarity. FLMI uses facility location formulation that saturates once query requirements are met, encouraging broader coverage. This creates a spectrum from strict relevance to diversity-emphasis.
- **Core assumption:** Different VideoQA tasks benefit differently from relevance vs. diversity tradeoffs.
- **Evidence anchors:**
  - [section 3.2] "GCMI lies at one end of the spectrum favoring only query-relevance... FLMI stands at the opposite extreme, emphasizing diversity and comprehensive query coverage"
  - [table 1] GCMI achieves 37.10% vs FLMI 35.13% (Video-LLaVA, 4 frames), suggesting query-relevance dominance
  - [corpus] K-frames work confirms scene-driven selection improves long video understanding, supporting diversity benefits.
- **Break condition:** For temporally-ordered tasks (Character Order, Egocentric Navigation), diversity-focused selection may miss sequential dependencies.

### Mechanism 3
- **Claim:** Performance gains are conditional on encoder capacity—CLIP's spatial strength benefits object/attribute tasks but temporal weaknesses hurt sequence reasoning.
- **Mechanism:** SMI operates entirely on encoder embeddings. CLIP ViT-B/32 captures spatial relationships and object semantics well but lacks temporal modeling. Tasks requiring spatial reasoning (Object Existence, State Change) benefit; temporal reasoning tasks (Character Order, Egocentric Navigation) degrade.
- **Core assumption:** Encoder limitations are the primary bottleneck, not the SMI optimization itself.
- **Evidence anchors:**
  - [section 6.2.2] "the encoder used does not capture changes over time or causal relationships between frames, leading to this accuracy drop"
  - [section 7] "CLIP does not extract temporal features, so the selected frames lack temporal context... for VideoQA tasks that require temporal reasoning, especially in short videos, uniform sampling tends to outperform"
  - [corpus] Moment sampling paper addresses temporal limitations through motion features, suggesting encoder replacement paths.
- **Break condition:** When temporal reasoning is required AND video length is short (<10s), uniform sampling may capture temporal context better than query-based selection.

## Foundational Learning

- **Concept:** Submodular Functions and Diminishing Returns
  - **Why needed here:** Core mathematical property enabling diverse subset selection with greedy optimality guarantees.
  - **Quick check question:** Given frame embeddings [0.9, 0.8], [0.85, 0.75], [0.3, 0.2], which pair maximizes diversity under diminishing returns?

- **Concept:** Mutual Information in Set Functions
  - **Why needed here:** SMI extends submodular optimization to query-aware selection by measuring shared information between query and frame sets.
  - **Quick check question:** If I_f(A;B) = f(A) + f(B) - f(A∪B), what happens when A and B are completely unrelated?

- **Concept:** CLIP Joint Embedding Space
  - **Why needed here:** Enables direct similarity computation between text queries and image frames—foundation for query-based selection.
  - **Quick check question:** Why can't CLIP embeddings capture "before/after" relationships between frames?

## Architecture Onboarding

- **Component map:** Video → Frame Extraction → CLIP Vision Encoder (ViT-B/32) → Frame Embeddings → SMI Functions (FLMI/GCMI) → Selected Frames → Video-LLaVA/LLaVA-NeXT → Answer

- **Critical path:** Embedding quality → SMI selection quality → VLM context quality. The encoder is the leverage point.

- **Design tradeoffs:**
  - **GCMI vs FLMI:** GCMI for precision tasks (localization, attributes); FLMI for coverage tasks (summarization, broad queries)
  - **Frame count:** 4 frames favor efficiency; 12 frames favor accuracy but hit GPU memory limits
  - **Encoder choice:** CLIP for spatial tasks; temporal encoders needed for sequence reasoning

- **Failure signatures:**
  - Accuracy drop on Character Order, Egocentric Navigation → temporal encoder needed
  - Short video underperformance (<5s) → uniform sampling may suffice
  - Similar scores across tasks → encoder embeddings may lack discriminative power

- **First 3 experiments:**
  1. **Baseline validation:** Replicate uniform vs FLMI/GCMI on MVBench subset (5 tasks: OE, SC, CO, EN, AL) to verify 4% gain pattern and identify temporal task degradation.
  2. **Encoder ablation:** Replace CLIP ViT-B/32 with VideoMAE or ViViT encoder on failing temporal tasks (CO, EN) to test if temporal features recover performance. Assumption: This should close the gap with uniform sampling.
  3. **Frame count sensitivity:** Test 4/8/12 frames on both spatial-heavy and temporal-heavy tasks to find optimal budget per task type. Expect spatial tasks to plateau at 8 frames, temporal tasks to need 12+.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating video-specific encoders that capture temporal dependencies (e.g., VideoMAE) resolve the performance degradation seen in temporal reasoning tasks like Egocentric Navigation?
- **Basis in paper:** [Explicit] The authors conclude that "CLIP does not extract temporal features... The framework... can be replaced with encoders that capture temporal dependencies to address this limitation."
- **Why unresolved:** The current method relies on CLIP, which processes frames as static images, failing to capture the motion or causal relationships required for specific temporal tasks.
- **What evidence would resolve it:** Experiments replacing the CLIP encoder with a temporal encoder within the SMI framework, showing improved accuracy on temporal sub-tasks compared to the static baseline.

### Open Question 2
- **Question:** Is query-based selection inherently suboptimal for short-duration videos compared to uniform sampling?
- **Basis in paper:** [Inferred] The authors observe that "for VideoQA tasks that require temporal reasoning, especially in short videos (around 5 seconds), uniform sampling tends to outperform query-based selection."
- **Why unresolved:** It is unclear if this underperformance is due to the specific SMI functions struggling with dense, short sequences or if uniform sampling provides a necessary "safety net" for temporal context that query-selection discards.
- **What evidence would resolve it:** A comparative analysis on a filtered dataset of short videos (<6s) testing various SMI functions against uniform sampling to identify the specific boundary conditions where selection fails.

### Open Question 3
- **Question:** How robust is the SMI-based selection when applied to more powerful or larger vision-language encoders?
- **Basis in paper:** [Explicit] The authors state that the success of the method "is highly dependent on the encoder used to convert frames and images into embeddings."
- **Why unresolved:** The study primarily relies on CLIP ViT-B/32; it remains untested whether the "diminishing returns" property of submodular functions holds effectively when using higher-dimensional or more semantically rich embeddings from larger models.
- **What evidence would resolve it:** Benchmarking the framework with state-of-the-art foundational models (e.g., larger CLIP variants or GPT-4V based encoders) to measure if the 4% accuracy gain scales or diminishes.

## Limitations
- Encoder dependency: CLIP ViT-B/32 lacks temporal reasoning capabilities, causing accuracy drops on temporally-ordered tasks (Character Order, Egocentric Navigation)
- GPU memory constraint limits frame extraction to 14 frames maximum, insufficient for longer videos
- Empirical evaluation restricted to single dataset (MVBench), raising generalization concerns

## Confidence
**High confidence** in the core mechanism: Submodular mutual information functions demonstrably provide theoretical guarantees for diverse subset selection, and the greedy algorithm is well-established. The 4% accuracy improvement over uniform sampling is supported by the experimental results presented.

**Medium confidence** in task-dependent performance differences: While the paper shows GCMI outperforming FLMI on specific tasks, the underlying reasons (relevance vs. diversity tradeoffs) require more extensive ablation studies across diverse video domains to confirm.

**Low confidence** in encoder-agnostic claims: The performance degradation on temporal tasks reveals the method's strong dependence on encoder capabilities. The paper acknowledges this limitation but doesn't provide solutions or alternative encoder recommendations for tasks requiring temporal reasoning.

## Next Checks
1. **Encoder ablation study**: Replace CLIP ViT-B/32 with temporal-aware encoders (VideoMAE, ViViT) on temporal reasoning tasks (Character Order, Egocentric Navigation) to quantify performance recovery and identify if encoder limitations are the primary bottleneck.

2. **Cross-dataset generalization**: Evaluate the method on alternative VideoQA benchmarks (e.g., MSRVTT-QA, ActivityNet-QA) with different video characteristics and question distributions to assess performance stability across domains.

3. **Temporal feature integration**: Implement a hybrid approach combining query-based selection with motion features or optical flow embeddings to enhance temporal reasoning capabilities while maintaining the diversity benefits of SMI functions.