---
ver: rpa2
title: Identifying Key Challenges of Hardness-Based Resampling
arxiv_id: '2504.07031'
source_url: https://arxiv.org/abs/2504.07031
tags:
- hardness
- class
- samples
- data
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenge of performance gaps across
  classes in machine learning, often attributed to variations in class hardness. The
  authors explore hardness-based resampling as a potential approach to mitigate these
  disparities by oversampling hard classes and undersampling easy ones.
---

# Identifying Key Challenges of Hardness-Based Resampling

## Quick Facts
- **arXiv ID:** 2504.07031
- **Source URL:** https://arxiv.org/abs/2504.07031
- **Reference count:** 40
- **Primary result:** Hardness-based resampling does not meaningfully affect class-wise performance disparities in CIFAR-10/100 despite theoretical expectations.

## Executive Summary
This paper investigates whether hardness-based resampling can mitigate performance gaps across classes in machine learning models. The authors propose using the Area Under the Margin (AUM) hardness estimator to identify easy and hard classes, then applying oversampling to hard classes and undersampling to easy ones. Contrary to theoretical expectations, their experiments on CIFAR-10 and CIFAR-100 show that this approach fails to reduce class-wise performance disparities. The study identifies key challenges unique to hardness-based imbalance, including the lack of robust hardness measures, intricate oversampling methods, and the influence of imbalance levels. The authors also demonstrate that data pruning can introduce beneficial data imbalance that improves overall performance and reduces class gaps.

## Method Summary
The authors implement a hardness-based resampling approach using the Area Under the Margin (AUM) hardness estimator. They train an ensemble of 8 ResNet-18 models on CIFAR-10 and CIFAR-100 datasets to compute AUM scores for each instance. Classes are ranked by average AUM, and resampling ratios are calculated using scaling factors to skew the distribution. Easy classes are undersampled (removing lowest AUM samples) while hard classes are oversampled using Random, SMOTE, or weighted sampling methods. A new ensemble of 8 models is trained on these resampled datasets, and class-wise performance is compared against the baseline ensemble. The primary metric is the "recall gap" (standard deviation) between easy and hard classes.

## Key Results
- Hardness-based resampling fails to reduce class-wise performance gaps in both CIFAR-10 and CIFAR-100 datasets
- The effectiveness of hardness-based resampling is highly dependent on the level of imbalance introduced
- Data pruning experiments show that introducing controlled imbalance can improve overall performance and reduce class-wise gaps
- AUM hardness estimator shows more stable behavior compared to alternative estimators like EL2N and Forgetting

## Why This Works (Mechanism)
The paper investigates the relationship between sample hardness and model performance, exploring whether adjusting the training data distribution based on hardness can improve class-wise performance consistency. The mechanism relies on the assumption that hard samples contain more informative learning signals that should be emphasized during training.

## Foundational Learning
- **Area Under the Margin (AUM)**: A hardness estimator that measures the margin between a sample's predicted score and the next highest score across training epochs. Needed to quantify sample difficulty for resampling decisions. Quick check: Verify AUM calculation correctly tracks margins across all 200 training epochs.
- **Ensemble-based hardness estimation**: Using multiple models to compute hardness scores provides more stable estimates than single models. Quick check: Compare hardness rankings across ensemble members for consistency.
- **Resampling ratios and scaling factors**: Mathematical relationship between hardness scores and target sample counts determines how much to oversample/undersample each class. Quick check: Validate that scaling factor α correctly transforms hardness distributions into resampling ratios.
- **Class-wise performance metrics**: Recall and precision calculated per class rather than overall accuracy to capture disparities. Quick check: Ensure std dev of recall across classes (recall gap) is computed correctly.

## Architecture Onboarding

**Component Map:**
AUM Estimator -> Hardness Ranking -> Resampling Ratio Calculation -> Data Resampling -> Model Training -> Performance Evaluation

**Critical Path:**
AUM computation (ensemble of 8 ResNet-18) → Class hardness ranking → Resampling ratio calculation (using α) → Dataset creation (pruning/oversampling) → Ensemble training (8 models) → Class-wise performance comparison

**Design Tradeoffs:**
- Ensemble size (8) vs computational cost: Larger ensembles provide more stable hardness estimates but increase training time
- Resampling method (Random vs SMOTE vs weighted) vs implementation complexity: Simpler methods are easier to implement but may be less effective
- Scaling factor α vs imbalance level: Higher α creates more extreme imbalance but may hurt overall performance

**Failure Signatures:**
- High variance in resampling ratios across ensemble members indicates unstable hardness estimates
- Minimal change in recall gap between baseline and resampled experiments suggests hardness estimation issues
- Performance degradation when using extreme α values indicates oversampling/undersampling is too aggressive

**First Experiments:**
1. Validate AUM implementation by comparing hardness rankings against Figure 2's "Absolute Difference" stability metric
2. Test baseline ensemble performance on CIFAR-10/100 to establish reference recall/precision values
3. Implement simple random oversampling/undersampling before adding SMOTE complexity

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper does not specify the exact implementation details of SMOTE for image data (pixel space vs feature space)
- The specific weight initialization seeds for ensemble members are not provided, affecting reproducibility
- The results are limited to CIFAR-10 and CIFAR-100 datasets with ResNet-18 architecture

## Confidence

**High confidence:** The core finding that hardness-based resampling fails to reduce class-wise performance gaps is well-supported by the experimental results and detailed analysis of unstable hardness estimates.

**Medium confidence:** The proposed challenges (lack of robust hardness measures, intricate oversampling methods) are theoretically sound but require further empirical validation across different architectures and datasets.

**Low confidence:** The specific numerical results depend heavily on the exact implementation details of AUM calculation and SMOTE application, which are not fully specified in the paper.

## Next Checks

1. **Validate AUM implementation** by comparing hardness rankings against Figure 2's "Absolute Difference" stability metric across ensemble sizes to ensure correct margin tracking.

2. **Test SMOTE sensitivity** by implementing both pixel-space and feature-space variants to determine if the choice of SMOTE implementation affects the null result.

3. **Replicate data pruning experiments** from Section 4.2 to verify that introducing controlled imbalance can indeed improve performance and reduce class-wise gaps as claimed.