---
ver: rpa2
title: 'Semantic Retention and Extreme Compression in LLMs: Can We Have Both?'
arxiv_id: '2505.07289'
source_url: https://arxiv.org/abs/2505.07289
tags:
- compression
- quantization
- pruning
- performance
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates combining pruning and quantization for
  efficient LLM compression. It introduces a theoretical framework and novel metrics
  (SrCr) to quantify the trade-off between compression rate and semantic retention.
---

# Semantic Retention and Extreme Compression in LLMs: Can We Have Both?

## Quick Facts
- **arXiv ID**: 2505.07289
- **Source URL**: https://arxiv.org/abs/2505.07289
- **Reference count**: 40
- **Primary result**: Balanced joint compression (25% pruning + 4-bit quantization) achieves ~20% higher semantic retention than pure 3-bit quantization at equivalent compression rates.

## Executive Summary
This paper investigates combining pruning and quantization for efficient LLM compression. It introduces a theoretical framework and novel metrics (SrCr) to quantify the trade-off between compression rate and semantic retention. Experiments show that a balanced joint compression approach—specifically 25% pruning with 4-bit quantization—achieves approximately 20% higher semantic retention than pure 3-bit quantization at the same compression rate. The study also finds that semi-structured pruning patterns, particularly 2:8, maintain strong performance when combined with moderate quantization while offering potential hardware efficiency benefits. These results demonstrate that strategic combinations of pruning and quantization outperform single-method approaches at equivalent compression levels.

## Method Summary
The paper combines SparseGPT for one-shot unstructured/semi-structured pruning with GPTQ for post-training quantization, applying them sequentially to LLaMA-3.1-8B and Mistral-7B-v0.3 models. The method uses C4 dataset calibration (512 samples) and evaluates on MMLU-Pro, BBH, and MATH benchmarks using lm-evaluation-harness. Theoretical Compression Rate (TCr) enables fair comparison across different compression configurations, while Semantic Retention Compression Rate (SrCr) combines compression efficiency with task performance retention.

## Key Results
- Joint compression (25% pruning + 4-bit quantization) achieves ~20% higher semantic retention than pure 3-bit quantization at TCr=81.25%
- Semi-structured 2:8 pruning pattern maintains competitive performance with potential hardware acceleration benefits
- MATH benchmark shows extreme sensitivity to compression, with LLaMA-8B dropping from 17.7% to 0.2% at 4-bit quantization
- 2-bit quantization fails catastrophically across all configurations, producing near-random performance
- 50% pruning + 8-bit quantization underperforms pure 4-bit quantization at TCr=75%, indicating aggressive pruning dominates degradation

## Why This Works (Mechanism)

### Mechanism 1: Balanced Joint Compression Distribution
Distributing compression load across both pruning and quantization preserves more semantic capability than aggressive single-method compression at equivalent theoretical compression rates. When pruning removes 25% of weights and quantization reduces remaining weights to 4-bit (TCr = 81.25%), the information loss is spread across two complementary dimensions rather than concentrating precision loss in quantization alone.

### Mechanism 2: Sequential Approximation Validity via Bounded Error Accumulation
Applying pruning then quantization sequentially approximates true joint optimization within acceptable error bounds at moderate sparsity levels. The error ratio between quantizing all weights post-pruning versus only non-pruned weights approaches 1 as sparsity approaches zero.

### Mechanism 3: Semi-Structured Sparsity with Hardware-Aware Patterns
Semi-structured pruning patterns (particularly 2:8) maintain competitive semantic retention while enabling potential hardware acceleration benefits. N:M patterns constrain pruning to regular structures, reducing mask storage overhead and enabling hardware support.

## Foundational Learning

- **Hessian-based quantization (GPTQ)**: Understanding how second-order information enables post-training quantization by minimizing reconstruction error through layer-wise optimal weight updates.
  - Quick check: Why does GPTQ update weights in blocks of 128 columns rather than all at once?

- **N:M semi-structured sparsity**: Grasping how constraining N weights to be pruned per M consecutive weights trades theoretical compression flexibility for hardware efficiency and reduced mask overhead.
  - Quick check: What is the mask storage overhead difference between unstructured pruning at 25% sparsity versus 2:8 semi-structured pruning?

- **Perplexity limitations as compression metric**: Understanding why the paper introduces SrCr metrics—perplexity correlates poorly with actual capabilities under compression, particularly for knowledge-intensive tasks.
  - Quick check: Why might a compressed model maintain similar perplexity to its dense counterpart while showing degraded performance on MATH or BBH benchmarks?

## Architecture Onboarding

- **Component map**: SparseGPT (pruning) -> GPTQ (quantization) -> SrCr metrics (evaluation)
- **Critical path**: Apply SparseGPT pruning → obtain sparsity mask + pruned checkpoint → Apply GPTQ quantization to pruned checkpoint → quantized weights → Evaluate on benchmark suite → compute Sr and SrCr → Compare against quantization-only baseline at matching TCr
- **Design tradeoffs**: Case A (quantize all weights) vs. Case B (quantize only non-pruned): Case A trades theoretical optimality for implementation flexibility across technique combinations
- **Failure signatures**: MATH performance collapsing to near-zero at 4-bit (LLaMA: 17.7→0.2) indicates reasoning tasks are compression-sensitive; 2-bit quantization produces near-random performance across all models regardless of pruning configuration
- **First 3 experiments**: 1) Baseline characterization: Run pruning-only (25%, 33%, 50%) and quantization-only (8-bit, 4-bit, 3-bit) on target model; 2) Joint compression sweep: At TCr=81.25%, compare 25%+4-bit joint against 3-bit-only; 3) Pattern validation: Test 2:8 semi-structured pruning with 4-bit quantization against unstructured 25%+4-bit

## Open Questions the Paper Calls Out

### Open Question 1
Can unified joint compression algorithms that simultaneously optimize pruning and quantization outperform sequential approaches, and by what margin? The current work uses sequential approximation as a practical proxy, which may not capture the full potential of truly simultaneous optimization.

### Open Question 2
Do the optimal joint compression configurations (e.g., 25% pruning with 4-bit quantization) generalize to models beyond the 7-8B parameter scale tested? Information density differs across model sizes; larger models may exhibit different compression-to-retention trade-off curves.

### Open Question 3
Can the theoretical framework and SrCr metric be extended to incorporate additional compression methods such as low-rank factorization and knowledge distillation? The current TCr formula assumes sparsity and bit-width as primary compression dimensions; low-rank methods reduce parameters through different mechanisms.

### Open Question 4
What causes the dramatic MATH performance degradation under 4-bit quantization for LLaMA (0.2% vs. 17.7% baseline) while Mistral maintains reasonable performance (10.5%)? The paper documents this asymmetry but does not investigate whether it stems from architectural differences, multilingual vs. monolingual representations, or quantization algorithm interactions.

## Limitations
- Task-specific sensitivity: Reasoning-intensive tasks (MATH) are particularly vulnerable to compression, suggesting benefits may be benchmark-dependent
- Hardware acceleration gap: 2:8 semi-structured pattern shows strong retention but lacks current hardware support
- Sequential vs. joint optimization: Study uses sequential application rather than true joint optimization without direct comparison to simultaneous methods

## Confidence

**High Confidence (80-100%)**
- The 20% performance advantage of 25%+4-bit joint compression over 3-bit-only at equivalent TCr is well-supported
- The failure of 2-bit quantization across all configurations is unambiguous and consistently observed
- The observation that aggressive pruning (50%+) combined with lower-bit quantization (8-bit) underperforms pure 4-bit quantization is directly demonstrated

**Medium Confidence (40-80%)**
- The theoretical bounds justifying sequential approximation are mathematically sound but lack empirical validation against true joint optimization
- The hardware acceleration potential of 2:8 patterns is promising but contingent on future hardware developments
- The relative sensitivity of LLaMA vs. Mistral to compression is observed but lacks mechanistic explanation

**Low Confidence (0-40%)**
- Claims about general applicability to other model families beyond LLaMA/Mistral remain untested
- Specific thresholds for "balanced" vs. "aggressive" joint compression may shift with different model scales

## Next Checks

1. **True Joint Optimization Comparison**: Implement and compare against simultaneous pruning-quantization optimization methods to quantify the sequential approximation error across different sparsity levels.

2. **Hardware Performance Validation**: Benchmark the 2:8 pattern on future-generation hardware that supports it to verify the theoretical latency and memory efficiency gains versus the semantic retention cost.

3. **Task Decomposition Analysis**: Conduct fine-grained analysis of which specific MATH sub-tasks cause catastrophic failure at 4-bit quantization to identify whether certain reasoning patterns are inherently compression-sensitive.