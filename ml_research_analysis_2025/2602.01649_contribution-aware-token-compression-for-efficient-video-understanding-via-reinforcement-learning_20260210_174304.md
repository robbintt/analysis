---
ver: rpa2
title: Contribution-aware Token Compression for Efficient Video Understanding via
  Reinforcement Learning
arxiv_id: '2602.01649'
source_url: https://arxiv.org/abs/2602.01649
tags:
- video
- token
- tokens
- policy
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of video large language models
  due to redundant video tokens. It proposes a contribution-aware token compression
  algorithm (CaCoVID) that actively discovers optimal compressed token combinations
  based on their actual contribution to correct predictions, rather than relying on
  attention scores.
---

# Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.01649
- Source URL: https://arxiv.org/abs/2602.01649
- Reference count: 35
- Achieves 98.9% accuracy with 25% retention on LLaVA-OneVision-7B

## Executive Summary
This paper addresses the inefficiency of video large language models due to redundant video tokens by proposing CaCoVID, a contribution-aware token compression algorithm. Unlike attention-score-based methods, CaCoVID uses reinforcement learning to actively discover optimal compressed token combinations based on their actual contribution to correct predictions. The method introduces Online Combinatorial Space Sampling (OCSS) to efficiently explore token combinations and achieves state-of-the-art performance across diverse video understanding benchmarks while significantly reducing compression latency.

## Method Summary
CaCoVID employs a compression policy network that predicts per-token and per-frame contribution scores based on video and question tokens. During training, token combinations are sampled via OCSS, fed to a frozen LLM, and prediction correctness provides reward signals for policy optimization via group advantage computation. The method uses PPO-style clipped objectives and dynamic sample ratio adjustment to maintain stable learning. At inference, tokens are retained based on contribution scores with budget allocation across frames and inclusion of uniform spatio-temporal tokens.

## Key Results
- Achieves 98.9% accuracy with 25% retention ratio on LLaVA-OneVision-7B
- Achieves 97.5% accuracy with 25% retention ratio on Qwen2.5-VL-3B
- Outperforms state-of-the-art methods on LongVideoBench, MLVU, and VideoMME benchmarks
- Reduces compression latency while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1: Contribution-Aware Policy Learning via Prediction Feedback
The compression policy network learns to estimate token contribution more accurately than attention scores by directly optimizing for prediction correctness. During training, token combinations are sampled and fed to the frozen LLM; prediction correctness provides the reward signal for policy optimization via group advantage computation. The core assumption is that tokens consistently appearing in combinations leading to correct predictions have higher contribution that generalizes across similar video-question pairs.

### Mechanism 2: Online Combinatorial Space Sampling (OCSS)
OCSS partitions tokens by contribution scores before sampling, reducing the effective exploration space while maintaining diversity of sampled combinations. Tokens are sorted by estimated contribution and divided into l=1/(λ×r) combinatorial sub-spaces. A categorical distribution selects a sub-space proportionally to its total contribution; within the sub-space, multinomial sampling selects specific tokens. This reduces logarithmic exploration complexity by factor of 25 and improves convergence speed.

### Mechanism 3: Dynamic Sample Ratio with Experience Replay
The method adjusts token sampling ratio per sample based on reward history to improve exploration efficiency across difficulty levels. If average reward exceeds α_high (0.875), sampling ratio halves; if below α_low (0.125), ratio doubles. Each sample undergoes 5 iterations with experience replay from memory buffer. This maintains informative gradients when samples consistently achieve high or low rewards under fixed ratios.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) with clipped objective
  - Why needed here: The CPO algorithm builds on PPO-style clipping to stabilize policy updates during token selection learning
  - Quick check question: Can you explain why clipping the probability ratio prevents overly large policy updates?

- Concept: Video LLM tokenization pipeline (Vision Encoder → Projector → LLM)
  - Why needed here: CaCoVID operates on video tokens after projection but before LLM prefilling
  - Quick check question: Where does CaCoVID's policy network attach in the inference pipeline relative to the projector and LLM?

- Concept: Combinatorial vs. autoregressive exploration in RL
  - Why needed here: The paper explicitly contrasts its combinatorial token selection with autoregressive vocabulary exploration in standard LLM RL
  - Quick check question: Why does autoregressive exploration not apply to token selection, and what challenge does this create?

## Architecture Onboarding

- Component map: Compression Policy Network -> OCSS Sampler -> Frozen LLM -> Reward Evaluator -> Training Loop
- Critical path: Encode video → tokens (X_vid) → Policy network forward → contribution scores (Ŝ_t, Ŝ_f) → OCSS sampling → selected token indices → Frozen LLM inference → prediction → Reward computation → group advantage → CPO loss backprop → policy update
- Design tradeoffs:
  - λ (sub-space size factor): λ=2 balances exploration diversity vs. focus; larger λ increases ineffective combinations
  - Sampling ratio r during training: r=0.02 used; smaller ratios yield higher performance but risk missing necessary tokens
  - Single vs. multi-layer attention: Single layer sufficient; deeper networks add cost without gain
- Failure signatures:
  - Policy collapse: All tokens receive similar scores → no meaningful differentiation
  - Divergent dynamics without OCSS: Visualization shows unstable contribution maps
  - Blind-test contamination: Samples answerable without video provide zero learning signal
- First 3 experiments:
  1. Reproduce the OCSS ablation: Compare random, multinomial, and OCSS sampling on MLVU-dev with r=2% to validate exploration efficiency gains
  2. Attention baseline comparison: Implement attention-score-based selector on LLaVA-OneVision-7B setup; compare against trained policy on VideoMME splits to confirm contribution-score advantage
  3. Retention ratio sweep: Evaluate CaCoVID at r∈{10%, 15%, 20%, 25%} on held-out subset to establish accuracy-latency Pareto frontier

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the methodology and results.

## Limitations
- Reliance on exact-match rewards limits applicability to open-ended generation tasks where no single "correct" answer exists
- High training-time computational overhead from sampling multiple token groups per training step (24 groups × forward passes)
- Unclear generalizability of contribution scores across different LLM architectures

## Confidence
- High Confidence: Compression policy network architecture and PPO-style optimization framework
- Medium Confidence: Online Combinatorial Space Sampling mechanism and blind-test filtering methodology
- Low Confidence: Dynamic sample ratio adjustment mechanism and specific threshold choices

## Next Checks
1. Implement and train an attention-score-based compression policy network using the same architecture; compare contribution-aware vs attention-based compression on VideoMME splits at 25% retention ratio
2. Evaluate CaCoVID with λ∈{1, 2, 4} on held-out validation set; implement beam search baseline to test whether OCSS excludes useful cross-contribution-tier combinations
3. Test dynamic sample ratio mechanism with alternative thresholds (0.25/0.75, 0.33/0.67) and compare against fixed-ratio training; measure learning curve stability and final accuracy