---
ver: rpa2
title: Generative Sequential Notification Optimization via Multi-Objective Decision
  Transformers
arxiv_id: '2509.02458'
source_url: https://arxiv.org/abs/2509.02458
tags:
- notification
- learning
- arxiv
- decision
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Decision Transformer (DT) based framework
  for optimizing notification delivery by reframing policy learning as return-conditioned
  supervised learning. The approach uses quantile regression to model return-to-go
  distributions and employs a circular buffer for efficient sequence feature persistence
  in production.
---

# Generative Sequential Notification Optimization via Multi-Objective Decision Transformers

## Quick Facts
- arXiv ID: 2509.02458
- Source URL: https://arxiv.org/abs/2509.02458
- Reference count: 39
- Primary result: +0.72% increase in user sessions while reducing notification volume and maintaining relevance

## Executive Summary
This paper introduces a Decision Transformer (DT) based framework for optimizing notification delivery by reframing policy learning as return-conditioned supervised learning. The approach uses quantile regression to model return-to-go distributions and employs a circular buffer for efficient sequence feature persistence in production. Compared to a multi-objective CQL-based agent, the DT-based approach achieved a +0.72% increase in user sessions while reducing notification volume and maintaining notification relevance at LinkedIn. The framework demonstrates improved robustness, scalability, and modeling flexibility for real-world notification decision-making.

## Method Summary
The framework reframes sequential notification decision-making as return-conditioned supervised learning using Decision Transformers. Instead of traditional reinforcement learning, it trains a transformer model to predict actions conditioned on desired returns-to-go (RTG). The key innovation is using quantile regression to predict RTG distributions rather than single scalar values, which improves robustness during inference. The model predicts actions autoregressively while maintaining user state sequences through an efficient circular buffer mechanism. Training uses a combined loss function with cross-entropy for action prediction and pinball loss for RTG quantile prediction.

## Key Results
- Achieved +0.72% increase in user sessions compared to multi-objective CQL-based agent
- Reduced notification volume by -1.68% while maintaining stable click-through rates
- Demonstrated improved robustness and scalability for production deployment at LinkedIn

## Why This Works (Mechanism)
The approach works by reframing sequential decision-making as supervised learning conditioned on desired returns. By predicting RTG distributions through quantile regression rather than single values, the model can better handle uncertainty and provide more stable inference across different user contexts. The circular buffer efficiently maintains sequence features without expensive recomputation, enabling low-latency serving in production environments.

## Foundational Learning
- Decision Transformers (DTs): Autoregressive models that predict actions conditioned on desired returns, enabling sequence modeling without traditional RL
  - Why needed: Provides stable, scalable alternative to RL for complex sequential decision problems
  - Quick check: Verify the model can predict actions given various return prompts
- Quantile Regression: Predicts conditional quantiles of target distributions rather than point estimates
  - Why needed: Captures uncertainty and improves robustness compared to scalar RTG prediction
  - Quick check: Compare performance using different quantile sets (e.g., 0.25, 0.5, 0.75 vs single median)
- Return-to-Go (RTG): Cumulative discounted reward from current timestep to horizon
  - Why needed: Encodes long-term reward objectives for conditioning the policy
  - Quick check: Verify RTG calculation matches discounted sum of rewards over lookahead horizon

## Architecture Onboarding

Component map:
User Context + RTG Prompt -> Transformer Encoder -> Action Prediction Head + RTG Prediction Head -> Action Output

Critical path:
During inference: User context → RTG quantile input → RTG prediction → Action prediction (autoregressive)

Design tradeoffs:
- Context length T=4 vs T=8: Balance between accuracy and serving latency/cost
- Quantile regression vs scalar RTG: Improved robustness vs model complexity
- Transformer vs RNN: Better sequence modeling vs computational efficiency

Failure signatures:
- Poor generalization with arbitrary RTG prompts → Use quantile regression head to constrain prompts to learned distribution
- High serving latency with long contexts → Profile inference time; choose minimal effective context length

First experiments:
1. Implement minimal DT with quantile regression RTG prediction and validate on synthetic sequential task
2. Compare standard DT (scalar RTG) vs quantile regression DT on validation data
3. Measure inference latency across different context lengths to confirm accuracy-latency tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical architectural details (layer counts, attention heads, embedding dimensions) necessary for faithful reproduction
- Multi-objective reward vector formulation not fully specified, making evaluation setup difficult to replicate
- Offline evaluation methodology comparing against CQL-based agent lacks statistical significance testing and implementation details

## Confidence

| Claim Area | Confidence Level |
|------------|------------------|
| Core DT-based sequential optimization approach | High |
| Reported online metrics (+0.72% Sessions, -1.68% Volume) | Medium |
| Comparative advantage over CQL-based approaches | Low |

## Next Checks
1. Implement a minimal Decision Transformer with quantile regression RTG prediction and validate on a synthetic sequential decision-making task before applying to notification optimization
2. Conduct ablation studies comparing standard DT (scalar RTG) vs. quantile regression DT to verify the claimed robustness benefits in practice
3. Measure inference latency empirically across different context lengths (T=4 vs T=8) to confirm the claimed trade-off between accuracy and serving cost