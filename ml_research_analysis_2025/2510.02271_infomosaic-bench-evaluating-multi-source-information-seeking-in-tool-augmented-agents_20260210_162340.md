---
ver: rpa2
title: 'InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented
  Agents'
arxiv_id: '2510.02271'
source_url: https://arxiv.org/abs/2510.02271
tags:
- tool
- search
- tools
- information
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "InfoMosaic-Bench introduces the first benchmark for evaluating\
  \ multi-source information seeking in tool-augmented agents, addressing the limitation\
  \ of relying solely on web search. The benchmark covers six domains\u2014medicine,\
  \ finance, maps, video, web, and multi-domain integration\u2014requiring agents\
  \ to integrate domain-specific tools with general web search."
---

# InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents

## Quick Facts
- arXiv ID: 2510.02271
- Source URL: https://arxiv.org/abs/2510.02271
- Reference count: 40
- Key result: Web-only search achieves 38.2% accuracy on InfoMosaic-Bench; domain tools are inconsistent, and 22.4% of failures stem from tool misuse.

## Executive Summary
InfoMosaic-Bench is the first benchmark designed to evaluate multi-source information seeking in tool-augmented agents. Unlike prior work that relies solely on web search, this benchmark requires agents to integrate domain-specific tools (e.g., BioMCP, FMP, AMap) with general web search across six domains: medicine, finance, maps, video, web, and multi-domain integration. Using InfoMosaic-Flow, a scalable synthesis pipeline, the benchmark ensures tasks demand non-trivial multi-source reasoning by pruning tasks solvable by web search alone. Experiments with 14 state-of-the-art models show that web search is insufficient for domain-specific tasks, domain tools provide inconsistent benefits, and tool usage errors are a major failure mode.

## Method Summary
InfoMosaic-Bench consists of 621 benchmark samples across six domains, each requiring integration of general web search with domain-specific MCP tools. Tasks are synthesized using InfoMosaic-Flow, which combines an Organizer (Synthesizer/Refiner) and Workers (Executor/Verifier). Stage 1 generates initial QA tasks via domain tools; Stage 2 uses a web-only verifier to prune tasks solvable by search alone, ensuring non-triviality. Agents use the ReAct framework with OpenAI's tool-calling interface and a Python Sandbox. Evaluation uses an LLM-as-a-judge (accuracy vs. pass rate), with answers compared against ground truth.

## Key Results
- GPT-5 achieves only 38.2% accuracy and 67.5% pass rate using web-only search on InfoMosaic-Bench.
- Domain tools improve performance in some domains (Map, Video) but degrade it in others (Medical, Finance, Multi-domain).
- 22.4% of failures are due to incorrect tool usage or selection, with highest error rates in Finance and Multi-domain (largest tool sets).
- Performance plateaus after 8 tool calls, suggesting context or planning limits in the ReAct framework.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Web-only search fails for domain-specific tasks due to information insufficiency.
- Mechanism: Web content is noisy, inconsistently formatted, and lacks structured domain-specific knowledge. Tasks requiring precise facts (e.g., clinical trial data, financial filings) cannot reliably retrieve verifiable information from generic web search.
- Core assumption: Domain-specific tools provide structured, authoritative data that web search cannot easily surface.
- Evidence anchors:
  - [abstract]: "online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web"
  - [section]: "GPT-5 attains only 38.2% accuracy" with web-only search
  - [corpus]: HierSearch paper also addresses integration of local and web search; BrowseMaster notes reasoning depth limitations.
- Break condition: If web sources become structured and domain-curated, this mechanism weakens.

### Mechanism 2
- Claim: Adding domain tools does not guarantee improved performance; tool orchestration is a distinct bottleneck.
- Mechanism: Tool selection errors and usage errors increase with tool inventory size and parameter complexity. Finance and Multi-domain (largest toolsets) show highest selection-error rates. Usage errors correlate with parameter count (e.g., Bio, Multi-domain).
- Core assumption: Current LLMs lack robust planning/selection for heterogeneous tool spaces.
- Evidence anchors:
  - [abstract]: "22.4% of failures arise from incorrect tool usage or selection"
  - [section]: "Finance and Multi-domain host the largest toolsets and show markedly higher selection error rates"
  - [corpus]: Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems notes errors from user-agent-tool interactions.
- Break condition: If tool descriptions become standardized and models are explicitly trained on tool selection, error rates should drop.

### Mechanism 3
- Claim: Iterative refinement with web-based verification creates genuinely multi-source tasks.
- Mechanism: Stage 2 uses a web-only Verifier to attempt solving tasks. If web search succeeds, the Refiner rewrites/fuzzes conditions. This pruning removes single-source shortcuts, leaving only tasks requiring cross-tool integration.
- Core assumption: Verifier must be sufficiently capable; otherwise, false negatives/positives in difficulty assessment occur.
- Evidence anchors:
  - [section]: "Without pruning, many tasks collapse to trivial web lookups, inflating accuracy (45.1% → 31.3%)"
  - [corpus]: Weak direct corpus evidence on this specific mechanism.
- Break condition: If web search becomes powerful enough to solve all multi-condition tasks, refinement loop would never terminate or would over-prune.

## Foundational Learning

- Concept: Model Context Protocol (MCP)
  - Why needed here: MCP enables standardized tool interfaces across heterogeneous domains; the benchmark relies on MCP tools.
  - Quick check question: Can you explain how MCP differs from traditional API calling in LLM agents?

- Concept: ReAct Agent Framework
  - Why needed here: The benchmark evaluates agents using a ReAct-style loop (reason → act → observe).
  - Quick check question: What is the core loop in ReAct, and why does it help with tool-augmented tasks?

- Concept: Tool-Calling Evaluation (Accuracy vs. Pass Rate)
  - Why needed here: The paper distinguishes strict end-to-end accuracy from partial success (pass rate); understanding this distinction is critical for interpreting results.
  - Quick check question: Why might an agent achieve high pass rate but low accuracy?

## Architecture Onboarding

- Component map:
  - InfoMosaic-Flow: Organizer (Synthesizer/Refiner) ↔ Worker (Executor/Verifier)
  - Stage 1 (Information Seeking): Synthesizer → Executor with domain tools → initial QA
  - Stage 2 (Iterative Refinement): Refiner → Verifier (web-only) → pruning/fuzzing → final QA
  - Evaluation: ReAct agent + MCP tools + LLM judge for answer alignment

- Critical path:
  1. Seed data → Scenario proposing → Domain info gathering (tool calls) → Integrating → Draft QA
  2. Draft QA → Condition decomposition → Web-based verification → Fuzzing (if shortcut detected) → Final benchmark sample

- Design tradeoffs:
  - Organizer-worker separation reduces retrofitting but adds pipeline complexity.
  - Verifier limited to web search ensures non-triviality but may over-prune if web is too powerful.
  - Tool-call cap (20) prevents runaway execution but may truncate complex tasks.

- Failure signatures:
  - High tool-call count with low accuracy: suggests ineffective tool selection.
  - Large gap between pass rate and accuracy: indicates failure to integrate partial results.
  - Sudden accuracy drop in multi-domain: cross-source orchestration failure.

- First 3 experiments:
  1. Run web-only baseline on InfoMosaic-Bench; confirm low accuracy (~38% for GPT-5).
  2. Compare domain-tool vs. web-only per domain; note where tools help (Map, Video) vs. hurt (Medical, Finance).
  3. Categorize tool-call results (usage error, selection error, invalid, valid); correlate error distribution with domain tool complexity.

## Open Questions the Paper Calls Out
- What specific training or architectural interventions are required to reverse the performance degradation observed in high-stakes domains (Medicine, Finance) when domain-specific tools are provided?
- Can the InfoMosaic-Flow synthesis pipeline generalize effectively to interactive environments where tool usage results in state changes rather than static information retrieval?
- Is the observed performance plateau after 8 tool calls a fundamental limitation of the ReAct framework's linear reasoning, or does it stem from context length saturation?

## Limitations
- The benchmark relies on a proprietary verifier (GPT-5) that may not be accessible to all researchers, limiting reproducibility of the refinement loop.
- Tool-call failures (22.4%) are attributed to agent shortcomings, but insufficient analysis of whether the MCP tools themselves introduce noise or schema inconsistencies.
- The pruning mechanism may over-filter tasks if web search capabilities improve, making the benchmark less challenging over time.

## Confidence
- **High:** Web-only search is insufficient for domain-specific tasks; tool integration is a distinct bottleneck.
- **Medium:** Domain tools provide inconsistent benefits across domains; the cause is agent capability rather than tool quality.
- **Low:** The iterative refinement mechanism guarantees non-triviality of all benchmark tasks; depends on verifier capability and task distribution.

## Next Checks
1. Reproduce the web-only baseline on InfoMosaic-Bench using an open-weight model to verify the 38.2% accuracy claim is not GPT-5-specific.
2. Analyze tool-call traces from the benchmark to quantify MCP tool quality (e.g., API reliability, schema clarity) vs. agent error.
3. Stress-test the pruning mechanism by running the verifier on current web search models (e.g., GPT-4o, Claude-3.5) to assess if task difficulty degrades.