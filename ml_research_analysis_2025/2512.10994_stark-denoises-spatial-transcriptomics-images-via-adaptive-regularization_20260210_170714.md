---
ver: rpa2
title: STARK denoises spatial transcriptomics images via adaptive regularization
arxiv_id: '2512.10994'
source_url: https://arxiv.org/abs/2512.10994
tags:
- denoising
- spatial
- sprod
- graph
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STARK introduces an adaptive regularization approach for denoising
  spatial transcriptomics images, particularly effective in ultra-low sequencing depth
  regimes. The method employs kernel ridge regression with incrementally adaptive
  graph Laplacian regularization, iteratively updating both the image estimate and
  the graph structure to preserve cell identities and spatial information.
---

# STARK denoises spatial transcriptomics images via adaptive regularization

## Quick Facts
- arXiv ID: 2512.10994
- Source URL: https://arxiv.org/abs/2512.10994
- Reference count: 0
- Introduces adaptive graph Laplacian regularization for denoising spatial transcriptomics images in ultra-low sequencing depth regimes

## Executive Summary
STARK addresses the challenge of denoising spatial transcriptomics images when sequencing depth is extremely low (~100 reads/pixel), where standard methods fail to preserve biological information. The method combines kernel ridge regression with adaptive graph Laplacian regularization that iteratively updates both the image estimate and the graph structure connecting pixels. This approach preserves cell type boundaries while smoothing noise, and uniquely enables efficient interpolation at arbitrary spatial locations beyond measured pixels.

## Method Summary
STARK alternates between solving a kernel ridge regression problem for image denoising and updating the graph weights based on denoised estimates. The image estimation uses a modified representer theorem to reduce the infinite-dimensional RKHS optimization to tractable finite matrix operations. The graph Laplacian regularization is adaptive, combining spatial proximity with expression similarity that evolves as the image estimate improves. This block-convex alternating scheme guarantees convergence to stationary points and provides asymptotic consistency with rate O(R^{-1/2}) in terms of reads R.

## Key Results
- Outperforms competing methods (SPROD, GraphPCA, STAGATE) on biologically relevant metrics including label transfer accuracy and kNN overlap
- Demonstrates superior preservation of cell type information across tested sequencing depths
- Enables efficient interpolation at arbitrary spatial locations beyond measured pixels
- Shows theoretical convergence to stationary points and asymptotic consistency

## Why This Works (Mechanism)

### Mechanism 1
Adaptive graph weights enable edge-aware smoothing that preserves cell type boundaries. Graph weights combine spatial proximity with expression similarity, iteratively updating based on the current denoised estimate to become increasingly informed about true tissue boundaries. This enables anisotropic smoothing that respects biological structure.

### Mechanism 2
Representer theorem reduces infinite-dimensional RKHS optimization to tractable finite matrix operations. The optimal image can be written as a finite combination of kernel evaluations at observed pixels, transforming the image estimation into solving a finite-dimensional linear system.

### Mechanism 3
Block-convex alternating minimization guarantees convergence to stationary points despite non-convex overall objective. The objective is convex in the image estimate and convex in the graph weights separately, enabling tractable optimization with convergence guarantees via the Kurdyka-Łojasiewicz property.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS)**: Provides the function space framework where the image lives; enables both smoothness regularization and the representer theorem reduction. Quick check: Can you explain why pointwise evaluation f → f(q) must be continuous for the optimization problem to be well-defined?

- **Graph Laplacian Regularization**: Encodes the spatial prior that nearby pixels should have similar expression; the adaptive version learns which "nearby" pixels truly belong together. Quick check: What is the relationship between the graph Laplacian L_W and the Dirichlet energy that penalizes gradients?

- **Alternating Minimization / Block Coordinate Descent**: Enables tractable optimization of the non-convex objective by solving convex subproblems sequentially. Quick check: Why does block-convexity alone not guarantee convergence to a global minimum?

## Architecture Onboarding

- **Component map**: Input Y → Graph Initialization → Loop (F-update, W-update) → Output F̄(q)
- **Critical path**: Kernel matrix K construction → Pseudoinverse solve → Weight matrix W update
- **Design tradeoffs**:
  - Kernel choice: Exponential kernel (s=3/2 Sobolev) vs Matérn; exponential is computationally simpler but may under-smooth
  - Spatial threshold τ: Larger τ includes more neighbors (better smoothing, higher compute); authors use τ = 1.5l where l gives ~7 neighbors on average
  - Regularization ratio ω/λ: Higher ω emphasizes spatial coherence; higher λ emphasizes RKHS smoothness; auto-tuning sets λ based on expected noise level
  - Assumption: Auto-tuning heuristics assume early iterates are reasonable estimates
- **Failure signatures**:
  - Blurred boundaries: ω too high or s₁ too large relative to expression variation
  - Oscillating artifacts: λ too low, insufficient RKHS regularization
  - Sparse graphs: τ too small for irregular pixel spacing
  - Memory overflow: K (m×m) dominates; consider Nyström approximation for m > 10⁴
- **First 3 experiments**:
  1. Reproduction on MOSTA E9.5: Downsample to ~100 reads/pixel; verify label transfer accuracy ~0.8+ matches Figure 3
  2. Ablation: spatial-only variant: Fix W = W^sp throughout; quantify gap between adaptive and non-adaptive versions (Figure 5 shows ~5% gap)
  3. Interpolation stress test: Subsample to 10% pixels at fixed reads; evaluate whether recovered cell type labels match ground truth (Figure 6)

## Open Questions the Paper Calls Out

- Can non-asymptotic error bounds be established for STARK that explicitly quantify the tradeoffs between the number of genes, pixels, and reads? The paper currently provides only asymptotic convergence rates, leaving specific interactions between dataset dimensions undefined for finite samples.

- How should spatial transcriptomics denoising performance be evaluated when metrics like relative error (Frobenius norm) and label transfer accuracy yield contradictory rankings for competing methods? The paper demonstrates that minimizing standard matrix norms can bias results toward low-dimensional representations that destroy biologically relevant geometry.

- Can the STARK framework be reformulated using Gaussian process priors to enable Bayesian analysis and uncertainty quantification for spatial trajectory inference? The current method is posed as a point estimation problem without modeling the distribution of images necessary for Bayesian uncertainty quantification.

## Limitations

- Dependence on proper hyperparameter tuning, particularly the regularization parameter λ which must match the noise level
- Effectiveness of auto-tuning across diverse sequencing depths and tissue types remains unverified
- Assumption that spatial-only denoising provides a reasonable proxy for auto-tuning may fail in highly heterogeneous tissues

## Confidence

- **High Confidence**: Theoretical convergence guarantees (Theorem 3.5) and asymptotic consistency rate (O(R^{-1/2})) are mathematically rigorous
- **Medium Confidence**: Superior performance claims relative to SPROD, GraphPCA, and STAGATE on MOSTA E9.5 data, though limited to a single dataset
- **Low Confidence**: Generalization to other spatial transcriptomics platforms and sequencing depths beyond the tested regime (~100 reads/pixel)

## Next Checks

1. Test adaptive vs. fixed graph regularization on multiple datasets spanning different sequencing depths (10-1000 reads/pixel) to verify claimed universality
2. Evaluate interpolation performance when downsampling to 1% of pixels to test robustness of the spatial prior
3. Compare computational efficiency against SPROD and STAGATE on datasets with m > 10,000 pixels to assess scalability claims