---
ver: rpa2
title: Output Embedding Centering for Stable LLM Pretraining
arxiv_id: '2601.02031'
source_url: https://arxiv.org/abs/2601.02031
tags:
- loss
- z-loss
- output
- centering
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in large language models
  (LLMs), specifically the problem of output logit divergence that occurs during pretraining,
  particularly at high learning rates. The authors analyze the issue from the perspective
  of output embeddings' geometry and identify that anisotropic embeddings (non-uniform
  distribution in hidden space) are the root cause of this instability.
---

# Output Embedding Centering for Stable LLM Pretraining

## Quick Facts
- arXiv ID: 2601.02031
- Source URL: https://arxiv.org/abs/2601.02031
- Reference count: 10
- Key outcome: Output embedding centering (OEC) methods outperform z-loss in stabilizing LLM pretraining at high learning rates, with μ-loss requiring less hyperparameter tuning and adding minimal computational overhead.

## Executive Summary
This paper addresses training instability in large language models (LLMs), specifically the problem of output logit divergence that occurs during pretraining, particularly at high learning rates. The authors analyze the issue from the perspective of output embeddings' geometry and identify that anisotropic embeddings (non-uniform distribution in hidden space) are the root cause of this instability. They propose output embedding centering (OEC) as a new mitigation strategy, implemented in two ways: µ-centering (a deterministic operation that subtracts the mean output embedding) and µ-loss (a regularization method that penalizes the squared norm of the mean embedding). Experiments on decoder-only Transformer models (5 sizes ranging from 16M to 221M parameters) trained on 13.1 billion tokens show that both µ-centering and µ-loss outperform the commonly used z-loss in terms of training stability and learning rate sensitivity.

## Method Summary
The authors propose output embedding centering (OEC) as a stabilization method for LLM pretraining. Two implementations are provided: µ-centering, which deterministically subtracts the mean output embedding from all embeddings post-optimizer step, and µ-loss, which adds a regularization term λ·‖μ‖² to the loss function where μ is the mean output embedding. The theoretical analysis proves that both methods suppress output logit divergence by bounding the maximum logit magnitude. Experiments use decoder-only Transformers with 5 model sizes (16M-221M parameters), trained on 13.1B tokens using AdamW optimizer with cosine learning rate decay, comparing baseline, z-loss, µ-loss, and µ-centering across learning rate sweeps.

## Key Results
- Both µ-centering and µ-loss outperform z-loss in training stability and learning rate sensitivity
- µ-loss is significantly less sensitive to hyperparameter tuning than z-loss (optimal λ=10⁻¹ for z-loss vs λ=10⁻¹ for µ-loss)
- Both methods ensure convergence at high learning rates where z-loss fails (η≥0.1)
- Methods add minimal computational overhead (0-6.4% additional training time)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anisotropic output embeddings (shifted away from origin) are the root cause of logit divergence.
- Mechanism: Adam's second moment causes output embeddings to shift in a common direction (mean embedding μ grows), which increases mean logit via μ·h (Lemma 3). This common shift propagates to individual logits, eventually causing unbounded growth.
- Core assumption: Final hidden states h have non-trivial norm (usually satisfied via layer normalization).
- Evidence anchors:
  - [abstract]: "identify anisotropic embeddings (non-uniform distribution in hidden space) as the root cause of this instability"
  - [section 1]: "Stollenwerk and Stollenwerk (2025) identified the root cause... it is the second moment in Adam that causes the common shift of the embeddings"
  - [corpus]: Weak direct evidence; related papers (MSign, Edge of Stability) address instability but not embedding geometry specifically.
- Break condition: If output embeddings are initialized and maintained as isotropic (e.g., via Coupled Adam optimizer), the mechanism doesn't trigger.

### Mechanism 2
- Claim: Centering output embeddings reduces the upper bound on maximum logit magnitude.
- Mechanism: μ-centering subtracts the mean embedding, reducing max_i ‖e_i‖ when B_ratio ≤ 1 (Theorem 6). This shrinks the logit bound -max_i ‖e_i‖·‖h‖ ≤ l_j ≤ max_i ‖e_i‖·‖h‖ (Lemma 4), preventing unbounded growth.
- Core assumption: The condition B_ratio ≤ 1 holds empirically (verified in Appendix B for all 35 baseline models).
- Evidence anchors:
  - [section 2.3]: Theorem 6 proves "B_ratio ≤ 1 ⇔ max|l*_i| ≤ max|l_i|"
  - [section 4]: "both μ-centering and μ-loss restrict the logit bound such that the maximum logit remains stable"
  - [corpus]: No direct corpus evidence for this specific bound mechanism.
- Break condition: If embedding dot products with μ don't satisfy the B_ratio condition (counterexamples not found in experiments but theoretically possible).

### Mechanism 3
- Claim: Both regularization (μ-loss) and deterministic centering (μ-centering) suppress logit divergence more robustly than z-loss.
- Mechanism: μ-loss adds gradient signal λ·μ pushing the mean embedding toward zero during backprop. μ-centering directly enforces zero mean post-hoc. Unlike z-loss (which only prevents single positive or collective negative divergence), both methods prevent single positive AND single negative divergence.
- Core assumption: For μ-loss, λ ≥ 10⁻⁴ ensures sufficient regularization pressure.
- Evidence anchors:
  - [abstract]: "μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss"
  - [section 5]: "for larger values (λ ≥ 10⁻⁴), the training is stable and does not exhibit a strong dependency on the exact value of λ"
  - [corpus]: MSign paper addresses instability via optimizer modification, suggesting multiple valid intervention points exist.
- Break condition: For μ-loss with λ = 10⁻⁷ (too weak), divergence still occurs at high learning rates.

## Foundational Learning

- Concept: Softmax shift-invariance
  - Why needed: Understanding why μ-centering doesn't change model predictions (Proposition 5iii) - subtracting constant from all logits preserves probabilities.
  - Quick check question: If you add 5.0 to every logit before softmax, do the output probabilities change?

- Concept: Embedding anisotropy
  - Why needed: The core pathology being corrected - embeddings clustering in a narrow cone rather than distributing uniformly in hidden space.
  - Quick check question: Why might all embeddings having similar directions cause training instability?

- Concept: Adam optimizer moments
  - Why needed: Understanding the mechanistic origin of embedding shift (second moment v_t accumulates squared gradients asymmetrically).
  - Quick check question: How does Adam's second moment estimate differ from momentum in its effect on parameter updates?

## Architecture Onboarding

- Component map:
  Transformer backbone → hidden state h (dim H)
                         ↓
  Output embeddings E ∈ R^(V×H) → logits l_i = e_i · h
                         ↓
  Softmax → probabilities p_t
                         ↓
  Cross-entropy loss L + [optional: L_μ = λ·‖μ‖²]

  Post-optimizer step (μ-centering only):
  e_i ← e_i - μ, where μ = (1/V) Σ e_i

- Critical path:
  1. **Forward pass**: Compute logits via dot product (Eq. 3)
  2. **Loss computation**: Add μ-loss if using regularization variant
  3. **Backward pass**: Standard gradient computation
  4. **Optimizer step**: Adam update to output embeddings
  5. **Centering operation** (μ-centering only): Subtract mean from all embeddings

- Design tradeoffs:
  - **μ-centering**: Zero hyperparameters, deterministic, adds ~0.3-0.6% overhead
  - **μ-loss**: One hyperparameter (λ=10⁻⁴ default), integrates naturally into loss, adds ~0.2-0.7% overhead
  - **z-loss baseline**: Requires careful tuning (optimal λ=10⁻¹, not 10⁻⁴ as commonly used), still fails at η≥0.1

- Failure signatures:
  - Baseline: Logit mean diverges → loss spikes → NaN at high learning rates
  - z-loss: Occasional divergence even at λ=10⁻⁴, severe divergence at λ=10²
  - μ-loss: Divergence only if λ too small (10⁻⁷)
  - μ-centering: No observed divergence in experiments

- First 3 experiments:
  1. **Baseline reproduction**: Train 16M parameter model at η=0.3 without stabilization; verify logit mean grows unboundedly (Figure 3, top-left).
  2. **Method comparison at high LR**: Train 57M model at η∈{0.01, 0.03, 0.1} with z-loss (λ=10⁻⁴), μ-loss (λ=10⁻⁴), and μ-centering; compare learning rate sensitivity (Table 2ii).
  3. **Hyperparameter robustness**: Sweep λ∈{10⁻⁷, 10⁻⁴, 10⁻¹, 10²} for both z-loss and μ-loss on 29M model; verify μ-loss is stable across 3+ orders of magnitude while z-loss requires λ=10⁻¹ (Table 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do μ-centering and μ-loss maintain their stability benefits when scaling to LLMs with billions of parameters?
- Basis: [explicit] The authors explicitly limit their scope to models up to 221M parameters and state they "cannot make any reliable statements about their generalizability" to larger scales.
- Why unresolved: Instability dynamics often change qualitatively at massive scales (e.g., gradient noise), and it is unverified if anisotropic embeddings remain the primary driver of divergence in billion-parameter regimes.
- What evidence would resolve it: Pretraining results showing convergence and learning rate sensitivity improvements for models exceeding 1B parameters using OEC.

### Open Question 2
- Question: Which specific conditions favor the deterministic μ-centering over the regularization-based μ-loss in a production setting?
- Basis: [explicit] The paper concludes that while discussing theoretical trade-offs, "we do not provide a clear recommendation on which method is to be preferred in practice."
- Why unresolved: The experiments show nearly identical loss and sensitivity for both methods, obscuring whether the flexibility of μ-loss or the hyperparameter-free nature of μ-centering offers distinct practical advantages.
- What evidence would resolve it: Comparative benchmarks on training throughput, memory usage, and downstream task generalization across diverse architectures.

### Open Question 3
- Question: Is Output Embedding Centering compatible and synergistic with other symptom-based stability methods like logit soft-capping?
- Basis: [inferred] The paper focuses on replacing z-loss but cites logit soft-capping (Gemma 2) and max-z loss as alternative approaches without testing if OEC works orthogonally to them.
- Why unresolved: If OEC addresses the root cause (embedding geometry) while soft-capping addresses symptoms (numerical range), combining them might offer superior stability, or they might interfere.
- What evidence would resolve it: Ablation studies combining OEC with logit soft-capping to measure additive improvements in maximum stable learning rate.

## Limitations

- Experimental scope limited to relatively small models (16M-221M parameters) and 100K steps training
- Optimal hyperparameter λ=10⁻¹ for z-loss contradicts common practice (λ=10⁻⁴), suggesting existing implementations may be suboptimal
- Theoretical analysis assumes non-trivial hidden state norms via layer normalization, which is not universal
- No evaluation of downstream task performance or comparison with optimizer-level stabilization methods like MSign

## Confidence

**High Confidence:** The identification of anisotropic output embeddings as the root cause of logit divergence, and the basic mechanism of how μ-centering and μ-loss work (suppressing mean embedding growth to bound logits). The empirical demonstration that both methods outperform baseline and that μ-loss is more hyperparameter-robust than z-loss.

**Medium Confidence:** The theoretical bounds (Theorem 6, Lemma 3) accurately predict empirical behavior across all conditions. The claim that μ-loss with λ=10⁻⁴ is universally stable across learning rates. The assertion that these methods will scale to truly large models (billion+ parameters).

**Low Confidence:** The specific optimal hyperparameter values (particularly λ=10⁻¹ for z-loss) will generalize across different model architectures and datasets. The claim that μ-centering and μ-loss are the most effective intervention points compared to optimizer-level modifications like MSign.

## Next Checks

1. **Scale validation test:** Implement and evaluate μ-centering and μ-loss on a 1B+ parameter model trained for multiple epochs. Verify that the theoretical mechanism (embedding centering preventing logit divergence) holds at scale, and compare against MSign optimizer-level stabilization.

2. **Hyperparameter sensitivity analysis:** Conduct a systematic sweep of λ values (10⁻⁷ to 10²) across multiple model sizes and learning rates for both z-loss and μ-loss. Identify whether the optimal λ=10⁻¹ for z-loss is consistent across conditions, and whether μ-loss truly requires no tuning beyond λ=10⁻⁴.

3. **Mechanism ablation study:** Train models with and without layer normalization to test the assumption that hidden states have non-trivial norms. Additionally, compare against Coupled Adam (which maintains isotropic embeddings) to isolate whether output embedding centering is the only effective intervention point or if there are multiple valid approaches to preventing logit divergence.