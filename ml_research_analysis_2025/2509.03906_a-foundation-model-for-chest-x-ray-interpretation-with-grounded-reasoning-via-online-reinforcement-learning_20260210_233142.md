---
ver: rpa2
title: A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via
  Online Reinforcement Learning
arxiv_id: '2509.03906'
source_url: https://arxiv.org/abs/2509.03906
tags:
- deepmedix-r1
- reasoning
- qwen2
- vl-7b
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DeepMedix-R1 is a foundation model for chest X-ray interpretation
  that generates both diagnostic answers and interpretable reasoning steps tied to
  local image regions. It uses a sequential training pipeline: instruction fine-tuning
  on curated CXR data, cold-start reasoning with synthetic data, and online reinforcement
  learning via Group Relative Policy Optimization to improve grounded reasoning and
  performance.'
---

# A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.03906
- Source URL: https://arxiv.org/abs/2509.03906
- Reference count: 40
- DeepMedix-R1 achieves 14.54% better report generation and 57.75% better VQA performance than state-of-the-art models

## Executive Summary
DeepMedix-R1 is a foundation model for chest X-ray interpretation that generates diagnostic answers with interpretable reasoning steps tied to specific image regions. The model uses a sequential training pipeline: instruction fine-tuning on 351.5K CXR data samples, cold-start reasoning fine-tuning on 3.9K synthetic reasoning samples, and online reinforcement learning via Group Relative Policy Optimization (GRPO) to improve grounded reasoning. Quantitative evaluation shows significant improvements over state-of-the-art models, with expert review finding DeepMedix-R1 more interpretable and clinically plausible than baseline models.

## Method Summary
DeepMedix-R1 uses a sequential training approach with three stages. First, it undergoes instruction fine-tuning on 351.5K CXR instruction pairs. Second, it performs "cold-start" reasoning fine-tuning on 3.9K synthetic reasoning samples generated by GPT-4.1 and filtered for coordinate validity. Third, it applies online reinforcement learning via GRPO using a reward function that combines answer accuracy, coordinate validity, and format adherence. The model uses Qwen2.5-VL-7B as its base vision-language model and is evaluated on tasks including report generation, visual question answering, and image classification.

## Key Results
- Outperforms LLaVA-Rad by 14.54% and MedGemma by 31.32% on report generation tasks
- Achieves 57.75% better performance than LLaVA-Rad and 23.06% better than MedGemma on VQA tasks
- Expert review shows DeepMedix-R1 significantly outperforms Qwen2.5-VL-7B on grounded preference (0.7708 vs 0.2292)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The "Cold-Start" phase using synthetic reasoning data is likely critical for initializing the model's capacity to generate grounded reasoning chains before reinforcement learning (RL) begins.
- **Mechanism:** Standard instruction tuning produces answers but not necessarily the intermediate "reasoning steps" required for the final RL stage. By first fine-tuning on 3.9K synthetic samples (generated by GPT-4.1 and filtered for coordinate validity), the model enters the RL phase with a pre-existing policy for generating structured thought processes, preventing the exploration space from being too sparse.
- **Core assumption:** The synthetic reasoning data generated by advanced general-purpose LLMs provides a sufficiently accurate approximation of clinical reasoning to guide the initial policy without encoding significant hallucinations.
- **Evidence anchors:**
  - [abstract]: "exposed to high-quality synthetic reasoning samples to enable cold-start reasoning."
  - [Page 3]: "...overcome the model cold-start problems [30, 31]. The continuous fine-tuning upon it endows the model with preliminary capabilities in grounded understanding..."
  - [corpus]: Evidence is mixed/weak. While neighbors like *MedRAX* and *RadAgents* discuss reasoning, they focus on agentic workflows or external tool use rather than synthetic cold-start data for end-to-end models.
- **Break condition:** If the synthetic data contains logical errors or inconsistent grounding that passes the automated filter (Qwen2.5-VL-32B), the RL phase may optimize an initially flawed policy.

### Mechanism 2
- **Claim:** Group Relative Policy Optimization (GRPO) improves performance by optimizing the relative quality of outputs within a batch rather than relying on an absolute critic model.
- **Mechanism:** For a given query, the model samples a group of $G$ outputs. The reward model scores these outputs, and the advantage $\hat{A}_{i,t}$ is computed by normalizing rewards within the group (mean/std). This relative comparison allows the model to self-improve by favoring outputs that are statistically better than its other attempts for the same image, reducing the need for a perfectly calibrated absolute reward function.
- **Core assumption:** The defined reward function (comprising answer accuracy, coordinate validity, and format adherence) is robust enough that higher scores correlate with clinical utility.
- **Evidence anchors:**
  - [Page 10, Eq 2]: Defines the GRPO objective using group-relative advantages $\hat{A}_{i,t} = \frac{r_i - \text{mean}(r)}{\text{std}(r)}$.
  - [Page 3]: "...refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance."
  - [corpus]: Supported by neighbor *Enhancing Radiology Report Generation... using RL*, which notes RL improves over SFT, though DeepMedix specifically uses GRPO to avoid a critic model.
- **Break condition:** If the reward function is gameable (e.g., the model learns to output valid-looking but semantically empty coordinates to gain the $r_{coo}$ bonus), performance metrics will diverge from clinical reality.

### Mechanism 3
- **Claim:** Explicit reward engineering for spatial coordinates ($r_{coo}$) enforces visual grounding better than text-only training.
- **Mechanism:** The reward function includes a specific term for coordinate scores ($r_{coo,i}$) that incentivizes the model to generate bounding boxes. This forces the visual encoder to attend to specific anatomical regions to maximize the reward, linking the textual reasoning directly to image features.
- **Core assumption:** The text generation of coordinates (e.g., `[300, 120, 350, 400]`) is a sufficient proxy for the model actually "looking" at the correct region in the latent space.
- **Evidence anchors:**
  - [Page 11, Eq 5]: Defines the coordinate score reward: $r_{coo,i} = \max(N_{coo}(o_i) * 0.05 + \phi(o_i), 0.15)$.
  - [Page 8, Table 2]: Expert review shows DeepMedix-R1 significantly outperforms the baseline in "Grounded Preference" (0.7708 vs 0.2292).
  - [corpus]: Supported by *AnatomiX*, which emphasizes that grounding techniques often fail without anatomical correspondence, suggesting this is a known bottleneck the paper addresses via specific rewards.
- **Break condition:** If the model learns to hallucinate coordinates that satisfy the format but do not correspond to the pathology described (reward hacking), the grounding becomes superficial.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the core optimizer for Stage 3. Unlike standard PPO which requires a separate "Critic" model to estimate value, GRPO estimates value by comparing multiple outputs from the same model.
  - **Quick check question:** How does GRPO calculate the advantage $\hat{A}$ if it doesn't use a critic model? (Answer: It uses the mean and standard deviation of the rewards within a group of sampled outputs).

- **Concept: Cold-Start Reasoning**
  - **Why needed here:** The paper posits that RL alone is inefficient for complex reasoning tasks. The model needs a "warm start" to know *how* to generate reasoning steps before it can optimize them.
  - **Quick check question:** Why is synthetic data used for the cold start rather than immediately starting RL? (Answer: To equip the model with the preliminary capability to generate reasoning steps and avoid the "cold start" exploration problem).

- **Concept: Multimodal Reward Engineering**
  - **Why needed here:** The model must optimize for clinical factuality *and* spatial grounding simultaneously.
  - **Quick check question:** What three components make up the final reward $r_i$? (Answer: Answer score, Coordinate score, and Format score).

## Architecture Onboarding

- **Component map:**
  - Base Model: Qwen2.5-VL-7B
  - Training Pipeline: DeepMedix (Base) → Reasoning Cold Start → DeepMedix-R1 (Final)
  - Reward Module: Computes $r_{ans}$ (F1/BLEU), $r_{coo}$ (Coordinate validity), and $r_{fom}$ (Format)

- **Critical path:**
  1. **Data Prep:** Curate 351.5K CXR instruction pairs -> Generate 3.9K synthetic reasoning chains using GPT-4.1 -> Filter with Qwen2.5-VL-32B
  2. **Stage 1 & 2 (SFT):** Standard next-token prediction loss on instruction and reasoning data
  3. **Stage 3 (GRPO):** For each batch, sample $G$ outputs -> Compute rewards -> Normalize rewards to get advantages -> Update policy via clipping objective

- **Design tradeoffs:**
  - **Synthetic vs. Human Reasoning:** The paper uses synthetic reasoning data (3.9K) likely because acquiring *expert* reasoning chains with grounded coordinates is resource-intensive. The tradeoff is potential propagation of LLM hallucinations.
  - **Reward Complexity:** The composite reward ($r_{ans} + r_{coo} + r_{fom}$) is complex. While it enforces structure, it risks "reward hacking" where the model optimizes the metric rather than clinical accuracy.

- **Failure signatures:**
  - **Low Grounded Preference:** If the model scores high on text metrics but low on "Grounded Preference," it is likely hallucinating coordinates (generating plausible numbers that don't match the image).
  - **Reward Hacking:** If $r_{coo}$ is high but $r_{ans}$ is low, the model is generating valid coordinate formats for incorrect findings.

- **First 3 experiments:**
  1. **SFT Baseline:** Train DeepMedix (Stage 1 only) and evaluate on XrayBench to establish a baseline without reasoning.
  2. **Reward Validation:** Implement the reward calculation (Eq 4, 5, 6) and test it on dummy outputs to ensure it penalizes out-of-bounds coordinates and rewards format compliance.
  3. **Ablation on Cold Start:** Train a version skipping the "Cold Start" phase (going straight to GRPO) to verify if the synthetic reasoning data is strictly necessary for convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating structured medical domain knowledge (e.g., clinical heuristics or expert rules) into the reward function of the Group Relative Policy Optimization (GRPO) framework improve alignment and convergence compared to the current metric-based rewards?
- Basis in paper: [explicit] The Discussion section states that "domain knowledge in the medical field... can be effectively leveraged to design a well-informed reward function," suggesting this could reduce sparse reward challenges and enhance policy alignment.
- Why unresolved: The current study utilizes general metrics (e.g., BLEU, coordinate accuracy) for rewards, but does not test if explicit clinical rules improve the optimization process.
- What evidence would resolve it: Experiments comparing the current DeepMedix-R1 against a variant trained with a rule-based reward function, measuring convergence speed and clinical validity scores.

### Open Question 2
- Question: To what extent does the quality and fidelity of synthetic cold-start reasoning data constrain the upper bound of the model's grounded reasoning capabilities?
- Basis in paper: [explicit] The authors acknowledge the model "suffers from the limitation of data quality" and note that acquiring "latent medical reasoning process data is... indispensable" for training explainable foundation models.
- Why unresolved: While synthetic data enabled cold-starting, the paper does not determine if the synthetic nature of the data introduces a ceiling for reasoning accuracy or generalizability.
- What evidence would resolve it: An ablation study comparing model performance when trained on synthetic data versus high-quality, human-expert annotated reasoning traces.

### Open Question 3
- Question: Can advanced training methodologies like Reinforcement Learning with Human Feedback (RLHF) or hybrid neuro-symbolic approaches successfully mitigate hallucinations to achieve the near-perfect correctness required for clinical deployment?
- Basis in paper: [explicit] The paper highlights that the achieved correctness score of 0.7503 is "insufficient for real-world deployment" and explicitly lists RLHF and hybrid neuro-symbolic approaches as necessary future directions to fix this.
- Why unresolved: The current model relies on GRPO with automated rewards, which has not fully eliminated factual errors (hallucinations) in the reasoning process.
- What evidence would resolve it: Evaluation of a DeepMedix-R1 variant trained with RLHF, specifically reporting the percentage reduction in hallucination rates and the new correctness score against clinical thresholds.

## Limitations

- The reliance on synthetic reasoning data may propagate LLM hallucinations into the initial policy, with no comparison to human-annotated reasoning chains
- The coordinate reward system could incentivize superficial compliance with coordinate formats rather than genuine visual grounding
- Clinical validity claims are based on minimal expert review methodology without blind comparisons to ground truth

## Confidence

- **High confidence**: The sequential training pipeline (SFT → Cold Start → GRPO) and the overall architecture are clearly described and implementable.
- **Medium confidence**: The reported performance improvements over baselines are well-supported by quantitative metrics, though the choice of baselines (particularly the absence of other reasoning-focused models) limits the interpretation.
- **Low confidence**: The clinical validity of the generated reasoning steps is asserted through expert review scores, but the review methodology is minimally described. The claim that this represents true "grounded reasoning" rather than sophisticated hallucination requires independent validation.

## Next Checks

1. **Ablation on Synthetic Data Quality**: Systematically evaluate the impact of synthetic data quality by training parallel models with reasoning data generated by different LLMs (e.g., GPT-4.1 vs. Claude vs. human experts) to quantify how initial policy quality affects final RL outcomes.

2. **Reward Function Sensitivity Analysis**: Perform controlled experiments removing or modifying each reward component (answer score, coordinate score, format score) to empirically determine which aspects of the reward function are actually driving performance improvements versus potential reward hacking.

3. **Clinical Expert Validation on Blind Set**: Conduct blind expert review where radiologists evaluate reasoning steps from DeepMedix-R1 alongside ground truth reports without knowing which is which, using a standardized rubric for clinical accuracy, reasoning coherence, and visual grounding to establish true clinical utility beyond automated metrics.