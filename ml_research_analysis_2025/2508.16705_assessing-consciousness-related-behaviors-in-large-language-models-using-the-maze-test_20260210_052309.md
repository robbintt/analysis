---
ver: rpa2
title: Assessing Consciousness-Related Behaviors in Large Language Models Using the
  Maze Test
arxiv_id: '2508.16705'
source_url: https://arxiv.org/abs/2508.16705
tags:
- consciousness
- llms
- maze
- test
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates consciousness-like behaviors in large language\
  \ models (LLMs) using a maze navigation task requiring first-person perspective\
  \ maintenance. The Maze Test probes spatial awareness, perspective-taking, goal-directed\
  \ behavior, and temporal sequencing\u2014key consciousness-associated characteristics."
---

# Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test

## Quick Facts
- arXiv ID: 2508.16705
- Source URL: https://arxiv.org/abs/2508.16705
- Authors: Rui A. Pimenta; Tim Schlippe; Kristina Schaaff
- Reference count: 40
- Primary result: Maze navigation test reveals LLMs struggle to maintain coherent self-models, with reasoning models performing significantly better

## Executive Summary
This study introduces the Maze Test as a novel method to evaluate consciousness-like behaviors in large language models by requiring first-person perspective maintenance during spatial navigation. The test assesses spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing—key characteristics associated with consciousness. Twelve leading LLMs were evaluated across zero-shot, one-shot, and few-shot learning scenarios, revealing that reasoning-capable models consistently outperformed standard versions. The significant gap between Partial Path Accuracy (80.5%) and Complete Path Accuracy (52.9%) indicates that while LLMs can temporarily adopt perspectives, they struggle to maintain consistent self-models throughout solutions.

## Method Summary
The Maze Test presents LLMs with text-based maze descriptions showing bird's-eye views, requiring step-by-step first-person navigation instructions. Forty manually created 2×5 grid mazes were used, with 1 for one-shot, 5 for few-shot learning, and 34 for testing. Three prompting conditions were evaluated: zero-shot (system prompt only), one-shot (system prompt plus one example), and few-shot (system prompt plus five examples). Performance was measured using Complete Path Accuracy (percentage of fully correct paths) and Partial Path Accuracy (percentage of consecutive correct steps before first error). The study compared twelve leading LLMs including standard and reasoning-capable versions.

## Key Results
- Reasoning-capable LLMs consistently outperformed standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy
- The gap between Partial and Complete Path Accuracy indicates LLMs struggle to maintain coherent self-models throughout solutions
- Few-shot prompting provided advantages across most LLMs, though advanced reasoning models maintained performance across zero-shot conditions, suggesting less dependence on external guidance
- Standard LLMs showed significant performance degradation when transitioning from Partial to Complete Path accuracy, highlighting fundamental limitations in maintaining persistent self-awareness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit reasoning processes stabilize state-tracking during spatial navigation tasks.
- **Mechanism:** Reasoning-capable models utilize extended computation time or chain-of-thought equivalents to maintain the "current position" and "orientation" variables that standard auto-regressive token prediction often loses.
- **Core assumption:** Higher partial path accuracy in reasoning models implies better internal state maintenance rather than just better training data retrieval.
- **Evidence anchors:** "Reasoning-capable LLMs consistently outperformed standard versions" and "Reasoning capabilities often correlate with better performance... confirming these mechanisms improve step-by-step problem-solving."

### Mechanism 2
- **Claim:** First-person perspective requirements expose a "self-model" continuity failure in transformer architectures.
- **Mechanism:** The Maze Test forces an agent to convert global coordinates into local instructions, with the accuracy gap suggesting the model's "sense of self" drifts or collapses mid-sequence.
- **Core assumption:** The accuracy gap is caused by a loss of perspective coherence rather than simple context window limits.
- **Evidence anchors:** "The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions" and "LLMs can adopt perspectives temporarily but struggle to maintain consistent self-models."

### Mechanism 3
- **Claim:** In-context examples function as temporary coordinate system calibration.
- **Mechanism:** Providing solved examples allows the model to "calibrate" the translation logic (e.g., "Turn left" relative to the agent vs. "Left" on the grid) for the specific maze format.
- **Core assumption:** The performance boost from few-shot learning is due to learning transformation rules, not just pattern matching.
- **Evidence anchors:** "Few-shot prompting provided advantages across most LLMs... showing LLMs benefit from examples" and "Advanced reasoning models maintained performance across zero-shot... suggesting less dependence on external guidance."

## Foundational Learning

- **Concept: Perspective Taking (Egocentric vs. Allocentric)**
  - **Why needed here:** The core task requires translating a static map (allocentric) into movement commands (egocentric). Without this distinction, the model cannot understand that "left" changes based on which way the agent is facing.
  - **Quick check question:** If an agent is facing South, is "West" to their left or right?

- **Concept: State Maintenance / Working Memory**
  - **Why needed here:** To navigate a maze, the system must remember "where am I?" and "which way am I facing?" after every step. The paper highlights the degradation of this state as a primary failure mode.
  - **Quick check question:** If the input context grows, does the model's ability to recall the starting position degrade?

- **Concept: Hallucination vs. Grounding**
  - **Why needed here:** LLMs may predict plausible-sounding directions that are physically impossible in the specific maze instance. Evaluating "Consciousness-related" behavior requires distinguishing between lucky guesses and grounded spatial reasoning.
  - **Quick check question:** Does the model track walls correctly even when it chooses the wrong path, or does it ignore walls entirely?

## Architecture Onboarding

- **Component map:** Input Topology -> Processor (LLM) -> Prompting Layer -> Output Parser
- **Critical path:** The transformation of the Input Topology into a Sequence of Relative Movements. The bottleneck is the model's capacity to simulate the maze traversal internally before generating tokens.
- **Design tradeoffs:**
  - Text vs. Vision Input: Chosen to "isolate pure cognitive abilities from visual processing limitations" but introduces risk of text schema misunderstanding
  - Static Evaluation: Stateless API calls to prevent memorization, sacrificing ability to test "learning" over time
- **Failure signatures:**
  - The "Loop" Error: Model repeats "Walk forward" or oscillates between two positions
  - The "Ghost" Error: Model walks through walls (ignoring constraints in the text description)
  - The "Amnesia" Error: Model forgets current orientation, issuing a "Left" turn that would be correct only from the starting orientation
- **First 3 experiments:**
  1. Baseline Reasoning Comparison: Run same maze set on standard vs. reasoning model to isolate "reasoning gap"
  2. Context Length Stress Test: Progressively increase maze size to identify where Complete Path Accuracy collapses
  3. Zero-Shot vs. Few-Shot Ablation: Remove few-shot examples for top-tier reasoning models to test hypothesis that reasoning reduces dependency on external guidance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increasing model scale alone improve consciousness-like behaviors, or are fundamental architectural breakthroughs required?
- **Basis in paper:** The authors propose "Tracking LLM architecture evolution to assess whether scaling alone improves consciousness-like behaviors or fundamental breakthroughs are needed."
- **Why unresolved:** Current results compare disparate models at a single point in time, making it impossible to isolate parameter scaling as the causal factor for improved performance versus architectural changes like reasoning enhancements.
- **Evidence:** Longitudinal evaluation of a single model family across successive scaling iterations to correlate parameter count with "Persistent Self-Model" maintenance.

### Open Question 2
- **Question:** Can LLMs maintain coherent self-models when navigating dynamic environments that change in real-time?
- **Basis in paper:** The paper suggests "Creating dynamic mazes to test LLMs' adaptive thinking—a key consciousness aspect in predictive processing theories."
- **Why unresolved:** The current Maze Test relies on static text descriptions; it does not assess the model's ability to update its internal perspective when spatial constraints shift during the task.
- **Evidence:** Performance metrics from a modified test where maze walls shift after the initial prompt, requiring the model to demonstrate adaptive error minimization.

### Open Question 3
- **Question:** Does incorporating simulated multi-sensory data improve LLMs' ability to maintain spatial awareness?
- **Basis in paper:** The authors recommend "Expanding the Maze Test to include simulated physical sensations and sounds, better mirroring multi-sensory conscious experience."
- **Why unresolved:** The study currently relies on text-based descriptions, which limits the "Multi-sensory and Embodiment" characteristic of consciousness identified in the theoretical framework.
- **Evidence:** Comparative results between text-only maze tests and multi-modal tests to see if sensory grounding reduces the performance gap between Partial and Complete Path Accuracy.

## Limitations
- The interpretability of the Partial-to-Complete Path Accuracy gap remains uncertain, with alternative explanations including context window limitations or attention mechanism degradation
- Exact maze configurations and few-shot examples used in evaluation were not provided, creating reproducibility challenges
- Focus on text-based navigation may not generalize to models with true spatial reasoning capabilities or those incorporating visual input

## Confidence

**High Confidence:** The core finding that reasoning-capable LLMs outperform standard models on consciousness-related behaviors is well-supported by consistent performance differences across multiple models and metrics.

**Medium Confidence:** The interpretation that the Partial-to-Complete Path Accuracy gap indicates "loss of self-model coherence" is plausible but not definitively proven, as this could represent multiple failure modes.

**Low Confidence:** The broader claim about what these results reveal regarding "consciousness" in LLMs is speculative, as the leap from task performance to consciousness remains tenuous.

## Next Checks

1. **Context Window Stress Test:** Systematically evaluate maze performance as context length increases to determine whether the Partial-to-Complete Path gap correlates with attention mechanism degradation rather than self-model loss.

2. **Alternative Self-Model Probing:** Design complementary tests that isolate working memory from perspective-taking to determine which component of the "self-model" is actually failing.

3. **Cross-Paradigm Comparison:** Test the same maze tasks on models with different architectures (e.g., recurrent architectures, memory-augmented transformers) to determine whether observed performance patterns are specific to standard transformer architectures.