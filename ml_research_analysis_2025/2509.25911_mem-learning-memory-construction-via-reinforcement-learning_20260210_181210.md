---
ver: rpa2
title: "Mem-\u03B1: Learning Memory Construction via Reinforcement Learning"
arxiv_id: '2509.25911'
source_url: https://arxiv.org/abs/2509.25911
tags:
- memory
- arxiv
- preprint
- dataset
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of limited context windows\
  \ in large language models by proposing Mem-\u03B1, a reinforcement learning framework\
  \ that trains agents to effectively manage complex memory systems. The method formulates\
  \ memory construction as a sequential decision-making problem where agents process\
  \ information chunks, decide on memory operations, and receive rewards based on\
  \ downstream question-answering accuracy."
---

# Mem-α: Learning Memory Construction via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2509.25911
- **Source URL**: https://arxiv.org/abs/2509.25911
- **Reference count**: 40
- **Primary result**: RL-trained agents achieve 13× length generalization (30k→400k+ tokens) while maintaining strong QA accuracy

## Executive Summary
Mem-α addresses the challenge of limited context windows in large language models by training agents to construct and manage complex memory systems via reinforcement learning. The framework treats memory construction as a sequential decision-making problem where agents process information chunks, decide on memory operations (insert/update/delete), and receive rewards based on downstream question-answering accuracy. Despite training exclusively on sequences up to 30k tokens, agents successfully generalize to sequences exceeding 400k tokens, demonstrating remarkable robustness to extreme length extrapolation.

The approach uses a multi-component memory architecture (core, episodic, and semantic) and optimizes the memory construction policy using Group Relative Policy Optimization (GRPO). The system achieves significant improvements over existing memory-augmented agent baselines, particularly excelling at accurate retrieval and long-range understanding tasks. The framework's design choices—including the specialized memory components, composite reward structure, and fixed RAG evaluation pipeline—work together to enable effective learning while isolating the memory construction policy from retrieval mechanics.

## Method Summary
Mem-α trains LLM agents to manage hierarchical memory systems via reinforcement learning, using a three-component architecture (Core, Episodic, Semantic) and GRPO optimization. The agent processes context chunks and issues tool calls to update memory, receiving a composite reward based on downstream QA accuracy, tool execution success, compression ratio, and semantic validity. Training uses 562 instances from 8 datasets, with agents trained to handle sequences up to 30k tokens. The framework employs a fixed RAG pipeline (BM25 + Qwen3-32B generator) to evaluate memory quality, allowing the agent to focus on memory construction rather than retrieval learning. The entire system is implemented using the verl framework with Qwen3-4B as the agent backbone, trained on 32 H100 GPUs for 3 days.

## Key Results
- Mem-α achieves significant improvements over existing memory-augmented agent baselines on downstream QA tasks
- The system demonstrates 13× length generalization, successfully handling sequences exceeding 400k tokens despite training only on 30k-token sequences
- Agents show particularly strong performance on accurate retrieval and long-range understanding tasks
- The multi-component memory architecture enables effective management of diverse information types across different memory components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reinforcement learning enables agents to discover effective memory management strategies that generalize beyond training distribution.
- **Mechanism**: The RL framework treats memory construction as sequential decision-making. Agents process information chunks, issue memory operations (insert/update/delete), and receive reward signals derived from downstream question-answering accuracy. GRPO optimizes the policy by comparing rewards within sampled action groups, encouraging exploration of memory strategies.
- **Core assumption**: Memory quality can be effectively measured via QA accuracy on the final memory state; optimal strategies discovered on shorter sequences transfer to longer ones.
- **Evidence anchors**:
  - [abstract] "The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction."
  - [Section 3.1.2] Defines four reward components: correctness (r1), tool format (r2), compression (r3), memory content (r4).
  - [corpus] Related work (Mem-T, Fine-Mem) confirms reward design is critical for long-horizon memory agents, though sparse rewards remain challenging.

### Mechanism 2
- **Claim**: Multi-component memory architecture (core/semantic/episodic) provides sufficient expressiveness for diverse information types while remaining learnable.
- **Mechanism**: Core memory maintains a fixed-size summary (≤512 tokens) always in context. Semantic memory stores atomic facts. Episodic memory stores timestamped events. Each component has tailored operations: core supports only rewrite; semantic/episodic support insert/update/delete. This specialization reduces the action space complexity while preserving representational capacity.
- **Core assumption**: Information naturally partitions into summaries, facts, and events; agents can learn which component to target.
- **Evidence anchors**:
  - [abstract] "a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations"
  - [Section 3.3] "This design reflects different update patterns: semantic/episodic benefit from incremental modifications, core requires holistic revision."
  - [corpus] R³Mem, O-Mem similarly use structured multi-component designs; corpus evidence supports expressiveness claim but does not prove this specific partition is optimal.

### Mechanism 3
- **Claim**: Length generalization (30k→400k tokens) emerges because RL learns general memory management principles, not sequence-specific patterns.
- **Mechanism**: Training on diverse interaction patterns (conversations, documents, classification examples) with QA-based rewards forces agents to learn what information matters for answering questions, rather than memorizing positional patterns. The compression reward (r3) explicitly penalizes storing raw sequences, encouraging extraction of task-relevant content.
- **Core assumption**: Task-relevant information selection strategy is length-invariant; longer sequences don't fundamentally change what's worth remembering.
- **Evidence anchors**:
  - [abstract] "Despite training exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens"
  - [Section 4.2] "demonstrating the robustness of our training framework to extreme length extrapolation"
  - [corpus] No direct corpus evidence for this specific 13× extrapolation claim; related work (MemAgent, MEM1) uses simpler memory structures and doesn't report comparable generalization.

## Foundational Learning

- **Concept: Reinforcement Learning with Policy Optimization**
  - Why needed here: Mem-α uses GRPO (Group Relative Policy Optimization), a variant of PPO. You need to understand policy gradients, advantage estimation, and why RL (vs. supervised learning) is appropriate when ground-truth memory traces are unavailable.
  - Quick check question: Can you explain why the paper uses RL instead of supervised fine-tuning on expert memory traces?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Memory quality is evaluated via a RAG pipeline using BM25 retrieval from constructed memory to answer questions. Understanding retrieval, generation, and their coupling is essential.
  - Quick check question: How does the RAG evaluation pipeline connect memory construction to the reward signal?

- **Concept: Multi-component Memory Systems**
  - Why needed here: The architecture separates core (summary), semantic (facts), and episodic (events) memory. Each has different operations and use cases.
  - Quick check question: Why does core memory only support "update" while semantic/episodic support insert/update/delete?

## Architecture Onboarding

- **Component map**:
  - Agent (Qwen3-4B backbone) observes context chunk + current memory, outputs sequence of tool calls
  - Memory System: Core (512-token summary), Semantic (fact list), Episodic (timestamped event list)
  - Tool Interface: memory_insert, memory_update, memory_delete with typed arguments
  - Reward Computation: Correctness (QA via RAG), Format (execution success), Compression (size ratio), Content (LM-judged validity)
  - Training Loop: GRPO with group-normalized advantages, no KL penalty

- **Critical path**:
  1. Chunk arrives → Agent generates tool calls → Memory updated
  2. After all chunks → RAG retrieves from final memory → QA answers scored → Reward computed
  3. GRPO computes advantages → Policy updated

- **Design tradeoffs**:
  - **Fixed RAG pipeline vs. learnable retrieval**: Paper fixes retrieval (BM25) and generation (Qwen3-32B) to isolate write policy learning. Trade-off: may miss retrieval optimization opportunities.
  - **Compression reward (β=0.05) vs. memory completeness**: Higher β reduces memory size but hurts task performance (Table 4 shows β=0.4 drops avg from 0.642→0.509).
  - **Content reward (γ) criticality**: γ=0 causes catastrophic failure (Table 4, avg 0.543 vs. 0.642); model needs semantic validity signal.

- **Failure signatures**:
  - **Empty core memory**: Base Qwen3-4B leaves core memory empty (Table 5), losing high-level context
  - **Redundant episodic entries**: GPT-4.1-mini creates multiple entries with same timestamp (Table 5), wasting memory
  - **Missing assistant responses**: Baselines only record user behavior, losing answer-relevant context
  - **Tool format errors**: Qwen3-8B failed experiments due to systematic argument malformation (Appendix C.1)

- **First 3 experiments**:
  1. **Validate reward components**: Ablate γ (content reward) and β (compression) on validation set to reproduce Table 4 results; confirm γ=0 causes degradation.
  2. **Test length extrapolation**: Train on SQuAD/HotpotQA subsets with max 10k tokens, evaluate on full 30k+ instances to measure generalization scaling.
  3. **Memory utilization analysis**: Log per-component token usage and tool call frequencies across datasets to identify underutilized components; compare against GPT-4.1-mini baseline from Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Mem-α reinforcement learning framework be successfully adapted to real-world production systems without compromising latency or safety?
- Basis in paper: [explicit] The Conclusion states that extending Mem-α to real-world applications introduces challenges regarding latency, scalability, and safety that "warrant careful investigation."
- Why unresolved: The current framework is validated in simulated environments using pre-processed datasets, which abstract away the operational constraints of live systems.
- What evidence would resolve it: Performance metrics (throughput, latency) and safety audits from a deployment study connecting the agent to live databases.

### Open Question 2
- Question: Does integrating Mem-α with more sophisticated memory architectures like MIRIX provide significant structural advantages for complex reasoning tasks?
- Basis in paper: [explicit] The Conclusion suggests the current architecture "could benefit from integration with more sophisticated systems like MIRIX" for complex reasoning.
- Why unresolved: The paper validates a specific three-component architecture (Core, Episodic, Semantic) but does not test the framework's modularity against more advanced external systems.
- What evidence would resolve it: Ablation studies swapping the current memory instantiation for a MIRIX-based backend on complex reasoning benchmarks.

### Open Question 3
- Question: How does Mem-α perform on memory conflict resolution tasks when provided with realistic evaluation data?
- Basis in paper: [inferred] Section 3.4 notes the exclusion of the "Conflict Resolution" dimension due to a lack of realistic benchmarks, leaving this capability unverified.
- Why unresolved: The training dataset explicitly excludes contradictory evidence handling, so the agent's ability to overwrite or revise memory in complex scenarios remains unknown.
- What evidence would resolve it: Evaluation results on a non-synthetic conflict resolution dataset specifically designed to test memory revision.

## Limitations
- The evaluation relies on a fixed RAG pipeline rather than learned retrieval, potentially underestimating complete system performance
- Memory component design is empirically motivated but lacks theoretical justification
- The 400k+ token generalization claim lacks ablation studies on what drives this capability
- Training requires substantial computational resources (32 H100 GPUs, 3 days)

## Confidence
- **High confidence**: Core RL methodology, memory architecture design, and baseline comparisons
- **Medium confidence**: Length generalization results and their implications for real-world deployment
- **Medium confidence**: Reward component ablation studies and their interpretation

## Next Checks
1. Ablate the RAG retrieval component (try learned vs. BM25 retrieval) to isolate contribution of memory construction vs. retrieval quality
2. Train agents on progressively longer sequences (10k→50k→100k tokens) to identify scaling limits of generalization capability
3. Analyze the semantic validity reward (r4) distribution across memory components to identify which components require the most improvement