---
ver: rpa2
title: Saliency Map-Guided Knowledge Discovery for Subclass Identification with LLM-Based
  Symbolic Approximations
arxiv_id: '2511.07126'
source_url: https://arxiv.org/abs/2511.07126
tags:
- saliency
- maps
- time
- series
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a neuro-symbolic approach for identifying latent
  subclasses in time series classification tasks using gradient-based saliency maps
  and Large Language Models (LLMs). The method transforms multiclass problems into
  binary classification tasks, generates saliency maps from trained neural networks,
  and clusters signals using multivariate Dynamic Time Warping Barycenter Averaging
  (DBA) k-means.
---

# Saliency Map-Guided Knowledge Discovery for Subclass Identification with LLM-Based Symbolic Approximations

## Quick Facts
- arXiv ID: 2511.07126
- Source URL: https://arxiv.org/abs/2511.07126
- Reference count: 20
- Primary result: Multivariate clustering using saliency maps achieved mean sample coverage of 0.87 versus 0.73 for signal-only baselines, with consistent precision improvements across datasets.

## Executive Summary
This work presents a neuro-symbolic approach for identifying latent subclasses in time series classification tasks using gradient-based saliency maps and Large Language Models (LLMs). The method transforms multiclass problems into binary classification tasks, generates saliency maps from trained neural networks, and clusters signals using multivariate Dynamic Time Warping Barycenter Averaging (DBA) k-means. Cluster centroids are then processed by an LLM for symbolic approximation and fuzzy knowledge graph matching to discover underlying subclasses. Experimental results on three UCR time series datasets demonstrate significant improvements in both clustering and subclass identification when using saliency maps compared to signal-only baselines.

## Method Summary
The approach involves six key steps: (1) Z-normalization and downsampling of time series to 256 timesteps, (2) Label subsumption to convert multiclass problems into binary classification via alternating class assignment, (3) Training an XCM classifier with dataset-specific batch sizes and epochs, (4) Generating gradient-based saliency maps from the trained classifier, (5) Applying multivariate DBA k-means clustering using both the original signal and saliency maps as features, and (6) Using an LLM for symbolic approximation of cluster centroids and fuzzy matching to a predefined knowledge graph. The method bridges neural network-based pattern discovery with symbolic knowledge discovery through LLM-based matching to predefined knowledge graph properties.

## Key Results
- Multivariate clustering using saliency maps achieved mean sample coverage of 0.87 versus 0.73 for signal-only baselines
- Consistent precision improvements across all three evaluated datasets (Mallat, UWaveGestureLibraryAll, InsectWingbeatSound)
- Successful identification of latent subclasses through LLM-based symbolic approximation and fuzzy knowledge graph matching
- The approach effectively bridges subsymbolic cluster centroids with symbolic knowledge graph properties

## Why This Works (Mechanism)

### Mechanism 1
Incorporating gradient-based saliency maps as a secondary dimension in time series clustering appears to improve subclass separation compared to signal-only baselines. The saliency maps act as a "relevance guide" for the Dynamic Time Warping (DTW) distance metric. By clustering a bivariate series (raw signal + saliency map), the algorithm enforces similarity not just in shape but in the temporal location of decision-relevant features. This forces samples into distinct clusters if the model "attends" to different regions, even if their raw shapes are superficially similar.

### Mechanism 2
LLMs can effectively bridge subsymbolic cluster centroids (barycenters) and symbolic knowledge graph properties via symbolic approximation. Rather than forcing the LLM to ingest the entire cluster, the method provides the centroid (computed via DBA). The LLM generates a textual description of the centroid's shape (symbolic approximation) and performs fuzzy matching against pre-defined textual descriptions in a Knowledge Graph (KG). This maps mathematical clusters to semantic labels.

### Mechanism 3
Label subsumption (reducing multiclass to binary) facilitates the discovery of latent structures by forcing a binary discriminator to attend to intra-class differences. By grouping fine-grained classes into two coarse "super-classes" (0 and 1), the neural network must learn features that distinguish these large groups. The resulting saliency maps highlight features relevant to this binary split. When these maps are then clustered, they naturally separate based on which specific sub-features drove the binary decision, revealing the original finer-grained subclasses.

## Foundational Learning

- **Dynamic Time Warping Barycenter Averaging (DBA)**
  - Why needed: Standard averaging destroys time series shape alignment. DBA is required to compute centroids that the LLM analyzes, ensuring the average signal actually looks like a member of the class.
  - Quick check: Does the averaged signal look like a valid, smoothed version of the cluster members, or a jagged, meaningless noise burst?

- **Gradient-based Saliency Maps (e.g., Grad-CAM)**
  - Why needed: This is the core "attention" signal. These maps highlight time steps that contributed to the neural network's classification confidence, serving as the second channel for clustering.
  - Quick check: If the neural network is untrained (random weights), what would the saliency map look like? (Answer: Random noise, providing no clustering guidance).

- **Z-normalization**
  - Why needed: DTW is not scale-invariant. To cluster the raw signal and the saliency map together (multivariate), both must be normalized so one dimension doesn't dominate the distance metric.
  - Quick check: If a signal has a massive amplitude spike but irrelevant shape, would unnormalized clustering group it by shape or amplitude? (Answer: Amplitude would dominate; Z-norm fixes this).

## Architecture Onboarding

- **Component map:** Data Prep (Z-normalization & Downsampling) -> Binary Classifier (XCM) -> Saliency Generator (Gradient-based) -> Clustering (Multivariate DBA k-means) -> Neuro-Symbolic Bridge (LLM + Knowledge Graph)

- **Critical path:** The accuracy of the Binary Classifier dictates the quality of the Saliency Maps, which dictates the quality of the Clusters. If the classifier is weak (<60% acc), the pipeline produces garbage.

- **Design tradeoffs:**
  - *Dimensionality:* Downsampling to 256 steps improves runtime but risks losing high-frequency details necessary for subclass distinction.
  - *LLM Input:* Providing centroids as lists of floats vs. images. The paper suggests floats worked well, but image modality might be more robust for non-numeric reasoning models.
  - *k estimation:* Using the Elbow method is heuristic; the paper acknowledges this is a best-effort estimation for an unknown number of subclasses.

- **Failure signatures:**
  - *Low Coverage:* Mean sample coverage drops significantly below 0.73 (the baseline), indicating the saliency maps are confusing the clusters rather than helping.
  - *Hallucinated Matches:* LLM matches a centroid to a KG entry with high confidence, but the ground truth dominant class in that cluster is completely different (requires expert verification).
  - *Saliency Saturation:* Saliency maps highlight the entire time series uniformly, providing no discriminative power for clustering.

- **First 3 experiments:**
  1. **Baseline Validation:** Run DBA k-means on the raw signals only (univariate) to establish a baseline ARI/NMI.
  2. **Saliency Integration:** Re-run clustering using the multivariate approach (Signal + Saliency). Verify if ARI/NMI improves (checks Mechanism 1).
  3. **LLM Matching Stab:** Take the centroids from Experiment 2, pass them to the LLM, and check if the LLM's predicted class label matches the dominant ground truth label of the cluster (checks Mechanism 2).

## Open Questions the Paper Calls Out

### Open Question 1
How do different saliency map generation techniques (Grad-CAM, Grad-CAM++, Score-CAM, LayerCAM) compare for knowledge discovery in time series subclass identification? The paper suggests systematic comparison across generation methods using the same datasets and evaluation metrics would be valuable.

### Open Question 2
What is the optimal number of classes to create during label subsumption, and should semantically similar subclasses be grouped together or deliberately separated? An ablation study varying class counts and assignment strategies, measuring final subclass recovery rates, could resolve this.

### Open Question 3
Can ground truth-independent clustering metrics reliably filter low-quality clusters before LLM matching? Development of composite filtering metrics validated against LLM matching accuracy across diverse datasets is needed.

### Open Question 4
How well does the approach generalize to domains beyond sensor signals and to datasets with different inter-/intra-class distance characteristics? Systematic evaluation across datasets with varying class separability, sample homogeneity, and domain characteristics would assess robustness.

## Limitations
- Classifier dependency: The entire pipeline's effectiveness hinges on the quality of the binary classifier's saliency maps, with reported accuracies of only 0.6-0.7 on evaluated datasets.
- LLM reasoning validation: Evaluation primarily measures sample coverage rather than semantic accuracy, raising concerns about potential hallucination or superficial pattern matching.
- Dataset generalization: Results may not generalize beyond the three UCR datasets tested, which were selected with specific characteristics and biases.

## Confidence
- **High Confidence:** The clustering improvement mechanism (using saliency maps as relevance guides for DTW-based clustering) is well-grounded theoretically and supported by reported metric improvements.
- **Medium Confidence:** The neuro-symbolic bridging mechanism via LLM approximation is plausible given reported results, but evaluation lacks rigorous semantic validation.
- **Low Confidence:** The label subsumption strategy's effectiveness is questionable given wide variance in classifier performance across datasets and the paper's acknowledgment that poor permutations can lead to uninformative saliency maps.

## Next Checks
1. **Classifier Robustness Test:** Systematically evaluate the binary classifier's performance across all possible label subsumption permutations and establish a minimum accuracy threshold below which the pipeline should not proceed. Measure how classifier accuracy correlates with downstream clustering quality.

2. **Semantic Validation of LLM Matches:** For clusters where the LLM predicts a knowledge graph match, conduct expert verification comparing the predicted semantic label against the ground truth dominant class in the cluster. Calculate precision and recall of the semantic matching beyond just coverage metrics.

3. **Cross-Dataset Generalization Study:** Apply the complete pipeline to at least 5 additional UCR datasets spanning different characteristics (varying class counts, sample sizes, and temporal patterns) to assess robustness. Include both univariate and multivariate time series to test the method's generalizability beyond the current evaluation scope.