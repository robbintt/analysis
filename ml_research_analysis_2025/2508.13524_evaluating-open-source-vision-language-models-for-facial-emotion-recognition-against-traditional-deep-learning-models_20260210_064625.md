---
ver: rpa2
title: Evaluating Open-Source Vision Language Models for Facial Emotion Recognition
  against Traditional Deep Learning Models
arxiv_id: '2508.13524'
source_url: https://arxiv.org/abs/2508.13524
tags:
- learning
- vision
- deep
- facial
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares traditional deep learning models (VGG19, ResNet-50,
  EfficientNet-B0) with Vision-Language Models (VLMs) including Phi-3.5 Vision and
  CLIP on the FER-2013 dataset for facial emotion recognition. A novel pipeline integrating
  GFPGAN-based image restoration was introduced to address low-resolution and noisy
  image challenges.
---

# Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models

## Quick Facts
- **arXiv ID**: 2508.13524
- **Source URL**: https://arxiv.org/abs/2508.13524
- **Reference count**: 40
- **Primary result**: Traditional CNNs (EfficientNet-B0 at 86.44%) significantly outperform VLMs (CLIP at 64.07%, Phi-3.5 Vision at 51.66%) on FER-2013 with GFPGAN preprocessing

## Executive Summary
This study benchmarks traditional deep learning models (VGG19, ResNet-50, EfficientNet-B0) against open-source Vision-Language Models (VLMs) including Phi-3.5 Vision and CLIP for facial emotion recognition on the FER-2013 dataset. A novel pipeline integrating GFPGAN-based image restoration addresses the challenges of low-resolution and noisy images. Results demonstrate that specialized CNNs significantly outperform VLMs in this domain, with EfficientNet-B0 achieving 86.44% accuracy compared to CLIP's 64.07% and Phi-3.5 Vision's 51.66%. The study highlights VLMs' limitations in low-quality visual tasks and provides computational cost insights for practical deployment.

## Method Summary
The study compares CNNs trained from scratch on FER-2013 with zero-shot VLM inference. CNNs (VGG19, ResNet-50, EfficientNet-B0) were trained for 60/30 epochs with Adam optimizer, batch size 64, and cross-entropy loss. VLMs (CLIP-ViT-B/32 and Phi-3.5 Vision) were evaluated without fine-tuning using standard prompt templates. All inputs passed through a GFPGAN-based image restoration pipeline to enhance low-quality facial images. Performance was measured using accuracy, precision, recall, and F1-score across seven emotion classes.

## Key Results
- Traditional CNNs significantly outperformed VLMs: EfficientNet-B0 achieved 86.44% accuracy, ResNet-50 85.72%, while CLIP reached 64.07% and Phi-3.5 Vision only 51.66%
- VLMs showed particular weakness on subtle emotions like Fear (0.53% for Phi-3.5) and Disgust (3.10%), despite strong performance on Happy (81.10%) and Neutral (89.36%)
- The GFPGAN preprocessing pipeline was introduced to address low-resolution challenges, though its specific impact on VLM performance remains unquantified

## Why This Works (Mechanism)

### Mechanism 1
Specialized CNNs outperform VLMs on low-quality FER because they are trained end-to-end on the target data distribution, building robust internal representations from degraded inputs. VLMs suffer from domain shift when applied zero-shot to 48x48 grayscale images after pre-training on high-quality image-text pairs.

### Mechanism 2
GFPGAN-based generative restoration mitigates data quality issues by reconstructing facial details lost in downscaling. By hallucinating plausible high-frequency details, it transforms degraded inputs into a distribution more aligned with modern vision models' training data.

### Mechanism 3
VLMs struggle with fine-grained emotion categorization from low-quality inputs due to their reliance on global image-text embeddings. Emotion recognition from subtle facial movements requires precise spatial reasoning not well-supported by their general pre-training, causing performance collapse on ambiguous classes.

## Foundational Learning

**Vision-Language Models (VLMs) vs. Task-Specific CNNs**: CNNs learn task-optimal features via supervised training, while VLMs are generalists relying on broad pre-training, making them potentially weaker specialists without adaptation. *Quick check: What is the primary difference in how a ResNet-50 and a CLIP model are typically prepared for a new classification task?*

**The Domain Shift / Distribution Mismatch Problem**: VLMs trained on high-quality web images perform poorly on low-res, noisy faces because their learned features are no longer discriminative. *Quick check: How does the 48x48 grayscale nature of FER-2013 create a "distribution mismatch" for a VLM pre-trained on web-scraped images?*

**GFPGAN and Generative Restoration**: Unlike traditional super-resolution, GFPGAN "hallucinates" details using a generative prior, which is powerful but carries the risk of introducing unfaithful artifacts. *Quick check: Beyond sharpening edges, what primary risk does a generative model like GFPGAN introduce when restoring facial details?*

## Architecture Onboarding

**Component map**: Raw FER-2013 images → GFPGAN preprocessing → Model Zoo (CNNs trained from scratch / VLMs zero-shot inference) → Evaluation (accuracy, precision, recall, F1)

**Critical path**: For VLMs, success depends on GFPGAN preprocessing bridging the domain gap. For CNNs, the critical path is the training pipeline: data augmentation, optimizer settings, and epoch count determine performance.

**Design tradeoffs**: The central tradeoff is Accuracy vs. Generalization. CNNs offer higher accuracy (~86%) but require supervised training and are task-fixed. VLMs offer lower accuracy (~64%) but provide zero-shot flexibility and no training cost. A secondary tradeoff is Computational Cost vs. Performance, with VLMs being resource-intensive even in inference.

**Failure signatures**:
- VLM Misclassification of Subtle Emotions: High confusion between similar classes (e.g., Fear → Neutral at 53.6%)
- CNN Overfitting: Large gap between training and validation accuracy, or poor recall on minority classes like "disgust"
- GFPGAN Artifact Introduction: Sharp drop in accuracy for a specific test subset, indicating distorting artifacts

**First 3 experiments**:
1. **Baseline Establishment**: Train CNNs on original, unenhanced FER-2013 to quantify GFPGAN's contribution
2. **Preprocessing Ablation**: Run VLMs on FER-2013 in three conditions: raw images, simple upsampling, and GFPGAN-enhanced images
3. **VLM Fine-Tuning Experiment**: Fine-tune the best VLM on GFPGAN-enhanced dataset instead of using zero-shot inference

## Open Questions the Paper Calls Out

**Open Question 1**: To what extent does fine-tuning open-source VLMs on FER-2013 close the performance gap with traditional CNNs? The authors suggest fine-tuning could improve robustness but only evaluated zero-shot VLMs.

**Open Question 2**: Can hybrid architectures integrating CNN feature extractors with VLMs outperform standalone models for emotion recognition? The paper proposes integration could enhance both feature extraction and contextual understanding.

**Open Question 3**: How does GFPGAN-based image restoration specifically influence VLM visual embeddings compared to traditional deep learning models? Despite using GFPGAN, VLMs significantly underperformed, suggesting potential misalignment with VLM pre-training.

## Limitations

- VLMs were evaluated only in zero-shot mode without fine-tuning, potentially making the comparison unfair
- The specific contribution of GFPGAN preprocessing to performance improvements was not conclusively demonstrated through ablation studies
- Computational cost claims lack detailed measurements and substantiation
- The study relies on FER-2013 dataset, which has known limitations regarding class balance and label quality

## Confidence

- **High Confidence**: CNNs significantly outperform VLMs on FER-2013 (Direct evidence from reported metrics)
- **Medium Confidence**: VLMs have inherent limitations on low-quality visual tasks (Plausible given domain shift, needs further validation)
- **Low Confidence**: Specific contribution of GFPGAN preprocessing to performance (Not conclusively demonstrated with ablation study)

## Next Checks

1. **VLM Fine-Tuning Experiment**: Fine-tune CLIP on GFPGAN-enhanced FER-2013 using same protocol as CNNs to determine if performance gap stems from lack of adaptation

2. **Preprocessing Ablation**: Run VLMs on FER-2013 in three conditions: raw images, bicubic upsampling, and GFPGAN-enhanced images to isolate generative restoration's contribution

3. **Domain Shift Quantification**: Measure embedding distance between original FER-2013 and high-quality facial image dataset to quantify the domain shift VLMs face