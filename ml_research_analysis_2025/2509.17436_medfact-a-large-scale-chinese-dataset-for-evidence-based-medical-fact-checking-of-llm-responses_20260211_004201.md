---
ver: rpa2
title: 'MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking
  of LLM Responses'
arxiv_id: '2509.17436'
source_url: https://arxiv.org/abs/2509.17436
tags:
- claim
- liver
- evidence
- medical
- supported
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEDFACT is the first evidence-based Chinese medical fact-checking
  dataset targeting LLM-generated content, containing 1,321 questions and 7,409 claims
  across 23 medical topics. The dataset construction pipeline includes decomposition
  of LLM responses into atomic claims, check-worthiness detection, evidence retrieval
  from web sources, and veracity labeling using an "LLM-then-Human" approach.
---

# MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses

## Quick Facts
- **arXiv ID:** 2509.17436
- **Source URL:** https://arxiv.org/abs/2509.17436
- **Reference count:** 27
- **Primary result:** First evidence-based Chinese medical fact-checking dataset for LLM-generated content, with 1,321 questions and 7,409 claims across 23 medical topics.

## Executive Summary
MEDFACT is the first evidence-based Chinese medical fact-checking dataset specifically targeting LLM-generated content. The dataset contains 1,321 questions and 7,409 claims across 23 medical topics, addressing the critical need for verifying the factual accuracy of LLM responses in medical domains. The construction pipeline includes decomposition of LLM responses into atomic claims, check-worthiness detection, evidence retrieval from web sources, and veracity labeling using an "LLM-then-Human" approach. Experimental results demonstrate that while fine-tuned smaller models like Qwen3-4B outperform larger models, none surpass human performance, highlighting the complexity of medical fact-checking tasks.

## Method Summary
The MEDFACT dataset was constructed using a multi-stage pipeline starting with 1,500 questions sampled from webMedQA. LLM-generated responses (from Yi-Large-Turbo) were decomposed into atomic claims using DeepSeek-V2.5. Each claim underwent check-worthiness detection before evidence retrieval via Google Search API. The top 3 web documents were consolidated by GLM-4-Long. Veracity labeling employed an "LLM-then-Human" approach where GLM-4-Long provided preliminary labels refined by two trained annotators. The final dataset contains 7,409 claims across train/validation/test splits (924/199/198). Experimental evaluation included both in-context learning with GPT-4o, DeepSeek-V3, and Qwen3-32B, and fine-tuning with Qwen3-4B/8B, GLM-4-9B, and InternLM3-8B using standard hyperparameters (10 epochs, LR=1e-4, BS=4).

## Key Results
- Fine-tuned Qwen3-4B achieved the highest F1-score (44.65) among tested models, outperforming much larger models like 671B parameter DeepSeek-V3
- No model surpassed human performance on the dataset, indicating significant room for improvement
- The dataset revealed key challenges in handling medical ambiguity, semantic containment, and synonymy
- Error analysis identified three critical failure modes: semantic containment overlook, evidence misunderstanding, and medical synonymy misjudgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing LLM-generated medical responses into atomic claims isolates verifiable facts from context
- Mechanism: The pipeline breaks down a long, multi-sentence response into discrete, self-contained statements, preventing ambiguous or irrelevant information from affecting verification of specific facts
- Core assumption: Complex medical responses can be accurately partitioned into independent factual units that retain their original meaning
- Evidence anchors: Section 3.2 "Decomposition and Decontextualization" states this ensures each statement is self-contained without irrelevant background information

### Mechanism 2
- Claim: An "LLM-then-Human" labeling pipeline improves annotation efficiency while maintaining high data quality
- Mechanism: GLM-4-Long first assigns preliminary veracity labels, then human annotators review and refine these labels, using LLM for speed and initial filtering
- Core assumption: LLM's preliminary labels are sufficiently accurate to reduce overall human workload without introducing systematic bias
- Evidence anchors: Section 3.2 "Veracity Labeling" describes the approach; Cohen's Kappa of 81.54% suggests reasonable annotator consistency

### Mechanism 3
- Claim: Fine-tuning smaller LLMs on specialized medical fact-checking data can outperform much larger, general-purpose models
- Mechanism: While large models possess vast parametric knowledge, fine-tuning allows smaller models to adapt specifically to task definitions, evidence patterns, and label nuances
- Core assumption: Superior performance is due to task adaptation rather than fine-tuning process favoring smaller architectures
- Evidence anchors: Section 4.2 shows Qwen3-4B (Fine-tuned) achieving highest F1-score at 44.65 despite 4B vs 671B parameter disparity

## Foundational Learning

- **Factuality Evaluation of LLMs**: Why needed here - dataset built on premise that LLM-generated medical content contains hallucinations and inaccuracies needing systematic verification. Quick check: Why can't we directly trust the output of a powerful LLM like GPT-4 or DeepSeek for medical advice?
- **Atomic Claim Decomposition**: Why needed here - core data-processing step transforming complex paragraph into testable units. Quick check: How does breaking a long sentence into smaller parts help in finding evidence for or against it?
- **In-Context Learning (ICL) vs. Fine-Tuning**: Why needed here - experimental results heavily depend on understanding difference between providing examples in prompt (ICL) vs updating model's weights (Fine-tuning). Quick check: What is the fundamental difference in how a model learns the task between these two experimental settings?

## Architecture Onboarding

- **Component map**: Question Source (webMedQA) -> LLM Response Generator -> Decomposition/Claim Extraction Module -> Check-worthiness Filter -> Evidence Retriever (Google Search API) -> Veracity Labeling Module (LLM + Human Annotator) -> Final Fact-Checking Model (Trained/Fine-tuned)
- **Critical path**: Integrity of final dataset hinges on Veracity Labeling step. Incorrect label assignment (e.g., "Supported" instead of "Partially Supported") propagates errors to all downstream models
- **Design tradeoffs**: "LLM-then-Human" approach trades potential annotation bias for increased speed and cost-efficiency compared to purely human pipeline
- **Failure signatures**: Evidence Misunderstanding (misclassifies "uncertain" instances), Semantic Containment Overlook (fails to recognize evidence supporting subset doesn't fully support entire claim), Medical Synonymy Misjudgment (incorrectly predicts "Uncertain" due to failing to equate formal and colloquial terms)
- **First 3 experiments**: 1) Baselining on Raw Data - run standard LLM on undecomposed medical responses for baseline difficulty, 2) Component Ablation - test fact-checking model removing one intermediate step at a time, 3) Error Injection Analysis - manually inject specific error types into test set and evaluate model performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures be improved to recognize semantic containment, where specific evidence (e.g., "meditation") supports a broader claim (e.g., "stress management methods")?
- **Basis in paper:** Error Analysis section identifies "Semantic Containment Overlook" as critical error type where LLMs fail to verify broader claims based on specific evidence
- **Why unresolved:** Current models require explicit textual overlap rather than understanding hierarchical relationships between specific examples and general categories
- **What evidence would resolve it:** Model design improving "Supported" F1-score for specific-to-general inference cases without increasing false positives

### Open Question 2
- **Question:** What specific training interventions can resolve "Medical Synonymy Misjudgment" to better map formal medical terminology to common aliases?
- **Basis in paper:** Authors highlight this as key error category where models fail to identify semantic equivalence between formal terms (e.g., "psoriasis cycles") and common descriptions
- **Why unresolved:** Models often incorrectly predict "Uncertain" when evidence uses different but synonymous terminology than claim
- **What evidence would resolve it:** Successful integration of medical knowledge graphs or specialized attention mechanisms aligning performance on synonym-heavy claim-evidence pairs

### Open Question 3
- **Question:** Can synthetic data generation or adversarial learning effectively correct performance imbalance caused by dominance of "Supported" labels in dataset?
- **Basis in paper:** Limitations section notes imbalanced label distribution and calls for future work on generating synthetic data or applying adversarial learning
- **Why unresolved:** Current real-world distribution makes it difficult to train models that perform robustly on "Refuted" or "Not Applicable" categories
- **What evidence would resolve it:** Demonstrated improvements in macro F1-scores across minority classes using suggested data augmentation techniques

## Limitations
- Dataset's Chinese language focus limits applicability to English-language medical fact-checking
- Performance gap on medical synonymy and semantic containment issues suggests dataset may not fully capture real-world medical discourse complexity
- Reliance on LLM-then-Human labeling pipeline introduces potential systematic bias from LLM's initial classifications

## Confidence
- **High Confidence:** Dataset's existence, composition (1,321 questions, 7,409 claims, 23 topics), and general experimental finding that fine-tuned smaller models can outperform larger models
- **Medium Confidence:** Specific error analysis identifying medical synonymy, semantic containment, and evidence misunderstanding as failure modes based on sample of 50 predictions
- **Low Confidence:** Insufficient detail on specific prompts for claim decomposition and evidence retrieval, lack of quantified efficiency gains for LLM-then-Human labeling approach

## Next Checks
1. **Ablation of the Labeling Pipeline:** Conduct small-scale experiment comparing accuracy and efficiency of purely human-labeled subset against LLM-then-Human labeled data to quantify impact of initial LLM classifications
2. **Out-of-Distribution Medical Testing:** Evaluate best-performing fine-tuned model (Qwen3-4B) on separate, manually curated set of medical claims from different source or topic area to assess generalization capabilities
3. **Error Mode Stress Testing:** Systematically generate adversarial examples testing model's robustness to medical synonymy and semantic containment issues identified in error analysis, measuring performance degradation on targeted cases