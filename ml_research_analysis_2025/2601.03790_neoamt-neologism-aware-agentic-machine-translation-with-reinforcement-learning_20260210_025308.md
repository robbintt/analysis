---
ver: rpa2
title: 'NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning'
arxiv_id: '2601.03790'
source_url: https://arxiv.org/abs/2601.03790
tags:
- translation
- word
- table
- search
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeoAMT, a reinforcement learning-based framework
  for neologism-aware machine translation that leverages an agentic search process
  using Wiktionary. The authors construct a new multilingual dataset (Neko) from Wiktionary
  covering 16 languages and 75 translation directions, and develop a search toolkit
  for retrieving neologism definitions.
---

# NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.03790
- Source URL: https://arxiv.org/abs/2601.03790
- Reference count: 40
- Key outcome: NeoAMT significantly improves neologism translation accuracy with 22.34% exact success rate and 78.28 GEMBA score for other-language-to-English directions

## Executive Summary
This paper introduces NeoAMT, a reinforcement learning-based framework for neologism-aware machine translation that leverages an agentic search process using Wiktionary. The authors construct a new multilingual dataset (Neko) from Wiktionary covering 16 languages and 75 translation directions, and develop a search toolkit for retrieving neologism definitions. NeoAMT combines a novel reward design with an adaptive rollout generation approach based on translation difficulty. Experiments show that NeoAMT significantly improves neologism translation accuracy compared to baselines, achieving exact neologism translation success rates of 22.34% and overall translation quality scores of 78.28 on GEMBA evaluation for other-language-to-English directions.

## Method Summary
NeoAMT trains translation agents using Group Relative Policy Optimization (GRPO) with a multi-component reward design that balances neologism-specific accuracy with overall translation quality. The framework uses an agentic search toolkit to retrieve neologism definitions from Wiktionary during the translation process. The search toolkit employs bge-m3 embeddings and FAISS for dense retrieval of dictionary entries. Training employs adaptive rollout generation based on translation difficulty, using CometKiwi-DA-XL to estimate quality gaps and scale the number of rollouts exponentially for challenging examples. The model is trained for one epoch on the Neko dataset with batch size 32, using Qwen3-4B/8B base models.

## Key Results
- NeoAMT-8B achieves 22.34% exact neologism success rate vs 17.36% for Qwen3-8B baseline
- Overall translation quality reaches 78.28 GEMBA score for other-language-to-English directions
- Adaptive rollout generation (RQE) consistently improves performance across all metrics
- The framework handles 75 translation directions across 16 languages using the Neko dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agentic search with Wiktionary enables translation of neologisms absent from model parametric knowledge
- Mechanism: The model learns to interleave reasoning (inside tags) with targeted Wiktionary searches (inside tags). Retrieved definitions are then integrated into subsequent reasoning to produce accurate translations. This bypasses frozen parametric knowledge limitations.
- Core assumption: Neologisms have entries in Wiktionary; retrieval quality is sufficient; model can learn when and what to search.
- Evidence anchors:
  - [abstract] "NeoAMT...leveraging an agentic search process using Wiktionary"
  - [section 5.3] "86.90% of search queries are related to neologisms...96.43% of the model thinking paths after receiving the retrieved results use the retrieved neologism information for reasoning"
  - [corpus] Related work on dictionary-guided RL translation (arXiv:2508.19481) supports dictionary integration, but does not confirm agentic search efficacy specifically
- Break condition: Wiktionary lacks the neologism; retrieval fails to surface relevant entries; model ignores retrieved context

### Mechanism 2
- Claim: Multi-component reward design guides RL training toward both neologism accuracy and overall translation quality
- Mechanism: Three reward types: (1) neologism-specific reward (lemmatized span matching), (2) neural model reward (XCOMET-XL + CometKiwi-DA-XL weighted average), (3) format reward. Combined as R = 1{format} × (λRneo + (1−λ)Rneural). This balances specialized neologism accuracy with general semantic fidelity.
- Core assumption: Lemmatization captures neologism translations robustly; neural metrics correlate with human judgment; format compliance is necessary.
- Evidence anchors:
  - [section 3.1] "The final reward can be denoted as follows...R=1{format} · (λRneo + (1−λ)Rneural)"
  - [Table 3] NeoAMT-8B achieves 22.34% exact neologism success vs 17.36% for Qwen3-8B baseline
  - [corpus] No direct corpus evidence on this specific reward combination; related MT-RL work (MT-R1-Zero) uses mixed rewards but different formulation
- Break condition: Lemmatization errors on novel word forms; neural metrics fail to capture neologism-specific quality; λ weighting is suboptimal

### Mechanism 3
- Claim: Adaptive rollout generation based on translation difficulty improves sample efficiency and learning
- Mechanism: Translation difficulty v = Φ(x, yref) − Φ(x, ŷ) measures quality gap. Higher difficulty (v > 0) triggers more rollouts g = G·exp(r); lower difficulty gets fewer. This allocates compute toward harder examples where model underperforms relative to reference.
- Core assumption: CometKiwi-DA-XL quality estimates meaningfully distinguish difficulty; exponential scaling function is appropriate.
- Evidence anchors:
  - [section 3.2] "we encourage our model to generate more rollouts to increase the opportunity of obtaining better translations if the source text is challenging"
  - [Table 6-7] NeoAMT with RQE outperforms NeoAMT without RQE on all neologism-specific metrics
  - [corpus] No corpus evidence for RQE-based adaptive sampling in MT; ARPO (arXiv:2507.19849) uses entropy-based branching but for different tasks
- Break condition: Quality estimates are noisy or biased; difficulty metric doesn't correlate with actual learning value; sampling distribution becomes skewed

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm for training the translation agent; samples multiple outputs per input and optimizes relative rewards within groups
  - Quick check question: Can you explain why GRPO uses within-group advantage normalization instead of a global baseline?

- Concept: **Dense Retrieval with Multilingual Embeddings**
  - Why needed here: The search toolkit uses bge-m3 embeddings and FAISS for retrieving Wiktionary entries; understanding this pipeline is essential for debugging retrieval failures
  - Quick check question: What would happen if the embedding model had poor cross-lingual alignment for a specific language pair?

- Concept: **Neologism Detection and Lemmatization**
  - Why needed here: The neologism-specific reward requires identifying neologism spans in translations and lemmatizing them correctly using stanza
  - Quick check question: How would morphological complexity (e.g., agglutinative languages) affect lemmatization-based matching?

## Architecture Onboarding

- Component map:
  Neko Dataset -> Search Toolkit -> Prompt Template -> Policy Optimizer
  (Type 1/2/3 entries) (bge-m3 + FAISS) (reasoning → search → translation) (GRPO with rewards)

- Critical path:
  1. Source sentence with neologism → Prompt formatting
  2. Model generates reasoning → search query → retrieval → integration → translation
  3. Translation evaluated against reference → rewards computed
  4. RQE difficulty calculated → adaptive rollout count determined
  5. GRPO update with masked retrieved tokens

- Design tradeoffs:
  - Retrieval corpus size (3M records) vs. noise/quality; larger corpus may have more coverage but more irrelevant entries
  - Process reward adds query-relevance scoring but complicates training; paper shows mixed results (NeoAMT-4B benefits, NeoAMT-8B degrades slightly on exact match)
  - Maximum 3 search turns balances compute vs. exploration opportunity
  - λ=0.1 for neologism reward prioritizes overall quality; may underweight rare neologisms

- Failure signatures:
  - Model generates translations outside tags → format reward zeroed → no gradient signal
  - Retrieval returns irrelevant entries → model hallucinates or ignores context → human analysis shows 23.29% of related queries fail to retrieve definitions
  - Excessive search turns without convergence → token budget exhausted
  - RAG baseline (Table 27) shows higher neologism scores but lower overall quality, indicating retrieval-use conflicts

- First 3 experiments:
  1. **Baseline retrieval audit**: Sample 100 neologisms from test set; manually verify Wiktionary coverage and retrieval recall@5; identify coverage gaps
  2. **Reward ablation sweep**: Train with λ ∈ {0.0, 0.1, 0.3, 0.5} holding other hyperparameters fixed; plot neologism-specific vs. overall quality tradeoff
  3. **Search behavior analysis**: Log all search queries during inference; categorize into neologism-related, distractor, and failure modes; correlate with translation outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a translation-specific embedding model significantly improve retrieval accuracy and downstream neologism translation performance compared to the general-purpose bge-m3 model?
- Basis in paper: [Explicit] The authors state in the Limitations section that the general-purpose bge-m3 model was not specifically designed for machine-translation retrieval tasks and the human analysis indicates the retrieval component is the dominant performance bottleneck.
- Why unresolved: The authors used a general-purpose model due to availability and established performance, but did not train or evaluate a specialized retrieval component for this specific agentic MT task.
- What evidence would resolve it: An experiment fine-tuning a specialized multilingual embedding model on the Wiktionary data and comparing the resulting translation success rates against the bge-m3 baseline.

### Open Question 2
- Question: How can reinforcement learning reward functions be designed to enforce strict structural constraints on the model's intermediate reasoning steps?
- Basis in paper: [Explicit] The Limitations section notes that NeoAMT does not always follow the user-specified format during the thinking process, likely because the current reward design only applies constraints to the final translation output.
- Why unresolved: The current reward design prioritizes the final output quality and format, leaving the internal reasoning process less constrained and prone to formatting errors.
- What evidence would resolve it: The introduction of a "process reward" specifically penalizing malformed reasoning tags, followed by an analysis of the adherence rate to the specified thinking format.

### Open Question 3
- Question: Why does the Retrieval-Augmented Generation (RAG) baseline achieve higher neologism-specific scores than the agentic NeoAMT model, and can this retrieval strength be combined with agentic reasoning?
- Basis in paper: [Inferred] Table 27 in the appendix shows that the Qwen3-4B + RAG baseline significantly outperforms NeoAMT-4B in "Exact" and "Fuzzy" neologism matching, despite NeoAMT outperforming RAG in overall quality metrics (Table 4).
- Why unresolved: The paper highlights that RAG models often hallucinate or fail to follow instructions, but does not fully explain why RAG is more precise at exact term retrieval in isolation or how to hybridize this precision with the agent's better overall semantic handling.
- What evidence would resolve it: A detailed error analysis comparing the false positives/negatives of RAG vs. NeoAMT on neologism spans, and experiments integrating RAG-style hard retrieval constraints into the NeoAMT agent.

## Limitations

- **Coverage gaps**: The system depends heavily on Wiktionary coverage, which may be incomplete for many neologisms, especially in low-resource languages. The paper reports 76.71% retrieval success for neologism-related queries, leaving a significant portion of cases without dictionary support.
- **Metric validity concerns**: While the paper uses multiple evaluation metrics, the neologism-specific evaluation methodology relies on human annotation for reference spans. The quality and consistency of this annotation process is not detailed, raising questions about metric reliability.
- **Generalization limitations**: The evaluation focuses on other-language-to-English directions, leaving the English-to-other-language performance unverified.

## Confidence

**High confidence**: The core architectural framework (agentic search + RL training) is well-specified and the experimental improvements over baselines are statistically significant across multiple metrics. The Neko dataset construction methodology is clearly described.

**Medium confidence**: The adaptive rollout generation mechanism (RQE) shows consistent improvements, but the exponential scaling function's optimality is not established. The process reward shows mixed results across model sizes, suggesting sensitivity to implementation details.

**Low confidence**: The lemmatization-based neologism matching may have systematic failures for morphologically complex languages. The paper does not provide detailed error analysis for different language families or morphological types.

## Next Checks

1. **Retrieval coverage audit**: Systematically sample 200 neologisms from the test set and manually verify Wiktionary coverage and retrieval quality. This would quantify the actual blind spot size and identify language-specific coverage gaps.

2. **Cross-directional evaluation**: Implement and evaluate the English-to-other-language directions using the same methodology. This would test whether the agentic framework generalizes symmetrically across translation directions.

3. **Morphological robustness test**: Select 50 neologisms from agglutinative/fusional languages and manually verify lemmatization accuracy. Compare matching success rates across different morphological types to identify systematic failure modes.