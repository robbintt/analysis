---
ver: rpa2
title: Can ChatGPT Learn to Count Letters?
arxiv_id: '2502.16705'
source_url: https://arxiv.org/abs/2502.16705
tags:
- letters
- count
- llms
- tokens
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT struggles with simple tasks like counting letter occurrences
  due to its token-based architecture, which does not naturally align with individual
  characters. To address this, the authors fine-tuned GPT-4o on datasets of words
  paired with their counts for specific letters.
---

# Can ChatGPT Learn to Count Letters?

## Quick Facts
- arXiv ID: 2502.16705
- Source URL: https://arxiv.org/abs/2502.16705
- Authors: Javier Conde; Gonzalo Martínez; Pedro Reviriego; Zhen Gao; Shanshan Liu; Fabrizio Lombardi
- Reference count: 9
- Primary result: Fine-tuning GPT-4o on letter-counting examples reduced failure rates from ~15% to ~0.7% for trained letter

## Executive Summary
ChatGPT and similar LLMs struggle with counting letter occurrences in words due to their token-based architecture, which fragments text into subword units rather than preserving character boundaries. This paper demonstrates that fine-tuning GPT-4o on datasets of words paired with counts for specific letters can dramatically improve performance, reducing failure rates from approximately 15% to 0.7% for the trained letter. The fine-tuned model also generalizes to counting other letters, though with varying effectiveness depending on letter type.

## Method Summary
The authors fine-tuned GPT-4o using OpenAI's API on datasets containing 7,790 words paired with counts for specific target letters. Two separate word subsets were created: one for fine-tuning and one for testing, with even distribution of word lengths. The fine-tuning examples followed a prompt-completion format (e.g., "Count the letter 'r' in: strawberry" → "3"). The base model's performance was first measured on counting the target letter across the test set, then the fine-tuned model was evaluated on both the trained letter and untrained letters to assess generalization.

## Key Results
- Fine-tuned model reduced failure rates from ~15% to ~0.7% for the trained letter
- Cross-letter generalization observed: fine-tuning on "r" reduced failure rates for "m" from 11.77% to 1.60%
- Vowels showed weaker generalization: fine-tuning on "a" reduced failure rates for "r" from 8.42% to 12.61% (minimal improvement)

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Encoding Creates Structural Misalignment
- LLMs fail at letter counting because tokenization obscures character boundaries, splitting words into subword tokens based on frequency rather than alphabetic units
- Core assumption: The model lacks internal representation mapping tokens to constituent characters unless explicitly trained
- Evidence: "ChatGPT struggles with simple tasks like counting letter occurrences due to its token-based architecture" and "Tokens do not have to map to words, prefixes or any other language unit"

### Mechanism 2: Fine-Tuning Induces Token-to-Character Association Learning
- Fine-tuning on letter-counting examples enables the model to infer character composition within tokens through statistical regularities
- Core assumption: The model can form secondary representations associating tokens with their sub-token properties through gradient updates
- Evidence: "During the fine-tuning process, it seems that the model learns how many 'r' are in each token so that given a sequence of tokens that correspond to a word it can infer the number of 'r' in that sequence"

### Mechanism 3: Cross-Letter Generalization Via Shared Token Representations
- Fine-tuning on one letter transfers to others because the learned skill operates at the token-representation level, not the letter-specific level
- Core assumption: The fine-tuning procedure teaches a generalizable "counting procedure" rather than memorizing specific letter counts
- Evidence: "Also generalized well to count other letters with reduced failure rates" - fine-tuning on "r" reduced failure rates for "m" from 11.77% to 1.60%

## Foundational Learning

- **Tokenization**: Understanding why LLMs fail at character-level tasks requires grasping that models operate on arbitrary subword units, not letters
  - Why needed: Core to understanding the structural misalignment problem
  - Quick check: If you tokenize "measurement" with GPT-4o's tokenizer, how many tokens result?

- **Next-Token Prediction**: The fundamental operation of LLMs is sequential prediction; counting requires reasoning across tokens the model never explicitly enumerates
  - Why needed: Essential for understanding the computational cost tradeoff
  - Quick check: Why does predicting tokens instead of letters reduce computational cost?

- **Supervised Fine-Tuning (SFT)**: The paper's intervention relies on fine-tuning to adapt a pretrained model; understanding parameter updates from labeled examples is essential
  - Why needed: Core to the proposed solution
  - Quick check: What format would a fine-tuning example take for this task?

## Architecture Onboarding

- **Component map**: Tokenizer -> Embedding Layer -> Transformer Backbone -> Output Head -> Fine-Tuning Layer
- **Critical path**: Input text → Tokenizer → Token sequence → Embedding → Hidden states → Fine-tuned model → Output logits → Decoded answer (letter count)
- **Design tradeoffs**: Larger token vocabulary → fewer predictions per prompt but larger embedding matrix; character-level tokenization → simpler predictions but many more sequential steps
- **Failure signatures**: High error rates on character-level tasks; inconsistent results across words with similar lengths but different tokenizations; generalization gaps for letters not seen during fine-tuning
- **First 3 experiments**:
  1. Baseline measurement: Run base GPT-4o on counting "r" across 7,790 words to establish failure rate (~15% expected)
  2. Fine-tune and retest: Fine-tune GPT-4o on (word, count) pairs for "r" using separate 7,790-word subset; measure failure rate on held-out test set (target: <1%)
  3. Cross-letter transfer test: Apply "r"-fine-tuned model to count "a" and "m"; compare failure rates against base model to quantify generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does fine-tuning on consonants (e.g., 'r', 'm') facilitate cross-letter generalization, while fine-tuning on the vowel 'a' shows minimal transfer to other letters?
- **Basis in paper**: Table 4 shows that while fine-tuning on 'r' or 'm' significantly lowers error rates for other consonants, fine-tuning on 'a' leaves error rates for 'r' (8.42%) and 'm' (4.85%) relatively high compared to the base model
- **Why unresolved**: The authors note the asymmetry but do not explain if this is due to the frequency of 'a' in tokens, vowel-specific tokenization patterns, or distinct internal representations
- **What evidence would resolve it**: An analysis of token embeddings and attention patterns to determine if vowels are processed as distinct sub-token units differently than consonants

### Open Question 2
- **Question**: Does the fine-tuned model learn a dynamic counting algorithm, or does it simply memorize static character counts for specific tokens?
- **Basis in paper**: The authors hypothesize that the model learns "how many 'r' are in each token," implying a lookup-based mechanism rather than a procedural understanding of character sequences
- **Why unresolved**: The paper evaluates the model on standard dictionary words but does not test if the model can count letters in novel, made-up words (nonce words) or unseen token combinations
- **What evidence would resolve it**: Testing the fine-tuned model on out-of-distribution nonce words to see if it generalizes the counting logic or fails on unseen token-letter pairings

### Open Question 3
- **Question**: Do these generalization findings hold for languages with non-Latin scripts or different tokenization granularities?
- **Basis in paper**: The study is restricted to English and GPT-4o's specific tokenizer, which splits words based on frequency in English text
- **Why unresolved**: It is unclear if the model can learn to count in languages where tokens represent whole words (e.g., Chinese) or where sub-word tokenization behaves differently
- **What evidence would resolve it**: Replicating the fine-tuning experiment on a multilingual model using languages with distinct orthographic systems

## Limitations

- Limited generalization scope: Cross-letter transfer effectiveness varies significantly by letter type, with weaker generalization for vowels
- Reproducibility constraints: Missing fine-tuning hyperparameters, prompt formatting, and system messages prevent precise replication
- Tokenization variability: Results may be specific to GPT-4o's tokenizer rather than generalizable across LLM architectures

## Confidence

- **High Confidence**: The core finding that LLMs struggle with character-level counting due to tokenization is well-supported by both the paper's results and broader literature
- **Medium Confidence**: The mechanism explanation (token-to-character association learning through fine-tuning) is plausible but inferred rather than directly observed
- **Low Confidence**: The cross-letter generalization mechanism lacks empirical validation beyond showing reduced failure rates

## Next Checks

1. **Ablation on Letter Properties**: Systematically test generalization across letters varying in frequency, phonetic properties, and token distribution to determine which characteristics enable or hinder transfer

2. **Mechanistic Probe Experiments**: Use feature attribution or attention visualization techniques to examine whether the fine-tuned model develops specific token-level representations for character counting

3. **Tokenization Strategy Comparison**: Repeat the fine-tuning experiment using different tokenization approaches (WordPiece, SentencePiece, character-level) to isolate whether improvements stem from the fine-tuning procedure itself or from interactions with specific tokenizers