---
ver: rpa2
title: 'Theory of Mind in Action: The Instruction Inference Task'
arxiv_id: '2507.02935'
source_url: https://arxiv.org/abs/2507.02935
tags:
- human
- fs-cot
- instruction
- participants
- door
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Instruction Inference, a novel task to evaluate
  Theory of Mind (ToM) in AI agents by having them interpret indirect or ambiguous
  instructions in a collaborative environment. Tomcat, an LLM-based agent, is designed
  to infer the principal's intentions and generate optimal responses.
---

# Theory of Mind in Action: The Instruction Inference Task

## Quick Facts
- arXiv ID: 2507.02935
- Source URL: https://arxiv.org/abs/2507.02935
- Reference count: 40
- Primary result: Fs-CoT variant of Tomcat agent achieves up to 85% intent accuracy vs 50% for CP variant

## Executive Summary
This paper introduces the Instruction Inference task as a novel method to evaluate Theory of Mind (ToM) capabilities in AI agents. The task requires agents to interpret indirect or ambiguous instructions in collaborative environments by inferring the principal's latent intentions. Tomcat, an LLM-based agent, is designed to solve this task through two variants: Commonsense Prompt (CP) and Few-shot Chain-of-Thought (Fs-CoT). The study compares these variants across multiple LLM models (GPT-4o, DeepSeek-R1, Gemma-3-27B) against human performance baselines, demonstrating that Fs-CoT significantly outperforms CP in intent accuracy, action optimality, and plan optimality.

## Method Summary
The research introduces Tomcat, an LLM-based agent designed to interpret indirect instructions by inferring latent goals through Theory of Mind reasoning. Two Tomcat variants are implemented: CP uses commonsense reasoning prompts, while Fs-CoT employs few-shot Chain-of-Thought exemplars to guide structured reasoning. The agents are evaluated on three metrics: intent accuracy (correctly identifying the principal's goal), action optimality (generating appropriate actions), and plan optimality (creating coherent plans). The study includes 52 human participants as a performance baseline and tests three LLM models: GPT-4o, DeepSeek-R1, and Gemma-3-27B. Results show Fs-CoT consistently outperforms CP across all models, with GPT-4o and DeepSeek-R1 matching human performance while Gemma-3-27B underperforms, likely due to model size limitations.

## Key Results
- Fs-CoT achieves 85% intent accuracy versus 50% for CP variant
- Action optimality improves from 55% (CP) to 92.5% (Fs-CoT)
- Plan optimality increases from 25% (CP) to 75% (Fs-CoT)
- GPT-4o Fs-CoT and DeepSeek-R1 match human performance in ToM reasoning
- Gemma-3-27B underperforms across all metrics, likely due to model size limitations

## Why This Works (Mechanism)
Fs-CoT improves ToM capabilities by providing structured reasoning exemplars that guide agents through the inference process. The few-shot Chain-of-Thought approach supplies concrete examples of how to decompose ambiguous instructions, identify latent goals, and generate appropriate responses. This scaffolding enables agents to perform multi-step reasoning rather than relying solely on surface-level pattern matching. The exemplars demonstrate the reasoning path from indirect instruction to inferred intention to optimal action, helping agents internalize the ToM framework needed for the task.

## Foundational Learning
- Theory of Mind (ToM): The ability to attribute mental states to others and understand their intentions, beliefs, and knowledge. Needed to interpret indirect instructions that rely on understanding unstated goals. Quick check: Can the agent distinguish between literal and intended meanings of ambiguous statements?
- Chain-of-Thought (CoT) reasoning: A prompting technique that guides models through explicit reasoning steps. Needed to break down complex inference tasks into manageable components. Quick check: Does the agent generate intermediate reasoning steps before producing final answers?
- Intent inference: The process of deducing latent goals from observable actions or statements. Needed to bridge the gap between indirect instructions and appropriate responses. Quick check: Can the agent identify the underlying purpose behind ambiguous requests?
- Plan optimality: The quality of generated action sequences in achieving inferred goals. Needed to evaluate whether ToM reasoning translates into practical outcomes. Quick check: Are the generated actions coherent and goal-directed?

## Architecture Onboarding
Component map: Human participant data -> Evaluation metrics -> Tomcat agent variants (CP, Fs-CoT) -> LLM models (GPT-4o, DeepSeek-R1, Gemma-3-27B) -> Task scenarios -> Performance comparison
Critical path: Scenario presentation → Intent inference → Action generation → Plan creation → Performance evaluation
Design tradeoffs: Fs-CoT provides better performance but requires more complex prompt engineering versus CP's simplicity
Failure signatures: Gemma-3-27B shows consistent underperformance across all metrics, suggesting model capacity limitations
First experiments: 1) Test Gemma-3-27B with extended context length, 2) Compare Fs-CoT with different exemplar sets, 3) Evaluate cross-cultural scenario variations

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Human participant sample size of 52 may limit generalizability of baseline comparisons
- Evaluation metrics for intent, action, and plan optimality lack detailed explanation, potentially introducing subjectivity
- The explanation for Gemma-3-27B's underperformance attributes it solely to model size without controlled ablation studies

## Confidence
- Task design validity: High - methodologically sound with appropriate baselines
- Agent performance improvements: High - well-supported by comparative data
- Generalizability across domains: Medium - limited scenario diversity tested
- Explanation of Gemma-3-27B underperformance: Low - oversimplified attribution to model size

## Next Checks
1. Conduct ablation studies on Gemma-3-27B using different prompt formats and context lengths to isolate whether model size is the primary factor in its underperformance, rather than other architectural or training differences.

2. Test the Instruction Inference Task with a more diverse set of scenarios that vary in cultural context, complexity, and domain specificity to evaluate the generalizability of Fs-CoT's advantages.

3. Implement process-based evaluation to examine the intermediate reasoning steps generated by agents using Fs-CoT versus CP, providing direct evidence of whether and how latent goal inference actually occurs.