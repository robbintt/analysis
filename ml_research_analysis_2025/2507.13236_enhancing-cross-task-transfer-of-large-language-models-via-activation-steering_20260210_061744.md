---
ver: rpa2
title: Enhancing Cross-task Transfer of Large Language Models via Activation Steering
arxiv_id: '2507.13236'
source_url: https://arxiv.org/abs/2507.13236
tags:
- few-shot
- question
- tasks
- transfer
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-task transfer learning for large
  language models (LLMs) by steering activation patterns in the latent space rather
  than using traditional input expansion or parameter updates. The authors propose
  CAST (Cross-task Activation Steering Transfer), a method that first selects a small,
  influential, and diverse subset of examples from high-resource tasks using a similarity-based
  graph search, then extracts contrastive representation-enhanced activations from
  these examples and injects them into the forward pass of low-resource tasks.
---

# Enhancing Cross-task Transfer of Large Language Models via Activation Steering

## Quick Facts
- **arXiv ID:** 2507.13236
- **Source URL:** https://arxiv.org/abs/2507.13236
- **Reference count:** 40
- **Primary result:** Proposes CAST, a method using activation steering to transfer knowledge from high-resource to low-resource tasks without parameter updates, achieving up to 81.97% accuracy in cross-domain tasks and 94.88% in cross-lingual tasks.

## Executive Summary
This paper addresses cross-task transfer learning for large language models by steering activation patterns in the latent space rather than using traditional input expansion or parameter updates. The authors propose CAST (Cross-task Activation Steering Transfer), a method that first selects a small, influential, and diverse subset of examples from high-resource tasks using a similarity-based graph search, then extracts contrastive representation-enhanced activations from these examples and injects them into the forward pass of low-resource tasks. Experiments across cross-domain and cross-lingual settings show that CAST consistently outperforms competitive baselines, achieving superior scalability and lower computational costs compared to traditional in-context learning approaches.

## Method Summary
CAST operates by first selecting a small subset of influential and diverse examples from a high-resource source task using a graph-based search algorithm. For each selected example, the method extracts a contrastive representation by computing the difference between few-shot and zero-shot activations at a specific layer's final token position. During inference on a low-resource target task, this pre-computed steering vector is injected into the model's hidden state at the same layer, effectively transferring task-level features without modifying parameters or expanding input context. The method requires access to internal model representations and uses Llama3.1-8B-Instruct as the primary model for experiments.

## Key Results
- CAST achieves 81.97% accuracy in cross-domain transfer tasks, outperforming competitive baselines including few-shot ICL
- In cross-lingual settings, CAST reaches 94.88% accuracy while maintaining superior computational efficiency
- The method shows positive correlation between the number of source examples and model performance, unlike discrete ICL which plateaus
- CAST demonstrates robustness to task dissimilarity, performing better than traditional cross-task ICL in challenging transfer scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context examples induce consistent directional shifts in the latent space across different tasks.
- Mechanism: The paper observes that the activation difference vectors between few-shot and zero-shot prompts are nearly parallel across different domains (e.g., ARC-C vs. AGnews, ARC-C vs. QQP). This suggests the "enhanced information" or task-level features provided by in-context examples follow a consistent pattern/direction in the model's internal representation space, regardless of the specific task instance.
- Core assumption: This mechanism assumes that the latent space encodes general task-solving heuristics or representations that are transferable, rather than just instance-specific information.
- Evidence anchors:
  - [abstract]: "the enhanced activations induced by in-context examples have consistent patterns across different tasks."
  - [section 3.2]: "Furthermore, the difference vectors between these two prompting activations across domains are nearly parallel, suggesting that the enhanced information induced by in-context examples follows a consistent direction across domains."
- Break condition: This mechanism may break if the high-resource source task and low-resource target task are so fundamentally different that their latent activation patterns are orthogonal or conflicting, rather than parallel.

### Mechanism 2
- Claim: Steering with contrastive representations (Few-shot activation - Zero-shot activation) isolates task-level features.
- Mechanism: By computing the difference between a model's activation when given a few-shot prompt (with demonstrations) and a zero-shot prompt (without), the method attempts to subtract "instance-specific noise" or the base model's prior, leaving behind a vector that represents the "enhanced" information or the specific influence of the in-context examples.
- Core assumption: This assumes that the zero-shot activation serves as a clean baseline that captures the model's prior behavior without the influence of the specific task structure demonstrated by the examples.
- Evidence anchors:
  - [abstract]: "utilizes their contrastive representation-enhanced activations..."
  - [section 4.3]: "To eliminate instance-specific noise and capture general task-level features, we compute the average activation difference across all samples."
- Break condition: The mechanism fails if the zero-shot prompt's activation is not a suitable baseline or if the few-shot activation contains significant spurious signals that are not subtracted.

### Mechanism 3
- Claim: Injecting pre-computed steering vectors at inference improves efficiency and scalability over input expansion.
- Mechanism: Instead of concatenating examples into the context window (which consumes tokens and adds quadratic computational overhead), CAST pre-computes a steering vector from source task data. This vector is then injected as a constant addition to the hidden state at a specific layer during the forward pass for any target task query.
- Core assumption: The assumption is that the computational benefit of avoiding the attention mechanism's quadratic scaling outweighs the cost of pre-computing the vector, and that the steering vector's influence generalizes across different target queries.
- Evidence anchors:
  - [abstract]: "...without parameter updates or input expansion."
  - [section 5.3.2]: "In contrast, our method shows a positive correlation between the number of examples and model performance... our approach does not rely on expanding the input context, it completely avoids the context window constraints."
- Break condition: The mechanism breaks if the pre-computed vector is too specific to the source task samples used to create it, leading to negative transfer or poor generalization on the target task.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The entire method is built on the idea of extracting what ICL "does" to the model's internal state. Understanding ICL is the prerequisite to understanding what the paper is trying to capture and transfer.
  - Quick check question: What is the key difference between the inputs used to generate the "Few-shot" vs. "Zero-shot" activations in this paper?

- **Concept: Residual Stream & Inter-layer Communication**
  - Why needed here: The method injects a steering vector at a specific layer's hidden state. One must understand how information flows through the layers (the residual stream) to grasp how this intervention affects the final output.
  - Quick check question: Why does the paper recommend injecting the steering vector into a middle or late layer (e.g., around layers 16-20 in a 32-layer model) rather than the very first layer?

- **Concept: Cross-Task Transfer**
  - Why needed here: The core objective is to transfer knowledge from a high-resource task to a low-resource task. This requires understanding the principles of transfer learning—what kinds of knowledge are transferable and what are not.
  - Quick check question: According to the paper, what is a potential failure mode of traditional, discrete cross-task ICL when the source and target tasks are very dissimilar, and how does CAST aim to mitigate it?

## Architecture Onboarding

- **Component map:** Source Selector -> Steering Vector Generator -> Latent Steering Module
- **Critical path:** The most critical step is the Steering Vector Generator. If the computed vector does not represent a generalizable task-level concept, the entire transfer will fail. The quality of the source samples (Component 1) directly impacts this.
- **Design tradeoffs:**
  - Sample Size vs. Efficiency: More source samples lead to a more robust, averaged steering vector but increase pre-computation time.
  - Injection Layer: The paper finds middle layers (e.g., 16-20) are best. Earlier layers may be too raw, later layers too specialized for output generation.
  - Influence vs. Diversity: The source sample selection algorithm balances these two. Over-prioritizing diversity might miss crucial examples; over-prioritizing influence might create a redundant, non-diverse set.
- **Failure signatures:**
  - Negative Transfer: Performance on the target task drops below the zero-shot baseline.
  - Performance Plateau: Increasing the number of source samples yields no further gains.
  - Sensitivity to Injection Strength (λ): If λ is too low, the vector has no effect; if too high, it disrupts semantic coherence.
- **First 3 experiments:**
  1. Baseline Comparison: Implement CAST and compare against zero-shot and few-shot ICL baselines on a single target domain.
  2. Ablation on Vector Construction: Test whether "contrastive" (Few-shot - Zero-shot) activations are crucial versus just using Few-shot activations.
  3. Hyperparameter Scan for Injection Layer: Run a sweep, injecting at different layers to find the optimal intervention point.

## Open Questions the Paper Calls Out

- **Question:** Can activation steering approaches like CAST be adapted to work with closed-source LLMs that do not expose internal activations?
- **Question:** Does CAST generalize to vision-language models (VLMs), and if so, which activation layers and modalities should be targeted for steering?
- **Question:** What theoretical properties explain why few-shot/zero-shot activation differences are nearly parallel across diverse tasks, and can this be formally characterized?
- **Question:** Under what conditions does CAST cause negative transfer, and can failure modes be predicted from task characteristics?

## Limitations

- The method requires access to internal model representations, making it infeasible for closed-source LLMs
- The theoretical explanation for why contrastive representations isolate task-level features lacks external validation
- Generalizability to larger model scales and different model families remains unproven
- The robustness to task dissimilarity is suggested but not rigorously tested across diverse task pairs

## Confidence

- **High Confidence:** The empirical results showing CAST outperforming baselines (81.97% vs 78.84% accuracy) are well-supported by experimental data
- **Medium Confidence:** The theoretical explanation of why contrastive representations isolate task-level features has internal consistency but lacks external validation
- **Low Confidence:** The generalizability of the sample selection algorithm to completely different task types or larger model scales remains unproven

## Next Checks

1. **Zero-shot Baseline Sensitivity:** Systematically vary the zero-shot prompt structure and measure how resulting steering vectors and downstream performance change to test whether contrastive representation truly isolates task-level features.

2. **Task Dissimilarity Stress Test:** Design an experiment where source and target tasks are deliberately chosen to be maximally dissimilar to measure whether CAST maintains its performance advantage over discrete ICL.

3. **Steering Vector Transferability:** Extract steering vectors from one source task and apply them to multiple unrelated target tasks to determine if vectors encode general task-solving heuristics rather than instance-specific information.