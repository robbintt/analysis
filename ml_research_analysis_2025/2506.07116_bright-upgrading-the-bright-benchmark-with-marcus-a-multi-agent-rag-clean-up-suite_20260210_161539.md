---
ver: rpa2
title: 'BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up
  Suite'
arxiv_id: '2506.07116'
source_url: https://arxiv.org/abs/2506.07116
tags:
- bright
- content
- retrieval
- chunk
- marcus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIGHT+ addresses data quality issues in the BRIGHT benchmark by
  cleaning and restructuring web-derived documents that contain structural noise and
  semantic discontinuity. The authors develop MARCUS, an LLM-driven pipeline with
  dedicated agents for noise removal and semantic segmentation, focusing on seven
  StackExchange subdomains where quality issues are most severe.
---

# BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite

## Quick Facts
- arXiv ID: 2506.07116
- Source URL: https://arxiv.org/abs/2506.07116
- Reference count: 19
- Key outcome: BRIGHT+ cleans and restructures web-derived documents in BRIGHT, achieving 14-34 pp nDCG@10 gains for dense retrievers and 3-8 pp for BM25 while reducing corpus from 511K to 18K chunks (97% reduction)

## Executive Summary
BRIGHT+ addresses data quality issues in the BRIGHT benchmark by cleaning and restructuring web-derived documents that contain structural noise and semantic discontinuity. The authors develop MARCUS, an LLM-driven pipeline with dedicated agents for noise removal and semantic segmentation, focusing on seven StackExchange subdomains where quality issues are most severe. Experimental results show BRIGHT+ achieves substantial retrieval improvements: dense retrievers (BGE, GritLM, QWEN2) gain 14-34 percentage points in nDCG@10, while BM25 improves 3-8 points. The corpus is reduced from 511,497 to 18,187 chunks (97% reduction), with chunks shifting toward longer, semantically coherent units. Ablation studies confirm that semantic segmentation is the primary driver of performance gains, with cleaning providing incremental benefits. The dataset and pipeline are released to support future research on high-quality retrieval-augmented reasoning.

## Method Summary
MARCUS is a three-stage LLM-driven pipeline that cleans and restructures the BRIGHT benchmark corpus. The pipeline first aggregates documents into Gold Parts (answer-bearing spans) and Irrelevant Parts using BRIGHT's span annotations. SafeClean then conservatively removes interface artifacts from Gold Parts while preserving verbatim text, and FastClean aggressively filters irrelevant content. Finally, Splitter performs semantic segmentation to create minimal, coherent chunks. The method relies on provided LLM prompts and gold chunk annotations from BRIGHT to guide cleaning and segmentation decisions.

## Key Results
- BRIGHT+ achieves 14-34 percentage point gains in nDCG@10 for dense retrievers (BGE, GritLM, QWEN2) and 3-8 points for BM25
- Corpus reduced from 511,497 to 18,187 chunks (97% reduction) with chunks shifting to 300-800 tokens
- Semantic segmentation is primary driver of gains; MARCUS-S (split-only) achieves ~80% of full pipeline performance
- BM25 performance degrades 4.5 and 6.8 points on Earth Science and Stackoverflow subdomains respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based semantic segmentation is the primary driver of retrieval performance gains.
- Mechanism: The Splitter agent creates longer, semantically coherent chunks by placing boundaries only at genuine topic shifts. This preserves rhetorical flow and contextual cues that retrievers depend on for relevance matching. Chunks shift from fragmented sub-100 token units to 300-800 token self-contained passages.
- Core assumption: Dense retrievers perform better when semantic units are intact rather than arbitrarily split across passages.
- Evidence anchors: [abstract] "ablation studies confirm that semantic segmentation is the primary driver of performance gains"; [section 4.4] "MARCUS-S achieves better retrieval performance than MARCUS" across all retrievers (e.g., QWEN2: 45.17 vs 42.63)

### Mechanism 2
- Claim: Dual-agent cleaning with differentiated strategies preserves answer-bearing spans while removing noise.
- Mechanism: SafeClean conservatively removes only interface artifacts (headers, navigation, banners) from gold parts without paraphrasing. FastClean aggressively filters irrelevant documents, returning empty strings when no substantive content exists. This asymmetric approach protects ground-truth annotations while reducing corpus noise.
- Core assumption: The span-level gold annotations in BRIGHT correctly identify all answer-bearing content.
- Evidence anchors: [abstract] "MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity"; [section 3.3] "SafeClean preserves every annotated span verbatim while excising only interface-level clutter"

### Mechanism 3
- Claim: Reducing corpus size while improving semantic coherence shifts query-chunk similarity distributions higher.
- Mechanism: By removing 97% of chunks (511K to 18K), MARCUS eliminates low-signal fragments. The resulting chunks show higher average query-chunk similarity scores, reducing false-positive matches and improving retriever precision.
- Core assumption: Original fragmented chunks contained low-relevance content that diluted retrieval signals.
- Evidence anchors: [section 4.3] "Figure 5 shows a clear shift toward higher query-chunk similarity scores after cleaning. Low-score matches become less dominant, while mid- and high-score ranges grow noticeably"; [section 4.3] "total chunk count drops from 511,497 to 18,187 (a 97% reduction)"

## Foundational Learning

- Concept: **Semantic chunking vs. fixed-window segmentation**
  - Why needed here: Understanding why LLM-based splitting outperforms newline/token-based heuristics is essential for reproducing results.
  - Quick check question: Given a document with 5 distinct topic paragraphs, would fixed 512-token windows preserve or fragment the semantic boundaries?

- Concept: **Gold vs. irrelevant document classification**
  - Why needed here: The Aggregate step relies on span-level annotations to partition documents; without this understanding, the pipeline cannot be adapted.
  - Quick check question: If a document contains one answer-bearing sentence in a 500-word article, how should cleaning strategies differ for the gold span versus surrounding context?

- Concept: **Dense vs. sparse retriever sensitivity to corpus quality**
  - Why needed here: Results show dense retrievers gain 14-34 pp while BM25 gains only 3-8 pp; understanding why informs deployment choices.
  - Quick check question: Why would embedding-based models benefit more from semantic coherence improvements than lexical matching models?

## Architecture Onboarding

- Component map: Aggregate -> (SafeClean on gold + FastClean on irrelevant) -> Splitter -> BRIGHT+ corpus
- Critical path: Aggregate → (SafeClean on gold + FastClean on irrelevant) → Splitter → BRIGHT+ corpus
- Design tradeoffs:
  - MARCUS-S (split-only) vs. full MARCUS: Split-only achieves ~80% of retrieval gains with lower token cost; full pipeline adds marginal cleaning benefit for end-to-end RAG tasks
  - Chunk length vs. retrieval granularity: Longer chunks (300-800 tokens) improve context but may reduce precision for fact-specific queries
  - LLM cost vs. rule-based cleaning: MARCUS requires LLM inference for all 511K chunks; not scalable without optimization

- Failure signatures:
  - BM25 performance degradation on Stackoverflow (-6.8 pp) suggests cleaning altered term distributions in domain-specific corpora
  - FastClean returning empty strings for documents that may contain latent relevance signals
  - Splitter over-merging heterogeneous content into single chunks when no clear topic boundary exists

- First 3 experiments:
  1. Replicate MARCUS-S on a 10% sample of BRIGHT to validate semantic segmentation gains without full LLM cost
  2. Compare SafeClean vs. rule-based HTML stripping on gold documents to quantify LLM cleaning value-add
  3. Test chunk-length sensitivity: evaluate retrieval at 200/400/600 token targets to find optimal coherence-granularity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MARCUS be adapted to datasets lacking span-level gold annotations, and what minimal supervision is required for effective cleaning and segmentation?
- Basis in paper: [explicit] "the pipeline depends on BRIGHT-specific supervision, such as span-level annotations, which limits its direct applicability to benchmarks without similar metadata."
- Why unresolved: The current pipeline leverages gold chunk annotations to distinguish Gold Parts from Irrelevant Parts; generalization requires alternative weak supervision signals.
- What evidence would resolve it: Experiments applying MARCUS variants to benchmarks (e.g., BEIR, MTEB) using only query-document pairs, comparing retrieval performance against rule-based baselines.

### Open Question 2
- Question: What factors predict whether corpus cleaning will benefit or harm different retrieval paradigms (sparse vs. dense)?
- Basis in paper: [inferred] BM25 degrades 4.5 and 6.8 points on Earth Science and Stackoverflow respectively, while dense retrievers gain 14-34 points across all domains.
- Why unresolved: The authors note "detrimental shifts in term distribution caused by data rewriting" but do not characterize when these shifts occur or how to anticipate them.
- What evidence would resolve it: Analysis correlating domain-specific lexical statistics (term frequency distribution, vocabulary overlap) with BM25 performance change; predictive models for cleaning impact.

### Open Question 3
- Question: Does the 97% corpus reduction disproportionately impact edge-case queries or long-tail reasoning tasks not captured in current evaluations?
- Basis in paper: [inferred] The corpus reduction from 511,497 to 18,187 chunks is justified by nDCG@10 improvements, but evaluation covers only existing BRIGHT queries.
- Why unresolved: Aggressive filtering may preserve performance on benchmark queries while removing content relevant to queries outside the test distribution.
- What evidence would resolve it: Holdout analysis with synthetic or crowd-sourced queries targeting removed chunks; measuring recall@k for relevant documents that were eliminated.

### Open Question 4
- Question: How can MARCUS be optimized for computational efficiency without sacrificing semantic segmentation quality?
- Basis in paper: [explicit] "Its reliance on large LLMs incurs high computational cost, making it challenging to scale without further optimization."
- Why unresolved: The authors release the pipeline but do not address deployment costs or alternative model configurations.
- What evidence would resolve it: Ablation comparing smaller LLMs (e.g., 7B vs. 70B parameters) or distilled models for the Splitter agent, measuring both retrieval performance and throughput/latency.

## Limitations
- Unknown LLM model and configuration parameters (temperature, max_tokens, model version) for MARCUS agents
- Unclear format of BRIGHT gold chunk annotations and algorithm for reconstructing Gold Parts
- No external validation of the 97% corpus reduction strategy's completeness
- BM25 degradation on Stackoverflow suggests domain-specific limitations

## Confidence
**High Confidence Claims:**
- BRIGHT+ achieves substantial retrieval improvements (14-34 pp for dense retrievers, 3-8 pp for BM25) based on clearly specified metrics and controlled experiments
- Semantic segmentation is the primary driver of performance gains, supported by direct ablation studies comparing MARCUS-S vs full MARCUS
- The corpus reduction from 511,497 to 18,187 chunks is verifiable through the described pipeline

**Medium Confidence Claims:**
- LLM-based semantic segmentation outperforms fixed-window approaches due to preserved rhetorical flow and contextual cues
- Dual-agent cleaning strategy effectively preserves answer-bearing spans while removing noise
- Query-chunk similarity distributions shift higher after cleaning, improving retrieval precision

**Low Confidence Claims:**
- The exact mechanism by which semantic coherence improves dense retriever performance at the embedding level
- Whether the 97% corpus reduction maintains complete semantic coverage for all potential downstream applications
- The generalizability of results beyond the seven StackExchange subdomains tested

## Next Checks
1. **Reproduce core retrieval gains with minimal MARCUS-S implementation**: Implement only the Splitter agent on a 10% sample of BRIGHT to validate whether semantic segmentation alone achieves the reported ~80% of performance gains, using a clearly specified LLM model (e.g., GPT-3.5-Turbo with defined parameters).

2. **Validate gold span preservation across cleaning stages**: Implement span-level comparison between original BRIGHT annotations and SafeClean outputs to ensure no answer-bearing content is inadvertently removed, addressing the critical assumption that gold annotations are complete and accurate.

3. **Test domain generalization of corpus reduction strategy**: Apply MARCUS to a different high-quality benchmark (e.g., Natural Questions or MS MARCO) to evaluate whether the 97% reduction and semantic segmentation approach generalizes beyond StackExchange domains, particularly examining whether term distribution changes affect lexical matching models like BM25.