---
ver: rpa2
title: 'Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive
  and Proscriptive Constraints'
arxiv_id: '2512.20781'
source_url: https://arxiv.org/abs/2512.20781
tags:
- soft
- cirevl
- searle
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Soft Filtering (SoFT) is a training-free re-ranking module for
  zero-shot composed image retrieval (ZS-CIR) that addresses the limitations of single
  fused queries by explicitly modeling both prescriptive (must-have) and proscriptive
  (must-avoid) user intent. It uses multimodal LLMs to extract dual textual constraints
  from reference-modification pairs and reweights candidate similarity scores without
  modifying the base retrieval model.
---

# Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints

## Quick Facts
- arXiv ID: 2512.20781
- Source URL: https://arxiv.org/abs/2512.20781
- Reference count: 18
- Primary result: SoFT improves R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59) as a training-free re-ranking module.

## Executive Summary
SoFT (Soft Filtering) addresses limitations in zero-shot composed image retrieval (ZS-CIR) where single fused queries fail to capture the full complexity of user intent. The method decomposes modification texts into prescriptive (must-have) and proscriptive (must-avoid) constraints using multimodal LLMs, then reweights retrieval scores through a soft combination of reward and penalty signals. Applied atop base retrievers like CIReVL and SEARLE without retraining, SoFT consistently improves retrieval performance across multiple benchmarks. The paper also introduces a two-stage dataset pipeline that constructs multi-target triplets and refines them into single-target variants, enabling more comprehensive evaluation under varying ambiguity levels.

## Method Summary
SoFT operates as a training-free re-ranking module that extracts dual textual constraints from reference-modification pairs using a multimodal LLM. The LLM performs attribute classification to identify attributes to keep, add, or remove, then generates prescriptive queries (combining keep+add) and proscriptive queries (describing what to avoid). For each candidate image, SoFT computes three CLIP similarities: the base retrieval score, a reward score based on prescriptive constraint similarity, and a penalty score based on proscriptive constraint similarity. These are combined as s_SoFT = s_base · s_reward + (1 - s_penalty)/2, then interpolated with the base score via s_final = (1-λ)·s_base + λ·s_SoFT. The method requires no retraining of the base retriever and can be applied to any ZS-CIR model.

## Key Results
- SoFT improves R@5 to 65.25 on CIRR (+12.94) when applied to CIReVL
- Achieves mAP@50 of 27.93 on CIRCO (+6.13) with CIReVL
- Increases R@50 to 58.44 on FashionIQ (+4.59) with SEARLE
- Consistently outperforms baseline and existing training-free methods across all benchmarks
- Multi-target FashionIQ variant shows more reliable evaluation by acknowledging semantically valid alternatives

## Why This Works (Mechanism)

### Mechanism 1: Dual Constraint Extraction via LLM Attribute Parsing
- Claim: Decomposing user intent into prescriptive and proscriptive constraints may improve retrieval precision by explicitly separating positive and negative signals that are typically conflated in single fused queries.
- Mechanism: A multimodal LLM performs two-step reasoning: (1) Attribute Classification categorizes attributes into keep, add, and remove sets; (2) Constraint Generation combines keep+add into a prescriptive query and transforms remove into a proscriptive query describing what to avoid.
- Core assumption: LLMs can reliably infer implicit proscriptive intent from modification texts that lack explicit negations, and these textual constraints align with CLIP's embedding space for effective similarity computation.
- Evidence anchors:
  - [abstract] "SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints."
  - [section 3.1] "Unlike generic descriptions, the LLM is instructed to infer attributes critical for achieving the transformation."
  - [corpus] Related work (FineCIR, CoLLM) supports fine-grained semantic parsing for CIR, though explicit prescriptive/proscriptive decomposition is novel to this approach.
- Break condition: If the base retriever's feature space poorly aligns with natural language descriptions, or if the LLM generates constraints that describe attributes not captured by the visual encoder, the reweighting signal becomes noise.

### Mechanism 2: Soft Score Reweighting with Reward-Penalty Modulation
- Claim: Combining reward and penalty signals through convex combination with base scores appears to improve ranking by promoting alignment with prescriptive intent while suppressing proscriptive attributes.
- Mechanism: For each candidate image, three CLIP similarities are computed: s_base (original retrieval score), s_reward (similarity to prescriptive constraint), and s_penalty (similarity to proscriptive constraint). The SoFT score s_soFT = s_base · s_reward + (1 - s_penalty)/2 is then blended with s_base via s_final = (1-λ)·s_base + λ·s_soFT.
- Core assumption: The multiplicative reward term and additive penalty modulation preserve meaningful ranking order while being differentiable enough to capture fine-grained relevance differences.
- Evidence anchors:
  - [section 3.1] "This formulation enables soft constraint enforcement by promoting candidates aligned with the prescriptive intent while down-weighting those associated with the proscriptive constraint."
  - [table 4] Component-wise ablation shows reward-only improves SEARLE from 24.24% to 29.08% R@1 on CIRR (ViT-L/14), while penalty-only catastrophically drops to 4.82%.
  - [corpus] No direct corpus comparison for this specific reweighting formula; similar score modulation techniques are unexplored in neighbor papers.
- Break condition: The penalty term appears fragile—when applied alone on FashionIQ, it causes severe performance degradation (SEARLE drops from 25.56% to 3.59% R@10 average), suggesting CLIP embeddings may conflate semantically related but visually distinct attributes, causing over-penalization of valid candidates.

### Mechanism 3: Multi-Target Triplet Construction for Ambiguity-Aware Evaluation
- Claim: Expanding single-target benchmarks to include multiple semantically valid targets may provide more reliable evaluation by acknowledging that modification texts often specify under-constrained queries.
- Mechanism: Stage 1 retrieves candidates via three criteria (textual-to-modification, compositional, visual similarity), then an LLM-based visual assessor assigns confidence scores (threshold τ=0.85) to identify valid multi-targets. Stage 2 rewrites modification texts to focus on single targets by referencing contrastive distractors.
- Core assumption: LLM-based visual assessment can reliably identify semantically valid targets without human annotation, and contrastive prompting produces discriminative modification texts.
- Evidence anchors:
  - [section 3.2] "This approach ensures high-quality semantic coverage while requiring no manual annotation."
  - [section 4.3] On Multi-Target FashionIQ, SoFT consistently improves performance (SEARLE +SoFT reaches 45.50 mAP@5 vs 40.78 baseline), whereas standard FashionIQ shows instability.
  - [corpus] CIRCO already supports multi-target evaluation; related work (CoLLM) addresses ambiguity via rewriting but assumes single-target ground truth.
- Break condition: If the LLM assessor's confidence threshold is too permissive, spurious targets inflate evaluation metrics; if too strict, valid targets are missed, underestimating model performance.

## Foundational Learning

- Concept: **Composed Image Retrieval (CIR)**
  - Why needed here: The entire paper addresses CIR, where a query consists of a reference image plus a modification text. Understanding this task framing is prerequisite to grasping why single fused queries fail and why dual constraints help.
  - Quick check question: Given a reference image of a "red dress" and modification "make it blue," what is the target image? (Answer: A blue dress preserving other attributes like style, length, etc.)

- Concept: **Zero-Shot CIR (ZS-CIR) and Training-Free Methods**
  - Why needed here: SoFT operates as a training-free module atop existing ZS-CIR retrievers. Understanding why training-free approaches are desirable (avoid triplet annotation costs, improve generalization) explains the design constraint of not modifying base models.
  - Quick check question: Why can't SoFT simply be trained end-to-end with the base retriever? (Answer: The goal is zero-shot, plug-and-play compatibility with any retriever without supervision.)

- Concept: **CLIP Vision-Language Alignment**
  - Why needed here: All similarity computations rely on CLIP's joint embedding space. The assumption that textual constraints map to semantically meaningful regions in this space underpins the entire approach.
  - Quick check question: If CLIP embeddings for "white shirt" and "cream shirt" are nearly identical, how would this affect SoFT's penalty term? (Answer: Valid cream-colored targets might be incorrectly penalized when "white" is proscriptive.)

## Architecture Onboarding

- Component map:
  Input: (Reference Image, Modification Text)
       ↓
  [LLM: Dual Constraint Extraction Prompt]
       ↓
  Prescriptive Query ─────┐
                          ├─→ [CLIP Text Encoder] ─→ Constraint Features
  Proscriptive Query ─────┘
       ↓
  Base Retriever (CIReVL/SEARLE) ─→ Candidate Images + s_base
       ↓
  [CLIP Image Encoder] ─→ Candidate Features
       ↓
  Similarity Computation: s_reward, s_penalty
       ↓
  Score Reweighting: s_final = (1-λ)·s_base + λ·s_soFT
       ↓
  Re-ranked Results

- Critical path:
  1. **Prompt engineering for constraint extraction** (most failure-prone)—must generate prescriptive/proscriptive queries that are concrete, absolute, and grounded in reference image visual clues.
  2. **Similarity computation alignment**—constraint features and candidate features must be in comparable CLIP embedding space.
  3. **λ tuning**—Table S1-S3 show SEARLE optimal at λ≈0.2, CIReVL at λ=1.0; wrong choice degrades performance.

- Design tradeoffs:
  - **λ=1.0 vs λ<1.0**: Full reliance on SoFT scores (λ=1.0) works for CIReVL but severely degrades SEARLE on some benchmarks; suggests different retrievers produce base scores with varying reliability.
  - **Penalty inclusion**: Ablation shows penalty-only is catastrophic for SEARLE but beneficial for CIReVL; penalty sensitivity correlates with base retriever characteristics.
  - **LLM choice**: Paper uses GPT-4o (temperature=0.0); smaller models may produce less reliable constraint extraction.

- Failure signatures:
  - **Penalty over-suppression**: On FashionIQ, penalty-only causes ~15-20% absolute R@10 drops for SEARLE; CLIP conflates fine-grained fashion attributes.
  - **Domain sensitivity**: Mixed results on standard FashionIQ vs consistent gains on Multi-Target FashionIQ suggest single-target evaluation penalizes retrieval of semantically valid alternatives.
  - **LLM constraint quality**: If modification text is highly abstract ("more professional"), LLM may generate vague constraints that don't improve discrimination.

- First 3 experiments:
  1. **Reproduce CIReVL+SoFT on CIRR validation split** with λ=1.0, verify R@5 improvement from ~52% to ~65%. Use provided code and GPT-4o API. Log per-sample s_reward and s_penalty to identify cases where penalty misfires.
  2. **Ablate constraint extraction quality** by replacing LLM-generated constraints with (a) original modification text only, (b) template-based "a photo of [modification]" to quantify LLM contribution vs prompt structure.
  3. **Test penalty robustness on domain shift**: Apply SoFT to FashionIQ with penalty term disabled (reward-only), compare to full SoFT. If reward-only outperforms full SoFT, confirm CLIP embedding limitations in fine-grained domains per ablation findings in Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the interpolation weight λ be automatically adapted per retriever or query, rather than requiring manual tuning?
- Basis in paper: [explicit] "SEARLE exhibits sensitivity to the choice of λ... Consequently, treating λ as a tunable parameter rather than a fixed constant may be beneficial for achieving stable performance."
- Why unresolved: The paper manually sets λ=0.2 for SEARLE and λ=1.0 for CIReVL based on empirical observation but offers no principled method for automatic selection.
- What evidence would resolve it: A dynamic λ estimation method that correlates with retriever characteristics or per-query ambiguity metrics, validated across multiple base retrievers.

### Open Question 2
- Question: Why does the proscriptive (penalty) constraint cause severe performance degradation when applied alone, and how can negative constraints be made more reliable?
- Basis in paper: [explicit] "SEARLE shows strong sensitivity to the penalty term: using it alone leads to a severe performance drop... the penalty signal can become unreliable when applied in isolation, highlighting the need for balanced integration."
- Why unresolved: The penalty term alone drops SEARLE's CIRCO mAP@5 from 11.68 to 0.28, suggesting CLIP embeddings misalign with textually-derived negative constraints.
- What evidence would resolve it: Analysis of penalty-induced false negatives and alternative negative constraint formulations (e.g., contrastive embeddings, fine-tuned encoders) that maintain prescriptive-proscriptive balance.

### Open Question 3
- Question: Can SoFT maintain effectiveness when deployed with smaller, open-source LLMs instead of GPT-4o?
- Basis in paper: [inferred] All experiments use GPT-4o exclusively via API, with reported costs of $2.69–$17.68 per benchmark for SoFT alone.
- Why unresolved: The constraint extraction quality depends on LLM reasoning capability; using cheaper models could reduce quality but is untested.
- What evidence would resolve it: Comparative experiments with LLaMA, Mistral, or smaller GPT variants showing constraint quality versus cost trade-offs.

### Open Question 4
- Question: How does SoFT perform on domain-specific CIR tasks beyond fashion and general scenes (e.g., medical imaging, satellite imagery)?
- Basis in paper: [inferred] Evaluation is limited to CIRCO, CIRR, and FashionIQ; the method's reliance on CLIP embeddings and natural language constraints may face challenges in specialized visual domains.
- Why unresolved: Fine-grained domains already expose CLIP limitations; specialized domains may have different attribute-annotation vocabularies and ambiguity patterns.
- What evidence would resolve it: Cross-domain experiments measuring performance gaps and analyzing whether prescriptive/proscriptive decomposition remains semantically meaningful in technical domains.

## Limitations
- Domain sensitivity of penalty term causes severe performance degradation on FashionIQ (SEARLE drops from 25.56% to 3.59% R@10 average) when penalty is applied.
- LLM dependency and high API costs ($15–18 per benchmark for SoFT alone; $100+ for dataset construction) limit practical deployment.
- Standard FashionIQ evaluation assumes single-target ground truth, penalizing retrieval of semantically valid alternatives.

## Confidence
- **High Confidence**: The dual constraint extraction mechanism (prescriptive/proscriptive) provides theoretical benefits for CIR tasks where modification texts conflate positive and negative signals. The ablation study demonstrating reward-only improvements (SEARLE from 24.24% to 29.08% R@1 on CIRR) provides empirical support.
- **Medium Confidence**: The specific convex combination formula and λ tuning values are justified by ablation results but show strong dependence on base retriever characteristics (λ=1.0 for CIReVL vs λ=0.2 for SEARLE). The generalizability of these optimal settings to other retrievers remains untested.
- **Low Confidence**: The multi-target dataset construction pipeline's reliability is questionable due to its dependence on LLM-based visual assessment without human validation. The claim that this provides "more reliable evaluation" is based on internal consistency rather than external verification.

## Next Checks
1. **Penalty Term Robustness Across Domains**: Systematically test SoFT with penalty disabled (reward-only) on FashionIQ, CIRR, and CIRCO to quantify domain-specific penalty effects. Compare performance curves as λ penalty contribution varies from 0.0 to 1.0 to identify optimal penalty weighting per domain.

2. **LLM Substitution Impact**: Replace GPT-4o with smaller multimodal models (e.g., LLaVA, GPT-3.5-turbo) for constraint extraction while maintaining the same prompt structure. Measure performance degradation and token cost reduction to establish practical deployment thresholds.

3. **Cross-Retriever Generalization**: Apply SoFT with the same λ=0.2 setting across multiple base retrievers beyond SEARLE (e.g., ConText-CIR, CoLLM) to test whether the reported λ sensitivity is universal or retriever-specific. Track which components (reward, penalty, λ) require tuning for each retriever.