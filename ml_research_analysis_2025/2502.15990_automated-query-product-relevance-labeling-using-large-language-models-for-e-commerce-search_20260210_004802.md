---
ver: rpa2
title: Automated Query-Product Relevance Labeling using Large Language Models for
  E-commerce Search
arxiv_id: '2502.15990'
source_url: https://arxiv.org/abs/2502.15990
tags:
- relevance
- search
- query
- data
- query-product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to automate query-product relevance
  labeling for e-commerce search using Large Language Models (LLMs). The core idea
  is to use prompt engineering techniques such as Chain-of-Thought (CoT) prompting,
  In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum
  Marginal Relevance (MMR) to guide LLMs in generating accurate relevance labels.
---

# Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search

## Quick Facts
- **arXiv ID:** 2502.15990
- **Source URL:** https://arxiv.org/abs/2502.15990
- **Authors:** Jayant Sachdev; Sean D Rosario; Abhijeet Phatak; He Wen; Swati Kirti; Chittaranjan Tripathy
- **Reference count:** 33
- **Primary Result:** LLM-based approaches achieve accuracy comparable to human-labeled data while significantly reducing time and cost for query-product relevance labeling in e-commerce search.

## Executive Summary
This paper presents a method to automate query-product relevance labeling for e-commerce search using Large Language Models (LLMs). The approach employs prompt engineering techniques including Chain-of-Thought prompting, In-context Learning, and Retrieval Augmented Generation with Maximum Marginal Relevance to guide LLMs in generating accurate relevance labels. Experiments on open-source datasets (ESCI and WANDS) and proprietary Walmart Mexico data demonstrate that LLM-based methods achieve human-comparable accuracy while offering significant time and cost savings. The best-performing configuration using LLM5 with 16 Few-Shot examples and RAG with MMR achieves weighted F1 scores approaching human-level performance, showing strong potential for scalable query-product relevance labeling.

## Method Summary
The method uses LLMs to classify query-product pairs into relevance categories through prompt engineering. The approach involves three key techniques: In-Context Learning (ICL) with few-shot examples, Chain-of-Thought (CoT) reasoning prompts, and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR) for diverse example selection. The system retrieves relevant labeled examples from a vector store, constructs prompts with instructions and examples, and uses LLMs to generate relevance labels. The approach was tested on three datasets with varying class granularities and compared against human-labeled baselines.

## Key Results
- LLM5 with 16 Few-Shot examples and RAG with MMR achieves weighted F1 approaching human-level performance (human: 0.452, LLM5: 0.489) on Walmart Mexico dataset
- Increasing diversity through lower λ_MMR values improves accuracy metrics by adding novel information to retrieved examples
- In-Context Learning significantly improves over vanilla prompting (LLM5 + 16_FS: 0.656 Acc vs LLM5 + VANILLA: 0.415 Acc on WANDS)
- Open-source LLM2 achieves competitive results (0.726 Accuracy) with lower latency and privacy benefits compared to proprietary LLM5

## Why This Works (Mechanism)

### Mechanism 1: MMR Diversity Improves Classification
- Diverse few-shot examples retrieved via MMR improve classification accuracy over purely similarity-based retrieval by preventing redundancy and maximizing marginal information gain
- Core assumption: LLMs benefit more from diverse examples than from redundant similar examples
- Evidence: LLM2 + 16_FS_RAG_MMR_0 achieves 0.726 Accuracy vs LLM2 + 16_FS_RAG at 0.609 on WANDS
- Break condition: If diverse examples are too semantically distant, the LLM may hallucinate

### Mechanism 2: In-Context Learning Aligns LLM to Task
- ICL conditions the model's predictive distribution on demonstrated input-output patterns without weight updates
- Core assumption: Pre-trained LLM has sufficient reasoning capability to map demonstrations to new queries
- Evidence: LLM5 + 16_FS (0.656 Acc) significantly outperforms LLM5 + VANILLA (0.415 Acc) on WANDS
- Break condition: If labeling schema complexity exceeds model's context length or reasoning capacity

### Mechanism 3: Chain-of-Thought Disambiguates Complex Attributes
- CoT forces intermediate reasoning steps before final output, preventing "jumping to conclusions" errors
- Core assumption: Generated reasoning chain is faithful to model's internal logic
- Evidence: Mixed results show LLM5 + 8_FS_RAG_COT (0.730) slightly beats LLM5 + 8_FS_RAG (0.726) on WANDS
- Break condition: If model hallucinates features to support incorrect label during reasoning

## Foundational Learning

- **Maximum Marginal Relevance (MMR)**
  - Why needed: Standard retrieval often selects redundant examples; MMR ensures few-shot examples cover different aspects of the query
  - Quick check: If λ_MMR = 0, does the system prioritize relevance or diversity?

- **RAG vs. Fine-Tuning**
  - Why needed: RAG allows instant updates to knowledge (examples) without retraining model weights
  - Quick check: Does adding a new labeled example to vector store require retraining LLM weights?

- **Relevance Granularity (ESCI/WANDS)**
  - Why needed: Binary relevance insufficient for e-commerce; system must distinguish between Exact, Substitute, and Complement matches
  - Quick check: Is a screen protector for a phone an "Exact" match or a "Complement" to "iPhone" query?

## Architecture Onboarding

- **Component map:** Vector Store (ChromaDB) -> Retriever (MMR) -> Prompt Assembler -> LLM Engine (vLLM) -> Parser
- **Critical path:** Retrieval quality determines prompt quality; poor retrieval leads to hallucination or naive predictions
- **Design tradeoffs:**
  - LLM Choice: LLM5 offers higher accuracy (~0.83 F1) but privacy concerns and cost; LLM2 offers lower latency (0.3s/record) and privacy but slightly lower accuracy (~0.72 F1)
  - MMR Lambda: Tuning λ_MMR is dataset-specific; paper suggests grid search needed to balance relevance vs diversity
- **Failure signatures:** Schema drift (wrong label names), reasoning loops (repetitive justifications), redundant context (generic predictions)
- **First 3 experiments:**
  1. Run VANILLA vs. 16_FS (Random) on 500-sample hold-out set to measure lift from In-Context Learning alone
  2. A/B test FS_RAG (Pure Similarity) vs. FS_RAG_MMR (λ=0.5) to quantify value of diversity in few-shot selection
  3. Benchmark LLM2 vs. LLM5 on same input set to determine if accuracy delta justifies cost/latency delta

## Open Questions the Paper Calls Out

- How does inclusion of noisy product descriptions and catalog attributes impact labeling accuracy relative to increased computational cost and latency?
- How can evaluation frameworks be adapted to accurately assess LLM performance when model predictions appear semantically superior to ground truth labels?
- To what extent does domain-specific fine-tuning of open-source models improve performance compared to RAG-based few-shot prompting strategies?

## Limitations

- Relies on proprietary datasets and model APIs, particularly Walmart Mexico data and Gemini 1.5 (LLM5) results, which cannot be independently verified
- Significant accuracy gap between open-source (LLM2) and proprietary models (~0.72 vs ~0.83 F1) raises questions about source of gains
- Lacks ablation testing for Chain-of-Thought mechanism with inconsistent benefits across datasets
- Optimal MMR diversity parameter (λ) is dataset-specific, requiring expensive grid search that may not scale to production

## Confidence

- **High Confidence:** Core claim that In-Context Learning with few-shot examples improves over vanilla prompting (demonstrated across all datasets and models)
- **Medium Confidence:** MMR diversity improves retrieval quality (shown on WANDS but not consistently on ESCI)
- **Low Confidence:** Proprietary model superiority claims (based on inaccessible datasets and APIs)

## Next Checks

1. Reproduce fundamental few-shot learning benefit by running VANILLA vs. 16_FS configurations on ESCI with Llama-3-8B to verify ~0.30 accuracy lift
2. Test MMR sensitivity by implementing λ_MMR parameter sweeps on small open dataset to identify optimal diversity settings for different query distributions
3. Benchmark cost-latency tradeoffs by measuring LLM2 inference time and cost per 1,000 queries against accuracy loss compared to LLM5 to determine production viability