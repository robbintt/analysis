---
ver: rpa2
title: Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification
  and NER
arxiv_id: '2510.07566'
source_url: https://arxiv.org/abs/2510.07566
tags:
- pre-finetuning
- text
- downstream
- classification
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of deploying efficient natural
  language processing models on mobile devices that must handle both text classification
  and named entity recognition tasks. The authors propose a multi-task pre-finetuning
  framework using task-primary LoRA modules, where a shared encoder backbone is optimized
  jointly for both tasks while separate LoRA adapters are maintained for each task
  family.
---

# Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER

## Quick Facts
- **arXiv ID**: 2510.07566
- **Source URL**: https://arxiv.org/abs/2510.07566
- **Reference count**: 25
- **Primary result**: MTPF-TPL with task-primary LoRA modules achieves +0.8% NER and +8.8% text classification improvements over base models while maintaining deployment efficiency through a shared backbone

## Executive Summary
This work addresses the challenge of deploying efficient natural language processing models on mobile devices that must handle both text classification and named entity recognition tasks. The authors propose a multi-task pre-finetuning framework using task-primary LoRA modules, where a shared encoder backbone is optimized jointly for both tasks while separate LoRA adapters are maintained for each task family. This approach resolves interference between pre-finetuning objectives that occur with naive multi-task training. Experiments on 21 downstream tasks show average improvements of +0.8% for NER and +8.8% for text classification compared to base models, with performance comparable to individually pre-finetuned models while maintaining deployment efficiency through a single shared backbone.

## Method Summary
The approach uses task-primary LoRA (TPL) modules attached to the last 2 transformer layers of a shared encoder backbone. For NER pre-finetuning, knowledge is distilled from NuNER via k-means clustering of token embeddings to generate pseudo-labels. For text classification, contrastive learning with InfoNCE loss is applied to sentence pairs. The shared backbone is jointly optimized with both tasks, while each LoRA module is updated exclusively by its respective task loss. This prevents interference while maintaining a single efficient model for deployment.

## Key Results
- MTPF-TPL achieves average +0.8% NER and +8.8% text classification improvements over base models across 21 downstream tasks
- Performance matches individually pre-finetuned models while maintaining deployment efficiency
- Task-primary LoRA modules resolve interference between NER and text classification pre-finetuning objectives
- Restricting LoRAs to last 2 layers preserves downstream adaptability while reducing pre-finetuning conflicts
- Knowledge distillation enables lightweight encoders to acquire entity-aware representations despite limited capacity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-primary LoRA modules resolve representational conflicts between NER and text classification pre-finetuning objectives.
- Mechanism: Each LoRA module captures task-specific transformations (entity-type clustering for NER, sentence-level semantic compression for text classification) while the shared backbone accumulates mutually compatible representations through joint optimization.
- Core assumption: The interference observed in naïve multi-task pre-finetuning originates primarily in task-specific fine-grained transformations rather than in all encoder layers.
- Evidence anchors:
  - [abstract] "we propose a simple yet effective multi-task pre-finetuning framework based on task-primary LoRA modules, which enables a single shared encoder backbone with modular adapters"
  - [section 5] "pre-finetuning strategies for NER and text classification impose conflicting requirements on token embeddings, leading to incompatible optimization directions"
  - [corpus] Weak direct corpus support; related work GLiNER2 uses unified multi-task IE but doesn't address pre-finetuning interference specifically.

### Mechanism 2
- Claim: Restricting task-primary LoRAs to the last two transformer layers preserves downstream adaptability while reducing pre-finetuning conflicts.
- Mechanism: Earlier layers retain general linguistic features from pre-training, while task-specific transformations in final layers allow LoRA initialization to serve as effective starting points for downstream adaptation without over-constraining the representation space.
- Core assumption: The conflict between NER and text classification manifests more strongly in higher-level semantic processing than in lower-level feature extraction.
- Evidence anchors:
  - [section 6] "applying task-primary LoRAs only to the last few layers is key to achieving this adaptability"
  - [section 4, figure 4] "downstream performance on NER peaks when LoRAs are applied only to the last two layers"
  - [corpus] No direct corpus validation for layer-restricted pre-finetuning LoRA.

### Mechanism 3
- Claim: Knowledge distillation from a larger pre-finetuned NER model enables lightweight encoders to acquire entity-aware representations despite limited capacity.
- Mechanism: Pseudo-labels generated via clustering of teacher token embeddings provide training signal that captures entity-type structure without requiring the student to directly replicate the teacher's full embedding space.
- Core assumption: Cluster IDs preserve sufficient entity-type discriminability from the teacher's 200K-concept space to guide student learning.
- Evidence anchors:
  - [section 4] "we distill knowledge from NuNER into lightweight models... apply mini-batch k-means clustering to group them, assigning pseudo labels based on cluster IDs"
  - [section 4] "pre-finetuning for NER yields substantial gains in low-resource settings (e.g., +8.4% F1 when using only 10% of the original training data)"
  - [corpus] KoGNER (arxiv 2503.15737) uses knowledge graph distillation for biomedical NER, providing indirect support for distillation-based NER transfer.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Core parameter-efficient mechanism for task-primary modules; understanding rank, alpha, and target matrices is essential for reproducing results.
  - Quick check question: Given a weight matrix W of dimension d×k, what is the parameter count for LoRA with rank r, and how does α affect the effective learning rate?

- Concept: Contrastive learning with in-batch negatives (InfoNCE loss)
  - Why needed here: Text classification pre-finetuning relies on this objective; improper temperature τ or negative sampling will degrade sentence embeddings.
  - Quick check question: If batch size is reduced from 1024 to 256, what happens to negative sample diversity and how should τ be adjusted?

- Concept: Knowledge distillation via pseudo-labeling
  - Why needed here: NER pre-finetuning uses clustered teacher embeddings as supervision; understanding clustering quality vs. downstream performance is critical.
  - Quick check question: How would you detect if k-means cluster count (200 in this paper) is too high or too low for your target entity types?

## Architecture Onboarding

- Component map:
  - Shared encoder backbone: MiniLM (33M) or DistilBERT (67M) — frozen at inference, jointly updated during pre-finetuning
  - Task-primary LoRA modules: rank 32, α 64, applied to last 2 transformer layers (key/query matrices + MLP)
  - NER pre-finetuning head: linear classifier predicting cluster-based pseudo-labels
  - Text classification pre-finetuning head: contrastive loss with in-batch negatives (no classifier head)
  - Downstream adaptation: LoRA fine-tuning (NER) or linear probing (text classification)

- Critical path:
  1. Prepare pre-finetuning data: NuNER-derived pseudo-labels for NER (~24.4M words), sentence pairs for TC (~895M pairs from SentenceTransformers datasets)
  2. Initialize task-primary LoRAs on last 2 layers, train jointly with separate optimizers or alternating batches
  3. Validate: Check that TC loss decreases without NER performance collapse (and vice versa)
  4. Deploy: Freeze backbone, distribute task-primary LoRAs to applications

- Design tradeoffs:
  - LoRA rank vs. parameter cost: Higher rank improves individual task performance but increases per-application storage; rank 32 is chosen as a practical balance
  - Number of LoRA-augmented layers: More layers → lower pre-finetuning loss but reduced downstream adaptability for NER (Figure 4 shows this inversion)
  - Backbone freezing during pre-finetuning: Freezing eliminates interference but loses joint optimization benefits; this paper allows backbone updates

- Failure signatures:
  - NER downstream accuracy drops despite pre-finetuning: Check if TC loss dominated training (imbalance in task sampling or loss scaling)
  - Text classification embeddings collapse (all sentences similar): NER LoRA may have overwritten TC signal; verify LoRAs are task-isolated during gradient updates
  - Low-resource NER fails to improve: Cluster count may be mismatched to domain entity types; re-run k-means with domain-specific validation

- First 3 experiments:
  1. Reproduce single-task pre-finetuning baselines: Train NER-only and TC-only with full LoRA on all layers to establish upper bounds (expect ~+0.8% NER, ~+8.8% TC vs. base per Table 1).
  2. Ablate LoRA layer count: Train MTPF-TPL with LoRA on layers {1, 2, 4, 6, all} to confirm the 2-layer optimum on held-out validation sets.
  3. Low-resource stress test: Train downstream NER with 10% data using MTPF-TPL vs. base model; expect ~+8% F1 gap preservation per Table 11.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MTPF-TPL framework be extended to multilingual settings while maintaining interference mitigation?
- Basis in paper: [explicit] "we conduct experiments exclusively using English pre-trained models and English-language datasets... extending our approach to multilingual settings remains an important direction for future work."
- Why unresolved: The authors restrict experiments to English but deployment on mobile devices requires multilingual capabilities for global applications.
- What evidence would resolve it: Experiments applying MTPF-TPL to multilingual models (e.g., mBERT, XLM-R) across diverse languages.

### Open Question 2
- Question: Would task-specific LoRA configurations (rank, number of augmented layers) yield further performance improvements over the unified configuration used?
- Basis in paper: [explicit] "task-specific adapter configurations could potentially offer further improvements in downstream performance or efficiency."
- Why unresolved: For consistency, the authors used identical LoRA settings across both task families; optimal per-task settings remain unexplored.
- What evidence would resolve it: Ablation studies varying rank and layer assignment independently for NER vs. text classification.

### Open Question 3
- Question: Can the MTPF-TPL approach scale to three or more task families without introducing new interference patterns?
- Basis in paper: [inferred] The paper addresses two task families (NER and text classification) and analyzes their pairwise interference, but mobile applications may require additional capabilities (e.g., QA, summarization).
- Why unresolved: The mechanism for resolving interference between two objectives may not generalize when multiple objectives compete for shared backbone updates.
- What evidence would resolve it: Experiments adding a third pre-finetuning objective (e.g., question answering) with corresponding task-primary LoRA.

## Limitations

- The mechanism explanation for why LoRA modules resolve interference lacks quantitative analysis of where and how conflicts manifest across transformer layers.
- Knowledge distillation quality assessment lacks direct evaluation of pseudo-label fidelity and cluster assignment quality.
- Architecture-specific assumptions limit generalizability; results may not transfer to other efficient architectures or alternative parameter-efficient methods.

## Confidence

**High Confidence (8-10/10)**: The experimental demonstration that MTPF-TPL achieves comparable performance to individually pre-finetuned models while maintaining deployment efficiency. The ablation showing layer-restricted LoRA is crucial for downstream adaptability. The low-resource NER improvement claims are supported by direct comparisons.

**Medium Confidence (5-7/10)**: The mechanism explanation for why LoRA modules resolve interference. While empirically validated, the paper doesn't provide detailed analysis of how token representations evolve during multi-task pre-finetuning with and without LoRA. The knowledge distillation quality assessment lacks direct evaluation of pseudo-label fidelity.

**Low Confidence (1-4/10)**: Claims about general applicability to other task pairs or encoder architectures. The paper focuses exclusively on NER+TC with two specific models. No analysis of failure modes when task pairs have more severe representational conflicts or when LoRA rank/alpha parameters differ significantly.

## Next Checks

1. **Interference quantification study**: Run the same MTPF-TPL framework but with intermediate model checkpoints. Measure and visualize token embedding similarity (e.g., using Procrustes distance or centered kernel alignment) between pre-finetuning phases and downstream tasks to empirically demonstrate where and how LoRA modules prevent interference.

2. **Teacher model ablation**: Repeat the NER pre-finetuning knowledge distillation using different teacher models (e.g., FLERT, LUKE) or different clustering approaches (spherical k-means, hierarchical clustering). Measure downstream performance variance to establish sensitivity to teacher quality and clustering parameters.

3. **Cross-task generalization test**: Apply the MTPF-TPL framework to a different task pair (e.g., text classification + relation extraction, or summarization + question answering). Compare performance against both individually pre-finetuned models and the naive multi-task baseline to test whether the 2-layer LoRA restriction and task-primary module approach generalizes beyond NER+TC.