---
ver: rpa2
title: 'DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization'
arxiv_id: '2510.04474'
source_url: https://arxiv.org/abs/2510.04474
tags:
- reasoning
- length
- value
- arxiv
- drpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overthinking problem in large reasoning
  models, where models generate unnecessarily long reasoning paths even for simple
  questions, increasing computational cost. The authors identify that GRPO's group-relative
  advantage function is ill-suited for composite rewards like correctness and length,
  as it can assign negative advantages to correct but long rollouts.
---

# DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization

## Quick Facts
- arXiv ID: 2510.04474
- Source URL: https://arxiv.org/abs/2510.04474
- Authors: Gang Li; Yan Chen; Ming Lin; Tianbao Yang
- Reference count: 36
- Primary result: DRPO achieves 77% reasoning length reduction on GSM8k with only 1.1% performance loss using a 1.5B model

## Executive Summary
This paper addresses the overthinking problem in large reasoning models, where models generate unnecessarily long reasoning paths even for simple questions, increasing computational cost. The authors identify that GRPO's group-relative advantage function is ill-suited for composite rewards like correctness and length, as it can assign negative advantages to correct but long rollouts. They propose Decoupled Reward Policy Optimization (DRPO), which decouples learning signals for positive and negative data by normalizing length rewards only within correct rollouts. DRPO integrates an optimized positive data distribution under KL regularization into a discriminative objective, with a tractable closed-form solution derived.

## Method Summary
DRPO addresses the overthinking problem in reasoning models by decoupling the learning signals for correct and incorrect rollouts when optimizing for composite rewards. The key insight is that GRPO's group-relative advantage normalization inappropriately mixes correct and incorrect rollouts, leading to negative advantages for correct but long solutions. DRPO instead normalizes length rewards only within correct rollouts, preserving positive learning signals. The method also optimizes the positive data distribution under KL regularization, integrating this into a discriminative objective. The authors derive a tractable closed-form solution for this optimization, making DRPO practical to implement. Experiments demonstrate significant improvements in efficiency over six baseline methods while maintaining high accuracy on mathematical reasoning tasks.

## Key Results
- DRPO achieves 77% length reduction with only 1.1% performance loss on GSM8k using a 1.5B model
- Outperforms six efficient reasoning baselines in the Pareto frontier of accuracy vs. efficiency
- Shows strong performance compared to GRPO with KL regularization and other length-aware methods

## Why This Works (Mechanism)
DRPO works by properly separating the learning signals for correct and incorrect reasoning paths. In traditional GRPO, the group-relative advantage function normalizes rewards across all rollouts, which can penalize correct but lengthy solutions by comparing them to shorter incorrect ones. By decoupling the normalization to only occur within correct rollouts, DRPO ensures that the model learns to prefer shorter correct solutions without being discouraged by their length relative to incorrect paths. The additional optimization of the positive data distribution under KL regularization helps the model focus on the most informative correct examples.

## Foundational Learning
- **Group-relative advantage normalization**: Why needed - to enable comparison between different rollouts in RLHF; Quick check - verify that normalization occurs within the correct group only
- **Composite reward optimization**: Why needed - to balance multiple objectives like correctness and efficiency; Quick check - confirm rewards are properly weighted and normalized
- **KL regularization in policy optimization**: Why needed - to prevent policy collapse and maintain exploration; Quick check - monitor KL divergence between old and new policies
- **Discriminative objectives in RL**: Why needed - to provide stable learning signals; Quick check - verify the objective function properly balances discrimination and reward optimization

## Architecture Onboarding

Component map: Input Question -> Reasoning Generator -> Reward Computation -> DRPO Optimization -> Updated Policy

Critical path: The critical path involves generating reasoning traces, computing correctness and length rewards, applying DRPO's decoupled normalization, and updating the policy through the optimized discriminative objective with KL regularization.

Design tradeoffs: DRPO trades off some implementation complexity (requiring separate handling of correct/incorrect rollouts) for significant efficiency gains. The method requires maintaining separate statistics for correct and incorrect rollouts but avoids the pitfall of penalizing correct but lengthy solutions.

Failure signatures: Potential failures include: 1) If the separation between correct and incorrect rollouts is not clean, the decoupling may not work effectively; 2) If the KL regularization is too strong, the policy may not adapt sufficiently; 3) If the discriminative objective is not properly balanced, the model may focus too much on length reduction at the expense of accuracy.

First experiments: 1) Verify that DRPO produces positive advantages for correct rollouts even when they are long; 2) Compare the length distribution of solutions before and after DRPO training; 3) Test whether the model maintains accuracy when forced to use shorter reasoning paths.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach is specifically designed for correctness and length rewards, with unclear generalization to more complex reward combinations
- Experimental validation is limited to mathematical reasoning tasks, without testing on other reasoning domains
- The paper does not quantify the additional computational overhead during training

## Confidence

High Confidence: The core technical contribution (decoupled reward normalization) is well-justified theoretically, and the GSM8k results are reproducible based on the described methodology.

Medium Confidence: The performance claims are convincing within the tested domain but require further validation across diverse reasoning tasks.

Low Confidence: The scalability of DRPO to more complex reward structures and its practical deployment overhead are not sufficiently explored.

## Next Checks
1. Test DRPO on non-mathematical reasoning tasks (e.g., strategyQA, coding problems, or multi-hop commonsense reasoning) to assess generalizability of length reduction and performance trade-offs.

2. Conduct ablation studies by systematically disabling each DRPO component (decoupled normalization, KL-regularized distribution, discriminative objectives) to measure their individual impacts on length reduction and accuracy.

3. Measure wall-clock training time, GPU memory consumption, and gradient update complexity for DRPO compared to GRPO to quantify practical deployment costs.