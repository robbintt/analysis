---
ver: rpa2
title: BiHRNN -- Bi-Directional Hierarchical Recurrent Neural Network for Inflation
  Forecasting
arxiv_id: '2503.01893'
source_url: https://arxiv.org/abs/2503.01893
tags:
- hierarchical
- inflation
- data
- bihrnn
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the BiHRNN model, which improves inflation
  forecasting by leveraging the hierarchical structure of CPI data through bidirectional
  information flow between levels. The model uses hierarchical informative constraints
  on RNN parameters to enhance accuracy across all hierarchy levels without the computational
  inefficiency of a unified model.
---

# BiHRNN -- Bi-Directional Hierarchical Recurrent Neural Network for Inflation Forecasting

## Quick Facts
- arXiv ID: 2503.01893
- Source URL: https://arxiv.org/abs/2503.01893
- Authors: Maya Vilenko
- Reference count: 8
- Primary result: BiHRNN significantly outperforms traditional RNN approaches in inflation forecasting accuracy across US, Canada, and Norway datasets

## Executive Summary
This paper introduces the BiHRNN model, which improves inflation forecasting by leveraging the hierarchical structure of CPI data through bidirectional information flow between levels. The model uses hierarchical informative constraints on RNN parameters to enhance accuracy across all hierarchy levels without the computational inefficiency of a unified model. Tested on CPI datasets from the US, Canada, and Norway, BiHRNN significantly outperforms traditional RNN approaches in forecasting accuracy, with its bidirectional architecture playing a key role in these improvements. Results show lower RMSE and higher correlation metrics compared to baselines, demonstrating robust performance in both granular and aggregate predictions. The model's ability to capture complex dependencies across hierarchical levels makes it a strong framework for inflation forecasting, with potential for further refinement and application to additional markets.

## Method Summary
BiHRNN uses GRU units at each hierarchy node to forecast inflation, incorporating bidirectional constraints between parent and child nodes. The model first pretrains an HRNN baseline with parent-to-child constraints, then freezes these weights and adds child-to-parent constraints to create the bidirectional flow. The loss function combines MSE with L2 penalties on parameter distances between related nodes, weighted by their empirical correlations. Training uses Optuna TPE for hyperparameter tuning, with λ₁ controlling parent regularization and λ₂ controlling child regularization. The model processes monthly log-change rates of CPI data, preserving hierarchical coherence across aggregation levels while avoiding the computational cost of a unified model.

## Key Results
- BiHRNN achieves significantly lower RMSE than HRNN baseline across all three countries (US, Canada, Norway)
- Bidirectional information flow contributes to accuracy improvements at all hierarchy levels, not just aggregates
- Correlation-weighted child constraints adapt regularization strength based on actual parent-child relationships
- Frozen pretraining constraints stabilize hierarchical learning and reduce overfitting compared to fully learnable alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional information flow improves forecasting accuracy across all hierarchy levels
- Mechanism: Parent-node constraints propagate top-down signals (aggregate trends inform granular predictions), while child-node constraints aggregate bottom-up signals (disaggregated patterns refine aggregate forecasts). The dual regularization terms in the loss function balance these flows.
- Core assumption: Disaggregated CPI components contain predictive signals that inform aggregate indices, and vice versa.
- Evidence anchors:
  - [abstract] "BiHRNN facilitates bidirectional information flow between hierarchical levels, where higher-level nodes influence lower-level ones and vice versa."
  - [section 3.2.1] "By incorporating these two constraints, BiHRNN enables information to flow in both directions—downward from parent to child and upward from child to parent."
  - [corpus] Related work on hierarchical LSTMs (Lin et al., 2022) supports multi-level dependency capture, though bidirectional hierarchical RNNs for economic forecasting remain underexplored.
- Break condition: If child nodes have near-zero correlation with parents, or if hierarchy is shallow (≤2 levels), bottom-up regularization may add noise without benefit.

### Mechanism 2
- Claim: Fixed frozen constraints from pretraining stabilize hierarchical learning and reduce overfitting
- Mechanism: Pre-train HRNN baseline, freeze weights, then use as fixed reference points during BiHRNN training. This anchors parameter space and prevents drift while allowing bidirectional refinement.
- Core assumption: Pretrained HRNN weights capture meaningful hierarchical relationships worth preserving.
- Evidence anchors:
  - [section 3.2.4] "By leveraging frozen weights, the BiHRNN anchors predictions, ensuring that hierarchical relationships are preserved and reducing the risk of overfitting."
  - [section 5.4] BiHRNN consistently outperforms HRNN across all three datasets (US, Canada, Norway) in RMSE and correlation metrics.
  - [corpus] No direct corpus comparison for frozen-constraint training in hierarchical RNNs found.
- Break condition: If pretraining is underfit or trained on insufficient data, frozen weights propagate poor priors.

### Mechanism 3
- Claim: Correlation-weighted child constraints adapt regularization strength to actual parent-child relationships
- Mechanism: Child regularization weights (w_i) reflect empirical correlation between node and parent. High-correlation children exert stronger influence; low-correlation children contribute less.
- Core assumption: Pearson correlation adequately captures predictive relationship strength between hierarchy levels.
- Evidence anchors:
  - [section 3.1.2] "Higher τθn values indicate stronger parameter connections between node n and its parent πn."
  - [section 3.1.2] HRNN sets τθn = e^(α+Cn) where Cn is Pearson correlation between node and parent.
  - [corpus] Hierarchical time series reconciliation literature (Wickramasuriya et al., 2019) uses similar correlation-weighting principles.
- Break condition: If correlations are unstable over time or spurious, weighting mechanism may amplify noise.

## Foundational Learning

- Concept: Gated Recurrent Units (GRUs)
  - Why needed here: GRUs form the core predictive unit at each hierarchy node, capturing temporal dependencies in CPI sequences.
  - Quick check question: Can you explain the roles of the update gate (z) and reset gate (r) in controlling information flow through a GRU cell?

- Concept: Hierarchical Time Series Forecasting
  - Why needed here: CPI data is organized in multi-level hierarchies; predictions must be coherent across aggregation levels.
  - Quick check question: What is the difference between top-down, bottom-up, and middle-out reconciliation strategies?

- Concept: Regularization via Parameter Constraints
  - Why needed here: The loss function adds L2 penalties on parameter distances between related nodes; understanding this is essential for tuning λ₁ and λ₂.
  - Quick check question: How does increasing λ₁ (parent regularization) versus λ₂ (child regularization) differently affect model behavior?

## Architecture Onboarding

- Component map:
  Input layer (log-change rates per CPI index) -> GRU nodes (one per hierarchy node) -> Constraint layer (parent and child parameter regularization) -> Output layer (next-timestep predictions) -> Frozen reference (pretrained HRNN weights)

- Critical path:
  1. Data prep → compute log-change rates, build hierarchy graph
  2. Pretrain HRNN → obtain frozen weights per node
  3. Initialize BiHRNN → same architecture, add dual constraints
  4. Training loop → forward pass through GRUs, compute Loss_BiHRNN (MSE + λ₁·l_parent + λ₂·l_child), backprop
  5. Hyperparameter tuning → Optuna TPE search over λ₁, λ₂, learning rate, hidden size

- Design tradeoffs:
  - Unified model vs. per-node models: BiHRNN uses per-node GRUs with shared constraints, avoiding unified-model computational cost while capturing hierarchy
  - Frozen vs. learnable constraints: Freezing reduces overfitting but limits adaptation if pretraining was suboptimal
  - λ₁/λ₂ balance: Domain-dependent; higher λ₂ suited when disaggregated data is more informative (e.g., volatile sectors)

- Failure signatures:
  - Exploding loss during early training → check learning rate, gradient clipping
  - Headline predictions worse than baseline → may need to disable hierarchical constraints for root node (paper suggests headline-only data works best for headline predictions)
  - Child constraints cause divergence → verify weight normalization, check for near-zero correlations causing numerical instability

- First 3 experiments:
  1. Replicate HRNN baseline on US CPI Level 0-2; verify RMSE matches Table 5.1 before adding bidirectional constraints.
  2. Ablation study: train BiHRNN with only parent constraints (λ₂=0) vs. only child constraints (λ₁=0) vs. both; compare to isolate each direction's contribution.
  3. Hyperparameter sweep: vary λ₁ ∈ [0.01, 1.0] and λ₂ ∈ [0.01, 1.0] on validation split; plot RMSE surface to identify optimal balance for each dataset (US, Canada, Norway may differ).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would extending BiHRNN to incorporate exogenous economic variables (e.g., monetary policy indicators, commodity prices, geopolitical risk indices) improve forecasting accuracy beyond using historical CPI alone?
- Basis in paper: [explicit] Conclusion states: "Further enhancements will focus on incorporating additional features" and Related Work discusses FEWNet's success using "economic policy uncertainty and geopolitical risk" alongside price data.
- Why unresolved: Current BiHRNN only uses historical CPI values; no exogenous variables were tested despite their demonstrated importance in related work.
- What evidence would resolve it: Systematic experiments adding economic exogenous variables to BiHRNN and measuring RMSE/correlation improvements on same test sets.

### Open Question 2
- Question: Why does bidirectional hierarchical information flow fail to improve headline CPI predictions, and is this finding consistent across longer forecast horizons (3-month, 6-month, 12-month ahead)?
- Basis in paper: [explicit] Results section states: "Headline data alone is sufficient for accurate headline predictions and yields the best results. Attempts to incorporate additional regularization terms do not enhance prediction performance."
- Why unresolved: The bidirectional architecture was motivated by capturing cross-level dependencies, yet it doesn't help at the aggregate level; no explanation or horizon analysis provided.
- What evidence would resolve it: Ablation studies across multiple forecast horizons comparing headline-only vs. hierarchical models, with analysis of when/if hierarchical information becomes beneficial.

### Open Question 3
- Question: Would unfreezing the parent and child constraint weights during BiHRNN training (instead of keeping them fixed from HRNN pretraining) yield better accuracy, and what is the trade-off with computational cost?
- Basis in paper: [inferred] Section 3.2.4 describes freezing HRNN weights as fixed constraints, justified for stability and overfitting reduction, but this prevents joint optimization of the bidirectional flow.
- Why unresolved: The frozen-weight approach was a design choice without comparison to learnable alternatives; the paper claims it reduces overfitting but doesn't quantify the trade-off.
- What evidence would resolve it: Experiments comparing fixed vs. learnable constraint weights with matching computational budgets, measuring both accuracy and training time.

### Open Question 4
- Question: Can BiHRNN be adapted to handle time-varying hierarchical structures, given that CPI category definitions and weights change periodically?
- Basis in paper: [inferred] Section 4.1 notes "new indexes have been introduced while others have been discontinued, causing shifts in the hierarchical structure," yet the model assumes a fixed hierarchy.
- Why unresolved: No mechanism was proposed for handling structural changes; the model treats hierarchy as static despite acknowledged real-world changes.
- What evidence would resolve it: Experiments on simulated or real datasets with known structural breaks, comparing static vs. adaptive hierarchy handling approaches.

## Limitations
- The paper lacks explicit details on hyperparameter ranges for λ₁ and λ₂, which are critical for reproducing the balance between parent and child constraints
- Training details including learning rate, optimizer settings, epochs, and early stopping criteria are omitted, making consistent convergence difficult
- The computation of child weights (w_i) is mentioned but not fully specified, introducing ambiguity in how correlations are transformed into regularization strengths

## Confidence
- **High**: The bidirectional hierarchical structure and its role in improving accuracy are well-supported by RMSE and correlation metrics across three countries
- **Medium**: The frozen-constraint pretraining mechanism is plausible but lacks direct corpus validation for economic forecasting
- **Low**: The claim that child constraints consistently improve headline predictions is not fully substantiated; the paper suggests headline-only data may work better for headline forecasts

## Next Checks
1. **Hyperparameter Sensitivity**: Re-run the ablation study with finer-grained λ₁ and λ₂ grids to map the RMSE surface and identify optimal balances for each dataset
2. **Correlation Weight Stability**: Test the robustness of child weights by simulating time-varying correlations and observing their impact on forecasting accuracy
3. **Frozen Constraint Efficacy**: Compare BiHRNN with and without frozen constraints to quantify the reduction in overfitting and assess whether pretraining quality affects performance