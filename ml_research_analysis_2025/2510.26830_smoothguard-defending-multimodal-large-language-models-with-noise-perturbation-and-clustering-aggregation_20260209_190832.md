---
ver: rpa2
title: 'SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation
  and Clustering Aggregation'
arxiv_id: '2510.26830'
source_url: https://arxiv.org/abs/2510.26830
tags:
- adversarial
- noise
- arxiv
- utility
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SmoothGuard, a lightweight and model-agnostic
  defense framework that enhances the robustness of multimodal large language models
  (MLLMs) against adversarial attacks through randomized noise injection and clustering-based
  prediction aggregation. The method perturbs continuous modalities (e.g., images
  and audio) with Gaussian noise, generates multiple candidate outputs, and applies
  embedding-based clustering to filter out adversarially influenced predictions.
---

# SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation

## Quick Facts
- **arXiv ID:** 2510.26830
- **Source URL:** https://arxiv.org/abs/2510.26830
- **Reference count:** 30
- **Primary result:** Reduces attack success rate from ~96% to 0% on multimodal benchmarks while maintaining utility within 5% degradation

## Executive Summary
SmoothGuard is a lightweight, model-agnostic defense framework that protects multimodal large language models (MLLMs) against adversarial attacks on continuous modalities like images and audio. The method combines randomized Gaussian noise injection with embedding-based clustering aggregation to filter out adversarially influenced predictions. By generating multiple noisy samples, encoding outputs, and selecting the majority cluster's centroid prediction, SmoothGuard achieves robust defense without requiring model fine-tuning or architectural modifications. Extensive experiments on POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate significant robustness improvements while preserving competitive utility.

## Method Summary
SmoothGuard perturbs continuous modality inputs (images/audio) with Gaussian noise at optimal variance (0.1-0.2), generates 10 samples (9 noisy + 1 original), and feeds them to the MLLM. Outputs are encoded using RoBERTa-base, clustered via k-means (k=2), and the final prediction is selected from the majority cluster's centroid. The method leverages the clustering effect where clean and adversarial outputs form separable distributions in embedding space, enabling effective filtering of malicious predictions while maintaining semantic integrity for benign inputs.

## Key Results
- Reduces attack success rate from 96.8% to 0% on POPE benchmark
- Maintains utility with <5% degradation on benign inputs
- Optimal noise variance identified as σ ∈ [0.1, 0.2] through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Randomized Gaussian noise injection disrupts adversarial perturbations while preserving semantic content within an optimal noise range.
- **Mechanism:** Adversarial attacks rely on precise perturbation vectors to cross decision boundaries. Gaussian noise (σ²) acts as a stochastic filter that probabilistically destroys adversarial signals while preserving robust semantic features.
- **Core assumption:** Adversarial perturbations are fragile to random noise corruption, whereas clean inputs maintain semantic consistency under perturbation.
- **Evidence anchors:** [abstract] "Our method perturbs continuous modalities (e.g., images and audio) with Gaussian noise"; [section III.B] "For the image input, Gaussian perturbations with variance σ²img are applied: δimg ~ N(0, σ²imgI)"
- **Break condition:** If noise level exceeds optimal range (>0.3), input distribution distortion impairs utility. If adversarial perturbations are robust to Gaussian noise, defense degrades.

### Mechanism 2
- **Claim:** Embedding-based k-means clustering (k=2) separates adversarial outputs from clean predictions based on distributional divergence in representation space.
- **Mechanism:** Adversarially influenced predictions cluster differently in embedding space from robust predictions. RoBERTa-base encoding followed by k=2 clustering exploits the clustering effect where clean inputs form tight, semantically coherent clusters while adversarial perturbations push outputs into distinct regions.
- **Core assumption:** Adversarial and clean outputs form separable clusters in embedding space.
- **Evidence anchors:** [abstract] "applies embedding-based clustering to filter out adversarially influenced predictions"; [section III.C] "In practice, we often observe a dominant clean cluster covering most of the perturbed samples"
- **Break condition:** If adversarial outputs embed within the clean cluster (semantic similarity), clustering fails to filter. If both clusters are similar in size, majority voting becomes unreliable.

### Mechanism 3
- **Claim:** Majority cluster selection via centroid proximity ensures final outputs align with stable, consensus predictions.
- **Mechanism:** After clustering, selecting the prediction closest to the majority cluster's centroid provides a denoised aggregate that minimizes influence from adversarial outliers. Including one unperturbed input preserves baseline utility under benign conditions.
- **Core assumption:** The majority cluster represents clean/stable predictions; minority cluster contains adversarial corruptions.
- **Evidence anchors:** [abstract] "The final answer is selected from the majority cluster, ensuring stable responses even under malicious perturbations"; [section III.C] "Including the unperturbed input ensures that, under benign conditions, the final prediction remains close to the model's original utility"
- **Break condition:** If adversarial samples dominate the sampling (attack success rate high with low noise), majority cluster contains malicious outputs.

## Foundational Learning

- **Concept:** Randomized smoothing and certified robustness
  - **Why needed here:** SmoothGuard builds on randomized smoothing principles from computer vision, extending to multimodal settings.
  - **Quick check question:** Can you explain why adding noise provides robustness guarantees under certain distributional assumptions?

- **Concept:** Adversarial attack vectors in multimodal models
  - **Why needed here:** Understanding how perturbations propagate across modalities (image→text, audio→text) informs defense placement.
  - **Quick check question:** What distinguishes a targeted adversarial perturbation from random noise in terms of decision boundary manipulation?

- **Concept:** K-means clustering and embedding space geometry
  - **Why needed here:** Core aggregation mechanism relies on clustering semantic embeddings; understanding cluster separation quality is critical.
  - **Quick check question:** Why might k=2 be appropriate here versus higher k values? What assumptions does this make about the output distribution?

## Architecture Onboarding

- **Component map:** Input (Image/Audio + Text) -> Noise Injection Module (Gaussian, σ=0.1-0.2) -> N-fold Sampling (default: 10 samples = 9 noisy + 1 original) -> MLLM Inference (Qwen2.5-VL-7B / LLaVA-1.5-7B) -> Output Embedding (RoBERTa-base encoder) -> K-Means Clustering (k=2) -> Majority Cluster Selection -> Centroid Computation -> Final Output

- **Critical path:** Noise variance selection (σ) directly controls robustness-utility tradeoff; Sample count N affects inference cost and voting stability; Embedding encoder choice influences cluster quality

- **Design tradeoffs:**
  - **Robustness vs. Utility:** σ ∈ [0.1, 0.2] optimal; >0.3 degrades utility, <0.05 provides insufficient robustness
  - **Inference cost:** N=10 multiplies inference time ~10x (parallelizable but requires infrastructure)
  - **Model agnosticism:** Works without fine-tuning but requires compatible embedding encoder
  - **Modal coverage:** Validated on image; audio experiments pending

- **Failure signatures:**
  - Utility drops >5% on benign inputs → noise level too high
  - ASR remains >40% → noise level insufficient or adaptive attack
  - Clustering produces balanced clusters (50/50) → k=2 assumption violated
  - Semantic drift in outputs → embedding encoder mismatched to task domain

- **First 3 experiments:**
  1. **Noise sweep validation:** Run ablation on your target MLLM with σ ∈ {0.05, 0.1, 0.15, 0.2, 0.3, 0.4} on POPE subset; plot accuracy vs. noise curve to identify optimal range for your model.
  2. **Attack-specific robustness test:** Generate adversarial images using the HuggingFace-generalized pipeline; measure ASR with/without SmoothGuard using LlamaGuard-7B classifier.
  3. **Latency benchmark:** Measure end-to-end inference time with N=10 samples on your deployment hardware; evaluate parallelization opportunities vs. sequential processing.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does SmoothGuard maintain its robustness and utility effectiveness when applied to audio-specific benchmarks and adversarial attacks?
- **Basis in paper:** [explicit] The authors state in the abstract and conclusion: "future work extending the framework to audio-focused benchmarks to validate its generality across modalities."
- **Why unresolved:** The experimental evaluation focused exclusively on vision-language tasks, while audio modality handling was described but not empirically validated.
- **What evidence would resolve it:** Empirical results on audio-language benchmarks demonstrating Attack Success Rates (ASR) and utility preservation comparable to the reported vision results.

### Open Question 2
- **Question:** Is the fixed k=2 clustering assumption sufficient for complex attacks that may induce diverse output distributions?
- **Basis in paper:** [inferred] The method relies on the assumption that "robust models tend to embed semantically-equivalent inputs into tight, well-separated clusters," specifically setting k=2 (clean vs. adversarial).
- **Why unresolved:** Adversarial attacks might result in multi-modal output distributions that do not fit a binary clustering model, potentially causing the majority selection to fail or select an adversarial output.
- **What evidence would resolve it:** Analysis of output embedding distributions under various attacks to see if they consistently form two distinct clusters or require adaptive cluster numbers.

### Open Question 3
- **Question:** Can SmoothGuard defend against adversarial attacks that specifically manipulate the text modality?
- **Basis in paper:** [inferred] The methodology explicitly notes: "We keep text unchanged to preserve semantic integrity," focusing noise injection solely on continuous modalities (image/audio).
- **Why unresolved:** By preserving text inputs exactly, the framework may remain vulnerable to optimized text prompts or suffix attacks that do not require image perturbations to bypass safety filters.
- **What evidence would resolve it:** Evaluation of defense performance against text-only jailbreaks or multimodal attacks where the primary vector is the text prompt.

## Limitations

- **Attack generation ambiguity:** Universal adversarial image generation process remains underspecified with missing parameters like optimization objectives and iteration counts.
- **Modal generalization gap:** Experiments confined to image-based attacks; audio modality applicability remains theoretical without empirical validation.
- **Clustering assumption fragility:** k=2 clustering relies on empirically observed but theoretically ungrounded assumption that adversarial outputs form separable minority clusters.

## Confidence

**High confidence (Robustness gains):** ASR reduction from 96.8% to 0% on POPE and 75.1% to 0% on MM-SafetyBench is well-documented with clear experimental setup.

**Medium confidence (Optimal noise range):** σ ∈ [0.1, 0.2] range is empirically derived but may shift with different MLLMs, attack types, or evaluation datasets.

**Medium confidence (Utility preservation):** <5% degradation on benign inputs reported, but baseline comparison unclear and lacks absolute benchmarks.

## Next Checks

1. **Adaptive attack stress test:** Implement white-box adversarial attacks that optimize perturbations specifically to evade Gaussian noise injection (e.g., using expectation over transformation or gradient masking techniques). Measure whether SmoothGuard maintains ASR <10% under these stronger attacks.

2. **Cross-modal robustness validation:** Extend the defense framework to audio inputs using standard audio MLLMs. Apply the same noise injection and clustering pipeline to adversarial audio samples and measure ASR reduction compared to the baseline.

3. **Inference cost-benefit analysis:** Measure end-to-end latency with N=10 samples on production hardware, including parallel processing overhead. Calculate the utility-robustness-cost tradeoff curve by varying N and σ to identify Pareto-optimal configurations for different deployment scenarios.