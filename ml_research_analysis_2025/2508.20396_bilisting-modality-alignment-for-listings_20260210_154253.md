---
ver: rpa2
title: 'BiListing: Modality Alignment for Listings'
arxiv_id: '2508.20396'
source_url: https://arxiv.org/abs/2508.20396
tags:
- listing
- airbnb
- text
- embeddings
- bilisting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes BiListing, a method to align text and images
  of Airbnb listings using large language models and pretrained vision-language models.
  The approach addresses the challenge of combining diverse unstructured data from
  multiple listing images and text documents into a single meaningful representation.
---

# BiListing: Modality Alignment for Listings

## Quick Facts
- arXiv ID: 2508.20396
- Source URL: https://arxiv.org/abs/2508.20396
- Reference count: 27
- Primary result: 0.425% NDCB gain in Airbnb search ranking, yielding tens of millions in incremental revenue

## Executive Summary
BiListing addresses the challenge of aligning Airbnb listing images and text by adapting the CLIP framework to handle multiple photos and text attributes per listing. The method generates visual-only text summaries (Visual Profiles) using Mixtral 8x7b, then trains a PhotoSet Transformer to aggregate multiple image embeddings into a unified representation. Offline and online evaluations demonstrate significant improvements in retrieval accuracy and ranking performance, with the model showing particular strength in zero-shot search and cold-start scenarios.

## Method Summary
BiListing extends CLIP-style contrastive learning to accommodate multiple images per listing through a PhotoSet Transformer that aggregates image embeddings. The approach uses a two-stage training process: first freezing the text encoder to train the PhotoSet Transformer, then fine-tuning with unfrozen text layers. Visual Profiles are generated from listing text and structured data using Mixtral 8x7b to create visual-only descriptions. The model compresses image embeddings using Optimal Product Quantization and employs SigLIP loss instead of standard CLIP loss for better alignment.

## Key Results
- Mean Rank (T->I) of 6.17, significantly outperforming CLIP baseline (MR 20.5)
- 0.425% Normalized Discounted Cumulative Gain (NDCB) improvement in Airbnb's search ranking model
- Achieved tens of millions in incremental revenue through improved listing alignment
- Demonstrated effectiveness in zero-shot search and cold-start listing scenarios

## Why This Works (Mechanism)
The method works by creating a unified embedding space where listing images and text representations are meaningfully aligned. By generating Visual Profiles that focus exclusively on visual elements, the model learns to match text descriptions with corresponding images. The PhotoSet Transformer effectively aggregates multiple image embeddings while preserving important visual information, and the two-stage training approach ensures stable optimization. The use of SigLIP loss provides better alignment signals than traditional contrastive losses.

## Foundational Learning

**Optimal Product Quantization (OPQ):** A compression technique that reduces high-dimensional embeddings to fixed-size codes while preserving semantic similarity. Why needed: Enables efficient storage and computation with large embedding spaces. Quick check: Verify compressed embeddings maintain cosine similarity to original embeddings.

**SigLIP Loss:** A modern contrastive loss function that uses similarity-invariant sampling. Why needed: Provides better alignment than standard CLIP loss, as evidenced by MR 20.5 vs 6.17. Quick check: Compare retrieval metrics when using SigLIP vs standard CLIP loss.

**PhotoSet Transformer:** A transformer architecture that aggregates multiple image embeddings into a single listing representation. Why needed: Handles variable numbers of listing images while preserving visual information. Quick check: Test with different numbers of images per listing to verify consistent performance.

## Architecture Onboarding

**Component Map:** Mixtral 8x7b -> Visual Profiles -> Text Tower (CLIP) <- SigLIP Loss -> PhotoSet Transformer -> Vision Tower (CLIP)

**Critical Path:** Visual Profile generation → Text Tower encoding → SigLIP loss computation → PhotoSet Transformer training → Vision Tower encoding → Final embedding alignment

**Design Tradeoffs:** Uses LLM-based summarization to bypass CLIP's 77-token limit vs architectural context extension solutions. Chose SigLIP over CLIP loss for better alignment. Implemented two-stage training (coarse then fine alignment) for stability.

**Failure Signatures:** Out-of-memory errors during training (batch size 32k), poor text-to-image retrieval indicating misaligned Visual Profiles, alignment collapse when using standard CLIP loss instead of SigLIP.

**First Experiments:**
1. Train PhotoSet Transformer with SigLIP loss, freeze text encoder
2. Generate Visual Profiles with different prompt templates and measure impact
3. Compare Mean Rank with SigLIP vs standard CLIP loss

## Open Questions the Paper Calls Out

**Context Extension Evaluation:** The paper suggests evaluating how techniques like Long-CLIP or Tulip could enhance the model by extending CLIP's native context window to handle raw listing text instead of relying on LLM summarization.

**LLM Sensitivity Analysis:** The authors call for expanded ablation studies on different LLMs (Llama, GPT-4) for generating Visual Profiles to understand if Mixtral 8x7b is optimal or if the prefilling strategy is universally effective.

**Positional Encoding Impact:** Section 7 identifies the need to quantify the marginal contribution of positional encoding in the PhotoSet Transformer to retrieval accuracy, given the heuristic that hosts upload important photos first.

**Cold-Start Performance Analysis:** While claiming to overcome cold-start problems, the paper calls for deeper analysis of performance on listings with no interaction history, especially given that the Legacy model outperformed BiListing on "Listing Tenure" and "Review count."

## Limitations
- Missing exact prompt template for Visual Profile generation with Mixtral 8x7b
- Lack of detail on the preliminary filtering model used to remove misaligned training data
- Revenue impact attribution unclear without ablation studies isolating BiListing's contribution

## Confidence
- **High Confidence:** Core architecture and two-stage training procedure are well-specified with clear ablation results
- **Medium Confidence:** PhotoSet Transformer and OPQ implementation details described but not fully specified
- **Low Confidence:** Reproducibility of Visual Profile generation and exact data filtering methodology

## Next Checks
1. Test multiple prompt templates for Visual Profile generation with Mixtral 8x7b, varying instructions about visual features, and measure impact on retrieval metrics
2. Implement both SigLIP and standard CLIP loss functions, train PhotoSet Transformer with both, and verify the reported performance gap (MR 20.5 vs 6.17)
3. Create controlled experiments with different quality levels of Visual Profiles (human-written vs LLM-generated, with/without non-visual content) and measure degradation in retrieval performance