---
ver: rpa2
title: 'On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and
  Perturbations'
arxiv_id: '2510.21689'
source_url: https://arxiv.org/abs/2510.21689
tags:
- explanations
- detection
- confidence
- lime
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that post-hoc explainability methods (CAM,
  LIME, and perturbation-based explanations) can validate computer vision models for
  conservation monitoring. Applied to aerial imagery of harbor seals in Glacier Bay
  National Park, these methods reveal that the Faster R-CNN detector relies on seal
  torsos and contours rather than background context, with LayerCAM achieving 67.7%
  attribution ratio and 94.7% maximum saliency hit rate within bounding boxes.
---

# On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations

## Quick Facts
- **arXiv ID**: 2510.21689
- **Source URL**: https://arxiv.org/abs/2510.21689
- **Reference count**: 23
- **Primary result**: Gradient-based and perturbation explainability methods validate that a Faster R-CNN seal detector relies on animal contours, not background context, with 67.7% attribution ratio and 94.7% saliency hit rate.

## Executive Summary
This paper demonstrates that post-hoc explainability methods (CAM, LIME, and perturbation-based explanations) can validate computer vision models for conservation monitoring. Applied to aerial imagery of harbor seals in Glacier Bay National Park, these methods reveal that the Faster R-CNN detector relies on seal torsos and contours rather than background context, with LayerCAM achieving 67.7% attribution ratio and 94.7% maximum saliency hit rate within bounding boxes. Perturbation tests showed high faithfulness: masking or noise reduced confidence by 0.97-0.87 with 80-97% flip rates, while blur caused smaller changes. Explanations also identified systematic errors, such as confusion between seals and black ice/rocks. By pairing object detection with explainability, the study moves beyond "black-box" predictions toward auditable, decision-supporting tools that increase trust and guide model improvements in ecological conservation.

## Method Summary
The study applies post-hoc explainability methods to validate a Faster R-CNN object detector trained on aerial imagery of harbor seals. The detector uses ResNet-50 with FPN, pretrained on COCO and fine-tuned on 1,744 images. Three explainability approaches are employed: (1) gradient-based CAM variants (LayerCAM, HiResCAM) applied to the backbone's layer4 to generate spatial relevance maps; (2) LIME with detector-aware weighting and SLIC superpixel segmentation; and (3) perturbation-based deletion tests using mask, noise, and blur operations with greedy forward selection. Evaluation metrics include attribution ratio, maximum saliency hit rate, flip rate, and confidence drop under perturbation.

## Key Results
- LayerCAM achieved 67.7% attribution ratio and 94.7% maximum saliency hit rate, indicating strong localization fidelity within seal bounding boxes.
- Perturbation tests showed high faithfulness: mask and noise edits reduced confidence by 0.97-0.87 with 80-97% flip rates, while blur caused smaller changes (0.27 confidence drop, 21.33% flip rate).
- Attribution maps consistently highlighted seal torsos and contours rather than background context, with systematic errors identified in false positives (confusion with black ice/rocks).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based CAM methods localize detections to biologically meaningful regions rather than background context.
- **Mechanism:** LayerCAM and HiResCAM compute element-wise products between activations and gradients at internal convolutional layers, producing spatial relevance maps without global pooling. This preserves fine-grained spatial fidelity needed for small, low-contrast animals in aerial imagery.
- **Core assumption:** Gradients with respect to class logits accurately reflect feature importance; high-activation regions correspond to features the model actually uses.
- **Evidence anchors:**
  - [abstract] "Explanations concentrate on seal torsos and contours rather than surrounding ice/rock"
  - [Section 4] "LayerCAM achieved an attribution ratio of 67.69±25.13%, with a maximum-saliency hit rate of 94.70% (125/132 boxes)"
  - [corpus] Weak direct corpus support; related work on perturbation-based explanations exists (arXiv:2510.03317) but does not validate CAM specifically.
- **Break condition:** If attribution maps highlight background context (ice, water, rocks) more than animal regions, or if maximum saliency consistently falls outside bounding boxes, the mechanism fails.

### Mechanism 2
- **Claim:** Perturbation-based deletion tests demonstrate faithfulness by showing that removing high-attribution regions suppresses detection confidence.
- **Mechanism:** A greedy forward-selection algorithm iteratively perturbs (masks, adds noise, or blurs) the most important superpixels within or near target bounding boxes until confidence drops below threshold τ=0.5 or the object is no longer detected.
- **Core assumption:** Perturbation removes evidence rather than introducing artifacts; confidence drops reflect genuine feature necessity rather than distribution shift.
- **Evidence anchors:**
  - [abstract] "Perturbation tests showed high faithfulness: masking or noise reduced confidence by 0.97-0.87 with 80-97% flip rates"
  - [Section 4] "Blur caused a smaller average decrease (0.27) and a lower flip rate (21.33%)" — indicating intact contours matter more than fine texture
  - [corpus] arXiv:2510.03317 (Photorealistic Inpainting for Perturbation-based Explanations) validates perturbation approaches in ecological monitoring but notes masking can introduce artifacts.
- **Break condition:** If perturbations cause no confidence change, or if blur (which preserves structure) flips predictions as readily as masking, the mechanism may conflate artifact sensitivity with feature necessity.

### Mechanism 3
- **Claim:** Triangulating across multiple explainability methods (CAM, LIME, perturbations) reveals systematic error modes that single methods miss.
- **Mechanism:** Each method has different sensitivities—CAM to layer choice, LIME to superpixel segmentation, perturbations to edit type. When multiple methods highlight the same confounding features (e.g., black ice), confidence in the diagnosis increases.
- **Core assumption:** Method-specific artifacts are uncorrelated; convergent evidence reflects true model behavior rather than explainer bias.
- **Evidence anchors:**
  - [Section 5] "Using multiple methods reduces over-reliance on any single explainer... and strengthens confidence in the explanations when different methods consistently highlight the same visual features"
  - [Section 4] False positive analysis showed "attribution maps consistently highlighted the confounding structures" across LayerCAM, HiResCAM, and LIME
  - [corpus] arXiv:2506.15499 (Pixel-level Certified Explanations) explicitly addresses attribution fragility, supporting the need for method triangulation.
- **Break condition:** If methods disagree substantially on failure modes, or if a single method's explanations contradict behavioral tests, triangulation provides no diagnostic benefit.

## Foundational Learning

- **Concept:** **Class Activation Mapping (CAM) and gradient-based attribution**
  - Why needed here: Understanding how CAM variants convert internal representations into spatial heatmaps is essential for interpreting localization fidelity results.
  - Quick check question: If LayerCAM shows 94.7% max saliency hit rate but only 67.7% attribution ratio, what does this tell you about the spatial distribution of importance?

- **Concept:** **Object detection confidence calibration**
  - Why needed here: Perturbation tests measure confidence drops, but detector confidence scores are often miscalibrated; understanding this limitation is critical for interpreting faithfulness metrics.
  - Quick check question: A perturbation reduces confidence from 0.95 to 0.55 without flipping the detection. Is this a "successful" faithfulness test? What threshold choice would change your answer?

- **Concept:** **Superpixel segmentation for LIME and perturbations**
  - Why needed here: Both LIME and perturbation-based explanations depend on image segmentation; poor segmentation (e.g., merging seal and ice into one superpixel) can produce misleading attributions.
  - Quick check question: The paper uses SLIC in LAB color space with high compactness. Why might this fail for dark seals on dark ice, and how would you detect such failures?

## Architecture Onboarding

- **Component map:** Image -> Faster R-CNN (ResNet-50 + FPN) -> detections (boxes, classes, confidence scores) -> Explainers (LayerCAM/HiResCAM, LIME, Perturbations) -> Attribution maps and perturbation results -> Evaluation metrics

- **Critical path:**
  1. Image → Faster R-CNN → detections (boxes, classes, confidence scores)
  2. For each detection: extract gradients from `layer4` → compute LayerCAM/HiResCAM → upsample and normalize
  3. In parallel: segment image → run LIME perturbations → fit surrogate model → generate explanation map
  4. For faithfulness: segment → greedy perturbation loop → track confidence until flip or iteration limit
  5. Aggregate metrics across test set; manually review false positives with overlaid attributions

- **Design tradeoffs:**
  - LayerCAM vs. HiResCAM: LayerCAM had higher attribution ratio (67.7% vs. 58.6%) but HiResCAM may be more faithful for certain architectures (paper uses both)
  - Perturbation type: Mask/noise are destructive (high flip rates) but may introduce artifacts; blur preserves structure (low flip rate) but tests texture reliance
  - LIME weighting modes: Confidence-weighted vs. area-weighted vs. uniform—choice depends on whether you want to explain high-confidence detections or all instances equally

- **Failure signatures:**
  - Low attribution ratio (<50%) with high max saliency hit-rate: explainer focuses on small subregions (may be correct for partial occlusion or incorrect if missing broader context)
  - High flip rate with blur but low flip rate with mask: model relies on texture rather than structure (unusual; inverse is expected)
  - CAM and LIME disagree on false positive explanations: suggests explainer artifacts rather than genuine model behavior; investigate segmentation quality

- **First 3 experiments:**
  1. **Reproduce localization fidelity metrics** on a held-out subset: compute attribution ratio and max saliency hit-rate for LayerCAM, HiResCAM, and LIME; verify reported numbers (67.7%, 94.7%, etc.)
  2. **Ablate perturbation types**: Run deletion tests with mask, noise, blur, and black-fill on 20 images; compare flip rates and confidence drops to validate the claim that intact contours (not color) drive detection
  3. **Failure mode audit**: Isolate all false positives in the test set; overlay CAM and LIME attributions; manually categorize error sources (black ice, rocks, annotation gaps) and propose targeted augmentations

## Open Questions the Paper Calls Out
- **Open Question 1:** Can targeted data augmentation based on XAI-identified failure modes (e.g., black ice) significantly reduce false positive rates in retrained models?
  - Basis in paper: The Discussion suggests "targeted data augmentation to include more negative examples of black ice" as a specific next step for model improvement.
  - Why unresolved: The paper identifies the systematic error (confusion between seals and black ice) but does not experimentally verify if incorporating these hard negatives actually improves model performance.
  - What evidence would resolve it: A comparative evaluation of model precision and recall before and after fine-tuning with an XAI-informed dataset containing the identified confounding features.

- **Open Question 2:** To what degree do high flip rates in perturbation tests reflect genuine causal reliance on features versus sensitivity to out-of-distribution (OOD) artifacts?
  - Basis in paper: The Discussion notes that "deletion edits... can induce distribution shift that conflates causal evidence removal with artifacts."
  - Why unresolved: Standard perturbations like masking and noise drastically alter image statistics, making it unclear if the model drops detection due to missing semantic evidence or simply because the input looks unnatural.
  - What evidence would resolve it: Experiments comparing standard perturbations against in-painting or generative infilling methods that remove semantic content while preserving background statistics.

- **Open Question 3:** Does the localization fidelity and diagnostic utility of this XAI workflow generalize to other detection architectures (e.g., YOLO, transformers) or complex ecological environments?
  - Basis in paper: The authors state they "do not claim habitat- or model-general conclusions" and limit their scope to a Faster R-CNN case study.
  - Why unresolved: The study is restricted to one species and architecture; the robustness of attribution metrics like "attribution ratio" in denser habitats or different model backbones is unverified.
  - What evidence would resolve it: Replication of the XAI evaluation pipeline on diverse conservation datasets (e.g., dense vegetation, marine environments) using various detector architectures.

## Limitations
- Dataset is domain-specific (aerial seal imagery from one national park), limiting generalizability to other ecological monitoring contexts.
- Attribution metrics depend on precise bounding box annotations and may not transfer to cases of partial occlusion or unclear borders.
- Perturbation tests use standard edit types (mask/noise/blur) that may conflate causal feature removal with out-of-distribution artifacts.

## Confidence
- **High**: Gradient-based methods localize detections to animal regions rather than background; perturbation tests confirm feature importance; method triangulation identifies systematic errors.
- **Medium**: Attribution ratios and hit-rates are reproducible with same dataset; perturbation faithfulness generalizes across edit types.
- **Low**: LIME weighting choices (confidence vs. area) meaningfully affect explanations; explanations generalize to other ecological monitoring contexts.

## Next Checks
1. **Cross-domain transferability**: Apply LayerCAM/LIME to a different aerial detection task (e.g., bird colonies) and compare attribution ratios and hit-rates.
2. **Perturbation ablation study**: Test additional edit types (salt-and-pepper noise, structured patterns) to validate that mask/noise/blur capture the full space of feature importance.
3. **Annotation sensitivity**: Randomly dilate/erode bounding boxes by 10-20% and measure changes in attribution ratio and hit-rate to quantify dependence on precise annotations.