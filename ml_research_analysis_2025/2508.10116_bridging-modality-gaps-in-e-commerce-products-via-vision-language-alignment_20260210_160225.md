---
ver: rpa2
title: Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment
arxiv_id: '2508.10116'
source_url: https://arxiv.org/abs/2508.10116
tags:
- e-commerce
- product
- visual
- opal
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating structured, schema-compliant
  product descriptions in e-commerce from images, focusing on bridging the modality
  gap between visual and textual information. The authors propose Optimized Preference-Based
  AI for Listings (OPAL), a framework that fine-tunes multimodal large language models
  (MLLMs) using two data refinement methods: MLLM-Assisted Conformity Enhancement
  (MACE) for aligning text and image modalities, and LLM-Assisted Contextual Understanding
  (LACU) for enriching context through multi-turn conversations.'
---

# Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment

## Quick Facts
- arXiv ID: 2508.10116
- Source URL: https://arxiv.org/abs/2508.10116
- Reference count: 39
- Generates structured, schema-compliant product descriptions from images, achieving 53.40% improvement in Final Submission Rate

## Executive Summary
This paper addresses the challenge of generating structured, schema-compliant product descriptions in e-commerce from images by proposing OPAL (Optimized Preference-Based AI for Listings). The framework bridges the modality gap between visual and textual information through two data refinement methods: MACE (MLLM-Assisted Conformity Enhancement) for aligning text and image modalities, and LACU (LLM-Assisted Contextual Understanding) for enriching context through synthetic multi-turn conversations. The method combines visual instruction tuning with Direct Preference Optimization (DPO) to improve accuracy and reduce hallucinations, showing significant improvements over baselines in Rouge-L F1, Aspect F1, and Schema-Compliance Recall metrics.

## Method Summary
OPAL is a two-stage fine-tuning framework that processes raw e-commerce image-description pairs through MACE (using a large MLLM to filter non-visually grounded attributes) and LACU (using a text-only LLM to generate synthetic conversational data). The refined data is then used to fine-tune a vision-language model through visual instruction tuning on JSON-formatted structured outputs, followed by DPO training on preference pairs where the model's own incorrect generations are penalized against ground truth. The pipeline uses a frozen vision encoder with a trainable LLM backbone, operating on 1M raw pairs refined to 890K MACE-aligned and 800K LACU-enriched samples.

## Key Results
- Rouge-L F1 improved from 0.42 to 0.63 compared to baselines
- Aspect F1 increased from 0.45 to 0.52 through the OPAL pipeline
- Schema-Compliance Recall reached 0.82, significantly higher than baseline of 0.71
- Achieved 53.40% improvement in Final Submission Rate compared to retrieval-based baseline

## Why This Works (Mechanism)

### Mechanism 1: Conformity-Driven Noise Filtering (MACE)
The paper uses a large MLLM (InternVL2.5-78B) to strip attributes from product metadata that cannot be verified in the image, forcing the downstream model to rely on visual evidence rather than spurious textual correlations. This reduces hallucinations by enforcing stricter alignment between generated text and visual features. The core assumption is that the majority of errors stem from training on misaligned metadata rather than limitations in the visual encoder.

### Mechanism 2: Contextual Enrichment via Synthetic Dialogue (LACU)
A text-only LLM simulates buyer-seller dialogues based on product images and metadata, exposing the model to rare attributes in a conversational context. This synthetic data compensates for domain knowledge limitations of generic pretraining by effectively fine-tuning the model's attention on niche visual features that generic datasets miss. The core assumption is that a text-only LLM can accurately synthesize visual reasoning steps based solely on metadata descriptions.

### Mechanism 3: Preference-Based Hallucination Reduction (DPO)
Direct Preference Optimization constructs "preference pairs" where the model's own incorrect outputs are compared against ground truth, maximizing the likelihood of the chosen output. This explicitly penalizes generation patterns that are plausible but factually incorrect relative to the image. The core assumption is that the "Judge" model (InternVL2.5-78B) provides a sufficiently accurate proxy for human evaluation to generate high-quality preference pairs.

## Foundational Learning

- **Concept: Modality Gap**
  - **Why needed here:** E-commerce data is inherently misaligned (e.g., text mentions "100% cotton" but the image is just a shirt). Understanding this gap is crucial to grasping why naive fine-tuning fails (hallucination).
  - **Quick check question:** Why would a model trained on raw listings generate "Free Shipping" when looking at an image of a toaster?

- **Concept: Visual Instruction Tuning**
  - **Why needed here:** OPAL relies on formatting visual data into instruction-response pairs (JSON) rather than simple captions. This is the bridge that turns an MLLM into a structured data extraction tool.
  - **Quick check question:** How does training on "Question: What is this? Answer: A shoe" differ from training on "Classify this image: {Class: Shoe}" in terms of model behavior?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper uses DPO instead of Reinforcement Learning (RLHF) to align the model. Understanding DPO is necessary to implement the proposed stability improvements.
  - **Quick check question:** In DPO, do you need to train a separate "Reward Model" network, or does the policy model update directly using the preference data?

## Architecture Onboarding

- **Component map:** Raw Data -> MACE (InternVL2.5-78B) -> LACU (LLaMA 3.1) -> Refined JSON/Conversations -> Vision Encoder (Frozen) + LLM Backbone (Trainable) -> SFT Stage -> DPO Stage -> Structured JSON Output

- **Critical path:**
  1. **Data Cleaning (MACE):** Ensuring the training signal is visually grounded (prevents garbage-in, garbage-out)
  2. **SFT (Visual Instruction Tuning):** The foundational step where the model learns the schema and basic visual mapping
  3. **DPO Alignment:** The refinement step that boosts schema compliance and reduces hallucination

- **Design tradeoffs:**
  - Synthetic Data Quality: LACU uses text-only LLMs to generate visual conversations, scaling cheaply but risking "hallucination drift" if metadata is wrong
  - Judge Reliability: Heavy dependency on a large 78B model to grade a smaller 8B model; if the 78B model fails on a niche category, DPO data becomes noisy
  - Frozen Vision Encoder: Reduces compute cost but may limit adaptation to highly specialized e-commerce image styles

- **Failure signatures:**
  - Empty JSON Outputs: Likely a schema mismatch in the instruction template or over-regularization from DPO
  - Repetitive Promotional Text: Suggests MACE filtering failed or the model is overfitting to common noisy prefixes in pretraining corpus
  - Niche Item Misclassification: Indicates LACU step did not cover this specific item category, or backbone model lacks requisite "world knowledge"

- **First 3 experiments:**
  1. **MACE Ablation:** Train a baseline on raw vs. MACE-filtered data, evaluating the rate of "non-visual" attributes appearing in output
  2. **LACU Data Volume Scaling:** Train models with varying amounts (0%, 50%, 100%) of synthetic conversational data to measure impact on rare brand recall
  3. **Schema Compliance Test:** Run OPAL inference on held-out validation set and measure "Schema-Compliance Recall" to ensure DPO enforces required output structure

## Open Questions the Paper Calls Out
1. How can OPAL be extended to effectively aggregate information and reason across multiple product images (e.g., close-ups, angles) rather than relying on a single main image?
2. How can the OPAL framework be expanded into an agentic system that dynamically queries external knowledge bases or interacts with sellers to resolve ambiguous visual inputs?
3. Can the strict visual grounding in MACE be relaxed to allow the model to infer non-visual attributes (e.g., material composition, dimensions) by correlating visual features with probabilistic external knowledge, rather than discarding them?

## Limitations
- Heavy reliance on large proprietary models (InternVL2.5-78B for MACE, LLaMA 3.1 for LACU) creates significant reproducibility barriers
- Synthetic dialogue generation assumes metadata accuracy without verification, creating potential hallucination cascades
- Schema-compliance evaluation methodology lacks transparency regarding edge case handling and ambiguous attribute boundaries

## Confidence
- **High Confidence**: Core methodology (MACE for filtering, DPO for alignment), quantitative improvements on established metrics, basic ablation study showing sequential training benefits
- **Medium Confidence**: Real-world deployment results (53.40% improvement), qualitative examples of hallucination reduction, generalization to unseen items
- **Low Confidence**: Exact prompt templates for MACE/LACU, complete evaluation protocol details, robustness across diverse e-commerce domains

## Next Checks
1. **MACE Filtering Validation**: Manually audit 100 randomly selected filtered attributes to verify the 78B model correctly identified non-visual information, measuring false positive/negative rates
2. **DPO Preference Quality Audit**: Sample 50 preference pairs from the training set to assess whether the "Judge" model consistently distinguishes valid from invalid generations across product categories
3. **Cross-Schema Generalization Test**: Evaluate OPAL on two additional e-commerce schemas (e.g., fashion vs. electronics) to measure schema compliance degradation when switching domains