---
ver: rpa2
title: 'SwinECAT: A Transformer-based fundus disease classification model with Shifted
  Window Attention and Efficient Channel Attention'
arxiv_id: '2507.21922'
source_url: https://arxiv.org/abs/2507.21922
tags:
- fundus
- transformer
- swinecat
- swin
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SwinECAT, a Transformer-based model for fundus
  disease classification that combines the Shifted Window (Swin) Attention with Efficient
  Channel Attention (ECA) mechanisms. The model addresses the challenge of accurately
  classifying fundus images containing small lesions and subtle inter-disease differences.
---

# SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention

## Quick Facts
- **arXiv ID**: 2507.21922
- **Source URL**: https://arxiv.org/abs/2507.21922
- **Reference count**: 40
- **Primary result**: SwinECAT achieves 88.29% accuracy, 0.88 weighted F1-score, and 0.90 macro F1-score on EDID dataset

## Executive Summary
This paper introduces SwinECAT, a Transformer-based model for fundus disease classification that combines Shifted Window (Swin) Attention with Efficient Channel Attention (ECA) mechanisms. The model addresses the challenge of accurately classifying fundus images containing small lesions and subtle inter-disease differences. SwinECAT leverages Swin Attention to capture local spatial structures and long-range dependencies, while the lightweight ECA mechanism enhances feature channel discrimination. Evaluated on the Eye Disease Image Dataset (EDID) with 16,140 fundus images across 9 disease categories, SwinECAT achieved 88.29% accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. These results significantly outperform baseline models including the original Swin Transformer and other vision models. The proposed architecture demonstrates superior performance while maintaining parameter efficiency, making it well-suited for medical image classification tasks.

## Method Summary
SwinECAT is a hybrid Transformer-based architecture that integrates Shifted Window Attention and Efficient Channel Attention mechanisms. The model builds upon the Swin Transformer foundation, utilizing shifted window attention to capture both local spatial structures and long-range dependencies in fundus images. The Efficient Channel Attention module is incorporated to enhance feature discrimination across different disease categories by weighting feature channels according to their importance. The model is trained on the Eye Disease Image Dataset (EDID) containing 16,140 fundus images across 9 disease categories. The architecture maintains parameter efficiency while achieving superior classification performance compared to baseline models.

## Key Results
- Achieved 88.29% accuracy on EDID dataset with 16,140 fundus images across 9 disease categories
- Obtained weighted F1-score of 0.88 and macro F1-score of 0.90, outperforming baseline models
- Demonstrated superior performance while maintaining parameter efficiency compared to other vision models

## Why This Works (Mechanism)
SwinECAT works by combining the strengths of two complementary attention mechanisms. The Shifted Window Attention captures local spatial structures through non-overlapping window partitions and establishes long-range dependencies through window shifting operations between consecutive layers. This hierarchical approach allows the model to progressively learn multi-scale representations of fundus images. The Efficient Channel Attention mechanism operates in parallel to weigh feature channels based on their importance for disease classification, enhancing the model's ability to distinguish between similar-looking fundus conditions. The combination addresses the specific challenges of fundus disease classification: small lesion detection and subtle inter-disease differences. The lightweight ECA module adds minimal computational overhead while significantly improving classification accuracy, making the architecture both effective and efficient for medical image analysis.

## Foundational Learning
- **Shifted Window Attention**: A hierarchical attention mechanism that partitions images into non-overlapping windows and shifts them between layers to capture both local and global features. Why needed: Fundus images contain lesions of varying sizes that require multi-scale feature extraction. Quick check: Verify window sizes and shifting patterns in the architecture.
- **Efficient Channel Attention**: A lightweight channel weighting mechanism that enhances feature discrimination by emphasizing important channels. Why needed: Different disease categories have subtle differences that require channel-specific feature enhancement. Quick check: Confirm ECA operates in parallel with other layers without significant computational overhead.
- **Transformer-based medical imaging**: Adapting self-attention mechanisms for medical image classification tasks. Why needed: Traditional CNNs may miss long-range dependencies crucial for disease diagnosis. Quick check: Compare attention patterns with standard medical imaging approaches.
- **Multi-class fundus disease classification**: Handling 9 disease categories with subtle visual differences. Why needed: Accurate diagnosis requires distinguishing between similar-looking conditions. Quick check: Verify class distribution balance in the dataset.
- **Parameter efficiency in medical models**: Designing architectures that maintain performance while minimizing parameters. Why needed: Medical applications require models that can run on limited hardware resources. Quick check: Compare parameter counts with baseline models.
- **Hierarchical feature learning**: Progressive feature extraction across multiple scales. Why needed: Fundus lesions vary in size and require different levels of detail. Quick check: Examine feature maps at different layers.

## Architecture Onboarding
**Component Map**: Input Image -> Swin Attention Layers -> ECA Module -> Classification Head
**Critical Path**: Image → Patch Embedding → Swin Attention Blocks (with Window Partition & Shift) → ECA → MLP Head → Output
**Design Tradeoffs**: The architecture prioritizes accuracy over computational efficiency by incorporating ECA, but maintains reasonable parameter counts. The shifted window approach trades some global context for improved local feature capture, which is beneficial for small lesion detection.
**Failure Signatures**: The model may struggle with extremely small lesions that fall below the window partition size, or with diseases that have highly similar visual features across channels where ECA cannot effectively discriminate.
**First 3 Experiments**: 1) Train baseline Swin Transformer without ECA on EDID dataset, 2) Add ECA module and evaluate performance improvement, 3) Test with different window sizes to optimize local vs global feature balance.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is based on a single dataset (EDID) with 16,140 images across 9 disease categories, limiting generalizability to other clinical settings or disease distributions
- Study lacks cross-validation results and confidence intervals for performance metrics, making statistical significance of improvements unclear
- Parameter efficiency claims are not substantiated with direct comparisons to the number of parameters in competing models
- Paper lacks ablation studies demonstrating the individual contributions of Swin Attention and ECA mechanisms to overall performance

## Confidence
- **High confidence**: The architectural integration of Swin Attention and ECA mechanisms is technically sound and represents a valid approach to the problem
- **Medium confidence**: The reported performance improvements over baselines, as the evaluation methodology lacks statistical validation and uses a single dataset
- **Low confidence**: Claims about parameter efficiency and clinical applicability without supporting evidence or implementation details

## Next Checks
1. Conduct cross-validation experiments on the EDID dataset with confidence intervals to establish statistical significance of performance improvements
2. Perform ablation studies to quantify the individual contributions of Swin Attention and ECA mechanisms to overall model performance
3. Evaluate model performance on additional independent fundus image datasets to assess generalizability across different clinical settings and disease distributions