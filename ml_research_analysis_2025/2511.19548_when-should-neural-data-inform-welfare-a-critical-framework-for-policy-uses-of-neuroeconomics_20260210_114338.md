---
ver: rpa2
title: When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses
  of Neuroeconomics
arxiv_id: '2511.19548'
source_url: https://arxiv.org/abs/2511.19548
tags:
- welfare
- neural
- policy
- https
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops a model-based framework linking neural signals,\
  \ computational decision models, and normative welfare criteria to clarify when\
  \ neural data can legitimately inform policy welfare analysis. Drawing on actor\u2013\
  critic reinforcement learning and dual-self models, it formalizes the path from\
  \ neural activity (e.g., dopamine prediction errors) to value functions, and then\
  \ to welfare judgments, emphasizing that welfare claims require explicit normative\
  \ standards beyond computational modeling."
---

# When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics
## Quick Facts
- arXiv ID: 2511.19548
- Source URL: https://arxiv.org/abs/2511.19548
- Authors: Yiven; Zhu
- Reference count: 0
- Key outcome: Develops framework linking neural signals to welfare analysis through computational models and normative criteria

## Executive Summary
This paper develops a model-based framework that clarifies when neural data can legitimately inform welfare analysis for policy purposes. The framework bridges actor-critic reinforcement learning models, dual-self decision models, and normative welfare criteria to create explicit conditions under which neural evidence can constrain welfare judgments. The authors emphasize that while neural data can refine welfare models, they cannot replace normative argumentation, requiring policymakers to explicitly specify which welfare standard applies. The work extends to NeuroAI by treating both brains and artificial agents as value-learning systems, though with caution about interpreting internal reward signals as welfare measures.

## Method Summary
The framework employs a model-based approach that links neural signals (such as dopamine prediction errors) to value functions through computational decision models, then connects these to normative welfare criteria. Drawing on actor-critic reinforcement learning and dual-self models, the analysis derives necessary conditions for neural evidence to inform welfare analysis: measurement validity, model identifiability, normative transparency, and policy feasibility. The framework is applied to case studies including addiction, neuromarketing, and environmental policy, and extended to compare biological and artificial value-learning systems.

## Key Results
- Derives four necessary conditions for neural data to legitimately inform welfare analysis: measurement validity, model identifiability, normative transparency, and policy feasibility
- Formalizes the path from neural activity to value functions to welfare judgments using actor-critic reinforcement learning models
- Demonstrates framework application to addiction, neuromarketing, and environmental policy contexts
- Extends framework to NeuroAI, treating brains and artificial agents as value-learning systems while distinguishing computational signals from welfare measures

## Why This Works (Mechanism)
The framework works by creating explicit theoretical bridges between observable neural phenomena and normative welfare claims through computational decision models. By formalizing the relationship between dopamine prediction errors and value learning in actor-critic systems, it provides a mechanistic pathway from neural data to economic preferences. The dual-self modeling approach captures the tension between immediate rewards and long-term well-being, while the requirement for normative transparency ensures that welfare judgments remain grounded in explicit ethical standards rather than purely technical analysis.

## Foundational Learning
- **Actor-Critic Reinforcement Learning**: Why needed - provides computational model linking neural signals to value learning; Quick check - verify dopamine prediction error corresponds to temporal difference error in model
- **Dual-Self Decision Models**: Why needed - captures conflict between impulsive and deliberative decision systems; Quick check - confirm model can reproduce hyperbolic discounting behavior
- **Normative Welfare Criteria**: Why needed - establishes ethical foundation for interpreting preferences as welfare; Quick check - verify welfare judgment depends on explicitly stated criterion
- **Measurement Validity**: Why needed - ensures neural signals actually reflect intended psychological constructs; Quick check - correlate neural measure with behavioral validation task
- **Model Identifiability**: Why needed - guarantees unique mapping from data to theoretical parameters; Quick check - test whether different parameter sets produce identical predictions

## Architecture Onboarding
**Component Map**: Neural Signals -> Computational Model -> Value Function -> Normative Criterion -> Welfare Judgment

**Critical Path**: Measurement validity → Model identifiability → Normative transparency → Policy feasibility

**Design Tradeoffs**: Technical precision vs. normative flexibility; empirical rigor vs. practical applicability; model complexity vs. interpretability

**Failure Signatures**: 
- Invalid neural measurements produce spurious welfare inferences
- Unidentified models generate multiple incompatible welfare interpretations
- Absent normative standards lead to arbitrary welfare conclusions
- Unfeasible policies arise from technically sound but practically impossible recommendations

**3 First Experiments**:
1. Apply framework to existing fMRI addiction dataset to test measurement validity condition
2. Simulate welfare inference under varying levels of neural measurement noise
3. Evaluate policy feasibility using neuromarketing case study with real-world constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Neural measurement noise and individual heterogeneity may violate clean mapping assumptions between neural signals and value functions
- Framework cannot resolve which normative welfare criterion should apply, only that some criterion must be explicitly chosen
- Extension to NeuroAI remains largely theoretical with limited empirical grounding for comparing biological and artificial value-learning systems

## Confidence
- High: Mathematical formalism linking neural signals to value functions
- Medium: Policy applicability conditions and derived necessary criteria
- Low: Cross-domain generalizability (brain-to-AI transfer claims)

## Next Checks
1. Empirical test of the measurement validity condition using existing fMRI datasets from economic decision tasks
2. Simulation study evaluating how different levels of neural measurement noise affect welfare inference accuracy
3. Case study applying the checklist to an actual neuromarketing dataset to assess practical feasibility of the framework's conditions