---
ver: rpa2
title: 'HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation
  within the Model Context Protocol'
arxiv_id: '2508.07602'
source_url: https://arxiv.org/abs/2508.07602
tags:
- tool
- hgmf
- tools
- server
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hierarchical Gaussian Mixture Framework
  (HGMF), a probabilistic pruning method for scalable tool invocation in Large Language
  Models (LLMs). HGMF addresses the challenge of selecting the correct tool from large,
  hierarchically-structured libraries, which is hindered by LLM context limitations
  and semantic noise from irrelevant options.
---

# HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol

## Quick Facts
- **arXiv ID:** 2508.07602
- **Source URL:** https://arxiv.org/abs/2508.07602
- **Reference count:** 19
- **Primary result:** Hierarchical GMM pruning improves tool selection accuracy by up to 40% over MCP-zero while reducing inference latency.

## Executive Summary
This paper introduces HGMF, a probabilistic pruning framework for scalable tool invocation in Large Language Models (LLMs). The framework addresses the challenge of selecting correct tools from large, hierarchically-structured libraries by first clustering and filtering servers based on query likelihood, then applying the same GMM-based clustering and filtering to tools within selected servers. This two-stage hierarchical process produces a compact, high-relevance candidate set that simplifies final selection for the LLM. Experiments demonstrate significant improvements in tool selection accuracy across eight LLMs while reducing inference latency compared to baseline methods.

## Method Summary
HGMF operates through a hierarchical two-stage pruning process. First, it encodes all server and tool descriptions using all-MiniLM-L6-v2 embeddings with L2 normalization. The framework then fits Gaussian Mixture Models to server embeddings (with K_s = ⌈√M⌉ components) and filters servers based on query likelihood scores, retaining the top N_s clusters. For each selected server, it repeats the process with tool embeddings (K_t = ⌈√|T_i|⌉ components), retaining the top N_t clusters. The framework incorporates regularization (inter-class and intra-class) to stabilize GMM fitting in low-data regimes. Finally, an LLM reranker generates ideal descriptions for the filtered candidates and applies a similarity scoring formula to produce the final ranking.

## Key Results
- HGMF achieves up to 40% higher accuracy than MCP-zero across eight different LLMs
- The framework reduces inference latency by filtering out irrelevant tools before LLM processing
- Regularization improves performance in low-shot scenarios (41-401 tools) by 14-28% compared to unregularized GMMs
- Hierarchical pruning maintains high accuracy even as tool library sizes increase from 41 to 2797 tools

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Search Space Decomposition
- **Claim:** Decomposing tool selection into server-level then tool-level pruning reduces noise and computational overhead versus flat retrieval.
- **Mechanism:** Server-level GMM likelihood scoring identifies relevant servers, constraining the tool-level search to semantically coherent subsets rather than the full library.
- **Core assumption:** Relevant tools cluster within semantically coherent servers, and query-server similarity predicts query-tool relevance within that server.
- **Evidence anchors:** [abstract] and [section 2.2] describe hierarchical pruning producing compact, high-relevance candidate sets; related work (ScaleMCP, NaviAgent) addresses orchestration but not hierarchical pruning specifically.
- **Break condition:** If tools within servers are semantically heterogeneous or if the correct tool's server has poor query matching, server-level pruning may discard relevant candidates irrecoverably.

### Mechanism 2: Probabilistic Soft Clustering via Gaussian Mixture Models
- **Claim:** GMM-based soft clustering captures query-to-cluster affinity more robustly than hard thresholding or single-point similarity.
- **Mechanism:** GMMs model clusters as Gaussian distributions with means and covariances; query likelihood provides graded relevance scores rather than binary decisions.
- **Core assumption:** The embedding space is sufficiently well-structured that semantically related tools/servers form Gaussian-like clusters with meaningful boundaries.
- **Evidence anchors:** [abstract] and [section 1] highlight GMM adaptation for probabilistic soft clustering; related work on tool invocation doesn't compare GMM vs. hard clustering.
- **Break condition:** If embeddings are highly non-Gaussian (multimodal, heavy-tailed, or uniformly scattered), GMM assumptions may mischaracterize cluster structure, leading to spurious likelihood rankings.

### Mechanism 3: Regularization for Low-Data Cluster Stability
- **Claim:** Inter-class and intra-class regularization improve pruning reliability when cluster parameter estimates become unstable due to sparse data.
- **Mechanism:** Inter-class regularization prevents cluster center aggregation while intra-class regularization constrains covariance trace to prevent over-elongation; covariance floor prevents singular matrices.
- **Core assumption:** Without regularization, small sample sizes cause GMM components to overfit or collapse, producing unreliable likelihood scores.
- **Evidence anchors:** [abstract] and [section 2.2] describe regularization promoting separation and ensuring compactness in small sample regimes; no corpus papers explicitly test GMM regularization in tool retrieval.
- **Break condition:** If regularization weights are mis-tuned, clusters may become too rigid (underfitting) or remain unstable; the paper lacks adaptive tuning guidance.

## Foundational Learning

- **Concept: Gaussian Mixture Models and Expectation-Maximization**
  - **Why needed here:** HGMF relies on fitting GMMs to server and tool embeddings; understanding EM's iterative estimation of means, covariances, and mixture weights is essential for debugging convergence failures.
  - **Quick check question:** Given a 2D dataset with two overlapping clusters, can you sketch how EM would assign soft membership probabilities differently from K-means?

- **Concept: Semantic Embeddings and L2 Normalization**
  - **Why needed here:** The framework uses all-MiniLM-L6-v2 embeddings projected onto a unit hypersphere; L2 normalization makes cosine similarity equivalent to dot products, affecting likelihood computations.
  - **Quick check question:** If two embeddings have L2 norms of 5 and 2 before normalization, what happens to their dot product after L2 normalization compared to before?

- **Concept: Hierarchical Retrieval-Reranking Pipelines**
  - **Why needed here:** HGMF is a two-stage retrieval system (prune → LLM rerank); understanding tradeoffs between recall at stage 1 and precision at stage 2 is critical for tuning N_s and N_t.
  - **Quick check question:** If you increase the number of retained clusters (N_s) from 2 to 5, what happens to recall and to LLM inference cost?

## Architecture Onboarding

- **Component map:** Query embedding → Server GMM likelihood ranking → Tool GMM likelihood ranking (within selected servers) → LLM description generation → Cosine similarity scoring → Top-ranked pair returned
- **Critical path:** Query embedding → Server GMM likelihood ranking → Tool GMM likelihood ranking (within selected servers) → LLM description generation → Cosine similarity scoring → Top-ranked pair returned
- **Design tradeoffs:**
  - **K_s, K_t (number of GMM components):** Larger K captures finer structure but risks overfitting and slower convergence; heuristic is ⌈√n⌉ but not validated across scales
  - **N_s, N_t (clusters retained):** Higher values improve recall but increase LLM context load; paper doesn't report sensitivity analysis
  - **Regularization weights (λ_inter, β_intra, w_balance, regcovar):** Strong regularization stabilizes low-data regimes but may suppress legitimate cluster variance; tuning appears manual
  - **LLM choice for reranking:** Larger models (Qwen-14b, Phi4-14b) achieve higher accuracy but add latency; small models (Phi3-3.8b) show weakest gains

- **Failure signatures:**
  - **Cluster collapse:** All tools assigned to one GMM component → check if regcovar is too low or sample size is extremely small
  - **Empty candidate set:** No server clusters exceed likelihood threshold → inspect query embedding quality and server description coverage
  - **Wrong server selected, correct tool exists within it:** Server-level pruning correct but tool-level GMM discards relevant cluster → N_t may be too low
  - **High latency despite pruning:** LLM reranking step dominates → candidate set T′ may still be too large; reduce N_s or N_t

- **First 3 experiments:**
  1. **Baseline replication on MCP-tools:** Implement embedding + server-level GMM + tool-level GMM without regularization; measure accuracy vs. MCP-zero at 4 sample sizes (41, 401, 1721, 2797 tools). Confirm ~78% baseline and ~82% HGMF as reported in Table 1.
  2. **Ablation on regularization:** Run HGMF with (a) no regularization, (b) inter-class only, (c) intra-class only, (d) full regularization. Plot accuracy vs. sample size to reproduce Figure 3 and quantify the 14–28% low-shot gain claim.
  3. **Sensitivity sweep on N_s and N_t:** Fix sample size at 1721 tools; vary N_s ∈ {1, 2, 3, 5, 8} and N_t ∈ {1, 2, 3, 5} per server. Measure accuracy, candidate set size, and LLM inference time to identify operating points where accuracy plateaus but latency continues to grow.

## Open Questions the Paper Calls Out
- **Adaptive clustering parameters:** Can data-driven mechanisms for determining K outperform the current square-root heuristic?
- **Graph-structured libraries:** How can HGMF be extended to model complex, graph-structured tool libraries rather than strictly hierarchical server-tool relationships?
- **Very small toolset regimes:** How can the framework mitigate performance degradation in small toolset scenarios acknowledged by the authors?

## Limitations
- Hyperparameter sensitivity (N_s, N_t, regularization weights) was not systematically studied, leaving performance bounds unclear across different library sizes
- The "up to 40% improvement" claim depends heavily on MCP-zero as a weak baseline; no comparison to stronger competitive methods
- No ablation on embedding quality—if tool/server descriptions are ambiguous, GMM likelihood rankings may be meaningless regardless of regularization

## Confidence
- **High:** Hierarchical pruning structure reduces search space (supported by clear methodology and intuitive appeal)
- **Medium:** Probabilistic soft clustering via GMM improves over hard thresholds (plausible but lacks external validation)
- **Medium:** Regularization stabilizes low-data clusters (reasonable assumption but untested against alternatives)

## Next Checks
1. **Robustness sweep:** Vary N_s ∈ {1, 2, 3, 5} and N_t ∈ {1, 2, 3, 5} across all sample sizes; plot accuracy vs. latency to identify diminishing returns
2. **Cross-dataset generalization:** Apply HGMF to a non-MCP tool library (e.g., HuggingFace or LangChain) and measure accuracy drop; this tests embedding and clustering assumptions beyond the original corpus
3. **Baseline strengthening:** Replace MCP-zero with a semantic search retriever (BM25 or dense retrieval) and re-run the 8 LLM accuracy comparison; if HGMF's gains shrink, the 40% figure may be inflated