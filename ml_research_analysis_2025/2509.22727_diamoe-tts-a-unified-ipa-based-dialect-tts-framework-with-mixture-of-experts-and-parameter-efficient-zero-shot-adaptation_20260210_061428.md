---
ver: rpa2
title: 'DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts
  and Parameter-Efficient Zero-Shot Adaptation'
arxiv_id: '2509.22727'
source_url: https://arxiv.org/abs/2509.22727
tags:
- speech
- dialect
- dialects
- data
- phoneme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building high-quality, low-resource
  dialect TTS systems, tackling issues like scarce data, inconsistent orthographies,
  and phonetic variation. The authors propose DiaMoE-TTS, a unified IPA-based framework
  built on F5-TTS that standardizes phonetic representations and introduces a dialect-aware
  Mixture-of-Experts (MoE) module to model phonological differences.
---

# DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation

## Quick Facts
- arXiv ID: 2509.22727
- Source URL: https://arxiv.org/abs/2509.22727
- Reference count: 0
- Low-resource dialect TTS with zero-shot adaptation capabilities, handling inconsistent orthographies and phonetic variation across Chinese dialects

## Executive Summary
DiaMoE-TTS addresses the challenge of building high-quality, low-resource dialect TTS systems by proposing a unified IPA-based framework built on F5-TTS. The system introduces a dialect-aware Mixture-of-Experts (MoE) module to model phonological differences and employs parameter-efficient adaptation using LoRA and Conditioning Adapters for rapid transfer to new dialects. The framework demonstrates natural and expressive speech generation, achieving competitive zero-shot performance on unseen dialects and specialized domains like Peking Opera using only a few hours of data.

## Method Summary
The DiaMoE-TTS framework follows a four-stage training procedure: (0) initialization with F5-TTS pretrained checkpoint, (1-2) joint multidialect IPA training with MoE added in stage 2, and (3) PEFT adaptation. The method standardizes phonetic representations using IPA to handle inconsistent orthographies across Chinese dialects, then employs a Mixture-of-Experts module on text embeddings to model dialect-specific phonological variations. For new dialects, the framework uses parameter-efficient fine-tuning with LoRA and Conditioning Adapters while keeping the backbone frozen, enabling adaptation with extremely limited data (approximately 3 hours).

## Key Results
- Achieves competitive zero-shot performance on unseen dialects
- Demonstrates natural and expressive speech generation for specialized domains like Peking Opera
- Successfully adapts to new dialects using only ~3 hours of training data

## Why This Works (Mechanism)
The framework works by creating a unified IPA space that standardizes phonetic representations across dialects, then uses Mixture-of-Experts to learn dialect-specific variations in the text embedding space. The parameter-efficient adaptation allows the model to learn new dialect characteristics without catastrophic forgetting of previously learned dialects.

## Foundational Learning
1. **IPA Phonetic Transcription**: International Phonetic Alphabet provides a standardized representation of speech sounds - needed for handling inconsistent orthographies across dialects; quick check: verify IPA coverage for target dialect phonemes
2. **Mixture-of-Experts (MoE)**: Routing mechanism that activates specialized sub-networks based on input - needed to model dialect-specific phonological patterns; quick check: monitor expert activation entropy during training
3. **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like LoRA that add small trainable parameters while freezing the base model - needed for low-resource adaptation; quick check: verify backbone weights remain frozen during adaptation
4. **Conditional Adapter Layers**: Small neural networks inserted into pretrained models for efficient adaptation - needed for rapid dialect transfer; quick check: ensure adapter dimensions match base model requirements
5. **Dialect Classification Loss**: Auxiliary loss function that guides MoE specialization - needed to encourage distinct expert representation for different dialects; quick check: monitor classification accuracy on dialect labels

## Architecture Onboarding

**Component Map**: Text Input -> IPA Normalization -> Text Embedding -> MoE Layer -> Main Model -> Speech Output

**Critical Path**: The core inference flow passes through IPA normalization, text embeddings, the MoE layer for dialect routing, and then the main F5-TTS backbone to generate speech.

**Design Tradeoffs**: The framework balances between joint training for shared representation learning and parameter-efficient adaptation for low-resource dialects. Using MoE allows the model to maintain distinct dialect characteristics without training separate models, while the frozen backbone with PEFT prevents catastrophic forgetting.

**Failure Signatures**: Style averaging across dialects indicates MoE gating is not learning distinct patterns; poor adaptation quality suggests insufficient data or improper PEFT configuration; speaker identity collapse indicates backbone freezing issues.

**First Experiments**:
1. Train MoE module with varying numbers of experts (K=2, 4, 8) and evaluate dialect classification accuracy
2. Test adaptation performance with different LoRA ranks (r=8, 16, 32) on low-resource dialects
3. Compare frozen vs. unfrozen backbone performance during Stage 3 adaptation

## Open Questions the Paper Calls Out

**Open Question 1**: How effectively does the DiaMoE-TTS framework generalize to non-Sinitic language families with phonological systems distinct from the currently supported Chinese dialects?
- Basis in paper: The conclusion states future work will expand with broader dialect coverage and advance open-data-driven multilingual speech synthesis
- Why unresolved: Current experiments and IPA space are optimized for Chinese dialects without demonstrating cross-family transfer
- What evidence would resolve it: Zero-shot or few-shot adaptation results on non-Sino-Tibetan languages

**Open Question 2**: Does the dialect classification loss used to train the Mixture-of-Experts (MoE) module impose rigid boundaries that hinder synthesis of continuous dialect spectra or code-switched speech?
- Basis in paper: MoE routing is guided by dialect classification loss encouraging distinct expert specialization
- Why unresolved: Paper evaluates distinct dialect categories but not intermediate accents or mixed dialect speech
- What evidence would resolve it: Evaluation of speaker similarity and naturalness for blended or intermediate dialects

**Open Question 3**: What is the absolute lower bound of data required for parameter-efficient adaptation (Stage 3) to succeed without collapsing speaker identity?
- Basis in paper: Method demonstrates success with approximately 3 hours of data but doesn't define failure points
- Why unresolved: Specific trade-off between adaptation quality and data volume below 3-hour threshold remains unexplored
- What evidence would resolve it: Systematic ablation study measuring performance from 5 minutes to 3 hours

## Limitations
- Core architectural details like MoE expert count and Conditioning Adapter implementation are underspecified
- Limited evaluation data availability, particularly for commercial datasets used in experiments
- Absence of detailed ablation studies on MoE and adapter components limits generalizability assessment

## Confidence

**High**: Problem formulation and overall training pipeline are clearly described and reproducible
**Medium**: Experimental results demonstrate improvements, but lack of architectural details limits full verification
**Medium**: Zero-shot adaptation claims are supported but would benefit from more extensive evaluation

## Next Checks

1. Implement MoE module with different numbers of experts (K=2, 4, 8) and evaluate impact on dialect classification accuracy and MOS scores
2. Conduct ablation studies comparing full DiaMoE-TTS against variants without MoE, with different adapter types, and with frozen vs. unfrozen backbones during Stage 3
3. Test zero-shot capability by training on subset of dialects and evaluating on truly unseen dialects, measuring MOS and WER degradation