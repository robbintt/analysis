---
ver: rpa2
title: Effectiveness of Prompt Optimization in NL2SQL Systems
arxiv_id: '2505.20591'
source_url: https://arxiv.org/abs/2505.20591
tags:
- prompt
- optimization
- nl2sql
- query
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a prompt optimization framework for NL2SQL
  systems, arguing that production environments require high-precision, high-performance
  SQL generation rather than just accurate SQL queries. The key idea is to identify
  a static set of domain-specific exemplars through iterative optimization rather
  than relying on retrieval-based methods.
---

# Effectiveness of Prompt Optimization in NL2SQL Systems

## Quick Facts
- arXiv ID: 2505.20591
- Source URL: https://arxiv.org/abs/2505.20591
- Reference count: 40
- Primary result: Iterative two-agent prompt optimization improves NL2SQL accuracy while reducing prompt size and execution latency

## Executive Summary
This paper introduces an iterative prompt optimization (IPO) framework for NL2SQL systems that addresses production requirements for high-precision and high-performance SQL generation. The key innovation is using two LLM agents—a Proposer that generates instructions and exemplars, and a SQL Generator that evaluates their quality—to iteratively refine prompts. The approach implicitly prunes schema information to reduce prompt size and extends to multi-objective optimization incorporating SQL execution latency. Results show IPO achieves higher accuracy than baseline methods while generating more concise prompts, and the multi-objective variant improves both accuracy and efficiency.

## Method Summary
The framework uses iterative refinement between two LLM agents to optimize NL2SQL prompts. The Proposer generates candidate prompts with instructions and exemplars, while the SQL Generator evaluates them on a validation set and returns accuracy plus error examples. This feedback updates the Proposer prompt over multiple iterations. The method also prunes schema information to reduce prompt size and extends to multi-objective optimization by incorporating SQL execution latency using a new benchmark with SQL variants and timing data.

## Key Results
- IPO achieves higher accuracy than baseline methods (RES, ORES, MIPROv2) on BIRD dataset
- IPO-generated prompts are significantly more concise (6,495 tokens vs. 23,749 for RES)
- Multi-objective optimization reduces maximum latency from 18.5s to 10.2s while maintaining accuracy
- Iterative refinement between two agents produces more informative exemplars than random or single-pass selection

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement between two agents produces more informative exemplars than random or single-pass selection methods. The Proposer generates candidate prompts; the SQL Generator evaluates them and returns accuracy plus correct/incorrect examples. This feedback updates the Proposer prompt, guiding subsequent iterations toward exemplars that address prior failures. Core assumption: The LLM can diagnose which exemplars caused errors and propose better ones.

### Mechanism 2
Implicit schema pruning reduces prompt length without degrading accuracy, improving inference efficiency. During exemplar generation, the Proposer includes only schema elements directly relevant to each example, rather than full database schemas. This reduces token count while preserving task-relevant context. Core assumption: Pruned schemas retain sufficient information for the SQL Generator to generalize to new queries.

### Mechanism 3
Multi-objective optimization can jointly improve accuracy and execution latency by exposing the LLM to SQL variants with timing data. The framework extends IPO to include latency as an optimization signal using a benchmark with multiple SQL variants per query and measured execution times. Prompts can then demonstrate efficient vs. inefficient patterns. Core assumption: LLMs can learn efficiency patterns from exemplars showing latency comparisons.

## Foundational Learning

- **In-Context Learning (ICL) for NL2SQL**: Understanding how LLMs use exemplars in prompts to perform tasks without weight updates is essential for grasping the entire framework.
  - Quick check: Can you explain why adding relevant exemplars to a prompt improves SQL generation, and what "lost-in-the-middle" refers to?

- **Bayesian Optimization for Hyperparameter Tuning**: ORES uses this to select the number of exemplars (k); understanding why grid search is impractical here is essential.
  - Quick check: Why would Bayesian optimization be preferred over random search when each trial requires expensive LLM calls?

- **Multi-Objective Optimization Trade-offs**: The paper introduces latency as a second objective; practitioners must understand Pareto fronts and how to weight competing goals.
  - Quick check: If accuracy and latency conflict, how would you decide the acceptable trade-off for a production system?

## Architecture Onboarding

- **Component map**: Schema Retrieval -> Proposer Agent -> SQL Generator Agent -> Feedback Loop -> Optimized Prompt
- **Critical path**: Training data → Validation split → Proposer generates initial prompt → SQL Generator evaluates → Feedback loop (5 iterations) → Final optimized prompt → Deploy as static prompt for inference
- **Design tradeoffs**: IPO iterations vs. optimization time (more iterations improve prompts but cost more); Prompt size vs. coverage (fewer tokens reduce cost but may omit edge-case patterns); Accuracy vs. latency (multi-objective may sacrifice ~0.25% accuracy for 45% latency reduction)
- **Failure signatures**: Accuracy plateaus early (likely validation set too small or unrepresentative); Prompt length grows unbounded (Proposer not pruning schema); Latency optimization fails (benchmark timing data may not match production DB characteristics)
- **First 3 experiments**: 1) Replicate RES vs. ORES vs. MIPROv2 vs. IPO on BIRD dev split to validate accuracy claims; 2) Ablate schema pruning: Compare IPO-generated prompts with forced full-schema inclusion; 3) Multi-objective pilot: On BIRD-MULTI subset, run IPO with accuracy-only vs. accuracy+latency objectives

## Open Questions the Paper Calls Out

The paper explicitly identifies the need to explore how to balance accuracy and latency objectives in the multi-objective optimization framework, as different production scenarios may prioritize strict accuracy or fast query execution differently.

## Limitations

- The iterative optimization mechanism relies on the Proposer's ability to meaningfully interpret error feedback from the SQL Generator, a capability assumed but not independently verified.
- Schema pruning effectiveness across diverse database complexities remains untested.
- The multi-objective latency optimization assumes execution-time patterns generalize across different database engines and workloads.

## Confidence

- **High confidence**: IPO's accuracy improvements over baseline methods (Table 1 results are robust)
- **Medium confidence**: Schema pruning benefits (validated on BIRD but not diverse schemas)
- **Medium confidence**: Multi-objective latency optimization (preliminary results but limited generalizability)

## Next Checks

1. **Generalization stress test**: Apply IPO to schemas with >10 tables and complex relationships to test pruning limits and exemplar relevance.
2. **Ablation of feedback quality**: Compare IPO variants where Proposer receives only accuracy vs. full error examples to quantify feedback contribution.
3. **Production latency validation**: Run IPO-optimized prompts against production database to measure actual latency improvements vs. benchmark-reported gains.