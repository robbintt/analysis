---
ver: rpa2
title: 'MODEST: Multi-Optics Depth-of-Field Stereo Dataset'
arxiv_id: '2511.20853'
source_url: https://arxiv.org/abs/2511.20853
tags:
- uni00000003
- uni00000011
- depth
- uni00000013
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MODEST, a novel high-resolution (5472\xD7\
  3648px) stereo DSLR dataset with 18000 images, systematically varying focal length\
  \ (28-70mm) and aperture (f/2.8-f/22) across 9 real scenes. The dataset includes\
  \ challenging visual elements like optical illusions, reflective surfaces, and transparent\
  \ glass walls."
---

# MODEST: Multi-Optics Depth-of-Field Stereo Dataset

## Quick Facts
- arXiv ID: 2511.20853
- Source URL: https://arxiv.org/abs/2511.20853
- Reference count: 40
- Primary result: Introduces MODEST, a 18,000-image high-resolution stereo dataset systematically varying focal length and aperture to reveal optical generalization gaps in depth, DoF, and deblurring models

## Executive Summary
MODEST is a novel stereo dataset that systematically varies focal length (28-70mm) and aperture (f/2.8-f/22) across 9 real scenes with challenging visual elements like optical illusions, reflective surfaces, and transparent glass walls. The dataset reveals that state-of-the-art monocular and stereo depth models struggle with optical illusions despite having stereo parallax, while depth-of-field and deblurring methods show significant performance degradation on real optics compared to synthetic data. The dataset enables controlled analysis of geometric and optical effects for monocular/stereo depth estimation, depth-of-field rendering, deblurring, and 3D reconstruction.

## Method Summary
MODEST captures each scene at 10 focal lengths and 5 apertures, creating 50 optical configurations per scene. The dataset includes calibration images per focal length, stereo pairs with optical illusions and challenging surfaces, and provides evaluation metrics for depth estimation (gradient consistency, planarity, IQR uncertainty, ICP error) and DoF/deblurring (PSNR, SSIM, LPIPS). Models are evaluated zero-shot using f/2.8 images as input and f/22 images as reference for DoF/deblurring tasks. The dataset reveals systematic performance biases and optical generalization gaps that single-configuration datasets cannot expose.

## Key Results
- State-of-the-art monocular and stereo depth models struggle with optical illusions, showing structured depth errors despite visually plausible outputs
- Depth-of-field and deblurring methods show ~20% PSNR degradation on real optics compared to synthetic data benchmarks
- Depth estimation models exhibit performance bias around 60-65mm focal length, with optimal performance when test focal lengths match training data field-of-view distributions

## Why This Works (Mechanism)

### Mechanism 1
Systematic focal length and aperture variation exposes optical generalization gaps in depth and DoF models trained on limited-optics datasets. The dataset creates 50 optical configurations per scene that force models to handle varying depth-of-field, circle-of-confusion sizes, and field-of-view changes. Models that perform well on benchmarks with fixed optics have likely learned scene priors rather than optically-grounded physical relationships.

### Mechanism 2
Monocular and stereo depth models rely on semantic priors that are systematically deceived by optical illusions, even when stereo parallax is available. Optical illusions introduce misleading visual cues (texture, shading, printed backdrops) that models weight more heavily than geometric evidence. Stereo models show visually plausible depth but quantitative error maps reveal "structured depth errors largely invisible to human inspection" spatially correlated with illusions.

### Mechanism 3
Focal length creates a performance bias in depth models, with optimal performance at moderate focal lengths (60-65mm) where training data field-of-view matches test conditions. The ablation study shows inflection points around 60-65mm for constant aperture f/2.8, suggesting models trained on wide-angle datasets perform better when test focal lengths match training field-of-view distributions.

## Foundational Learning

- Concept: Stereo calibration (intrinsics/extrinsics from checkerboard patterns)
  - Why needed here: Each focal length has dedicated calibration images; using incorrect calibration propagates to depth errors
  - Quick check question: Can you explain why focal length changes require separate calibration sets?

- Concept: Circle of Confusion (CoC) and depth-of-field relationships
  - Why needed here: DoF rendering evaluation requires understanding how aperture, focal length, and focus distance jointly determine blur magnitude
  - Quick check question: Given aperture f/2.8 vs f/22 at 50mm focal length, which produces larger CoC for out-of-focus regions?

- Concept: Photometric consistency assumptions in classical MVS
  - Why needed here: The paper shows COLMAP/OpenMVS fail on glossy/transparent surfaces; understanding why helps diagnose when to use learning-based alternatives
  - Quick check question: Why does normalized cross-correlation fail on specular highlights?

## Architecture Onboarding

- Component map: Dataset -> Calibration images (per focal length) -> Inference images (5 apertures per focal length) -> Stereo pairs (left/right) -> Evaluation metrics (gradient, planarity, IQR, ICP)
- Critical path: Load stereo pair -> Apply calibration -> Run depth/DoF/deblur model -> Compute metrics against reference (f/22 all-in-focus for DoF/deblur; consensus depth for depth evaluation)
- Design tradeoffs: High resolution (5472Ã—3648) provides fine detail but increases memory (COLMAP needs >50GB RAM); consider downsampling for initial experiments. Classical MVS at f/9 balances depth-of-field vs. texture for matching - wider apertures blur regions, narrower apertures introduce diffraction.
- Failure signatures:
  - Depth models: Localized errors at optical illusion regions (printed backdrops, mirrors)
  - DoF models: Incorrect focus plane prediction (foreground/background reversal, over-blurring)
  - Deblurring models: Struggle with point-light bokeh (LED regions in Restormer)
  - Classical MVS: Floating points, lost thin structures, warped planar regions on low-texture/glossy surfaces

- First 3 experiments:
  1. Run Foundation Stereo on Scene 1 at 50mm f/2.8; compute all four error metrics; visualize correlation with optical illusion regions.
  2. Evaluate BokehMe on focal length sweep (28mm, 50mm, 70mm) at f/2.8 input; plot PSNR/SSIM vs. focal length to replicate Table 2 trends.
  3. Run COLMAP and MapAnything on Scene 1 at 28mm f/9; compare point cloud density and noise near transparent/reflective surfaces.

## Open Questions the Paper Calls Out

### Open Question 1
How can depth estimation models be trained to reduce reliance on semantic priors when geometric evidence conflicts with learned expectations, particularly for optical illusions? Current SOTA models produce inconsistent depth maps on illusory scenes, revealing learned semantic priors dominate over explicit geometric reasoning even when stereo parallax is available.

### Open Question 2
What architectural or training paradigm changes would enable shallow depth-of-field models to learn physically grounded blur formation rather than dataset-specific blur style correlations? DoF rendering models show ~20% PSNR degradation on MODEST, frequently misplace focal regions, blur incorrect image portions, and cannot predict focus planes from depth and lens parameters.

### Open Question 3
Why do depth estimation models exhibit a performance bias around 60-65mm focal length, and how should training datasets be designed to eliminate this bias? Ablation studies reveal clear inflection points at 60-65mm across four different error metrics, suggesting systematic training data field-of-view bias that remains uncharacterized.

### Open Question 4
Can deblurring architectures be designed to explicitly model aperture-driven optical blur rather than treating it as generic degradation? Current transformer and state-space deblurring models trained on motion blur datasets fail to generalize to aperture-produced optical blur with its depth-dependent characteristics.

## Limitations
- Real-world complexity introduces variability that limits direct comparison with synthetic benchmarks
- Performance degradation on real optics versus synthetic data lacks isolation of contributing factors (optical aberrations, sensor noise, scene complexity)
- Focal length performance bias around 60-65mm lacks corpus support for whether it reflects training dataset distributions or fundamental optical properties

## Confidence
- Mechanism 1 (optical generalization gaps): Medium - Strong experimental evidence but limited understanding of contributing factors
- Mechanism 2 (illusion robustness): Medium - Clear error patterns but unclear if architectural or data-driven
- Mechanism 3 (focal length bias): Low - Single dataset observation without supporting literature

## Next Checks
1. Train models on MODEST focal length subsets (28-50mm vs 50-70mm) and test cross-generalization to measure focal length distribution effects
2. Apply domain adaptation techniques to illusion-containing regions to isolate semantic versus optical failure modes
3. Evaluate MODEST-trained models on alternative stereo datasets with different focal length distributions to test optical generalization beyond the dataset