---
ver: rpa2
title: Imputation-free Learning of Tabular Data with Missing Values using Incremental
  Feature Partitions in Transformer
arxiv_id: '2504.14610'
source_url: https://arxiv.org/abs/2504.14610
tags:
- data
- missing
- values
- learning
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an imputation-free incremental attention learning
  (IFIAL) method for tabular data with missing values. It uses two attention masks
  retrofitted to a transformer to directly streamline tabular data without imputing
  or initializing missing values.
---

# Imputation-free Learning of Tabular Data with Missing Values using Incremental Feature Partitions in Transformer

## Quick Facts
- **arXiv ID:** 2504.14610
- **Source URL:** https://arxiv.org/abs/2504.14610
- **Reference count:** 40
- **Primary result:** IFIAL achieves superior average classification performance rank compared to 11 state-of-the-art learning methods across 17 diverse tabular datasets without requiring missing value imputation.

## Executive Summary
This paper introduces an imputation-free incremental attention learning (IFIAL) method for tabular data with missing values. The approach uses two attention masks retrofitted to a transformer to directly process missing data without imputation or initialization. It incrementally learns partitions of overlapping, fixed-size feature sets to enhance the transformer's performance. Across 17 diverse tabular datasets, IFIAL achieves superior average classification performance rank compared to 11 state-of-the-art learning methods with or without missing value imputations. The method demonstrates robustness to varying types and proportions of missing data.

## Method Summary
The method trains a Feature-Tokenized Transformer (FTT) incrementally on overlapping fixed-size feature partitions sorted by ascending missing rate. Two attention masks are retrofitted: M1 sets attention columns for missing features to -∞ (softmax → 0), and M2 zeroes attention rows for missing features via 0/1 masking. Features are partitioned with size k = d/2 and 50% overlap. The FTT uses 2 encoder layers, 8 heads, 128-dim embedding, and is trained sequentially across partitions with Adam optimizer (lr=1e-5, batch=128, up to 300 epochs with early stopping).

## Key Results
- IFIAL achieves superior average classification performance rank compared to 11 state-of-the-art methods across 17 datasets
- The method demonstrates robustness to varying types and proportions of missing data (10-50% MCAR/MNAR)
- A feature partition size equal to half the original feature space yields the best trade-off between computational efficiency and predictive performance

## Why This Works (Mechanism)

### Mechanism 1
Transformers can process tabular data containing missing values directly, without synthetic imputation, by selectively disabling attention weights for missing feature pairs. The method constructs two binary attention masks retrofitted into the self-attention layer. First, a column mask ($M_1$) assigns $-\infty$ to query-key pairs involving a missing feature; the softmax operation transforms this to zero attention weight. Second, a row mask ($M_2$) zeroes out the output embedding contributions from any feature that is missing for the current sample. This ensures the model attends only to observed values ($X^{obs}$).

### Mechanism 2
Incremental learning using fixed-size, overlapping feature partitions mitigates computational complexity and stabilizes learning in high-dimensional, sparse feature spaces. Features are sorted by missing rate (lowest to highest) and partitioned into fixed-size sets of size $k$ with a 50% overlap ($ceil(k/2)$). The transformer is trained sequentially: it trains on Partition 1, updates weights on Partition 2, and continues until all partitions are processed. This reduces the self-attention complexity from $O(d^2)$ to $O(k^2)$ per step.

### Mechanism 3
Sorting features by missing rate before partitioning acts as a curriculum learning strategy, grounding the model in dense data before introducing sparse data. The algorithm calculates the missing rate for every feature $f_j$ and sorts them in ascending order. The first partitions presented to the model contain the most complete data, allowing the transformer to stabilize its weights on reliable statistics before encountering partitions with high missingness (up to 50%).

## Foundational Learning

- **Concept:** Self-Attention Mechanism (Scaled Dot-Product)
  - **Why needed here:** The core innovation (Attention Masks) modifies the standard $Softmax(QK^T / \sqrt{d})$ operation. You must understand how logits are converted to weights to see why $-\infty$ (in $M_1$) results in zero attention and how $M_2$ suppresses output vectors.
  - **Quick check question:** If a key vector represents a missing value, what value must the attention logit approach to ensure the model effectively "ignores" this key during the Softmax step?

- **Concept:** Missing Data Mechanisms (MCAR vs. MNAR)
  - **Why needed here:** The paper explicitly benchmarks performance against these mechanisms. The "Imputation-free" claim is strengthened by showing robustness to MNAR, which typically defeats imputation models that assume relationships exist between observed and missing data.
  - **Quick check question:** In a dataset where "Wealth" is missing specifically for the wealthiest individuals (MNAR), would a mean-imputation strategy likely under- or over-estimate the true mean, and how does IFIAL avoid this specific error?

- **Concept:** Incremental/Continual Learning
  - **Why needed here:** IFIAL is not trained in a single batch. It updates model weights $P$ times. Understanding how neural networks update weights without forgetting previous data (catastrophic forgetting) is key to understanding why the "overlap" strategy is included.
  - **Quick check question:** Why is a 50% overlap between partitions theoretically better than non-overlapping partitions when training a model sequentially?

## Architecture Onboarding

- **Component map:** Raw Tabular Data + Missing Indicator Vector -> Sort features by Missing Rate -> Create overlapping partitions -> Feature-Tokenized Transformer (FTT) -> Masking Module (generates $M_1$ and $M_2$) -> Sequential Training Loop -> Classification Head
- **Critical path:** The **Masking Module** is the highest-risk component. Implementing $M_1$ involves modifying the logits *before* Softmax (adding $-\infty$), while $M_2$ involves element-wise multiplication of the attention matrix *after* Softmax (multiplying by 0). Confusing these steps will result in gradient flow issues or information leakage.
- **Design tradeoffs:**
  - **Partition Size ($k$):** The paper suggests $k = d/2$ (half the feature space).
    - *Small $k$:* Faster, lower memory ($O(k^2)$), but risks missing long-range feature interactions.
    - *Large $k$:* Captures more interactions, but approaches the $O(d^2)$ computational bottleneck and reduces the "incremental" benefit.
  - **Imputation vs. Speed:** While IFIAL saves time on imputation, the sequential training on partitions can be slower than single-pass methods like XGBoost.
- **Failure signatures:**
  - **Performance Collapse at Low Missingness:** If the data is 99% complete, standard FTT or XGBoost should win. If IFIAL fails here, check if the incremental sorting/partitioning is breaking natural feature correlations.
  - **Gradient Vanishing:** If $M_2$ is implemented incorrectly (masking the input instead of attention weights), the model may fail to backpropagate errors for features in later partitions.
  - **Memory Overflow:** If $d$ is extremely large and $k=d/2$ is used, the $O(k^2)$ attention matrix may still OOM.
- **First 3 experiments:**
  1.  **Sanity Check (MCAR):** Run IFIAL on the "Breast-cancer" or "Diabetes" datasets with 10% MCAR missingness. Compare AUC against a baseline Mean-Imputation + FTT. IFIAL should be competitive or superior.
  2.  **Stress Test (Missing Rate):** Fix one dataset (e.g., Credit-g) and increment missing rates from 10% to 50%. Plot the AUC decay curve. IFIAL's decay should be gentler than MICE or GAIN baselines.
  3.  **Ablation (Partition Size):** On a medium-sized dataset (e.g., Kc2, $d=21$), run IFIAL with $k=2$, $k=d/2$ (10), and $k=d$ (no partitioning). Verify that $k=d/2$ provides the reported "best trade-off" between accuracy and speed.

## Open Questions the Paper Calls Out

### Open Question 1
Does IFIAL consistently underperform relative to sophisticated imputation methods when the missing value rate is very low (<10%) and the missingness mechanism is Missing Completely At Random (MCAR)? The authors identify this as a specific limitation, acknowledging that their imputation-free approach may not be superior in all data quality scenarios.

### Open Question 2
Does the performance advantage of IFIAL over tree-based methods diminish statistically as dataset sample sizes scale significantly (e.g., n > 100,000)? The paper notes that the "performance gain of the proposed method over other methods diminishes when the sample size is very large, particularly at a low missing rate," but does not verify the point at which the performance gap closes.

### Open Question 3
Can the incremental feature partition strategy and attention masking be adapted to other deep learning architectures, such as ResNets or MLP-Mixers? The current implementation is specifically retrofitted to a feature-tokenized transformer, leaving the generalizability of the mechanism to other architectures unexplored.

## Limitations

- The incremental partitioning strategy's efficacy relies heavily on the assumption that feature dependencies are locally dense enough to be captured within small partitions, which lacks direct evidence.
- The robustness to MNAR missingness is claimed but not deeply validated; the specific MNAR simulation procedure is referenced but not detailed.
- The overlap strategy (50%) is presented as optimal but lacks ablation studies across a range of overlap percentages to justify this specific choice.

## Confidence

- **High Confidence:** The core mechanism of using attention masks (M1 and M2) to avoid imputation is well-defined and theoretically sound with clear equations and implementation details.
- **Medium Confidence:** The incremental learning with fixed-size, overlapping partitions is a reasonable approach to mitigate computational complexity, but the specific choice of k=d/2 and 50% overlap is presented without exhaustive justification through ablation studies.
- **Low Confidence:** The curriculum learning effect of sorting features by missing rate is posited but not empirically validated; there is no direct evidence that starting with low-missing-rate features genuinely stabilizes the model for later, sparser partitions.

## Next Checks

1. **Feature Dependency Analysis:** Analyze the correlation structure within the partitions to verify that strong feature interactions are not split across non-overlapping partitions, validating the local density assumption.
2. **Overlap Ablation Study:** Run IFIAL with varying overlap percentages (25%, 50%, 75%) on a representative dataset to determine if 50% is indeed optimal or if another value provides a better trade-off.
3. **MNAR Pattern Diversity:** Implement and test IFIAL on a wider variety of MNAR patterns beyond the single referenced procedure to robustly validate the "imputation-free" claim against diverse MNAR challenges.