---
ver: rpa2
title: 'MASS: Overcoming Language Bias in Image-Text Matching'
arxiv_id: '2501.11469'
source_url: https://arxiv.org/abs/2501.11469
tags:
- bias
- mass
- language
- image
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MASS, a framework to reduce language bias
  in image-text matching by leveraging pointwise mutual information between image
  and text tokens. MASS is an inference-time method that can be applied to any visual-language
  model that outputs image-conditional text likelihood, without requiring additional
  training.
---

# MASS: Overcoming Language Bias in Image-Text Matching

## Quick Facts
- arXiv ID: 2501.11469
- Source URL: https://arxiv.org/abs/2501.11469
- Reference count: 25
- Primary result: MASS reduces language bias in image-text matching by computing token-level PMI, achieving best gender bias reduction and maintaining compositionality on Winoground/SVO-Probes.

## Executive Summary
MASS is an inference-time framework that reduces language bias in image-text matching by leveraging pointwise mutual information (PMI) between image and text tokens. It works by computing the difference between image-conditional token likelihood and marginal text likelihood, isolating the visual-textual association component. MASS can be applied to any autoregressive visual-language model without additional training, achieving significant bias reduction while maintaining or improving performance on compositionality benchmarks.

## Method Summary
MASS computes a similarity score between an image and text caption by estimating pointwise mutual information (PMI) at the token level. For each token, it calculates the ratio of conditional likelihood (given both image and previous tokens) to marginal likelihood (approximated using a null black-filled image). The final score is the average of these per-token PMI values. This approach isolates the image-text association from language priors, reducing bias from common linguistic patterns while preserving visual grounding.

## Key Results
- MASS significantly reduces language bias compared to CLIP and raw token likelihood on color, number, and gender bias benchmarks
- Maintains or improves performance on linguistic compositionality tests like Winoground and SVO-Probes
- Achieves best gender bias reduction in image retrieval tasks, outperforming previous debiasing methods
- Shows strong improvements in understanding linguistic structures in compositionality benchmarks

## Why This Works (Mechanism)

### Mechanism 1: PMI-Based Bias Reduction
Subtracting marginal text likelihood from image-conditional likelihood isolates the visual-textual association component, reducing reliance on language priors. The log-likelihood decomposition separates "linguistic plausibility" from "association likelihood," with PMI per token measuring how much more likely a token co-occurs with the image than by chance.

### Mechanism 2: Null Image Approximation
A black-filled "null image" provides a computationally efficient proxy for marginal text likelihood, avoiding expensive Monte Carlo sampling. This yields 2× speedup vs. Monte Carlo with N≥2 samples while maintaining accuracy.

### Mechanism 3: Token-Level Aggregation
Token-level PMI aggregation preserves fine-grained visual grounding signals that sequence-level similarity functions conflate with bag-of-words statistics. Averaging per-token PMI scores gives higher weight to visually grounded tokens and discounts high-frequency but non-visual tokens.

## Foundational Learning

- **Pointwise Mutual Information (PMI)**: Measures how much more likely x and c co-occur than by chance (PMI(x;c) = log p(x|c)/p(x)). Why needed: MASS is fundamentally a PMI-based method; understanding PMI is essential for grasping the core mechanism.

- **Autoregressive Language Modeling**: Requires token-level likelihoods p(xt|x<t, c) from autoregressive captioning models. Why needed: MASS requires this factorization to compute per-token PMI scores.

- **Contrastive vs. Generative VL Objectives**: MASS applies to Token-Level (TL) captioning objectives, not CLIP-style Image-Text Contrastive (ITC) learning. Why needed: Understanding the output differences between these model types is crucial for applying MASS.

## Architecture Onboarding

- **Component map**: Backbone VLM -> Null Image Generator -> PMI Computer -> Similarity Score Output
- **Critical path**: 1) Preprocess image and tokenize text, 2) Forward pass with real image, 3) Forward pass with null image, 4) Compute per-token PMI, 5) Aggregate to final score
- **Design tradeoffs**: Null image vs. Monte Carlo (2× faster but less rigorous), token averaging vs. weighted aggregation (simple but may dilute signal), backbone choice affects performance and memory requirements
- **Failure signatures**: High variance across similar captions, negative PMI for correct matches, no improvement over TL baseline
- **First 3 experiments**: 1) Validate null-image approximation against Monte Carlo on gender bias benchmark, 2) Test MASS on Natural Colors Dataset with grayscale images, 3) Reproduce gender bias tradeoff curve on MS-COCO retrieval

## Open Questions the Paper Calls Out

### Open Question 1
Is the "null image" a robust universal approximation for marginal text likelihood across different visual domains and model architectures? The paper provides an empirical heuristic without proving universal optimality or error-free performance.

### Open Question 2
Why does MASS degrade performance relative to ITM baselines specifically on object-modification tasks within the SVO-Probes benchmark? The paper identifies the performance gap but doesn't investigate whether this is due to the aggregation method or marginal likelihood estimation.

### Open Question 3
Can an inference-time method relying solely on text-prior subtraction effectively mitigate biases that are visual or multimodal in origin? MASS explicitly subtracts text-only likelihood but doesn't account for biases embedded within the visual encoder or the intersection of visual and textual stereotypes.

## Limitations

- Null image approximation quality is empirically heuristic with no theoretical guarantees
- Performance tied to quality of underlying VLM's likelihood estimates
- Simple token averaging may dilute signal for captions with many abstract tokens
- Effectiveness on open-ended retrieval tasks and intersectional biases not evaluated

## Confidence

**High Confidence**:
- MASS significantly reduces language bias compared to baselines
- Maintains or improves performance on compositionality tests
- Achieves best gender bias reduction in image retrieval tasks

**Medium Confidence**:
- Null image approximation is a good alternative to Monte Carlo sampling
- Token-level PMI aggregation preserves visual grounding signals
- MASS generalizes across different autoregressive VLMs

**Low Confidence**:
- Null image approximation is sufficient for all text tokens and image-text pairs
- MASS will perform equally well on open-ended retrieval tasks
- Alternative aggregation methods would not improve MASS performance

## Next Checks

1. **Validate Null Image Approximation**: Compare MASS with null image vs. MASS with Monte Carlo sampling (N=8) on gender bias benchmark; report correlation of scores and absolute difference in bias reduction.

2. **Test Null Image on Grayscale Images**: Run MASS on Natural Colors Dataset using grayscale images; verify MASS prefers "The fruit is gray" over natural colors for >90% of samples.

3. **Evaluate on Open-Ended Retrieval**: Apply MASS to MS-COCO/Flicker30k; measure both retrieval performance (Recall@K) and bias metrics; compare retrieval-bias tradeoff curve against baselines.