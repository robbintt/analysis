---
ver: rpa2
title: 'scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell
  Omics'
arxiv_id: '2506.01883'
source_url: https://arxiv.org/abs/2506.01883
tags:
- data
- fetch
- sampling
- scdataset
- cells
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: scDataset addresses the data loading bottleneck for training deep
  learning models on single-cell datasets containing hundreds of millions of cells.
  The core innovation combines block sampling and batched fetching to achieve quasi-random
  sampling that balances I/O efficiency with minibatch diversity.
---

# scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics

## Quick Facts
- arXiv ID: 2506.01883
- Source URL: https://arxiv.org/abs/2506.01883
- Reference count: 35
- Primary result: Achieves 100× speedup over true random sampling on 100M-cell datasets while matching model performance

## Executive Summary
scDataset addresses the fundamental data loading bottleneck for training deep learning models on single-cell omics datasets containing hundreds of millions of cells. The method combines block sampling and batched fetching to achieve quasi-random sampling that balances I/O efficiency with minibatch diversity. On the Tahoe-100M dataset, scDataset delivers over two orders of magnitude speedup compared to true random sampling while maintaining model performance across multiple classification tasks.

## Method Summary
scDataset implements a block sampling strategy where the dataset is partitioned into contiguous blocks (block_size b) that are sampled randomly. Batched fetching (fetch_factor f) then loads multiple cells from each block in a single I/O operation. This quasi-random sampling approach maintains sufficient minibatch diversity while dramatically reducing the number of random I/O operations. The method integrates directly with PyTorch's DataLoader and works natively with AnnData files, requiring no preprocessing or data duplication.

## Key Results
- Achieves over 100× reduction in end-to-end training time compared to true random sampling
- Maintains matching macro F1-scores across four classification tasks (cell line, drug, MoA broad/fine)
- Demonstrates two orders of magnitude improvement in throughput (samples/sec) on 100M-cell datasets
- Provides theoretical bounds on minibatch diversity as a function of block size and fetch factor

## Why This Works (Mechanism)
The core insight is that single-cell datasets exhibit natural clustering in storage (cells from the same plate often stored contiguously). True random sampling requires expensive random I/O operations, while block sampling exploits this locality to reduce I/O costs. The batched fetching mechanism ensures each minibatch contains diverse samples by drawing from multiple blocks. This creates a controlled trade-off: larger block sizes improve I/O efficiency but reduce randomness, while larger fetch factors improve minibatch diversity at the cost of more I/O operations.

## Foundational Learning
**Block Sampling**: Partitioning dataset into contiguous blocks for efficient I/O
- Why needed: Reduces random I/O operations by exploiting storage locality
- Quick check: Verify block_size ≥ 16 for meaningful speedup

**Batched Fetching**: Loading multiple samples from each block in single I/O operation
- Why needed: Maintains minibatch diversity while preserving I/O efficiency
- Quick check: Ensure fetch_factor ≥ 64 for adequate diversity

**Quasi-Random Sampling**: Controlled randomness through block-level sampling
- Why needed: Balances I/O efficiency with statistical requirements for training
- Quick check: Monitor batch entropy; should exceed 3.0 for good diversity

## Architecture Onboarding
**Component Map**: DataLoader -> BlockShufflingStrategy -> AnnDataBackend -> Storage
**Critical Path**: Random block selection → Batch assembly from blocks → I/O fetch → Data collation → Model input
**Design Tradeoffs**: I/O efficiency vs. sampling randomness (controlled by b and f parameters)
**Failure Signatures**: Low throughput despite correct setup → block_size or fetch_factor too small; poor model performance → insufficient minibatch diversity

**First Experiments**:
1. Benchmark throughput with different (b, f) combinations on small subset
2. Verify batch entropy meets theoretical bounds for chosen parameters
3. Compare F1-scores against streaming baseline on single classification task

## Open Questions the Paper Calls Out
**Open Question 1**: Can automated profiling robustly recommend optimal (block_size, fetch_factor) parameters across diverse dataset organizational patterns and hardware configurations?
- Basis: Experimental profiling feature exists but lacks validation across heterogeneous clustering structures
- Resolution: Systematic benchmarks on datasets with varying clustering patterns

**Open Question 2**: What throughput gains can scDataset achieve when combined with Zarr v3's cloud-native sharded storage?
- Basis: Zarr v3 offers concurrent I/O and rust-accelerated access, but no empirical comparison provided
- Resolution: Controlled experiments comparing scDataset+Zarr vs. scDataset+HDF5

**Open Question 3**: How does quasi-random sampling affect model convergence on tasks beyond classification?
- Basis: Validation limited to linear classification; unsupervised/generative tasks untested
- Resolution: Training foundation models (scVI, scGPT) with scDataset vs. true random sampling

## Limitations
- Performance highly dependent on dataset's natural clustering structure
- Optimal parameter selection requires manual tuning or experimental profiling
- Limited validation beyond single-cell omics classification tasks

## Confidence
**High Confidence**: Core algorithmic contribution and theoretical bounds are mathematically sound
**Medium Confidence**: Empirical performance claims based on specific experimental conditions
**Low Confidence**: Generalizability claims to other domains lack empirical validation

## Next Checks
1. Implement multiple gene preprocessing pipelines to assess sensitivity of results
2. Benchmark across different storage configurations (HDD, SATA SSD, NVMe)
3. Apply scDataset to non-single-cell datasets with natural clustering for cross-domain validation