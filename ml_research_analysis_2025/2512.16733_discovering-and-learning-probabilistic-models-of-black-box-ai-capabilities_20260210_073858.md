---
ver: rpa2
title: Discovering and Learning Probabilistic Models of Black-Box AI Capabilities
arxiv_id: '2512.16733'
source_url: https://arxiv.org/abs/2512.16733
tags:
- capability
- state
- learning
- agent
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PCML, a framework for learning interpretable
  probabilistic models of black-box AI (BBAI) capabilities. PCML discovers capabilities
  by actively querying a BBAI with distinguishing policies generated via Monte Carlo
  Tree Search, aiming to expose disagreements between optimistic and pessimistic capability
  models.
---

# Discovering and Learning Probabilistic Models of Black-Box AI Capabilities

## Quick Facts
- arXiv ID: 2512.16733
- Source URL: https://arxiv.org/abs/2512.16733
- Reference count: 17
- Primary result: PCML learns interpretable probabilistic models of BBAI capabilities with significantly lower variational distance than random exploration

## Executive Summary
This paper presents PCML, a framework for learning interpretable probabilistic models of black-box AI (BBAI) capabilities. PCML discovers capabilities by actively querying a BBAI with distinguishing policies generated via Monte Carlo Tree Search, aiming to expose disagreements between optimistic and pessimistic capability models. The learned models describe capabilities, their conditions, and probabilistic outcomes. Theoretical results show PCML's soundness, completeness, and convergence properties. Empirically, PCML achieves significantly lower variational distance than random exploration across diverse domains (e.g., 60% lower on Minigrid, 20% lower on SayCan), revealing surprising limitations and side-effects of BBAIs while enabling safer usage and design.

## Method Summary
PCML actively learns probabilistic capability models by maintaining two bounding models (optimistic and pessimistic) and using MCTS to find policies that maximize their disagreement. The algorithm begins with random exploration to discover initial capabilities, then iteratively partitions states by observed effects to construct both models. Distinguishing policies are synthesized to force the BBAI into states where model disagreement is maximal, with observed outcomes used to refine the models. Two implementations exist: PCML-E tracks exact probability distributions for accuracy, while PCML-S uses sampling for scalability. The process continues until the models converge or no distinguishing policy can be found.

## Key Results
- PCML-E and PCML-S achieve 60% and 20% lower variational distance respectively compared to random exploration in Minigrid
- On SayCan, PCML-S achieves 20% lower variational distance, though convergence is slower due to high stochasticity
- Learned models reveal surprising limitations (e.g., inability to turn right in Minigrid) and side-effects (e.g., unintended object picking in SayCan)

## Why This Works (Mechanism)

### Mechanism 1: Disagreement-Driven Active Querying
PCML accelerates model learning by prioritizing interactions that reveal maximal uncertainty between two bounding models. The algorithm maintains an Optimistic Model ($M_{opt}$) that assumes capabilities apply broadly, and a Pessimistic Model ($M_{pess}$) that only trusts observed data. MCTS finds a "distinguishing policy"—a sequence of capabilities where the predicted outcomes of these two models diverge significantly (high Total Variation distance). By executing this specific policy, the system forces the agent into a state where the uncertainty is highest, thereby pruning the hypothesis space efficiently. If the BBAI is highly stochastic (e.g., SayCan), the overlap in outcome distributions is high, reducing distinguishability and slowing convergence.

### Mechanism 2: Effect-Partitioned Generalization
The system learns complex conditional rules by grouping states that exhibit identical stochastic behaviors. Rather than learning a separate rule for every state, PCML groups abstract states into equivalence classes ($\Phi_c$) based on the set of observed effects. It then generalizes by assigning a single conditional rule to this entire partition. The "condition" for the rule is derived logically to cover the states in the partition (pessimistic) or exclude states in other partitions (optimistic). If the abstraction is too coarse (e.g., representing a robot's location only by "room" when precise coordinates matter for grasping), distinct effects will appear within a single partition, preventing consistent rule learning.

### Mechanism 3: Convergence via Model Collapse
Iteratively resolving disagreements guarantees that the learned model converges to the true agent model within the limits of the abstraction. The loop functions as a binary search on the hypothesis space. The Optimistic model provides an upper bound on capabilities, and the Pessimistic model provides a lower bound. Every query resolves ambiguity, effectively tightening these bounds. Theoretical results assert that if the true model is expressible in the vocabulary, the pessimistic and optimistic models will eventually coincide ($M_{pess} \equiv M_{opt}$), signifying convergence. If the agent is non-stationary (e.g., learning online while being tested), the "True Model" shifts, and the bounds may never collapse.

## Foundational Learning

- **Concept: PDDL (Planning Domain Definition Language)**
  - Why needed here: PCML outputs models in a PDDL-style format. You must understand how actions are defined by parameters, preconditions (conditions), and effects (outcomes) to interpret the learned capabilities.
  - Quick check question: Can you write a PDDL action for "PickUp(Object)" that has a probabilistic chance of failure?

- **Concept: Total Variation Distance**
  - Why needed here: This metric quantifies the "disagreement" between the optimistic and pessimistic models. It is the reward signal for the MCTS planner.
  - Quick check question: If distribution $P$ has probability 1.0 for state $A$ and distribution $Q$ has probability 1.0 for state $B$, what is the Total Variation Distance? (Answer: 1.0).

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed here: The active learning component uses MCTS to plan sequences of capabilities. You need to understand the balance between exploitation (actions that likely lead to disagreement) and exploration (trying new actions).
  - Quick check question: In the context of this paper, what serves as the "reward" for the MCTS agent?

## Architecture Onboarding

- **Component map:** Environment Interface -> Abstraction Layer -> Transition Database -> Model Learner -> Query Synthesizer (MCTS)
- **Critical path:** The Query Synthesizer. If the MCTS cannot find a policy that distinguishes $M_{pess}$ from $M_{opt}$ (e.g., if the state space is fully explored or too stochastic), the learning loop terminates.
- **Design tradeoffs:**
  - PCML-E vs. PCML-S: PCML-E tracks exact probability distributions over states (accurate but memory intensive). PCML-S uses sampling (scalable but noisier).
  - Pessimistic vs. Optimistic output: The system returns $M_{pess}$ by default for safety (it only claims what it has seen the agent do).
- **Failure signatures:**
  - High Variance/Slow Convergence: If the agent acts randomly (high stochasticity), the intersection of support sets between models is small, and MCTS rewards are flat (seen in SayCan results).
  - Abstraction Mismatch: If the agent succeeds/fails based on a variable not in the abstraction (e.g., lighting conditions), the learner sees "identical" states with different outcomes and may fail to partition correctly.
- **First 3 experiments:**
  1. Deterministic Validation: Run PCML on a deterministic GridWorld where the agent always succeeds. Verify $M_{pess} \equiv M_{opt}$ quickly.
  2. Stochastic Analysis: Introduce a 50% slip probability in the GridWorld. Verify that the learned capability model reflects $\approx 0.5$ probability for the slip effect.
  3. Ablation on Abstraction: Deliberately provide a "bad" abstraction function (e.g., remove the "key possessed" predicate from a Key-Door domain). Observe if the algorithm fails to distinguish states and produces high-variational-distance models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can capability models distinguish between genuine structural constraints and implicit planning preferences in black-box agents?
- Basis in paper: [explicit] The conclusion states that "real-world BBAIs often exhibit implicit or context-dependent preferences between multiple valid plans" and that "Distinguishing such preferences from genuine structural constraints remains an open challenge."
- Why unresolved: The current PCML framework learns a single model based on observed behavior. It cannot inherently tell if an agent failed to execute a path because it was impossible (structural) or simply because its internal policy preferred an alternative route (preference).
- What evidence would resolve it: A modified learning algorithm that generates distinct models for "hard constraints" (what the agent can do) versus "soft preferences" (what the agent tends to do), validated by showing it can predict outcomes when the agent is forced to act against its typical preferences.

### Open Question 2
- Question: How can the PCML framework be adapted to dynamically identify relevant intents through user interaction?
- Basis in paper: [explicit] The paper notes that "In practice the set of relevant intents will depend on a user's current tasks and deployment; such sets can be acquired with user interaction in future work."
- Why unresolved: The current implementation uses the set of all achievable single-literal intents for evaluation, which may be inefficient or insufficient for specific real-world user needs.
- What evidence would resolve it: An interactive variant of PCML where a user provides high-level objectives, and the system demonstrably prioritizes learning capabilities relevant to those objectives over others.

### Open Question 3
- Question: Can the requirement for a pre-defined abstraction function be relaxed to allow for the simultaneous learning of symbolic abstractions and capability models?
- Basis in paper: [explicit] The authors state, "Since several directions of research address the problem of learning such abstraction functions... we focus on the problem of learning capability models given an abstraction function."
- Why unresolved: The framework relies heavily on the abstraction function $\alpha$ to map environment states to symbolic predicates. If this mapping is poor or unavailable, PCML cannot operate.
- What evidence would resolve it: An end-to-end system that learns the predicate vocabulary (abstraction) while learning the probabilistic capability models, achieving comparable variational distance to PCML without the fixed abstraction input.

## Limitations

- The framework assumes access to a reliable simulator that can exactly reset/revert states, which may not hold for complex real-world BBAIs.
- The abstraction function $\alpha$ is critical—if it omits relevant features, learned models may be incorrect.
- The convergence guarantee depends on the true model being expressible in the vocabulary, but expressiveness limits are not quantified.
- In highly stochastic domains (SayCan), the intersection of support sets between models becomes small, leading to flat MCTS rewards and slow convergence.
- The paper uses a single abstraction per domain without exploring how abstraction quality affects results.

## Confidence

- **High confidence**: Theoretical convergence results (Theorems 1-3) and their proofs are sound within the model assumptions. The variational distance metric and its interpretation are well-established.
- **Medium confidence**: The empirical results show PCML outperforms random exploration, but the difference in SayCan is modest (20%) and may reflect domain difficulty rather than algorithm limitations.
- **Medium confidence**: The learned PDDL models capture capabilities accurately, though the claim that they enable "safer usage and design" is supported mainly by examples rather than systematic safety validation.

## Next Checks

1. **Abstraction Sensitivity Test**: Systematically degrade the abstraction function quality (e.g., by removing predicates) and measure the resulting increase in variational distance to quantify abstraction impact.

2. **Stationarity Test**: Run PCML on a non-stationary BBAI that changes behavior mid-learning (e.g., through online adaptation) and verify that convergence guarantees fail as predicted.

3. **Sample Efficiency Analysis**: Compare the number of queries needed for PCML vs. random exploration to reach a fixed variational distance threshold across multiple random seeds and domain configurations.