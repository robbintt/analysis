---
ver: rpa2
title: 'Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods
  on Performance and Explainability'
arxiv_id: '2504.16056'
source_url: https://arxiv.org/abs/2504.16056
tags:
- training
- student
- online
- available
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how different knowledge distillation methods
  affect the performance and explainability of small language models. The authors
  systematically compare four distillation methods: few-shot prompting, critique-revision
  prompting for data generation, and multitask, counterfactual, and combined multitask+counterfactual
  training.'
---

# Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability

## Quick Facts
- arXiv ID: 2504.16056
- Source URL: https://arxiv.org/abs/2504.16056
- Authors: Daniel Hendriks; Philipp Spitzer; Niklas Kühl; Gerhard Satzger
- Reference count: 40
- Primary result: Multitask training yields best performance; critique-revision + multitask+counterfactual maximizes explainability

## Executive Summary
This paper systematically investigates how different knowledge distillation methods affect the performance and explainability of small language models. The authors compare four distillation methods—few-shot prompting, critique-revision prompting, multitask training, counterfactual training, and their combinations—using T5-base and T5-large models distilled from LLaMA-2-13B. Results demonstrate that multitask training provides superior accuracy on CommonsenseQA, while combining critique-revision data generation with multitask+counterfactual training produces the highest human-rated explanation quality. The study reveals a fundamental tradeoff between accuracy and explainability in model compression.

## Method Summary
The authors employ a teacher-student knowledge distillation framework where LLaMA-2-13B generates training data for T5 student models. Data generation uses three-stage critique-revision prompting to create richer explanations, with counterfactual examples added for training robustness. Four training methods are compared: multitask (joint answer and explanation optimization), counterfactual (correct vs incorrect reasoning), and their combinations. Models are evaluated on CommonsenseQA using both accuracy metrics and human judgments of explanation quality across completeness, contrastiveness, and soundness dimensions.

## Key Results
- Multitask training significantly outperforms counterfactual training for accuracy (12.66*** for 220M, 6.24*** for 770M models)
- Critique-revision prompting significantly improves human-perceived explanation completeness and contrastiveness (p=0.0081 and p=0.0004)
- Combined multitask+counterfactual with critique-revision produces highest explainability scores but sacrifices 2-4% accuracy
- Counterfactual training alone does not improve explainability beyond multitask training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask training outperforms counterfactual training for student model accuracy on commonsense reasoning tasks.
- Mechanism: Joint optimization on answer prediction and explanation generation provides denser learning signal than counterfactual training's mixed correct/incorrect reasoning objective. Consistent signal reduces gradient conflict during optimization.
- Core assumption: Student model has sufficient capacity to simultaneously learn both answer and explanation patterns without catastrophic interference.
- Evidence anchors:
  - [abstract]: "multitask training yields the best performance"
  - [section V-A]: "multitask training is superior to counterfactual training" with Tukey-Kramer test showing MT:Unrevised significantly outperforms CF:Unrevised across both model sizes
  - [corpus]: Weak/missing - neighboring papers focus on distribution mismatch rather than multitask vs. counterfactual comparison
- Break condition: Performance degradation when scaling to datasets where answer-explanation correlations are weak or contradictory.

### Mechanism 2
- Claim: Critique-revision prompting improves perceived explanation quality without improving task accuracy.
- Mechanism: Three-stage iterative refinement (initial generation → self-critique → revision) produces explanations that differentiate between answer choices and include more context-relevant details. Richer supervision signal learned even though longer explanations may introduce noise for answer prediction.
- Core assumption: Teacher model's self-critique reliably identifies genuine explanation deficiencies rather than spurious critiques.
- Evidence anchors:
  - [abstract]: "combining critique-revision prompting with multitask+counterfactual training produces the highest explainability scores"
  - [section V-B]: "critique-revision prompting positively impacts the student model's explainability" with significant improvements in completeness and contrastiveness; regression shows MT+CF:Revised has β=0.284*** vs baseline
  - [corpus]: Weak/missing - no direct validation of critique-revision in neighboring distillation literature
- Break condition: Critique quality degrades on domains where teacher lacks strong self-evaluation capabilities.

### Mechanism 3
- Claim: Counterfactual training does not improve explainability beyond multitask training alone when evaluated by humans.
- Mechanism: Learning to recognize incorrect reasoning patterns (L_incorrect) does not transfer to generating higher-quality correct explanations. Human evaluation reveals this objective introduces noise that offsets robustness gains.
- Core assumption: Learning incorrect reasoning patterns transfers to generating higher-quality correct explanations.
- Evidence anchors:
  - [abstract]: "combining multitask and counterfactual training does not improve the explainability"
  - [section V-B]: "counterfactual training alone does not enhance explainability beyond multitask training" with MT+CF:Unrevised showing no significant improvement over MT:Unrevised
  - [corpus]: Weak/missing - corpus papers do not address counterfactual training's effect on human-perceived explanation quality
- Break condition: Task domains where explanation faithfulness is more critical than plausibility to human evaluators.

## Foundational Learning

- Concept: **Knowledge Distillation Fundamentals**
  - Why needed here: Understanding teacher-student paradigm is prerequisite to evaluating why different distillation methods produce divergent performance/explainability tradeoffs.
  - Quick check question: Can you explain why a smaller student model might fail to fully capture a teacher's reasoning even with perfect training data?

- Concept: **Multi-Objective Optimization with Gradient Conflict**
  - Why needed here: Paper compares methods that optimize multiple loss terms simultaneously; understanding gradient interference explains why MT succeeds while CF introduces noise.
  - Quick check question: When jointly minimizing L_answer and L_explanation, what would cause one objective to dominate gradient updates?

- Concept: **Human-Grounded Explainability Evaluation**
  - Why needed here: Paper's key contribution shows LLM-based explainability metrics diverge from human judgments; interpreting results requires understanding evaluation methodology tradeoffs.
  - Quick check question: Why might an explanation score high on automated faithfulness metrics but low on human-rated contrastiveness?

## Architecture Onboarding

- Component map: Teacher Model (LLaMA-2-13B) → Data Generation Pipeline (Initial explanation → Self-critique → Revised explanation) → Training Data (unrevised OR revised explanations + counterfactuals) → Student Model (T5-base 220M / T5-large 770M) → Training Loop (Multitask, Counterfactual, Combined losses)

- Critical path:
  1. Data generation quality (critique-revision) → explanation richness → human-perceived completeness/contrastiveness
  2. Training method selection (MT vs. CF) → gradient signal quality → accuracy performance
  3. Model size (220M vs. 770M) → capacity to leverage complex explanations → performance/explainability tradeoff

- Design tradeoffs:
  - Accuracy vs. Explainability: MT:Unrevised maximizes accuracy; MT+CF:Revised maximizes explainability but sacrifices ~2-4% accuracy
  - Computational cost: Critique-revision requires 3x teacher inference passes per sample; counterfactual generation adds 2x data preparation
  - Model size sensitivity: Larger students (770M) handle revised explanations better; smaller students (220M) show performance degradation with longer explanations

- Failure signatures:
  - Critique-revision harms accuracy (especially 770M model: -1.90* vs. MT:Unrevised) → check if explanation length exceeds student's effective context utilization
  - Counterfactual training underperforms baseline → gradient conflict between L_correct and L_incorrect may dominate; verify loss weighting
  - Low human ratings on contrastiveness → verify explanations explicitly reference why correct answer differs from distractors

- First 3 experiments:
  1. Reproduce MT:Unrevised baseline on CQA with T5-base to validate training setup (target: ~70-72% accuracy matching paper's Figure 4a).
  2. Ablate critique-revision stages: train separate students on (a) unrevised explanations, (b) revised explanations, (c) critique-only to isolate which stage drives completeness/contrastiveness gains.
  3. Test loss weighting sensitivity: vary α in L_combined = α·L_multitask + (1-α)·L_counterfactual to determine if counterfactual signal can be tempered to provide benefit without introducing noise (current paper uses α=0.5).

## Open Questions the Paper Calls Out
None

## Limitations
- Conclusions based on human evaluations from 26 Amazon Mechanical Turk workers, potentially unrepresentative of broader populations
- Study focuses on single dataset (CommonsenseQA) and task domain, limiting generalizability to other reasoning tasks
- Narrow range of student model capacities (T5-base vs T5-large) may not scale to significantly larger or smaller models

## Confidence
- **High Confidence**: Multitask training outperforms counterfactual training for accuracy (p < 0.001 in multiple comparisons) well-supported by statistical tests and consistent across both model sizes
- **Medium Confidence**: Critique-revision prompting improves human-perceived explanation quality supported but based on limited human evaluators and single dataset
- **Low Confidence**: Counterfactual training does not improve explainability beyond multitask training based on pairwise comparisons with modest effect sizes (< 0.03 on Likert scales)

## Next Checks
1. Cross-dataset validation: Replicate training method comparisons (MT, CF, MT+CF) on additional reasoning datasets like StrategyQA or SocialIQA to test generalizability of accuracy vs. explainability tradeoff findings.

2. Evaluation methodology stress test: Compare human evaluation results against automated explainability metrics (BERTScore, BLEURT, or faithfulness scores) to quantify divergence between human and model-based assessments of explanation quality.

3. Teacher model ablation study: Train student models using explanations generated by different teacher models (LLaMA-2-7B, LLaMA-2-33B) to test whether critique-revision benefits depend on teacher model capacity and self-critique reliability.