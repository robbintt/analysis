---
ver: rpa2
title: 'The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented
  Generation in Multi-Turn Legal Conversation'
arxiv_id: '2510.15722'
source_url: https://arxiv.org/abs/2510.15722
tags:
- legal
- retrieval
- generation
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the 3rd place solution for the CCIR CUP 2025
  Legal Knowledge Retrieval and Generation task, achieving a final score of 39.38
  on the test set. The approach employs a multi-component Retrieval-Augmented Generation
  framework that includes query rewriting for multi-turn conversation handling, multi-route
  retrieval combining dense and sparse methods, legal literature filtering to reduce
  noise, and reranking for improved candidate selection.
---

# The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation

## Quick Facts
- arXiv ID: 2510.15722
- Source URL: https://arxiv.org/abs/2510.15722
- Reference count: 16
- 3rd place in CCIR CUP 2025 with final score of 39.38

## Executive Summary
This paper presents a comprehensive Retrieval-Augmented Generation framework for multi-turn legal conversations, achieving 3rd place in the CCIR CUP 2025 competition. The system addresses the challenge of retrieving and generating contextually appropriate legal responses across complex conversational threads. Through a multi-component approach including query rewriting, hierarchical legal literature filtering, multi-route retrieval, and reranking, the solution achieved a final score of 39.38 with strong performance in both retrieval (NDCG@5: 41.51) and generation (BERT-FNN: 35.26, Keyword Accuracy: 56.07). The framework demonstrates how careful integration of LLM-based components can significantly improve legal RAG systems' ability to handle real-world conversational scenarios.

## Method Summary
The solution employs a five-stage pipeline: (1) Query rewriting using Qwen3-235B-a22b to convert multi-turn conversational context into standalone search queries while extracting legal keywords, (2) Hierarchical legal literature filtering with Qwen3-14B to prune irrelevant legal codes and articles, reducing noise by 73% while retaining 96% of relevant content, (3) Multi-route retrieval combining dense retrieval (Qwen3-Embedding-8B with 50 documents per literature, α=1.0) and sparse retrieval (BM25 with 1,000 documents), (4) Article filtering selecting top 500 candidates, and (5) Reranking with fine-tuned Qwen3-Reranker-8B (top 5 selected) followed by generation with Qwen3-235B-a22b using 2-shot in-context learning for syllogistic legal reasoning style. The reranker was fine-tuned on the Swift framework with hard negative sampling, selecting 5 negatives from the top-10 retrieved candidates.

## Key Results
- Final score of 39.38 on CCIR CUP 2025 test set (41.51 NDCG@5, 35.26 BERT-FNN, 56.07 Keyword Accuracy)
- Legal literature filtering provided the largest improvement (1.32 points) by reducing irrelevant legal codes by 73% while retaining 96% of relevant ones
- Hard negative sampling in reranking fine-tuning achieved average improvement of 0.08 in NDCG@5
- Query rewriting achieved 87% accuracy in manual analysis for capturing user intent in multi-turn conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query rewriting converts multi-turn conversational context into standalone search queries, improving retrieval relevance for follow-up questions.
- Mechanism: An LLM (Qwen3-235B-a22b) analyzes historical questions alongside the current question to resolve coreferences and extract core legal intent, outputting both a rewritten query and 3-5 key legal terms wrapped in tags.
- Core assumption: The LLM can accurately infer which historical context is relevant without over-interpreting user intent.
- Evidence anchors:
  - [abstract] "query rewriting for multi-turn conversation handling"
  - [section 5.3.1] "Manual analysis of 100 randomly sampled conversations showed that 87% of rewritten queries better captured user intent"
  - [corpus] MTRAG benchmark paper confirms multi-turn RAG evaluation is understudied, supporting this mechanism's importance
- Break condition: Vague descriptions with complex pronoun references or implicit legal relationships may cause over-interpretation (32% of errors per Section 6.3).

### Mechanism 2
- Claim: Hierarchical legal literature filtering reduces retrieval noise by first pruning irrelevant legal codes before article-level selection.
- Mechanism: Two-stage LLM filtering—first determining if the user's question relates to a legal code's domain (e.g., Administrative Penalties Law), then filtering individual articles within remaining codes for relevance to the specific question.
- Core assumption: Legal questions typically apply to only a small subset of the corpus, and LLMs can reliably make binary relevance judgments.
- Evidence anchors:
  - [abstract] "legal literature filtering providing the largest improvement (1.32 points)"
  - [section 5.3.1] "filtered out 73% of irrelevant legal codes while retaining 96% of relevant ones"
  - [corpus] Weak direct corpus evidence on hierarchical filtering; most related RAG papers focus on retrieval strategies rather than domain-aware pre-filtering
- Break condition: Questions about emerging legal issues or recently enacted laws not well-represented in training data may be incorrectly filtered.

### Mechanism 3
- Claim: Hard negative sampling during reranker fine-tuning improves discrimination among superficially similar legal provisions.
- Mechanism: Rather than random negatives, the system selects 5 negative samples from the top-10 retrieved candidates (excluding the annotated positive), forcing the reranker to learn fine-grained distinctions between relevant and near-miss articles.
- Core assumption: The hardest negatives (those retrieved but not annotated as positive) contain learnable signals that distinguish true relevance from semantic similarity.
- Evidence anchors:
  - [section 5.1.3] "randomly selected 5 provisions from the top 10 most relevant ones... to serve as negative samples"
  - [section 5.3.1] "fine-tuned reranking model achieved... average improvement of 0.08 in NDCG@5"
  - [corpus] Multi-Stage Verification-Centric Framework paper similarly emphasizes verification-centric approaches in multi-modal RAG, supporting multi-stage refinement patterns
- Break condition: Vocabulary mismatches between user queries and legal text may persist even after reranking (28% of errors).

## Foundational Learning

- Concept: NDCG@5 (Normalized Discounted Cumulative Gain)
  - Why needed here: This is the primary retrieval evaluation metric; understanding position-weighted relevance scoring is essential for debugging reranking improvements.
  - Quick check question: If a relevant document moves from position 5 to position 2, does NDCG@5 increase or decrease?

- Concept: Dense vs. Sparse Retrieval Trade-offs
  - Why needed here: The multi-route retrieval strategy explicitly combines both; knowing when each excels helps diagnose retrieval gaps.
  - Quick check question: Which retrieval method would better handle an exact statute number match versus a paraphrased legal concept?

- Concept: In-Context Learning (ICL) for Style Transfer
  - Why needed here: Response generation uses 2-shot examples to enforce "syllogistic" legal reasoning style without fine-tuning.
  - Quick check question: What happens to output consistency if the provided examples differ in citation format?

## Architecture Onboarding

- Component map:
  - Query Rewriting (Qwen3-235B-a22b) → Multi-route Retrieval (Qwen3-Embedding-8B + BM25) → Literature Filtering (Qwen3-14B, two stages) → Reranking (fine-tuned Qwen3-Reranker-8B) → Response Generation (Qwen3-235B-a22b with 2-shot ICL)

- Critical path:
  - Query rewriting → dense retrieval → literature filtering → reranking → top-5 articles to generation. Sparse retrieval (BM25) provides parallel candidates merged before filtering.

- Design tradeoffs:
  - Using Qwen3-14B for filtering instead of larger model balances latency vs. accuracy (500 candidates processed)
  - Dense retriever weight α=1.0 means BM25 provides expansion candidates but dense scores dominate final ranking before reranking
  - Top-5 articles for generation caps context length but may omit marginally relevant provisions

- Failure signatures:
  - Context Misunderstanding (32%): Over-interpreting intent in vague queries—check if rewritten query adds concepts not in original
  - Retrieval Gaps (28%): Vocabulary mismatch—examine if query terms match legal article terminology
  - Generation Inconsistencies (40%): Style drift or incomplete citations—verify 2-shot examples align with target output format

- First 3 experiments:
  1. Reproduce vanilla RAG baseline (score ~30.74) using only dense retrieval + generation without filtering or reranking to establish component isolation.
  2. Ablate hard negative sampling in reranker fine-tuning by comparing against random negatives to quantify the +0.08 NDCG@5 gain.
  3. Test literature filtering threshold sensitivity by varying the number of candidates passed to reranking (currently 500) and measuring recall/precision trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system be enhanced to handle complex multi-step legal reasoning or the integration of multiple conflicting legal principles?
- Basis in paper: [explicit] Section 8 states the current system "struggles with cases requiring complex multi-step legal reasoning or integration of multiple legal principles."
- Why unresolved: The current architecture prioritizes finding relevant single provisions via reranking rather than synthesizing logical chains across documents.
- What evidence would resolve it: Success on a benchmark specifically designed to require multi-hop reasoning or the synthesis of conflicting statutes.

### Open Question 2
- Question: What mechanisms are necessary to adapt the static legal corpus to support queries regarding emerging jurisprudence or recently enacted laws?
- Basis in paper: [explicit] Section 8 identifies the "static nature of our legal corpus" as a limitation that prevents handling "recent legal developments."
- Why unresolved: The framework relies on a fixed dataset of 17,229 articles, lacking a pipeline for dynamic knowledge updates.
- What evidence would resolve it: Performance maintenance on a temporal dataset containing laws enacted after the training corpus cut-off.

### Open Question 3
- Question: Can evaluation metrics be developed to assess legal reasoning quality beyond semantic similarity and keyword coverage?
- Basis in paper: [explicit] Section 8 notes that current metrics focus on "retrieval accuracy and semantic similarity" but fail to assess "legal reasoning quality."
- Why unresolved: Metrics like BERT-FNN measure textual overlap but do not verify the logical validity or practical utility of the generated legal advice.
- What evidence would resolve it: A human-annotated evaluation framework grading responses on logical validity and argument soundness.

## Limitations
- Dataset Dependency: Performance metrics are specific to the CCIR CUP 2025 competition dataset, which is not publicly available, limiting reproducibility and generalization.
- Hyperparameter Sensitivity: Critical reranker fine-tuning hyperparameters (learning rate, batch size, epochs) are unspecified, potentially affecting reported performance gains.
- Single Competition Context: The 3rd place ranking reflects performance on a specific task; the absolute impact of components may not generalize to domains without hierarchical legal structure.

## Confidence

**High Confidence**: The query rewriting mechanism (87% accuracy in manual analysis) and multi-route retrieval combination (Qwen3-Embedding-8B + BM25) are well-documented with clear implementation paths. The 2-shot in-context learning approach for response generation is a standard, reproducible technique.

**Medium Confidence**: The hierarchical legal literature filtering shows strong reported performance (73% noise reduction, 96% recall) but relies on LLM judgment quality that may vary with model versions or prompt variations. The hard negative sampling strategy is theoretically sound but requires careful implementation to match the reported 0.08 NDCG@5 improvement.

**Low Confidence**: The overall system integration assumes optimal parameter settings (α=1.0 for dense retrieval dominance, top-5 generation cutoff) that weren't systematically explored. The failure mode distribution (32% context misunderstanding, 28% retrieval gaps, 40% generation inconsistencies) suggests significant brittleness in edge cases.

## Next Checks

1. **Dataset Availability Impact**: Test whether the reported 39.38 score is achievable on alternative legal RAG benchmarks (e.g., COLIEE competition data or synthetic legal QA pairs) to assess generalization beyond the CCIR CUP 2025 dataset.

2. **Reranker Fine-Tuning Replication**: Implement the reranker fine-tuning with both hard negative sampling (top-10 random selection) and random negative sampling, measuring the actual NDCG@5 difference to validate the claimed 0.08 improvement.

3. **Literature Filtering Threshold Sensitivity**: Systematically vary the number of candidates passed from literature filtering to reranking (test 200, 500, 1000 candidates) while measuring recall of relevant articles and final generation quality to determine if the current 500-candidate cutoff is optimal.