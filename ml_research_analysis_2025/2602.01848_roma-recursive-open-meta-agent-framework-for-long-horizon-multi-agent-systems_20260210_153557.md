---
ver: rpa2
title: 'ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems'
arxiv_id: '2602.01848'
source_url: https://arxiv.org/abs/2602.01848
tags:
- roma
- gepa
- task
- writing
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ROMA introduces a recursive, modular meta-agent framework that\
  \ addresses brittleness, context limits, and opacity in long-horizon multi-agent\
  \ systems. By standardizing agent construction around four roles\u2014Atomizer,\
  \ Planner, Executor, and Aggregator\u2014ROMA enables transparent, hierarchical\
  \ execution traces and controlled context growth through recursive decomposition\
  \ and bounded aggregation."
---

# ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems

## Quick Facts
- **arXiv ID:** 2602.01848
- **Source URL:** https://arxiv.org/abs/2602.01848
- **Reference count:** 40
- **Primary result:** Recursive meta-agent framework improving reasoning depth and interpretability in long-horizon multi-agent tasks

## Executive Summary
ROMA introduces a recursive, modular meta-agent framework designed to address brittleness, context limits, and opacity in long-horizon multi-agent systems. By standardizing agent construction around four roles—Atomizer, Planner, Executor, and Aggregator—ROMA enables transparent, hierarchical execution traces and controlled context growth through recursive decomposition and bounded aggregation. This design supports heterogeneous multi-agent systems and parallel execution. To adapt ROMA to specific tasks without fine-tuning, we introduce GEPA+, a multi-component prompt optimization method that jointly optimizes prompts across ROMA's roles via a structured, K-way proposal and selection process. Empirically, ROMA with GLM-4.6 improves SEAL-0 reasoning accuracy by 9.9% over Kimi-Researcher and, when combined with GEPA+, enables DeepSeek-V3 to match leading closed-source models like Claude Sonnet 4.5 on EQ-Bench long-form writing. ROMA also achieves top performance on reasoning (FRAMES, SimpleQA) and scientific task design (AbGen) benchmarks. Results demonstrate that recursive, modular architectures scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

## Method Summary
ROMA's method centers on a recursive decomposition of tasks into atomic units, processed through a standardized four-role architecture: Atomizer (task parsing), Planner (strategy formulation), Executor (action execution), and Aggregator (context management). This recursive process allows tasks to be broken down into smaller, manageable subtasks, each handled by a fresh instance of the meta-agent. GEPA+ optimizes task-specific prompts by generating K candidate prompts per component and selecting the best combination via hierarchical evaluation, avoiding the need for fine-tuning. The framework supports both sequential and parallel execution of agents, with the Aggregator ensuring bounded context growth and preventing exponential expansion of the task hierarchy.

## Key Results
- ROMA with GLM-4.6 achieves 9.9% higher reasoning accuracy on SEAL-0 than Kimi-Researcher.
- DeepSeek-V3 with GEPA+ matches Claude Sonnet 4.5 on EQ-Bench long-form writing tasks.
- ROMA attains state-of-the-art performance on FRAMES, SimpleQA, and AbGen benchmarks.

## Why This Works (Mechanism)
ROMA's recursive structure enables systematic breakdown of complex, long-horizon tasks into atomic units, each processed by a fresh meta-agent instance. This prevents context overload and maintains interpretability through transparent, hierarchical execution traces. The four-role standardization ensures modularity and adaptability across diverse tasks and agent types. GEPA+ further enhances task-specific performance by jointly optimizing prompts for each role, enabling model-agnostic deployment without fine-tuning.

## Foundational Learning
- **Recursive decomposition**: Essential for breaking long-horizon tasks into manageable subtasks; verify by checking trace depth and context size.
- **Bounded context aggregation**: Prevents exponential growth of context; validate by measuring context length at each recursion level.
- **Modular role standardization**: Enables flexible agent assembly; test by swapping roles or adding new agent types.
- **Joint prompt optimization (GEPA+)**: Adapts ROMA to new tasks without fine-tuning; confirm by comparing performance with and without GEPA+.
- **Hierarchical execution traces**: Provides interpretability; assess by evaluating trace clarity and user comprehension.
- **Parallel agent execution**: Improves efficiency; measure speedup vs. sequential execution.

## Architecture Onboarding

**Component Map:** Atomizer -> Planner -> Executor -> Aggregator (recursive loop)

**Critical Path:** Task input → Atomizer (parsing) → Planner (strategy) → Executor (action) → Aggregator (context) → (optional) recursion

**Design Tradeoffs:**
- **Modularity vs. overhead**: Four-role standardization adds abstraction but enables flexibility; assess by measuring latency and success rate.
- **Recursion depth vs. context growth**: Deeper recursion improves reasoning but risks context explosion; monitor context size and performance at each level.
- **Interpretability vs. complexity**: Hierarchical traces aid understanding but may become unwieldy; evaluate trace usability and clarity.

**Failure Signatures:**
- **Context explosion**: Aggregator fails to bound context; check context length at each recursion.
- **Stalled recursion**: Planner fails to decompose tasks further; monitor recursion depth and subtask generation.
- **Suboptimal prompt selection**: GEPA+ fails to find effective prompts; compare prompt performance across candidates.

**First Experiments:**
1. Run ROMA on a simple, well-defined task (e.g., arithmetic reasoning) to verify basic recursion and role execution.
2. Test ROMA's context control by measuring context size and performance at increasing recursion depths.
3. Evaluate GEPA+'s effectiveness by comparing task performance with and without prompt optimization.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are based on single runs or limited datasets; robustness across domains is untested.
- Scalability of recursive decomposition is not validated at deeper recursion levels.
- Interpretability claims rely on execution traces, but their comprehension and utility are not empirically verified.
- Framework is validated only on two model families; generalizability to other architectures is unproven.

## Confidence
- **Performance claims:** Medium (single runs, limited datasets)
- **Architectural claims:** High (design logic is sound), Low (empirical validation of scalability and interpretability is lacking)

## Next Checks
1. Conduct cross-dataset evaluation to test ROMA's performance stability beyond SEAL-0 and EQ-Bench.
2. Perform ablation studies isolating the Aggregator's role in controlling context size and measuring reasoning depth at higher recursion levels.
3. Validate ROMA's interpretability claims by conducting user studies to assess comprehension of execution traces.