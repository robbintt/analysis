---
ver: rpa2
title: 'AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists'
arxiv_id: '2506.08140'
source_url: https://arxiv.org/abs/2506.08140
tags:
- code
- task
- data
- tasks
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoSDT, an automatic pipeline that scales
  data-driven scientific discovery tasks by collecting high-quality coding tasks from
  real-world workflows. AutoSDT uses LLMs to search diverse sources, select ecologically
  valid tasks, and synthesize accurate task instructions and code solutions.
---

# AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists

## Quick Facts
- arXiv ID: 2506.08140
- Source URL: https://arxiv.org/abs/2506.08140
- Reference count: 40
- AutoSDT-5K is the largest open dataset for data-driven scientific discovery, containing 5,404 tasks across 4 scientific disciplines and 756 Python packages.

## Executive Summary
AutoSDT is an automatic pipeline that scales data-driven scientific discovery tasks by collecting high-quality coding tasks from real-world workflows. The system uses LLMs to search diverse sources, select ecologically valid tasks, and synthesize accurate task instructions and code solutions. The resulting AutoSDT-5K dataset contains 5,404 tasks across four scientific disciplines and 756 Python packages. Expert evaluation confirms high quality: 93% of tasks are ecologically valid and 92.2% of programs are functionally correct. Training models on AutoSDT-5K significantly improves performance on two challenging benchmarks—AutoSDT-Coder-32B matches GPT-4o's success rate of 7.8% on ScienceAgentBench and improves DiscoveryBench hypothesis matching score to 8.1, closing the gap between open-weight and proprietary models.

## Method Summary
AutoSDT is a three-stage pipeline that automatically collects data-driven scientific discovery coding tasks from GitHub and PapersWithCode repositories. The system first uses GPT-4o to expand seed keywords into domain-specific queries, then filters repositories and Python files to identify scientific workflows that use datasets and produce scientific outputs. Selected code is adapted for standalone execution using Claude-3.7-Sonnet with up to three self-debug iterations, then back-translated into task instructions using GPT-4o. The resulting AutoSDT-5K dataset contains 5,404 tasks covering four scientific disciplines (biology, chemistry, neuroscience, geoscience) and 756 Python packages. Qwen2.5-Coder models are fine-tuned on this dataset using LlamaFactory with full-parameter SFT, then evaluated on ScienceAgentBench and DiscoveryBench benchmarks.

## Key Results
- AutoSDT-5K contains 5,404 tasks across four scientific disciplines and 756 Python packages, making it the largest open dataset for data-driven discovery
- Expert evaluation confirms 93% ecological validity and 92.2% functional correctness of the collected tasks
- AutoSDT-Coder-32B matches GPT-4o's 7.8% success rate on ScienceAgentBench and achieves 8.1 hypothesis matching score on DiscoveryBench

## Why This Works (Mechanism)

### Mechanism 1
LLM-driven keyword expansion increases source diversity compared to manual keyword selection. Seed keywords are expanded by an LLM into domain-specific queries (e.g., "neuroscience" → "neuroimaging," "neuroplasticity," "neuroinformatics"), which are then used to search GitHub and PapersWithCode APIs. This doubles repository discovery in some disciplines.

### Mechanism 2
Multi-criteria LLM filtering identifies ecologically valid data-driven discovery tasks. An LLM evaluates each Python file against three criteria: (1) functionality related to scientific workflows, (2) uses datasets as input, (3) produces scientific outputs. This filters out utilities, configs, and non-scientific code.

### Mechanism 3
Iterative adaptation with self-debugging produces standalone executable programs. Claude-3.7-Sonnet adapts code for standalone execution (fixing imports, paths), then the code is executed in a conda environment. Errors trigger regeneration for up to 3 iterations. Surviving programs are back-translated into task instructions.

## Foundational Learning

- **Ecological validity in scientific tasks**: Why needed here—AutoSDT prioritizes tasks that scientists actually encounter in their workflows. Understanding this helps distinguish between "synthetic" and "real" tasks. Quick check question: Would a domain expert recognize this task as something they might do in their research?

- **LLM back-translation (code → instruction)**: Why needed here—AutoSDT generates task instructions by having an LLM describe what a program does. This is the reverse of typical code generation. Quick check question: Given a code snippet, can you write a clear instruction that describes its goal without revealing implementation details?

- **Dependency extraction for code portability**: Why needed here—Scientific code often depends on datasets, models, and local modules. AutoSDT must identify and package these to create standalone workspaces. Quick check question: Given a Python file and repository structure, can you identify all files it imports or reads?

## Architecture Onboarding

- Component map: Seed keywords → AutoSDT-Search → AutoSDT-Select → AutoSDT-Adapt → AutoSDT-5K dataset
- Critical path: AutoSDT-Select's LLM-based scientific code filtering is the quality gate; poor filtering propagates invalid tasks downstream
- Design tradeoffs: GPT-4o vs. Claude-3.7-Sonnet (GPT-4o for most stages, Claude for code adaptation); 3-iteration debug limit (trades completeness vs. API cost); file-level tasks only (cannot capture notebook-style exploratory analysis)
- Failure signatures: Low task validity (>15% rejected by experts)—check LLM filtering prompts for ambiguity; low code correctness (>20% programs fail execution)—check dependency extraction or conda environment setup; high API costs—check for unnecessary re-runs in self-debug loop
- First 3 experiments: 1) Reproduce AutoSDT-Search on a new discipline (e.g., materials science): provide seed keywords, run keyword expansion and repository search, measure repository yield; 2) Validate AutoSDT-Select on 50 manually labeled files: run LLM filtering, compare against human labels, calculate precision/recall; 3) Test AutoSDT-Adapt on 20 programs: run adaptation and self-debug, measure execution success rate and compare expert-rated correctness vs. reported 92.2%

## Open Questions the Paper Calls Out

- Can an automatic pipeline reliably generate instance-specific evaluation scripts for AutoSDT-5K tasks to support reinforcement learning? The authors state in Limitations that the lack of ground-truth evaluation scripts "limits its usability in some settings such as reinforcement learning" and call for an automatic framework.

- Does synthesizing chain-of-thought (CoT) rationales from solution programs enable open-weight models to close the performance gap with proprietary reasoning models? The authors note a gap with models like OpenAI o1 and identify "generating effective long chain-of-thought (CoT) rationales" from code as a promising direction.

- How does the performance of AutoSDT-Coder change when integrated into agentic frameworks involving iterative self-debugging? The paper "leave[s] the experimentation with agent frameworks such as OpenHands CodeAct and self-debug for future work."

## Limitations
- Scientific code filtering may misclassify complex workflows that don't follow standard patterns
- Dependency extraction may fail for code relying on external APIs, databases, or complex data pipelines
- Task diversity limited to repositories with ≥10 stars, potentially missing valuable workflows from smaller or private codebases

## Confidence

- **High confidence**: Task and code quality metrics (93% validity, 92.2% correctness) given expert evaluation; benchmark performance improvements (AutoSDT-Coder-32B matches GPT-4o on ScienceAgentBench); dataset scale (5,404 tasks, 756 packages)
- **Medium confidence**: Scientific workflow representation (4 disciplines, 756 packages); code adaptation mechanism (84.4% success preserving functionality); dependency extraction (assumes standard Python patterns)
- **Low confidence**: Keyword expansion effectiveness; repository filtering criteria; handling of complex scientific dependencies; generalizability beyond target disciplines

## Next Checks
1. A/B test keyword expansion: Compare repository yield and task diversity using LLM-expanded keywords versus manually curated discipline-specific keywords across all four target domains
2. Error analysis of scientific filtering: Manually audit 100 filtered-out Python files to calculate false negative rate and identify systematic patterns where LLM filtering misclassifies scientific code
3. Semantic drift validation: For 50 randomly selected programs, compare original and adapted code functionality using automated unit testing and expert code review to quantify semantic drift during the self-debug loop