---
ver: rpa2
title: 'MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical
  Imaging Modalities'
arxiv_id: '2511.20650'
source_url: https://arxiv.org/abs/2511.20650
tags:
- detection
- medical
- medrov
- uni00000048
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedROV introduces the first real-time open-vocabulary detection
  model for medical imaging, addressing the limitation of traditional detectors that
  can only recognize predefined categories. The method curates Omnis, a large-scale
  dataset with 600K detection samples across nine imaging modalities, and incorporates
  BioMedCLIP for improved vision-language alignment.
---

# MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities

## Quick Facts
- arXiv ID: 2511.20650
- Source URL: https://arxiv.org/abs/2511.20650
- Reference count: 40
- Key outcome: 40 mAP50 improvement over previous foundation model, 3+ mAP50 improvement over closed-set detectors, 70 FPS inference speed

## Executive Summary
MedROV introduces the first real-time open-vocabulary detection model for medical imaging, addressing the limitation of traditional detectors that can only recognize predefined categories. The method curates Omnis, a large-scale dataset with 600K detection samples across nine imaging modalities, and incorporates BioMedCLIP for improved vision-language alignment. A pseudo-labeling strategy handles missing annotations from multi-source datasets. MedROV achieves significant performance improvements while maintaining real-time inference speeds.

## Method Summary
MedROV fine-tunes YOLO-World on Omnis, a dataset with 577K training images from 35 public datasets across nine imaging modalities. The method replaces CLIP with BioMedCLIP text encoder for domain-specific alignment and introduces pseudo-labeling to handle missing annotations from multi-source datasets. Training runs for 20 epochs on 4× NVIDIA A100 40GB GPUs with batch size 128, learning rate 0.0002, and weight decay 0.05. The model processes 3D volumes as 2D slices with specific intensity normalization per modality.

## Key Results
- 40 mAP50 improvement over previous foundation model (YOLO-World zero-shot)
- 3+ mAP50 improvement over closed-set detectors on Omnis
- Maintains 70 FPS inference speed while achieving state-of-the-art open-vocabulary performance

## Why This Works (Mechanism)

### Mechanism 1: Dataset-Class Presence Matrix for Missing Annotation Handling
- **Claim:** Pseudo-labeling unannotated objects prevents training penalization and improves learning efficiency when merging multi-source datasets.
- **Mechanism:** A matrix M categorizes each (dataset, class) pair as: (1) annotated, (0) possibly present but unannotated, or (-1) cannot exist. During training, predictions with low IoU to ground truth AND high confidence (≥0.9) are added as pseudo-labels for M[d,c]=0 cases.
- **Core assumption:** High-confidence detections that don't overlap ground truth represent valid unannotated objects rather than false positives.
- **Evidence anchors:** [Section 3.3] introduces the dataset-class presence matrix; [Table 4] shows pseudo-labeling improves Base+Novel mAP50 from 48.4 to 50.8.

### Mechanism 2: BioMedCLIP Text Encoder for Domain-Specific Vision-Language Alignment
- **Claim:** Replacing CLIP with BioMedCLIP text encoder improves novel class detection through better medical terminology alignment.
- **Mechanism:** BioMedCLIP, pretrained on 15M biomedical image-text pairs (PMC-15M), provides domain-specific text embeddings that better capture medical semantic relationships than generic CLIP.
- **Core assumption:** Domain-specific pretraining transfers to detection tasks despite different training objectives.
- **Evidence anchors:** [Section 3.3] uses BioMedCLIP as text encoder; [Table 4] shows BioMedCLIP improves novel class mAP50 from 48.0 to 48.4.

### Mechanism 3: BioMedCLIP Image Features for Semantic Correction
- **Claim:** Extracting visual features from predicted regions and replacing negative text embeddings improves generalization to unseen categories.
- **Mechanism:** For predictions on M[d,c]=1 or M[d,c]=-1 classes, high-confidence boxes are cropped, passed through BioMedCLIP image encoder, and the extracted features replace negative text embeddings.
- **Core assumption:** Visual features from BioMedCLIP provide better semantic grounding than text labels for ambiguous predictions.
- **Evidence anchors:** [Section 3.3] describes replacing text features with BioMedCLIP image features; [Table 4] shows full pipeline achieves 81.8 mAP50 (base) and 51.3 mAP50 (base+novel).

## Foundational Learning

- **Concept: Open-Vocabulary Object Detection (OVOD)**
  - **Why needed here:** Core paradigm shift from closed-set (fixed categories) to detecting any class described by text prompts.
  - **Quick check question:** Can you explain why a model trained on "lung, liver, heart" cannot detect "pneumonia" without retraining?

- **Concept: Vision-Language Alignment via Contrastive Learning**
  - **Why needed here:** MedROV uses region-text contrastive loss to align visual features with text embeddings, enabling zero-shot detection.
  - **Quick check question:** How does contrastive loss differ from classification loss, and why does it enable generalization to unseen classes?

- **Concept: Foundation Model Knowledge Transfer**
  - **Why needed here:** BioMedCLIP provides pretrained representations; understanding how/when this transfers is critical.
  - **Quick check question:** What is the difference between using BioMedCLIP as a frozen feature extractor vs. fine-tuning it end-to-end?

## Architecture Onboarding

- **Component map:** Input Image → YOLO Backbone → Multi-scale Features → RepVL-PAN (Vision-Language Fusion) → Detection Head → Bounding Boxes → [Training only] Pseudo-labeling + Feature Alignment → Text Labels → BioMedCLIP Text Encoder → Text Embeddings

- **Critical path:** Text encoder → RepVL-PAN fusion → Detection head. Errors in vision-language fusion cascade directly to detection quality.

- **Design tradeoffs:**
  - BioMedCLIP improves novel classes but slightly hurts base classes (tradeoff ~0.3 mAP50)
  - High confidence threshold (0.9) reduces noise but may miss valid pseudo-labels
  - Real-time speed (70 FPS) maintained but at cost of not using heavier architectures

- **Failure signatures:**
  - Near-zero mAP on medical data with vanilla YOLO-World → domain gap (need Omnis fine-tuning)
  - Model detects valid objects but gets penalized → missing annotations (enable pseudo-labeling)
  - Same label assigned to distinct structures → weak vision-language alignment (check BioMedCLIP integration)

- **First 3 experiments:**
  1. **Baseline sanity check:** Run vanilla YOLO-World zero-shot on Omnis validation set to quantify domain gap (expect ~0.02 mAP50 per Table 1).
  2. **Ablation on pseudo-labeling thresholds:** Sweep IoU threshold T ∈ [0.2, 0.4] and confidence C ∈ [0.7, 0.95] on a held-out subset to find optimal settings for your target modality.
  3. **Cross-modality generalization test:** Train on CT-only subset, evaluate on MRI/X-ray to measure modality gap and verify BioMedCLIP transfer effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MedROV framework be effectively extended to 2D and 3D medical image segmentation tasks without compromising real-time inference speeds?
- **Basis in paper:** [explicit] The conclusion states that future work will focus on "extending the approach to 2D and 3D segmentation tasks."
- **Why unresolved:** The current architecture is optimized for bounding box detection (OVOD) rather than dense pixel-wise prediction, and the paper does not propose a mechanism for mask generation.
- **What evidence would resolve it:** A modified MedROV architecture capable of outputting segmentation masks, benchmarked against state-of-the-art models like MedSAM on Dice scores and FPS.

### Open Question 2
- **Question:** How does MedROV generalize to a curated open-vocabulary test set containing categories significantly broader than those in Omnis?
- **Basis in paper:** [explicit] The authors identify the limitation of current benchmarks and state future work involves "curating an open-vocabulary test set with a broader range of categories."
- **Why unresolved:** The current evaluation relies on held-out classes from existing datasets, which may not sufficiently test the model's ability to detect truly novel or rare pathologies unseen during any stage of training.
- **What evidence would resolve it:** Evaluation of zero-shot performance on a new dataset comprising rare pathologies and anatomical structures excluded from the Omnis training distribution.

### Open Question 3
- **Question:** Does the conversion of 3D medical volumes into independent 2D slices degrade performance in detecting abnormalities that require spatial context?
- **Basis in paper:** [inferred] The methodology section notes that 3D modalities (CT, MRI) are processed into "in-plane 2D slices for consistency," potentially discarding inter-slice spatial information.
- **Why unresolved:** While the model performs well on 2D slices, it is unclear if treating slices independently hinders the detection of longitudinal abnormalities (e.g., fractures or vessel paths) that span multiple slices.
- **What evidence would resolve it:** A comparative study analyzing detection consistency across adjacent slices or a comparison against a 3D-aware detection baseline.

## Limitations

- Pseudo-labeling relies on high-confidence thresholds that may miss valid annotations while reducing false positives
- Domain adaptation from BioMedCLIP's scientific literature pretraining to diverse clinical imaging remains unproven
- Real-time performance (70 FPS) may not scale to larger models or more complex medical imaging scenarios

## Confidence

- **High confidence:** Dataset curation methodology and basic framework integration (YOLO-World + BioMedCLIP)
- **Medium confidence:** Pseudo-labeling effectiveness and missing annotation handling across heterogeneous datasets
- **Low confidence:** Generalization performance to completely unseen medical imaging domains beyond the nine curated modalities

## Next Checks

1. Cross-modal generalization test: Train exclusively on CT data, evaluate on MRI and X-ray to quantify modality-specific adaptation limits
2. Pseudo-label quality analysis: Manually inspect 100 high-confidence pseudo-labels to estimate false positive rates and refine threshold selection
3. Computational efficiency scaling: Measure inference time and memory usage when doubling input resolution or increasing model depth to validate 70 FPS claim across hardware configurations