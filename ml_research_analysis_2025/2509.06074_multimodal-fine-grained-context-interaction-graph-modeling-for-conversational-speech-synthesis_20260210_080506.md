---
ver: rpa2
title: Multimodal Fine-grained Context Interaction Graph Modeling for Conversational
  Speech Synthesis
arxiv_id: '2509.06074'
source_url: https://arxiv.org/abs/2509.06074
tags:
- prosody
- interaction
- speech
- word-level
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating natural prosody
  in conversational speech synthesis by modeling fine-grained semantic and prosodic
  interactions in multimodal dialogue history (MDH). While existing methods focus
  on utterance-level interactions, they overlook the critical role of word-level semantics
  and prosody in influencing subsequent utterances.
---

# Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis

## Quick Facts
- **arXiv ID**: 2509.06074
- **Source URL**: https://arxiv.org/abs/2509.06074
- **Reference count**: 16
- **One-line result**: MFCIG-CSS achieves N-DMOS of 3.980 (+0.122) and P-DMOS of 3.899 (+0.104) on DailyTalk, outperforming all baseline models in conversational speech synthesis.

## Executive Summary
This paper introduces MFCIG-CSS, a novel framework for conversational speech synthesis that explicitly models fine-grained semantic and prosodic interactions in multimodal dialogue history. Unlike previous utterance-level approaches, MFCIG-CSS constructs two specialized interaction graphs—a semantic interaction graph (SIG) and a prosody interaction graph (PIG)—to capture word-level interactions and their influence on subsequent utterances. The framework integrates these graphs with a FastSpeech 2-based speech synthesizer, enabling the generation of speech with more natural conversational prosody. Experiments on the DailyTalk dataset demonstrate significant improvements over baseline models across both subjective (DMOS) and objective (MAE, MCD) metrics.

## Method Summary
MFCIG-CSS addresses the limitations of utterance-level context modeling in conversational speech synthesis by constructing two parallel multimodal interaction graphs. The semantic interaction graph (SIG) captures word-level semantic interactions and their influence on subsequent utterances, while the prosody interaction graph (PIG) does the same for prosodic features. Both graphs use sequential GraphSAGE encoding to aggregate features forward through the dialogue, with special interaction nodes ($I_s$, $I_p$) that integrate dialogue-level context. These interaction features are then concatenated with target utterance features in a FastSpeech 2-based synthesizer. The framework uses frozen pre-trained encoders (TOD-BERT, Sentence-BERT, Wav2Vec2.0) to initialize graph nodes, avoiding end-to-end training complexity.

## Key Results
- MFCIG-CSS achieves N-DMOS of 3.980 (+0.122) and P-DMOS of 3.899 (+0.104) on DailyTalk dataset.
- Ablation study shows removing both SIG and PIG drops N-DMOS from 3.980 to 3.592 (-0.388), confirming the importance of interaction graphs.
- Objective metrics show MAE-P of 0.439 (+0.011) and MCD of 9.53 (+0.37), indicating improved prosody expressiveness.

## Why This Works (Mechanism)

### Mechanism 1: Word-Level Semantic-Prosody Interaction via Dual Graphs
The framework uses two parallel graph structures to encode complementary information: SIG aggregates word-level text and prosody features into utterance-level semantic nodes, while PIG does the same for prosody prediction. Each graph contains three branches for word-level semantic interaction, word-level prosody interaction, and an utterance-level backbone with special interaction nodes ($I_s$, $I_p$) that integrate features via average pooling. This design captures how key words in dialogue history carry semantic and prosodic signals that directly influence appropriate responses.

### Mechanism 2: Sequential Graph Convolution with Forward Message Passing
The framework employs sequential aggregation across dialogue turns using GraphSAGE, where each turn's features depend on the previous turn's output. This creates a directed flow where earlier utterances influence later representations through the equation $F^t_{i+1} = \text{SAGE}(F^t_i, W^t_{i,1\to q}, W^s_{i,1\to q})$. The special interaction nodes serve as sinks for aggregated dialogue-level features, enabling the model to capture cumulative context effects.

### Mechanism 3: Multimodal Feature Initialization from Pretrained Encoders
The framework leverages frozen pre-trained models (TOD-BERT, Sentence-BERT, Wav2Vec2.0) for node initialization, providing rich semantic and prosodic representations without requiring end-to-end joint training. Text nodes use TOD-BERT for word-level and Sentence-BERT for utterance-level features, while speech nodes use Wav2Vec2.0 with average pooling for word-level prosody and Wav2Vec2.0-IEMOCAP for utterance-level prosody.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Message Passing**
  - Why needed here: Understanding how SAGE aggregates neighbor nodes helps debug interaction graph failures and identify bottlenecks in feature propagation.
  - Quick check question: Can you explain how GraphSAGE differs from standard GCNs in handling inductive learning on unseen graphs?

- **Multimodal Representation Alignment**
  - Why needed here: SIG and PIG fuse text and speech features; misalignment between modalities (e.g., timing offsets, embedding space discrepancies) can degrade graph effectiveness.
  - Quick check question: Why might averaging frame-level Wav2Vec2.0 features to word-level lose critical prosodic timing information?

- **Autoregressive vs. Non-Autoregressive TTS (FastSpeech 2)**
  - Why needed here: The speech synthesizer uses FastSpeech 2 architecture; understanding its duration prediction and variance adaptor clarifies how interaction features ($I'_s$, $I'_p$) influence prosody.
  - Quick check question: Where in the FastSpeech 2 pipeline would you inject context-derived prosody features to affect pitch and energy prediction?

## Architecture Onboarding

- **Component map**: Input dialogue → Feature extraction (multimodal) → Graph initialization ($G_s$, $G_p$) → Sequential SAGE encoding → Average pooling to $I'_s$, $I'_p$ → Concatenation with target text features → Acoustic decoder → Vocoder → Speech output

- **Critical path**: The core processing flow begins with feature extraction from pretrained encoders, followed by graph construction and sequential encoding. The interaction nodes are then averaged and concatenated with target features before being passed to the FastSpeech 2 synthesizer.

- **Design tradeoffs**:
  - Frozen vs. fine-tuned encoders: Freezing reduces training cost but may limit adaptation to DailyTalk-specific prosody patterns.
  - Dual graphs vs. unified graph: Separate SIG/PIG allows modular ablation but doubles parameters; unified graph could enable cross-interactions at word level.
  - Sequential vs. bidirectional aggregation: Forward-only simplifies implementation but may miss backward context dependencies.

- **Failure signatures**:
  1. Flat prosody in synthesized speech: Check if $I'_p$ features have near-zero variance.
  2. Semantic drift in long dialogues: SIG may over-smooth features; inspect $F^t_i$ norms across turns.
  3. Mismatch between emotion and content: Wav2Vec2.0-IEMOCAP may generalize poorly to DailyTalk speaker styles.

- **First 3 experiments**:
  1. Ablation on graph depth: Reduce SAGE layers from default to 1 layer to test if deep message passing is necessary for DailyTalk's ~9-turn dialogues.
  2. Feature visualization: t-SNE plot of $I'_s$ and $I'_p$ embeddings colored by dialogue emotion category to verify semantic/prosodic clustering.
  3. Long-dialogue stress test: Synthetic dialogues with 15-20 turns to identify when sequential aggregation degrades.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed interaction graph modules be effectively integrated into non-parallel TTS architectures, such as VITS or discrete token-based systems?
  - Basis in paper: The authors state in Section 5 (Limitations) that MFCIG-CSS is currently implemented only based on the FastSpeech 2 architecture and plan to extend it to VITS-based and discrete token-based architectures in the future.

- **Open Question 2**: Does explicitly modeling finer-grained acoustic features (emotion, emphasis, pauses) as nodes in the interaction graphs significantly improve prosodic expressiveness over the current word-level modeling?
  - Basis in paper: Section 4 (Conclusion) and Section 5 (Limitations) note that the current model uses word-level features and does not yet incorporate specific acoustic attributes like emotion, emphasis, or pauses.

- **Open Question 3**: Is the MFCIG-CSS framework robust when applied to conversational datasets with more than two speakers or highly spontaneous, overlapping speech?
  - Basis in paper: The experiments are conducted exclusively on the DailyTalk dataset (Section 3.1), which consists of two-speaker dialogues with an average of 9 turns. The methodology does not discuss handling multi-party conversations or the noise typical of fully spontaneous speech.

## Limitations
- The framework relies on frozen pre-trained encoders, which may not capture domain-specific prosody patterns in DailyTalk.
- The sequential message passing assumes forward-flowing dialogue influence, potentially missing backward references common in natural conversations.
- No ablation study isolates the effect of word-level vs. utterance-level text features, making it unclear if word-level modeling specifically drives the improvements.

## Confidence
- **High Confidence**: The overall framework design (dual graphs + sequential aggregation) is logically sound and well-explained.
- **Medium Confidence**: The claim that word-level modeling specifically drives the gains is plausible but not directly validated by ablation.
- **Low Confidence**: The generalization of results to longer dialogues (>9 turns) or different domains is untested.

## Next Checks
1. Ablation on graph depth: Reduce SAGE layers from default to 1 layer to test if deep message passing is necessary for DailyTalk's ~9-turn dialogues.
2. Feature visualization: t-SNE plot of $I'_s$ and $I'_p$ embeddings colored by dialogue emotion category to verify semantic/prosodic clustering.
3. Long-dialogue stress test: Synthetic dialogues with 15-20 turns to identify when sequential aggregation degrades.