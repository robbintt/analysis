---
ver: rpa2
title: 'Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA'
arxiv_id: '2506.21569'
source_url: https://arxiv.org/abs/2506.21569
tags:
- property
- language
- retrieval
- prompt
- assertion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatically generating SystemVerilog
  Assertions (SVAs) from natural language property descriptions, a critical but labor-intensive
  task in hardware verification. To improve LLM performance in this task, the authors
  propose a customized retrieval-augmented generation (RAG) framework that combines
  dynamic splitting for database construction, HybridRetrieval for enhanced context
  retrieval, and an SVA operator-based rechecking mechanism for refining generated
  assertions.
---

# Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA

## Quick Facts
- **arXiv ID:** 2506.21569
- **Source URL:** https://arxiv.org/abs/2506.21569
- **Reference count:** 34
- **Key outcome:** Hybrid-NL2SVA achieves 59.05% functionality match improvement for lightweight models through integrated RAG and fine-tuning

## Executive Summary
This paper addresses the challenge of automatically generating SystemVerilog Assertions (SVAs) from natural language property descriptions in hardware verification. The authors propose a customized retrieval-augmented generation (RAG) framework that combines dynamic splitting for database construction, HybridRetrieval for enhanced context retrieval, and an SVA operator-based rechecking mechanism. They also develop a synthetic fine-tuning dataset with prompt-guided explanations that teach LLMs the layer-by-layer construction of SVAs. Experimental results demonstrate that their approach significantly outperforms baseline methods, particularly for lightweight models.

## Method Summary
The method combines two main approaches: a customized RAG framework and prompt-guided fine-tuning. The RAG framework uses dynamic splitting to construct a context-preserving database from textbooks, HybridRetrieval combining semantic and keyword-guided operator retrieval, and an SVA operator-based rechecking mechanism. For fine-tuning, they generate a synthetic dataset of 4,070 assertions with prompt-guided explanations that teach the layer-by-layer construction process of SVAs. The framework is evaluated on a dataset of 40 Verilog designs and 229 formally verified assertions using Cadence JasperGold formal verification tools.

## Key Results
- RAG framework improves functionality match by 58.42% over GPT-4o-mini
- Fine-tuned Qwen2.5-Coder-7B-Instruct with HybridRetrieval achieves 59.05% improvement in functionality match over base Qwen model
- The prompt-guided fine-tuning approach significantly outperforms standard fine-tuning for lightweight models

## Why This Works (Mechanism)

### Mechanism 1: Context-Preserving Database Construction
The Dynamic Splitting technique constructs a database by identifying code snippets in textbooks and grouping them with their immediately surrounding explanatory text (paragraph before + code + paragraph after). This ensures that when the model retrieves a code example, it also retrieves the human-readable explanation of that code, preserving the mapping between syntax and semantics.

### Mechanism 2: Keyword-Guided Operator Retrieval (HybridRetrieval)
The system first prompts an LLM to extract keywords from the user's query, maps these keywords to specific SVA operators, and retrieves documentation/examples specifically for those operators. This is combined with global semantic search to provide a dual-view of the requirements, addressing the limitation of standard semantic search in capturing precise temporal logic requirements.

### Mechanism 3: Layer-by-Layer Synthesis Reasoning (Prompt-Guided Finetuning)
The fine-tuning dataset uses prompt-guided explanations that decompose the generation of an SVA into a step-by-step derivation. This effectively fine-tunes the model on the reasoning process required to build valid temporal logic, rather than just memorizing syntax patterns, teaching lightweight models the hierarchical construction process of assertions.

## Foundational Learning

- **Concept: SystemVerilog Assertion (SVA) Hierarchy**
  - Why needed here: The paper's entire fine-tuning strategy is predicated on the 4-layer structure of concurrent assertions (Boolean, Sequence, Property, Verification).
  - Quick check question: In the assertion `assert property (@(posedge clk) $rose(req) |=> ack);`, which layer does `$rose(req)` belong to?

- **Concept: Temporal Logic Operators (Implications)**
  - Why needed here: The paper explicitly identifies the confusion between overlapping (`|->`) and non-overlapping (`|=>`) implications as a primary failure mode.
  - Quick check question: If an antecedent matches in cycle `T`, does the consequent for `|->` evaluate in cycle `T` or `T+1`?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The system relies on "Dynamic Splitting" and "HybridRetrieval." Distinguishing between standard vector search and the paper's hybrid approach is necessary to understand performance gains.
  - Quick check question: Why might a standard semantic vector search struggle to find relevant context for the phrase "signal value from two cycles ago"?

## Architecture Onboarding

- **Component map:** Input (Verilog + NL) -> Keyword Extractor -> Operator Mapper -> Vector DB (Code-Centric Chunks) -> Context Combiner -> LLM (GPT-4o-mini or Qwen-7B) -> Initial SVA -> Operator Extractor -> Rechecker Prompt -> Refined SVA
- **Critical path:** The Prompt-Guided Finetuning data. The largest jump in the lightweight model (Qwen) comes from the fine-tuning dataset that teaches the layer-by-layer reasoning. If this dataset is low quality or hallucinated, the model's core ability to generate functional SVAs collapses.
- **Design tradeoffs:** The HybridRetrieval and SVA Rechecking mechanisms require multiple inference calls per assertion, improving functionality match but significantly increasing latency compared to single-shot generation. The paper assumes lightweight models need explicit reasoning traces, while larger models may achieve similar results with just RAG.
- **Failure signatures:** Timing mismatch (using `|->` instead of `|=>`), keyword blindness (failing to retrieve `$past()` context due to synonym use), finetuning regression (valid for simple cases but failing on unseen operators).
- **First 3 experiments:** 1) Retrieval Ablation: test global semantic search vs keyword-guided search on 20 designs. 2) Rechecker Validation: manually inspect 10 cases where rechecking modified initial generation. 3) Few-Shot vs Finetuning: compare Qwen-7B (finetuned) against baseline using Hybrid-RAG as few-shot prompt.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the SVA operator-based rechecking mechanism degrade Functionality Match (FM) for the base Qwen model despite improving Syntax Correctness (SC)? The paper shows that applying SOR to base Qwen increases SC (70.31% to 87.77%) but decreases FM (45.85% to 44.10%).

### Open Question 2
To what extent do potential reasoning errors or hallucinations in the GPT-4o-mini generated explanations propagate to the fine-tuned student models? The fine-tuning dataset consists of synthetic prompt-guided explanations produced by OpenAI o4-mini.

### Open Question 3
How does the reliance on textbook-derived code databases limit the framework's generalizability to industrial-scale or proprietary hardware designs? The retrieval database is constructed by scraping 67 hardware-design textbooks, which typically contain pedagogical rather than industrial examples.

## Limitations

- The evaluation relies on Cadence JasperGold formal verification tools, which are proprietary and require expensive licenses, creating a barrier to independent validation.
- The 229-assertion evaluation dataset and the specific 10 textbooks used for database construction are not publicly available, making direct reproduction challenging.
- The framework's latency requirements are not discussed, though multiple inference calls could make it impractical for interactive design flows.

## Confidence

- **High Confidence:** Claims about context-preserving database construction are well-supported by methodology description and logical reasoning about semantic links between code and explanations.
- **Medium Confidence:** HybridRetrieval effectiveness is supported by experimental results, but specific contribution of each component could be better isolated.
- **Medium Confidence:** Prompt-guided fine-tuning approach shows strong results, but synthetic dataset generation could introduce bias, and improvements may be partially due to dataset quality.

## Next Checks

1. **Dataset Validation:** Manually inspect 50 randomly selected assertions from the 4,070-synthetic fine-tuning dataset to verify prompt-guided explanations accurately capture layer-by-layer construction process.
2. **Operator Retrieval Precision:** Test keyword extraction and operator mapping pipeline on 30 assertions containing ambiguous temporal language to measure precision in selecting correct SVA operators.
3. **Latency Benchmarking:** Measure end-to-end latency for generating 50 assertions using complete Hybrid-NL2SVA pipeline versus single-shot GPT-4o-mini generation to quantify practical performance tradeoff.