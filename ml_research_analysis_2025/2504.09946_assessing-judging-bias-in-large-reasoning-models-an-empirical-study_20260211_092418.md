---
ver: rpa2
title: 'Assessing Judging Bias in Large Reasoning Models: An Empirical Study'
arxiv_id: '2504.09946'
source_url: https://arxiv.org/abs/2504.09946
tags:
- bias
- reasoning
- lrms
- datasets
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Reasoning Models (LRMs) remain vulnerable to evaluation biases
  despite their advanced reasoning capabilities. While LRMs show improved robustness
  on fact-related content compared to LLMs, they exhibit significant position bias
  and a novel "superficial reflection bias" where phrases mimicking reasoning (e.g.,
  "wait, let me think...") disproportionately influence judgments toward later options.
---

# Assessing Judging Bias in Large Reasoning Models: An Empirical Study

## Quick Facts
- **arXiv ID**: 2504.09946
- **Source URL**: https://arxiv.org/abs/2504.09946
- **Reference count**: 11
- **Primary result**: Large Reasoning Models remain vulnerable to evaluation biases despite advanced reasoning capabilities

## Executive Summary
This empirical study systematically evaluates judging biases in Large Reasoning Models (LRMs) across four cognitive bias types: bandwagon, authority, position, and distraction. While LRMs demonstrate improved robustness on fact-related content compared to traditional LLMs, they exhibit significant position bias and a novel "superficial reflection bias" where reasoning-mimicking phrases disproportionately influence judgments toward later options. The research evaluates three mitigation strategies—specialized system prompts, in-context learning, and self-reflection—finding varying effectiveness across bias types and task categories, with self-reflection showing particular promise for factual content due to LRMs' stronger reasoning capabilities.

## Method Summary
The study employs controlled experimental design using four DPO datasets (Emerton-DPO, Orca-DPO, Py-DPO, Truthy-DPO) and fact-related datasets from MMLU-Pro (Math, Chemistry, History, Psychology). Models are evaluated under bias injection conditions (bandwagon, authority, position, distraction) with temperature set to 0.7. Bias injection templates include appending "90% people believe..." for bandwagon, citation-like authority markers, option position swaps, and distraction sentences. Three mitigation strategies are tested: targeted system prompts, in-context learning with 5 examples per bias type, and self-reflection prompts. Performance is measured using Accuracy (alignment with ground truth) and Robustness Rate (consistency before/after bias injection), with results averaged over three runs.

## Key Results
- LRMs exhibit a novel "superficial reflection bias" where phrases like "wait, let me think..." significantly influence judgments toward later options
- Position bias remains a significant vulnerability across LRMs, with models preferring options appearing after reasoning cues
- Self-reflection reduces biases by up to 10% in preference datasets and 16% in fact-related datasets
- Specialized system prompts reduce biases by up to 19% in preference alignment datasets and 14% in fact-related datasets
- In-context learning provides up to 27% improvement on preference tasks but inconsistent results on factual tasks

## Why This Works (Mechanism)

### Mechanism 1: Superficial Reflection Bias
- Claim: LRMs disproportionately favor options appearing after reasoning-mimicking phrases because they interpret preceding content as intermediate reasoning steps
- Core assumption: The model has learned a positional heuristic where recency after reasoning cues correlates with deliberateness
- Evidence anchors: Abstract identifies this novel bias; Section 3.3 Investigation demonstrates significant preference shifts with reasoning-interjection phrases; Figure 2 shows insertion of "wait, wait, wait... let me think about it" shifts preference toward later answers

### Mechanism 2: Self-Reflection Mitigation Leverages LRM Reasoning Architecture
- Claim: Self-reflection prompts reduce biases more effectively in LRMs than LLMs because they engage the model's native multi-step reasoning processes
- Core assumption: The model's reasoning capabilities are sufficient to identify its own biases when explicitly prompted to audit its process
- Evidence anchors: Abstract notes 11% improvement on bandwagon bias and 9% on authority bias with self-reflection; DS-R1 achieves 22% improvement on authority bias; "From Noisy Traces to Stable Gradients" supports reasoning traces as central to LRM architecture

### Mechanism 3: Task-Type Moderates Robustness—Fact-Related vs. Preference Content
- Claim: LRMs exhibit greater bias robustness on fact-related datasets than on subjective preference datasets because objective content provides verifiable anchors for reasoning
- Core assumption: The model's parametric knowledge is sufficiently accurate for the factual domain being evaluated
- Evidence anchors: Abstract states LRMs demonstrate better robustness on fact-related datasets; Section 3.4 shows DS-R1 experiences 18% accuracy decrease from distraction in Emerton-DPO but only 4% fluctuation in fact-related datasets; self-reflection yields 8% average gain on bandwagon bias in Chemistry vs. only 10% in Truthy-DPO

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: The entire paper evaluates how models perform when deployed as automated evaluators
  - Quick check question: Can you explain why position bias matters more in a pairwise comparison judge than in a single-answer generation task?

- Concept: **Chain-of-Thought and Reasoning Traces**
  - Why needed here: LRMs are distinguished by generating intermediate reasoning steps; the "superficial reflection bias" exploits this architectural feature
  - Quick check question: What is the difference between a model that outputs reasoning tokens and one that doesn't, in terms of vulnerability to reasoning-mimicking text?

- Concept: **Robustness Rate (RR) vs. Accuracy**
  - Why needed here: The paper uses both metrics—accuracy measures correctness, RR measures consistency post-bias-injection
  - Quick check question: If a model maintains high RR but low accuracy after bias injection, what does that imply about its bias susceptibility?

## Architecture Onboarding

- Component map: Judge Model (M) -> Bias Injection Module -> Mitigation Layer -> Evaluation Engine
- Critical path: 1. Baseline judgment collection → 2. Bias injection → 3. Post-injection judgment → 4. Metric computation → 5. Mitigation strategy application → 6. Re-evaluation
- Design tradeoffs:
  - System prompts: Reliable across bias types (~14-19% reduction) but require prior knowledge of which biases exist
  - In-context learning: Strongest on preference tasks (up to 27%) but highly variable on factual tasks; effectiveness depends on example quality and model architecture
  - Self-reflection: Best for LRMs on factual content (16% reduction); no bias-specific knowledge required, but less effective for LLMs and preference tasks
- Failure signatures:
  - Self-reflection on LLMs: Minimal improvement (LLMs lack native multi-step reasoning)
  - ICL on factual tasks with weak models: Negative gains possible (e.g., GPT-4o: -13% on Chemistry bandwagon bias with ICL)
  - Position bias mitigation with superficial reasoning cues present: Standard position prompts insufficient; must explicitly address reflection-mimicking text
- First 3 experiments:
  1. Establish baseline bias susceptibility: Run target LRM on Truthy-DPO and a factual dataset with all four bias injections; record Accuracy and RR to identify dominant vulnerability
  2. Test mitigation-by-strategy interaction: Apply each mitigation (system prompt, ICL, self-reflection) separately; compare which strategy most effectively addresses your model's dominant bias
  3. Probe superficial reflection bias: Insert "wait, let me think..." variants between options in pairwise comparisons; measure shift toward later-position preference to confirm this bias exists in your model family

## Open Questions the Paper Calls Out

- How do the identified judging biases in LRMs manifest in complex, real-world evaluation scenarios beyond the controlled experimental settings?
  - Basis: Study focuses on controlled settings rather than complex real-world applications
  - Why unresolved: Controlled benchmarks may not capture nuance of actual deployment contexts
  - What evidence would resolve it: Testing LRM judges in authentic deployment scenarios with human evaluation comparison

- Can more comprehensive bias mitigation methods be developed that specifically target LRM reasoning mechanisms rather than relying on prompts or post-hoc corrections?
  - Basis: Conclusion states hope this work will benefit development of new bias mitigation methods specifically tailored to LRMs
  - Why unresolved: Current approaches show variable effectiveness and do not address root causes within LRM reasoning processes
  - What evidence would resolve it: Development and evaluation of training-level interventions or architecture modifications addressing bias formation during reasoning process itself

## Limitations

- The "superficial reflection bias" mechanism assumes uniform model training on reasoning structure, but architectural variations between LRM families may modulate susceptibility differently
- Cross-dataset generalizability is constrained by specific DPO and MMLU-Pro fact datasets used; performance on specialized or domain-specific tasks may diverge substantially
- The interaction between mitigation strategies (e.g., combining system prompts with self-reflection) remains unexplored, potentially leaving synergistic effects unaccounted for

## Confidence

- **High Confidence**: Position bias findings and self-reflection effectiveness on fact-related tasks are well-supported by consistent numerical improvements across multiple models
- **Medium Confidence**: The superficial reflection bias mechanism is plausible but requires further validation across diverse LRM architectures beyond DeepSeek
- **Low Confidence**: Task-type moderation claims lack strong corpus support and may be dataset-specific rather than a general principle

## Next Checks

1. **Architectural Validation**: Test superficial reflection bias across diverse LRM families (e.g., OpenAI o-series, Claude) to confirm whether the positional heuristic is universal or model-specific

2. **Mitigation Synergy Testing**: Apply combined mitigation strategies (e.g., self-reflection + targeted system prompt) to measure potential additive or multiplicative effects

3. **Domain Generalization**: Evaluate LRMs on niche factual datasets (e.g., medical or legal reasoning) to test whether task-type moderation holds under domain-specific expertise requirements