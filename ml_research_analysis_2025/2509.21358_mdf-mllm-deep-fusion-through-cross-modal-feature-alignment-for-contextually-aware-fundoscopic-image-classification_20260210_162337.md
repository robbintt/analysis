---
ver: rpa2
title: 'MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually
  Aware Fundoscopic Image Classification'
arxiv_id: '2509.21358'
source_url: https://arxiv.org/abs/2509.21358
tags:
- mllm
- fusion
- u-net
- disease
- mdf-mllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately classifying retinal
  diseases from fundus images using multimodal AI, specifically targeting the limitations
  of existing models in capturing fine-grained spatial features critical for diagnosing
  conditions like glaucoma, diabetic retinopathy, and retinitis pigmentosa. The proposed
  MDF-MLLM architecture integrates a U-Net segmentation model with a multimodal large
  language model (LLaMA 3.2 11B Vision-Instruct) through multi-depth cross-attention
  fusion, enabling the model to combine low-level image details with high-level contextual
  reasoning.
---

# MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification

## Quick Facts
- **arXiv ID:** 2509.21358
- **Source URL:** https://arxiv.org/abs/2509.21358
- **Reference count:** 40
- **Primary result:** MDF-MLLM achieves 94% accuracy (56% improvement over baseline) in classifying fundus images as acquired vs. inherited retinal diseases.

## Executive Summary
This study presents MDF-MLLM, a multimodal AI architecture that significantly improves the classification of retinal diseases from fundus images by integrating a U-Net segmentation model with a multimodal large language model (LLaMA 3.2 11B Vision-Instruct) through multi-depth cross-attention fusion. The architecture addresses the limitation of existing models in capturing fine-grained spatial features critical for diagnosing conditions like glaucoma, diabetic retinopathy, and retinitis pigmentosa. By injecting U-Net skip connections into MLLM cross-attention layers and using FiLM-based modulation, MDF-MLLM combines low-level image details with high-level contextual reasoning, achieving state-of-the-art performance on a dataset of 1,305 fundus image-text pairs.

## Method Summary
MDF-MLLM fuses a U-Net segmentation model with LLaMA 3.2 11B Vision-Instruct through a multi-depth cross-attention mechanism. The U-Net encoder's skip connections (from four layers) are projected into the MLLM embedding space (d=4096) and injected into specific cross-attention blocks (layers 3, 18, 28, 38). Feature-wise Linear Modulation (FiLM) is applied to synchronize semantic context between the U-Net and MLLM, with MLLM outputs generating modulation vectors that affine-transform U-Net skip features. The model is fully fine-tuned end-to-end on 1,305 fundus image-text pairs from three datasets, achieving binary classification (Acquired vs. Inherited diseases) through generated text.

## Key Results
- MDF-MLLM achieves 94% accuracy, representing a 56% improvement over the baseline MLLM (60% accuracy).
- Recall and F1-scores improve by up to 67% and 35%, respectively, with the highest gains observed for inherited diseases with rich clinical metadata.
- Ablation studies confirm the effectiveness of the multi-scale fusion approach, with fully unfrozen training outperforming frozen variants (accuracy ranging from 37% to 78% when components are frozen).

## Why This Works (Mechanism)

### Mechanism 1
Injecting U-Net skip connections into MLLM cross-attention layers enhances low-level spatial feature retention critical for medical image classification. Skip connections from a ResNet50-U-Net are patched, projected into the MLLM embedding space (d=4096), and fused with vision features via scaled cross-attention before entering the language decoder. This assumes the MLLM's native vision encoder loses fine-grained spatial details necessary for diagnosing conditions like retinitis pigmentosa, and U-Net features provide complementary priors.

### Mechanism 2
Bidirectional feature modulation via FiLM synchronizes semantic context between the U-Net and MLLM. MLLM cross-attention outputs are pooled to generate modulation vectors (γ, β) that affine-transform U-Net skip connections, conditioning the segmentation features on high-level text context before decoding. This assumes high-level semantic understanding from text/image context should refine low-level feature extraction to ensure spatial features are relevant to the diagnostic task.

### Mechanism 3
Full fine-tuning of both U-Net and MLLM components is necessary to achieve reported performance gains; freezing components breaks the alignment. End-to-end training allows fusion blocks to adjust both the spatial feature extractor and language decoder simultaneously. This assumes pre-trained weights of individual models are not inherently aligned for this specific cross-modal task, and freezing prevents necessary feature space adaptation.

## Foundational Learning

- **Concept: U-Net Skip Connections**
  - **Why needed here:** Architecture relies on capturing multi-scale features (edges, vessels) from the encoder and passing them to fusion blocks.
  - **Quick check question:** Can you explain why simply taking the final output of a CNN encoder might fail to preserve the spatial resolution needed for small lesion detection?

- **Concept: Cross-Attention in Transformers**
  - **Why needed here:** This is the bridge allowing U-Net's visual features to query MLLM's embedding space.
  - **Quick check question:** In a cross-attention block, which modality provides the Query (Q) and which provides the Key (K) and Value (V) in this specific implementation? (Hint: See Equations 4-6).

- **Concept: Feature-wise Linear Modulation (FiLM)**
  - **Why needed here:** Used to "modulate" U-Net features based on MLLM's understanding of the text context.
  - **Quick check question:** How does applying a scaling factor (γ) and a shift factor (β) to a feature map change its information content without altering its spatial dimensions?

## Architecture Onboarding

- **Component map:**
  Input: Fundus Image (resized to 512x512) + Clinical Text Prompt
  Visual Stream: ResNet50-U-Net Encoder -> Skip Connections (Layers 1-4)
  Language Stream: LLaMA 3.2 11B Vision Encoder -> Transformer Layers
  Fusion Interface: Cross-Attention Fusion (injects U-Net patches into MLLM layers 3, 18, 28, 38) + U-Net Fusion (FiLM modulates U-Net skip features using MLLM embeddings)
  Output: Binary classification (Acquired vs. Inherited) via generated text

- **Critical path:** Alignment relies on the projection layer mapping U-Net features to MLLM dimension (d=4096). If this projection is biased or untrained, cross-attention receives garbage data.

- **Design tradeoffs:**
  - Training Speed Mismatch: U-Net optimized at epoch 18 while MLLM required 55 epochs. Engineers must balance early stopping for U-Net vs. starvation for MLLM.
  - Frozen vs. Unfrozen: Freezing saves compute but dropped accuracy to 37% in worst-case configurations.

- **Failure signatures:**
  - Hallucination: Base MLLM misclassified inherited diseases (e.g., predicting "Best Disease" when it was "AR Retinitis Pigmentosa").
  - "None" Classification: Model generates text that fails to mention any disease category.
  - Gradient Misalignment: If fusion signals are too strong (high α), they may disrupt pre-trained MLLM knowledge.

- **First 3 experiments:**
  1. Baseline Validation: Run unmodified LLaMA 3.2 11B Vision model on dataset to reproduce 60% accuracy baseline.
  2. Ablation on Freezing: Train three variants: (a) Frozen U-Net/Unfrozen MLLM, (b) Unfrozen U-Net/Frozen MLLM, (c) Both Unfrozen. Compare convergence speeds and final accuracy to verify Table 1 results.
  3. Epoch Monitoring: Track validation loss for U-Net segmentation and MLLM classification separately to identify exact epoch where "training speed mismatch" occurs and adjust learning rate schedulers.

## Open Questions the Paper Calls Out
- How to optimize the training speed mismatch between U-Net (converges at epoch 18) and MLLM (requires 55 epochs) components?
- How to extend the model's capabilities to segmentation tasks beyond classification?

## Limitations
- Reported 94% accuracy is based on a relatively small dataset (1,305 images) with binary classification; generalization to larger, multi-class clinical settings remains unproven.
- Multi-depth fusion approach introduces significant architectural complexity without theoretical justification for why specific layers (3, 18, 28, 38) were chosen.
- Training speed mismatch between U-Net and MLLM suggests potential optimization instability, though detailed convergence analysis is not provided.

## Confidence
- **High Confidence:** Ablation study showing fully unfrozen training outperforms frozen variants is well-supported by quantitative evidence (Table 1).
- **Medium Confidence:** Core mechanism of U-Net skip connection injection into cross-attention layers is theoretically sound with supporting empirical results, though exact contribution of each fusion component is not isolated.
- **Low Confidence:** Clinical significance of classification improvements and model's robustness to real-world variations (different imaging devices, patient demographics) are not adequately addressed.

## Next Checks
1. Validate MDF-MLLM on a larger, multi-class fundus image dataset (e.g., IDRiD or MESSIDOR) to assess generalization beyond binary classification.
2. Conduct an ablation study isolating the contribution of cross-attention fusion vs. FiLM modulation to determine which mechanism drives performance gains.
3. Evaluate the model on fundus images from different imaging devices and clinical centers to assess robustness to domain shift and ensure clinical applicability.