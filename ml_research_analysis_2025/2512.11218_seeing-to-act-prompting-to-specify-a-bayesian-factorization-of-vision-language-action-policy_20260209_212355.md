---
ver: rpa2
title: 'Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language
  Action Policy'
arxiv_id: '2512.11218'
source_url: https://arxiv.org/abs/2512.11218
tags:
- language
- action
- object
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Bayesian factorization approach for Vision-Language-Action
  (VLA) models to address catastrophic forgetting and language grounding issues. The
  key idea is to decompose the VLA policy into a vision-action prior and a language-conditioned
  likelihood, which naturally handles modality imbalance in VLA datasets.
---

# Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy

## Quick Facts
- arXiv ID: 2512.11218
- Source URL: https://arxiv.org/abs/2512.11218
- Authors: Kechun Xu; Zhenjie Zhu; Anzhe Chen; Shuqi Zhao; Qing Huang; Yifei Yang; Haojian Lu; Rong Xiong; Masayoshi Tomizuka; Yue Wang
- Reference count: 40
- Key outcome: Bayesian factorization approach for VLA models addresses catastrophic forgetting and language grounding issues, showing superior generalization on simulation benchmarks and real-world tasks.

## Executive Summary
This paper addresses catastrophic forgetting and language grounding issues in Vision-Language-Action (VLA) models by decomposing the policy into a vision-action prior and a language-conditioned likelihood. The approach naturally handles modality imbalance in VLA datasets where language diversity is low relative to visual/action diversity. By training the prior first on vision-action pairs and then the likelihood on language-augmented data, the method preserves language grounding while leveraging pre-trained foundation models. The factorization is instantiated for both pre-contact (pose prediction) and post-contact (trajectory generation) manipulation phases.

## Method Summary
The method factorizes the VLA policy π(a|v,ℓ) ∝ π_p(a|v) × L(ℓ|v,a) using a two-stage training approach. First, a vision-action prior π_p(a|v) is trained on balanced vision-action pairs without language interference. Second, a language-conditioned likelihood L(ℓ|v,a) is trained on (v,a,ℓ) triples while freezing the prior, forcing the model to use language rather than ignore it. For pre-contact tasks, the prior uses AnyGrasp to generate grasp pose candidates, while the likelihood selects via text-aware vision tokens. For post-contact tasks, a diffusion-based trajectory generator serves as the prior, with language injected through additional cross-attention layers in the likelihood.

## Key Results
- Outperforms existing methods on simulation benchmarks (LIBERO, rigid object pick-place, articulated object manipulation) with up to 30% improvement on unseen tasks
- Demonstrates superior generalization to unseen objects and environments compared to single-stage approaches
- Successfully preserves language grounding while leveraging pre-trained foundation models, avoiding the shortcut learning that plagues joint training approaches

## Why This Works (Mechanism)

### Mechanism 1: Prior-Likelihood Factorization Separates Action Learning from Language Grounding
The decomposition mitigates shortcut learning caused by modality imbalance in VLA datasets. The prior π_p(a|v) learns visuomotor affordances from balanced vision-action pairs without language interference, while the likelihood L(ℓ|v,a) aligns pre-computed action proposals with instructions using lightweight adaptation. This separation prevents the model from ignoring language when trained jointly, as language diversity is structurally lower than visual/action diversity.

### Mechanism 2: Two-Stage Training Enforces Sequential Dependency
Training the prior first, then the likelihood with frozen prior embeddings, forces the model to use language rather than ignore it. Stage 1 learns θ_VA minimizing L_VA ≈ -E[log π_p(a|v)]. Stage 2 minimizes -E[log π(a|f_θVA(v), ℓ)] where gradients must flow through language conditioning to improve loss below the vision-only shortcut.

### Mechanism 3: Pre/Post-Contact Decomposition Leverages Specialized Foundation Models
Separating manipulation into pre-contact (pose prediction) and post-contact (trajectory generation) enables use of pre-trained action primitives with different data availabilities. Pre-contact uses off-the-shelf grasp models (AnyGrasp) as priors generating discrete pose candidates; likelihood selects via cross-attention over text-aware vision tokens. Post-contact trains diffusion prior on trajectories, then late-injects language through additional attention layers.

## Foundational Learning

- **Bayes' Rule and Probabilistic Factorization**: Why needed here: The core insight is P(a|v,ℓ) ∝ P(a|v) × P(ℓ|v,a). Understanding how priors and likelihoods compose is essential to grasp why sequential training works.
  - Quick check question: Given π(a|v,ℓ) ∝ π_p(a|v) × L(ℓ|v,a), what happens to the posterior if L(ℓ|v,a) is uniform?

- **Mutual Information and Conditional Entropy**: Why needed here: Section V proves shortcut learning via I(a;ℓ|v) ≤ H(ℓ|v) ≤ ε. Reading these derivations requires comfort with I(X;Y|Z) = H(X|Z) - H(X|Y,Z).
  - Quick check question: If H(ℓ|v) = 0 (language fully determined by vision), what is I(a;ℓ|v)?

- **Diffusion Models for Trajectory Generation**: Why needed here: Post-contact prior uses DDPM/DDIM with DiT backbone. Understanding denoising objectives, FiLM conditioning, and action chunking is required to implement or modify the architecture.
  - Quick check question: Why does the paper use 100-step DDPM for training but 20-step DDIM for inference?

## Architecture Onboarding

- **Component map:**
  - Pre-contact Prior: AnyGrasp (frozen) → N grasp pose candidates (6-DoF + confidence)
  - Pre-contact Likelihood: CLIP ViT-L/14 → vision patches + text tokens → text-aware weighting → 4× cross-attention blocks with 2D RoPE → softmax over N candidates
  - Post-contact Prior: Multi-view CLIP tokens → 4× self-attention + RayPE → DiT (4× self+cross) → action head predicts 16-32 step chunks
  - Post-contact Likelihood: Frozen prior + 4 additional self+cross attention layers → text-aware vision + text tokens injected via cross-attention

- **Critical path:**
  1. Collect VLA demonstrations with language annotations
  2. Train post-contact prior on vision-action pairs (400k iterations, language-agnostic)
  3. Freeze prior, add likelihood layers, train on remaining data (200k iterations)
  4. For pre-contact: skip prior training, use AnyGrasp directly; train likelihood only

- **Design tradeoffs:**
  - R1 vs. R2 pre-training: R1 for small datasets with limited language diversity, R2 for large datasets ≥1k trajectories with distribution shift
  - CLIP vs. Qwen tokens: CLIP outperforms Qwen, likely due to stricter vision-text alignment in shared embedding space
  - Classifier guidance vs. latent adaptation: Classifier guidance fails catastrophically due to poor OOD generalization of learned classifiers

- **Failure signatures:**
  - Language grounding failure: Policy grasps wrong object with correct motion → check if text-aware vision tokens are computed, verify cross-attention receives both vision and text
  - Prior collapse: Low action diversity, mode collapse → check prior training data split, ensure likelihood trains on held-out (v,a,ℓ) tuples
  - Grasp pose selection failure: Valid poses rejected → inspect pre-contact likelihood softmax, verify RoPE encoding matches training
  - Trajectory drift: Smooth motion but wrong direction → check if FiLM conditioning on history actions is active, verify proprioception injection

- **First 3 experiments:**
  1. Train single-stage VLA (prior+likelihood jointly) vs. two-stage on LIBERO-Object. Measure success rate on unseen instructions. Expect >15% gap.
  2. Ablate text-aware vision: Remove s_i weighting in Eq. 4 for pre-contact likelihood. Expect UO performance drop.
  3. Apply R1 to rigid object pick-place (should underperform) and R2 to LIBERO (should overfit limited language). Verify R1 is for H(ℓ|v)≈0 datasets, R2 for H(ℓ|v)>0.

## Open Questions the Paper Calls Out
- How can the vision–action prior be unified with large-scale VLMs to improve scalability?
- Does integrating explicit memory mechanisms and tactile representations enhance performance on long-horizon, contact-rich manipulation tasks?
- Can incorporating observation history prevent failures in long-horizon tasks where the visual state is occluded?

## Limitations
- Performance advantage demonstrated primarily on synthetic benchmarks with controlled language variation, limiting evidence for real-world robustness
- Single real-robot experiment provides limited evidence for deployment across diverse physical environments and sensor modalities
- Claims about catastrophic forgetting mitigation rely on assumption of low language diversity that may not hold in all real-world datasets

## Confidence
- **High confidence**: Mathematical foundation of Bayes' rule factorization and two-stage training mechanism that enforces sequential dependency
- **Medium confidence**: Generalizability of pre/post-contact decomposition approach and CLIP ViT-L/14 performance claims
- **Medium confidence**: Claims about language grounding preservation across semantic paraphrases

## Next Checks
1. Test the method on a real-world VLA dataset with high language diversity (e.g., ALFRED) to verify factorization remains beneficial when H(ℓ|v) approaches H(ℓ)
2. Evaluate policy performance when transferred between substantially different visual domains (simulation to real, different robot platforms) while maintaining language grounding
3. Systematically vary instruction phrasing while keeping underlying task constant to quantify policy's sensitivity to linguistic variations and verify language grounding preservation across semantic paraphrases