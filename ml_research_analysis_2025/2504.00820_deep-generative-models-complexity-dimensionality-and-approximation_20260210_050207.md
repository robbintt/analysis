---
ver: rpa2
title: 'Deep Generative Models: Complexity, Dimensionality, and Approximation'
arxiv_id: '2504.00820'
source_url: https://arxiv.org/abs/2504.00820
tags:
- distribution
- manifold
- dimension
- network
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical underpinnings of how generative
  models approximate distributions on Riemannian manifolds, especially when the input
  dimension is less than the manifold's intrinsic dimension. Prior work assumed the
  input dimension must be at least d+1 for a d-dimensional manifold, but this work
  shows that even a one-dimensional input can approximate such distributions using
  space-filling curve concepts.
---

# Deep Generative Models: Complexity, Dimensionality, and Approximation

## Quick Facts
- **arXiv ID**: 2504.00820
- **Source URL**: https://arxiv.org/abs/2504.00820
- **Reference count**: 15
- **Primary result**: Deep generative models can approximate distributions on a d-dimensional manifold using input dimension m < d, but required network width grows super-exponentially with decreasing approximation error.

## Executive Summary
This paper investigates the theoretical foundations of deep generative models when approximating distributions on Riemannian manifolds, particularly when the input dimension is smaller than the manifold's intrinsic dimension. The authors demonstrate that while traditional theory requires input dimension m ≥ d+1 for a d-dimensional manifold, their results show that even a one-dimensional input can approximate such distributions through space-filling curve concepts. The key finding reveals a fundamental trade-off: when the input dimension is underestimated, the required network width grows super-exponentially with approximation error, creating practical limitations for implementation.

## Method Summary
The authors analyze deep generative models through the lens of optimal transport theory and regularity of Monge-Ampère equations. They prove approximation guarantees for ReLU networks by studying the pushforward of input distributions through the generator network. The theoretical framework examines how network width requirements scale with approximation error and input dimension, establishing explicit bounds that reveal the super-exponential growth when m < d. Empirical simulations validate these theoretical findings by training networks to approximate distributions on unit squares, cylinders, and cubes with varying input dimensions.

## Key Results
- Deep generative models can approximate d-dimensional manifolds using input dimension m < d (down to m=1) through space-filling curve mechanisms
- When input dimension is underestimated (m < d), required network width grows super-exponentially with decreasing approximation error
- The trade-off between input dimension, model complexity, and approximation accuracy is characterized through Monge-Ampère equation regularity theory
- Empirical simulations confirm theoretical predictions, showing larger networks are needed when input dimension is lower than manifold dimension

## Why This Works (Mechanism)

### Mechanism 1: Space-filling Curve Approximation
- **Claim:** Deep generative models can approximate distributions on a $d$-dimensional manifold using an input dimension $m$ that is strictly smaller than $d$ (down to $m=1$).
- **Mechanism:** The network constructs an $\epsilon$-space-filling curve (or manifold) in the ambient space. As the required approximation error $\epsilon$ decreases, the network learns to fold the low-dimensional input manifold with increasing irregularity to cover the target support, effectively "filling" the higher-dimensional structure.
- **Core assumption:** The target data distribution is supported on a compact $d$-dimensional Riemannian manifold and has a density bounded away from zero.
- **Evidence anchors:**
  - [abstract] Mentions utilizing "space-filling curve concepts" to allow inputs of arbitrary dimension.
  - [section 3] Theorem 3.1 proves approximation feasibility for any $m \ge 1$.
  - [corpus] "Riemannian Neural Optimal Transport" (ArXiv: 2602.03566) supports the extension of these geometric approximation principles to complex manifolds.
- **Break condition:** If the target distribution does not lie on a compact manifold (e.g., heavy tails in ambient space) or has disjoint supports, the single continuous curve approximation may fail.

### Mechanism 2: Complexity-Erro-Dimension Trade-off
- **Claim:** When input dimension $m$ is underestimated relative to manifold dimension $d$ ($m < d$), the required network width grows super-exponentially with decreasing approximation error.
- **Mechanism:** The proof relies on regularity theory for Monge-Ampère equations. As $\epsilon \to 0$, the density of the approximating manifold spreads to cover volume, forcing the infimum of the density to approach zero. This degrades the Hölder exponent $\alpha(m, \epsilon) \to 0$, causing the complexity bound (width) to explode.
- **Core assumption:** ReLU activation functions and bounded network weights.
- **Evidence anchors:**
  - [abstract] Highlights the "super-exponential complexity bound" when input dimension is underestimated.
  - [section 5.1.2] Explicitly links the volume growth of the approximating manifold to the degradation of the regularity constant $\alpha$.
  - [corpus] "Dimensionality reduction and width of deep neural networks" (ArXiv: 2511.06821) provides complementary theoretical grounding on how topological complexity drives width requirements.
- **Break condition:** If computational resources limit network width, the model will fail to converge to the target distribution below a certain error threshold when $m < d$.

### Mechanism 3: Optimal Transport Regularization
- **Claim:** The generator approximates the target distribution by minimizing the Wasserstein-1 distance between the pushforward input distribution and the target measure.
- **Mechanism:** The generator $g$ pushes forward a simple input measure $\rho$ (e.g., uniform) to $g_\sharp(\rho)$. The network parameters are optimized to minimize the discrepancy $W_1(g_\sharp(\rho), Q)$. Statistical guarantees ensure the empirical minimizer converges at a rate of $O(n^{-1/(d+\delta)})$.
- **Core assumption:** The input distribution has a density bounded away from infinity and zero on the unit cube.
- **Evidence anchors:**
  - [section 2.1] Defines the generative modeling objective as minimizing discrepancy.
  - [section 4] Simulations use Wasserstein loss to train networks to fill 2D/3D structures.
  - [corpus] Corpus signals indicate high relevance of "Riemannian Neural Optimal Transport" for the underlying mathematical framework.
- **Break condition:** The bounds on network weights $\kappa$ depend on the diameter of the manifold; if the target manifold is unbounded, the specific complexity bounds derived here may not hold.

## Foundational Learning

- **Concept: The Manifold Hypothesis**
  - **Why needed here:** The entire theoretical argument rests on the assumption that high-dimensional data lies on a low-dimensional Riemannian manifold. Without this, the distinction between $m$ and $d$ is meaningless.
  - **Quick check question:** Can you explain why real-world images (high pixel count) might actually be described by far fewer intrinsic variables (pose, lighting)?

- **Concept: Wasserstein Distance (Optimal Transport)**
  - **Why needed here:** This is the metric used to quantify "approximation error." Unlike KL-divergence, it handles disjoint supports and provides the geometric structure needed for the proof.
  - **Quick check question:** If you had to move piles of dirt (distribution A) to fill holes (distribution B), how would you measure the "cost" of that transport?

- **Concept: Hölder Continuity & Regularity**
  - **Why needed here:** The "super-exponential" result stems from the degradation of the Hölder exponent $\alpha$ in the solution to the transport mapping. Understanding this regularity is key to understanding the complexity cost.
  - **Quick check question:** Does a function with a very high derivative (sharp corners) require more or fewer parameters to approximate than a smooth function?

## Architecture Onboarding

- **Component map:** Input latent vector $z \in \mathbb{R}^m$ -> Deep ReLU Network (Generator) $g_\theta: \mathbb{R}^m \to \mathbb{R}^D$ -> Generated sample $x \in \mathbb{R}^D$
- **Critical path:** Selecting the input dimension $m$ relative to the intrinsic dimension $d$
- **Design tradeoffs:**
  - **High $m$ ($m > d$):** Computationally cheaper. Complexity grows polynomially. Safer if $d$ is unknown.
  - **Low $m$ ($m < d$):** Theoretically possible but practically expensive. Complexity grows super-exponentially. Requires massive width to "fill" the space.
  - **Matched $m = d$:** Optimal efficiency, but requires precise knowledge of $d$.
- **Failure signatures:**
  - **Underfitting (Space-filling failure):** When $m < d$ and network width is insufficient, the generated samples will form a sparse "thread" or "sheet" through the data cloud rather than populating the full volume (see Figure 3 vs Figure 2 in paper).
  - **Mode Collapse:** If the density assumption (lower bound $c > 0$) is violated, the transport map may degenerate.
- **First 3 experiments:**
  1. **Baseline Mapping:** Train a 2-layer ReLU network to map $[0,1]^2$ (Uniform) $\to$ $[0,1]^2$ (Uniform). Verify low error with small width.
  2. **Dimension Underestimation:** Train a deep network (5+ layers) to map $[0,1]^1$ (Uniform) $\to$ $[0,1]^2$ (Uniform). Plot the "space-filling curve" evolution and observe the need for significantly larger width to achieve comparable error.
  3. **Complexity Scaling:** Fix the target distribution and error threshold. Plot required network width against input dimension $m$ to visualize the exponential spike when $m < d$.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis critically depends on the data lying on a compact Riemannian manifold with a density bounded away from zero, which may not hold for many real-world datasets
- The super-exponential width requirements create a sharp boundary between theoretical possibility and practical impossibility, with no concrete estimates of computational feasibility
- The specific super-exponential complexity bounds rely on regularity theory for Monge-Ampère equations that may be conservative in practice

## Confidence
- **High Confidence**: The space-filling curve mechanism and the basic trade-off between input dimension and complexity are well-supported by both theoretical proofs and empirical simulations
- **Medium Confidence**: The specific super-exponential complexity bounds and their implications for practical implementation, while theoretically sound, may not fully capture real-world performance due to simplifications in the model
- **Medium Confidence**: The extension of these results to non-smooth or non-compact manifolds remains an open question that requires further investigation

## Next Checks
1. **Boundary of Feasibility**: Systematically test the minimum width required to achieve specific approximation errors when $m < d$ for various manifold types and target dimensions. Identify the point where super-exponential growth makes training impractical.

2. **Real-world Data Testing**: Apply the framework to high-dimensional real datasets (e.g., images, point clouds) to validate whether the manifold assumption holds and whether the theoretical complexity predictions match empirical observations.

3. **Alternative Architectures**: Investigate whether architectures beyond standard ReLU networks (e.g., residual connections, attention mechanisms) can mitigate the complexity penalty when $m < d$, potentially through more efficient space-filling strategies.