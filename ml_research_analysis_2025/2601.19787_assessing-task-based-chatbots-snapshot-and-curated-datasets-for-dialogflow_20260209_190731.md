---
ver: rpa2
title: 'Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow'
arxiv_id: '2601.19787'
source_url: https://arxiv.org/abs/2601.19787
tags:
- chatbots
- dialogflow
- chatbot
- testing
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TOFU-D and COD, two curated datasets of Dialogflow
  chatbots from GitHub, totaling 1,788 and 185 chatbots respectively. The datasets
  capture diverse domains, languages, and implementation patterns, addressing the
  lack of large-scale resources for chatbot quality research.
---

# Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow

## Quick Facts
- arXiv ID: 2601.19787
- Source URL: https://arxiv.org/abs/2601.19787
- Reference count: 40
- Two curated datasets: TOFU-D (1,788 Dialogflow chatbots) and COD (185 curated chatbots) from GitHub, capturing diverse domains, languages, and implementation patterns for chatbot quality research.

## Executive Summary
This paper addresses the lack of large-scale resources for studying chatbot quality and security by presenting TOFU-D and COD, two curated datasets of Dialogflow chatbots from GitHub. The datasets enable systematic analysis of chatbot architectures, revealing significant gaps in test coverage and security vulnerabilities, particularly in webhook configurations and exception handling. The study highlights the need for automated quality assessment tools and systematic, multi-platform research to improve chatbot reliability and security.

## Method Summary
The authors created TOFU-D by searching GitHub for repositories containing "Dialogflow" plus "chatbot" or "agent," filtering for the presence of an `agent` file, classifying ES vs. CX chatbots, and removing duplicates. COD was curated from TOFU-D by selecting chatbots with at least one intent, one entity, a webhook service, English support, Dialogflow v2, and a starred repository. Security assessment used Bandit static analysis on Python webhook code, while test coverage used Botium to generate test cases for 10 COD chatbots.

## Key Results
- TOFU-D contains 1,788 Dialogflow chatbots from 12,796 initial repositories after filtering.
- COD (185 chatbots) captures diverse domains including e-commerce, healthcare, education, and travel.
- Preliminary analysis revealed significant gaps in test coverage and frequent security vulnerabilities, particularly in webhook configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted repository filtering based on structural file signatures significantly improves chatbot dataset precision.
- Mechanism: Multi-stage filtering process: initial keyword search ("Dialogflow" + ["chatbot" OR "agent"]), followed by confirming the presence of the required `agent` configuration file. This structural validation removes repositories that merely mention the technology without containing actual chatbot implementations, excluding 11,086 out of 12,796 repositories.
- Core assumption: The presence of a valid `agent` file indicates a deployable Dialogflow chatbot, and its absence indicates a non-chatbot repository.
- Evidence anchors:
  - [abstract] Mentions "snapshot of 1,788 Dialogflow chatbots from GitHub" derived through a specific methodology.
  - [section 2.1] "Repository classification: ...we checked for the presence of the agent file... This step excluded 11,086 repositories without chatbots".
  - [corpus] A related paper ("Towards the Assessment of Task-based Chatbots...") uses a similar "TOFU-R" snapshot methodology for Rasa, suggesting the generalizability of the multi-stage filtering approach for different platforms.
- Break condition: If repositories contain `agent` files that are placeholders, corrupted, or used for non-chatbot purposes, the precision of the initial dataset (TOFU-D) would decrease.

### Mechanism 2
- Claim: Curated dataset selection via structural and functional complexity constraints creates a more relevant subset for quality assurance research.
- Mechanism: The COD dataset is created by filtering TOFU-D based on three criteria: dialog complexity (>= 1 intent AND >= 1 entity), functional complexity (presence of a webhook service), and utility (English support, Dialogflow v2, starred repository). This removes trivial examples and ensures chatbots have backend logic, a critical component for security and integration testing.
- Core assumption: Chatbots without webhooks, entities, or English support are less relevant for the specific goals of "quality and security" research aimed at generalizable, complex systems.
- Evidence anchors:
  - [abstract] "COD, a curated subset of TOFU-D... The datasets capture a wide range of domains...".
  - [section 2.2] "We define as relevant the chatbots selected... based on three criteria: a minimal dialog complexity... functional complexity... and utility".
  - [corpus] No direct corpus evidence on the *effectiveness* of this specific curation, but related work (ASYMOB, TOFU-R) establishes the precedent for curated datasets in chatbot research. The corpus confirms curation is a standard practice but doesn't validate this paper's specific criteria.
- Break condition: If quality assurance challenges are prominent in simple, non-webhook chatbots or non-English chatbots, then the COD dataset would systematically exclude these important research subjects.

### Mechanism 3
- Claim: Automated testing (Botium) and static analysis (Bandit) reveal distinct, complementary quality and security deficiencies in task-based chatbots.
- Mechanism: Botium is used to generate test cases, revealing coverage gaps in intents (greeting, fallback) and entities. Bandit statically analyzes Python backend code, identifying security vulnerabilities like missing timeouts, weak random number generation, and API misconfigurations. The two tools target different layers: conversational flow and backend implementation.
- Core assumption: The issues identified by these tools (e.g., "flask debug true," "uncovered entities") represent genuine quality and security problems that would affect a real deployment.
- Evidence anchors:
  - [abstract] "A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities".
  - [section 4] "To explore the effectiveness of test case generation tools... we observed missing tests for intents... To explore how vulnerable chatbots are, we used Bandit...".
  - [corpus] Related work ("Test Case Generation for Dialogflow Task-Based Chatbots") explicitly uses Botium for test generation, validating the choice of tool. Another paper ("I know it's not right...") discusses trust in chatbots for security policy, making the security findings relevant.
- Break condition: False positives from Bandit or limitations in Botium's test generation logic could lead to an inaccurate representation of the chatbots' actual quality and security posture.

## Foundational Learning

- Concept: **Task-Based Chatbots vs. LLMs**
  - Why needed here: The paper explicitly distinguishes its subject (task-based chatbots) from general-purpose LLMs (e.g., ChatGPT). Understanding this distinction is critical because their architectures, failure modes, and testing strategies differ fundamentally. Task-based chatbots rely on structured intents, entities, and flows, while LLMs are probabilistic engines.
  - Quick check question: Which component handles a user's request for "ordering a large pepperoni pizza" in a task-based chatbot? (Answer: A specific intent, likely `order-pizza`, with `size` and `topping` entities.)

- Concept: **Webhooks**
  - Why needed here: The paper's functional complexity criterion and security analysis heavily focus on webhooks. A reader must understand that a webhook is an HTTP callback that allows a chatbot to execute custom backend logic (e.g., querying a database, calling an external API). Most security vulnerabilities found (e.g., "all interfaces binding," "request without timeout") are in this backend code.
  - Quick check question: If a chatbot needs to check real-time flight availability, which component is most likely responsible? (Answer: The webhook service, triggered by a webhook intent.)

- Concept: **Static Analysis vs. Dynamic Testing**
  - Why needed here: The paper's validation uses two distinct approaches. Bandit performs static analysis on the source code without executing it, finding potential vulnerabilities. Botium performs dynamic testing by generating inputs and executing the chatbot to check its behavior. Understanding this distinction is key to interpreting the paper's findings on coverage gaps (dynamic) vs. code-level flaws (static).
  - Quick check question: Which method, static analysis or dynamic testing, would be most effective at discovering that a chatbot's backend uses a deprecated, insecure library? (Answer: Static analysis.)

## Architecture Onboarding

- Component map:
  1.  **Conversational Agent (Dialogflow):** The frontend interface. Contains **Intents** (user goals), **Entities** (data types), **Utterances** (training phrases), and **Flows** (conversation logic). Manages NLU.
  2.  **Webhook Service (Backend):** A server (often Node.js/Python) implementing business logic. Triggered by specific intents to perform actions like API calls or database lookups. This is where most security vulnerabilities were found.
  3.  **Fulfillment Layer:** The mechanism connecting the Dialogflow agent to the webhook service. May involve **Cloud Functions** (serverless execution).

- Critical path:
  1.  **User Input** -> Dialogflow Agent's NLU matches an **Intent**.
  2.  If the intent requires dynamic data, a **Webhook Request** is sent to the **Webhook Service**.
  3.  The Webhook Service processes the request (potentially introducing a security risk) and returns a **Webhook Response**.
  4.  The Agent uses this response to generate the final **Bot Response** to the user.

- Design tradeoffs:
  - **TOFU-D (Comprehensive) vs. COD (Curated):** TOFU-D offers breadth (1,788 chatbots) for population-level studies but includes many trivial examples. COD offers depth and relevance (185 validated chatbots) for targeted quality assurance research but is smaller and may exclude certain types of simple or non-English bots.
  - **Botium (Dynamic) vs. Bandit (Static):** Botium tests conversational flow and correctness but may miss internal code vulnerabilities. Bandit identifies code-level security risks but cannot assess conversational logic or runtime behavior.

- Failure signatures:
  - **Uncovered Intents/Entities:** Botium fails to generate tests for certain intents (e.g., fallbacks) or entities, leaving parts of the dialog untested.
  - **Webhook Vulnerabilities:** Static analysis flags issues like `flask debug true` (debug mode enabled in production), `request without timeout` (risk of hanging), or `hardcoded SQL string` (injection risk).
  - **Integration Failures:** Chatbots fail to deploy or function correctly due to misconfigured entities or outdated Dialogflow versions (v1 vs. v2).

- First 3 experiments:
  1.  **Reproduce Security Scan:** Clone the COD dataset. Run the Bandit static analyzer on the Python webhook services for a subset of chatbots. Verify if the reported vulnerabilities (e.g., "all interfaces binding") are genuine and exploitable.
  2.  **Test Coverage Expansion:** Use Botium to generate tests for a chatbot from the COD dataset. Manually write additional tests for the "uncovered" intents identified in the paper (e.g., fallbacks). Compare the original and expanded test suite coverage metrics.
  3.  **Cross-Platform Analysis:** Select a simple task (e.g., "order a pizza"). Implement it as a Dialogflow ES chatbot and as a Rasa chatbot. Analyze how the core concepts (intents, entities, actions) translate between the two architectures, and where a webhook would be necessary in each. This explores the "multi-platform perspective" the authors advocate for.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated test generation techniques be enhanced to cover fallback behaviors, complex intent preconditions, and entity variations that current tools fail to address?
- Basis in paper: [explicit] The preliminary assessment revealed that Botium missed tests for fallback behaviors (100% of chatbots), greeting intents, and preconditions, while leaving entities uncovered or only partially covered.
- Why unresolved: Existing tools struggle with the nuance of conversational flow and state management required for these specific test scenarios.
- What evidence would resolve it: Novel algorithms or heuristics that demonstrate statistically significant improvements in coverage for these specific gap areas when applied to the COD dataset.

### Open Question 2
- Question: What specific security mitigations are required to address the webhook-based vulnerabilities (e.g., API misconfigurations, arbitrary code execution) prevalent in commercial chatbot platforms?
- Basis in paper: [explicit] The analysis identified security issues specific to Dialogflowâ€™s webhook architecture, such as binding to all network interfaces (41%) and potential arbitrary code execution (32%), which differed from issues in open-source platforms.
- Why unresolved: These vulnerabilities stem from platform-specific configurations and backend integrations that standard static analyzers may not catch or prioritize correctly.
- What evidence would resolve it: The development of a specialized static analysis rule-set or linter that automatically detects and guides the patching of these webhook misconfigurations.

### Open Question 3
- Question: Do the security vulnerability patterns and testing gaps identified in the curated COD subset generalize to the broader population of task-based chatbots across different platforms?
- Basis in paper: [inferred] The study is described as a "preliminary assessment" based on a small subset (10 chatbots for testing, 69 for security), and the authors explicitly call for "systematic, multi-platform research."
- Why unresolved: The current data offers a snapshot; without cross-platform and large-scale analysis, it is unclear if these are systemic issues or specific to the sample.
- What evidence would resolve it: A large-scale empirical study correlating quality metrics and security flaws across the full TOFU-D dataset and comparable datasets like BRASATO (Rasa).

## Limitations

- The security and test coverage analysis is based on a small sample (10 chatbots from COD), limiting generalizability of quantitative findings.
- The COD dataset, while curated for relevance, may systematically exclude certain types of simple or non-English chatbots where quality assurance challenges are prominent.
- Platform-specific findings (Dialogflow ES/CX) may not generalize to other task-based chatbot frameworks without further multi-platform research.

## Confidence

- **High**: The dataset collection methodology (TOFU-D and COD creation) and the identification of security vulnerabilities in webhook configurations
- **Medium**: The reported test coverage gaps from Botium analysis and the classification of chatbot domains using GPT-4o
- **Medium**: The security vulnerability frequency claims based on the 10-chatbot sample
- **Low**: The quantitative claims about vulnerability frequency and test coverage percentages based on the 10-chatbot sample

## Next Checks

1. Scale the Botium and Bandit analyses to the full COD dataset (185 chatbots) to validate the preliminary findings on test coverage gaps and security vulnerabilities.
2. Conduct a multi-platform comparison study applying the same quality and security assessment methodology to Rasa chatbots from the TOFU-R dataset.
3. Perform manual verification of the top 10 most critical security vulnerabilities identified by Bandit to assess false positive rates and actual exploitability.