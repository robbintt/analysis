---
ver: rpa2
title: 'THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for
  Anomaly Detection in Time Series'
arxiv_id: '2510.03911'
source_url: https://arxiv.org/abs/2510.03911
tags:
- anomaly
- time
- detection
- series
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces THEMIS, a novel framework for time series\
  \ anomaly detection that leverages foundation model embeddings from the Chronos\
  \ time series foundation model. Instead of relying on direct anomaly prediction\
  \ or simple thresholded forecast errors, THEMIS extracts high-quality temporal representations\
  \ from the Chronos encoder and applies specialized outlier detection algorithms\u2014\
  specifically Local Outlier Factor and Spectral Decomposition on the self-similarity\
  \ matrix\u2014to detect anomalies."
---

# THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series

## Quick Facts
- arXiv ID: 2510.03911
- Source URL: https://arxiv.org/abs/2510.03911
- Reference count: 39
- Achieves 78.78% F1 on MSL benchmark without training or labeled anomalies

## Executive Summary
THEMIS is a zero-shot time series anomaly detection framework that leverages foundation model embeddings from Chronos to identify anomalies without requiring labeled data or extensive hyperparameter tuning. Instead of direct anomaly prediction or thresholded forecast errors, THEMIS extracts high-quality temporal representations from the Chronos encoder and applies specialized outlier detection algorithms—specifically Local Outlier Factor and Spectral Decomposition on the self-similarity matrix—to detect anomalies. The method is designed to be modular and robust, avoiding the need for extensive tuning while providing interpretability and stability across different parameter settings.

## Method Summary
THEMIS processes univariate time series by first extracting fixed-length embeddings (512 samples, 768 dimensions) from a frozen Chronos-T5 model. These embeddings are organized into a Windowed Absolute Similarity Matrix (WASM) using cosine similarity with absolute values to capture both positively and negatively correlated patterns. Two anomaly scoring mechanisms are employed: Local Outlier Factor for local contextual anomalies and Spectral Residual Scoring, which computes the norm of each point's projection onto the dominant subspace after eigendecomposition. Scores are normalized and thresholded using the SPOT algorithm to flag anomalies. The framework requires no training or labeled anomaly data, making it truly zero-shot.

## Key Results
- Achieves state-of-the-art F1 = 78.78% on MSL benchmark, outperforming trained anomaly detection models
- Competitive performance on SMAP (F1 = 73.21%) and SWaT* (F1 = 71.59%) datasets
- Demonstrates hyperparameter robustness with stable performance across different eigenvector counts and batch sizes
- Spectral residual scoring outperforms LOF and mean similarity methods, particularly on MSL dataset

## Why This Works (Mechanism)

### Mechanism 1: Semantic Separation via Forecasting Pretraining
The model learns to encode seasonality, trends, and noise to minimize forecasting loss. Because anomalies are rare and difficult to forecast, their embeddings map to sparse regions in the latent space rather than the dense manifold of normal behavior. This creates natural separation between normal and anomalous patterns in the embedding space.

### Mechanism 2: Spectral Residual Scoring on Self-Similarity
Anomalies manifest as low-energy residuals in the spectral decomposition of the temporal similarity matrix. Normal temporal behavior is highly self-similar and can be compressed into a low-rank subspace, whereas anomalies are rank-deficient or disruptive to this structure. Points poorly aligned with the dominant subspace receive higher anomaly scores.

### Mechanism 3: Absolute Similarity for Anti-Correlation
Using absolute value of cosine similarity allows detection of anti-correlated anomalies that standard similarity metrics might miss. Anomalous behavior is uncorrelated or structurally distinct, rather than merely an inverse reflection of a normal pattern.

## Foundational Learning

**Concept: Spectral Graph Theory / PCA**
- Why needed here: The Spectral Residual adapter relies on eigendecomposition. Top eigenvectors capture "signal" (normal background) and remaining vectors capture "noise" or "outliers."
- Quick check question: Why does a low projection norm onto the top-k eigenvectors indicate an anomaly rather than a normal point?

**Concept: Extreme Value Theory (EVT) & SPOT**
- Why needed here: THEMIS uses SPOT algorithm to set thresholds, modeling tail distribution of scores rather than using static thresholds.
- Quick check question: How does SPOT differ from a simple 95th percentile threshold in terms of sensitivity to score distribution drift?

**Concept: Foundation Model Context Windows**
- Why needed here: Framework relies on Chronos with fixed context window (L=512). Understanding tokenization is critical for diagnosing why anomalies might be missed.
- Quick check question: What happens to detection capability if anomaly duration exceeds model's context length of 512?

## Architecture Onboarding

**Component map:** Input -> Chronos Encoder -> Embeddings -> WASM Construction -> Spectral Residual Scoring -> SPOT Thresholding

**Critical path:** Construction of WASM is the bottleneck. Batch size B and window length L must balance memory constraints against need for sufficiently large similarity matrix for stable spectral decomposition.

**Design tradeoffs:**
- Batch Size (B): Paper uses B=16. Larger B improves spectral stability but increases memory quadratically (O(B²)).
- Adapter Selection: Spectral scoring is more robust to hyperparameters than LOF, but LOF may catch local contextual anomalies better in highly non-stationary data.
- Univariate Constraint: Current architecture processes channels independently, ignoring multivariate correlations.

**Failure signatures:**
- False Positives on Drift: Concept drift may cause new "normal" embeddings to appear sparse, triggering false alarms.
- High Memory OOM: Increasing context or batch size without GPU memory scaling will crash during N×N similarity matrix construction.

**First 3 experiments:**
1. Hyperparameter Robustness Check: Vary eigenvector count k ∈ {2, 5, 10, 15, 20} on MSL/SMAP to verify stable performance.
2. Adapter Ablation: Compare Spectral Scoring vs. LOF vs. Mean Similarity to confirm Spectral Scoring superiority.
3. Baseline Comparison: Compare anomaly scores from Chronos embeddings versus Chronos forecast errors (MSE) to validate embeddings outperform direct forecasting residuals.

## Open Questions the Paper Calls Out
- Can THEMIS be effectively adapted for multivariate time series anomaly detection to handle inter-channel dependencies?
- Do time series foundation models specifically designed for observability (e.g., Toto) offer superior embeddings for anomaly detection compared to forecasting-focused models like Chronos?
- Does increasing computational resources and batch size during embedding generation lead to detection of subtler anomalies?

## Limitations
- Performance bounded by diversity and quality of pretraining corpus—may fail on domain-specific anomalies not seen during pretraining
- Assumes normal behavior forms tight clusters while anomalies remain sparse, which is empirical rather than theoretically guaranteed across all domains
- Current framework processes univariate series only, ignoring multivariate correlations that may be critical for real-world applications

## Confidence

**High Confidence:** Modular framework design and zero-shot nature are well-established. Spectral residual mechanism has strong theoretical grounding in PCA literature.

**Medium Confidence:** Claim that foundation model embeddings outperform direct forecast errors relies heavily on benchmark results and may degrade with less diverse pretraining corpora.

**Low Confidence:** Robustness to hyperparameters is empirically demonstrated but not theoretically proven. Anti-correlation detection benefit across all anomaly types lacks comprehensive ablation.

## Next Checks
1. Cross-Domain Transfer Test: Apply THEMIS to ECG or financial domains where pretraining corpus differs substantially from test data to measure foundation model dependency.
2. Anomaly Type Sensitivity: Systematically inject synthetic anomalies of different types (trend breaks, spikes, periodicity loss, anti-correlated patterns) to measure detection versus miss rates.
3. Foundation Model Ablation: Replace Chronos embeddings with embeddings from a foundation model trained on narrow corpus (e.g., only temperature data) to validate "universal backbone" assumption.