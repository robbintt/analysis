---
ver: rpa2
title: Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation
arxiv_id: '2507.00054'
source_url: https://arxiv.org/abs/2507.00054
tags:
- distillation
- teacher
- arxiv
- reasoning
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning capabilities
  in small language models (SLMs) through knowledge distillation. The core method,
  AdvDistill, introduces a reward-guided dataset distillation framework that uses
  multiple teacher responses with rule-based rewards to guide the student model training.
---

# Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation

## Quick Facts
- arXiv ID: 2507.00054
- Source URL: https://arxiv.org/abs/2507.00054
- Reference count: 22
- Primary result: AdvDistill 1.5B achieves 91.52% accuracy on GSM-8K mathematical reasoning benchmark

## Executive Summary
This paper introduces AdvDistill, a reward-guided dataset distillation framework designed to enhance reasoning capabilities in small language models (SLMs). The approach leverages multiple teacher responses with rule-based rewards to guide student model training, employing group relative advantages to weight responses during training and incorporating a contrastive penalty for incorrect responses. The framework demonstrates significant improvements on mathematical reasoning tasks while highlighting important trade-offs in computational efficiency and template adherence.

## Method Summary
AdvDistill implements a reward-guided dataset distillation framework that trains small language models using multiple teacher responses weighted by group relative advantages. The method employs rule-based rewards to evaluate teacher responses, then uses these evaluations to guide the student model training process. A contrastive penalty component helps the model distinguish between correct and incorrect reasoning paths. The framework processes teacher responses through a reward mechanism that calculates relative advantages within groups, allowing the student model to learn from the most effective reasoning approaches while avoiding incorrect patterns.

## Key Results
- AdvDistill 1.5B achieves 91.52% accuracy on GSM-8K (vs 72.85% for SFTDistilled and 42.45% for base)
- AdvDistill 1.5B achieves 69.09% accuracy on GSM-PLUS (vs 51.10% for SFTDistilled and 30.12% for base)
- AdvDistill 1.5B shows 23.57% accuracy on MMLU-PRO (vs 30.07% for SFTDistilled), indicating domain-specific effectiveness

## Why This Works (Mechanism)
The framework improves reasoning capabilities by leveraging multiple teacher responses with rule-based rewards to guide student model training. Group relative advantages enable the model to identify and prioritize the most effective reasoning approaches from diverse teacher outputs. The contrastive penalty component helps the student model learn to distinguish between correct and incorrect reasoning paths, reinforcing successful strategies while avoiding common errors. This multi-response, reward-weighted approach allows the student model to learn from the collective strengths of multiple teachers rather than being limited to single-response training.

## Foundational Learning
- Dataset distillation: Why needed - Reduces large training datasets to smaller, more efficient versions while preserving key information; Quick check - Compare training time and model performance between full dataset and distilled dataset approaches
- Reward-guided learning: Why needed - Provides explicit feedback signals to guide model behavior beyond simple loss minimization; Quick check - Measure performance improvements when varying reward function design
- Group relative advantages: Why needed - Enables comparison of multiple responses within context to identify superior reasoning approaches; Quick check - Evaluate performance when using absolute versus relative reward weighting
- Contrastive learning in reasoning: Why needed - Helps models distinguish between correct and incorrect reasoning patterns; Quick check - Test model robustness to reasoning errors with and without contrastive penalty
- Multi-teacher distillation: Why needed - Leverages diverse reasoning approaches from multiple sources; Quick check - Compare performance using single teacher versus multiple teachers
- Rule-based reward systems: Why needed - Provides consistent, interpretable feedback for reasoning tasks; Quick check - Assess performance variation across different rule sets

## Architecture Onboarding
Component map: Teacher models -> Response collection -> Rule-based reward evaluation -> Group relative advantage calculation -> Student model training with weighted responses -> Contrastive penalty application
Critical path: The training pipeline processes multiple teacher responses through reward evaluation, calculates group relative advantages, then uses these weighted responses to train the student model with contrastive penalties. This sequence ensures the student learns from the most effective reasoning approaches while avoiding incorrect patterns.
Design tradeoffs: The framework trades computational efficiency for reasoning performance, requiring multiple teacher evaluations per training example. While this increases training time and resource requirements, it enables the student model to learn from diverse reasoning approaches and avoid common errors through contrastive learning.
Failure signatures: The method may show reduced template adherence due to focus on reasoning quality over format consistency. Performance may degrade on tasks outside the training domain, as evidenced by lower MMLU-PRO scores. Computational overhead may become prohibitive for very large-scale applications.
First experiments:
1. Compare AdvDistill performance against baseline models on mathematical reasoning benchmarks (GSM-8K, GSM-PLUS)
2. Evaluate template adherence and format consistency between AdvDistill and SFTDistilled outputs
3. Measure computational overhead and training time differences between AdvDistill and standard distillation approaches

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Computational overhead from multiple teacher evaluations and reward calculations may limit real-world deployment
- Reduced template adherence in AdvDistill outputs suggests trade-offs between reasoning performance and format consistency
- Performance degradation on non-mathematical reasoning tasks (MMLU-PRO) indicates domain-specific effectiveness limitations

## Confidence
- Mathematical reasoning improvements: High - Substantial and consistent performance gains across multiple math benchmarks
- Domain transferability: Medium - Clear task-specific strengths and weaknesses observed
- Computational efficiency: Low - Insufficient detailed analysis of additional training costs relative to performance gains

## Next Checks
1. Evaluate AdvDistill performance on additional reasoning domains (scientific reasoning, logical inference, multi-hop reasoning) to assess generalizability beyond mathematics
2. Conduct ablation studies isolating the impact of group relative advantages weighting versus contrastive penalties on both performance and computational overhead
3. Perform human preference studies comparing AdvDistill outputs against SFTDistilled outputs across multiple criteria including reasoning quality, template adherence, and response coherence