---
ver: rpa2
title: List Replicable Reinforcement Learning
arxiv_id: '2512.00553'
source_url: https://arxiv.org/abs/2512.00553
tags:
- algorithm
- action
- policy
- list
- rtrunc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces list replicability to reinforcement learning,
  aiming to produce a small, fixed set of near-optimal policies across training runs.
  Existing RL algorithms suffer from instability and can output exponentially many
  policies due to sensitivity to stochastic noise.
---

# List Replicable Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.00553
- Source URL: https://arxiv.org/abs/2512.00553
- Reference count: 40
- Primary result: Introduces list replicability to RL, achieving polynomial list complexity and reduced policy variance via a robust planning algorithm with lexicographic action selection within a tolerance band.

## Executive Summary
This paper introduces list replicability to reinforcement learning, aiming to produce a small, fixed set of near-optimal policies across training runs. Existing RL algorithms suffer from instability and can output exponentially many policies due to sensitivity to stochastic noise. To address this, the authors propose a robust planning algorithm that selects actions lexicographically within a tolerance band drawn from a uniform distribution, limiting sensitivity to small estimation errors. This idea is extended into two RL algorithms: a weakly list-replicable method that truncates unreachable states, and a strongly list-replicable method that ensures both the returned policy and the sequence of executed policies lie in a small list. Theoretical guarantees show polynomial list complexity (O(|S|²|A|H²) for weak and O(|S|³|A|H³) for strong replicability) and near-optimal sample complexity. Experiments on CartPole, Acrobot, MountainCar, and NameThisGame demonstrate that the robust planner improves stability in practice when integrated into standard RL frameworks, at the cost of a modest decrease in performance.

## Method Summary
The core innovation is the Robust Planner, which modifies standard greedy action selection by choosing the lexicographically first action whose Q-value is within a tolerance band below the maximum. This tolerance, $r_{\text{action}}$, is drawn from a uniform distribution and acts as a stabilizing mechanism against small estimation errors. The method is integrated into both tabular Q-learning and deep RL frameworks (DQN). Two theoretical variants are proposed: weakly list-replicable algorithms that truncate unreachable states, and strongly list-replicable algorithms that ensure both the returned policy and the sequence of executed policies lie in a small list. The algorithms guarantee polynomial list complexity and near-optimal sample complexity, addressing the fundamental instability of standard RL methods.

## Key Results
- Theoretical guarantees: Polynomial list complexity (O(|S|²|A|H²) for weak and O(|S|³|A|H³) for strong replicability) and near-optimal sample complexity.
- Experimental results: The robust planner improves stability across 25-50 runs on CartPole, Acrobot, MountainCar, and NameThisGame, reducing policy variance at the cost of modest performance drops.
- List size reduction: The method limits the number of distinct policies produced, contrasting with the exponential blowup seen in standard RL under stochastic noise.

## Why This Works (Mechanism)
The mechanism relies on introducing controlled stochasticity via a tolerance band in action selection. By selecting actions lexicographically within this band, the algorithm reduces sensitivity to small Q-value fluctuations caused by stochastic transitions or function approximation errors. This bounded randomness ensures that minor estimation errors do not lead to divergent policies across runs, thus stabilizing learning and limiting the number of unique policies generated.

## Foundational Learning
- **List replicability**: The property that an RL algorithm produces only a small, fixed list of near-optimal policies across runs. *Why needed*: Addresses the instability and policy variance inherent in standard RL. *Quick check*: Verify that the number of distinct policies is polynomial in problem size, not exponential.
- **Robust planning with tolerance bands**: Action selection within a band below the maximum Q-value, using lexicographic ordering. *Why needed*: Reduces sensitivity to estimation errors and stochastic noise. *Quick check*: Confirm that action selection logic correctly implements the tolerance band and tie-breaking.
- **Uniform distribution over tolerances**: Randomizing the tolerance $r_{\text{action}}$ from a uniform distribution. *Why needed*: Ensures fairness and prevents systematic bias in action selection. *Quick check*: Verify the distribution of tolerances and their effect on action selection.
- **Polynomial list complexity bounds**: Theoretical guarantees that the number of distinct policies is bounded by a polynomial in the MDP parameters. *Why needed*: Provides a rigorous foundation for the stability claims. *Quick check*: Confirm the derivation of the O(|S|²|A|H²) and O(|S|³|A|H³) bounds.
- **Discretization for continuous states**: Mapping continuous state spaces to discrete bins for tabular methods. *Why needed*: Enables the application of tabular RL algorithms to continuous environments. *Quick check*: Verify the binning logic and boundary handling for MountainCar.
- **Lexicographic ordering in action selection**: Choosing the smallest index action within the tolerance band. *Why needed*: Ensures deterministic tie-breaking and consistency across runs. *Quick check*: Confirm that the implementation strictly follows lexicographic order.

## Architecture Onboarding

**Component map**: Environment (Gymnasium) -> Discretizer (for tabular) -> Q-function approximator (tabular or DQN) -> Robust Planner (action selection) -> Policy evaluation.

**Critical path**: For each state, compute Q-values -> apply Robust Planner (tolerance band + lexicographic selection) -> execute action -> update Q-values -> repeat.

**Design tradeoffs**: The tolerance band introduces a bias-variance tradeoff: larger tolerances increase stability but may reduce performance. Lexicographic tie-breaking ensures determinism but may introduce subtle biases if not implemented carefully. Discretization simplifies tabular methods but can lose information in continuous spaces.

**Failure signatures**: Performance drops to zero if $r_{\text{action}}$ is too large; no improvement in variance if lexicographic tie-breaking is not strict or if the tolerance band is misimplemented. Integration issues may arise if the Robust Planner is not correctly hooked into the action selection loop.

**First experiments**:
1. Verify the lexicographic tie-breaking in the Robust Planner: confirm it selects the smallest index action within the tolerance band, not just the max.
2. Reproduce the MountainCar tabular experiment with 25 seeds for $r_{\text{action}} \in \{0, 0.001, 0.005, 0.02\}$ and compare mean return and std dev.
3. Integrate the Robust Planner into a DQN implementation and test on CartPole-v1 with 25 seeds to assess variance reduction.

## Open Questions the Paper Calls Out
None.

## Limitations
- Missing implementation details: exact random seeds and discretization boundaries for MountainCar are not specified.
- Integration ambiguity: the Robust Planner's integration into deep RL (DQN) is not fully specified, leaving uncertainty about hyperparameters.
- Performance trade-off: stability improvements come at the cost of a modest decrease in performance, especially for larger tolerance values.

## Confidence
- **High** confidence in theoretical guarantees, as they are derived analytically.
- **Medium** confidence in reproducing the reported stability improvements for tabular settings, pending empirical validation.
- **Low** confidence in deep RL integration due to lack of clear pseudocode for Algorithm 1 and ambiguous "lexicographic" tie-breaking rule.

## Next Checks
1. Verify the lexicographic tie-breaking in the Robust Planner: confirm it selects the smallest index action within the tolerance band, not just the max.
2. Reproduce the MountainCar tabular experiment with 25 seeds for $r_{\text{action}} \in \{0, 0.001, 0.005, 0.02\}$ and compare mean return and std dev.
3. Integrate the Robust Planner into a DQN implementation and test on CartPole-v1 with 25 seeds to assess variance reduction.