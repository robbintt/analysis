---
ver: rpa2
title: 'Cyclic Ablation: Testing Concept Localization against Functional Regeneration
  in AI'
arxiv_id: '2509.25220'
source_url: https://arxiv.org/abs/2509.25220
tags:
- deception
- ablation
- features
- cycle
- deceptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study tested whether deceptive behavior in language models
  can be surgically removed by targeting specific neural features. The authors proposed
  cyclic ablation: an iterative procedure combining sparse autoencoders, targeted
  feature ablation, and adversarial training on DistilGPT-2.'
---

# Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI

## Quick Facts
- arXiv ID: 2509.25220
- Source URL: https://arxiv.org/abs/2509.25220
- Reference count: 10
- Primary result: Deception in language models regenerates via alternative neural pathways after targeted feature ablation, while general competence degrades

## Executive Summary
This study tested whether deceptive behavior in language models can be surgically removed by targeting specific neural features. The authors proposed cyclic ablation: an iterative procedure combining sparse autoencoders, targeted feature ablation, and adversarial training on DistilGPT-2. Each cycle identified high-scoring deception features, ablated them, then allowed the model to recover via fine-tuning on deceptive examples. The primary outcome was functional regeneration: after every ablation, deception scores rebounded within a few epochs, showing the behavior was not localized. However, each ablation-recovery cycle caused a steady rise in perplexity from ~41.5 to ~56.5, indicating progressive degradation of general language competence. This suggests deception is distributed and entangled with core linguistic representations, making direct removal harmful to overall model performance.

## Method Summary
The method involves training sparse autoencoders (SAEs) on MLP activations from DistilGPT-2 layers 2-3 using datasets of truthful and deceptive statements. Each cycle computes a Deception Score for features, selects top-K candidates for ablation (by zeroing SAE decoder weights), then stress-tests the model via 5 epochs of fine-tuning on deceptive examples. The process repeats 10 times, with features re-identified each cycle and previously ablated features masked during scoring. Perplexity on WikiText is monitored to track collateral damage to general language competence.

## Key Results
- Deception scores rebounded to baseline or higher within 3-5 epochs after each ablation
- Perplexity increased monotonically from ~41.5 to ~56.5 across 10 cycles
- Feature indices became invalid after fine-tuning, requiring SAE retraining each cycle
- Top-K ablation candidates changed significantly across cycles, indicating shifting representations

## Why This Works (Mechanism)

### Mechanism 1: Functional Regeneration via Adversarial Reconfiguration
Ablating identified "deception features" causes only temporary suppression; the model recovers deceptive capability by reassigning function to alternative neural pathways. After ablation zeros out specific SAE decoder weights, adversarial fine-tuning on deceptive examples drives gradient-based reorganization. The model discovers redundant or polysemantic circuits that can approximate the ablated function.

### Mechanism 2: SAE-Based Feature Attribution for Targeted Ablation
Sparse autoencoders decompose MLP activations into features that correlate with specific behaviors, enabling identification of ablation targets via a Deception Score. SAEs trained on MLP layers 2-3 produce sparse activation patterns. The Deception Score ranks features by differential activation between deceptive and truthful datasets.

### Mechanism 3: Polysemantic Feature Entanglement Causes Collateral Damage
Features identified as "deception-related" are polysemantic—they participate in multiple computations, so their ablation degrades general linguistic competence. When ablated features contribute to both deceptive and legitimate language processing, removing them forces the model to use less efficient workaround pathways.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**
  - Why needed here: SAEs are the core tool for decomposing model activations into interpretable, sparse feature directions that can be scored and ablated.
  - Quick check question: Can you explain why sparsity helps isolate interpretable features from dense activations?

- **Feature Ablation via Decoder Zeroing**
  - Why needed here: The ablation method—zeroing SAE decoder weights to make activations orthogonal to a feature vector—is the intervention mechanism being tested.
  - Quick check question: How does setting decoder weights to zero differ from zeroing encoder activations, and what does orthogonality achieve?

- **Perplexity as Language Model Quality Metric**
  - Why needed here: Perplexity tracks collateral damage; its steady rise signals that ablation harms general competence, not just the targeted behavior.
  - Quick check question: What does a perplexity increase from ~41.5 to ~56.5 imply about the model's probability assignments over text?

## Architecture Onboarding

- **Component map:**
  DistilGPT-2 (82M params) -> MLP layers 2-3 activations -> SAE encoder -> sparse latent features -> SAE decoder -> residual stream
  Deception Score calculator -> top-K feature selector -> ablation mask (zeros decoder weights)
  Adversarial fine-tuning loop on D_deception dataset (5 epochs per cycle)

- **Critical path:**
  1. Train SAE on MLP activations from D_truth + D_deception
  2. Compute Deception Score for all features; select top-K (K ≤ 50, constrained by perplexity threshold)
  3. Ablate by zeroing decoder weights for selected features
  4. Fine-tune on D_deception (stress test)
  5. Re-evaluate Deception Score and perplexity; repeat from step 2 with retrained SAE

- **Design tradeoffs:**
  - Full residual stream SAE coverage vs. catastrophic perplexity spike (paper chose MLP-only)
  - Higher K (more aggressive ablation) vs. faster perplexity degradation
  - Static feature indices vs. retraining SAE each cycle (static failed; retraining required)

- **Failure signatures:**
  - SAE feature indices become invalid after fine-tuning—semantics shift, requiring SAE retraining
  - Deception Score appears low but function persists via previously ablated features; must zero activations of all prior-ablated features during scoring
  - Perplexity rises monotonically across cycles (41.5 → 56.5) while Deception Score oscillates (drops post-ablation, rebounds post-fine-tuning)

- **First 3 experiments:**
  1. Baseline SAE feature stability test: Train SAE, fine-tune model on small dataset without ablation, re-query same feature indices—measure activation pattern drift to quantify representation shift.
  2. Random feature ablation control: Ablate K random (non-high-scoring) features, run stress test, measure perplexity and Deception Score change to isolate effect of targeted vs. untargeted ablation.
  3. Single-cycle deep dive: Run one full ablation-recovery cycle with detailed logging of which features reactivate, which new features emerge in top-K, and layer-wise perplexity contribution to localize damage sources.

## Open Questions the Paper Calls Out

- Do larger language models exhibit the same functional regeneration and competence degradation patterns under cyclic ablation as observed in DistilGPT-2? The authors explicitly note the study was conducted on a "relatively small model" and that "larger models may possess greater redundancy and different regeneration mechanisms."
- Can cyclic ablation effectively remove more subtle forms of deception, or does the method only work on the specialized, explicit falsehoods tested in this study? The authors acknowledge their "definition of deception was highly specialized" and list "investigating more subtle forms of deception" as an important next step.
- Is the observed degradation of linguistic competence a specific result of removing deception-related features, or a general consequence of any cumulative ablation? The authors state that "addition of control runs with ablation of random features could give insights and make evidence stronger."

## Limitations

- Findings rely on DistilGPT-2 specifically, leaving unclear whether results generalize to larger models or different architectures
- The study does not prove SAE-identified features are causally responsible for deception versus merely correlating with it
- SAE architecture and training details are unspecified, creating potential reproducibility barriers

## Confidence

- **High confidence:** The empirical observation that deception scores rebound after ablation while perplexity increases monotonically is clearly demonstrated. The methodology for computing the Deception Score and implementing feature ablation is well-specified.
- **Medium confidence:** The interpretation that this reflects distributed, entangled representations is plausible but not definitively proven. Alternative explanations (such as SAE feature drift or optimization artifacts) remain possible.
- **Low confidence:** Claims about SAE features being causally responsible for deception require stronger validation. The assumption that perplexity increase directly measures "harm to general competence" conflates multiple possible degradation mechanisms.

## Next Checks

1. Run ablation on random non-deception features with identical procedure. If perplexity increases similarly but deception scores remain stable, this would support that SAE features are not specifically causal for deception but reflect general representation perturbation.

2. After identifying high-scoring deception features, directly intervene by editing model weights at the circuit level (e.g., using causal tracing to identify specific neurons) rather than SAE-based ablation. Compare whether deception can be selectively removed without perplexity degradation, which would validate or refute the entanglement hypothesis.

3. Repeat the cyclic ablation procedure on different model families (LLaMA, Pythia) and scales (1B, 7B, 13B parameters). Document whether functional regeneration and perplexity degradation patterns hold, or if larger models show different resilience profiles, to establish the scope of the distributed representation phenomenon.