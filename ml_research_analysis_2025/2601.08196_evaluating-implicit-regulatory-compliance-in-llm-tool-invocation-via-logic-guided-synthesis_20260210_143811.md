---
ver: rpa2
title: Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided
  Synthesis
arxiv_id: '2601.08196'
source_url: https://arxiv.org/abs/2601.08196
tags:
- safety
- llms
- compliance
- test
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present LOGISAFETYGEN, a framework that translates
  unstructured regulatory policies into Linear Temporal Logic (LTL) constraints and
  uses logic-guided fuzzing to synthesize valid execution traces. This approach bridges
  the "ambiguity gap" by formalizing regulations, the "validity gap" by ensuring executable
  traces, and the "inference gap" by masking safety steps to test whether LLMs can
  autonomously infer compliance requirements.
---

# Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis

## Quick Facts
- **arXiv ID**: 2601.08196
- **Source URL**: https://arxiv.org/abs/2601.08196
- **Reference count**: 13
- **Primary result**: LOGISAFETYGEN achieves 100% safety-critical API coverage vs 85% for LLM baseline, exposing significant compliance gaps in frontier models

## Executive Summary
LOGISAFETYGEN is a framework that evaluates whether large language models can autonomously infer and comply with implicit regulatory constraints when invoking tools. The system translates unstructured regulatory policies into Linear Temporal Logic (LTL) constraints, uses logic-guided fuzzing to synthesize valid execution traces, and masks safety steps in user instructions to test autonomous compliance inference. The framework constructs LOGISAFETYBENCH, a benchmark of 240 human-verified tasks across financial, healthcare, and IoT domains.

Evaluation of 13 state-of-the-art LLMs reveals that while larger models achieve higher functional correctness, they frequently violate implicit safety constraints—demonstrating that scaling does not resolve compliance failures. The logic-guided fuzzer achieves 100% coverage of safety-critical APIs versus 85% for baseline LLM generation, and the benchmark exposes significant inference gaps between task success and regulatory adherence.

## Method Summary
The framework operates through a three-stage pipeline: (1) LLM extracts regulations from policy documents and maps them to LTL templates (Operational Restriction and Instruction Adherence) with signature validation, (2) logic-guided fuzzer uses bottom-up DFS with dual-constraint pruning (precondition satisfaction and LTL monitor) to synthesize compliant traces, and (3) safety masking removes safety-critical API calls from ground-truth traces, then a generator-evaluator multi-agent pipeline creates goal-oriented or workflow-oriented instructions. GPT-5-Mini serves as the backbone for extraction and instruction synthesis, with evaluation using dual-oracle metrics measuring both functional correctness and safety compliance.

## Key Results
- Logic-guided fuzzing achieves 100% safety-critical API coverage versus 85% for LLM baseline generation
- GPT-5 achieves 75% Safe Success Rate while other models show 45-55% rates, with larger models frequently violating safety constraints
- Safe Success Rate drops significantly (28% vs 75%) when switching from workflow-oriented to goal-oriented prompts in Smart Home IoT domain
- Coverage metrics (ATC) show 14-62 percentage point improvements over LLM baselines across domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting unstructured regulations into Linear Temporal Logic (LTL) constraints enables deterministic compliance verification.
- Mechanism: The framework extracts atomic constraints from policy documents and maps them to two LTL templates: (1) Operational Restrictions (¬((¬P1)UP2)) forbidding sensitive actions until checks complete, and (2) Instruction Adherence (□(P1→⋄P2)) requiring trigger-action sequences. A signature validator grounds predicates against the API schema, eliminating hallucinated functions.
- Core assumption: Regulatory compliance in high-stakes domains can be adequately captured by temporal ordering constraints on API calls.
- Evidence anchors:
  - [abstract] "translates unstructured regulatory policies into Linear Temporal Logic (LTL) constraints"
  - [Section 3.2] Describes dual-template formulation and signature validation pipeline
  - [corpus] Weak direct evidence; ToolSafe addresses runtime safety but not LTL-based formalization
- Break condition: Regulations requiring probabilistic judgment, semantic parameter inspection, or constraints beyond temporal ordering will not be captured.

### Mechanism 2
- Claim: Logic-guided fuzzing generates structurally diverse, guaranteed-executable traces that outperform LLM-based generation for safety-critical API coverage.
- Mechanism: A constraint-satisfaction fuzzer treats trace construction as bounded search with dual pruning: (1) Precondition Satisfaction validates API executability, (2) Safety Satisfaction checks LTL compliance via runtime monitor. Bottom-up DFS with lazy evaluation prunes invalid branches immediately.
- Core assumption: Safety violations arise from rare state combinations that probabilistic LLM sampling will miss due to sampling bias.
- Evidence anchors:
  - [abstract] "uses logic-guided fuzzing to synthesize valid execution traces"
  - [Section 3.3] Details the dual-constraint pruning and bottom-up search strategy
  - [Section 5.2] Reports 100% safety-critical API coverage vs 85% for LLM baseline; ATC improvements of 14-62 percentage points
- Break condition: Highly complex API state spaces may still exceed tractable search; fuzzing bounded by trace length limits.

### Mechanism 3
- Claim: Masking safety steps in user instructions creates an "inference gap" that exposes LLMs' tendency to prioritize task completion over implicit compliance.
- Mechanism: Ground-truth compliant traces τ* are filtered to remove safety-critical APIs (τbus = Filter(τ*, ¬Asafe)). Generator-Evaluator agents synthesize instructions that describe only business goals/workflows, forcing LLMs to infer omitted regulatory steps from context.
- Core assumption: Real-world users will not explicitly specify compliance requirements; autonomous inference is necessary for safe deployment.
- Evidence anchors:
  - [abstract] "masking safety steps to test whether LLMs can autonomously infer compliance requirements"
  - [Section 3.4] Describes masking function M and two instruction typologies (goal-oriented vs workflow-oriented)
  - [Section 5.3] Shows GPT-5 performance drops from 75% to 28% when switching from workflow to goal-oriented prompts in Smart Home IoT
- Break condition: If LLMs are given explicit access to regulation documents in prompts, the inference challenge reduces to retrieval and application rather than autonomous inference.

## Foundational Learning

- Concept: Linear Temporal Logic (LTL/LTLf)
  - Why needed here: Foundation for formalizing regulatory constraints as verifiable oracles; operators like ALWAYS (□), EVENTUALLY (⋄), and UNTIL (U) express temporal safety properties.
  - Quick check question: Can you express "authentication must precede any fund transfer" using LTL operators?

- Concept: Fuzzing and Coverage Metrics
  - Why needed here: Logic-guided fuzzing replaces probabilistic LLM generation with systematic state-space exploration; coverage metrics (ATC, API coverage) quantify test quality.
  - Quick check question: Why does Adjacent Transition Coverage (ATC) capture more safety-relevant behavior than simple API coverage?

- Concept: Tool Invocation Semantics and Preconditions
  - Why needed here: Valid traces require actions to be executable in current states; precondition validation prevents hallucinated or semantically invalid API calls.
  - Quick check question: What happens if a fuzzer proposes an API call without validating its preconditions against the current environment state?

## Architecture Onboarding

- Component map: Policy Extraction & LTL Generation -> Signature Validator -> Logic-Guided Fuzzer with Dual-Constraint Pruning -> Safety Masking Filter -> Generator-Evaluator Multi-agent Pipeline -> Dual-Oracle Evaluator (Functional + Safety)
- Critical path: The LTL oracle quality directly constrains fuzzer output; masking fidelity determines inference gap authenticity. Human verification sits at two checkpoints: LTL formula acceptance (73.9% rate) and final task verification (70.6% acceptance).
- Design tradeoffs: Restricting to two LTL templates enables automation but limits constraint expressiveness (cannot capture parameter-level safety). Using fuzzing over LLM generation improves coverage but requires custom implementation (~3600 LOC).
- Failure signatures: (1) High functional success + low safety compliance = "unsafe success" (models complete tasks via violations); (2) Workflow > Goal performance gap indicates planning deficits; (3) Semantic errors despite low syntax errors indicate API grounding failures.
- First 3 experiments:
  1. Replicate coverage comparison: Run GPT-5-Mini baseline vs fuzzer on one domain, measure Safety-Critical API Coverage and ATC.
  2. Ablate instruction typology: Test same model on goal-oriented vs workflow-oriented prompts, quantify inference gap magnitude.
  3. Scale model comparison: Evaluate 3+ models across API density levels (Financial=4, Healthcare=7, IoT=10 safety APIs) to confirm combinatorial stress effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can safety oracles be evolved beyond temporal ordering to capture fine-grained data-flow and parameter-level constraints, such as verifying transaction limits or detecting malicious payloads?
- **Basis in paper:** [explicit] The authors explicitly identify the "Representation Limits of Safety Oracles" as a limitation, noting that the current two-template LTL approach focuses on API ordering and misses parameter-level semantics.
- **Why unresolved:** Current templates formalize *when* an action occurs (Operational Restriction) or *if* an outcome follows (Instruction Adherence), but cannot inspect the semantic validity of the arguments passed to those APIs.
- **What evidence would resolve it:** An extension of LOGISAFETYGEN that successfully synthesizes and validates constraints on function arguments (e.g., amount < limit) using more sophisticated logic representations.

### Open Question 2
- **Question:** Why do certain model architectures exhibit inverse scaling trends for safety, where increased capability correlates with higher rates of regulatory non-compliance?
- **Basis in paper:** [inferred] The results section highlights "Divergent Scaling Trends," noting that while GPT-5 improves with scale, the Gemini family defies this logic (e.g., Gemini-Pro commits more adherence violations than Gemini-Flash).
- **Why unresolved:** The paper observes this phenomenon but does not determine if it stems from training data composition, specific architectural decisions, or over-optimization for functional utility over constraint adherence.
- **What evidence would resolve it:** A mechanistic interpretability study or an ablation study identifying the specific training factors that cause capable models within the same family to regress in safety compliance.

### Open Question 3
- **Question:** Can automated test generation frameworks overcome the "creative priors" of their seeding LLMs to autonomously generate structurally novel edge cases?
- **Basis in paper:** [inferred] The authors note that while fuzzing ensures validity, the initial diversity is "partially bounded by the creative priors of the backbone LLM," limiting structural novelty without human guidance.
- **Why unresolved:** The current hybrid approach (LLM seeding + logic fuzzing) relies on the generator's existing knowledge distribution, potentially missing rare, "unknown unknown" scenarios that require reasoning outside the training manifold.
- **What evidence would resolve it:** A demonstration of a generator creating valid, diverse traces for safety APIs that were under-represented or absent in the seeding model's original training data.

## Limitations
- The framework's effectiveness depends on the assumption that regulatory compliance can be reduced to LTL temporal ordering constraints, excluding regulations requiring probabilistic reasoning or semantic parameter validation
- The fuzzer's performance degrades exponentially with trace length and API state complexity, limiting scalability to real-world workflows
- The safety masking methodology assumes ground-truth compliant traces are available, which may not hold for novel domains without expert curation

## Confidence
- **High Confidence**: Logic-guided fuzzing achieving 100% safety-critical API coverage vs 85% for LLM baseline; the coverage comparison is directly measurable and empirically demonstrated.
- **Medium Confidence**: The inference gap methodology (masking safety steps) reliably exposes LLM compliance failures; while demonstrated, the assumption that users will never specify safety requirements may not hold in practice.
- **Low Confidence**: The dual-template LTL formulation captures all relevant regulatory constraints; the restriction to two templates inherently limits expressiveness, and no evaluation demonstrates handling of more complex temporal properties.

## Next Checks
1. **Generalizability Test**: Apply LOGISAFETYGEN to a new regulatory domain (e.g., autonomous vehicle safety or financial anti-money laundering) without human-verified ground truth, measuring LTL extraction accuracy and compliance detection rates.
2. **Prompting Intervention**: Evaluate whether providing explicit regulatory document access to LLMs (rather than requiring autonomous inference) eliminates the safety compliance gap, testing the fundamental assumption about user behavior.
3. **Scalability Benchmark**: Systematically vary API state space size and trace length to quantify the exponential complexity floor of logic-guided fuzzing, identifying practical limits for deployment in complex industrial workflows.