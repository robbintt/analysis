---
ver: rpa2
title: 'Equilibrium Policy Generalization: A Reinforcement Learning Framework for
  Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games'
arxiv_id: '2511.00811'
source_url: https://arxiv.org/abs/2511.00811
tags:
- uni00000013
- policy
- pursuer
- evader
- equilibrium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework for pursuit-evasion
  games that can generalize to unseen graph structures without fine-tuning. The core
  idea is to train policies across diverse graphs against equilibrium policies for
  each individual graph, using dynamic programming for exact equilibria in no-exit
  cases and a heuristic matching approach for multi-exit scenarios.
---

# Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games

## Quick Facts
- arXiv ID: 2511.00811
- Source URL: https://arxiv.org/abs/2511.00811
- Reference count: 40
- Zero-shot generalization of pursuit-evasion policies across unseen graph structures

## Executive Summary
This paper proposes a reinforcement learning framework for pursuit-evasion games that can generalize to unseen graph structures without fine-tuning. The core idea is to train policies across diverse graphs against equilibrium policies for each individual graph, using dynamic programming for exact equilibria in no-exit cases and a heuristic matching approach for multi-exit scenarios. A sequence model with shortest-path distance features enables cross-graph representation. Experiments show the trained pursuer and evader policies achieve strong zero-shot performance across real-world graphs, outperforming shortest-path strategies and matching or exceeding fine-tuned state-of-the-art results in multi-exit settings. The approach also scales to large graphs and varying numbers of pursuers.

## Method Summary
The framework trains policies to generalize across graph structures by learning against graph-specific equilibrium oracles. For no-exit graphs, dynamic programming computes exact equilibria; for multi-exit graphs, a bipartite matching heuristic generates reference actions. The policy architecture uses a 6-layer masked self-attention encoder with a pointer network decoder, trained via Discrete Soft Actor-Critic with an additional KL-divergence term encouraging similarity to the oracle's action. The state representation uses normalized shortest-path distances from each agent to all others and exits, enabling cross-graph generalization. Training uses 152 graphs (76 Dungeon environment maps discretized to 100 and 500 nodes) with a batch size of 128, learning rate of 1e-5, and KL coefficient of 0.1.

## Key Results
- Zero-shot performance on real-world graphs outperforms shortest-path strategies and matches or exceeds fine-tuned state-of-the-art in multi-exit settings
- Cross-graph generalization achieved without fine-tuning on target graphs
- Successful scaling to large graphs (500 nodes) and varying numbers of pursuers (m > 3)
- Ablation studies confirm shortest-path distance features are essential for generalization

## Why This Works (Mechanism)
The framework leverages equilibrium oracles as reference policies during training, allowing the agent to learn robust strategies that approximate optimal play across diverse graph structures. By representing states as shortest-path distances rather than raw coordinates, the policy captures graph topology in a way that transfers across different graph instances. The KL-divergence term in the loss function ensures the learned policy stays close to the oracle's actions while still learning value through standard RL objectives. This combination enables the agent to develop strategies that generalize to unseen graphs by focusing on relative positioning and graph connectivity rather than memorizing specific graph layouts.

## Foundational Learning
- **Dynamic Programming for Equilibrium Computation**: Computes exact optimal policies for no-exit graphs by backward induction through state space
  - Why needed: Provides ground-truth optimal behavior for training
  - Quick check: Verify DP returns finite values and correct minimax policies on simple test graphs

- **Bipartite Matching for Multi-Exit Heuristic**: Generates reference actions by solving a minimum-weight matching problem between pursuers and exits
  - Why needed: Scalable approach to approximate equilibria when exact computation is intractable
  - Quick check: Confirm matching algorithm produces reasonable pursuer-evader assignments on test cases

- **Shortest-Path Distance Features**: Represents agent positions as normalized distances to all other agents and exits
  - Why needed: Enables graph-agnostic state representation that transfers across structures
  - Quick check: Verify feature normalization and that distances capture meaningful topological relationships

## Architecture Onboarding
- **Component Map**: Graph Sampler -> State Preprocessor (Floyd-Warshall) -> Sequence Model (Encoder + Pointer Network) -> Discrete SAC -> Oracle Query -> Policy Update
- **Critical Path**: State features → model prediction → action selection → environment step → oracle action → policy update
- **Design Tradeoffs**: Distance-based features sacrifice local detail for generalization; sequence model trades CNN locality for permutation invariance
- **Failure Signatures**: 
  - "Blind" agents (no strategic movement) indicate incorrect state representation
  - Poor performance on training graphs suggests oracle implementation errors
  - Unstable training points to incorrect KL coefficient
- **First Experiments**:
  1. Train on single graph with DP oracle, verify convergence to near-optimal performance
  2. Test zero-shot transfer to structurally similar graphs
  3. Ablate state features to confirm distance representation is essential

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Procedural generation details for training graphs are unspecified, requiring assumptions about Dungeon environment defaults
- Value-decomposition network implementation for >3 pursuers is only briefly mentioned without specifics
- Exact termination condition logic for complex graph structures may vary from interpretation

## Confidence
- **High confidence** in the core RL framework and equilibrium oracle approach, as these are well-specified and follow standard algorithms
- **Medium confidence** in generalization results, as performance is demonstrated but depends on potentially unreproducible training data
- **Medium confidence** in scaling claims, since the extension to larger graphs and more pursuers is mentioned but details are sparse

## Next Checks
1. Verify state representation: Confirm that shortest-path distance features (not raw coordinates) are essential for cross-graph generalization by replicating the ablation study results
2. Debug Oracle implementation: Test Algorithm 1 on simple no-exit graphs to ensure finite value propagation and correct minimax policies
3. Hyperparameter sensitivity: Validate that KL coefficient β=0.1 is optimal by testing performance across a range of values and checking for blind imitation or unstable training