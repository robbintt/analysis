---
ver: rpa2
title: Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens
  of Summarization
arxiv_id: '2503.01670'
source_url: https://arxiv.org/abs/2503.01670
tags:
- hallucination
- knowledge
- factual
- qwen2
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first systematic evaluation of LLMs\u2019\
  \ ability to detect mixed-context hallucinations in summarization, distinguishing\
  \ between factual (factually correct but unfaithful) and non-factual (factually\
  \ incorrect) hallucinations. An automated pipeline was developed to construct a\
  \ balanced benchmark dataset (FHSumBench) of 1,336 summaries with evenly distributed\
  \ hallucination types."
---

# Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens of Summarization

## Quick Facts
- **arXiv ID:** 2503.01670
- **Source URL:** https://arxiv.org/abs/2503.01670
- **Reference count:** 40
- **Primary result:** Retrieval-based evaluators outperform direct generation methods for detecting mixed-context hallucinations in summarization, with reflection retrieval showing best performance across model scales.

## Executive Summary
This paper presents the first systematic evaluation of LLMs' ability to detect mixed-context hallucinations in summarization, distinguishing between factual (factually correct but unfaithful) and non-factual (factually incorrect) hallucinations. An automated pipeline was developed to construct a balanced benchmark dataset (FHSumBench) of 1,336 summaries with evenly distributed hallucination types. Experiments across direct generation and retrieval-based evaluators of varying scales (0.5B to 72B parameters) show that model scaling does not guarantee improved performance. The primary bottleneck is detecting factual hallucinations, which requires effective integration of LLMs' intrinsic knowledge with external evidence. Retrieval-based methods (particularly reflection retrieval) significantly improve accuracy by reducing knowledge bias, with smaller models benefiting more from evidence retrieval.

## Method Summary
The study constructs FHSumBench by injecting factual and non-factual entity descriptions into correct summaries from XEnt and FactCollect datasets. Evaluators are categorized into direct generation (Vanilla, +ICL, +CoT) and retrieval-based methods (Knowledge Retrieval, Concurrent Retrieval, Reflection Retrieval). The reflection retrieval approach uses a two-stage process: first-stage document retrieval for faithfulness evaluation, followed by targeted knowledge retrieval based on the model's reflection on missing information. Experiments run inference-only evaluations across multiple model sizes using vLLM on 4× A100-80GB GPUs, with retrieval via Llama-Index. Performance is measured using macro precision, recall, F1-score, and per-category accuracy for the three hallucination types.

## Key Results
- Retrieval-based methods significantly outperform direct generation evaluators across all model scales, with reflection retrieval showing best performance
- Model scaling (0.5B to 72B parameters) does not guarantee improved hallucination detection performance
- Factual hallucinations present the most persistent bottleneck, with all methods showing lowest accuracy for this category
- Smaller models benefit more from retrieval augmentation compared to larger models
- Retrieval reduces knowledge bias, with 47-58% of error cases corrected when using retrieval-based methods

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-based evaluation mitigates intrinsic knowledge bias
Incorporating external evidence retrieval improves LLMs' ability to detect mixed-context hallucinations by providing explicit external evidence that reduces reliance on potentially incomplete or biased intrinsic knowledge. The model evaluates claims against both source documents and retrieved knowledge bases, forcing separation between faithfulness and factuality. This works when retrieved evidence is accurate, relevant, and comprehensive enough to support reliable judgments.

### Mechanism 2: Factual hallucinations present a persistent bottleneck due to knowledge-integration conflicts
Detecting factual hallucinations (unfaithful but factually correct) is harder than detecting non-factual hallucinations because it requires simultaneously recognizing content as unfaithful to the source while acknowledging its factual correctness. This creates conflict between the model's intrinsic knowledge (which confirms factuality) and the evaluation task (which requires flagging unfaithfulness), leading to systematic misclassification toward "no hallucination."

### Mechanism 3: Reflection retrieval enables more targeted evidence gathering
The two-stage reflection retrieval process outperforms simpler retrieval approaches by first establishing faithfulness baseline through document retrieval, then having the model reflect on what information is missing and generate targeted queries for knowledge retrieval. This sequential approach prevents information overload and ensures retrieved knowledge is relevant to specific ambiguous claims rather than all entities in the summary.

## Foundational Learning

- **Concept: Mixed-context hallucination (factual vs. non-factual)**
  - Why needed: This paper's core contribution is distinguishing hallucinations that are factually correct but unfaithful (factual hallucinations) from those that are both unfaithful and factually incorrect (non-factual hallucinations).
  - Quick check: A summary states "Paris, the capital of France" when the source document only mentions "Paris" without specifying the country. Is this a factual or non-factual hallucination?

- **Concept: Retrieval-augmented generation (RAG) for evaluation**
  - Why needed: The paper compares direct generation evaluators (using only intrinsic knowledge) with retrieval-based evaluators that incorporate external evidence.
  - Quick check: Why might retrieving all available knowledge about an entity harm evaluation accuracy compared to retrieving only relevant evidence?

- **Concept: Knowledge bias in LLMs**
  - Why needed: The paper identifies intrinsic knowledge bias as the primary cause of errors, especially for factual hallucinations.
  - Quick check: If an LLM's training data states "Belfast is in the UK" but a source document mentions "Belfast" without location context, how might this affect hallucination detection?

## Architecture Onboarding

- **Component map:** Dataset Construction Pipeline -> Evaluators (Direct Generation + Retrieval-Based) -> Evaluation Framework (Claim Extraction + Evidence Retrieval + Scoring)
- **Critical path:** Build balanced benchmark (FHSumBench) → Run baseline evaluators (EntFA, Hybrid Score) → Test direct generation methods across model scales → Implement and compare retrieval strategies → Analyze per-category performance and error cases
- **Design tradeoffs:**
  - Retrieval coverage vs. precision: Wikidata has low entity coverage (35.9%) but high precision; LLM-generated knowledge has high coverage (94% accuracy) but may introduce hallucinations
  - Retrieval strategy complexity: Reflection retrieval is more effective but computationally expensive; knowledge retrieval is simpler but less targeted
  - Model scale vs. benefit: Larger models show more stable performance but don't guarantee better results; smaller models benefit more from retrieval augmentation
- **Failure signatures:**
  - High false negative rate for factual hallucinations (misclassified as "no hallucination")
  - Performance plateau or degradation at largest model scales (72B, GPT-4o)
  - Low retrieval coverage causing evaluation gaps
  - Overcorrection in iterative reflection loops
- **First 3 experiments:**
  1. Run vanilla judge, +ICL, and +CoT on FHSumBench with 3 different model sizes (8B, 32B, 72B) to quantify the factual hallucination bottleneck and knowledge bias effects
  2. Implement all four retrieval methods (Hybrid, KR, CR, RR) on a single model (e.g., Qwen2.5-14B) to identify which approach best addresses the knowledge integration challenge
  3. Test variations of RR (Wikidata vs. LLM-generated KB, k=1 vs. k=3 retrieval, single vs. iterative reflection) to understand which components contribute most to performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific information fusion architectures can effectively balance intrinsic parametric knowledge with external context to improve the detection of factual hallucinations?
- **Basis:** The conclusion states there is an "urgent need for research into more effective information fusion approaches," noting that while current methods help smaller models, a fundamental challenge remains in utilizing knowledge effectively.
- **Why unresolved:** The paper demonstrates that simply adding retrieval (RAG) is insufficient, as models still struggle to decide when to trust external evidence over internal knowledge.
- **What evidence would resolve it:** Development of a new attention mechanism or gating function that successfully upweights retrieved evidence over parametric knowledge specifically for "factual but unfaithful" instances.

### Open Question 2
- **Question:** How do training data composition and internal information flow contribute to the "intrinsic knowledge bias" that hinders LLM-as-a-judge performance?
- **Basis:** Section 7 calls for "deeper exploration" to understand "training data updates, internal information flow, and mechanisms for integrating internal and external knowledge" to reduce bias.
- **Why unresolved:** The paper identifies that LLMs rely on prior knowledge even when inappropriate (causing false negatives in factual hallucinations), but does not isolate the architectural or training causes.
- **What evidence would resolve it:** Mechanistic interpretability studies (e.g., probing classifiers) that trace how "priorknowledge" features override "context" features in the model's residual stream during evaluation.

### Open Question 3
- **Question:** Does the performance plateau observed in larger models (72B+) for mixed-context evaluation persist across non-news domains or tasks requiring extended context?
- **Basis:** RQ4 (Section 5.3) notes that "scaling up model size does not necessarily correlate with improved performance," yet Section 7 suggests this limitation might evolve in "more flexible scenarios... extended dialogues and more intricate task workflows."
- **Why unresolved:** The study is restricted to summarization; it is unclear if the lack of scaling benefits is an artifact of the task simplicity or a fundamental limitation of current model alignment.
- **What evidence would resolve it:** A scaling law analysis performed on the FHSumBench task across diverse domains (e.g., code, dialogue) to see if the performance curve remains flat or negative for 70B+ models.

### Open Question 4
- **Question:** Can current retrieval-based evaluators accurately detect discourse-level or event-based hallucinations, or are they limited to the entity-level inconsistencies tested in FHSumBench?
- **Basis:** The Limitations section states the work "does not address discourse-level hallucinations or event-based hallucinations," focusing exclusively on entity knowledge injection.
- **Why unresolved:** The benchmark construction relies on appositive entity modifications (e.g., "Belfast, a city in America"), leaving the models' ability to reason about temporal or logical event conflicts untested.
- **What evidence would resolve it:** Evaluation of the proposed Reflection Retrieval method on a dataset constructed with event-hallucinations (e.g., swapped verbs or incorrect temporal relations) rather than entity attributes.

## Limitations
- Low knowledge base coverage (35.9%) from Wikidata creates evaluation gaps requiring GPT-4o-generated knowledge bases
- Reflection retrieval effectiveness depends on model's ability to generate appropriate reflective queries, which varies across models
- Benchmark construction relies on controlled fact injection that may not fully capture real-world summarization errors
- Limited evaluation to summarization task without testing on diverse domains or discourse-level hallucinations

## Confidence
- **High Confidence:** Retrieval-based methods consistently outperform direct generation evaluators across multiple model sizes and retrieval strategies
- **Medium Confidence:** Reflection retrieval shows best performance on FHSumBench but advantage is less pronounced on M-XSum dataset
- **Low Confidence:** Claim that retrieval "reduces knowledge bias" lacks direct quantitative measurement of bias reduction

## Next Checks
1. Measure actual knowledge base coverage achieved in GPT-4o-generated KB and assess consistency across different entity types and domains
2. Test reflection retrieval on out-of-domain datasets and with models of varying capabilities to verify consistent performance advantage
3. Implement direct measurement of knowledge bias by comparing evaluator judgments with and without retrieval on summaries containing known factual correctness vs. faithfulness conflicts