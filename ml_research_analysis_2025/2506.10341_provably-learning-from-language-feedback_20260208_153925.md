---
ver: rpa2
title: Provably Learning from Language Feedback
arxiv_id: '2506.10341'
source_url: https://arxiv.org/abs/2506.10341
tags:
- feedback
- action
- learning
- hypothesis
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learning from Language Feedback (LLF), a
  formal framework for interactive learning where agents learn to maximize reward
  solely through natural language feedback rather than scalar rewards. The authors
  propose the transfer eluder dimension as a new complexity measure to characterize
  the hardness of LLLF problems, capturing how efficiently language feedback reduces
  uncertainty about rewards.
---

# Provably Learning from Language Feedback

## Quick Facts
- arXiv ID: 2506.10341
- Source URL: https://arxiv.org/abs/2506.10341
- Reference count: 40
- Agents learn to maximize reward solely through natural language feedback rather than scalar rewards

## Executive Summary
This paper introduces Learning from Language Feedback (LLF), a formal framework for interactive learning where agents learn to maximize reward solely through natural language feedback rather than scalar rewards. The authors propose the transfer eluder dimension as a new complexity measure to characterize the hardness of LLLF problems, capturing how efficiently language feedback reduces uncertainty about rewards. They develop HELiX, a provably efficient algorithm that achieves regret bounds scaling with the transfer eluder dimension, demonstrating cases where learning from rich language feedback can be exponentially faster than traditional reward-based learning.

## Method Summary
The framework treats LLF as a contextual bandit problem where the agent observes language feedback $O_t$ after taking action $A_t$ and must maximize cumulative reward without ever observing numerical rewards. HELiX maintains a set of hypotheses about the reward function, using a verifier to compute consistency between each hypothesis and observed feedback. The algorithm prunes hypotheses with cumulative verifier loss exceeding a threshold, then either exploits consensus (when all remaining hypotheses agree on the optimal action) or explores using upper confidence bounds based on the most optimistic hypothesis.

## Key Results
- Transfer eluder dimension captures how efficiently language feedback reduces uncertainty, shrinking hypothesis spaces rapidly when feedback is informative
- HELiX achieves regret bounds scaling with transfer eluder dimension, showing exponential speedups in cases where feedback reveals key information (e.g., first mistake in reasoning chains)
- Empirical results on Wordle, Battleship, and Minesweeper show HELiX consistently outperforms greedy LLM baselines, with exploitation-exploitation strategy significantly improving performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rich language feedback can lower the "transfer eluder dimension," allowing agents to learn exponentially faster than learning from scalar rewards alone.
- **Mechanism:** The framework introduces the **transfer eluder dimension** to measure how efficiently feedback reduces uncertainty about the reward function. Unlike standard eluder dimensions which measure dependence on past rewards, this metric measures dependence on past *verifier loss* (consistency between feedback and hypotheses). If feedback is highly informative (e.g., revealing the first mistake in a chain of reasoning), this dimension remains small, shrinking the hypothesis space rapidly.
- **Core assumption:** The feedback is **reward-informative** (Definition 5), meaning the verifier can distinguish hypotheses based on feedback to the same extent as their reward differences.
- **Evidence anchors:**
  - [abstract] "transfer eluder dimension... captures the intuition that information in the feedback changes the learning complexity"
  - [Section 3.2] Demonstrates cases (e.g., Bitwise feedback) where transfer eluder dimension is 1, while reward-based complexity is exponential ($2^L$).
  - [corpus] *Eluder dimension: localise it!* (arXiv:2601.09825) highlights standard eluder dimension limits, reinforcing the need for the "transfer" variant introduced here to capture information richness.
- **Break condition:** If feedback is uninformative or adversarial (e.g., random text), the transfer eluder dimension becomes infinite, reverting learning complexity to standard worst-case bounds.

### Mechanism 2
- **Claim:** A "verifier" function allows the agent to translate semantic language feedback into a mathematical loss signal for hypothesis elimination.
- **Mechanism:** The agent maintains a set of plausible text hypotheses ($H_t$). Upon receiving language feedback $O_t$, the **verifier** computes a loss $\ell(A_t, O_t, \eta)$ for each hypothesis $\eta$. Hypotheses with cumulative losses exceeding a confidence threshold are pruned from the version space. This effectively approximates a "gradient" for language, steering the policy without explicit numerical rewards.
- **Core assumption:** **Assumption 2 (Verifier):** The agent has access to a verifier that can quantify alignment between feedback and a hypothesis.
- **Evidence anchors:**
  - [Section 2.3] "We introduce the notion of a verifier... assessing whether a hypothesis is consistent with observed feedback."
  - [Section 3.4] Algorithm 1 updates the confidence set $H_t$ by filtering hypotheses based on cumulative verifier loss.
- **Break condition:** If the verifier (often implemented via an LLM) is noisy or fails to ground the semantics of the feedback (e.g., assigns low loss to contradictory hypotheses), the true hypothesis $\eta^*$ may be erroneously eliminated, causing divergence.

### Mechanism 3
- **Claim:** Separating **exploration** (optimism) from **exploitation** (consensus) prevents over-exploration when feedback is sufficient to identify the optimal action but not the full reward landscape.
- **Mechanism:**
  1. **Exploitation:** The algorithm checks if all remaining hypotheses agree on a single optimal action (consensus). If so, it executes it immediately.
  2. **Exploration:** If hypotheses disagree, it selects the action that maximizes the reward under the most optimistic hypothesis (Upper Confidence Bound style).
  This dual strategy avoids the trap of exploring to learn the *exact* reward of suboptimal arms if the goal is simply to find the *best* arm.
- **Core assumption:** The LLM can generate diverse, valid hypotheses and score actions consistently across them.
- **Evidence anchors:**
  - [Section 3.4] "An additional design in HELiX compared to standard UCB is an explicit exploitation step... ensuring that the algorithm will not over-explore."
  - [Figure 2] Visualizes the "Thought Sampling with Cross-Verify" architecture implementing this strategy.
- **Break condition:** If the LLM fails to generate diverse hypotheses (collapsing to a single mode), the algorithm behaves greedily, losing exploration capabilities and failing in sparse feedback environments.

## Foundational Learning

- **Concept: Eluder Dimension**
  - **Why needed here:** This is the baseline complexity measure for bandit problems with function approximation. Understanding it is required to grasp why the paper's "transfer" variant is necessary to quantify the value of rich feedback.
  - **Quick check question:** Can you explain why an action is considered "$\epsilon$-independent" of previous actions in a standard bandit setting?

- **Concept: Hypothesis Testing & Version Spaces**
  - **Why needed here:** HELiX relies on maintaining a "confidence set" (version space) of hypotheses. You must understand how eliminating hypotheses based on observed data works to follow the algorithm's pruning logic.
  - **Quick check question:** How does the size of the version space relate to the certainty of the agent's policy?

- **Concept: Multi-Armed Bandits (UCB/Thompson Sampling)**
  - **Why needed here:** HELiX is a UCB-style algorithm adapted for language. The principles of balancing exploration (trying new things) and exploitation (using known good things) are central to the paper's regret guarantees.
  - **Quick check question:** In standard UCB, how does the confidence bonus change as an action is taken more frequently?

## Architecture Onboarding

- **Component map:** π_LLM (generates N hypotheses and actions) -> π_ref (generates M random actions) -> Cross-Verifier (scores all hypothesis-action pairs) -> Decision Engine (checks consensus or selects optimistic action)

- **Critical path:** The **Cross-Verify** step (Section 4, Figure 2). This is where the rubber meets the road—converting the LLM's internal "thoughts" into a matrix of numerical scores that drive the theoretical HELiX algorithm.

- **Design tradeoffs:**
  - **Finite vs. Infinite Hypothesis Space:** The theory assumes a large/infinite space $\mathcal{H}$, but the implementation uses a finite sample of $N$ thoughts. If $N$ is too small, the agent may miss the correct hypothesis. If too large, latency and cost explode.
  - **LLM as Verifier:** Using an LLM to score actions ($R_{LLM}$) is flexible but introduces noise and potential self-consistency hallucinations, risking the elimination of the true hypothesis.

- **Failure signatures:**
  - **Premature Exploitation:** The algorithm locks onto a suboptimal "consensus" action because the LLM-sampled hypotheses were not diverse enough to capture the true uncertainty.
  - **Hallucinated Constraints:** The verifier assigns high loss to the true hypothesis due to semantic misunderstanding of the feedback, causing the agent to learn from a completely wrong model of the world.

- **First 3 experiments:**
  1. **Baselines:** Compare HELiX against a "Greedy" LLM agent (which just picks the first thought) on *Wordle* or *Battleship* to validate that the exploration-exploitation logic actually helps.
  2. **Ablation on "Exploitation Step":** Run HELiX (No exploitation) vs. Full HELiX to see if preventing over-exploration improves sample efficiency, particularly in deterministic environments.
  3. **Sensitivity to $N$:** Sweep the number of thought samples ($N$) to find the sweet spot where hypothesis diversity is sufficient without incurring excessive API costs/latency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does there exist a complexity measure that both lower-bounds regret and informs practical LLF algorithm design?
- **Basis in paper:** [explicit] "The transfer eluder dimension assumes worst-case verifier behavior... Closing this gap by developing a complexity measure that both lower-bounds regret and informs practical algorithm design remains an important open question."
- **Why unresolved:** Transfer eluder dimension provides an upper bound but not a lower bound; some LLF problems with unbounded transfer eluder dimension are trivially solvable (e.g., when feedback directly reveals the optimal action).
- **What evidence would resolve it:** A formal lower-bound analysis showing tightness, or a new complexity measure with matching upper and lower regret bounds.

### Open Question 2
- **Question:** Can the DEC (Decision-Estimation Coefficient) framework be adapted to LLF in a way that remains implementable with LLMs?
- **Basis in paper:** [explicit] "A promising direction is to adapt DEC (Foster et al., 2024) to the LLF setting. However, the existing algorithm is not directly implementable using LLMs."
- **Why unresolved:** The DEC framework offers tighter theoretical characterization but requires computational primitives incompatible with how LLMs generate and evaluate hypotheses.
- **What evidence would resolve it:** An LLM-compatible algorithm achieving DEC-based regret bounds, or proof that such adaptation is fundamentally limited.

### Open Question 3
- **Question:** What training methods could instill in LLMs the properties needed for effective LLF (consistent hypothesis generation, fair cross-hypothesis scoring)?
- **Basis in paper:** [inferred] From Section 4 discussion: "Further research is needed to verify whether current LLMs possess these properties, and, if not, to determine what forms of training could instill them."
- **Why unresolved:** Current LLMs may not consistently produce diverse, faithful hypotheses or score actions fairly across different hypotheses, but targeted training procedures remain unexplored.
- **What evidence would resolve it:** Systematic evaluation of LLM hypothesis quality before/after specialized training, showing improved LLF performance correlates with improved consistency metrics.

## Limitations
- The framework's theoretical guarantees rely heavily on the assumption that language feedback is reward-informative and that the verifier can accurately assess hypothesis consistency.
- LLM-based verifiers introduce noise and potential semantic misunderstandings that could invalidate the hypothesis elimination process.
- The computational complexity of maintaining and scoring large hypothesis spaces may limit scalability beyond the tested environments.

## Confidence
- **High confidence**: The theoretical framework for transfer eluder dimension and its relationship to learning complexity is well-founded and mathematically rigorous.
- **Medium confidence**: The HELiX algorithm's empirical performance gains are demonstrated, but the results depend on specific LLM implementations and hyperparameters that aren't fully specified.
- **Medium confidence**: The claim that language feedback can reduce learning complexity exponentially compared to scalar rewards is supported theoretically but requires more extensive empirical validation across diverse environments.

## Next Checks
1. **Robustness testing**: Evaluate HELiX performance when the LLM verifier is deliberately made noisier or when feedback is partially adversarial to test the algorithm's tolerance to semantic ambiguity.
2. **Scalability assessment**: Test HELiX on environments with larger state/action spaces (e.g., longer words in Wordle or larger Battleship grids) to identify practical limits of the approach.
3. **Cross-LLM validation**: Implement HELiX using different LLM architectures (e.g., GPT-4, Llama) to determine if the approach depends on specific model properties or is more generally applicable.