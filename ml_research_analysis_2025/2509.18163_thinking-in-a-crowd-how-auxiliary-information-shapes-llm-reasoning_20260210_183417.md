---
ver: rpa2
title: 'Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning'
arxiv_id: '2509.18163'
source_url: https://arxiv.org/abs/2509.18163
tags:
- information
- reasoning
- thinking
- misleading
- helpful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how auxiliary information impacts the reasoning
  performance of large language models (LLMs) with explicit thinking modes. The authors
  introduce SciAux, a dataset derived from ScienceQA, containing questions augmented
  with helpful, irrelevant, and misleading context snippets.
---

# Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning

## Quick Facts
- arXiv ID: 2509.18163
- Source URL: https://arxiv.org/abs/2509.18163
- Authors: Haodong Zhao; Chenyan Zhao; Yansi Li; Zhuosheng Zhang; Gongshen Liu
- Reference count: 11
- One-line primary result: Thinking mode amplifies LLM vulnerability to misleading auxiliary information, causing accuracy drops up to 19 points on ScienceQA-derived questions.

## Executive Summary
This paper investigates how auxiliary information quality impacts the reasoning performance of large language models with explicit thinking modes. The authors introduce SciAux, a controlled dataset where each ScienceQA question is paired with helpful, irrelevant, and misleading context snippets. Experiments with Qwen3-8B and Hunyuan-7B-Instruct reveal that while helpful information improves accuracy, misleading information causes catastrophic performance drops that are amplified by the thinking process. The study demonstrates that explicit reasoning can deepen vulnerability to misinformation, highlighting the need for models to critically evaluate reasoning inputs rather than simply generating longer thought chains.

## Method Summary
The study evaluates Qwen3-8B and Hunyuan-7B-Instruct on SciAux, a dataset of 6,828 ScienceQA-derived questions each augmented with six auxiliary snippets (two helpful, two misleading, two irrelevant). Models are tested under thinking ON/OFF conditions across baseline, single-snippet, and paired-snippet scenarios. Accuracy serves as the primary metric, with response token counts and failure mode classifications providing additional analysis. The Qwen model toggles thinking via an `enable_thinking` parameter, while Hunyuan uses a `/no_think` prefix to disable thinking.

## Key Results
- Thinking mode amplifies both benefits of helpful context and harms of misleading context, creating an asymmetrical vulnerability
- Misleading information causes accuracy drops up to 19 points when thinking is enabled (Qwen3-8B with M1+M2)
- Thinking-mode failures cluster into five distinct categories, with over-analysis and misapplication being most prevalent
- The marginal benefit of thinking mode decreases monotonically as information quality declines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit thinking mode amplifies both the benefits of helpful context and the harms of misleading context, creating an asymmetrical vulnerability.
- Mechanism: When thinking mode is active, the model generates longer reasoning traces (~950 tokens with misleading info vs. ~447 tokens with helpful info for Qwen3-8B), indicating deeper engagement with auxiliary content. This engagement causes the model to rationalize and integrate misleading premises into its reasoning chain rather than rejecting them.
- Core assumption: The extended token length reflects genuine deliberative processing rather than mere verbosity.
- Evidence anchors:
  - [abstract] "While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process."
  - [section 4.4, Figure 5] Shows average response tokens: Qwen3-8B with thinking mode produces 950.3 tokens for misleading vs. 447.3 for helpful information.
  - [corpus] Weak corpus signal; no directly comparable studies on thinking-mode amplification effects found in neighbors.
- Break condition: If models are trained with explicit misinformation rejection objectives, or if auxiliary information includes credibility signals, the amplification effect should diminish.

### Mechanism 2
- Claim: The marginal benefit of thinking mode decreases monotonically as information quality declines.
- Mechanism: Thinking mode provides structured decomposition that helps when reasoning from clean or helpful inputs. However, when inputs contain noise or falsehoods, the same decomposition process propagates errors through each reasoning step, reducing or reversing net gains.
- Core assumption: The decomposition process does not include intrinsic verification of premise validity.
- Evidence anchors:
  - [section 4.6] "Overall, the benefit of thinking mode decreases as the quality of the provided information declines: it is largest with no information, moderate with Helpful and Irrelevant information, and minimal or negative with Misleading information."
  - [Table 2] Qwen baseline thinking gain: +9.87 points; with M1+M2: +2.16 points (and -19.19 relative to baseline with thinking on).
  - [corpus] No directly comparable gradient analysis found; corpus focuses on misinformation detection rather than reasoning-mode interaction.
- Break condition: If models implement step-wise premise verification (e.g., self-rag style critique), the monotonic decline pattern should flatten.

### Mechanism 3
- Claim: Thinking-mode failures cluster into five distinct categories, with over-analysis and misapplication being most prevalent.
- Mechanism: Explicit reasoning overrides intuitive correct responses through excessive granularity, incorrect rule application, or failure to prioritize relevant information paths. This resembles human cognitive biases where deliberation can undermine expertise-based intuition.
- Core assumption: The classification scheme is exhaustive and mutually exclusive for the observed failure modes.
- Evidence anchors:
  - [section 5, Table 5] Defines five failure types: Over-analysis and Misinterpretation (26.39% Qwen, 28.91% Hunyuan), Misapplication and Overgeneralization (27.78% Qwen, 18.55% Hunyuan), Correct Reasoning/Flawed Execution, Factual Errors/Hallucinations, Indecision/Flawed Prioritization.
  - [Figure 6] Shows distribution of failure types for "Intuitive Leap" cases (correct without thinking, incorrect with thinking).
  - [corpus] Related work on hallucination (Ji et al., 2023) cited but not empirically connected to this taxonomy.
- Break condition: If models are trained with calibration on reasoning depth, over-analysis rates should decrease.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The paper's "thinking mode" is an architectural implementation of CoT. Understanding that CoT improves performance on complex tasks but doesn't guarantee reasoning fidelity is essential background.
  - Quick check question: Can you explain why a correct final answer doesn't necessarily indicate sound intermediate reasoning steps?

- Concept: Retrieval-Augmented Generation (RAG) Vulnerabilities
  - Why needed here: The paper's motivation stems from RAG systems retrieving noisy or misleading documents. Understanding the "garbage in, garbage out" problem in RAG contexts frames why auxiliary information quality matters.
  - Quick check question: What are three failure modes when a retrieval system returns documents with mixed relevance?

- Concept: Cognitive Biases in Human vs. Machine Reasoning
  - Why needed here: The paper draws explicit parallels between LRM failure modes and human cognitive heuristics (over-analysis, misapplication). This framing helps interpret why deliberation can degrade performance.
  - Quick check question: How does the "Intuitive Leap" failure category mirror human expertise reversal under explicit analysis?

## Architecture Onboarding

- Component map:
  - SciAux Dataset -> Qwen3-8B/Hunyuan-7B-Instruct -> Accuracy Metric -> Failure Mode Classification

- Critical path:
  1. Load SciAux item with selected auxiliary snippet(s).
  2. Apply model-specific thinking toggle.
  3. Generate response with structured prompt (auxiliary info → question → choices → answer prompt).
  4. Parse final answer index; log response tokens and intermediate reasoning trace.

- Design tradeoffs:
  - Controlled vs. realistic retrieval: SciAux uses synthetic auxiliary info rather than real retrieval outputs, trading ecological validity for experimental control.
  - Binary thinking toggle: Models support ON/OFF but not intermediate reasoning depths; limits granularity of analysis.
  - Single-hop evaluation: ScienceQA focuses on bounded reasoning tasks; may not generalize to multi-turn or open-ended scenarios.

- Failure signatures:
  - **High token count + low accuracy**: Strong indicator of misleading information rationalization (see Figure 5, misleading bar).
  - **Intuitive Leap pattern**: Correct in OFF mode, incorrect in ON mode suggests over-analysis or misapplication (Table 4 shows 1.05% Qwen, 7.50% Hunyuan).
  - **Compounding error with multiple misleading snippets**: M1+M2 condition shows largest drops (Table 2: -19.19 from baseline for Qwen with thinking ON).

- First 3 experiments:
  1. Replicate baseline vs. helpful vs. misleading comparison on a 500-question subset, logging response tokens and per-step reasoning traces for failure classification.
  2. Introduce a credibility-prefix condition ("This information may be unreliable") to test whether explicit warnings mitigate thinking-mode vulnerability.
  3. Test mixed-retrieval scenarios (H1+M1) with varied snippet ordering to assess position effects on error propagation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training interventions specifically designed to filter or critique input context mitigate the "double-edged sword" effect where thinking mode amplifies errors from misleading information?
- Basis in paper: [explicit] The authors conclude that the challenge is "not merely to make models 'think', but to endow them with the critical faculty to evaluate the information upon which their reasoning is based."
- Why unresolved: The current study only evaluates existing models (Qwen and Hunyuan) under various information conditions but does not propose or test a training methodology to solve the identified vulnerability.
- What evidence would resolve it: A follow-up study showing that a model fine-tuned on source verification or contradiction detection maintains high accuracy in "Thinking ON" mode even when presented with M1/M2 snippets.

### Open Question 2
- Question: Does the tendency for thinking mode to rationalize misleading premises persist or diminish in frontier-scale models (e.g., 70B+ parameters)?
- Basis in paper: [inferred] The experimental scope is limited to two specific models of moderate size (Qwen3-8B and Hunyuan-7B-Instruct).
- Why unresolved: It is undetermined if the observed vulnerability is an artifact of model capacity/alignment in the 7B-8B range or a fundamental property of explicit reasoning mechanisms in Large Reasoning Models.
- What evidence would resolve it: Replicating the SciAux experiments on larger parameter versions (e.g., Qwen3-72B) to see if the magnitude of the accuracy drop ($\Delta_{base}$) decreases or stays constant.

### Open Question 3
- Question: What specific architectural components or attention mechanisms drive the "Intuitive Leap" failure mode, where explicit reasoning overrides a correct intuitive answer?
- Basis in paper: [explicit] The authors identify "Intuitive Leap" (Correct w/o Thinking, Incorrect w/ Thinking) as a counter-intuitive failure mode but note that "failures are not attributable to a single predominant cause."
- Why unresolved: The paper provides a taxonomy of errors (Over-analysis, Misapplication) but does not mechanistically explain why the reasoning process overrides correct intuition in specific instances.
- What evidence would resolve it: An interpretability analysis (e.g., attention head visualization) comparing token focus in "Thinking ON" vs. "OFF" modes for samples specifically in the "Intuitive Leap" category.

## Limitations

- The synthetic auxiliary snippets may not fully capture the complexity and ambiguity of real-world retrieval outputs
- Binary thinking toggle prevents analysis of intermediate reasoning depths
- Single-hop ScienceQA format may not generalize to multi-step or open-ended reasoning tasks
- Failure classification scheme lacks explicit validation of mutual exclusivity and exhaustiveness
- Study does not explore whether model-specific factors moderate thinking-mode amplification effect

## Confidence

- High confidence: The core finding that misleading information causes accuracy drops that are amplified by thinking mode
- Medium confidence: The five-category failure taxonomy
- Medium confidence: The monotonic decline hypothesis for thinking-mode benefits

## Next Checks

1. Replicate the study with real retrieval outputs from a RAG system to assess ecological validity of the synthetic-auxiliary approach
2. Test whether explicit credibility warnings ("this information may be unreliable") reduce thinking-mode vulnerability to misleading content
3. Conduct a finer-grained analysis of reasoning depth by testing models with intermediate thinking parameters or varying reasoning prompt structures