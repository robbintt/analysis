---
ver: rpa2
title: 'TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking
  Framework for Turkish'
arxiv_id: '2512.23065'
source_url: https://arxiv.org/abs/2512.23065
tags:
- turkish
- tabibert
- language
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TabiBERT, a ModernBERT-based Turkish monolingual
  encoder trained from scratch on 1 trillion tokens, and TabiBench, a standardized
  benchmark with 28 datasets across 8 task categories. TabiBERT achieves state-of-the-art
  performance among Turkish models with 77.58 average score, outperforming BERTurk
  by 1.62 points and establishing new records on 5/8 tasks including +9.55 on question
  answering and +2.41 on code retrieval.
---

# TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish

## Quick Facts
- arXiv ID: 2512.23065
- Source URL: https://arxiv.org/abs/2512.23065
- Reference count: 18
- Primary result: State-of-the-art ModernBERT foundation model for Turkish with 77.58 average score across 28 datasets

## Executive Summary
TabiBERT introduces a ModernBERT-based Turkish monolingual encoder trained from scratch on 1 trillion tokens, achieving state-of-the-art performance with 77.58 average score. The model outperforms BERTurk by 1.62 points and establishes new records on 5/8 task categories including +9.55 on question answering and +2.41 on code retrieval. TabiBERT supports 8,192-token context length, achieves 2.65× faster inference, and uses 41% less memory than multilingual alternatives while maintaining competitive performance.

## Method Summary
TabiBERT is a ModernBERT-based Turkish monolingual encoder trained from scratch on 1 trillion tokens of Turkish web, scientific, code, and mathematical content. The model employs rotary positional embeddings and FlashAttention optimizations to achieve superior performance and efficiency. TabiBERT is evaluated on TabiBench, a standardized benchmark with 28 datasets across 8 task categories, using fixed splits and task-appropriate metrics to address evaluation fragmentation in Turkish NLP.

## Key Results
- Achieves 77.58 average score across 28 datasets, outperforming BERTurk by 1.62 points
- Sets new state-of-the-art records on 5/8 task categories including +9.55 on question answering
- Supports 8,192-token context length (16× baseline) with 2.65× faster inference and 41% less memory usage

## Why This Works (Mechanism)
TabiBERT's performance advantages stem from its ModernBERT architecture with rotary positional embeddings and FlashAttention optimizations, which enable better handling of long-range dependencies and improved computational efficiency. The extensive pretraining on 1 trillion Turkish tokens, including specialized content like scientific literature and code, provides comprehensive language understanding. The unified TabiBench framework ensures consistent and comparable evaluation across diverse Turkish NLP tasks.

## Foundational Learning
1. ModernBERT Architecture
   - Why needed: Addresses limitations of original BERT architecture for modern NLP tasks
   - Quick check: Verify model handles 8,192-token context length effectively

2. Rotary Positional Embeddings
   - Why needed: Improves long-range dependency modeling without increasing memory usage
   - Quick check: Test model performance on tasks requiring long context understanding

3. FlashAttention Optimization
   - Why needed: Reduces memory consumption and speeds up attention computation
   - Quick check: Measure actual inference speed and memory usage improvements

4. Large-Scale Pretraining
   - Why needed: Enables comprehensive language understanding through extensive exposure
   - Quick check: Validate pretraining data diversity and quality

5. Task-Specific Fine-tuning
   - Why needed: Adapts foundation model to specific downstream NLP tasks
   - Quick check: Ensure proper hyperparameter tuning for each task category

6. Benchmark Standardization
   - Why needed: Provides consistent evaluation framework for model comparison
   - Quick check: Verify fixed splits prevent data leakage across evaluations

## Architecture Onboarding

Component map:
Input Text -> Tokenizer -> Embedding Layer -> ModernBERT Encoder (with Rotary PE & FlashAttention) -> Task-Specific Head

Critical path: The attention mechanism in the ModernBERT encoder is the critical path, as it determines the model's ability to capture dependencies and relationships in the input text.

Design tradeoffs:
- Monolingual vs multilingual: Focused on Turkish for better performance but limits cross-lingual capabilities
- Large-scale pretraining: Improves general language understanding but requires significant computational resources
- Rotary positional embeddings: Better long-range modeling but may have compatibility issues with some downstream tasks

Failure signatures:
- Performance degradation on tasks requiring cross-lingual understanding
- Potential overfitting to Turkish-specific patterns that may not generalize
- Memory issues when handling extremely long sequences despite optimizations

First experiments:
1. Evaluate model performance on Turkish question answering with varying context lengths
2. Test inference speed and memory usage across different batch sizes and hardware configurations
3. Compare performance on scientific vs general web text to assess domain adaptation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section raises several implicit questions about cross-lingual capabilities, reproducibility of the pretraining process, and the comprehensiveness of the benchmark framework.

## Limitations
- Exclusive focus on monolingual Turkish performance without cross-lingual evaluation
- Limited comparison to other ModernBERT variants in the Turkish NLP ecosystem
- Pretraining methodology lacks detailed hyperparameter configurations and training stability metrics

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Superior performance on Turkish monolingual tasks | High |
| Architectural advantages from ModernBERT design | Medium |
| Pretraining scale of 1 trillion tokens | Medium |
| Benchmark comprehensiveness and standardization | Low |

## Next Checks
1. Conduct cross-lingual evaluation of TabiBERT on Turkish-English code-switching datasets and Turkish-English parallel corpora to assess its multilingual capabilities beyond monolingual performance.

2. Perform ablation studies isolating the contributions of ModernBERT architecture, rotary positional embeddings, and FlashAttention to quantify their individual impact on the performance improvements.

3. Implement rigorous pretraining verification by releasing training logs, convergence curves, and conducting hyperparameter sensitivity analysis to validate the claimed 1 trillion token training scale and efficiency gains.