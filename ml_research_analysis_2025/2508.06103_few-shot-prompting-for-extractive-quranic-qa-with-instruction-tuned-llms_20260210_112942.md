---
ver: rpa2
title: Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs
arxiv_id: '2508.06103'
source_url: https://arxiv.org/abs/2508.06103
tags:
- answer
- question
- arabic
- quranic
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of extractive question answering
  (QA) on the Quran, which requires handling complex classical Arabic, unique terminology,
  and deep semantic meaning. The authors propose two approaches: fine-tuning transformer-based
  models (e.g., AraBERT, AraELECTRA) and using few-shot prompting with instruction-tuned
  large language models (LLMs) such as Gemini and DeepSeek.'
---

# Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs

## Quick Facts
- arXiv ID: 2508.06103
- Source URL: https://arxiv.org/abs/2508.06103
- Reference count: 24
- One-line primary result: Instruction-tuned LLMs with Arabic few-shot prompts outperform fine-tuned models on Quranic extractive QA, achieving pAP@10 of 0.637

## Executive Summary
This paper tackles the challenging task of extractive question answering on the Quran, which requires handling complex classical Arabic, unique terminology, and deep semantic meaning. The authors propose two complementary approaches: fine-tuning transformer-based models (e.g., AraBERT, AraELECTRA) and using few-shot prompting with instruction-tuned large language models (LLMs) such as Gemini and DeepSeek. They develop a specialized Arabic prompt framework and implement a post-processing system involving subword alignment, overlap suppression, and semantic filtering to improve precision and reduce hallucinations. Evaluation on the Quran QA 2023 Shared Task shows that instruction-tuned LLMs with Arabic prompts outperform traditional fine-tuned models, achieving a pAP@10 score of 0.637 with the best configuration. The results demonstrate the effectiveness of prompt-based instruction tuning for low-resource, semantically rich QA tasks.

## Method Summary
The authors evaluate two distinct approaches for Quranic extractive QA. The first fine-tunes transformer models (AraBERT, AraELECTRA, CAMeLBERT, BERT-large) on a combined dataset of QRCDv1.2, QUQA, and ARCD, initialized from TyDi QA checkpoints using SimpleTransformers. The second approach uses few-shot prompting with instruction-tuned LLMs (Gemini, DeepSeek) via API, employing a structured Arabic prompt with three diverse examples (single-answer, multi-answer, zero-answer) and a post-processing pipeline including subword alignment, non-maximum suppression for overlapping spans, and semantic filtering of uninformative answers. Both approaches are evaluated on the Quran QA 2023 Shared Task using pAP@10.

## Key Results
- Instruction-tuned LLMs with few-shot prompting achieve pAP@10 of 0.637, outperforming fine-tuned models
- Fine-tuned models using combined datasets (QRCD+QUQA+ARCD) achieve best score of 0.503 with AraBERTv02
- Post-processing pipeline improves precision by filtering hallucinations and enforcing exact span extraction
- Three-shot prompt with Arabic instructions and diverse examples effectively guides LLM output

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning for Specialized Extraction
Structured few-shot prompts enable instruction-tuned LLMs to perform extractive QA without parameter updates, potentially matching or exceeding fine-tuned models. The prompt provides three diverse examples alongside explicit task instructions, allowing the pre-trained model to recognize the span extraction pattern and apply its existing linguistic knowledge to Quranic Arabic through pattern matching and attention mechanisms.

### Mechanism 2: Post-Processing Pipeline for Precision Enhancement
A rule-based post-processing pipeline reduces false positives and improves alignment between model output and the source text. The pipeline applies subword alignment to correct tokenization artifacts, Non-Maximum Suppression (NMS) to remove redundant or overlapping predicted spans, and a semantic filter to remove uninformative answers. This multi-stage filtering removes hallucinations and enforces the constraint that answers must be exact spans from the passage.

### Mechanism 3: Domain Adaptation via Multi-Source Fine-Tuning
Combining domain-specific Quranic datasets with general Arabic MRC data improves model performance on the low-resource target task. Pre-training on a large, general multilingual dataset (TyDi QA) provides foundational linguistic patterns. Fine-tuning on a combined corpus of the target dataset (QRCDv1.2) and related datasets (QUQA, ARCD) exposes the model to more Quranic question-answer patterns and Arabic syntactic structures than the target dataset alone would allow.

## Foundational Learning

- **Extractive Question Answering (QA)**: The core task definition where the system must select a substring from a given passage as the answer, not generate new text. Why needed: This defines the fundamental problem being solved.
  - Quick check question: Can you explain the difference between an extractive answer and a generative answer in the context of the Quran QA task?

- **Few-Shot Prompting**: The primary method used to achieve state-of-the-art results with LLMs. Why needed: Understanding how to design and use prompts is critical for reproducing the results.
  - Quick check question: Why were three specific types of examples (single-answer, multi-answer, zero-answer) chosen for the prompt?

- **Partial Average Precision at rank 10 (pAP@10)**: The evaluation metric. Why needed: Understanding it is necessary to interpret the results and the system's goals.
  - Quick check question: Why is pAP@10 considered a more suitable metric than Exact Match (EM) for Quranic QA?

## Architecture Onboarding

- **Component map**: Data Preparation Module -> Modeling Core (Fine-tuning Branch, Prompting Branch) -> Post-Processing Pipeline (Subword Alignment -> NMS -> Semantic Filtering -> Re-ranking) -> Evaluation Interface

- **Critical path**: Input Question & Passage -> (Prompting Branch) Formatted Prompt -> LLM API -> Raw Text Answer -> Post-Processing Pipeline -> Ranked Answer Spans -> pAP@10 Evaluation

- **Design tradeoffs**:
  - LLMs (Gemini/DeepSeek) vs. Fine-tuned Transformers: LLMs via API offer higher performance and ease of use (no training) but incur ongoing cost and require internet. Fine-tuned models require upfront training cost/resources but can run locally with lower latency and no per-query cost.
  - Few-shot vs. Fine-tuning: Few-shot is data-efficient but limited by the context window and model's inherent capabilities. Fine-tuning can adapt a model more deeply but requires significantly more labeled data and compute.
  - Combined Dataset vs. Target-Only: Training on more data can improve generalization but risks negative transfer if the additional data is too dissimilar or noisy.

- **Failure signatures**:
  - LLM Hallucination: Model generates an answer not present in the passage. Mitigation: Explicit prompt instructions and post-processing filters.
  - Span Boundary Misalignment: Model's answer contains extra or missing tokens. Mitigation: Subword alignment and NMS in post-processing.
  - Negative Transfer: Fine-tuned model performs worse than pre-trained baseline. Mitigation: Evaluate on a held-out set during training, try smaller learning rates.
  - High Latency/Cost: LLM API calls are slow or expensive. Mitigation: Use smaller, fine-tuned models for lower-stakes or high-volume queries.

- **First 3 experiments**:
  1. Baseline Comparison: Run both a standard AraBERT model fine-tuned on QRCDv1.2-only data and a zero-shot Gemini model with a basic prompt. Compare their pAP@10 scores to establish a performance floor.
  2. Ablation on Prompt Engineering: Start with a basic prompt. Iteratively add: (a) task instructions, (b) the three diverse few-shot examples. Measure the pAP@10 improvement from each addition to isolate the contribution of prompt design.
  3. Ablation on Post-Processing: Take the best-performing model configuration from experiment 2. Run it with and without the post-processing pipeline enabled. Analyze the difference in pAP@10 to quantify the value added by the filtering stages.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does incorporating formal tafsir (exegesis) resources into the training data enhance the semantic accuracy and interpretive depth of extractive Quranic QA models? The conclusion states that "Future directions will be increasing training data using tafsir resources."

- **Open Question 2**: Can ensemble techniques effectively combine the extractive precision of fine-tuned transformers with the semantic flexibility of instruction-tuned LLMs to outperform single-model architectures? The authors list "investigating some ensemble techniques for improving the precision and clarity of the answers" as a future direction.

- **Open Question 3**: How does the performance of instruction-tuned LLMs on extractive Quranic QA degrade as the complexity of required inference shifts from literal span matching to metaphorical or spiritual interpretation? The introduction highlights the difficulty of distinguishing "literal and metaphorical languages," yet the results focus heavily on the pAP@10 metric for span extraction.

## Limitations

- Exact prompt templates and few-shot examples are not fully specified, which could significantly impact reproducibility and performance.
- Post-processing pipeline details (thresholds, stopword lists) are underspecified, making it difficult to replicate the reported improvements.
- Comparison between LLM and fine-tuned model performance is based on different data configurations (combined vs. target-only training), complicating direct interpretation of superiority claims.

## Confidence

- **Reproducibility of results**: Medium-Lower - The core methods are described but critical implementation details are missing.
- **Effectiveness of proposed mechanisms**: Medium - The mechanisms are well-motivated but their quantitative contributions are not fully isolated.
- **Novelty of the approach**: High - The combination of few-shot prompting with instruction-tuned LLMs for this specific task is innovative.

## Next Checks

1. Replicate the three-shot prompt structure using publicly available Arabic QA examples and test on a held-out subset of QRCD to measure sensitivity to prompt design.

2. Implement the post-processing pipeline with configurable thresholds and perform an ablation study to quantify the contribution of each filtering stage to final pAP@10.

3. Conduct a controlled experiment comparing instruction-tuned LLMs (few-shot) vs. fine-tuned models on the same training data (QRCD-only) to isolate the effect of model type from data configuration.