---
ver: rpa2
title: 'Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in
  LLMs'
arxiv_id: '2510.05987'
source_url: https://arxiv.org/abs/2510.05987
tags:
- sampling
- correctness
- arxiv
- calibration
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the trade-off between exploration and accuracy
  in large language model (LLM) reasoning. Existing decoding methods either increase
  exploration at uncertain steps (e.g., higher temperature, larger candidate sets)
  or improve reliability by filtering low-confidence samples post-generation, but
  these approaches conflict because they conflate different sources of uncertainty.
---

# Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs

## Quick Facts
- **arXiv ID:** 2510.05987
- **Source URL:** https://arxiv.org/abs/2510.05987
- **Authors:** Xueyan Li; Guinan Su; Mrinmaya Sachan; Jonas Geiping
- **Reference count:** 40
- **Primary result:** Proposed correctness-first decoding methods (Calibrated-ε and Calibrated-TopK) achieve consistent gains across math and general reasoning benchmarks, with Calibrated-ε improving majority-voted accuracy on GSM8K from 68.4% to 74.3% (maj@8) and pass@4 on AIME24 from 71.4% to 77.3%.

## Executive Summary
This paper addresses the fundamental trade-off between exploration and accuracy in LLM reasoning by challenging the conventional wisdom of using temperature or larger candidate sets at uncertain steps. The authors demonstrate that uncertainty in reasoning tasks comes from two distinct sources - low-confidence correct steps versus genuinely ambiguous reasoning - and that conflating these sources leads to suboptimal decoding strategies. They propose a novel framework that maps token probabilities to estimated correctness rather than confidence, enabling adaptive sampling that maintains accuracy while preserving exploration where beneficial. The approach shows consistent improvements across multiple reasoning benchmarks without adding significant inference cost.

## Method Summary
The authors propose three decoding strategies that leverage per-token correctness estimates: Greedy-Threshold applies greedy decoding when confidence is very low, Calibrated-TopK adaptively truncates the candidate set based on rank-wise correctness mapping, and Calibrated-ε implements a continuous mapping from probability to correctness. The key innovation is calibrating decoding decisions based on the relationship between token probability and expected correctness rather than raw confidence scores. This is achieved by training a linear mapping (in log-log space) between probability and correctness using a held-out calibration set, then using this mapping to determine sampling parameters at inference time. The methods are model-agnostic and can be applied post-hoc to existing LLMs with minimal implementation complexity.

## Key Results
- Calibrated-ε and Calibrated-TopK achieve the largest improvements, with Calibrated-ε improving majority-voted accuracy on GSM8K with Qwen2.5-1.5B-Instruct from 68.4% to 74.3% (maj@8)
- On AIME24 with GPT-OSS-20B, Calibrated-ε increases pass@4 from 71.4% to 77.3%
- The methods show consistent gains across diverse benchmarks including MATH, AIME24, and general reasoning tasks
- All proposed methods add negligible inference cost while outperforming traditional decoding strategies like top-k and temperature scaling

## Why This Works (Mechanism)
The paper identifies that standard decoding strategies conflate two types of uncertainty: steps where the model is uncertain but likely correct versus steps where the model is uncertain and likely wrong. By explicitly modeling the relationship between token probability and correctness rather than confidence, the proposed methods can distinguish between these cases and adapt sampling accordingly. When correctness is expected to be high, the methods can afford to explore more; when correctness is expected to be low, they can either be more conservative (greedy) or reduce sampling to avoid accumulating errors. This targeted approach to uncertainty management enables better trade-offs between exploration and accuracy than uniform temperature or candidate set adjustments.

## Foundational Learning
- **Per-token correctness estimation**: Understanding how to assign binary correctness labels to individual token predictions in reasoning tasks is crucial for implementing the proposed methods. Quick check: Can you design a pipeline to automatically label token-level correctness using reference solutions?
- **Log-log linear mapping**: The core technique maps probability to correctness in log-log space, assuming a linear relationship. Quick check: Plot log(probability) vs log(correctness) for your calibration set to verify the linear assumption holds.
- **Rank-wise calibration**: The method assumes correctness decreases monotonically with token rank in the probability distribution. Quick check: Sort tokens by probability and plot empirical correctness rate to verify the monotonic relationship.
- **Majority voting for reasoning**: The paper uses majority voting across multiple samples to determine final answer correctness. Quick check: Implement majority voting that handles both single-token answers and multi-token sequences.
- **Exploration-accuracy trade-off**: Understanding how decoding parameters affect the balance between finding correct solutions and avoiding error accumulation. Quick check: Analyze how pass@k changes with k for different decoding strategies on a reasoning benchmark.
- **Calibration set methodology**: The approach requires a held-out set with token-level correctness labels for training the probability-correctness mapping. Quick check: Design a protocol for creating calibration sets that are representative of target reasoning tasks.

## Architecture Onboarding

**Component Map:** Token Probability Distribution -> Correctness Estimator -> Decoding Strategy Selector -> Output Sampler

**Critical Path:** The most important path is from the token probability distribution through the correctness mapping to the decoding strategy selection, as this determines where and how much to sample.

**Design Tradeoffs:** The main tradeoff is between calibration accuracy (requiring larger, more representative calibration sets) and practical constraints (smaller sets are faster to create but may yield noisier mappings). The linear assumption in log-log space simplifies implementation but may not capture complex relationships in all domains.

**Failure Signatures:** The methods fail when the probability-correctness mapping is noisy or sparse, when reasoning tasks have non-monotonic correctness-rank relationships, or when the calibration set is not representative of target tasks. Performance degradation is typically gradual rather than catastrophic.

**First Experiments:**
1. Validate the linear probability-correctness relationship on your target domain by plotting calibration data
2. Implement and test Greedy-Threshold on a small reasoning benchmark to establish baseline improvements
3. Compare Calibrated-TopK with standard top-k decoding on multiple samples to quantify the accuracy-exploration trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can correctness-first decoding improve creative or open-ended generation, or does the lack of a single "ground truth" make the probability-correctness mapping unreliable?
- **Basis in paper:** [Explicit] Appendix A.9 states the authors "refrain from drawing conclusions about the effectiveness of our proposed samplers for creative writing" despite observing similar calibration trends.
- **Why unresolved:** The study focused on tasks with closed-form answers where correctness is binary; open-ended tasks lack a single gold token sequence to define rank-wise correctness.
- **Evidence to resolve:** Evaluation on creative writing benchmarks (e.g., LitBench) using human or model-based preference metrics rather than exact match accuracy.

### Open Question 2
- **Question:** How can calibration be effectively adapted for "thinking" models that generate long sequences of intermediate tokens?
- **Basis in paper:** [Explicit] Section 4.4 notes that for thinking models, "calibration on short, instruction-style datasets [is] less representative of their actual behavior."
- **Why unresolved:** The paper relied on generic ε-sampling for long-reasoning models because standard calibration grids built on short answers are misaligned with extended reasoning traces.
- **Evidence to resolve:** Development and testing of calibration datasets specifically designed for long chain-of-thought traces.

### Open Question 3
- **Question:** How robust is the Calibrated-ε method when the linear mapping between probability and correctness is noisy or sparse?
- **Basis in paper:** [Explicit] Section 5 and Appendix A.6 note that "data sparsity and noise degrades this map" and show that performance gains diminish when the linear fit is poor (e.g., on MBPP).
- **Why unresolved:** The method assumes a stable log-log linear relationship, which breaks down in domains with noisy validation signals.
- **Evidence to resolve:** Analysis of performance stability across varying calibration set sizes and domains, specifically measuring the correlation between regression error (MSE) and accuracy drops.

## Limitations
- The methods assume access to per-token correctness estimates, which may not be readily available for all reasoning tasks or could introduce computational overhead
- Empirical validation focuses primarily on mathematical and general reasoning benchmarks, leaving open questions about performance on open-ended or subjective reasoning tasks
- The theoretical framing around the exploration-accuracy trade-off is compelling but remains largely conceptual with limited formal analysis

## Confidence
- **High confidence:** Empirical results showing consistent accuracy improvements across multiple benchmarks and model sizes
- **Medium confidence:** Theoretical claims about the exploration-accuracy trade-off and correctness-based sampling
- **Medium confidence:** Generalization to tasks beyond mathematical reasoning

## Next Checks
1. Evaluate correctness-first decoding on open-ended reasoning tasks where correctness is subjective or requires external validation
2. Test the methods with different correctness estimation approaches (e.g., self-consistency, confidence scores) to assess robustness
3. Measure real-world inference latency and resource costs when implementing per-token correctness estimation at scale