---
ver: rpa2
title: Enhancing Rumor Detection Methods with Propagation Structure Infused Language
  Model
arxiv_id: '2508.07209'
source_url: https://arxiv.org/abs/2508.07209
tags:
- rumor
- detection
- plms
- solm
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving language models for
  rumor detection on social media by addressing the mismatch between general pretraining
  corpora and social media text characteristics. The authors propose a continue pretraining
  strategy called Post Engagement Prediction (PEP), which incorporates information
  from propagation structures into pretrained language models.
---

# Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model

## Quick Facts
- **arXiv ID:** 2508.07209
- **Source URL:** https://arxiv.org/abs/2508.07209
- **Reference count:** 23
- **Primary result:** PEP boosts rumor detection accuracy by 1.0-3.7% and outperforms state-of-the-art methods

## Executive Summary
This paper addresses the challenge of improving language models for rumor detection on social media by tackling the corpus mismatch between general pretraining data and social media text characteristics. The authors propose a continue pretraining strategy called Post Engagement Prediction (PEP) that incorporates propagation structure information into pretrained language models through self-supervised relation prediction tasks. Using two large-scale unlabeled datasets with propagation structures (UTwitter and UWeibo) and TwitterCorpus (269GB), they train SoLM, a Twitter-tailored language model. Extensive experiments demonstrate that PEP significantly enhances rumor detection performance across various models and datasets, with SoLM achieving competitive results even without additional high-level modules.

## Method Summary
The method employs a two-stage continue pretraining approach. First, a language model is pretrained on TwitterCorpus using standard Masked Language Modeling (MLM) to address vocabulary and tokenization issues specific to social media text. Second, the model is further trained on UTwitter/UTWeibo datasets using both MLM and PEP objectives. PEP involves predicting three structural relations—Root, Branch, and Parent—between posts in propagation trees using self-supervised learning. The model learns to position interacting posts relationally in vector space, capturing stance and sentiment interactions crucial for rumor detection. The final model, SoLM, uses a custom vocabulary of 52k tokens and special tokens for social media elements like mentions and URLs.

## Key Results
- PEP improves rumor detection accuracy by 1.0-3.7% across various models and datasets
- SoLM alone achieves competitive results without additional high-level modules
- PEP outperforms current state-of-the-art methods on multiple benchmark datasets
- The strategy effectively captures stance and sentiment interactions through propagation structure

## Why This Works (Mechanism)

### Mechanism 1: Structural Infusion via Self-Supervised Relation Prediction
The PEP strategy improves rumor detection by forcing the language model to internalize the topology of information diffusion alongside text semantics. The model is trained to predict three structural relations—Root, Branch, and Parent—using the dot product of node embeddings, aligning the vector space such that interacting posts are positioned relationally. This captures stance and sentiment interactions that standard pretraining misses.

### Mechanism 2: Resolution of Corpus Mismatch
Performance gains stem from aligning the model's pretraining distribution with the specific linguistic idiosyncrasies of social media. By pretraining on TwitterCorpus and UTwitter, SoLM adapts its vocabulary and tokenization to handle "user mentions," "urls," and "emojis" which standard PLMs typically mishandle, preventing "symbol processing shortfalls."

### Mechanism 3: Enhanced Feature Initialization for High-Level Models
PEP provides a superior "foundation" for high-level graph models (GNNs) by converting structural heuristics into dense semantic vectors before the GNN processing stage. When these embeddings are fed into GNNs, the models require fewer layers/iterations to converge on the correct classification because the interaction features are already partially encoded in the node embeddings.

## Foundational Learning

- **Propagation Trees (Rumor Cascades):** Understanding how social media discussions form directed tree graphs is crucial since PEP relies on treating conversations as structured graphs rather than flat text. *Quick check:* Can you identify Root, Branch, and Parent nodes in a simple conversation thread?

- **Self-Supervised Learning (Auxiliary Objectives):** PEP uses auxiliary tasks (predicting edges) to continue pretraining. Understanding how to construct loss functions from unlabeled structural data is essential. *Quick check:* How would you generate a label matrix for "Parent Prediction" if you only had the edge list of a graph?

- **Domain Adaptation in NLP:** The paper highlights corpus mismatch between Wikipedia/Books and social media. You need to understand why models trained on formal text fail on Tweets. *Quick check:* Why does the authors' use of custom vocabulary and special tokens improve performance over standard BERT?

## Architecture Onboarding

- **Component map:** TwitterCorpus/UTwitter data -> Custom tokenizer -> SoLM (BERT-base) -> PEP heads (Root/Branch/Parent prediction) -> Downstream GNN (BiGCN/GACL)
- **Critical path:** Stage 1: Train on pure text (TwitterCorpus) with standard MLM; Stage 2: Continue training on UTwitter/UWeibo using combined Loss (MLM + PEP); Evaluation: Feed embeddings into downstream GNN or classifier
- **Design tradeoffs:** Training from scratch takes 14 days on 8x A800s vs. PEP on existing RoBERTa takes 1 day on 1 GPU; PEP adds O(n²) computational overhead during pretraining
- **Failure signatures:** Identity Collapse (too high PEP loss weight), Short Context Failure (overfitting to conversation threads)
- **First 3 experiments:** 1) Tokenizer Validation: Verify @user and emojis map to single tokens; 2) Structural Probe: Check Parent Prediction head accuracy on UTwitter batch; 3) Ablation: Train BiGCN on Twitter16 using Standard BERT vs. SoLM embeddings

## Open Questions the Paper Calls Out

### Open Question 1
Can the PEP strategy effectively generalize to other social media application tasks beyond rumor detection, such as content recommendation or user behavior analysis? The paper has only validated performance on rumor detection tasks so far.

### Open Question 2
How does the PEP strategy perform in dynamic, real-time environments where information and propagation structures evolve rapidly, as opposed to the static benchmark datasets used? The authors note that rumor detection faces challenges like rapid updates and fast dissemination.

### Open Question 3
Can a single PLM trained with PEP using data from one platform (e.g., Twitter) transfer effectively to structurally different platforms (e.g., Reddit or YouTube) without requiring platform-specific retraining? The study validates on Twitter and Weibo but doesn't test on platforms with fundamentally different interaction graphs.

### Open Question 4
Would integrating additional topological relations, such as "sibling" relations used in attention-based baselines, into the PEP loss function yield further performance gains? The current set of relations might be suboptimal compared to structural features used by SOTA GNN methods.

## Limitations
- The strategy fundamentally relies on having complete propagation trees, which may not always be available in real-world scenarios
- Requires massive proprietary datasets (TwitterCorpus 269GB, UTwitter/UWeibo) that may be difficult to obtain due to API restrictions
- PEP training involves O(n²) computational complexity for pairwise relation matrices, creating scalability constraints

## Confidence

**High Confidence Claims:**
- Corpus mismatch problem and custom vocabulary/tokenization solution
- Overall improvement trend (1.0-3.7% accuracy gains) across multiple datasets
- Two-stage pretraining strategy technical soundness

**Medium Confidence Claims:**
- Specific mechanism of capturing "stance and sentiment interactions"
- PEP providing superior feature initialization for GNNs

**Low Confidence Claims:**
- SoLM "alone achieves competitive results" claim
- Generalizability to other rumor types beyond tested datasets

## Next Checks

1. **Structure Ablation Study:** Test PEP performance on artificially flattened propagation trees (depth=1) versus deep trees to quantify structural information contribution versus social-media-specific tokenization.

2. **Cross-Platform Generalization:** Evaluate SoLM on a non-Twitter/Weibo dataset (e.g., Reddit or Facebook conversations) to assess whether social-media-specific pretraining transfers across platforms.

3. **Real-World Deployment Simulation:** Implement a test where propagation structures are progressively hidden or incomplete to measure performance degradation and determine practical deployment limitations.