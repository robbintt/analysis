---
ver: rpa2
title: Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement
  Learning
arxiv_id: '2508.10371'
source_url: https://arxiv.org/abs/2508.10371
tags:
- learning
- human
- recognition
- activity
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FA VOR, a few-shot human activity recognition
  method that leverages visual reinforcement learning with multimodal large language
  models (MLLMs). The approach generates multiple candidate responses with reasoning
  traces and final answers, then optimizes the model using Group Relative Policy Optimization
  (GRPO) with rule-based verifiable rewards.
---

# Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.10371
- Source URL: https://arxiv.org/abs/2508.10371
- Reference count: 22
- Primary result: FA VOR achieves 55.78% average accuracy in 1-shot settings, improving by +14.78% over baseline and +0.04% over SFT

## Executive Summary
This paper introduces FA VOR, a few-shot human activity recognition method that leverages visual reinforcement learning with multimodal large language models (MLLMs). The approach generates multiple candidate responses with reasoning traces and final answers, then optimizes the model using Group Relative Policy Optimization (GRPO) with rule-based verifiable rewards. Extensive experiments on four human activity recognition datasets (UCF-50, UCF-101, UCF-Crime, HMDB51) demonstrate FA VOR's superiority over supervised fine-tuning, particularly in challenging scenarios and higher-shot settings.

## Method Summary
FA VOR employs visual reinforcement learning to enhance MLLM performance in few-shot human activity recognition. The method generates multiple candidate responses with reasoning traces, then uses GRPO to optimize the model based on rule-based verifiable rewards. A key innovation is the use of frozen vision modules, which allows efficient few-shot learning while maintaining strong performance. The approach is evaluated across 1-shot to 16-shot settings on four benchmark datasets, demonstrating consistent improvements over supervised fine-tuning baselines.

## Key Results
- FA VOR achieves 55.78% average accuracy in 1-shot settings, improving by +14.78% over baseline and +0.04% over SFT
- On UCF-101, FA VOR shows +29.62% improvement over baseline in 1-shot settings
- In 16-shot settings, FA VOR achieves 63.55% average accuracy (+22.55% over baseline)
- Even with frozen vision modules, FA VOR delivers substantial performance gains (79.69% vs 80.14% with trainable modules)

## Why This Works (Mechanism)
FA VOR leverages the reasoning capabilities of MLLMs through visual reinforcement learning, allowing the model to explore multiple reasoning paths and learn from verifiable rewards. The Group Relative Policy Optimization (GRPO) algorithm enables efficient policy updates by comparing groups of responses rather than individual ones, making the learning process more stable and effective in few-shot scenarios. The use of rule-based verifiable rewards provides clear feedback signals for the reinforcement learning process, guiding the model toward more accurate and well-formatted responses.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Foundation models that integrate vision and language understanding
  - *Why needed*: Essential for processing both visual inputs and generating textual reasoning
  - *Quick check*: Verify Qwen2.5-VL-3B-Instruct is indeed a multimodal model

- **Visual Reinforcement Learning**: RL applied to visual understanding tasks
  - *Why needed*: Enables exploration of reasoning strategies in few-shot settings
  - *Quick check*: Confirm GRPO algorithm is properly implemented

- **Group Relative Policy Optimization (GRPO)**: Reinforcement learning algorithm for policy optimization
  - *Why needed*: More stable and efficient than individual-based policy updates
  - *Quick check*: Verify group comparison mechanism in the implementation

- **Rule-based Verifiable Rewards**: Simple reward functions based on accuracy and format
  - *Why needed*: Provides clear, computable feedback for RL training
  - *Quick check*: Ensure reward calculation is correctly implemented

- **Few-shot Learning**: Learning from very limited labeled examples
  - *Why needed*: Addresses the challenge of data scarcity in HAR
  - *Quick check*: Verify shot configurations (1, 2, 4, 16) are properly implemented

## Architecture Onboarding

### Component Map
Qwen2.5-VL-3B-Instruct (MLLM) -> Visual Encoder -> Reasoning Generator -> GRPO Optimizer -> Reward Function

### Critical Path
Image input → Visual encoder → MLLM reasoning → Multiple candidate generation → Reward evaluation → Policy update

### Design Tradeoffs
- Frozen vs trainable vision modules: Computational efficiency vs potential accuracy gains
- Rule-based vs learned rewards: Simplicity and verifiability vs potential for more nuanced feedback
- Multiple candidates vs single response: Better exploration vs increased computation

### Failure Signatures
- Low reward signals across all candidates indicating poor task understanding
- Policy updates that don't improve accuracy over training epochs
- Reasoning traces that are repetitive or don't explore diverse strategies

### First Experiments to Run
1. Verify baseline performance on each dataset with standard SFT approach
2. Test FA VOR with frozen vision modules on 1-shot setting
3. Compare performance between frozen and trainable vision configurations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can visual reinforcement learning be extended to video-based HAR to capture temporal dynamics of human activities?
- Basis in paper: [inferred] The methodology extracts single frames from videos, losing temporal information. Activities like "diving" vs. "swimming" or "standing up" vs. "sitting down" require motion understanding that single frames cannot provide.
- Why unresolved: The paper focuses on image-based recognition; temporal modeling with RL-based reasoning traces is unexplored.
- What evidence would resolve it: Experiments comparing single-frame vs. video-sequence performance; temporal reward functions that evaluate reasoning across frames.

### Open Question 2
- Question: How do the dynamics between frozen and trainable vision modules affect reasoning emergence in MLLMs during visual reinforcement learning?
- Basis in paper: [explicit] The ablation study concludes that "intricate dynamics between vision and language modules merit further investigation," noting that frozen vision modules achieve 79.69% vs. 80.14% with trainable modules.
- Why unresolved: The paper demonstrates both configurations work but doesn't explain why freezing helps or what trade-offs exist.
- What evidence would resolve it: Systematic study varying vision module trainability with analysis of learned representations and attention patterns.

### Open Question 3
- Question: How does FA VOR's performance generalize across different MLLM architectures and scales?
- Basis in paper: [inferred] All experiments use only Qwen2.5-VL-3B-Instruct; no comparison with other MLLMs (LLaVA, InternVL, etc.) or larger models.
- Why unresolved: The method's architectural dependence is unknown; GRPO effectiveness may vary with model capacity.
- What evidence would resolve it: Cross-architecture experiments with standardized evaluation protocols and scaling analysis across model sizes.

### Open Question 4
- Question: Can more sophisticated reward functions beyond simple accuracy and format rewards further improve HAR performance?
- Basis in paper: [inferred] The reward function is deliberately simple (+1 accuracy, +1 format), but reasoning quality, temporal coherence, or uncertainty-aware rewards remain unexplored.
- Why unresolved: Complex reward modeling is avoided but may provide additional learning signals.
- What evidence would resolve it: Ablation studies with semantic similarity rewards, confidence-weighted accuracy, or multi-objective reward designs.

## Limitations
- Reliance on rule-based verifiable rewards may not capture full complexity of HAR tasks and could introduce bias toward easily verifiable reasoning paths
- Absence of comparisons with other few-shot learning approaches or state-of-the-art vision-language models specifically designed for action recognition
- Limited dataset diversity (only four datasets) may not represent the full range of real-world human activity recognition challenges

## Confidence
- FA VOR's superiority over supervised fine-tuning: High
- Scalability benefits in higher-shot scenarios: Medium
- Effectiveness of frozen vision modules approach: Medium
- Generalizability across diverse human activity recognition tasks: Low

## Next Checks
1. Evaluate FA VOR on additional diverse activity recognition datasets including those with complex multi-person interactions, fine-grained activities, and real-world noisy video conditions to assess generalizability beyond the current benchmark selection.

2. Compare FA VOR against recent few-shot vision-language models specifically designed for action recognition (e.g., models from the CLIP family or specialized video-text foundation models) to establish its position relative to the state of the art in this domain.

3. Conduct an analysis of the rule-based reward functions to identify potential biases or limitations in the reward design, including testing alternative reward formulations and examining whether the reinforcement learning process is optimizing for the intended recognition capabilities or exploiting reward specification loopholes.