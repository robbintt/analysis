---
ver: rpa2
title: 'When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers'
arxiv_id: '2511.21718'
source_url: https://arxiv.org/abs/2511.21718
tags:
- social
- micm
- llms
- attack
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MICM, a novel jailbreak attack that manipulates
  the underlying ideological orientation of LLM outputs through conceptual triggers,
  rather than generating overtly harmful content. Drawing on conceptual morphology
  theory, MICM uses carefully selected concept-embedded triggers to steer model responses
  toward extremist ideological stances without triggering conventional safety filters.
---

# When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers

## Quick Facts
- **arXiv ID**: 2511.21718
- **Source URL**: https://arxiv.org/abs/2511.21718
- **Reference count**: 15
- **Primary result**: MICM achieves near-perfect jailbreak success rates by manipulating LLM ideological orientation through conceptual triggers rather than explicit harmful content

## Executive Summary
This paper introduces MICM (Morphological Ideological Configuration Manipulation), a novel jailbreak attack that exploits conceptual morphology theory to manipulate LLM outputs toward extremist ideological stances without triggering conventional safety filters. Unlike traditional jailbreaks that generate overtly harmful content, MICM uses carefully selected concept-embedded triggers (CETs) that preserve the structural configuration of target ideologies while remaining semantically innocuous. Evaluated across five advanced LLMs, MICM achieved attack success rates near 1.0 with minimal rejection, demonstrating a critical blind spot in current safety mechanisms that focus on explicit harm detection rather than covert value manipulation.

## Method Summary
MICM leverages conceptual morphology theory to decompose ideologies into structured configurations of social/political concepts. The attack identifies the conceptual configuration of a target ideology (neo-Nazism in this study), then substitutes objectionable concepts with semantically harmless CETs that preserve the ideological structure. These CETs are embodied expressions with specific storytelling angles that teach LLMs how to connect abstract concepts to real-world events. The method uses a fixed prompt template with placeholders for CETs and incident descriptions, evaluated through a five-dimensional Ideological Alignment Score (IAS) framework. The attack achieves its effectiveness by exploiting the blind spot in current safety mechanisms, which are optimized for explicit harm detection rather than tracking aggregate value shifts or ideological alignment.

## Key Results
- MICM achieved near-perfect Attack Success Rates (ASR ≈ 1.0) across five evaluated LLMs including GPT-4o and DeepSeek-R1
- The attack maintained minimal rejection rates (R_rej = 0 for most models), significantly outperforming baseline methods
- CETs preserved ideological structure while evading word-level and semantic-level safety filters, demonstrating that current safety mechanisms cannot detect covert value manipulation

## Why This Works (Mechanism)

### Mechanism 1
Ideologies can be decomposed into structured configurations of social/political concepts that function as unique "fingerprints," enabling indirect manipulation through concept injection rather than explicit harmful instructions. MICM uses conceptual morphology theory to identify the conceptual configuration of a target ideology, then substitutes objectionable concepts with semantically innocuous CETs that preserve the ideological structure while evading safety filters. The core assumption is that LLMs can capture nuanced relationships between abstract concepts without explicit guidance. Evidence shows CETs enable ideological alignment, though the assumption about LLM reasoning remains unverified outside this study's experiments. This mechanism would degrade if safety training models conceptual configurations rather than just harmful token patterns.

### Mechanism 2
CETs function as compressed in-context learning signals that embody abstract concepts while providing "storytelling angles" connecting concepts to real-world events, reducing reliance on implicit model reasoning. Rather than directly injecting abstract conceptual configurations, CETs provide embodied expressions with contextual framing that explicitly encode relationships between concepts and real-world incidents. The core assumption is that embodied, context-rich triggers produce stronger ideological steering than raw abstract concepts. Evidence from ablation studies supports CETs' effectiveness, though the in-context learning compression hypothesis lacks direct corpus validation. This would fail if models develop robust conceptual grounding mechanisms that detect manipulation framing devices.

### Mechanism 3
Current safety mechanisms optimized for explicit harm detection have a systematic blind spot for covert value manipulation because they evaluate outputs against word-level and semantic-level harmfulness metrics rather than ideological alignment or aggregate value structure. MICM achieves near-zero rejection rates because CETs are individually innocuous; harm emerges only in the aggregate ideological orientation, which safety filters aren't designed to evaluate. The core assumption is that safety mechanisms primarily detect explicit harm indicators rather than tracking gradual value orientation shifts. Evidence shows MICM's success demonstrates this blind spot, though safety mechanism analysis wasn't directly conducted. This would be addressed if safety frameworks expand to include ideological alignment detection and aggregate value tracking.

## Foundational Learning

- **Concept: Conceptual Morphology Theory (Freeden 1996)**
  - Why needed here: This political science framework provides the theoretical foundation for decomposing ideologies into structured concept configurations—the core attack vector MICM exploits
  - Quick check question: Can you explain why conceptual configuration is described as an ideology's "fingerprint" rather than just a list of beliefs?

- **Concept: Ideology as Aggregate Value Structure**
  - Why needed here: Understanding that ideologies encode coherent value frameworks (not just isolated opinions) clarifies why manipulating concept configurations shifts entire output orientations rather than individual claims
  - Quick check question: What distinguishes an ideology from a set of preferences according to the paper's framing?

- **Concept: In-Context Learning Compression**
  - Why needed here: The paper analogizes CETs to compressed in-context learning signals; understanding this mechanism helps explain why CETs outperform raw concept injection
  - Quick check question: How do CETs differ from standard in-context learning examples in their information compression approach?

## Architecture Onboarding

- **Component map**: CET Identification Pipeline -> Prompt Template T -> IAS Evaluation Framework -> Attack Classification
- **Critical path**: 1. Identify target ideology and extract conceptual configuration C_I from political science literature 2. Map each concept in C_I to corresponding CETs from curated database 3. Construct prompt using template T with CETs and incident description 4. Evaluate output using IAS framework via multi-model ensemble 5. Classify as successful attack if IAS ≥ 6
- **Design tradeoffs**: Threat level vs. success rate (sacrifices highest threat likelihood for bypassing safety), Specificity vs. generalization (fixed template trades query-specific optimization for model-agnostic portability), Manual curation vs. automation (CET identification requires domain expertise)
- **Failure signatures**: High rejection rate (CETs contain filter-triggering patterns), Low IAS despite low rejection (model generates coherent but ideologically misaligned output), High variance across models (CETs not context-agnostic), Direct concept injection outperforming CETs (safety filters not primary barrier)
- **First 3 experiments**: 1. Baseline replication on single model to measure IAS and rejection rate compared to direct ideology prompt baseline 2. CET ablation study to identify which technique categories contribute most to ideological steering effectiveness 3. Cross-ideology transfer test to validate MICM's ideology-agnostic capabilities beyond neo-Nazism

## Open Questions the Paper Calls Out

### Open Question 1
How can safety mechanisms detect covert conceptual manipulation without over-censoring benign political discourse? The authors explicitly call for future research on developing comprehensive methods to detect ideological alignment of LLM-generated content rather than just explicit harm. This remains unresolved because current safety filters are designed for overt toxicity while MICM exploits abstract value structures using semantically innocuous phrases difficult to distinguish from legitimate socio-political discussion. Evidence would be a defense framework that significantly reduces MICM's Attack Success Rate while maintaining low rejection rates for benign political queries.

### Open Question 2
Is the MICM attack equally effective against diverse extremist ideologies beyond neo-Nazism? The authors acknowledge neo-Nazism is not the only ideological orientation MICM can attack, yet they only evaluate this single target. This remains unresolved because different ideologies may possess conceptual configurations harder to decompose into functional CETs. Evidence would be experimental results applying MICM to distinct ideologies showing consistent Attack Success Rates comparable to near-perfect scores reported for neo-Nazism.

### Open Question 3
Can CET identification be automated to remove dependency on manual expert curation? The methodology relies on over 20 sociology and political science research studies to manually identify the 168 triggers used. This remains unresolved because manual expert analysis limits scalability and introduces selection bias. Evidence would be an algorithmic approach that generates valid CETs capable of achieving high IAS scores without human intervention.

## Limitations
- The attack's effectiveness across different extremist ideologies beyond neo-Nazism remains untested, limiting claims about ideology-agnostic capabilities
- Manual CET identification process creates scalability constraints and potential selection bias based on literature sources
- The five-dimensional IAS framework relies on human evaluation, introducing subjectivity and potential cultural/contextual bias

## Confidence
- **Near-perfect attack success rates (ASR ≈ 1.0)**: Medium confidence - Based on empirical results across five models but limited to neo-Nazism target
- **CETs preserve ideological structure while evading filters**: Medium confidence - Supported