---
ver: rpa2
title: 'Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable
  Decision Policies in Noisy Time Series'
arxiv_id: '2601.09949'
source_url: https://arxiv.org/abs/2601.09949
tags:
- discrete
- spline
- kinematic
- data
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kinematic Tokenization, a continuous-time
  representation method that reconstructs explicit splines from noisy time series
  data and tokenizes local spline coefficients (position, velocity, acceleration,
  jerk). Applied to financial time series data, the method generates 9-dimensional
  tokens combining price and volume dynamics.
---

# Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series

## Quick Facts
- arXiv ID: 2601.09949
- Source URL: https://arxiv.org/abs/2601.09949
- Reference count: 19
- This paper introduces a continuous-time representation method that reconstructs explicit splines from noisy time series data and tokenizes local spline coefficients (position, velocity, acceleration, jerk).

## Executive Summary
This paper introduces Kinematic Tokenization, a continuous-time representation method that reconstructs explicit splines from noisy time series data and tokenizes local spline coefficients (position, velocity, acceleration, jerk). Applied to financial time series data, the method generates 9-dimensional tokens combining price and volume dynamics. Across a multi-asset daily-equity testbed, several discrete baselines collapsed to an absorbing cash policy under a risk-averse asymmetric classification objective, whereas the continuous spline tokens sustained calibrated, non-trivial action distributions and stable policies. This demonstrates that explicit continuous-time tokens can improve learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

## Method Summary
Kinematic Tokenization optimizes stochastic differential equations to reconstruct explicit continuous-time splines from noisy measurements, then tokenizes local spline coefficients (position, velocity, acceleration, jerk). For each rolling window, the method solves variational problems to fit cubic splines to log-transformed prices and quartic splines to log-transformed volumes, extracting 9-dimensional tokens per interval. The tokens undergo dual window anchoring (subtracting initial position/volume) and Z-score normalization before being processed by a causal Transformer decoder. Applied to financial time series with asymmetric classification losses, this approach enables sustained, calibrated trading policies where discrete baselines collapse to cash-only positions.

## Key Results
- Several discrete baselines collapsed to an absorbing cash policy under asymmetric classification losses (w_sell = 10.0), while continuous spline tokens sustained calibrated, non-trivial action distributions
- Spline tokens demonstrated stable, selective policies that rationally justified risk exposure under abstention-inducing losses
- The framework successfully tokenized 9-dimensional kinematic state (price [position/velocity/acceleration/jerk] + volume [intensity/higher-order]) from noisy financial time series

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimization-based spline fitting separates latent signal from measurement noise more effectively than discrete finite differences, enabling higher-order derivative extraction without noise amplification.
- **Mechanism:** The optimization problem (Eq. 1) maximizes likelihood of a continuous spline x(t) under a stochastic dynamics model, where the regularization parameter α² = σ²_p/σ²_m controls the tradeoff between process noise (smoothness) and measurement noise (fit quality). This yields explicit cubic spline coefficients rather than noisy discrete approximations.
- **Core assumption:** The underlying signal evolves smoothly according to stochastically-forced second-order dynamics, and observed data contains additive measurement noise separable from true motion.
- **Evidence anchors:**
  - [abstract]: "reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients"
  - [Section 4.6.1]: "discrete differentiation amplifies measurement noise (Var(Δy) ≈ 2Var(y))"
  - [corpus]: CT-UIO paper similarly uses "Non-Uniform B-spline" for continuous-time localization with improved robustness over discrete methods
- **Break condition:** If α is poorly tuned (too small → overfitting noise; too large → oversmoothing genuine dynamics), or if the second-order dynamics assumption is grossly violated (e.g., jump discontinuities, regime shifts faster than sampling rate).

### Mechanism 2
- **Claim:** Explicit kinematic tokens (velocity, acceleration, jerk) provide signal-to-noise ratio sufficient to overcome asymmetric loss penalties that rationally induce abstention in discrete representations.
- **Mechanism:** The asymmetric loss (w_sell = 10.0) creates a decision threshold where the model must have high confidence to predict "Buy." Discrete representations lack sufficient information gain to overcome the prior probability of loss, causing collapse to the "Liquidation Equilibrium." Spline tokens provide cleaner kinematic signatures that enable the model to rationally justify risk exposure.
- **Core assumption:** The underlying process has genuine momentum/acceleration structure that predictive models can exploit if noise is adequately suppressed.
- **Evidence anchors:**
  - [abstract]: "several discrete baselines collapsed to an absorbing cash policy... whereas the continuous spline tokens sustained calibrated, non-trivial action distributions"
  - [Section 6.1]: "discrete input features do not provide sufficient information gain to overcome the prior probability of loss"
  - [corpus]: No direct corpus evidence for this specific mechanism; related work (BEAST) uses splines for action sequences but doesn't address asymmetric losses
- **Break condition:** If the true signal-to-noise ratio of the domain is fundamentally too low (no exploitable momentum structure), or if the asymmetric penalty is so extreme that no representation can justify action.

### Mechanism 3
- **Claim:** Dual Window Anchoring (subtracting initial position/volume from all tokens in a sequence) induces stationarity, preventing mean-reversion bias where models predict returns to dataset-wide statistics.
- **Mechanism:** By forcing every training sequence to begin at relative origin (0, 0), the model learns relative dynamics rather than absolute levels. Z-score normalization of higher-order derivatives addresses scale disparities across kinematic terms.
- **Core assumption:** Financial time series exhibit non-stationarity in levels but stationarity in relative dynamics (returns, derivatives) within reasonable time windows.
- **Evidence anchors:**
  - [Section 3.4]: "forces every training sequence to begin at a relative origin of (0.0, 0.0)"
  - [Section 3.4]: "Raw price levels (c0) and volume intensities (c̃0) are non-stationary, causing 'mean reversion bias'"
  - [corpus]: No direct corpus evidence for this normalization technique
- **Break condition:** If the context window spans regime changes where even relative dynamics are non-stationary, or if Z-score normalization statistics computed on training data don't transfer to test regimes with different volatility characteristics.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs)**
  - Why needed here: The optimization framework (Eq. 1-2) formulates spline fitting as maximum likelihood estimation under a stochastic dynamics model with process noise and measurement noise. Understanding this decomposition is essential for tuning α.
  - Quick check question: If you double α, does the spline become smoother or rougher? Why?

- **Concept: Explicit vs. Implicit Representations**
  - Why needed here: The paper emphasizes that unlike Kalman filters (implicit, recursive, discrete), their method produces an explicit continuous function x(t) defined for all t, enabling analytical derivatives. This distinction determines what downstream operations are possible.
  - Quick check question: Can a Kalman filter give you acceleration at an arbitrary time between measurement updates? Can this method?

- **Concept: Asymmetric Loss Functions and Decision Thresholds**
  - Why needed here: The core experimental result hinges on understanding why asymmetric penalties (w_sell = 10.0) cause discrete baselines to collapse while continuous tokens survive. This requires understanding the implicit decision threshold imposed by class weights.
  - Quick check question: If w_sell = 10.0 and w_buy = 2.0, what posterior probability for "Buy" is required to predict "Buy" over "Sell" at inference time?

## Architecture Onboarding

- **Component map:** Log(P_t) → y_k → Optimization Engine (Eq. 1) → Spline Coefficients c_0...c_3 → Token Extractor → 9D Tokens → Normalization Layer → Transformer Backbone → LoRA Adapters → Ternary Classification

- **Critical path:** Rolling window extraction (strictly historical data only—no look-ahead) → Local spline optimization on that window (α=5 used as proof-of-concept) → Coefficient extraction at interval boundaries → Anchoring to window start, Z-score normalization → Transformer inference → State machine translates to portfolio action

- **Design tradeoffs:**
  - **α parameter:** Controls smoothing strength. Paper uses α=5 without systematic tuning; optimal value likely domain-specific. Recommendation: cross-validate on held-out regime data.
  - **Spline order:** Cubic for price (second-order dynamics), quartic for volume (phenomenological smoother). Volume model is heuristic, not physics-derived.
  - **Shallow vs. deep:** "Short & Wide" architecture (4 layers, 512 dim) prioritizes feature bandwidth over reasoning depth—appropriate for 9D coupled kinematic state but may limit complex temporal reasoning.
  - **Context window (T=64):** Balances receptive field against computational cost. Paper doesn't ablate this.

- **Failure signatures:**
  - **Liquidation Equilibrium:** Model outputs monotonic "Sell" predictions → all baselines except SplineGPT exhibit this. Check action distribution, not just accuracy.
  - **Mean reversion bias:** Without window anchoring, model predicts returns to dataset mean. Check if predictions correlate with absolute price level.
  - **Derivative explosion:** High-order terms (c_3, c̃_4) can have extreme outliers. Check gradient norms; implement clipping (norm ≤ 1.0 as paper does).

- **First 3 experiments:**
  1. **Reproduce the baseline collapse:** Train RawGPT (discrete values, no physics) under asymmetric loss (w_sell = 10.0). Verify convergence to Liquidation Equilibrium. This validates the experimental setup and demonstrates the problem the paper solves.
  2. **Ablate α sensitivity:** Vary α ∈ {1, 3, 5, 10, 20} on a single asset (e.g., NVDA). Measure action rate, Sharpe ratio, and calibration. This determines if α=5 is robust or lucky.
  3. **Test token component importance:** Systematically mask individual kinematic channels (e.g., zero out acceleration c_2, or jerk c_3) and measure performance degradation. This identifies which derivatives actually drive the signal—the paper claims importance but doesn't ablate individual channels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stochastic filter parameter $\alpha$ be optimized statistically rather than heuristically?
- Basis in paper: [explicit] Section 3.2 states the value $\alpha=5$ is a "proof of concept" and invites "future detailed statistical studies" for optimization.
- Why unresolved: The parameter currently relies on manual selection; its sensitivity relative to signal-to-noise ratios is not formally derived.
- Evidence: A systematic ablation study correlating varying $\alpha$ values against reconstruction error and downstream Sharpe ratios.

### Open Question 2
- Question: To what extent does the log-volume approximation degrade the theoretical validity of the volume derivatives?
- Basis in paper: [inferred] Section 3.2 acknowledges that treating the integral of state as consistent with the log of the sum is a "known approximation" used primarily for stability.
- Why unresolved: The paper relies on empirical robustness to justify the approximation, leaving its impact on the fidelity of "energy" derivative tokens unproven.
- Evidence: A comparative analysis against a ground-truth volume accumulation model to quantify information loss in the derivative channels.

### Open Question 3
- Question: Can the framework effectively disentangle objective-induced abstention from representation failure?
- Basis in paper: [explicit] The Conclusion proposes future studies using action-rate diagnostics to "disentangle objective-induced abstention from representational insufficiency."
- Why unresolved: It remains unclear if the baseline "Liquidation Equilibrium" is purely a tokenization failure or partially a rational response to the asymmetric loss function.
- Evidence: Experiments applying the asymmetric loss to synthetic data with known signal properties to isolate the loss effect from the tokenization effect.

### Open Question 4
- Question: Does the method generalize to domains with true physical first-principles?
- Basis in paper: [inferred] The paper claims the framework is "domain-agnostic" (Section 1.3) but validates it only on financial data where physics are "imposed" rather than inherent.
- Why unresolved: The utility of kinematic tokens in systems with rigorous governing laws (e.g., robotics) is hypothesized but unverified.
- Evidence: Validation on a physical control task (e.g., inertial navigation) where governing equations are known and strictly enforced.

## Limitations

- **α Parameter Sensitivity:** The paper uses α=5 for spline regularization without systematic ablation. While this value was chosen based on the signal-to-noise characteristics of the data, the optimal smoothing strength likely varies across domains, noise regimes, and time windows. This hyperparameter could significantly impact performance, particularly in domains with different sampling rates or noise characteristics than daily equity data.

- **Discrete vs. Continuous Representation Gap:** The paper contrasts discrete baseline tokens (RawGPT) against continuous spline tokens, but doesn't systematically compare against other continuous representations (e.g., Kalman filters, wavelet transforms, or other spline methods). The improvement might stem from continuity per se rather than the specific spline formulation.

- **Generalization to Regime Shifts:** The method assumes underlying dynamics follow stochastically-forced second-order systems. This assumption may break down during market regime shifts, structural breaks, or periods of extreme volatility where higher-order dynamics dominate. The paper doesn't test robustness across major market regime changes.

## Confidence

- **High Confidence:** Spline fitting optimization (well-established mathematics, explicit equations, clear physical interpretation)
- **Medium Confidence:** Asymmetric loss overcoming (compelling experimental demonstration but mechanism not fully characterized)
- **Medium Confidence:** Dual window anchoring (sound stationarity argument but limited independent ablation)
- **Low Confidence:** Volume model justification (explicitly phenomenological, lacks theoretical grounding)

## Next Checks

1. **α Sensitivity Ablation:** Systematically vary α across multiple orders of magnitude (e.g., α ∈ {0.1, 1, 3, 5, 10, 20, 50}) on a diverse set of assets. Measure not just Sharpe ratio but also action distribution stability, calibration metrics, and robustness to noise injection. This will determine if α=5 is optimal or if the method is robust across a range of smoothing parameters.

2. **Discrete Baseline Enhancement:** Test whether discrete baselines can achieve similar performance if enhanced with explicit derivative computation (e.g., Savitzky-Golay filters for smoothing before differentiation). This would isolate whether the improvement comes from continuity itself versus the specific spline representation.

3. **Regime Shift Robustness:** Evaluate performance across major market regime changes (e.g., dot-com bubble, 2008 financial crisis, COVID-19 crash) and during periods of elevated volatility. Measure degradation in Sharpe ratio and action distribution stability compared to baseline periods. This tests whether the method's assumptions about smooth second-order dynamics break down under stress conditions.