---
ver: rpa2
title: 'When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory,
  and Incomplete Task Descriptions'
arxiv_id: '2507.20439'
source_url: https://arxiv.org/abs/2507.20439
tags:
- task
- descriptions
- code
- llms
- unclear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that code generation LLMs struggle with
  task descriptions containing ambiguity, incompleteness, and contradictions, leading
  to a 20-40% drop in accuracy (Pass@1) on standard benchmarks like HumanEval and
  MBPP. Larger models (32-34B parameters) show some robustness improvements but still
  fail to correctly interpret flawed descriptions, with up to 60-90% of generated
  code being semantically incorrect even when syntactically valid.
---

# When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions

## Quick Facts
- arXiv ID: 2507.20439
- Source URL: https://arxiv.org/abs/2507.20439
- Reference count: 40
- Key outcome: Code generation LLMs show 20-40% accuracy drops on benchmarks with ambiguous, contradictory, or incomplete task descriptions.

## Executive Summary
This paper systematically evaluates how well code generation language models handle task descriptions containing ambiguity, incompleteness, and contradictions. Through controlled mutation of established coding benchmarks (HumanEval and MBPP), the study reveals significant performance degradation across all defect types, with contradictory descriptions causing the most severe impact. While larger models show some robustness improvements, they still fail to correctly interpret flawed descriptions, often generating syntactically valid but semantically incorrect code. The research introduces new evaluation metrics including Runnable-but-Incorrect Rate (RIR) to capture the safety risks of code that executes but solves the wrong problem.

## Method Summary
The study generates unclear task descriptions by systematically mutating clear ones from HumanEval and MBPP benchmarks using controlled mutations. These mutations create incomplete, ambiguous, and contradictory variants that are validated by expert reviewers. The mutated descriptions are then evaluated using multiple code generation models including GPT-4, Qwen-32B, and others. Performance is measured using standard metrics (Pass@1) alongside novel metrics like Runnable-but-Incorrect Rate (RIR) and classification accuracy (MCC) to assess whether models can detect unclear prompts. The experiments run on infrastructure with 4× Intel Xeon Silver 4416+ CPUs and 4× NVIDIA L40S GPUs.

## Key Results
- Contradictory descriptions reduce accuracy by up to 40%, with GPT-4 dropping from 73.8% Pass@1 to only 6.7%
- Ambiguous descriptions produce semantically misaligned outputs, leading to elevated AttributeError rates
- Incomplete descriptions cause structural errors including TypeError (37.3%) and SyntaxError (41.3%)
- Runnable-but-Incorrect Rate (RIR) reaches 60-90% on flawed prompts, indicating code that runs but is wrong
- Larger models (32-34B parameters) show marginal robustness improvements but still fail on contradictory requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic ambiguity likely induces the model to statistically hallucinate rather than reason, producing valid code that solves the wrong problem.
- **Mechanism:** Ambiguity creates an open-ended search space. Without precise constraints, the model defaults to high-probability patterns from its training distribution rather than deriving intent from the specific prompt. This results in high "runnable but incorrect" rates because the code functions, just not as the user intended.
- **Core assumption:** The model prioritizes syntactic completion and token probability over verifying semantic alignment with a potentially underspecified user intent.
- **Evidence anchors:**
  - [abstract] "ambiguous ones produce semantically misaligned outputs"
  - [section] 6.3 "Ambiguous task descriptions... tend to generate semantically flawed logic... marked by elevated AttributeError... from misinterpreting vague specifications"
  - [corpus] "Hidden in Plain Sight" (Neighbor paper) confirms LLMs struggle specifically in "underspecified" scenarios where instructions rely on implicit context.
- **Break condition:** If the ambiguous phrasing statistically correlates strongly with a single implementation in the training data, the model may succeed by coincidence (prior matching) rather than reasoning.

### Mechanism 2
- **Claim:** Contradictory requirements appear to trigger a "satisficing" behavior where the model attempts to fulfill conflicting conditions simultaneously, resulting in logical inconsistency or runtime crashes.
- **Mechanism:** Instruction-tuned models are optimized to follow provided directions. When directions are mutually exclusive (e.g., "return a string" vs "return an integer"), the model lacks the metacognitive guardrails to reject the task. Instead, it often generates code that tries to satisfy both or defaults to the most recent instruction, leading to the highest error rates.
- **Core assumption:** Models lack an internal "consistency checker" that validates logical preconditions before code synthesis begins.
- **Evidence anchors:**
  - [abstract] "contradictory ones result in logically inconsistent code"
  - [section] 6.2 "contradictory descriptions have the most severe impact, reducing accuracy by up to 40%... GPT-4 achieves 73.8% Pass@1 on original... but only 6.7% on contradictory ones"
  - [corpus] "Uncovering Systematic Failures" supports this by showing LLMs fail to verify code against specifications, implying they cannot self-correct logical conflicts.
- **Break condition:** If the contradiction is subtle or buried in irrelevant context, the model may ignore the conflicting clause entirely, accidentally stabilizing performance.

### Mechanism 3
- **Claim:** Incomplete descriptions likely force the model to "fill in the blanks" with default assumptions, causing structural errors like type mismatches or missing imports.
- **Mechanism:** When constraints are omitted (e.g., input types), the model must infer the missing logic. If this inference is wrong, it generates structural mismatches (e.g., expecting a list but receiving an int). Unlike semantic errors (code runs but is wrong), these often result in immediate runtime exceptions.
- **Core assumption:** The model treats omitted information as an invitation to guess based on common coding patterns rather than a signal to halt or request clarification.
- **Evidence anchors:**
  - [abstract] "incomplete descriptions cause structural errors"
  - [section] 6.3 "Incomplete task descriptions: are particularly prone to fundamental runtime failures... TypeError (37.3%)... SyntaxError (41.3%)"
  - [corpus] Corpus signals are weak regarding structural error specifics; reliance is primarily on the paper's Section 6.3.
- **Break condition:** If the missing detail is trivial (e.g., standard library default), the model's training priors may correctly fill the gap.

## Foundational Learning

- **Concept:** **Runnable-but-Incorrect Rate (RIR)**
  - **Why needed here:** Standard metrics like Pass@1 fail to capture the danger of code that compiles but behaves incorrectly. The paper shows RIR skyrocketing to 60-90% on flawed prompts, which is a critical safety risk.
  - **Quick check question:** Why does a high Successful Execution Rate (SER) create a false sense of security when deploying code LLMs?

- **Concept:** **Model Scale vs. Robustness**
  - **Why needed here:** The paper suggests a non-linear relationship; larger models (32B+) improve robustness but do not solve the underlying failure to detect contradictions.
  - **Quick check question:** If a 7B model fails on a contradictory prompt, why shouldn't you assume a 70B model will automatically succeed?

- **Concept:** **Metacognitive Failure (RQ1)**
  - **Why needed here:** Unlike humans, who ask questions when requirements are unclear, LLMs attempt generation regardless of prompt quality (MCC near 0).
  - **Quick check question:** Why is the inability to classify a prompt as "unclear" (RQ1) arguably a more fundamental failure than generating bad code (RQ2)?

## Architecture Onboarding

- **Component map:** Input (Mutated Task Description) -> Processor (Code LLM) -> Evaluation (Pass@1 & RIR)
- **Critical path:** The transition from **Syntactic Validity** (High SER) to **Semantic Correctness** (Low RIR). The system breaks when the model generates plausible-looking code that fails test cases due to misinterpreted intent.
- **Design tradeoffs:** Tuning for "helpfulness" (always generating an answer) vs. "honesty" (refusing unclear prompts). The paper demonstrates current models are tuned for helpfulness to a fault, generating bad code rather than rejecting bad prompts.
- **Failure signatures:**
  - **Incomplete Prompts:** High rate of `TypeError`, `SyntaxError` (Structural failure)
  - **Ambiguous Prompts:** High rate of `AttributeError`, `KeyError` (Semantic/assumption failure)
  - **Contradictory Prompts:** High `ValueError` or logical dead ends (Consistency failure)
- **First 3 experiments:**
  1. **Baseline Robustness Check:** Run a set of 50 clear coding tasks and their "Contradictory" mutations through your target model. Measure the delta in Pass@1 to confirm the 30-40% drop suggested in [section] 6.2.
  2. **RIR Analysis:** Execute the generated code from the first experiment. Specifically measure the "Runnable but Incorrect Rate" to verify if the model is producing "silent errors" (code that runs but is wrong).
  3. **Classification Probe:** Test if the model can self-correct. Feed the failed prompts back to the model with the instruction "Identify any ambiguities or contradictions in this prompt," to see if it can detect the flaws it just failed to solve (RQ1 validation).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific fine-tuning strategies on noisy or unclear data improve model robustness without compromising performance on clear task descriptions?
- **Basis in paper:** [explicit] The conclusion states that future research should "investigate training strategies... that enable models to better interpret, detect, and recover from vague or under-specified task descriptions."
- **Why unresolved:** The study evaluates pre-trained models off-the-shelf; it does not attempt to train or fine-tune models to handle the identified defect types.
- **What evidence would resolve it:** A comparison of Pass@1 scores on both clear and mutated benchmarks between a baseline model and a model fine-tuned on the proposed dataset of unclear descriptions.

### Open Question 2
- **Question:** To what extent can architectural modifications or prompting techniques enable LLMs to actively detect ambiguity and request clarification rather than generating incorrect code?
- **Basis in paper:** [inferred] RQ1 results show models cannot reliably classify clear vs. unclear descriptions (MCC ~0.5). The introduction notes that unlike humans, models "cannot natively react... and would nevertheless attempt to produce a solution."
- **Why unresolved:** The current study only measures the models' inability to classify inputs; it does not test mechanisms for interactive clarification or rejection of invalid prompts.
- **What evidence would resolve it:** An evaluation of a dialogue-based agent's ability to identify logical gaps in mutated prompts and generate valid clarification questions verified by human experts.

### Open Question 3
- **Question:** Do the observed robustness degradation patterns (e.g., 20-40% accuracy drops) generalize to statically typed languages or complex, multi-module software projects?
- **Basis in paper:** [explicit] The "Threats to validity" section notes the findings are derived from Python tasks and may not cover "other programming languages, multimodule projects, or domain-specific coding scenarios."
- **Why unresolved:** The experiments were restricted to self-contained Python functions (HumanEval/MBPP), leaving the impact of strict type systems or architectural complexity on ambiguity handling unknown.
- **What evidence would resolve it:** Replicating the mutation methodology on benchmarks involving static languages (e.g., Java, Rust) or repository-level coding tasks and comparing the Pass@1 degradation rates.

## Limitations

- The mutation process relies heavily on GPT-4's ability to create valid test cases, introducing potential model-induced bias in the dataset construction.
- Expert validation was limited to 5 reviewers, which may not capture the full spectrum of interpretation for ambiguous requirements.
- The study focuses primarily on coding benchmarks, limiting generalizability to other domains where requirements ambiguity might manifest differently.

## Confidence

- **High Confidence:** The observed performance degradation (20-40% Pass@1 drop) on mutated benchmarks is well-supported by the experimental results across multiple model sizes.
- **Medium Confidence:** The attribution of specific error types to particular defect categories (structural vs. semantic) is reasonable but could benefit from more granular analysis of failure modes.
- **Medium Confidence:** The conclusion that larger models show only marginal robustness improvements is supported by the data, though the relationship between model scale and ambiguity handling warrants further investigation.

## Next Checks

1. **Cross-Domain Validation:** Apply the mutation methodology to non-coding benchmarks (e.g., QA or reasoning tasks) to assess whether the observed robustness patterns generalize beyond code generation.
2. **Human-in-the-Loop Evaluation:** Conduct a user study where developers attempt to resolve the ambiguous/contradictory prompts before code generation, measuring whether human clarification can mitigate the performance drop.
3. **Fine-tuning Experiment:** Train models on augmented data containing controlled amounts of ambiguous and contradictory examples to test whether exposure during training improves robustness to such defects.