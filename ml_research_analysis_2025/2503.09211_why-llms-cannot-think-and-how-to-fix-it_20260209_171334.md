---
ver: rpa2
title: Why LLMs Cannot Think and How to Fix It
arxiv_id: '2503.09211'
source_url: https://arxiv.org/abs/2503.09211
tags:
- conversation
- thought
- decisions
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current LLMs cannot form genuine "thoughts"
  in their feature space due to deterministic architectures and population-level training.
  It defines thoughts as hidden, non-deterministic information that influences output
  probabilities, which existing models lack.
---

# Why LLMs Cannot Think and How to Fix It

## Quick Facts
- arXiv ID: 2503.09211
- Source URL: https://arxiv.org/abs/2503.09211
- Authors: Marius Jahrens; Thomas Martinetz
- Reference count: 2
- Key outcome: Current LLMs cannot form genuine "thoughts" in feature space due to deterministic architectures and population-level training; authors propose introducing randomness and counterfactual training to enable internal decision-making.

## Executive Summary
This paper argues that current large language models cannot form genuine "thoughts" because their deterministic architectures and population-level training prevent the formation of hidden, non-deterministic information that influences output probabilities. The authors define thoughts as hidden information in the model's feature space that affects outputs beyond what's present in the input sequence, which existing models mathematically cannot represent. They propose introducing randomness through random vectors or embedding noise, combined with fine-tuning on counterfactual examples that link instance identifiers to decision-consistent continuations.

## Method Summary
The authors propose modifying LLM architectures to inject randomness at the feature level (via random vectors or embedding noise) and fine-tuning on counterfactual examples that link conversation instance identifiers to decision-consistent continuations. The approach aims to enable models to represent decisions internally in feature space rather than relying on output sampling. Training ensures that same instance identifiers lead to consistent internal decisions while maintaining overall population-level distribution. The method could be implemented through ORPO (pre-training-finetuning-objective) or feature reconstruction approaches.

## Key Results
- Current LLM architectures mathematically cannot host hidden decision states due to deterministic computation from input sequences
- Population-level training forces models to represent superpositions of all possible continuations rather than instance-specific decisions
- The proposed approach of random vector injection with counterfactual training could enable feature-space decision representations and improve reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current LLM architectures mathematically cannot host hidden decision states because the model state is deterministically computed from the input sequence.
- Mechanism: In decoder-only transformers, State_t = f(X_1, ..., X_t). Since H(Y_t|X_1,...,t) - H(Y_t|State_t, X_1,...,t) = 0, the conditional mutual information I(State_t; Y_t|X_1,...,t) = 0. No information exists in the state beyond what the input sequence provides.
- Core assumption: "Thought" requires hidden information that influences output probabilities non-deterministically.
- Evidence anchors: [Section 4.1]: "The only source of randomness in these models is the sampling process from the model's output token probability distribution."; [Section 3.2]: Mathematical formulation using conditional mutual information.

### Mechanism 2
- Claim: Population-level training prevents instance-specific decision collapse.
- Mechanism: The language modeling objective trains P(model outputs x_t | x_1,...,x_{t-1}) = E_{human,instance}[P(Human outputs x_t | human, instance, x_1,...,x_{t-1})]. The model learns a superposition of all possible continuations rather than committing to one.
- Core assumption: Enabling instance-specific decisions requires allowing the model to deviate from population-level predictions while maintaining the population distribution in expectation.
- Evidence anchors: [Section 4.2]: "This forces the predicted distribution to represent the entire population of speakers and possible conversations, rather than specific instances."; [Section 4.2.1, Equation 6]: Proposes E_instance[P(...)] = E_{human,instance}[P(...)] as the modified objective.

### Mechanism 3
- Claim: Injecting randomness and training on instance-linked counterfactuals enables feature-space decision representations.
- Mechanism: A random vector (conversation instance identifier) conditions the model state. Training presents multiple continuations for the same instance that reflect the same unspoken decision, forcing the model to encode that decision in its features to predict consistently.
- Core assumption: The model will generalize from training instances to collapse output distributions for novel conversations when given a random seed.
- Evidence anchors: [Section 6.1]: "The random vector or its seed can be interpreted as a unique identifier for each conversation instance."; [Section 6.1.2]: Describes preference optimization (ORPO) and feature reconstruction as training approaches.

## Foundational Learning

- Concept: Conditional mutual information I(X;Y|Z)
  - Why needed here: This measure formalizes whether the model state contains information about outputs beyond the input sequence. The paper proves this equals zero for current architectures.
  - Quick check question: Given that you know the full input sequence, does knowing the model's internal state tell you anything additional about the output distribution?

- Concept: Symmetry breaking in sequential decisions
  - Why needed here: Forming a coherent argument requires committing to one reasoning path. Current models rely on output sampling to break symmetries, which cannot happen in feature space.
  - Quick check question: If three reasoning paths have equal probability, how can the model begin processing one specifically before any tokens are sampled?

- Concept: Counterfactual training examples
  - Why needed here: To learn instance-specific decisions, the model must see multiple continuations arising from the same hidden decision, teaching it to represent decisions internally.
  - Quick check question: If a model chooses "rock" in Rock-Paper-Scissors, how would you construct training examples that reward consistent internal representation without explicitly stating the choice?

## Architecture Onboarding

- Component map: Input tokens → Embedding layer → [Proposed: Random vector injection point] → Transformer blocks → Output logits → Sampling

- Critical path:
  1. Identify injection point for randomness (input start, embedding noise, or text-space integer)
  2. Construct training dataset with instance identifiers and decision-consistent continuations
  3. Fine-tune to map instance identifiers → internal decision representations → collapsed output distributions

- Design tradeoffs:
  - Random vector at conversation start vs. noise per token: Former gives single decision coherence; latter enables independent sequential decisions
  - Text-space random integer vs. embedding noise: Text is architecture-agnostic but may cause unwanted statistical dependencies
  - Network depth limits sequential thought steps without recurrence (Section 6.2)

- Failure signatures:
  - Output distribution remains uniform across choices despite instance identifier
  - Different continuations from same instance show inconsistent "decisions" (Figure 2 demonstrates this in current models)
  - Model ignores random seed, treating it as noise

- First 3 experiments:
  1. Rock-Paper-Scissors test: Inject random seed, prompt model to "choose but don't tell," then sample multiple continuations. Check if distribution collapses to one option consistently.
  2. Branching consistency probe: Create conversation where model claims a decision, branch with identical seed but minor prompt rephrasings, verify decision robustness.
  3. Speculative decoding improvement: Train with instance identifiers, measure if distilled model better predicts full model outputs given same seed (Section 5.3 prediction).

## Open Questions the Paper Calls Out

- Question: What is the optimal method for introducing non-determinism to enable feature-space decision making?
  - Basis in paper: [explicit] Section 7.4 identifies "the optimal method for introducing randomness" (e.g., random vectors, embedding noise, text-space integers) as a key unanswered question.
  - Why unresolved: The paper proposes multiple implementation strategies but provides no empirical data comparing their efficacy.
  - What evidence would resolve it: Comparative experiments evaluating decision consistency and model performance across different randomization injection points.

- Question: Which fine-tuning strategy most effectively incentivizes models to form consistent internal decision representations?
  - Basis in paper: [explicit] Section 7.4 explicitly asks for the "best fine-tuning strategy," listing naive fine-tuning, ORPO, and feature reconstruction as candidates.
  - Why unresolved: While theoretical justifications are given for each, the authors note that empirical validation remains future work.
  - What evidence would resolve it: Benchmarks measuring the stability of "thoughts" across continuations using different optimization techniques.

- Question: Does constraining hidden reasoning to language (hidden Chain-of-Thought) hinder abstract thinking compared to feature-space thoughts?
  - Basis in paper: [explicit] Section 7.3 states "Experiments are necessary to determine if this limitation hinders abstract thinking," particularly for multimodal models.
  - Why unresolved: It is currently unknown if language-bound thoughts are less efficient or capable than purely feature-based representations.
  - What evidence would resolve it: Ablation studies comparing abstract reasoning performance between models using hidden verbal thoughts and those using the proposed feature-space method.

## Limitations
- The proposed solutions remain largely conceptual without empirical validation, despite sound theoretical foundation
- Training data requirements are unclear - minimum dataset size and construction methodology for counterfactual examples remain undefined
- Analysis focuses specifically on decoder-only transformers, with uncertain transferability to encoder-decoder architectures

## Confidence
- High Confidence: The theoretical foundation proving current architectures cannot host hidden decision states (Mechanism 1)
- Medium Confidence: The population-level training collapse mechanism (Mechanism 2)
- Low Confidence: The proposed solution of random vector injection and counterfactual training (Mechanism 3)

## Next Checks
1. Rock-Paper-Scissors Baseline Test: Implement the proposed random seed injection and train on synthetic decision-consistent data. Measure whether output distributions collapse to single choices within instances while maintaining population-level diversity across instances.

2. Feature Space Probing: After training with instance identifiers, use linear probes or feature attribution methods to verify that internal representations encode decision information beyond what's present in the input sequence.

3. Speculative Decoding Benchmark: Train a distilled model using the proposed approach and measure improvements in speculative decoding accuracy compared to standard distillation, validating the claim that internal decision representations improve prediction efficiency.