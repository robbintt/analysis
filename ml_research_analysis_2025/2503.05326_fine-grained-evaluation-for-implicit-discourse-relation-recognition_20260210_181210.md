---
ver: rpa2
title: Fine-Grained Evaluation for Implicit Discourse Relation Recognition
arxiv_id: '2503.05326'
source_url: https://arxiv.org/abs/2503.05326
tags:
- discourse
- senses
- level-2
- data
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fine-grained analysis of pre-trained language
  models for implicit discourse relation recognition, a challenging NLP task due to
  the absence of explicit connectives between text spans. The study examines model
  performance across 16 level-2 discourse relations in the PDTB 3.0 dataset, revealing
  difficulties in recognizing certain relations and identifying confusing sense pairs.
---

# Fine-Grained Evaluation for Implicit Discourse Relation Recognition

## Quick Facts
- arXiv ID: 2503.05326
- Source URL: https://arxiv.org/abs/2503.05326
- Authors: Xinyi Cai
- Reference count: 10
- This paper presents a fine-grained analysis of pre-trained language models for implicit discourse relation recognition, revealing difficulties in recognizing certain relations and identifying confusing sense pairs.

## Executive Summary
This paper presents a fine-grained analysis of pre-trained language models for implicit discourse relation recognition, a challenging NLP task due to the absence of explicit connectives between text spans. The study examines model performance across 16 level-2 discourse relations in the PDTB 3.0 dataset, revealing difficulties in recognizing certain relations and identifying confusing sense pairs. Through semi-manual annotation, the authors augment data for underrepresented relations, achieving significant improvements in F1 scores. The analysis uncovers that models rely heavily on surface linguistic cues for certain relations, and predictions across sense levels can be inconsistent. The work provides insights for future research directions, including data augmentation strategies, model testing on difficult data, and leveraging hierarchical information between discourse relation levels.

## Method Summary
The paper fine-tunes three pre-trained language models (BERT-large, ALBERT-xlarge, RoBERTa-large) on the PDTB 3.0 dataset for implicit discourse relation recognition. The models use the [CLS] token embedding for classification into 16 level-2 senses (excluding "Exception"). Training employs 10-fold cross-validation with 8:1:1 splits, max 10 epochs, early stopping patience of 5 steps, batch size of 8, and learning rate of 2e-6. The authors conduct semi-manual annotation to augment data for underrepresented senses, analyze surface cue reliance through manual inspection, and examine inconsistencies between level-1 and level-2 predictions.

## Key Results
- Data augmentation significantly improves level-2 sense recognition for underrepresented relations (Disjunction: 0.0% to 48.28% F1; Cause+Belief: +37.95% F1)
- Models rely heavily on surface linguistic cues (to-infinitives, negation, syntactic patterns) for certain relations, with cue presence correlating strongly with correct predictions
- Separate classification of level-1 and level-2 senses produces inconsistent predictions in most cases, with correct level-1 but wrong level-2 being the dominant pattern

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Data augmentation selectively improves level-2 sense recognition for specific relation types, but not universally.
- **Mechanism:** Adding annotated examples for underrepresented senses (Disjunction, Equivalence, Cause+Belief) increases F1 scores by 24-48 percentage points (RoBERTa), because model confidence improves with exposure to rare patterns. However, senses like Substitution and Similarity show no improvement or decline, suggesting inherent semantic difficulty rather than data scarcity.
- **Core assumption:** The semi-manual annotation quality (0.80 inter-annotator agreement) sufficiently approximates ground truth to provide useful training signal.
- **Evidence anchors:**
  - [abstract] "The annotated data significantly help improve implicit discourse relation recognition for level-2 senses."
  - [Section 5.2, Table 2] Disjunction improved from 0.0% to 48.28% F1; Cause+Belief improved by 37.95% with added data.
  - [corpus] Related work on "Synthetic Data Augmentation for Cross-domain IDRR" suggests LLM-generated augmentation also shows promise, though domain transfer remains challenging.
- **Break condition:** If annotation quality drops below ~0.7 agreement, or if added data introduces systematic biases (e.g., over-representing certain connective patterns), performance gains may reverse.

### Mechanism 2
- **Claim:** Pre-trained models rely heavily on surface linguistic cues for certain level-2 senses, creating brittle performance.
- **Mechanism:** The paper identifies five surface cues—*to-infinitives* (Purpose: 98.7% in correct vs 46.5% in wrong predictions), *and+verb* (Asynchronous: 62.4% vs 4.7%), *negation* (Substitution: 93.9% vs 49.6%), *same subject* (Substitution: 61.3% vs 5.6%), and *similar syntactic pattern* (Conjunction: 53.5% vs ~4%)—that strongly correlate with correct predictions. Models appear to use these as proxies rather than learning deeper semantic relations.
- **Core assumption:** These surface cues are over-represented in training data and models learn statistical associations rather than abstract discourse semantics.
- **Evidence anchors:**
  - [Section 6, Table 3] Quantitative breakdown of cue presence in correct vs incorrect predictions across multiple senses.
  - [Section 6] "If there is no negation, it is much difficult for the model to identify these senses."
  - [corpus] "Discursive Circuits" work (arXiv 2510.11210) investigates which transformer components process discourse—relevant but doesn't directly confirm surface-cue reliance.
- **Break condition:** If test data systematically excludes these surface cues, model performance on affected senses would likely degrade substantially.

### Mechanism 3
- **Claim:** Separate classification of level-1 and level-2 senses produces inconsistent predictions, wasting hierarchical signal.
- **Mechanism:** When models predict level-1 (4-way: Expansion, Comparison, Contingency, Temporal) and level-2 (16-way) independently, they frequently assign incompatible labels (e.g., level-1 = Contingency, level-2 = Conjunction which belongs to Expansion). Most inconsistencies are correct level-1 but wrong level-2.
- **Core assumption:** The PDTB sense hierarchy encodes meaningful constraints that could regularize predictions if jointly modeled.
- **Evidence anchors:**
  - [Section 7] "Most are cases that models correctly predict level-one senses but wrongly for level-2 senses."
  - [Section 7, Figure 2] Visualization showing substantial inconsistent predictions across all three models.
  - [corpus] "Multi-Lingual IDRR with Multi-Label Hierarchical Learning" explicitly models hierarchical dependencies—supporting this direction.
- **Break condition:** Joint hierarchical models add complexity; if hierarchy is poorly calibrated to actual semantic relationships, constraints may introduce new errors.

## Foundational Learning

- **Concept: PDTB Sense Hierarchy (Level-1 → Level-2)**
  - **Why needed here:** The paper's entire analysis depends on understanding that 4 top-level senses (Expansion, Comparison, Contingency, Temporal) branch into 16+ level-2 senses, and that inconsistencies between levels reveal model limitations.
  - **Quick check question:** If a model predicts "Temporal" at level-1 but "Conjunction" at level-2, why is this inconsistent?

- **Concept: Implicit vs. Explicit Discourse Relations**
  - **Why needed here:** The task difficulty stems from absent connectives; understanding that explicit relations provide "free" lexical cues (connectives like "however," "because") clarifies why implicit recognition is harder and why surface cues become proxies.
  - **Quick check question:** Why would removing a connective from an explicit relation create a valid implicit training example?

- **Concept: Class Imbalance Effects on F1**
  - **Why needed here:** Macro-F1 is heavily influenced by rare classes (Disjunction: 30 examples, Similarity: 31); understanding this explains why macro-F1 (43.82-53.07%) is much lower than accuracy and why data augmentation targets specific senses.
  - **Quick check question:** Why might adding 221 examples to Disjunction (from 30 to 251) produce a larger F1 gain than adding 1,500 to Conjunction?

## Architecture Onboarding

- **Component map:** [Arg1 text] + [Arg2 text] → [CLS] token aggregation → Pre-trained encoder (BERT/ALBERT/RoBERTa) → Linear classification head (W ∈ R^{K×H}) → Softmax → Level-2 sense prediction (K=16)

- **Critical path:** The [CLS] representation must encode cross-argument relationships. If pre-training didn't capture discourse-relevant patterns (e.g., next-sentence prediction helps), fine-tuning on limited PDTB data cannot fully compensate.

- **Design tradeoffs:**
  - Separate level-1/level-2 classifiers vs. joint hierarchical prediction: Simpler implementation vs. consistency guarantees
  - Cross-validation (10-fold) vs. single split: More reliable estimates on sparse data vs. computational cost
  - Semi-manual vs. fully automatic annotation: Quality control vs. scalability

- **Failure signatures:**
  - Near-zero F1 on rare senses (Disjunction: 0.0%) despite reasonable training accuracy → class imbalance + insufficient feature learning
  - High accuracy on senses with salient surface cues but random performance when cues absent → over-reliance on shortcuts
  - Level-1/level-2 prediction mismatch → no hierarchical constraint enforcement

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune RoBERTa-large on PDTB 3.0 with 10-fold cross-validation; verify macro-F1 ~53% and identify which level-2 senses your implementation struggles with.
  2. **Ablation on surface cues:** Filter test set to exclude examples containing identified cues (e.g., remove to-infinitives from Purpose test cases); measure F1 drop to quantify cue reliance.
  3. **Hierarchical consistency enforcement:** Add a simple post-hoc constraint that forces level-2 predictions to be children of level-1 predictions; measure consistency improvement and any accuracy tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can models be trained to reduce reliance on surface linguistic cues (e.g., to-infinitives, negation) while maintaining performance on level-2 discourse relation recognition?
- Basis in paper: [explicit] Section 6 shows models rely heavily on surface cues for certain relations (e.g., 98.7% of correctly predicted Purpose examples contain to-infinitives; 93.9% of correctly predicted Substitution examples contain negation).
- Why unresolved: The paper demonstrates the dependency but does not propose methods to overcome it. Models may perform poorly when these cues are absent.
- What evidence would resolve it: Experiments comparing model performance on test sets with and without these surface cues, or training approaches that explicitly minimize cue dependency.

### Open Question 2
- Question: How can hierarchical information between level-1 and level-2 senses be effectively integrated to ensure consistent predictions across both levels?
- Basis in paper: [explicit] Section 7 reveals inconsistent predictions across sense levels (e.g., models correctly predict "Expansion" at level-1 but incorrectly predict "Cause" at level-2), and Section 8 recommends utilizing hierarchical information.
- Why unresolved: Current models classify each level separately, ignoring the one-to-many mapping between levels, leading to logical inconsistencies.
- What evidence would resolve it: A joint prediction model that incorporates hierarchical constraints, demonstrating improved consistency and overall performance.

### Open Question 3
- Question: Which linguistic features or semantic knowledge do models lack that causes confusion between specific level-2 sense pairs (e.g., Synchronous vs. Cause, Asynchronous vs. Conjunction)?
- Basis in paper: [explicit] Section 5.3 identifies systematically confused pairs and hypothesizes causes (e.g., models fail to distinguish Synchronous from Cause because they lack temporal knowledge).
- Why unresolved: The paper identifies the confusion but does not investigate what features or knowledge could resolve it.
- What evidence would resolve it: Targeted feature analysis or knowledge injection experiments showing improved discrimination between confusing pairs.

## Limitations

- The semi-manual annotation quality (0.80 inter-annotator agreement) may introduce systematic biases if annotators over-rely on surface cues similar to those models already exploit.
- The analysis of surface cue reliance cannot definitively prove that models fail to learn deeper semantic relations versus using multiple complementary strategies.
- Inconsistent level-1/level-2 predictions may reflect genuine semantic ambiguity in the PDTB hierarchy rather than purely modeling failures.

## Confidence

- **High Confidence:** Data augmentation improves rare class performance (Disjunction: 0.0% → 48.28% F1) with clear quantitative backing from the 10-fold experiments.
- **Medium Confidence:** Surface cue reliance is demonstrated through statistical correlation, but the causal mechanism (shortcut learning vs. multi-strategy processing) remains unproven.
- **Medium Confidence:** Hierarchical inconsistency wastes signal, supported by quantitative mismatch rates but without comparison to alternative hierarchical modeling approaches.

## Next Checks

1. **Annotation Quality Stress Test:** Systematically perturb the augmented dataset by introducing controlled noise (e.g., randomly relabel 10-20% of examples) and measure the degradation in F1 gains to establish the minimum annotation quality threshold for effective augmentation.

2. **Cue Absence Performance:** Filter the test set to create a "cue-free" subset excluding examples containing any of the five identified surface cues, then evaluate model performance on this subset to quantify the actual performance penalty when surface patterns are absent.

3. **Hierarchical Modeling Comparison:** Implement a joint hierarchical classification model (e.g., using conditional probability constraints or multi-task learning) and compare its level-1/level-2 consistency and overall accuracy against the separate classification approach to determine if joint modeling resolves the inconsistency problem.