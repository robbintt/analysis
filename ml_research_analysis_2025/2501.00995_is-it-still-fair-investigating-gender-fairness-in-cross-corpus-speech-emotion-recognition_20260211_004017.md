---
ver: rpa2
title: Is It Still Fair? Investigating Gender Fairness in Cross-Corpus Speech Emotion
  Recognition
arxiv_id: '2501.00995'
source_url: https://arxiv.org/abs/2501.00995
tags:
- fairness
- gender
- cross-corpus
- emotion
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender fairness in cross-corpus speech
  emotion recognition (SER), addressing the gap in fairness generalizability across
  different corpora. While cross-corpus SER models excel in performance, they often
  introduce gender biases when applied to target corpora.
---

# Is It Still Fair? Investigating Gender Fairness in Cross-Corpus Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2501.00995
- Source URL: https://arxiv.org/abs/2501.00995
- Reference count: 23
- Primary result: Combined Fairness Adaptation (CFA) approach significantly reduces gender bias in cross-corpus speech emotion recognition compared to baseline methods.

## Executive Summary
This study investigates gender fairness in cross-corpus speech emotion recognition (SER), addressing the gap in fairness generalizability across different corpora. While cross-corpus SER models excel in performance, they often introduce gender biases when applied to target corpora. The authors propose a Combined Fairness Adaptation (CFA) approach that integrates fairness mechanisms for both source and target genders through adversarial training and contrastive loss. Experimental results using MSP-Podcast and BIIC-Podcast datasets demonstrate that the CFA model significantly reduces gender bias (∆SP and ∆EO) compared to baseline methods like PA-ReW and PA-FairW.

## Method Summary
The CFA approach combines emotion classification with adversarial gender neutralization and cross-corpus gender alignment. The model uses wav2vec2.0 features processed through a transformer encoder, with an emotion classifier (EC) and auxiliary gender classifier (GC) that includes a reverse gradient layer. A contrastive loss aligns gender features across source and target corpora. The total loss combines emotion classification, cross-corpus gender similarity, and adversarial gender neutralization through L_total = L_EC + 0.5×L_GSim − 0.5×L_GC. Training occurs in two stages: pre-training EC on the source corpus, then joint training with mixed mini-batches from both corpora.

## Key Results
- CFA achieves significant fairness improvements: for Anger, ∆SP of 0.256 compared to 0.363 for PA-ReW
- PA-CFA maintains competitive performance: 75.30 UAR for Anger versus 76.46 for PA baseline
- Source-only fairness methods (PA-ReW) fail to generalize: ∆SP of 0.159 on MSP-P but 0.321 on BIIC-P for Anger
- Gender detection accuracy drops to ~35-36% for PA-CFA, indicating successful gender neutralization

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Gender Neutralization via Gradient Reversal
Gradient reversal during backpropagation produces emotion-discriminative but gender-agnostic representations. A Gender Classifier (GC) branch receives shared embeddings from the Emotion Classifier (EC); during backpropagation, a reverse gradient layer flips the gradient sign, penalizing the EC for encoding gender-predictive information. This forces intermediate representations to minimize gender detectability while preserving emotion signals.

### Mechanism 2: Cross-Corpus Gender Alignment via Contrastive Loss
Explicitly enforcing feature-space proximity for same-gender samples across corpora reduces corpus-specific gender biases. A contrastive loss (L_GSim) minimizes Euclidean distance between same-gender embeddings from source and target corpora while maximizing distance for different-gender pairs. This aligns gender representations across domains, preventing the model from learning corpus-specific gender-emotion correlations.

### Mechanism 3: Joint Source-Target Fairness Optimization
Simultaneously incorporating fairness signals from both corpora during training prevents source-only fairness methods from failing at generalization. Mini-batches combine source (MSP-P) and target (BIIC-P) samples; the total loss balances emotion classification, cross-corpus gender alignment, and adversarial gender neutralization. This ensures the model learns fairness constraints applicable to both domains.

## Foundational Learning

- **Transfer Learning in SER**: Why needed here: Cross-corpus SER requires adapting models trained on source corpora to target corpora with different speakers, languages, and recording conditions. Quick check: Can you explain why a model trained on American English emotional speech might underperform or exhibit bias when applied to Taiwanese Mandarin speech?

- **Adversarial Debiasing**: Why needed here: The CFA architecture uses adversarial training to remove protected attribute information (gender) from learned representations. Quick check: How does a gradient reversal layer differ from simply removing gender from the loss function?

- **Fairness Metrics (ΔSP, ΔEO)**: Why needed here: Quantifying fairness requires understanding Statistical Parity (equal positive prediction rates across groups) and Equalized Odds (equal TPR/FPR across groups). Quick check: If a model achieves ΔSP ≈ 0 but ΔEO is high, what type of unfairness might still exist?

## Architecture Onboarding

- **Component map**: wav2vec2.0 features -> Transformer encoder -> Emotion Classifier (EC) -> Emotion prediction; Gender Classifier (GC) branch with reverse gradient -> Gender prediction; Contrastive Module -> Cross-corpus gender alignment loss

- **Critical path**: 1. Pre-train EC on source corpus (MSP-P) for emotion classification; 2. Enable joint training with mixed mini-batches (source + target); 3. Apply gradient reversal to GC branch; 4. Compute contrastive loss across same-gender pairs from different corpora; 5. Aggregate losses and update all parameters jointly

- **Design tradeoffs**: Fairness vs. accuracy: Table III shows PA-CFA has slightly lower UAR (e.g., 75.30 vs. 76.46 for Anger) than PA baseline—acceptable tradeoff per authors; Training complexity: Requires target gender labels during training; inference does not need them; Hyperparameter sensitivity: α, β set to 0.5; margin m=1 for contrastive loss—sensitivity not extensively analyzed

- **Failure signatures**: High gender detection accuracy from EC embeddings → gradient reversal not working; Large ΔSP/ΔEO gap between source and target → contrastive alignment insufficient; Emotion UAR drops >5% → over-regularization of gender features affecting emotion signals

- **First 3 experiments**: 1. Baseline validation: Reproduce PA and PA-ReW results on MSP-P → BIIC-P transfer; 2. Ablation study: Train PA-CFA without L_GSim (equivalent to PA-Adv); 3. Corpus swap: Reverse direction (BIIC-P as source, MSP-P as target)

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific acoustic or prosodic features contribute most to gender bias in cross-corpus SER, and can feature-side analysis identify where fairness degradation originates? The conclusion states future research will refine fairness mechanism through feature-side analysis to pinpoint specific areas where fairness issues arise in cross-corpus SER settings.

- **Open Question 2**: Does the Combined Fairness Adaptation approach generalize to other protected attributes beyond gender, such as age, accent, or native language? The introduction notes that "unfairness can manifest across multiple levels" and prior work aimed at "neutralizing different attributes, such as gender or age." However, the study exclusively examines gender as the protected attribute.

- **Open Question 3**: How does gender fairness generalization behave in multi-class emotion classification scenarios rather than binary tasks? The methodology states "For a detailed analysis, we analyze each emotion individually rather than as a 4-category SER task."

- **Open Question 4**: Is the observed performance-fairness trade-off (slight UAR decrease with CFA) inherent to fairness mechanisms, or can architecture improvements achieve Pareto-optimal outcomes? Table III shows PA-CFA has slightly lower overall UAR than PA across all emotions, and the authors note "this minor performance drop is a worthwhile trade-off for improved GF."

## Limitations
- CFA requires target gender labels during training, limiting applicability to fully unsupervised transfer scenarios
- Empirical evaluation focuses on a single source-target corpus pair (MSP-P to BIIC-P), raising questions about performance across diverse linguistic and cultural contexts
- The trade-off between fairness improvements and minor accuracy degradation (e.g., 75.30 vs. 76.46 UAR for Anger) warrants further investigation

## Confidence
- **High Confidence**: The mechanism of adversarial gender neutralization via gradient reversal is theoretically sound and empirically validated through gender detection accuracy reduction
- **Medium Confidence**: The effectiveness of contrastive loss for cross-corpus gender alignment is demonstrated, but the assumption of sufficient gender feature similarity across corpora requires validation in diverse language pairs
- **Medium Confidence**: The fairness metrics (ΔSP, ΔEO) appropriately quantify group-level disparities, though they may not capture intersectional fairness issues or fine-grained performance differences

## Next Checks
1. **Cross-linguistic validation**: Evaluate CFA performance when transferring between non-English corpora (e.g., Spanish to Mandarin) to test the generalizability of cross-corpus gender alignment assumptions
2. **Ablation study with target-only gender labels**: Assess CFA performance when target gender labels are only available for a subset of training data to evaluate semi-supervised adaptation capabilities
3. **Longitudinal fairness assessment**: Monitor fairness metrics over extended training periods to detect potential fairness-performance trade-off convergence or degradation patterns