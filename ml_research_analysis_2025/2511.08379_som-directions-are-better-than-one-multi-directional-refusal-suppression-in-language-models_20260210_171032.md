---
ver: rpa2
title: 'SOM Directions are Better than One: Multi-Directional Refusal Suppression
  in Language Models'
arxiv_id: '2511.08379'
source_url: https://arxiv.org/abs/2511.08379
tags:
- directions
- refusal
- harmful
- space
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-directional approach for suppressing
  refusal behavior in large language models using Self-Organizing Maps (SOMs). Instead
  of encoding refusal as a single direction as in prior work, the method trains SOMs
  on harmful prompt representations to identify multiple neurons, each representing
  a localized region of the refusal manifold.
---

# SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models

## Quick Facts
- arXiv ID: 2511.08379
- Source URL: https://arxiv.org/abs/2511.08379
- Reference count: 40
- Multi-directional approach using SOMs achieves significantly higher attack success rates (59.11% vs 0% for Llama2-7B) compared to single-direction baselines

## Executive Summary
This paper addresses the problem of refusal suppression in safety-aligned language models by proposing a multi-directional approach using Self-Organizing Maps (SOMs). Rather than encoding refusal behavior as a single direction in representation space, the method identifies multiple neurons on the refusal manifold, each representing a localized region. The approach demonstrates superior performance compared to traditional single-direction methods, achieving significantly higher attack success rates across seven different safety-aligned models while maintaining lower computational overhead than specialized jailbreak algorithms.

## Method Summary
The method trains a 4×4 hexagonal SOM on harmful prompt representations to identify multiple neurons, each representing a localized region of the refusal manifold. For each neuron, a direction is derived by subtracting the harmless centroid. Bayesian optimization is then used to select the optimal subset of these directions. During inference, the ablation operator is applied at every layer, iteratively projecting harmful representations onto the subspace orthogonal to all selected directions. This multi-directional approach captures the refusal manifold more effectively than single-direction methods while maintaining computational efficiency.

## Key Results
- Multi-directional approach achieves significantly higher attack success rates compared to single-direction baselines (e.g., 59.11% vs 0% for Llama2-7B)
- Outperforms specialized jailbreak algorithms while maintaining lower computational overhead
- Mechanistic analysis shows ablating multiple directions compresses harmful representations and shifts them toward harmless distributions
- The method generalizes across seven different safety-aligned models including Llama2, Llama3, Qwen, Gemma2, and Mistral variants

## Why This Works (Mechanism)
The multi-directional approach works by capturing the complex, multi-modal structure of the refusal manifold in representation space. Unlike single-direction methods that attempt to encode all refusal behavior along one vector, SOMs identify multiple localized regions where harmful prompts cluster. Each neuron represents a different "facet" of refusal behavior, and by ablating multiple directions simultaneously, the method can compress the entire harmful representation space more effectively. This prevents the model from reconstructing refusal behavior even when it might attempt to circumvent a single direction.

## Foundational Learning

**Self-Organizing Maps (SOMs)**: Unsupervised neural networks that learn topology-preserving mappings from high-dimensional data to lower-dimensional grids. Needed to identify localized regions on the refusal manifold; quick check: visualize SOM neurons in PCA space to verify they span harmful prompt regions.

**Bayesian Optimization (BO)**: Sequential optimization technique for finding optimal parameters in expensive black-box functions. Needed to efficiently search for the best subset of directions; quick check: monitor ASR trend from MD-2 to MD-7 to detect search stagnation.

**Representation Space Ablation**: Technique of projecting representations onto subspaces orthogonal to identified directions. Needed to suppress harmful behaviors while preserving general functionality; quick check: verify refusal token probability reduction at selected layer before full pipeline.

## Architecture Onboarding

**Component Map**: Data Preparation -> SOM Training -> Direction Derivation -> Bayesian Optimization -> Ablation Operator -> Evaluation

**Critical Path**: The core execution path involves extracting representations, training SOM, deriving directions, searching for optimal subset via BO, and applying ablation during generation. The layer selection (l*) is critical as it determines where the ablation has maximum impact.

**Design Tradeoffs**: Fixed 4×4 SOM topology provides computational efficiency but may miss subtle refusal variations; Bayesian optimization ensures optimal direction selection but requires 128-512 trials per model; applying ablation at every layer maximizes effectiveness but increases inference overhead.

**Failure Signatures**: 
- ASR stagnation or decrease when increasing k indicates poor direction quality or insufficient BO trials
- Layer selection failure shows minimal refusal token probability reduction under single-direction ablation
- SOM visualization reveals insufficient coverage of harmful prompt regions or excessive neuron overlap

**First Experiments**:
1. Extract last-token representations at all layers for harmful/harmless prompts and identify best layer l* by measuring refusal token probability drop under SD ablation
2. Train 4×4 hexagonal SOM on harmful representations and visualize neurons in PCA space to verify manifold coverage
3. Run Bayesian optimization search for k∈[2,7] directions on validation set and monitor ASR trend to detect search stagnation

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to seven chat-tuned models and single synthetic harm benchmark (HARMBENCH), limiting generalizability
- Fixed 4×4 SOM topology may miss subtle refusal variations in higher-dimensional representation space
- Compressed harmful representations might retain safety-relevant features that could be exploited in future attacks
- Bayesian optimization search requires 128-512 trials per model, potentially prohibitive at scale

## Confidence

- **High confidence**: SOMs effectively identify multiple localized directions on the refusal manifold that outperform single-direction baselines in controlled benchmark settings
- **Medium confidence**: The multi-directional approach generalizes across different model architectures and safety training regimes (Qwen, Llama2, Llama3, Gemma2, Mistral)
- **Low confidence**: The method provides a scalable and computationally efficient solution for real-world deployment, given the BO search requirements and fixed SOM topology

## Next Checks

1. Test MD ablation performance on base models (without chat tuning) and non-GPT-style architectures to verify cross-architecture generalization claims

2. Evaluate whether compressed harmful representations retain semantic content that could enable alternative refusal suppression or safety mechanism bypass

3. Assess computational efficiency by measuring inference overhead of multi-directional ablation versus single-direction approaches across different SOM grid sizes and BO search budgets