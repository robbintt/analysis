---
ver: rpa2
title: Structure-Aware Prototype Guided Trusted Multi-View Classification
arxiv_id: '2511.21021'
source_url: https://arxiv.org/abs/2511.21021
tags:
- multi-view
- learning
- view
- evidence
- views
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of trusted multi-view classification
  (TMVC) where information from multiple sources is heterogeneous, inconsistent, or
  conflicting. Existing TMVC methods either ignore latent neighborhood structures
  or rely on computationally expensive graph-based methods, leading to high costs
  and unreliable results.
---

# Structure-Aware Prototype Guided Trusted Multi-View Classification

## Quick Facts
- **arXiv ID:** 2511.21021
- **Source URL:** https://arxiv.org/abs/2511.21021
- **Reference count:** 40
- **Key outcome:** Achieves competitive downstream performance and robustness compared to prevalent TMVC methods while maintaining computational efficiency through structure-aware prototype learning without explicit graph construction.

## Executive Summary
This paper addresses trusted multi-view classification (TMVC) where information from multiple sources is heterogeneous, inconsistent, or conflicting. Existing TMVC methods either ignore latent neighborhood structures or rely on computationally expensive graph-based methods, leading to high costs and unreliable results. To overcome these limitations, the authors propose a structure-aware prototype-guided framework that introduces class-level prototypes to implicitly capture neighborhood structures without explicit graph construction. This approach simplifies intra-view neighbor relation learning and enables dynamic alignment of intra- and inter-view structures for more efficient and consistent cross-view consensus discovery. The framework includes a prototype-guided fine-grained fusion strategy that dynamically adjusts view contributions at the class level. Extensive experiments on six public multi-view datasets demonstrate that the proposed method achieves competitive downstream performance and robustness compared to prevalent TMVC methods while maintaining computational efficiency.

## Method Summary
The proposed method constructs class-level prototypes via average pooling of view-specific features within each mini-batch, rather than building pairwise sample graphs. These prototypes serve as anchors representing class characteristics. Three loss functions enforce prototype quality: Contrastive Prototype Learning Loss (features cluster around correct prototypes), Label Alignment Loss (prototypes align with class labels), and Neighbor Structure Alignment Loss (prototypes capture local neighborhood via KNN). A prototype-guided fine-grained fusion strategy dynamically adjusts view contributions at the class level by computing fusion weights based on belief opinion values, prototype correlation values (cross-view structural alignment via KL divergence), and uncertainty reduction. The method integrates evidential deep learning to produce uncertainty estimates that feed into the fusion mechanism, enabling robust decision-making under view conflicts without rigid fusion rules.

## Key Results
- Achieves competitive accuracy on six public multi-view datasets (PIE, HandWritten, ALOI, NUS-WIDE-OBJECT, MOSI, Food-101) compared to prevalent TMVC methods
- Demonstrates superior robustness to view conflicts through synthetic noise and label misalignment experiments
- Reduces computational complexity from O(n²d+n³Ld) to O(n²d+nKd) compared to graph-based approaches
- Shows effective uncertainty estimation where prototype-derived evidence uncertainty inversely correlates with prediction correctness

## Why This Works (Mechanism)

### Mechanism 1: Structure-Aware Prototype Learning
The method constructs class-level prototypes via average pooling of view-specific features within each mini-batch, rather than building pairwise sample graphs. These prototypes serve as anchors representing class characteristics. Three loss functions enforce prototype quality: Contrastive Prototype Learning Loss (features cluster around correct prototypes), Label Alignment Loss (prototypes align with class labels), and Neighbor Structure Alignment Loss (prototypes capture local neighborhood via KNN). The geometric relationships essential for robust representation learning can be adequately captured through class-level prototypes rather than sample-level pairwise relationships, achieving computational efficiency without sacrificing essential structure.

### Mechanism 2: Prototype-Guided Fine-Grained Fusion (PFF)
For each view m and class k, PFF computes a fusion weight based on three components: (1) Belief Opinion Value (normalized evidence strength), (2) Prototype Correlation Value (cross-view structural alignment via KL divergence between prototypes), and (3) uncertainty reduction (subtracting prototype uncertainty). These are combined as: v_k^(m) = B_k^(m) + C_k^(m) - u_k^(m), then normalized to produce final weights w_k^(m). View reliability varies at the class level (different classes may rely on different views), and prototype-derived evidence uncertainty inversely correlates with prediction correctness, enabling dynamic adjustment of view contributions.

### Mechanism 3: Evidential Deep Learning Integration
Each view produces evidence E^(m) through a neural network, converted to Dirichlet parameters α = e + 1. Belief masses b_k and uncertainty u are derived via: b_k^(m) = e_k^(m)/S^(m), u^(m) = K/S^(m). EDL loss combines classification loss (using Digamma function) and divergence loss (KL regularization). The uncertainty estimates feed into the PFF mechanism for adaptive fusion. Evidence theory provides a principled framework for uncertainty quantification that aligns with prototype representations through the shared evidence space, enabling robust decision-making under view conflicts.

## Foundational Learning

- **Concept: Dempster-Shafer Theory and Subjective Logic**
  - Why needed here: The entire framework builds on evidence theory (belief masses, uncertainty, evidence combination). Without understanding how EDL maps neural network outputs to Dirichlet distributions and belief masses, the PFF mechanism's components (B_k^(m), u^(m)) are opaque.
  - Quick check question: Can you explain how the Dirichlet parameters α relate to belief masses b and uncertainty u in a K-class setting?

- **Concept: Multi-view Learning and View Conflicts**
  - Why needed here: The paper addresses TMVC where views may be heterogeneous, inconsistent, or conflicting. Understanding what causes view conflicts (noise, misalignment, adversarial perturbations) is essential for appreciating why prototype-guided fusion helps.
  - Quick check question: What are two common sources of view conflicts in multi-view data, and why is traditional DST fusion sensitive to them?

- **Concept: Prototype Learning and Metric Learning**
  - Why needed here: The method relies on prototypes as class-level anchors with contrastive learning (pulling features toward positive prototypes, pushing from negative prototypes). Understanding KL divergence as a metric and how contrastive losses work is crucial.
  - Quick check question: In the Contrastive Prototype Learning Loss, what does the margin λ enforce, and why use KL divergence rather than Euclidean distance?

## Architecture Onboarding

- **Component map:**
  - Input layer: Multi-view data X^(m) for m ∈ [1, M] views
  - Feature encoders Ψ^(m)(·): View-specific DNNs producing features F^(m) ∈ R^(N×d)
  - Prototype module: Average pooling per class → prototypes p_k^(m)
  - Evidence extractor Φ(·): Maps features and prototypes to evidence space E^(m) and q_k^(m)
  - EDL layer: Converts evidence to Dirichlet parameters → belief masses b_k^(m), uncertainty u^(m)
  - PFF module: Computes B_k^(m) (belief), C_k^(m) (correlation), combines with uncertainty → fusion weights w_k^(m)
  - Output: Fused evidence e = [e_1, ..., e_K], class predictions, uncertainty estimates

- **Critical path:**
  1. Feature extraction per view (Ψ^(m))
  2. Prototype construction via mini-batch average pooling (Eq. 2)
  3. Evidence extraction for both samples and prototypes (Φ(·))
  4. Three prototype losses (L_c, L_t, L_n) + EDL loss (L_EDL)
  5. PFF weight computation → weighted evidence fusion (Eq. 19)

- **Design tradeoffs:**
  - Prototype dimension (set to 1024): Higher dimensions capture more structure but increase memory/compute; paper does not ablate this extensively.
  - KNN neighbor count |R|: Controls local vs. global structure balance; moderate values work best (Figure 8a).
  - Loss weights (α, β, γ): Require dataset-specific tuning; paper shows sensitivity analysis (Figure 8b-d).
  - Mini-batch prototype construction: Computationally efficient but may introduce noise if batch distribution doesn't reflect full dataset.

- **Failure signatures:**
  - Prototype collapse: All prototypes become similar; mitigated by Label Alignment Loss with inter-class separation term