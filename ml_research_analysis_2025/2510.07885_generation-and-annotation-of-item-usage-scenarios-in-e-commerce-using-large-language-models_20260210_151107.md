---
ver: rpa2
title: Generation and annotation of item usage scenarios in e-commerce using large
  language models
arxiv_id: '2510.07885'
source_url: https://arxiv.org/abs/2510.07885
tags:
- scenarios
- item
- usage
- complementary
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of large language models (LLMs) to
  generate item usage scenarios for complementary recommendation in e-commerce. Instead
  of relying on historical co-occurrence data, we hypothesized that people imagine
  specific usage contexts to identify complementary items.
---

# Generation and annotation of item usage scenarios in e-commerce using large language models

## Quick Facts
- arXiv ID: 2510.07885
- Source URL: https://arxiv.org/abs/2510.07885
- Reference count: 0
- Primary result: 82.9% of LLM-generated scenarios were unanimously judged plausible by human annotators

## Executive Summary
This study demonstrates that large language models can generate plausible item usage scenarios from product category names alone, providing a scalable foundation for complementary recommendation in e-commerce. Using GPT-4o-mini and hierarchical category paths, the researchers generated scenarios for 300 product categories and had 15 annotators evaluate their plausibility. The results show that LLM-generated scenarios can serve as interpretable, context-rich intermediate representations that avoid the cold-start problem of traditional co-occurrence-based recommendation systems.

## Method Summary
The researchers sampled 300 leaf categories from a 4-level Japanese e-commerce taxonomy and used GPT-4o-mini (temperature 0.6) to generate approximately 10 usage scenarios per category. Prompts combined the leaf category name with its full hierarchical path. Fifteen annotators (3 per scenario) evaluated plausibility on a binary scale. The top-5 filtering approach was found to reduce implausible scenarios by over 68% while maintaining coverage.

## Key Results
- 82.9% of 2,925 scenarios received unanimous plausible ratings from all three annotators
- Only 3.0% were deemed implausible by two or more annotators
- Plausibility rates varied significantly among individual annotators (2.38% to 21.5% implausibility)
- Filtering to top 5 scenarios per category reduced implausible cases by 68%

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate plausible item usage scenarios from category names alone, providing context-rich intermediate representations for complementary recommendation. The LLM draws on encoded world knowledge about how items are functionally used in real-world situations. When prompted with a product category (e.g., "Box Tissues"), it retrieves and compositions common usage contexts (e.g., "wiping hands during meals") that implicitly suggest what complementary items might be needed.

### Mechanism 2
Hierarchical category paths reduce semantic ambiguity by providing taxonomic context that disambiguates leaf category names. The prompt includes both the leaf category name and its full hierarchical path (e.g., "Household Goods / Kitchenware > ... > Box Tissues"). This constrains the LLM's interpretation space by anchoring the item within a product taxonomy.

### Mechanism 3
Scenario quality degrades with generation position; early outputs are more reliable. LLMs exhibit positional bias where earlier tokens receive more attention and planning resources. When asked to generate "as many scenarios as possible," later scenarios become increasingly abstract or tangential.

## Foundational Learning

- **Complementary vs. substitute recommendations**: Understanding that complementary items are used *together* (camera + SD card), not as alternatives, is essential for evaluating whether generated scenarios make sense. Quick check: Given "headphones," would "earbuds" be a complement or substitute?

- **Cold-start problem in recommendation systems**: The paper positions scenario generation as an alternative to history-based methods that fail on new items without purchase logs. Quick check: Why can't collaborative filtering recommend items with zero purchase history?

- **Inter-annotator agreement and subjectivity**: The high variance in annotator judgments (2.38% to 21.5% implausibility rates) signals that complementary relationships are inherently subjective. Quick check: If three annotators disagree on a scenario's plausibility, is the scenario "wrong" or just personally contextual?

## Architecture Onboarding

Product Category DB → Prompt Constructor (hierarchy + instructions) → LLM (GPT-4o-mini) → Scenario Generator → Position Filter (top 5) → Human/Auto Evaluator → Scenario Store

Critical path:
1. Define category vocabulary from your e-commerce taxonomy
2. Construct prompts with full hierarchical paths
3. Generate scenarios with temperature ~0.6
4. Filter to first N scenarios (suggested: 5)
5. Validate on sample before production deployment

Design tradeoffs:
- **Temperature (0.6)**: Higher = more creative but less reliable; lower = more deterministic but repetitive
- **Number of scenarios generated**: More scenarios increase coverage but also implausible cases
- **Category granularity**: Leaf categories are more specific but may have sparse training knowledge; parent categories are broader but may generate generic scenarios

Failure signatures:
- **Item misinterpretation**: Scenarios describe use of a different item embedded in the category name (e.g., holder → held item)
- **Non-usage scenarios**: Describes purchasing/maintenance rather than functional use
- **Overly abstract scenarios**: Item plays only auxiliary role in the scenario
- **Unrealistic contexts**: Rare or improbable real-world situations

First 3 experiments:
1. Replicate on your own category taxonomy with 50-100 categories and 3+ human annotators to establish baseline plausibility for your domain
2. A/B test filtering thresholds (top 3 vs. top 5 vs. top 10) to find the point where implausibility rate exceeds acceptable bounds
3. Test category name disambiguation strategies (e.g., adding explicit "this is a holder for X, not X itself" instructions) for known problematic patterns

## Open Questions the Paper Calls Out

**Open Question 1**
Can complementary item pairs be reliably extracted from LLM-generated scenarios and effectively integrated into live recommendation systems? The current study only validated the plausibility of the scenarios themselves; it did not test the technical feasibility of parsing these scenarios to identify specific products or measure the resulting recommendation accuracy.

**Open Question 2**
Does the high plausibility of generated scenarios persist in specialized or technical domains outside of general household goods? The authors "plan to expand our evaluation to a broader range of categories, including specialized domains," acknowledging the current study was limited to familiar household items.

**Open Question 3**
To what extent do LLM-generated usage scenarios improve user trust and perceived explainability compared to traditional recommendation methods? While the scenarios are logically sound, it remains untested whether users actually prefer these context-based explanations or if they effectively influence purchasing decisions.

## Limitations
- Item misinterpretation remains the dominant failure mode, especially when leaf category names contain substrings of more common items
- Positional quality decay is systematic but unexplained, with later scenarios becoming increasingly abstract or tangential
- Annotator subjectivity reveals fundamental ambiguity in complementary relationships, with high variance in individual judgments

## Confidence
- **High confidence**: Core finding that LLM-generated scenarios achieve 82.9% unanimous plausibility ratings
- **Medium confidence**: Proposed mechanisms explaining success, though relying on general LLM behavior patterns
- **Low confidence**: Generalizability to domains beyond household/kitchen items from single Japanese e-commerce platform

## Next Checks
1. **Cross-domain replication test:** Apply the same methodology to 50-100 categories from a different product domain (e.g., electronics or clothing) to assess whether the 82.9% plausibility rate holds across domains with different semantic structures.

2. **Category disambiguation experiment:** Systematically test whether adding explicit disambiguation instructions to the prompt (e.g., "This is a holder, not the item it holds") reduces item misinterpretation rates for known problematic patterns.

3. **Long-tail category analysis:** Evaluate scenario plausibility rates for categories with varying popularity or specificity to determine whether the approach scales to niche items where training data may be sparse.