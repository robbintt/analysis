---
ver: rpa2
title: Learning from Self Critique and Refinement for Faithful LLM Summarization
arxiv_id: '2512.05387'
source_url: https://arxiv.org/abs/2512.05387
tags:
- scrpo
- summary
- preference
- critique
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in LLM-generated
  summaries by introducing a self-supervised training framework called SCRPO (Self
  Critique and Refinement-based Preference Optimization). The core method idea is
  to construct a preference dataset by having the same LLM critique and refine its
  own summaries, then train the LLM on this dataset using preference learning.
---

# Learning from Self Critique and Refinement for Faithful LLM Summarization

## Quick Facts
- **arXiv ID**: 2512.05387
- **Source URL**: https://arxiv.org/abs/2512.05387
- **Reference count**: 20
- **Primary result**: SCRPO achieves MiniCheck score of 0.806 on CNNDM vs 0.746 for inference-time refinement

## Executive Summary
This paper addresses the critical problem of hallucinations in LLM-generated summaries by introducing SCRPO (Self Critique and Refinement-based Preference Optimization). The method constructs a self-supervised preference dataset by having a model critique and refine its own summaries, then trains on this data using preference learning. Experiments demonstrate SCRPO significantly outperforms both state-of-the-art self-supervised methods and inference-time refinement in terms of faithfulness metrics while maintaining or improving overall summary quality.

## Method Summary
SCRPO operates by first generating initial summaries from unlabeled documents, then using the same LLM to critique these summaries by extracting atomic facts and checking entailment with the source document. Summaries failing the faithfulness check are refined using critique feedback. The method then creates preference triplets by selecting initial summaries with high hallucination scores as rejected candidates and refined versions from low-scoring initial summaries as chosen candidates. These triplets are used to fine-tune the base model using DPO with NLL regularization and LoRA adapters. The approach requires no human-labeled preference data and demonstrates particular effectiveness for 3B-7B parameter models.

## Key Results
- SCRPO achieves 0.806 MiniCheck score on CNNDM compared to 0.746 for inference-time refinement
- Outperforms state-of-the-art self-supervised learning methods across all three tested benchmarks
- Maintains or improves general summary quality (GEval scores) while significantly enhancing faithfulness
- Demonstrates 3B-7B models can self-improve, while <3B models fail to benefit from the approach

## Why This Works (Mechanism)
The mechanism works by leveraging the LLM's internal knowledge to identify and correct its own hallucinations. By extracting atomic facts from generated summaries and checking their entailment with source documents, the model creates an internal preference signal that captures faithfulness. The refinement step then uses this critique to produce improved summaries, creating a natural preference pairing. This self-supervised approach avoids the need for expensive human annotations while producing high-quality preference data that directly addresses the hallucination problem.

## Foundational Learning
- **Preference Learning**: Understanding how models learn from pairwise comparisons rather than direct supervision - needed to grasp DPO mechanics, check by implementing basic pairwise ranking
- **Factual Consistency Verification**: Methods for checking if generated content is supported by source text - needed for critique pipeline, check by testing NLI models on known hallucination examples
- **LoRA Fine-tuning**: Parameter-efficient adaptation techniques - needed for practical implementation, check by successfully applying LoRA to a base model
- **Fact Extraction from Text**: Techniques for identifying atomic propositions in generated text - needed for critique mechanism, check by implementing Table 6 extraction prompts
- **Hallucination Detection Metrics**: Understanding tools like MiniCheck and GPT4-Likert - needed for evaluation, check by computing scores on benchmark examples

## Architecture Onboarding

**Component Map**: Document -> Initial Summary Generation -> Fact Extraction -> NLI Entailment Check -> Critique Score -> Refinement (if needed) -> Preference Triplet Formation -> DPO+NLL Training

**Critical Path**: The pipeline from initial summary generation through critique and refinement to preference triplet formation is the critical path, as errors at any step propagate through the entire training process. The NLI model's reliability in detecting subtle hallucinations is particularly crucial.

**Design Tradeoffs**: The method trades off compute cost during training (generating multiple summaries per document) for improved faithfulness without human annotation costs. The selection strategy balances faithfulness maximization against general quality preservation, with "Extreme selection" chosen to maintain utility.

**Failure Signatures**: Poor faithfulness scores indicate either critique pipeline failures (NLI not detecting hallucinations) or refinement step inadequacies. Quality degradation suggests the preference selection strategy is too aggressive in prioritizing faithfulness over fluency.

**First Experiments**:
1. Implement and validate the critique pipeline (fact extraction + NLI) on a small human-annotated subset
2. Test the full preference data generation pipeline on 10 documents to verify triplet formation
3. Run a minimal DPO fine-tuning with synthetic preference data to confirm training mechanics

## Open Questions the Paper Calls Out

**Open Question 1**: What are the specific failure modes of SCRPO in smaller language models (e.g., <3B parameters) that cause performance degradation? The paper establishes that 0.5B and 1.5B models experience significant faithfulness drops but does not isolate whether the failure stems from insufficient critique capability, poor refinement, or the preference learning process itself.

**Open Question 2**: Can the SCRPO framework generalize effectively to other long-form text generation tasks prone to hallucination? While the method shows promise for summarization, its self-critique mechanism is tailored to summary-to-document alignment, and it remains unclear if the "internal knowledge" of faithfulness transfers to tasks like dialogue generation or open-ended question answering.

**Open Question 3**: How can the preference triplet selection strategy be optimized to bridge the gap between faithfulness and general quality? Table 2 results suggest a trade-off where the best faithfulness signal may harm the model's general utility, indicating current selection methods may not extract the optimal training signal.

## Limitations
- Critical hyperparameters (sample size N, DPO scaling β, NLL weight α) are not specified
- The critique pipeline reliability depends heavily on NLI model performance and specific prompts
- Claims about smaller model failures are asserted but not empirically validated with systematic experiments
- No ablation studies on the impact of different components on final performance

## Confidence

**High confidence**: The core methodology and general training pipeline are clearly described and reproducible
**Medium confidence**: Experimental results showing SCRPO's superiority, though exact replication is challenging without hyperparameter details
**Low confidence**: Claims about SCRPO's failure on smaller models due to "insufficient critique capability" lack empirical validation

## Next Checks

1. **Critique Pipeline Validation**: Test the atomic fact extraction and NLI-based hallucination detection on a small human-annotated subset to verify the critique mechanism's reliability before scaling to full data generation

2. **Hyperparameter Sensitivity Analysis**: Once baseline reproduction is achieved, systematically vary N (initial sample size), β (DPO scaling), and α (NLL weight) to understand their impact on faithfulness vs quality trade-offs

3. **Cross-Dataset Generalization Test**: Apply the trained SCRPO model to an unseen summarization dataset (e.g., BillSum or Newsroom) to assess whether the self-supervised critique mechanism generalizes beyond the training domains