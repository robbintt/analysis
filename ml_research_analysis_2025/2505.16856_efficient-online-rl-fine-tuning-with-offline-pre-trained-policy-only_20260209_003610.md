---
ver: rpa2
title: Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only
arxiv_id: '2505.16856'
source_url: https://arxiv.org/abs/2505.16856
tags:
- uni00000013
- uni00000048
- uni0000004e
- uni00000003
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PORL, an online RL fine-tuning method that
  operates solely with a pre-trained policy, eliminating the need for pre-trained
  Q-functions or offline data. The method addresses the limitations of offline-to-online
  RL approaches, which rely on pre-trained Q-functions that suffer from pessimistic
  underestimation of out-of-distribution state-action pairs.
---

# Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only

## Quick Facts
- **arXiv ID:** 2505.16856
- **Source URL:** https://arxiv.org/abs/2505.16856
- **Reference count:** 30
- **One-line primary result:** Proposes PORL, an online RL fine-tuning method that uses only a pre-trained policy, achieving competitive performance with state-of-the-art offline-to-online RL methods and outperforming methods initialized with random Q-functions.

## Executive Summary
This paper addresses the challenge of online reinforcement learning (RL) fine-tuning when only a pre-trained policy is available, without access to pre-trained Q-functions or offline datasets. The proposed method, PORL, introduces an epsilon-greedy pre-sampling stage to rapidly initialize a uniform Q-function, avoiding the pessimistic underestimation common in offline pre-trained Q-functions. Experimental results on AntMaze, Adroit, Kitchen, and RLBench tasks demonstrate that PORL achieves competitive performance with state-of-the-art offline-to-online RL methods and enables direct fine-tuning of imitation learning policies, a capability not available in prior methods.

## Method Summary
PORL operates in two stages: a pre-sample stage and a fine-tune stage. In the pre-sample stage, the pre-trained policy is frozen and interacts with the environment using epsilon-greedy exploration to collect data, which is used to rapidly train a randomly initialized Q-function ensemble (10 networks) with a high Update-to-Data (UTD) ratio of 16. This initializes the Q-function with a more uniform value distribution. In the fine-tune stage, the policy is unfrozen and standard SAC updates are performed, using the initialized Q-function to guide exploration and learning. The method eliminates the need for offline pre-trained Q-functions or offline datasets, relying solely on the pre-trained policy checkpoint.

## Key Results
- PORL achieves competitive performance with state-of-the-art offline-to-online RL methods on AntMaze, Adroit, Kitchen, and RLBench tasks.
- PORL outperforms methods initialized with random Q-functions, demonstrating the effectiveness of the pre-sampling stage.
- PORL enables fine-tuning of imitation learning policies directly, a capability not available in prior offline-to-online RL methods.
- Ablation studies confirm the importance of the pre-sample stage and epsilon-greedy exploration for successful fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1: Elimination of Pessimistic Value Underestimation
Offline RL methods enforce conservatism (pessimism) to handle distributional shift, causing the Q-function to underestimate the value of out-of-distribution (OOD) state-action pairs. If this Q-function is kept for online fine-tuning, it discourages the policy from exploring novel, high-reward regions. By initializing a "uniform" Q-function via online pre-sampling, the policy receives unbiased gradients that encourage expansion into unseen regions. Evidence includes visualizations showing Q-offline persisting in pessimistic estimation for OOD regions even after 500k steps, whereas Q-online maintains a more uniform Q-value distribution.

### Mechanism 2: Actor-Critic Alignment via Frozen Pre-Sampling
Freezing the pre-trained policy while rapidly training the Q-function from scratch prevents the "forgetting" of the policy's capabilities typically caused by random critic initialization. The pre-sample stage isolates Q-function learning, allowing the critic to bootstrap specifically on data generated by the pre-trained policy. This aligns the critic's value estimates with the actor's actual performance distribution before policy gradient updates begin. The paper notes that randomized critics result in degradation of the actor's performance.

### Mechanism 3: Distribution Expansion via Epsilon-Greedy
Injecting randomness (epsilon-greedy) during the pre-sampling phase creates a broader state-action distribution, which helps train a Q-function that generalizes better to the online exploration phase. A deterministic pre-trained policy produces a narrow data manifold. A Q-function trained only on this manifold will have high uncertainty/error outside it. By using epsilon-greedy, the agent forces the collection of "transitioning" or "noisy" data points, expanding the support of the replay buffer and preventing the newly initialized Q-function from overfitting to a thin slice of the state space.

## Foundational Learning

- **Concept: Conservative Q-Learning (CQL) & Pessimism**
  - **Why needed here:** Understanding why standard offline RL (like CQL) fails online is crucial. Offline methods intentionally penalize uncertainty (OOD actions) to prevent "distributional shift."
  - **Quick check question:** Why would a Q-function that is "safe" for offline training be "bad" for online fine-tuning?

- **Concept: Actor-Critic (SAC) Interdependence**
  - **Why needed here:** The core problem is the mismatch between a good Actor (policy) and a bad Critic (Q-function). Understanding how the Critic directs the Actor via gradients explains why the pre-sample stage is necessary to prevent "forgetting."
  - **Quick check question:** If a Critic underestimates the value of a specific action, how does the Actor's policy change in response?

- **Concept: Update-to-Data (UTD) Ratio**
  - **Why needed here:** The paper leverages a "High UTD" (updating the network multiple times per environment step) to rapidly bootstrap the Q-function during the pre-sample stage.
  - **Quick check question:** Why is a high UTD ratio useful when you have very little data (like in the pre-sample stage), but potentially risky when data is abundant?

## Architecture Onboarding

- **Component map:** Pre-trained Policy $\pi_{pre}$ -> Epsilon-Greedy Sampler -> Replay Buffer $D$ -> Randomly initialized Q-ensemble (10 networks) -> High UTD TD-Learning -> Q-Function -> Policy Gradient Updates

- **Critical path:**
  1. **Pre-Sample Loop:** Freeze $\pi_{pre}$. Interact with Env via $\epsilon$-greedy $\to$ Store in $D$ $\to$ Update Q-Networks heavily (UTD=16). **Stop when $T$ steps reached.**
  2. **Fine-Tune Loop:** Unfreeze $\pi$. Interact with Env $\to$ Store in $D$ $\to$ Update Q-Networks (UTD=16) $\to$ Update $\pi$ via Policy Gradient.

- **Design tradeoffs:**
  - **Pre-sample Steps ($T$):** Too few steps $\to$ Q-function is still effectively random $\to$ Policy degrades (forgetting). Too many steps $\to$ Wasted samples if policy is already good.
  - **Epsilon ($\epsilon$):** High $\epsilon$ $\to$ Better Q-generalization (wider data) but lower average reward during pre-sampling. Low $\epsilon$ $\to$ Risk of Q-overfitting to narrow policy distribution.

- **Failure signatures:**
  - **Initial Performance Drop:** Indicates the Pre-sample stage was insufficient (Q is not aligned with Actor) or UTD is too low.
  - **Stagnation:** Q-function might be overfitted to the pre-sample data; check if $\epsilon$ was set too low (lack of diversity).

- **First 3 experiments:**
  1. **Sanity Check (Q-Init):** Compare PORL against (a) Random Q-init and (b) Offline-pretrained Q-init on a standard D4RL task (e.g., AntMaze) to verify the "Uniform vs. Pessimistic" hypothesis.
  2. **Ablation (Pre-sample Size):** Run a sweep of pre-sample steps (e.g., 0, 1k, 5k, 20k) to find the minimum viable data threshold for stable Q-initialization.
  3. **Capability Test (IL Fine-tuning):** Take a Behavior Cloning (BC) policy (which has *no* Q-function) and apply PORL. This validates the method's unique selling point of fine-tuning without offline RL components.

## Open Questions the Paper Calls Out

- **Question:** Can PORL be effectively adapted to fine-tune large-scale policy architectures, such as diffusion-based or transformer-based models, which present different optimization challenges than standard MLPs?
- **Question:** How does PORL perform in real-world robotic fine-tuning scenarios where the pre-trained policy originates from a simulator and online interaction costs are high?
- **Question:** How can the dynamics and stability between the actor and critic during the fine-tuning phase be quantitatively evaluated beyond simple performance metrics?
- **Question:** Is there an adaptive mechanism to determine the optimal number of pre-sample steps based on task complexity and policy capability?

## Limitations
- The paper's claim that eliminating offline Q-functions and using uniform Q-initialization is superior relies heavily on the "pessimism hypothesis," but the evidence is indirect and does not isolate whether the performance gains come from uniform initialization specifically or simply from having a better-aligned Q-function.
- The pre-trained policy quality is not quantified, leaving open the possibility that PORL works well because the starting policy is already competent rather than due to the method's design.
- The broad applicability claim, especially the ability to fine-tune IL policies without any offline RL components, is only demonstrated on one IL baseline (BC), and the performance gains over other fine-tuning methods are not quantified.

## Confidence

- **High Confidence:** The experimental results showing PORL outperforms random Q-initialization and achieves competitive performance with state-of-the-art offline-to-online methods. The ablation studies on pre-sample steps and epsilon-greedy are well-controlled and directly support the mechanism claims.
- **Medium Confidence:** The mechanism claim that offline Q-functions are "pessimistic" and that this pessimism is the primary bottleneck. The visualizations of Q-value distributions are compelling, but the paper does not provide quantitative measures of pessimism.
- **Low Confidence:** The broad applicability claim, especially the ability to fine-tune IL policies without any offline RL components. While the ablation supports this, the paper only demonstrates this on one IL baseline, and the performance gains over other fine-tuning methods are not quantified.

## Next Checks

1. **Pessimism Isolation Test:** Implement a variant of PORL that uses an offline pre-trained Q-function but applies a "pessimism removal" step (e.g., clamping negative values to zero) during the pre-sample stage. Compare its performance against standard PORL to directly test if pessimism removal is the key mechanism.
2. **Policy Quality Sweep:** Evaluate PORL across a range of pre-trained policy qualities (e.g., policies trained for 10%, 50%, and 100% of the standard training steps). This would determine if PORL's benefits scale with policy competence or are independent of it.
3. **Task Complexity Scaling:** Apply PORL to a more complex task (e.g., D4RL `walker2d-medium-replay-v2`) with longer episodes and sparser rewards. This would test if the pre-sample stage remains effective when the state space and exploration challenges are significantly larger.