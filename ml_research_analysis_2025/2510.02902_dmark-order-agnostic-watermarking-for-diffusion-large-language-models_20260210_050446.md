---
ver: rpa2
title: 'DMark: Order-Agnostic Watermarking for Diffusion Large Language Models'
arxiv_id: '2510.02902'
source_url: https://arxiv.org/abs/2510.02902
tags:
- watermarking
- dllms
- text
- generation
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DMark, the first watermarking framework designed
  for diffusion-based large language models (dLLMs). The key innovation addresses
  the fundamental incompatibility between traditional watermarking methods and dLLMs'
  non-sequential generation process.
---

# DMark: Order-Agnostic Watermarking for Diffusion Large Language Models

## Quick Facts
- arXiv ID: 2510.02902
- Source URL: https://arxiv.org/abs/2510.02902
- Reference count: 40
- Primary result: First watermarking framework for dLLMs achieving 92.0-99.5% detection rates

## Executive Summary
DMark introduces the first watermarking framework specifically designed for diffusion-based large language models (dLLMs), addressing the fundamental incompatibility between traditional watermarking methods and dLLMs' non-sequential generation process. The key innovation lies in exploiting dLLMs' unique properties: parallel logit computation for all positions, bidirectional attention, and iterative refinement. DMark introduces three complementary strategies—predictive watermarking, bidirectional watermarking, and their combination—that inject watermark signals even when tokens are generated out of order or when context is unavailable. Experiments demonstrate significant improvements over naive adaptations of existing methods, achieving 92.0-99.5% detection rates at 1% false positive rate across multiple dLLMs while maintaining text quality.

## Method Summary
DMark extends the KGW watermarking framework to work with dLLMs by addressing the challenge of non-sequential token generation. The framework uses a pre-computed bit matrix M ∈ {0,1}^{|V|×|V|} that encodes green list relationships for O(1) lookup during generation. During decoding, DMark applies predictive watermarking when neighbors are unavailable (predicting tokens from parallel logits), bidirectional watermarking that exploits both forward and backward dependencies, and combines these approaches for maximum detection strength. The method maintains the core KGW principle of partitioning vocabulary into green and red lists based on hash functions, but adapts the green list construction to handle dLLMs' arbitrary generation order through context prediction and bidirectional constraints.

## Key Results
- Achieves 92.0-99.5% true positive rate at 1% false positive rate across multiple dLLMs
- Outperforms naive KGW adaptation (49.6-71.2% TPR) by exploiting dLLM-specific properties
- Maintains text quality with perplexity around 4-6 using δ=2.0 and γ=0.5
- Demonstrates robustness against common text manipulations including deletion, insertion, and shuffling

## Why This Works (Mechanism)

### Mechanism 1: Predictive Watermarking via Parallel Logit Exploitation
DMark exploits dLLMs' parallel logit computation by predicting missing context tokens from available logits. When a neighbor token is unavailable during generation, the framework predicts it using argmax of the logits at that position, then uses this predicted token to construct the green list for watermarking. This enables watermark injection at positions lacking actual neighbors, with predicted tokens enabling statistically detectable watermark signals to accumulate across the sequence.

### Mechanism 2: Bidirectional Constraint Exploitation
DMark leverages dLLMs' bidirectional attention by using both forward and backward green lists. Forward green lists use preceding tokens to determine which tokens are green at current positions, while backward green lists identify tokens that would make subsequent tokens green-listed. When either neighbor exists, the corresponding constraint applies; when both exist, both constraints compound, effectively doubling the constraint opportunities compared to unidirectional watermarking.

### Mechanism 3: Predictive-Bidirectional Synergy
The combined approach always constructs both forward and backward green lists using either actual or predicted neighbors, applying combined bias regardless of generation order. This guarantees watermark injection at every position while maximizing signal strength. The prediction errors in forward and backward directions are partially independent, allowing signal accumulation despite individual inaccuracies.

## Foundational Learning

- **Concept: KGW Watermarking (Green/Red List Partitioning)**
  - Why needed here: DMark extends KGW's partitioning scheme to non-sequential generation
  - Quick check question: Given a secret key s and preceding token x_{i-1}, how does KGW partition vocabulary V into green and red lists?

- **Concept: Diffusion Language Model Decoding**
  - Why needed here: DMark exploits dLLMs' unique properties (parallel logits, arbitrary order, iterative refinement)
  - Quick check question: Why can dLLMs compute logits for all positions simultaneously when generating, unlike AR models?

- **Concept: Statistical Hypothesis Testing for Detection**
  - Why needed here: Watermark detection uses z-scores based on green token overrepresentation
  - Quick check question: If n=200 tokens, γ=0.5, and 120 tokens are green, what does a high z-score indicate about the watermark hypothesis?

## Architecture Onboarding

- **Component map:** Precomputation -> Bit matrix M -> Context checker -> Green list constructor -> Logit biaser -> Softmax sampler -> Detection
- **Critical path:** Pre-compute M once per (seed, γ) combination; at each denoising step and position i: check neighbors → query M for green lists → add bias to logits; detection: count green tokens → compute z-score → threshold comparison
- **Design tradeoffs:** δ (bias strength) vs PPL; γ (green ratio) vs robustness; text length requirements (L<50 requires δ≥5.0, L>150 works with δ=2.0)
- **Failure signatures:** Low TPR with high FPR (δ too low); high PPL >10 (δ too high); paraphrasing attack success (expected limitation)
- **First 3 experiments:** 1) Baseline validation with LLaDA, measure context availability (~67%), confirm weak detection (49-71% TPR); 2) Ablation comparing Predictive-only vs Bidirectional-only vs Predictive-Bidirectional, verify synergistic gains (74-88% → 92-99.5% TPR); 3) Parameter sweep testing γ∈{0.25, 0.5, 0.75} × δ∈{1.0, 2.0, 5.0, 10.0}, plot z-score vs PPL to identify practical operating region

## Open Questions the Paper Calls Out

- **Open Question 1:** Can DMark's robustness against paraphrasing be improved without sacrificing text quality, given that GPT-5-nano paraphrasing reduces detection to only 51.2% TPR?
- **Open Question 2:** How does DMark perform across different remasking strategies beyond the low-confidence approach evaluated?
- **Open Question 3:** What is the computational overhead of DMark's predictive-bidirectional approach during generation?
- **Open Question 4:** Can adversarial attacks specifically designed for diffusion-based watermarking circumvent DMark?

## Limitations

- Paraphrasing attacks remain a critical weakness, with TPR dropping to 51.2-72.1% under aggressive paraphrasing at 1% FPR
- Detection performance varies substantially with text length: short texts (L<50) require δ≥5.0 for reliable detection
- The framework assumes access to dLLMs' parallel logit computations, which may not be available in all implementations

## Confidence

**High Confidence (Empirical validation with strong statistical evidence):**
- Superior detection rates (92.0-99.5% TPR vs 49.6-71.2% for KGW adaptation)
- Quality preservation with recommended parameters (PPL ~4-6)
- Robustness against common text manipulations

**Medium Confidence (Theoretically sound but limited empirical scope):**
- Predictive watermarking's effectiveness despite prediction errors
- Bidirectional constraint exploitation doubling detection opportunities
- Parameter sensitivity claims

**Low Confidence (Extrapolation beyond tested conditions):**
- Performance on multilingual corpora
- Behavior with non-standard dLLM architectures
- Long-term robustness against evolving paraphrasing techniques

## Next Checks

**Validation Check 1: Neighbor Prediction Accuracy Analysis**
Quantify how prediction accuracy degrades across denoising steps and its correlation with watermark detection strength by measuring prediction accuracy for missing neighbors at each denoising step and correlating with per-position z-scores.

**Validation Check 2: Cross-Architectural Generalization Test**
Evaluate DMark's performance on dLLMs with different attention mechanisms and decoding strategies including LLaDA with multi-head attention variations, Dream-v0 with different block sizes, and transformer-based dLLMs with learned unmasking policies.

**Validation Check 3: Adaptive Parameter Optimization**
Develop dynamic parameter selection based on text characteristics and denoising state by implementing a controller that adjusts γ based on text length, δ based on denoising step and prediction confidence, and bias distribution between forward and backward constraints.