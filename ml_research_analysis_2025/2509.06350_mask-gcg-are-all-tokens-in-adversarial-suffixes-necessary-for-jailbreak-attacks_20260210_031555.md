---
ver: rpa2
title: 'Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?'
arxiv_id: '2509.06350'
source_url: https://arxiv.org/abs/2509.06350
tags:
- tokens
- mask-gcg
- attack
- arxiv
- suffixes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether all tokens in adversarial suffixes
  used for jailbreak attacks on LLMs are necessary. The authors propose Mask-GCG,
  a method that applies learnable token masking to identify and prune low-impact tokens
  from these suffixes.
---

# Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?

## Quick Facts
- arXiv ID: 2509.06350
- Source URL: https://arxiv.org/abs/2509.06350
- Reference count: 0
- Key outcome: Mask-GCG achieves up to 10.5% suffix compression without compromising attack success rates

## Executive Summary
This work investigates whether all tokens in adversarial suffixes used for jailbreak attacks on LLMs are necessary. The authors propose Mask-GCG, a method that applies learnable token masking to identify and prune low-impact tokens from these suffixes. By integrating attention-guided initialization and adaptive pruning, Mask-GCG distinguishes high-impact tokens from redundant ones, reducing suffix length while preserving attack effectiveness. Evaluated across three GCG variants and three LLMs, the method achieves up to 10.5% suffix compression without compromising attack success rates. It also reduces average attack time by 16.8% and improves suffix stealthiness with a 24% reduction in perplexity. These findings reveal significant token redundancy in adversarial suffixes and offer insights for developing more efficient and interpretable LLM prompts.

## Method Summary
Mask-GCG introduces a token masking mechanism to optimize adversarial suffixes by pruning redundant tokens while maintaining attack effectiveness. The method employs attention-guided initialization to prioritize tokens based on their importance scores derived from clean prefix attention weights. It then uses adaptive pruning to iteratively remove low-impact tokens, guided by a masking threshold that balances suffix compression with attack success preservation. The framework integrates with three GCG variants and is evaluated across three LLMs, demonstrating improved suffix efficiency, reduced attack time, and enhanced stealthiness through lower perplexity scores.

## Key Results
- Achieves up to 10.5% suffix compression without compromising attack success rates
- Reduces average attack time by 16.8% through shorter suffix lengths
- Improves suffix stealthiness with a 24% reduction in perplexity scores

## Why This Works (Mechanism)
Mask-GCG works by leveraging the observation that adversarial suffixes contain redundant tokens that do not contribute significantly to jailbreak attack success. The method identifies these redundant tokens through attention-guided importance scoring and adaptive pruning, allowing it to maintain attack effectiveness while reducing suffix length. The pruning process exploits the fact that certain tokens in adversarial suffixes have diminishing returns on attack success, making them safe to remove without degrading performance. This token-level optimization enables more efficient prompt engineering by focusing computational resources only on high-impact tokens.

## Foundational Learning

**Adversarial suffixes** - Short token sequences appended to prompts to bypass LLM safety filters
- Why needed: Core attack vector being optimized
- Quick check: Can be generated using standard GCG variants

**Token masking mechanisms** - Learnable binary masks applied to individual tokens in a sequence
- Why needed: Enables selective pruning of redundant tokens
- Quick check: Should reduce suffix length while maintaining attack success

**Attention-guided importance scoring** - Uses attention weights from clean prefixes to estimate token importance
- Why needed: Provides initialization for pruning process
- Quick check: High-attention tokens should correlate with attack success

**Adaptive pruning** - Iterative removal of low-importance tokens based on performance thresholds
- Why needed: Balances compression against attack effectiveness
- Quick check: Should maintain success rate above baseline threshold

## Architecture Onboarding

**Component map**: Clean prefix attention -> Token importance scores -> Learnable masks -> Pruned suffix -> Attack evaluation -> Mask update

**Critical path**: The most critical components are the attention-guided initialization and adaptive pruning loop, as they directly determine which tokens are preserved and ultimately control both suffix length and attack success rate.

**Design tradeoffs**: The method trades some computational overhead during the pruning phase against significant gains in attack efficiency and stealth. The attention-based initialization assumes that clean prefix attention patterns are indicative of adversarial suffix importance, which may not hold for all attack types or model architectures.

**Failure signatures**: If the pruning threshold is too aggressive, attack success rates will drop below baseline levels. If attention-guided initialization is unreliable for a given model, the pruning may remove critical tokens early in the process. The method may also fail to generalize across different LLM families if their attention mechanisms behave differently from those tested.

**3 first experiments**:
1. Apply Mask-GCG to a standard GCG-generated suffix and measure compression ratio while maintaining baseline success rate
2. Compare perplexity scores of original vs. pruned suffixes to quantify stealth improvement
3. Test pruned suffixes across multiple target models to assess cross-model generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on GPT family models with limited testing on Llama-2-70B-Chat and Vicuna-v1.5-13B
- Does not compare against alternative suffix optimization approaches like greedy decoding or beam search pruning
- Attention-guided initialization assumes clean prefix attention weights reliably indicate adversarial token importance

## Confidence
**High confidence**: The core finding that adversarial suffixes contain redundant tokens is well-supported by empirical evidence showing successful attack preservation after pruning. The reduction in perplexity and attack time are directly measurable outcomes with clear experimental validation.

**Medium confidence**: The claim that Mask-GCG improves suffix stealthiness is supported by perplexity reduction metrics, but the security implications of this improvement are not fully explored. The relationship between lower perplexity and actual detection avoidance by safety mechanisms remains unproven.

**Medium confidence**: The assertion that Mask-GCG provides insights for interpretable LLM prompts is reasonable but speculative. While token pruning reveals redundancy, the connection to prompt interpretability requires additional theoretical or empirical support beyond the current scope.

## Next Checks
1. **Cross-model generalization testing**: Evaluate Mask-GCG's pruning effectiveness on a broader range of LLM families including open-source models with different training objectives (code generation, reasoning, multilingual) to assess whether the attention-based initialization remains valid across architectures.

2. **Baseline comparison study**: Conduct controlled experiments comparing Mask-GCG against alternative suffix optimization methods such as length-constrained beam search, gradient-based token importance ranking, and manual suffix editing to establish relative performance and identify complementary approaches.

3. **Safety mechanism robustness evaluation**: Test pruned suffixes against multiple safety detection systems (both black-box API safety filters and white-box interpretability tools) to determine whether the perplexity reduction translates to improved evasion of real-world safety mechanisms, including adaptive defenses that may learn to recognize pruning patterns.