---
ver: rpa2
title: 'A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges,
  and Emerging Trends'
arxiv_id: '2507.09861'
source_url: https://arxiv.org/abs/2507.09861
tags:
- trainable
- document
- frameworks
- visual
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews recent MLLM-based frameworks for visually-rich
  document understanding, focusing on OCR-dependent and OCR-free paradigms. Key findings
  include the evolution of multimodal feature encoding and fusion methods, diverse
  training strategies (pretraining, instruction tuning, fine-tuning), and the critical
  role of datasets in model development.
---

# A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends

## Quick Facts
- arXiv ID: 2507.09861
- Source URL: https://arxiv.org/abs/2507.09861
- Reference count: 39
- Primary result: Comprehensive survey comparing OCR-dependent and OCR-free MLLM frameworks for visually rich document understanding, analyzing performance trends, training strategies, and emerging challenges

## Executive Summary
This survey systematically reviews recent MLLM-based frameworks for visually rich document understanding, focusing on the evolution of OCR-dependent and OCR-free paradigms. The paper analyzes how multimodal feature encoding and fusion methods have advanced, examines diverse training strategies (pretraining, instruction tuning, fine-tuning), and highlights the critical role of datasets in model development. Key findings show OCR-dependent models excelling in structured text extraction tasks while OCR-free approaches rapidly advance on complex visual understanding, though significant challenges remain in synthetic data quality, long document understanding, and domain adaptation.

## Method Summary
The survey methodology involves systematic literature review of 39 papers focusing on MLLM-based visually rich document understanding frameworks. Analysis covers model architectures, training paradigms (pretraining on large-scale documents, instruction tuning on synthetic/benchmark datasets, supervised fine-tuning), and performance across benchmark datasets. The paper compares OCR-dependent frameworks that leverage external OCR tools with OCR-free approaches that process document images directly, examining their respective strengths, limitations, and emerging trends. Performance evaluation spans multiple benchmarks including FUNSD, CORD, DocVQA, ChartVQA, and InfoVQA, with attention to architectural innovations, training strategies, and scalability challenges.

## Key Results
- OCR-dependent models achieve >80% accuracy on structured text extraction tasks (FUNSD, CORD) by leveraging external OCR tools and document encoders
- OCR-free models now match or exceed OCR-dependent performance on visual understanding tasks, with DocVQA accuracy reaching 89.6-92.0% versus 78-85% for OCR-dependent
- Major challenges include synthetic data quality, long document understanding (multi-page), domain adaptation, and computational scalability for high-resolution inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR-dependent frameworks achieve strong performance on structured text extraction by leveraging external tools to bypass expensive visual text recognition.
- Mechanism: Off-the-shelf OCR/parsers extract text and bounding boxes → these structured inputs are encoded by LLMs or multimodal encoders (e.g., LayoutLMv3) → fusion modules align text-layout representations with language space. This transfers the burden of text recognition to proven OCR tools.
- Core assumption: External OCR tools provide sufficiently accurate text and layout extraction; downstream errors from OCR quality are acceptable.
- Evidence anchors:
  - [abstract] "OCR-dependent models rely on external OCR tools and document encoders"
  - [Page 2] "OCR-dependent frameworks leverage off-the-shelf tools to extract textual and layout information... typically fed into multimodal encoders to generate joint representations"
  - [Page 14, Table 4] OCR-dependent models (GPE, PDF-WuKong, DocLayLLM) achieve >80% on FUNSD, CORD, SROIE
  - [corpus] Related work (ROAP) confirms reading-order modeling and attention optimization improve layout transformers in KIE tasks
- Break condition: When OCR quality degrades significantly (handwritten, low-quality scans, degraded documents), cumulative OCR errors propagate and end-to-end approaches become necessary.

### Mechanism 2
- Claim: OCR-free frameworks achieve end-to-end document understanding by training vision encoders on high-resolution inputs with compression modules to manage token explosion.
- Mechanism: High-resolution document images are split into sub-images (shape-adaptive cropping) → vision encoders (ViT, Swin) extract fine-grained visual features → compression modules (Resamplers, Q-Former, H-Reducer) reduce visual token count → LLM generates responses directly.
- Core assumption: Sufficient pretraining data with text recognition, detection, and layout-aware objectives enables the vision encoder to learn implicit OCR capabilities.
- Evidence anchors:
  - [Page 2] "OCR-free approaches... bypass text extraction by directly processing document images... Accurate comprehension... requires high-resolution images, which lead to lengthy visual sequences and necessitate visual compression modules"
  - [Page 4] "High-resolution images support fine-grained information extraction, but efficiently processing the resulting large number of visual features remains challenging"
  - [Page 14, Table 4] OCR-free models (Texthawk2, Marten, PP-DocBee) now match or exceed OCR-dependent on DocVQA (89.6-92.0 vs 78-85)
  - [corpus] Neighbors confirm: Docs2Synth shows synthetic data enables retriever frameworks for scanned documents; ALDEN demonstrates RL for long-document navigation
- Break condition: When computational resources are limited or pretraining data is insufficient for the target domain, the model fails to generalize and OCR-dependent approaches may be more practical.

### Mechanism 3
- Claim: Multimodal fusion through layout-aware encoding provides the critical bridge between visual perception and semantic understanding in document tasks.
- Mechanism: Three modalities (text, visual, layout) are encoded separately → layout is integrated via positional embeddings (2D), prompt-based formatting (bounding boxes as text), or training objectives (text localization, grounding) → fusion occurs through neural attention, CoT reasoning, or task-oriented supervision.
- Core assumption: Layout information carries semantic meaning essential for understanding document structure (reading order, hierarchical relationships, spatial proximity).
- Evidence anchors:
  - [Page 5] "Unlike natural scene images, VRDs feature dense text and complex layout structures"
  - [Page 5-6] "Methods for encoding layout information can be categorized into positional encoding-based, prompt-based, and task-oriented approaches"
  - [Page 6] "Prompt-based Fusion... Luo et al. (2024) utilizes a LayoutCoT approach that divides reasoning into question analysis, region localization, and answer generation"
  - [corpus] ROAP explicitly models reading-order and attention-prior, confirming layout's role in transformer optimization
- Break condition: When documents have highly irregular layouts not represented in training data, or when cross-page reasoning is required (most frameworks are single-page), layout encoding fails to capture necessary semantic relationships.

## Foundational Learning

- Concept: **Vision Transformer (ViT) patch embedding and resolution scaling**
  - Why needed here: OCR-free models depend on ViT-style encoders; understanding how image resolution affects token count and how cropping strategies preserve semantic continuity is essential.
  - Quick check question: Given a 1024×1024 document image with patch size 16, how many visual tokens are generated before compression? (Answer: 4096)

- Concept: **Multimodal alignment via contrastive learning and adaptor modules**
  - Why needed here: Feature alignment between vision encoders and LLMs determines fusion quality; projectors (MLP) vs adaptors (LoRA, Q-Former) have different trainability and efficiency tradeoffs.
  - Quick check question: Why might a frozen vision encoder with trainable LoRA adaptor outperform full fine-tuning on low-resource document domains? (Answer: Preserves pretrained visual knowledge while enabling domain-specific alignment with fewer parameters)

- Concept: **2D positional encoding for document layout**
  - Why needed here: Documents have spatial structure where position carries meaning (form fields, table cells); standard 1D positional embeddings fail to capture XY relationships.
  - Quick check question: How does LayoutLMv3's 2D positional encoding differ from standard ViT positional embeddings? (Answer: Separate X and Y coordinate embeddings combined, enabling spatial relationship modeling)

## Architecture Onboarding

- Component map:
  Document image → Vision encoder (ViT/Swin) → Visual compression/adaptor → LLM backbone → Generated answer
  (OCR-dependent variant: OCR tools → Text/layout encoder → Fusion module → LLM)

- Critical path:
  1. Start with **OCR-free baseline** (mPLUG-DocOwl1.5 or UReader) for end-to-end understanding
  2. Add **visual compression module** if processing high-resolution documents (>448px)
  3. Introduce **layout-aware pretraining tasks** (text recognition, grounding) if OCR-free performance is insufficient
  4. Consider **OCR-dependent hybrid** (DoCo, InstructDr) if target domain has structured forms with reliable OCR
  5. Apply **instruction tuning** on domain-specific QA pairs for task alignment

- Design tradeoffs:
  - **OCR-dependent vs OCR-free**: Accuracy on structured extraction vs end-to-end flexibility; OCR adds pipeline complexity but reduces pretraining burden
  - **High-res vs low-res input**: Fine-grained text recognition vs computational cost; cropping preserves resolution but may break semantic continuity
  - **Frozen vs trainable LLM**: Preserves general knowledge vs domain adaptation; freezing reduces cost but limits multimodal alignment depth
  - **Single-page vs multi-page**: Most frameworks support single-page only; multi-page requires retrieval + cross-page reasoning (underexplored)

- Failure signatures:
  - **Low OCR quality documents** (handwritten, degraded scans): OCR-dependent models produce garbled text; OCR-free models need more pretraining
  - **Long documents (>10 pages)**: Token limits exceeded; retrieval-based approaches lose cross-page dependencies
  - **Out-of-domain layouts**: Pretrained layout representations fail; requires domain-specific instruction tuning or synthetic data
  - **Small text/dense tables**: Low-resolution inputs miss fine details; requires high-res processing with compression

- First 3 experiments:
  1. **Benchmark OCR-free baseline on target domain**: Run mPLUG-DocOwl1.5 or Texthawk2 on 50 representative documents; measure DocVQA-style accuracy and identify failure modes (layout types, text density, resolution requirements)
  2. **Ablate visual compression ratio**: Test different compression factors (4x, 8x, 16x token reduction) on high-res documents; trade off accuracy vs inference speed to find optimal point for deployment constraints
  3. **Compare OCR-dependent vs OCR-free on structured extraction**: Select 100 form-like documents; run both paradigm types (e.g., DocLayLLM vs DocOwl1.5); quantify gap on KIE metrics (F1 on FUNSD/CORD-style fields) to determine if OCR pipeline overhead is justified

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-page VRDU frameworks be improved to capture semantic and logical dependencies across document entities that current retrieval-based methods miss?
- Basis in paper: [explicit] The authors state that existing multi-page approaches "often fall short in capturing semantic and logical dependencies across document entities" and that "multi-hop and multimodal reasoning... remain underexplored."
- Why unresolved: Current methods rely on retrievers to identify relevant pages before processing, which fails to maintain continuous contextual understanding across long sequences.
- What evidence would resolve it: A model architecture or mechanism that demonstrates superior performance on cross-page reasoning benchmarks (e.g., MPDocVQA) without relying on disjointed page retrieval steps.

### Open Question 2
- Question: What verification mechanisms, such as LLM-based evaluators or human-in-the-loop strategies, are most effective for ensuring the authenticity and reliability of synthetic instruction-tuning data?
- Basis in paper: [explicit] The paper notes that synthetic datasets generated by OCR tools and LLMs often lack validation, resulting in "low-quality or inaccurate pairs," and explicitly calls for research into "human-in-the-loop and reinforcement learning approaches."
- Why unresolved: Synthetic data generation is currently a "black box" process that often fails to capture real user input distributions or task relevance, particularly in low-resource domains.
- What evidence would resolve it: A standardized evaluation framework showing that models trained on verified synthetic datasets outperform those trained on unverified data, specifically in zero-shot domain adaptation tasks.

### Open Question 3
- Question: How can MLLM-based frameworks bridge the performance gap with fine-tuned BERT-style models on domain-specific tasks without incurring prohibitive computational costs?
- Basis in paper: [explicit] The survey highlights that despite scaling laws, "a significant performance gap remains compared to fine-tuned BERT-style VRDU models on domain-specific tasks" due to a lack of fine-grained semantics.
- Why unresolved: Large models rely on heterogeneous pre-training corpora that lack the specific layout cues and semantics required for niche domains, and fine-tuning them is resource-intensive.
- What evidence would resolve it: An adaptation technique (e.g., parameter-efficient tuning) that allows a general MLLM to match or exceed the accuracy of a specialized LayoutLM model on benchmarks like FUNSD or CORD with limited training data.

### Open Question 4
- Question: What architectural innovations in multi-agent systems are required to effectively process diverse document formats (e.g., charts, tables) and cross-domain scenarios?
- Basis in paper: [explicit] The authors argue that future research should "investigate a broader range of agent types and architectural innovations to address diverse formats... and fine-grained elements like charts and tables."
- Why unresolved: Current agent-based approaches are limited and generally integrate basic tools like PDF parsers, struggling with complex visual elements and cross-domain generalizability.
- What evidence would resolve it: A multi-agent framework where specialized agents (e.g., a "chart agent," "table agent") successfully collaborate to parse complex documents, showing higher accuracy than monolithic models.

## Limitations

- Quantitative benchmarking is challenging due to lack of direct head-to-head comparisons across all frameworks, with performance claims aggregated from individual papers using different evaluation protocols
- Training details are often incomplete, particularly regarding synthetic data generation pipelines, specific hyperparameters, and visual compression ratios
- Most frameworks focus on single-page documents, with multi-page reasoning and cross-page dependencies remaining underexplored
- Survey focuses on English-language benchmarks, potentially limiting generalizability to other languages or script systems

## Confidence

- **High confidence**: The distinction between OCR-dependent and OCR-free paradigms is well-established with clear architectural differences documented across multiple frameworks. Performance trends showing OCR-dependent models excelling at structured extraction (FUNSD, CORD >80% F1) while OCR-free models advance rapidly on visual tasks (DocVQA >90% accuracy) are consistently reported.
- **Medium confidence**: Claims about synthetic data quality and its impact on model performance are supported by qualitative observations but lack comprehensive quantitative analysis across different synthetic generation methods. The effectiveness of visual compression modules shows clear trends but specific compression ratios vary significantly between frameworks.
- **Low confidence**: Domain adaptation effectiveness and scalability challenges are discussed theoretically but lack systematic empirical validation across diverse real-world document types and production-scale deployments.

## Next Checks

1. **Cross-paradigm benchmarking**: Implement head-to-head comparisons of OCR-dependent (DocLayLLM) and OCR-free (mPLUG-DocOwl1.5) models on identical document sets, measuring both accuracy and inference latency to quantify the OCR pipeline overhead tradeoff.

2. **Multi-page reasoning evaluation**: Extend current single-page frameworks to process multi-page documents (10+ pages) from the IIT-CDIP dataset, measuring performance degradation and identifying bottlenecks in cross-page layout understanding and retrieval-augmented generation.

3. **Synthetic data authenticity validation**: Generate synthetic documents with controlled layout complexity and text quality variations, then measure how different levels of synthetic authenticity affect model generalization on real-world benchmarks, isolating the impact of synthetic data quality from model architecture choices.