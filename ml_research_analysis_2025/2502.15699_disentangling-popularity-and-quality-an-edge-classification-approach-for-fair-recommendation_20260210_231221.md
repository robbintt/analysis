---
ver: rpa2
title: 'Disentangling Popularity and Quality: An Edge Classification Approach for
  Fair Recommendation'
arxiv_id: '2502.15699'
source_url: https://arxiv.org/abs/2502.15699
tags:
- items
- graph
- fairness
- popularity
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles popularity bias in recommender systems, where
  frequently interacted items are unfairly favored over high-quality but less popular
  items. The authors propose a graph neural network-based approach that disentangles
  popularity from quality by introducing an edge classification technique to differentiate
  between popularity bias and genuine quality disparities.
---

# Disentangling Popularity and Quality: An Edge Classification Approach for Fair Recommendation

## Quick Facts
- arXiv ID: 2502.15699
- Source URL: https://arxiv.org/abs/2502.15699
- Reference count: 38
- Primary result: Proposed edge classification approach improves fairness metrics (2-74%) while maintaining competitive accuracy on three real-world datasets.

## Executive Summary
This paper addresses popularity bias in recommender systems, where frequently interacted items are unfairly favored over high-quality but less popular items. The authors propose a graph neural network-based approach that disentangles popularity from quality by introducing an edge classification technique to differentiate between popularity bias and genuine quality disparities. They employ cost-sensitive learning to adjust misclassification penalties, ensuring underrepresented yet relevant items are not overlooked. Experiments on three real-world datasets show significant improvements in fairness metrics while maintaining competitive accuracy, with only minor variations compared to state-of-the-art methods.

## Method Summary
The approach modifies LightGCN by introducing a quality-aware edge filtering step that removes edges based on baseline estimation error for low-degree items, replacing the BPR loss function with binary cross-entropy edge classification, and applying cost-sensitive re-weighting to the loss function. The method filters edges where low-degree items have fewer than two-thirds positive errors relative to baseline predictions, then trains using negative sampling with 80% from unpopular items while applying weighted penalties to misclassification of negative samples.

## Key Results
- Improves fairness metrics (PRU, PRI, EO) by 2-74% compared to baseline methods
- Maintains competitive accuracy with only minor NDCG/Recall variations vs. state-of-the-art
- Cost-sensitive weighting (λ=0.1-0.3) effectively balances fairness and accuracy trade-offs
- Edge classification successfully disentangles quality-based popularity from popularity caused by model bias

## Why This Works (Mechanism)

### Mechanism 1: Quality-Aware Edge Filtering
If low-degree items contain a mix of high-quality and low-quality interactions, removing edges based on baseline estimation error may reduce noise without exacerbating popularity bias. The model computes a baseline rating $b_{ui} = \mu + b_u + b_i$ to isolate user-item affinity from global/user/item biases. For low-degree items (degree $< \gamma$), if fewer than two-thirds of interactions result in a positive error (actual rating $>$ baseline), the edge is pruned. This distinguishes unpopular items that are genuinely relevant from those that are low-quality.

### Mechanism 2: Edge Classification (BPR Replacement)
If the Bayesian Personalized Ranking (BPR) loss function inherently correlates higher scores with higher node degrees, replacing it with binary cross-entropy (edge classification) may decouple prediction scores from popularity. Instead of a pairwise ranking loss that pushes observed interactions above unobserved ones (which are mostly unpopular), the model frames recommendation as binary edge classification (exist vs. non-exist). It optimizes Negative Log-Likelihood (NLL), treating the prediction task as distinguishing true edges from sampled negative edges.

### Mechanism 3: Cost-Sensitive Re-weighting
If unpopular items are disproportionately misclassified as irrelevant due to sparse training signals, up-weighting the loss penalty for negative classifications may recover their representation. The loss function $L_{total} = (1 - \lambda)L_{c=1} + (1 + \lambda)L_{c=0}$ applies a higher penalty for misclassifying a "negative" (un-interacted) item as irrelevant. Since unpopular items dominate the "un-interacted" class, this forces the model to be less confident about dismissing them.

## Foundational Learning

- **Light Graph Convolutional Networks (LightGCN)**
  - Why needed here: This paper modifies the standard LightGCN architecture. You must understand how LightGCN aggregates neighborhood information (simple weighted sum aggregator) to see where the authors introduce their edge classification and data filtering modifications.
  - Quick check question: How does removing the non-linear activation function in LightGCN change the propagation of popularity bias compared to standard GCNs?

- **Bayesian Personalized Ranking (BPR) Loss**
  - Why needed here: The core motivation of the paper is the failure mode of BPR. Understanding the BPR objective ($max(\sigma(y_u - y_i))$) is required to understand why the authors claim it favors high-degree items and why they switch to Edge Classification.
  - Quick check question: In BPR, why does sampling a "negative" item $j$ uniformly from the un-interacted set implicitly bias the gradient toward popular items?

- **Baseline Estimators (for Rating Prediction)**
  - Why needed here: The paper repurposes a classic rating prediction technique (Baseline Estimators) for a new task: edge quality filtering. You need to know how $b_u$ and $b_i$ capture user/item bias to understand how the paper isolates "quality" from "popularity."
  - Quick check question: If an item is extremely popular (high $b_i$) but a specific user rates it low, would the error $e_{ui}$ be positive or negative, and how would Algorithm 1 treat that edge?

## Architecture Onboarding

- **Component map:** Input -> Pre-processor (Alg 1) -> Encoder (LightGCN) -> Prediction -> Loss (Cost-Sensitive Binary Cross Entropy)
- **Critical path:** The **Pre-processing stage (Algorithm 1)** is the most brittle component. If the quality estimation logic fails (e.g., in implicit feedback datasets where "ratings" are binary), the graph pruning may remove the wrong edges. Ensure your data has explicit feedback or a reliable proxy for ratings before running Alg 1.
- **Design tradeoffs:**
  - **Accuracy vs. Fairness ($\lambda$):** Increasing $\lambda$ improves fairness (PRU/PRI) but consistently degrades NDCG/Recall after a tipping point (e.g., $\lambda > 0.2$).
  - **Noise vs. Sparsity ($\gamma$):** Increasing $\gamma$ (degree threshold) retains more noisy edges but helps low-degree items stay connected; decreasing it cleans the graph but may orphan items.
- **Failure signatures:**
  - **Accuracy Collapse:** NDCG drops significantly -> $\lambda$ is likely too high; the model is over-penalizing negative predictions.
  - **No Fairness Gain:** PRU/PRI remain high -> Algorithm 1 may have filtered out all relevant long-tail items, or $\lambda$ is too low (default 0.1).
- **First 3 experiments:**
  1. **Sanity Check (Alg 1):** Run the edge filtering on a validation set. Report the % of edges removed and the average degree of removed items to ensure you aren't just deleting the whole tail.
  2. **Lambda Sweep:** Fix $\gamma=20$. Vary $\lambda \in [0.0, 0.4]$. Plot NDCG vs. PRU to find the "knee" in the curve where fairness gains outweigh accuracy loss (Paper suggests $\approx 0.1$).
  3. **Loss Ablation:** Train with BPR (Standard) vs. Cross-Entropy ($\lambda=0$) vs. Cost-Sensitive ($\lambda=0.1$). This isolates the improvement from the *loss function change* vs. the *cost-sensitive weighting*.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several limitations emerge from the methodology:

- **Implicit Feedback Adaptation:** The quality filtering mechanism requires explicit ratings to compute baseline estimation errors, making it inapplicable to implicit feedback datasets without modification.
- **Quality Filter False Negatives:** The static thresholding heuristic may filter out high-quality "niche" items that naturally have low interaction degrees and mixed reviews.
- **Negative Sampling Impact:** The paper uses uniform negative sampling but doesn't explore how different sampling strategies (popularity-biased, hard negatives) affect the edge classification framework's performance.

## Limitations

- **Explicit Rating Requirement:** The edge pruning mechanism relies on baseline estimation using explicit ratings, making it inapplicable to implicit feedback datasets without adaptation.
- **Dataset-Specific Hyperparameters:** The cost-sensitive weighting parameter λ is tuned differently across datasets (0.1 for CDs/Bookcrossing, 0.3 for Electronics) without systematic explanation.
- **Quality Proxy Validity:** The baseline estimation error as a proxy for "quality" in sparse interaction regimes lacks theoretical justification and validation on datasets without explicit ratings.

## Confidence

- **High confidence:** The core claim that BPR loss inherently correlates predictions with item popularity is well-supported by both the paper's analysis and corpus literature identifying BPR as a bias source.
- **Medium confidence:** The effectiveness of edge classification (replacing BPR with binary cross-entropy) is demonstrated through empirical results, but the specific advantage over other fairness interventions (regularization, causal methods) is not fully explored.
- **Low confidence:** The baseline estimation method for quality filtering is novel but lacks validation on datasets without explicit ratings, and the theoretical justification for using estimation error as a quality proxy is limited.

## Next Checks

1. **Implicit Feedback Adaptation:** Implement the method on a binary implicit feedback dataset and document whether the quality filtering step must be modified or removed, noting any accuracy/fairness trade-offs.
2. **Lambda Sensitivity Analysis:** Conduct a systematic λ sweep (0.0, 0.1, 0.2, 0.3, 0.4) on all three datasets and plot fairness vs. accuracy curves to identify dataset-specific tipping points and validate the paper's reported values.
3. **Comparison to Causal Methods:** Re-run the experiments using the "Taming Recommendation Bias with Causal Intervention" approach from the corpus as a baseline to quantify whether the edge classification approach provides unique advantages over causal fairness interventions.