---
ver: rpa2
title: Do Language Models Agree with Human Perceptions of Suspense in Stories?
arxiv_id: '2508.15794'
source_url: https://arxiv.org/abs/2508.15794
tags:
- suspense
- human
- story
- text
- delatorre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether language models can rate story
  suspense similarly to humans by replicating four classic psychology studies on human
  suspense perception. The authors replaced human participants with various large
  language models (LMs), testing their ability to detect suspense levels, track suspense
  trajectories across story segments, and maintain ratings under adversarial text
  manipulations.
---

# Do Language Models Agree with Human Perceptions of Suspense in Stories?

## Quick Facts
- arXiv ID: 2508.15794
- Source URL: https://arxiv.org/abs/2508.15794
- Reference count: 40
- LMs can detect suspense presence but struggle with intensity ratings and tracking suspense trajectories

## Executive Summary
This study investigates whether large language models can rate story suspense similarly to humans by replicating four classic psychology studies on human suspense perception. The authors systematically tested various LMs (GPT-3.5, GPT-4, Claude, Llama) on their ability to detect suspense levels, track suspense trajectories across story segments, and maintain ratings under adversarial text manipulations. While LMs could identify the presence or absence of suspense with reasonable accuracy, they consistently failed to estimate suspense intensity on continuous scales and diverged from human judgments, particularly at critical narrative turning points.

## Method Summary
The researchers conducted a comprehensive evaluation by having multiple language models rate suspense in stories while systematically varying the presentation format (single paragraphs vs. segmented stories), text order (chronological vs. reverse), and introducing adversarial manipulations. They compared LM ratings against established human benchmarks from classic suspense psychology studies, using both binary classification tasks (suspense present/absent) and continuous rating scales. The study tested different prompting strategies and model architectures to understand the extent and nature of LM limitations in suspense perception.

## Key Results
- LMs accurately identified the presence or absence of suspense in stories but struggled with intensity ratings on continuous scales
- LMs failed to track suspense trajectories across story segments, often missing critical turning points that humans consistently identify
- Adversarial text manipulations had minimal impact on LM suspense ratings, suggesting fundamentally different processing from humans

## Why This Works (Mechanism)
LMs process text through statistical pattern matching rather than experiential understanding, leading to different suspense perception mechanisms than humans.

## Foundational Learning
- Narrative structure comprehension - understanding how stories build tension through plot development
  - Why needed: Essential for tracking suspense trajectories across story segments
  - Quick check: Can the model identify narrative turning points that change story direction

- Temporal reasoning - processing cause-and-effect relationships in story sequences
  - Why needed: Critical for understanding how suspense builds and resolves over time
  - Quick check: Can the model maintain consistent ratings when story order is reversed

- Emotional inference - detecting implied emotional states and tension from textual cues
  - Why needed: Suspense often relies on subtle emotional indicators rather than explicit statements
  - Quick check: Can the model identify suspense in stories with minimal explicit emotional language

## Architecture Onboarding
Component map: Text input -> Embedding layer -> Transformer blocks -> Attention mechanism -> Output layer -> Suspense rating

Critical path: Text input flows through embedding and transformer layers, where attention mechanisms identify relevant context for suspense evaluation, culminating in output layer producing the final rating.

Design tradeoffs: LMs prioritize statistical coherence over experiential understanding, trading human-like suspense perception for general text generation capabilities.

Failure signatures: Inconsistent ratings across story segments, inability to detect subtle suspense cues, and resistance to adversarial manipulations that affect human perception.

First experiments:
1. Test binary suspense detection (present/absent) across diverse story types
2. Evaluate segment-by-segment rating consistency in segmented stories
3. Assess impact of text order manipulation on suspense ratings

## Open Questions the Paper Calls Out
The study does not explicitly identify open questions for future research.

## Limitations
- Focus on English-language texts limits generalizability to other languages and cultural contexts
- Tested prompting strategies may not fully capture the complexity of human suspense evaluation
- Limited scope of adversarial attacks may not reveal all model vulnerabilities

## Confidence
- High confidence in observation that LMs struggle with continuous suspense rating scales
- Medium confidence in conclusions about LM inability to track suspense trajectories
- Low confidence in interpretation that adversarial text manipulations have minimal impact

## Next Checks
1. Test additional model architectures and more diverse prompting strategies to determine if alternative approaches yield better human-like suspense ratings
2. Conduct cross-linguistic validation using stories from different cultural contexts to assess whether findings generalize beyond English-language texts
3. Implement more sophisticated adversarial attacks that target specific suspense-related features to better understand model vulnerabilities and processing differences from humans