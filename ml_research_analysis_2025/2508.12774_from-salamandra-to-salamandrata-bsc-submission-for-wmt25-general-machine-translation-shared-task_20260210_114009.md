---
ver: rpa2
title: 'From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine
  Translation Shared Task'
arxiv_id: '2508.12774'
source_url: https://arxiv.org/abs/2508.12774
tags:
- translation
- salamandra
- language
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SALAMANDRATA, a family of large language
  models trained for translation tasks across 38 European languages, available in
  2B and 7B parameter versions. The models undergo a two-stage training pipeline:
  continual pre-training on parallel data pivoting through three bridge languages
  (English, Spanish, Catalan), followed by supervised fine-tuning on high-quality
  translation instructions.'
---

# From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task

## Quick Facts
- arXiv ID: 2508.12774
- Source URL: https://arxiv.org/abs/2508.12774
- Reference count: 36
- Primary result: SALAMANDRATA-7B achieved 87.2 COMET points on WMT24++ test sets using MBR decoding

## Executive Summary
SALAMANDRATA is a family of translation-focused large language models trained on 38 European languages, available in 2B and 7B parameter versions. The models undergo continual pre-training on parallel data pivoting through three bridge languages (English, Spanish, Catalan) followed by supervised fine-tuning on translation instructions. For WMT25, the 7B variant was extended to support additional non-European languages through vocabulary expansion and continued training. The approach demonstrates strong performance on translation tasks while maintaining robustness to character-level noise.

## Method Summary
The SALAMANDRATA models employ a two-stage training pipeline: (1) continual pre-training on parallel data pivoted through English, Spanish, and Catalan bridge languages, and (2) supervised fine-tuning on high-quality translation instructions. For WMT25 adaptation, vocabulary expansion was used to add support for new languages by preserving shared token embeddings and initializing new ones as the average of existing embeddings. The models were trained using causal language modeling objectives with NeMo, followed by supervised fine-tuning with FastChat. Inference employed beam search, Minimum Bayes Risk (MBR), and Tuned Re-ranking (TRR) strategies using COMET and COMET-KIWI metrics.

## Key Results
- SALAMANDRATA-7B achieved 87.2 COMET points on WMT24++ test sets with MBR decoding
- Instruction tuning improved robustness to character-level noise compared to base models
- SALAMANDRATA-V2 showed lowest fertility (1.88) among baseline models, outperforming NLLB (2.00) and MADLAD400 (2.33)

## Why This Works (Mechanism)

### Mechanism 1: Bridge Language Transfer for Multilingual Alignment
Using multiple bridge languages (English, Spanish, Catalan) during continual pre-training creates richer cross-lingual representations and improves zero-shot translation between non-English pairs compared to English-centric approaches. Parallel data pivoting through multiple bridge languages establishes semantic alignment paths across languages, allowing the model to learn transferable representations through multiple linguistic routes rather than a single English bottleneck.

### Mechanism 2: Vocabulary Expansion with Embedding Initialization
Expanding tokenizer vocabulary for new languages and initializing new token embeddings as the average of existing embeddings provides a stable starting point that can be rapidly optimized through continued pre-training. When adding support for new languages, tokens shared between old and new tokenizers retain their original embeddings, preserving learned knowledge while new tokens receive embeddings initialized in a semantically reasonable region.

### Mechanism 3: Quality-Aware Decoding for Translation Selection
Minimum Bayes Risk (MBR) decoding using COMET metrics consistently selects higher-quality translations from candidate pools compared to standard beam search. Instead of selecting the single most probable sequence, MBR generates multiple candidate translations and selects the one that maximizes expected utility relative to a quality metric, directly optimizing for translation quality as measured by the evaluation metric.

## Foundational Learning

- **Concept: Continual Pre-training (CPT)**
  - Why needed here: SALAMANDRATA uses CPT as the primary mechanism to adapt a general-purpose LLM to translation tasks. Understanding CPT is essential to grasp why the model improves and what data format is required.
  - Quick check question: Can you explain why CPT uses a specific prompt template for parallel data ("source_lang: source \n target_lang: target") rather than standard instruction format?

- **Concept: Cross-lingual Transfer**
  - Why needed here: The paper demonstrates transfer from Hindi to Bhojpuri (low-resource). Understanding how related languages enable transfer is crucial for predicting which language pairs will work.
  - Quick check question: Why would removing English→Hindi data cause English→Bhojpuri BLEU to drop from 9.32 to 0.35?

- **Concept: Instruction Tuning for MT**
  - Why needed here: The paper shows instruction tuning improves not just translation quality but also robustness to noise. This is the final alignment step.
  - Quick check question: Why does adding non-MT tasks (chat, code) to instruction tuning degrade BLEU scores?

## Architecture Onboarding

- **Component map:**
  SALAMANDRA Base Model (2B/7B) → Tokenizer Replacement (V2 only - adds 5 new languages) → CPT-V1: Continual Pre-training (424B tokens, 38 languages) → IT-V1: Supervised Fine-tuning (135k instructions) → [Optional] CPT-V2 + IT-V2 for WMT25 languages → Inference: Beam Search / TRR / MBR

- **Critical path:**
  1. Data preparation is the bottleneck - LABSE filtering, Lingua off-target detection, Bifixer deduplication, format conversion
  2. Vocabulary adaptation requires careful embedding handling - Preserve shared tokens, average-initialize new tokens, recover through CPT
  3. Instruction data curation determines final quality - Pre-translation/post-translation tasks help slightly; chat/code tasks harm performance

- **Design tradeoffs:**
  - Multi-bridge vs. English-only: Better zero-shot transfer vs. simpler pipeline
  - Vocabulary expansion vs. character-level fallback: Better tokenization vs. potential forgetting
  - MBR vs. beam search: Higher COMET quality vs. 10-20x slower inference

- **Failure signatures:**
  - Target-side collapse: Off-target translations from many-to-one alignments. Mitigation: random sampling per language pair.
  - Catastrophic forgetting: Performance drops on original languages after adding new ones. Mitigation: subsample existing direction data.
  - Low-resource failure: Direct training fails. Mitigation: include related higher-resource languages (e.g., Hindi for Bhojpuri).

- **First 3 experiments:**
  1. Reproduce CPT-V1 → IT-V1 pipeline on a single language pair (English-Spanish). Verify COMET improvement after each stage.
  2. Ablate bridge languages - train with only English vs. all three on a subset. Measure zero-shot performance on non-English pairs.
  3. Compare decoding strategies (beam search, TRR, MBR) on held-out test set. Compare COMET, BLEU, and human evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
How can the trade-off between general instruction-following capabilities and specialized translation quality be mitigated? The Conclusion states that incorporating Chat and Code instructions degrades BLEU scores and suggests future work could explore methods to "mitigate this trade-off."

### Open Question 2
Does pivoting through multiple bridge languages (English, Spanish, Catalan) during continual pre-training improve zero-shot cross-lingual transfer compared to English-only pivoting for this specific architecture? The Introduction claims English-centric data biases models, and the method uses three pivots to avoid this, but no ablation study is provided to validate this multi-pivot strategy.

### Open Question 3
Is initializing new vocabulary embeddings with the average of existing embeddings the most effective strategy for language expansion? Section 3.1 details initializing new token embeddings with the "average of all existing embeddings" based on expectation, but does not compare this specific initialization heuristic against other viable options.

## Limitations

- Incomplete methodological details, particularly instruction sampling ratios and seeds for target-side collapse mitigation
- Potential overfitting to COMET metric, with concerns about bias in system comparisons when using re-ranking techniques
- Lack of direct empirical validation for claimed benefits of multi-bridge language pivoting

## Confidence

**High Confidence Claims:**
- The two-stage training pipeline (CPT followed by instruction tuning) improves translation quality over base SALAMANDRA models
- Including related languages during pre-training helps low-resource language pairs
- Chat and code instruction types degrade translation-specific performance

**Medium Confidence Claims:**
- Multi-bridge language pivoting provides benefits beyond data volume
- Vocabulary expansion with embedding initialization is superior to random initialization
- MBR decoding using COMET metrics consistently improves translation quality

**Low Confidence Claims:**
- The specific impact of vocabulary expansion on long-term model performance
- Whether MBR improvements generalize beyond COMET to human evaluation
- The exact contribution of each bridge language to zero-shot transfer performance

## Next Checks

1. **Direct mechanism validation for bridge languages**: Conduct controlled experiments comparing English-only pivoting against multi-bridge pivoting on identical data volumes, measuring zero-shot translation quality between non-English pairs to isolate whether the benefit comes from transfer paths or simply more data.

2. **Alternative vocabulary initialization comparison**: Implement random initialization of new embeddings during vocabulary expansion and compare against the averaging approach after identical continued training to directly test whether the initialization strategy contributes to the observed fertility improvements.

3. **Metric generalization study**: Evaluate MBR-selected translations using human evaluation and alternative metrics (BLEU, ChrF) alongside COMET to determine if the decoding strategy provides general quality improvements or metric-specific optimization.