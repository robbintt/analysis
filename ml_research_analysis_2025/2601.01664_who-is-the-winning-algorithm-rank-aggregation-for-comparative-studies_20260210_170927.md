---
ver: rpa2
title: Who is the Winning Algorithm? Rank Aggregation for Comparative Studies
arxiv_id: '2601.01664'
source_url: https://arxiv.org/abs/2601.01664
tags:
- algorithm
- algorithms
- which
- page
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying the best-performing
  machine learning algorithm from a collection of competing methods based on their
  rankings across a benchmark of datasets. While the standard approach is to count
  wins, the author argues that more information is contained in complete rankings
  (e.g., how often each algorithm finishes second, third, etc.).
---

# Who is the Winning Algorithm? Rank Aggregation for Comparative Studies

## Quick Facts
- arXiv ID: 2601.01664
- Source URL: https://arxiv.org/abs/2601.01664
- Authors: Amichai Painsky
- Reference count: 26
- Primary result: Novel rank aggregation framework estimates algorithm win probabilities more accurately than standard counting-wins approach

## Executive Summary
This paper addresses the challenge of identifying the best-performing machine learning algorithm from a collection of competing methods based on their rankings across a benchmark of datasets. While the standard approach is to count wins, the author argues that more information is contained in complete rankings (e.g., how often each algorithm finishes second, third, etc.). The core contribution is a novel framework that estimates the win probability for each algorithm using a weighted linear combination of their rankings across datasets.

The framework generalizes the standard maximum likelihood estimator (MLE), which only considers wins, by allowing weights on all ranks. Two weighting strategies are proposed: a conservative, data-independent scheme optimized for worst-case performance, and a flexible, data-dependent scheme using leave-one-out cross-validation. Empirical results on synthetic and real-world datasets show that the proposed method significantly improves upon existing approaches, including the MLE, Borda Count, and Bradley-Terry models, in terms of both KL divergence and total variation distance.

## Method Summary
The method estimates algorithm win probabilities using a weighted linear combination of rank statistics. Given a matrix of rankings across datasets, it computes counts of how often each algorithm finishes at each rank position. Two weighting schemes are proposed: (1) a minimax optimization that provides data-independent weights with provable error bounds, and (2) a leave-one-out cross-validation approach that adapts weights to the empirical ranking distribution. The framework generalizes the standard MLE by incorporating information from all rank positions, not just first-place finishes.

## Key Results
- The proposed method significantly improves upon existing approaches, including MLE, Borda Count, and Bradley-Terry models
- Data-dependent LOO weights achieved lower cross-validation loss than MLE in real-world benchmarks (e.g., 1.78 vs 1.86 on TabZilla)
- Minimax data-independent weights provide conservative performance guarantees with provable error bounds
- Method provides more reliable and decisive conclusions about algorithmic performance in comparative studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted linear combination of rank statistics can estimate win probability more accurately than counting wins alone.
- Mechanism: The estimator $\hat{p}_w = \frac{1}{n}\sum_{j=1}^{m} w_j r^{(j)}$ assigns non-negative weights $w_j$ to each rank position (1st, 2nd, 3rd, etc.), where $r^{(j)}$ counts how often each algorithm finishes at position $j$. The weights sum to 1, ensuring valid probability estimates.
- Core assumption: Information about an algorithm's winning potential is distributed across its full ranking distribution, not just first-place finishes.
- Evidence anchors:
  - [abstract] "estimates the win probability for each algorithm using a weighted linear combination of their rankings across datasets"
  - [Section 3, Eq. 2] Formal definition of $\hat{p}_w$ generalizing MLE
  - [corpus] Weak direct evidence; related work on rank aggregation exists but doesn't validate this specific weighting scheme
- Break condition: If the true win probability is determined solely by first-place finishes (i.e., lower ranks contain no signal), weights $w_1=1, w_{j>1}=0$ become optimal and the method collapses to MLE.

### Mechanism 2
- Claim: Minimax optimization over worst-case distributions yields conservative, data-independent weights with provable error bounds.
- Mechanism: Solve $\min_w \max_{P \in \Delta_{m!}} E[D_{TV}(p, \hat{p}_w)]$ analytically (Theorem 1) or via convex optimization (Theorem 2). Theorem 1 gives $w^* = 1 - \frac{1}{2n+2}$ for the top-2 ranks case, approaching 1 as $n$ grows.
- Core assumption: The worst-case distribution is adversarial and unlikely in practice; bounds may be loose for real data.
- Evidence anchors:
  - [Section 4, Theorem 1] Closed-form bound: $\sqrt{m} \sqrt{\frac{2(1-w)^2 + 1}{n(w^2 + (1-w)^2)}}$
  - [Section 6.1, Figure 4] Synthetic experiments show minimax methods outperform MLE in small-sample regimes
  - [corpus] No corpus validation of minimax bounds specifically
- Break condition: When $n$ is large or the true distribution is highly skewed (dominant algorithms), $w^* \to 1$ and the bound offers no improvement over MLE.

### Mechanism 3
- Claim: Leave-one-out cross-validation produces data-dependent weights that adapt to the empirical ranking distribution.
- Mechanism: For each dataset $i$, compute $\hat{p}_w^{[-i]}$ using remaining $n-1$ datasets, then minimize $L^{loo}_{KL} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{m} I(y_i=j)\log(\hat{p}_{w,j}^{[-i]})$ via convex optimization. This estimates expected KL divergence without overfitting.
- Core assumption: The LOO loss is approximately unbiased for the true expected loss; rankings follow a consistent (though unknown) distribution across datasets.
- Evidence anchors:
  - [Section 5, Algorithm 1] Formal LOO procedure with convex optimization
  - [Section 6.2, Tables 1-3] Real-world benchmarks show lower cross-validation loss vs. MLE (e.g., 1.78 vs 1.86 on TabZilla, p=0.0075)
  - [corpus] Weak; corpus lacks LOO-specific validation for rank aggregation
- Break condition: If datasets are highly heterogeneous or $n$ is very small, LOO estimates become unstable and may overfit to noise.

## Foundational Learning

- Concept: **Multinomial distribution and MLE for discrete probabilities**
  - Why needed here: The full ranking distribution $P$ over $m!$ outcomes is multinomial; understanding MLE properties explains why simply counting wins is the baseline.
  - Quick check question: Given 5 datasets and 3 algorithms, what is the MLE for an algorithm that wins 2 datasets?

- Concept: **Bias-variance decomposition**
  - Why needed here: The proofs in Section 7 use bias-variance decomposition to derive upper bounds on estimation error; weight selection trades off bias (from weighting non-winning ranks) against variance reduction.
  - Quick check question: If $w_1 < 1$, does the estimator gain or lose bias compared to MLE? What about variance?

- Concept: **Leave-one-out cross-validation and its unbiasedness**
  - Why needed here: Algorithm 1 relies on LOO being nearly unbiased for expected loss; this connects to classical results like Good-Turing estimation.
  - Quick check question: Why does LOO avoid overfitting better than minimizing empirical loss directly on all $n$ datasets?

## Architecture Onboarding

- Component map:
  Input rankings -> Rank counter -> Weight optimizer -> Probability estimator

- Critical path:
  1. Parse benchmark results into per-dataset rankings
  2. Compute $r^{(1)}, r^{(2)}, \dots, r^{(K)}$ (typically $K=3$)
  3. Choose scheme: data-independent (Theorem 2) or data-dependent (Algorithm 1)
  4. Solve for optimal weights $w$ via convex optimization
  5. Output $\hat{p}_w$ and identify top algorithm(s)

- Design tradeoffs:
  - **K selection**: Higher $K$ uses more information but increases optimization complexity and may overfit small $n$. Paper uses $K=3$ as default.
  - **Scheme choice**: Data-independent is conservative and robust to distribution misspecification; data-dependent adapts better but assumes i.i.d. datasets.
  - **Weight monotonicity constraint**: Enforcing $w_1 \geq w_2 \geq \dots$ is intuitive and stabilizes optimization, but may exclude valid solutions.

- Failure signatures:
  - All weights collapse to $w_1 \approx 1$: Either $n$ is large, or the ranking distribution is heavily skewed (few dominant algorithms). Check if this is data-driven or optimization failure.
  - Highly non-uniform weights for uniform-looking data: May indicate LOO instability with very small $n$.
  - Negative or non-normalized $\hat{p}_w$: Constraint violation in optimizer; check feasibility region.

- First 3 experiments:
  1. **Sanity check**: Apply both schemes to synthetic Zipf-distributed rankings (heavy-tailed, known $p$). Verify LOO weights approach $w_1 \to 1$ as skew increases, matching intuition that wins dominate.
  2. **Small-n stress test**: Use $n \leq 10$ datasets with $m=5$ algorithms. Compare minimax (Theorem 2, $K=2,3$) vs. MLE vs. LOO. Expect minimax to outperform when $n$ is very small.
  3. **Real benchmark replication**: Download TabZilla data from [18], reproduce Table 1 results. Confirm XGBoost vs. CatBoost ordering difference between MLE and proposed method stems from second-place counts (5 vs. 16).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed rank aggregation framework be naturally adapted for applications beyond machine learning benchmarks, such as predicting winners in horse races or other competitive environments?
- Basis in paper: [explicit] The conclusion states, "We consider such applications and their natural adaptions for our future work," specifically referencing scenarios like horse racing where past performance data is used to predict future winners.
- Why unresolved: The current framework is formulated specifically for fixed sets of $m$ algorithms and i.i.d. datasets; translating this to domains with varying participants or different dependency structures requires further development.
- What evidence would resolve it: A modification of the estimator and weighting schemes applied to a sports or gaming dataset, demonstrating performance against domain-specific baselines like Elo ratings.

### Open Question 2
- Question: Can a data-independent minimax weighting scheme be derived for the Kullback-Leibler (KL) divergence loss under valid finite-sample constraints?
- Basis in paper: [explicit] Section 4 notes that "minimax analysis of discrete distributions under KL divergence require additional assumptions and constraints... Therefore, we refrain from a worst-case KL analysis and study this divergence only in a data-dependent regime."
- Why unresolved: The KL divergence is unbounded in finite samples if the estimated probability is zero, making standard minimax analysis problematic without specific constraints on the distribution $P$.
- What evidence would resolve it: A theoretical derivation of weights $w^*$ that minimize the worst-case expected KL divergence, potentially bound to a restricted class of distributions where $p_j$ is non-zero.

### Open Question 3
- Question: Can the theoretical upper bounds for the data-independent scheme be tightened by imposing realistic constraints on the ranking distribution $P$ to avoid overly conservative weights?
- Basis in paper: [explicit] Page 12 discusses the worst-case bound: "imposing additional constraints on P to overcome this issue requires additional assumptions on the underlying model," noting that the current worst-case distribution is "pathological" and unlikely in practice.
- Why unresolved: The current minimax weights optimize for a pathological distribution (where one algorithm is always second), which renders the theoretical gains marginal compared to the MLE in general cases.
- What evidence would resolve it: A corollary or new theorem showing improved convergence rates or lower constant factors for the upper bound when $P$ belongs to a restricted family (e.g., Plackett-Luce or Mallows models).

### Open Question 4
- Question: Is the assumption that dataset rankings are independent and identically distributed (i.i.d.) necessary for the consistency of the data-dependent leave-one-out estimator?
- Basis in paper: [inferred] Section 3 states the framework assumes "X_i are independent and identically distributed observations from P," but acknowledges this is a "strong" assumption that is merely "commonly used." Real-world benchmarks often contain datasets with varying sizes or shared origins, potentially violating i.i.d. conditions.
- Why unresolved: The empirical validation relies on benchmarks that may violate this assumption, yet the theoretical guarantees (convexity of the loss, unbiasedness of LOO) rely heavily on it.
- What evidence would resolve it: A sensitivity analysis showing the degradation of the estimator's accuracy as the correlation between datasets increases, or a theoretical extension relaxing the i.i.d. requirement.

## Limitations
- The framework assumes consistent ranking distributions across datasets, which may not hold with heterogeneous or biased datasets
- Theoretical minimax bounds may be overly conservative due to optimizing for pathological worst-case distributions
- Leave-one-out cross-validation assumes near-unbiased loss estimation, which may break down with small or correlated datasets

## Confidence
- **High confidence** in the theoretical framework and derivation of minimax bounds (Theorems 1-2). The mathematical machinery is rigorous and well-established.
- **Medium confidence** in the practical utility of LOO cross-validation for weight selection. While theoretically sound, the assumption of near-unbiased LOO loss may break down with highly heterogeneous datasets.
- **Medium confidence** in empirical superiority over existing methods. The results are compelling but limited to specific benchmark suites; generalization to other domains remains to be tested.

## Next Checks
1. **Heterogeneous dataset test**: Apply the framework to benchmarks with known algorithmic domain shifts (e.g., UCI datasets grouped by feature type) and measure performance degradation.
2. **Small-sample robustness**: Systematically evaluate $n \in \{5, 10, 20\}$ for $m \in \{3, 5, 10\}$ to quantify overfitting risk of LOO weights.
3. **Alternative rank information**: Extend experiments beyond top-3 ranks to $K=5$ or $K=10$ and measure gains in estimation accuracy.