---
ver: rpa2
title: Evaluating LLaMA 3.2 for Software Vulnerability Detection
arxiv_id: '2503.07770'
source_url: https://arxiv.org/abs/2503.07770
tags:
- code
- vulnerability
- https
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the application of Large Language Models
  (LLMs) to software vulnerability detection using a refined real-world dataset. The
  authors apply pre-processing to remove irrelevant code elements and errors, and
  fine-tune LLaMA 3.2 to perform binary vulnerability classification.
---

# Evaluating LLaMA 3.2 for Software Vulnerability Detection

## Quick Facts
- arXiv ID: 2503.07770
- Source URL: https://arxiv.org/abs/2503.07770
- Reference count: 40
- Primary result: LLaMA 3.2 achieves 66% F1-score on vulnerability detection after SCoPE preprocessing and LoRA fine-tuning

## Executive Summary
This paper investigates the application of Large Language Models to software vulnerability detection using a refined real-world dataset. The authors apply pre-processing to remove irrelevant code elements and errors, and fine-tune LLaMA 3.2 to perform binary vulnerability classification. They show that pre-processing improves performance, with LLaMA 3.2 achieving an F1-Score of 66%, outperforming the baseline of 47%. The study demonstrates that lightweight LLMs can be effective for vulnerability detection on real-world data when combined with proper data refinement.

## Method Summary
The study uses LLaMA 3.2 (1.23B parameters) fine-tuned with LoRA on a balanced subset of 10,000 C/C++ functions from the DiverseVul dataset. Pre-processing uses the SCoPE framework to remove comments, whitespace, and normalize identifiers. The model is trained for 4 epochs with learning rate 2e-5 and batch size 32, evaluated using F1-score on a held-out test set.

## Key Results
- LLaMA 3.2 achieves 66% F1-score after SCoPE preprocessing and LoRA fine-tuning
- Pre-processing improves performance from baseline of 47% F1
- 7,901 erroneous entries identified and removed during preprocessing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-processing source code by removing non-essential elements improves vulnerability detection performance in fine-tuned LLMs.
- **Mechanism:** The SCoPE framework normalizes code by removing comments, reducing redundant whitespace, and substituting programmer-defined identifiers with generic tokens. This forces the model to learn from semantic structure rather than potentially misleading naming conventions.
- **Core assumption:** Programmer-defined identifiers correlate poorly with actual vulnerability presence and may introduce spurious patterns.
- **Evidence anchors:** Abstract states preprocessing led to improvement; section notes SCoPE reduced token length from 374.13 to 300.38 tokens, forcing learning without relying on misleading identifiers.
- **Break condition:** If identifier names encode domain-specific security semantics (e.g., `decrypt_*`, `auth_*`), removing them could eliminate useful signal.

### Mechanism 2
- **Claim:** Identifying and removing erroneous/contradictory entries from vulnerability datasets improves model training stability and downstream F1-Score.
- **Mechanism:** During SCoPE processing, the authors discovered 7,901 duplicated entries with conflicting labels and faulty samples. Removing these eliminates label noise that would otherwise confuse gradient descent during fine-tuning.
- **Core assumption:** Label contradictions and non-code artifacts in training data cause the model to learn inconsistent decision boundaries.
- **Evidence anchors:** Abstract notes several inconsistencies identified in raw dataset; section describes examples of faulty data including entries with only comments or identical code with opposite labels.
- **Break condition:** If removed entries contained legitimate edge cases or rare vulnerability patterns rather than true errors, dataset diversity and recall on rare CWEs would decrease.

### Mechanism 3
- **Claim:** Lightweight LLMs (1.23B parameters) with LoRA-based fine-tuning can achieve competitive vulnerability detection on real-world data when combined with data refinement.
- **Mechanism:** LoRA reduces trainable parameters, enabling efficient fine-tuning on consumer-grade hardware while preserving the pre-trained model's language understanding. The model adapts its general multilingual knowledge to the binary classification task.
- **Core assumption:** Pre-trained language representations in LLaMA 3.2 transfer sufficiently to code semantics for vulnerability patterns without requiring code-specific pre-training.
- **Evidence anchors:** Abstract states LLaMA 3.2 demonstrates competitive performance; corpus includes neighboring paper supporting ongoing exploration of LLaMA architectures for this task.
- **Break condition:** If vulnerability detection requires deep code-specific structural reasoning not captured in text-only LLM pre-training, performance gains from LoRA alone will plateau.

## Foundational Learning

- **Concept: F1-Score vs. Accuracy in Imbalanced Datasets**
  - **Why needed here:** The DiverseVul dataset is heavily imbalanced (~18,945 vulnerable vs. ~330,492 non-vulnerable functions). The baseline NatGen shows 92% accuracy but only 47% F1, demonstrating how accuracy misleads in skewed distributions.
  - **Quick check question:** If a model predicts "non-vulnerable" for all samples, what would the accuracy be? Why does F1-Score better reflect detection utility?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The authors fine-tune LLaMA 3.2 on a single RTX A4500 using LoRA to reduce memory/compute costs while maintaining performance.
  - **Quick check question:** What is the key difference between LoRA and full fine-tuning in terms of which parameters are updated during training?

- **Concept: Token Context Windows and Truncation**
  - **Why needed here:** The paper sets a 1024-token limit; 5.48% of original functions exceed this, and SCoPE reduces this truncation rate by shortening code.
  - **Quick check question:** How might truncating a function at token 1024 affect detection of vulnerabilities that depend on code appearing later in the function?

## Architecture Onboarding

- **Component map:** Raw C/C++ functions from DiverseVul -> SCoPE preprocessing (comment removal, identifier normalization, duplicate detection) -> LLaMA 3.2 + LoRA adapters -> Binary classification output

- **Critical path:** 1) Load raw DiverseVul -> apply SCoPE -> identify/remove erroneous entries (7,901 duplicates/conflicts) 2) Create balanced subset (5,000 vulnerable + 5,000 non-vulnerable) 3) Split 70/15/15 (train/validation/test) 4) Fine-tune LLaMA 3.2 with LoRA (4 epochs, lr=2e-5, batch=32) 5) Evaluate on held-out test set; report Accuracy, Precision, Recall, F1

- **Design tradeoffs:**
  - Balanced vs. real-world distribution: Balanced training improves F1 but may not generalize to production's severe class imbalance
  - Token limit vs. completeness: 1024 tokens reduce compute but truncate longer functions; risk of missing vulnerabilities in tail code
  - Lightweight model vs. capacity: 1.23B parameters enable deployment on lower-end devices but may underperform larger code-specific models on complex CWEs

- **Failure signatures:**
  - High accuracy + low F1: Model biased toward majority class (non-vulnerable)
  - Over-reliance on identifier names: Model predictions change significantly when variable names are systematically renamed
  - Label leakage: Identical code snippets appearing in both train and test with different labels due to dataset noise
  - Unstable loss: Training loss oscillates without convergingâ€”indicates learning rate too high or label noise

- **First 3 experiments:**
  1. Ablation on pre-processing: Train LLaMA 3.2 on raw vs. SCoPE-processed data (same subset); isolate F1 delta from normalization
  2. Identifier adversarial test: Rename all user-defined identifiers in test set (e.g., `var1`, `func1`); compare predictions to assess semantic vs. surface learning
  3. Class imbalance sensitivity: Train on 10K balanced vs. stratified imbalanced subset (matching original ~5% vulnerable ratio); evaluate precision/recall tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training on the full DiverseVul dataset significantly improve vulnerability detection performance compared to the 10,000-entry subset used in this study?
- **Basis in paper:** Section 6 states, "One direction is to expand the scope of the study by using a larger portion of the DiverseVul dataset, as only a subset was used in this initial research."
- **Why unresolved:** The authors limited the training data to a balanced subset of 10,000 entries to optimize computational efficiency, leaving the potential gains from the full dataset unstudied.
- **What evidence would resolve it:** Fine-tuning the model on the complete refined dataset and comparing the resulting F1-Score against the 66% baseline established in this work.

### Open Question 2
- **Question:** To what extent do programmer-controlled elements (such as variable names) or adversarial manipulations influence the model's predictions?
- **Basis in paper:** Section 6 notes, "Another direction could be to explore the robustness of the model, as the results suggest that programmer-controlled elements may influence its predictions."
- **Why unresolved:** While the results suggest removing variable names helps the model focus on semantics, the specific robustness of the model against adversarial attacks based on these elements was not quantified.
- **What evidence would resolve it:** A robustness evaluation using adversarial samples where identifier names are mutated to mislead the classifier, compared against the processed dataset performance.

### Open Question 3
- **Question:** Can techniques like prompt engineering or Retrieval-Augmented Generation (RAG) enhance the explainability and accuracy of LLaMA 3.2 for vulnerability detection?
- **Basis in paper:** Section 6 suggests, "advanced techniques such as prompt engineering and retrieval-augmented generation could be explored to further improve both the performance and explainability of the model."
- **Why unresolved:** The current study focused on binary classification via fine-tuning, without utilizing context-aware prompting or external knowledge retrieval to justify predictions.
- **What evidence would resolve it:** Experiments integrating RAG or chain-of-thought prompting to measure improvements in detection metrics and the qualitative validity of generated explanations.

## Limitations

- Dataset refinement transparency: The exact criteria for labeling entries as "erroneous" are not specified, introducing uncertainty about whether legitimate rare vulnerability patterns might have been inadvertently removed.
- Generalizability concerns: The balanced 5,000/5,000 subset used for evaluation differs significantly from the real-world vulnerability distribution (approximately 1.5% vulnerable in DiverseVul), potentially limiting real-world applicability.
- Identifier normalization effects: The mechanism assumes programmer-defined identifiers provide little semantic value for vulnerability detection, but this contribution is not directly quantified through controlled experiments.

## Confidence

- **High confidence:** The observed F1-score improvement from 47% to 66% when applying SCoPE preprocessing is well-supported by the experimental results presented.
- **Medium confidence:** The claim that lightweight LLMs can effectively detect vulnerabilities when combined with data refinement is supported, but the specific advantage over larger or code-specialized models is not directly compared.
- **Low confidence:** The specific contribution of individual SCoPE components (identifier normalization vs. comment removal vs. duplicate detection) to the overall performance improvement cannot be precisely quantified from the current results.

## Next Checks

1. **Ablation study on preprocessing components:** Train separate models using (a) raw data, (b) data with only identifier normalization, (c) data with only duplicate removal, and (d) full SCoPE preprocessing. Compare F1-scores to isolate the contribution of each preprocessing step.

2. **Imbalanced dataset evaluation:** Evaluate the fine-tuned model on the full DiverseVul dataset (approximately 18,945 vulnerable vs. 330,492 non-vulnerable functions) rather than the balanced subset. Report precision, recall, and F1-score for both classes to assess real-world applicability.

3. **Cross-dataset generalization test:** Test the model trained on DiverseVul against functions from a different vulnerability dataset (such as Devign or SySeVR) without additional fine-tuning. This would validate whether the model has learned generalizable vulnerability patterns or merely dataset-specific artifacts.