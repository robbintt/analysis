---
ver: rpa2
title: 'Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised
  Machine Learning'
arxiv_id: '2601.01023'
source_url: https://arxiv.org/abs/2601.01023
tags:
- distance
- dataset
- distances
- datasets
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a task- and model-aware framework for measuring
  similarity between wireless datasets, enabling applications such as dataset selection/augmentation,
  simulation-to-real (sim2real) comparison, task-specific synthetic data generation,
  and informing decisions on model training/adaptation to new deployments. We evaluate
  candidate dataset distance metrics by how well they predict cross-dataset transferability:
  if two datasets have a small distance, a model trained on one should perform well
  on the other.'
---

# Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning

## Quick Facts
- arXiv ID: 2601.01023
- Source URL: https://arxiv.org/abs/2601.01023
- Reference count: 40
- This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments.

## Executive Summary
This paper presents a novel framework for measuring similarity between wireless datasets using task- and model-aware distance metrics. The framework evaluates dataset distances by how well they predict cross-dataset transferability - if two datasets have small distances, models trained on one should perform well on the other. The authors apply this framework to both unsupervised (CSI compression with autoencoders) and supervised (beam prediction with CNNs) wireless tasks, achieving Pearson correlations exceeding 0.85 between dataset distances and actual task performance.

## Method Summary
The framework combines UMAP embeddings with Wasserstein and Euclidean distances to create dataset similarity metrics that are aware of both the specific task and model architecture being used. For supervised tasks, the authors develop a label-aware distance metric that integrates supervised UMAP embeddings with penalties for dataset imbalance. The evaluation approach measures how well these distance metrics predict actual cross-dataset transferability by training models on one dataset and testing on another, then correlating these performance results with the calculated dataset distances.

## Key Results
- Achieved Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance
- UMAP-based embeddings combined with Wasserstein and Euclidean distances performed best for CSI compression task
- Derived label-aware distance metric integrating supervised UMAP and imbalance penalties for beam prediction task
- Outperformed traditional baseline distance metrics across both supervised and unsupervised wireless tasks

## Why This Works (Mechanism)
The framework works by creating task-specific representations of datasets through UMAP embeddings that capture the relevant structure for the target wireless application. By measuring distances in these task-aware embedding spaces rather than raw feature spaces, the framework can better predict whether models trained on one dataset will generalize to another. The combination of multiple distance metrics (Wasserstein and Euclidean) captures both distributional and geometric similarities between datasets.

## Foundational Learning

**UMAP Embeddings**: Dimensionality reduction technique that preserves local and global data structure
- Why needed: Creates task-aware representations that capture relevant patterns
- Quick check: Verify embedding preserves local neighborhoods and global topology

**Wasserstein Distance**: Measures distance between probability distributions
- Why needed: Captures distributional differences between datasets
- Quick check: Confirm distributions are properly normalized before comparison

**Euclidean Distance**: Standard geometric distance metric
- Why needed: Provides baseline similarity measure in embedding space
- Quick check: Verify consistent scaling across all dataset features

## Architecture Onboarding

Component map: Raw Wireless Data -> UMAP Embedding -> Distance Calculation (Wasserstein + Euclidean) -> Transferability Prediction

Critical path: Dataset preparation and preprocessing -> UMAP embedding generation -> Distance metric computation -> Correlation with cross-dataset performance

Design tradeoffs: UMAP dimensionality vs. computational cost; choice of distance metrics vs. interpretability; task-specific vs. general-purpose embeddings

Failure signatures: Poor correlation between distances and performance may indicate inadequate embedding dimensionality, inappropriate distance metrics for the task, or insufficient dataset diversity

First experiments:
1. Verify UMAP embeddings preserve local structure by checking k-nearest neighbor relationships
2. Test sensitivity of distance metrics to dataset size and class balance
3. Validate correlation between calculated distances and actual cross-dataset transferability on held-out data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on only two specific wireless tasks (CSI compression and beam prediction), limiting generalizability claims
- No sensitivity analysis provided for UMAP embedding dimensions and hyperparameter choices
- Computational overhead and scalability for large-scale wireless datasets not discussed
- Integration of label-aware distances for supervised tasks lacks theoretical justification for imbalance penalty mechanism

## Confidence
- High confidence in core methodology and experimental design
- Medium confidence in generalizability claims across wireless tasks and datasets
- Low confidence in framework's scalability and computational efficiency for real-world deployment

## Next Checks
1. Evaluate the framework across diverse wireless tasks beyond CSI compression and beam prediction
2. Conduct sensitivity analysis on UMAP embedding parameters and distance metric hyperparameters
3. Benchmark computational efficiency and scalability on large-scale wireless datasets