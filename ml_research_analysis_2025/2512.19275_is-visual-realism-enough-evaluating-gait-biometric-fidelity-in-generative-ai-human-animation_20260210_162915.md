---
ver: rpa2
title: Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative
  AI Human Animation
arxiv_id: '2512.19275'
source_url: https://arxiv.org/abs/2512.19275
tags:
- gait
- motion
- recognition
- visual
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether state-of-the-art generative AI models\
  \ can preserve gait-specific biometric information during human animation synthesis.\
  \ Four animation models\u2014MagicAnimate, AnimateAnyone, UniAnimate, and MTVCrafter\u2014\
  were assessed across two tasks: Gait Restoration (reconstructing motion from reference\
  \ videos) and Identity Transfer (animating one identity with another's motion)."
---

# Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation

## Quick Facts
- arXiv ID: 2512.19275
- Source URL: https://arxiv.org/abs/2512.19275
- Reference count: 40
- Models evaluated: MagicAnimate, AnimateAnyone, UniAnimate, MTVCrafter

## Executive Summary
This study reveals a fundamental disconnect between visual quality and biometric fidelity in generative AI human animation. While state-of-the-art animation models achieve high visual fidelity (SSIM up to 0.82, PSNR up to 19.51), they fail to preserve gait-specific biometric information required for identification. The research demonstrates that current gait recognition models primarily rely on appearance rather than temporal dynamics, as evidenced by near-zero recognition accuracy when texture is disentangled from motion in identity transfer experiments.

## Method Summary
The study evaluates four animation models across two tasks using 250 identities each from SUSTech1K and CCGR-MINI datasets, generating 16,000 videos total. Task 1 (Gait Restoration) tests motion reconstruction fidelity under three scenarios: baseline, off-pose misalignment, and in-the-wild conditions. Task 2 (Identity Transfer) evaluates whether models can animate one identity's appearance with another's motion while preserving both components. Visual quality is measured using SSIM, PSNR, and LPIPS on segmentation-masked outputs. Biometric fidelity is assessed using four gait recognition models (DeepGaitV2, SkeletonGait, SkeletonGait++, BigGait) across RGB, silhouette, and skeleton modalities, reporting Rank-1, Rank-5, and mAP scores.

## Key Results
- Visual quality metrics achieved high scores (SSIM up to 0.82, PSNR up to 19.51) while biometric performance remained low (Rank-1 accuracy often below 15%)
- MagicAnimate outperformed skeleton-guided models on biometric metrics in controlled scenarios due to dense pose conditioning
- Identity transfer experiments revealed gait recognition models rely primarily on appearance, with recognition accuracy collapsing when texture was disentangled from motion
- Models exhibited systematic structural biases, particularly body thickening, that degraded silhouette-based identification cues

## Why This Works (Mechanism)

### Mechanism 1: Texture-Dominant Recognition Collapse
Current gait recognition models operate primarily as appearance-based Re-Identification systems rather than temporal dynamics analyzers. When texture (clothing, body shape) is disentangled from motion via identity transfer, recognition accuracy collapses to near-zero on RGB-based evaluators, revealing that spatial features dominate over temporal gait patterns in current architectures.

### Mechanism 2: Dense Pose Guidance Preserves Kinematics Better Than Sparse Skeletons
Dense-pose conditioning (MagicAnimate) maintains superior structural and kinematic fidelity compared to sparse 2D skeleton guidance (AnimateAnyone, UniAnimate) under controlled conditions. Dense pose provides volumetric surface constraints that prevent the "systematic structural bias" of body thickening observed in skeleton-guided models, preserving silhouette-based identification cues.

### Mechanism 3: Visual Quality Metrics Are Poor Proxies for Behavioral Fidelity
Standard reconstruction metrics (SSIM, PSNR, LPIPS) correlate weakly with biometric preservation; high visual fidelity can mask kinematic inconsistencies. Pixel-level similarity rewards texture accuracy while penalizing neither temporal incoherence nor structural distortions that break gait signatures—evaluations are frame-wise, not sequence-wise.

## Foundational Learning

- Concept: **Diffusion-Based Image Animation with Pose Guidance**
  - Why needed here: All four evaluated models use latent diffusion with pose conditioning; understanding how motion guidance injects temporal signals into the denoising process is prerequisite to diagnosing fidelity failures.
  - Quick check question: Can you explain why sparse 2D keypoints might fail to constrain body volume during texture warping in a diffusion U-Net?

- Concept: **Gait Recognition Modalities (Silhouette, Skeleton, RGB)**
  - Why needed here: The paper evaluates across four modalities with different inductive biases; knowing what each captures (shape vs. dynamics vs. appearance) is essential for interpreting the modality gap results.
  - Quick check question: Why would a silhouette-based gait model (DeepGaitV2) be more sensitive to body shape distortion than an RGB model (BigGait)?

- Concept: **Identity Disentanglement in Generative Models**
  - Why needed here: Task 2 (Identity Transfer) tests whether models can separate "who" (reference identity) from "how" (driving motion); this is the core diagnostic for behavioral DeepFake capability.
  - Quick check question: If a model perfectly preserves reference identity appearance but achieves 0% accuracy against driving motion on SkeletonGait, what does this imply about its motion transfer?

## Architecture Onboarding

- Component map: Reference image + Driving video → Motion guidance extractor (DensePose / 2D Skeleton / SMPL joints) → Latent Diffusion Model with temporal attention → Generated video → Segmentation mask → Modality extractor (Silhouette/Skeleton/RGB) → Four gait recognition models → Unified GaitBase embedding head

- Critical path: Motion guidance extraction quality directly limits animation fidelity; temporal attention mechanism determines motion coherence over frames; appearance encoder vs. motion encoder balance controls identity-motion disentanglement; evaluator modality choice determines which failures are detectable

- Design tradeoffs:
  - Dense-pose vs. sparse skeleton: Dense-pose (MagicAnimate) → better kinematic fidelity, worse robustness to pose estimation errors; Sparse skeleton (AnimateAnyone) → better texture preservation, worse volumetric consistency
  - Unified noise input vs. frame-by-frame: UniAnimate's unified noise improves long-video coherence but may sacrifice fine temporal dynamics
  - 2D vs. 3D motion tokens: MTVCrafter's 4DMoT captures spatio-temporal cues but shows high sensitivity to initialization noise

- Failure signatures:
  - Visual inconsistencies: Hallucinated accessories/faces, missing backpacks, aspect ratio distortion (body thickening)
  - Behavioral inconsistencies: Foot sliding, missing arm motion, unnatural pose transitions
  - Modality-specific collapse: High RGB Rank-1 + near-zero Skeleton Rank-1 on driving motion → texture-only transfer

- First 3 experiments:
  1. Reproduction of Task 1 Baseline: Run all four animation models on 50 SUSTech1K identities with first-frame reference; compute SSIM/PSNR and all four gait evaluator Rank-1 scores to validate the visual-biometric gap
  2. Ablation on Motion Guidance Density: Replace DensePose with 2D skeleton in MagicAnimate (or vice versa in AnimateAnyone) on matched architecture to isolate guidance strategy effect from backbone differences
  3. Temporal Consistency Probe: Compute optical flow divergence between generated and ground-truth sequences; correlate with SkeletonGait Rank-1 to test whether temporal metrics predict biometric fidelity better than SSIM

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating biometric-aware objectives (e.g., temporal skeleton consistency losses) into diffusion training improve behavioral fidelity without compromising visual quality?
- Basis in paper: The Conclusion states future research "must move beyond frame-wise quality, integrating biometric-aware objectives... into diffusion training" to ensure behavioral authenticity.
- Why unresolved: Current models optimize for pixel-level or perceptual similarity (SSIM, LPIPS), which fails to preserve the subtle kinematic cues required for identification.
- What evidence would resolve it: Training a model with these specific losses that achieves significantly higher Rank-1 accuracy on skeleton-based gait recognition while maintaining current visual metric baselines.

### Open Question 2
How can gait recognition architectures be redesigned to prioritize temporal dynamics over static texture to prevent collapse during identity transfer?
- Basis in paper: The authors expose a "fundamental flaw" where recognition models act as "appearance-based Re-Identification models," failing to identify subjects when texture is disentangled from motion.
- Why unresolved: The Identity Transfer task showed near-zero recognition accuracy against the driving motion, proving current models ignore the behavioral component of gait.
- What evidence would resolve it: A gait recognition model that maintains high Rank-1 accuracy for the "Driving Identity" even when the visual texture belongs to a different "Reference Identity."

### Open Question 3
Is the performance collapse of dense-guidance models in "in-the-wild" scenarios primarily attributable to failures in the intermediate pose estimation algorithms?
- Basis in paper: The authors note that MagicAnimate's performance collapsed in the In-the-Wild scenario, "likely due to DensePose estimation errors," suggesting the input guidance failure may be the bottleneck rather than the animation model itself.
- Why unresolved: The study evaluates full pipelines but does not isolate the impact of noisy pose extraction versus the generative model's capacity to handle noise.
- What evidence would resolve it: An ablation study testing the generative models using ground-truth or high-fidelity motion guidance in complex environments to isolate generation capability from guidance noise.

## Limitations
- Study relies on specific gait recognition architectures whose performance may not generalize to alternative temporal models or larger-scale evaluation sets
- Texture-conditional recognition collapse observed in identity transfer experiments could reflect dataset-specific biases rather than fundamental architectural limitations
- Analysis focuses on RGB video outputs without exploring audio-visual synchronization or other multimodal biometric cues

## Confidence

- **High Confidence**: Visual quality-biometric fidelity gap (supported by quantitative metrics across multiple models and datasets)
- **Medium Confidence**: Dense-pose superiority claims (based on comparative results but without ablation on guidance strategy alone)
- **Medium Confidence**: Recognition model bias toward appearance (strong evidence from transfer experiments but limited to tested architectures)

## Next Checks

1. **Architecture Ablation Study**: Replace motion guidance strategies (DensePose ↔ 2D skeleton) within matched model architectures to isolate guidance density effects from backbone differences.

2. **Temporal Consistency Correlation**: Compute sequence-level metrics (optical flow divergence, motion perceptual loss) and correlate with biometric scores to validate whether temporal coherence predicts gait preservation better than frame-wise SSIM.

3. **Evaluator Generalization Test**: Apply the four animation models to an external gait dataset (not SUSTech1K/CCGR-MINI) and evaluate with the same gait recognition models to test whether modality collapse patterns persist across domains.