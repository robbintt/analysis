---
ver: rpa2
title: Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression
  Recognition
arxiv_id: '2505.16372'
source_url: https://arxiv.org/abs/2505.16372
tags:
- recognition
- temporal
- micro-expression
- fusion
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dynamic micro-expression
  recognition (DMER), where the brief and localized nature of micro-expressions leads
  to recognition accuracy as low as 50% even for professionals. To tackle this, the
  authors propose TSFmicro, a novel temporal and spatial feature fusion framework.
---

# Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition

## Quick Facts
- arXiv ID: 2505.16372
- Source URL: https://arxiv.org/abs/2505.16372
- Reference count: 40
- Primary result: Proposed TSFmicro achieves significant improvements over state-of-the-art methods on CASME II, SAMM, and CAS(ME)3 datasets

## Executive Summary
This paper addresses the challenge of dynamic micro-expression recognition (DMER), where the brief and localized nature of micro-expressions leads to recognition accuracy as low as 50% even for professionals. To tackle this, the authors propose TSFmicro, a novel temporal and spatial feature fusion framework. The framework integrates a Retention Network (RetNet) for capturing temporal dynamics and a transformer-based network for spatial feature extraction. A key innovation is the parallel time-space fusion method, which fuses spatio-temporal information in high-dimensional feature space to create complementary "where-how" relationships, enriching the semantic information for the model. Experiments on three well-known micro-expression datasets (CASME II, SAMM, and CAS(ME)3) demonstrate that TSFmicro outperforms state-of-the-art methods, achieving significant improvements in accuracy, unweighted F1 score, and unweighted average recall across both 3/4-class and 5/7-class classification tasks.

## Method Summary
TSFmicro is a dual-stream network that processes temporal and spatial features separately before fusing them. The temporal branch computes difference frames between apex and onset frames, then processes them through a 5-layer Retention Network (RetNet) with output dimension 14×14×512. The spatial branch processes onset frames through a 2-layer transformer with patch embedding and position embedding, also outputting 14×14×512 features. The two streams are fused via element-wise summation, followed by a 1024-unit fully connected layer, normalization, Swish activation, and average pooling before classification. The model is trained with AdamW optimizer (lr=0.0008), batch size 32, for 50 epochs using cross-entropy loss and exponential learning rate decay. LOSO cross-validation is used for evaluation.

## Key Results
- TSFmicro outperforms state-of-the-art methods on CASME II, SAMM, and CAS(ME)3 datasets
- Achieves 87.50% accuracy on CASME II 5-class classification (vs 82.25% for T-S(early) fusion)
- Demonstrates significant improvements in unweighted F1 score and unweighted average recall
- Late fusion strategy (T-S(late)) consistently outperforms early and sequential fusion approaches

## Why This Works (Mechanism)

### Mechanism 1: Temporal Dynamics via Difference Frames and Retention Decay
- **Claim:** If temporal features extracted as difference between apex and onset frames are processed through a retention mechanism with temporal decay, then the model may better capture micro-expression motion dynamics than using raw frame sequences.
- **Mechanism:** The difference frame (f_apex - f_onset) isolates motion information from static appearance. RetNet's retention mechanism introduces explicit temporal decay during sequence processing, which may help the model weight recent/peaked motion more heavily than baseline states.
- **Core assumption:** Micro-expression information is primarily encoded in the transformation from neutral (onset) to peak (apex), not in individual frames.
- **Evidence anchors:** [abstract] "framework integrates a Retention Network (RetNet) for capturing temporal dynamics"; [section 3.1] Eq. 1: f_t = f_apex - f_onset; Eq. 2 shows retention mechanism with decay factor γ
- **Break condition:** If onset-apex alignment is inaccurate, or if micro-expressions don't follow a clear onset-apex progression, difference frames may introduce noise rather than signal.

### Mechanism 2: Shallow Spatial Branch with Position Embedding for Localization
- **Claim:** Restricting the spatial transformer to only 2 layers may prevent extraction of identity-related features while preserving location information needed to ground temporal dynamics.
- **Mechanism:** Deep networks tend to encode identity information (who the person is) alongside expression information. By using only 2 transformer layers with explicit position embeddings, the spatial branch focuses on "where" features (geometric properties, AU locations) without over-learning identity cues.
- **Core assumption:** Identity features require deeper network capacity; localization can be achieved with shallow architectures plus positional priors.
- **Evidence anchors:** [section 3.2] "we only used two layers of Transformer block to avoid extracting the identity information that is not related to the micro-expression motion"; [section 3.2] Eq. 8: position embedding added to patch tokens
- **Break condition:** If the dataset has high identity variance or the 2-layer network is insufficient for the geometric complexity of expressions, localization accuracy may degrade.

### Mechanism 3: Late Parallel Fusion for Complementary "Where-How" Representations
- **Claim:** Fusing temporal and spatial features in high-dimensional space after independent feature extraction (late fusion) may outperform early or sequential fusion by preserving modality-specific representations.
- **Mechanism:** Temporal branch learns "how" motion occurs; spatial branch learns "where" on the face. Late fusion via element-wise summation in the 14×14×512 feature space allows both representations to mature independently before semantic combination, forming richer "where-how" relationships.
- **Core assumption:** Temporal and spatial features are complementary and should not interfere during early representation learning.
- **Evidence anchors:** [abstract] "parallel time-space fusion method...creates complementary 'where-how' relationships"; [table 7] T-S(late) achieves 87.50% ACC vs 82.25% for T-S(early) on CASME II 5-class
- **Break condition:** If one modality is significantly noisier or if the feature spaces are misaligned in scale/semantics, late fusion may not achieve complementary effects.

## Foundational Learning

- **Concept: Retention Networks (RetNet) vs. Transformers**
  - **Why needed here:** The temporal branch uses RetNet, not standard attention. Understanding retention's decay mechanism (vs. attention's full-sequence visibility) is essential for interpreting why it may suit temporal dynamics.
  - **Quick check question:** Can you explain how retention decay differs from attention's positional encoding in handling long-range dependencies?

- **Concept: Micro-expression Onset-Apex Temporal Structure**
  - **Why needed here:** The entire temporal branch relies on onset-apex framing. Without understanding this temporal model, the difference-frame approach won't make sense.
  - **Quick check question:** Given a video sequence, how would you identify onset and apex frames, and what could go wrong if misidentified?

- **Concept: Early vs. Late Fusion in Multimodal Learning**
  - **Why needed here:** The paper compares four fusion strategies. Understanding feature-level vs. decision-level fusion helps interpret the ablation results.
  - **Quick check question:** What are the trade-offs between fusing at the input level (early) vs. at the representation level (late) in terms of feature interference and computational cost?

## Architecture Onboarding

- **Component map:** Input preprocessing (MTCNN face cropping, resize to 224×224) → Temporal branch (difference frame → Conv Stem → 5-layer RetNet) and Spatial branch (onset frame → Patch slicing → Position embedding → 2-layer Transformer) → Parallel fusion (element-wise sum) → FC(1024) → Norm → Swish → Avg pooling → Classification head

- **Critical path:** The onset-apex alignment and difference computation directly determine temporal signal quality. If this fails, no amount of RetNet sophistication will help.

- **Design tradeoffs:** Shallow spatial branch (2 layers) trades expression modeling capacity for identity disentanglement; Late fusion trades early interaction for representation independence; Difference frames trade sequence richness for motion isolation

- **Failure signatures:** Confusion between similar negative emotions (e.g., repression vs. disgust): Temporal features may not be discriminative enough without spatial grounding; Cross-cultural performance gaps (SAMM underperforms CASME II): Spatial branch may not generalize to varied facial morphologies; Class imbalance sensitivity (negative emotions): Small-sample categories may be overwhelmed by majority classes in late fusion

- **First 3 experiments:**
  1. Reproduce LOSO validation on CASME II with provided hyperparameters (lr=0.0008, batch=32, AdamW) to establish baseline. Compare 3-class vs. 5-class to understand difficulty scaling.
  2. Ablate fusion strategies: Run T-S(early), T-to-S, S-to-T, and T-S(late) on a held-out split. Verify that late fusion outperforms early fusion by the reported margins (~5% ACC on CASME II).
  3. Visualize attention/retention weights on correct vs. incorrect predictions to identify whether failures stem from temporal decay misalignment or spatial localization errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transfer learning or contrastive learning techniques effectively mitigate the generalization bottleneck observed in cross-cultural micro-expression scenarios?
- Basis in paper: [explicit] The Conclusion states that future research should "explore methods such as transfer learning and contrast learning to improve the model's ability to generalise to cross-cultural scenarios."
- Why unresolved: The current model shows a performance drop on the cross-cultural SAMM dataset compared to the mono-cultural CASME II dataset, indicating a bottleneck in adapting to different cultural expression habits.
- What evidence would resolve it: Demonstrating improved accuracy and UF1 scores on diverse datasets (like SAMM) when pre-training on cultural data or utilizing contrastive loss functions to align features across cultural domains.

### Open Question 2
- Question: How can the framework's robustness be improved for under-represented negative emotions in datasets with severe class imbalance, such as SAMM?
- Basis in paper: [explicit] The Conclusion identifies class imbalance as a major unresolved issue, specifically noting that "the number of negative emotion samples is low" and this restricts performance in practical applications.
- Why unresolved: While spatial-temporal fusion helped sensitivity to categories with limited samples, the model still struggles with specific negative emotions (e.g., contempt) in the SAMM dataset compared to positive ones.
- What evidence would resolve it: Implementing specific strategies to handle imbalance (e.g., re-weighting, oversampling) and showing a statistically significant increase in recall for minority classes like "Contempt" or "Fear".

### Open Question 3
- Question: Is the TSFmicro framework applicable to domains beyond standard micro-expression recognition, such as continuous psychological state monitoring or security analysis?
- Basis in paper: [explicit] The Conclusion suggests that "future research could explore the application of TSFmicro to domains other than micro-expression recognition, such as psychological state monitoring or expression analysis in the security domain."
- Why unresolved: The current validation is limited to standard, segmented classification tasks (CASME II, SAMM, CAS(ME)3) and has not been tested on continuous video streams or in-the-wild security scenarios.
- What evidence would resolve it: Successful deployment and evaluation of the model on continuous video datasets for mental health monitoring or real-world surveillance footage.

### Open Question 4
- Question: What are the specific interpretability characteristics of the semantic features learned by the parallel time-space fusion method?
- Basis in paper: [explicit] The Conclusion calls for "further research into... the interpretability of the learned features" to gain further insights and improvements.
- Why unresolved: While Grad-CAM visualizations show the model focuses on relevant facial regions, the specific semantic meaning of the "where-how" relationships in the high-dimensional feature space remains opaque.
- What evidence would resolve it: A study analyzing the latent space features to qualitatively and quantitatively define the semantic attributes captured by the fusion module.

## Limitations

- The framework's effectiveness relies heavily on precise onset-apex frame detection, which the paper does not validate as a separate step
- The shallow 2-layer spatial transformer may lack sufficient capacity to capture complex geometric patterns across diverse facial morphologies
- The paper reports LOSO validation results but does not provide cross-dataset generalization tests or sensitivity analyses for hyperparameter choices

## Confidence

- **High confidence:** The architectural framework (dual-stream network with late fusion) is well-specified and reproducible. The reported performance improvements over baselines on all three datasets are statistically significant and methodologically sound within the paper's experimental design.
- **Medium confidence:** The mechanism explanations for why late fusion outperforms early fusion and why shallow spatial layers prevent identity feature extraction are logically coherent but lack direct empirical validation through ablation studies or attention visualization.
- **Low confidence:** The specific implementation details of the RetNet architecture (hidden dimensions, attention heads, decay schedule) and Conv Stem parameters are underspecified, making exact reproduction challenging without assumptions.

## Next Checks

1. **Onset-apex alignment validation:** Conduct a controlled experiment where manually verified onset-apex pairs are compared against automatically detected pairs to quantify the impact of detection accuracy on final recognition performance.

2. **Cross-dataset generalization test:** Train TSFmicro on CASME II and evaluate directly on SAMM and CAS(ME)3 without fine-tuning to assess whether the shallow spatial branch generalizes across different facial feature distributions.

3. **RetNet vs. Transformer temporal branch ablation:** Replace the RetNet temporal branch with a standard transformer encoder while keeping all other components identical to isolate the contribution of retention decay versus standard attention mechanisms.