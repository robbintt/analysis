---
ver: rpa2
title: Constrained Auto-Regressive Decoding Constrains Generative Retrieval
arxiv_id: '2504.09935'
source_url: https://arxiv.org/abs/2504.09935
tags:
- retrieval
- distribution
- corpus
- relevant
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the theoretical limitations of generative
  retrieval (GR) models that use constrained auto-regressive decoding. The study focuses
  on two key aspects: (1) the impact of constraints imposed by downstream corpora,
  and (2) the use of beam search during generation.'
---

# Constrained Auto-Regressive Decoding Constrains Generative Retrieval

## Quick Facts
- **arXiv ID:** 2504.09935
- **Source URL:** https://arxiv.org/abs/2504.09935
- **Reference count:** 40
- **Primary result:** Both corpus constraints and beam search introduce inherent limitations in generative retrieval, creating trade-offs between precision and recall that depend on the concentration of the relevance distribution.

## Executive Summary
This paper investigates the theoretical limitations of generative retrieval (GR) models that use constrained auto-regressive decoding. The study focuses on two key aspects: (1) the impact of constraints imposed by downstream corpora, and (2) the use of beam search during generation. Through analysis of an idealized Bayes-optimal GR model, the authors demonstrate that applying corpus-specific constraints creates a mismatch between predicted and ground-truth marginal distributions, with error quantified by KL divergence and dependent on the Simpson diversity index. Additionally, beam search using marginal distributions can achieve perfect top-1 precision but suffers from poor recall performance, especially when the relevance distribution is sparse.

## Method Summary
The paper analyzes a Bayes-optimal generative retrieval model under constrained decoding and beam search. For constraints, it measures KL divergence between predicted and ground-truth marginals when the model is applied to a subset of its training corpus. For beam search, it evaluates recall when ranking branches by marginal probability sums in sparse relevance settings. The theoretical analysis uses the Simpson diversity index to quantify the concentration of relevance distributions. Experiments validate these findings on synthetic corpora with controlled relevance distributions and on real-world data (MS MARCO) using external DocID mappings and relevance scores from SLIM++.

## Key Results
- Corpus constraints introduce a mismatch between predicted and ground-truth marginals, with error quantified by KL divergence dependent on Simpson diversity index
- Beam search using marginal probabilities achieves perfect top-1 precision but poor recall when relevance distribution is sparse
- There exists a fundamental trade-off: low concentration helps mitigate constraint errors but worsens beam search recall, while high concentration helps beam search but increases constraint mismatch

## Why This Works (Mechanism)

### Mechanism 1: Future-Constraint Unawareness
- **Claim:** Constrained decoding introduces persistent error in marginal probability estimation because the model cannot anticipate future validity constraints during current token generation.
- **Mechanism:** In a Bayes-optimal GR model, the predicted probability of a token is the sum of probabilities over all possible continuations in the complete corpus. When a downstream corpus constraint is applied, the ground-truth marginal distribution should only sum over valid continuations. Because standard auto-regressive decoding generates token-by-token, it is "unaware" of future constraints, creating a mismatch proportional to the variance of relevant document density (Simpson diversity index).
- **Core assumption:** The model is applied to a downstream corpus that is a subset of the training data, and documents are sampled i.i.d. with probability p.
- **Break condition:** If the downstream corpus is identical to the training corpus or if the relevance distribution is perfectly uniform.

### Mechanism 2: Marginal Distribution Overtaking
- **Claim:** Beam search using marginal probabilities can fail to retrieve relevant documents even when the most relevant document is easily identifiable.
- **Mechanism:** Beam search ranks branches based on the sum of probabilities of all documents within that branch. In sparse relevance settings, a branch containing a single highly relevant document may have a lower total marginal probability than a branch containing many mediocre but slightly probable non-relevant documents, causing "non-relevant branches overtaking relevant ones."
- **Core assumption:** The relevance distribution is sparse with a thick tail.
- **Break condition:** If the "amplification" strategy is used or if relevant documents are densely packed into single branches.

### Mechanism 3: The Concentration Dilemma
- **Claim:** Mitigating the error from constraints conflicts with optimizing beam search recall, creating a design trade-off controlled by the "concentration" of the relevance distribution.
- **Mechanism:** Constraints prefer low concentration (uniform distribution) to reduce variance, while beam search prefers high concentration (sharp peaks) to ensure relevant branches out-weigh non-relevant branches. Optimizing for one worsens the other.
- **Core assumption:** The docID space and relevance distribution can be structured but cannot simultaneously satisfy uniform variance and sharp peaks.
- **Break condition:** If the model has access to the true joint distribution at inference time or if the corpus is extremely small.

## Foundational Learning

- **Concept: Auto-regressive Factorization & Marginalization**
  - **Why needed here:** The paper's core argument relies on the difference between the joint probability of a document (the target) and the marginal probability of a prefix used by beam search.
  - **Quick check question:** Can you explain why maximizing the probability of the next token (marginal) does not guarantee maximizing the probability of the final sequence (joint)?

- **Concept: Simpson Diversity Index**
  - **Why needed here:** This metric quantifies the "concentration" of the relevance distribution and is the mathematical key to understanding why the constraint error occurs.
  - **Quick check question:** If the Simpson diversity index of a branch is high, does that imply the relevant documents are uniformly spread out or clustered? (Answer: Clustered/Concentrated).

- **Concept: Bayes-Optimal Modeling**
  - **Why needed here:** The paper isolates structural flaws by assuming a "perfect" model, distinguishing model training errors from inference/decoding errors.
  - **Quick check question:** If a Bayes-optimal model still makes errors on a downstream task, is the problem in the training data or the inference strategy? (Answer: Inference strategy/constraints).

## Architecture Onboarding

- **Component map:** Query Input → Model Logits → Constraint Masking (Trie Lookup) → Renormalization → Branch Selection (Beam Search) → Top-k DocIDs

- **Critical path:** Query Input → Model Logits → Constraint Masking (Trie Lookup) → Renormalization → Branch Selection (Beam Search) → Top-k DocIDs

- **Design tradeoffs:**
  - **Aggregation (Clustering):** Grouping semantic docs into the same branch helps Beam Search recall but increases the risk of "Constraint Mismatch" (high variance) and wastes code space (redundancy).
  - **Amplification:** Training only on "max-relevant" successors improves beam search but requires massive data filtering and risks losing "soft" relevance signals.

- **Failure signatures:**
  - **High Precision, Low Recall:** You retrieve the best document but miss the other 9 in the top-10. (Sign of Mechanism 2).
  - **Performance Drop on Small Corpora:** The model performs worse when constrained to a small subset of a large index due to sampling variance. (Sign of Mechanism 1).

- **First 3 experiments:**
  1. **Synthetic Validation:** Create a synthetic corpus of random strings. Measure KL divergence between ground-truth marginals and model-predicted marginals under varying "sparsity" levels to validate Eq. 11.
  2. **Recall vs. Beam Width:** On a real dataset (e.g., MS MARCO), plot Recall@k vs. Precision@1. Specifically, identify "branch overtaking" by checking if the ground-truth document's prefix survives the first decoding step.
  3. **Constraint Ablation:** Run the model on a fixed corpus vs. a randomly sampled subset. Observe if performance degrades as the subset size decreases (confirming the constraint error bound).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do corpus constraints and beam search jointly influence the error bounds in generative retrieval, given that the paper analyzes them in isolation?
- **Basis in paper:** [explicit] Section 8 states, "We have not studied how the two factors affect each other when using constrained beam search."
- **Why unresolved:** The authors decoupled the analysis of constraints and beam search to establish theoretical baselines separately.
- **What evidence would resolve it:** A theoretical derivation or empirical study demonstrating whether the errors from constraints and beam search compound linearly or non-linearly when applied simultaneously.

### Open Question 2
- **Question:** How can the theoretical trade-offs regarding the concentration of relevance distributions be utilized to optimize training properties in corpus-specific generative retrieval models?
- **Basis in paper:** [explicit] The conclusion proposes future work to "study how these results can be used for analyzing training properties of corpus-specific GR models."
- **Why unresolved:** The current work establishes theoretical inference limits but does not define how to incorporate these insights into the training loss or data sampling.
- **What evidence would resolve it:** A training framework that successfully regularizes the model based on the Simpson diversity index or tail distribution thickness to improve generalization.

### Open Question 3
- **Question:** Can incorporating learnable decoding strategies during the training of a differentiable search index overcome the recall limitations inherent in standard beam search?
- **Basis in paper:** [explicit] The conclusion suggests, "Incorporating learnable decoding strategies during the training of a differentiable search index may also be of interest in this field."
- **Why unresolved:** The paper proves that standard beam search using marginal distributions is theoretically prone to pruning relevant docIDs, but it does not propose a specific alternative decoding mechanism.
- **What evidence would resolve it:** A generative retrieval model trained with a decoding-aware objective that achieves higher top-k recall than standard constrained beam search on the same corpus.

## Limitations

- The theoretical analysis assumes a Bayes-optimal model with access to the true relevance distribution, which may not reflect practical scenarios where models are trained on finite data.
- The abstraction of DocID generation as random sampling from a uniform distribution is a simplification that may not capture the structure of real-world DocID spaces.
- The analysis of beam search limitations is based on a worst-case synthetic setting with a single relevant document; performance in more realistic multi-relevant document scenarios may differ.

## Confidence

- **High Confidence:** The core argument that constrained auto-regressive decoding introduces a mismatch between predicted and ground-truth marginal distributions is well-supported by theoretical derivation and synthetic experiments.
- **Medium Confidence:** The claim that beam search using marginal probabilities can achieve high Precision@1 but poor Recall is validated in synthetic experiments, but real-world performance requires further validation.
- **Low Confidence:** The proposed solutions of "Aggregation" and "Amplification" are discussed theoretically, but their practical effectiveness and trade-offs are not thoroughly evaluated.

## Next Checks

1. **Empirical Validation of "Overtaking":** Design an experiment on a real dataset (e.g., MS MARCO) where you can artificially create a sparse relevance distribution. Track the survival of the ground-truth document's prefix at each decoding step to directly observe if non-relevant branches overtake relevant ones.

2. **Evaluate Aggregation vs. Amplification Trade-offs:** Implement both the "Aggregation" (clustering) and "Amplification" (max-relevant filtering) strategies on a standard GR model. Quantify their impact on both the constraint error (KL divergence) and beam search recall/precision. Measure the increase in DocID space redundancy caused by aggregation.

3. **Analyze Multi-Step Impact:** Extend the theoretical analysis and experiments to measure the cumulative effect of constraint mismatch and beam search errors over multiple decoding steps. Investigate if the errors compound or if there are mitigation strategies (e.g., lookahead, re-ranking) that can be applied.