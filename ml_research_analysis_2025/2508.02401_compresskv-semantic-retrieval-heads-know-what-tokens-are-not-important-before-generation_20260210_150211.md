---
ver: rpa2
title: 'CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before
  Generation'
arxiv_id: '2508.02401'
source_url: https://arxiv.org/abs/2508.02401
tags:
- cache
- heads
- attention
- tokens
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CompressKV introduces a KV cache compression framework for GQA-based\
  \ LLMs that improves long-context performance by identifying and leveraging Semantic\
  \ Retrieval Heads\u2014attention heads that retrieve semantically important tokens\
  \ and their contexts\u2014rather than relying on all heads equally. It selects important\
  \ tokens using these specialized heads and allocates cache budgets across layers\
  \ based on layer-specific compression errors measured offline, avoiding additional\
  \ online computation."
---

# CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation

## Quick Facts
- arXiv ID: 2508.02401
- Source URL: https://arxiv.org/abs/2508.02401
- Reference count: 35
- Primary result: Retains over 97% accuracy with 3% KV cache on LongBench question-answering tasks

## Executive Summary
CompressKV introduces a KV cache compression framework that leverages specialized attention heads called Semantic Retrieval Heads to identify semantically important tokens and contexts for long-context language models. The framework improves long-context performance by selectively retaining important tokens based on these heads rather than treating all attention heads equally. CompressKV measures layer-specific compression errors offline to allocate cache budgets across layers without adding online computation overhead. The method consistently outperforms state-of-the-art baselines on LongBench and Needle-in-a-Haystack benchmarks, demonstrating significant efficiency gains while maintaining high accuracy.

## Method Summary
CompressKV operates by first identifying Semantic Retrieval Heads through attention analysis, which are specialized attention heads that retrieve semantically important tokens and their contexts. The framework then uses these heads to select important tokens for retention while discarding less critical ones. Cache budgets are allocated across different layers based on offline measurements of layer-specific compression errors, ensuring optimal resource utilization without introducing additional computational overhead during inference. This approach enables effective long-context processing by focusing computational resources on the most semantically relevant information.

## Key Results
- Retains over 97% accuracy with only 3% of KV cache on LongBench question-answering tasks
- Achieves 90% accuracy with just 0.07% KV storage on Needle-in-a-Haystack
- Consistently outperforms state-of-the-art KV cache compression baselines

## Why This Works (Mechanism)
CompressKV works by recognizing that not all attention heads contribute equally to semantic understanding in long-context scenarios. Semantic Retrieval Heads are identified as those that consistently retrieve important contextual information across different positions in the input sequence. By leveraging these specialized heads to guide token selection, the framework can prioritize semantically relevant information while discarding noise. The offline measurement of compression errors allows for intelligent budget allocation across layers, ensuring that critical information at different depths of the model's processing is preserved appropriately.

## Foundational Learning
- **Attention Heads**: Individual components in transformer models that attend to different parts of the input; needed to understand how information is processed at different levels, quick check: verify model has multiple attention heads per layer
- **KV Cache**: Stores key and value vectors for previously processed tokens; needed for efficient autoregressive generation, quick check: confirm cache stores K and V matrices
- **Semantic Retrieval**: Process of identifying and extracting meaningful information from context; needed to distinguish important from unimportant tokens, quick check: validate retrieval heads capture task-relevant information
- **Layer-wise Processing**: Transformers process information through multiple stacked layers; needed to understand how information transforms at different depths, quick check: confirm model has multiple transformer layers
- **Compression Error Measurement**: Quantifying information loss when reducing cache size; needed to optimize budget allocation, quick check: verify error metrics correlate with downstream performance

## Architecture Onboarding
- **Component Map**: Input Sequence -> Attention Heads -> Semantic Retrieval Head Identification -> Token Selection -> KV Cache Compression -> Output Generation
- **Critical Path**: The framework identifies Semantic Retrieval Heads during an offline phase, uses these heads to guide token selection during inference, and allocates cache budgets based on measured compression errors to maintain accuracy while reducing memory footprint.
- **Design Tradeoffs**: Prioritizes offline computation for error measurement to avoid online overhead, accepts some accuracy loss for significant memory savings, focuses on QA tasks where semantic retrieval is most critical
- **Failure Signatures**: Performance degradation when attention patterns are highly dynamic, reduced effectiveness on non-QA tasks where semantic importance differs, potential issues with model architectures that don't have clear Semantic Retrieval Heads
- **First Experiments**: 1) Test on different question-answering datasets to verify baseline effectiveness, 2) Measure compression error correlation with actual performance degradation, 3) Evaluate token selection quality by examining which tokens are retained vs discarded

## Open Questions the Paper Calls Out
The paper acknowledges uncertainty about how well the Semantic Retrieval Head identification generalizes across different model architectures beyond the tested ones. It also notes that the effectiveness of the approach for tasks beyond question-answering, such as code generation or creative writing, remains unexplored. The framework's performance with highly dynamic attention patterns that change significantly during generation is another area of uncertainty.

## Limitations
- Results primarily validated on question-answering tasks, limiting generalization to other long-context applications
- Semantic Retrieval Head identification may not transfer well to different model architectures
- Offline error measurement assumes stable attention patterns that may not hold for dynamic reasoning tasks
- Performance claims may mask degradation in more complex reasoning scenarios not captured by evaluation metrics

## Confidence
- **Technical Framework Design**: High - methodology is clearly defined and reproducible
- **Performance Claims**: Medium - well-supported by experiments but limited to specific benchmarks
- **Generalization Claims**: Low - not extensively explored across different architectures and tasks

## Next Checks
1. Test CompressKV on non-QA long-context tasks like code completion or document summarization to assess generalization
2. Evaluate performance with dynamic context updates where attention patterns shift significantly during generation
3. Compare Semantic Retrieval Head identification robustness across different model families (e.g., LLaMA, Mistral, Claude) to validate architecture independence