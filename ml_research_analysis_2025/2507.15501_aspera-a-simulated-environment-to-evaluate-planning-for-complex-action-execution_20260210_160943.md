---
ver: rpa2
title: 'ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution'
arxiv_id: '2507.15501'
source_url: https://arxiv.org/abs/2507.15501
tags:
- assistant
- user
- generation
- which
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASPERA is a framework for evaluating large language models' ability
  to execute complex digital assistant tasks by generating programs from custom assistant
  libraries. It addresses data availability and evaluation robustness challenges through
  an interactive human-LLM system that generates tasks, state initialization programs,
  and correctness evaluation programs.
---

# ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution

## Quick Facts
- **arXiv ID:** 2507.15501
- **Source URL:** https://arxiv.org/abs/2507.15501
- **Reference count:** 40
- **Primary result:** Program generation grounded in custom assistant libraries is significantly more challenging for LLMs compared to dependency-free code generation, with top models achieving only 45-80% task success even when given full library documentation.

## Executive Summary
ASPERA is a framework for evaluating large language models' ability to execute complex digital assistant tasks by generating programs from custom assistant libraries. It addresses data availability and evaluation robustness challenges through an interactive human-LLM system that generates tasks, state initialization programs, and correctness evaluation programs. The framework includes Asper-Bench, a dataset of 250 challenging tasks. Evaluation shows that program generation grounded in custom assistant libraries is significantly more challenging for LLMs compared to dependency-free code generation, with top models achieving only 45-80% task success even when given full library documentation. Selecting relevant primitives from large libraries adds further difficulty, highlighting the gap between current LLM capabilities and practical digital assistant requirements.

## Method Summary
ASPERA evaluates LLMs through a three-component pipeline: Action Execution Programs (AEPs) that implement task logic using custom assistant primitives, State Initialization Programs (SIPs) that set up the environment state, and Evaluation Programs (EPs) that verify goal completion. The framework uses interactive human-LLM co-generation to create diverse tasks and includes Asper-Bench with 250 tasks. Two evaluation settings are compared: CCK (Complete Code Knowledge) where models have access to full library documentation, and PS (Primitive Selection) where models must first select relevant primitives from the library. Tasks are automatically generated from focus instructions or developer-authored queries, with state and evaluation programs generated by LLMs and reviewed by developers.

## Key Results
- Program generation grounded in custom assistant libraries is significantly more challenging for LLMs compared to dependency-free code generation
- Top models achieve only 45-80% task success even when given full library documentation (CCK setting)
- Selecting relevant primitives from large libraries adds further difficulty, with PS setting achieving only 11-28% success
- Performance degrades disproportionately with increasing program depth, cyclomatic complexity, and primitive composition requirements
- GPT-3.5-turbo shows 34% execution errors while frontier models like o1 have 62% task completion errors and 28% handback control errors

## Why This Works (Mechanism)

### Mechanism 1: Documentation Grounding Reduces Hallucination in Program Synthesis
- Claim: Providing complete assistant library documentation during generation improves LLM task success compared to forcing models to first select relevant primitives.
- Mechanism: When LLMs have direct access to all primitive signatures, types, and docstrings, they can perform grounded composition without inferring dependencies from incomplete information, reducing attribute hallucination and type mismatch errors.
- Core assumption: The documentation accurately reflects runtime behavior and the model can parse structured API specifications reliably.
- Evidence anchors:
  - [abstract]: "program generation grounded in custom assistant libraries is a significant challenge to LLMs compared to dependency-free code generation"
  - [section 5]: CCK setting (full documentation) achieves 45-80% task success vs. PS setting (primitive selection first) achieving only 11-28%
  - [corpus]: Related work on tool documentation (Hsieh et al. 2023, cited in paper) suggests documentation enables zero-shot tool use; however, corpus papers do not directly replicate ASPERA's custom library setting
- Break condition: If documentation is incomplete, ambiguous, or exceeds context window, grounding fails and performance degrades to PS-level or worse

### Mechanism 2: Compositional Complexity Correlates with Performance Degradation
- Claim: Tasks requiring deeper abstract syntax trees, higher cyclomatic complexity, and more primitive composition cause disproportionate success drops for less capable models.
- Mechanism: As program depth and branching increase, models must maintain more state, track more dependencies, and perform multi-step reasoningâ€”capabilities that scale with model size and inference-time compute.
- Core assumption: Success rates on Asper-Bench tasks reflect generalizable compositional reasoning ability, not just memorization of similar patterns.
- Evidence anchors:
  - [section 3, Figure 5]: Task success drops from ~80% to ~30% as cyclomatic complexity increases from 0-2 to 11-19
  - [section 6.3]: "o1 can successfully complete a much larger proportion of tasks which require generating complex programs compared to GPT-4o"
  - [corpus]: Corpus papers on embodied planning (SDA-PLANNER, Grounding Language Models) show similar complexity-performance tradeoffs but in different domains (robotics, mobile UI)
- Break condition: If tasks can be decomposed into cached subprograms or solved via simple pattern matching, complexity effects diminish

### Mechanism 3: Human-LLM Co-Generation Ensures Task Diversity and Evaluation Robustness
- Claim: Interactive generation with developer oversight produces more diverse, challenging tasks than fully automated synthesis while enabling executable correctness checks.
- Mechanism: Developers provide focus instructions and filter outputs, counteracting LLM repetition bias; LLMs generate state initialization and evaluation programs that would be expensive to hand-craft, enabling goal-oriented verification beyond database diffs.
- Core assumption: Developers can identify low-quality or redundant tasks, and LLM-generated validation programs correctly encode success criteria.
- Evidence anchors:
  - [section 2.3.1]: "To address this issue, we employ three techniques... condition task generation on interactively specified focus instructions"
  - [section 2.3.3]: "EPs thus implement goal-oriented evaluation even though the environment state is implicit"
  - [corpus]: ProPerSim and SimulatorArena corpus papers also use simulation for assistant evaluation but do not include ASPERA's human-in-the-loop program generation
- Break condition: If human oversight is insufficient or biased, dataset quality suffers; if evaluation programs have bugs, correctness measurements become unreliable

## Foundational Learning

- **Semantic parsing for code generation**
  - Why needed here: ASPERA requires translating natural language queries into executable Python programs using custom APIs
  - Quick check question: Given the query "Schedule lunch with my team tomorrow at noon," can you identify which primitives from Table 1 would be needed?

- **Tool-augmented LLM evaluation**
  - Why needed here: Understanding how to measure functional correctness beyond syntactic validity is essential for interpreting Asper-Bench results
  - Quick check question: Why does the EP need to capture state before AEP execution (Figure 2, line 14)?

- **Compositional generalization in sequence models**
  - Why needed here: The performance gap between simple and complex tasks reveals limitations in how models compose learned patterns
  - Quick check question: How might an AST depth of 10 vs. 6 change the number of primitive compositions required?

## Architecture Onboarding

- **Component map**: AEP (Action Execution Program) -> SIP (State Initialization Program) -> EP (Evaluation Program) -> Execution Environment

- **Critical path**:
  1. Developer defines focus instruction or authors query
  2. LLM generates AEP with docstring containing natural language query
  3. LLM generates SIP to initialize databases (e.g., create employees, events)
  4. LLM generates EP with assertions validating goal completion
  5. Human reviews and edits generated programs
  6. Evaluator runs candidate AEP against SIP+EP pairs

- **Design tradeoffs**:
  - Simulation fidelity vs. coverage: Current library covers calendar/room booking but not messaging, files, or multi-app workflows
  - Documentation completeness vs. selection overhead: CCK provides full docs (2,422 words) but may be impractical for large-scale deployment; PS requires selection but reduces context
  - Single-turn vs. multi-turn: Current evaluation assumes single complex query; real assistants must handle clarification dialogs

- **Failure signatures**:
  - Execution errors (syntax, type mismatches): More common in smaller models (GPT-3.5-turbo: 34% of errors)
  - Task completion errors (wrong action, logic bugs): Dominant in frontier models (o1: 62% of errors)
  - Handback control errors (unnecessary RequiresUserInput): Suggest over-caution or misinterpreting policy (o1: 28% of errors)
  - Primitive selection failures: Low recall (0.41 for add_event) in PS setting

- **First 3 experiments**:
  1. Reproduce CCK vs. PS gap on a 20-task subset to validate evaluation pipeline
  2. Analyze error distribution for a specific model (e.g., GPT-4o) by running Table 6's debugging protocol on failed tasks
  3. Test documentation compression (e.g., module summaries vs. full docstrings) to measure robustness to context reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated correctness checks via generated evaluation programs, introducing potential false positives/negatives if programs contain bugs
- Current assistant library covers only 7 database types and excludes messaging, file systems, and cross-application workflows critical for real-world assistants
- Performance differences between CCK and PS settings may be inflated by context window effects rather than fundamental primitive selection ability

## Confidence
- **High confidence** in documented performance gaps between models and general finding that program generation grounded in custom libraries remains challenging for current LLMs
- **Medium confidence** in mechanism explanations for why documentation grounding helps and claims about compositional complexity effects
- **Low confidence** in generalizability to production assistant settings due to simplified library scope and single-turn query assumption

## Next Checks
1. **Documentation Format Ablation**: Systematically reduce documentation size and structure (module summaries, signature-only, no docstrings) while measuring performance in CCK mode to isolate the contribution of documentation completeness versus context window effects.

2. **Error Analysis Replication**: Manually verify a random sample of 50 failed tasks per model using the debugging protocol from Table 6 to confirm automated evaluation program accuracy and classify true vs. false negatives.

3. **Cross-Library Generalization**: Port 20% of Asper-Bench tasks to a different custom library with overlapping functionality but different API signatures, then measure whether performance patterns replicate or indicate task-specific memorization.