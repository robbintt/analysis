---
ver: rpa2
title: Data-driven Mesoscale Weather Forecasting Combining Swin-Unet and Diffusion
  Models
arxiv_id: '2503.19354'
source_url: https://arxiv.org/abs/2503.19354
tags:
- usion
- predictor
- mesoscale
- corrector
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel architecture for mesoscale weather
  forecasting that combines a deterministic Swin-Unet model with a diffusion model.
  The architecture addresses the limitation of data-driven weather prediction models
  in representing fine-scale features essential for mesoscale predictions, such as
  heavy rainfall events.
---

# Data-driven Mesoscale Weather Forecasting Combining Swin-Unet and Diffusion Models

## Quick Facts
- arXiv ID: 2503.19354
- Source URL: https://arxiv.org/abs/2503.19354
- Reference count: 8
- Primary result: Proposed architecture combining Swin-Unet with diffusion model improves FSS by 24% for heavy rainfall prediction while maintaining flexibility for model updates.

## Executive Summary
This study introduces a novel architecture for mesoscale weather forecasting that combines a deterministic Swin-Unet model with a diffusion model. The key innovation is training these components independently, allowing the diffusion model to remain unchanged when the deterministic model is updated. The diffusion model probabilistically adjusts residuals to enhance high spatial frequency structures while preserving the lower-frequency structures predicted by the deterministic model. Evaluated using Fractions Skill Score and power spectral analysis, the approach demonstrates improved accuracy compared to predictions without the diffusion model, particularly for strong rainfall events.

## Method Summary
The proposed architecture trains a Swin-Unet-based deterministic Predictor and a diffusion-based probabilistic Corrector independently. The Predictor uses MSE loss and outputs 6-hour ahead forecasts. The Corrector is trained on unpaired ground truth data to learn the distribution of meteorological fields. At inference, Gaussian noise is added to the Predictor's output at a level determined by power spectral density analysis, then the diffusion model denoises this input to generate probabilistic corrections that enhance fine-scale features while preserving the Predictor's low-frequency structure.

## Key Results
- 24% improvement in FSS at 100km spatial scale for 20-30 kg m⁻² h⁻¹ precipitation thresholds
- Power spectral analysis shows better agreement with ground truth for structures finer than 500km
- Unpaired training approach allows the Corrector to remain unchanged when the Predictor is updated
- RMSE improvements limited by preservation of Predictor's low-frequency positional errors

## Why This Works (Mechanism)

### Mechanism 1
- Frequency-domain noise injection selectively preserves low-frequency structures while enabling diffusion-based regeneration of high-frequency details. The noise level σ is computed from power spectral density at the wavenumber where the Predictor's spectrum drops below 0.1 of the true spectrum. Gaussian noise at this level corrupts high-frequency components but leaves low-frequency structures intact, allowing the reverse diffusion process to regenerate plausible high-frequency details.

### Mechanism 2
- Unpaired training of the diffusion model decouples it from the deterministic Predictor, enabling modular updates without joint retraining. The diffusion model learns the unconditional probabilistic distribution of meteorological fields from ground truth alone. At inference, the Predictor's output with added noise serves as a conditioning signal through the denoising process, analogous to downscaling tasks where low-resolution inputs guide generation.

### Mechanism 3
- Diffusion-based correction improves precipitation FSS for high-intensity thresholds by restoring attenuated fine-scale variance that MSE-trained models suppress. MSE loss penalizes squared deviations, encouraging mean-field predictions that blur spatial details. The diffusion model, trained to match the true PSD, reintroduces variance at scales where the Predictor is under-powered, particularly benefiting variables with strong fine-scale structure and extreme precipitation intensities.

## Foundational Learning

- **Swin Transformer with shifted windows**: Why needed - The Predictor uses Swin-Unet, which replaces CNN convolutions with Swin Transformer blocks to capture both local and multi-scale spatial relationships critical for mesoscale dynamics. Quick check - How does the shifted window mechanism enable information exchange between adjacent local windows without global attention?

- **Diffusion models via stochastic differential equations (SDEs)**: Why needed - The Corrector uses score-based SDEs. The forward SDE gradually adds noise; the reverse SDE learns to denoise by estimating the score function. Quick check - In score-based diffusion, what does the neural network actually learn to predict—the clean data, the noise, or the score?

- **Power spectral density (PSD) and spatial scales**: Why needed - The paper's core innovation links PSD analysis to noise-level selection. PSD decomposes spatial fields into variance contributions at different wavelengths. Quick check - If a model's PSD shows reduced power at wavelengths finer than 500 km, what does this imply about its ability to represent mesoscale convective systems?

## Architecture Onboarding

- **Component map**: Surface/Upper-air variables (45 channels) -> Swin-Unet Predictor -> 6-hour prediction -> Add noise (σ=2.148) -> Diffusion U-Net Corrector (20 steps) -> 16 ensemble members -> PMM for precipitation / Mean for other variables

- **Critical path**: Normalize inputs → Predictor forward pass → 6-hour prediction → Compute σ-based noise → Add to Predictor output → Reverse diffusion (20 steps) → Corrected ensemble member → Repeat for 16 members → PMM for precipitation, mean for others

- **Design tradeoffs**: Fixed σ across all variables simplifies implementation but limits RMSE improvement; unpaired vs. paired diffusion training enables modular updates but may reduce correction specificity; ensemble size of 16 provides probabilistic spread but multiplies inference cost

- **Failure signatures**: Spatial smoothing in Predictor output if FSS improvement is minimal; boundary artifacts at y-direction boundaries; RMSE degradation in upper-air variables for variables with weak fine-scale structure

- **First 3 experiments**: 1) Baseline reproducibility: Train Predictor alone, evaluate RMSE and PSD against ground truth 2) Noise level sensitivity: Vary PSD drop threshold from 0.05 to 0.3 and measure impact on PSD restoration, FSS, and RMSE 3) Ablation on paired vs. unpaired training: Compare FSS and RMSE to paired Corrector following StormCast methodology

## Open Questions the Paper Calls Out

- **Generalization to observational data**: How does the performance change when validated against actual observational data rather than reanalysis datasets? The authors state that further validation using actual observational data remains a crucial area for future work.

- **Variable-specific noise optimization**: Can optimizing the noise level (σ) independently for each meteorological variable improve the trade-off between high-frequency enhancement and low-frequency preservation? The paper suggests adjusting the noise level for each variable could help strike a balance between improving prediction accuracy and preserving the Predictor's results.

- **Positional error refinement**: Can the Corrector mechanism be refined to reduce positional errors in heavy rainfall predictions without compromising the probabilistic generation of fine-scale details? The authors acknowledge that the accuracy of the predicted location remains limited due to preservation of Predictor's low-frequency positional errors.

## Limitations

- **Data access and normalization**: Relies on proprietary RCDSJRA-55 dataset with specific 512×768 crop; exact normalization parameters for all 45 input channels are not provided

- **Computational overhead**: Ensemble approach with 16 members and diffusion-based correction multiplies inference cost without runtime comparisons or efficiency metrics

- **Generalization scope**: Effectiveness for other spatiotemporal forecasting tasks beyond mesoscale weather prediction remains untested

## Confidence

- **High confidence**: Core mechanism of using diffusion models to restore high-frequency details in MSE-trained predictions is well-supported by FSS improvements (24% at 100km scale) and PSD analysis

- **Medium confidence**: Unpaired training approach provides flexibility as claimed, but tradeoff with correction specificity versus paired methods is not quantified

- **Low confidence**: Universal applicability of fixed σ=2.148 across all variables is questionable, given RMSE degrades for some upper-air variables

## Next Checks

1. **Normalization sensitivity**: Reproduce Predictor training with varying normalization schemes (per-channel vs. global) to determine impact on spectral attenuation and Corrector performance

2. **Per-variable σ optimization**: Implement variable-specific noise levels based on individual PSD profiles and measure tradeoff between FSS improvement and RMSE degradation

3. **Paired vs. unpaired correction comparison**: Train paired Corrector using Predictor's outputs as conditioning and compare FSS, RMSE, and spectral fidelity to quantify flexibility-performance tradeoff