---
ver: rpa2
title: 'AMRG: Extend Vision Language Models for Automatic Mammography Report Generation'
arxiv_id: '2508.09225'
source_url: https://arxiv.org/abs/2508.09225
tags:
- report
- generation
- mammography
- clinical
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMRG, the first end-to-end framework for
  automatic mammography report generation using vision-language models. The method
  leverages MedGemma-4B-it, a domain-specialized VLM, and applies parameter-efficient
  fine-tuning via LoRA adapters to adapt it to mammography report generation.
---

# AMRG: Extend Vision Language Models for Automatic Mammography Report Generation

## Quick Facts
- arXiv ID: 2508.09225
- Source URL: https://arxiv.org/abs/2508.09225
- Authors: Nak-Jun Sung; Donghyun Lee; Bo Hwa Choi; Chae Jung Park
- Reference count: 40
- Primary result: First end-to-end framework for automatic mammography report generation using MedGemma-4B-it VLM with LoRA fine-tuning

## Executive Summary
This paper introduces AMRG, the first end-to-end framework for automatic mammography report generation using vision-language models. The method leverages MedGemma-4B-it, a domain-specialized VLM, and applies parameter-efficient fine-tuning via LoRA adapters to adapt it to mammography report generation. The model is trained and evaluated on the DMID dataset, which contains paired high-resolution mammograms and diagnostic reports. AMRG achieves strong performance with ROUGE-L of 0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582. Qualitative analysis shows improved diagnostic consistency and reduced hallucinations compared to other models.

## Method Summary
AMRG fine-tunes MedGemma-4B-it, a domain-specialized VLM, using LoRA adapters for efficient adaptation to mammography report generation. The approach applies a multi-stage preprocessing pipeline to standardize breast orientation and enhance lesion visibility. Training uses causal language modeling loss with teacher forcing, where LoRA parameters, embedding tokens, and output head are trained while the backbone remains frozen. The method is evaluated on the DMID dataset (510 cases) using both NLP metrics (ROUGE, METEOR, CIDEr) and clinical metrics (BI-RADS accuracy).

## Key Results
- AMRG achieves ROUGE-L of 0.5691, METEOR of 0.6152, and CIDEr of 0.5818 on DMID test set
- BI-RADS accuracy reaches 0.5582, demonstrating clinical alignment capability
- Domain-specialized MedGemma-4B-it outperforms generalist Qwen2.5-VL-7B across six of nine evaluation metrics
- LoRA rank r=32 is optimal for DMID's 407 training cases; r=64 causes overfitting

## Why This Works (Mechanism)

### Mechanism 1
Domain-specialized pretraining enables superior clinical alignment over raw model scale. MedGemma-4B-it is pre-trained on diverse medical image-text pairs (radiology, dermatology, pathology, ophthalmology), encoding clinical terminology and visual patterns before mammography-specific fine-tuning. This prior knowledge reduces the learning burden during PEFT, allowing the model to preserve fine-grained lesion descriptors (e.g., "spiculated mass," "architectural distortion") that generalist VLMs often omit or misclassify. Core assumption: Pretrained medical knowledge transfers across imaging modalities within radiology-adjacent domains. Evidence: MedGemma-4B demonstrates superior performance across six of nine evaluation metrics, reinforcing that architectural alignment with clinical priors is more impactful than sheer model size. Break condition: If target modality has fundamentally different visual semantics without modality-specific pretraining data, transfer benefits may degrade.

### Mechanism 2
LoRA adapter capacity must be tuned to dataset size to avoid overfitting on small corpora. LoRA introduces low-rank updates (ΔW = AB) to linear layers. The rank (r) controls expressiveness; scaling factor (α) modulates update magnitude. On DMID (407 training cases), r=32 balances capacity and generalization, while r=64 degrades performance across all metrics, indicating memorization of training patterns rather than generalization. Core assumption: Overfitting manifests as performance degradation on held-out validation/test sets when adapter capacity exceeds data complexity. Evidence: Increasing the rank to r = 64 leads to degraded performance despite higher representational capacity, suggesting that larger LoRA modules may induce overfitting on relatively small datasets like DMID. Break condition: If dataset scales to thousands of cases, higher ranks may become beneficial; optimal r likely scales with data volume.

### Mechanism 3
Multi-stage preprocessing standardizes anatomical alignment and improves lesion visibility. The pipeline applies (1) Otsu's thresholding for breast region isolation, (2) ROI cropping to remove blank margins, (3) horizontal flipping for left-right orientation consistency, and (4) CLAHE in LAB color space for local contrast enhancement. This reduces irrelevant background noise and enforces anatomical consistency, enabling the vision encoder to focus on clinically relevant regions. Core assumption: Standardized orientation and enhanced contrast reduce variance in the input distribution, improving feature extraction. Evidence: Together, these steps yield a robust and uniform image representation suitable for instruction-tuned report generation. Break condition: If preprocessing removes diagnostically relevant features (e.g., skin thickening at breast periphery during aggressive cropping), performance may degrade.

## Foundational Learning

- **Vision-Language Models (VLMs)**: AMRG is built on MedGemma-4B-it, a multimodal transformer that jointly processes images and text. Understanding how visual encoders map to language decoders is essential for debugging generation failures. Quick check: Can you explain how a vision encoder's output embeddings are consumed by a language decoder in an autoregressive generation loop?

- **Low-Rank Adaptation (LoRA)**: The entire AMRG adaptation strategy relies on LoRA for efficient fine-tuning. Understanding rank, scaling factor, and where adapters are inserted is critical for reproducing results or extending to new datasets. Quick check: Given a weight matrix W ∈ R^d×k, how does LoRA modify it during training, and which parameters are frozen?

- **BI-RADS Scoring System**: Clinical evaluation uses BI-RADS accuracy as a key metric. The model must generate reports that map to correct BI-RADS categories (0-5), and understanding this taxonomy is necessary for interpreting clinical correctness. Quick check: What is the clinical difference between BI-RADS 3 and BI-RADS 4a, and why might a model confuse them?

## Architecture Onboarding

- **Component map**: MedGemma-4B-it (SigLIP vision encoder + clinical language model) -> LoRA adapters (all linear layers) -> Preprocessing pipeline (Otsu threshold → ROI crop → resize → flip → CLAHE) -> Training (CLM loss with teacher forcing)

- **Critical path**: Load MedGemma-4B-it with pretrained weights → Insert LoRA adapters (target: all linear layers) → Apply preprocessing pipeline to all mammograms → Format input: image + instruction prompt → model → Compute CLM loss on generated report tokens → Backprop through LoRA parameters only

- **Design tradeoffs**: r=32 vs. r=64: Moderate rank prevents overfitting on DMID's 407 training cases; higher rank risks memorization. Domain VLM vs. Generalist VLM: MedGemma-4B outperforms Qwen2.5-VL-7B on clinical metrics despite being smaller; domain pretraining > scale. Preprocessing aggressiveness: CLAHE enhances lesions but may introduce artifacts if clip limit is too high.

- **Failure signatures**: Low BI-RADS accuracy with high NLP scores: Model generates fluent text but lacks clinical grounding (observed in Qwen2.5-VL-7B). Hallucinated findings: Model mentions entities not in ground truth (e.g., "calcified lymph node"); indicates weak visual-text alignment. Generic terminology: Outputs like "large irregular soft opacity" instead of specific descriptors suggest underfitting or insufficient domain knowledge.

- **First 3 experiments**: Baseline reproduction: Fine-tune MedGemma-4B with LoRA (r=32, α=16) on DMID; verify ROUGE-L ≥0.56 and BI-RADS accuracy ≥0.55. LoRA ablation: Sweep r ∈ {16, 32, 64} and α ∈ {8, 16}; confirm r=64 degrades performance on this dataset size. Backbone comparison: Fine-tune Qwen2.5-VL-7B and Phi-3.5-VL under identical LoRA settings; quantify domain pretraining benefit via METEOR and BI-RADS accuracy deltas.

## Open Questions the Paper Calls Out

### Open Question 1
Can a mammography-specific evaluation framework combining standard NLP metrics with lesion-level agreement analysis better quantify clinical correctness than current surface-level metrics? Basis: The authors state they will develop a mammography-specific evaluation framework that combines standard NLP metrics with lesion-level agreement analysis. This is unresolved because current standard metrics (BLEU, ROUGE) fail to fully reflect clinical correctness or nuanced reasoning, relying instead on surface-level similarity. Evidence to resolve: Successful creation and validation of an evaluation protocol that correlates higher with expert radiologist assessment than existing NLP metrics.

### Open Question 2
To what extent can fact-aware decoding or lesion-aware prompting strategies reduce hallucinations in generated mammography reports? Basis: The authors note they will explore fact-aware decoding and prompt refinement strategies—such as lesion-aware prompting inspired by PromptMRG—to reduce hallucinations and improve factual alignment. This is unresolved because the current AMRG model occasionally exhibits hallucinations and unsupported statements, which pose potential patient safety concerns. Evidence to resolve: A comparative study showing a statistically significant reduction in hallucination rates using these specific decoding or prompting techniques compared to the standard PEFT approach.

### Open Question 3
Does scaling the training data to a multi-institutional dataset mitigate the overfitting observed in high-capacity LoRA configurations (e.g., rank=64)? Basis: Section 5.2 notes that performance degraded at rank=64 due to overfitting on the small DMID dataset. Section 5.3 proposes constructing an expanded, multi-institutional dataset as future work to address generalization limits. This is unresolved because it is currently unclear if the drop in performance at higher LoRA ranks is an intrinsic limitation of the adaptation method or merely a symptom of the limited data size (407 training cases). Evidence to resolve: Re-running the LoRA ablation study (r=64) on the proposed expanded dataset to observe if higher ranks yield performance gains rather than degradation.

## Limitations

- Dataset scale: DMID contains only 510 total cases with 407 for training, raising concerns about overfitting and limiting generalizability to other institutions or populations.
- Cross-modal transfer assumption: The paper assumes knowledge transfers effectively from radiology-adjacent domains to mammography without empirical validation of this transfer mechanism.
- BI-RADS extraction method: The evaluation relies on extracting BI-RADS and density scores from generated free-text reports, but the extraction method is not specified, creating potential evaluation inconsistencies.

## Confidence

- **High confidence**: LoRA rank optimization findings (r=32 optimal for DMID size), domain pretraining advantage over generalist VLMs (MedGemma vs. Qwen2.5-VL-7B), and preprocessing pipeline effectiveness for standardizing input.
- **Medium confidence**: Clinical alignment improvements over existing models, as this represents the first VLM-based approach for mammography report generation without direct comparison to all prior methods.
- **Low confidence**: Cross-modality transfer benefits from MedGemma's pretraining, as the paper doesn't empirically validate which pretraining domains contribute most to mammography performance.

## Next Checks

1. **Dataset size scaling**: Validate whether r=32 remains optimal as dataset size increases by testing on larger mammography datasets (if available) or synthetic data augmentation approaches.
2. **Pretraining domain analysis**: Conduct ablation studies to identify which specific medical imaging domains in MedGemma's pretraining contribute most to mammography performance.
3. **BI-RADS extraction validation**: Implement and evaluate multiple BI-RADS extraction methods (regex, fine-tuned classifier, LLM-based extraction) to ensure evaluation consistency and identify potential biases.