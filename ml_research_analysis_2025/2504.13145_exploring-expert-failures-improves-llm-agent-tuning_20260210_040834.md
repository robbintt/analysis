---
ver: rpa2
title: Exploring Expert Failures Improves LLM Agent Tuning
arxiv_id: '2504.13145'
source_url: https://arxiv.org/abs/2504.13145
tags:
- expert
- action
- search
- chair
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Exploring Expert Failures (EEF) is a method that improves LLM agent
  fine-tuning by identifying and leveraging beneficial actions from failed expert
  trajectories. While standard Rejection Sampling Fine-Tuning (RFT) focuses on successful
  trajectories, EEF analyzes failed expert trajectories to extract useful intermediate
  actions that can guide the agent through complex subtasks.
---

# Exploring Expert Failures Improves LLM Agent Tuning

## Quick Facts
- arXiv ID: 2504.13145
- Source URL: https://arxiv.org/abs/2504.13145
- Reference count: 28
- Key outcome: EEF achieves 62% win rate on WebShop, first to surpass 0.81 score and exceed 81 on SciWorld

## Executive Summary
Exploring Expert Failures (EEF) is a method that improves LLM agent fine-tuning by extracting beneficial actions from failed expert trajectories. While standard Rejection Sampling Fine-Tuning (RFT) focuses only on successful trajectories, EEF analyzes failed expert trajectories to identify intermediate actions that lead to successful transitions while discarding harmful ones. This approach addresses RFT's local minima problem by leveraging previously inaccessible expert knowledge, achieving state-of-the-art results by being the first method to surpass 0.81 score on WebShop and 81 on SciWorld.

## Method Summary
EEF extends RFT by simulating agent execution from selected expert states at fixed intervals along both successful and failed trajectories. When simulation from state s_i succeeds but fails from state s_{i+l}, actions between these states are flagged as potentially harmful and excluded from training. The method operates in three phases: (1) Behavior Cloning on positive expert trajectories with masked loss, (2) Exploration from initial states and M=5 simulated states per expert trajectory, and (3) Reinforcement Training that identifies "recovery states" where prior expert actions degraded performance, training only on post-state actions. EEF achieves superior results with fewer iterations than RFT by utilizing expert knowledge from failed trajectories that would otherwise be discarded.

## Key Results
- EEF achieves 62% win rate on WebShop (11k), outperforming RFT (53.6%) and GPT-4 (35.6%)
- First method to surpass 0.81 score on WebShop and exceed 81 on SciWorld
- EEF-GPT-3&4 variant achieves 68.5% win rate on SciWorld with score 81.3
- Outperforms RFT×6 baseline with fewer iterations and lower computational cost

## Why This Works (Mechanism)

### Mechanism 1: Beneficial Action Extraction from Failed Trajectories
- **Claim:** Failed expert trajectories contain recoverable partial insights that can improve agent performance when harmful actions are excluded
- **Mechanism:** EEF simulates agent execution from selected expert states at fixed intervals. If the agent succeeds from state s_i but fails from state s_{i+l}, actions between these states are flagged as potentially harmful and excluded. Actions preceding successful simulations are retained for training
- **Core assumption:** Expert failures are non-uniform; early trajectory segments may be correct while failure occurs in later steps
- **Evidence anchors:** [abstract] "failed expert trajectories can often provide valuable guidance, e.g., plans and key actions" - [section 3.1] "failed expert trajectories still consist of helpful actions or thoughts to solve the problem. The failure may stem from mistakes in the actions of the last few steps"

### Mechanism 2: Recovery Learning from Harmful States
- **Claim:** Training on recovery actions from states following harmful expert actions enables agents to escape local minima
- **Mechanism:** EEF identifies "need-recover" states where prior expert actions degraded agent performance. If D+ contains solutions from these states, EEF trains only on post-state actions, teaching recovery without reinforcing preceding mistakes
- **Core assumption:** Recovery behaviors generalize across structurally similar subtasks; agents can learn to recognize and correct error states
- **Evidence anchors:** [abstract] "retaining actions that lead to successful transitions while discarding harmful ones" - [section 3.1, Box 2] GPT-3 makes an error (clicking wrong item) then attempts recovery via Back action

### Mechanism 3: Out-of-Distribution Subtask Coverage Expansion
- **Claim:** Leveraging failed expert trajectories reduces the OOD subtask gap that limits RFT performance
- **Mechanism:** RFT iteratively samples from distribution biased toward simpler subtasks. EEF injects training signal from failed expert trajectories on complex subtasks, providing coverage for states RFT never reaches through self-exploration
- **Core assumption:** Complex subtasks share partial structure with simpler ones that experts handle correctly early in trajectories
- **Evidence anchors:** [abstract] "many complex subtasks remain unsolved and persistently out-of-distribution (OOD)" - [section 3.1] "roughly 50% of these subtasks remain unsolved and thus persist as OOD" after standard RFT

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formalization for LLM Agents**
  - **Why needed here:** EEF operates on states, actions, transitions, and sparse rewards. Understanding state-based simulation and reward-based action selection requires MDP intuition
  - **Quick check question:** Given a failed expert trajectory τ_e = [s_0, a_0, s_1, ..., s_T], how would you determine which action sequence to exclude if simulation from s_2 succeeds but simulation from s_4 fails?

- **Concept: Rejection Sampling Fine-Tuning (RFT)**
  - **Why needed here:** EEF extends RFT as its foundation. Understanding RFT's iterative exploration-success filtering loop clarifies what EEF adds (failed-trajectory mining) and what it preserves (SFT-only training, no reward model)
  - **Quick check question:** Why does RFT's simplicity bias emerge, and what specific mechanism does EEF introduce to counter it?

- **Concept: Sparse Reward Credit Assignment**
  - **Why needed here:** EEF receives only final trajectory rewards (0/1). Inferring which intermediate actions are beneficial requires simulation-based counterfactual evaluation
  - **Quick check question:** If R(τ_s_l) = 1 but R(τ_s_{2l}) = 0, what can you conclude about actions a_l through a_{2l-1}?

## Architecture Onboarding

- **Component map:** Behavior Cloning Phase -> Exploration Phase -> Reinforcement Training Phase -> Positive Trajectory Repository (D+)
- **Critical path:** 1. Filter De → D+ (positive trajectories only) - 2. SFT on D+ (behavior cloning) - 3. For each iteration i: explore C_train + simulate from expert states → add positives to D+ - 4. Identify important states: all s_0 + first s_need_recover per trajectory - 5. For each important state, retrieve solution from D+ (prefer fewer expert actions) - 6. Train with loss masked to actions after important state only - 7. Repeat for I iterations (default I=4)
- **Design tradeoffs:**
  - M (simulations per trajectory): Higher M improves harmful-action detection granularity but increases compute O(M × |De|). Default M=5
  - Skip length l: Auto-computed; longer trajectories get coarser sampling. Trade-off: coverage vs. precision
  - Solution selection criterion: Preferring fewer expert actions preserves learning capacity but may exclude higher-quality solutions with more expert steps
  - Weak expert inclusion: GPT-3.5 trajectories (30× cheaper) can be added; EEF's filtering mitigates quality risk but adds complexity
- **Failure signatures:**
  1. Navigation skill deficit: Low Next/Back action usage in WebShop; check attempt rate vs. success rate (Table 4 shows GPT-4 attempts Next 16.8% but succeeds only 2.8%)
  2. Persistent OOD subtasks: Win rate plateaus below expert level; inspect which subtask categories remain unsolved
  3. Harmful action leakage: Performance degrades vs. RFT baseline; verify loss masking excludes actions before important states
- **First 3 experiments:**
  1. Reproduction on WebShop 3k: Run EEF (M=5, I=3) with Llama3-8B; compare win rate and reward vs. RFT, RFT×6, SFT-POS baselines. Track navigation skill acquisition
  2. Ablation on M: Fix base model (SFT-POS), run single iteration with M ∈ {1, 2, 5}; plot win rate vs. simulation budget to quantify efficiency gain over RFT×N baseline
  3. Weak expert integration test: Compare EEF-GPT-4 vs. EEF-GPT-3&4 on WebShop 11k; measure cost reduction and performance delta to assess practical benefit of cheaper expert data

## Open Questions the Paper Calls Out
- Can a binary search strategy replace the fixed-interval simulation to more efficiently and accurately pinpoint beneficial actions within failed trajectories?
- How should resources be optimally allocated between high-quality, high-cost experts and low-quality, low-cost experts when generating training data?
- Can incorporating expert trajectories as main branches in Monte Carlo Tree Search (MCTS) improve the solving of out-of-distribution (OOD) subtasks compared to the current iterative approach?

## Limitations
- State restoration mechanism: The paper assumes intermediate state serialization/restore is available but doesn't specify implementation details
- Simulation vs. exploration balance: No clear guidance on optimal ratio between exploration from initial states vs. expert trajectory simulations
- Weak expert value proposition: Unclear whether improvement comes from EEF's method or simply from additional training data

## Confidence
- **High confidence:** The core mechanism of extracting beneficial actions from failed trajectories is theoretically sound and empirically validated on benchmark tasks
- **Medium confidence:** The computational efficiency claims (fewer iterations than RFT×6) are supported but depend heavily on implementation specifics not fully disclosed
- **Low confidence:** Claims about generalization to new environments beyond WebShop and SciWorld require additional validation, particularly given the reliance on expert trajectory availability

## Next Checks
1. **State restoration validation:** Implement and test state serialization/restore functionality on WebShop with 100+ trajectories to verify reliability and consistency across environment types
2. **Weak expert ablation study:** Run EEF with only GPT-4 experts vs. EEF-GPT-3&4 on identical training budgets to isolate the contribution of cheaper expert data
3. **OOD subtask analysis:** Map which specific subtask categories remain unsolved by EEF vs. RFT to validate the claimed reduction in out-of-distribution coverage gaps