---
ver: rpa2
title: Cost Efficient Fairness Audit Under Partial Feedback
arxiv_id: '2510.03734'
source_url: https://arxiv.org/abs/2510.03734
tags:
- algorithm
- fairness
- cost
- audit
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of auditing fairness in machine
  learning models under partial feedback, where true labels are only available for
  positively classified individuals. A novel cost model is introduced to reflect real-world
  costs like credit assessment and potential defaults.
---

# Cost Efficient Fairness Audit Under Partial Feedback

## Quick Facts
- **arXiv ID**: 2510.03734
- **Source URL**: https://arxiv.org/abs/2510.03734
- **Reference count**: 40
- **Primary result**: Achieves ~50% cost reduction in fairness auditing under partial feedback through rejection sampling and truncated learning

## Executive Summary
This paper addresses the fundamental challenge of auditing fairness in machine learning models when true labels are only available for positively classified individuals. Under a realistic cost model that reflects real-world expenses like credit assessments and potential defaults, the authors introduce RS-Audit, a rejection sampling algorithm that reduces audit costs by approximately 50% compared to a natural baseline. They further propose Exp-Audit, which leverages truncated sample learning and maximum-a-posteriori oracles under mixture model assumptions, achieving cost independence from the fairness threshold ε. The theoretical guarantees are validated through experiments on real-world datasets like Adult Income and Law School, demonstrating strong empirical performance while providing a comprehensive framework for cost-efficient fairness auditing in settings with incomplete label feedback.

## Method Summary
The paper proposes a cost-efficient framework for auditing equalized odds fairness under partial feedback. The RS-Audit algorithm uses rejection sampling to estimate conditional probabilities P[Y=y|A=a] by sampling only from relevant groups, achieving 50% cost reduction. The Exp-Audit algorithm exploits mixture model assumptions, using truncated sample learning to recover distribution parameters from positively classified samples, then employing a MAP oracle for label-free estimation. Under a separation condition on mixture components, this approach achieves ε-independent audit costs for the dominant term.

## Key Results
- RS-Audit achieves approximately 50% lower audit cost compared to baseline through rejection sampling
- Exp-Audit reduces costs by a constant factor independent of fairness threshold ε under mixture model assumptions
- Theoretical guarantees validated on Adult Income and Law School datasets with strong empirical performance
- Cost savings are most pronounced when c_lab >> c_feat (default scenario with expensive labels)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rejection sampling reduces audit cost by ~50% compared to naive baseline
- **Mechanism**: RS-Audit estimates conditional probabilities P[Y=y|A=a] instead of joint probabilities P[Y=y, A=a], sampling only from groups relevant to each estimation. By ignoring individuals with A≠a when estimating q_y|a, the algorithm avoids requesting labels for irrelevant groups, directly reducing the cost term.
- **Core assumption**: The audit cost accrues only when requesting true labels; observing features for non-target groups provides no benefit for conditional probability estimation.
- **Evidence anchors**:
  - [abstract] "RS-Audit... achieving approximately 50% lower audit cost"
  - [Section 4.1] "the gap in sample complexity of baseline over RS-Audit is roughly... [expression with P[f=0,A≠a]"
  - [corpus] Weak direct validation; neighbor papers address different auditing contexts (LLM auditing, distribution shift)

### Mechanism 2
- **Claim**: Truncated sample learning enables parameter recovery from partial feedback without requesting labels for negatively classified individuals
- **Mechanism**: The classifier f induces truncation—only positively classified samples appear in historical data. By treating Dy,a (positively classified individuals with known Y=y, A=a) as truncated samples from Eθ*_{y,a}, TruncEst subroutine recovers θ*_{y,a} without additional label queries, leveraging the structure of exponential family distributions.
- **Core assumption**: (i) P[f=1|Y=y, A=a] ≥ α > 0 (positivity), (ii) Eθ(X) is log-concave, (iii) parameter set Θ contains ball around true parameters, (iv) sufficient samples in historical data.
- **Evidence anchors**:
  - [Section 5] "Dy,a contains only those samples from Eθ*_{y,a} for whom f=1"
  - [Appendix B.2] References Theorem 3.1 from Lee et al. (2023) for truncated sample recovery guarantees
  - [corpus] No direct validation; corpus papers don't address truncated sample learning in fairness context

### Mechanism 3
- **Claim**: MAP oracle provides label-free mixture weight estimation with ε-independent cost
- **Mechanism**: After recovering θ parameters via truncated learning, Exp-Audit constructs MAP oracle Ma(x) = argmax_y{eq_y|a · Eθ_{y,a}(x)}. Under separation condition (1), this oracle correctly classifies samples with high probability. By counting oracle-assigned labels over R samples, q_y|a is estimated without requesting true labels, making the dominant cost term independent of ε.
- **Core assumption**: Separation condition ||θ*_1,a - θ*_0,a|| ≥ Ω(√log(q_M,a/(q_m,a·ε))) ensures MAP oracle accuracy; Lipschitzness and bounded curvature of log-partition function W.
- **Evidence anchors**:
  - [Section 5, Eq. (1)] Explicit separation assumption linking parameter distance to log(1/ε)
  - [Lemma 14] Shows MAP oracle error scales as (ε/q_M,a)² under separation condition
  - [corpus] No direct validation; separation conditions for mixture learning appear in Mahalanabis (2011) for Gaussians, extended here

## Foundational Learning

- **Concept: Negative Binomial Concentration**
  - Why needed here: Both RS-Audit and baseline use stopping times defined by "sample until τ successes observed"; concentration bounds for negative binomial distribution justify (1±ε/12) multiplicative accuracy guarantees
  - Quick check question: Can you derive why Lemma 23 gives P[N ∈ (1±ε)τ/p] ≥ 1-2exp(-τε²/4)?

- **Concept: Exponential Family Distributions**
  - Why needed here: Mixture model assumes features follow exponential family; understanding log-partition function W, sufficient statistics T(X), and parameter recovery is essential for TruncEst and MAP oracle construction
  - Quick check question: For multivariate Gaussian Eθ(X), can you identify θ, T(X), and W(θ)?

- **Concept: Equalized Odds Fairness**
  - Why needed here: The audit tests whether max_{y,a,a'}|P[f=1|Y=y,A=a] - P[f=1|Y=y,A=a']| exceeds ε; understanding this metric determines what probabilities must be estimated
  - Quick check question: Why does equalized odds require estimating P[f=1|Y=y,A=a] for both y∈{0,1}, unlike equal opportunity?

## Architecture Onboarding

- **Component map**:
  ```
  [Historical Data D] → Split by (Y,A) → [Dy,a subsets]
                                    ↓
                            [TruncEst] → θ̂_{y,a} estimates
                                    ↓
  [Online Samples] → [OnlineSample] → eq_y|a (rough estimate)
                                    ↓
                         [MAP Oracle Ma] → Proxy labels
                                    ↓
                         [Count R_y] → bq_y|a (refined estimate)
                                    ↓
                         [EOD Calculation] → FAIR/UNFAIR decision
  ```

- **Critical path**:
  1. Ensure historical data has sufficient samples per (y,a) subgroup for TruncEst (check |Dy,a| ≥ Ω(d/ε²))
  2. Verify separation condition holds for estimated parameters (||θ̂_1,a - θ̂_0,a|| ≥ threshold)
  3. Monitor MAP oracle accuracy on held-out labeled samples before trusting label-free estimation

- **Design tradeoffs**:
  - **Black-box (RS-Audit)** vs. **Mixture (Exp-Audit)**: RS-Audit requires no distributional assumptions but cost scales as O(1/ε²); Exp-Audit requires strong assumptions but cost becomes ε-independent for dominant term
  - **Cost parameters (c_feat, c_lab)**: If c_lab ≫ c_feat (default scenario), Exp-Audit's advantage is largest; if c_feat dominates, both algorithms have similar asymptotic costs
  - **Separation threshold**: Tighter separation enables cheaper auditing but restricts applicable problem instances

- **Failure signatures**:
  - If bΔ oscillates wildly as τ increases: likely insufficient samples or data quality issues in historical D
  - If Exp-Audit consistently underestimates true EOD (see Figure 1, plot 3): MAP oracle may have systematic bias from separation condition being barely satisfied
  - If audit cost doesn't decrease with larger historical datasets: check positivity assumption (P[f=1|Y=y,A=a] may be approaching 0 for some subgroups)

- **First 3 experiments**:
  1. **Validate RS-Audit on Adult dataset with varying τ**: Plot cost and bΔ vs. τ for both baseline and RS-Audit; verify ~50% cost reduction and convergence to true EOD (replicate Figure 1, plots 1-2)
  2. **Synthetic mixture experiment with controlled separation**: Generate Gaussian mixture with known θ* and separation distance; sweep ε and verify Exp-Audit cost is ε-independent while RS-Audit scales as 1/ε² (replicate Figure 1, plots 3-4)
  3. **Ablation on separation condition**: Deliberately violate separation by reducing ||θ*_1,a - θ*_0,a||; observe degradation in MAP oracle accuracy and corresponding increase in required R samples—establishes practical lower bound on separation for cost gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the audit cost complexity when the classifier is unknown but belongs to a known hypothesis class (e.g., threshold classifiers), rather than being a fixed, known function?
- Basis in paper: [explicit] The conclusion states, "Another interesting future work would be to explore the audit cost complexity of unknown classifiers from a known hypothesis class, e.g., threshold classifiers."
- Why unresolved: The current work assumes the auditor has full access to the fixed classifier $f$ during the audit process.
- What evidence would resolve it: A theoretical analysis or algorithm that characterizes the audit cost bounds for a hypothesis class $\mathcal{H}$ where $f \in \mathcal{H}$.

### Open Question 2
- Question: What other structural data assumptions, aside from the mixture of exponential families, allow for auditing algorithms with lower costs than the black-box lower bound?
- Basis in paper: [explicit] The conclusion asks, "...one can also ask what are other data assumptions under which algorithms with lower audit cost can be designed."
- Why unresolved: The paper provides a complete characterization only for the black-box (non-parametric) and specific mixture model settings.
- What evidence would resolve it: Identification of a novel structural assumption and a corresponding auditing algorithm that achieves a cost reduction similar to or better than Exp-Audit.

### Open Question 3
- Question: Can the cost efficiency of Exp-Audit be maintained if the strict separation condition between mixture components (Equation 1) is relaxed or removed?
- Basis in paper: [inferred] The theoretical guarantees for Exp-Audit rely on a separation condition (Eq. 1) to ensure the MAP oracle can assign proxy labels accurately.
- Why unresolved: It is unknown if the cost remains independent of $\epsilon$ when the conditional distributions for different labels overlap significantly, potentially degrading the MAP oracle's accuracy.
- What evidence would resolve it: A theoretical bound or empirical study showing the performance of Exp-Audit under varying degrees of distribution overlap (violating the separation condition).

## Limitations

- Separation condition required for Exp-Audit may not hold in real-world datasets, limiting practical applicability
- TruncEst implementation details from referenced work [LWZ23] are not fully specified, creating reproducibility challenges
- Experiments focus on specific datasets and fairness thresholds, limiting generalizability to other domains

## Confidence

- RS-Audit cost reduction claim (50%): **High** - Well-supported by theoretical analysis and experimental validation
- Exp-Audit ε-independent cost claim: **Medium** - Strong theoretical foundation but relies on separation condition that may not hold in practice
- Mixture model assumptions validity: **Medium-Low** - Reasonable for synthetic experiments but unverified for real-world data distributions

## Next Checks

1. **Separation Condition Verification**: Implement systematic measurement of ||θ̂_1,a - θ̂_0,a|| across different datasets and verify when it meets the separation threshold. Document the fraction of real-world scenarios where this condition holds.

2. **Cost-Agnostic Performance Test**: Run both algorithms on Adult Income and Law School datasets with varying ε values (0.01, 0.05, 0.1, 0.2) and compare not just cost but also audit accuracy. This would reveal whether Exp-Audit maintains accuracy advantages beyond cost savings.

3. **Sensitivity to Cost Parameters**: Systematically vary c_feat/c_lab ratios from 0.1 to 10 and measure algorithm performance. This would identify the precise cost regime where Exp-Audit provides meaningful advantages over RS-Audit.