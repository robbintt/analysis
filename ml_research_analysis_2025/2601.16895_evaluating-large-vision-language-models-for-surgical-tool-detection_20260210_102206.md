---
ver: rpa2
title: Evaluating Large Vision-language Models for Surgical Tool Detection
arxiv_id: '2601.16895'
source_url: https://arxiv.org/abs/2601.16895
tags:
- surgical
- detection
- instrument
- vlms
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of large vision-language\
  \ models (VLMs) for surgical tool detection, a fundamental visual perception task\
  \ in robotic surgery. Three state-of-the-art VLMs\u2014Qwen2.5, LLaVA1.5, and InternVL3.5\u2014\
  are assessed on the GraSP dataset under zero-shot and fine-tuned settings, with\
  \ Grounding DINO as a baseline."
---

# Evaluating Large Vision-language Models for Surgical Tool Detection

## Quick Facts
- **arXiv ID**: 2601.16895
- **Source URL**: https://arxiv.org/abs/2601.16895
- **Reference count**: 15
- **One-line primary result**: Qwen2.5 achieves superior surgical tool detection performance, excelling in instrument recognition while maintaining balanced localization capabilities.

## Executive Summary
This study evaluates three state-of-the-art large vision-language models (VLMs)—Qwen2.5, LLaVA1.5, and InternVL3.5—for surgical tool detection, a fundamental task in robotic surgery. The models are assessed on the GraSP dataset under zero-shot and fine-tuned settings, with Grounding DINO as a baseline. Qwen2.5 consistently demonstrates superior instrument recognition performance, while Grounding DINO excels in localization. Fine-tuning substantially reduces errors across all models, but the relative performance trends remain consistent with zero-shot results. These findings highlight the potential of VLMs for advancing general-purpose surgical AI systems.

## Method Summary
The study evaluates three VLMs (Qwen2.5-7B, LLaVA1.5-7B, InternVL3.5-8B) on surgical tool detection using the GraSP dataset, which contains 3449 frames from 13 robot-assisted radical prostatectomy videos. The models are tested in both zero-shot and fine-tuned settings, with LoRA fine-tuning (rank-8, 5 epochs) applied for domain adaptation. Grounding DINO serves as a baseline detection model. The evaluation uses TIDE error analysis with confidence-independent metrics, comparing classification and localization performance across 7 instrument classes.

## Key Results
- Qwen2.5 consistently achieves superior instrument recognition performance across both zero-shot and fine-tuned settings.
- Grounding DINO demonstrates stronger localization performance but inferior classification compared to Qwen2.5.
- Fine-tuning substantially reduces errors across all models and error categories, though relative performance trends remain consistent.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large-scale image-text pretraining enables semantic transfer to specialized surgical domains, improving instrument classification even without domain-specific training.
- **Mechanism**: VLMs encode rich visual-semantic associations from massive image-text corpora. When prompted with instrument category names, these pre-trained associations activate relevant visual features, enabling zero-shot recognition of surgical tools despite domain shift.
- **Core assumption**: Semantic concepts learned from natural images transfer sufficiently to surgical instrument variants.
- **Evidence anchors**: Qwen2.5 shows superior instrument recognition while Grounding DINO demonstrates stronger localization; Qwen benefits from large-scale image-text pretraining facilitating effective semantic transfer.

### Mechanism 2
- **Claim**: Detection-specialized architectures (Grounding DINO) maintain spatial localization advantages over general-purpose VLMs even after domain adaptation.
- **Mechanism**: Object detection models incorporate spatial attention mechanisms, anchor box priors, and regression heads optimized for bounding box prediction. These architectural inductive biases persist across domain shifts, whereas VLMs generate bounding boxes through text generation, which is inherently less spatially precise.
- **Core assumption**: Spatial encoding learned from natural images transfers to surgical scenes more robustly than semantic encoding.
- **Evidence anchors**: Grounding DINO demonstrates stronger localization performance; the baseline continues to perform better in localization and background error categories.

### Mechanism 3
- **Claim**: Parameter-efficient LoRA fine-tuning reduces errors across all categories but preserves relative model strengths and architectural biases.
- **Mechanism**: LoRA updates low-rank adapters while freezing backbone weights, allowing task-specific adaptation without overwriting pre-trained representations. This preserves the semantic vs. spatial trade-offs embedded in each architecture's pre-training.
- **Core assumption**: Low-rank adaptation is sufficient for surgical domain adaptation without requiring full fine-tuning.
- **Evidence anchors**: Fine-tuning substantially reduces errors across all categories for all models; the performance trend remains consistent with the zero-shot setting.

## Foundational Learning

- **Concept: Zero-shot vs. Fine-tuned Inference in VLMs**
  - **Why needed here**: The paper evaluates both deployment scenarios; understanding when prompting suffices vs. when adaptation is required is critical for surgical AI deployment.
  - **Quick check question**: Can you explain why the zero-shot prompt includes instrument category names while the fine-tuned prompt does not?

- **Concept: TIDE Error Decomposition Framework**
  - **Why needed here**: Standard mAP requires confidence scores, which VLMs don't produce. TIDE enables confidence-independent error analysis.
  - **Quick check question**: What is the difference between "Classification Error" and "Both Classification and Localization Error" in TIDE?

- **Concept: Open-Set Object Detection**
  - **Why needed here**: Grounding DINO is an open-set detector, meaning it can detect objects beyond training classes via text prompts—critical for understanding its surgical generalization.
  - **Quick check question**: How does open-set detection differ from traditional closed-set object detection?

## Architecture Onboarding

- **Component map**: Surgical Image → VLM Encoder (Qwen/LLaVA/InternVL) → Text Prompt → Bounding Box + Class Output
  ↓ (alternative path)
  Grounding DINO → Text Prompt → Bounding Box + Class + Confidence

- **Critical path**:
  1. Prompt engineering (Table 2 shows zero-shot prompts require explicit category lists)
  2. LoRA adapter injection (Rank-8 for 5 epochs)
  3. IoU thresholding (0.5 foreground, 0.1 background for TIDE)

- **Design tradeoffs**:
  - Qwen2.5: Superior classification → choose when instrument identification is priority
  - Grounding DINO: Superior localization → choose when precise bounding boxes matter
  - LLaVA/InternVL: Not recommended for surgical tool detection in current form

- **Failure signatures**:
  - LLaVA zero-shot: Predicts all 7 categories per image with random boxes (4,220 Cls+Loc errors)
  - InternVL zero-shot: Under-generates predictions (2,623/2,861 missed GT)
  - Qwen zero-shot: Background misclassification (337 background errors vs. GDINO's 272)
  - GDINO fine-tuned: Duplicate predictions (79 duplicate errors vs. Qwen's 0)

- **First 3 experiments**:
  1. Reproduce Qwen2.5 zero-shot on GraSP test split using the exact prompt from Table 2; verify classification error rate (~1,313 errors on 2,861 instances).
  2. Fine-tune Qwen2.5 with LoRA Rank-8 and measure localization error reduction (paper shows 229→140); experiment with Rank-16 or Rank-32 to test if localization improves further.
  3. Hybrid ensemble test: Use Qwen2.5 for classification + Grounding DINO for localization on same images; evaluate if complementary strengths can be combined (paper does not explore this).

## Open Questions the Paper Calls Out

- **Question**: Can Qwen's superior instrument recognition capabilities be effectively leveraged for higher-level surgical tasks such as phase recognition, action recognition, and step recognition?
  - **Basis in paper**: The authors state: "Future research could extend this work by leveraging Qwen across multiple surgical tasks, such as phase, action, and step recognition."
  - **Why unresolved**: This study only evaluated surgical tool detection; no experiments were conducted on downstream surgical workflow tasks.
  - **What evidence would resolve it**: Evaluation of Qwen on benchmark surgical datasets with phase, action, and step annotations (e.g., Cholec80, M2CAI workflow datasets) comparing against specialized unimodal baselines.

- **Question**: How can the complementary strengths of VLMs (superior classification) and detection-focused models (superior localization) be combined into a unified surgical detection system?
  - **Basis in paper**: The paper shows Qwen achieves superior classification while Grounding DINO demonstrates stronger localization, suggesting a hybrid approach could outperform either alone.
  - **Why unresolved**: No architecture or training strategy was explored to integrate semantic understanding from VLMs with spatial precision from detection models.
  - **What evidence would resolve it**: A combined model or ensemble approach evaluated on the same GraSP dataset showing improved performance across both classification and localization error categories.

## Limitations

- Dataset-specific performance: The GraSP dataset contains only 7 instrument classes from prostate surgery, limiting generalizability to other surgical domains with different tools or procedures.
- Prompt engineering sensitivity: The study uses specific prompts for zero-shot inference, but these prompts may not be optimal and relative performance could shift significantly with different formulations.
- Architectural constraints: VLMs generate bounding boxes as text tokens rather than through spatial regression, creating inherent localization limitations that may not be overcome by fine-tuning alone.

## Confidence

- **High confidence**: Qwen2.5's superior classification performance (supported by quantitative error analysis showing consistently lower classification errors across both zero-shot and fine-tuned settings).
- **Medium confidence**: Semantic transfer mechanism for surgical instrument recognition (supported by comparative performance but requires testing on more diverse surgical datasets).
- **Low confidence**: Generalization to other surgical domains (based on single dataset, GraSP, without cross-dataset validation).

## Next Checks

1. **Cross-dataset validation**: Test Qwen2.5 on at least two additional surgical datasets (e.g., JIGSAW, Cholec80) to assess domain generalization beyond prostate surgery tools.
2. **Prompt optimization study**: Systematically evaluate how different prompt formulations (category lists, prompt templates, temperature settings) affect VLM performance to establish robustness to prompt engineering.
3. **Hybrid system evaluation**: Implement and test a hybrid approach combining Qwen2.5's classification with Grounding DINO's localization to determine if complementary strengths can be integrated for improved overall performance.