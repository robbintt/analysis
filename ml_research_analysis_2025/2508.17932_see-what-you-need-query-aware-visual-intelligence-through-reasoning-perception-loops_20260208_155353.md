---
ver: rpa2
title: 'See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception
  Loops'
arxiv_id: '2508.17932'
source_url: https://arxiv.org/abs/2508.17932
tags:
- video
- reasoning
- visual
- understanding
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CA VIA addresses the limitation of static pipelines in long-form
  video understanding by introducing a closed-loop framework where reasoning continuously
  guides visual extraction. Unlike prior methods that decouple perception from reasoning,
  CA VIA employs hierarchical reasoning-guided localization, cross-modal semantic
  bridging for targeted extraction, and confidence-driven iterative synthesis.
---

# See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops

## Quick Facts
- **arXiv ID**: 2508.17932
- **Source URL**: https://arxiv.org/abs/2508.17932
- **Reference count**: 29
- **Primary result**: State-of-the-art performance on long-form video QA benchmarks: EgoSchema 65.7% (+5.3%), NExT-QA 76.1% (+2.6%), IntentQA 73.8% (+6.9%)

## Executive Summary
CA VIA introduces a closed-loop reasoning-perception framework for long-form video understanding that overcomes the limitations of static, decoupled pipelines. By iteratively coordinating LLMs and VLMs through hierarchical localization, targeted extraction, and confidence-driven synthesis, the system dynamically adapts visual extraction to reasoning needs rather than exhaustively processing all frames. The approach achieves significant accuracy gains on challenging benchmarks by focusing computational resources on query-relevant visual evidence through adaptive, iterative refinement cycles.

## Method Summary
CA VIA operates as a training-free, closed-loop system that coordinates LLMs and VLMs for long-form video question answering. The framework begins with hierarchical reasoning-guided localization, partitioning video captions into blocks and retrieving semantically relevant segments through coarse-to-fine search. An LLM analyzes retrieved context to identify information gaps, then generates targeted instructions for the VLM to extract specific visual details through spatial or temporal prompts. The system iteratively synthesizes answers while monitoring confidence scores, continuing refinement until reaching a threshold or maximum iterations. This dynamic coordination enables query-specific visual detail extraction rather than uniform processing, with Qwen2.5-VL-7B serving as the visual backbone and various LLMs handling reasoning tasks.

## Key Results
- Achieves 65.7% accuracy on EgoSchema (full), representing +5.3% improvement over prior methods
- Reaches 76.1% accuracy on NExT-QA, improving by +2.6% over state-of-the-art
- Obtains 73.8% accuracy on IntentQA, demonstrating +6.9% advancement
- Shows strong positive correlation (r=0.932) between iteration count and accuracy on NExT-QA
- Outperforms static pipelines by focusing computational resources on reasoning-relevant visual evidence

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Reasoning-Guided Localization
The system filters video content through coarse-to-fine semantic search, partitioning captions into blocks and scoring them against the main query and decomposed sub-questions. This significantly reduces irrelevant visual noise and computational cost compared to uniform sampling by focusing on high-scoring blocks before performing fine frame selection.

### Mechanism 2: Cross-Modal Semantic Bridging (Targeted Prompting)
The LLM converts reasoning gaps into specific visual instructions that extract more relevant information than generic captioning. By identifying what visual information is missing and generating targeted instructions (e.g., "Focus on the tool in the right hand"), the system forces the VLM to attend to specific details rather than generating broad summaries.

### Mechanism 3: Confidence-Driven Iterative Synthesis
The framework iteratively updates visual context based on answer confidence, improving final accuracy by fixing premature conclusions. After initial synthesis, the system evaluates confidence and triggers new retrieval/extraction cycles if confidence is low, treating the current answer as a hypothesis to be verified or corrected.

## Foundational Learning

- **Concept: Coarse-to-Fine Retrieval**
  - Why needed here: Long videos cannot be fully processed in context windows, requiring efficient mechanisms to discard irrelevant content before expensive visual analysis
  - Quick check question: Does the system retrieve frames based on pixel similarity or text semantic similarity? (Answer: Text/Caption similarity)

- **Concept: Visual Chain-of-Thought (CoT)**
  - Why needed here: Standard VLMs answer immediately, but CAVIA requires meta-cognitive steps to identify what it doesn't see before looking again
  - Quick check question: In CAVIA, does the VLM generate the answer directly after seeing the video? (Answer: No, the LLM first generates instructions for the VLM based on missing info)

- **Concept: Query Decomposition**
  - Why needed here: Complex questions contain multiple temporal and causal dependencies that single retrieval passes might miss
  - Quick check question: How does the system handle a multi-part question? (Answer: It decomposes it into sub-questions q_i to guide retrieval of different caption blocks)

## Architecture Onboarding

- **Component map**: Retriever -> Planner (LLM) -> Executor (VLM) -> Aggregator -> Confidence Check -> (Loop)
- **Critical path**: Input Question -> Query Decomposition -> Caption Block Retrieval -> Gap Analysis -> Instruction Generation -> VLM Execution -> Context Update -> Answer Synthesis -> Confidence Check -> (Loop if low confidence)
- **Design tradeoffs**: 
  - Latency vs. Accuracy: Iterative loops improve accuracy (up to 4 iterations) but linearly increase latency and token cost
  - Caption Dependency: System is "training-free" but heavily dependent on pre-existing caption quality
- **Failure signatures**:
  - Premature Termination: Stops after 1 iteration due to overconfidence in hallucination
  - Drift: Gap Analysis focuses on irrelevant details in later iterations, degrading answer quality
- **First 3 experiments**:
  1. Ablation on Iterations: Run pipeline with max iterations 1, 2, 3, 4 on NExT-QA to validate correlation between loops and accuracy
  2. Localization Baseline: Replace Hierarchical Retrieval with uniform frame sampling to quantify efficiency/accuracy gains
  3. Prompt Mode Analysis: Compare Temporal vs. Spatial instructions on different question types (IntentQA vs. NExT-QA)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can adaptive termination criteria be designed to dynamically stop the reasoning-perception loop based on query complexity and information saturation rather than fixed confidence thresholds? (Explicit in Limitations section)
- **Open Question 2**: What architectural optimizations or distillation methods are required to reduce latency for deployment in real-time, streaming video applications? (Explicit in Limitations section)
- **Open Question 3**: To what extent does the framework's performance depend on the fidelity of initial video captions, and can the iterative loop self-correct for poor initial semantic descriptions? (Explicit warning about caption quality dependency)
- **Open Question 4**: Does the iterative caption enhancement mechanism mitigate or inadvertently propagate hallucinations generated by the VLM during visual extraction phases? (Inferred from caption update mechanism)

## Limitations

- The framework's effectiveness heavily relies on the quality of initial captions, with poor textual descriptions potentially limiting benefits from dynamic prompting
- Cumulative cost of repeated LLM-VLM interactions may limit deployment in latency-sensitive applications
- Current implementation uses fixed confidence thresholds for terminating iterations, which may not be optimal across all query types

## Confidence

- **High confidence**: Overall architecture and motivation are well articulated, with benchmark improvements (+5.3% on EgoSchema, +2.6% on NExT-QA, +6.9% on IntentQA) consistent with claimed gains
- **Medium confidence**: Ablation on iteration count shows strong correlation (r=0.932) between loops and accuracy, but lacks statistical significance testing or variance across runs
- **Low confidence**: "Training-free" framing is somewhat misleading as effectiveness hinges on caption quality and prompt engineering, which are underspecified

## Next Checks

1. **Caption dependency test**: Re-run pipeline with captions from two different models (e.g., BLIP-2 vs. LLaVA) on EgoSchema subset to measure performance impact
2. **Confidence threshold sweep**: Systematically vary confidence threshold (0.7, 0.8, 0.85, 0.9) and max iterations (1-5) on NExT-QA to map accuracy-latency tradeoff
3. **Gap analysis probe**: Log and categorize LLM's identified gaps across iterations to check for drift or diminishing returns in later cycles