---
ver: rpa2
title: Collaboration and Conflict between Humans and Language Models through the Lens
  of Game Theory
arxiv_id: '2509.04847'
source_url: https://arxiv.org/abs/2509.04847
tags:
- should
- strategies
- answer
- language
- authors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the cooperative behavior of language models
  in interactive settings through iterated prisoner's dilemma games. The authors pit
  language models against 240 classical strategies in an Axelrod-style tournament,
  finding that language models perform on par with or better than the best classical
  strategies, achieving an average advantage of 12.6 wins per round.
---

# Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory

## Quick Facts
- arXiv ID: 2509.04847
- Source URL: https://arxiv.org/abs/2509.04847
- Authors: Mukul Singh; Arjun Radhakrishna; Sumit Gulwani
- Reference count: 39
- Primary result: LLMs achieve 12.6 win advantage over classical strategies in IPD tournaments

## Executive Summary
This paper examines cooperative behavior of language models in iterated prisoner's dilemma games against 240 classical strategies. Language models perform on par with or better than the best classical strategies, exhibiting key cooperative traits like niceness, provocability, and generosity. The study reveals that LLMs adapt to strategy changes in opponents, though more slowly than humans, and provides systematic characterization of long-term cooperative behaviors in language model agents.

## Method Summary
The study pits language models against 240 classical Axelrod tournament strategies in iterated prisoner's dilemma games. The reward structure uses H=5, R=3, P=1, L=0 with either fixed 50-round games or indefinite play with 0.05 termination probability. Each game runs with 20 random seeds. The methodology includes strategy-switch experiments where opponents change tactics mid-game to measure adaptation speed, and human participants for comparison. LLMs receive history-formatted prompts and output cooperate/defect decisions, with behavioral metrics tracking win rates, cooperation patterns, and adaptation timing.

## Key Results
- LLMs achieve an average advantage of 12.6 wins per round against classical strategies
- Models exhibit key cooperative properties: niceness, provocability, and generosity
- In strategy switch experiments, models detect and adapt to opponent changes within a few rounds, though slower than humans (24.5 rounds vs 3.2-5.4 rounds)
- Behavioral analysis reveals models closely follow characteristics of the best classical strategies

## Why This Works (Mechanism)

### Mechanism 1: Emergence of Cooperative Traits
Language models achieve high performance not through rigid rules but by exhibiting behavioral traits—niceness, provocability, and generosity—that correlate with successful cooperation. The model processes game state and selects actions based on implicit patterns learned during pre-training which align with "reciprocal altruism." It mimics behavioral output of strong strategies: cooperates initially, defects when exploited, and occasionally forgives to restore cooperation.

### Mechanism 2: Context-Dependent Strategy Switching
Models detect strategy changes by processing cumulative move history as a context sequence. When opponent patterns shift (e.g., from CCC to CDD), the model recognizes the break and updates action probabilities. The paper notes conflicting evidence on speed: text claims models are slower (24.5 rounds), while Table 1 suggests AI (3.7 rounds) adapts faster than humans (5.4 rounds) in specific conditions.

### Mechanism 3: Instruction-Following Priors
The model's cooperative or competitive posture is heavily mediated by prompt framing, which sets the initial strategy before any moves. The prompt acts as initial state—competitive framing suppresses niceness, while collaborative framing initiates cooperation. The paper mitigates biased data by selecting neutral prompts.

## Foundational Learning

- **Concept: Iterated Prisoner's Dilemma (IPD) Reward Structure**
  - Why needed: To understand why models choose to Cooperate or Defect and how "winning" is defined
  - Quick check: If H=5, R=3, P=1, L=0, why is mutual cooperation (3,3) often preferred over temptation to defect (5,0) in iterated version? (Answer: Long-term mutual cooperation score outweighs short-term gain which invites retaliation)

- **Concept: Niceness, Provocability, and Generosity**
  - Why needed: These are the three core metrics used to evaluate LLM's behavioral personality
  - Quick check: If model Cooperates R1, Defects R2 after opponent defects, and Cooperates R3 despite opponent defecting, which traits shown? (Answer: Niceness, Provocability, Generosity)

- **Concept: Axelrod-style Tournament**
  - Why needed: The paper positions LLMs against an ecosystem of 240 classical algorithms, not one strategy
  - Quick check: Why is winning round-robin tournament different from maximizing score against single opponent? (Answer: Must perform well on average against diverse archetypes)

## Architecture Onboarding

- **Component map:** Game Engine -> Strategy Bank -> LLM Agent -> Evaluator
- **Critical path:**
  1. Initialize game with neutral prompt
  2. Format history into string format required by model API
  3. Call Model API → Extract Move
  4. Compare with Opponent Move → Update Scores
  5. Loop for N rounds or until strategy switch trigger
- **Design tradeoffs:**
  - Fixed (50 rounds) encourages "end-game" defection logic; Indefinite (p=0.05) tests pure long-term cooperation
  - Truncating history saves tokens but blinds model to long-arc strategy shifts
  - Temperature >0 allows "Generosity" (probabilistic forgiveness) but high temp makes model erratic
- **Failure signatures:**
  - Rapid Oscillation: Model flips C→D→C unpredictably (Cause: Ambiguous prompt or high temperature)
  - Total Defection: Model always outputs "Defect" (Cause: Prompt biased or model detects losing position)
  - Hallucinated History: Model references rounds that didn't happen (Cause: Context window confusion)
- **First 3 experiments:**
  1. Baseline Self-Play: Run LLM against itself to see if it converges to Mutual Cooperation
  2. vs. Tit-for-Tat: Verify model maintains cooperation and doesn't trigger "death spiral"
  3. Strategy Switch Test: Run vs. "Always Cooperate" for 20 rounds, then switch to "Always Defect"

## Open Questions the Paper Calls Out

### Open Question 1
Do language models maintain cooperative strategies like niceness and provocability when deployed in multi-party, n-player games rather than dyadic interactions? The study is restricted to IPD with only two players, and observational data from LLM agents in Axelrod-style tournaments with n > 2 players or coalition formation games would resolve this.

### Open Question 2
Is the slower adaptation speed of language models (24.5 rounds vs. 3.2–5.4 for humans) an artifact of limited context windows or a fundamental constraint of current inference mechanisms? The paper notes slower adaptation but doesn't investigate whether the bottleneck is prompt history limits vs. reasoning capability. Ablation studies comparing adaptation speeds across models with varying context window sizes would resolve this.

### Open Question 3
To what extent are the observed cooperative behaviors robust to variations in system prompts or framing of game instructions? The methodology acknowledges instructions heavily influence behavior but selects one "consistent variant," leaving sensitivity to prompt engineering unexplored. A sensitivity analysis measuring behavioral metrics across prompts with varying cooperative or competitive framing would resolve this.

## Limitations

- Internal contradiction: Adaptation speed claims conflict between text (24.5 rounds) and Table 1 data (3.7 vs 5.4 rounds)
- Key specifications missing: Specific LLM models, complete prompts, and temperature settings remain unspecified
- Limited scope: Restricted to dyadic IPD games without addressing multi-party or coalition dynamics
- Potential strategic behavior: Models might appear cooperative while defecting opportunistically when detection risk is low

## Confidence

- **High Confidence:** Performance metrics (12.6 win advantage) against classical strategies are straightforward to verify
- **Medium Confidence:** Behavioral trait analysis depends on prompt quality and interpretation of stochastic outputs
- **Low Confidence:** Adaptation speed claims are contradictory within the paper itself

## Next Checks

1. **Resolve internal contradiction:** Replicate strategy switch experiments to determine whether LLMs or humans adapt faster, using exact same protocols and measuring "rounds to detect" systematically
2. **Prompt sensitivity test:** Run full tournament across multiple prompt variants (neutral, competitive, cooperative) to quantify how strongly framing affects performance and behavioral metrics
3. **Context window validation:** Test models with truncated vs full history to measure impact of context length on adaptation capability and long-term strategy detection