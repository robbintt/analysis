---
ver: rpa2
title: 'Inner-Probe: Discovering Copyright-related Data Generation in LLM Architecture'
arxiv_id: '2410.04454'
source_url: https://arxiv.org/abs/2410.04454
tags:
- copyright
- data
- copyrighted
- text
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Inner-Probe addresses the challenge of identifying which specific
  copyrighted sub-datasets influence Large Language Model (LLM) outputs and filtering
  non-copyrighted responses. It leverages multi-head attention (MHA) outputs from
  the LLM generation process, using a lightweight LSTM-based framework to analyze
  sub-dataset contributions in a supervised manner.
---

# Inner-Probe: Discovering Copyright-related Data Generation in LLM Architecture

## Quick Facts
- arXiv ID: 2410.04454
- Source URL: https://arxiv.org/abs/2410.04454
- Reference count: 40
- Primary result: Framework for identifying copyrighted training data influence and filtering non-copyrighted responses with >94.9% accuracy and 93.83% filtering accuracy

## Executive Summary
Inner-Probe addresses the challenge of tracing copyrighted content in LLM outputs by analyzing Multi-Head Attention (MHA) outputs during generation. The framework uses a lightweight LSTM-based architecture to identify which specific copyrighted sub-datasets influenced the output and filter non-copyrighted responses. By extracting pivotal tokens from MHA features and leveraging both supervised classification and unsupervised contrastive learning, Inner-Probe achieves state-of-the-art performance while being 3x more efficient than existing methods.

## Method Summary
Inner-Probe operates by extracting MHA outputs from frozen LLMs during the generation process, then applying token extraction strategies (Interval or Variance-based) to reduce the context to a fixed-size representation. This representation is processed by an LSTM backbone to classify sub-dataset contributions in a supervised manner. For filtering non-copyrighted responses, the framework employs unsupervised contrastive learning on the MHA features, using the pre-trained LSTM as a prior to identify distributional outliers via Mahalanobis distance.

## Key Results
- Achieves over 94.9% accuracy in sub-dataset contribution analysis across 8 Pile dataset classes
- Filters non-copyrighted data with 93.83% accuracy and AUC of 0.954, outperforming existing methods
- Training completes in under 1.5 hours on a single A100 GPU, 3x more efficient than alternatives

## Why This Works (Mechanism)

### Mechanism 1: MHA vs FFN Causality
MHA outputs encode causal dependencies of training data more effectively than FFN, as evidenced by non-diagonal inverse covariance matrices showing flexible token-wise dependencies. FFN's position-invariant transformations create near-diagonal covariance, blocking deeper causal learning.

### Mechanism 2: Information Bottleneck Token Extraction
Fixed token extraction using Information Bottleneck principles preserves class-relevant information while discarding redundancy. The framework extracts k pivotal tokens that maximize mutual information between the class and extracted tokens while minimizing redundancy with full input.

### Mechanism 3: Contrastive Filtering with LSTM Prior
Supervised LSTM training creates structured embeddings that serve as a prior for unsupervised contrastive learning. This forces the model to learn high-level structural features of copyrighted text, allowing non-copyrighted responses to be identified as distributional outliers.

## Foundational Learning

- **Information Bottleneck (IB) Principle**: Used to justify why extracting only k tokens improves performance by balancing compression and prediction. Quick check: How does KIB balance I(Y;O) minimization with I(C;O) maximization?

- **Attention Covariance & Causality**: Explains why MHA is probed instead of FFN based on inverse covariance matrix structure. Quick check: Why does diagonal inverse covariance in FFN suggest reduced suitability for tracing data lineage?

- **Mahalanobis Distance for OOD Detection**: Mathematical basis for filtering module, measuring standard deviations from distribution mean while accounting for variable correlations. Quick check: What defines the mean and covariance matrix for calculating distance of non-copyrighted responses?

## Architecture Onboarding

- **Component map**: LLM (Frozen) -> MHA Extractor -> Token Extractor (Interval/A-Var) -> LSTM Backbone -> Head 1 (Classifier) + Head 2 (Projector)

- **Critical path**: 1) Hook into LLM generation loop to capture MHA outputs during prefilling/decoding; 2) Apply token extraction logic (Algorithm 1); 3) Verify LSTM input dimensions match LLM hidden size × number of layers

- **Design tradeoffs**: Token selection k=5 is sweet spot (higher k adds noise, lower k misses context); GPT-series shows clearest "trajectory" signals when trained on target data

- **Failure signatures**: Accuracy drops ~3% for sequences 256→512 tokens (mitigate by segmentation); Sparse class collapse degrades attribution accuracy significantly

- **First 3 experiments**: 1) Visualize MHA vs FFN UMAP projections for models trained vs untrained on known dataset; 2) Compare Interval vs Variance extraction on 3-class Pile subset; 3) Plot Mahalanobis distance distributions for known non-copyrighted texts to find threshold δ

## Open Questions the Paper Calls Out

- **Can Inner-Probe adapt to Large Multimodal Models (LMMs)**: Future work will verify effectiveness across non-textual modalities, but current architecture is text-specific
- **Performance on low-data copyrighted sub-datasets**: Framework degrades significantly when specific classes have extremely few training samples
- **Long-context generation without segmentation**: Current 3% accuracy drop for long sequences requires segmentation; architectural alternatives remain unexplored

## Limitations

- Core assumptions about MHA causality may not generalize to different LLM architectures with linear attention or other mechanisms
- Token extraction strategy relies on heuristic choice of k=5 without comprehensive ablation across all datasets
- Filtering performance depends on distributional assumptions that may fail when non-copyrighted data shares stylistic features with copyrighted training data

## Confidence

- **High Confidence**: Supervised LSTM classifier for sub-dataset attribution is well-validated with clear metrics
- **Medium Confidence**: MHA causality argument supported by inverse covariance analysis but untested across diverse architectures
- **Low Confidence**: Unsupervised filtering approach depends on distributional assumptions not robustly tested across diverse data types

## Next Checks

1. **Generalization Across Architectures**: Replicate MHA vs FFN causality analysis on at least two additional LLM architectures to confirm signal is not idiosyncratic to GPT-Neo/J

2. **Token Extraction Ablation**: Systematically vary k (1, 3, 5, 7, 10) and extraction strategy on Pile subset, measuring both KIB and downstream accuracy

3. **Filtering Robustness**: Construct test set where non-copyrighted data intentionally shares high-level stylistic features with copyrighted training data, measuring false negative rates