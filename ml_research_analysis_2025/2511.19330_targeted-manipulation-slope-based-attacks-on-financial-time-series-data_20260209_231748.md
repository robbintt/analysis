---
ver: rpa2
title: 'Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data'
arxiv_id: '2511.19330'
source_url: https://arxiv.org/abs/2511.19330
tags:
- adversarial
- attacks
- data
- attack
- slope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses adversarial attacks on financial time-series
  forecasting models, specifically targeting N-HiTS models trained on S&P 500 stock
  data. Two novel slope-based attack methods are introduced: General Slope Attack
  (GSA) and Least-Squares Slope Attack (LSSA), which manipulate forecast trends by
  altering the slope of predictions.'
---

# Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data

## Quick Facts
- arXiv ID: 2511.19330
- Source URL: https://arxiv.org/abs/2511.19330
- Reference count: 29
- Key outcome: Two novel slope-based attacks (GSA, LSSA) successfully manipulate N-HiTS financial forecasting models, doubling forecast slopes while evading discriminator defenses with ~28% specificity

## Executive Summary
This paper introduces General Slope Attack (GSA) and Least-Squares Slope Attack (LSSA), two novel adversarial methods targeting financial time-series forecasting models. Unlike traditional point-wise attacks, these methods manipulate the slope of predictions by embedding trend-based objectives directly into the loss function. Tested against N-HiTS models trained on S&P 500 data, GSA and LSSA achieve significant slope manipulation (doubling upward slopes, reducing downward slopes below zero) while using minimal perturbations (2% relative epsilon). The attacks successfully bypass standard discriminator-based defenses, reducing a CNN's specificity to 28% and accuracy to 57%. The study also highlights broader ML security vulnerabilities by demonstrating how attacks can target the entire inference pipeline, not just the model itself.

## Method Summary
The paper presents two slope-based adversarial attack methods (GSA and LSSA) targeting N-HiTS financial forecasting models. GSA uses discrete endpoint slope calculations while LSSA employs least-squares regression across all prediction points, both optimizing trend manipulation through iterative gradient updates. The attacks are tested against baseline methods (FGSM, BIM, MI-FGSM, SIM, TIM) using 2% relative epsilon perturbation on S&P 500 stock data. Additionally, a GAN architecture (A-GAN) is developed to generate synthetic adversarial examples, though it suffers from mode collapse. The study also demonstrates a malware-based attack vector that compromises the entire ML pipeline by intercepting model calls and inserting adversarial perturbations.

## Key Results
- GSA and LSSA successfully double forecast slopes in upward direction (Gen. Slope increases from 3.37×10⁻² to 6.76×10⁻²)
- Attacks reduce CNN discriminator specificity to 28% and accuracy to 57%, demonstrating effective stealth
- Slope-based attacks outperform baseline methods (FGSM, BIM, MI-FGSM, SIM, TIM) in trend manipulation while maintaining input realism
- A-GAN generates realistic synthetic data (low MMD scores) but suffers from mode collapse, limiting practical effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Embedding slope objectives directly into the loss function enables targeted manipulation of temporal trends without requiring per-input target sequences
- GSA uses discrete endpoint slope (m = (y₂-y₁)/(x₂-x₁)), LSSA uses least-squares regression slope across all prediction points, both feeding into directional loss functions (c·e^(-tdm) for directional attacks or c·m² for zero-slope targets)
- Core assumption: Financial forecasting models encode trend information in differentiable ways with respect to input perturbations
- Evidence: Slope doubling demonstrated empirically, related work on time-series attacks supports stealthiness claims
- Break condition: Non-differentiable model predictions (e.g., discrete quantization) would prevent gradient-based optimization

### Mechanism 2
- Minimal perturbation magnitudes (ε = 2%) paired with temporally-constrained noise preserve input realism while evading discriminator-based defenses
- Attacks use relative epsilon and clip perturbations within ε-neighborhood; SIM baseline uses cosine similarity checks
- Core assumption: Detection models trained on point-wise or distributional anomalies do not effectively capture trend-level manipulations
- Evidence: CNN discriminator accuracy drops to ~52-56% on slope attacks versus higher accuracy on baseline attacks
- Break condition: Defenders training discriminators specifically on slope/statistical trend features would likely improve detection rates

### Mechanism 3
- Attacking the inference pipeline (not just the model) bypasses gradient access restrictions and input filtering
- Malware injected into `__init__.py` intercepts model calls, removes `torch.no_grad()` constraints, and inserts adversarial perturbation code
- Core assumption: Deployed ML systems trust their local package environment and lack integrity verification
- Evidence: Proposed malware payload structure demonstrates practical feasibility
- Break condition: Hash-verified installations or sandboxed inference environments would detect tampering

## Foundational Learning

- **Adversarial perturbations in regression vs. classification**
  - Why needed: Financial forecasting is regression; loss functions and attack objectives differ from image classification
  - Quick check: Can you explain why minimizing L1 loss to a target sequence differs from maximizing cross-entropy against a true label?

- **N-HiTS hierarchical interpolation**
  - Why needed: The victim model samples time-series at multiple rates via MaxPool and hierarchical interpolation; perturbations must propagate through this multi-scale structure
  - Quick check: How does multi-rate sampling affect where gradients flow during backpropagation?

- **Wasserstein GANs with gradient penalty (WGAN-GP)**
  - Why needed: The A-GAN uses conditional WGAN-GP; understanding critic loss, gradient penalty, and mode collapse is essential for reproducing results
  - Quick check: Why does Wasserstein distance help training stability compared to vanilla GAN divergence?

## Architecture Onboarding

- **Component map**: S&P 500 CRSP data -> N-HiTS victim model (100-day encoder, 20-day forecast) -> GSA/LSSA attack modules -> CNN discriminator defense -> A-GAN for synthetic adversarial examples

- **Critical path**: 1) Train N-HiTS on CRSP S&P 500 data (360 recordings, 75/10/15 split) 2) Generate PyTorch features (rolling stats, log returns, EMA, day-of-week) 3) Run iterative attacks (ε = 2%) on first 300 days 4) Evaluate slope distortion and error metrics 5) Train CNN discriminator on adversarial vs. real inputs

- **Design tradeoffs**: GSA preserves mid-series points (stealthier) but only shifts endpoints; LSSA alters full trend but is more detectable. A-GAN generates realistic data (low MMD) but suffers mode collapse—outputs converge to similar patterns. Adversarial training is infeasible for N-HiTS due to rolling-window feature dependencies and data volume.

- **Failure signatures**: Gradient flow blocked (check for `torch.no_grad()` or detached tensors), mode collapse in A-GAN (outputs show identical initial surge + drop-off patterns), discriminator overfitting (high accuracy on training attack types, near-chance on novel attacks)

- **First 3 experiments**: 1) Reproduce GSA/LSSA on APO stock with ε = 2%; verify slope doubling via Table 1 metrics 2) Train CNN discriminator on FGSM/BIM attacks only; test generalization to GSA/LSSA 3) Implement A-GAN with reduced adversarial loss scaling (α = 0.1–0.35); measure MMD and mode diversity across 100 generated samples

## Open Questions the Paper Calls Out

- **Cross-architecture effectiveness**: Do GSA and LSSA retain their efficacy against other standard time-series architectures like LSTMs and CNNs? The study focused exclusively on N-HiTS architecture.

- **Adversarial training defense**: Can standard adversarial training effectively defend against slope-based attacks in financial forecasting? Implementation constraints prevented inclusion of adversarial training for the primary victim model.

- **Domain generalizability**: Can slope-based attacks successfully manipulate temporal characteristics in non-financial domains such as energy or traffic forecasting? Research scope was limited to financial stock data.

- **A-GAN stability**: How can the A-GAN architecture be stabilized to prevent mode collapse while preserving adversarial capabilities? The current A-GAN generates limited diversity, making adversarial examples easier to detect.

## Limitations
- Attack effectiveness limited to N-HiTS architecture; generalizability to other forecasting models (LSTMs, Transformers, ARIMA) remains untested
- A-GAN suffers from severe mode collapse, producing near-identical synthetic outputs that limit practical deployment
- Malware pipeline attack represents extreme threat model assuming full system compromise rather than more realistic attack surfaces

## Confidence
- **High confidence**: Slope-based attack mechanisms (GSA/LSSA) and their demonstrated effectiveness in manipulating forecast trends with minimal perturbation
- **Medium confidence**: Stealthiness claims against discriminator-based defenses, as the CNN discriminator used may not represent state-of-the-art detection methods
- **Low confidence**: Practical scalability of A-GAN for generating diverse adversarial examples, given observed mode collapse

## Next Checks
1. Test GSA/LSSA against a Transformer-based forecasting model (e.g., Informer) to evaluate cross-architecture effectiveness and determine if hierarchical interpolation is essential to attack success
2. Implement and evaluate a more sophisticated discriminator (e.g., temporal convolutional network with attention) trained specifically on slope and statistical trend features to assess whether current stealth claims hold against stronger defenses
3. Investigate alternative GAN architectures (e.g., StyleGAN or Progressive GAN adapted for time-series) to address mode collapse in A-GAN and improve synthetic data diversity while maintaining low MMD scores