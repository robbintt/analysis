---
ver: rpa2
title: NOMAD Projection
arxiv_id: '2505.15511'
source_url: https://arxiv.org/abs/2505.15511
tags:
- projection
- nomad
- data
- t-sne
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOMAD Projection, a new method for unstructured
  data visualization that enables distributed computation on multiple GPUs by approximating
  an upper bound on the InfoNC-t-SNE loss. The method partitions data using K-Means
  clustering and distributes clusters across GPUs, eliminating the need for inter-device
  communication during positive force calculation while minimizing communication for
  negative forces.
---

# NOMAD Projection

## Quick Facts
- **arXiv ID**: 2505.15511
- **Source URL**: https://arxiv.org/abs/2505.15511
- **Reference count**: 27
- **Primary result**: NOMAD Projection achieves 6.1% ± 0.3% neighborhood preservation on PubMed in 1.47 hours using 8 H100 GPUs, comparable to OpenTSNE's 6.2% in 8 hours on CPUs.

## Executive Summary
NOMAD Projection is a new method for unstructured data visualization that enables distributed computation on multiple GPUs by approximating an upper bound on the InfoNC-t-SNE loss. The method partitions data using K-Means clustering and distributes clusters across GPUs, eliminating the need for inter-device communication during positive force calculation while minimizing communication for negative forces. Theoretical analysis shows NOMAD Projection provides an upper bound approximation to InfoNC-t-SNE. Empirical results demonstrate that NOMAD Projection achieves similar or superior neighborhood preservation and random triplet accuracy compared to state-of-the-art GPU implementations of t-SNE and UMAP on ArXiv and ImageNet datasets. The method's scalability is showcased by computing the first complete data map of 60 million articles from Multilingual Wikipedia in approximately 5.8 hours.

## Method Summary
NOMAD Projection partitions data via K-Means clustering and distributes clusters across GPUs, enabling zero-communication positive force computation since neighbors are guaranteed to reside within the same cluster. The method approximates InfoNC-t-SNE by replacing individual negative samples with weighted cluster means, requiring only O(|R|) communication complexity for negative forces through all-gather operations. This upper bound approximation is derived using Jensen's inequality and first-order Taylor expansion around cluster means. The algorithm computes exact k-nearest neighbors within clusters to form an ANN graph, then uses distributed SGD with linear learning rate decay from n/10 to 0, initialized from PCA embeddings.

## Key Results
- Achieves 6.1% ± 0.3% neighborhood preservation on PubMed in 1.47 hours using 8 H100 GPUs, comparable to OpenTSNE's 6.2% in 8 hours on CPUs
- Computes the first complete data map of 60 million articles from Multilingual Wikipedia in approximately 5.8 hours
- Demonstrates 5.4x speedup over CPU-based OpenTSNE while maintaining comparable quality metrics
- Shows consistent quality across ArXiv, ImageNet, and PubMed datasets with multi-GPU scaling

## Why This Works (Mechanism)

### Mechanism 1: K-Means Clustering for Zero-Communication Positive Force Computation
Partitioning data via K-Means clustering enables each GPU to compute positive spring forces independently, eliminating inter-device communication during this phase. K-Means creates clusters where each point's approximate nearest neighbors reside within the same cluster. By sharding complete clusters (not individual points) across GPUs, all neighbors required for positive force calculation remain local to each device.

Core assumption: K-Means clusters sufficiently approximate true k-nearest neighbor relationships for visualization purposes.

### Mechanism 2: Upper Bound Approximation via Cluster Mean Substitution
Replacing individual negative samples with weighted cluster means produces an upper bound approximation of the InfoNC-t-SNE loss, enabling efficient distributed computation. By applying Jensen's inequality to the log-sum-exp term in InfoNC-t-SNE and using Taylor expansion around cluster means, the negative force computation reduces to communicating only cluster means rather than all data points.

Core assumption: The first-order Taylor approximation holds sufficiently well for optimization.

### Mechanism 3: Minimal Inter-Device Communication via All-Gathered Cluster Means
After each epoch, only broadcasting cluster means (not full data) enables accurate negative force approximation with O(|R|) communication complexity instead of O(n). Negative forces are computed as weighted sums over cluster means, with optional exact sampling for a subset of non-approximated cells.

Core assumption: Cluster means provide sufficient statistics for repulsive force approximation at scale.

## Foundational Learning

- **Concept: InfoNC-t-SNE and Contrastive Learning Framework for DR**
  - Why needed here: NOMAD is explicitly derived as an upper bound on InfoNC-t-SNE. Understanding the spring system analogy (attractive forces for positive pairs, repulsive for negative pairs) is essential for grasping why the approximation works.
  - Quick check question: Can you explain why replacing individual negative samples with cluster means might preserve the repulsive effect while reducing communication?

- **Concept: K-Means Clustering and ANN Index Construction**
  - Why needed here: NOMAD uses K-Means not just for clustering but as the ANN index itself—neighbors are only sought within clusters. Understanding this coupling is critical for debugging partition quality issues.
  - Quick check question: What happens to neighbor recall if K-Means converges to a poor local minimum with unbalanced clusters?

- **Concept: Distributed GPU Communication Patterns (All-Gather)**
  - Why needed here: The key innovation is minimizing communication to only cluster means. Understanding all-gather operations helps diagnose scaling bottlenecks.
  - Quick check question: Why is all-gathering cluster means cheaper than all-gathering gradients or embeddings?

## Architecture Onboarding

- **Component map**: K-Means ANN Index -> Cluster Sharder -> Positive Force Calculator -> Cluster Mean Aggregator -> Negative Force Calculator -> SGD Optimizer
- **Critical path**: K-Means quality -> ANN graph connectivity -> Positive force accuracy -> Cluster mean representativeness -> Negative force accuracy -> Final embedding quality
- **Design tradeoffs**:
  - More clusters (higher |R|) -> better negative force approximation but more communication overhead
  - Larger clusters -> better local ANN recall but higher per-GPU memory pressure
  - Multi-GPU scaling improves neighborhood preservation but may slightly degrade random triplet accuracy
- **Failure signatures**:
  - High imbalance in cluster sizes -> OOM on some GPUs, underutilization on others
  - Poor K-Means convergence -> fragmented ANN graph, visible as disconnected regions in visualization
  - Excessive approximation (˜R too small) -> loss of global structure, poor random triplet accuracy
- **First 3 experiments**:
  1. Single-GPU baseline comparison: Run NOMAD on ArXiv/ImageNet with 1 GPU, compare NP@10 and triplet accuracy vs t-SNE-CUDA and RapidsUMAP
  2. Scaling sweep: Run PubMed-scale dataset on 1, 2, 4, 8 GPUs. Measure wall-clock time, communication overhead percentage, and quality metrics
  3. Ablation on partition granularity: Vary the number of clusters |R| while holding GPU count constant. Plot communication volume vs. random triplet accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can NOMAD Projection be effectively extended to multi-node networks, and how do inter-node latency constraints impact its scaling efficiency compared to single-node performance? The current implementation is restricted to a single node (multiple GPUs), and the authors note that the extension is non-trivial due to hardware communication profiles.

### Open Question 2
How can the partitioning strategy be modified to mitigate the observed degradation in global structure preservation (random triplet accuracy) when scaling across multiple GPUs? The K-Means partitioning isolates clusters onto different devices, severing the positive spring forces between points in different clusters that typically help maintain global coherence.

### Open Question 3
Can the upper-bound approximation of InfoNCE used in NOMAD Projection be successfully applied to improve efficiency in general contrastive learning or language modeling tasks? The paper validates the method only on the data mapping (visualization) problem; its utility as a general surrogate loss for representation learning remains untested.

## Limitations
- Method critically depends on K-Means producing clusters that preserve k-nearest neighbor relationships, though this dependency is not extensively validated across diverse data distributions
- Practical tightness of the approximation for different cluster configurations and data types remains largely empirical
- Focus on neighborhood preservation metrics may not fully capture potential degradation in global structure when using cluster-based approximations

## Confidence

- **High confidence**: The distributed computation architecture and elimination of inter-GPU communication for positive forces are well-established technical contributions with clear theoretical grounding
- **Medium confidence**: The upper bound approximation proof and its practical implementation are sound, but the real-world approximation quality across diverse datasets needs broader validation
- **Medium confidence**: Empirical results showing superior scalability and comparable quality are convincing for tested datasets, but generalization to other data types and scales requires further verification

## Next Checks

1. **K-Means failure modes**: Systematically test NOMAD Projection on datasets where K-Means is known to fail (high-dimensional, non-globular clusters, varying densities) and measure degradation in NP@10 and triplet accuracy
2. **Communication-accuracy trade-off**: Design controlled experiments varying the number of clusters (R) and exact samples (˜R) to map the full Pareto frontier of communication cost versus approximation accuracy
3. **Cross-dataset generalization**: Evaluate NOMAD on diverse data types beyond ArXiv, ImageNet, and PubMed (e.g., genomics, temporal data, multi-modal embeddings) to assess robustness of the approximation across domains