---
ver: rpa2
title: 'Flatten Graphs as Sequences: Transformers are Scalable Graph Generators'
arxiv_id: '2502.02216'
source_url: https://arxiv.org/abs/2502.02216
tags:
- graph
- graphs
- generation
- dataset
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoGraph, a scalable autoregressive model
  for attributed graph generation using decoder-only transformers. The key innovation
  is transforming graphs into random sequences of tokens through a reversible process,
  enabling modeling as sequences without expensive node features.
---

# Flatten Graphs as Sequences: Transformers are Scalable Graph Generators

## Quick Facts
- arXiv ID: 2502.02216
- Source URL: https://arxiv.org/abs/2502.02216
- Authors: Dexiong Chen; Markus Krimmel; Karsten Borgwardt
- Reference count: 40
- One-line primary result: Scalable autoregressive transformer model for attributed graph generation with optimal linear complexity and state-of-the-art performance

## Executive Summary
AutoGraph introduces a novel approach to graph generation by transforming graphs into random sequences of tokens, enabling autoregressive modeling with decoder-only transformers. This method achieves optimal linear complexity with respect to edges, making it highly scalable for large sparse graphs. The approach demonstrates state-of-the-art performance on synthetic and molecular benchmarks while offering up to 100x faster generation and 3x faster training compared to leading diffusion models. AutoGraph also supports substructure-conditioned generation without fine-tuning and shows promising transferability capabilities.

## Method Summary
The paper proposes AutoGraph, a scalable autoregressive model for attributed graph generation that leverages decoder-only transformers. The key innovation is transforming graphs into random sequences of tokens through a reversible process, allowing graph generation to be modeled as sequence generation. This transformation enables the use of standard transformer architectures without expensive node features, achieving optimal linear complexity with respect to edges. The model can generate attributed graphs by predicting sequences of tokens that encode node attributes, edges, and graph structure in a unified framework.

## Key Results
- Achieves state-of-the-art performance on synthetic and molecular benchmarks
- Demonstrates up to 100x faster generation and 3x faster training than leading diffusion models
- Achieves optimal linear complexity with respect to edges for large sparse graphs
- Supports substructure-conditioned generation without fine-tuning
- Shows promising transferability capabilities across different graph domains

## Why This Works (Mechanism)
The method works by converting graph structures into sequential token representations that can be processed by standard transformers. By randomizing the sequence order, the model learns to reconstruct graph structure without requiring explicit node ordering. The reversible transformation ensures that generated sequences can be converted back into valid graphs. This approach eliminates the need for complex graph neural network architectures while maintaining the ability to capture complex graph patterns. The autoregressive nature allows for efficient parallel computation during training and controlled generation during inference.

## Foundational Learning

**Graph token sequence transformation**: Why needed: To enable transformer architectures to process graph data efficiently. Quick check: Verify that the transformation is bijective and preserves all graph information.

**Autoregressive modeling**: Why needed: To generate graphs sequentially in a controlled manner. Quick check: Confirm that each generated token conditions on all previous tokens.

**Decoder-only transformers**: Why needed: To leverage powerful sequence modeling capabilities without additional complexity. Quick check: Validate that the transformer architecture can handle the variable-length sequences.

**Optimal linear complexity**: Why needed: To ensure scalability for large graphs. Quick check: Measure computational complexity as a function of edge count.

## Architecture Onboarding

Component map: Graph -> Token Sequence Transformation -> Transformer Encoder -> Token Sequence Generation -> Graph Reconstruction

Critical path: The core pipeline flows from graph transformation through the transformer model to sequence generation and final graph reconstruction. The token sequence representation serves as the bridge between graph and sequence domains.

Design tradeoffs: The method sacrifices explicit graph structure preservation during intermediate steps for computational efficiency and scalability. The random sequence order eliminates node ordering bias but requires the model to learn structure from unordered data.

Failure signatures: Performance degradation may occur when graph attributes have complex dependencies, when graphs have highly regular structures that don't benefit from sequence modeling, or when the reversible transformation introduces information loss.

First experiments: 1) Validate basic sequence generation on simple graph types. 2) Test attribute prediction accuracy on synthetic graphs with known properties. 3) Benchmark generation speed against baseline methods on small graphs.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the broader applicability of the method to extremely large-scale graphs beyond those tested, the constant factors and practical runtime overhead in real-world scenarios, and the generalizability to other graph types and domains beyond synthetic and molecular benchmarks.

## Limitations
- Current benchmarks may not fully represent real-world scale challenges for extremely large graphs
- Constant factors and practical runtime overhead in real-world scenarios remain unclear
- Generalizability to other graph types and domains requires further investigation beyond synthetic and molecular benchmarks

## Confidence
High: The technical innovation of transforming graphs into sequences of tokens using a reversible process is well-supported and novel.

Medium: The claims of achieving up to 100x faster generation and 3x faster training than leading diffusion models are based on specific benchmarks and may vary across different graph types and sizes.

Low: The transferability capabilities and potential for building graph foundation models are promising but speculative, requiring more extensive validation across diverse domains.

## Next Checks
1. Evaluate AutoGraph's performance and scalability on real-world large-scale graphs from domains not covered in the current benchmarks, such as social networks or web graphs.

2. Conduct ablation studies to quantify the impact of each component of the AutoGraph method on its overall performance and efficiency.

3. Test the transferability of AutoGraph by pre-training on one graph domain and fine-tuning on another, measuring performance across multiple domain pairs to assess generalizability.