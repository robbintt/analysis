---
ver: rpa2
title: 'FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias
  and Stereotypes'
arxiv_id: '2506.23111'
source_url: https://arxiv.org/abs/2506.23111
tags:
- identity
- bias
- social
- india
- stereotypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces INDIC-BIAS, a comprehensive benchmark designed
  to evaluate fairness in large language models (LLMs) across 85 Indian identity groups
  spanning caste, religion, region, and tribe. The authors consulted experts to curate
  over 1,800 socio-cultural topics and generated 20,000 real-world scenario templates
  organized into three tasks: plausibility, judgment, and generation.'
---

# FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes

## Quick Facts
- **arXiv ID**: 2506.23111
- **Source URL**: https://arxiv.org/abs/2506.23111
- **Reference count**: 40
- **Primary result**: 14 popular LLMs show strong negative biases against marginalized Indian identities across caste, religion, region, and tribe, with stereotype association rates exceeding 50% and reaching 79% in generation tasks.

## Executive Summary
This paper introduces INDIC-BIAS, a comprehensive benchmark for evaluating fairness in large language models (LLMs) across 85 Indian identity groups spanning caste, religion, region, and tribe. The authors created over 1,800 socio-cultural topics and generated 20,000 real-world scenario templates organized into three tasks: plausibility, judgment, and generation. Their evaluation of 14 popular LLMs revealed strong negative biases against marginalized identities, with models frequently reinforcing stereotypes. The study found that LLMs struggle to mitigate bias even when explicitly asked to rationalize decisions, with stereotype association rates particularly high in generation tasks (exceeding 50% on average, up to 79% for larger models).

## Method Summary
The INDIC-BIAS benchmark evaluates LLM fairness using three tasks: Plausibility (choosing which identity is "more likely" in a scenario), Judgment (assigning blame/responsibility in ambiguous scenarios), and Generation (assessing long-form response quality and stereotype associations). The benchmark covers 85 Indian identity groups across caste, religion, region, and tribe, with 20,000+ manually verified scenario templates. Identity preferences are measured using ELO ratings via Bradley-Terry model, while stereotype associations are quantified using Stereotype Association Rate (SAR). An LLM-as-judge approach uses Llama-3.3-70B-Instruct for generation task evaluation, validated against human annotators (>90% agreement).

## Key Results
- LLMs exhibit strong negative biases against marginalized Indian identities, with RSM (Rank Shift Metric) showing consistently negative values for disadvantaged groups
- Stereotype association rates exceed 50% on average in generation tasks, reaching 79% for larger models like Llama-70B and Gemma-27B
- Explicit reasoning prompts (CoT) fail to consistently reduce bias, with varying effects across model families
- Criminal activities show the highest polarization in model judgments, with significant differences in how identities are treated in related scenarios

## Why This Works (Mechanism)

### Mechanism 1: Template-Based Identity Substitution Reveals Differential Treatment
Pairwise identity comparison in controlled scenarios exposes hidden biases that single-identity evaluations miss. Scenario templates contain identity placeholders that can be systematically replaced. By presenting models with identical scenarios differing only in identity labels and asking for preference/plausibility judgments, differential treatment becomes measurable through ELO rankings and RSM.

### Mechanism 2: Stereotype Amplification in Open-Ended Generation
Free-form generation tasks expose deeper stereotypical associations than constrained tasks because models can surface implicit biases without forced choices. Models generate long-form content when presented with scenarios containing subtly embedded stereotypes, and evaluator LLMs check whether stereotypes are correctly associated with their target identities.

### Mechanism 3: Limited Mitigation from Explicit Reasoning Prompts
Chain-of-thought prompting does not consistently reduce bias and may increase it in some models. Models were prompted to reason through decisions before selecting identities, but results varied—some models showed improved refusal rates while others showed no improvement or even reduced refusal (increased bias).

## Foundational Learning

- **Concept: Allocative vs. Representational Harms**
  - Why needed here: The paper explicitly identifies both harm types—allocative (unequal resource distribution) and representational (misrepresentation). Understanding this distinction is prerequisite to interpreting the three tasks.
  - Quick check question: When a model recommends vocational careers to Dalit students but academic careers to Brahmin students, is this allocative or representational harm? (Answer: Allocative—unequal opportunity distribution.)

- **Concept: ELO Rating Systems for Comparative Evaluation**
  - Why needed here: The paper uses Bradley-Terry model-based ELO ratings to rank identity preferences from pairwise comparisons. This differs from absolute metrics—ELO reflects relative standing within a population of identities.
  - Quick check question: If Identity A has ELO 1200 in positive scenarios and 1100 in negative scenarios, while Identity B has ELO 1100 in positive and 1200 in negative, what does RSM indicate? (Answer: Identity A has RSM = -100, indicating positive bias; Identity B has RSM = +100, indicating negative bias.)

- **Concept: LLM-as-Judge Paradigm with Validation**
  - Why needed here: The generation task uses Llama-3.3-70B-Instruct as an evaluator to assess response quality and stereotype associations. This enables scaling evaluation beyond human feasibility, but requires validation.
  - Quick check question: If evaluator-LLM agreement were 60% instead of 90%, what would be the implications for SAR measurements? (Answer: SAR measurements would be unreliable; stereotypes flagged by the evaluator might not align with human perception.)

## Architecture Onboarding

- **Component map**: Taxonomy Layer (expert-curated topics) → Scenario Generation Layer (GPT-4o templates) → Template Population Layer (identity placeholders) → Model Evaluation Layer (14 LLMs, 3 tasks) → Metric Computation Layer (ELO/RSM/SAR) → LLM Evaluator Layer (Llama-3.3-70B-Instruct)

- **Critical path**: Define identity groups → Consult sociologists for bias/stereotype taxonomy → Generate scenario templates via GPT-4o → Human verification → Populate templates with identity pairs → Run model evaluations → Compute metrics → LLM-as-judge evaluation → Construct-level analysis

- **Design tradeoffs**:
  - Scale vs. Depth: 20,000 templates provide broad coverage but limit per-identity scenario depth
  - Automated vs. Manual: GPT-4o generation enables scale but requires 5-28% human filtering
  - Pairwise vs. Absolute: ELO-based comparisons reveal relative bias but don't indicate absolute fairness
  - Evaluator LLM vs. Human: Llama-3.3-70B-Instruct enables scalable evaluation but introduces potential evaluator bias

- **Failure signatures**:
  1. High refusal rates (>60%) on stereotype prompts
  2. Low SAR variance across identities
  3. RSM near zero for all identities
  4. Evaluation-human agreement <85%

- **First 3 experiments**:
  1. Establish baseline metrics for a new model on caste identity pairs, compute ELO rankings and RSM
  2. Validate evaluator LLM with human annotators on 50 generation responses
  3. Test mitigation intervention with CoT prompting on judgment task

## Open Questions the Paper Calls Out

### Open Question 1
Can intersectional identity analysis (e.g., caste+religion, region+religion combinations) reveal compounded or distinct bias patterns beyond those observed for individual identity axes? The authors state they do not consider intersectional identities and leave this analysis for future work.

### Open Question 2
Why does Chain-of-Thought (CoT) prompting fail to consistently reduce bias across different model families, improving refusal rates for some (Gemma-27B) while worsening them for others (Mixtral)? The mechanism by which CoT affects bias-related decisions appears model-dependent.

### Open Question 3
What explains the counterintuitive finding that larger models exhibit higher stereotype association rates (up to 79% for Llama-70B and Gemma-27B) compared to smaller models? This contradicts the expectation that larger, more capable models should be better aligned and less prone to stereotyping.

### Open Question 4
What mitigation strategies can effectively reduce the documented allocative and representational harms toward Indian identities without compromising model utility? The study is limited to probing and detecting biases without proposing mitigation strategies.

## Limitations
- The benchmark was developed using GPT-4o-generated scenarios with human verification filtering 5-28% of content, potentially missing less-documented socio-cultural contexts
- The LLM-as-judge approach introduces potential evaluator bias, though validated at >90% human agreement
- The benchmark does not address intersectional identities, leaving analysis of compounded bias patterns for future work

## Confidence
- **High Confidence (9/10)**: Core finding of negative bias against marginalized identities supported by multiple independent metrics across 14 models
- **Medium Confidence (7/10)**: Generation tasks amplify stereotypes more than constrained tasks, though interpretation depends on LLM evaluator reliability
- **Low Confidence (5/10)**: CoT prompting's inconsistent effects on bias reduction across model families not well understood

## Next Checks
1. **Human Evaluator Replication**: Select 100 generation task responses from most biased model-category combinations; have three independent human annotators assess stereotype associations and response quality; compute inter-annotator agreement.

2. **Temporal Stability Test**: Re-run evaluation on same 14 models after 6 months using subset of 1,000 templates; compare RSM and SAR values to assess whether identified biases persist or shift over time.

3. **Cross-Cultural Generalization**: Apply INDIC-BIAS methodology to another culturally diverse context (e.g., African identities or European regional identities) using same three-task structure to test methodology generalizability.