---
ver: rpa2
title: 'CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset'
arxiv_id: '2509.14161'
source_url: https://arxiv.org/abs/2509.14161
tags:
- language
- speech
- code-switched
- pairs
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CS-FLEURS is a new dataset for developing and evaluating code-switched
  speech recognition and translation systems across 52 languages and 113 unique language
  pairs. The dataset includes 4 test sets: 1) 14 X-English pairs with real read speech,
  2) 16 X-English pairs with generative TTS, 3) 60 {Arabic, Mandarin, Hindi, Spanish}-X
  pairs with generative TTS, and 4) 45 X-English lower-resourced pairs with concatenative
  TTS.'
---

# CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset

## Quick Facts
- arXiv ID: 2509.14161
- Source URL: https://arxiv.org/abs/2509.14161
- Reference count: 0
- New dataset for code-switched speech recognition and translation across 52 languages and 113 unique language pairs

## Executive Summary
CS-FLEURS is a new dataset for developing and evaluating code-switched speech recognition and translation systems across 52 languages and 113 unique language pairs. The dataset includes 4 test sets: 1) 14 X-English pairs with real read speech, 2) 16 X-English pairs with generative TTS, 3) 60 {Arabic, Mandarin, Hindi, Spanish}-X pairs with generative TTS, and 4) 45 X-English lower-resourced pairs with concatenative TTS. Additionally, it provides 128 hours of generative TTS training data across 16 X-English pairs. The dataset uses synthetically generated code-switched text from LLM prompts and simple alignment-then-swap methods. Experiments show that Whisper struggles with transcribing code-switched speech, especially for distinct-script pairs, but performs better on translation. Training on synthetic code-switched data improves model performance on both seen and unseen language pairs.

## Method Summary
The CS-FLEURS dataset is created through a pipeline of text generation and speech synthesis. Code-switched text is generated either through GPT-4o prompts with human validation (for 14 real speech pairs) or through an align-then-swap method using AwesomeAlign (for 99 pairs). The swap method identifies word alignments between monolingual sentences and swaps 30% of nouns, verbs, and adjectives. Speech is synthesized using either XTTS-v2 (voice conversion from FLEURS speakers) or MMS-TTS (concatenative synthesis). Quality filtering uses MMS-ZS forced alignment to remove the bottom 5% FAS per pair. The dataset provides 128 hours of training data across 16 X-English pairs, with synthetic speech for 99 test pairs and real human-read speech for 14 pairs.

## Key Results
- Whisper struggles with code-switched ASR, especially for distinct-script pairs (3x higher error rates than same-script pairs)
- ST-to-English performance is more robust than direct ASR transcription, bypassing mixed-script output challenges
- Training on synthetic code-switched data improves performance on both seen (14.38→12.67 CER) and unseen (31.77→26.24 CER) language pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on synthetic code-switched data improves ASR performance on both seen and unseen language pairs.
- **Mechanism:** Synthetic data exposes the model to code-switching patterns (word-level insertions, script transitions) that are underrepresented in monolingual training corpora. The align-then-swap method provides consistent code-mixing frequency (CMI standard deviation 4.0 vs 10.8 for LLM methods), creating stable training signal across pairs.
- **Core assumption:** The model learns transferable code-switching representations that generalize beyond specific language pairs seen during training.
- **Evidence anchors:**
  - [abstract] "Training on synthetic code-switched data improves model performance on both seen and unseen language pairs"
  - [section] Table 9: Seen pairs improved from 14.38→12.67 CER; unseen pairs improved from 31.77→26.24 CER with augmentation
  - [corpus] Related paper "AsyncSwitch" (arXiv:2506.14190) addresses asynchronous text-speech adaptation for code-switched ASR but reports similar scalability challenges with synthetic audio generation
- **Break condition:** If synthetic speech artifacts (speaker discontinuities in MMS-TTS, unnatural prosody in XTTS) dominate learning signal, performance gains may not transfer to natural code-switched speech.

### Mechanism 2
- **Claim:** Code-switched speech translation (ST) to English is more robust than direct transcription (ASR).
- **Mechanism:** ST bypasses the need to produce orthographically correct mixed-script output. The decoder only generates English tokens, eliminating script-switching errors in the output space while allowing the encoder to leverage cross-lingual representations.
- **Core assumption:** Whisper's encoder representations sufficiently capture semantic content across mixed-language input, even when decoder struggles with script alternation.
- **Evidence anchors:**
  - [abstract] "Whisper struggles with transcribing code-switched speech, especially for distinct-script pairs, but performs better on translation"
  - [section] Figure 2: ST BLEU scores on CS-FLEURS are comparable to monolingual FLEURS (e.g., READ-TEST: 18.46 vs 23.81 BLEU), while ASR CER is 2x+ higher
  - [corpus] No direct corpus evidence found for ST vs ASR comparison in code-switching; this appears to be a novel finding
- **Break condition:** If translation target is not English (especially distinct-script targets), the advantage may disappear—paper explicitly notes "we leave benchmarking ST for non-English targets to future work."

### Mechanism 3
- **Claim:** Distinct-script language pairs cause significantly higher ASR error than same-script pairs.
- **Mechanism:** Decoding code-switched speech requires the model to switch between tokenizers/character sets within a single utterance. Same-script pairs (e.g., Spanish-English) share Latin characters, reducing output vocabulary conflicts. Distinct-script pairs (e.g., Mandarin-English) require orthogonal character set decisions at switch points.
- **Core assumption:** The bottleneck is primarily in the decoder's ability to produce mixed-script output, not in acoustic-linguistic alignment.
- **Evidence anchors:**
  - [abstract] "struggles with transcribing code-switched speech, especially for distinct-script pairs"
  - [section] Table 8: Same-script CER ranges 7.32-28.26; distinct-script CER ranges 32.33-51.62 (3x higher on average)
  - [section] Page 4: "code-switched ASR performance is limited by the ability to interchangeably produce two distinct scripts within a single utterance-level decoding"
  - [corpus] Related paper "UniCoM" addresses universal code-switching generation but doesn't directly validate the script-decoding hypothesis
- **Break condition:** If acoustic quality differs systematically between same-script and distinct-script synthetic speech (not controlled in paper), some degradation may be TTS artifact rather than decoding difficulty.

## Foundational Learning

- **Concept:** Matrix Language-Frame (MLF) Model
  - **Why needed here:** CS-FLEURS uses MLF to structure all code-switched pairs—one language provides grammatical skeleton (matrix), another provides inserted elements (embedded). Understanding this is essential for interpreting which language drives syntactic structure vs. lexical borrowing.
  - **Quick check question:** In a "Mandarin-English" pair per CS-FLEURS notation, which language provides the grammatical structure and which provides embedded words?

- **Concept:** Code-Mixing Index (CMI)
  - **Why needed here:** CMI quantifies switching frequency in generated text. The paper optimizes for consistent CMI (lower variance) across pairs to ensure comparable difficulty. Table 6 shows align-then-swap achieves SD=4.0 vs LLM's SD=10.8.
  - **Quick check question:** If CMI varies widely across language pairs in your test set, what validity concern does this create for cross-pair performance comparison?

- **Concept:** Forced Alignment Score (FAS) for Quality Filtering
  - **Why needed here:** FAS from MMS-ZS provides language-universal intelligibility metric for filtering synthetic speech. Table 7 shows filtered speech has worse FAS (-0.68 vs -0.26) and higher CER (32.95 vs 18.46). Understanding FAS thresholds is critical for reproducing the dataset pipeline.
  - **Quick check question:** Why can't WER/CER alone filter synthetic speech quality before human evaluation?

## Architecture Onboarding

- **Component map:** FLEURS Sentences (2009/utt/lang) → Text Generation Branch: LLM Path (GPT-4o) → Human validation → CS-FLEURS-READ OR Align-then-Swap → XTTS-v2 / MMS-TTS → FAS Filtering → CS-FLEURS-XTTS/MMS → Quality Control: MMS-ZS forced alignment → Filter bottom 5% FAS per pair → Speech Synthesis: XTTS-v2 (17 langs, voice conversion from FLEURS speakers) OR MMS-TTS (concatenative, 100ms silence gaps)

- **Critical path:** Text generation quality → TTS intelligibility → FAS filtering threshold. The align-then-swap method's consistency (low CMI variance) is what enables scaling to 113 pairs without per-pair human validation.

- **Design tradeoffs:**
  - **LLM vs Align-then-Swap:** LLM produces more natural morphological code-switching (fluency rating 1.28 vs implicit lower for swap) but has 10%+ reject rate and high API cost. Swap is cheap and consistent but lacks morphological adaptation.
  - **XTTS vs MMS-TTS:** XTTS provides accented speech with natural prosody but can produce unintelligible output (Table 7: CER 32.95 filtered vs 18.46 accepted). MMS-TTS guarantees content correctness but has speaker discontinuities (SCD 1.87-1.90).
  - **Read vs Synthetic:** READ set enables human validation but limited to 14 pairs where bilingual speakers available. Synthetic scales to 113 pairs but quality is auto-filtered only.

- **Failure signatures:**
  - **High reject rate on LLM output:** Indicates GPT-4o prompt doesn't generalize to language pair. Check if morphological example was provided (10/14 languages had it).
  - **XTTS producing unintelligible speech:** Check FAS before filtering—Table 7 shows 5% filtered. If >10% filtered, consider MMS-TTS fallback.
  - **Distinct-script pairs failing:** This is expected (Table 8), but if CER >60% on XTTS-TEST1/2, check romanization preprocessing for Mandarin/Japanese/Korean.
  - **Training augmentation not helping:** Verify CS-FLEURS training data uses same matrix-embedded direction as evaluation. Paper only provides X-English training (English embedded), so won't help {Arabic, Mandarin, Hindi, Spanish}-X test pairs.

- **First 3 experiments:**
  1. **Reproduce Whisper-Large-v3 baseline** on CS-FLEURS-READ test set (14 pairs). Target: CER ~19.83, ST BLEU ~18.46 per Figure 2. This validates your evaluation pipeline against paper benchmarks.
  2. **Ablate FAS filtering threshold** on XTTS-TEST1. Test 0% (no filter), 5% (paper default), 10% filtering. Measure impact on Whisper CER to understand quality-efficiency tradeoff.
  3. **Train with CS-FLEURS augmentation** on 2-3 held-out pairs from READ-TEST. Compare: (a) FLEURS-only baseline, (b) FLEURS + XTTS-TRAIN, (c) FLEURS + realigned swap data. This tests whether cheaper text generation maintains training gains.

## Open Questions the Paper Calls Out

- **Question:** How do models perform on code-switched speech translation (ST) when the target language is not English?
  - **Basis in paper:** [explicit] The authors state, "we leave benchmarking ST for non-English targets to future work."
  - **Why unresolved:** The paper evaluates translation performance only for X-English pairs, leaving the 60 non-English pairs (e.g., Arabic-X) unexplored for translation tasks.
  - **What evidence would resolve it:** Benchmarking BLEU scores on the XTTS-TEST2 set, which contains {Arabic, Mandarin, Hindi, Spanish}-X pairs.

## Limitations

- **Synthetic data reliance:** 99% of dataset pairs use synthetic speech, which may introduce TTS artifacts that don't generalize to natural code-switched speech
- **English-only ST evaluation:** Translation performance is only evaluated for English targets, leaving non-English ST completely unexplored
- **Distinct-script bottleneck:** ASR performance is 3x worse for distinct-script pairs, suggesting fundamental decoding challenges not addressed by current approaches

## Confidence

- **High Confidence:** The comparative performance between ASR and ST-to-English (ST performs better) is well-supported by Figure 2 and consistent with the decoder bottleneck hypothesis for mixed-script output.
- **Medium Confidence:** The claim that synthetic data improves performance on unseen pairs is supported by the 5.5 CER point improvement, but this is based on only 2 unseen pairs in READ-TEST.
- **Low Confidence:** The assertion that align-then-swap is "more consistent" than LLM generation, while backed by CMI variance metrics (4.0 vs 10.8), doesn't translate to clear downstream quality differences.

## Next Checks

1. **Natural Speech Validation:** Collect a small subset (5-10 pairs) of naturally occurring code-switched speech from real bilingual conversations and evaluate Whisper/ESPnet models to validate whether synthetic data performance transfers to natural speech patterns.

2. **Non-English ST Benchmarking:** Extend the ST evaluation to include 2-3 non-English target languages (e.g., Spanish-English, French-English) to test whether the ASR-ST performance gap persists when the target script differs from the matrix language.

3. **Fine-grained TTS Quality Analysis:** Conduct a controlled experiment comparing XTTS vs MMS-TTS output for identical text samples across 5 language pairs, measuring speaker continuity, prosodic naturalness, and intelligibility to quantify the actual quality tradeoff between the two synthesis methods.