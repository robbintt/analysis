---
ver: rpa2
title: Adding simple structure at inference improves Vision-Language Compositionality
arxiv_id: '2506.09691'
source_url: https://arxiv.org/abs/2506.09691
tags:
- text
- segments
- image
- should
- siglip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes adding simple structure at inference time to
  improve vision-language compositionality in dual encoder models. The method divides
  images into crops and captions into text segments, then matches image crops to text
  segments using a pretrained vision-language model, aggregating similarities to compute
  the final image-text score.
---

# Adding simple structure at inference improves Vision-Language Compositionality

## Quick Facts
- arXiv ID: 2506.09691
- Source URL: https://arxiv.org/abs/2506.09691
- Reference count: 40
- Key outcome: Inference-time structured matching improves vision-language compositionality in dual encoder models

## Executive Summary
This paper introduces a simple inference-time method to enhance compositionality in vision-language models. The approach divides images into crops and captions into text segments, then matches each crop to each segment using a pretrained vision-language model. By aggregating these fine-grained similarities, the method computes a final image-text score that better captures compositional relationships. Evaluated on bidirectional retrieval and SWAP cases, the technique consistently improves CLIP and SigLIP performance, particularly for attribute-object binding tasks. The work demonstrates that adding structure during inference can effectively address compositionality limitations without requiring additional training.

## Method Summary
The proposed method enhances vision-language compositionality by adding structure at inference time. Images are divided into fixed grid crops while captions are segmented into meaningful text portions using automatic segmentation. A pretrained vision-language model computes similarity scores between each image crop and text segment pair. These fine-grained similarities are then aggregated to produce the final image-text score. The approach maintains the efficiency of dual encoder architectures while introducing the compositional reasoning typically associated with more complex models.

## Key Results
- Consistent performance improvements across multiple datasets and models (CLIP and SigLIP)
- Image crops are essential for performance gains, while text segments introduce some errors
- Significant improvements in attribute-object binding tasks
- Method works effectively for bidirectional retrieval and SWAP case evaluations

## Why This Works (Mechanism)
The method works by breaking down the holistic image-text matching problem into smaller, more manageable compositional units. By comparing specific image regions (crops) with relevant text segments, the model can better capture attribute-object relationships and other compositional elements that holistic matching might miss. The aggregation of fine-grained similarities allows the model to reason about how different parts of the image relate to different parts of the caption, effectively simulating the compositional reasoning that more complex architectures provide.

## Foundational Learning

**Vision-Language Dual Encoders** - Why needed: Understanding the baseline architecture for image-text matching. Quick check: Models like CLIP and SigLIP that encode images and text separately before computing similarity.

**Compositional Reasoning** - Why needed: Grasping why standard models struggle with attribute-object binding and complex relationships. Quick check: Ability to correctly match "red apple" vs "green apple" when both objects appear in an image.

**Text Segmentation** - Why needed: Understanding how to automatically divide captions into meaningful compositional units. Quick check: Breaking "a small red ball on a green table" into ["a small", "red ball", "on a green table"].

**Image Cropping Strategies** - Why needed: Recognizing how spatial decomposition affects compositional understanding. Quick check: Grid-based vs attention-based cropping approaches and their impact on matching accuracy.

## Architecture Onboarding

**Component Map:** Image -> Crop Generator -> Vision Encoder -> Crop Embeddings -> Similarity Calculator -> Aggregator -> Final Score
Text -> Segmenter -> Text Encoder -> Segment Embeddings -> Similarity Calculator -> Aggregator -> Final Score

**Critical Path:** The most performance-critical components are the crop generation and similarity computation between crops and segments. The aggregation strategy must effectively combine multiple similarity scores while the segmentation quality directly impacts final performance.

**Design Tradeoffs:** Fixed grid cropping offers simplicity and computational efficiency but may miss compositional elements not aligned with the grid. Automatic text segmentation is convenient but introduces errors. The method trades some computational overhead for improved compositional reasoning.

**Failure Signatures:** Poor performance when compositional elements don't align with fixed grid crops, or when text segmentation fails to identify meaningful boundaries. Degradation occurs when the pretrained vision-language model lacks sufficient compositional understanding to begin with.

**3 First Experiments:**
1. Baseline holistic matching vs structured matching on a simple attribute-object dataset
2. Ablation study: crops only vs segments only vs both vs baseline
3. Sensitivity analysis: different crop numbers and segmentation granularities

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are bounded by the quality of the underlying pretrained model's representations
- Automatic text segmentation introduces errors that can propagate through the matching process
- Fixed grid-based cropping may miss important compositional elements not aligned with the grid structure
- Computational overhead is not fully characterized, raising concerns about scalability

## Confidence

**Inference-time compositional improvements** (High): Consistent performance improvements across multiple datasets, models, and experimental conditions with robust bidirectional retrieval and SWAP case evaluations.

**Crops vs text segments contribution** (Medium): Ablation studies clearly show image crops are essential, but text segment error analysis lacks systematic quantification and exploration of alternatives.

**Generalization to other vision-language models** (Medium): Promising results with CLIP and SigLIP, but limited testing on broader architectures and domains leaves generalization uncertain.

## Next Checks
1. **Computational overhead analysis:** Systematically benchmark the exact computational cost of the crop-based approach versus standard inference, including memory usage and inference time per image pair.

2. **Alternative cropping strategies:** Evaluate adaptive or attention-based cropping methods that focus on compositional elements rather than fixed grids to improve performance while reducing computational costs.

3. **Text segmentation quality improvement:** Implement and test alternative text segmentation approaches, such as using language models to identify compositional boundaries or incorporating syntactic/semantic information for more accurate segments.