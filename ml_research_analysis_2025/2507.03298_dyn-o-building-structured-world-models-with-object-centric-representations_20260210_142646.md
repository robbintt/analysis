---
ver: rpa2
title: 'Dyn-O: Building Structured World Models with Object-Centric Representations'
arxiv_id: '2507.03298'
source_url: https://arxiv.org/abs/2507.03298
tags:
- dyn-o
- world
- object-centric
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dyn-O introduces an object-centric world model that learns structured
  representations from pixel observations. The method uses a pretrained Cosmos encoder
  with slot attention to extract object-specific features, guided by segmentation
  masks during training via an annealing schedule.
---

# Dyn-O: Building Structured World Models with Object-Centric Representations

## Quick Facts
- **arXiv ID:** 2507.03298
- **Source URL:** https://arxiv.org/abs/2507.03298
- **Reference count:** 40
- **Primary result:** Achieves 20-step LPIPS of 0.33 vs 0.42, FVD of 361.3 vs 692.5, and PSNR of 16.34 vs 15.70 compared to DreamerV3 on Procgen environments

## Executive Summary
Dyn-O introduces an object-centric world model that learns structured representations from pixel observations using a frozen Cosmos encoder and slot attention mechanism. The method uses segmentation masks from SAM2 during training, which are gradually removed via an annealing schedule, enabling the model to learn object-specific features without explicit supervision. A Mamba-based state space model predicts dynamics in the slot space, with further disentanglement into static (time-invariant) and dynamic (time-varying) components for fine-grained control. Evaluated on seven Procgen environments, Dyn-O outperforms DreamerV3 in rollout prediction accuracy while enabling diverse trajectory generation through static feature manipulation.

## Method Summary
Dyn-O combines a pretrained Cosmos encoder with slot attention to extract object-specific features from pixel observations, guided by segmentation masks during training through a logarithmic annealing schedule. A Mamba-based state space model predicts dynamics in this slot space, modeling object interactions with self-attention. The method further disentangles features into static and dynamic components using contrastive learning and a Wasserstein discriminator with LeCam regularization. This structured representation enables precise control over object properties and generation of diverse trajectories while preserving object dynamics, achieving superior prediction accuracy compared to existing methods on Procgen benchmark environments.

## Key Results
- Achieves 20-step LPIPS of 0.33 vs 0.42, FVD of 361.3 vs 692.5, and PSNR of 16.34 vs 15.70 compared to DreamerV3
- Static-dynamic disentanglement enables diverse trajectory generation while preserving object dynamics
- Outperforms baseline models on seven Procgen environments including bigfish and coinrun
- Successfully learns object-centric representations without explicit supervision after initial mask-guided training

## Why This Works (Mechanism)
Dyn-O's success stems from combining frozen visual features with attention-based object discovery, using segmentation masks to bootstrap slot assignment before removing the crutch. The Mamba dynamics model captures temporal dependencies while the static-dynamic disentanglement enables fine-grained control over object properties. The annealing schedule ensures the model doesn't collapse when supervision is removed, while the adversarial training enforces meaningful separation between object identity and motion.

## Foundational Learning
- **Slot Attention:** Why needed: Enables learning object-centric representations from pixels without explicit supervision; Quick check: Visualize slot reconstructions to verify distinct objects appear in separate slots
- **Mamba State Space Models:** Why needed: Efficiently captures long-range temporal dependencies in object dynamics; Quick check: Monitor prediction loss for signs of gradient explosion or vanishing
- **Static-Dynamic Disentanglement:** Why needed: Separates object identity from motion to enable controlled generation; Quick check: Perform "Swap Static" test to verify trajectories remain unchanged
- **Annealing Schedule:** Why needed: Gradually removes dependency on segmentation masks without causing slot collapse; Quick check: Track slot binding metrics (FG-ARI) throughout training
- **Contrastive Learning:** Why needed: Enforces meaningful separation between static and dynamic feature spaces; Quick check: Monitor discriminator loss for stability
- **Wasserstein GAN with LeCam Regularization:** Why needed: Provides stable adversarial training for feature disentanglement; Quick check: Verify discriminator doesn't collapse to trivial solutions

## Architecture Onboarding
- **Component Map:** Pixel Observations -> Frozen Cosmos Encoder -> Slot Attention -> Static/Dynamic Disentanglement -> Mamba SSM Dynamics -> Future Prediction
- **Critical Path:** Encoder output → Slot Attention → Dynamics Model → Prediction Head; Segmentation masks only affect Slot Attention training
- **Design Tradeoffs:** Using frozen Cosmos features trades computational efficiency for potentially less task-specific adaptation; Mamba vs Transformer balances efficiency with expressivity
- **Failure Signatures:** Slot collapse (loss of object identity), discriminator collapse (poor disentanglement), gradient instability (training divergence)
- **First Experiment:** 1) Implement and visualize SAM2 mask-to-slot assignment; 2) Replicate "Swap Static" experiment to verify disentanglement; 3) Train with/without LeCam regularization to measure impact on stability

## Open Questions the Paper Calls Out
- Does improved prediction accuracy translate to better sample efficiency or policy performance in downstream model-based RL tasks?
- Can diffusion-based decoders enhance visual fidelity of generated rollouts compared to current Cosmos decoder?
- Can the method generalize to real-world video complexity beyond 2D game environments?
- Is the computational overhead of segmentation-guided training compatible with online reinforcement learning interaction rates?

## Limitations
- Critical implementation details missing for SAM2-to-slot mapping and disentanglement network architecture
- External gradient modification method referenced without implementation specifics
- Limited evaluation to 2D game environments, raising questions about real-world applicability
- Heavy reliance on pretrained components (Cosmos encoder, SAM2) which may not transfer to all domains

## Confidence
- **High confidence:** Core architectural framework (Cosmos + Slot Attention + Mamba dynamics)
- **Medium confidence:** Overall training procedure and loss formulations
- **Low confidence:** Precise implementation details for mask-to-slot mapping and disentanglement networks

## Next Checks
1. Implement and visualize the SAM2 mask-to-slot assignment mechanism; verify distinct objects consistently map to separate slots before and after mask removal
2. Replicate the "Swap Static" experiment from Figure 6; confirm that trajectories remain unchanged when static features are swapped between slots
3. Train with and without the LeCam regularization term; measure impact on discriminator stability and final disentanglement quality