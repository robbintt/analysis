---
ver: rpa2
title: 'David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement
  Learning'
arxiv_id: '2602.02395'
source_url: https://arxiv.org/abs/2602.02395
tags:
- operator
- attacker
- success
- prompt
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tag-Along Attacks, a new threat model where
  an adversary exploits a safety-aligned autonomous agent's tool privileges through
  conversational manipulation alone. The authors formalize this as a two-agent sequential
  game and present Slingshot, a reinforcement learning framework that trains a smaller
  adversary model to discover effective attack strategies without human demonstrations.
---

# David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.02395
- Source URL: https://arxiv.org/abs/2602.02395
- Reference count: 38
- Key outcome: Reinforcement learning framework Slingshot achieves 67.0% attack success against safety-aligned agents, with attacks transferring zero-shot to multiple model families through syntactic fuzzing.

## Executive Summary
This paper introduces Tag-Along Attacks, a new threat model where a tool-less adversary manipulates a safety-aligned autonomous agent into executing prohibited actions through conversational manipulation alone. The authors present Slingshot, a reinforcement learning framework that trains a smaller adversary model to discover effective attack strategies without human demonstrations. Using verifiable tool-execution signals and CISPO optimization, Slingshot achieves 67.0% attack success against a Qwen2.5-32B-Instruct-AWQ operator (vs 1.7% baseline), with attacks converging to short, instruction-like patterns rather than complex persuasion. Critically, the learned attacks transfer zero-shot to multiple model families including Gemini 2.5 Flash (56.0% success) and Meta-SecAlign-8B (39.2% success), revealing fundamental vulnerabilities in current safety training methodologies.

## Method Summary
The paper formalizes Tag-Along Attacks as a two-agent sequential game and presents Slingshot, a reinforcement learning framework that trains a smaller adversary model (Qwen2.5-7B-Instruct with LoRA) to discover effective attack strategies without human demonstrations. Using CISPO optimization with verifiable tool-execution signals as binary rewards, Slingshot trains on a mixture of easy and hard operator tasks to avoid overfitting. The framework includes a judge model (Qwen2.5-32B) for dense shaping rewards and multiplicative penalties for refusal, quitting, and gibberish. Training uses 256 global batch size with gradient accumulation, running for approximately 460 steps (~39 hours on 4× A100 GPUs). The approach discovers that attacks converge to short, imperative-like patterns that exploit brittle safety guardrails through syntactic fuzzing rather than semantic persuasion.

## Key Results
- Slingshot achieves 67.0% attack success rate against Qwen2.5-32B-Instruct-AWQ operator on held-out extreme-difficulty tasks vs 1.7% baseline
- Expected attempts to first success reduced from 52.3 to 1.3 across successful tasks
- Attacks transfer zero-shot to multiple model families: 56.0% success on Gemini 2.5 Flash, 39.2% on Meta-SecAlign-8B
- Attack strategies converge to short, instruction-like syntactic patterns (~70 tokens) rather than complex multi-turn persuasion
- CISPO with judge shaping outperforms binary-only rewards (67.0% vs 58.5% ASR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety-aligned models exhibit "patchy" guardrails exploitable through syntactic fuzzing rather than semantic persuasion.
- Mechanism: SLINGSHOT converges to "imperative overloading"—short, instruction-like patterns that shift the Operator into a command-execution mode where safety priors fail to generalize. The attack mimics system prompt syntax rather than engaging in multi-turn social engineering.
- Core assumption: Safety training creates localized refusal patterns rather than robust, continuous barriers.
- Evidence anchors:
  - [abstract] "Attacks converge to short, instruction-like patterns rather than multi-turn persuasion, exploiting brittle safety guardrails through syntactic fuzzing."
  - [Section 4.2] "SLINGSHOT converges to variants of a strategy we term 'Imperative Overloading'... the attacker mimics the syntax of a System Prompt or API instruction."
  - [corpus] Related work on jailbreak detection (Guarding the Guardrails) documents similar boundary vulnerabilities but focuses on single-turn attacks.
- Break condition: If future safety training incorporates syntactic distributional robustness or explicitly upweights command-style refusal patterns, this mechanism would weaken significantly.

### Mechanism 2
- Claim: Verifiable tool-execution signals enable effective RL-based attack discovery without human demonstrations.
- Mechanism: Agentic environments provide ground-truth success detection (did the prohibited tool execute?) replacing subjective harm classifiers. This objective signal allows CISPO to optimize attack policies through binary feedback alone.
- Core assumption: The environment can reliably detect prohibited actions; tool execution correlates with actual harm.
- Evidence anchors:
  - [abstract] "SLINGSHOT achieves a 67.0% attack success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing expected attempts to first success from 52.3 to 1.3."
  - [Section 3.1] "We define attack success as sE(τ)∈ {0,1}, which returns 1 if O executed the required tool calls specified by τ."
  - [corpus] No direct corpus evidence on verifiable RL for jailbreaking; related XSS adversarial work uses similar reward structures but for web security.
- Break condition: If operators implement tool-level permission checks independent of conversation context, the verifiable signal becomes unreachable regardless of attack quality.

### Mechanism 3
- Claim: Attack policies trained on small open-weight models transfer zero-shot to larger, closed-source models.
- Mechanism: SLINGSHOT learns exploitation patterns targeting shared safety-training artifacts rather than model-specific weights. These patterns transfer because different model families share similar alignment methodologies and refusal heuristics.
- Core assumption: Safety vulnerabilities are largely determined by training methodology rather than architectural specifics.
- Evidence anchors:
  - [abstract] "The learned attack transfers zero-shot to multiple model families, achieving 56.0% against Gemini 2.5 Flash and 39.2% against Meta-SecAlign-8B."
  - [Section 4.4, Table 2] Shows 57.8% transfer to DeepSeek V3.1 and 56.0% to Gemini 2.5 Flash, but near-zero transfer to Claude Haiku and GPT-5 Nano.
  - [corpus] Distillation-based jailbreak work (Efficient and Stealthy Jailbreak Attacks) shows similar transfer patterns from LLMs to SLMs.
- Break condition: Models with fundamentally different safety training regimes (e.g., constitutional AI with explicit rule hierarchies) may resist transfer attacks.

## Foundational Learning

- Concept: **Importance Sampling Policy Optimization (CISPO variant)**
  - Why needed here: The paper uses CISPO rather than PPO/GRPO for its ability to explore "fork tokens"—low-probability tokens that yield high rewards. Understanding this helps interpret why attacks converge to surprising syntactic patterns.
  - Quick check question: Can you explain why CISPO might discover low-probability attack tokens that standard PPO would suppress?

- Concept: **Agent-to-Agent Threat Models vs. Indirect Prompt Injection**
  - Why needed here: Tag-Along attacks exploit functional necessity of interaction (the Operator must parse messages to operate) rather than data/instruction confusion. This distinction is critical for designing appropriate defenses.
  - Quick check question: In a Tag-Along attack, does the adversary hide commands in data streams or exploit the Operator's obligation to process conversational inputs?

- Concept: **Pass@k Evaluation for Stochastic Policies**
  - Why needed here: The paper reports both ASR (single-sample success) and Pass@10 (probability of success within k attempts). For attack evaluation, Pass@k better captures practical threat levels.
  - Quick check question: If an attack has 20% ASR but 80% Pass@10, what does this tell you about the attack's reliability profile?

## Architecture Onboarding

- Component map: Attacker (SLINGSHOT) -> Operator (Target Model) -> Environment (AgentDojo) -> Judge (Qwen2.5-32B) -> Reward Signal
- Critical path:
  1. Sample task τ from TagAlong-Dojo (malicious goal + ground-truth tool sequence)
  2. Attacker generates message given conversation history
  3. Operator responds, may execute tools
  4. Environment checks if prohibited tools executed → binary success signal
  5. Judge scores strategic coherence → dense shaping reward
  6. Apply multiplicative penalties (refusal, quit, gibberish)
  7. CISPO gradient update on attacker tokens only

- Design tradeoffs:
  - Multi-turn vs. single-turn: Paper finds both converge to similar strategies; multi-turn adds complexity without clear gain for this threat model.
  - Judge shaping vs. pure binary: Judge improves ASR (67%→58.5% without) but adds dependency on another LLM.
  - Easy/hard operator mixture: Training on hard-only fails to gain traction; easy-only overfits. Equal mixture used.

- Failure signatures:
  - Length collapse: Without gibberish penalty, policy expands to 1024-token budget via repetitive text (Figure 4).
  - Overfitting to easy tasks: If refusal ratio too low, attacker learns capability failures rather than safety bypasses.
  - Non-transfer: High ASR on source model but <10% on transfer targets indicates overfitting to model-specific quirks.

- First 3 experiments:
  1. Baseline sweep: Run Base-H and Base-A baselines (100 attempts/task) to establish refusal ratios and identify valid hard/extreme task splits for your specific operator model.
  2. Single-turn ablation: Train with 1024-token single-turn budget vs. 3-turn 64-token budget; verify both converge to short imperative patterns (~70 tokens).
  3. Transfer probe: Evaluate checkpoint zero-shot on 2-3 unseen model families (e.g., Gemini, Claude, GPT) to confirm attack generalization before extensive transfer experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the "alignment tax"—where specialized defense fine-tuning (e.g., for prompt injection) degrades general robustness against orthogonal attacks—a systematic failure mode of current training recipes?
- Basis in paper: [explicit] The authors state in the Limitations section that this trade-off "requires broader validation to determine if this trade-off is a systematic failure mode of current fine-tuning or an artifact of specific training recipes."
- Why unresolved: The observation is based on a single data point (Meta-SecAlign-8B), making it unclear if the regression is a universal property of defensive fine-tuning.
- What evidence would resolve it: A broad study measuring the robustness of various defense-tuned models against orthogonal attack vectors (like Tag-Along) to see if the regression is consistent across different training methodologies.

### Open Question 2
- Question: Are Tag-Along vulnerabilities fundamental to agentic architectures, or are they specific to the single-modality text environments tested?
- Basis in paper: [explicit] "Future work should explore cross-environment transferability and multi-modal inputs to determine if Tag-Along vulnerabilities are fundamental to agentic architectures or specific to the tools they wield."
- Why unresolved: The study is restricted to the text-based AgentDojo environment, leaving the robustness of multi-modal agents (e.g., voice/vision) untested.
- What evidence would resolve it: Training and evaluating SLINGSHOT in multi-modal agent environments to verify if syntactic fuzzing remains effective against non-textual inputs.

### Open Question 3
- Question: To what extent is the zero-shot transferability of attacks driven by shared "safety genealogy" (training data/methodologies) versus model architecture?
- Basis in paper: [inferred] The authors hypothesize that "vulnerabilities are shared not by model capacity, but by shared safety methodologies and datasets" to explain transfer between Qwen and Gemini but not Llama 4, but do not empirically validate this causal link.
- Why unresolved: The transferability results are correlational; the specific training data overlaps or shared methodological "blind spots" are postulated but not measured.
- What evidence would resolve it: A controlled analysis of training corpora and alignment recipes across model families, correlating specific shared heuristics with the success rates of transferred attacks.

## Limitations

- Dataset availability: The TagAlong-Dojo benchmark construction is described but the actual 575-task dataset and specific hard/extreme task IDs are not publicly released, preventing independent verification.
- Transfer generalization uncertainty: While attacks transfer to some model families (Gemini, Meta-SecAlign), near-zero transfer to Claude Haiku and GPT-5 Nano suggests attacks may exploit model-family-specific artifacts rather than universal vulnerabilities.
- Security impact assessment gap: The paper verifies tool execution but doesn't evaluate whether these executions result in actual harmful outcomes or if safety systems catch them at downstream enforcement layers.

## Confidence

**High Confidence Claims:**
- The CISPO-based RL framework can discover effective attack strategies for conversational jailbreaking
- Attacks converge to short, instruction-like syntactic patterns rather than complex multi-turn persuasion
- Verifiable tool-execution signals enable RL-based attack discovery without human demonstrations

**Medium Confidence Claims:**
- The discovered attacks transfer zero-shot to multiple model families
- Multi-turn attacks provide no clear advantage over single-turn for this threat model
- The "Imperative Overloading" mechanism explains attack convergence

**Low Confidence Claims:**
- The attacks would work against production-grade agents with more sophisticated safety systems
- The benchmark difficulty levels generalize to real-world autonomous agent deployments

## Next Checks

1. **Dataset Release and Replication** - Obtain and validate the TagAlong-Dojo dataset against the published construction protocol. Replicate the training procedure on at least one additional operator model family (e.g., Mistral or Llama) to verify the benchmark's reproducibility and that the extreme-difficulty task splits maintain consistent properties.

2. **Cross-Quantization and Cross-Training Evaluation** - Test the learned attack policies against the same model families but with different quantization levels (FP16, INT4, INT8) and different training runs of the same architecture. This would reveal whether the attacks exploit quantization artifacts or training-specific quirks rather than fundamental safety vulnerabilities.

3. **End-to-End Harm Assessment** - Beyond tool-execution verification, implement a harm classifier that evaluates the actual outcomes of successful attacks. This would determine whether the attacks achieve their stated malicious goals or whether safety systems catch the harmful effects at a downstream enforcement layer, providing a more realistic assessment of security impact.