---
ver: rpa2
title: Directed Graph-alignment Approach for Identification of Gaps in Short Answers
arxiv_id: '2504.04473'
source_url: https://arxiv.org/abs/2504.04473
tags:
- answer
- student
- answers
- alignment
- gaps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a directed graph alignment approach for identifying
  gaps in student short answers by comparing them against model answers. The method
  constructs directed graphs from subject-predicate-object triples of student and
  model answers, aligns them using a similarity flooding algorithm that considers
  edge directionality and predicate similarity, and extracts gaps as unaligned nodes
  or edge pairs.
---

# Directed Graph-alignment Approach for Identification of Gaps in Short Answers

## Quick Facts
- arXiv ID: 2504.04473
- Source URL: https://arxiv.org/abs/2504.04473
- Authors: Archana Sahu; Plaban Kumar Bhowmick
- Reference count: 7
- Primary result: Directed graph alignment using SPO triples and Similarity Flooding outperforms undirected baselines on gap identification tasks across multiple datasets.

## Executive Summary
This paper proposes a directed graph alignment approach for identifying gaps in student short answers by comparing them against model answers. The method constructs directed graphs from subject-predicate-object triples of student and model answers, aligns them using a similarity flooding algorithm that considers edge directionality and predicate similarity, and extracts gaps as unaligned nodes or edge pairs. Three filter variants (threshold, exact, best) are explored for alignment pruning. The approach is evaluated on a new gap-annotated dataset derived from UNT, SciEntsBank, and Beetle datasets. Performance metrics (Macro-PQ, Macro-RQ, Macro-F1Q) show promising results, with the exact and best filter models generally outperforming the baseline undirected graph approach. The method handles gaps at word, phrase, or sentence level and provides formative feedback to students.

## Method Summary
The approach extracts subject-predicate-object triples from student and model answers using ClausIE, then builds directed graphs with canonicalized predicates (using Word2Vec and K-means clustering). Graph alignment is performed using Similarity Flooding, which propagates node similarity scores based on edge connections and predicate labels. Three filtering strategies (threshold, exact, best) are applied to generate one-to-one alignments, from which gaps are identified as unaligned nodes. The system is evaluated on datasets from UNT, SciEntsBank, and Beetle, with performance measured using question-based Macro-Precision, Macro-Recall, and Macro-F1 scores.

## Key Results
- The directed graph alignment approach with predicate canonicalization shows improved performance over baseline undirected graph methods
- Exact and Best filter variants generally outperform the Threshold filter, particularly on SciEntsBank dataset
- System struggles with large differences in answer pair sizes and noisy triple extraction from ClausIE
- Manual evaluation on 130 sentences shows significant improvement when using manually-extracted triples versus automatically-extracted ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preserving edge directionality during graph alignment appears to improve gap detection accuracy for answers where relational semantics are critical.
- **Mechanism:** Unlike undirected baselines, this method constructs directed graphs from subject-predicate-object (SPO) triples. The alignment algorithm (Similarity Flooding) propagates similarity scores only along edges that match in both direction and predicate label (canonicalized). This forces distinct alignments (e.g., preventing "Oil" from aligning with "Water" simply because they are neighbors, if the relational direction contradicts it).
- **Core assumption:** The semantic validity of an answer relies on the specific direction of relationships (e.g., "A is less dense than B" ≠ "B is less dense than A").
- **Evidence anchors:** [abstract] "considers edge directionality and predicate similarity"; [section 4] "direction-induced alignment forces the subject-subject... node-pairs to align... [resulting in] appropriate gap... detection."
- **Break condition:** Fails if the answer relies primarily on unordered concept lists (e.g., definition by enumeration) where directionality adds noise rather than signal.

### Mechanism 2
- **Claim:** Enforcing one-to-one node alignment reduces false negatives in gap identification compared to many-to-many approaches.
- **Mechanism:** The system applies an "Exact" or "Best" filter (using the Hungarian algorithm) after similarity propagation. This forces a strict bijection between model and student nodes. If a model node cannot find a unique, high-confidence match, it is marked as a gap, whereas threshold-based filters might leave it weakly aligned.
- **Core assumption:** A concept in a model answer maps to at most one concept in a student answer.
- **Evidence anchors:** [section 7.5] "FA-E and FA-B... perform better... due to the stringent one-to-one alignment strategy... [which] facilitates discovery of true gaps."; [table 5] Shows FA-E (Exact Filter) outperforming FA-T (Threshold) on Macro-F1 for SciEntsBank.
- **Break condition:** Fails if the student uses multiple distinct phrases to describe a single model concept, causing the system to over-penalize valid redundancy as misalignment.

### Mechanism 3
- **Claim:** Canonicalizing predicates via clustering improves alignment robustness against lexical variation.
- **Mechanism:** Predicates (e.g., "moves", "transports") are converted to vectors (Word2Vec) and clustered using K-means. Edges are then labeled with cluster IDs rather than raw text. This allows the alignment algorithm to treat semantically similar structural links as identical.
- **Core assumption:** Semantically similar verbs imply the same underlying relational logic in the context of the answer.
- **Evidence anchors:** [section 6.1] "predicates... are clustered... [and] replaced with respective group IDs."; [figure 3] Shows distinct predicates mapped to `c_0` and `c_1` to facilitate matching.
- **Break condition:** Fails if the clustering granularity is too coarse (grouping antonyms) or too fine (splitting synonyms), breaking the structural matching signal.

## Foundational Learning

- **Concept: Similarity Flooding Algorithm**
  - **Why needed here:** This is the core engine for graph matching. It determines how "similar" two nodes are based not just on their text, but on the similarity of their neighbors.
  - **Quick check question:** If Node A matches Node X, and Node A points to Node B while Node X points to Node Y, how does the algorithm update the similarity score of pair (B, Y)?

- **Concept: Subject-Predicate-Object (SPO) Triple Extraction**
  - **Why needed here:** The system relies on transforming unstructured text into a directed graph. Understanding how ClausIE (the IE tool used) extracts these triples is vital for debugging graph construction.
  - **Quick check question:** How would the sentence "Water breaks earth materials into pieces" be represented as a set of triples?

- **Concept: Bipartite Matching (Hungarian Algorithm)**
  - **Why needed here:** Used in the "Best Filter" (FA-B) to select the optimal set of alignments that maximizes total similarity without double-mapping nodes.
  - **Quick check question:** Why would a greedy selection strategy ("Exact Filter") potentially yield a lower total similarity score than the Hungarian method ("Best Filter")?

## Architecture Onboarding

- **Component map:** Input -> ClausIE Extraction -> Word2Vec+K-Means Canonicalization -> Directed Graph Builder -> Similarity Flooding Aligner -> Filter (Threshold/Exact/Best) -> Gap Inference

- **Critical path:** The **Canonicalizer** and **Aligner**. If the predicate clustering fails (e.g., grouping "is" with "is not"), the PCG construction will create false edges, causing the Similarity Flooding to converge on incorrect alignments.

- **Design tradeoffs:**
  - **Strict vs. Loose Alignment:** The "Exact/Best" filters increase Precision but may lower Recall for verbose answers compared to the "Threshold" filter.
  - **Directed vs. Undirected:** Directed graphs improve performance on relation-heavy answers (SciEntsBank) but degraded performance on the Beetle dataset (which contained many auxiliary verbs and less action-verb variety), where the baseline System-I performed better.

- **Failure signatures:**
  - **Noisy Graphs:** Over-extraction of triples by ClausIE leads to large, disconnected graph components (Section 8.1).
  - **Size Mismatch:** If the student answer is significantly longer/shorter than the model, the Similarity Flooding algorithm fails to find sufficient context for stable propagation (Section 8.2).

- **First 3 experiments:**
  1. **Unit Test the IE Pipeline:** Pass the 130 manually annotated sentences from Section 8.1 through ClausIE. Compare the extracted triples against the manual ground truth to quantify the "IE noise" bottleneck.
  2. **Ablate the Filter:** Run the system on the UNT dataset switching only the filter (FA-T → FA-E → FA-B). Verify the paper's claim that strict alignment improves Macro-F1.
  3. **Directionality Stress Test:** Curate 10 pairs of answers where the student reverses the relationship (e.g., A < B vs B < A). Run System-I (undirected) vs. FA-B (directed) to confirm the directed gap detection catches the error.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of advanced Information Extraction (IE) tools or dependency parsers improve the quality of extracted triples compared to the current ClausIE implementation?
- **Basis:** [explicit] The paper states in Section 8.1 that "triples generated using the IE tool seem inaccurate and redundant," leading to improper graphs, and lists this as a limitation in the Conclusion.
- **Why unresolved:** The current reliance on ClausIE creates noisy graph structures that degrade alignment performance, evidenced by the authors finding that manually extracted triples improved Macro-F1Q scores significantly (e.g., from 0.3143 to 0.3860 on a subset).
- **Evidence:** A comparative study evaluating the system's performance (Macro-F1Q) when using alternative triple extraction methods (e.g., dependency-based parsers) versus the current approach on the same datasets.

### Open Question 2
- **Question:** How can the graph alignment strategy be adapted to effectively handle pairs of student and model answers that differ significantly in length and structural complexity?
- **Basis:** [explicit] Section 8.2 identifies the "Large Difference in Answer Pair Size" as a limitation where the Similarity Flooding Algorithm fails to find sufficient context for alignment.
- **Why unresolved:** The current algorithm struggles to align graphs effectively when the student answer is verbose compared to the model answer (or vice versa), leading to incorrect gap detection.
- **Evidence:** Proposing and testing a modified alignment algorithm (e.g., using graph summarization or hierarchical matching) that stabilizes performance on answer pairs with high length variance.

### Open Question 3
- **Question:** Would replacing the Word2Vec CBOW model with sub-word embeddings (e.g., FastText) or contextual embeddings improve gap identification, particularly for Out-Of-Vocabulary (OOV) terms?
- **Basis:** [explicit] Appendix B states: "In future, FastText vector building process could be used instead of CBOW model from Word2vec, so that OOV terms are handled in a better manner."
- **Why unresolved:** The current CBOW model ignores OOV terms during vector construction, potentially losing crucial semantic information needed for accurate node similarity calculations in specialized domains.
- **Evidence:** An ablation study measuring the impact of FastText or BERT-based embeddings on alignment accuracy and Macro-F1Q scores, specifically analyzing performance on technical terminology present in the UNT or Beetle datasets.

### Open Question 4
- **Question:** Can a hybrid approach be developed to switch between directed and undirected graph alignment based on the linguistic properties of the answer pairs, such as predicate richness?
- **Basis:** [inferred] Section 7.5 notes that the proposed directed model underperformed on the Beetle dataset because answers often contained only auxiliary verbs with little variety in action verbs, rendering edge directionality less informative.
- **Why unresolved:** The rigid application of directed alignment penalizes datasets where the semantic structure is flat or defined by state-verbs rather than action-verbs, causing the baseline undirected model to outperform the proposed method in these specific cases.
- **Evidence:** Developing a metric to classify answer pairs by "predicate richness" and conditionally applying the directed model or the baseline undirected model, then comparing the aggregate performance against the current singular approach.

## Limitations
- The system's reliance on ClausIE for triple extraction introduces significant noise, particularly for complex sentences, as acknowledged by the authors.
- The approach assumes a strict one-to-one mapping between model and student concepts, which may over-penalize valid semantic redundancy in student answers.
- Predicate clustering via Word2Vec+K-means is sensitive to vector training parameters and cluster granularity, neither of which are fully specified in the paper.

## Confidence
- **High:** The directed graph construction and alignment mechanism for preserving relational semantics.
- **Medium:** The superiority of exact/best filter strategies over threshold filtering, supported by quantitative results but lacking ablation studies on noisy data.
- **Low:** The generalizability of results across all three datasets, given the inconsistent performance patterns (System-I outperforming on Beetle, while directed graphs excel on SciEntsBank).

## Next Checks
1. **Triple Extraction Audit:** Run ClausIE on the 130 manually-annotated sentences from Section 8.1 and compute precision/recall against the ground truth to quantify the "IE noise" bottleneck.
2. **Filter Ablation Study:** Systematically evaluate all three filter variants (FA-T, FA-E, FA-B) on each dataset to verify the claimed performance hierarchy and identify failure modes.
3. **Directionality Stress Test:** Construct adversarial test cases where students reverse relationships (e.g., "A is less dense than B" vs "B is less dense than A") to confirm directed graphs catch these errors while undirected baselines fail.