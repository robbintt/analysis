---
ver: rpa2
title: Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications
  via Activation Perturbation
arxiv_id: '2508.06676'
source_url: https://arxiv.org/abs/2508.06676
tags:
- watermark
- watermarking
- activation
- pruning
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of protecting the intellectual
  property of Kolmogorov-Arnold Networks (KAN) through watermarking, as existing deep
  neural network watermarking methods fail to adapt to KAN's unique architecture with
  learnable activation functions. The authors propose Discrete Cosine Transform-based
  Activation Watermarking (DCT-AW), which embeds watermarks by perturbing activation
  outputs in the frequency domain using DCT and IDCT, ensuring task independence and
  compatibility across classification and regression tasks.
---

# Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation

## Quick Facts
- **arXiv ID:** 2508.06676
- **Source URL:** https://arxiv.org/abs/2508.06676
- **Reference count:** 21
- **Primary result:** Proposes DCT-AW watermarking for KANs achieving 96.23% MNIST accuracy with 100% robustness against fine-tuning and pruning attacks

## Executive Summary
This paper addresses the critical challenge of protecting intellectual property in Kolmogorov-Arnold Networks (KANs), a novel neural architecture featuring learnable activation functions that render existing deep neural network watermarking methods ineffective. The authors introduce Discrete Cosine Transform-based Activation Watermarking (DCT-AW), a technique that embeds watermarks by perturbing activation outputs in the frequency domain using DCT and IDCT transformations. The approach maintains task independence, making it compatible with both classification and regression tasks, while demonstrating minimal impact on model performance with 96.23% accuracy on MNIST and 93.73% on Fashion MNIST.

## Method Summary
The paper proposes a two-phase training algorithm that alternates between normal training and activation neuron update phases for every batch. The watermark is embedded by applying Discrete Cosine Transform to the activation outputs, perturbing the frequency domain representation, and then applying inverse DCT to obtain the watermarked activations. This approach leverages the unique characteristic of KANs where activation functions are parameterized and learnable, allowing watermark information to be embedded directly into these parameters. The method ensures that the watermark is robust against common removal attacks while maintaining the original task performance.

## Key Results
- Achieved 96.23% main accuracy on MNIST and 93.73% on Fashion MNIST with embedded watermark
- Demonstrated 100% watermark accuracy against fine-tuning and pruning attacks
- Maintained 93.69% watermark accuracy even after retraining following pruning attacks
- Showed task independence across classification and regression applications

## Why This Works (Mechanism)
The mechanism exploits KAN's unique architecture where activation functions are parameterized and learnable, unlike fixed activations in traditional neural networks. By embedding watermarks in the frequency domain of activation outputs through DCT and IDCT, the method creates a robust signature that persists through common removal attacks. The two-phase training algorithm ensures that watermark embedding does not compromise the primary task learning by separating the watermark optimization from the main task optimization.

## Foundational Learning

**KAN Architecture** - KANs replace traditional fixed activation functions with learnable univariate functions, typically parameterized as splines. Why needed: This fundamental difference from DNNs invalidates existing watermarking approaches that rely on weight-based embedding. Quick check: Verify that activation functions in KAN are indeed parameterized and learnable.

**Discrete Cosine Transform** - DCT converts activation outputs from spatial to frequency domain, enabling watermark embedding in frequency coefficients. Why needed: Frequency domain manipulation provides robustness against spatial-domain attacks like pruning and fine-tuning. Quick check: Confirm that DCT/IDCT operations are differentiable for backpropagation.

**Activation Perturbation** - The watermark is embedded by adding controlled perturbations to activation outputs in the frequency domain. Why needed: Direct weight-based watermarking fails in KANs due to the absence of traditional weight parameters. Quick check: Verify that perturbations remain imperceptible while maintaining watermark detectability.

## Architecture Onboarding

**Component Map:** Input -> KAN Layers with Learnable Activations -> DCT Transform -> Watermark Embedding -> IDCT Transform -> Output

**Critical Path:** The watermark embedding process operates on the activation outputs after each layer's computation but before the final output, ensuring the watermark is integrated into the model's decision-making process while maintaining task performance.

**Design Tradeoffs:** The method prioritizes robustness over minimal overhead, requiring double forward/backward passes per batch. This tradeoff favors security and reliability over computational efficiency, which is acceptable for IP protection scenarios.

**Failure Signatures:** Watermark detection failure indicates successful attack or poor embedding, while significant accuracy degradation suggests watermark perturbation is too aggressive. Both metrics must be monitored to ensure proper balance.

**First Experiments:**
1. Test watermark detection rate on clean model before any attacks
2. Evaluate accuracy degradation when increasing watermark strength
3. Measure watermark robustness against simple pruning (20% weights removed)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed DCT-AW watermark survive model extraction or knowledge distillation attacks where an attacker trains a surrogate model?
- **Basis in paper:** [inferred] The paper explicitly evaluates robustness against fine-tuning, pruning, and retraining in Sections III-B and VI, but omits discussion on model extraction attacks, which are a standard threat vector in the DNN watermarking literature.
- **Why unresolved:** Since the watermark is embedded in the specific activation parameters of the KAN architecture, it is unclear if the "signature" would transfer to a student model that mimics the input-output behavior but uses a different parameter set or architecture.
- **What evidence would resolve it:** Experimental results showing the watermark detection rate in a student model trained via knowledge distillation using the watermarked KAN as the teacher.

### Open Question 2
- **Question:** Does embedding the watermark in Layer 0 remain effective and efficient in significantly deeper KAN architectures?
- **Basis in paper:** [inferred] Table III provides an ablation study comparing Layer 0 and Layer 1, concluding Layer 0 is superior. However, the experimental scope appears limited to shallow architectures (implied by the focus on MNIST/Fashion-MNIST and the Layer 0/1 dichotomy).
- **Why unresolved:** The "weaker influence on the final output" observed in Layer 0 might not hold in deep networks where subsequent layers could potentially "wash out" the perturbation during forward propagation, or the accumulated signal loss might degrade main task accuracy more than observed in shallow nets.
- **What evidence would resolve it:** Performance and robustness metrics (main acc. and wm. acc.) for DCT-AW applied to deep KANs (e.g., 6+ layers) on complex datasets.

### Open Question 3
- **Question:** What is the computational overhead and convergence impact of the two-phase training algorithm compared to standard KAN training?
- **Basis in paper:** [inferred] Algorithm 1 introduces a two-step loop for every batch: a "Normal Training Phase" and a separate "Activation Neuron Update Phase," effectively requiring double the forward and backward passes for the activation layer per batch.
- **Why unresolved:** The paper reports final accuracy (functionality preservation) but does not report training time, memory usage, or convergence rates (epochs to converge), leaving the efficiency of the proposed embedding method unquantified.
- **What evidence would resolve it:** A comparative analysis of wall-clock training time and loss convergence curves between DCT-AW and a baseline clean KAN model.

## Limitations
- Experimental validation primarily limited to image classification tasks (MNIST and Fashion MNIST) with limited exploration of other data modalities
- Computational overhead during inference with embedded watermarks not quantified or addressed
- Potential security vulnerabilities specific to KAN architectures not discussed in detail

## Confidence
*High Confidence:* The core technical approach of using DCT-based activation perturbation for KAN watermarking is well-founded and the experimental results on benchmark datasets are reproducible. The claim about maintaining task independence across classification and regression tasks is supported by the methodology.

*Medium Confidence:* The robustness claims against specific attacks (fine-tuning, pruning, retraining after pruning) are based on controlled experiments but may not generalize to all attack scenarios. The assertion that existing DNN watermarking methods fail for KAN requires further validation across a broader range of existing techniques.

*Low Confidence:* The paper lacks discussion of potential security vulnerabilities specific to KAN architectures, such as targeted attacks on the learnable activation functions. The claim about "superior robustness" is relative to unspecified baselines and needs clearer benchmarking.

## Next Checks
1. Evaluate DCT-AW performance on non-image datasets and regression tasks with varying complexity to validate task independence claims across diverse applications.

2. Conduct comprehensive security analysis including adaptive attacks specifically designed to target KAN's learnable activation functions and DCT-based watermarking scheme.

3. Measure and analyze the computational overhead during inference with embedded watermarks to assess practical deployment feasibility in resource-constrained environments.