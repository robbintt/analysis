---
ver: rpa2
title: Dataset Creation and Baseline Models for Sexism Detection in Hausa
arxiv_id: '2510.27038'
source_url: https://arxiv.org/abs/2510.27038
tags:
- sexism
- hausa
- detection
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first Hausa dataset for sexism detection
  and evaluates both traditional and deep learning models on it. The dataset was created
  through translation of an English sexism corpus and a user study with 66 native
  speakers to capture culturally specific expressions and definitions.
---

# Dataset Creation and Baseline Models for Sexism Detection in Hausa

## Quick Facts
- arXiv ID: 2510.27038
- Source URL: https://arxiv.org/abs/2510.27038
- Reference count: 0
- First Hausa dataset for sexism detection with few-shot GPT-5 achieving 86% F1

## Executive Summary
This paper presents the first Hausa dataset for sexism detection, created through translation of an English corpus and a user study with 66 native speakers. The study captures culturally specific expressions of sexism and evaluates both traditional models (SVM, BERT variants, XML-R) and few-shot LLM approaches (GPT-5, Deepseek, Grok). Results show XML-R achieving 84% F1-score and few-shot GPT-5 with 5 examples reaching 86% F1, outperforming fine-tuned baselines. The work highlights the importance of integrating community input for low-resource language NLP tasks and demonstrates the potential of few-shot learning for Hausa.

## Method Summary
The dataset was created through a two-stage process: (1) translation of an English sexism corpus by bilingual experts, followed by re-annotation to align with local linguistic norms, and (2) a user study with 66 native Hausa speakers to explore definitions and examples of sexism in everyday discourse. The dataset was then used to evaluate traditional machine learning models (SVM, BERT variants, XML-R) through fine-tuning, as well as few-shot prompting approaches with LLMs (GPT-5, Deepseek, Grok). Models were assessed using standard classification metrics including F1-score, precision, and recall.

## Key Results
- XML-R achieved 84% F1-score when fine-tuned on the Hausa sexism dataset
- Few-shot GPT-5 with 5 examples reached 86% F1, outperforming fine-tuned baselines
- Models struggled with culturally nuanced expressions, particularly clarification-seeking sentences ending in "ne" and sarcastic content
- Performance degraded when increasing few-shot examples from 5 to 10, dropping to 73% F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Culturally grounded annotations from native speakers improve model alignment with local norms of sexism
- Mechanism: A two-stage user study (n=66) elicits definitions and examples of sexism directly from native Hausa speakers. This input informs the thematic coding of the dataset, ensuring labels reflect how sexism is perceived and articulated locally
- Core assumption: Native speakers can consistently articulate and label nuanced, culturally specific sexist expressions
- Evidence anchors: [abstract] "user study (n=66) involving native speakers to explore how sexism is defined and articulated in everyday discourse"
- Break condition: If participant demographics are skewed (as noted: 87% male) or if social desirability bias affects responses, the captured definitions may not represent the full spectrum of sexism

### Mechanism 2
- Claim: Cross-lingual data augmentation via human translation extends high-resource datasets but requires re-annotation for cultural fit
- Mechanism: An existing English sexism corpus is translated by bilingual experts and then re-annotated to align with Hausa linguistic norms
- Core assumption: Sexism has some universal conceptual anchors that survive translation, but its expression is culturally modulated
- Evidence anchors: [abstract] "dataset was created through translation of an English sexism corpus... to capture culturally specific expressions"
- Break condition: If translation fails to capture implicit or idiomatic sexism, or if the source English dataset lacks cultural relevance, the re-annotation step may be insufficient

### Mechanism 3
- Claim: Few-shot prompting of large language models can outperform fine-tuned models in low-resource settings by leveraging latent multilingual knowledge
- Mechanism: A pre-trained LLM is provided with a few (e.g., 5) high-quality, culturally relevant examples in the prompt
- Core assumption: The LLM's pre-training corpus contained sufficient Hausa text and cross-lingual understanding of social concepts like sexism
- Evidence anchors: [abstract] "few-shot GPT-5 with 5 examples reaching 86% F1, outperforming fine-tuned baselines"
- Break condition: If the LLM's pre-training data has poor coverage of Hausa or the specific cultural context of sexism, few-shot performance will be unstable or poor

## Foundational Learning

**Concept: Few-Shot Learning**
- Why needed here: This is the technique that achieved the best results (86% F1) in the paper. Understanding how to select and format examples for the prompt is critical
- Quick check question: Why would giving a model 10 examples lead to worse performance than 5 examples, as observed in this study?

**Concept: Cross-Lingual Transfer**
- Why needed here: The dataset creation relies on transferring knowledge from an English corpus. The success of the project depends on managing the limitations of this transfer
- Quick check question: Why is direct machine translation of a sexism dataset considered problematic, and what human-in-the-loop step does this paper use to mitigate it?

**Concept: Thematic Analysis**
- Why needed here: To convert unstructured text from the user study into a structured understanding of how sexism is conceptualized, which informs the entire dataset and model evaluation
- Quick check question: What are the four main themes of sexism identified from the Hausa speakers' definitions, and how might they differ from themes found in an English-speaking context?

## Architecture Onboarding

**Component map:**
- Data Layer: English Sexism Corpus -> Human Translation & Re-annotation + User Study -> Thematic Coding -> Hausa Sexism Dataset (binary: sexist/not sexist)
- Model Layer: Fine-Tuning Path (SVM, BERT, mBERT, XML-R) + Few-Shot Path (GPT-5, Deepseek, Grok)
- Evaluation Layer: Metrics (F1, Precision, Recall) on held-out test set + Qualitative error analysis on nuanced cases

**Critical path:** The quality and representativeness of the dataset is the primary determinant of success. For few-shot learning, the selection of the k examples in the prompt is the most critical, low-latency control point

**Design tradeoffs:**
- Fine-Tuning vs. Few-Shot: Fine-tuning (XML-R, 84% F1) offers more control but requires data curation and training. Few-shot (GPT-5, 86% F1) is faster to implement and slightly better but relies on a proprietary model and is sensitive to prompt design
- Data Source: Collecting native examples is authentic but slow. Translating augments volume but risks cultural misalignment. The paper hybridizes both

**Failure signatures:**
- Clarification-Seeking Confusion: Sentences ending in "ne" are often misclassified as sexist
- Sarcasm & Idiom Blindness: Models miss sexist meaning wrapped in sarcasm or idiomatic expressions
- Few-Shot Overloading: Performance drops when too many examples (e.g., 10) are provided

**First 3 experiments:**
1. Establish Baselines: Re-train SVM and XML-R models on the provided dataset to confirm baseline metrics (SVM 0.65, XML-R 0.84 F1)
2. Optimize Few-Shot Prompting: Systematically vary the number (k=1, 3, 5, 7, 10) and selection strategy of examples in the GPT-5 prompt to find the optimal configuration
3. Targeted Error Analysis: Construct a small, hand-curated challenge set from the error cases in Table 4. Evaluate all models on this set to quantify and compare their failure modes on cultural nuance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can context-aware modeling techniques reduce false positives on clarification-seeking and sarcastic Hausa expressions?
- Basis in paper: [explicit] Authors state models struggle with "clarification-seeking or sarcastic expressions (as in sentences ending with 'ne')" and show false positives in Table 4
- Why unresolved: Current models treat utterances in isolation; particles like "ne" alter illocutionary force depending on discourse context, which static classifiers cannot capture
- What evidence would resolve it: A study comparing context-aware (e.g., conversation-level, discourse-aware) models vs. sentence-level baselines on a held-out set of clarification-seeking/sarcastic items

### Open Question 2
- Question: Why does few-shot GPT-5 performance degrade when increasing from 5 to 10 examples?
- Basis in paper: [inferred] Table 1 shows F1 drops from 0.86 (5-shot) to 0.73 (10-shot), contradicting typical few-shot scaling expectations
- Why unresolved: Potential causes (example selection, redundancy, prompt length, model distraction) are unexplored; current results are anecdotal without ablation
- What evidence would resolve it: Controlled ablations varying example quantity, diversity, ordering, and length; diagnostic metrics on attention/uncertainty

### Open Question 3
- Question: How does annotator gender composition affect sexism annotation outcomes in Hausa?
- Basis in paper: [explicit] The user study had only 12.9% female participants; authors explicitly note this skew
- Why unresolved: Gendered lived experience likely shapes sexism perception; male-dominant annotations may miss or underweight certain categories
- What evidence would resolve it: Re-annotation of the dataset with gender-balanced annotators, analysis of inter-group disagreement patterns, and model retraining

### Open Question 4
- Question: Does the translation-plus-re-annotation pipeline transfer effectively to other low-resource African languages?
- Basis in paper: [explicit] The conclusion states future work will explore "the broader application of this approach to other LRLs"
- Why unresolved: Cross-lingual transfer success may depend on typological similarity to English, presence in pretraining data, and cultural comparability of gender norms
- What evidence would resolve it: Replicating the methodology for 2–3 diverse LRLs (e.g., Yorùbá, Amharic, Swahili), comparing baseline vs. few-shot performance

## Limitations
- Data Representativeness: The user study had 87% male participants, potentially skewing the conceptualization of sexism toward male perspectives
- Model Generalization: Performance on out-of-domain sexist content or different Hausa dialects remains unknown
- Translation Quality: Reliance on human translation introduces potential biases and cultural misalignments that may not be fully captured in re-annotation

## Confidence
- High Confidence: The dataset creation methodology and the general trend that few-shot learning outperformed fine-tuning
- Medium Confidence: The specific F1 scores (84% for XML-R, 86% for few-shot GPT-5) are reliable for the reported test set
- Low Confidence: The assertion that culturally grounded annotations significantly improve model performance beyond standard approaches lacks direct comparative evidence

## Next Checks
1. **Cross-Dialect Evaluation**: Test the trained models on sexist content from different Hausa-speaking regions to assess dialectal robustness
2. **Human Evaluation Study**: Conduct an independent annotation study with a more balanced demographic sample to validate the dataset labels
3. **Longitudinal Performance Monitoring**: Implement the few-shot GPT-5 approach on a monthly basis with different example sets to measure consistency and identify any prompt sensitivity or model drift over time