---
ver: rpa2
title: Safety Alignment via Constrained Knowledge Unlearning
arxiv_id: '2505.18588'
source_url: https://arxiv.org/abs/2505.18588
tags:
- knowledge
- unlearning
- safety
- harmful
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CKU, a novel safety alignment approach for
  LLMs that enhances resistance to jailbreak attacks by selectively unlearning harmful
  knowledge while preserving useful information. The core method involves knowledge
  localization and retention through neuron scoring in MLP layers, followed by gradient-based
  unlearning of harmful content with regularization.
---

# Safety Alignment via Constrained Knowledge Unlearning

## Quick Facts
- arXiv ID: 2505.18588
- Source URL: https://arxiv.org/abs/2505.18588
- Authors: Zesheng Shi; Yucheng Zhou; Jing li
- Reference count: 36
- This paper introduces CKU, a novel safety alignment approach for LLMs that enhances resistance to jailbreak attacks by selectively unlearning harmful knowledge while preserving useful information.

## Executive Summary
This paper introduces CKU, a novel safety alignment approach for LLMs that enhances resistance to jailbreak attacks by selectively unlearning harmful knowledge while preserving useful information. The core method involves knowledge localization and retention through neuron scoring in MLP layers, followed by gradient-based unlearning of harmful content with regularization. Experimental results show that CKU significantly improves safety with minimal utility loss, achieving up to 4x improvement in jailbreak resistance on Llama2-7B-Chat and Llama3-8B-Instruct compared to baselines. The method maintains model performance with only 0.15% accuracy reduction while substantially enhancing safety metrics.

## Method Summary
CKU works by first identifying neurons associated with useful knowledge using SNIP importance scoring on the Alpaca dataset, then locking the top 80% of neurons during unlearning to preserve this knowledge. The unlearning process uses gradient ascent on harmful examples from AdvBench, but is constrained to specific MLP layers (8-12 in Llama2-7B) and regularized to prevent over-forgetting. The approach combines neuron-level knowledge localization with layer-specific unlearning, using a locked neuron rate of 0.8 and regularization parameter λ=1.5 to achieve optimal safety-utility trade-offs.

## Key Results
- CKU achieves up to 4x improvement in jailbreak resistance compared to baselines on Llama2-7B-Chat and Llama3-8B-Instruct
- Maintains model performance with only 0.15% accuracy reduction on utility benchmarks
- Neuron sensitivity analysis shows optimal results with 80% neuron locking and unlearning focused on MLP layers 8-12
- Demonstrates superior generalization across various attack methods including AIM, AutoDAN, GCG, and generation exploitation attacks

## Why This Works (Mechanism)

### Mechanism 1: Neuron-Level Knowledge Localization via SNIP Scoring
- Claim: Selectively identifying and preserving neurons that encode useful knowledge allows targeted unlearning of harmful content while maintaining model capabilities.
- Mechanism: The paper uses SNIP (Single-shot Network Pruning) to compute importance scores I(W,x) = |W ⊙ ∇_W L(x)| for each neuron across MLP layers. Neurons are ranked by average importance scores over a calibration dataset (Alpaca), with the top p% designated as Knowledge-Related Neurons (KRNs). During unlearning, gradients of KRNs are pruned (set to zero), preventing their modification.
- Core assumption: Assumption: Useful knowledge is concentrated in a subset of high-importance neurons that can be reliably identified via gradient-based scoring.
- Evidence anchors:
  - [abstract]: "CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge."
  - [section 4.1]: "To estimate importance of each neuron wij in the weight matrix W of a linear layer, we use a first-order approximation: I(W, x) = |W ⊙ ▽_W L(x)|"
  - [corpus]: Related work "Unraveling LLM Jailbreaks Through Safety Knowledge Neurons" similarly investigates neuron-level safety mechanisms, suggesting this is a plausible direction.
- Break condition: If important knowledge is distributed across many neurons rather than concentrated, or if harmful and useful knowledge heavily overlap in the same neurons, the scoring and separation approach may fail.

### Mechanism 2: Layer-Specific Unlearning Targeting Mid-Depth MLP Layers
- Claim: Restricting unlearning to specific MLP layer ranges (layers 8-12 in Llama2-7B) maximizes safety gains while minimizing utility loss.
- Mechanism: The paper empirically identifies that unlearning applied only to MLP layers (vs. all layers or non-MLP layers) achieves the best safety-utility trade-off. Further experimentation shows that targeting layers 8-12 yields optimal results: approximately 4x safety improvement with only 0.15% accuracy reduction.
- Core assumption: Assumption: Safety-critical knowledge has different layer-wise distribution than general utility knowledge, and mid-depth layers are particularly important for safety-relevant representations.
- Evidence anchors:
  - [section 5.3]: "unlearning training approach, when applied with fixed neurons in MLP layers 8 to 12, yields the highest utility score... average accuracy decreases by only approximately 0.15% relative to the base model, while safety metrics show an improvement of more than fourfold."
  - [section 5.1, Figure 3]: "Performing unlearning training only on the MLP layers results in utility closest to the base model and best safety performance."
  - [corpus]: Weak direct evidence in corpus for this specific layer range; this appears to be an empirical finding specific to the reported experiments.
- Break condition: If the optimal layer range varies significantly across model architectures, sizes, or training procedures, this approach would require re-calibration for each new model.

### Mechanism 3: Constrained Gradient Ascent with Neuron Locking
- Claim: Combining gradient ascent on harmful knowledge with neuron locking (NLR=0.8) and regularization (λ=1.5) provides controlled unlearning that removes harmful responses without catastrophic forgetting.
- Mechanism: The unlearning process uses gradient ascent (θ = θ + η∇_θ L_unlearn) to maximize loss on harmful examples, effectively "forgetting" harmful patterns. The loss is constrained as L = max(0, λ + L_f), where λ prevents excessive unlearning. Concurrently, 80% of high-importance neurons are locked (gradients pruned), preserving their knowledge.
- Core assumption: Assumption: Gradient ascent can selectively degrade harmful knowledge representations while the locking mechanism protects essential knowledge, and the regularization parameter λ prevents over-unlearning.
- Evidence anchors:
  - [section 4.3-4.4]: "we employ GA method... The objective for unlearning training is defined as follows: L_f = 1/|D_f| Σ log(p(y_i|T(x), y_{<i}))... we aim to set a constraint λ for unlearning objective... L = max(0, λ + L_f)"
  - [section 5.2, Figure 4]: "when the NLR is set to 0.8, the model's safety performance shows an improvement of more than threefold"
  - [corpus]: Related work "Open Problems in Machine Unlearning for AI Safety" discusses challenges in unlearning, suggesting this is an active area with open questions about reliability.
- Break condition: If harmful and useful knowledge share significant neural substrates, locking neurons may preserve harmful knowledge while gradient ascent may degrade useful knowledge.

## Foundational Learning

- Concept: **MLP Layers as Knowledge Storage in Transformers**
  - Why needed here: The entire CKU approach depends on understanding that MLP layers are primary knowledge storage sites in LLMs, which motivates focusing unlearning on these layers.
  - Quick check question: Can you explain why the paper focuses on MLP layers rather than attention layers for knowledge localization?

- Concept: **Gradient Ascent for Machine Unlearning**
  - Why needed here: The core unlearning operation uses gradient ascent (ascending on loss) rather than the typical gradient descent, which may be counter-intuitive.
  - Quick check question: How does gradient ascent on the loss function cause the model to "forget" specific knowledge?

- Concept: **Neuron Importance Scoring (SNIP)**
  - Why needed here: The method relies on SNIP scoring to identify which neurons store valuable knowledge worth preserving.
  - Quick check question: What does the first-order approximation I(W,x) = |W ⊙ ∇_W L(x)| actually measure about each neuron?

## Architecture Onboarding

- Component map:
  Alpaca Dataset → Neuron Scoring (SNIP) → Rank Neurons → Top 80% = KRNs (locked) / Bottom 20% = mutable
  ↓
  AdvBench (harmful) → Gradient Ascent Training → Only updates non-KRN neurons in MLP layers 8-12
  ↓
  Regularized Loss L = max(0, λ + L_f) → Trained Model

- Critical path:
  1. Neuron importance scoring on calibration data (one-time, before unlearning)
  2. Layer selection (layers 8-12 for Llama2-7B architecture)
  3. NLR configuration (0.8 = lock top 80% of neurons per layer)
  4. λ regularization setting (1.5 for balance)
  5. Gradient ascent training on harmful dataset (1 epoch, lr=4e-6)

- Design tradeoffs:
  - **NLR (Neuron Locking Rate)**: Higher NLR preserves more utility but may preserve harmful knowledge; paper finds 0.8 optimal
  - **Layer selection**: More layers = more safety gain but more utility loss; layers 8-12 are empirically optimal for Llama2-7B
  - **λ value**: Lower values allow more aggressive unlearning but risk over-degradation; λ=1.5 balances safety/utility
  - **Unlearning epochs**: Paper uses 1 epoch; more epochs may improve safety but increase utility loss

- Failure signatures:
  - **Over-unlearning**: Model becomes overly refusful, refusing benign requests (λ too low or too many epochs)
  - **Under-unlearning**: ASR remains high after training (NLR too high, λ too high, or layer selection incorrect)
  - **Catastrophic forgetting**: Significant drop on utility benchmarks (NLR too low or unlearning applied to wrong layers)
  - **Inconsistent neuron selection**: Random neuron selection shows ~2x worse utility preservation than SNIP-based selection (Table 3)

- First 3 experiments:
  1. **Baseline validation**: Replicate the MLP-only unlearning experiment (Figure 3) on your model to confirm MLP layers are the critical target for your architecture.
  2. **NLR sweep**: Run unlearning with NLR values [0.5, 0.6, 0.7, 0.8, 0.9] to find optimal locking rate for your specific model and dataset combination.
  3. **Layer range ablation**: Test different layer ranges (e.g., 4-8, 8-12, 12-16, all MLP layers) to identify which layers yield the best safety-utility trade-off for your model architecture.

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture-specific findings: The optimal layer range (8-12) and NLR value (0.8) are derived specifically from Llama2-7B-Chat experiments and may not generalize across different model architectures.
- Knowledge localization reliability: The SNIP-based neuron scoring assumes harmful and useful knowledge are spatially separable in neural space, which may not hold if these knowledge types overlap significantly.
- Evaluation scope: Safety improvements are tested against specific attack methods and datasets, with performance against novel or hybrid attack strategies remaining untested.

## Confidence
- **High confidence**: The core mechanism of neuron-level knowledge localization through SNIP scoring is technically sound and well-supported by the literature on network pruning.
- **Medium confidence**: The safety improvements (4x reduction in ASR) are substantial and consistent across multiple attack methods, but depend heavily on the specific layer selection and NLR parameters.
- **Medium confidence**: The utility preservation (0.15% accuracy reduction) is impressive but was measured on specific benchmarks, with real-world impact on diverse downstream tasks remaining uncertain.

## Next Checks
1. **Architecture transfer test**: Apply CKU to a different model family (e.g., Mistral, Qwen) and verify whether the optimal layer range remains similar or requires re-calibration, measuring both safety gains and utility retention.
2. **Adversarial robustness test**: Evaluate the trained model against hybrid attack methods that combine multiple jailbreak techniques or novel prompt engineering strategies not included in the original training set.
3. **Knowledge leakage assessment**: Perform targeted probing experiments to verify that harmful knowledge is truly unlearned rather than merely suppressed, testing for potential knowledge resurfacing under specific contexts or attack patterns.