---
ver: rpa2
title: Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across
  Table-to-Text Serialization Approaches
arxiv_id: '2508.21512'
source_url: https://arxiv.org/abs/2508.21512
tags:
- loan
- llms
- data
- fairness
- serialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how serialization formats and in-context learning
  affect the performance and fairness of large language models (LLMs) in loan approval
  tasks across three geographically distinct datasets. Using six serialization methods
  and ten LLMs, including finance-specific variants, the researchers find that most
  LLMs underperform a logistic regression baseline in zero-shot settings, with only
  specific serialization formats (e.g., GReaT, LIFT) yielding modest gains.
---

# Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches

## Quick Facts
- **arXiv ID**: 2508.21512
- **Source URL**: https://arxiv.org/abs/2508.21512
- **Reference count**: 40
- **One-line primary result**: LLMs generally underperform logistic regression baselines in zero-shot loan approval, with serialization format and in-context learning significantly affecting both performance and fairness outcomes.

## Executive Summary
This study evaluates how serialization formats and in-context learning affect the performance and fairness of large language models (LLMs) in loan approval tasks across three geographically distinct datasets. Using six serialization methods and ten LLMs, including finance-specific variants, the researchers find that most LLMs underperform a logistic regression baseline in zero-shot settings, with only specific serialization formats (e.g., GReaT, LIFT) yielding modest gains. In-context learning improves performance by 4.9-59.6% but does not consistently reduce fairness disparities, which remain highly dependent on serialization and dataset. Notably, finance-trained models often amplify gender biases, underscoring the critical role of data representation in ensuring both accuracy and fairness in high-stakes decision-making.

## Method Summary
The paper evaluates ten LLMs (including LLaMA-3, Gemma-2, and finance-specific FinMA variants) on three loan datasets (Ghana, Germany, US) using six serialization formats to convert tabular data into text prompts. Models are tested in zero-shot and few-shot (2-8 examples) settings with gender-balanced examples. Performance is measured via weighted F1 score, while fairness is assessed using Equality of Opportunity and Statistical Parity metrics with gender as the sensitive attribute. Results are compared against logistic regression baselines and aggregated across three prompt variations per dataset.

## Key Results
- Most LLMs underperform logistic regression baselines in zero-shot settings across all datasets
- GReaT and LIFT serialization formats achieve higher F1 scores but exacerbate fairness disparities
- In-context learning improves performance by 4.9-59.6% but does not consistently reduce fairness disparities
- Finance-trained models (FinMA-7B variants) often amplify gender biases compared to general-purpose models
- Serialization format choice significantly affects both performance and fairness outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Serialization format choice causally influences both LLM predictive performance and fairness outcomes in loan approval tasks.
- Mechanism: Different serialization methods (JSON, GReaT, LIFT, Text, etc.) alter how feature-value pairs are tokenized and attended to by LLMs, changing the effective representation and thus which features influence predictions. Natural language-like formats (GReaT, LIFT) may improve model comprehension but can also amplify attention to sensitive attributes embedded in narrative structures.
- Core assumption: LLMs' token attention patterns systematically shift based on serialization format, affecting feature importance in decision-making.
- Evidence anchors:
  - "the choice of serialization format significantly affects both performance and fairness in LLMs, with certain formats such as GReaT and LIFT yielding higher F1 scores but exacerbating fairness disparities"
  - "Serialization methods can significantly influence loan approval or denial, which, in turn, may have long-term consequences for individuals wrongly denied loans."
- Break condition: If LLMs were equally robust to all serialization formats (i.e., format-invariant performance and fairness), this mechanism would not hold.

### Mechanism 2
- Claim: In-context learning improves performance through pattern recognition but does not systematically reduce fairness disparities.
- Mechanism: ICL provides task-relevant examples that help LLMs infer the decision boundary via pattern matching, improving F1 scores. However, if few-shot examples are not carefully balanced or if the model's prior biases are strong, the examples may reinforce rather than mitigate existing disparities. The paper uses gender-balanced examples but still observes inconsistent fairness effects.
- Core assumption: Few-shot examples primarily improve task understanding rather than bias mitigation unless explicitly designed for fairness.
- Evidence anchors:
  - "while ICL improved model performance by 4.9-59.6% relative to zero-shot baselines, its effect on fairness varied considerably across datasets"
  - "Model performance improves with more example shots, improving LLM decision-making for loan approval"
  - "Fairness in few-shot learning is highly context-dependent. While more examples can sometimes reduce disparities, the impact is not universal"
- Break condition: If ICL consistently reduced fairness disparities across all datasets and models, the mechanism would need revision to explain systematic fairness improvement.

### Mechanism 3
- Claim: Finance-domain fine-tuning can amplify historical gender biases present in training data.
- Mechanism: Domain-specific models (FinMA-7B-full, FinMA-7B-NLP) are fine-tuned on financial corpora that may contain historical lending patterns reflecting gender discrimination. This fine-tuning can entrench these biases more deeply than in general-purpose models, leading to higher disparity in equality of opportunity and statistical parity metrics.
- Core assumption: Financial training data contains historically embedded gender biases that are reinforced during fine-tuning rather than neutralized.
- Evidence anchors:
  - "finance-trained models often amplify gender biases"
  - "It is interesting to note that this model [FinMA-7B-full], among the other models selected in this study, is the only one fine-tuned for finance. This, therefore, opens up interesting research directions for further investigating the fairness of downstream tasks"
  - "Financial-based models exhibit greater disparities in EO mean difference and high performance does not equate to fairness"
- Break condition: If finance-specific models consistently showed lower or equal bias compared to general-purpose models, the mechanism would be falsified.

## Foundational Learning

- Concept: **Table-to-Text Serialization**
  - Why needed here: LLMs cannot directly process tabular data; serialization converts structured feature-value pairs into natural language or semi-structured text. The paper evaluates six formats, showing this choice materially affects outcomes.
  - Quick check question: Given a loan applicant row with features {age: 32, income: $50,000, gender: female}, how would JSON vs. LIFT serialization differ in representation?

- Concept: **Equality of Opportunity (EO) and Statistical Parity (SP)**
  - Why needed here: These are the core fairness metrics used. EO measures whether qualified applicants have equal approval rates across groups; SP measures whether approval rates are independent of protected attributes regardless of qualification.
  - Quick check question: If a model approves 80% of qualified males but only 60% of qualified females, which fairness metric does this violate?

- Concept: **In-Context Learning (ICL) with Few-Shot Examples**
  - Why needed here: The primary intervention studied for improving LLM performance. Understanding how example selection (balanced by gender in this study) affects both accuracy and fairness is central to the paper's contribution.
  - Quick check question: Why might adding more few-shot examples improve F1 score but not necessarily reduce gender-based fairness disparities?

## Architecture Onboarding

- Component map: Dataset preprocessing -> Stratified split by gender -> Serialization conversion -> Prompt construction -> LLM inference -> Metric computation
- Critical path: 1) Dataset preprocessing → stratified split by gender 2) Serialization of test samples into target format 3) Prompt construction (zero-shot or n-shot with balanced examples) 4) LLM inference via log-likelihood prediction over class tokens 5) Metric computation (F1, EO, SP) aggregated across prompts and serialization formats
- Design tradeoffs:
  - Natural language vs. structured serialization: LIFT/GReaT may improve comprehension but can increase bias; JSON/List are more neutral but may underutilize LLM capabilities
  - More few-shot examples: Improves F1 but fairness effects are dataset-dependent and can worsen disparities (EO differences >0.10 observed in Ghana with n=8)
  - Finance-specific models: Designed for domain relevance but show higher gender disparity; general-purpose models may be fairer but less accurate
  - Model scale: Larger models (Gemma-2-27b vs. 9b) show performance gains but also increased fairness sensitivity
- Failure signatures:
  - LLM performs at or below zero/one model baseline (indicates failure to learn task structure)
  - High variance across prompt variations (Germany dataset showed significantly higher prompt sensitivity)
  - EO difference >0.10 in few-shot settings (indicates emerging fairness disparity)
  - Finance-specific model showing higher bias than general-purpose models (indicates domain fine-tuning may have amplified historical bias)
- First 3 experiments:
  1. Zero-shot baseline establishment: Run all 10 models on default JSON serialization across three datasets, compute F1/EO/SP, compare against logistic regression. Expected: most LLMs underperform baseline.
  2. Serialization format sweep: For the best-performing model per dataset, evaluate all six serialization formats. Hypothesis: GReaT/LIFT may improve F1 but increase fairness disparity; identify format with best accuracy-fairness tradeoff.
  3. Few-shot scaling with fairness monitoring: Increment n-shot examples (2, 4, 6, 8) for top 3 models, track both F1 improvement and EO/SP changes. Alert if EO difference exceeds 0.10 at any shot level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can interpretability methods successfully identify consistent feature reliance in LLMs for tabular loan decisions, given that current token attribution experiments proved inconclusive?
- Basis in paper: The Limitations section states, "We conducted token attribution experiments to better understand the reasoning... results were inconclusive," noting that no consistent feature focus was found.
- Why unresolved: Current attribution techniques failed to explain decision-making logic, leaving a gap in understanding how LLMs process serialized tabular data.
- What evidence would resolve it: Successful application of alternative interpretability methods that reveal specific, consistent features driving predictions across different serialization formats.

### Open Question 2
- Question: How can fairness-aware optimization be effectively integrated to prevent finance-specific LLMs from amplifying historical gender biases?
- Basis in paper: The Future Work section suggests exploring "fairness-aware optimization," while the Results show that finance-tuned models (FinMA) often amplify gender biases compared to general-purpose models.
- Why unresolved: The study indicates that domain-specific training currently correlates with increased fairness disparities rather than mitigation.
- What evidence would resolve it: A training methodology that maintains financial domain accuracy while achieving statistical parity and equality of opportunity scores superior to general-purpose baselines.

### Open Question 3
- Question: To what extent does comprehensive prompt engineering affect the reliability of the observed performance-fairness trade-offs compared to serialization choices?
- Basis in paper: The Limitations section notes that a "comprehensive exploration of prompt engineering techniques is beyond this work’s scope," acknowledging that only three simple prompts were tested per task.
- Why unresolved: It remains unclear if the observed fairness issues are artifacts of specific prompt structures or inherent to the model-data interaction.
- What evidence would resolve it: A comparative study of advanced prompt strategies (e.g., Chain-of-Thought) demonstrating significant shifts in fairness metrics independent of serialization formats.

## Limitations
- The study only analyzes gender-based fairness disparities, ignoring other protected attributes like race, age, or income
- The paper lacks ablation studies on prompt engineering, making it difficult to isolate the effects of serialization format from prompt structure
- No causal investigation into whether finance model bias amplification stems from training data quality or fine-tuning methodology

## Confidence

- **High Confidence**: LLMs underperform logistic regression baselines in zero-shot settings (well-supported by comparative analysis across ten models and three datasets)
- **Medium Confidence**: Finance-specific fine-tuning amplifies gender biases (plausible but lacks causal investigation into data vs. methodology)
- **Low Confidence**: LIFT and GReaT formats yield best accuracy-fairness tradeoff (not universally supported; results highly dataset-dependent)

## Next Checks
1. **Ablation on Prompt Structure**: Isolate effects of serialization format from prompt engineering by holding serialization constant while varying prompt templates, and vice versa, across all three datasets.
2. **Multi-Attribute Fairness Analysis**: Extend fairness evaluation beyond gender to include race (US dataset) and age (Germany dataset) to test whether serialization effects persist across different protected attributes.
3. **Causal Investigation of Finance Model Bias**: Conduct an ablation study comparing finance models fine-tuned on historical vs. balanced data to determine whether observed bias amplification is due to data provenance or fine-tuning methodology.