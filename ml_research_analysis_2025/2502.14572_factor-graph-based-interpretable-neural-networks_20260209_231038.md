---
ver: rpa2
title: Factor Graph-based Interpretable Neural Networks
arxiv_id: '2502.14572'
source_url: https://arxiv.org/abs/2502.14572
tags:
- factor
- concept
- explanations
- graph
- again
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGAIN is an interpretable neural network designed to generate comprehensible
  concept-level explanations under unknown perturbations. It addresses the challenge
  of generating comprehensible explanations when the input data is infused with malicious
  perturbations, which can misguide the model to produce incomprehensible explanations.
---

# Factor Graph-based Interpretable Neural Networks

## Quick Facts
- arXiv ID: 2502.14572
- Source URL: https://arxiv.org/abs/2502.14572
- Reference count: 40
- AGAIN achieves nearly 100% identification rate and 98% success rate in recognizing and rectifying explanations affected by unknown perturbations

## Executive Summary
AGAIN is an interpretable neural network designed to generate comprehensible concept-level explanations under unknown perturbations. The approach directly integrates logical rules expressed as a factor graph during inference to identify and rectify logical errors in explanations without learning perturbations. This addresses the challenge of generating comprehensible explanations when input data is infused with malicious perturbations that can misguide models to produce incomprehensible explanations. Unlike existing solutions that rely on adversarial training and assume known perturbations, AGAIN provides a perturbation-agnostic approach that maintains interpretability.

## Method Summary
AGAIN constructs a factor graph that encodes logical rules about what constitutes valid explanations. During inference, the system identifies logical errors in generated explanations by analyzing the factor graph structure, then rectifies these errors through graph-based inference. The approach avoids the need for adversarial training and explicit perturbation modeling by focusing on the logical consistency of explanations themselves. The method is evaluated across three diverse datasets (CUB, MIMIC-III EWS, and Synthetic-MNIST) to demonstrate its effectiveness in maintaining comprehensibility under various types of unknown perturbations.

## Key Results
- Achieves nearly 100% identification rate (IR) for recognizing explanations affected by perturbations
- Attains up to 98% average success rate (SR) in rectifying explanations to be logically consistent
- Demonstrates superior performance compared to state-of-the-art baselines in terms of comprehensibility under unknown perturbations

## Why This Works (Mechanism)
The mechanism works by shifting the focus from learning to resist perturbations to understanding what makes explanations logically consistent. By encoding domain knowledge as factor graphs, the system can detect when an explanation violates logical rules regardless of how the perturbation was introduced. The graph structure allows for efficient inference of corrections by propagating constraints through the explanation components, ensuring that the final output satisfies all logical requirements.

## Foundational Learning

1. **Factor Graphs** - Why needed: Provide a framework for representing complex relationships between variables using factor nodes and variable nodes
   Quick check: Can represent the factorization of a function as a bipartite graph with edges connecting factors to their arguments

2. **Adversarial Perturbations** - Why needed: Understanding how small input modifications can dramatically affect model outputs and explanations
   Quick check: Small, intentional input modifications designed to cause model misbehavior while being imperceptible to humans

3. **Interpretable Neural Networks** - Why needed: Methods for generating explanations that humans can understand and verify
   Quick check: Explanations should be concept-level rather than just highlighting input regions

## Architecture Onboarding

**Component Map:** Input Data -> Neural Network Backbone -> Explanation Generator -> Factor Graph Checker -> Rectified Explanation

**Critical Path:** The critical path flows from the neural network's explanation generation through the factor graph checker, where logical violations are identified and corrected. This represents the core innovation of the approach.

**Design Tradeoffs:** The method sacrifices the ability to learn perturbation-specific defenses in exchange for perturbation-agnostic robustness. This makes it more broadly applicable but potentially less optimal for known perturbation types.

**Failure Signatures:** The system may struggle with novel logical relationships not captured in the factor graph, or with highly complex perturbations that create cascading logical errors difficult to resolve through graph-based inference alone.

**First Experiments:** 1) Test on clean data to establish baseline explanation quality 2) Introduce simple logical violations to verify detection and correction 3) Apply known perturbation types to measure robustness compared to adversarial training approaches

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations

- The evaluation methodology for "comprehensibility" remains unclear despite high performance claims
- The factor graph construction likely embeds implicit assumptions about logical errors that may not generalize across all domains
- While perturbation-agnostic, the approach may be less effective than perturbation-specific methods when perturbation types are known

## Confidence

High confidence in the methodology for interpretability without adversarial training
Medium confidence in the perturbation-agnostic claims due to potential implicit assumptions in factor graph construction
High confidence in the specific experimental results on the three evaluated datasets

## Next Checks

1. Conduct ablation studies removing specific logical rules from the factor graph to quantify their individual contributions to performance and identify which rules are most critical for robustness.

2. Test AGAIN's performance on datasets with different underlying data distributions and explanation requirements to assess generalizability beyond the three evaluated domains.

3. Implement a controlled experiment introducing perturbations of varying severity and types not present in training to rigorously evaluate the "unknown perturbation" claims and measure performance degradation thresholds.