---
ver: rpa2
title: Emotion Classification In-Context in Spanish
arxiv_id: '2505.20571'
source_url: https://arxiv.org/abs/2505.20571
tags:
- spanish
- accuracy
- dataset
- classification
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of emotion classification in
  Spanish customer feedback by proposing a Custom Stacking Ensemble (CSE) model that
  combines TF-IDF and BERT embeddings. Traditional approaches often rely on translated
  datasets, which lose semantic integrity and contextual nuances.
---

# Emotion Classification In-Context in Spanish

## Quick Facts
- arXiv ID: 2505.20571
- Source URL: https://arxiv.org/abs/2505.20571
- Authors: Bipul Thapa; Gabriel Cofre
- Reference count: 23
- Primary result: Custom Stacking Ensemble (CSE) achieves 93.3% accuracy on native Spanish emotion classification, outperforming individual models and fine-tuned transformers

## Executive Summary
This study addresses emotion classification in Spanish customer feedback by proposing a Custom Stacking Ensemble (CSE) model that combines TF-IDF and BERT embeddings. Traditional approaches often rely on translated datasets, which lose semantic integrity and contextual nuances. The CSE approach integrates four base classifiers—Logistic Regression, KNN, Bagging with LGBM, and AdaBoost—with a one-vs-rest Logistic Regression meta-model. Experimental results show the CSE achieves 93.3% accuracy on native Spanish data, significantly outperforming individual models and fine-tuned transformer models. The findings highlight the advantages of using native Spanish datasets and combining traditional vectorization with BERT embeddings for improved emotion classification accuracy.

## Method Summary
The CSE model combines TF-IDF and BERT embeddings as hybrid features, then applies stacking ensemble learning with four diverse base classifiers (Logistic Regression, KNN, Bagging+LGBM, AdaBoost) under a one-vs-rest Logistic Regression meta-model. The approach uses 80/20 train/test split with 5-fold cross-validation on a 1045-sample Spanish healthcare feedback dataset. Key hyperparameters include C=0.1 for logistic regression, n_neighbors=3 for KNN, and 50 estimators for boosting algorithms.

## Key Results
- CSE achieves 93.3% accuracy on native Spanish emotion classification task
- Native Spanish data outperforms machine-translated data (93.3% vs 88.0% accuracy)
- CSE outperforms individual models (best individual: 88.0%) and fine-tuned transformers (88.0%)

## Why This Works (Mechanism)

### Mechanism 1
Combining TF-IDF with BERT embeddings improves classification accuracy over either technique alone. TF-IDF captures discriminative word importance through term frequency statistics, while BERT provides contextual semantic embeddings that encode sentence-level meaning and word position. The concatenation creates a richer feature space where both statistical patterns and deep semantic structures are available to classifiers. Core assumption: The two representations provide complementary rather than redundant information. Evidence: Hybrid approach achieves 93.3% accuracy vs individual models. Break condition: If dataset has very short texts (<10 tokens), TF-IDF may add minimal value.

### Mechanism 2
Stacking diverse base classifiers under a logistic regression meta-model captures complementary decision boundaries that single models miss. Four base models with different inductive biases—Logistic Regression (linear), KNN (instance-based/local), LGBM with Bagging (gradient boosting with variance reduction), AdaBoost (adaptive focus on hard examples)—each produce class probabilities. The meta-model learns to weight these predictions optimally rather than simple averaging. Core assumption: Base model errors are partially uncorrelated. Evidence: CSE achieves 93.3% accuracy vs 88.0% for best individual model. Break condition: If base models are too correlated, diversity benefits diminish.

### Mechanism 3
Native Spanish data outperforms machine-translated equivalents for emotion classification. Translation loses semantic integrity and contextual nuances specific to the original language (idioms, cultural expressions, register). MarianMT translation from Spanish to English introduces approximation errors that compound during classification, reducing signal quality. Core assumption: Translation quality impacts emotion classification accuracy. Evidence: Spanish dataset achieves 93.3% accuracy vs 88.0% for English-translated version. Break condition: Confounding factors like BERT's pre-training corpus language distribution could contribute.

## Foundational Learning

- **TF-IDF Vectorization**
  - Why needed here: Provides sparse statistical features capturing which words discriminate emotion classes; forms half of the hybrid representation
  - Quick check question: Given a corpus with documents ["bueno servicio", "malo servicio", "servicio excelente"], which term has the highest IDF weight?

- **Stacking Ensemble Learning**
  - Why needed here: Core architecture of CSE; understanding how meta-models combine base predictions is essential for debugging and extending the system
  - Quick check question: If all base classifiers predict class probabilities [0.6, 0.3, 0.1] for the same input, what would a simple averaging meta-model output, and why might a trained meta-model differ?

- **BERT Contextual Embeddings**
  - Why needed here: Provides dense semantic representations; understanding tokenization, padding, and embedding extraction is required for reproduction
  - Quick check question: Why does BERT produce different embeddings for "banco" in "el banco del parque" vs. "el banco financiero"? How does this help emotion classification?

## Architecture Onboarding

- Component map: Raw Spanish Text → Preprocessing (dedupe, lowercase, missing value handling) → TF-IDF Vector + BERT Embeddings → Base Classifiers (LR | KNN | LGBM | Ada) → Meta-Model (One-vs-Rest LR) → Final Prediction (Positive/Neutral/Negative)

- Critical path: Preprocessing quality directly affects both vectorizers—unhandled duplicates or noise propagate through. Feature concatenation alignment (TF-IDF and BERT must correspond to same samples). Base model probability calibration (meta-model expects well-calibrated inputs).

- Design tradeoffs: TF-IDF vs. BERT-only vs. Hybrid (TF-IDF is fast and interpretable; BERT captures context; hybrid increases dimensionality and compute but conditionally improves accuracy). Model diversity vs. maintenance cost (four base models increase complexity but provide stacking benefits). 5-fold CV vs. hold-out (CV provides robustness but increases training time 5×).

- Failure signatures: Base models all agree but are wrong (low diversity; add different model families). Meta-model overfits (cross-validation predictions may leak; ensure proper hold-out). BERT embeddings dominate (if TF-IDF features are sparse and high-dimensional, they may be ignored; consider feature scaling).

- First 3 experiments: 1) Ablation test: Run CSE with TF-IDF only, BERT only, and combined. Verify that combined > either alone. 2) Base model correlation analysis: Compute prediction agreement matrix across base models; high correlation (>0.85) suggests need for more diversity. 3) Translation validation: If working with multilingual data, compare native vs. machine-translated performance on a held-out subset to quantify translation loss.

## Open Questions the Paper Calls Out

### Open Question 1
Would the CSE approach maintain its performance advantage over transformer-based models when scaled to larger Spanish datasets (e.g., tens of thousands of samples)? The authors state future research involves expanding datasets and note BERT "typically requires significantly larger datasets to perform effectively." The current study uses only 1,045 entries, which may be insufficient for transformer models. Evidence would require replicating experiments on larger native Spanish datasets (10K+ samples).

### Open Question 2
How does the CSE model perform across different Spanish dialects and regional variations (e.g., Mexican, Argentine, Peninsular Spanish)? The introduction notes Spanish presents unique challenges due to diverse regional variations, but the study uses data from a single source without examining dialectal differences. Evidence would require evaluating the trained CSE model on annotated Spanish feedback datasets from multiple Spanish-speaking countries.

### Open Question 3
Can the CSE approach generalize to other domains beyond healthcare customer feedback? The dataset is exclusively from the healthcare sector, and while methodology is presented as generalizable, no cross-domain validation was performed. Healthcare feedback may have domain-specific vocabulary not representative of other sectors. Evidence would require testing on Spanish customer feedback from at least two other industry domains.

### Open Question 4
Would incorporating additional base classifiers with diverse inductive biases (e.g., SVM, neural networks) into the stacking ensemble further improve performance? The paper selected four specific base models but didn't justify exhaustively why these were chosen or explore alternative combinations. Evidence would require systematic experiments adding SVM, Random Forest, or shallow neural networks as base models.

## Limitations
- Limited dataset size (1045 samples) raises questions about generalizability to larger corpora
- No comparison against alternative ensemble architectures (voting, blending) or hyperparameter optimization strategies
- No computational efficiency metrics reported for production deployment considerations
- Single domain focus (healthcare) without cross-domain validation

## Confidence

- **High Confidence**: Native Spanish data outperforms machine-translated data (93.3% vs 88.0% accuracy) with direct experimental comparison on identical samples
- **Medium Confidence**: Combining TF-IDF with BERT embeddings improves accuracy, supported by ablation results but lacking external validation
- **Low Confidence**: Specific superiority of this four-model stacking configuration over other ensemble architectures, as alternative designs weren't benchmarked

## Next Checks

1. **Cross-domain validation**: Test the CSE model on Spanish sentiment datasets from different domains (e.g., social media, product reviews) to assess generalizability beyond healthcare feedback

2. **Translation quality control**: Compare performance using human-translated Spanish data versus MarianMT to isolate whether the accuracy gap is due to translation quality or inherent model differences

3. **Computational efficiency analysis**: Measure training and inference times for CSE versus fine-tuned transformer models to evaluate practical deployment trade-offs, particularly for real-time applications