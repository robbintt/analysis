---
ver: rpa2
title: 'Antisocial behavior towards large language model users: experimental evidence'
arxiv_id: '2601.09772'
source_url: https://arxiv.org/abs/2601.09772
tags:
- page
- participants
- actual
- self-reported
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study experimentally tested whether people punish peers for
  using large language models (LLMs) by giving participants the option to destroy
  part of others' earnings. In a real-effort emoji-counting task, Phase I participants
  completed tasks with or without LLM assistance.
---

# Antisocial behavior towards large language model users: experimental evidence

## Quick Facts
- arXiv ID: 2601.09772
- Source URL: https://arxiv.org/abs/2601.09772
- Authors: Paweł Niszczota; Cassandra Grützner
- Reference count: 40
- Key outcome: Participants destroyed 36% of earnings of exclusive LLM users, with punishment increasing monotonically as actual LLM use intensified

## Executive Summary
This study experimentally tested whether people punish peers for using large language models by giving participants the option to destroy part of others' earnings. In a real-effort emoji-counting task, Phase I participants completed tasks with or without LLM assistance. In Phase II, new participants spent their own money to reduce the bonus payments of these performers. On average, participants destroyed 36% of the earnings of those who relied exclusively on LLMs, with punishment increasing monotonically as actual LLM use intensified. Disclosure of LLM use created a credibility gap: self-reported zero use was punished more than actual zero use, suggesting suspicion of dishonesty. Conversely, at high levels of use, actual reliance led to harsher punishment than self-reported reliance.

## Method Summary
Two-phase behavioral experiment where Phase I participants (N=143) complete emoji-counting tasks with/without LLM access, and Phase II participants (N=491) decide whether to spend personal tokens (1:5 destruction ratio) to reduce Phase I participants' bonus earnings. Beta regression mixed-effects models analyze proportion of target's bonus burned, with random intercepts for participants and block order, and random slope for intensity in Model 2.

## Key Results
- 36% average destruction of earnings for exclusive LLM users
- Punishment increased monotonically with actual LLM use (OR=1.39 per intensity unit, p<0.001)
- Self-reported null use punished more harshly than actual null use (OR=1.37, p<0.001)

## Why This Works (Mechanism)

### Mechanism 1: Inequity Aversion and Effort-Based Deservingness
- Claim: LLM use triggers costly punishment because it violates fairness norms around effort-reward proportionality
- Mechanism: When LLM users achieve outcomes with less personal effort, observers experience disutility from disadvantageous inequality, motivating costly sanctions to restore perceived fairness
- Core assumption: Punishment is primarily driven by fairness concerns rather than arbitrary antisocial tendencies
- Evidence anchors:
  - [abstract] "On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use"
  - [section] "sanctions tend to escalate with the size of the undeserved advantage (Falk et al., 2005)"
  - [corpus] Limited direct corpus support; related work on cultural alignment in LLMs (arxiv:2601.02858) addresses bias but not punishment behavior
- Break condition: If baseline money burning in control conditions equals or exceeds LLM-use conditions, inequity aversion is not the primary driver

### Mechanism 2: Signaling Credibility Gap Under Information Asymmetry
- Claim: Self-reported non-use is punished more than observable non-use because observers suspect dishonesty
- Mechanism: When LLM use is possible but unobservable, declarations of "no use" function as non-credible signals, triggering suspicion of strategic underreporting to avoid stigma
- Core assumption: Observers maintain ambient distrust when they cannot verify claims about AI assistance
- Evidence anchors:
  - [abstract] "self-reported null use was punished more harshly than actual null use, suggesting that declarations of 'no use' are treated with suspicion"
  - [section] "Self-reported is an indicator variable... OR (log-odds) is larger than one (positive) and statistically significant" (Table 3)
  - [corpus] No corpus papers directly address this signaling mechanism
- Break condition: If self-reported null use is punished less than or equal to actual null use, signaling credibility is not a factor

### Mechanism 3: Character-Based Attribution Mediates Punishment
- Claim: Punishment intensity correlates with perceived laziness and incompetence of LLM users
- Mechanism: Observers infer negative character traits from LLM use, and these inferences predict punitive behavior
- Core assumption: Punishment targets the user's character rather than the technology itself
- Evidence anchors:
  - [section] "Higher levels of LLM use correlated with lower competence ratings and higher laziness ratings, which in turn predicted greater willingness to burn money"
  - [section] "the more regularly people use LLMs, the less their punishing behavior increased with the intensity of use"
  - [corpus] Weak corpus support; related work on human-robot perception (arxiv:2507.16480) addresses trait attribution but not punishment
- Break condition: If laziness/competence ratings do not correlate with money burning behavior, character attribution is not the mediating pathway

## Foundational Learning

- Concept: Money Burning Game (Zizzo, 2003)
  - Why needed here: This experimental paradigm measures costly punishment by giving participants real resources to destroy others' earnings without personal gain
  - Quick check question: If participants can burn money at a 1:5 ratio (1 token spent destroys 5 tokens), what is the maximum punishment possible with a 40-token endowment against a 100-token target?

- Concept: Inequity Aversion (Fehr & Schmidt, 1999)
  - Why needed here: Provides the theoretical framework for why people incur costs to reduce others' unfairly acquired advantages
  - Quick check question: Why would someone pay to reduce another's earnings even when they receive no material benefit?

- Concept: Signaling Theory Under Information Asymmetry
  - Why needed here: Explains why self-reports of behavior are treated differently than observable behavior when verification is impossible
  - Quick check question: In a context where AI use is stigmatized but unobservable, why might an honest declaration of non-use backfire?

## Architecture Onboarding

- Component map: Phase I emoji-counting tasks -> Phase II money burning decisions -> Statistical analysis via beta regression
- Critical path:
  1. Phase I participants complete 5 tasks, earning up to £1.00 bonus
  2. Phase II participants review all 13 target cases (1 control + 6 actual use + 6 self-reported use)
  3. Strategy method: One random decision implemented for real payment
- Design tradeoffs:
  - Emoji counting task ensures internal validity but limits ecological validity for complex knowledge work
  - Strategy method captures all decisions but may differ from one-shot choices
  - Unequal Phase I condition sizes are economically efficient but require careful statistical handling
- Failure signatures:
  - High baseline money burning in control condition (>20%) suggests confounding antisocial motivations
  - Non-monotonic punishment curves would contradict inequity aversion predictions
  - No difference between observable and self-reported conditions invalidates signaling mechanism
- First 3 experiments:
  1. Replicate with a creative or strategic task (e.g., writing, problem-solving) to test generalizability beyond counting tasks
  2. Add a "no disclosure option" condition to test whether explicit null-use declarations invite more suspicion than silence
  3. Measure mediation directly: test whether laziness/competence ratings fully mediate the LLM-use-to-punishment relationship using structural equation modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do social sanctions against LLM users persist or diminish when AI assists with open-ended, creative, or strategic tasks rather than well-defined, mechanical tasks?
- Basis in paper: [explicit] The authors state: "Future research should investigate whether these punitive responses persist or transform in contexts where AI acts as a collaborator on open-ended problems rather than a solver of well-defined tasks."
- Why unresolved: The study used only an emoji-counting task; task type was not manipulated
- What evidence would resolve it: A replication with varied task types (creative writing, strategic reasoning, personal tasks) comparing punishment rates across domains

### Open Question 2
- Question: Do more subtle forms of social sanction—such as reduced trust, lower performance evaluations, or exclusion from collaboration—operate similarly to costly punishment?
- Basis in paper: [explicit] The authors note: "In real-world settings, disapproval of AI use may manifest in more subtle ways... These less overt but equally damaging forms of social sanction were not measured here and represent a critical avenue for future study."
- Why unresolved: Only direct monetary punishment was measured; indirect sanctions were not operationalized
- What evidence would resolve it: Studies measuring trust ratings, cooperation decisions, or evaluation outcomes toward LLM users in workplace or educational settings

### Open Question 3
- Question: How does the option to remain silent about AI use (non-disclosure) affect punishment compared to forced self-reporting?
- Basis in paper: [explicit] The authors acknowledge: "We did not give the self-reported LLM use condition the option not to disclose" and suggest this may have made null-use declarations appear "especially 'loud'" and more suspicious
- Why unresolved: The self-report condition forced disclosure; silence was not an available option
- What evidence would resolve it: An experiment comparing three conditions: observable use, voluntary disclosure (including silence option), and forced self-report

### Open Question 4
- Question: Do cultural differences moderate antisocial punishment toward LLM users?
- Basis in paper: [inferred] The sample was UK-only and recruited via Prolific; the authors note crowdsourcing samples differ from general populations and that AI is evaluated differently across domains and cultural contexts
- Why unresolved: No cross-cultural comparison was conducted; generalizability beyond the UK is unknown
- What evidence would resolve it: Cross-cultural replications comparing punishment rates across societies with differing norms around technology, effort, and fairness

## Limitations

- Emoji-counting task limits ecological validity for complex knowledge work
- Artificial 1:5 destruction ratio may exaggerate punishment behavior
- Phase I sample size (N=143) constrained by economic efficiency, limiting power for interaction detection

## Confidence

**High confidence:** The finding that LLM users face real social sanctions (36% average earnings destruction) is well-supported by the monotonic relationship between actual use intensity and punishment (OR=1.39, p<0.001). The experimental paradigm is validated, and the statistical methods are appropriate.

**Medium confidence:** The signaling mechanism showing that self-reported null use is punished more harshly than actual null use is supported (OR=1.37, p<0.001) but requires additional conditions to rule out alternative explanations such as self-selection effects or order effects in the strategy method design.

**Medium confidence:** The mediation through character attribution (laziness and incompetence ratings predicting punishment) is supported by correlations but not tested with formal mediation analysis. The correlational nature limits causal inference.

## Next Checks

1. **Generalizability to Complex Tasks**: Replicate the experiment using a creative or strategic task (e.g., writing, problem-solving) to test whether the punishment pattern holds beyond simple counting tasks. This addresses the primary ecological validity concern and tests whether the fairness violation is task-specific or generalizable.

2. **Mediation Analysis for Character Attribution**: Conduct formal mediation analysis (e.g., structural equation modeling) to test whether perceived laziness and incompetence fully mediate the relationship between LLM use and punishment. This would validate the proposed mechanism and quantify its explanatory power.

3. **Alternative Disclosure Conditions**: Add a condition where participants can choose to disclose or not disclose LLM use (rather than being assigned to self-report vs. observable). This would test whether explicit null-use declarations are uniquely suspect compared to strategic silence, isolating the signaling mechanism from selection effects.