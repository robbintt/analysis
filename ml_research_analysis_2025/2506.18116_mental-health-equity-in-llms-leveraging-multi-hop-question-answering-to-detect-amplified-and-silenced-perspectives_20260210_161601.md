---
ver: rpa2
title: 'Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect
  Amplified and Silenced Perspectives'
arxiv_id: '2506.18116'
source_url: https://arxiv.org/abs/2506.18116
tags:
- bias
- mental
- health
- across
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a novel multi-hop question answering (MHQA)
  framework to systematically detect intersectional biases in large language models
  (LLMs) when responding to mental health queries. By analyzing the Interpretable
  Mental Health Instruction (IMHI) dataset, the framework reveals how LLMs generate
  disparate responses across demographic intersections involving age, race, gender,
  and socioeconomic status.
---

# Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives

## Quick Facts
- arXiv ID: 2506.18116
- Source URL: https://arxiv.org/abs/2506.18116
- Reference count: 0
- Four leading models evaluated for intersectional bias in mental health responses using multi-hop question answering framework

## Executive Summary
This study presents a novel multi-hop question answering (MHQA) framework to systematically detect intersectional biases in large language models (LLMs) when responding to mental health queries. By analyzing the Interpretable Mental Health Instruction (IMHI) dataset, the framework reveals how LLMs generate disparate responses across demographic intersections involving age, race, gender, and socioeconomic status. Four leading models were evaluated: Claude 3.5 Sonnet, Jamba 1.6, Gemma-3, and Llama-4. The MHQA approach demonstrates superior detection of subtle bias patterns compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. Two debiasing techniques—Roleplay Simulation and Explicit Bias Reduction—achieved 66-94% bias reductions across different model-category combinations using few-shot prompting with BBQ dataset examples. The findings highlight critical areas where LLMs reproduce and amplify mental healthcare biases, providing actionable insights for more equitable AI development in sensitive healthcare domains.

## Method Summary
The study develops a multi-hop question answering framework to detect intersectional biases in LLMs responding to mental health queries. The IMHI dataset (Dreaddit and MultiWD subsets) provides 3,626 mental health posts tagged with demographic attributes and conditions. Posts are labeled using Claude 3.5 Sonnet with zero/few-shot prompting across five dimensions: age, gender, race, socioeconomic status, and mental health conditions. Questions are generated from templates asking how demographic factors "positively" or "negatively" affect symptoms, coping, or relationships. The MHQA format requires aggregating evidence from three source posts per question, with artificial posts generated when real data is insufficient. Four models are evaluated under zero-shot and few-shot conditions with debiasing interventions (Roleplay Simulation and Explicit Bias Reduction) using BBQ dataset examples. Bias is scored across sentiment/tone, demographic factors, and mental health condition dimensions.

## Key Results
- MHQA framework identified amplification points where biases magnify through sequential reasoning compared to conventional methods
- Complex intersectional identities (e.g., Black, female, low-income, depressed) received vaguer or more pathologizing responses than single-dimension identities
- Two debiasing techniques achieved 66-94% bias reductions across model-category combinations using few-shot BBQ examples
- Gemma-3 retained highest absolute bias scores even after interventions, showing partial resistance to prompting techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-hop questioning reveals bias through sequential reasoning amplification
- Mechanism: By requiring models to connect demographic information with mental health concepts across multiple reasoning steps, MHQA exposes "amplification points" where small initial biases magnify through the chain. The paper frames this as detecting bias "in final outputs and the reasoning pathways themselves."
- Core assumption: Biases that remain hidden in single-turn queries will compound when models must traverse internal knowledge connecting demographics to clinical concepts.
- Evidence anchors:
  - [abstract] "identifying amplification points where biases magnify through sequential reasoning"
  - [section 1] "By analyzing how LLMs traverse their internal knowledge to connect demographic information with mental health concepts, we identify instances where bias emerges in final outputs and the reasoning pathways themselves"
  - [corpus] Limited direct corpus support; neighboring papers focus on QA benchmarks rather than amplification mechanisms specifically
- Break condition: If models process each hop independently without carrying forward context, amplification would not occur. Evidence does not establish the internal causality of amplification—only that MHQA detects more bias than conventional methods.

### Mechanism 2
- Claim: Few-shot BBQ examples orient models toward equitable reasoning patterns
- Mechanism: Pre-classified examples from the BBQ dataset embedded at prompt start provide implicit guidance toward context-sensitive reasoning without explicit anti-bias instructions. This achieved 66-94% bias reductions across model-category combinations.
- Core assumption: Models can generalize fairness patterns from BBQ's multiple-choice format to open-ended mental health responses.
- Evidence anchors:
  - [abstract] "achieving 66-94% bias reductions across different model-category combinations using few-shot prompting with BBQ dataset examples"
  - [section 2.5] "These curated few-shot examples were embedded at the beginning of each prompt, serving as implicit guidance to the model... without directly instructing it to mitigate bias"
  - [corpus] BBQ is a known bias benchmark (Parrish et al., 2022 cited), but corpus lacks replication studies for this specific transfer mechanism
- Break condition: Effectiveness varies significantly by model. Gemma-3 retained the highest absolute bias scores even after few-shot intervention, suggesting the mechanism does not transfer uniformly across architectures.

### Mechanism 3
- Claim: Intersectional identities compound bias beyond additive effects
- Mechanism: Responses to complex intersections (e.g., "Black, female, low-income, and depressed") received vaguer or more pathologizing replies than single-dimension marginalized identities, suggesting LLMs amplify rather than mitigate at intersections.
- Core assumption: The observed response degradation at intersections reflects model reasoning behavior, not merely training data sparsity.
- Evidence anchors:
  - [section 3.4] "Posts reflecting complex intersections—such as 'Black, female, low-income, and depressed'—frequently received vague or pathologizing replies, indicating that LLMs tend to amplify rather than mitigate bias at identity intersections"
  - [section 3.4] "responses to low-income young adults with depression were more negatively framed than those to high-income users with the same condition"
  - [corpus] Magee et al. (2021, CHI) on intersectionality in AI bias research is cited; corpus shows growing interest but limited mechanistic validation
- Break condition: Dataset imbalance (seniors: 55 tags vs. children: 994) may confound intersectional findings. The paper acknowledges underrepresentation of key groups, making it difficult to separate model behavior from data sparsity effects.

## Foundational Learning

- Concept: **Multi-hop Question Answering**
  - Why needed here: MHQA requires integrating multiple information sources to answer complex questions. The paper uses 3-source prompts to simulate how LLMs might aggregate evidence in real deployment.
  - Quick check question: Can you explain why asking "How does being Hispanic positively affect anxiety symptoms?" across three sources differs from asking the same question about a single source?

- Concept: **Intersectional Bias**
  - Why needed here: The framework's core insight is that bias at demographic intersections (e.g., Black + low-income + depressed) differs qualitatively from single-axis bias. Standard evaluations miss these compound effects.
  - Quick check question: If a model shows no bias on gender alone and no bias on income alone, would you expect it to show no bias on gender-plus-income interactions?

- Concept: **Prompt-Based Debiasing**
  - Why needed here: Two techniques (Roleplay Simulation, Explicit Bias Reduction) produced different results across models. Understanding prompting mechanics is essential for selecting interventions.
  - Quick check question: Why might explicitly instructing a model to avoid bias sometimes increase bias (as happened with Jamba 1.6's mental health condition bias rising to 0.520)?

## Architecture Onboarding

- Component map:
  - IMHI dataset → Dreaddit (stress) + MultiWD (wellness) subsets → Claude 3.5 Sonnet tagging across 5 dimensions
  - Template-based question generation targeting symptom/coping/treatment factors with positive/negative framing variants
  - MHQA pipeline: 3-source aggregation (real posts + artificially generated posts when <3 available) → LLM response generation
  - Evaluation layer: Bias scoring across sentiment/tone, demographic, and mental health condition dimensions
  - Intervention layer: Few-shot BBQ examples, Roleplay Simulation, Explicit Bias Reduction prompts

- Critical path:
  1. Tag posts with demographic/condition labels (Table 1 categories)
  2. Generate positive/negative effect questions (Figure 5 templates)
  3. Assemble 3-source MHQA prompts (supplement with Claude-generated posts if needed)
  4. Collect model responses under controlled parameters (≤120 words)
  5. Score bias across three dimensions
  6. Apply debiasing intervention and re-score

- Design tradeoffs:
  - **Artificial post generation**: Required to meet 3-source requirement but introduces potential skew (176 artificial posts for race vs. 4 for gender). Paper acknowledges this may bias evaluation.
  - **BBQ transfer**: Using multiple-choice QA examples for open-ended mental health responses assumes pattern generalization that may not hold across domains.
  - **Question framing**: Suggestive wording ("positively/negatively affect") was chosen to probe bias, but may prime certain response patterns.

- Failure signatures:
  - **Jamba 1.6 explicit debiasing**: Mental health bias increased from 0.344 → 0.520, suggesting misalignment between reasoning chains and fairness expectations
  - **Gemma-3 persistent sentiment bias**: Highest zero-shot sentiment bias (0.771) and remained highest after few-shot (0.341), indicating partial resistance to prompting interventions
  - **Claude condition-specific paradox**: Lower tone/demographic bias than Jamba but higher mental health condition bias (0.380 vs. 0.344), suggesting surface-level neutrality may obscure deeper issues

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot MHQA on a held-out subset of IMHI posts to verify bias score distributions match Table 2 before implementing interventions.
  2. **Intervention ablation**: Test Roleplay Simulation vs. Explicit Bias Reduction in isolation (not combined) to isolate which mechanism drives reductions for each model.
  3. **Intersectional stress test**: Construct synthetic posts targeting underrepresented intersections (e.g., senior + high-income + Asian + bipolar) to evaluate whether findings generalize beyond dataset skews or are artifacts of data imbalance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the artificial post generation procedure impact the validity of bias measurements for underrepresented demographic categories, particularly for race and socioeconomic status?
- Basis in paper: [explicit] The authors acknowledge that "the demographic categories with more artificial posts as sources may skew results in terms of bias evaluation," with 176 artificially generated posts for race compared to only 4 for gender (Figure 8).
- Why unresolved: The paper does not quantify or control for the effect of synthetic data on bias scores, leaving unclear whether detected biases reflect model behavior or artifacts of the augmentation process.
- What evidence would resolve it: Ablation studies comparing bias scores using only authentic posts versus augmented datasets, or statistical controls for the proportion of synthetic content per demographic category.

### Open Question 2
- Question: Why do explicit debiasing prompts reduce bias in some models (Llama-4) while amplifying mental health condition bias in others (Jamba 1.6 increased from 0.344 to 0.520)?
- Basis in paper: [explicit] The authors observe that "direct bias reduction instructions may inadvertently amplify certain biases in some models" and note "misalignment between reasoning chains and the nuanced expectations of fairness."
- Why unresolved: The paper reports the phenomenon but does not investigate the mechanistic or architectural causes of model-specific debiasing failures.
- What evidence would resolve it: Layer-wise analysis of attention patterns during debiasing, or controlled experiments varying model scale, training data composition, and alignment procedures.

### Open Question 3
- Question: How do bias patterns generalize to demographic intersections involving more than two identity dimensions (e.g., Black, female, low-income, senior)?
- Basis in paper: [inferred] The dataset shows severe underrepresentation of seniors (55 tags), American Indian/Indigenous (4 tags), and middle-to-high income groups (5–6 tags), and the authors state "Future work must expand intersectional bias evaluation across additional demographic dimensions."
- Why unresolved: The current framework primarily examines pairwise intersections; higher-order intersections remain unexplored due to sparse data, yet the paper notes these groups receive "vague or pathologizing replies."
- What evidence would resolve it: Targeted data collection for higher-order intersections, followed by MHQA evaluation with statistical power analysis for multi-way interaction effects.

## Limitations
- Bias scoring methodology is not specified, making exact numerical results unverifiable
- Artificial post generation introduces potential evaluation bias, particularly for underrepresented demographics
- The causal mechanism of bias amplification through sequential reasoning is asserted but not empirically demonstrated

## Confidence

- **High confidence**: The framework's practical utility in detecting intersectional bias patterns across multiple models. The consistent identification of amplification points and the varying effectiveness of debiasing techniques across models is well-supported by the data presented.
- **Medium confidence**: The specific numerical bias scores and debiasing effectiveness percentages (66-94% reductions). While the patterns are plausible, the lack of scoring methodology makes these exact values unverifiable.
- **Low confidence**: The causal claim that MHQA's multi-hop mechanism specifically amplifies bias through sequential reasoning. The evidence shows MHQA detects more bias than conventional methods, but doesn't prove the amplification mechanism rather than enhanced detection sensitivity.

## Next Checks
1. **Bias scoring protocol validation**: Implement the exact bias scoring methodology (once specified) and verify inter-annotator agreement or classifier accuracy before interpreting model comparisons.
2. **Intersectional synthetic data stress test**: Generate synthetic posts for severely underrepresented intersections (e.g., senior + high-income + Asian + bipolar) to determine whether amplification findings generalize beyond dataset skews.
3. **Mechanism isolation experiment**: Compare bias scores for identical demographic-mental health pairs using single-hop versus multi-hop question formats to empirically test whether the multi-hop structure itself causes amplification versus merely exposing hidden biases.