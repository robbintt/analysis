---
ver: rpa2
title: Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks
  and Benchmarks
arxiv_id: '2510.11040'
source_url: https://arxiv.org/abs/2510.11040
tags:
- medical
- tasks
- llms
- evaluation
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DoctorFLAN, a large-scale Chinese medical
  dataset with 92,000 Q&A instances across 22 clinical tasks and 27 specialties, designed
  to train large language models (LLMs) as doctor-facing assistants rather than patient-facing
  tools. Through expert surveys, the authors identified real-world clinical workflow
  tasks and built DoctorFLAN using GPT-4-polished responses with manual verification.
---

# Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks

## Quick Facts
- arXiv ID: 2510.11040
- Source URL: https://arxiv.org/abs/2510.11040
- Reference count: 40
- This paper introduces DoctorFLAN, a large-scale Chinese medical dataset with 92,000 Q&A instances across 22 clinical tasks and 27 specialties, designed to train large language models (LLMs) as doctor-facing assistants rather than patient-facing tools.

## Executive Summary
This paper introduces DoctorFLAN, a large-scale Chinese medical dataset with 92,000 Q&A instances across 22 clinical tasks and 27 specialties, designed to train large language models (LLMs) as doctor-facing assistants rather than patient-facing tools. Through expert surveys, the authors identified real-world clinical workflow tasks and built DoctorFLAN using GPT-4-polished responses with manual verification. They also introduced DoctorFLAN-test (550 single-turn items) and DotaBench (74 multi-turn conversations) for evaluation. Experimental results show that DoctorFLAN significantly improves the performance of open-source LLMs in medical contexts. Models fine-tuned on DoctorFLAN, such as DotaGPT, outperform general and medical LLMs, particularly in diagnosis and treatment tasks. Human evaluation confirms high consistency with automatic scores (Pearson r = 0.82), validating the dataset and benchmarks as effective tools for advancing doctor-centric medical LLM development.

## Method Summary
The study constructs DoctorFLAN by first surveying 71 licensed physicians to identify 22 clinical tasks across four workflow phases (Pre-diagnosis, Diagnosis, Treatment, Post-treatment). Medical data sources (MCQs, encyclopedia entries, existing datasets) are preprocessed with Jaccard deduplication and regex-based task classification. The refinement pipeline involves manual instruction alignment, GPT-4 response generation using original data as reference context, and expert verification for correctness (100%) and practicality (99.9%). The mixed training corpus combines 92K DoctorFLAN instances with 101K general instructions and 51K medical QA pairs. Supervised fine-tuning is performed on Yi-6B or Baichuan2-7B-Base using 4× NVIDIA A100 GPUs with specified hyperparameters. Evaluation employs DoctorFLAN-test (550 single-turn) and DotaBench (74 multi-turn) with GPT-4-as-judge scoring across four metrics (Accuracy, Coherence, Relevance, Thoroughness).

## Key Results
- DoctorFLAN significantly improves open-source LLM performance in medical contexts, particularly for diagnosis and treatment tasks (11.6-29.8% improvement over base models)
- Models fine-tuned on DoctorFLAN (e.g., DotaGPT) outperform both general-purpose LLMs and patient-facing medical LLMs on clinical workflow tasks
- Human evaluation confirms high consistency with automatic scores (Pearson correlation r = 0.82), validating the evaluation methodology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data aligned with complete clinical workflows improves LLM performance on doctor-assistance tasks
- Mechanism: By mapping 22 expert-identified tasks across 4 clinical phases to training samples, models learn task-specific reasoning patterns that transfer to real clinical settings. The two-stage inspiration-feedback survey with licensed physicians ensures tasks reflect actual workflow needs.
- Core assumption: The 22 tasks generalize across the 27 surveyed specialties and represent comprehensive coverage of doctors' daily work
- Evidence anchors:
  - [abstract]: "comprising 92,000 Q&A instances across 22 clinical tasks and 27 specialties"
  - [section]: "DoctorFLAN notably improves the performance of open-source LLMs in medical contexts, facilitating their alignment with physician workflows"
  - [corpus]: MMedAgent-RL paper supports multi-specialty collaboration for diverse medical reasoning
- Break condition: If task taxonomy fails to generalize to specialties or clinical settings not represented in the 71 valid survey responses

### Mechanism 2
- Claim: Reference-enhanced GPT-4 refinement with manual verification produces high-quality training data
- Mechanism: Raw medical data undergoes two-stage refinement: (1) instruction alignment by medical professionals, (2) GPT-4 response generation using original data as reference context. Manual verification against reference answers then filters for correctness (100%) and practicality (99.9%).
- Core assumption: GPT-4 reliably generates medically accurate content when grounded in authoritative references
- Evidence anchors:
  - [abstract]: "built DoctorFLAN using GPT-4-polished responses with manual verification"
  - [section]: "Our results demonstrate correctness (100%) and practicality (99.9%)"
  - [corpus]: MedCoT-RAG paper supports reference-augmented generation for reducing hallucinations in medical QA
- Break condition: If GPT-4 introduces subtle clinical errors that pass expert review, or if reference materials contain inaccuracies

### Mechanism 3
- Claim: Doctor-centric fine-tuning outperforms patient-centric medical LLMs on clinical workflow tasks
- Mechanism: Patient-facing models trained on online consultations learn simplified patient advice patterns. DoctorFLAN trains models to produce detailed, knowledge-intensive responses with clinical reasoning—required for Diagnosis and Treatment phases where models showed 11.6-29.8% improvement over base models.
- Core assumption: Doctor-assistance requires fundamentally different response characteristics (depth, technical reasoning) than patient consultation
- Evidence anchors:
  - [abstract]: "models fine-tuned on DoctorFLAN... outperform general and medical LLMs, particularly in diagnosis and treatment tasks"
  - [section]: "Virtual doctor models originally designed to provide medical advice to patients... perform relatively poorly in tasks related to doctor workflow assistance"
  - [corpus]: Weak corpus evidence—related papers focus on patient-facing or general medical applications, not doctor-assistant specialization
- Break condition: If the doctor-patient task distinction is less clear in practice, or if doctor-facing responses require contextual adaptation the training data doesn't capture

## Foundational Learning

- **Concept: Clinical Workflow Phase Structure**
  - Why needed here: Understanding Pre-diagnosis → Diagnosis → Treatment → Post-treatment progression is essential for task mapping and interpreting phase-specific performance drops
  - Quick check question: Which phase would "Differential Diagnosis" belong to, and why do models struggle more there than in Pre-diagnosis tasks?

- **Concept: Supervised Fine-Tuning with Mixed Corpora**
  - Why needed here: DotaGPT uses 92K DoctorFLAN + 101K general instructions + 51K medical QA to balance domain expertise with general capability retention
  - Quick check question: Why did DISC-MedLLM underperform its general-purpose backbone (Baichuan-13B-Chat) on DoctorFLAN tasks?

- **Concept: LLM-as-Judge Evaluation with Reference Grounding**
  - Why needed here: The paper uses GPT-4 for automatic evaluation with reference-based scoring (Accuracy, Coherence, Relevance, Thoroughness), achieving r=0.82 correlation with human evaluation
  - Quick check question: What biases might GPT-4 have when evaluating medical responses, and how does reference grounding mitigate them?

## Architecture Onboarding

- **Component map:**
  1. Data Sources: Medtiku (MCQs), 120Ask (encyclopedia), PromptCBLUE (existing datasets)
  2. Preprocessing: Jaccard deduplication (threshold=0.8) → Regex-based task classification (95%+ expert agreement)
  3. Refinement Pipeline: Manual instruction alignment → GPT-4 reference-enhanced polishing → Expert verification
  4. Training: Mixed-corpus SFT on Yi-6B / Baichuan2-7B backbones (4×A100, 3 epochs, lr=5e-5)
  5. Evaluation: DoctorFLAN-test (550 single-turn) + DotaBench (74 multi-turn) with GPT-4 scoring

- **Critical path:** Expert survey → Task taxonomy → Data collection → Regex task mapping → Reference-enhanced refinement → Manual verification → Mixed-corpus SFT → Benchmark evaluation

- **Design tradeoffs:**
  - GPT-4 refinement vs. pure expert annotation: Lower cost but potential for subtle errors
  - Single-turn (DoctorFLAN) vs. multi-turn (DotaBench): Broader task coverage vs. conversational realism
  - Mixed corpus vs. pure medical: Retains general capability but dilutes domain density

- **Failure signatures:**
  - DISC-MedLLM underperforms general Baichuan-13B-Chat → over-specialization on wrong task distribution
  - All models show Diagnosis/Treatment phase score drops → knowledge-intensive tasks need more training signal
  - Patient-facing models (BianQue, HuatuoGPT) fail on doctor-assistance tasks → training objective mismatch

- **First 3 experiments:**
  1. Reproduce baseline scores for Baichuan2-7B-Chat and Yi-6B-Chat on DoctorFLAN-test to validate evaluation pipeline
  2. Ablate mixed corpus: train Baichuan2-7B-Base on DoctorFLAN only (no general instructions) to isolate dataset contribution
  3. Cross-benchmark generalization test: evaluate DotaGPT on CMB-Exam and CMExam to measure specialization vs. breadth tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the task definitions and methodology of DoctorFLAN be effectively transferred to English or other languages to achieve comparable performance in doctor-centric assistance?
- Basis in paper: [explicit] The authors state in the Limitations section that "DoctorFLAN is currently only available in Chinese... consequently, it cannot be guaranteed that DotaGPT trained on DoctorFLAN will perform well in languages other than the one on which it has been tested."
- Why unresolved: The current study is restricted to Chinese medical data and LLMs, leaving the cross-lingual applicability of the proposed workflow-aligned tasks unverified.
- What evidence would resolve it: Construction of an English (or multi-lingual) DoctorFLAN dataset using the same heuristic-feedback methodology, followed by comparative benchmarking of models trained on this new dataset.

### Open Question 2
- Question: Does the use of DotaGPT in actual clinical workflows improve physician efficiency or diagnostic accuracy compared to standard care or general-purpose LLMs?
- Basis in paper: [inferred] The paper validates DotaGPT on static benchmarks (DoctorFLAN-test, DotaBench) and via human evaluation of text quality, but explicitly advises caution for "real-world doctor-oriented interactions."
- Why unresolved: Benchmark performance and subjective text quality ratings do not necessarily translate to improved clinical outcomes or time savings in a live hospital environment.
- What evidence would resolve it: A prospective study or clinical trial measuring physician turnaround time, diagnostic error rates, and user satisfaction when integrated into live hospital information systems.

### Open Question 3
- Question: What is the optimal ratio of general instruction data to domain-specific medical data required to fine-tune a doctor-centric assistant without causing catastrophic forgetting or excessive specialization?
- Basis in paper: [inferred] The authors note that DISC-MedLLM (fine-tuned purely on medical data) underperformed its general base model, while DotaGPT succeeded using a mixed corpus (DoctorFLAN + ShareGPT + CMExam), suggesting a critical but undefined balance is needed.
- Why unresolved: The paper demonstrates that a mixed approach works better than pure medical fine-tuning, but does not explore the sensitivity of model performance to the specific proportions of general versus medical data.
- What evidence would resolve it: A systematic ablation study varying the ratios of general-to-medical training data and measuring the impact on both medical benchmarks (DoctorFLAN-test) and general capability benchmarks.

### Open Question 4
- Question: How can doctor-centric LLMs be enhanced to include self-correction mechanisms and references to mitigate hallucinations, as explicitly requested by clinical experts?
- Basis in paper: [explicit] The survey results (Section 4.4) report that "39.4% [of doctors] express concerns about the LLM's inability to provide accurate references" and emphasize "incorporating self-correction mechanisms to improve... reliability."
- Why unresolved: While the paper identifies this as a critical user need, the proposed DotaGPT model focuses on response generation quality without integrating specific architectural features for citation verification or self-correction.
- What evidence would resolve it: Integration of Retrieval-Augmented Generation (RAG) or self-consistency checks into the DotaGPT architecture, evaluated on a new metric specifically measuring citation accuracy and hallucination rates.

## Limitations

- Expert survey sample (71 physicians) may not capture full diversity of Chinese medical specialties and workflows
- GPT-4 used for both data refinement and evaluation introduces potential circular validation bias
- Dataset and benchmarks currently limited to Chinese language, restricting cross-lingual applicability

## Confidence

- Dataset construction methodology: Medium
- Performance improvement claims: Medium
- Evaluation validity (GPT-4-as-judge): Low-Medium
- Task taxonomy generalizability: Medium-Low

## Next Checks

1. **Human Expert Validation Study**: Have 10-15 independent medical specialists evaluate 100 randomly sampled DoctorFLAN responses across all 22 tasks using the same four metrics (Accuracy, Coherence, Relevance, Thoroughness). Compare their scores against GPT-4 evaluations to quantify systematic bias and establish confidence intervals for the claimed improvements.

2. **Cross-Specialty Generalization Test**: Evaluate DotaGPT and baseline models on specialty-specific clinical scenarios not represented in the original 27 specialties (e.g., rare diseases, surgical specialties). This would test whether the 22-task framework truly generalizes beyond the surveyed specialties.

3. **Error Analysis of GPT-4-Polished Responses**: Systematically analyze 500 GPT-4-refined responses where the reference answer and GPT-4 output differ. Have clinical experts categorize error types (clinical inaccuracies, missing context, inappropriate recommendations) to quantify the risk of subtle errors introduced during the automated refinement process.