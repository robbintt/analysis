---
ver: rpa2
title: 'TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models
  through Reverse Route'
arxiv_id: '2509.18173'
source_url: https://arxiv.org/abs/2509.18173
tags:
- route
- llms
- geospatial
- cognition
- routes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TurnBack, a benchmark for evaluating large
  language models' geospatial route cognition through reverse route tasks. The authors
  created a dataset of 36,000 routes across 12 global cities and developed PathBuilder,
  a tool converting natural language instructions to geometric paths and vice versa.
---

# TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route

## Quick Facts
- arXiv ID: 2509.18173
- Source URL: https://arxiv.org/abs/2509.18173
- Authors: Hongyi Luo; Qing Cheng; Daniel Matos; Hari Krishna Gadi; Yanfeng Zhang; Lu Liu; Yongliang Wang; Niclas Zeller; Daniel Cremers; Liqiu Meng
- Reference count: 40
- Primary result: LLMs achieve <15% return rates on reverse route tasks despite high confidence scores

## Executive Summary
This paper introduces TurnBack, a benchmark for evaluating large language models' geospatial route cognition through reverse route tasks. The authors created a dataset of 36,000 routes across 12 global cities and developed PathBuilder, a tool converting natural language instructions to geometric paths and vice versa. Testing 11 state-of-the-art LLMs revealed significant limitations: return rates remained below 15% even for easy routes, similarity scores were generally low, and robustness across multiple trials was poor despite high confidence scores. The study identifies two key disorders in geospatial cognition - disorientation and superficiality - suggesting LLMs rely on semantic inversions rather than true spatial reasoning.

## Method Summary
The TurnBack benchmark evaluates LLMs on geospatial route cognition by presenting them with forward navigation instructions and requiring reverse route generation. The dataset contains 36,000 pedestrian routes (500-2,500m) across 12 cities sourced from OpenStreetMap and OpenRouteService. The evaluation uses PathBuilder to convert LLM-generated natural language instructions into geometric paths, then computes return rates and similarity metrics (Hausdorff distance, Fréchet distance, Jaccard index). Models are tested across difficulty levels (Easy/Medium/Hard) and evaluated for robustness across multiple trials. The benchmark isolates route cognition from visual perception by using language-only inputs.

## Key Results
- Return rates below 15% even for easy routes across all tested LLMs
- High confidence scores (86-96%) despite low similarity scores (24-67%)
- Two identified disorders: disorientation (32-85% routes ending far from start) and superficiality (semantic inversions without spatial reasoning)
- Vector map assistance improves return rates from 6.4% to 43.7%, but the mechanism remains unclear

## Why This Works (Mechanism)

### Mechanism 1: Semantic Inversion Heuristic (Superficiality Disorder)
LLMs default to pattern-based directional word inversion rather than genuine spatial transformations, exploiting statistical regularities in training data by swapping "left"↔"right," "north"↔"south," and reversing step order without maintaining coherent internal spatial representation.

### Mechanism 2: Tokenization Fragments Metric Continuity
Sub-word tokenizers destroy numerical relationships required for coordinate-based spatial reasoning by dividing coordinates like "52.5167" into orthogonal sub-word units, breaking the principle of metric continuity where small numerical changes should produce small embedding-space changes.

### Mechanism 3: Cross-Entropy Objective Ignores Metric Proximity
Next-token prediction loss provides no gradient signal proportional to geometric error magnitude, where predicting 52.5168° vs. 90.00° incurs identical cross-entropy loss despite one error being millimeters and the other thousands of kilometers.

## Foundational Learning

- **Geospatial Cognition Hierarchy (Werner et al., 1997)**: The benchmark positions itself at the Route level (between Landmark and Survey knowledge). Quick check: Can you explain why "What's near the Eiffel Tower?" tests different knowledge than "Return to your starting point following the reverse path?"

- **Route Reversal as Spatial Cognition Probe**: This task requires two sub-capabilities: (1) spatially locating the endpoint relative to start, and (2) navigating back. Quick check: Why is route reversal cognitively harder than forward route following but easier than full survey-level planning?

- **Hausdorff/Fréchet Distance for Path Similarity**: The benchmark uses geometric similarity metrics (not just success/failure) to quantify how wrong routes are. Quick check: If two paths share start/end points but one loops around a block, which metric would better capture the difference?

## Architecture Onboarding

- **Component map**: OpenStreetMap → Route Generation → Navigation Instructions (NL) → LLM (Route Reversal) → PathBuilder ← Reversed Instructions (NL) ← PathBuilder ← Geometric Path → Similarity Metrics → Benchmark Scores

- **Critical path**: PathBuilder is the bottleneck—converting LLM natural language output back to geometric paths. Appendix E documents failure modes: semantic ambiguity ("slightly" = 11-44°), roundabouts, and node/street mismatches. First debugging step: validate PathBuilder accuracy on ground-truth forward routes before trusting reversal evaluation.

- **Design tradeoffs**: Language-only vs. vision-assisted: Paper shows vector maps boost return rate from 6.4% → 43.7% (Table 7), but this conflates visual perception with route reasoning. Benchmark intentionally uses language-first to isolate cognition. Temperature=0 vs. sampling: Ablation (Table 11) shows randomness persists even at T=0, suggesting architectural—not sampling—issues.

- **Failure signatures**: Disorientation (routes end far from start; 32-85% disorientation rates, Table 5), Superficiality (high confidence on semantically inverted commands vs. lower confidence on actual reasoning, Table 6), Misalignment (4-16% outputs lack initial absolute direction), Inconsistency (same prompt yields divergent paths; robustness 13-69).

- **First 3 experiments**: 1) Baseline sanity check: Run PathBuilder on forward-route instructions to establish ceiling accuracy (Table 10 shows 90-96% across cities). 2) Difficulty calibration: Test GPT-4o on 50 easy routes from a single city with known ground truth. 3) Confidence vs. accuracy correlation: Extract token probabilities for direction words. Plot confidence against similarity score to confirm high confidence ≠ geometric correctness.

## Open Questions the Paper Calls Out

### Open Question 1
Can numeric-aware tokenizers or geospatial cell vocabularies effectively resolve the metric continuity bottleneck inherent in standard sub-word tokenizers? The paper identifies that current tokenizers fragment coordinates but does not experimentally validate proposed architectural solutions. Evidence needed: Comparative benchmarks of models with numeric-aware tokenizers on the TurnBack dataset.

### Open Question 2
Do metric-aware training objectives significantly enhance geometric fidelity in route generation compared to standard next-token cross-entropy loss? While the paper diagnoses the objective misalignment, it does not test alternative loss functions. Evidence needed: Fine-tuning existing LLMs using custom loss functions incorporating Hausdorff or Fréchet distances.

### Open Question 3
Does the performance improvement observed with vector map assistance stem from genuine spatial abstraction or merely from visual anchoring? The paper demonstrates improvement but the specific cognitive mechanism remains undetermined. Evidence needed: Ablation studies using abstract topological graphs versus photorealistic imagery.

## Limitations

- Benchmark may systematically difficult for any model, not just LLMs
- PathBuilder tool could conflate parsing difficulty with model cognition failures
- Similarity metrics may not capture meaningful differences in route quality
- Results may not generalize beyond pedestrian routes in major global cities

## Confidence

**High confidence**: Empirical findings of low return rates (<15%) and high confidence despite geometric errors are reproducible and well-documented.

**Medium confidence**: Architectural diagnosis (tokenization fragments continuity, cross-entropy ignores metric proximity) provides coherent explanation but alternative hypotheses exist.

**Low confidence**: Assertion that specialized numeric-aware tokenizers and geodesic operators would enable "robust" performance is speculative.

## Next Checks

1. **PathBuilder accuracy validation**: Test PathBuilder on ground-truth forward route instructions across all cities to establish baseline parsing accuracy. If PathBuilder fails to correctly parse valid instructions >10% of the time, the benchmark conflates parsing errors with model cognition failures.

2. **Architecture ablation study**: Compare LLM performance against a specialized spatial reasoning model (e.g., graph neural network) on identical route reversal tasks. If specialized architectures also fail, the limitation is task-specific rather than LLM-specific.

3. **Task simplification gradient**: Create a controlled progression from landmark queries through simple forward navigation to full route reversal. Map exactly where performance collapses to identify whether the failure is in spatial reasoning, sequential composition, or geometric precision.