---
ver: rpa2
title: 'GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination
  Mitigation'
arxiv_id: '2502.05911'
source_url: https://arxiv.org/abs/2502.05911
tags:
- arxiv
- samples
- didk
- question
- rait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAIT introduces a gradient-driven approach to Refusal-Aware Instruction
  Tuning that improves LLM hallucination mitigation. The method uses gradient influence
  to select training samples that reduce hallucinations and applies adaptive weighting
  to balance refusal and accuracy.
---

# GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation

## Quick Facts
- arXiv ID: 2502.05911
- Source URL: https://arxiv.org/abs/2502.05911
- Reference count: 14
- Key outcome: GRAIT achieves hallucination reduction (Pw) of 6.9-27.1% and THS scores of 36.4-36.6 on MMLU and TriviaQA benchmarks, significantly outperforming baselines like CRaFT and R-Tuning.

## Executive Summary
GRAIT introduces a gradient-driven approach to Refusal-Aware Instruction Tuning (RAIT) that improves LLM hallucination mitigation by strategically selecting training samples based on their gradient influence and applying adaptive weighting to balance refusal and accuracy. The method partitions training data by model correctness, selects unknown-knowledge samples whose gradients align with the average gradient direction of the unknown set, and weights samples by their stable influence to reduce over-refusal. On comprehensive benchmarks including MMLU, TriviaQA, ARC-c, and NQ, GRAIT achieves significant improvements in THS (Truthful Helpfulness Score) while maintaining accuracy and reducing hallucination rates compared to state-of-the-art RAIT methods.

## Method Summary
GRAIT is a three-stage framework that improves RAIT by: (1) partitioning training data into known-knowledge (ik) and unknown-knowledge (idk) sets based on model correctness, with idk samples modified to refusal responses; (2) selecting idk samples via gradient influence ranking, where samples with gradients most similar to the average idk gradient receive higher scores; and (3) applying stable influence weighting during fine-tuning, where samples are weighted by the difference between their refusal influence and over-refusal influence to balance hallucination reduction with accuracy preservation. The framework uses LoRA for efficient gradient computation and employs a 1:4 ratio of ik to idk samples with temperature τ=0.05 for softmax weighting.

## Key Results
- Achieves THS improvements of 1.1-7.2% over baselines like CRaFT and R-Tuning across multiple benchmarks
- Reduces hallucination rates (Pw) by 6.9-27.1% while maintaining competitive accuracy (Pc) scores
- Demonstrates consistent performance gains on both in-domain (MMLU, TriviaQA) and out-of-domain (ARC-c, NQ) evaluations
- Shows robustness to parameter choices with THS scores remaining stable across Tc thresholds 0.3-0.7

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Aligned Refusal Sample Selection
- **Claim**: Selecting idk samples whose gradients align with the average gradient direction of the unknown-knowledge set maximally reduces hallucination
- **Mechanism**: From first-order Taylor expansion, minimizing validation loss on unknown samples corresponds to maximizing influence I(xo_idk, xu_idk). Samples with gradients similar to Didk mean gradient have higher Refusal Influence scores and contribute more to learning the refusal paradigm
- **Core assumption**: Gradient directions encode knowledge state information, and similarity to set-mean gradients predicts transfer to unseen unknown samples
- **Evidence anchors**: [abstract] "employs gradient-driven sample selection to effectively minimize hallucinations"; [Section 4.1] "samples with gradients similar to the average gradient direction of Didk are the most effective in reducing the model's hallucination rate"
- **Break condition**: If gradient distributions shift during training (distribution drift), static selection becomes suboptimal

### Mechanism 2: Stable Influence Weighting Against Over-Refusal
- **Claim**: Weighting training samples by the difference between refusal influence and over-refusal influence mitigates accuracy degradation from excessive conservatism
- **Mechanism**: Over-refusal correlates with samples having similar influence on both Didk and Dik validation sets. By computing Ista(x) = ⟨∇L(x), [∇L̄_idk - ∇L̄_ik_yidk]⟩ and converting to softmax weights, samples with larger stable influence receive higher weights, preserving correct-answer behavior while learning refusal
- **Core assumption**: The gradient means of ik and idk sets are approximately orthogonal (verified empirically at 0.008 vs. self-products of 0.103 and 0.513)
- **Evidence anchors**: [abstract] "introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal"; [Section 5.3] Equation 9-11 defining Ista and weight computation; Figure 5 shows Pearson correlation of 0.886 between Iref and over-refusal influence
- **Break condition**: If τ temperature is set too high (→1), weights become uniform and the mechanism nullifies

### Mechanism 3: Correctness-Based Dataset Partitioning
- **Claim**: Partitioning training data by model correctness (with threshold Tc=0.5) creates distinct knowledge-state sets amenable to targeted training
- **Mechanism**: MCQA uses ground-truth token probability; OEQA uses N=10 generation samples with majority voting. Samples above Tc form Dik (kept unchanged); below Tc form Didk (answers replaced with refusal responses)
- **Core assumption**: Model correctness on a sample indicates genuine knowledge vs. knowledge gap, rather than surface pattern matching or noise
- **Evidence anchors**: [Section 5.1] Detailed correctness computation for MCQA and OEQA; [Table 3] ik-top selection outperforms ik-random and ik-bottom, supporting that correctness signals meaningful knowledge separation
- **Break condition**: If Tc is poorly calibrated, the ik/idk split mislabels samples

## Foundational Learning

- **Concept: Influence Functions for Training Data Selection**
  - Why needed here: GRAIT uses influence to select samples whose gradients maximize desired loss reduction on validation sets
  - Quick check question: Can you explain why ⟨∇L_train, ∇L_val⟩ approximates the change in validation loss from training on that sample?

- **Concept: Gradient-Based Knowledge State Probing**
  - Why needed here: The paper assumes gradients encode internal knowledge states; Appendix A.3 empirically validates orthogonality between ik and idk gradient distributions
  - Quick check question: Why might gradient directions (not magnitudes) be more informative for knowledge-state inference?

- **Concept: Over-Refusal Trade-off in Safety Training**
  - Why needed here: RAIT methods inherently face a precision-recall trade-off between hallucination reduction and helpfulness preservation
  - Quick check question: How does THS (Truthful Helpfulness Score) formalize the trade-off between Pc (accuracy) and Pw (error rate)?

## Architecture Onboarding

- **Component map**: Correctness Evaluator → Dataset Splitter → Gradient Computer → Refusal Influence Ranker → Stable Influence Weighter → Weighted SFT Trainer

- **Critical path**: Correctness computation → ik/idk split → gradient extraction for Didk → Iref ranking → Ista computation → weighted fine-tuning. Errors in correctness computation propagate to all downstream stages.

- **Design tradeoffs**:
  - Sample ratio (ik:idk = 1:4): Paper uses fixed ratio; altering this changes refusal-vs-accuracy balance
  - τ temperature: Lower τ increases weight differentiation; Table 10 shows τ=0.05 works best
  - Tc threshold: Table 11 shows insensitivity in 0.3-0.7 range; default 0.5 is reasonable
  - Gradient dimensionality reduction: LoRA + random projection enables tractable computation; may lose gradient information

- **Failure signatures**:
  - High Pr, low Pc: Over-refusal—Ista weighting not working; check τ and weight normalization
  - High Pw, low Pr: Under-refusal—Refusal Influence selection not filtering ineffective samples; verify Iref computation
  - Poor OOD generalization: ik/idk split may not transfer; verify train/test distribution assumptions (Assumption 1)

- **First 3 experiments**:
  1. Ablation on ik selection strategy: Replicate Table 3 (ik-top vs. ik-random vs. ik-bottom) on your target dataset to validate correctness-based separation
  2. Temperature sensitivity sweep: Run τ ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 1.0} (Table 10) to find optimal weighting for your model
  3. Gradient orthogonality check: Compute ⟨E[∇L_idk], E[∇L_ik]⟩ on your data (Appendix A.3) to validate the orthogonality assumption before full pipeline deployment

## Open Questions the Paper Calls Out

- **Open Question 1**: How would incorporating dynamic gradient trajectory influence throughout the RAIT process improve model refusal behavior compared to GRAIT's static treatment?
  - Basis in paper: [explicit] Authors state: "the GRAIT framework currently treats the training process as static, rather than incorporating the dynamic influence of gradient trajectories throughout the RAIT process"
  - Why unresolved: GRAIT computes influence at a single point; temporal dynamics of how sample influence changes across training steps remain unexplored
  - What evidence would resolve it: A study comparing static vs. trajectory-aware influence selection showing performance differences on hallucination and over-refusal metrics

- **Open Question 2**: Can GRAIT's gradient-driven approach be leveraged for more nuanced identification of LLM knowledge boundaries beyond the current binary ik/idk split?
  - Basis in paper: [explicit] "the idk and ik sets are divided through a straightforward query of the LLMs; future work could explore ways to leverage GRAIT for more nuanced identification of knowledge boundaries within LLMs for splitting"
  - Why unresolved: Current split relies on correctness threshold; fine-grained confidence levels or partial knowledge states are not captured
  - What evidence would resolve it: Experiments with multi-level refusal granularity (e.g., "uncertain," "partially confident") demonstrating improved calibration

- **Open Question 3**: Why does over-refusal persist despite GRAIT's adaptive weighting, and can the strong correlation (Pearson 0.886) between refusal influence and over-refusal influence be broken?
  - Basis in paper: [explicit] "Over-Refusal can only be alleviated, but not completely eliminated... we identified a strong correlation between the two, with a Pearson Correlation Coefficient of 0.886"
  - Why unresolved: The fundamental coupling between learning to refuse and over-refusing suggests shared gradient directions that current weighting cannot fully decouple
  - What evidence would resolve it: Analysis of whether orthogonalization techniques or adversarial training can reduce this correlation while preserving refusal learning

## Limitations
- The gradient-based sample selection mechanism relies on the assumption that gradient directions encode knowledge state information, but this relationship has not been rigorously validated beyond empirical observation
- The framework treats training as static despite acknowledging gradient distribution drift, which could degrade performance in longer training runs
- The orthogonal assumption between ik and idk gradient distributions may not generalize to domains with different knowledge distributions

## Confidence
- **High confidence** in the experimental methodology and benchmark results, which show consistent improvements across multiple models and datasets
- **Medium confidence** in the theoretical foundations, particularly the influence function derivations and their connection to knowledge state inference
- **Low confidence** in the general applicability of the gradient orthogonality assumption across diverse domains and model architectures

## Next Checks
1. Replicate the orthogonality test (⟨E[∇L_idk], E[∇L_ik]⟩) on a different dataset domain to verify the fundamental assumption holds beyond MMLU and TriviaQA
2. Conduct a controlled experiment varying the ik:idk ratio (beyond the fixed 1:4) to determine optimal balance for different model sizes and task types
3. Implement a dynamic weighting mechanism that updates Ista weights during training to address the static training limitation and test whether this improves long-horizon performance