---
ver: rpa2
title: 'SoK: Machine Unlearning for Large Language Models'
arxiv_id: '2506.09227'
source_url: https://arxiv.org/abs/2506.09227
tags:
- unlearning
- arxiv
- data
- forgetting
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This SoK paper presents a new taxonomy for large language model
  unlearning methods based on their underlying intentions rather than technical characteristics.
  It distinguishes between removal-intended methods that aim to truly eliminate internal
  knowledge and suppression-intended methods that only control behavioral outputs.
---

# SoK: Machine Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2506.09227
- Source URL: https://arxiv.org/abs/2506.09227
- Reference count: 40
- One-line primary result: Gradient ascent-based unlearning methods may not truly remove knowledge but instead suppress outputs by creating forgetting detection signals.

## Executive Summary
This SoK paper introduces a new taxonomy for large language model unlearning methods based on their underlying intentions rather than technical characteristics. It distinguishes between removal-intended methods that aim to truly eliminate internal knowledge and suppression-intended methods that only control behavioral outputs. The paper reveals that popular gradient ascent-based approaches may not truly remove knowledge but instead treat forgetting data as a signal to suppress outputs. It surveys evaluation metrics and benchmarks, identifies limitations in current practices, and discusses practical challenges including sequential unlearning, scalability, and real-world deployment.

## Method Summary
The paper provides a comprehensive survey and taxonomy of LLM unlearning methods, categorizing them into removal-intended (modify model parameters to eliminate knowledge) and suppression-intended (intervene at inference to control outputs). Key methods analyzed include Gradient Ascent (LGA = -Ltrain), NPO with bounded divergence, RMU (representation re-steering), Task Arithmetic, rejection fine-tuning, and RAG-based suppression. The evaluation framework considers both unlearning effectiveness (MIA attacks, adversarial robustness) and model utility (retain set performance, general benchmarks). The paper uses synthetic benchmarks like TOFU and real-world datasets like MUSE and WMDP to assess these methods.

## Key Results
- Gradient ascent-based methods appear to remove knowledge but actually create suppression signals that detect forgetting data and trigger unlearning behavior
- Representation re-steering methods (RMU) redirect forgetting data to refusal/degraded regions rather than deleting stored information
- Token-level selectivity can reduce utility degradation by targeting only highly memorized samples or low-probability tokens
- Current evaluation practices have significant limitations, particularly in distinguishing true removal from behavioral suppression

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent Creates Suppression Signals, Not True Removal
Fine-tuning with reversed loss causes the model to distinguish forgetting data in representation space rather than erasing it. When forgetting data appears in prompts, the model increases output error rather than recalling erased information. This works because knowledge is distributed across parameters in ways that cannot be cleanly inverted by reversing the training signal.

### Mechanism 2: Representation Re-steering Maps Forgetting Data to Refusal/Degraded Regions
Methods like RMU suppress knowledge by redirecting hidden representations toward random vectors or refusal regions rather than deleting stored information. Fine-tuning specific transformer layers to minimize ||hθ(l)(xf) − cu||² for forgetting data while preserving ||hθ(l)(xr) − href(l)(xr)||² for retaining data causes the model to output incoherent or refusal responses when representations fall in these degraded regions.

### Mechanism 3: Token-Level Selectivity Reduces Utility Degradation
Applying unlearning uniformly to all tokens harms model utility because many tokens in forgetting sequences are generic (e.g., "The author of") and unrelated to target knowledge. Selective forgetting methods identify high-value tokens (high memorization scores, low probability tokens, or semantically relevant tokens) and apply GA only to these, reducing collateral damage to syntactic patterns needed for general language use.

## Foundational Learning

- **Gradient Ascent vs Descent in Optimization**: Understanding why reversing the training loss (maximizing instead of minimizing) produces qualitatively different effects than simply undoing training. Quick check: Why does maximizing the negative log-likelihood not simply "subtract" the learned knowledge?

- **KL Divergence Between Probability Distributions**: Many retaining losses and fine-grained probability methods use KL divergence to constrain how far the unlearned model deviates from the original. Quick check: What does minimizing KL(pθ || pref) preserve about the original model's behavior?

- **Representation Geometry in Transformer Hidden States**: Re-steering methods manipulate where forgetting data falls in the high-dimensional representation space; understanding cluster structure is essential. Quick check: If forgetting data representations are mapped to random vectors, what prevents retain data from accidentally being pulled toward those regions?

## Architecture Onboarding

- **Component map**: Input (x) → Embedding → [Transformer Layers with Hidden Representations h(l)] → Output Logits → Token Probabilities p(y|x). Removal-intended: Modifies parameters θ via GA, NPO, Task Arithmetic. Suppression-intended: Intervenes at input (ICL, RAG, agents), representation (RMU, SAE), or output (logits difference, retrieval blocking).

- **Critical path**: 1) Define forgetting set Df and retaining set Dr, 2) Choose intention: removal (modify θ) vs suppression (intervene at inference), 3) Apply method-specific loss/objective, 4) Evaluate unlearning effectiveness (MIA, adversarial robustness) AND utility (retain set performance, general benchmarks).

- **Design tradeoffs**: GA-based removal: Strong behavioral change but utility degradation and adversarial vulnerability. Representation re-steering: Controllable but may not survive subsequent fine-tuning or quantization. Input-space (RAG/ICL): Preserves utility perfectly but requires runtime infrastructure and doesn't address weight-level persistence. Sequential unlearning: RAG-based scales naturally; parameter-modifying methods accumulate utility loss (e.g., 3% × 20 iterations ≈ 45% cumulative degradation).

- **Failure signatures**: Adversarial prompts (GCG, soft tokens) recover forgotten content → suppression is superficial. Fine-tuning on benign data restores forgotten knowledge → representations persisted. Retain set performance drops → excessive gradient conflict or over-generalization. Quantization post-unlearning fails → unlearning relied on precise weight configurations.

- **First 3 experiments**: 1) Baseline comparison on TOFU: Implement vanilla GA vs NPO vs RMU on a 7B model fine-tuned on synthetic author data; measure ROUGE recall on forget set, probability of ground truth, and retain set accuracy. 2) Adversarial robustness test: Apply GCG attack with 50 iterations to recover forgotten knowledge; report recovery rate and compare against soft token attack false positive rates. 3) Sequential unlearning simulation: Submit 10 unlearning requests in sequence using a parameter-modifying method; track cumulative utility loss on a held-out benchmark and compare against RAG-based approach.

## Open Questions the Paper Calls Out

### Open Question 1
Can removal-intended unlearning methods truly eliminate the influence of forgetting data from model parameters, or do they merely achieve behavioral suppression? No theoretical framework currently guarantees such removal in LLMs, nor are there conclusive empirical methods to verify it. What evidence would resolve it: Theoretical tools to trace parameter regions influenced by specific data, new interpretability or causal attribution methods to isolate and invert parameter changes without utility degradation, and benchmarks that test under adversarial or compositional prompts.

### Open Question 2
In real-world unlearning deployments, what exactly should users provide in their unlearning requests—abstract concepts or concrete corpora? There is still no practical and reliable answer to this question. What evidence would resolve it: Development of commercial unlearning services with documented interfaces, user studies on request formulation, and standardized formats for unlearning requests that balance specificity with usability.

### Open Question 3
How can sequential unlearning be performed without catastrophic utility degradation as unlearning requests accumulate over time? Even if each step results in just a 3% drop in utility, after 20 iterations, the model could lose up to 45% of its utility. What evidence would resolve it: Methods that maintain bounded utility loss across many sequential unlearning operations, empirical studies showing stable performance after hundreds of sequential requests, or alternative architectures (like RAG) that naturally support incremental updates.

## Limitations

- The central claim that gradient ascent-based methods create suppression signals rather than true knowledge removal remains theoretically plausible but empirically contested
- The distinction between removal-intended and suppression-intended methods may oversimplify the reality that all unlearning methods likely operate on a spectrum of removal vs. suppression
- The assertion that token-level selectivity meaningfully improves utility degradation is weakly supported by corpus evidence

## Confidence

- **High Confidence**: The taxonomy framework based on intention (removal vs. suppression) is well-grounded and provides useful structure for categorizing existing methods. The survey of evaluation metrics and benchmarks is comprehensive and accurately represents the current state of the field.

- **Medium Confidence**: The characterization of gradient ascent as creating "signals" rather than removing knowledge is supported by attack results but relies on indirect inference. The representation re-steering mechanism is technically sound but the claim that this represents suppression rather than removal is debatable—redirected representations could be considered a form of removal.

- **Low Confidence**: The assertion that token-level selectivity meaningfully improves utility degradation is weakly supported by the corpus evidence. The sequential unlearning degradation estimates (3% × 20 iterations) appear speculative without empirical validation across multiple method types.

## Next Checks

1. **Representation Space Analysis**: Conduct probing experiments to determine whether forgetting data representations in GA-unlearned models cluster separately from retain/unseen data. If forgetting representations merge with retain clusters rather than forming distinct suppression regions, this would support the signal hypothesis over the removal hypothesis.

2. **Cross-Method Recovery Comparison**: Systematically compare adversarial recovery rates across removal-intended methods (GA, NPO, Task Arithmetic) versus suppression-intended methods (RMU, RAG-based). If recovery rates are similar across all methods, this would suggest the removal/suppression distinction is less meaningful than claimed.

3. **Sequential Unlearning Benchmark**: Implement and measure actual cumulative utility degradation for parameter-modifying methods across 10-20 sequential unlearning iterations, comparing against RAG-based approaches. This would validate or refute the claimed exponential utility degradation pattern.