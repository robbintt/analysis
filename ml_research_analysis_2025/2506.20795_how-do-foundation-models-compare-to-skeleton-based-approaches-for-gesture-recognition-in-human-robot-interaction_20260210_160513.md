---
ver: rpa2
title: How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition
  in Human-Robot Interaction?
arxiv_id: '2506.20795'
source_url: https://arxiv.org/abs/2506.20795
tags:
- gesture
- recognition
- gemini
- gestures
- hd-gcn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares foundation models (FMs) to traditional skeleton-based
  approaches for dynamic gesture recognition in human-robot interaction. We introduce
  NUGGET, a novel dataset of 11,569 sequences featuring intuitive upper-body gestures
  designed for mobile robotics.
---

# How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?

## Quick Facts
- arXiv ID: 2506.20795
- Source URL: https://arxiv.org/abs/2506.20795
- Reference count: 40
- Primary result: HD-GCN achieves 94.4% accuracy while zero-shot VLMs perform at 42.1% accuracy

## Executive Summary
This study evaluates foundation models against traditional skeleton-based approaches for dynamic gesture recognition in human-robot interaction. The authors introduce NUGGET, a novel dataset of 11,569 sequences featuring intuitive upper-body gestures designed for mobile robotics. Three methods are compared: HD-GCN (skeleton-based graph convolutional network), V-JEPA (vision foundation model with classification head), and Gemini Flash 2.0 (multimodal VLM used zero-shot). HD-GCN achieves the highest performance with 94.4% accuracy, while V-JEPA demonstrates strong generalization at 90.1% accuracy using attentive probing. In contrast, Gemini Flash 2.0 performs significantly worse at 42.1% top-1 accuracy, highlighting limitations of zero-shot VLMs for gesture recognition without fine-tuning.

## Method Summary
The study introduces NUGGET, a novel dataset of 11,569 gesture sequences designed specifically for mobile robotics applications. Three distinct approaches are evaluated: HD-GCN, a skeleton-based graph convolutional network that processes joint positions and motion features; V-JEPA, a vision foundation model enhanced with a classification head through attentive probing; and Gemini Flash 2.0, a multimodal VLM evaluated in zero-shot settings without fine-tuning. The evaluation focuses on dynamic gesture recognition performance metrics including accuracy and F1-score across the same dataset and experimental conditions.

## Key Results
- HD-GCN achieves highest performance at 94.4% accuracy and 93.0% F1-score
- V-JEPA demonstrates strong generalization with 90.1% accuracy using attentive probing
- Gemini Flash 2.0 performs significantly worse at 42.1% top-1 accuracy in zero-shot settings

## Why This Works (Mechanism)
The superior performance of skeleton-based approaches like HD-GCN stems from their specialized architecture for processing skeletal joint relationships and motion dynamics. These models directly encode the structural and temporal dependencies inherent in human gestures through graph convolutional networks. Vision foundation models like V-JEPA show promise through their ability to generalize from pre-training on diverse visual data, but require task-specific adaptation (attentive probing) to achieve competitive performance. Zero-shot VLMs like Gemini Flash 2.0 struggle because they lack gesture-specific representations and haven't been fine-tuned on relevant data.

## Foundational Learning
- Skeleton-based gesture representation: Needed to capture joint relationships and motion dynamics; Quick check: Verify HD-GCN's graph structure properly models human anatomy
- Vision foundation model adaptation: Required for leveraging pre-trained visual knowledge; Quick check: Confirm attentive probing effectively bridges foundation model to gesture task
- Zero-shot VLM limitations: Critical for understanding when fine-tuning is necessary; Quick check: Evaluate if additional prompts or few-shot examples improve Gemini Flash 2.0 performance

## Architecture Onboarding

Component map: Input video -> Skeleton extraction -> HD-GCN processing -> Gesture classification OR Input video -> V-JEPA feature extraction -> Attentive probing -> Classification head OR Input video -> Gemini Flash 2.0 multimodal processing -> Zero-shot classification

Critical path: For HD-GCN, skeleton extraction accuracy directly impacts performance; for V-JEPA, the quality of attentive probing determines classification accuracy; for Gemini Flash 2.0, prompt engineering and context window management are critical.

Design tradeoffs: HD-GCN offers highest accuracy but requires skeleton extraction; V-JEPA balances accuracy with generalization potential but needs adaptation; Gemini Flash 2.0 provides flexibility but requires fine-tuning for practical use.

Failure signatures: HD-GCN fails with occluded joints or unusual body proportions; V-JEPA struggles with gestures outside its pre-training distribution; Gemini Flash 2.0 fails when gestures don't match its training data or prompt interpretation.

First experiments: 1) Test skeleton extraction robustness across different body types, 2) Evaluate V-JEPA performance with varying attentive probing depths, 3) Test Gemini Flash 2.0 with few-shot fine-tuning examples

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The study only evaluates three specific approaches, potentially missing other viable foundation model architectures
- Results are based on a single novel dataset (NUGGET), which may not generalize to all gesture recognition scenarios
- Zero-shot VLM performance may improve with different prompting strategies not explored in this study

## Confidence
- HD-GCN performance claim: High confidence (measured with clear metrics on controlled dataset)
- V-JEPA generalization claim: Medium confidence (requires attentive probing adaptation)
- Gemini Flash 2.0 limitations: High confidence (consistent poor performance across evaluations)

## Next Checks
- Test HD-GCN with occluded or noisy skeleton data to assess robustness limits
- Evaluate V-JEPA with different attentive probing depths and attention mechanisms
- Experiment with few-shot fine-tuning approaches for Gemini Flash 2.0 to establish performance ceiling