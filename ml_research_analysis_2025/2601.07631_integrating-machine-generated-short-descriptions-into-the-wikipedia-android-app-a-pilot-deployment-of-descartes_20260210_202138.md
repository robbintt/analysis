---
ver: rpa2
title: 'Integrating Machine-Generated Short Descriptions into the Wikipedia Android
  App: A Pilot Deployment of Descartes'
arxiv_id: '2601.07631'
source_url: https://arxiv.org/abs/2601.07631
tags:
- descriptions
- editors
- machine-generated
- short
- descartes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Descartes, a multilingual language model, was integrated into the
  Wikipedia Android app to assist editors in generating short article descriptions.
  The pilot involved 375 editors across 12 languages, with over 3,900 articles and
  2,125 machine-generated descriptions accepted.
---

# Integrating Machine-Generated Short Descriptions into the Wikipedia Android App: A Pilot Deployment of Descartes

## Quick Facts
- arXiv ID: 2601.07631
- Source URL: https://arxiv.org/abs/2601.07631
- Reference count: 9
- Over 3,900 articles and 2,125 machine-generated descriptions accepted in 12-language pilot

## Executive Summary
Descartes, a multilingual language model, was integrated into the Wikipedia Android app to assist editors in generating short article descriptions. The pilot involved 375 editors across 12 languages, with community graders rating accepted descriptions at 90% quality score of 3 or higher (out of 5), comparable to human-written descriptions. Editors adopted suggestions both directly and with modifications, while revert and report rates remained low. The experiment showed slightly higher user retention in the treatment group. Based on these results, several improvements were identified for production deployment, including restricting biographies of living people, using only the first beam, filtering disambiguation pages, and correcting capitalization errors.

## Method Summary
The pilot deployed Descartes[notype] to generate short descriptions for Wikipedia articles across 12 languages. Editors with at least 3 prior edits could access suggestions, while those with 50+ edits could work on biographies of living people. The system used beam search with 2 beams to generate candidates, presented to editors for review and potential modification. Community graders evaluated accepted descriptions on a 1-5 quality scale. The model was served via Cloud VPS (CPU-only, up to 10s latency) during the pilot and later moved to LiftWing.

## Key Results
- 90% of accepted machine-generated descriptions received quality scores of 3 or higher
- Average quality score of 4.1 for machine-generated vs 4.2 for human-written descriptions
- Slightly higher user retention in treatment group (24.1% vs 22.6% at 7 days)
- Low revert rate of 2.3% and low report rate overall

## Why This Works (Mechanism)

### Mechanism 1
Presenting AI-generated suggestions as editable drafts—rather than auto-publishing—preserves quality while reducing editor effort. The UI surfaces up to two beam-search candidates; editors accept as-is, modify, or reject. This human-in-the-loop pattern filters errors before publication. Accepted machine descriptions (unmodified) averaged grade 4.2; modified ones averaged 4.1—both comparable to human-written 4.2.

### Mechanism 2
First-beam candidates are higher quality than second-beam candidates in this task. Beam search keeps top-k partial sequences; the first beam represents the globally highest-scoring output under the model. In the experiment, beam-1 descriptions achieved avg grade 4.2 vs 4.0 for beam-2, and were selected 64.7% of the time despite random ordering.

### Mechanism 3
Suggestive AI tools can modestly improve editor retention compared to unassisted editing. Lower-friction contributions (pre-generated suggestions) may reduce dropout for newer editors. The treatment group showed slightly higher return rates at 3-, 7-, and 14-day horizons (e.g., 7-day: 24.1% vs 22.6%).

## Foundational Learning

- Concept: Beam search decoding
  - Why needed here: Understand how Descartes generates multiple candidate descriptions and why latency increases with beam count.
  - Quick check question: If you increase beams from 2 to 4, what happens to both latency and the probability of finding the globally optimal sequence?

- Concept: Human-in-the-loop AI
  - Why needed here: The entire system relies on editors as quality filters; guardrails (experience thresholds, sensitive-topic restrictions) are meaningful only if humans remain engaged reviewers.
  - Quick check question: What failure mode emerges if 90% of users auto-accept suggestions without reading them?

- Concept: Content sensitivity categorization (e.g., BLP—biographies of living persons)
  - Why needed here: The pilot explicitly restricts BLP suggestions to editors with 50+ edits; understanding Wikipedia's content policies is prerequisite to designing guardrails.
  - Quick check question: Why might a hallucinated date in a living person's biography be higher-risk than the same error in a historical article?

## Architecture Onboarding

- Component map: Editor opens short-description tool in Android app -> App preloads suggestions from API -> Editor reveals, reviews, and accepts/modifies/rejects -> Published description enters normal Wikipedia patrol workflows -> Community graders periodically evaluate quality

- Critical path:
  1. Editor opens short-description tool in Android app.
  2. App preloads suggestions from API (background fetch to mask ~10s latency on CPU-only serving).
  3. Editor reveals, reviews, and accepts/modifies/rejects.
  4. Published description enters normal Wikipedia patrol workflows.
  5. Community graders periodically evaluate quality.

- Design tradeoffs:
  - Latency vs quality: Fewer beams = faster but potentially lower-quality candidates. Pilot chose 2 beams; production may use 1.
  - Coverage vs risk: Enabling more languages increases coverage but requires language-specific graders and error analysis (Gujarati and Arabic underperformed in pilot).
  - Automation vs oversight: Auto-accepting suggestions would scale faster but removes the quality filter that kept revert rates low (2.3%).

- Failure signatures:
  - High revert/report rates on accepted suggestions → model quality degradation or guardrail gaps.
  - Low suggestion reveal rate → latency too high or UX friction.
  - Language-specific grade collapse → insufficient training data; requires corpus augmentation for that language.
  - Capitalization/date hallucination spikes → missing post-processing filters.

- First 3 experiments:
  1. A/B test first-beam-only vs two-beam UI in one language; measure quality grades, latency, and acceptance rates.
  2. Add date-validation post-filter; compare hallucinated-date report rates before/after.
  3. Pilot in a new language with <1000 existing short descriptions; evaluate whether model suggestions accelerate coverage growth without degrading average quality below human baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can an automated verification mechanism effectively filter date hallucinations by accurately cross-referencing generated dates against the source article text? The authors note that "Incorrect dates" were a common error (14% of reports) and suggest that "descriptions with a date should only be shown if there is support for that date in the text," but do not implement or validate a method for programmatically verifying date support within the article text.

### Open Question 2
Is increasing the volume of training data sufficient to close the quality gap for underperforming languages (e.g., Arabic, Gujarati), or are architectural changes required? The paper states that for underperforming languages, "the most effective way to improve is by increasing the number of short descriptions in that language in the training set," but the efficacy of the proposed data augmentation solution remains untested.

### Open Question 3
Is the exclusion of biographies of living people (BLPs) a necessary safety guardrail, or is it overly conservative given the lack of negative signals in the pilot assessments? There is a discrepancy between the empirical data from the pilot (which showed no specific issues with BLPs) and the conservative safety recommendation, leaving the actual risk level undefined.

## Limitations
- Quality comparison between machine and human descriptions based on community grader assessments with unspecified inter-rater reliability
- Retention findings limited to short-term effects (3-14 days) that may reflect novelty rather than sustained engagement
- Coarse 1-5 grading scale may not capture meaningful differences between machine (4.1) and human (4.2) average grades

## Confidence

- **High Confidence**: Editors' ability to improve machine-generated descriptions through modification (supported by direct quality comparisons: 4.2 unmodified vs 4.1 modified vs 4.2 human baseline). Low revert and report rates (2.3%) validate the human-in-the-loop mechanism.

- **Medium Confidence**: First-beam superiority claim (64.7% selection rate, 4.2 vs 4.0 average grade), though the difference is modest and task-specific. The beam-search quality alignment assumption needs periodic revalidation.

- **Low Confidence**: Retention benefits (24.1% vs 22.6% at 7 days) due to short observation window and potential confounding factors. The causal link between the feature and retention is not firmly established.

## Next Checks

1. Conduct longitudinal retention analysis extending beyond 14 days to distinguish novelty effects from sustained engagement patterns.

2. Implement and evaluate date-validation post-processing filters by comparing hallucinated-date report rates before and after deployment.

3. Test first-beam-only vs two-beam UI in a controlled A/B experiment measuring quality grades, latency, and acceptance rates to validate the production decision.