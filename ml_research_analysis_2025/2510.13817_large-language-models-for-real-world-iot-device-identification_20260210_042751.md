---
ver: rpa2
title: Large Language Models for Real-World IoT Device Identification
arxiv_id: '2510.13817'
source_url: https://arxiv.org/abs/2510.13817
tags:
- vendor
- device
- devices
- accuracy
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of IoT device identification
  in real-world, open-set environments where network metadata is often incomplete,
  noisy, or intentionally obfuscated. The authors reframe device identification as
  a language modeling task over heterogeneous network metadata, using large language
  models (LLMs) to generate high-fidelity vendor pseudolabels from the IoT Inspector
  dataset.
---

# Large Language Models for Real-World IoT Device Identification

## Quick Facts
- arXiv ID: 2510.13817
- Source URL: https://arxiv.org/abs/2510.13817
- Reference count: 40
- Primary result: 98.25% top-1 accuracy and 90.73% macro accuracy across 2,015 vendors for IoT device identification

## Executive Summary
This paper introduces a novel approach to IoT device identification by treating network metadata as a language modeling task using large language models. The authors leverage the IoT Inspector dataset to generate high-fidelity vendor pseudolabels and instruction-tune a quantized LLaMA 3.1 8B model with curriculum learning. The resulting model demonstrates exceptional accuracy (98.25% top-1, 90.73% macro) in identifying IoT device vendors across 2,015 categories, while showing resilience to common real-world challenges including missing fields, protocol drift, and adversarial manipulation. The approach provides a scalable and interpretable foundation for IoT device identification at scale in open-set environments.

## Method Summary
The authors reframe IoT device identification as a language modeling problem over heterogeneous network metadata. They use the IoT Inspector dataset to generate high-fidelity vendor pseudolabels through automated labeling techniques. A quantized LLaMA 3.1 8B model is then instruction-tuned using curriculum learning, which progressively exposes the model to more complex identification tasks. This approach allows the model to learn robust representations that can handle incomplete, noisy, or obfuscated network metadata while maintaining high accuracy across a large vendor space.

## Key Results
- Achieves 98.25% top-1 accuracy in vendor identification across 2,015 vendor categories
- Maintains 90.73% macro accuracy, demonstrating balanced performance across all vendors
- Demonstrates resilience to missing fields, protocol drift, and adversarial manipulation
- Validated on independent IoT testbed with strong generalization performance

## Why This Works (Mechanism)
The approach works by treating network metadata as sequential language data, allowing LLMs to leverage their pattern recognition capabilities for structured identification tasks. The curriculum learning strategy enables gradual skill acquisition, where the model first learns basic identification patterns before progressing to more complex scenarios involving missing data or adversarial conditions. The use of high-quality pseudolabels from IoT Inspector provides a large-scale training foundation, while the quantized LLaMA 3.1 8B model balances computational efficiency with sufficient capacity for the task.

## Foundational Learning
- Language modeling for structured data: Why needed - enables leveraging LLM capabilities for non-text tasks; Quick check - verify model can process heterogeneous metadata formats
- Curriculum learning: Why needed - facilitates gradual skill acquisition for complex identification scenarios; Quick check - monitor learning curves across curriculum stages
- Pseudolabel generation: Why needed - provides large-scale labeled data without manual annotation; Quick check - validate pseudolabel quality through human verification on sample dataset
- Quantized model deployment: Why needed - enables practical deployment on resource-constrained devices; Quick check - benchmark inference latency on target hardware
- Open-set recognition: Why needed - handles unknown devices in real-world deployment; Quick check - test with out-of-distribution device samples

## Architecture Onboarding
Component map: Network metadata -> Preprocessing -> LLaMA 3.1 8B -> Classification -> Vendor prediction
Critical path: Raw network metadata flows through preprocessing into the quantized LLM, which generates vendor predictions through classification layer
Design tradeoffs: Computational efficiency vs. accuracy (quantization), model size vs. deployment constraints, training data scale vs. labeling quality
Failure signatures: Performance degradation with extreme missing data, misclassification of similar vendors, sensitivity to specific protocol fields
First experiments:
1. Test model on complete vs. incomplete metadata to quantify missing data impact
2. Evaluate cross-vendor confusion patterns to identify ambiguous vendor pairs
3. Measure inference latency on representative edge hardware to validate deployment feasibility

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Real-world robustness remains unverified across diverse adversarial conditions and extreme missing data scenarios
- Focus on vendor-level identification may limit applicability to scenarios requiring device model or firmware version granularity
- Quantization impact on edge device performance was not explicitly evaluated

## Confidence
High confidence in technical approach and methodology due to detailed description of instruction-tuning process and curriculum learning strategy. Medium confidence in reported accuracy metrics (98.25% top-1, 90.73% macro) based on internal validation and single external testbed without extensive cross-dataset testing. Low confidence in resilience claims to adversarial manipulation, missing fields, and protocol drift due to lack of comprehensive validation across multiple real-world scenarios or attack vectors.

## Next Checks
1. Conduct large-scale cross-dataset evaluation using diverse IoT network datasets (e.g., IoT-23, UNSW IoT, custom enterprise datasets) to assess generalization performance and identify potential domain-specific weaknesses.
2. Perform systematic adversarial testing with controlled attacks (e.g., packet injection, timing manipulation, field obfuscation) to quantify model robustness and identify failure modes under realistic threat scenarios.
3. Evaluate the quantized model's performance on representative edge hardware (e.g., Raspberry Pi, NVIDIA Jetson) to verify practical deployment feasibility and assess accuracy-latency tradeoffs in resource-constrained environments.