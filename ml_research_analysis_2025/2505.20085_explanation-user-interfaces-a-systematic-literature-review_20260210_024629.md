---
ver: rpa2
title: 'Explanation User Interfaces: A Systematic Literature Review'
arxiv_id: '2505.20085'
source_url: https://arxiv.org/abs/2505.20085
tags:
- https
- user
- explanations
- design
- international
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review identifies and analyzes Explanation
  User Interfaces (XUIs) for explainable AI systems. The study examines 146 publications
  to understand how XUIs are designed, evaluated, and what influences their development
  across different domains and user types.
---

# Explanation User Interfaces: A Systematic Literature Review

## Quick Facts
- arXiv ID: 2505.20085
- Source URL: https://arxiv.org/abs/2505.20085
- Reference count: 40
- Key outcome: Systematic review of 146 publications on XUIs, finding visual explanations dominate and developing the HERMES framework for practitioners.

## Executive Summary
This systematic literature review examines Explanation User Interfaces (XUIs) across 146 publications to understand how explainable AI systems are designed and evaluated. The study reveals that visual explanations, particularly heatmaps, dominate XUI design, while trust remains the most frequently evaluated metric. The review identifies significant gaps between algorithmic transparency and practical usability, leading to the development of HERMES, a web platform that provides practitioners with evidence-based design guidelines and frameworks. HERMES enables filtering recommendations based on application domain, user type, AI model, and explanation modality to guide design decisions.

## Method Summary
The study conducted a systematic literature review using search strings combining XAI and user interface terms across ACM Digital Library, IEEE Xplore, and Scopus. After initial retrieval of 7,084 publications, 146 relevant papers were selected based on publication date (>= 2013), English language, and venue quality (A*/A/B/Q1/Q2 rankings). The final dataset was manually classified across 13 dimensions including application domain, user type, AI model, explanation modality, and evaluation methods. The HERMES framework was synthesized from these classifications to provide practitioners with design guidance.

## Key Results
- Visual explanations dominate XUI design, with heatmaps being the most common visualization technique across domains
- Neural networks are the most frequently explained AI models, paired with feature importance, counterfactual explanations, and SHAP values
- Trust is the most commonly evaluated metric (33 studies), while transparency is explicitly assessed in only 8 studies
- Domain experts in high-stakes applications prioritize helpfulness over transparency for decision-making support

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual explanation modalities may improve interpretability across diverse user types and data contexts
- Mechanism: Visual representations translate complex AI reasoning into graphical patterns that leverage humans' pre-attentive visual processing, potentially reducing cognitive load compared to textual or numerical explanations alone
- Core assumption: Users possess baseline visual literacy and the cognitive capacity to map visual patterns to causal relationships
- Evidence anchors: [abstract] "visual explanations dominate across domains, with tabular and image data being most common"; [Section 5.2] "Visual explanations dominate, likely due to their intuitive and accessible nature... Heatmaps stand out as the most frequently used visualization"
- Break condition: When users lack visual literacy, when explanations require precise numerical values, or when multimodal data cannot be meaningfully reduced to visual form

### Mechanism 2
- Claim: Interactivity in XUIs potentially enhances user comprehension and trust calibration
- Mechanism: Interactive elements enable users to actively test model boundaries and receive immediate feedback, supporting mental model formation through exploratory learning rather than passive consumption
- Core assumption: Users are motivated to engage and possess sufficient domain knowledge to formulate meaningful exploratory queries
- Evidence anchors: [Section 5.3] "Interactive XUIs allow users to manipulate explanation parameters, explore different perspectives, or request additional details, potentially enhancing comprehension and trust"; [Section 2.3] "Interactivity, in particular, plays a central role in the explanation process"
- Break condition: When interaction introduces cognitive overload, when users lack expertise to guide exploration, or when real-time response is not feasible

### Mechanism 3
- Claim: User-centered evaluation methods may align better with domain experts' decision-making needs than algorithmic metrics
- Mechanism: Qualitative methods capture how explanations integrate into actual workflows, revealing whether explanations support task performance rather than merely displaying model internals
- Core assumption: Domain experts can articulate their reasoning needs and researchers can translate these into design requirements
- Evidence anchors: [Section 6.2] "Interviews are most commonly used for domain experts, particularly in high-stakes fields like healthcare and finance"; [Section 8] "Domain experts in high-stakes applications tend to prioritise helpfulness over transparency... primary concern is whether an explanation supports their decision-making process"
- Break condition: When experts lack metacognitive awareness of their needs, when evaluation contexts differ from deployment contexts, or when resources prevent iterative user involvement

## Foundational Learning

- Concept: XAI technique taxonomy (feature importance, counterfactuals, Shapley values, saliency masks)
  - Why needed here: Selecting appropriate XAI technique depends on AI model type and data characteristics—neural networks pair with saliency masks, tabular data with feature importance
  - Quick check question: Given a neural network classifying medical images for domain experts, which XAI technique would you prioritize and why?

- Concept: Human-Centered Design (HCD) iteration cycle
  - Why needed here: XUI design requires empirical validation with actual users; the SLR found 53 controlled experiments and 28 interviews as primary evaluation methods
  - Quick check question: Describe how you would structure a user study to evaluate whether explanations improve task performance for financial analysts

- Concept: Trust calibration vs. transparency tradeoff
  - Why needed here: The SLR found transparency explicitly assessed in only 8 studies while trust appeared in 33, suggesting a gap between what researchers measure and what users need
  - Quick check question: How would you design an evaluation that measures both transparency perception and appropriate trust calibration?

## Architecture Onboarding

- Component map: Application Domain -> User Type -> AI Model -> AI Task -> XAI Model -> Explanation Modality -> Evaluation Method -> Evaluation Metrics (13 total dimensions)

- Critical path: Define domain and user type first → constrain AI model/task → select XAI technique based on model compatibility → choose explanation modality (visual/text/hybrid) → plan evaluation method appropriate to user type

- Design tradeoffs: Visual vs. textual modality (accessibility vs. precision); interactive vs. static (engagement vs. complexity); local vs. global explanations (instance-specific vs. model-wide understanding); feature importance vs. counterfactuals (attribution vs. actionable guidance)

- Failure signatures: Explanations that overwhelm with detail; XAI techniques mismatched to model type (e.g., decision trees for neural networks); evaluation focused on AI experts when end-users are domain experts; static explanations for high-stakes decisions requiring exploration

- First 3 experiments:
  1. Audit existing XUI against HERMES dimensions: identify which of the 6 entry points are undefined or mismatched (e.g., domain expert users receiving AI-expert-oriented explanations)
  2. Run a small-scale A/B test comparing visual-only vs. visual+text explanations for your primary user type, measuring trust, understandability, and task performance
  3. Conduct 5-10 semi-structured interviews with target users to surface which evaluation metrics (helpfulness, transparency, confidence) matter most for their workflow, then align evaluation design accordingly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Explanation User Interfaces (XUIs) be effectively designed to leverage Large Language Models (LLMs) for conversational explanations while mitigating risks associated with model opacity and hallucinations?
- Basis in paper: [explicit] The authors identify the growing role of LLMs as a key future challenge, noting they offer human-like explanations but remain opaque and prone to hallucinations that undermine user trust
- Why unresolved: LLMs represent a novel intersection with XAI that reshapes interaction paradigms, yet current XUI design principles have not yet adapted to ensure these models are transparent and controllable
- What evidence would resolve it: Empirical studies comparing user trust calibration and error detection rates in LLM-based conversational XUIs versus traditional static or visual XUIs

### Open Question 2
- Question: Does the HERMES framework effectively bridge the gap between academic literature and real-world application by improving the design and evaluation process for practitioners?
- Basis in paper: [explicit] The authors explicitly state in Section 8.1 and Section 11 that "user studies to evaluate and validate the platform are needed" to confirm if the tool successfully aids practitioners
- Why unresolved: While the framework synthesizes 146 papers into actionable guidelines, it has not yet undergone empirical validation with designers and developers to prove its utility in live development cycles
- What evidence would resolve it: Results from user studies where practitioners utilize HERMES to build XUIs, measuring the efficiency of the design process and the usability of the resulting interfaces

### Open Question 3
- Question: How can "transparency" be operationalized and explicitly measured in user studies, given the current discrepancy between its theoretical importance and its lack of evaluation?
- Basis in paper: [inferred] Section 6.1 highlights that despite transparency being a core design requirement, it was explicitly assessed in only eight studies, suggesting a lack of standardized measurement methods
- Why unresolved: The field lacks a consistent, validated metric for transparency, leading researchers to prioritize easier-to-measure factors like trust or usability instead
- What evidence would resolve it: The development and validation of a standardized scale or proxy metrics for "perceived transparency" that correlates with objective system parameters

## Limitations

- The review excludes non-English papers and works before 2013, creating temporal and linguistic blind spots
- Classification of explanation techniques and user types relies on manual coding that may not capture nuanced distinctions across studies
- The focus on published research rather than deployed systems potentially misses practical implementation challenges that emerge in real-world settings

## Confidence

- **High Confidence**: The dominance of visual explanations and heatmaps as primary visualization techniques is well-supported across the corpus, with consistent patterns in application domains and AI models studied
- **Medium Confidence**: The relationship between user types and evaluation methods shows clear patterns but may be influenced by publication bias toward certain methodologies
- **Low Confidence**: The effectiveness of specific XUI design choices on actual user outcomes remains uncertain due to the predominance of proxy metrics like trust and understandability rather than task performance measures

## Next Checks

1. **Replicate the SLR methodology** with expanded inclusion criteria (non-English papers, pre-2013 works) to assess the stability of observed patterns across a broader literature base

2. **Conduct a focused validation study** comparing visual vs. multimodal explanations for a specific high-stakes domain (e.g., medical diagnosis), measuring both proxy metrics (trust) and task performance (diagnostic accuracy)

3. **Perform an expert review** of the HERMES framework by having practitioners from diverse domains (healthcare, finance, education) attempt to design XUIs using the platform, documenting usability issues and gaps in the guidance provided