---
ver: rpa2
title: 'GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities
  of MLLMs with Origami Folding Tasks'
arxiv_id: '2512.22207'
source_url: https://arxiv.org/abs/2512.22207
tags:
- reasoning
- spatial
- folds
- arxiv
- crease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GamiBench is a benchmark for evaluating spatial reasoning and
  2D-to-3D planning in multimodal large language models using origami-inspired folding
  tasks. It introduces 372 origami instances (186 regular, 186 impossible) paired
  with six viewpoint renders each, covering three visual question-answering tasks:
  3D fold prediction, valid viewpoint recognition, and impossible fold detection.'
---

# GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks

## Quick Facts
- arXiv ID: 2512.22207
- Source URL: https://arxiv.org/abs/2512.22207
- Reference count: 1
- Primary result: Benchmark reveals significant limitations in current MLLMs' spatial reasoning and geometric understanding capabilities

## Executive Summary
GamiBench is a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning capabilities of multimodal large language models (MLLMs) through origami-inspired folding tasks. The benchmark introduces 372 origami instances (186 regular, 186 impossible) paired with six viewpoint renders each, covering three visual question-answering tasks: 3D fold prediction, valid viewpoint recognition, and impossible fold detection. Experiments with 21 models, including GPT-5 and Gemini-2.5-Pro, reveal that even leading models struggle with multi-step spatial reasoning and maintaining consistency across views, with accuracy dropping sharply for complex folds.

## Method Summary
GamiBench provides a standardized evaluation framework using 2D crease patterns paired with 3D folded shape renders from six canonical viewpoints. The benchmark employs multiple-choice VQA sets with 4 possible 3D renders (A-D) plus an "impossible" option (E). Models are evaluated on three tasks: predicting the correct 3D fold from 2D crease patterns, recognizing valid viewpoints, and detecting impossible folds. The evaluation measures not just final accuracy but also viewpoint consistency (VC) and impossible fold selection rate (IFSR) across simple (<40 creases) and complex (≥40 creases) folds.

## Key Results
- GPT-5 achieved 59.5% accuracy on 3D fold prediction, outperforming other models but still showing significant limitations
- Most models had 0-14% accuracy on impossible fold detection, with Llama-4 series at 0%, indicating limited sensitivity to geometric infeasibility
- On average, models performed 10-15% better on simple folds than on complex ones, demonstrating scaling brittleness in spatial reasoning
- Viewpoint consistency scores revealed that high accuracy doesn't guarantee geometric understanding, as models struggled to maintain correctness across different viewpoints

## Why This Works (Mechanism)

### Mechanism 1: Cross-View Geometric Consistency Testing
- Evaluating whether a model maintains correct 3D understanding across viewpoint changes reveals latent spatial representations that single-view accuracy masks
- If a model truly understands 3D geometry rather than surface-level visual matching, correctness should be view-invariant for the same object
- Conditional viewpoint consistency measures persistence of correctness under rotation

### Mechanism 2: Physical Feasibility Discrimination via Constraint Violation
- Introducing deliberately impossible crease patterns tests whether models encode physical plausibility constraints
- Models must classify impossible folds (violating flat-foldability axioms like Kawasaki's and Maekawa's theorems) as "Option E: impossible"
- This tests whether models exhibit calibrated skepticism rather than just pattern matching

### Mechanism 3: Complexity-Controlled Scaling Stress Test
- Stratifying folds by crease count isolates whether spatial reasoning degrades gracefully or collapses under geometric load
- Simple folds test localized 2D-to-3D mapping while complex folds require multi-step constraint tracking
- The 10-15% average accuracy drop from simple to complex quantifies scaling brittleness

## Foundational Learning

- **Concept: 2D-to-3D Mental Transformation**
  - Why needed: The core task requires inferring a 3D folded structure from a 2D crease pattern—this is mental simulation of spatial transformations
  - Quick check: Given a square with diagonal creases (mountain/valley assigned), can you mentally predict the folded 3D configuration?

- **Concept: Flat-Foldability Axioms (Kawasaki's and Maekawa's Theorems)**
  - Why needed: Impossible fold detection requires understanding physical constraints—e.g., Kawasaki's theorem (sum of alternating angles around a vertex = 180°) and Maekawa's (M – V = ±2 for flat-foldable vertices)
  - Quick check: At a 4-crease vertex with angles 60°, 90°, 60°, 150°, is flat-foldability possible? (No: 60 + 60 ≠ 90 + 150)

- **Concept: Multi-View Geometric Coherence**
  - Why needed: Viewpoint Consistency measures whether spatial understanding is view-invariant
  - Quick check: If a pyramid looks like a triangle from the side and a square with center point from above, do both views represent the same object?

## Architecture Onboarding

- **Component map:** Input 2D crease pattern + text prompt → Multiple-choice candidates (A-D + E) → Model response → Ground truth verification via Oriedita CAMv and Origami Simulator → Evaluation metrics (Accuracy, VC, IFSR)
- **Critical path:** Load crease pattern → verify foldability status → render 3D folded mesh from canonical viewpoints → construct multiple-choice set → present to model with standardized prompt → if primary correct, re-test with alternate viewpoint for VC
- **Design tradeoffs:** Multiple-choice format enables scalable evaluation but introduces positional bias; crease-count complexity is coarse; synthetic origami domain limits real-world generalization; single prompt configuration limits reproducibility
- **Failure signatures:** High accuracy + low VC → surface visual matching without 3D understanding; high IFSR + low impossible accuracy → model over-uses "impossible" option; low IFSR + low impossible accuracy → model never detects infeasibility; better complex than simple performance → crease count is poor difficulty proxy
- **First 3 experiments:** 1) Baseline calibration: run all 21 models and confirm pipeline reproduces reported Accuracy/VC/IFSR within ±2%; 2) Ablation on prompt framing: test whether removing "Option E: impossible" instruction changes IFSR; 3) Cross-domain transfer probe: evaluate whether models trained on GamiBench-style data improve on related spatial benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic origami data limits generalizability to real-world spatial reasoning tasks involving material deformation and lighting variation
- Multiple-choice format introduces potential positional bias and may not fully capture continuous nature of 3D spatial understanding
- Complexity stratification based solely on crease count is a coarse proxy that doesn't account for symmetry or other geometric properties
- API changes over time affect reproducibility for proprietary models, and single prompt configuration limits assessment of model robustness

## Confidence
- **High confidence:** Benchmark design methodology and core findings about MLLMs struggling with multi-step spatial reasoning are well-supported
- **Medium confidence:** Specific accuracy percentages and model rankings may vary with API versions and prompt engineering
- **Medium confidence:** Interpretation that crease count alone doesn't capture true planning difficulty is reasonable but requires additional validation

## Next Checks
1. Evaluate whether models trained or fine-tuned on GamiBench-style data show improved performance on related spatial benchmarks (GSR-Bench, LEGO-Puzzles, 3DSRBench) to assess domain transfer
2. Re-run experiments using alternative complexity measures beyond crease count (e.g., symmetry score, constraint density) to validate scaling findings
3. Implement continuous 3D prediction task (e.g., mesh generation) alongside multiple-choice evaluation to better capture spatial understanding capabilities