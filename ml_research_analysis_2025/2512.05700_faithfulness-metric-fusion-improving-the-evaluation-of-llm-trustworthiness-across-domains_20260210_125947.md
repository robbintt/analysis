---
ver: rpa2
title: 'Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness
  across domains'
arxiv_id: '2512.05700'
source_url: https://arxiv.org/abs/2512.05700
tags:
- metrics
- faithfulness
- evaluation
- answer
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve faithfulness evaluation
  in large language models (LLMs) by fusing multiple elementary metrics. The authors
  combine metrics like n-gram, embedding-based, graph-based, and LLM-as-a-judge into
  a single fused metric using an Explainable Boosting Machine (EBM) to weight each
  metric's importance.
---

# Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains

## Quick Facts
- **arXiv ID:** 2512.05700
- **Source URL:** https://arxiv.org/abs/2512.05700
- **Reference count:** 40
- **One-line primary result:** Fusing multiple elementary faithfulness metrics via EBM improves correlation with human judgments across all tested domains.

## Executive Summary
This paper proposes a method to improve faithfulness evaluation in large language models by fusing multiple elementary metrics into a single, more reliable metric. The authors combine n-gram, embedding-based, graph-based, and LLM-as-a-judge metrics using an Explainable Boosting Machine (EBM) to weight each metric's importance based on its alignment with human judgments. The fused metric consistently outperforms individual metrics across question answering, conversational QA, and dialogue summarization domains, with improvements ranging from 0.005 to 0.136 in correlation with human judgments. The approach addresses the inherent biases of individual metrics and leverages their complementary strengths to create a more robust evaluation framework.

## Method Summary
The method involves generating LLM responses across multiple domains (short/long-form QA, conversational QA, summarization) using a finetuned Llama 3.1-8B model. Elementary faithfulness metrics are computed for each response, including n-gram/ROUGE variants, embedding-based BERTScore, lexical/semantic matches, AMR graph-based comparisons via STOG BART Large, and LLM-as-a-judge evaluations using both Likert scores and continuous confidence values from token logits. These metrics are then fused using an Explainable Boosting Machine (EBM) trained to predict human judgments from the metric inputs, with domain-specific weightings learned for each evaluation category.

## Key Results
- The fused metric consistently outperformed individual metrics across all tested domains (QA, conversational QA, summarization).
- Graph-based metrics, while individually weak, contributed significantly to the fused metric, especially in longer, more ambiguous domains like long-form QA and summarization.
- The best-performing LLM-as-a-judge method used token logits to provide continuous faithfulness values rather than binary classifications.
- Improvements in correlation with human judgments ranged from 0.005 to 0.136 depending on the domain.

## Why This Works (Mechanism)

### Mechanism 1: Complementary Bias Compensation via Metric Fusion
The fusion approach compensates for individual metric biases by combining metrics with opposing tendenciesâ€”LLM-as-a-judge metrics tend to over-rate faithfulness while matching metrics tend to under-rate it. The EBM learns optimal weights for each metric based on their alignment with human judgments, creating a balanced evaluation that captures a wider range of features. This works because errors in individual metrics are not perfectly correlated and their feature-capturing capabilities (lexical, semantic, structural) are complementary.

### Mechanism 2: Graph-Based Structuring for Long-Form Ambiguity
Abstract Meaning Representation (AMR) graphs improve evaluation in long-form, ambiguous domains by converting text into structured semantic format for comparison. AMR extraction identifies core entities and their relationships, stripping away syntactic noise. This allows comparison via graph-matching metrics (e.g., SMatch) which is more robust for detecting factual consistency in paraphrased or complex text than n-gram or embedding-based methods. The mechanism assumes faithfulness is primarily a function of entity-relationship structure.

### Mechanism 3: Logit-Based Granularity for LLM-as-a-Judge
Using an LLM judge's token logits converts binary predictions into continuous confidence scores, providing more granular faithfulness evaluation. Instead of simple "Faithful/Not-Faithful" labels, this method uses the model's internal confidence for that token. A prediction of "Faithful" with 99% confidence receives a different score than one with 51% confidence, capturing degrees of uncertainty. The mechanism assumes the LLM is well-calibrated such that its internal confidence correlates with true likelihood of faithfulness.

## Foundational Learning

- **Concept: Explainable Boosting Machine (EBM)**
  - **Why needed here:** This is the core fusion algorithm that learns feature importances for combining metrics. Understanding it as a glass-box, tree-based model is crucial for interpreting why certain metrics are weighted more heavily in different domains.
  - **Quick check question:** How does an EBM differ from a Random Forest in terms of interpretability, and what does it output that allows for direct weighting of metrics?

- **Concept: Abstract Meaning Representation (AMR)**
  - **Why needed here:** This is the structural mechanism for evaluating faithfulness in complex text. One must grasp that it is a semantic graph formalism where nodes are concepts and edges are relations.
  - **Quick check question:** What is the core principle of SMatch, and why would comparing semantic graphs be more robust than comparing n-grams for long-form text?

- **Concept: LLM-as-a-Judge Calibration**
  - **Why needed here:** The paper's most effective metric relies on this principle. Understanding that an LLM's stated label and its internal confidence can differ is key to the mechanism's function.
  - **Quick check question:** What is a "logit" in this context, and how does its conversion to a probability provide a more nuanced faithfulness score than a single label?

## Architecture Onboarding

- **Component Map:** Dataset Layer (ELOQUENCE datasets) -> Data Processor -> Prompts -> Generation Layer (Llama 3.1-8B) -> Evaluation Layer (Multiple evaluators run in parallel) -> Fusion Layer (EBM) -> Validation Layer (Human judgments)

- **Critical Path:** The most critical path is the Evaluation -> Fusion -> Validation pipeline. A failure in any single evaluator can be tolerated, but incorrect metric computation or a broken fusion model training script invalidates the entire output. The validation step against human judgment is the key success criterion.

- **Design Tradeoffs:**
  - Complexity vs. Performance: Using a full AMR parser is computationally expensive but essential for long-form domains.
  - LLM Judge Choice: Using the same model family for both generation and judgment is convenient but introduces potential bias.
  - Granularity: Fact-based evaluation (splitting summaries into points) is more accurate than full-text evaluation but requires more sophisticated prompting.

- **Failure Signatures:**
  - AMR Parse Failures: If outputs are garbled, the parser will produce disconnected or empty graphs, leading to zero faithfulness scores from graph metrics.
  - EBM Overfitting: If the human judgment dataset is too small, the fused metric will achieve perfect scores on training data but fail on new domains.
  - Logit Calibration Collapse: If the judge LLM is always >95% confident, the "LLM Conf." metric will become effectively binary again.

- **First 3 Experiments:**
  1. Ablation Study on Fusion: Run the evaluation pipeline but train the EBM by removing one class of metrics at a time. Measure the drop in correlation to human judgment to validate each component's contribution.
  2. Domain Transfer Test: Train the EBM fusion model on a dataset from one domain and test its performance directly on another without retraining. This tests the generalizability of the learned metric weights.
  3. Judge Calibration Analysis: For the "LLM Conf." metric, plot the distribution of confidence scores for both human-judged-faithful and -unfaithful samples. A well-calibrated model should show a clear separation.

## Open Questions the Paper Calls Out

- **Can a single set of universal metric weights achieve high correlation across all domains, or is domain-specific tuning inherently necessary?**
  - The study optimized and evaluated weights independently for each specific domain (QA, ConvQA, Summarization) but did not test the performance of a generalized model trained on all domains simultaneously.

- **How robust is the fused metric when evaluating the outputs of significantly larger or closed-source LLMs compared to the Llama 3.1-8B model used in this study?**
  - The methodology specifies the use of a "finetuned Llama 3.1-8B instruct model" for all response generation, leaving the evaluation of other model architectures unexplored.

- **Can additional features or metrics be incorporated to close the performance gap in high-ambiguity domains like conversational QA and summarization?**
  - The authors identify conversational QA and summarization as "the most challenging" domains, where correlations remain moderate despite fusion.

## Limitations

- The evaluation is limited to three specific domains (question answering, conversational QA, dialogue summarization), making claims of universal improvement domain-specific rather than truly universal.
- The effectiveness of the logit-based LLM-as-a-judge method hinges on the assumption that Llama 3.1-8B's logits are well-calibrated for faithfulness prediction, which is not validated.
- The paper assumes the STOG BART Large model reliably extracts semantic structure from both source and generated text, but does not validate AMR quality or provide error analysis for parser failures.

## Confidence

**High Confidence Claims:**
- The fused metric improves correlation with human judgments compared to individual metrics within the tested domains.
- Graph-based metrics contribute significantly to the fused metric in longer, more ambiguous domains.
- The EBM provides interpretable feature importance weights that vary by domain.

**Medium Confidence Claims:**
- The logit-based LLM-as-a-judge method provides more granular faithfulness evaluation than binary classification.
- The complementary bias compensation mechanism effectively balances evaluation.
- The specific metric weights learned by EBM represent optimal combinations for faithfulness evaluation.

**Low Confidence Claims:**
- The fused metric would perform equally well on domains not included in the ELOQUENCE dataset.
- The calibration of Llama 3.1-8B logits is sufficient for reliable confidence-based faithfulness scoring.
- AMR-based evaluation would remain effective if applied to significantly longer or more complex text forms.

## Next Checks

1. **Domain Transfer Experiment:** Train the EBM fusion model on one domain (e.g., short-form QA) and evaluate its performance on a held-out domain (e.g., summarization) without retraining to validate generalizability.

2. **Calibration Analysis for LLM-as-a-Judge:** Create a calibration plot showing the distribution of confidence scores for human-judged-faithful versus human-judged-unfaithful samples and compute Expected Calibration Error (ECE) to quantify whether stated confidence aligns with empirical accuracy.

3. **AMR Parser Robustness Test:** Conduct an ablation study where LLM-generated responses containing known hallucinations are passed through the AMR parser, then manually verify whether the resulting graphs correctly capture semantic structure and whether graph-based metrics identify the hallucinated portions.