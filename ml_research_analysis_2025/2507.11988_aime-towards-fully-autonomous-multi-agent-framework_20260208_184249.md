---
ver: rpa2
title: 'Aime: Towards Fully-Autonomous Multi-Agent Framework'
arxiv_id: '2507.11988'
source_url: https://arxiv.org/abs/2507.11988
tags:
- dynamic
- actor
- agents
- aime
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Aime is a multi-agent framework designed to overcome the limitations
  of rigid, static plan-and-execute systems by enabling dynamic, reactive planning
  and execution. It introduces three core innovations: a Dynamic Planner that continuously
  refines strategy based on real-time feedback, an Actor Factory that assembles specialized
  agents on-demand with tailored tools and knowledge, and a centralized Progress Management
  Module that maintains coherent system-wide state awareness.'
---

# Aime: Towards Fully-Autonomous Multi-Agent Framework

## Quick Facts
- arXiv ID: 2507.11988
- Source URL: https://arxiv.org/abs/2507.11988
- Reference count: 6
- Aime achieves 77.6% success rate on GAIA, 66.4% on SWE-bench Verified, and 92.3% on WebVoyager

## Executive Summary
Aime introduces a dynamic multi-agent framework that overcomes the limitations of rigid, static plan-and-execute systems by enabling continuous adaptation through real-time feedback integration. The framework comprises three core innovations: a Dynamic Planner that continuously refines strategy, an Actor Factory that assembles specialized agents on-demand with tailored tools and knowledge, and a centralized Progress Management Module that maintains coherent system-wide state awareness. By replacing the fixed planner-executor model with a fluid architecture where agent roles and capabilities evolve during execution, Aime demonstrates state-of-the-art performance across three challenging benchmarks.

## Method Summary
Aime operates through a continuous feedback loop where the Dynamic Planner maintains a hierarchical task list and dispatches executable subtasks based on real-time execution feedback. The Actor Factory instantiates specialized agents with functional tool bundles and composed system prompts containing persona, tool descriptions, and knowledge. Dynamic Actors execute via ReAct loops while reporting progress through dual communication protocols to the centralized Progress Management Module. The system replaces static plan-and-execute with dynamic, reactive planning where both strategic task decomposition and tactical action selection adapt within single iterations based on execution outcomes.

## Key Results
- Achieves 77.6% success rate on GAIA (general reasoning), surpassing specialized state-of-the-art frameworks
- Attains 66.4% resolution rate on SWE-bench Verified (software engineering tasks)
- Demonstrates 92.3% success rate on WebVoyager (web navigation benchmark)

## Why This Works (Mechanism)

### Mechanism 1
Continuous planner feedback integration improves task adaptation over static systems. The Dynamic Planner produces dual outputs per iteration—an updated global task list (strategic) and an immediate executable action (tactical)—enabling re-planning within single iterations when subtasks fail. This assumes the underlying LLM can effectively reason about plan modifications based on partial execution outcomes. Evidence shows this formulation enables remarkable adaptability, with the planner making both strategic and tactical adjustments in a single iteration when subtasks fail. Break condition occurs if unreliable or delayed feedback causes cascading incoherent task decompositions.

### Mechanism 2
On-demand actor instantiation with bundled toolkits reduces selection errors compared to fixed agent pools. The Actor Factory groups tools into functional bundles (e.g., 'WebSearch,' 'FileSystem') and composes customized system prompts from five modular components: persona, tool descriptions, knowledge, environment context, and output format. This assumes tool bundles provide sufficient coverage for task requirements. Evidence indicates this bundle-based approach ensures functional completeness and reduces critical tool omissions. Break condition occurs when subtasks require unexpected tool combinations spanning multiple bundles, leading to incomplete or misaligned toolkits.

### Mechanism 3
Centralized progress tracking with dual communication protocols reduces context loss during agent handoffs. The Progress Management Module maintains a hierarchical progress list as the single source of truth, while actors communicate via real-time Update Progress tool calls for milestones and structured conclusion reports with status, summary, and reference pointers. This assumes actors autonomously report progress at appropriate moments. Evidence shows combining shared data structures with dual protocols maintains context explicitly and efficiently throughout the task lifecycle. Break condition occurs if actors under-report progress (missing critical context) or over-report (creating noise), degrading planner situational awareness.

## Foundational Learning

- **ReAct paradigm (Reasoning-Action-Observation loops)**: Dynamic Actors operate via ReAct cycles; understanding this iterative pattern is essential for debugging actor behavior. Quick check: Can you explain why ReAct separates reasoning from action execution rather than generating both simultaneously?

- **Hierarchical task decomposition**: The Dynamic Planner maintains nested task structures; understanding parent-child relationships encoding dependencies is crucial. Quick check: Given a task "Plan a conference trip," what would be reasonable subtasks, and which could execute in parallel vs. sequentially?

- **Tool-augmented LLM agents**: Actors select from tool bundles; understanding tool schemas and invocation patterns is prerequisite for extending the toolkit. Quick check: How does an LLM agent decide which tool to invoke, and what information must the tool description provide?

## Architecture Onboarding

- **Component map**: Dynamic Planner → Orchestrator → Actor Factory → Instantiator → Dynamic Actors → Executors → Progress Management Module → State store
- **Critical path**: User request → Dynamic Planner decomposes into progress list → Planner dispatches subtask → Actor Factory instantiates actor with persona/tools/knowledge → Actor executes ReAct loop → Actor updates progress (real-time + final report) → Planner evaluates and iterates → Cycle repeats until completion
- **Design tradeoffs**: Bundle granularity vs. flexibility (coarse bundles reduce errors but may include unnecessary tools; fine-grained bundles increase flexibility but raise selection complexity); Centralized state vs. scalability (single progress list simplifies coordination but may become bottleneck for large teams); Planner intervention frequency (more frequent re-planning improves adaptability but increases LLM calls and latency)
- **Failure signatures**: Stuck planner (progress list shows repeated dispatches without completion—planner unable to adapt strategy); Silent actors (no progress updates for extended periods—actor lacks appropriate tools or encountered unrecoverable error); Context drift (actors produce misaligned outputs—progress list accumulated inconsistent state updates)
- **First 3 experiments**: 1) Trace a single GAIA question end-to-end to verify feedback loop operation; 2) Intentionally inject a failing subtask to observe planner adjustment within one iteration; 3) Compare actor instantiation for similar subtasks to verify consistent persona/tool selections

## Open Questions the Paper Calls Out

- How can the framework maintain coordination efficiency when scaling to significantly larger agent teams? The authors state future work will focus on enhancing scalability for larger agent teams, as the centralized Progress Management Module may create a synchronization bottleneck.

- How can agents be empowered to autonomously generate new tools rather than relying on pre-curated bundles? The conclusion identifies the need for agents to autonomously acquire new capabilities, reducing reliance on pre-curated tools, as the current Actor Factory is limited to selecting from pre-packaged bundles.

- What is the trade-off between Aime's dynamic re-planning and execution latency or token cost? The paper reports success rates but does not analyze the overhead of continuous feedback loops, which likely invoke the LLM more frequently than static models, potentially impacting efficiency.

## Limitations
- Missing technical specifications for LLM model, prompt templates, and tool schemas prevent direct reproduction
- Evaluation lacks ablation studies isolating contributions of individual architectural innovations
- Centralized progress management may face scalability bottlenecks with larger agent teams
- Relative contribution of innovations uncertain without controlled baseline comparisons

## Confidence
- **High Confidence**: Architectural design principles and component interactions are clearly specified with explicit equations and design rationale
- **Medium Confidence**: Empirical results demonstrate strong performance but lack ablation studies or controlled baseline comparisons
- **Low Confidence**: Reproducibility limited by missing implementation details including exact LLM model, prompt templates, and tool definitions

## Next Checks
1. Implement Aime without the Dynamic Planner (using static planner) and measure performance degradation on GAIA to quantify planner's contribution to 77.6% success rate
2. Systematically enumerate all tools used across successful SWE-bench tasks to verify whether Actor Factory's bundling strategy prevents critical tool omissions or if gaps exist
3. Extend centralized Progress Management Module to handle 10× concurrent actors (simulating parallel subtasks) and measure latency and coherence degradation to validate scalability concerns