---
ver: rpa2
title: Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update
arxiv_id: '2509.23502'
source_url: https://arxiv.org/abs/2509.23502
tags:
- segmentation
- polyp
- kernel
- decoder
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for polyp segmentation
  in colonoscopy images, addressing challenges such as varying polyp shapes, sizes,
  and low-contrast boundaries. The core method integrates a Dynamic Kernel (DK) mechanism
  with a global Encoder Attention (EA) module.
---

# Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update

## Quick Facts
- arXiv ID: 2509.23502
- Source URL: https://arxiv.org/abs/2509.23502
- Reference count: 17
- Primary result: Proposes EA-DK framework achieving 92.93% Dice on Kvasir-SEG, outperforming state-of-the-art polyp segmentation methods

## Executive Summary
This paper introduces a novel framework for polyp segmentation in colonoscopy images that addresses challenges of varying polyp shapes, sizes, and low-contrast boundaries. The core innovation combines a Dynamic Kernel mechanism with a global Encoder Attention module, where the attention module aggregates multi-scale features from all encoder layers to generate a global context vector that initializes the dynamic kernel. This kernel is iteratively updated across decoder stages, allowing the model to refine segmentation predictions and accurately delineate complex polyp boundaries. The framework also employs Unified Channel Adaptation in the decoder to standardize feature dimensions, enhancing computational efficiency without compromising accuracy.

## Method Summary
The proposed framework uses a Res2Net-50 backbone with a novel Encoder Attention (EA) module that aggregates multi-scale features from all five encoder stages through self-attention to produce a global context vector. This vector initializes a Dynamic Kernel (DK) that is iteratively updated across four decoder stages using a gated mechanism. The decoder employs Unified Channel Adaptation (UCA) to standardize all features to 32 channels via 1×1 convolutions. The model is trained with combined BCE and Dice loss using SGD optimizer (lr=4e-4, momentum=0.9) with polynomial decay schedule on Kvasir-SEG and CVC-ClinicDB datasets with 80/20 train/test splits.

## Key Results
- Achieves Dice score of 92.93% on Kvasir-SEG and 94.40% on CVC-ClinicDB
- Outperforms state-of-the-art models including ResUNet and DoubleU-Net
- High recall (94.11% and 95.16%) and precision (93.24% and 94.50%) indicating accurate polyp detection with minimal false positives

## Why This Works (Mechanism)
The method works by leveraging global contextual information through the EA module to initialize a dynamic kernel that adapts to local features at each decoder stage. The multi-scale attention aggregation captures long-range dependencies across the entire feature hierarchy, while the gated kernel update mechanism allows the model to progressively refine segmentation boundaries by combining global context with stage-specific information. The UCA standardization ensures computational efficiency by reducing channel dimensionality while maintaining representational power.

## Foundational Learning
- **Encoder-decoder architecture**: Standard U-Net structure needed for segmentation tasks; verify by checking if basic U-Net baseline achieves reasonable Dice scores
- **Attention mechanisms**: Self-attention used to aggregate multi-scale features; quick check: visualize attention weights to ensure different encoder stages contribute differently
- **Dynamic kernels**: Learnable parameters updated during inference; quick check: monitor kernel magnitude stability during training
- **Channel adaptation**: 1×1 convolutions for dimensionality reduction; quick check: compare memory usage with and without UCA
- **Multi-scale feature fusion**: Combining features from different encoder stages; quick check: ablation showing impact of removing EA module
- **Gated update mechanisms**: Element-wise multiplication for controlled information flow; quick check: analyze gate activation patterns during training

## Architecture Onboarding
- **Component map**: Input → Res2Net-50 encoder → EA module (global context G) → DK initialization → Iterative DK updates (stages 4→1) → UCA decoder → Output
- **Critical path**: EA global context → DK initialization → Gated iterative updates → Final prediction
- **Design tradeoffs**: Global context provides comprehensive information but increases computational cost; UCA reduces channels for efficiency but may lose some detail
- **Failure signatures**: NaN losses from unstable kernel updates; overfitting on small datasets; attention collapse to uniform distribution
- **First experiments**:
  1. Implement EA module with attention visualization to verify meaningful multi-scale feature aggregation
  2. Debug DK update mechanism by monitoring kernel magnitude stability and gate activation patterns
  3. Conduct ablation studies comparing full framework against baselines with individual components removed

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Training duration (epochs/iterations) and validation frequency not specified, making exact reproduction difficult
- MLP architecture details for kernel generation are missing, affecting reproducibility of the dynamic kernel initialization
- Exact attention dimensions and number of heads in EA module not detailed, potentially impacting implementation consistency

## Confidence
- Method claims (EA-DK integration): Medium - conceptually sound but missing key hyperparameters
- Reported metrics (Dice, IoU, etc.): Medium - results appear competitive but exact reproduction uncertain
- Implementation details (architectural components): Medium - sufficient for faithful reimplementation with potential performance variations

## Next Checks
1. Implement the EA module with attention visualization to verify that pooled features from different encoder stages contribute meaningfully to the global context vector
2. Debug the DK update mechanism by monitoring kernel magnitude stability and gate activation patterns during training to prevent numerical instability
3. Conduct ablation studies comparing the full EA-DK-UCA framework against baselines with individual components removed to quantify each module's contribution to performance gains