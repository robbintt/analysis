---
ver: rpa2
title: Synthetic Data Generation for Phrase Break Prediction with Large Language Model
arxiv_id: '2507.18044'
source_url: https://arxiv.org/abs/2507.18044
tags:
- human
- phrase
- annotations
- data
- break
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first empirical study of LLM-generated
  synthetic data for phrase break prediction in TTS systems. It compares LLM annotations
  with traditional human audio/text annotations across English, French, and Spanish.
---

# Synthetic Data Generation for Phrase Break Prediction with Large Language Model

## Quick Facts
- arXiv ID: 2507.18044
- Source URL: https://arxiv.org/abs/2507.18044
- Reference count: 0
- Primary result: LLM-generated phrase break annotations match or exceed human annotation quality while reducing annotation costs

## Executive Summary
This paper presents the first empirical study of LLM-generated synthetic data for phrase break prediction in TTS systems. It compares LLM annotations with traditional human audio/text annotations across English, French, and Spanish. The approach uses few-shot prompting to generate consistent, high-quality phrase break annotations without requiring audio or extensive manual effort. Results show that LLM-generated annotations match or exceed human annotation quality in consistency (Krippendorff's α > 0.85), human evaluation scores, and F1 performance when used to train TTS models. The method generalizes across languages and reduces annotation costs while maintaining or improving model performance.

## Method Summary
The study uses GPT-4o mini to generate phrase break annotations through few-shot prompting, with k examples ranging from 2 to 256. Cross-lingual settings combine English and target language examples (total k=16). The model marks AP (Accent Phrase), IP (Intonation Phrase), SB (Sentence Boundary) using "/" and "#" symbols. Generated annotations are filtered and used to train MiniLM for downstream phrase break prediction. The approach is evaluated against human annotations (H-A from audio, H-T from linguistic experts) using Krippendorff's α, human scores, and macro-F1 metrics.

## Key Results
- LLM annotations achieve Krippendorff's α > 0.85, matching or exceeding human annotation consistency
- Cross-lingual transfer works effectively, with French and Spanish benefiting from balanced English+target language examples
- Downstream models trained on LLM-generated data achieve comparable or better F1 scores than those trained on human annotations
- Zero-shot prompting fails completely (AG = 0, α ≈ -0.28), demonstrating the necessity of few-shot examples

## Why This Works (Mechanism)

### Mechanism 1: Few-shot Pattern Alignment
- Claim: Providing task-specific examples aligns LLM outputs with human annotation conventions.
- Mechanism: Few-shot examples establish output formatting rules and implicit decision boundaries for phrase break placement, enabling the LLM to generalize from syntactic patterns in the examples to new sentences.
- Core assumption: The LLM's pre-training includes implicit prosodic knowledge that examples can activate.
- Evidence anchors:
  - [abstract] "uses few-shot prompting to generate consistent, high-quality phrase break annotations"
  - [section 4.2.1] "As k increases, these values progressively align with human annotations, ultimately converging with H-A and H-T"
  - [corpus] Limited direct corpus evidence for phrase break few-shot mechanisms; neighbor papers focus on other speech tasks.
- Break condition: Performance saturates at k≥64 (Section 5.2.1: "F1-score gains reaching saturation"), suggesting example quality matters more than quantity beyond a threshold.

### Mechanism 2: Text-to-Prosody Inference via Syntactic Cues
- Claim: LLMs can predict phrase breaks from text alone by inferring prosodic boundaries from syntactic structure.
- Mechanism: The model analyzes clause boundaries, punctuation, and syntactic constituency to predict where natural pauses would occur in spoken output, bypassing the need for audio data.
- Core assumption: Text-prosody mapping is sufficiently regular to be captured through syntactic patterns without acoustic information.
- Evidence anchors:
  - [section 4.2.2] "H-T & LLM... achieving α ≥ 0.85, indicating a highly reliable level of agreement"
  - [section 5.2.1] "LLM annotations aligned with H-T while maintaining reasonable agreement with H-A"
  - [corpus] Neighbor paper on SSML prosody control supports viability of text-based prosody manipulation.
- Break condition: May fail for languages or speaking styles where text-prosody correspondence is weaker or highly speaker-dependent.

### Mechanism 3: Cross-lingual Knowledge Transfer
- Claim: English few-shot examples improve phrase break annotation quality in other languages under specific conditions.
- Mechanism: The LLM transfers shared structural knowledge (e.g., clause-based phrasing principles) from English examples to target languages, augmented by target-language examples for language-specific patterns.
- Core assumption: Prosodic principles share cross-linguistic regularities that transfer via LLM representations.
- Evidence anchors:
  - [section 5.2.2] "French and English share structural similarities... human score peaked at En12 + X4"
  - [section 5.2.2] "Spanish, a syllable-timed language, differs from stress-timed English... excessive reliance disrupts prosodic alignment"
  - [corpus] Neighbor on multilingual language modeling (Zhang et al.) suggests structural similarities exist but vary by language pair.
- Break condition: Transfer degrades when prosodic typology differs significantly; Spanish performance drops with En16+X0 (human score: 70.90).

## Foundational Learning

- Concept: Phrase Break Types (AP, IP, SB)
  - Why needed here: The annotation scheme uses three break levels; understanding their distinction is essential for prompt design and evaluation.
  - Quick check question: Which break type marks a phonetic pause mid-sentence vs. a sentence boundary?

- Concept: Krippendorff's Alpha (Inter-annotator Reliability)
  - Why needed here: The paper uses α > 0.85 as a quality threshold; you must interpret what this metric signifies.
  - Quick check question: Would α = 0.5 indicate acceptable agreement for phrase break annotation?

- Concept: Language Typology (Stress-timed vs. Syllable-timed)
  - Why needed here: Cross-lingual transfer effectiveness depends on prosodic similarity between source and target languages.
  - Quick check question: Why might English→French transfer work better than English→Spanish for phrase breaks?

## Architecture Onboarding

- Component map: Raw text → Prompt assembly (system prompt + k few-shot examples + target sentence) → GPT-4o mini → Annotated text → Data filtering → MiniLM fine-tuning → Phrase break prediction inference

- Critical path: (1) Few-shot example curation → (2) Prompt template design → (3) Annotation generation → (4) Quality validation (agreement/human score) → (5) Downstream model training

- Design tradeoffs:
  - k value: Higher k improves alignment but plateaus (~64); marginal gains diminish.
  - Cross-lingual mix: Balanced (En8+X8) vs. English-dominant (En16+X0); optimal varies by language pair.
  - Ground truth choice: H-T (higher consistency) vs. H-A (captures acoustic variability).

- Failure signatures:
  - Zero-shot mode: Complete misalignment (AG = 0, α ≈ -0.28); break symbols misplaced or overused.
  - Excessive English examples for dissimilar languages: Spanish human score drops to 70.90 at En16+X0.
  - High k without quality filtering: Human score "slightly declining at higher k" suggests some examples introduce noise.

- First 3 experiments:
  1. Baseline validation: Generate annotations at k=16 for a target language; compute Krippendorff's α against available human annotations.
  2. Cross-lingual sweep: Test En4+X12, En8+X8, En12+X4, En16+X0 configurations to identify optimal transfer ratio.
  3. Downstream sanity check: Train MiniLM on synthetic vs. human annotations; compare macro-F1 on held-out test set to verify practical utility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based synthetic data generation effectively generalize to other speech prosody tasks, such as pitch accent or duration prediction, which require continuous rather than discrete labels?
- Basis in paper: [Explicit] The introduction notes that while LLMs are successful in NLP, the speech domain remains "relatively unexplored" despite having "more challenging data requirements."
- Why unresolved: This study exclusively validates the method on phrase break prediction (a classification task with discrete labels like AP, IP, SB), leaving continuous prosodic tasks untested.
- What evidence would resolve it: Applying the proposed synthetic generation method to train models for pitch or duration prediction and evaluating the Mean Squared Error (MSE) against natural speech.

### Open Question 2
- Question: How does the structural distance between the target language and English impact the effectiveness of cross-lingual synthetic data transfer?
- Basis in paper: [Explicit] Section 5.2.2 observes that "excessive reliance" on English examples "disrupts prosodic alignment" for Spanish (syllable-timed) more than for French, suggesting a linguistic dependency.
- Why unresolved: The study only evaluates English, French, and Spanish; it is unclear if the positive transfer holds for languages with drastically different syntax or prosody (e.g., tonal languages).
- What evidence would resolve it: Evaluating the F1 performance and human scores of models trained on LLM data for typologically diverse languages (e.g., Mandarin, Arabic) using English prompts.

### Open Question 3
- Question: Can LLMs be refined to capture the "natural speech variations" found in audio-oriented (H-A) annotations rather than just the syntactic consistency of text-oriented (H-T) annotations?
- Basis in paper: [Inferred] Section 4.2.2 notes that LLMs align strongly with H-T (α ≥ 0.85) but have lower agreement with H-A because H-A captures speaker traits like intonation and speaking rate.
- Why unresolved: The current prompting strategy instructs the LLM to act as a "linguistic expert," which prioritizes syntactic correctness over the variability inherent in spontaneous speech.
- What evidence would resolve it: Incorporating speaker-style or paralinguistic context into the prompt and measuring if the synthetic data distribution shifts to match the variance of H-A datasets.

## Limitations
- Few-shot example composition is underspecified - exact examples for different k values remain unknown
- MiniLM training hyperparameters (learning rate, batch size, optimizer) are not detailed
- Study focuses on three Western European languages; performance on typologically diverse languages is unknown
- Method relies on Kaldi-based MiniLM, so performance with other TTS architectures is unverified

## Confidence

**High Confidence**: Cross-lingual few-shot prompting consistently outperforms zero-shot approaches across all three languages (English, French, Spanish). The assertion that LLM-generated annotations match or exceed human annotation quality is well-supported by Krippendorff's α > 0.85, human evaluation scores, and downstream F1 performance.

**Medium Confidence**: The claim that LLM annotations "match or exceed" human annotation quality depends on which human reference is used (H-A vs. H-T). While H-T shows better consistency with LLM outputs, H-A represents more acoustically-grounded annotations that may capture nuances the LLM misses. The cross-lingual transfer mechanism works best between structurally similar languages, but the underlying reasons require further investigation.

**Low Confidence**: The mechanism by which LLMs infer prosody from text alone without acoustic input is not fully explained. While syntactic patterns provide strong cues, the model's implicit prosodic knowledge from pretraining is assumed rather than empirically validated. The optimal k value (~64) appears to plateau, but the relationship between example quality and quantity remains unclear.

## Next Checks

1. **Example Quality vs. Quantity Analysis**: Systematically vary both the number of few-shot examples (k) and their quality (measured by diversity of syntactic structures, prosodic patterns) to determine whether performance gains come from more examples or better examples. This would clarify whether the saturation point at k=64 represents a true ceiling or simply diminishing returns from lower-quality examples.

2. **Cross-Linguistic Typology Impact Study**: Extend the cross-lingual transfer experiments to include languages with different prosodic typologies (e.g., tone languages like Mandarin, morphologically rich languages like Finnish, or languages with different stress patterns). This would test the robustness of the transfer mechanism beyond stress-timed vs. syllable-timed languages and validate whether the LLM's prosodic inference generalizes across typological boundaries.

3. **Ablation of Pretraining vs. Few-shot Learning**: Train a baseline model on the same few-shot examples using traditional supervised learning (without LLM pretraining) to quantify how much performance depends on the LLM's pretraining versus the few-shot examples themselves. This would isolate the contribution of implicit prosodic knowledge from the effectiveness of the few-shot prompting mechanism.