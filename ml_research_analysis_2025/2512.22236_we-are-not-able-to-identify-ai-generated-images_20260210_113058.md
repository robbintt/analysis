---
ver: rpa2
title: We are not able to identify AI-generated images
arxiv_id: '2512.22236'
source_url: https://arxiv.org/abs/2512.22236
tags:
- images
- ai-generated
- real
- accuracy
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether humans can reliably distinguish
  real images from AI-generated ones. Participants classified 20 images (from a balanced
  set of 120 real and AI-generated portraits) as real or AI-generated.
---

# We are not able to identify AI-generated images

## Quick Facts
- arXiv ID: 2512.22236
- Source URL: https://arxiv.org/abs/2512.22236
- Authors: Adrien Pavão
- Reference count: 3
- Participants achieved 54% accuracy distinguishing real from AI-generated portraits

## Executive Summary
This study investigates whether humans can reliably distinguish real images from AI-generated ones. Participants classified 20 images (from a balanced set of 120 real and AI-generated portraits) as real or AI-generated. The dataset was carefully curated using CC12M real images and MidJourney-generated counterparts, with selection favoring highly realistic synthetic outputs. In total, 165 users completed 233 sessions, achieving an average accuracy of 54%—only slightly above random guessing. Performance showed little improvement across repeated attempts, and participants required an average of 7.3 seconds per image, with accuracy improving for slower responses. These results indicate that human judgment alone is insufficient to reliably detect AI-generated content, even in relatively simple portrait scenarios. The findings underscore the need for greater awareness, ethical guidelines, and technical detection tools as synthetic media becomes increasingly indistinguishable from reality.

## Method Summary
The study used a web-based experiment where participants classified 20 images as "Real" or "AI-generated." The dataset consisted of 120 images total (60 real from CC12M, 60 AI-generated via MidJourney v7 using CC12M text descriptions as prompts). Real images were filtered from CC12M using "a man"/"a woman" prompts with celebrities removed. AI outputs were manually curated for realism. Participants' response times and classifications were recorded, with unique user tracking for repeated sessions. Accuracy was calculated across all sessions, and response-time distributions were analyzed.

## Key Results
- Average classification accuracy was 54%, only slightly above random guessing
- Response times averaged 7.3 seconds per image
- Accuracy improved with longer inspection times, with correct responses exceeding incorrect ones around 15 seconds
- Performance showed limited improvement across repeated attempts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended deliberate inspection improves detection accuracy, but gains remain marginal.
- Mechanism: Slower responses allow systematic artifact scanning (lighting inconsistencies, texture anomalies) that rapid intuitive judgments miss. However, the signal-to-noise ratio is sufficiently weak that even careful analysis yields only slight improvement.
- Core assumption: Detectable visual artifacts exist in AI-generated images but are subtle enough to require sustained attention.
- Evidence anchors:
  - [abstract] "Response times averaged 7.3 seconds, and some images were consistently more deceptive than others."
  - [section] "The curves for correct and incorrect responses intersect around 15 seconds, suggesting that deliberate inspection is required to exceed chance performance."
  - [corpus] Roca et al. (2025) report consistent findings on human detection limits across image types.
- Break condition: If future models eliminate systematic artifacts entirely, extended viewing time would no longer provide detection advantage.

### Mechanism 2
- Claim: Human visual intuition provides negligible signal for distinguishing curated AI portraits from real photographs.
- Mechanism: Participants relied on heuristics (skin texture, lighting, background coherence) that are no longer diagnostic with state-of-the-art generators. The ~54% accuracy indicates detection cues are either absent or below perceptual threshold for most users.
- Core assumption: Participants were attempting genuine classification rather than guessing randomly.
- Evidence anchors:
  - [abstract] "average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts."
  - [section] "visual cues distinguishing real from AI-generated portraits are either extremely subtle or effectively absent for most users."
  - [corpus] Frank et al. (2023) found similar near-chance performance across audio, images, and text modalities.
- Break condition: If specific training enables humans to learn diagnostic cues not captured in this protocol.

### Mechanism 3
- Claim: Curation toward realism creates a detection task harder than random AI sampling but more ecologically valid.
- Mechanism: By manually selecting the most convincing outputs from multiple generations, the dataset mimics real-world sharing behavior where users post only their best results. This trades external validity for elevated difficulty.
- Core assumption: Posted synthetic media in the wild undergoes similar quality filtering.
- Evidence anchors:
  - [abstract] "carefully curated AI-generated counterparts produced with MidJourney."
  - [section] "In real online settings, people who share AI-generated portraits typically select the most realistic outputs rather than posting the first attempt."
  - [corpus] TrueFake dataset (arXiv:2504.20658) addresses similar real-world case scenarios with social media compression effects.
- Break condition: If randomly sampled outputs from future models achieve equivalent realism without curation.

## Foundational Learning

- Concept: Binary classification baselines
  - Why needed here: Interpreting 54% accuracy requires understanding that 50% represents random guessing on balanced two-class problems.
  - Quick check question: If a dataset had 70% real and 30% AI images, what baseline would a "always guess real" strategy achieve?

- Concept: Selection bias in evaluation datasets
  - Why needed here: The manual curation of "most realistic" AI outputs deliberately biases the dataset toward hard cases, affecting generalizability of accuracy claims.
  - Quick check question: Would accuracy on this curated dataset predict performance on randomly sampled AI outputs from the same model?

- Concept: Response-time dynamics in perceptual decisions
  - Why needed here: The 15-second crossover point where correct responses exceed incorrect ones reveals that detection requires deliberative rather than intuitive processing.
  - Quick check question: What does it mean when fast decisions show chance-level accuracy but slow decisions show above-chance accuracy?

## Architecture Onboarding

- Component map: CC12M sampling -> prompt extraction -> MidJourney generation -> manual curation -> balanced set (60 real/60 AI) -> Web-based 20-image sequence -> binary classification -> response time logging -> session linking -> Accuracy aggregation -> response-time distribution -> per-image difficulty ranking

- Critical path:
  1. Ensure balanced class distribution with matched prompts/subjects
  2. Prevent duplicate images within sessions (sampling without replacement)
  3. Link repeated sessions to same user for learning-curve analysis
  4. Exclude celebrity images that enable recognition-based detection

- Design tradeoffs:
  - Small curated dataset (120 images) enables difficulty control but limits statistical power and generalization
  - Manual curation increases ecological validity for "best-effort" synthetic media but reduces representativeness of all AI outputs
  - Binary forced-choice format simplifies analysis but loses confidence-signal information

- Failure signatures:
  - If participants systematically prefer one class (response bias), accuracy decomposition needed
  - If specific images show extreme accuracy (near 0% or 100%), may indicate artifacts or memorization
  - If response times cluster at very low values (<2 seconds), participants may not be engaging seriously

- First 3 experiments:
  1. Extend to video/audio modalities to test whether temporal artifacts improve human detection rates.
  2. Add confidence ratings (1-5 scale) to determine whether uncertainty correlates with incorrect classifications.
  3. Recruit demographically diverse participants to assess whether AI familiarity affects detection performance (current sample likely skewed toward tech-aware users).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific visual characteristics (e.g., lighting, framing, pose) determine whether an AI-generated portrait is inherently deceptive or detectable?
- Basis in paper: [explicit] The authors note that accuracy varied significantly by image (26% to 87%) and state that "a more systematic study of these factors could provide insight into what makes an image inherently deceptive."
- Why unresolved: This study only quantified the variance in difficulty across images without isolating the specific visual artifacts or aesthetic features causing the deception.
- What evidence would resolve it: A follow-up study using controlled pairs of AI images where specific parameters (lighting, background complexity) are systematically altered to measure their impact on user detection rates.

### Open Question 2
- Question: Does collecting confidence scores reveal reliable internal uncertainty in users who fail to distinguish AI-generated images?
- Basis in paper: [explicit] The authors suggest that "replacing binary answers with confidence scores could reveal whether users have reliable uncertainty estimates even when their classifications are incorrect."
- Why unresolved: The experiment forced a binary choice (Real/AI), potentially masking whether users were truly confident in their wrong guesses or simply operating at chance levels.
- What evidence would resolve it: Experimental data where participants rate their certainty on a scale (e.g., 1-5) for each classification, allowing for the analysis of calibration between confidence and accuracy.

### Open Question 3
- Question: How does detection accuracy degrade under real-world "scrolling-speed" viewing conditions compared to the deliberate inspection allowed in this study?
- Basis in paper: [explicit] The paper posits that in real-world contexts like social media, images are evaluated "in a fraction of a second," implying accuracy would be lower than the observed 54%.
- Why unresolved: Participants in this study averaged 7.3 seconds per image and performed better with longer inspection times; the study did not test time-constrained scenarios typical of casual browsing.
- What evidence would resolve it: A variant of the experiment enforcing strict time limits (e.g., under 1 second per image) to simulate social media feed scrolling.

### Open Question 4
- Question: Do human detection capabilities generalize to non-static modalities such as video, audio, or multimodal content?
- Basis in paper: [explicit] The authors list the focus on still images as a limitation and propose "extending the protocol to video, audio, or multimodal content" as a direction for future work.
- Why unresolved: It remains unclear if the temporal artifacts in video or auditory inconsistencies in audio make detection easier, or if the findings for static images hold true across other media.
- What evidence would resolve it: Application of the same classification protocol (real vs. AI) to synthetic video and audio datasets to compare performance benchmarks against the 54% image accuracy.

## Limitations
- Small sample size (165 users, 233 sessions) limits statistical power and generalizability
- Self-selection bias: participants voluntarily joined an AI-detection experiment, likely more AI-aware than general population
- Manual curation of "most realistic" AI outputs creates an artificially difficult task that may overestimate real-world detection difficulty

## Confidence
- High confidence: The 54% accuracy result and response-time patterns are directly measured from the experimental data
- Medium confidence: The claim that human judgment alone is insufficient for reliable detection follows logically from the data
- Medium confidence: The interpretation that AI generators have reached a level where human detection is near-random relies on the specific experimental setup

## Next Checks
1. Replicate the experiment with a larger, more diverse participant pool and multiple generative models to test whether the 54% accuracy threshold holds across different populations and AI systems.

2. Test detection performance on videos and audio where temporal artifacts might provide additional cues, or examine whether adding confidence ratings reveals systematic uncertainty patterns.

3. Compare performance on manually curated vs. randomly sampled AI outputs to quantify how curation affects detection difficulty and assess ecological validity.