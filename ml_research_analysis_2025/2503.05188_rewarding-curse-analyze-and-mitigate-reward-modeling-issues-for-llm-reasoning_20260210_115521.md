---
ver: rpa2
title: 'Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning'
arxiv_id: '2503.05188'
source_url: https://arxiv.org/abs/2503.05188
tags:
- performance
- reward
- wang
- reasoning
- mcts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic analysis of reward modeling issues
  in large language model (LLM) reasoning, identifying three key problems: performance
  degradation on simple questions, inverse long-tail phenomenon in distinguishing
  low-frequency negatives, and suboptimal performance with high sampling diversity.
  The authors mathematically model the RM-based inference process and conduct comprehensive
  experiments to validate these issues across different search strategies.'
---

# Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning

## Quick Facts
- arXiv ID: 2503.05188
- Source URL: https://arxiv.org/abs/2503.05188
- Reference count: 40
- Key outcome: OCTS improves reasoning accuracy by up to 3.2% vs baselines by addressing RM failure modes through early stopping, clustering selection, and direct path expansion

## Executive Summary
This paper systematically analyzes reward modeling issues in LLM reasoning, identifying three key problems: performance degradation on simple questions, inverse long-tail phenomenon for low-frequency negatives, and suboptimal performance with high sampling diversity. The authors mathematically model the RM-based inference process and conduct comprehensive experiments across different search strategies to validate these issues. Based on these findings, they propose Optimal Clustering Tree Search (OCTS), an algorithm that combines early stopping for simple questions, clustering-based selection to mitigate low-frequency negatives, and direct path expansion to control diversity. Extensive experiments demonstrate that OCTS improves reasoning accuracy by up to 3.2% compared to baseline methods across multiple datasets and models.

## Method Summary
OCTS is an inference algorithm that generates N samples from a policy model, checks for early stopping based on answer diversity (≤1 unique answers), clusters responses by final answer, sums RM scores within clusters, selects top-k clusters, then uses direct path expansion from selected prefixes to generate remaining steps. The method is evaluated against Self-Consistency, Best-of-N, MCTS, and Beam Search baselines on MATH-500, GSM8K, and OlympiadBench datasets using various policy and reward models.

## Key Results
- RM-based methods degrade on simple questions compared to self-consistency voting
- Inverse long-tail phenomenon: low-frequency incorrect answers receive highest RM scores among negatives
- Excessive sampling diversity degrades RM performance
- OCTS achieves up to 3.2% accuracy improvement over baselines
- Algorithm shows consistent gains across different datasets and model combinations

## Why This Works (Mechanism)

### Mechanism 1: Early Stopping for Simple Questions
- Claim: Bypassing the reward model on simple questions prevents performance degradation from RM misclassification.
- Mechanism: OCTS generates initial samples and counts unique answers. If answer diversity is low (≤1 unique answers in experiments), the algorithm stops early and returns the majority-voted answer, avoiding RM scoring entirely.
- Core assumption: Low answer diversity correlates with question simplicity and high model confidence.
- Evidence anchors:
  - [abstract] "RM can impair LLM's performance when solving simple problems"
  - [Section 3.2] "BoN performs worse on simple questions but better on difficult questions"
  - [corpus] Related work on self-consistency (Wang et al., 2023) supports majority voting as reliable for high-confidence cases.
- Break condition: If answer diversity threshold is misconfigured or model is systematically overconfident on hard questions, early stopping may return incorrect answers.

### Mechanism 2: Frequency-Weighted Clustering Selection
- Claim: Aggregating reward scores by answer cluster counteracts the inverse long-tail phenomenon where RMs over-score rare incorrect responses.
- Mechanism: Responses are clustered by final answer. Cluster scores sum individual RM scores within each cluster. Top-k clusters are selected, then the highest-scored path within each cluster is chosen. This implicitly weights by frequency.
- Core assumption: Correct answers cluster together with higher total frequency; low-frequency incorrect answers receive lower aggregated scores.
- Evidence anchors:
  - [abstract] "inverse long-tail phenomenon in distinguishing low-frequency negatives"
  - [Section 4.3] "incorrect answers with low occurrence frequencies receive the highest scores among all negative examples"
  - [corpus] Limited direct evidence; Self-Consistency of Internal Reward Models paper suggests ensemble approaches improve robustness.
- Break condition: If a low-frequency answer is actually correct (edge-case reasoning paths), clustering may suppress it.

### Mechanism 3: Direct Path Expansion for Diversity Control
- Claim: Reducing tree intermediate states limits exposure to high-quality negative samples that confuse RM scoring.
- Mechanism: Instead of step-by-step tree expansion (standard MCTS), OCTS uses selected paths as prefixes and generates all remaining steps at once. This reduces tree width/depth and limits diversity at intermediate nodes.
- Core assumption: Excessive intermediate branching generates adversarial-like negatives that exploit RM generalization gaps.
- Evidence anchors:
  - [abstract] "suboptimal performance with high sampling diversity"
  - [Section 5.2] "excessive diversity from both width and depth can degrade the RM's performance"
  - [corpus] PPO collapse in Long-CoT paper discusses related value estimation failures under high variance.
- Break condition: If task requires explicit intermediate verification (multi-step proofs), reduced branching may miss correct paths.

## Foundational Learning

- Concept: **Best-of-N (BoN) Inference**
  - Why needed here: Baseline RM-guided selection; generates N responses, selects highest-scored. Understanding its failure modes motivates OCTS design.
  - Quick check question: Given 32 samples with scores [0.9, 0.85, 0.82, ...], which does BoN select?

- Concept: **Outcome vs Process Reward Models (ORM vs PRM)**
  - Why needed here: Paper evaluates both; PRM scores reasoning steps, ORM scores final answers. Different failure patterns require different mitigation strategies.
  - Quick check question: If a reasoning path has correct steps but wrong final answer, how would ORM vs PRM score it?

- Concept: **Self-Consistency Voting**
  - Why needed here: Used as baseline and in OCTS early stopping; relies on answer frequency rather than RM scores.
  - Quick check question: With answers [A, A, B, A, C], what does self-consistency return?

## Architecture Onboarding

- Component map:
  Question → [Exploration: N samples] → [Difficulty Check: answer count] → if simple: [Majority Vote] → Output → if complex: [Clustering by answer] → [Cluster scoring: sum(RM scores)] → [Top-k cluster selection] → [Direct expansion from prefix] → [Final selection by RM] → Output

- Critical path: Exploration → Difficulty estimation (answer count) → Clustering → Direct expansion. Errors in difficulty estimation cascade to wrong strategy selection.

- Design tradeoffs:
  - Early stopping threshold: Lower threshold = more RM usage (higher cost, potential degradation on easy questions); higher threshold = more majority voting (faster, but may miss complex reasoning)
  - Cluster count k: k=1 for ORM, k=2 for PRM in experiments; higher k preserves diversity but increases compute
  - Expansion depth vs width: Direct expansion reduces diversity (good for RM) but may miss exploration paths

- Failure signatures:
  - Accuracy drops on easy questions → early stopping threshold too high
  - Accuracy plateaus despite increased sampling → inverse long-tail issue; check clustering weights
  - MCTS-style trees grow too deep → diversity explosion; verify direct expansion is active

- First 3 experiments:
  1. Reproduce Figure 2: Run BoN with Skywork RM on MATH-500 binned by difficulty. Verify SC outperforms BoN on Level 1-2 questions.
  2. Reproduce Figure 7: For highest-scored negative samples, histogram their answer frequencies. Confirm inverse long-tail (low-frequency negatives receive high RM scores).
  3. Ablate OCTS: Disable clustering (use individual path scores), measure accuracy drop on MATH-500 with Qwen2.5-3B + Skywork ORM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified RM failure modes—specifically performance degradation on simple problems and the inverse long-tail phenomenon—generalize to non-mathematical reasoning domains?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "we refrain from analyzing the problem of reward models in more reasoning tasks such as commonsense and logic because the LLM’s performance on these tasks is already sufficiently strong."
- Why unresolved: The study restricted its scope to mathematical reasoning benchmarks (MATH, GSM8K) to ensure robust analysis, leaving other domains unexplored.
- What evidence would resolve it: Replicating the analysis on benchmarks like CommonsenseQA or logical inference datasets to see if simple problems still cause RM performance drops.

### Open Question 2
- Question: Can the "inverse long-tail phenomenon" (where RMs score low-frequency negatives highly) be mitigated by modifying the Reward Model training data distribution?
- Basis in paper: [inferred] In Section 4.3, the authors hypothesize that the phenomenon occurs because "training data primarily consists of paired responses... covering only a restricted response distribution." However, they do not experimentally verify this by retraining RMs with broader data.
- Why unresolved: The paper analyzes the inference behavior but does not conduct experiments to alter the training phase to test the causality of this hypothesis.
- What evidence would resolve it: Training a new RM with a dataset specifically augmented with diverse negative responses and observing if the inverse long-tail effect diminishes during inference.

### Open Question 3
- Question: To what extent do issues inherent in Reward Model training, such as reward hacking, contribute to the inference-time failures analyzed in this paper?
- Basis in paper: [explicit] The authors note in the Limitations: "potential problems that may arise during the training of RM are not explored. This is because the training process involves significant randomness... making it challenging to pinpoint the key factors."
- Why unresolved: The paper focuses on mitigating inference-time symptoms (via OCTS) rather than diagnosing the root causes found in the training pipeline.
- What evidence would resolve it: An analysis correlating specific training dynamics or data attributes of the RMs with the severity of the observed inference-time issues.

## Limitations
- Limited to mathematical reasoning domains; generalization to other reasoning tasks is untested
- Inverse long-tail phenomenon lacks theoretical explanation for underlying mechanism
- Early stopping threshold is heuristic and may not generalize across different model sizes or domains
- Clustering mechanism assumes correct answers naturally group together, which may not hold for problems with multiple valid solution strategies

## Confidence
- **High confidence**: Performance degradation on simple questions (well-supported by controlled experiments comparing BoN vs Self-Consistency across difficulty levels)
- **Medium confidence**: Inverse long-tail phenomenon (empirically observed but mechanism not fully explained)
- **Medium confidence**: Diversity control via direct path expansion (theoretical justification present but limited ablation studies)

## Next Checks
1. **Difficulty threshold sensitivity analysis**: Vary the early stopping threshold (unique answers ≤ 1) across a range (0, 1, 2, 3) and measure accuracy impact on MATH-500 Level 1-2 questions to determine optimal threshold generalization.

2. **Clustering mechanism ablation**: Disable answer clustering and use individual path scores instead of cluster-weighted scores. Compare performance on MATH-500 to quantify the contribution of frequency weighting to the 3.2% accuracy gain.

3. **Low-frequency answer recovery test**: Intentionally generate correct but low-frequency answers in a controlled experiment (e.g., using diverse prompting strategies) and measure whether OCTS clustering suppresses them compared to baseline methods.