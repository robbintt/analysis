---
ver: rpa2
title: 'Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads
  in LLM Reasoning'
arxiv_id: '2512.10978'
source_url: https://arxiv.org/abs/2512.10978
tags:
- heads
- cognitive
- reasoning
- answer
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates the functional roles of attention
  heads in large language models (LLMs) through the lens of cognitive science. It
  introduces CogQA, a benchmark dataset that decomposes complex reasoning questions
  into subquestions annotated with eight distinct cognitive functions (e.g., retrieval,
  logical reasoning, math calculation).
---

# Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning

## Quick Facts
- arXiv ID: 2512.10978
- Source URL: https://arxiv.org/abs/2512.10978
- Reference count: 40
- One-line primary result: Identifies attention heads ("cognitive heads") responsible for distinct cognitive functions in LLMs and validates their importance through targeted interventions

## Executive Summary
This work systematically investigates the functional roles of attention heads in large language models (LLMs) through the lens of cognitive science. It introduces CogQA, a benchmark dataset that decomposes complex reasoning questions into subquestions annotated with eight distinct cognitive functions (e.g., retrieval, logical reasoning, math calculation). A multi-class probing method is then used to identify attention heads associated with each cognitive function across multiple LLM families. The analysis reveals that attention heads exhibit functional specialization—termed cognitive heads—characterized by sparsity, universality, and layered organization. These heads show clear clustering by function and hierarchical structure, and targeted interventions (masking or enhancing) demonstrate their critical role in supporting specific reasoning capabilities.

## Method Summary
The methodology involves constructing the CogQA dataset from existing reasoning benchmarks, then extracting attention head activations for subquestions. A multi-class probing classifier (2-layer MLP) is trained to predict cognitive functions from head activations, with importance scores computed via gradient × activation attribution. Cognitive heads are identified through thresholding and elbow selection. Targeted interventions validate causal roles through masking (scaling output by ε=0.001) and enhancement (shifting activations along correct-incorrect direction). The framework systematically analyzes sparsity, universality, and layered organization across eight cognitive functions.

## Key Results
- Attention heads show functional specialization, with fewer than 7% exceeding importance thresholds for any function
- Cognitive heads cluster by function and layer, with retrieval heads in middle layers and math heads in higher layers
- Masking cognitive heads causes significant performance degradation (near-zero accuracy for retrieval) while masking random heads shows only marginal effects
- Enhancing cognitive head activations improves reasoning accuracy, demonstrating their causal role in supporting specific cognitive functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-class probing can identify attention heads responsible for distinct cognitive functions
- Mechanism: For each subquestion annotated with a cognitive function, head activations are extracted from top-k semantically important tokens. A two-layer MLP classifier is trained to predict the cognitive function from these activations. Gradient × activation attribution then computes importance scores I(c)_j for each head, revealing which heads contribute most to each function class.
- Core assumption: The activation patterns of functionally specialized heads contain discriminative information that linear projections can extract.
- Evidence anchors:
  - [abstract] "By applying a multi-class probing method, we identify the attention heads responsible for these functions."
  - [section 3.2] "To interpret the contribution of individual heads to each function, we use a gradient-based attribution method... I(c)_j = E[∂ŷc/∂x̄j · x̄j]"
  - [corpus] Related work "Causal Head Gating" (arxiv 2505.13737) proposes similar causal attribution but via learned soft gates rather than post-hoc probing.

### Mechanism 2
- Claim: Cognitive heads are sparse and functionally clustered across layers
- Mechanism: Importance score distributions show that fewer than 7% of heads exceed significance thresholds for any function. Retrieval heads cluster in middle layers; math heads in higher layers. PCA on head ranking vectors reveals functional clustering—reasoning/inference/decision heads group together, math heads form distinct clusters.
- Core assumption: Functional specialization emerges from training and is reflected in consistent spatial patterns across architectures.
- Evidence anchors:
  - [section 4.1] "fewer than 7% of all heads have importance scores above 0.001 across the eight functions... Retrieval contains the highest proportion of salient heads (6.45% exceeding 0.01)"
  - [section 4.3] "The results reveal clear clustering: heads linked to reasoning, inference, and decision-making group closely, while those related to mathematical computation form a distinct cluster"
  - [corpus] "Investigating The Functional Roles of Attention Heads in Vision Language Models" (arxiv 2512.10300) reports similar modular organization in VLMs, suggesting cross-modal generality.

### Mechanism 3
- Claim: Cognitive heads causally support reasoning; masking them degrades performance while enhancing them improves accuracy
- Mechanism: Intervention via head ablation (scaling output by ε=0.001) suppresses contributions. Negative intervention on retrieval heads reduces retrieval accuracy to near-zero. Positive intervention shifts activations along the direction dir^l_h = E[x^l_h | correct] - E[x^l_h | incorrect], improving task performance.
- Core assumption: Importance scores identify heads with genuine causal roles, not just correlational patterns.
- Evidence anchors:
  - [section 4.2] "masking cognitive heads leads to a significant decline in performance, whereas masking an equal number of random heads results in only marginal degradation"
  - [section 4.4] "enhancing the activation of retrieval heads along their corresponding functional directions leads to improved performance on the retrieval task"
  - [corpus] "LLMs Process Lists With General Filter Heads" (arxiv 2510.26784) confirms causal mediation analysis can identify function-specific heads, supporting the intervention methodology.

## Foundational Learning

- Concept: **Multi-head attention and residual stream**
  - Why needed here: Head outputs x^m_l are value vectors projected into the residual stream; understanding how they compose is essential for extraction and intervention.
  - Quick check question: Given L layers and M heads per layer, what is the dimensionality of the full head feature set X̄ extracted per token?

- Concept: **Probing classifiers**
  - Why needed here: The entire detection framework relies on training an auxiliary classifier to predict cognitive functions from intermediate representations.
  - Quick check question: Why does gradient × activation attribution provide more interpretable importance scores than raw weights or attention patterns alone?

- Concept: **Chain-of-thought decomposition**
  - Why needed here: CogQA decomposes complex questions into subquestions, enabling fine-grained function annotation. The probing framework depends on this structure.
  - Quick check question: How does including preceding subquestions as context differ from standard single-turn probing, and what does this emulate?

## Architecture Onboarding

- Component map:
  CogQA dataset -> Head feature extractor -> Multi-class probing MLP -> Importance scoring -> Elbow selection -> Intervention validation

- Critical path: Dataset construction → Head extraction → Probing training → Importance scoring → Elbow selection → Intervention validation

- Design tradeoffs:
  - Single vs. multi-function annotation: Current design assumes one function per subquestion; real reasoning may engage multiple
  - Top-k token selection vs. full-sequence averaging: Top-k targets semantic content but may miss structural tokens important for syntax
  - Binary masking vs. graduated scaling: Hard ablation is cleaner for causal claims but less fine-grained than learned gating

- Failure signatures:
  - Probing accuracy <75% suggests features lack discriminative power—check token selection or augmentation
  - Random head masking produces similar degradation to cognitive head masking → importance scores not capturing causal roles
  - Positive intervention degrades performance → activation direction computed on noisy or mismatched data

- First 3 experiments:
  1. Replicate probing on held-out CogQA split; verify test accuracy matches Table 5 (~80-85% across models)
  2. Ablation study: Compare top-k token selection vs. first/last/meaning_first/full token strategies using Table 8 metrics
  3. Intervention sanity check: Mask top-30 retrieval heads vs. 30 random heads on Extractive_QA; expect near-zero vs. ~baseline accuracy per Table 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do attention heads exhibit polysemanticity, where single heads support multiple cognitive functions simultaneously or hierarchically?
- Basis in paper: [explicit] "We assume one head corresponds to one function, while in practice a head may support multiple functions, vary with context, or reflect hierarchical compositions. These complexities are not fully addressed in our current framework."
- Why unresolved: The current multi-class probing framework assigns discrete labels, which cannot capture many-to-many relationships between heads and functions.
- What evidence would resolve it: Multi-label probing experiments with heads allowed multiple function assignments, and context-dependent analysis of head behavior across diverse reasoning scenarios.

### Open Question 2
- Question: Would extending the cognitive function taxonomy beyond eight categories reveal additional specialized attention heads or reconfigure the identified hierarchical structure?
- Basis in paper: [explicit] "We focus on eight predefined cognitive functions, which, though representative, may not capture the full spectrum of LLM capabilities; future work could extend this taxonomy with finer-grained or emergent functions."
- Why unresolved: The current taxonomy may conflate distinct cognitive operations under broad labels, masking finer specialization patterns.
- What evidence would resolve it: Systematic ablation studies using expanded taxonomies (e.g., separating deduction from induction within logical reasoning), testing whether new function categories yield distinct head clusters.

### Open Question 3
- Question: Can dynamic head activation based on cognitive function detection improve reasoning performance compared to static enhancement?
- Basis in paper: [explicit] "Identifying cognitively relevant heads could inform model design, including dynamic head activation, improved chain-of-thought prompting, targeted fine-tuning, or modular architectures—directions we leave for future exploration."
- Why unresolved: The paper demonstrates positive intervention with static enhancement but does not test adaptive activation during inference.
- What evidence would resolve it: Experiments implementing real-time cognitive function detection followed by selective head enhancement, comparing against uniform enhancement baselines on complex multi-step reasoning benchmarks.

## Limitations
- Generalization across architectures: Findings from 4-8B parameter models may not transfer to larger frontier models or multimodal variants
- Functional annotation granularity: Single-function annotations per subquestion may mask complex interaction patterns between heads
- Intervention causality strength: Masking/enhancement shows performance changes but may not represent true causal mediation analysis

## Confidence
- **High Confidence**: Functional head identification through multi-class probing, sparsity of cognitive heads (<7% exceed thresholds), and basic clustering patterns across architectures
- **Medium Confidence**: Layered organization patterns (retrieval in middle, math in higher layers) and the effectiveness of targeted interventions, pending broader replication
- **Low Confidence**: Universality claims across all LLM families and the assumption that single-function annotations capture true cognitive complexity

## Next Checks
1. **Cross-architecture generalization test**: Apply the same methodology to GPT-4 family models and smaller 1-2B models to verify whether sparsity and layering patterns hold across the full parameter spectrum

2. **Multi-function probing experiment**: Modify the CogQA dataset to allow multiple cognitive function annotations per subquestion and retrain probing classifiers to detect head combinations rather than single functions

3. **Causal mediation validation**: Implement backdoor adjustment or do-calculus style interventions (e.g., conditioning on confounding variables) to distinguish true causal effects from spurious correlations in head function assignments