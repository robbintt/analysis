---
ver: rpa2
title: Basis Transformers for Multi-Task Tabular Regression
arxiv_id: '2506.06926'
source_url: https://arxiv.org/abs/2506.06926
tags:
- tabular
- data
- number
- values
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes basis transformers, a novel architecture specifically
  designed to handle the challenges of tabular data including textual information,
  variable numbers of columns, and unseen data without metadata besides column names.
  The key innovation lies in integrating sign-magnitude numeric representation into
  transformer-based architectures and transforming regression problems into multi-label
  classification problems, enabling scale-invariant training.
---

# Basis Transformers for Multi-Task Tabular Regression

## Quick Facts
- **arXiv ID:** 2506.06926
- **Source URL:** https://arxiv.org/abs/2506.06926
- **Reference count:** 40
- **Primary result:** Basis transformer achieves 0.338 improvement in median R² score with lowest standard deviation across 34 OpenML-CTR23 tasks

## Executive Summary
This paper introduces basis transformers, a novel architecture designed to address the unique challenges of tabular regression data. The key innovation integrates sign-magnitude numeric representation into transformer architectures and reformulates regression problems as multi-label classification tasks, enabling scale-invariant training. The model demonstrates significant performance improvements on the OpenML-CTR23 benchmark while maintaining a parameter count five times smaller than baseline approaches. Notably, it surpasses pretrained LLM baselines even when initialized from randomized weights, suggesting the architectural innovations are more impactful than pretraining in this domain.

## Method Summary
The basis transformer addresses tabular data challenges through a novel encoding scheme that handles both numeric and textual information within a unified transformer framework. By converting regression targets into classification labels, the model achieves scale invariance during training. The architecture leverages sign-magnitude representation to encode numerical values, allowing the transformer to process mixed data types effectively. This approach transforms traditional regression into a multi-label classification problem where each possible output value becomes a separate class, enabling the model to handle unseen numerical ranges through learned basis representations.

## Key Results
- Achieves 0.338 improvement in median R² score compared to baselines on OpenML-CTR23 benchmark
- Demonstrates lowest standard deviation across 34 evaluated tasks, indicating consistent performance
- Uses five times fewer parameters than the best-performing baseline while maintaining superior results

## Why This Works (Mechanism)
The effectiveness stems from addressing the fundamental mismatch between transformer architectures (designed for sequences) and tabular data (structured, heterogeneous). By encoding numerical values in sign-magnitude form, the model preserves magnitude information while maintaining differentiability. The regression-to-classification transformation enables scale-invariant training, preventing the model from overfitting to specific value ranges. The basis representation allows the transformer to generalize to unseen numerical values through learned basis vectors, effectively creating a continuous output space within a discrete classification framework.

## Foundational Learning

**Sign-Magnitude Representation**
- Why needed: Enables transformers to process numerical values while preserving magnitude information
- Quick check: Verify that positive and negative values with same magnitude produce similar embeddings

**Multi-Label Classification for Regression**
- Why needed: Allows scale-invariant training and prevents overfitting to specific value ranges
- Quick check: Confirm that model predictions remain stable when input scales change

**Basis Vector Decomposition**
- Why needed: Enables generalization to unseen numerical values through learned basis representations
- Quick check: Test model performance on values outside training range

## Architecture Onboarding

**Component Map:**
Column names and data → Sign-magnitude encoding → Transformer layers → Multi-label classification head → Regression output

**Critical Path:**
Input encoding → Attention mechanism → Basis vector transformation → Classification → Regression conversion

**Design Tradeoffs:**
- Regression-to-classification transformation: Loses exact precision but gains scale invariance
- Sign-magnitude encoding: More complex than simple normalization but handles mixed signs better
- Parameter efficiency: Achieves better performance with fewer parameters through architectural innovation

**Failure Signatures:**
- Poor performance on highly skewed distributions
- Difficulty handling categorical features with high cardinality
- Potential instability when regression targets span multiple orders of magnitude

**3 First Experiments:**
1. Test sign-magnitude encoding performance vs. standard normalization on synthetic data
2. Evaluate regression-to-classification transformation on a simple linear regression task
3. Compare basis vector generalization on numerical extrapolation tasks

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Evaluation limited to OpenML-CTR23 regression benchmark, raising generalizability concerns
- Sign-magnitude representation may not be optimal for all numerical encoding scenarios
- Regression-to-classification transformation could introduce approximation errors

## Confidence

**High Confidence:**
- Superior median R² scores on OpenML-CTR23 benchmark
- Lower parameter count with maintained or improved performance
- Effective handling of mixed numeric and textual data

**Medium Confidence:**
- Scale-invariant training generalizes beyond evaluated benchmark
- Performance advantage holds for tabular data from different domains
- True "unseen data" handling as claimed

**Low Confidence:**
- Surpassing pretrained LLM baselines from randomized weights based on single benchmark
- Universal superiority for all tabular regression tasks
- Practical significance of performance gains in real-world applications

## Next Checks
1. Cross-domain validation: Test on diverse tabular datasets from healthcare, finance, and scientific domains
2. Robustness to data structure changes: Systematically evaluate handling of missing, additional, and reordered columns
3. Scalability analysis: Test performance and efficiency on datasets 10x-100x larger than current benchmark