---
ver: rpa2
title: Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders
arxiv_id: '2505.15970'
source_url: https://arxiv.org/abs/2505.15970
tags:
- saes
- hierarchical
- vision
- imagenet
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the question of whether deep vision models
  encode hierarchical relationships found in the ImageNet taxonomy. To investigate
  this, the authors apply Sparse Autoencoders (SAEs) to the DINOv2 vision foundation
  model, treating SAEs as a probe to extract semantically meaningful, sparse features
  from model activations.
---

# Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.15970
- Source URL: https://arxiv.org/abs/2505.15970
- Reference count: 17
- Primary result: SAEs reveal increasing taxonomic structure in deeper layers of DINOv2

## Executive Summary
This paper investigates whether deep vision models encode hierarchical relationships present in the ImageNet taxonomy. The authors apply Sparse Autoencoders (SAEs) to DINOv2, a vision foundation model, treating SAEs as probes to extract semantically meaningful, sparse features from model activations. Using metrics like Lowest Common Hypernym (LCH) Height and Ontological Coverage, they quantify how well extracted features correspond to hierarchical groupings in WordNet-based ImageNet classes.

The study finds that early layers of DINOv2 contain little informative class-related structure, but later layers progressively encode hierarchical concepts. Layer 36, in particular, exhibits numerous SAE heads that activate on coherent, higher-order groups such as whales, sharks, or woodwind instruments, indicating strong taxonomic alignment. Relevancy maps confirm that SAE-identified features focus on semantically relevant image regions, demonstrating SAEs as an effective tool for revealing hierarchical semantic structure in vision models.

## Method Summary
The authors apply Sparse Autoencoders (SAEs) to the DINOv2 vision foundation model as probes to extract semantically meaningful, sparse features from model activations. They define two key metrics: Lowest Common Hypernym (LCH) Height, which measures the depth of shared semantic categories, and Ontological Coverage, which quantifies how well extracted features align with WordNet-based ImageNet classes. By analyzing SAE activations across different layers of DINOv2, they track how hierarchical structure emerges and becomes more pronounced in deeper layers.

## Key Results
- Early layers of DINOv2 contain minimal class-related hierarchical structure
- Layer 36 shows numerous SAE heads activating on coherent taxonomic groups (e.g., whales, sharks, woodwind instruments)
- SAE-extracted features demonstrate strong alignment with WordNet-based ImageNet hierarchy, confirmed by relevancy maps

## Why This Works (Mechanism)
SAEs work as effective probes because they learn to represent high-dimensional activations in a sparse, interpretable basis. When applied to vision models, they can identify neurons that activate on semantically coherent groups rather than individual classes. This sparsity allows for clearer interpretation of what concepts each feature represents, making it possible to measure hierarchical relationships using taxonomic metrics. The progressive emergence of hierarchical structure in deeper layers suggests that vision models build increasingly abstract and semantically organized representations as information flows through the network.

## Foundational Learning
- **Sparse Autoencoders**: Neural networks trained to reconstruct inputs through a bottleneck with sparsity constraints; needed to extract interpretable features from high-dimensional activations; quick check: verify reconstruction quality and activation sparsity
- **WordNet taxonomy**: Lexical database organizing words into hierarchical semantic categories; needed as ground truth for measuring hierarchical alignment; quick check: confirm ImageNet-1k class mapping to WordNet synsets
- **Ontological Coverage metric**: Measures proportion of dataset covered by SAE-extracted hierarchical features; needed to quantify how well features align with taxonomic structure; quick check: verify metric calculation across different layer depths
- **Lowest Common Hypernym (LCH) Height**: Measures semantic depth of shared categories; needed to assess granularity of hierarchical relationships; quick check: validate hypernym extraction for sample image pairs
- **Vision foundation models**: Large-scale pretrained vision models like DINOv2; needed as subjects for probing hierarchical structure; quick check: confirm model architecture and pretraining details
- **Relevancy maps**: Visualization showing which image regions activate SAE features; needed to validate semantic coherence of extracted features; quick check: verify saliency mapping methodology

## Architecture Onboarding

**Component Map**
DINOv2 -> SAE Probing -> Feature Extraction -> Taxonomic Analysis -> LCH/OC Metrics

**Critical Path**
Input images → DINOv2 forward pass → SAE training on activations → Feature activation analysis → Hierarchical metric computation → Relevancy map visualization

**Design Tradeoffs**
- SAE sparsity vs reconstruction accuracy: Higher sparsity improves interpretability but may lose information
- Layer selection for probing: Different layers capture different levels of abstraction
- Taxonomic granularity: Coarser vs finer hierarchical groupings affect metric sensitivity

**Failure Signatures**
- Low ontological coverage across all layers suggests poor alignment between model features and taxonomy
- Uniform activation patterns indicate lack of sparse, interpretable features
- Inconsistent relevancy maps suggest features don't correspond to semantically relevant regions

**First 3 Experiments**
1. Apply SAE probing to early layers (1-10) and verify minimal hierarchical structure
2. Train SAEs on layer 36 and identify specific taxonomic groups being captured
3. Generate relevancy maps for key SAE features to validate semantic coherence

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Narrow scope: Only evaluates DINOv2 on ImageNet, limiting generalizability to other architectures or vision tasks
- Taxonomy dependence: Relies on WordNet-based ImageNet classes, which may not capture all semantically meaningful groupings
- Correlation vs causation: Does not establish whether detected hierarchical features are functionally important for model behavior

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SAEs can extract sparse, semantically coherent features from vision models | High |
| Extracted features show increasing alignment with taxonomic structure in deeper layers | High |
| SAEs are a generally effective tool for revealing hierarchical semantic structure in vision models | Medium |
| Hierarchical features detected by SAEs are functionally important for model behavior | Low |

## Next Checks

1. Test SAE-based hierarchical feature extraction on diverse vision architectures (e.g., ConvNets, other transformers) and datasets to assess generalizability
2. Evaluate whether SAE-extracted hierarchical features improve or correlate with performance on semantic segmentation, retrieval, or few-shot classification tasks
3. Compare SAE findings with alternative probing methods (e.g., linear classifiers, attention visualization) to validate the robustness of detected hierarchical structures