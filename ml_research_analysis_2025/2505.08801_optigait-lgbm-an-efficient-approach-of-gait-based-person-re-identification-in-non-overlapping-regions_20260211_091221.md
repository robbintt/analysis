---
ver: rpa2
title: 'OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification
  in Non-Overlapping Regions'
arxiv_id: '2505.08801'
source_url: https://arxiv.org/abs/2505.08801
tags:
- gait
- data
- recognition
- learning
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes OptiGait-LGBM, a gait-based person re-identification
  system designed to address challenges such as uncontrolled outdoor environments,
  non-overlapping camera views, and varying illumination. The approach uses a skeletal
  model to extract gait features from landmark positions, reducing memory usage and
  computational cost.
---

# OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions

## Quick Facts
- arXiv ID: 2505.08801
- Source URL: https://arxiv.org/abs/2505.08801
- Reference count: 40
- Primary result: 76% test accuracy using skeletal landmark features and LightGBM classifier

## Executive Summary
OptiGait-LGBM proposes a gait-based person re-identification system for non-overlapping camera views in uncontrolled outdoor environments. The approach extracts 23 skeletal landmarks per frame using MediaPipe, computes 7 Euclidean-distance-based features, and applies camera correction factors to normalize measurements across views. A LightGBM classifier with Optuna-tuned hyperparameters achieves faster training and lower memory usage than Random Forest and CatBoost baselines while maintaining competitive accuracy. The system addresses challenges of varying illumination, camera distance, and limited computational resources through its skeletal model approach.

## Method Summary
The system extracts 23 body landmarks from video frames using MediaPipe Pose, then computes 7 gait features (height, hand length, leg length, step length, foot clearance, body wideness, shoulder-hip ratio) using Euclidean distances. Camera correction factors normalize height measurements across non-overlapping views. The dataset (RUET-GAIT) contains 9,974 frames from 4 subjects across 4 cameras. LightGBM classifier is trained with Optuna hyperparameter optimization, using 8,773 training rows, 2,194 validation, and 1,201 test samples.

## Key Results
- LightGBM achieved 76% test accuracy, outperforming Random Forest (74%) and CatBoost (73%)
- Training time: LGBM 3.05s vs Random Forest 3.32s vs CatBoost 15.83s
- Memory usage: LGBM 554MB vs RF 560MB vs CatBoost 556MB
- Per-class F1-scores ranged from 0.59 (Person 3 test) to 0.89 (Person 1 test)

## Why This Works (Mechanism)

### Mechanism 1
Skeletal landmark extraction provides appearance-invariant gait features that reduce memory and computational requirements compared to silhouette or video-based approaches. MediaPipe extracts 23 body landmarks per frame as coordinate pairs, which are transformed into 7 engineered features using Euclidean distance calculations. By operating on frame-level spatial data rather than sequential video frames, the system eliminates temporal storage requirements.

### Mechanism 2
Camera correction factors normalize height and derived features across non-overlapping camera views with varying distances and angles. A reference camera establishes baseline height measurements, and correction factors are computed as the ratio of average person height in each camera to the reference camera. These factors scale all spatial features to a normalized coordinate space.

### Mechanism 3
LightGBM's leaf-wise growth strategy with GOSS and EFB optimizations achieves faster training and lower memory usage than Random Forest and CatBoost for this classification task. GOSS retains all instances with large gradients while randomly sampling from small-gradient instances, focusing computational effort on harder-to-classify examples. EFB combines mutually exclusive sparse features, reducing histogram complexity.

## Foundational Learning

- **Concept: Gradient Boosting vs. Random Forest**
  - Why needed here: The paper positions LGBM against Random Forest and CatBoost; understanding why boosting iteratively corrects errors while bagging reduces variance explains performance differences
  - Quick check question: Given a model with high bias on gait features, would boosting or bagging provide greater improvement?

- **Concept: Pose Estimation Landmarks**
  - Why needed here: MediaPipe extracts 33 landmarks; understanding which joints are biomechanically relevant for gait (ankles, hips, knees) versus which are noise (facial landmarks) informs feature selection
  - Quick check question: Why might ear-to-heel distance be more stable than wrist-to-shoulder distance for person identification?

- **Concept: Non-overlapping Camera Re-identification**
  - Why needed here: The core problem assumes no continuous tracking between cameras; the system must match identities using only gait signatures without trajectory information
  - Quick check question: How does the re-identification challenge differ when cameras overlap versus when they don't?

## Architecture Onboarding

- **Component map:** Video footage → Frame extraction → MediaPipe Pose → 23 landmark coordinates → 7 Euclidean distance features → Camera correction scaling → LightGBM classifier → Person ID output

- **Critical path:** Camera calibration (reference height establishment) → Landmark detection reliability → Feature normalization → Classifier training

- **Design tradeoffs:** Non-sequential data reduces memory but loses temporal dynamics; 23 landmarks vs. full 33 reduces noise but may discard discriminative upper-body motion; LGBM vs. deep learning offers faster training but may underperform on larger datasets

- **Failure signatures:** Low recall on specific persons suggests overfitting or insufficient feature discriminability; frame discard rate increasing indicates poor lighting/occlusion conditions; correction factors >1.5 indicate extreme camera distance variation

- **First 3 experiments:** 1) Replicate 4-person classification on RUET-GAIT to verify ~76% test accuracy; 2) Train LGBM with subsets of 7 features to identify most discriminative ones; 3) Shuffle correction factors across cameras and measure accuracy drop

## Open Questions the Paper Calls Out
1. How can the OptiGait-LGBM model be adapted to accurately identify individuals engaged in dynamic movements such as running or jumping?
2. Can the proposed system effectively perform re-identification in crowded environments where occlusion and overlapping subjects occur?
3. Does the efficiency of the non-sequential spatial approach hold when scaling from 4 individuals to a large-scale, real-world surveillance database?
4. Can the integration of 3D gait analysis improve the robustness of the re-identification process without negating the model's low computational cost?

## Limitations
- Dataset limited to only 4 subjects, constraining generalizability to real-world scenarios
- Camera correction mechanism assumes linear scaling that may fail under significant perspective distortion
- Non-sequential feature extraction discards temporal gait dynamics (cadence, velocity, acceleration)
- MediaPipe pose estimator reliability untested in extreme outdoor lighting and occlusion conditions

## Confidence
- High confidence: Efficiency claims (training time, memory usage) comparing LGBM against Random Forest and CatBoost
- Medium confidence: Accuracy and F1-score results given limited dataset size
- Low confidence: Generalization of camera correction factors across different camera models and environmental conditions

## Next Checks
1. Test the model on a larger, more diverse gait dataset (e.g., OU-ISIR, CASIA-B) to assess real-world performance
2. Implement and compare a version using sequential features (velocity, acceleration, cadence) versus current frame-level approach
3. Create controlled experiments with known camera perspective distortions to test scalar correction factor approach under non-linear conditions