---
ver: rpa2
title: Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware
arxiv_id: '2512.22298'
source_url: https://arxiv.org/abs/2512.22298
tags:
- behavior
- temporal
- driver
- alert
- deployment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a real-time driver monitoring system that runs on low-cost
  edge hardware (Raspberry Pi 5 and Google Coral Edge TPU) and recognizes 17 driver
  behaviors in-cabin using a single camera. The pipeline combines a compact vision
  model, confounder-aware labeling, and a temporal decision head to reduce false positives
  and generate stable alerts.
---

# Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware

## Quick Facts
- arXiv ID: 2512.22298
- Source URL: https://arxiv.org/abs/2512.22298
- Reference count: 40
- Primary result: Real-time driver monitoring on low-cost edge hardware recognizing 17 behaviors at 16-25 FPS

## Executive Summary
This work presents a real-time driver monitoring system that runs on low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU) to recognize 17 driver behaviors in-cabin using a single camera. The pipeline combines a compact vision model, confounder-aware labeling, and a temporal decision head to reduce false positives and generate stable alerts. The optimized system achieves 16 FPS on Raspberry Pi 5 (under 60 ms latency) and 25 FPS on Coral Edge TPU (under 40 ms latency), supporting near real-time, reliable monitoring in diverse driving conditions.

## Method Summary
The system uses a YOLOv8-cls medium backbone (~15.8M parameters) with a 17-class head, trained on >800,000 labeled frames with driver-disjoint 80/10/10 splits. Training employs focal loss with capped class weights (γ=1.5), Adam optimizer (lr=5e-4), and 50 epochs at 320×320 resolution. Post-training INT8 quantization with 200-500 calibration images enables deployment on edge hardware. A temporal decision head enforces persistence (K=25 frames, τ=0.75) and hysteresis (τoff=0.60, M=3) to suppress transient misclassifications and generate stable alerts.

## Key Results
- Achieves 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms)
- Achieves 25 FPS on Coral Edge TPU (under 40 ms latency end-to-end)
- Frame-level macro-F1 of 80.7% with temporal decision head reducing false alerts/min by 68.2%
- Event-level metrics show time-to-detect of 1.2 seconds for sustained behaviors

## Why This Works (Mechanism)

### Mechanism 1
Explicitly modeling visually similar behaviors as separate classes reduces systematic false positives that would otherwise inflate nuisance alerts. The 17-class taxonomy includes both safety-critical behaviors AND their visual confounders (e.g., "Makeup/Hand on Hair" and "Control Panel/GPS") as distinct classes, providing the classifier alternative explanations for ambiguous hand-to-face and hand-to-console postures. This assumes the dominant source of nuisance alerts is confusion between distinct but visually similar activities, not fundamental recognition failures on target behaviors.

### Mechanism 2
Requiring sustained confidence over a multi-frame window suppresses transient misclassifications while preserving genuine behavior events. Equation 4 enforces that class c must be both the arg-max AND above confidence threshold τ for K consecutive frames before alerting; hysteresis via lower release threshold (τoff = 0.60, M = 3 frames) prevents boundary jitter. This assumes safety-relevant behaviors (phone use, drowsiness) persist over longer durations than transient gestures/glances.

### Mechanism 3
Full-integer quantization with complete accelerator operator mapping enables stable low-latency inference on low-cost edge hardware. Post-training INT8 quantization using a calibration set spanning classes and lighting conditions; staged export pipeline (PyTorch→ONNX→TFLite→Edge-TPU) ensures all operators map to accelerator without CPU fallback that would introduce jitter. This assumes quantization-induced accuracy degradation is acceptable given latency/throughput gains; operator compatibility is the primary deployment bottleneck for embedded execution.

## Foundational Learning

- **Concept: Per-frame vs. Event-level evaluation**
  - Why needed here: The paper explicitly separates frame-level macro-F1 from event-level metrics (false alerts/min, fragmentation) because high frame accuracy doesn't guarantee usable alert streams; operational DMS depends on stable event boundaries
  - Quick check question: If a classifier achieves 95% frame accuracy but produces 50 fragmented alerts per minute, is it operationally useful for driver feedback?

- **Concept: Integer quantization and calibration**
  - Why needed here: INT8 quantization is essential for meeting latency targets on CPU-only edge hardware; poor calibration can cause accuracy collapse, particularly on rare/small-object classes
  - Quick check question: What happens to softmax outputs if calibration samples don't cover rare classes like "yawning" or "smoking"?

- **Concept: Persistence window sizing relative to FPS**
  - Why needed here: K = 25 frames at 16 FPS (Pi 5) = 1.56 seconds vs. K = 25 at 25 FPS (Coral) = 1.0 seconds; same K yields different real-time behavior across platforms
  - Quick check question: If deploying on a slower platform at 10 FPS, should K stay at 25 or scale proportionally to maintain equivalent time windows?

## Architecture Onboarding

- **Component map:** Capture/decode (camera I/O, ~6ms) → Preprocess (resize/normalize, ~4ms) → Per-frame inference (INT8 CNN, 22–38ms depending on platform) → Postprocess (class mapping + temporal gate, ~5ms) → Overlay/I/O (UI/logging, ~3–4ms)
- **Critical path:** Inference dominates latency (38ms on Pi 5 CPU, 22ms on Coral Edge-TPU per Table 3). Persistence gating is the dominant contributor to effective time-to-alert for sustained behaviors. Camera capture and I/O are non-negligible (~6ms + ~3–4ms), making inference-only benchmarks misleading for end-to-end system planning.
- **Design tradeoffs:** Input resolution: 320×320 chosen as balance between small-object sensitivity and throughput; higher resolutions showed diminishing macro-F1 gains with substantial latency penalty (Figure 3). Persistence window K: Longer windows reduce false alerts but increase time-to-detect; default K = 25 (~1s) is tunable. Taxonomy granularity: 17 classes improve confounder separation but require more labeled data and increase per-class sparsity.
- **Failure signatures:** Partial Edge-TPU mapping (CPU fallback) → unpredictable latency spikes destabilizing persistence logic. Quantization collapse on small objects (phones, cigarettes) → these classes become invisible if calibration under-represents them. FPS drop below persistence requirement → K frames no longer spans meaningful time window, causing alert fragmentation. High false-alert rate on confounder classes → indicates taxonomy gaps or insufficient confounder training examples.
- **First 3 experiments:** 1) Validate end-to-end timing on target hardware: Run full pipeline (capture→alert) with per-stage instrumentation; verify sustained FPS and p95 latency match reported values (~16 FPS Pi 5, ~25 FPS Coral, Table 5). 2) Ablate temporal decision head: Compare K = 1 (frame-only), K = 10, K = 25, and EMA smoothing baselines on held-out sequences; measure false alerts/min and fragmentation to isolate persistence gating benefit (replicate Table 4 methodology). 3) Test quantization robustness per-class: Compare FP32 vs. INT8 per-class F1 scores; identify classes (likely small-object: phone modes, smoking) with >5% degradation for targeted calibration enrichment.

## Open Questions the Paper Calls Out

### Open Question 1
How can the single-camera DMS architecture be extended to an Occupant Monitoring System (OMS) that efficiently perceives multiple occupants and robustly separates their concurrent activities? The current system focuses on a single subject (the driver) with a single-label formulation; moving to a full cabin context introduces occlusion-heavy, multi-subject complexities not addressed by the current model.

### Open Question 2
What is the optimal structured representation for "cabin-state" that captures temporal semantics—such as onset, offset, and stability—suitable for downstream decision layers? The current output focuses on event-level alerts; a structured state representation that aggregates occupant context over time for higher-level consumption is proposed as a future direction but not yet implemented.

### Open Question 3
How can stable OMS outputs be effectively mapped to hierarchical vehicle policies to trigger graded responses (e.g., deferring prompts vs. escalating warnings) in Agentic Vehicles? While the system provides the inputs (behavioral evidence), the policy layer that consumes this data to make context-aware decisions is identified as a necessary integration step for human-centered intelligence.

### Open Question 4
Can the system be adapted to handle simultaneous overlapping behaviors (multi-label) without violating the strict latency constraints of the current low-cost edge hardware? The computational budget on Raspberry Pi 5 and Coral Edge TPU necessitated a single-label approach; it is unclear if multi-label inference can run at the required ~16–25 FPS on these specific devices.

## Limitations
- The paper assumes public DMS datasets can substitute for the proprietary data without empirical validation of taxonomy mapping accuracy
- No ablation study explicitly isolates the confounder-aware label design benefit from other architectural choices
- Small-object class performance (phone, smoking) under INT8 quantization is not characterized per-class, creating potential blind spots

## Confidence

- **High Confidence**: End-to-end timing measurements and latency targets (Table 3, 5)
- **Medium Confidence**: Temporal decision head effectiveness on reducing false alerts (requires external replication)
- **Medium Confidence**: INT8 quantization stability across all 17 classes (calibration methodology described but not validated per-class)

## Next Checks
1. Profile the complete inference pipeline on Coral Edge-TPU to confirm 100% accelerator mapping and identify any CPU fallback causing latency spikes
2. Compare FP32 vs INT8 per-class F1 scores on held-out data to identify classes where quantization causes >5% degradation, particularly for small-object detection
3. Test temporal decision head performance across different FPS rates (10, 16, 25) to verify K=25 maintains consistent real-time behavior across platforms