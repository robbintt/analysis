---
ver: rpa2
title: A biological vision inspired framework for machine perception of abutting grating
  illusory contours
arxiv_id: '2508.17254'
source_url: https://arxiv.org/abs/2508.17254
tags:
- illusory
- perception
- visual
- grating
- contours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between deep neural networks (DNNs)
  and human perception in recognizing illusory contours, specifically abutting grating
  illusions. The authors propose a novel deep network called ICPNet, inspired by the
  visual cortex circuits.
---

# A biological vision inspired framework for machine perception of abutting grating illusory contours

## Quick Facts
- arXiv ID: 2508.17254
- Source URL: https://arxiv.org/abs/2508.17254
- Authors: Xiao Zhang; Kai-Fu Yang; Xian-Shi Zhang; Hong-Zhi You; Hong-Mei Yan; Yong-Jie Li
- Reference count: 40
- Key outcome: ICPNet achieves 89.39% top-1 accuracy on AG-MNIST vs 49.02% baseline

## Executive Summary
This paper addresses the gap between deep neural networks and human perception in recognizing illusory contours, specifically abutting grating illusions. The authors propose ICPNet, a novel deep network architecture inspired by visual cortex circuits, which incorporates multi-scale feature extraction, feedback modulation, and shape constraints to perceive illusory contours that don't exist in the physical input. ICPNet significantly outperforms state-of-the-art models on both AG-MNIST and AG-Fashion-MNIST datasets, demonstrating that biologically-inspired mechanisms can enhance machine perception of visual illusions.

## Method Summary
ICPNet is a four-stage deep network architecture that mimics the ventral visual stream (V1→V2→V4→IT). The network includes a multi-scale feature projection module for extracting features at different spatial scales, a feature interaction attention module that integrates feedforward and feedback features to propagate global shape context to local processing, and an edge fusion module that incorporates shape constraints through auxiliary edge detection. The model is trained jointly on classification and edge detection tasks, with the edge branch providing shape priors that guide the network to focus on foreground contours rather than background texture.

## Key Results
- ICPNet achieves 89.39% top-1 accuracy on AG-MNIST, compared to 49.02% for baseline models and much lower for other state-of-the-art architectures
- The three proposed mechanisms (feedback modulation, multi-scale receptive fields, shape bias) contribute additively, with the full ICPNet architecture showing significant improvements over individual components
- Performance degrades at specific grating intervals (I=8, I=12), suggesting interval-dependent perceptual biases that the authors partially mitigate through input distribution mixing

## Why This Works (Mechanism)

### Mechanism 1: Feedback Modulation for Global-Local Integration
- Claim: Feedback connections from higher to lower processing stages enable illusory contour perception by propagating global shape context to early feature extraction
- Mechanism: Features from later stages (simulating V4, IT) are upsampled and fused with earlier stage features (simulating V1, V2) via the Feature Interaction Attention Module (FIAM). This allows local edge detection to be modulated by global perceptual hypotheses, mimicking cortical feedback circuits where higher areas with larger receptive fields influence lower-area processing
- Core assumption: Illusory contours require integration of local discontinuity cues with global shape priors—a constructive perceptual process rather than pure feedforward extraction
- Evidence anchors: [abstract] "feature interaction attention module for integrating feedforward and feedback features"; [section 4.3, Table 3] "feedback modulation with global information contributes to illusory contour perception"; [corpus] "From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models"

### Mechanism 2: Multi-Scale Receptive Field Design
- Claim: Kernel sizes scaled approximately 2× between stages, combined with dilated convolutions, enable perception of illusory contours spanning different spatial extents
- Mechanism: The MFP block extracts parallel features at dilation rates r=2,4,6,7, summed and shuffled. Stage-wise kernel sizes follow biological RF scaling (V2 ≈ 2× V1, V4 ≈ 2× V2). This captures both fine-grained grating terminations and broader contour continuity signals
- Core assumption: Abutting grating illusions create illusory edges at multiple spatial scales that must be detected simultaneously
- Evidence anchors: [abstract] "multi-scale feature projection (MFP) module is designed to extract multi-scale representations"; [section 3.3] "multi-scale features is essential for abutting grating illusory contour perception"; [section 4.3, Table 3] Group 4 vs Group 3 shows significant gains at intervals 6, 10, 14 with MFP added

### Mechanism 3: Shape Bias via Auxiliary Edge Detection
- Claim: Joint training with an edge detection task imposes shape constraints that shift network attention toward foreground contours and away from background texture
- Mechanism: A frozen LVP-Net pre-trained on BSDS-VOC generates edge pseudo-labels. The Edge Fusion Module (EFM) merges side-outputs top-down to produce edge predictions, optimized via class-balanced cross-entropy. The shared backbone learns shape-prioritized representations that transfer to illusory contour perception
- Core assumption: Human shape bias—the tendency to prioritize form over texture—is causally relevant to illusory contour perception
- Evidence anchors: [abstract] "edge fusion module (EFM) injects shape constraints that guide the network to concentrate on the foreground"; [section 3.5] "edge detection task is introduced to provide shape constraints"; [figure 7] Grad-CAM visualizations show Group 6 (with EFM) focuses on foreground while Group 5 attends to background

## Foundational Learning

- Concept: **Ventral Stream Hierarchy (V1→V2→V4→IT)**
  - Why needed here: ICPNet's four-stage architecture directly maps to this biological hierarchy. Understanding why RF sizes increase and why feedback matters requires familiarity with cortical processing
  - Quick check question: Why would a neuron in IT (inferior temporal cortex) be better suited for representing the global shape of an illusory digit than a V1 neuron?

- Concept: **Dilated (Atrous) Convolution**
  - Why needed here: Both MFP and core blocks use dilated depthwise separable convolutions to expand receptive fields without parameter explosion
  - Quick check question: A 3×3 kernel with dilation rate r=3 has what effective receptive field size on the input?

- Concept: **Class-Balanced Loss for Edge Detection**
  - Why needed here: Edge pixels are vastly outnumbered by non-edge pixels; the loss function (Equation 5-7) must counteract this imbalance
  - Quick check question: In the edge loss formulation, what happens to gradient contributions from non-edge pixels when λ is increased?

## Architecture Onboarding

- Component map: Input → MFP → Stage 1 (GL→SL→IL) → Stage 2 (GL→SL→IL) → Stage 3 (GL→SL→IL) → Stage 4 (GL→SL→IL) → FIAM cascade → Classifier
- Critical path:
  1. Input (224×224) through MFP produces initial multi-scale representation
  2. Feedforward: Stage 1 → Stage 2 → Stage 3 → Stage 4 (each GL→SL→IL)
  3. Feedback: Stage 4 features upsampled → FIAM with Stage 3 → Stage 3 upsampled → FIAM with Stage 2 → etc.
  4. Bottom-up aggregation: IL outputs from all stages fused via FIAM cascade → classifier
  5. Parallel edge branch: IL outputs → EFM cascade → sigmoid → edge map

- Design tradeoffs:
  - Depthwise separable + dilated convolutions reduce parameters but may limit representational capacity vs. standard convolutions
  - Frozen edge detector provides stable supervision but cannot adapt to distribution shift
  - γ=0.01 chosen empirically; higher values overly constrain classification, lower values lose shape guidance
  - No data augmentation beyond horizontal flip to isolate architectural effects

- Failure signatures:
  - Accuracy collapse at intervals 8 and 12 across all configurations → interval-dependent perceptual bias
  - Steep accuracy drop when >40% of grating lines removed (Table 4) → insufficient inducer density
  - Grad-CAM shows background attention when EFM removed (Figure 7) → missing shape constraint
  - Random-chance accuracy for baseline and other SOTA models (Table 6) confirms standard architectures lack this capability

- First 3 experiments:
  1. Replicate ablation Groups 1-6 on AG-MNIST subset (intervals 6 and 10 sufficient) to verify component contributions; expect ~40% gap between Group 1 and Group 6 at I=6
  2. Generate Grad-CAM visualizations for Group 5 (no EFM) vs Group 6 (full) on 10 samples each from I=6 and I=10; quantify foreground vs background attention ratio
  3. Run the mixed-interval protocol from Figure 8 (I=10 at 0.7 probability, I=12 at 0.3) to confirm interval bias is adjustable through input distribution manipulation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ICPNet architecture, specifically its feedback connections and multi-scale projections, generalize to perceive other types of visual illusions beyond the abutting grating, such as Kanizsa figures or geometric illusions?
- **Basis in paper:** [explicit] The authors state in the conclusion that "Future research will focus on augmenting the ability of model to detect various illusory contours."
- **Why unresolved:** The current study strictly evaluates the model on abutting grating illusions (AG-MNIST and AG-Fashion-MNIST) and does not test other classes of visual illusions
- **What evidence would resolve it:** Evaluation of the pre-trained ICPNet on datasets containing diverse illusion types (e.g., illusory contours formed by pacmen, depth illusions) without architecture modification

### Open Question 2
- **Question:** Does the incorporation of biological feedback modulation and shape constraints in ICPNet transfer effectively to complex natural image understanding tasks, such as camouflaged object detection or occlusion-aware segmentation?
- **Basis in paper:** [explicit] The authors suggest the framework "holds the potential to inform visual model design in challenging tasks, such as camouflaged object detection, occlusion-aware segmentation, and underwater object detection."
- **Why unresolved:** The experiments are limited to digit and fashion item classification on simple backgrounds; the model has not been validated on high-diversity natural image datasets like COCO or ImageNet
- **What evidence would resolve it:** Benchmarking ICPNet or its modules on standard natural image detection/segmentation benchmarks to see if the illusory contour perception capability translates to real-world robustness

### Open Question 3
- **Question:** What is the fundamental architectural or optimization constraint that causes the model's specific perceptual bias against abutting grating intervals of 8 and 12 pixels?
- **Basis in paper:** [inferred] The authors note a "relative degradation of our model at these two intervals" (8 and 12) and hypothesize it stems from an "intrinsic perceptual bias," which they partially mitigate but do not fully explain or resolve architecturally
- **Why unresolved:** While the authors demonstrate that mixing intervals can alleviate the bias, they do not identify the specific network component or feature extraction failure that creates this vulnerability in the first place
- **What evidence would resolve it:** A mechanistic interpretability study (e.g., analyzing the receptive field alignments or frequency responses at these specific stages) or a structural modification that equalizes performance across all intervals without data mixing

## Limitations

- The network architecture is highly specialized for abutting grating illusions and may not generalize to other illusion types or natural image tasks
- Use of a frozen pre-trained edge detector (LVP-Net) limits adaptability to different datasets and prevents the shape constraint from learning domain-specific features
- Anomalous performance drops at specific intervals (I=8, I=12) suggest potential overfitting to particular spatial frequencies rather than robust perceptual mechanisms
- Biological plausibility claims rely on indirect architectural analogies rather than neurophysiological validation

## Confidence

- High confidence: ICPNet's superior performance on AG-MNIST and AG-Fashion-MNIST datasets (validated through controlled ablation studies)
- Medium confidence: The three proposed mechanisms (feedback modulation, multi-scale receptive fields, shape bias) are logically consistent with architectural design and ablation results
- Low confidence: Direct biological correspondence claims between ICPNet components and cortical circuits, as these remain speculative without neurophysiological validation

## Next Checks

1. Conduct cross-dataset transfer experiments: Test ICPNet on non-gridded illusory contour datasets (e.g., Kanizsa figures) to verify generalization beyond abutting gratings
2. Implement dynamic edge supervision: Replace frozen LVP-Net with a trainable edge branch to assess whether adaptive shape constraints improve performance and domain adaptability
3. Perform interval sensitivity analysis: Systematically vary grating intervals beyond the tested range (I=4-16) to map the full perceptual capacity and identify whether the I=8/12 anomalies persist across scales