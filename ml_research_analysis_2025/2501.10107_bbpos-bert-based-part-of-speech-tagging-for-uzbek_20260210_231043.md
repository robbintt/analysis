---
ver: rpa2
title: 'BBPOS: BERT-based Part-of-Speech Tagging for Uzbek'
arxiv_id: '2501.10107'
source_url: https://arxiv.org/abs/2501.10107
tags:
- uzbek
- dataset
- language
- bert
- tags
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first BERT-based part-of-speech tagging
  models for Uzbek, achieving 91% average accuracy across Latin and Cyrillic scripts,
  outperforming both multilingual mBERT and existing rule-based taggers. The study
  introduces a new 500-sentence benchmark dataset annotated with Universal POS tags,
  the first publicly available resource for Uzbek POS tagging.
---

# BBPOS: BERT-based Part-of-Speech Tagging for Uzbek

## Quick Facts
- arXiv ID: 2501.10107
- Source URL: https://arxiv.org/abs/2501.10107
- Reference count: 15
- First BERT-based POS models for Uzbek, achieving 91% average accuracy across scripts

## Executive Summary
This work introduces the first BERT-based part-of-speech tagging models for Uzbek, a highly agglutinative Turkic language. The authors fine-tune monolingual BERT models for both Latin and Cyrillic scripts, achieving 91% average accuracyâ€”significantly outperforming multilingual mBERT and rule-based baselines. They create and release the first publicly available 500-sentence UPOS-annotated dataset for Uzbek. The study demonstrates that neural approaches can capture Uzbek morphology better than rule-based systems, though it highlights challenges with subword tokenization for complex inflected words and script-specific preprocessing requirements.

## Method Summary
The authors fine-tune two monolingual BERT models (TahrirchiBERT for Latin script, UzBERT for Cyrillic) for token classification with 17 UPOS tags. They use 5-fold cross-validation on a 500-sentence dataset (5,831 tokens) with 80/20 splits, evaluating via accuracy and F1-score using seqeval. Training uses lr=2e-5, batch_size=16, epochs=5, weight_decay=0.01 with transformers' Trainer. The dataset combines news and fiction sources and is made publicly available on Hugging Face.

## Key Results
- 91% average accuracy across Latin and Cyrillic scripts, outperforming multilingual mBERT (~83%) and rule-based baselines
- First publicly available 500-sentence UPOS-annotated benchmark dataset for Uzbek POS tagging
- Models capture morphological inflection patterns and context sensitivity, including intermediate POS changes through affixes

## Why This Works (Mechanism)
The success stems from leveraging pre-trained monolingual BERT models fine-tuned on task-specific data. These models have learned general syntactic patterns during self-supervised pre-training, which can be adapted to Uzbek POS tagging with limited labeled data. The context-sensitive embeddings capture morphological relationships that rule-based systems miss, particularly for highly inflected words where single tokens encode complex grammatical information. The UPOS tagset provides a universal framework that works across both scripts while maintaining linguistic validity for Uzbek.

## Foundational Learning

**Concept: Morphological Agglutination**
- Why needed here: Uzbek is highly agglutinative; a single word can carry the information of a full sentence in English. Understanding this is critical to interpreting tokenization issues and model behavior.
- Quick check question: How does the structure of "Kelmaganlardanmisiz?" differ from a typical English sentence, and what challenge does this pose for token-level taggers?

**Concept: BERT Fine-tuning**
- Why needed here: The entire experimental setup relies on fine-tuning pre-trained BERT models. One must understand that this is a supervised learning step adapting a general model to a specific task with a small dataset.
- Quick check question: Why is fine-tuning a pre-trained model often preferred over training a POS tagger from scratch for a low-resource language?

**Concept: Universal Dependencies (UD) / UPOS**
- Why needed here: The dataset and evaluation are based on the Universal POS tagset. Understanding the difference between this universal schema and language-specific grammars (like traditional Uzbek POS) is key to data creation and result interpretation.
- Quick check question: What is the primary advantage of using the Universal POS tagset for a low-resource language like Uzbek, as opposed to a traditional grammar-based tagset?

## Architecture Onboarding

**Component map:** Raw Text -> Script-Specific Tokenizer -> Pre-trained Encoder -> Classification Head -> UPOS Tag

**Critical path:** The success of this path is contingent on the quality of the tokenizer and the relevance of the pre-trained encoder's knowledge.

**Design tradeoffs:**
- Monolingual vs. Multilingual: Monolingual models (UzBERT, TahrirchiBERT) outperform multilingual mBERT (~91% vs ~83% accuracy) but require language-specific pre-training data and tokenizers
- Script (Latin vs. Cyrillic): Both scripts are actively used. A practical system may need parallel models or a transliteration step, as a single model cannot handle both without significant performance loss or complex preprocessing
- Tokenizer vs. Morphology: Standard subword tokenizers (BPE, WordPiece) are statistical. They may not align with linguistic morpheme boundaries, which limits the model's interpretability and ability to handle novel, highly inflected words

**Failure signatures:**
- Subword Explosion: Overly inflected words are split into many subword tokens. The paper notes this limitation, which can lead to incorrect tagging of complex words
- Tokenizer Mismatch: The Latin model fails on words with modifier letters (o', g', ') because the tokenizer treats them as delimiters. This is a pretokenization error, a critical failure mode for inference on non-standard text
- Rare Tag Blindness: The model achieves 0.0 F1-score for several UPOS tags (DET, INTJ, SCONJ, SYM, X) due to their extreme underrepresentation in the training data. The model cannot learn what it (almost) never sees

**First 3 experiments:**
1. Reproduce Baselines: Fine-tune bert-base-multilingual-cased and one monolingual model (e.g., UzBERT) on a single fold of the provided dataset. Compare accuracy to the paper's 91% (monolingual) and ~86% (mBERT) benchmarks to validate the setup
2. Tokenizer Stress Test: Create a small evaluation set of highly agglutinative words (like "Kelmaganlardanmisiz?"). Feed them through the fine-tuned model and analyze the subword tokenization vs. the predicted POS tags. Confirm the model can produce sensible tags for multi-morphemic words despite the subword split
3. Failure Case Analysis: Identify all instances of the five failed POS tags (DET, INTJ, SCONJ, SYM, X) in the evaluation set. Hypothesize whether failure is purely due to data sparsity or if the tags are linguistically ambiguous. This directly addresses the stated limitation

## Open Questions the Paper Calls Out

**Open Question 1:** Can morphologically informed tokenizers replace BPE/WordPiece to better handle highly inflected Uzbek words as single tokens? [explicit] "this requires BERT models to be pre-trained using morphological or morphologically informed tokenizers rather than relying on subword tokenization methods like BPE and WordPiece"

**Open Question 2:** How much annotated data is needed for the five unlearned POS tags (DET, INTJ, SCONJ, SYM, X) to achieve reliable F1-scores above zero? [inferred] Models achieved 0.0 F1 on five tags due to low representation; dataset has only 16 DET, 11 INTJ, 9 SCONJ, 9 X, and 1 SYM tokens

**Open Question 3:** Would pre-training with standardized alphabet normalization for o', g', and ' significantly improve Latin Uzbek model performance? [explicit] Authors call for "pre-training monolingual Uzbek models that apply normalization rules to standardize the singular form of the above letters"

**Open Question 4:** How would a unified Uzbek POS tagset that captures morpheme-induced intermediate POS shifts affect annotation consistency and downstream utility? [explicit] "the success of neural models... could inspire the linguistic community to develop a unified and comprehensive POS tagset for Uzbek, one that considers how morphemes influence word-level POS shifts"

## Limitations

- Dataset size (500 sentences, 5,831 tokens) limits learning of rare POS tags and may not capture full complexity of Uzbek morphology
- Subword tokenization mismatches with linguistic morpheme boundaries, particularly problematic for highly agglutinative words
- Script-specific models require parallel training or transliteration preprocessing, creating deployment complexity

## Confidence

**High Confidence (95%+):** The 91% average accuracy benchmark for monolingual models is well-supported by the experimental design with 5-fold cross-validation and clear metrics.

**Medium Confidence (70-95%):** Claims about context sensitivity and morphological inflection capture are supported by qualitative observations but lack systematic quantitative analysis.

**Low Confidence (below 70%):** Generalizability to truly unseen, highly complex Uzbek words remains uncertain due to acknowledged subword tokenization limitations and lack of systematic evaluation of out-of-vocabulary handling.

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate the fine-tuned models on an independent Uzbek text corpus from a different domain (e.g., social media, technical documents, or dialectal speech) to assess whether the 91% accuracy holds beyond the news and fiction training data.

2. **Rare Tag Recovery Experiment:** Design an augmentation experiment where the five underrepresented POS tags (DET, INTJ, SCONJ, SYM, X) are artificially oversampled or weighted more heavily during training. Measure whether targeted data augmentation or class-weighted loss functions can improve F1-scores on these rare tags without significantly degrading overall accuracy.

3. **Transliteration and Script-Mixing Analysis:** Create a test set with mixed Latin and Cyrillic script text and evaluate whether transliteration preprocessing can enable a single model to handle both scripts effectively. Compare the performance of script-specific models versus a unified approach with transliteration preprocessing to quantify the practical deployment costs mentioned in the limitations.