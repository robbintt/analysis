---
ver: rpa2
title: 'MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary
  Team Medical Consultation'
arxiv_id: '2503.13856'
source_url: https://arxiv.org/abs/2503.13856
tags:
- medical
- patient
- consultation
- round
- doctor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MDTeamGPT, a self-evolving multi-agent framework
  for multi-disciplinary team medical consultations. The framework addresses challenges
  in medical MDT consultations such as long dialogue histories causing cognitive burden,
  lack of effective experience extraction, and limited knowledge generalization.
---

# MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary Team Medical Consultation

## Quick Facts
- arXiv ID: 2503.13856
- Source URL: https://arxiv.org/abs/2503.13856
- Reference count: 40
- Primary result: 90.1% accuracy on MedQA and 83.9% on PubMedQA with self-evolving dual knowledge bases

## Executive Summary
MDTeamGPT introduces a multi-agent framework that simulates multidisciplinary team consultations using specialized LLM agents to improve medical diagnosis accuracy. The system addresses three key challenges: cognitive burden from long dialogue histories, inability to extract experience from consultations, and limited knowledge generalization. By combining a residual discussion structure with consensus aggregation, role-specialized agents, and two complementary knowledge bases (CorrectKB and ChainKB), the framework achieves state-of-the-art performance on medical question-answering benchmarks while demonstrating effective self-evolution through experience accumulation.

## Method Summary
The framework employs a Primary Care Doctor to route patient cases to 3-5 relevant specialists from a pool of 8 medical roles. Agents engage in multi-round discussions where each specialist generates independent opinions, then refines them based on structured summaries from the Lead Physician. The Lead Physician compresses all opinions into four categories (Consistency, Conflict, Independence, Integration) after each round, and agents only reference the previous two rounds' summaries through a residual discussion structure. A Safety/Ethics Reviewer validates final outputs, while a Chain-of-Thought Reviewer routes successful cases to CorrectKB and unsuccessful cases with error reflections to ChainKB. Both knowledge bases use embedding similarity for retrieval, enhancing future consultations.

## Key Results
- Achieves 90.1% accuracy on MedQA and 83.9% on PubMedQA test sets using gpt-4-turbo
- Outperforms single-agent approaches (77.4%/75.3%) and other multi-agent baselines (83.7%/76.8%)
- Knowledge bases improve performance: CorrectKB alone achieves 87.3%/83.5%, combined with ChainKB reaches 90.1%/83.9%
- Cross-dataset generalization: KBs trained on one dataset improve performance on the other by 2-3%

## Why This Works (Mechanism)

### Mechanism 1: Residual Discussion Structure Reduces Cognitive Load
Limiting each specialist agent to only the previous two rounds of structured discussion summaries improves reasoning accuracy by reducing context noise. The lead physician compresses all specialist opinions into four categories after each round, and from round 3 onward, agents reference only these structured summaries rather than the full dialogue history. This filters redundant information while preserving divergent viewpoints.

### Mechanism 2: Dual Knowledge Bases Enable Self-Evolution via Contrastive Learning
Separately storing successful reasoning chains (CorrectKB) and failed reasoning with error reflections (ChainKB) improves generalization more than storing correct cases alone. After each consultation, the Chain-of-Thought Reviewer routes cases to either knowledge base based on outcome correctness, enabling contrastive learning from both successes and failures.

### Mechanism 3: Role-Specialized Agents with Structured Consensus
Assigning domain-specific roles to LLM agents and aggregating through a lead physician improves diagnostic accuracy over single-agent or unstructured multi-agent approaches. The Primary Care Doctor routes cases to relevant specialists, each eliciting domain knowledge latent in the base LLM, with consensus reached through structured synthesis.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Both knowledge bases and specialist agents rely on explicit reasoning chains
  - Quick check question: Can you explain why "Let's think step by step" improves LLM accuracy on reasoning tasks?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The framework retrieves top-K similar cases from CorrectKB and ChainKB using embedding similarity
  - Quick check question: How does embedding-based retrieval differ from keyword search, and what are its failure modes?

- **Multi-Agent Consensus Protocols**
  - Why needed here: The framework's convergence criteria (unanimous agreement or majority vote) mirror established group decision-making methods
  - Quick check question: What are the tradeoffs between unanimous consensus, majority voting, and weighted aggregation in multi-agent systems?

## Architecture Onboarding

- **Component map**: Patient Agent -> Primary Care Doctor -> Specialist Agents -> Historical Shared Pool -> Lead Physician -> Safety/Ethics Reviewer -> Chain-of-Thought Reviewer -> CorrectKB/ChainKB

- **Critical path**: Patient input → Primary Care Doctor assigns 3-5 specialists → Round 1: Specialists independently generate opinions → Lead Physician synthesizes → Store in H → Rounds 2+: Specialists refine using previous two rounds' summaries → Check consensus → Post-consensus: Safety Reviewer validates → CoT Reviewer stores to appropriate KB → Next patient: Retrieve similar cases from KBs to enhance prompts

- **Design tradeoffs**:
  - More specialists vs. efficiency: MedQA averages 3-5 specialists; more roles increase coverage but extend latency
  - Residual depth: Limiting to two prior rounds reduces noise but may discard useful long-term context
  - Knowledge base size: Performance stabilizes around 600 consultation rounds; smaller KBs may underfit, larger may introduce noise

- **Failure signatures**:
  - Consensus not reached after 10 rounds → falls back to majority vote; tie → random selection
  - ChainKB retrieval surfaces irrelevant error cases → prompt contamination
  - Lead Physician summaries miss key conflicts → agents converge on incorrect diagnosis
  - Weaker models (LLaMA3-8B) show steeper improvement slopes but lower absolute ceilings

- **First 3 experiments**:
  1. Baseline replication: Run single-agent GPT-4-turbo on MedQA subset with and without CoT to verify ~77-78% baseline
  2. Ablation by component: Disable one mechanism at a time on 100-case subset to confirm Table 2 patterns
  3. Cross-dataset generalization: Build KBs from MedQA training set, test on PubMedQA to expect 2-3% gains over no-KB baseline

## Open Questions the Paper Calls Out

- **Question**: Does incorporating advanced reasoning algorithms or external tools into the specialist agents significantly improve diagnostic accuracy?
  - Basis: Authors state agents are "defined quite simply" and suggest incorporating techniques like Tree-of-Thought or external tools
  - Why unresolved: Current implementation relies on base LLM's capabilities without structured reasoning workflows
  - What evidence would resolve it: Ablation studies comparing specialist agents with ToT prompting or external medical database access

- **Question**: How robust is the framework's generalizability when applied to a wider variety of medical datasets beyond MedQA and PubMedQA?
  - Basis: Paper notes experiments used only these two datasets and future work involves testing on broader ranges
  - Why unresolved: These datasets may share structural similarities that don't represent clinical note diversity
  - What evidence would resolve it: Evaluation on diverse datasets like MedMCQA, MIMIC-III discharge summaries, or clinical case reports

- **Question**: Can the framework maintain high diagnostic accuracy in real-world clinical settings with noisy data and time constraints?
  - Basis: Authors explicitly list "Testing in Real-World Scenarios" as a limitation
  - Why unresolved: Controlled experiments may not reflect hospital workflow complexities and resource constraints
  - What evidence would resolve it: Deployment in simulated or actual clinical environments to measure latency, cost, and diagnostic utility

## Limitations

- Prompt template fidelity is uncertain as Appendix A is marked "simplified," making exact reproduction difficult
- Residual structure benefit is untested - the paper doesn't compare against full dialogue history or different residual depths
- Knowledge base contamination risk exists as error reflections may introduce irrelevant patterns as KB scales
- Consensus threshold definition is vague - "all agents agree" lacks specification of strictness requirements

## Confidence

- **High Confidence**: Comparative results showing MDTeamGPT outperforming single-agent and baseline multi-agent approaches (Table 1)
- **Medium Confidence**: Dual knowledge base mechanism's contribution - while combined KB improves performance, ChainKB's modest contribution lacks independent verification
- **Low Confidence**: Residual discussion structure's specific benefit - claimed cognitive load reduction lacks ablation data

## Next Checks

1. **Ablation Study on Residual Depth**: Run the framework with residual depths of 1, 2, 3, and unlimited (full history) on a 100-case subset to quantify the specific benefit of limiting to two prior rounds versus alternatives.

2. **Error Reflection Quality Audit**: Manually sample 20 ChainKB entries to verify error reflections capture transferable reasoning patterns rather than superficial observations. Check for consistency and relevance across different medical domains.

3. **Cross-Dataset Generalization Stress Test**: Train KBs on MedQA, then test on PubMedQA and a third medical dataset (e.g., MedMCQA or USMLE-style questions from another source). Measure retrieval relevance and accuracy gains to validate true generalization versus dataset-specific memorization.