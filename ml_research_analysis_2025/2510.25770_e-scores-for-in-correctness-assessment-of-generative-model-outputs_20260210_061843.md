---
ver: rpa2
title: E-Scores for (In)Correctness Assessment of Generative Model Outputs
arxiv_id: '2510.25770'
source_url: https://arxiv.org/abs/2510.25770
tags:
- response
- e-scores
- responses
- distortion
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of assessing the (in)correctness
  of generative model outputs, particularly large language models, when correctness
  labels are unavailable at test time. While prior work uses conformal prediction
  with p-values to provide statistical guarantees, these methods are vulnerable to
  p-hacking when users adaptively choose tolerance levels post-hoc.
---

# E-Scores for (In)Correctness Assessment of Generative Model Outputs

## Quick Facts
- arXiv ID: 2510.25770
- Source URL: https://arxiv.org/abs/2510.25770
- Reference count: 40
- Authors: Guneet S. Dhillon; Javier González; Teodora Pandeva; Alicia Curth
- Primary result: E-scores provide statistical guarantees for post-hoc tolerance selection in assessing generative model output correctness, addressing p-hacking vulnerabilities of prior p-score methods

## Executive Summary
This work addresses the problem of assessing the (in)correctness of generative model outputs, particularly large language models, when correctness labels are unavailable at test time. While prior work uses conformal prediction with p-values to provide statistical guarantees, these methods are vulnerable to p-hacking when users adaptively choose tolerance levels post-hoc. To overcome this limitation, the authors propose e-scores—measures of incorrectness based on e-values—that maintain statistical guarantees under post-hoc tolerance selection. Theoretical analysis shows that e-scores control a post-hoc notion of error called size distortion, generalizing prior non-post-hoc guarantees. Experiments on two settings—mathematical factuality and property constraints satisfaction—demonstrate that e-scores reliably bound size distortion by 1, achieve lower error rates than p-values, and maintain competitive precision-recall trade-offs. The method also extends to broader response sets and applies to any generative model.

## Method Summary
The method computes e-scores by comparing a test response's correctness score against calibration data statistics. An oracle estimator provides correctness probabilities for (prompt, response) pairs. The method transforms these into non-conformity scores and computes e-scores using a formula that depends on the test response's score and the sum of maximum incorrect scores from calibration data. This approach maintains statistical guarantees even when users select tolerance thresholds after observing the scores, addressing the p-hacking vulnerability of prior p-score methods. The core assumption is exchangeability between calibration and test data.

## Key Results
- E-scores control size distortion by 1 under exchangeability, maintaining statistical guarantees for post-hoc tolerance selection
- Experimental results show e-scores achieve lower error rates than p-values while maintaining competitive precision-recall trade-offs
- The method works across different correctness types (factuality, constraint satisfaction) and scales to broader response sets
- E-scores provide computationally efficient filtering through constant-time scoring after calibration aggregation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the user selects a tolerance level $\alpha$ after observing model outputs (post-hoc), e-scores maintain a statistical guarantee on the error rate, unlike p-values which invalidate guarantees in this setting.
- **Mechanism:** E-scores are derived from e-values (betting scores). The method upper bounds "size distortion"—the expected ratio of the error indicator to the user's post-hoc $\alpha$. Because the expectation of an e-value is bounded by 1, the method theoretically ensures that the "price" of adaptively choosing $\alpha$ does not distort the error guarantee (Eq. 3).
- **Core assumption:** The calibration prompts and test prompts are exchangeable (i.i.d. or permutation invariant).
- **Evidence anchors:**
  - [abstract] "e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion."
  - [section 6.2] Theorem 1 states that under exchangeability, e-scores upper bound size distortion by 1.
  - [corpus] Weak support; related works focus on calibration (e.g., "Generalized Correctness Models") but do not explicitly discuss the "size distortion" metric for post-hoc selection.
- **Break condition:** If there is significant distribution shift between calibration and test data, exchangeability fails, and the size distortion bound ($\leq 1$) is not theoretically guaranteed.

### Mechanism 2
- **Claim:** E-scores provide validity guarantees even when the "oracle estimator" (a proxy model predicting correctness) is imperfect.
- **Mechanism:** The method does not require a perfect oracle. It computes a non-conformity score $f$ using an estimator $\hat{o}$. The e-score formula (Eq. 4) effectively compares the test response's "correctness" score against the maximum "correctness" score of known incorrect calibration responses ($f^*$). The statistical guarantee relies on the relative ranks/values rather than the absolute accuracy of $\hat{o}$.
- **Core assumption:** The estimator $\hat{o}$ provides a signal that correlates with correctness (high for correct, low for incorrect), though the theoretical bound holds under exchangeability regardless of approximation error (Sec 6.2).
- **Evidence anchors:**
  - [section 4] "despite the possible errors in the oracle approximation... our e-scores achieve our desideratum."
  - [section 5.3] "The choice of the oracle estimator makes a difference... QwenPRM achieves higher precision and recall."
  - [corpus] Related work "Generalized Correctness Models" validates that learning calibrated correctness predictors from historical patterns is feasible.
- **Break condition:** If the oracle estimator is adversarial (e.g., assigns high scores to incorrect responses), the filtered set will be empty or useless, though the "guarantee" (size distortion $\le 1$) theoretically holds (trivially, by excluding everything).

### Mechanism 3
- **Claim:** Converting oracle estimates into e-scores enables computationally efficient filtering that scales better with calibration data size than p-score methods.
- **Mechanism:** P-score methods typically require sorting or ranking against the entire calibration set for every test point. The e-score method (Eq. 4) replaces this with a summation over a specific statistic of the calibration set ($\sum f^*$). This aggregates the calibration data into a constant term, allowing for constant-time scoring per test response after a single aggregation pass.
- **Core assumption:** The aggregation statistic ($f^*$, the max score of incorrect responses) sufficiently summarizes the calibration distribution for the denominator.
- **Evidence anchors:**
  - [section 5] "e-scores compute a sum... This requires constant memory and time... amortized over all test prompt-response pairs."
  - [section 4] Eq. 4 shows the dependence on the sum of calibration statistics rather than ranks.
  - [corpus] No specific corpus evidence regarding the computational architecture of e-scores vs p-scores.
- **Break condition:** N/A (Computational claim).

## Foundational Learning

- **Concept: Exchangeability**
  - **Why needed here:** This is the core assumption (Theorem 1) required for the statistical guarantees to hold. It implies that the test data is drawn from the same distribution as the calibration data (or at least, the order doesn't matter).
  - **Quick check question:** If I use calibration data from a math benchmark (GSM8K) but test on creative writing prompts, is the exchangeability assumption valid?

- **Concept: P-hacking (Post-hoc Selection)**
  - **Why needed here:** The paper frames its primary contribution as solving the "p-hacking" vulnerability of prior work. Understanding this requires knowing why adjusting $\alpha$ *after* seeing results invalidates standard p-value guarantees.
  - **Quick check question:** Why does choosing $\alpha=0.01$ *after* seeing that all p-scores are $<0.01$ invalidate the "probability of error $\le \alpha$" guarantee of standard conformal prediction?

- **Concept: E-values**
  - **Why needed here:** E-scores are the reciprocal of e-values. Unlike p-values (probabilities), e-values are "betting scores" with expectation $\le 1$.
  - **Quick check question:** If an e-value is 10, what does that imply about the evidence against the null hypothesis (that the response is correct) compared to an e-value of 1?

## Architecture Onboarding

- **Component map:** Oracle Estimator ($\hat{o}$) -> Calibration Aggregator -> E-Scorer -> Filter
- **Critical path:** Generating the response -> Running the Oracle Estimator -> Computing the E-score using the pre-computed calibration sum -> User decides $\alpha$
- **Design tradeoffs:**
  - **P-scores vs. E-scores:** P-scores may achieve higher recall (include more correct responses) but break under post-hoc $\alpha$. E-scores guarantee size distortion $\le 1$ for any post-hoc $\alpha$ but tend to be more conservative (lower recall) to maintain that safety.
  - **Oracle Quality:** A better oracle (e.g., QwenPRM vs. MathShepherdPRM) directly improves the precision-recall trade-off, but the *guarantee* holds even with weak oracles (trivially via empty sets).
- **Failure signatures:**
  - **Size Distortion > 1:** Indicates exchangeability violation (distribution shift between calibration and test).
  - **Empty Filtered Sets:** Occurs if the e-scores are very high (oracle detects likely incorrectness) or $\alpha$ is set too low.
  - **High E-scores for Correct Responses:** Indicates a failing or inverted Oracle Estimator.
- **First 3 experiments:**
  1. **Verify Size Distortion:** Implement the worst-case analysis (Sec B.1) on a validation set to confirm the empirical mean size distortion is $\le 1$.
  2. **Baselines Comparison:** Compare e-scores vs. p-scores on a benchmark like ProcessBench (Sec 5.1), plotting Error vs. $\alpha$ to visualize how p-scores cross the identity line (error > $\alpha$) while e-scores stay below.
  3. **Ablation on Oracle:** Swap the Oracle Estimator (e.g., QwenPRM vs. a simple length-based heuristic) to measure the impact on the precision-recall curve while verifying the size distortion guarantee remains intact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can oracle estimators be trained or fine-tuned for specific use-cases to strengthen e-scores beyond using pre-trained models?
- Basis in paper: [explicit] In Section 7, the authors state: "Our experiments show that the choice of the oracle estimator matters. While we use pre-trained ones in our experiments (cf. Section 5), they could be trained for specific use-cases to strengthen the e-scores."
- Why unresolved: The paper demonstrates results using off-the-shelf pre-trained models (QwenPRM, MathShepherdPRM, UltraRM) without exploring custom training procedures that might better align with the e-score framework.
- What evidence would resolve it: Experiments comparing pre-trained vs. custom-trained oracle estimators on the same benchmarks, showing measurable improvements in size distortion control and precision-recall trade-offs.

### Open Question 2
- Question: What alternative post-hoc notions of error exist beyond size distortion, and how do they compare for assessing LLM correctness?
- Basis in paper: [explicit] In Section 7, the authors note: "while we consider size distortion as our post-hoc notion of error, other candidates exist, though not well studied. Koning [2024] discusses some alternatives, which could be investigated in the future."
- Why unresolved: The paper focuses exclusively on size distortion as the error metric for post-hoc guarantees, leaving other potential metrics unexplored.
- What evidence would resolve it: A systematic comparison of alternative post-hoc error metrics on the same experimental settings, evaluating their theoretical properties and empirical performance.

### Open Question 3
- Question: How does the choice of f transformation (Eq. 5) affect the practical performance of e-scores across different correctness types?
- Basis in paper: [inferred] The paper presents three transformation options and a combined approach (Eq. 6), using the combined version by default, but does not provide theoretical or empirical guidance on when each option is optimal.
- Why unresolved: While all transformations theoretically satisfy the size distortion guarantee, their practical precision-recall trade-offs may differ significantly depending on the correctness type (factuality vs. constraint satisfaction).
- What evidence would resolve it: Ablation studies showing performance of individual transformations across benchmarks, with analysis connecting transformation properties to correctness type characteristics.

## Limitations

- **Distribution shift risk:** The core theoretical guarantee (size distortion ≤ 1) depends critically on the exchangeability assumption between calibration and test data, which may fail in practice when distributions differ substantially.
- **Oracle quality dependence:** While the method claims robustness to imperfect oracles, the precision-recall trade-offs clearly depend on oracle quality, and an adversarial oracle could trivially achieve the size distortion bound by filtering everything.
- **Practical interpretability:** The size distortion metric, while theoretically sound, is less intuitive than traditional error rates for practitioners and provides a weaker guarantee than standard conformal prediction's "error rate ≤ α" when α is chosen pre-hoc.

## Confidence

**High confidence:** The theoretical framework for e-scores and size distortion is sound and mathematically rigorous. The proof that e-scores control size distortion under exchangeability is correct and builds on established e-value theory.

**Medium confidence:** The experimental validation shows e-scores work on two specific settings (mathematical factuality and property constraints), but the generality claim to "any generative model" remains unproven. The sample sizes and benchmark diversity are limited.

**Low confidence:** The computational efficiency claim (constant-time scoring vs. sorting-based p-scores) is stated but not empirically verified. No runtime comparisons or memory usage measurements are provided to support this architectural advantage.

## Next Checks

1. **Distribution shift stress test:** Systematically evaluate size distortion when calibration and test sets come from different domains (e.g., calibrate on GSM8K math problems, test on creative writing or code generation). Measure how quickly size distortion exceeds 1 as domain divergence increases.

2. **Oracle robustness boundary:** Create a controlled ablation study where the oracle estimator is progressively degraded (e.g., by adding noise, using weaker models, or inverting predictions) to identify the minimum oracle quality threshold where e-scores become practically useless despite maintaining the theoretical size distortion bound.

3. **Computational benchmarking:** Implement both e-score and p-score methods and measure actual runtime and memory usage across varying calibration set sizes (10² to 10⁶ examples). Verify the claimed constant-time advantage of e-scores and identify crossover points where each method becomes preferable.