---
ver: rpa2
title: Decoding Neighborhood Environments with Large Language Models
arxiv_id: '2505.08163'
source_url: https://arxiv.org/abs/2505.08163
tags:
- llms
- image
- accuracy
- prompt
- indicators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) to
  decode neighborhood environments using Google Street View images, addressing the
  challenge of scaling environmental assessments without extensive labeled training
  data. A YOLOv11 model was trained as a baseline, achieving 99.13% mAP50 and 96.3%
  F1 score for detecting six environmental indicators.
---

# Decoding Neighborhood Environments with Large Language Models

## Quick Facts
- arXiv ID: 2505.08163
- Source URL: https://arxiv.org/abs/2505.08163
- Reference count: 39
- Key outcome: LLM-based environmental indicator detection achieves 84-88% accuracy without training, with majority voting reaching 88.5%

## Executive Summary
This study demonstrates that large language models with vision capabilities can decode neighborhood environments from Google Street View images without requiring extensive labeled training data. The research compares a traditional YOLOv11 baseline (achieving 99.13% mAP50) against four different LLMs for detecting six environmental indicators. The results show that while supervised learning remains superior in accuracy, LLMs provide a viable zero-shot alternative that achieves over 88% accuracy when using majority voting across multiple models. The study also reveals significant performance variations based on prompt structure and language choice, with English prompts outperforming others by 15-20% in recall.

## Method Summary
The researchers collected 1,200 Google Street View images (640×640 pixels) from Robeson and Durham counties, NC, with four directional views per location, and manually labeled 1,927 objects across six environmental indicator classes. They trained a YOLOv11 Nano model as a baseline on a 70/20/10 train/validation/test split. For the LLM evaluation, they used parallel prompting with four different models (ChatGPT 4o mini, Gemini 1.5 Pro, Claude 3.7, Grok-2) and implemented majority voting across the top three models requiring at least two agreements. The study tested prompt engineering variations and evaluated performance across different languages.

## Key Results
- YOLOv11 baseline achieved 99.13% mAP50 and 96.3% F1 score for detecting six environmental indicators
- Gemini 1.5 Pro achieved highest individual LLM accuracy at 88%, with ChatGPT 4o mini at 84%
- Majority voting across top three LLMs improved accuracy to over 88%
- English prompts outperformed other languages by 15-20% in recall, with Chinese achieving only 1% recall for sidewalks
- Parallel prompting structure achieved 92% recall versus 80% for sequential prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained vision-language models can identify environmental indicators in street-level imagery without task-specific training, achieving 84-88% accuracy.
- Mechanism: LLMs with vision capabilities leverage massive pretraining on paired image-text data to associate visual features with semantic labels. When prompted with indicator names, the models retrieve learned visual-linguistic representations to classify presence/absence.
- Core assumption: The environmental indicators in the target domain are sufficiently represented in the pretraining distribution of the vision-language models.
- Evidence anchors:
  - [abstract] "LLMs such as ChatGPT and Gemini as tools for decoding neighborhood environments... without any training effort"
  - [section IV-C1] Gemini 1.5 Pro achieved 88% accuracy, ChatGPT 4o mini 84% using parallel prompts
  - [corpus] Weak direct corpus support for this specific mechanism; related work StreetLens addresses neighborhood assessment but focuses on human-centered AI agents rather than zero-shot LLM classification
- Break condition: If target indicators are highly domain-specific (e.g., local infrastructure variants not seen during pretraining), zero-shot performance may degrade substantially.

### Mechanism 2
- Claim: Majority voting across multiple LLMs improves accuracy by reducing individual model biases and errors.
- Mechanism: Each LLM has different training data, architectures, and failure modes. By aggregating predictions from the top three models and requiring at least two to agree, random errors are filtered while consistent signals are reinforced.
- Core assumption: LLM errors are partially uncorrelated—models fail on different instances rather than systematically failing on the same cases.
- Evidence anchors:
  - [abstract] "Majority voting across the top three LLMs improved accuracy to over 88%"
  - [section IV-C2] Majority voting achieved 88.5% accuracy vs. individual model accuracies of 84-88%
  - [corpus] No direct corpus evidence for LLM ensemble voting in environmental detection; this is a gap in related literature
- Break condition: If models share systematic biases (e.g., all struggle with single-lane road detection due to similar visual definitions), voting provides limited improvement.

### Mechanism 3
- Claim: Prompt structure and language significantly impact LLM performance on visual classification tasks, with parallel prompts and English language yielding 15-20% higher recall.
- Mechanism: Parallel prompts reduce context-switching overhead and maintain consistent visual attention. English prompts benefit from dominant representation in LLM pretraining corpora, enabling more reliable semantic-to-visual mapping.
- Core assumption: The prompt format determines how effectively the model retrieves and applies its pretrained visual-linguistic knowledge.
- Evidence anchors:
  - [abstract] "English prompts outperforming others by 15–20% in recall"
  - [section IV-C1] Parallel prompts achieved 92% recall (Gemini) vs. 80% for sequential prompts
  - [section IV-C3] Chinese prompts achieved only 1% recall for sidewalks vs. much higher for English
  - [corpus] Bareiß et al. [29] cited in paper confirms English prompts outperform other languages for NLI tasks
- Break condition: If non-English deployment is required without fine-tuning or few-shot examples, performance may be inadequate for production use.

## Foundational Learning

- Concept: Object Detection Metrics (mAP50, Precision, Recall, F1)
  - Why needed here: The YOLOv11 baseline establishes performance benchmarks using these metrics; understanding them is essential for comparing LLM outputs to supervised learning.
  - Quick check question: If a model has high recall (0.91) but low precision (0.49) for single-lane roads, what does this tell you about its error pattern?

- Concept: Zero-Shot vs. Few-Shot Learning
  - Why needed here: The paper evaluates LLMs in zero-shot mode (no labeled examples provided), contrasting with traditional supervised approaches requiring extensive labeling.
  - Quick check question: What is the tradeoff between zero-shot LLM inference (88% accuracy, no training) and supervised YOLOv11 (99% mAP50, requires labeling)?

- Concept: Vision-Language Model Alignment
  - Why needed here: Understanding how models like CLIP and GPT-4o learn joint embeddings of images and text explains why LLMs can "see" environmental indicators.
  - Quick check question: Why might a vision-language model struggle with fine-grained distinctions (e.g., single-lane vs. multi-lane road) even when it recognizes both concepts?

## Architecture Onboarding

- Component map: Google Street View API -> 640×640 images from 4 cardinal directions -> LabelMe annotation -> YOLOv11 Nano training -> LLM API calls -> Majority voting across top 3 models

- Critical path: 1. Image collection via GSV API (coordinates → 4-direction images) 2. Manual labeling with LabelMe (ground truth for evaluation) 3. YOLOv11 training (baseline benchmark) 4. LLM inference with parallel prompts 5. Majority voting for final predictions

- Design tradeoffs:
  - YOLOv11: Highest accuracy (99% mAP50) but requires labeling effort and GPU training
  - Single LLM: No training required, 84-88% accuracy, API costs per image
  - LLM Ensemble: ~88.5% accuracy, 3× API costs, reduced individual model bias
  - Prompt language: English optimal but limits global deployment equity

- Failure signatures:
  - Single-lane road detection: LLMs tend to classify partial roadway views as single-lane regardless of actual lane count (68% accuracy with voting)
  - Non-English prompts: Chinese prompts achieved 1% sidewalk recall—severe visual-linguistic alignment failure
  - Noise sensitivity: YOLOv11 accuracy drops to ~60% at low SNR (5-15 dB)

- First 3 experiments:
  1. Replicate parallel prompt evaluation on a sample of 50 images across all 4 LLMs to validate accuracy claims and understand per-indicator variance.
  2. Test temperature variation (0.1 vs. 1.0 vs. 1.5) on Gemini to confirm paper's finding that parameter tuning has minimal impact on classification accuracy.
  3. Implement majority voting ensemble on the same 50-image sample and compare individual vs. ensemble accuracy to quantify voting benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating multiple consecutive images or varied viewing directions significantly improve LLM detection accuracy for environmental indicators that are partially occluded in single frames?
- Basis in paper: [explicit] The authors state in the Discussion, "In the future, we will incorporate multiple consecutive images in different directions to improve performance, especially for indicators that may be partially occluded in single frames."
- Why unresolved: The current methodology relied solely on a single-frame image input for each location, limiting the model's ability to see around obstructions.
- What evidence would resolve it: Experiments comparing single-frame recall rates against multi-frame inputs for specific occluded indicators (e.g., sidewalks blocked by parked cars).

### Open Question 2
- Question: How can prompt engineering or fine-tuning strategies be optimized to close the 15–20% performance gap between English and non-English prompts (e.g., Spanish, Chinese) in environmental decoding tasks?
- Basis in paper: [explicit] The authors note that non-English prompts currently reduce accuracy due to imbalanced training data, and optimizing prompts for cross-lingual robustness "remains an open challenge."
- Why unresolved: The study observed the disparity but did not develop a solution to align visual cues with translated terminology effectively.
- What evidence would resolve it: A comparative study showing that specific few-shot prompting or regional fine-tuning can elevate non-English recall to levels comparable with English.

### Open Question 3
- Question: Do the accuracy gains from majority voting across multiple LLMs justify the associated increase in computational costs and API latency compared to single-model inference?
- Basis in paper: [inferred] While the authors demonstrate that majority voting improves accuracy to over 88%, they acknowledge it introduces "practical barriers such as computational costs and API latency."
- Why unresolved: The paper evaluates technical accuracy (F1/mAP) but does not provide a cost-benefit analysis regarding the financial or temporal efficiency of running multiple proprietary models simultaneously.
- What evidence would resolve it: A benchmark analysis measuring the resource expenditure (time/cost) per image relative to the marginal accuracy gain of the ensemble method.

## Limitations

- Geographic limitation: Single dataset from two North Carolina counties limits generalizability across different urban/rural contexts
- Single-lane road detection weakness: LLMs achieve only 68% accuracy for single-lane road classification due to visual-linguistic alignment challenges
- Language bias: English prompts outperform others by 15-20% in recall, raising equity concerns for global deployment
- No labeled dataset released: Prevents independent validation and benchmarking efforts

## Confidence

- **High confidence**: YOLOv11 baseline metrics (99.13% mAP50, 96.3% F1) - standard supervised learning results with established evaluation protocols
- **Medium confidence**: LLM accuracy claims (84-88% individual, 88.5% ensemble) - API-based evaluation introduces variability in model versions and response consistency
- **Low confidence**: Generalizability claims - single dataset location, specific indicator set, and English prompt bias limit broader applicability

## Next Checks

1. **Replicate single-lane road detection**: Test LLM performance specifically on single-lane vs. multi-lane road classification using images with clear lane markings to understand the 68% accuracy limitation
2. **Cross-linguistic validation**: Evaluate the same prompt structure in Spanish and French to quantify language performance gaps and identify mitigation strategies
3. **Geographic transferability test**: Apply the established LLM pipeline to a different urban context (different country or US region) to assess baseline performance decay and identify domain-specific adaptation needs