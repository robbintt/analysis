---
ver: rpa2
title: 'TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language
  Models'
arxiv_id: '2509.24803'
source_url: https://arxiv.org/abs/2509.24803
tags:
- reasoning
- series
- time
- tasks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Time series reasoning remains underexplored due to a lack of datasets
  requiring genuine reasoning and models with temporal priors. We introduce TSR-SUITE,
  a comprehensive suite of four reasoning-critical tasks covering perception, extrapolation,
  and decision-making, with 23K samples including 2.3K curated via human-guided annotation.
---

# TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models

## Quick Facts
- arXiv ID: 2509.24803
- Source URL: https://arxiv.org/abs/2509.24803
- Reference count: 40
- Primary result: Time series reasoning model achieving Top-2 across four tasks, with 40.6% improvement on causality discovery and 6% gains on forecasting validity

## Executive Summary
Time series reasoning remains underexplored due to a lack of datasets requiring genuine reasoning and models with temporal priors. This work introduces TSR-SUITE, a comprehensive suite of four reasoning-critical tasks covering perception, extrapolation, and decision-making, with 23K samples including 2.3K curated via human-guided annotation. Building on this, TimeOmni-1 is trained in two stages: supervised fine-tuning with hierarchical reasoning traces to inject temporal priors, followed by reinforcement learning with task-grounded rewards to refine reasoning. The model achieves Top-2 performance across all tasks, supporting a train-once, use-across-tasks paradigm for time series reasoning.

## Method Summary
TimeOmni-1 uses a two-stage training approach on a base Qwen2.5-7B-Instruct model. Stage 1 applies supervised fine-tuning on 2.3K human-curated hierarchical reasoning traces from TSR-SUITE, injecting temporal priors through structured decomposition patterns. Stage 2 refines reasoning via reinforcement learning using Group Relative Policy Optimization with task-specific rewards: binary exact-match for discrete tasks, exponential decay of MAE plus length bonuses for forecasting, and format enforcement. Joint training across all four tasks yields mutual gains through cross-task transfer, where precursor capabilities (perception, extrapolation) support downstream decision-making.

## Key Results
- Achieved Top-2 performance across all four time series reasoning tasks
- Improved causality discovery accuracy by 40.6% over GPT-4.1 (ID)
- Increased valid response rates by over 6% on event-aware forecasting
- Joint training yields mutual gains, supporting train-once, use-across-tasks paradigm

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Reasoning Trace Injection
Temporal reasoning priors are established through supervised fine-tuning on structured reasoning traces. Human-guided templates constrain the LLM into consistent decomposition patterns (e.g., length check → magnitude sanity → pattern matching → event alignment → elimination). SFT internalizes these templates as reusable priors. Without templates, LLMs produce inconsistent, generic math-style reasoning that fails to capture temporal dependencies.

### Mechanism 2: Two-Stage Curriculum (Priors → Refinement)
Reinforcement learning for time series reasoning only stabilizes after temporal priors are anchored via SFT. Direct RL on a base model yields negative or marginal improvements because the reward signal cannot distinguish genuine temporal reasoning from spurious patterns in the pretraining corpus. SFT provides stable initialization in the temporal knowledge space before RL exploration.

### Mechanism 3: Task-Grounded Reward Design
Different time series task types require distinct reward parameterizations with careful normalization. For discrete-output tasks, binary exact-match rewards (0/1). For sequence-output tasks, exponential decay of MAE + length-matching bonus. A format reward (0/1) enforces parseable outputs before evaluating correctness, preventing gradient signal corruption from malformed outputs.

### Mechanism 4: Joint Training Cross-Task Transfer
Joint training across perception, extrapolation, and decision-making yields mutual gains because precursor capabilities provide useful representations for downstream tasks. Perception (scenario understanding, causality discovery) and extrapolation (event-aware forecasting) build representations that support decision-making. Joint training shares these representations while maintaining task-specific specialization.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed: The entire TimeOmni-1 architecture builds on generating intermediate reasoning traces before final answers. Understanding CoT is prerequisite to grasping why hierarchical annotation templates matter.
  - Quick check: Can you explain why prompting an LLM with "let's think step-by-step" improves performance on multi-step arithmetic problems, even without additional training?

- **Concept: Policy Gradient / PPO / GRPO**
  - Why needed: Stage 2 uses Group Relative Policy Optimization (GRPO) to refine reasoning. You need to understand how policy gradients differ from supervised learning and why clipping/KL penalties stabilize training.
  - Quick check: Sketch the PPO objective. Why does the clipping mechanism prevent overly large policy updates?

- **Concept: Multi-Task Learning & Transfer**
  - Why needed: The "train-once, use-across-tasks" paradigm assumes capabilities transfer. Understanding when transfer helps vs. hurts is critical for diagnosing joint training failures.
  - Quick check: In multi-task learning, what is "negative transfer"? Give one scenario where adding a second task might hurt performance on the first.

## Architecture Onboarding

- **Component map:** Raw Time Series Data (10 domains) → Task Formulation (4 tasks → discrete/sequence outputs) → Hierarchical Annotation Pipeline → TSR-SUITE (23K samples, 2.3K human-curated) → Stage 1: SFT on CoT data → Stage 2: GRPO with task-grounded rewards → TimeOmni-1 (unified model)

- **Critical path:**
  1. Design human-guided reasoning templates per task (see Appendix D.1 for examples)
  2. Run LLM Analyzer with templates on raw QA pairs → retain correct solutions
  3. Route incorrect solutions to human reviewers → verify solvability → LLM Rewriter polishes into structured CoT
  4. Combine Step-1 + Step-2 CoT data → SFT training (1 epoch, lr=1e-5, max_seq_len=8192)
  5. Run GRPO from SFT checkpoint (3 epochs, lr=1e-6, KL-penalty=0.04, 8 rollouts per update)
  6. Evaluate on ID and OOD testbeds; check success rate before computing task metrics

- **Design tradeoffs:**
  - SFT data quantity vs. quality: 2.3K human-curated traces vs. 23K total. Paper shows even <1K high-quality traces significantly boost performance, but larger RL data (Stage 2) matters for generalization
  - Single-task vs. joint training: Joint training wins across all tasks but requires careful batch balancing to prevent one task from dominating gradients
  - Reward granularity: Binary rewards (discrete tasks) vs. continuous exponential decay (forecasting). Imbalanced reward scales can skew optimization

- **Failure signatures:**
  - Low Success Rate (SR <80%): Model not following instruction format; check format reward enforcement and prompt consistency
  - Over-thinking on simple tasks: Reasoning not calibrated; may indicate insufficient task difficulty discrimination in training data
  - OOD collapse (ID >> OOD gap >20%): Priors overfit to in-distribution patterns; consider domain augmentation or more diverse training data
  - Stage 2 regression (performance drops after RL): KL penalty too low or reward scaling unstable; verify SFT checkpoint quality before RL

- **First 3 experiments:**
  1. Ablate Stage 1: Skip SFT, apply RL directly to base model. Expect performance drop or negative gains (per Figure 6). Confirms prior-injection necessity.
  2. Ablate human-guided templates: Replace structured templates with generic "think step-by-step" prompts during annotation. Expect consistency drop (per Finding 2). Confirms template value.
  3. Single-task vs. joint training comparison: Train four separate models (one per task) vs. one unified model under identical compute budget. Expect joint training to match or exceed single-task performance on all tasks (per Table 2). Confirms transfer benefits.

## Open Questions the Paper Calls Out

- Does the "train-once, use-across-tasks" paradigm for time series reasoning scale to a broader set of tasks beyond the four studied, or does task interference emerge at larger scales?
- Can hierarchical CoT annotation quality be validated objectively, or does it inherently encode annotator biases about what constitutes valid temporal reasoning?
- Are the exponential decay reward functions for sequence-output tasks optimal, or do they introduce gradient artifacts that limit policy optimization?
- Does the two-stage training dependency (Stage 1 priors → Stage 2 RL) hold across different base model scales and architectures?

## Limitations

- TSR-SUITE dataset is not publicly released, limiting reproducibility
- Only tested on Qwen2.5-7B base model scale, leaving cross-scale generalization unknown
- Exponential decay reward functions lack ablation against alternative formulations
- No inter-annotator agreement metrics for hierarchical annotation quality validation

## Confidence

- **High**: Two-stage training dependency (Stage 1 → Stage 2) demonstrated with clear ablations
- **High**: Joint training mutual gains confirmed through systematic comparison with single-task training
- **Medium**: Task-grounded reward design effectiveness based on single formulation without alternatives tested
- **Medium**: Hierarchical annotation template impact supported by one consistency experiment
- **Low**: Cross-task transfer benefits beyond tested four tasks remain unproven
- **Low**: Scalability to larger model scales and broader task sets not demonstrated

## Next Checks

1. Run ablation comparing Stage 2-only vs Stage 1→2 training to confirm prior-injection necessity
2. Test joint vs single-task training under identical compute budget to verify transfer benefits
3. Implement format reward enforcement and validate success rate ≥80% before task metric evaluation