---
ver: rpa2
title: An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory
  Disease Diagnosis
arxiv_id: '2507.08050'
source_url: https://arxiv.org/abs/2507.08050
tags:
- data
- training
- learning
- respiratory
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a Privacy-preserving Federated Few-shot Learning
  (PFFL) framework to address the challenges of diagnosing respiratory diseases with
  limited labeled data while protecting patient privacy. The framework combines meta-learning
  via a novel Meta-Differentially Private Stochastic Gradient Descent (Meta-DPSGD)
  algorithm with federated learning to train diagnostic models across distributed
  medical institutions without centralizing sensitive data.
---

# An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis

## Quick Facts
- **arXiv ID:** 2507.08050
- **Source URL:** https://arxiv.org/abs/2507.08050
- **Reference count:** 40
- **Key result:** PFFL achieves >0.8 accuracy across privacy budgets with only 1.0% precision reduction at Îµ=16

## Executive Summary
This study introduces a Privacy-preserving Federated Few-shot Learning (PFFL) framework to diagnose respiratory diseases using limited labeled data while protecting patient privacy. The framework combines meta-learning via Meta-Differentially Private Stochastic Gradient Descent (Meta-DPSGD) with federated learning to train diagnostic models across distributed medical institutions without centralizing sensitive data. Meta-DPSGD incorporates differential privacy noise during training to prevent model inversion attacks while maintaining diagnostic accuracy. Experimental results demonstrate strong cross-modal and multi-disease diagnostic capabilities, significantly improving performance for data-scarce institutions by up to 59.5% in accuracy.

## Method Summary
PFFL implements a three-stage process: local meta-learning with privacy, parameter aggregation, and global model deployment. Each client runs Meta-DPSGD to learn on few-shot tasks while adding calibrated Gaussian noise to