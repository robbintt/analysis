---
ver: rpa2
title: Learning Compositional Functions with Transformers from Easy-to-Hard Data
arxiv_id: '2505.23683'
source_url: https://arxiv.org/abs/2505.23683
tags:
- gradient
- hop2
- have
- layer
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the learnability of a k-fold composition task,
  which requires computing an interleaved composition of k input permutations and
  k hidden permutations, and can be expressed by a transformer with O(log k) layers.
  On the negative front, the authors prove a Statistical Query (SQ) lower bound showing
  that any SQ learner that makes only polynomially-many queries to an SQ oracle for
  the k-fold composition task distribution must have sample size exponential in k.
---

# Learning Compositional Functions with Transformers from Easy-to-Hard Data

## Quick Facts
- arXiv ID: 2505.23683
- Source URL: https://arxiv.org/abs/2505.23683
- Reference count: 40
- Key outcome: A transformer with O(log k) layers can learn k-fold permutation composition efficiently when trained on easy-to-hard data, but requires exponential samples if trained only on the full k-fold task.

## Executive Summary
This work studies the learnability of k-fold composition tasks, where a model must compute an interleaved composition of k input permutations and k hidden permutations. The authors prove a Statistical Query (SQ) lower bound showing that learning this task from data containing only k-hop examples requires sample complexity exponential in k. However, they also show that this function class can be efficiently learned with polynomial sample complexity by gradient descent on an O(log k)-depth transformer when trained on easy-to-hard data (a curriculum) or a mixture of all difficulty levels. The results highlight the necessity and sufficiency of having both easy and hard examples in the training distribution for transformers to learn complex compositional tasks.

## Method Summary
The paper studies k-fold composition tasks where a model must compute σ_k ∘ π_k ∘ ... ∘ σ_1 ∘ π_1(x) given k input permutations σ and index x. The authors construct a transformer with L = log₂k + 1 layers that can express this task using fixed value matrices and zero-initialized key-query matrices. They prove an SQ lower bound showing exponential sample complexity is required when training only on k-fold data, then demonstrate polynomial sample complexity is achievable through curriculum learning (training on 1-hop, 2-hop, 4-hop, ... k-hop data sequentially) or mixed training (training on all hop lengths simultaneously).

## Key Results
- Proves an SQ lower bound: any SQ learner making polynomially many queries requires exponential samples to learn the k-fold composition task
- Shows O(log k)-depth transformers can express the k-fold composition task using a recursive attention pattern
- Demonstrates polynomial sample complexity is achievable via curriculum learning or mixed training with easy-to-hard data
- Establishes that easy examples provide crucial gradient signals that enable learning of harder compositional tasks

## Why This Works (Mechanism)

### Mechanism 1: Logarithmic Depth Enables Parallel Composition
- **Claim:** A transformer with O(log k) layers can express the k-fold composition task
- **Mechanism:** Each layer doubles the composition depth through parallel attention, with layer ℓ computing 2ℓ-hop compositions by attending to position (i+2ℓ-1, hop_{2ℓ-1}^i(j))
- **Core assumption:** Embedding dimension d = poly(Nk) is sufficiently large to store intermediate hop values
- **Evidence anchors:** Theorem 1 shows construction; Appendix A.1 details recursive layer construction
- **Break condition:** Non-power-of-two k requires larger embedding dimension Θ(k²N)

### Mechanism 2: Exponential Statistical Hardness Without Intermediate Supervision
- **Claim:** Learning k-fold composition from only k-hop examples requires exponential samples for SQ learners
- **Mechanism:** Function class contains nearly orthogonal functions; polynomial queries require tiny tolerance τ ≤ N^{-Ω(k)}, implying sample size n ≥ N^{Ω(k)}
- **Core assumption:** No intermediate hop labels are provided; learner is captured by SQ model
- **Evidence anchors:** Theorem 2 proves lower bound; Lemma 2 constructs orthogonal functions
- **Break condition:** Lower bound avoided if training includes intermediate hop examples

### Mechanism 3: Implicit Curriculum via Easy-to-Hard Data Distribution
- **Claim:** Training on easy-to-hard data enables polynomial sample complexity
- **Mechanism:** Gradient dynamics create automatic curriculum: early layers learn from all tasks, later layers only receive signal once earlier layers are trained
- **Core assumption:** Zero initialization, fixed value matrices, sufficient learning rate
- **Evidence anchors:** Theorems 3-4 prove learning guarantees; Section 5.3 shows gradient dynamics
- **Break condition:** Guarantee assumes specific architecture and initialization

## Foundational Learning

- **Concept: Statistical Query (SQ) Learning**
  - **Why needed here:** SQ framework proves computational-statistical gap; essential for understanding hardness result
  - **Quick check question:** Can you explain why an SQ lower bound (requiring many queries or tiny tolerance) suggests that gradient descent might need exponentially many samples for this task?

- **Concept: Attention-Only Transformer Architecture**
  - **Why needed here:** Theoretical construction and analysis depend on specific attention mechanisms
  - **Quick check question:** In a single-layer self-attention head, how do the key-query matrix (W_KQ) and output-value matrix (W_OV) determine which token a query position attends to and what information it copies?

- **Concept: Curriculum Learning**
  - **Why needed here:** Central to positive result; shows how easy examples enable learning hard tasks
  - **Quick check question:** What is the core idea behind curriculum learning, and how does it differ from training on the full target task distribution from the start?

## Architecture Onboarding

- **Component Map:** Embedding Layer (ϕ) → L = log₂(k) + 1 Attention Layers → Readout Layer (Ψ)
- **Critical Path:**
  1. Input encoding: Map (i, j, σ_i(j)) to block-structured embedding
  2. Layer 1: W_KQ^(1) learns to attend to (i, π_i(j)); W_OV^(1) copies hop^1 embedding
  3. Layer ℓ: W_KQ^(ℓ) attends to (i+2ℓ-2, hop_{2ℓ-2}^i(j)); W_OV^(ℓ) copies hop_{2ℓ-1}^i(j)
  4. Final readout: Ψ_L projects to predict hop_k^1(j) = f_π(σ, j)
- **Design Tradeoffs:**
  - Depth vs. Width: O(log k) depth requires poly(Nk) width; shallower networks need more width
  - Curriculum vs. Mixed Data: Curriculum is theoretically cleaner; mixed data is more practical
  - Fixed vs. Learned Value Matrices: Fixed matrices simplify proof but are unrealistic
- **Failure Signatures:**
  - Training only on k-hop data: Loss plateaus, model doesn't learn
  - Insufficient easy examples in mixed data: Learning may be slow or fail
  - Non-zero initialization of W_KQ: Gradient dynamics may not follow implicit curriculum
  - Learning W_OV from scratch: Gradient vanishes at zero initialization
- **First 3 Experiments:**
  1. Implement k-fold composition task; train transformer only on k-hop data; observe failure
  2. Implement curriculum learning; train stage by stage on 1-hop, 2-hop, 4-hop, ... k-hop; plot hop-specific losses
  3. Implement mixed data training; compare dynamics and performance to curriculum approach

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the learned model be distilled into a smaller dimensional model, or can a smaller model with trainable embeddings still learn the task efficiently? (Section 7)
- **Open Question 2:** What initialization schemes or data distributions enable learning of value matrices WOV given their vanishing gradient at zero initialization? (Section 7)
- **Open Question 3:** Does analysis extend to general k (not powers of two), requiring larger embedding dimension Θ(k²N)? (Section 7)
- **Open Question 4:** Is easy-to-hard data strictly necessary, or can alternative strategies suffice? (Inferred from gap between lower and upper bounds)

## Limitations

- Theoretical results rely on idealized assumptions (fixed value matrices, zero initialization, one-step updates) that may not hold in practice
- SQ lower bound implications for standard gradient descent are inferred rather than rigorously proven
- Mixed data theoretical guarantee assumes specific error tolerance; practical emergence of implicit curriculum not extensively validated
- Results are for synthetic task; direct applicability to real-world compositional problems requires further investigation

## Confidence

- **Claim Cluster 1: Transformer Architecture Can Express the Task (High):** Rigorous proof with explicit construction
- **Claim Cluster 2: Exponential Hardness Without Curriculum (Medium):** Strong theoretical result but direct implications for standard learning not proven
- **Claim Cluster 3: Curriculum/Mixed Data Enables Efficient Learning (Medium):** Detailed theoretical analysis but practical robustness not fully explored

## Next Checks

1. **Empirical Validation of the SQ Lower Bound:** Implement gradient-based learner (e.g., SGD on small MLP) and attempt to learn k-fold composition from k-hop data only; plot learning curve to verify plateau at high loss

2. **Robustness to Initialization:** Test curriculum learning with random initialization (Gaussian/uniform) instead of zero initialization; verify if model still learns the task

3. **Impact of Learning Value Matrices:** Modify implementation to learn W_OV from scratch (initialize to small values) instead of fixing; verify if model still learns composition task