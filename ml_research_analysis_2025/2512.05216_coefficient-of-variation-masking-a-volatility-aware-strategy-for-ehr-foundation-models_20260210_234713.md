---
ver: rpa2
title: 'Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation
  Models'
arxiv_id: '2512.05216'
source_url: https://arxiv.org/abs/2512.05216
tags:
- masking
- clinical
- cv-masking
- cv-based
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CV-Masking, a volatility-aware pretraining
  strategy for EHR foundation models that improves representation learning by prioritizing
  clinically uncertain, volatile laboratory tests during masked autoencoder training.
  The approach uses the coefficient of variation to assign masking probabilities,
  creating a natural curriculum that focuses model capacity on challenging biomarkers
  while maintaining learning signals for stable ones.
---

# Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models

## Quick Facts
- arXiv ID: 2512.05216
- Source URL: https://arxiv.org/abs/2512.05216
- Reference count: 17
- Improves EHR foundation model pretraining by 71% with CV-Masking approach

## Executive Summary
This paper introduces CV-Masking, a volatility-aware pretraining strategy for EHR foundation models that prioritizes clinically uncertain, volatile laboratory tests during masked autoencoder training. The approach uses the coefficient of variation to assign masking probabilities, creating a natural curriculum that focuses model capacity on challenging biomarkers while maintaining learning signals for stable ones. Experiments on 100 high-frequency laboratory tests from MIMIC-IV show CV-Masking achieves significant improvements in both reconstruction quality and downstream clinical prediction performance.

The method is implemented within a Value-Only Masked Autoencoder architecture that aligns with clinical workflows and is available as open-source. Results demonstrate 71% of laboratory tests with improved reconstruction (Cohen's d=0.73, p<0.000009), enhanced downstream clinical prediction performance (e.g., +0.031 AUROC for in-ICU mortality), and accelerates convergence by 50% compared to random masking.

## Method Summary
CV-Masking introduces a volatility-aware pretraining strategy for EHR foundation models that uses the coefficient of variation (CV) to determine masking probabilities during masked autoencoder training. The approach calculates CV for each laboratory test across patients, then assigns higher masking probabilities to tests with greater volatility. This creates a curriculum that prioritizes clinically uncertain, variable biomarkers while still providing learning signals for stable ones. The method is implemented within a Value-Only Masked Autoencoder architecture that focuses on the temporal dynamics of laboratory values without requiring additional clinical context.

## Key Results
- 71% of laboratory tests showed improved reconstruction quality with CV-Masking (Cohen's d=0.73, p<0.000009)
- Downstream clinical prediction improved, including +0.031 AUROC for in-ICU mortality prediction
- 50% faster convergence compared to random masking baseline
- 2.1× greater sensitivity to corrupted historical context, indicating deeper reliance on patient-specific temporal patterns

## Why This Works (Mechanism)
CV-Masking works by leveraging the coefficient of variation as a proxy for clinical uncertainty, assigning higher masking probabilities to volatile laboratory tests that are more challenging to predict. This creates a natural curriculum where the model first learns to reconstruct stable, predictable biomarkers before tackling more uncertain ones. The volatility-aware masking forces the model to develop richer representations for clinically ambiguous cases, improving its ability to capture meaningful patterns in patient data. By focusing pretraining capacity on the most uncertain aspects of clinical data, CV-Masking enables better generalization to downstream clinical prediction tasks.

## Foundational Learning
- Coefficient of Variation (CV): A normalized measure of dispersion that quantifies volatility relative to the mean, essential for identifying clinically uncertain biomarkers
  - Why needed: CV provides a scale-invariant way to compare volatility across laboratory tests with different units and magnitudes
  - Quick check: CV = σ/μ, where σ is standard deviation and μ is mean; values >1 indicate high volatility

- Masked Autoencoder Architecture: A self-supervised learning framework that predicts masked input values from observed context
  - Why needed: Enables pretraining on unlabeled EHR data by reconstructing missing values
  - Quick check: Input: [observed values, masked values]; Output: reconstructed values for masked positions

- Value-Only Temporal Modeling: Focusing pretraining on the temporal dynamics of laboratory values without requiring additional clinical context
  - Why needed: Aligns with clinical workflows where practitioners often work with laboratory trends
  - Quick check: Models learn patterns from sequences of values over time without external features

## Architecture Onboarding

**Component Map:**
Value-Only Masked Autoencoder -> CV-Masking Module -> Laboratory Test Sequence Input

**Critical Path:**
Laboratory test sequences → CV calculation → Masking probability assignment → Masked reconstruction → Learned representations

**Design Tradeoffs:**
- Simplicity vs. Expressiveness: Value-only approach sacrifices some context for computational efficiency and clinical alignment
- Volatility proxy choice: CV may not capture all clinically relevant uncertainty patterns
- Single dataset focus: Controlled evaluation vs. generalizability concerns

**Failure Signatures:**
- Overfitting to volatile tests at expense of stable ones
- CV miscalculation leading to incorrect masking priorities
- Temporal misalignment in laboratory test sequences

**3 First Experiments:**
1. Ablation study comparing CV-Masking against random masking on reconstruction quality
2. Downstream prediction performance comparison across multiple clinical tasks
3. Sensitivity analysis to CV threshold selection and volatility metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation restricted to 100 high-frequency laboratory tests from single dataset (MIMIC-IV)
- Coefficient of variation may not capture all clinically relevant uncertainty patterns
- Modest clinical effect sizes (+0.031 AUROC) that may vary across prediction tasks

## Confidence
- CV-Masking reconstruction improvements (High): 71% improvement rate with strong statistical significance (p<0.000009) and Cohen's d=0.73
- Downstream clinical prediction gains (Medium): Statistically significant but modest effect sizes that may vary across tasks
- 50% convergence acceleration (Medium): Demonstrated but dependent on specific training configurations

## Next Checks
1. Evaluate CV-Masking across diverse clinical prediction tasks including medication response prediction, disease progression modeling, and multi-modal outcomes to assess generalizability beyond mortality prediction
2. Test the approach on multiple EHR datasets from different healthcare systems to verify robustness to institutional data variations and population differences
3. Conduct ablation studies comparing CV-Masking against alternative volatility metrics (e.g., entropy-based measures, clinical uncertainty scores) to validate the choice of coefficient of variation as the optimal proxy