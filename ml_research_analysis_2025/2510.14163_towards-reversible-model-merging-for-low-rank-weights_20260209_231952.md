---
ver: rpa2
title: Towards Reversible Model Merging For Low-rank Weights
arxiv_id: '2510.14163'
source_url: https://arxiv.org/abs/2510.14163
tags:
- merging
- task
- low-rank
- each
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reversible Model Merging (RMM), a method
  that reframes model merging as constructing a compact basis from which original
  task-specific models can be reconstructed via linear combination, rather than collapsing
  all models into a single set of weights. The authors demonstrate that conventional
  merging strategies applied to low-rank compressed models lead to catastrophic performance
  degradation due to amplified task interference and loss of coupling between low-rank
  factors.
---

# Towards Reversible Model Merging For Low-rank Weights

## Quick Facts
- arXiv ID: 2510.14163
- Source URL: https://arxiv.org/abs/2510.14163
- Reference count: 33
- Key outcome: RMM achieves 72.22% average GLUE score with PT-SVD at rank 16, compared to only 31.67-31.88% for baselines while using 69% of storage

## Executive Summary
This paper introduces Reversible Model Merging (RMM), a method that reframes model merging as constructing a compact basis from which original task-specific models can be reconstructed via linear combination, rather than collapsing all models into a single set of weights. The authors demonstrate that conventional merging strategies applied to low-rank compressed models lead to catastrophic performance degradation due to amplified task interference and loss of coupling between low-rank factors. RMM provides a closed-form solution for selecting an optimal basis of model weights and task-specific coefficients for linear combination. Experiments across diverse datasets and model scales show that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.

## Method Summary
RMM treats model merging as a basis selection problem where task vectors (model deltas) are represented as linear combinations of basis vectors. For each layer and task vector position, the algorithm collects task vectors across all models, performs SVD to find the top-p principal components as basis vectors, and computes reconstruction coefficients. This creates a compact representation that stores only p(r+n)+r parameters per position instead of rn. At inference, task-specific models are reconstructed on-demand using the stored basis and coefficients, enabling task-specific inference without compromising low-rank efficiency.

## Key Results
- RMM with p=3 achieves 72.22% average GLUE score vs 31.67-31.88% for baselines using PT-SVD at rank 16
- RMM with p=2 achieves 60.58% average vs 40.95% for Task Arithmetic baseline on GLUE
- Storage efficiency: RMM uses only 69% of storage required to keep all individual models
- Performance gains increase as compression rank decreases (r=16,32) where baseline failure is most severe

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conventional merging catastrophically fails on low-rank compressed models because separate merging of low-rank factors destroys their coupled structure.
- **Mechanism:** Methods like Task Arithmetic merge A and B matrices independently. When rank-r factors (r ≪ m, d) are combined across tasks, their product structure breaks—the coupled relationship that defines Δ̂ = AB is lost. Limited expressive capacity of low-rank representations makes them highly susceptible to destructive interference.
- **Core assumption:** Low-rank factors encode task-specific information in a coupled, non-orthogonal manner that cannot be preserved through independent linear combination.
- **Evidence anchors:**
  - [section 4] "We attribute this degradation to two factors: (i) Amplified task interference: compression discards much of the representation capacity... (ii) Loss of coupling between A and B: conventional merging treats low-rank matrices as independent factors"
  - [figures 1-2] Show performance drops from ~85% to ~31-42% when merging LoRA/PT-SVD models versus full fine-tuned models
  - [corpus] Related work "Decouple and Orthogonalize" corroborates that LoRA merging requires special treatment due to factor coupling
- **Break condition:** If low-rank factors were already orthogonal across tasks or if compression rank were sufficiently high (near full rank), independent merging might not fail catastrophically.

### Mechanism 2
- **Claim:** The optimal compact basis for reconstructing task vectors is the top-p principal components of the task vector distribution.
- **Mechanism:** For each task vector position (row in A, column in B), RMM collects task vectors {x₁,...,xₙ} across all models. The optimization problem min_{W,C} ||X - CW^T||²_F with orthonormal W has a closed-form solution: W* consists of top-p right singular vectors of centered X (PCA solution). This maximizes preserved variance while minimizing reconstruction error.
- **Core assumption:** Task vectors at each position share a common low-dimensional subspace that can be approximated with p dimensions where p < r < n.
- **Evidence anchors:**
  - [section 5] Theorem 1 proves "The optimal basis W* that minimizes equation 5 is given by the top-p eigenvectors of the sample covariance matrix X^TX"
  - [appendix A.1] Complete derivation using Rayleigh quotient theorem
  - [corpus] Weak/missing: No direct corpus comparison to other basis selection methods; assumption of PCA optimality unchallenged
- **Break condition:** If task vectors are not well-approximated by a low-dimensional subspace (high intrinsic dimensionality), or if tasks are maximally diverse, reconstruction accuracy degrades.

### Mechanism 3
- **Claim:** Reversible reconstruction allows near-perfect recovery of individual task models by storing basis + coefficients + mean, enabling task-specific inference without compromising low-rank efficiency.
- **Mechanism:** Storage requires only p(r+n)+r parameters per task vector position versus nr for all original vectors. At inference, x̂ᵢ = C*[i,:]W^T + μ reconstructs each task vector. The trade-off between storage and accuracy is tunable via p: p=2 stores ~39-73% depending on n; p=3 stores ~52-85% but achieves higher accuracy.
- **Core assumption:** The target task is known or can be identified at inference time (via user specification or router oracle).
- **Evidence anchors:**
  - [tables 1-2] RMM p=3 achieves 60.58-72.22% average vs. 31-42% for baselines on PT-SVD; 55.40-61.35% vs. ~40% for LoRA
  - [figures 4-5] Storage ratio decreases sublinearly as n grows, demonstrating scalability
  - [corpus] "Modular Delta Merging" paper explores similar reversible composition with orthogonal constraints, suggesting independent validation of the reversibility concept
- **Break condition:** If task identity cannot be determined at inference, or if storage for p≥2 basis models exceeds practical limits for edge deployment.

## Foundational Learning

- **Concept: Low-rank matrix factorization (SVD, LoRA)**
  - Why needed here: The entire paper operates on low-rank representations Δ̂ = AB where A∈R^{m×r}, B∈R^{r×d}. Understanding how compression works is essential to grasp why merging fails.
  - Quick check question: Given a weight matrix W∈R^{1024×1024} compressed to rank 16 via SVD, what are the dimensions of factors A and B, and how many parameters are saved?

- **Concept: Task vectors (deltas) and model merging arithmetic**
  - Why needed here: RMM builds on the task arithmetic framework where deltas (θ_fine-tuned - θ_pretrained) are combined. Understanding this linear algebra is prerequisite.
  - Quick check question: If model A has delta Δ_A and model B has delta Δ_B, what is the merged model under simple averaging? What could go wrong?

- **Concept: Principal Component Analysis (PCA) and variance preservation**
  - Why needed here: RMM's core algorithm is PCA applied to task vectors. Theorem 1 proves that top singular vectors minimize reconstruction error—the same objective as PCA.
  - Quick check question: If you have 100 task vectors in R^{64}, and you compute the top 3 principal components, what fraction of variance might you capture? How would you check?

## Architecture Onboarding

- **Component map:**
  - Input: n low-rank models {θᵢ}, each with layers containing A^l_i ∈ R^{m×r}, B^l_i ∈ R^{r×d}
  - Per-position processor: For each layer l and each task vector position (m rows + d columns per layer):
    - Gather X = [x₁...xₙ]^T ∈ R^{n×r}
    - Center: μ = mean(X), X ← X - μ
    - SVD: X = UΣV^T
    - Select: W* = V[:, 1:p], C* = XW*
    - Store: (W*, C*, μ)
  - Reconstructor: Given task index i, compute x̂ᵢ = C*[i,:]W*^T + μ for all positions, concatenate into Â^l_i, B̂^l_i
  - Output: Reconstructed low-rank delta Δ̂ᵢ for inference

- **Critical path:**
  1. Verify all models share the same pretrained initialization θ_pre
  2. Extract low-rank deltas from each model (LoRA adapters or PT-SVD factors)
  3. For each layer, iterate over m+d task vector positions
  4. Apply SVD-based basis selection (O(n·r·min(n,r)) per position)
  5. Store compact representation; discard original task vectors
  6. At inference, reconstruct task-specific model on-demand

- **Design tradeoffs:**
  - **p (number of basis vectors):** Higher p → better reconstruction but more storage. Paper shows p=2 achieves ~47-57% average accuracy; p=3 achieves ~60-72%. Storage scales as p(r+n)+r per position.
  - **Compression rank r:** Lower r → more storage savings but less expressive power. RMM gains over baselines are larger at lower ranks (r=16,32) where baseline failure is most severe.
  - **Scalability:** Storage ratio decreases as n increases (sublinear scaling). For large n (federated/continual learning), RMM becomes more efficient.

- **Failure signatures:**
  - **Near-random performance on some tasks:** Suggests basis dimension p is too low; increase p
  - **Reconstruction error high despite large p:** Tasks may be too diverse; consider task clustering
  - **Storage approaching 100%:** Check if n is small or r is large; RMM benefits most when n ≫ p
  - **Negative metric values (e.g., STS-B at -3.53 in table 1):** Catastrophic baseline failure on specific tasks; indicates interference beyond simple linear combination

- **First 3 experiments:**
  1. **Reproduction on single dataset:** Take 3-5 RoBERTa-base models fine-tuned on GLUE tasks. Apply PT-SVD with r=32. Run RMM with p=2, p=3. Compare to Task Arithmetic baseline. Verify ~30-50 percentage point improvement.
  2. **Ablation on p:** Fix n=8 tasks, r=32. Sweep p ∈ {1,2,3,4,5}. Plot reconstruction error vs. storage. Identify knee point where gains diminish.
  3. **Scalability test:** Generate synthetic task vectors with controlled variance structure. Test n ∈ {10,50,100,500}. Measure storage ratio and reconstruction accuracy. Verify sublinear storage growth matches theoretical prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an efficient, data-free router be designed to identify the target task index during inference without relying on an oracle?
- **Basis in paper:** [explicit] The paper states the inference phase assumes the target task is "explicitly specified by the user or automatically determined by an oracle router."
- **Why unresolved:** The current evaluation relies on ground-truth task indices, leaving the practical deployment challenge of automatic task identification unaddressed.
- **What evidence would resolve it:** A mechanism integrated into RMM that accurately classifies the input task with negligible computational overhead.

### Open Question 2
- **Question:** Why does the relative performance gain of RMM diminish as the compression rank increases?
- **Basis in paper:** [explicit] The authors observe that "as the rank increases, the relative gain of RMM over baselines becomes smaller."
- **Why unresolved:** The paper suggests higher-rank representations may be less aligned, but does not confirm if the shared basis becomes fundamentally less efficient at representing high-dimensional task vectors.
- **What evidence would resolve it:** An analysis correlating basis orthogonality with rank size and a comparison of reconstruction error scaling laws.

### Open Question 3
- **Question:** Can the reconstruction basis be updated efficiently in online or continual learning settings without re-computing the SVD on all previous tasks?
- **Basis in paper:** [inferred] The paper motivates the work by citing "federated or continual learning scenarios," but the algorithm is designed for static, offline merging.
- **Why unresolved:** The current merging phase requires gathering all task vectors simultaneously, which is infeasible for streaming tasks where models arrive sequentially.
- **What evidence would resolve it:** An incremental update rule for the basis $W$ and coefficients $C$ that maintains accuracy comparable to the batch solution.

## Limitations

- The experimental scope is limited to GLUE benchmark tasks with relatively small model scales, leaving questions about scalability to larger models and more diverse task distributions.
- The choice of p=2 or p=3 for basis dimension appears somewhat arbitrary without systematic analysis of the reconstruction-storage tradeoff curve.
- While the paper provides theoretical justification for PCA-based basis selection, there is no empirical comparison against alternative basis selection methods.

## Confidence

- **High confidence:** The core mechanism of catastrophic baseline failure on low-rank models (Mechanism 1) is well-supported by empirical evidence showing ~30-40 percentage point drops in performance. The closed-form PCA solution for basis selection (Mechanism 2) is mathematically rigorous with complete derivation in the appendix.
- **Medium confidence:** The practical effectiveness of RMM across different compression methods (LoRA vs PT-SVD) and datasets is demonstrated, but the generalization to more diverse task sets and larger model scales remains untested. The storage-accuracy tradeoff claims are supported but not systematically explored.
- **Low confidence:** The assumption that task vectors across arbitrary tasks can be well-approximated by low-dimensional subspaces is largely untested. The paper doesn't explore scenarios where tasks might be highly diverse or orthogonal, which could break the basis reconstruction approach.

## Next Checks

1. **Systematic p-ablative analysis:** Run RMM with p ∈ {1,2,3,4,5,6} on the same GLUE tasks and plot reconstruction error vs. storage ratio. Identify the knee point where additional basis vectors provide diminishing returns, and verify the paper's choice of p=2 or p=3 is justified.

2. **Cross-task diversity stress test:** Generate synthetic task vectors with controlled correlation structure (highly correlated, moderately correlated, orthogonal). Apply RMM and measure how reconstruction accuracy degrades as task diversity increases, testing the limits of the low-dimensional subspace assumption.

3. **Large-scale model validation:** Apply RMM to larger models (Llama-7B, Mistral-7B) fine-tuned on 20+ diverse tasks from HELM or BIG-bench. Measure whether the performance gains observed on GLUE scale to more challenging, diverse tasks and larger parameter counts.