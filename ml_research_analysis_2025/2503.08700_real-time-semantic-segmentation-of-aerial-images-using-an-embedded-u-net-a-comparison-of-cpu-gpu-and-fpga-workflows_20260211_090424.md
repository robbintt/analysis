---
ver: rpa2
title: 'Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net:
  A Comparison of CPU, GPU, and FPGA Workflows'
arxiv_id: '2503.08700'
source_url: https://arxiv.org/abs/2503.08700
tags:
- neural
- u-net
- fpga
- workflow
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight U-Net model optimized for real-time
  semantic segmentation of aerial images using embedded computing platforms. The model
  reduces parameters and MAC operations by a factor of 16 while maintaining accuracy.
---

# Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows

## Quick Facts
- arXiv ID: 2503.08700
- Source URL: https://arxiv.org/abs/2503.08700
- Authors: Julien Posso; Hugo Kieffer; Nicolas Menga; Omar Hlimi; Sébastien Tarris; Hubert Guerard; Guy Bois; Matthieu Couderc; Eric Jenn
- Reference count: 40
- Primary result: Lightweight U-Net achieves 0.7108 IoU and 0.9546 accuracy on Inria dataset while reducing parameters and MACs by 16x

## Executive Summary
This paper presents a lightweight U-Net architecture optimized for real-time semantic segmentation of aerial images on embedded platforms. The model reduces computational complexity by a factor of 16 while maintaining high accuracy (0.7108 IoU, 0.9546 accuracy) on the Inria Aerial Image Labeling Dataset. The study comprehensively evaluates five different workflows (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN) across three hardware platforms (CPU, GPU, and FPGA), measuring performance metrics including latency, power consumption, and energy efficiency. Results demonstrate that FPGA with Vitis AI provides the best energy efficiency, while CPU with TVM offers the best balance of usability and performance.

## Method Summary
The research introduces a lightweight U-Net architecture by reducing channel depth to one-fourth of the original design, achieving a 16x reduction in parameters and MAC operations. Five deployment workflows were implemented and evaluated: TVM for CPU optimization, FINN for binary neural network FPGA deployment, Vitis AI for quantized FPGA inference, TensorFlow GPU for GPU deployment, and cuDNN for GPU acceleration. The evaluation used the Inria Aerial Image Labeling Dataset with 5000x5000 pixel images tiled to 256x256 patches. Performance metrics included Intersection over Union (IoU), accuracy, latency, power consumption, and energy efficiency across different hardware platforms including ZCU102 and ZCU104 FPGAs, Jetson Xavier AGX GPU, and ARM CPU cores.

## Key Results
- Lightweight U-Net achieves 0.7108 IoU and 0.9546 accuracy on validation set, comparable to baseline models
- FPGA with Vitis AI provides best energy efficiency at 53.5 mJ/image compared to 195.2 mJ/image for TensorFlow GPU
- CPU with TVM offers best balance of usability and performance despite lower throughput (~2 FPS)
- Model reduces parameters from 15.4M to 1.9M and MAC operations by factor of 16 while maintaining task-specific accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing U-Net channel depth to 1/4 of the original architecture significantly lowers computational load while maintaining task-specific accuracy.
- **Mechanism:** Architectural scaling leverages the quadratic relationship between channel count and Multiply-Accumulate (MAC) operations/parameters. By aggressively pruning channels (scaling factor 1/4), the model footprint reduces by 16x, fitting embedded memory constraints without suffering the accuracy collapse seen in more aggressive pruning.
- **Core assumption:** The feature complexity of the Inria Aerial Image Labeling Dataset allows for robust representation with significantly fewer learned filters than general-purpose datasets like ImageNet.
- **Evidence anchors:**
  - [section]: "We preserved the core structure... reducing the number of channels per layer to one-fourth... decreased... MAC... by a factor of 16" (Section III-B).
  - [table]: Table I shows the Lightweight U-Net achieving 0.7108 IoU vs the baseline 0.6467.
  - [corpus]: The neighbor paper "BioLite U-Net" also supports the viability of reduced U-Net architectures for edge deployment, though through different pruning techniques.
- **Break condition:** Accuracy degrades non-linearly if channel reduction exceeds the information capacity required for the specific segmentation task (as hinted in Figure 1 parameter sweep).

### Mechanism 2
- **Claim:** FPGA deployment using Vitis AI with 8-bit quantization provides the superior energy efficiency for this architecture compared to CPU and GPU workflows.
- **Mechanism:** The workflow maps the U-Net operations to a specialized Deep Learning Processor Unit (DPU) on the FPGA. Int8 quantization reduces memory bandwidth requirements and allows for lower-voltage operation, while the spatial architecture of the FPGA minimizes the data movement overhead that plagues GPUs.
- **Core assumption:** The post-training quantization calibration provided by Vitis AI effectively preserves the dynamic range of the float32 model without requiring extensive re-training.
- **Evidence anchors:**
  - [abstract]: "...FPGA with Vitis AI provides the best performance and energy efficiency..."
  - [table]: Table XIII shows Vitis AI achieving 53.5 mJ/image vs 195.2 mJ/image for TensorFlow GPU.
  - [corpus]: "Real-Time Semantic Segmentation on FPGA..." corroborates the suitability of FPGAs for accelerating semantic segmentation in autonomous systems.
- **Break condition:** Efficiency gains are negated if the model requires unsupported operators (custom layers) that force partial execution on the power-hungry ARM CPU.

### Mechanism 3
- **Claim:** CPU inference with TVM offers the highest workflow maturity and usability but is strictly limited by hardware throughput constraints.
- **Mechanism:** TVM optimizes the computational graph and memory layout via auto-scheduling (10,000 trials), bridging the gap between high-level Python frameworks and the ARM instruction set. This maximizes utilization of the CPU cores but cannot overcome the physical limitation of serial execution compared to parallel accelerators.
- **Core assumption:** The user prioritizes development speed and software ecosystem integration over raw real-time latency.
- **Evidence anchors:**
  - [section]: "...CPU with TVM offers the best balance of usability and performance" (Abstract).
  - [table]: Table VI shows ZCU104 CPU latency at 489.2 ms (non-quantized), significantly slower than FPGA/ASIC.
  - [corpus]: Corpus signals regarding "Hardware Compilation" support the mechanism of compilers bridging frameworks to hardware, though specific TVM data is internal to this paper.
- **Break condition:** The mechanism fails if "real-time" constraints are defined as high-FPS video processing (>10 FPS), which the ~500ms latency cannot support.

## Foundational Learning

- **Concept:** U-Net Architecture (Encoder-Decoder)
  - **Why needed here:** This is the base structure being optimized. Understanding the difference between the down-sampling (encoder) path and up-sampling (decoder) path is critical because they present different bottlenecks (MAC operations vs. parameter count).
  - **Quick check question:** Which path of the U-Net consumes the majority of MAC operations in this study?

- **Concept:** Quantization (Float32 to Int8/Binary)
  - **Why needed here:** The paper compares workflows that rely heavily on quantization (FINN, Vitis AI) vs. those that don't (TensorFlow, TVM). Understanding the trade-off between numeric precision and speed/energy is central to the results.
  - **Quick check question:** Did Int8 quantization in the Vitis AI workflow degrade the IoU compared to the Float32 baseline?

- **Concept:** MAC (Multiply-Accumulate) Operations
  - **Why needed here:** MAC count is the primary proxy for computational complexity used to justify the "lightweight" nature of the model and compare efficiency across platforms.
  - **Quick check question:** By what factor were MAC operations reduced in the lightweight model compared to the original U-Net?

## Architecture Onboarding

- **Component map:** Input: 5000x5000 Aerial Images → Pre-process: Tiled to 256x256 patches → Model: Lightweight U-Net (1.9M params) → Deployment Workflow: (Vitis AI / TVM / FINN) → Hardware: (ZCU102 FPGA / Jetson GPU / ARM CPU) → Post-process: Stitching.

- **Critical path:** The **Vitis AI Quantization & Compilation** step is the critical path for reproducing the "best" results. It involves: `Keras Model → Quantization (Int8 calibration) → Compilation (DPU instructions) → Board Deployment`.

- **Design tradeoffs:**
  - **TensorFlow GPU:** Maximum ease of use, high memory footprint (2.2GB), poor energy efficiency.
  - **TVM CPU:** High versatility/maturity, very low throughput (~2 FPS).
  - **Vitis AI FPGA:** Best energy efficiency (53.5 mJ/img), highest engineering complexity (requires hardware synthesis).
  - **FINN FPGA:** Potential for extreme efficiency, but currently brittle (bugs, missing layers like transposed convolution).

- **Failure signatures:**
  - **FINN:** Workflow fails if standard Keras layers (e.g., specific transposed convolutions) are used without custom HLS transformations or substitution (e.g., replacing transposed conv with nearest neighbor + conv).
  - **TVM:** Latency spikes or crashes if auto-scheduling is skipped or if quantization bugs are triggered (observed 3x slowdown with Int8 quantization).
  - **cuDNN:** Incomplete implementation; inability to run the full inference due to missing layer support in the library.

- **First 3 experiments:**
  1. **Baseline Profiling:** Train the lightweight U-Net (1/4 channels) in Keras and deploy directly to the Jetson AGX using standard TensorFlow to establish the "ease of use" baseline and verify accuracy (Target: ~0.71 IoU).
  2. **Vitis AI Synthesis:** Convert the Keras model to Int8 using Vitis AI quantization tools and synthesize the DPU bitstream for the ZCU102. Measure power and latency to verify the "efficiency" claim.
  3. **CPU Optimization:** Apply TVM auto-scheduling (10k trials) to the model for the ARM A53 core. Compare the latency (ms) against the unoptimized baseline to validate the compiler speedup.

## Open Questions the Paper Calls Out
None

## Limitations
- The lightweight U-Net architecture may not generalize to datasets with more complex semantic classes or higher-resolution imagery beyond aerial applications.
- The FPGA energy efficiency claims rely on post-training quantization which may not preserve accuracy for models with different dynamic ranges.
- Usability comparisons between workflows are largely qualitative and based on author experience rather than standardized metrics.

## Confidence
- **High:** Architectural scaling mechanism (channel reduction by 4x) with clear quantitative evidence of 16x reduction in MACs/parameters.
- **Medium:** FPGA energy efficiency claims, as quantization effectiveness may vary with different model characteristics.
- **Low:** Workflow usability comparisons, as assessments are qualitative and experience-based rather than empirically measured.

## Next Checks
1. **Cross-Dataset Generalization:** Evaluate the lightweight U-Net on additional aerial datasets (e.g., SpaceNet, DeepGlobe) to verify 0.71 IoU performance holds across different geographic regions and land-use patterns.
2. **Quantization Robustness:** Test Vitis AI quantization with different calibration dataset sizes and methods to determine minimum data requirements for maintaining accuracy without extensive re-training.
3. **Workflow Portability:** Attempt to deploy the same workflow optimizations (TVM auto-scheduling, Vitis AI quantization) on different hardware platforms (e.g., ZCU104 vs. ZCU102) to validate claimed benefits are not platform-specific artifacts.