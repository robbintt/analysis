---
ver: rpa2
title: 'QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold
  Networks'
arxiv_id: '2506.22340'
source_url: https://arxiv.org/abs/2506.22340
tags:
- quantum
- function
- functions
- basis
- qukan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QuKAN, a quantum implementation of Kolmogorov-Arnold
  Networks using Quantum Circuit Born Machines (QCBMs). By encoding B-spline basis
  functions into quantum superposition states, the model leverages quantum parallelism
  to evaluate multiple functions simultaneously.
---

# QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks

## Quick Facts
- **arXiv ID:** 2506.22340
- **Source URL:** https://arxiv.org/abs/2506.22340
- **Reference count:** 39
- **Primary result:** QuKAN achieves 97.9% accuracy on Moons dataset and matches classical KANs on Iris, outperforming standard VQC models.

## Executive Summary
This paper introduces QuKAN, a quantum implementation of Kolmogorov-Arnold Networks using Quantum Circuit Born Machines (QCBMs). By encoding B-spline basis functions into quantum superposition states, the model leverages quantum parallelism to evaluate multiple functions simultaneously. The authors propose both hybrid (combining classical and quantum components) and fully quantum residual functions. Experiments on binary classification tasks (moons and Iris datasets) show that QuKAN achieves high accuracy, outperforming standard VQC models with different embedding strategies and matching the performance of classical KANs. For function regression, QuKAN successfully fits both linear and non-linear functions. Ablation studies confirm that pre-training the QCBM on basis functions is crucial for effective learning, as models without pre-training plateau quickly. Overall, QuKAN demonstrates that quantum KANs are a viable, interpretable approach to quantum machine learning.

## Method Summary
QuKAN combines Kolmogorov-Arnold Networks with Quantum Circuit Born Machines by encoding B-spline basis functions into quantum superposition states. The approach uses a 2-layer architecture where the qubit register is split into label (spline index) and position (input value) components. The method involves two phases: pre-training a QCBM to encode 4 B-spline basis functions into a superposition state, then using this pre-trained state for a hybrid residual function that combines quantum measurements with classical SiLU activation. The model is trained for 20 epochs on classification tasks using the Moons and Iris datasets, achieving high accuracy while maintaining interpretability through the quantum encoding of basis functions.

## Key Results
- QuKAN achieves 97.9% accuracy on the Moons binary classification task, outperforming standard VQC models
- The model reaches 100% accuracy on the Iris dataset, matching classical KAN performance
- Ablation studies show pre-training is crucial, with pre-trained models achieving 97.9% vs 87% for non-pre-trained models
- QuKAN successfully fits both linear (2x₁ - 3x₂ + 1) and non-linear (ln(x₀/x₁)) regression functions

## Why This Works (Mechanism)
QuKAN works by leveraging quantum parallelism to encode multiple B-spline basis functions into a single quantum state through superposition. The QCBM pre-training phase learns to represent these basis functions in the amplitudes of a quantum state, which can then be efficiently evaluated for any input through projective measurements. The hybrid residual function combines the quantum-measured basis function values with classical SiLU activation, allowing the model to capture both linear and non-linear relationships. This approach maintains the interpretability of KANs while benefiting from quantum advantages in function evaluation and representation.

## Foundational Learning
- **Quantum Circuit Born Machines (QCBMs):** Quantum circuits used to generate probability distributions through Born rule measurements. Why needed: Enables encoding of basis functions into quantum superposition states for efficient evaluation.
- **B-spline basis functions:** Piecewise polynomial functions used as building blocks for function approximation. Why needed: Provide smooth, localized basis functions that can be efficiently encoded in quantum states.
- **Kolmogorov-Arnold Networks:** Two-layer networks that represent functions as sums of univariate functions. Why needed: Provide interpretable architecture that can be mapped to quantum operations.
- **Quantum superposition encoding:** Storing multiple function values simultaneously in quantum amplitudes. Why needed: Enables quantum parallelism for evaluating multiple basis functions at once.
- **Hybrid quantum-classical computation:** Combining quantum measurement outcomes with classical neural network components. Why needed: Leverages quantum advantages while maintaining compatibility with classical optimization.
- **Input discretization:** Mapping continuous inputs to discrete quantum basis states. Why needed: Required for encoding inputs into finite-dimensional quantum registers.

## Architecture Onboarding

**Component Map:** QCBM Pre-training -> Hybrid Residual Function -> Classification/Regression

**Critical Path:** The critical path is the QCBM pre-training phase, as the quality of the pre-trained state directly determines the model's ability to evaluate basis functions accurately. Without successful pre-training, the model cannot learn effective residual functions.

**Design Tradeoffs:** The choice of B-splines provides smooth, localized basis functions but requires careful discretization of inputs. The hybrid approach maintains interpretability while leveraging quantum advantages, but limits full exploitation of quantum computation. The position register size trades off between input resolution and quantum resource requirements.

**Failure Signatures:** 
- Accuracy plateauing around 87% indicates failed pre-training
- "Pixelated" or stepped predictions suggest insufficient position register qubits
- Poor convergence during pre-training indicates inadequate QCBM architecture or hyperparameters

**First Experiments:**
1. Verify QCBM pre-training convergence on basis functions before proceeding to full model training
2. Test different position register sizes to find optimal input resolution  
3. Compare QuKAN accuracy with and without pre-training on a held-out validation set

## Open Questions the Paper Calls Out

The paper acknowledges that VQC approaches with trainable observables have demonstrated improved performance but leaves direct comparison to those methods as future work. Additionally, while the authors adopted B-splines for consistency, they note that their method is not restricted to specific basis functions and that any trainable basis set could theoretically be used. The current validation is limited to toy classification and regression tasks, leaving open questions about scalability to high-dimensional datasets.

## Limitations
- Performance comparison limited to VQCs with fixed embeddings, not VQCs with trainable observables
- Only validated on simple toy datasets (Moons, Iris) and 2D function regression
- Requires input discretization which may lose information for complex datasets
- Scalability to high-dimensional data remains unexplored

## Confidence

**High:** Claims about QuKAN's architecture and classification performance are well-supported by experimental results showing 97.9% accuracy on Moons and 100% on Iris.

**Medium:** The claim that pre-training is crucial is supported by the ablation study (97.9% vs 87% accuracy), but limited to one ablation scope.

**Low:** Specific quantum circuit implementation details (ansatz structure, qubit counts, hyperparameters) are not fully specified, creating uncertainty in faithful reproduction.

## Next Checks
1. Verify QCBM pre-training convergence on basis functions before proceeding to full model training
2. Test different position register sizes to find optimal input resolution
3. Compare QuKAN accuracy with and without pre-training on a held-out validation set