---
ver: rpa2
title: 'MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support'
arxiv_id: '2602.00950'
source_url: https://arxiv.org/abs/2602.00950
tags:
- risk
- your
- safety
- health
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present MindGuard, a family of lightweight safety classifiers
  designed to detect actionable harm in mental health support conversations. Using
  a clinically grounded risk taxonomy developed with licensed psychologists, our classifiers
  distinguish between safe therapeutic content and imminent risks such as self-harm
  and harm to others.
---

# MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support

## Quick Facts
- arXiv ID: 2602.00950
- Source URL: https://arxiv.org/abs/2602.00950
- Authors: António Farinhas; Nuno M. Guerreiro; José Pombal; Pedro Henrique Martins; Laura Melton; Alex Conway; Cara Dochat; Maya D'Eon; Ricardo Rei
- Reference count: 40
- Primary result: Up to 98.2% AUROC and 26× reduction in false positives at high-recall operating points compared to general-purpose safeguards

## Executive Summary
We present MindGuard, a family of lightweight safety classifiers designed to detect actionable harm in mental health support conversations. Using a clinically grounded risk taxonomy developed with licensed psychologists, our classifiers distinguish between safe therapeutic content and imminent risks such as self-harm and harm to others. We train them on synthetic dialogues generated with controlled scenarios and LLM-as-a-judge supervision. Evaluated on a new expert-annotated dataset, our 4B and 8B models achieve up to 98.2% AUROC and reduce false positives at high-recall operating points by up to 26× compared to general-purpose safeguards. In adversarial system-level red teaming, MindGuard models lower attack success and harmful engagement rates by 70-76% relative to baselines, even outperforming models 30× larger. All resources, including models and data, are publicly released.

## Method Summary
MindGuard classifiers are trained on synthetic dialogues generated through a two-agent setup: a patient LLM following controlled risk scenarios and a clinician LLM providing therapeutic responses. A judge LLM labels all turns using 5-sample majority voting with full conversation context. The classifiers are finetuned Qwen3Guard-Gen models (4B/8B parameters) on these labeled turns, focusing on turn-level classification within conversation history. Evaluation uses a new expert-annotated dataset (MindGuard-testset) with 1134 turns from 67 conversations labeled by licensed clinical psychologists.

## Key Results
- Achieves up to 98.2% AUROC on expert-annotated test data
- Reduces false positives at high-recall operating points by up to 26× compared to general-purpose safeguards
- In adversarial red teaming, lowers attack success and harmful engagement rates by 70-76% relative to baselines
- Outperforms models 30× larger despite smaller parameter count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific risk taxonomies reduce false positives at high-recall operating points compared to general-purpose safeguards.
- Mechanism: The three-category taxonomy (safe/self-harm/harm-to-others) explicitly encodes clinically actionable distinctions rather than topic presence. This separates therapeutic disclosures (historical trauma, metaphorical language) from imminent risk signals, preventing over-classification of emotionally salient content as unsafe.
- Core assumption: False positives in mental health contexts carry therapeutic costs (disrupted rapport, reduced disclosure) distinct from content moderation settings.
- Evidence anchors: [abstract] "Our classifiers reduce false positives at high-recall operating points by up to 26× compared to general-purpose safeguards"; [section 2.2] "Explicitly defining a safe category allows systems to avoid treating all emotionally salient content as safety-relevant"; [corpus] Related work on therapeutic AI privacy risks (arxiv 2510.10805) documents over-disclosure harms, supporting the need for calibrated detection.
- Break condition: If deployment context requires zero tolerance for any risk-adjacent content (e.g., legal compliance regimes), the taxonomy's therapeutic framing may conflict with policy requirements.

### Mechanism 2
- Claim: Synthetic dialogue generation with LLM-as-judge supervision creates effective training signals for rare risk scenarios.
- Mechanism: A two-agent setup (patient LM following controlled scenarios + clinician LM responding naturally) generates multi-turn dialogues. The judge LM labels all turns with full-conversation context, using 5-sample majority voting. This provides supervision that reflects how risk emerges across turns rather than isolated utterance classification.
- Core assumption: The judge LM can reliably apply the taxonomy with sufficient consistency for training signal quality.
- Evidence anchors: [section 4.1.1] "Scenarios are organized hierarchically by risk category... and finer-grained subcategories (e.g., direct suicidal ideation, passive ideation)"; [section 4.1.2] "We run the judge multiple times and aggregate its predictions using majority voting"; [corpus] MindEval benchmark (arxiv 2511.18491) validates that multi-turn evaluation captures complexity missed by single-turn assessments.
- Break condition: If judge LM exhibits systematic bias (e.g., over-flagging certain demographic language patterns), training data will propagate this bias.

### Mechanism 3
- Claim: Turn-level classification with conversation history context captures risk escalation patterns that utterance-level classifiers miss.
- Mechanism: Each prediction conditions on the full preceding conversation history. This enables detection of gradual escalation (turns 1-5 benign, turn 6 unsafe) and de-escalation patterns, rather than treating each message independently.
- Core assumption: Risk signals are temporally dependent within a conversation.
- Evidence anchors: [section 3.1] "Given a user message mt at turn t and conversation history {m1, . . ., mt−1}, a classifier predicts a risk category"; [section 1, Figure 1 description] "MindGuard detects an unsafe turn and triggers downstream safety handling, whereas general-purpose safeguards fail to detect this signal"; [corpus] Related work on context-emotion aware dialogue (arxiv 2511.11884) shows context improves therapeutic response quality.
- Break condition: If conversations span multiple sessions with memory retention, single-conversation context may miss longitudinal risk patterns (acknowledged in section 6).

## Foundational Learning

- Concept: **High-recall operating points (FPR@90%TPR)**
  - Why needed here: Mental health safety prioritizes catching crises (high TPR) but excess false positives disrupt therapeutic engagement. Understanding this tradeoff is essential for threshold selection.
  - Quick check question: If a system achieves 95% TPR with 15% FPR, how many benign conversations are incorrectly flagged per 1000 interactions?

- Concept: **LLM-as-a-judge with self-consistency**
  - Why needed here: The training pipeline relies on judge LM labeling quality. Majority voting over multiple samples reduces stochastic errors.
  - Quick check question: Why might a single judge LM sample produce inconsistent labels for the same borderline turn?

- Concept: **Therapeutic vs. content-moderation safety framing**
  - Why needed here: General-purpose safeguards optimize for topic detection; mental health requires distinguishing intent and context. This conceptual shift underpins the taxonomy design.
  - Quick check question: Is "I sometimes think about death" safer or unsafe? What additional context would change your assessment?

## Architecture Onboarding

- Component map:
  - Patient LM (GLM-4.6) -> Clinician LM (proprietary) -> Judge LM (GLM-4.6, 5-sample voting) -> MindGuard classifier (Qwen3Guard-Gen 4B/8B finetuned) -> MindGuard-testset (1134 turns, 67 conversations, 3 expert annotators)

- Critical path: Scenario design → Dialogue generation → Judge labeling (5 votes) → Majority aggregation → Finetuning → Threshold calibration → Deployment

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic enables coverage of rare high-risk scenarios; real data (MindGuard-testset) reserved for evaluation only
  - Full-context labeling vs. turn-by-turn inference: Training uses future context for quality; inference operates causally
  - 4B vs. 8B model: Paper shows minimal AUROC gap (0.981 vs. 0.982); latency/throughput requirements should drive choice

- Failure signatures:
  - Systematic over-flagging of metaphorical language ("I'm drowning in work") → taxonomy not learned correctly
  - Missed escalation across turns → history context not utilized, check sequence length (max 4096 tokens)
  - No difference between 1-sample and 5-sample judge outputs → majority voting not adding value

- First 3 experiments:
  1. **Baseline comparison on testset**: Run MindGuard 4B, Llama Guard 3 8B, and gpt-oss-safeguard on MindGuard-testset, compute AUROC and FPR@90%TPR. Verify reproduction of Table 1.
  2. **Ablation on judge voting**: Train with 1-sample vs. 5-sample judge labels. Measure impact on testset performance to validate the supervision quality assumption.
  3. **Threshold sweep**: Plot ROC curves and identify operating points. Calibrate threshold for target TPR (e.g., 90-95%) and document resulting FPR for deployment planning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can longitudinal risk modeling across multiple sessions improve detection of gradual escalation patterns that turn-level classifiers miss?
- Basis in paper: [explicit] Section 7 states: "For future work, we aim to address 'longitudinal safety' by developing mechanisms to detect gradual risk escalation across multiple sessions."
- Why unresolved: Current classifiers operate turn-by-turn within single conversations and "may underrepresent more gradual patterns that emerge over time" (Section 6).
- What evidence would resolve it: Compare AUROC and FPR at high-recall operating points between turn-level and session-aggregated models on a dataset spanning multiple user sessions.

### Open Question 2
- Question: How do different downstream intervention strategies (e.g., response modulation vs. escalation protocols) affect therapeutic outcomes beyond attack success rates?
- Basis in paper: [inferred] Section 6 notes the intervention studied "abstracts clinical practice into a simplified control mechanism" and that "both overly cautious refusal and insufficiently responsive behavior can be harmful."
- Why unresolved: System-level evaluation only measured attack success and harmful engagement rates, not therapeutic outcomes or user experience with different intervention types.
- What evidence would resolve it: A/B testing different intervention protocols with real users, measuring therapeutic alliance, disclosure rates, and clinical outcome metrics.

### Open Question 3
- Question: Do MindGuard classifiers trained on synthetic dialogues generalize to diverse linguistic styles, demographics, and clinical presentations not covered in the training scenarios?
- Basis in paper: [inferred] Training data uses 300 manually constructed scenarios (Section 4.1.1) and test set involves only 7 patient archetypes (Appendix B.1), raising questions about coverage of the full spectrum of real-world presentations.
- Why unresolved: No evaluation on demographic subgroups or linguistic diversity; synthetic generation may not capture all naturalistic expression patterns.
- What evidence would resolve it: Stratified evaluation on held-out conversations across demographic groups, linguistic styles, and clinical presentations not represented in training scenarios.

### Open Question 4
- Question: How robust are the classifiers to adversarial manipulation specifically designed to evade mental health risk detection?
- Basis in paper: [explicit] Section 7: "Developing and evaluating more complex ART frameworks based on real-world clinical protocols is another pertinent direction."
- Why unresolved: Current ART evaluation uses gradual escalation strategies; more sophisticated attacks (e.g., encoded language, gradual desensitization) remain untested.
- What evidence would resolve it: Red teaming with attack protocols derived from observed real-world evasion tactics, reporting attack success rates across threat categories.

## Limitations
- Reliance on synthetic data generation for training raises questions about real-world generalization despite evaluation on expert-annotated test data
- Proprietary clinician LM used in dialogue generation is unavailable, introducing uncertainty in exact replication
- Scope limited to single-conversation context rather than longitudinal patterns across multiple sessions
- Threshold selection for deployment remains application-specific and requires careful calibration based on risk tolerance

## Confidence
- **High Confidence**: The taxonomy design and risk categories are clinically grounded and logically sound. The synthetic data generation pipeline is clearly specified. The evaluation methodology (expert annotation, multiple metrics) is rigorous. The red teaming framework and attack scenarios are well-defined.
- **Medium Confidence**: Claims about the 26× reduction in false positives at high-recall operating points and the 70-76% improvement in red teaming metrics depend on specific baseline choices and evaluation conditions that may vary in deployment.
- **Low Confidence**: The exact impact of the proprietary clinician LM versus substitutes, and the precise scenario library specifications that weren't fully disclosed.

## Next Checks
1. **Real-world pilot deployment**: Deploy MindGuard in a controlled mental health support environment with actual users to measure performance on genuine risk signals versus synthetic scenarios, documenting any systematic prediction errors.

2. **Bias audit across demographic groups**: Analyze false positive and false negative rates across different demographic characteristics in the test data to identify potential systematic biases in risk detection.

3. **Longitudinal risk pattern validation**: Extend evaluation to multi-session conversations to test whether the single-conversation context assumption holds when risk patterns span multiple interactions.