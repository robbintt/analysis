---
ver: rpa2
title: Counterfactual Query Rewriting to Use Historical Relevance Feedback
arxiv_id: '2502.03891'
source_url: https://arxiv.org/abs/2502.03891
tags:
- documents
- relevance
- query
- retrieval
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of using historical relevance
  feedback in evolving web search scenarios, where previously relevant documents may
  have changed or been deleted. The authors propose counterfactual query rewriting
  approaches that counterfactually assume superseded documents remain relevant and
  use them as relevance signals.
---

# Counterfactual Query Rewriting to Use Historical Relevance Feedback

## Quick Facts
- arXiv ID: 2502.03891
- Source URL: https://arxiv.org/abs/2502.03891
- Reference count: 40
- The paper proposes counterfactual query rewriting approaches that use historical relevance feedback to improve retrieval effectiveness on evolving web corpora, with keyqueries achieving substantial improvements over transformer baselines.

## Executive Summary
The paper addresses the challenge of using historical relevance feedback in evolving web search scenarios, where previously relevant documents may have changed or been deleted. The authors propose counterfactual query rewriting approaches that counterfactually assume superseded documents remain relevant and use them as relevance signals. They implement three approaches: boosting, relevance feedback expansion, and keyqueries, with keyqueries being the most effective. The evaluation on the CLEF LongEval dataset shows that query rewriting with historical relevance feedback significantly improves retrieval effectiveness (nDCG@10) compared to strong baselines including transformer-based models, even for newly created or modified documents. Keyqueries achieved the best performance, substantially outperforming the best transformer baseline while being much more efficient.

## Method Summary
The authors implement three counterfactual query rewriting approaches built on PyTerrier with BM25 as the base retrieval model. BM25Boost applies multiplicative score adjustments to documents with historical relevance using decay factors (λ=0.7 for relevant, μ=2 for highly relevant). BM25RF extracts top-k tf-idf terms from previously relevant documents and appends them to the query. BM25keyquery generates query expansions using RM3 terms from historical feedback documents, then enumerates minimal query combinations that would rank target documents in top-k positions against the current corpus, selecting the best performer by nDCG@10 on historical targets. The methods are evaluated on CLEF LongEval's 5 timestamp snapshots of French web search data, comparing against BM25, neural models (monoT5), and hybrid approaches.

## Key Results
- Keyqueries substantially outperformed the best transformer baseline (monoT5) on nDCG@10 while being much more efficient
- Query rewriting with historical relevance feedback significantly improved retrieval effectiveness on both seen and unseen documents
- BM25Boost achieved the largest improvements on seen documents (nDCG@10 from 0.155 to 0.355 for July 2022) but showed zero improvement on unseen documents

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Keyquery Generation
- Claim: Keyqueries derived from historically relevant documents improve retrieval on evolved corpora by encoding relevance signals into query terms that retrieve semantically similar documents.
- Mechanism: The system generates candidate query expansions using RM3 terms from previously relevant documents, then enumerates minimal query combinations that would rank the target documents in top-k positions against the current corpus. The winning keyquery is selected based on nDCG@10 on historical targets before being applied to the current corpus.
- Core assumption: Document changes are often incremental; vocabulary and semantic structure that made a document relevant persist sufficiently to guide retrieval of similar current documents.
- Evidence anchors: [abstract] "derive so-called keyqueries that rank the previously relevant documents to the top of the current corpus"; [section 3] "keyqueries uses them to remove overfitted or underfitted candidates"
- Break condition: When document content changes fundamentally or when query intent evolves independently of document changes, keyqueries may reinforce outdated relevance patterns.

### Mechanism 2: Relevance Feedback Term Extraction
- Claim: tf-idf weighted terms from historically relevant documents provide expansion vocabulary that generalizes to unseen documents better than direct document boosting.
- Mechanism: Extract top-k terms by tf-idf from the set of previously relevant documents, append to original query, then execute against current corpus. Term selection occurs offline using historical document versions.
- Core assumption: Discriminative terms from relevant documents retain predictive power even when source documents are deleted or modified.
- Evidence anchors: [section 3] "the top-k terms with the highest tf-idf scores are obtained for query expansion"; [table 3] BM25RF shows positive improvements on new documents in 4 of 5 timestamps (up to +0.022)
- Break condition: When terminology in a domain shifts rapidly, or when previously relevant documents used domain-specific jargon no longer present in current corpus.

### Mechanism 3: Historical Boosting with Decay Factors
- Claim: Score multipliers based on graded historical relevance provide a lightweight signal for known query-document pairs, though they cannot transfer to new documents.
- Mechanism: Apply multiplicative score adjustments using λ (0.7) for relevant documents and λ²μ (μ=2) for highly relevant documents, with (1-λ)² penalty for non-relevant history.
- Core assumption: Historically relevant documents that persist in the corpus remain relevant; temporal decay is not explicitly modeled but emerges from corpus churn.
- Evidence anchors: [section 3] Score formula with λ and μ parameters; [table 2] BM25Boost achieves substantial nDCG@10 improvements (0.155→0.355 for July 2022)
- Break condition: Cannot improve retrieval for queries on entirely new documents; ablation confirms zero improvement on unseen documents (Table 3).

## Foundational Learning

- Concept: Pseudo-Relevance Feedback (PRF) / RM3
  - Why needed here: Keyquery candidate generation uses RM3 terms; understanding PRF assumptions (top results are relevant) is prerequisite to understanding why keyqueries add ranking verification.
  - Quick check question: Given a query "machine learning," if PRF extracts terms from top-10 results but 3 are non-relevant, how might keyquery's ranking verification help?

- Concept: Test Collection Dynamics (Create/Update/Delete operations)
  - Why needed here: The entire approach depends on modeling corpus evolution; understanding how documents change informs when counterfactual assumptions break.
  - Quick check question: If 40% of documents remain unchanged (S3=1.0) and 50% change substantially (S3<0.8), what fraction of historical relevance judgments might be safely transferred?

- Concept: Keyquery Minimality and Specificity Criteria
  - Why needed here: Three criteria (top-k positioning, result count threshold, minimality) define what makes a valid keyquery; understanding these constraints is essential for implementation.
  - Quick check question: A query expansion retrieves target documents at rank 1 but returns only 3 total results—does it satisfy the keyquery criteria if the generality threshold is 25 results?

## Architecture Onboarding

- Component map: Historical feedback store -> Document version cache -> RM3 term generator -> Keyquery enumerator -> Ranking validator -> Score booster
- Critical path: Query received -> lookup in historical feedback store -> retrieve historical document versions (if history exists) -> extract RM3 terms -> enumerate keyquery candidates -> validate each candidate against current corpus (ranking + threshold checks) -> select highest-performing keyquery -> execute final retrieval -> apply score adjustments to known documents in results (if boosting enabled)
- Design tradeoffs:
  - Pre-computation vs. latency: Keyqueries can be pre-computed for known queries (efficient) but require recomputation when corpus changes significantly
  - Generality threshold (l parameter): Lower values increase candidate pool but risk overfitting; higher values may reject useful expansions
  - Feedback term count (k): More terms increase recall of relevant vocabulary but dilute original query intent
- Failure signatures:
  - Zero keyquery candidates pass validation -> vocabulary drift or corpus change too severe; fall back to original query
  - Boosting shows no improvement on ablation -> corpus has high churn rate; disable boosting or add temporal decay
  - Keyquery retrieves only historical documents -> overfitting to specific terms; increase generality threshold or reduce feedback term count
- First 3 experiments:
  1. Reproduce table 2 baseline comparison on LongEval subset: Implement BM25Boost, BM25RF, BM25keyquery with paper's parameters (λ=0.7, μ=2, k=10, top-10, min-25 results) to validate nDCG@10 improvements over monoT5.
  2. Ablation on document overlap: Remove all previously seen documents from evaluation, measure nDCG@10' delta for each approach to confirm generalization claims (should match Table 3 pattern: Boost=0, RF/keyquery show positive deltas).
  3. S3 similarity threshold sensitivity: Vary document change threshold (test at S3=0.7, 0.8, 0.9) to identify when counterfactual assumptions degrade performance; expect keyqueries to show graceful degradation as similarity decreases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can counterfactual query rewriting be adapted to handle scenarios where the user intent of a recurring query changes over time?
- Basis in paper: [explicit] The authors explicitly list handling "scenarios where the intent of a query might change" as a direction for future work.
- Why unresolved: The current approach counterfactually assumes historical relevance remains valid, which may introduce noise or bias if the underlying intent has drifted.
- What evidence would resolve it: A method capable of detecting intent drift and a subsequent evaluation showing performance maintenance or improvement on queries with labeled intent shifts.

### Open Question 2
- Question: Can relevance feedback observed for similar historical queries be effectively aggregated and re-used to support queries with sparse direct history?
- Basis in paper: [explicit] The conclusion asks "how relevance feedback observed for similar queries can be re-used" as an open question.
- Why unresolved: The current study is restricted to re-occurring exact query matches, limiting applicability to the long tail of unique or rare queries.
- What evidence would resolve it: Experiments comparing the effectiveness of keyqueries derived from exact matches versus those derived from semantic query neighborhoods.

### Open Question 3
- Question: How should keyquery generation and term expansion be modified to maintain effectiveness for natural language queries?
- Basis in paper: [explicit] The authors note that "natural language queries should not be expanded in the same way" as the short keyword queries used in their experiments.
- Why unresolved: Simple term appending or keyquery derivation may disrupt the syntactic and semantic structure of longer natural language inputs.
- What evidence would resolve it: An evaluation of the proposed rewriting techniques on a dataset containing conversational or fully formed questions.

## Limitations

- Keyquery enumeration algorithm details and computational optimizations are referenced but not described in sufficient detail
- Neural baseline implementations use default TIRA/TIREx configurations without tuning, potentially underestimating their capabilities
- Document change tracking methodology (S3 similarity metric implementation) is not fully specified

## Confidence

- **High Confidence**: Retrieval effectiveness improvements on seen documents (nDCG@10 increases of 0.2-0.3) are well-supported by ablation studies and significance testing
- **Medium Confidence**: Generalization to new documents is demonstrated but relies on term-based expansion mechanisms with limited temporal validation
- **Low Confidence**: Efficiency claims (keyqueries being "much more efficient" than transformer models) lack runtime measurements or computational complexity analysis

## Next Checks

1. Conduct parameter sensitivity analysis for λ, μ, k (feedback terms), and generality threshold l to identify optimal configurations and robustness boundaries
2. Implement and compare against more recent transformer-based query expansion approaches (e.g., ColBERTv2, Fusion-in-Decoder) to validate efficiency claims with runtime measurements
3. Perform document-level error analysis on cases where historical relevance feedback fails to improve retrieval, examining document change patterns and query evolution dynamics