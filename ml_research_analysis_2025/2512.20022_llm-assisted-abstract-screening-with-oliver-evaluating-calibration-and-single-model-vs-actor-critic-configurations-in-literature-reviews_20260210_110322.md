---
ver: rpa2
title: 'LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model
  vs. Actor-Critic Configurations in Literature Reviews'
arxiv_id: '2512.20022'
source_url: https://arxiv.org/abs/2512.20022
tags:
- review
- screening
- abstract
- critic
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OLIVER is an open-source pipeline that enables LLM-assisted abstract
  screening for systematic reviews. It uses structured prompts to enforce step-by-step
  reasoning against inclusion/exclusion criteria, requires binary decisions and confidence
  scores, and supports both single-model and actor-critic configurations.
---

# LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews

## Quick Facts
- arXiv ID: 2512.20022
- Source URL: https://arxiv.org/abs/2512.20022
- Authors: Kian Godhwani; David Benrimoh
- Reference count: 3
- Key outcome: Actor-critic configurations markedly improve LLM abstract screening calibration and discrimination, with lightweight models offering high recall at low cost.

## Executive Summary
OLIVER is an open-source pipeline for LLM-assisted abstract screening in systematic reviews. It uses structured prompts to enforce step-by-step reasoning against inclusion/exclusion criteria, requiring binary decisions and confidence scores. Tested on two non-Cochrane reviews, single models achieved high sensitivity (up to 100%) but showed poor calibration and high false-positive rates. The actor-critic framework markedly improved discrimination and calibration, reduced Brier scores, and lowered false positives while maintaining high AUCs. Lightweight models performed well, and the system enabled large-scale screening in minutes at low cost. Performance varied with review complexity, publication era, and open-access status. Results suggest LLMs can augment—not replace—human screening, with the actor-critic approach offering the most reliable and scalable solution.

## Method Summary
OLIVER uses structured prompts with ordered criterion checklists to generate binary screening decisions and confidence scores from LLMs. It supports both single-model and actor-critic configurations, where an actor proposes an initial classification and a critic independently re-evaluates with access to criteria and the actor's output. Aggregation rules include any-include, critic-veto, and agreement-required. The pipeline processes abstracts in parallel via provider-specific scripts, handles API rate limits, and computes standard screening metrics (sensitivity, specificity, AUC, Brier score, ECE). Two systematic reviews (821 and 7,741 abstracts) were used for evaluation.

## Key Results
- Actor-critic screening improved calibration (Brier scores dropped from 0.49-0.59 to 0.13-0.16) and maintained high AUCs (up to 0.91).
- Lightweight models (gpt-5-mini, grok-4-fast) frequently outperformed reasoning models in recall and cost-efficiency.
- Single models showed high sensitivity (up to 100%) but poor calibration and high false-positive rates.
- Performance degraded on older and closed-access abstracts, with Review 2 showing 46% average recall.
- Actor-critic configurations reduced false positives while maintaining sensitivity across both reviews.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Actor-critic configurations improve calibration and discrimination over single-model screening.
- Mechanism: The actor proposes an initial classification; the critic independently re-evaluates with access to criteria and the actor's output. Aggregating two independent signals reduces random variance in scoring, while the critic's corrective evaluation dampens overconfident errors. Mean-aggregated confidence scores further stabilize probability estimates.
- Core assumption: The two models make independent errors; correlation of failures would reduce ensemble benefit.
- Evidence anchors:
  - [abstract] "Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs."
  - [section] Tables 5-6 show Brier scores dropping from 0.49 (grok-4-fast) and 0.59 (gpt-5-nano) in single-model runs to 0.13-0.16 in actor-critic configurations; AUC rose to 0.91 in Review 1.
  - [corpus] Weak explicit corpus support for actor-critic specifically; neighboring papers focus on single-model or parallel ensemble screening without adjudication.
- Break condition: If actor and critic share systematic biases (e.g., both trained on similar corpora with similar gaps), calibration gains may diminish; performance is also review-dependent.

### Mechanism 2
- Claim: Lightweight, fast models can match or exceed advanced reasoning models in abstract screening recall.
- Mechanism: Abstract screening is a coarse filtering task where broader inclusion patterns matter more than deep deliberative reasoning. Lightweight models (gpt-5-mini, grok-4-fast) produce higher recall by being less conservative, albeit with more false positives, while remaining cost-efficient.
- Core assumption: The review context tolerates higher false positives if sensitivity is prioritized; human reviewers will filter downstream.
- Evidence anchors:
  - [abstract] "Lightweight models (e.g., gpt-5-mini, grok-4-fast) frequently outperformed more advanced models in recall and cost-efficiency."
  - [section] Table 3: gpt-5-mini achieved 100% sensitivity on Review 1 final includes; grok-4-fast achieved 92.06% sensitivity with 87.2% specificity. Advanced models like Claude Sonnet captured only 52% of true positives.
  - [corpus] SESR-Eval and AISysRev also suggest efficient LLM screening, but do not systematically compare lightweight vs. reasoning-tier models.
- Break condition: When criteria are highly specific or require nuanced domain reasoning, lightweight models may miss edge cases that reasoning models catch.

### Mechanism 3
- Claim: Dataset characteristics—publication era, abstract length, and open-access status—modulate LLM screening performance.
- Mechanism: Older papers tend to have shorter abstracts with fewer explicit eligibility signals. Closed-access papers are less likely to appear in model pretraining corpora, reducing prior exposure. Both factors lower recall, especially for reviews spanning older literature.
- Core assumption: LLMs partially rely on memorized content from pretraining; exposure improves classification.
- Evidence anchors:
  - [abstract] "LLMs can assist but not replace human screeners, particularly for complex reviews with older or less accessible literature."
  - [section] Review 2 (older, more closed-access) had 46% average recall; actor-critic any-include missed 55% of closed-access articles vs. 12% of open-access. Fisher's exact test showed significant differences (p<0.001).
  - [corpus] No direct corpus validation; neighboring papers do not analyze publication era or open-access effects.
- Break condition: If future models are trained on broader corpora including historical closed-access literature, this mechanism may weaken.

## Foundational Learning

- Concept: Calibration (Brier Score, ECE)
  - Why needed here: Single models exhibited poor alignment between reported confidence and correctness; understanding calibration is essential to interpreting LLM reliability in screening.
  - Quick check question: If a model reports 0.9 confidence on 100 decisions, how many should be correct if well-calibrated?

- Concept: Actor-Critic Architecture
  - Why needed here: OLIVER's primary contribution is structured two-model adjudication; engineers must understand how actor proposals and critic evaluations aggregate.
  - Quick check question: In a critic-veto configuration, what happens when the actor includes and the critic excludes?

- Concept: Sensitivity-Specificity Tradeoff in Screening
  - Why needed here: High-sensitivity configurations generate false positives; high-specificity configurations miss true includes. Workflow design depends on which error type is costlier.
  - Quick check question: For a systematic review, is it worse to miss a relevant study or to screen an irrelevant full text?

## Architecture Onboarding

- Component map:
  Frontend (Streamlit) -> Prompt Generator (structured checklist) -> Request Builder (JSONL) -> Parallel Processing Scripts (provider-specific) -> Actor-Critic Engine (two-pass evaluation) -> Aggregation Engine (any-include/critic-veto/agreement) -> Evaluation Module (metrics computation)

- Critical path:
  1. Define inclusion/exclusion criteria → 2. Generate JSONL requests → 3. Run parallel inference → 4. (If actor-critic) Run second model with adjudication → 5. Aggregate decisions and confidence → 6. Compute metrics against ground truth

- Design tradeoffs:
  - Any-include: Maximizes sensitivity, high false positives
  - Agreement-required: Higher specificity, lower false positives, may miss true includes
  - Lightweight vs. reasoning models: Cost and speed vs. nuanced criterion application

- Failure signatures:
  - Near-zero recall on large, older datasets (as in Review 2) → investigate prompt structure, publication era distribution, open-access status
  - High Brier scores despite high accuracy → model confidence misaligned; consider actor-critic or ensemble calibration
  - API throttling on large batches → verify parallel script rate limits and retry logic

- First 3 experiments:
  1. Run single-model screening with both standard and modified inclusion-biased prompts on a held-out review slice to quantify sensitivity/specificity shift.
  2. Compare actor-critic (any-include vs. agreement-required) on the same dataset to characterize false positive vs. false negative tradeoffs.
  3. Stratify results by publication era and open-access status to assess whether performance gaps replicate the Review 2 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating human feedback via the optional training phase improve LLM screening performance and calibration?
- Basis in paper: [explicit] "While OLIVER supports an optional training phase that incorporates human feedback, this feature was not formally evaluated in the current study, and its impact on screening performance and calibration remains an open question."
- Why unresolved: The authors implemented but did not test the training feature in their evaluation.
- What evidence would resolve it: A prospective study comparing screening accuracy, sensitivity, and calibration metrics with and without the training phase enabled across multiple reviews.

### Open Question 2
- Question: What is the relationship between model-generated justifications and both calibration and accuracy in LLM-assisted screening?
- Basis in paper: [explicit] "Questions remain about the relationship between calibration, accuracy, and model-generated justifications, and warrants further investigation to clarify whether and how LLMs appropriately apply screening criteria."
- Why unresolved: Output length constraints and API throttling prevented collection of model justifications at scale during the study.
- What evidence would resolve it: A study requiring models to output brief rationales alongside decisions, then analyzing whether justification quality correlates with calibration scores and correct classification.

### Open Question 3
- Question: To what extent does pre-training exposure to specific literature (e.g., open-access papers) bias LLM screening performance?
- Basis in paper: [inferred] The authors found OLIVER missed 55% of closed-access articles vs. only 12% of open-access articles in Review 2, but note this analysis was limited because "papers counted as 'closed-access' may have had open-access preprints or manuscripts via Sci-Hub that were missed by our methodology."
- Why unresolved: The open-access analysis was post-hoc and did not systematically verify full-text availability; causality between training data presence and screening performance cannot be established.
- What evidence would resolve it: Controlled experiments using LLMs with known training corpora, screening abstracts from papers with confirmed inclusion or exclusion from pre-training data.

### Open Question 4
- Question: How robust is actor-critic screening performance across heterogeneous review types, domains, and criterion complexity levels beyond the two reviews tested?
- Basis in paper: [explicit] "Future work should address these limitations through broader benchmarking, systematic prompt comparisons, and prospective evaluations in live review settings."
- Why unresolved: Only two reviews were evaluated, and performance varied substantially between them (high sensitivity in Review 1, poor sensitivity in Review 2).
- What evidence would resolve it: Multi-domain benchmarking across diverse review topics, criterion structures (PICO vs. non-PICO), publication date ranges, and abstract characteristics.

## Limitations

- Datasets are not publicly available, limiting independent validation of reported performance.
- Performance differences tied to publication era and open-access status rely on aggregate statistics without granular, reproducible breakdowns.
- Actor-critic architecture's benefits assume independent model errors, which was not empirically verified.
- Calibration gains are reported numerically but not decomposed by review complexity or domain.

## Confidence

- **High Confidence:** Lightweight models outperform reasoning models in sensitivity and cost-efficiency; actor-critic improves calibration and AUC over single models.
- **Medium Confidence:** Performance degradation on older and closed-access abstracts is real but may not generalize to all review domains.
- **Low Confidence:** Exact replication of prompt wording, API parameters, and dataset composition remains uncertain due to partial specification.

## Next Checks

1. Replicate actor-critic gains using synthetic or held-out review data with known labels to confirm calibration and discrimination improvements.
2. Systematically test model independence by comparing correlated vs. uncorrelated error patterns in actor-critic pairs.
3. Stratify performance by publication era and open-access status in a new dataset to validate generalizability of observed trends.