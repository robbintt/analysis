---
ver: rpa2
title: An Evaluation of LLMs Inference on Popular Single-board Computers
arxiv_id: '2511.07425'
source_url: https://arxiv.org/abs/2511.07425
tags:
- inference
- language
- performance
- across
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language model (LLM) inference performance
  on single-board computers (SBCs) by benchmarking 25 quantized open-source models
  across Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro. Using Ollama and Llamafile
  runtimes, the research measures generation throughput, memory usage, and power consumption
  under varying CPU configurations.
---

# An Evaluation of LLMs Inference on Popular Single-board Computers

## Quick Facts
- arXiv ID: 2511.07425
- Source URL: https://arxiv.org/abs/2511.07425
- Authors: Tung Nguyen, Tuyen Nguyen
- Reference count: 40
- Benchmarks 25 quantized LLMs on Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro

## Executive Summary
This study evaluates large language model inference performance on single-board computers by benchmarking 25 quantized open-source models across three popular SBC platforms. Using Ollama and Llamafile runtimes, the research measures generation throughput, memory usage, and power consumption under varying CPU configurations. Results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4× higher throughput and 30-40% lower power usage than Ollama. The Orange Pi 5 Pro demonstrates the best performance, supporting models up to 7B parameters with acceptable throughput.

## Method Summary
The study benchmarks 25 quantized models (q4_k_m) on three SBCs: Raspberry Pi 4 (4GB), Raspberry Pi 5 (8GB), and Orange Pi 5 Pro (6GB). Using Ollama and Llamafile runtimes on headless OS installations, researchers measure tokens/second, peak RAM usage, and power consumption while running three prompts (haiku, question, essay) four times each. CPU frequency is locked to performance mode with active thermal monitoring to prevent throttling.

## Key Results
- Llamafile achieves 3-4× higher throughput and 30-40% lower power consumption than Ollama on equivalent hardware
- Optimal inference on Orange Pi 5 Pro's heterogeneous CPU occurs with 4 performance cores only, not all 8 cores
- Reliable inference (≥5 tokens/second) is achievable only when model parameter count stays well below available RAM, with practical ceiling around 1.5B parameters for mid-range SBCs

## Why This Works (Mechanism)

### Mechanism 1: Runtime-Level Memory and Compute Optimization
Llamafile bundles model weights into single-file executables, eliminating external dependencies and allowing the OS to manage memory and optimization directly. This reduces runtime overhead and enables more efficient CPU utilization, particularly on heterogeneous architectures like the Orange Pi 5 Pro's octa-core RK3588S.

### Mechanism 2: CPU Core Count and Architecture Matching
The Orange Pi 5 Pro uses a big.LITTLE-style architecture with 4 performance cores and 4 efficiency cores. Llamafile's single-threaded optimization strategy benefits from concentrated compute on performance cores, while adding efficiency cores introduces scheduling overhead and memory contention without proportional throughput gains.

### Mechanism 3: Parameter-to-Memory Ratio Thresholding
Quantized models (q4_k_m) require approximately 0.5-0.7 GB per billion parameters plus runtime overhead. The 8GB Raspberry Pi 5 can accommodate models up to 1.5B with headroom for OS and inference runtime, while 3B+ models approach memory saturation, causing swap usage and throughput collapse.

## Foundational Learning

- **Concept: Quantization (q4_k_m specifically)**
  - Why needed here: All 25 models use 4-bit quantization to fit within SBC memory constraints. Understanding what precision is sacrificed and how it affects output quality is essential for interpreting results.
  - Quick check question: Can you explain why q4_k_m reduces model size by ~4× compared to FP16, and what types of tasks are most sensitive to quantization-induced accuracy loss?

- **Concept: Token Generation vs. Prompt Processing Throughput**
  - Why needed here: The paper distinguishes between "In TPS" (prompt encoding) and "Out TPS" (token generation). These have different computational profiles and scale differently with CPU cores.
  - Quick check question: Why would prompt processing be more parallelizable than autoregressive token generation, and how does this affect the core-count scaling observed in Figure 2?

- **Concept: Heterogeneous CPU Architectures (big.LITTLE)**
  - Why needed here: The Orange Pi 5 Pro's performance scales non-linearly with core count because it mixes high-performance and energy-efficient cores. Naive thread pooling across all cores degrades performance.
  - Quick check question: On a CPU with 4 performance cores and 4 efficiency cores, why might a 4-thread workload outperform an 8-thread workload for memory-bound inference tasks?

## Architecture Onboarding

- **Component map:**
Application Layer -> Runtime (Ollama or Llamafile) -> Quantized Model Weights (q4_k_m, .gguf format) -> Hardware Layer (RPi 4, RPi 5, Orange Pi 5 Pro)

- **Critical path:** Model selection (≤1.5B for RPi5, ≤1B for RPi4, ≤7B for Orange Pi) → Quantization format (q4_k_m confirmed working) → Runtime selection (Llamafile for throughput, Ollama for ease of use) → Core configuration (4 performance cores for Orange Pi, all cores for homogeneous CPUs) → Power budget verification.

- **Design tradeoffs:**
  - Llamafile vs. Ollama: Llamafile gives 3-4× throughput and better power efficiency but supports fewer models and requires per-model executable builds. Ollama has broader model support and easier deployment but higher overhead.
  - Core count vs. efficiency: On Orange Pi 5 Pro, 4 cores deliver optimal TPS/watt. Using all 8 cores increases power 55% (9W→14W) without proportional throughput gains.
  - Model size vs. responsiveness: Models ≤360M parameters exceed 20 TPS (suitable for interactive use). Models ≥3B drop below 5 TPS (batch tasks only).

- **Failure signatures:**
  - Throughput drops below 2 TPS with adequate hardware → likely memory saturation, check for swap usage
  - Performance degrades when adding cores on Orange Pi → thread scheduler assigning work to efficiency cores; pin threads to performance cores
  - Inconsistent TPS across runs → thermal throttling; verify CPU frequency governor is set to "performance" not "ondemand"
  - Llamafile crashes on model load → insufficient RAM for runtime + model; reduce model size or enable swap with performance penalty

- **First 3 experiments:**
  1. Run `smollm2:360m` and `tinyllama:1.1b` on your target SBC using both Ollama and Llamafile. Measure TPS, peak RAM, and power draw. Compare against paper benchmarks to validate your measurement setup.
  2. On Orange Pi 5 Pro, run `llama3.2:1b` with Llamafile using 4, 6, and 8 CPU cores. Plot TPS and power draw. Expect peak TPS/watt at 4 cores; document deviation.
  3. Incrementally test larger models (1B → 1.5B → 3B → 7B) on your deployment target until TPS drops below your acceptable threshold or inference fails. This establishes your practical model size limit for that hardware.

## Open Questions the Paper Calls Out

- How do dynamic quantization, hardware-specific scheduling, and mixed precision execution affect inference throughput and power consumption on SBCs compared to the static `q4_k_m` quantization used in this study?

- How does LLM inference performance scale across a broader range of edge hardware platforms and real-world workloads beyond the summarization prompts tested?

- Does the Llamafile runtime maintain its significant throughput and power efficiency advantages over Ollama on the Raspberry Pi 4 and 5, which were excluded from this comparison due to compatibility limitations?

## Limitations

- Evaluation limited to 25 quantized models on three specific SBC platforms, potentially missing model-architecture interactions
- Lack of detailed thermal management specifications makes reproducibility challenging across different environments
- Study focuses exclusively on CPU-based inference without considering GPU acceleration possibilities on supported hardware

## Confidence

**High Confidence (8/10):** Runtime-level optimization claims (Llamafile vs. Ollama performance differences) are well-supported by reported 3-4× throughput improvements and power savings.

**Medium Confidence (6/10):** CPU core scaling results on Orange Pi 5 Pro's heterogeneous architecture are plausible but require additional validation without deeper profiling data.

**Low Confidence (4/10):** Practical model size recommendations are based on specific quantization parameters and runtime configurations that may not generalize to newer model architectures or different quantization schemes.

## Next Checks

1. **Thermal Performance Validation:** Repeat the core scaling experiment on Orange Pi 5 Pro while logging CPU frequency and temperature data. Compare performance across different cooling solutions to establish thermal limits and identify whether performance degradation is thermally or scheduler-induced.

2. **Alternative Quantization Format Comparison:** Benchmark the same model set using q3_K_M and GPTQ quantization formats alongside the original q4_k_m. Measure whether these newer formats can push the viable model size ceiling upward while maintaining acceptable throughput.

3. **GPU Acceleration Assessment:** Implement and benchmark GPU-accelerated inference on the Orange Pi 5 Pro's Mali-G610 MC4 using OpenCL or Vulkan compute pipelines. Compare against CPU-only results to quantify whether the GPU can offset performance limitations with larger models.