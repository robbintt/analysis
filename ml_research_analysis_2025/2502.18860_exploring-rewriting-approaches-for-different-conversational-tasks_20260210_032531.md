---
ver: rpa2
title: Exploring Rewriting Approaches for Different Conversational Tasks
arxiv_id: '2502.18860'
source_url: https://arxiv.org/abs/2502.18860
tags:
- query
- question
- fusion
- conversational
- rewrite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates two query rewriting approaches,
  denoted as rewriting and fusion, across different conversational tasks including
  text-based question-answering and text-to-visualization generation. The proposed
  parameterized framework can recover both approaches at either extreme.
---

# Exploring Rewriting Approaches for Different Conversational Tasks

## Quick Facts
- arXiv ID: 2502.18860
- Source URL: https://arxiv.org/abs/2502.18860
- Reference count: 5
- Primary result: Query rewriting outperforms for text-based Q&A; query fusion outperforms for text-to-visualization tasks

## Executive Summary
This work systematically investigates two query rewriting approaches—query rewriting and query fusion—across conversational tasks including text-based question-answering and text-to-visualization generation. The parameterized framework can recover both approaches at either extreme. Experiments on three datasets show that for conversational question-answering, query rewriting performs best with 3.9% and 9.8% relative gains in cosine similarity and BERT F1 score over query fusion. For conversational data analysis tasks generating visualizations, query fusion achieves 7.6% and 5.2% relative gains in cosine similarity and BERT F1 score over query rewriting.

## Method Summary
The study compares two query rewriting approaches using a parameterized algorithm with an LLM, prompt, chat history k, and previous inputs/outputs. Query Rewrite uses the last k questions and responses (k=5), while Query Fusion uses only the previous rewritten query (k=1, no responses). Both approaches were evaluated on three proprietary datasets: text-based Q&A (179 questions), text-to-vis long (794 questions), and text-to-vis short (171 questions). Performance was measured using cosine similarity and BERT F1 score between generated and ground-truth rewritten query embeddings.

## Key Results
- For conversational question-answering, query rewriting achieved 3.9% and 9.8% relative gains in cosine similarity and BERT F1 score over query fusion
- For text-to-visualization tasks, query fusion achieved 7.6% and 5.2% relative gains in cosine similarity and BERT F1 score over query rewriting
- The optimal rewriting approach highly depends on the underlying use case and generative task

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Context Compression via Recursive Fusion
Query fusion outperforms for incremental modification tasks (e.g., text-to-vis) because it maintains a compact, recursively-built summary that captures the full conversation history without arbitrary window constraints. Each rewritten query becomes the "previous question" for the next turn, accumulating constraints through recursive operations.

### Mechanism 2: Response-Informed Rewriting for Q&A Dependence
Query rewrite outperforms for text-based Q&A because follow-up questions frequently reference information from prior answers, not just prior questions. The algorithm includes both previous inputs and outputs in context, enabling resolution of elliptical or anaphoric follow-ups.

### Mechanism 3: Fixed-k Window Limitation in Standard Rewrite
Standard rewrite with fixed k suffers in long conversations where critical context predates the k-window. With k=5, context from earlier turns is discarded, potentially missing essential information needed for accurate rewriting.

## Foundational Learning

- **Concept: Query Rewriting vs. Query Expansion**
  - Why needed here: The paper positions rewriting as making underspecified queries self-contained, distinct from expansion (adding terms)
  - Quick check question: If a user says "show it as a bar chart" after "compare revenue by country", is the resolution achieved via expansion or rewriting?

- **Concept: Context Window Management**
  - Why needed here: Algorithm 1 centers on selecting k and what to include (inputs, outputs, or prior rewrites)
  - Quick check question: What happens to rewrite quality when k=0 vs. k=5 vs. k=entire history in a 15-turn conversation?

- **Concept: Recursive State Accumulation**
  - Why needed here: Fusion's innovation is using the rewritten prior query as context, not the original prior query
  - Quick check question: After 10 turns of incremental chart modifications, does k=1 fusion encode more, less, or equal information compared to k=5 rewrite? Why?

## Architecture Onboarding

- **Component map:** User Query (Q) → Context Builder (selects from I, O, or R) → LLM (M) + Prompt (P) → Rewritten Query (Q̂) → History Selector (k)

- **Critical path:**
1. Receive current query Q
2. Build context C per approach (rewrite: last k I/O pairs; fusion: last rewritten query)
3. Apply task-specific prompt P
4. LLM generates Q̂
5. Route Q̂ to downstream retrieval or generation

- **Design tradeoffs:**
  - Rewrite (k=5, include O): Better when follow-ups depend on prior answers; risks noise from verbose responses
  - Fusion (k=1, use R): Better for incremental modification tasks; risks concept drift over long chains
  - k selection: Larger k improves coverage but increases latency and potential for conflicting signals

- **Failure signatures:**
  - Rewrite failure: User references "that thing from earlier" beyond k-window; output misses key attributes
  - Fusion failure: Stale constraints persist after user intent changes; cumulative rewrite drifts from original goal
  - Both fail: Topic shifts where prior context should be discarded but isn't

- **First 3 experiments:**
1. **Baseline comparison**: Implement both approaches (rewrite with k=5, fusion with k=1) on your task; evaluate against human-rewritten ground truth using cosine similarity and BERT F1
2. **k-ablation for rewrite**: Sweep k ∈ {1, 3, 5, 10} to identify optimal window for your typical conversation lengths
3. **Error categorization**: Manually inspect failures—distinguish topic-shift errors, long-range dependency misses, and response-dependent resolution failures—to inform approach selection

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified parameterized framework dynamically select the optimal rewriting strategy (Rewrite vs. Fusion) for a given query without manual task-specific tuning? The authors ask whether a single LLM-based query rewrite module can be universally effective across diverse conversational scenarios, or if specialized modules are needed.

### Open Question 2
Does the query fusion approach's capacity for recursive context accumulation generalize to other generative modalities such as text-to-image generation? The authors suggest fusion may be useful for conversational text-to-image generation but do not test this hypothesis.

### Open Question 3
How can the optimal context window size (k) be determined automatically for the query rewrite approach? The paper notes that selecting a good k is fundamentally challenging, but manually set k=5 without addressing adaptive selection.

### Open Question 4
To what extent do semantic similarity metrics (Cosine Similarity, BERT F1) correlate with the functional accuracy of the downstream generative task? The methodology relies entirely on embedding-based text similarity, but the ultimate goal is generating correct visualizations.

## Limitations

- Proprietary datasets used are not publicly available, limiting reproducibility and external validation
- Base LLM model and specific application prompts are not disclosed, making exact reproduction impossible
- Embedding model for evaluation metrics is unspecified, introducing potential variability in reported results

## Confidence

- **High confidence**: The general observation that query rewriting outperforms for Q&A and query fusion outperforms for incremental visualization tasks
- **Medium confidence**: The specific mechanism explanations are plausible but not directly validated through ablation studies
- **Low confidence**: The exact performance metrics (3.9%, 9.8%, 7.6%, 5.2% relative gains) cannot be independently verified

## Next Checks

1. **Mechanism ablation study**: Conduct controlled experiments isolating each mechanism (recursive compression vs response-inclusion vs window limitation) through targeted prompt variations
2. **Cross-dataset generalization**: Test both approaches on publicly available conversational datasets (e.g., QReCC, HotpotQA) to verify the task-dependent performance patterns
3. **Error taxonomy analysis**: Systematically categorize failure modes across both approaches using 100+ annotated examples to quantify the prevalence of topic-shift errors, long-range dependency misses, and concept drift