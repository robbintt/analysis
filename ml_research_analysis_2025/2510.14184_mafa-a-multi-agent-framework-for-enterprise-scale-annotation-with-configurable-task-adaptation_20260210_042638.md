---
ver: rpa2
title: 'MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable
  Task Adaptation'
arxiv_id: '2510.14184'
source_url: https://arxiv.org/abs/2510.14184
tags:
- annotation
- agent
- mafa
- agents
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAFA is a multi-agent framework that enables enterprise-scale annotation
  with configurable task adaptation. It addresses annotation backlogs in financial
  services by combining specialized agents with structured reasoning and a judge-based
  consensus mechanism.
---

# MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation

## Quick Facts
- arXiv ID: 2510.14184
- Source URL: https://arxiv.org/abs/2510.14184
- Authors: Mahmood Hegazy; Aaron Rodrigues; Azzam Naeem
- Reference count: 40
- Multi-agent framework achieving 13.8% higher Top-1 accuracy and eliminating 1M utterance backlog

## Executive Summary
MAFA is a multi-agent framework that addresses annotation backlogs in enterprise settings through configurable task adaptation. Deployed at JP Morgan Chase, it eliminated a 1 million utterance backlog while saving over 5,000 hours of manual annotation work annually. The system combines four specialized agents with a judge-based consensus mechanism to achieve 86% agreement with human annotators. By stratifying outputs by confidence levels, MAFA routes only ambiguous cases to human reviewers, enabling high automation while maintaining quality.

## Method Summary
MAFA uses four specialized ranker agents and one judge agent operating in parallel on GPT-4o. Two agents employ text-embedding-3-large with ANN retrieval, while two use structured prompting without embeddings. A query planner expands ambiguous queries before parallel processing. Each agent receives 8-15 unique few-shot examples via stratified sampling. The judge agent synthesizes outputs using weighted voting and domain-specific reranking, assigning confidence labels (HIGH/MEDIUM/LOW) that determine routing to auto-acceptance or human review. Parallel execution with 500ms timeouts reduces latency from ~2.8s to ~650ms.

## Key Results
- 13.8% higher Top-1 accuracy and 16.9% better F1 score compared to single-agent baselines
- Eliminated 1 million utterance backlog at JP Morgan Chase
- Saved over 5,000 hours of manual annotation work annually
- Achieved 86% agreement with human annotators

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Enhanced Ensemble Synthesis
The framework achieves higher accuracy by aggregating diverse reasoning paths through a judge-based consensus mechanism. Four specialized agents process utterances in parallel using unique few-shot examples and varied retrieval strategies. A Judge Agent synthesizes these outputs using weighted voting and domain-specific reranking, filtering out individual agent errors. The assumption is that agent errors are partially uncorrelated, allowing the Judge to identify consensus where single agents might hallucinate or misclassify.

### Mechanism 2: Structured Prompting for Reasoning Retention
Enforcing structured JSON outputs (Attentive Reasoning Queries) improves task adherence and reduces hallucination compared to free-form generation. By forcing agents to populate specific fields (`intent_analysis`, `relevant_annotations`, `reasoning`), the system combats the "lost in the middle" phenomenon, ensuring critical instructions are attended to at decision points.

### Mechanism 3: Confidence-Calibrated Human-AI Routing
Stratifying outputs by confidence levels allows the system to maximize automation while routing edge cases to humans. The system aggregates agent consensus and scores to assign a confidence label (HIGH/MEDIUM/LOW). Only LOW confidence items are routed for mandatory human review, while HIGH confidence items are auto-accepted, optimizing the cost-quality tradeoff.

## Foundational Learning

- **Mixture of Agents (MoA) / Ensemble Learning**
  - Why needed: MAFA relies on the principle that diverse models correct each other's errors. Understanding MoA is critical to grasping why the system uses 4 parallel agents instead of one larger prompt.
  - Quick check: If three agents return "Login Issue" with high scores and one returns "Card Lost" with a low score, how should the Judge Agent weight these inputs?

- **Prompt Engineering & ARQs (Attentive Reasoning Queries)**
  - Why needed: The system's stability is attributed to structured JSON prompting. You must understand how to define the JSON schema to ensure agents output parseable data and explicit reasoning.
  - Quick check: Why does the paper argue that JSON-based outputs reduce the "lost in the middle" phenomenon compared to standard Chain-of-Thought?

- **Confidence Calibration & Thresholding**
  - Why needed: The business value depends entirely on the confidence thresholds (85/10/5 split). Misunderstanding calibration leads to either overwhelming human reviewers or accepting too many errors.
  - Quick check: What metric should you monitor to determine if the "HIGH" confidence threshold is set too low?

## Architecture Onboarding

- **Component map**: Config Layer -> Query Planner -> Ranker Agents (x4) -> Judge Agent -> Router
- **Critical path**: The Parallel Execution Loop. If the Query Planner fails to expand a query effectively, or if the Ranker Agents timeout, the Judge receives incomplete context, forcing a fallback to simple score aggregation (degraded mode).
- **Design tradeoffs**: Latency reduced from ~2.8s (sequential) to ~650ms (parallel), but adding the Judge adds ~150ms. The paper argues the 6.9% accuracy gain justifies this cost. Using 4 agents + Judge increases API costs compared to a single call, but reduces the "human review" cost load significantly (claimed 85% reduction in single-agent cost).
- **Failure signatures**: Judge Timeout triggers fallback to simple score aggregation. JSON parsing failures use 3-stage recovery: direct parse -> regex extract -> clean markdown. Correlated failure occurs when all agents misclassify an ambiguous query similarly, resulting in High Confidence on a wrong label.
- **First 3 experiments**:
  1. Ablation Study Reproduction: Run the evaluation set with the Judge Agent disabled to verify the reported ~5.7% accuracy drop.
  2. Confidence Threshold Tuning: Vary the HIGH/MEDIUM threshold and plot the curve of "Hours Saved" vs. "Human Agreement Rate" to find the optimal operating point for your specific domain.
  3. Latency Stress Test: Send concurrent requests exceeding the 1,000 QPS baseline to identify the breaking point where timeouts trigger fallback mechanisms, verifying the claimed linear scalability up to 8 instances.

## Open Questions the Paper Calls Out

### Open Question 1
How can the MAFA architecture be extended to accurately resolve single utterances containing multiple simultaneous intents? The current framework assumes a single primary intent per utterance and lacks a mechanism to decompose complex queries into distinct, parallel annotation tasks.

### Open Question 2
Can integrating MAFA's low-confidence predictions into an active learning loop significantly reduce the human labeling effort required for domain adaptation? While the system already routes low-confidence cases to humans, it currently does not utilize this human-verified feedback to update agent prompts or fine-tune the underlying models automatically.

### Open Question 3
Is it possible to automate the selection of optimal annotation configurations (e.g., agent types, few-shot examples) based solely on the characteristics of the input dataset? Users must currently define parameters manually; the paper does not explore methods for the system to self-optimize its architectural topology or prompt strategies for new domains.

## Limitations
- Confidence calibration framework may not generalize beyond banking services
- Effectiveness depends on partial independence of agent errors, not empirically tested under correlated failure
- Specific few-shot examples per agent were not disclosed in sufficient detail to enable exact replication

## Confidence
- **High**: Claims regarding accuracy improvements (13.8% Top-1, 16.9% F1) and deployment metrics (86% human agreement, 5,000+ hours saved) are supported by internal validation and field deployment data
- **Medium**: The diversity-advantage hypothesis for ensemble synthesis is theoretically sound but relies on unverified assumptions about error independence
- **Low**: The exact few-shot examples and daily calibration methodology for judge weights remain proprietary, limiting reproducibility

## Next Checks
1. **Cross-Domain Calibration Test**: Deploy MAFA on a non-banking enterprise dataset and measure whether the 85/10/5 confidence thresholds maintain equivalent automation-to-quality ratios
2. **Correlated Failure Stress Test**: Systematically design adversarial test cases where all agents are likely to share the same error mode, then measure judge performance degradation
3. **Multi-Tenancy Performance Validation**: Scale MAFA to handle concurrent annotation tasks across multiple clients to verify the claimed linear scalability and identify any context-switching overhead