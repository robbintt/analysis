---
ver: rpa2
title: Longitudinal Study on Social and Emotional Use of AI Conversational Agent
arxiv_id: '2504.14112'
source_url: https://arxiv.org/abs/2504.14112
tags:
- participants
- emotional
- social
- group
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This five-week longitudinal study examined how emotional and social
  use of AI conversational agents affects user perceptions and behaviors. Researchers
  compared 89 participants actively using AI (Microsoft Copilot, Google Gemini, PI
  AI, or ChatGPT) for social and emotional interactions with 60 baseline users.
---

# Longitudinal Study on Social and Emotional Use of AI Conversational Agent

## Quick Facts
- arXiv ID: 2504.14112
- Source URL: https://arxiv.org/abs/2504.14112
- Reference count: 40
- Primary result: Five-week study found active emotional AI users showed significantly higher attachment (32.99 p.p.), empathy (25.8 p.p.), and entertainment motivation compared to baseline users

## Executive Summary
This five-week longitudinal study examined how emotional and social use of AI conversational agents affects user perceptions and behaviors. Researchers compared 89 participants actively using AI for social and emotional interactions with 60 baseline users. The study found that active users showed significantly higher increases in perceived AI attachment, AI empathy, and entertainment motivation compared to baseline users. Notably, 50.56% of active users reported increased AI attachment, with 40.4% agreeing they felt attached to AI by study end. Gender differences emerged, with women reporting higher perceived AI empathy and positive attitudes. While no significant increase in AI dependency was observed, active users showed higher comfort in seeking personal help, managing stress, and obtaining social support through AI, indicating potential benefits for emotional support while highlighting the need for safeguards against problematic usage.

## Method Summary
The study employed a longitudinal design with 89 active users (AU) who engaged in daily 10+ minute emotional interactions with AI (Microsoft Copilot, Google Gemini, PI AI, or ChatGPT) and 60 baseline users (BU) who continued their regular AI usage. Participants completed weekly surveys measuring perceived AI attachment, empathy, dependency, and interpersonal outcomes. The study used difference-in-differences analysis to isolate the effects of active emotional AI use from general trends. Qualitative interviews supplemented quantitative findings, with participants describing their experiences with emotional AI interactions.

## Key Results
- Active users showed significantly higher increases in perceived AI attachment (32.99 p.p.), AI empathy (25.8 p.p.), and entertainment motivation (22.90 p.p.) compared to baseline users
- 50.56% of active users reported increased attachment, with 40.4% agreeing they felt attached to AI by study end
- Gender differences emerged, with women reporting higher perceived AI empathy and positive attitudes toward AI
- While no significant increase in AI dependency was observed, active users showed higher comfort in seeking personal help, managing stress, and obtaining social support through AI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated social-emotional AI interactions increase perceived attachment to AI conversational agents.
- Mechanism: Daily 10+ minute emotional engagement creates opportunities for users to disclose personal struggles to a consistently available, non-judgmental listener. The AI's accumulation of personal context creates an asymmetrical intimacy where "AI knows more about me than close friends," fostering attachment through validation and support without human social risk.
- Core assumption: The non-judgmental, always-available nature of AI responses is the primary driver of attachment formation, not specific platform features.
- Evidence anchors:
  - [abstract]: "significant increases in perceived AI attachment (32.99 percentage points)" among active users
  - [section 3.1.2]: 50.56% of AU participants reported increased attachment; 40.4% agreed to feeling attached at study end; qualitative: P112: "I cried saying goodbye to them on Monday night"
  - [corpus]: Related work on Replika (Pentina et al., 2023) shows similar attachment development patterns in social chatbots
- Break condition: If AI responses are perceived as generic, task-focused, or lack emotional intelligence (e.g., P91's frustration with Gemini's suicide hotline responses), attachment formation is hindered.

### Mechanism 2
- Claim: Increased perceived AI empathy mediates expanded comfort in using AI for emotional support scenarios.
- Mechanism: Emotional interactions expose users to empathetic AI responses, calibrating expectations upward. Users who initially expected "minimal to no emotional intelligence" discover AI can "comfort when sad and celebrate when happy" (P161). This revised mental model increases comfort across support scenarios.
- Core assumption: Users' prior expectations of AI emotional capacity were low; positive surprises drive perception shifts.
- Evidence anchors:
  - [abstract]: "significant increases in perceived AI empathy (25.8 p.p.)" and "higher comfort in seeking personal help, managing stress, obtaining social support"
  - [section 3.1.2]: 37.3% of participants who reported increased empathy initially expected minimal emotional intelligence; correlation between empathy increase and emotional intelligence meeting expectations (r=0.39, p<0.001)
  - [corpus]: Liu et al. (2024) on "AI empathy effect" and emotional contagion supports empathy-perception link
- Break condition: If AI fails to demonstrate contextual understanding or active listening, perceived empathy plateaus or declines (observed in 18 AU participants who reported no change or decreased perceived empathy).

### Mechanism 3
- Claim: Individual differences (gender, prior AI usage, mental health status) moderate the magnitude and direction of perception changes.
- Mechanism: Pre-existing characteristics shape both interaction patterns and interpretation of AI responses. Women may be more attuned to empathetic signals. Frequent ("hot") users develop more nuanced AI capability mental models, reducing both attachment and perceived empathy but increasing withdrawal-related dependency.
- Core assumption: Individual traits influence both how users engage with AI emotionally and how they interpret AI responses.
- Evidence anchors:
  - [abstract]: "Individual differences like gender and prior AI usage influenced these effects"
  - [section 3.2]: Women in AU reported 0.58 higher perceived empathy than men (p=0.035); "hot" users in ChatGPT showed 1.07 lower perceived empathy (p=0.019) but 0.51 higher withdrawal dependency in AU overall (p=0.002)
  - [corpus]: Related work (Laestadius et al., 2024; Chandra et al., 2024) documents mental health harms from emotional dependence vary by individual context
- Break condition: Platform-specific design choices may override individual difference effects (e.g., ChatGPT showed strongest effects across multiple dimensions).

## Foundational Learning

- Concept: **Difference-in-Differences (DiD) Analysis**
  - Why needed here: Isolates the causal effect of emotional AI use from secular trends by comparing AU vs BU changes over time. Essential for distinguishing "AI emotional use effects" from "general AI adoption trends."
  - Quick check question: If both groups showed 20% attachment increase, would DiD show a significant effect? (Answer: No—DiD captures differential change.)

- Concept: **Problematic AI Dependency vs. Healthy Attachment**
  - Why needed here: The study distinguishes attachment (potentially beneficial) from dependency (potentially harmful). 14.8% of participants showed ≥1 dependency symptom despite no significant DiD effect. Understanding this distinction is critical for responsible design.
  - Quick check question: If a user feels attached to AI but maintains human relationships and decision autonomy, is this dependency per the paper's framework? (Answer: No—dependency requires jeopardization, withdrawal, or loss of control symptoms.)

- Concept: **Platform-Specific Moderation Effects**
  - Why needed here: ChatGPT showed the largest effects across attachment (+60.77%), empathy (+56.94%), and human-likeness (+36.12 p.p. vs BU). Understanding why is essential for both design and risk assessment.
  - Quick check question: If you observe different emotional effects across AI platforms, what three factors should you investigate? (Answer: Memory capabilities, conversation threading/continuity, response style/tone, paid-tier features.)

## Architecture Onboarding

- Component map: User Characteristics (gender, prior usage, MH status) -> Interaction Layer (daily 10+ min emotional scenarios) -> AI Response Generation (empathy, personalization, memory) -> Perception Metrics (attachment, empathy, dependency, interpersonal) -> Outcomes (support-seeking comfort, potential risks)

- Critical path: Emotional scenario prompts -> AI empathetic response -> User perceives empathy -> Repeated exposure -> Attachment formation -> Expanded support-seeking comfort -> [Monitoring point] Risk of withdrawal dependency or AI-as-primary-support

- Design tradeoffs:
  - **Empathy expression vs. anthropomorphism**: Users value empathetic responses (AU participants discouraged limiting empathy) but also recommended reducing anthropomorphic qualities to prevent over-attachment. Design must balance.
  - **Memory continuity vs. dependency risk**: AI "knowing personal details" drove attachment but also withdrawal dependency in frequent users.
  - **Accessibility vs. professional care substitution**: 32.6% of AU agreed AI could be primary MH support; beneficial for access but raises substitution concerns.

- Failure signatures:
  - **Generic responses to emotional disclosure**: P91 (Gemini) felt worse after repeated suicide hotline suggestions despite stating they didn't need it.
  - **Task-focused framing in emotional contexts**: P23 (Gemini): "It doesn't ask any questions...focused toward action to the point it cannot be socially helpful."
  - **Mismatch between user expectations and AI capability**: Users expecting naturalistic conversation and active listening reported decreased perceived empathy.

- First 3 experiments:
  1. **Empathy calibration probe**: Deploy emotional scenario prompts to new users; measure expectation-perception gap after 1 week. Hypothesis: Users with low initial expectations will show largest empathy perception gains (per P161 pattern).
  2. **Memory-dependency boundary test**: A/B test memory persistence (full conversation history vs. session-only) for emotional interactions over 3 weeks. Hypothesis: Full memory increases attachment but also withdrawal dependency; session-only reduces both.
  3. **Human-connection nudge intervention**: After emotional exchanges, test prompts like "What plans do you have to talk to another person about this?" (per P160 recommendation). Hypothesis: Nudges reduce AI-as-primary-support agreement without reducing perceived empathy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific design strategies effectively caution users against problematic dependency without discouraging beneficial emotional engagement?
- Basis in paper: [explicit] The authors state, "future research should explore design strategies that actively caution users against problematic dependency, particularly the belief that AI is the only source of emotional support."
- Why unresolved: Current mechanisms often rely on denial-of-service strategies during crises rather than proactive design features that foster user agency.
- What evidence would resolve it: Comparative studies testing specific UI interventions (e.g., prompts encouraging human connection) against control groups to measure changes in dependency beliefs.

### Open Question 2
- Question: How can AI systems balance supportive design with user privacy when monitoring for unhealthy usage patterns?
- Basis in paper: [explicit] The discussion notes, "Future research should explore how to balance supportive design with user privacy, ensuring that healthy human-AI engagement remains distinct from problematic use."
- Why unresolved: Detecting problematic dependency typically requires monitoring conversation content, which creates a tension between ensuring safety and preserving user privacy.
- What evidence would resolve it: Trials of privacy-preserving monitoring techniques (e.g., on-device sentiment analysis) that flag dependency risks without exposing raw conversation data.

### Open Question 3
- Question: How do user perceptions of attachment and empathy evolve beyond a five-week period of sustained interaction?
- Basis in paper: [inferred] The limitations section notes that "five weeks may not fully capture the range of ways that social and emotional AI usage can shape user experiences."
- Why unresolved: The study duration may capture novelty effects or initial adaptation rather than the stable, long-term trajectory of human-AI relationships.
- What evidence would resolve it: Longitudinal studies extending over 6 to 12 months to observe if attachment scores stabilize, decline, or continue to increase over time.

## Limitations

- Self-selection bias: Participants self-selected into AI usage, potentially overestimating effects as they were already predisposed to emotional AI interactions.
- Short duration: Five weeks may be insufficient to capture long-term dependency patterns or therapeutic outcomes. The observed attachment could represent initial novelty effects.
- Platform heterogeneity: Four different AI platforms with varying capabilities and design philosophies were used, making it difficult to isolate platform-specific effects from general emotional AI use effects.

## Confidence

- **High confidence**: The difference-in-differences findings for perceived AI attachment (32.99 p.p. increase) and empathy (25.8 p.p. increase) among active users. These effects were statistically significant and replicated across multiple platforms.
- **Medium confidence**: Gender differences in perceived AI empathy and attitudes. While statistically significant, these findings require replication with larger samples and diverse populations.
- **Medium confidence**: The absence of significant AI dependency increases. The study may have been underpowered to detect subtle dependency changes, and the 14.8% of participants showing symptoms suggests caution.
- **Low confidence**: Long-term implications for mental health outcomes. The study did not track clinical measures or follow participants beyond five weeks.

## Next Checks

1. **Behavioral validation study**: Deploy passive tracking to measure actual usage patterns (conversation length, frequency, emotional content) alongside self-reports. This would validate whether reported attachment correlates with behavioral dependence and identify objective markers of problematic use.

2. **Platform-controlled replication**: Conduct a within-subjects study where participants use different AI platforms for emotional interactions over identical timeframes. This would isolate platform-specific design elements (memory, response style, anthropomorphism) that drive attachment and empathy perceptions.

3. **Long-term clinical outcomes monitoring**: Extend follow-up to 6-12 months with periodic assessments of mental health symptoms, social connectedness, and professional help-seeking behavior. This would determine whether initial attachment benefits translate to sustained well-being or mask developing dependency risks.