---
ver: rpa2
title: 'Direct Preference Optimization with Rating Information: Practical Algorithms
  and Provable Gains'
arxiv_id: '2602.00603'
source_url: https://arxiv.org/abs/2602.00603
tags:
- rdpo
- rating
- ml-rdpo
- data
- rout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RDPO, RIPO, and ML-RDPO algorithms that leverage
  rating gap information in addition to pairwise preferences for aligning large language
  models. The key idea is to modify the direct preference optimization objective by
  incorporating rating differences, enabling faster statistical convergence and robustness
  to rating noise.
---

# Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains

## Quick Facts
- arXiv ID: 2602.00603
- Source URL: https://arxiv.org/abs/2602.00603
- Reference count: 40
- Primary result: RDPO and ML-RDPO achieve exponential improvements over DPO in sample complexity when ratings are accurate

## Executive Summary
This paper proposes RDPO, RIPO, and ML-RDPO algorithms that leverage rating gap information in addition to pairwise preferences for aligning large language models. The key idea is to modify the direct preference optimization objective by incorporating rating differences, enabling faster statistical convergence and robustness to rating noise. Theoretical analysis shows that RDPO and ML-RDPO achieve exponential improvements over DPO in sample complexity when ratings are accurate, and remain competitive when ratings are noisy. Empirically, RDPO and ML-RDPO achieve higher win rates against reference models on AlpacaEval and ArenaHard benchmarks compared to DPO, IPO, and other rating-augmented methods, with RDPO showing the most consistent performance across different model sizes and datasets.

## Method Summary
The paper introduces three algorithms that incorporate rating information into preference optimization: RDPO modifies the DPO loss by adding a term proportional to the rating gap (Δr̂) with weight β/β₁, RIPO uses a quadratic penalty on the rating gap, and ML-RDPO jointly models preferences and ratings using maximum likelihood with a Gaussian rating noise assumption. All methods are trained by fine-tuning base models (Zephyr-7B, Llama-3.1-8B, Mistral-7B) on the ultrafeedback_binarized dataset with rating gaps extracted from GPT-4 scores. Hyperparameters β₁ and V are ablated on Zephyr-7B before transfer to other models.

## Key Results
- RDPO achieves 68.7% win rate vs 58.3% for DPO on AlpacaEval benchmark
- ML-RDPO reduces sample complexity from O(e^{Rmax}) to O(R²max + V) when ratings are accurate
- RDPO maintains DPO-level performance even with 30% rating noise corruption
- Theoretical bounds show RDPO/ML-RDPO suboptimality scales with min{Err_DPO(N,δ), Err(r̂)}

## Why This Works (Mechanism)

### Mechanism 1
Incorporating rating gaps directly into the reward model enables faster convergence than ranking-only methods. RDPO modifies the RLHF objective to maximize a linear combination of latent reward r and rating estimate r̂, yielding a closed-form solution π*(a|x) ∝ π_ref(a|x) exp(r/β + r̂/β₁). The rating term β/β₁·Δr̂ shifts the implicit reward target, allowing the policy to learn precise preference magnitudes rather than just binary direction. Core assumption: Rating gaps Δr̂(x,a⁺,a⁻) approximate true latent reward differences Δr*(x,a⁺,a⁻) with bounded error Err(r̂). Evidence: Theoretical analysis shows exponential improvements over DPO in sample complexity when ratings are accurate. Break condition: If Err(r̂) >> Err_DPO(N,δ), the bound degrades to match standard DPO.

### Mechanism 2
Maximum-likelihood estimation of the joint rating-ranking distribution provides an alternative integration path with graceful handling of partial data. ML-RDPO assumes conditional independence of ratings and rankings given (x,a,a'), with Gaussian rating gaps ~N(Δr*, V). The loss L_ML-RDPO = -log σ(βΔθ) + (Δr̂ - βΔθ)²/(2V) decomposes into separate ranking (DPO) and rating (Distilled-DPO) terms, enabling training even when only subsets of data have both signals. Core assumption: Rating gaps are Gaussian-distributed with known variance V; preferences follow Bradley-Terry model. Evidence: Exponential improvement in sample complexity scaling from O(e^{Rmax}) to O(R²max + V). Break condition: Non-Gaussian rating distributions or heavy-tailed noise violate the V-dependent acceleration.

### Mechanism 3
Rating information breaks the exponential sample complexity dependence on reward range that limits all ranking-only offline algorithms. Standard DPO requires O(e^{Rmax}) samples because it must infer reward magnitudes from binary preferences alone. Rating gaps directly provide reward differences, reducing dependence to polynomial O(R²max + V) when V is small. This is an exponential improvement in the Rmax dimension. Core assumption: Offline setting with fixed dataset. Evidence: Theoretical analysis shows polynomial dependence on Rmax and V instead of exponential. Break condition: High rating variance V >> e^{Rmax}R²max eliminates the exponential advantage.

## Foundational Learning

- Concept: Bradley-Terry preference model
  - Why needed here: All three algorithms derive their loss functions from the assumption that preference probability P(a≻a'|x) = σ(r(x,a) - r(x,a')). Understanding this links reward differences to observed binary labels.
  - Quick check question: Given r(x,a)=2 and r(x,a')=-1, what is the probability a is preferred over a'?

- Concept: KL-regularized RLHF objective
  - Why needed here: The derivation of DPO and its variants depends on the closed-form solution to max_π E[r] - β·KL(π||π_ref). RDPO modifies this to include the rating term.
  - Quick check question: Why does adding KL regularization yield a closed-form solution rather than requiring iterative optimization?

- Concept: Concentrability coefficient C_π = E_{x~ν₀}[Σ_a π(a|x)²/π_data(a|x)]
  - Why needed here: Theoretical bounds in Theorems 4.3 and 4.4 involve C_max or C_*, which measure how far the learned policy can deviate from the data distribution. This determines the statistical rates achievable.
  - Quick check question: If π_data = π_ref and π is very different from π_ref, will C_π be large or small?

## Architecture Onboarding

- Component map: ultrafeedback_binarized dataset → RDPO/RIPO/ML-RDPO loss computation → fine-tuned LLM policy → win rate evaluation against reference models

- Critical path: Hyperparameter selection (β₁ for RDPO, V for ML-RDPO) must precede training. The paper ablated 5 values on Zephyr-7B before generalizing. Incorrect β₁/V destroys benefits.

- Design tradeoffs:
  - RDPO vs ML-RDPO: RDPO requires rating gaps for all pairs; ML-RDPO handles partial ratings but assumes Gaussian noise
  - Low β₁ (high trust in ratings): Faster convergence with accurate ratings; catastrophic with noisy ratings
  - High β₁ (low trust): Robust but limited acceleration

- Failure signatures:
  - Win rate drops below DPO baseline → β₁ too small for rating quality (over-trusting noisy ratings)
  - High variance across seeds → rating data corrupted or β₁/V mismatched
  - No improvement over DPO with small V/β₁ → rating gaps not actually informative (Err(r̂) large)

- First 3 experiments:
  1. Sanity check on synthetic ratings: Generate preference data with known r*, inject Gaussian noise to create r̂ with controlled V, verify ML-RDPO achieves theoretical sample complexity reduction over DPO
  2. β₁/V ablation: For each base model (Zephyr, Mistral, Llama), run grid search over {0.1, 0.5, 1.0, 2.0, 3.0} for V and {1/100, 1/40, 1/10, 1, 10} for β₁. Plot win rate vs hyperparameter to identify robust regions
  3. Robustness stress test: Swap 0%, 10%, 30% of chosen/rejected ratings in training data (Figure 3a protocol). Verify that large β₁ maintains DPO-level performance while small β₁ degrades

## Open Questions the Paper Calls Out

- Can alignment algorithms be developed to utilize individual absolute ratings rather than just the relative rating gap? The proposed algorithms only leverage the rating gap but may not be able to use all available information when individual ratings are available.

- How does the optimal level of trust in rating information (hyperparameter β₁) scale with model size? Experiments on smaller models suggest the model size plays a role in choosing the right level of trust, calling for deeper understanding of this phenomenon.

- Is the theoretical restriction to a constrained policy class Π' necessary for RDPO's convergence guarantees in practice? Theorem 4.3 requires minimization over a constrained policy class Π', but Section F.5 notes this restriction is optional in practice and might be avoided.

## Limitations

- Dataset dependency: Empirical validation relies on ultrafeedback_binarized dataset where rating gaps are extracted from GPT-4 scores. Quality and distribution of these rating gaps directly determines algorithm performance.

- Hyperparameter sensitivity: Optimal values of β₁ and V appear dataset-dependent. Values selected on Zephyr-7B were transferred to other models without further tuning, potentially limiting observed benefits.

- Theoretical assumptions: Exponential sample complexity improvements require accurate ratings and Gaussian-distributed rating noise. Real-world rating data often violates these assumptions through systematic biases or non-Gaussian noise.

## Confidence

- High confidence: The theoretical framework and mathematical derivations are internally consistent. The closed-form solutions for RDPO and ML-RDPO follow logically from their respective objectives.

- Medium confidence: Empirical results demonstrate consistent improvements over DPO, but magnitude of improvements varies across model sizes and datasets. Ablation studies provide reasonable evidence but are limited to a single dataset and model architecture.

- Low confidence: Generalization of hyperparameter choices (β₁=1/10, V=1/100) across different models and datasets. Performance under different types of rating noise beyond controlled 10% corruption. Behavior on datasets with substantially different rating distributions.

## Next Checks

1. **Synthetic rating noise study**: Generate synthetic preference data with known reward functions, inject various types of rating noise (Gaussian, heavy-tailed, systematic bias), and measure how RDPO/ML-RDPO performance degrades relative to theoretical predictions.

2. **Cross-dataset generalization**: Apply RDPO and ML-RDPO to datasets with different rating distributions (e.g., human ratings vs GPT-4 ratings, different rating scales) while keeping hyperparameters fixed. Measure whether optimal β₁/V values transfer or if dataset-specific tuning is required.

3. **Ranking-only baseline comparison**: Implement a version of DPO that uses implicit reward estimation from the rating gaps rather than explicit rating terms. Compare whether RDPO's advantage comes from explicit rating integration or simply from better reward estimation achievable without rating information.