---
ver: rpa2
title: Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments
arxiv_id: '2601.11093'
source_url: https://arxiv.org/abs/2601.11093
tags:
- text
- substring
- answer
- question
- replacement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Integrity Shield, a document-layer watermarking
  system designed to address academic integrity concerns arising from large language
  models' ability to solve entire exams directly from uploaded PDFs. The system embeds
  schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible
  appearance unchanged, using techniques such as invisible text, glyph remapping,
  and layout perturbations to influence model parsing.
---

# Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments

## Quick Facts
- arXiv ID: 2601.11093
- Source URL: https://arxiv.org/abs/2601.11093
- Authors: Ashish Raj Shekhar; Shiven Agarwal; Priyanuj Bordoloi; Yash Shah; Tejas Anvekar; Vivek Gupta
- Reference count: 40
- Primary result: Achieves 91-94% exam-level blocking and 89-93% signature retrieval across 30 exams spanning STEM, humanities, and medical reasoning.

## Executive Summary
Integrity Shield is a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs to prevent MLLMs from answering while enabling authorship attribution. By exploiting the render-parse gap in PDFs through invisible text, glyph remapping, and layout perturbations, the system achieves exceptionally high prevention rates (91-94%) and strong detection reliability (89-93%) across four commercial MLLMs. The system provides instructors with actionable, item-level evidence of AI reliance through an interactive interface for uploading exams, previewing watermark behavior, and inspecting pre/post AI performance and authorship evidence.

## Method Summary
Integrity Shield uses a four-stage pipeline: PDF parsing via PyMuPDF to extract question schemas, an LLM-based planner that assigns watermark tactics per item type (MCQ, T/F, long-form), a PDF rewriting engine that applies invisible text injection, CMap glyph remapping, and clipped overlays, and an authorship engine that checks responses against stored watermark signatures. The system uses two watermark variants—IS-v1 (lighter perturbations) and IS-v2 (denser multi-layer perturbations)—and evaluates across 30 exams using four commercial MLLMs with structured prompts. Prevention is measured as exam-level refusal/failure rate, while detection scores the proportion of responses following watermark signatures.

## Key Results
- Achieves 91-94% exam-level blocking of MLLM responses across 30 benchmark exams
- Maintains 89-93% signature retrieval reliability for authorship attribution
- Demonstrates schema-aware adaptation with consistent prevention across MCQ (3.6-7.8% residual accuracy), T/F (3.6-6.5%), and long-form (3.1-5.2%) questions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Document-layer perturbations can alter model interpretation without changing human-visible content.
- **Mechanism:** The system exploits the render-parse gap in PDFs—what humans see differs from what AI parsers ingest. By injecting invisible text, glyph remappings, and lightweight overlays at the PDF substrate, the system influences model parsing while preserving visual appearance.
- **Core assumption:** MLLM PDF parsers ingest raw PDF data structures rather than visual renderings.
- **Evidence anchors:** [abstract] "embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged, using techniques such as invisible text, glyph remapping, and layout perturbations"; [Section 3.2] "injects invisible text spans anchored near stems and options, applies CMap-based glyph remapping so that visually identical tokens are parsed differently by models"
- **Break condition:** If MLLMs switch to vision-based document understanding (parsing rendered pixels rather than PDF structures), document-layer perturbations would likely lose effectiveness.

### Mechanism 2
- **Claim:** Schema-aware watermarking improves prevention and detection by adapting tactics to question type.
- **Mechanism:** An LLM-driven planner assigns different watermark strategies per item: target distractors for MCQ/true-false questions, signature keyphrases for long-form questions. This adapts perturbation intensity to question structure.
- **Core assumption:** Different question types have different vulnerabilities; uniform perturbations are suboptimal.
- **Evidence anchors:** [abstract] "schema-aware, item-level watermarks"; [Section 3.1] "adapts watermark tactics to the item schema, treating MCQ, true/false, and Long-Form questions differently"; [Table 2] Shows consistent prevention across MCQ (3.6-7.8% residual accuracy), T/F (3.6-6.5%), and long-form (3.1-5.2%) with IS watermarks applied.
- **Break condition:** If question formats evolve or models become robust to schema-specific perturbations, the planner may need retraining.

### Mechanism 3
- **Claim:** Embedded signatures can be reliably recovered from model outputs for authorship attribution.
- **Mechanism:** Watermarks encode stable item-level signatures (target distractors or keyphrases). The authorship engine checks whether responses follow these signatures—for MCQ/T/F, exact distractor matching; for long-form, a judge LLM scores keyphrase alignment.
- **Core assumption:** Models produce outputs that consistently reflect the perturbed PDF content they ingested.
- **Evidence anchors:** [abstract] "encode stable, item-level signatures that can be reliably recovered from model or student responses... strong detection reliability (89-93% signature retrieval)"; [Section 3.2] "for each item, the authorship engine checks whether the response follows the stored watermark signature"; [Table 3] Demonstrates that IS-v1 and IS-v2 drive models to distinct distractors (B vs C), enabling separable attribution.
- **Break condition:** If students or models paraphrase responses heavily, signature retrieval for long-form questions may degrade (the paper does not report robustness to paraphrasing).

## Foundational Learning

- **Concept:** PDF structure vs. rendered output (CMaps, glyph encoding, invisible layers)
  - **Why needed here:** The entire system operates at the PDF substrate level; understanding how PDFs encode text separately from display is essential.
  - **Quick check question:** Can you explain how a CMap remapping could make "A" display as "A" but parse as "B"?

- **Concept:** MLLM document ingestion pipelines (PDF parsing vs. vision-based understanding)
  - **Why needed here:** Mechanism effectiveness depends on MLLMs parsing raw PDF data; if models shift to vision-only approaches, the system may fail.
  - **Quick check question:** Does GPT-4o's document mode extract text via OCR or direct PDF parsing for embedded fonts?

- **Concept:** Watermark detection vs. prevention objectives
  - **Why needed here:** The system serves dual purposes (blocking answers and attributing AI use); these have different design constraints.
  - **Quick check question:** If a watermark fully prevents a model from answering, can you still detect AI reliance from the output?

## Architecture Onboarding

- **Component map:** Document Ingestion Service -> Strategy Planner -> PDF Rewriting Engine -> Authorship & Calibration Service -> Web Frontend

- **Critical path:** Upload PDF → Ingestion (schema extraction) → Planner (tactic assignment) → Rewriting (watermark injection) → Verification (visual unchanged) → Calibration (model testing) → Distribution to students → Response analysis (signature retrieval)

- **Design tradeoffs:** IS-v1 vs. IS-v2 uses lighter vs. denser perturbations; prevention vs. detection emphasis creates tension; judge LLM for long-form adds dependency and latency

- **Failure signatures:** Models switch to vision-based parsing → Prevention and detection drop sharply; screen readers expose hidden text → Students notice; paraphrasing degrades long-form signature retrieval; watermark signatures collide → Attribution ambiguity

- **First 3 experiments:** 1) Baseline validation on single model: Verify IS-v1/IS-v2 reduce accuracy from 95%+ to <10% with no visual changes; 2) Cross-model transfer test: Apply watermarks generated for GPT to Claude, Grok, Gemini; 3) Signature retrieval robustness: Apply light paraphrasing to model responses and measure detection score degradation

## Open Questions the Paper Calls Out

- **Question:** How does Integrity Shield impact accessibility tools such as screen readers, and can the system preserve both watermark effectiveness and equitable access for students using assistive technologies?
  - **Basis in paper:** [explicit] The Limitations section states: "Real-world deployments may involve broader variation in... accessibility workflows (e.g., screen readers)."

- **Question:** How frequently must watermark configurations be recalibrated as MLLM PDF-parsing pipelines evolve, and what early-warning signals indicate declining prevention or detection performance?
  - **Basis in paper:** [explicit] The Limitations section notes: "As MLLMs and their PDF-parsing pipelines evolve, watermark robustness may drift, necessitating periodic re-calibration."

- **Question:** How does watermark effectiveness generalize across languages, non-Latin scripts, and document formats beyond standard university PDF assessments?
  - **Basis in paper:** [inferred] The benchmark uses English-language exams from Western universities; the Limitations mention "broader variation in domains, languages" without providing multilingual or cross-cultural evaluation.

## Limitations

- **Access Method Dependence**: Effectiveness hinges on models parsing PDFs via data structures rather than visual rendering, making it vulnerable to model updates shifting to vision-based understanding.

- **Signature Robustness for Long-Form**: Long-form attribution depends on an external LLM judge scoring keyphrase alignment, with no reported quantification of how paraphrasing affects signature retrieval rates.

- **Accessibility Implications**: Invisible text and glyph remapping may be exposed by screen readers or accessibility tools, potentially alerting students to tampering, but this was not tested.

## Confidence

- **High Confidence**: The paper's core empirical results (91-94% prevention, 89-93% detection) are internally consistent and supported by reported tables and methodology, assuming the stated experimental setup holds.

- **Medium Confidence**: The mechanism explanation (render-parse gap exploitation, schema-aware adaptation) is plausible and grounded in related work, but the paper does not directly validate that models ingest the specific PDF structures being perturbed.

- **Low Confidence**: Claims about real-world robustness (accessibility safety, paraphrase resistance, cross-model generalization beyond tested parsers) lack empirical backing in the paper.

## Next Checks

1. **Access Method Robustness Test**: Apply Integrity Shield watermarks to the same exam set and test against a vision-based parser (e.g., GPT-4o with document vision mode) to measure prevention/detection drop-off if models shift from structure to pixel parsing.

2. **Long-Form Signature Robustness**: Generate model responses to watermarked long-form questions, then apply controlled paraphrasing/translation. Measure how detection scores degrade to bound real-world retrieval reliability.

3. **Accessibility Exposure Audit**: Run watermarked PDFs through screen readers (NVDA, VoiceOver) and accessibility checkers (PAC3). Document any exposure of invisible text or overlays that could alert students.