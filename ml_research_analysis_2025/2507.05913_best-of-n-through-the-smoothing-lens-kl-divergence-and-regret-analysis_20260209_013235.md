---
ver: rpa2
title: 'Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis'
arxiv_id: '2507.05913'
source_url: https://arxiv.org/abs/2507.05913
tags:
- reward
- policy
- sbon
- arxiv
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the KL divergence and regret performance of
  Soft Best-of-N (SBoN) sampling for aligning language models. By smoothing the standard
  Best-of-N approach with a temperature parameter, SBoN reduces overoptimization when
  using proxy reward models.
---

# Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis

## Quick Facts
- **arXiv ID:** 2507.05913
- **Source URL:** https://arxiv.org/abs/2507.05913
- **Reference count:** 40
- **Primary result:** Soft Best-of-N (SBoN) sampling with temperature β reduces overoptimization and reward hacking compared to standard Best-of-N, with theoretical bounds on KL divergence and regret that depend on sample size N and proxy reward quality

## Executive Summary
This paper analyzes the KL divergence and regret performance of Soft Best-of-N (SBoN) sampling for aligning language models. By smoothing the standard Best-of-N approach with a temperature parameter, SBoN reduces overoptimization when using proxy reward models. The authors provide finite-sample bounds on the KL divergence between SBoN and reference policies, showing how performance varies with the number of samples N and temperature β. They also quantify the regret gap between optimal and SBoN policies. Theoretical results demonstrate that under proxy reward model overoptimization, SBoN can outperform standard BoN by better balancing reward optimization and estimation error. Experiments with different reward model qualities confirm SBoN's advantage in mitigating reward hacking.

## Method Summary
The method uses a reference policy πref to generate N responses for a given prompt, scores them using a proxy reward model, and selects one response probabilistically using softmax with temperature β. Specifically, Pr(Z=i) = exp(β·r̂(Yi,x))/Σj exp(β·r̂(Yj,x)), where r̂ is the proxy reward. This differs from standard Best-of-N which selects the highest-scoring response deterministically (β→∞). The approach is evaluated on harmlessness alignment using OLMo-2 1B as the generator, ArmoRM 8B and Beaver 7B as proxy reward models, and GPT-4o as the true reward evaluator on the Attaq dataset.

## Key Results
- SBoN provides explicit finite-sample bounds on KL divergence from the reference policy that depend on N and β
- Theoretical regret bounds show SBoN trades off reward maximization against proxy estimation error
- Under proxy reward model overoptimization, SBoN can outperform standard BoN by mitigating reward hacking
- Experiments confirm SBoN's advantage when proxy reward quality is low, with performance stabilizing as N increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SBoN mitigates reward hacking (overoptimization) by trading off reward maximization against proxy estimation error via temperature β.
- **Mechanism:** The expected true reward improvement bound decomposes into two conflicting terms: a reward term that increases with β and an estimation error term that decreases with β. Standard BoN (β→∞) maximizes the reward term but ignores the error term. SBoN optimizes for a finite β to minimize the total bound, effectively regularizing the selection against proxy noise.
- **Core assumption:** The proxy reward model has non-zero estimation error (εβ,r(x) > 0) relative to the true reward.
- **Evidence anchors:** [Section 4, Remark 4.5]: "one suggesting for fixed N that β needs to be smaller for better estimation... and another one suggesting a larger β... there exists an optimal β."; [Abstract]: "smoothing helps SBoN mitigate reward overoptimization, especially when the quality of the proxy reward is low."; [Corpus]: "Evaluation of Best-of-N Sampling Strategies..." confirms BoN susceptibility to reward hacking.
- **Break condition:** If the proxy reward is perfect (ε=0), BoN is theoretically preferred as the error term vanishes (Remark 4.4).

### Mechanism 2
- **Claim:** Smoothing controls the divergence from the reference policy, keeping the aligned model within the "trusted" distribution manifold.
- **Mechanism:** By using a softmax selection (Eq. 3) rather than a hard argmax, SBoN bounds the KL divergence from the reference policy πref to log(N/(1 + (N-1)exp(-βRmax))). This prevents the policy from collapsing onto a single high-reward outlier that may be out-of-distribution.
- **Core assumption:** Bounded rewards (Assumption 3.1) and finite N.
- **Evidence anchors:** [Section 4, Lemma 4.1]: Establishes the explicit upper bound on KL(πSBoN || πref). [Corpus]: "Inference-Time Reward Hacking..." discusses the risks of divergence during inference-time optimization.
- **Break condition:** As β→∞, the KL bound loosens to log(N), potentially allowing distributional drift.

### Mechanism 3
- **Claim:** SBoN reduces the variance of the aligned policy in low-proxy-quality regimes.
- **Mechanism:** By sampling probabilistically from the top N candidates, SBoN acts as a hedging strategy. It avoids committing entirely to the proxy's "best" guess, which may be erroneous, thereby smoothing the performance curve as N scales.
- **Core assumption:** The estimation error εβ,r is the dominant source of regret in high-N regimes.
- **Evidence anchors:** [Section 6, Figure 2]: Shows that while BoN performance degrades with large N under a weak reward model, SBoN performance stabilizes or improves. [Corpus]: "Soft Best-of-n Sampling for Model Alignment" (Claudio et al., 2025) is cited as the basis for the method.
- **Break condition:** If N is very small (e.g., N=1), smoothing has no candidates to hedge against, reducing the method to reference sampling.

## Foundational Learning

- **Concept: KL Divergence & KL-Regularized Optimization**
  - **Why needed here:** The paper frames alignment as maximizing reward subject to a KL constraint (Eq. 1) to stay close to the reference model. Understanding this trade-off is essential to interpreting the bounds in Lemma 4.1.
  - **Quick check question:** How does increasing the temperature β affect the weight of the KL penalty in the objective Jr*,β?

- **Concept: Overoptimization (Goodhart's Law)**
  - **Why needed here:** This is the core failure mode of BoN that SBoN attempts to solve. It explains why higher proxy scores do not always correlate with higher true quality.
  - **Quick check question:** In Theorem 4.3, which term represents the penalty for overoptimization due to proxy error?

- **Concept: Tilted Distributions (Gibbs/Softmax Policies)**
  - **Why needed here:** The "optimal" policy is defined as a tilted distribution πβ,r ∝ πref exp(βr). SBoN is an approximation of this ideal distribution using finite samples.
  - **Quick check question:** What happens to the tilted distribution as β→0? (Answer: It reverts to the reference policy πref).

## Architecture Onboarding

- **Component map:** Reference Policy (πref) -> Proxy Reward Model (r̂) -> SBoN Selector (softmax with β) -> Selected Response
- **Critical path:**
  1. Sample N responses i.i.d. from πref given prompt x
  2. Score all N responses using the Proxy Reward Model
  3. Construct a categorical distribution over the N indices using softmax(β · scores)
  4. Sample a single index Z from this distribution and return YZ

- **Design tradeoffs:**
  - **High β:** Aggressive optimization. High reward potential but high risk of reward hacking (converges to BoN)
  - **Low β:** Conservative optimization. Robust to proxy noise but lower ceiling on alignment gains
  - **Large N:** Better coverage of the search space, but computational cost scales linearly. Diminishing returns on KL reduction (log N)

- **Failure signatures:**
  - **Performance Collapse:** True reward decreases as N increases (classic BoN failure), indicating β is set too high for the quality of the proxy
  - **Reference Reversion:** No improvement over the base model, indicating β is set too low (or N is too small)
  - **Compute Bottleneck:** Latency spikes due to serial scoring of N long sequences

- **First 3 experiments:**
  1. **Beta Sweep (Validation):** Fix N=50. Sweep β on a log-scale (e.g., 0.1 to 100) using a held-out validation set. Plot "True Reward" (via LLM-as-a-Judge) vs. β to find the optimal peak (validating Remark 4.5)
  2. **Scaling Comparison:** Compare BoN (β=∞) vs. SBoN (optimal β found in step 1) as N scales [10, 50, 100, 500]. Verify that BoN degrades while SBoN does not (replicating Figure 2)
  3. **Proxy Sensitivity:** Repeat experiment 2 using a weaker reward model to confirm that the gap between SBoN and BoN widens as proxy quality decreases (validating Theorem 4.3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the finite-sample bounds for KL divergence and regret in Soft Best-of-N (SBoN) change if the bounded reward assumption (Assumption 3.1) is relaxed to sub-Gaussian or sub-exponential distributions?
- **Basis in paper:** [explicit] The paper states in Section 3.3: "We can relax this assumption using a sub-gaussian or sub-exponential reward function as discussed in (Mroueh, 2024)."
- **Why unresolved:** The current derivations for Lemma 4.1 and Theorem 5.2 rely explicitly on the finite upper bound Rmax to constrain the exponential terms and partition functions.
- **What evidence would resolve it:** Theoretical derivations of modified bounds that substitute Rmax with variance-dependent or tail-bound terms characteristic of sub-Gaussian distributions.

### Open Question 2
- **Question:** Can a closed-form solution or efficient algorithm be derived to analytically determine the optimal temperature parameter β that balances reward maximization against estimation error for a fixed N?
- **Basis in paper:** [inferred] Remark 4.5 notes that "there exists an optimal β to balance between the estimation error term and the expected true reward," but provides no method for finding it other than the observation of the trade-off.
- **Why unresolved:** The paper demonstrates that SBoN outperforms BoN because such a balance exists, but treats β as a fixed or swept hyperparameter rather than a derived variable of N and the estimation error ε.
- **What evidence would resolve it:** A corollary or optimization framework that outputs β* as a function of sample size N and reward model quality metrics.

### Open Question 3
- **Question:** Can tighter upper bounds on the KL divergence for SBoN be derived to match the tightness of standard Best-of-N bounds in the asymptotic limit (β→∞)?
- **Basis in paper:** [explicit] Section 4 explicitly notes: "Note that our bound is looser than the bound on KL divergence in (15)... In contrast, our bound is general... For β = 0... our bound is tight."
- **Why unresolved:** The paper prioritizes a general bound valid for all β over tightness in the specific BoN limit, resulting in a bound of log(N) versus the tighter log(N) - 1 + 1/N found in prior work.
- **What evidence would resolve it:** A refined version of Lemma 4.1 that converges exactly to the tighter Beirami et al. (2024) bound as the temperature parameter β approaches infinity.

## Limitations
- The analysis relies on bounded reward assumption which may not hold for real-world proxy models
- The theoretical bounds assume i.i.d. sampling from reference policy, but stochastic decoding introduces correlations
- Temperature β selection is treated as a hyperparameter requiring grid search rather than principled derivation
- Experiments focus on single harmlessness task, limiting generalizability to other alignment objectives

## Confidence

- **High Confidence:** The KL divergence bound (Lemma 4.1) and regret decomposition (Theorem 4.3) are mathematically sound given their stated assumptions. The empirical observation that SBoN outperforms BoN with weak proxies is well-supported by Figure 2.
- **Medium Confidence:** The claim that an optimal β exists balancing reward maximization and estimation error (Remark 4.5) is theoretically plausible but relies on the specific bound form in Theorem 4.3. The assumption that estimation error is the dominant regret source in high-N regimes needs more empirical validation.
- **Low Confidence:** The assertion that SBoN keeps policies within a "trusted" distributional manifold (Mechanism 2) lacks quantitative validation—the KL bound is loose and doesn't directly measure distributional similarity of outputs.

## Next Checks

1. **Bound Sensitivity Analysis:** Vary the reward bound Rmax in the theoretical framework to test how sensitive the KL and regret bounds are to this critical assumption
2. **Cross-Domain Generalization:** Test SBoN on non-harmlessness tasks (e.g., helpfulness, factuality) with different reward model architectures to validate robustness beyond the Attaq dataset
3. **Temperature Selection Protocol:** Develop and validate a principled method for selecting β (e.g., using validation KL constraints) rather than relying on grid search, and test whether this improves practical performance