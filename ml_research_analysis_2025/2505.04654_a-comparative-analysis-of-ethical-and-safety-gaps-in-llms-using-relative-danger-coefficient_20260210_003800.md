---
ver: rpa2
title: A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger
  Coefficient
arxiv_id: '2505.04654'
source_url: https://arxiv.org/abs/2505.04654
tags:
- unsafe
- prompt
- ethical
- instructions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ethical and safety gaps in Large Language
  Models (LLMs) using a new Relative Danger Coefficient (RDC) metric. The RDC assesses
  risk by categorizing responses into Good, Uncertain, Partially Unsafe, and Directly
  Unsafe outputs, weighted by severity and penalized for inconsistency, severity,
  repetition, and adversarial exploitability.
---

# A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient

## Quick Facts
- **arXiv ID**: 2505.04654
- **Source URL**: https://arxiv.org/abs/2505.04654
- **Reference count**: 25
- **Primary Result**: RDC metric reveals significant safety inconsistencies across LLM models, with no model immune to adversarial exploitation

## Executive Summary
This study introduces the Relative Danger Coefficient (RDC) as a novel metric to evaluate ethical and safety gaps in Large Language Models. The RDC categorizes responses into Good, Uncertain, Partially Unsafe, and Directly Unsafe outputs, applying severity weights and penalties for inconsistency, repetition, and adversarial exploitability. Testing across major LLM models (GPT variants, Gemini, DeepSeek) revealed substantial performance variations, with DeepSeek showing the highest ethical risks and Gemini the lowest. The findings demonstrate that all models remain vulnerable to persistent or creative adversarial prompts, highlighting the critical need for continuous human oversight and refinement of AI moderation systems.

## Method Summary
The study employed the Relative Danger Coefficient (RDC) metric to systematically evaluate LLM safety across multiple models. The RDC framework categorized model outputs into four risk tiers based on response content, then applied weighted scoring that accounted for severity, inconsistency, repetition, and vulnerability to adversarial exploitation. Models were tested using standardized prompts designed to probe ethical boundaries, with human annotators classifying responses according to the risk framework. The methodology included comparative analysis across model families and systematic stress-testing through creative and persistent adversarial prompts.

## Key Results
- DeepSeek model demonstrated the highest ethical risks with the highest RDC scores across tested scenarios
- Gemini model achieved the lowest RDC scores, indicating superior safety performance relative to other models
- All tested models showed vulnerability to adversarial exploitation, particularly under persistent or creative prompting strategies
- RDC scores ranged from 0 (fully safe) to 100 (high danger), with significant variation between model families

## Why This Works (Mechanism)
The RDC metric functions by creating a comprehensive risk assessment framework that goes beyond binary safe/unsafe classifications. By incorporating multiple dimensions of risk including severity weighting, inconsistency penalties, and explicit testing for adversarial exploitability, the metric captures the nuanced ways in which LLMs can fail safety requirements. The framework's strength lies in its ability to quantify both the likelihood and impact of safety failures, providing a more accurate representation of real-world risk than traditional evaluation methods.

## Foundational Learning
- **RDC Metric Framework**: The systematic approach to quantifying LLM safety risks through severity-weighted categories and penalty mechanisms
  - Why needed: Traditional binary safety assessments fail to capture the spectrum of risk and model vulnerabilities
  - Quick check: Verify that all four risk categories (Good, Uncertain, Partially Unsafe, Directly Unsafe) are consistently applied across models

- **Adversarial Prompting**: Systematic testing of model responses to creative and persistent prompts designed to bypass safety filters
  - Why needed: Real-world users may employ sophisticated techniques to circumvent AI safety measures
  - Quick check: Confirm that prompt variations maintain consistent intent while changing surface form

- **Severity Weighting Systems**: The methodology for assigning different risk weights to various types of unsafe outputs
  - Why needed: Not all safety failures carry equal risk; proportional weighting enables more accurate risk assessment
  - Quick check: Validate that severity weights align with domain-specific safety priorities

## Architecture Onboarding
**Component Map**: LLM Models -> RDC Scoring Engine -> Human Annotation -> Risk Classification -> Comparative Analysis
**Critical Path**: Model Input → Prompt Processing → Response Generation → Risk Categorization → Severity Scoring → RDC Calculation
**Design Tradeoffs**: Comprehensive risk assessment vs. evaluation complexity; human annotation quality vs. scalability; severity weighting accuracy vs. generalizability
**Failure Signatures**: Inconsistent safety responses across similar prompts; vulnerability to specific prompt patterns; disproportionate risk in certain domains
**First Experiments**: 
1. Test RDC consistency across identical prompts with varied timing
2. Compare RDC scores for semantically equivalent prompts with different surface forms
3. Evaluate model performance across different risk categories to identify specific vulnerability patterns

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Proprietary RDC metric formulation limits independent verification and replication
- Potential subjectivity in human annotator safety classifications affects reliability
- Limited scope to only a few major model families may not represent broader LLM landscape
- Lack of standardized adversarial prompt methodology introduces potential variability

## Confidence
- **Core Findings**: Medium confidence due to proprietary methodology and potential annotation subjectivity
- **Model Rankings**: Medium confidence in directional insights, though absolute RDC values require cautious interpretation
- **Adversarial Vulnerability Claims**: Medium confidence supported by systematic stress-testing, but practical implications need further investigation

## Next Checks
1. Conduct blinded annotation of safety classifications across multiple independent human evaluators to assess inter-rater reliability
2. Replicate the RDC calculations using a standardized adversarial prompt set across all models to ensure consistent stress-testing
3. Test model safety performance across additional domains and languages beyond the current scope to evaluate generalizability of findings