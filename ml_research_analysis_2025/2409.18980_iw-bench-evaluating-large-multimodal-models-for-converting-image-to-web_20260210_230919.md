---
ver: rpa2
title: 'IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web'
arxiv_id: '2409.18980'
source_url: https://arxiv.org/abs/2409.18980
tags:
- comparison
- weak
- elements
- page
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IW-BENCH, a new benchmark for evaluating
  large multimodal models on the image-to-web task. It addresses the challenge of
  accurately assessing web element completeness and layout relationships, which are
  overlooked by previous metrics like BLEU.
---

# IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web

## Quick Facts
- **arXiv ID:** 2409.18980
- **Source URL:** https://arxiv.org/abs/2409.18980
- **Reference count:** 31
- **Primary result:** Proposes IW-BENCH benchmark with 1200 image-web pairs and novel Element/Layout Accuracy metrics for evaluating LMMs on image-to-web code generation.

## Executive Summary
This paper introduces IW-BENCH, a comprehensive benchmark for evaluating large multimodal models (LMMs) on the task of converting webpage screenshots into functional HTML/CSS/JS code. The benchmark addresses critical limitations of existing evaluation metrics by proposing DOM-based Element and Layout Accuracy measures that capture both visible and invisible web elements. To enhance model performance, the authors develop a five-hop multimodal Chain-of-Thought prompting method incorporating scene-of-meaning injection, structured reasoning, and reflection. Experiments across three difficulty levels demonstrate significant improvements using the proposed approach, with GPT-4V achieving up to 45.8% element accuracy using five-hop MCoT.

## Method Summary
The method combines a novel evaluation framework with an enhanced inference approach. Evaluation uses two metrics: Element Accuracy, which parses DOM trees to assess six attributes (Tag, Text Content, Attribute, Style, JavaScript, Children) for both visible and invisible elements; and Layout Accuracy, which uses longest common subsequence analysis on linearized DOM elements to measure structural preservation. The inference approach employs a five-hop multimodal Chain-of-Thought: (1) SoM Prompt Injection overlays visual markers on images, (2) Inferring Elements identifies components via markers, (3) Inferring Layout determines spatial relationships, (4) Inferring Web Code generates HTML/CSS/JS, and (5) Reflection renders the code, compares it visually with the input, and self-corrects iteratively. The benchmark includes 1200 image-web pairs across Simple, Medium, and Complex difficulty levels.

## Key Results
- GPT-4V achieves 45.8% element accuracy and 44.5% layout accuracy using five-hop MCoT, significantly outperforming baseline approaches.
- The SoM module and reflection steps show measurable improvements in ablation studies, with accuracy stabilizing after 2-3 reflection iterations.
- Human evaluation validates the reliability of the proposed Element and Layout Accuracy metrics, demonstrating strong correlation with human judgment.
- Performance varies across difficulty levels, with Complex pages showing the largest performance gaps between models and prompting strategies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-hop Chain-of-Thought with SoM improves element and layout extraction by grounding visual attention on specific components before inferring structural relationships.
- **Mechanism:** SoM annotation overlays numeric labels and bounding boxes onto webpage screenshots, providing explicit visual anchors. Subsequent inference steps force sequential reasoning: identify elements, infer layout, generate code, reflect and correct. This staged reasoning likely reduces hallucination by conditioning code generation on intermediate, validated representations.
- **Core assumption:** The visual grounding provided by SoM labels successfully directs the model's attention to relevant elements, and the sequential reasoning steps effectively propagate this information without significant information loss between hops.
- **Evidence anchors:** Proposes a five-hop multimodal Chain-of-Thought prompting method, incorporating scene-of-meaning injection. SoM prompt injection identifies key elements; Inferring Elements and Inferring Layout force structured analysis. "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought" (arXiv:2508.03560) validates the benefits of layout-guided reasoning in design-to-code tasks.
- **Break condition:** Significant performance degradation when SoM labels are removed or when steps are combined (tested via ablation of SoM module).

### Mechanism 2
- **Claim:** Element Accuracy and Layout Accuracy metrics provide more robust evaluation than traditional string-similarity metrics by parsing the DOM structure.
- **Mechanism:** By constructing and traversing the Document Object Model (DOM) tree of both generated and reference HTML, these metrics evaluate six attributes (Tag, Text Content, Attribute, Style, JavaScript, Children) and structural ordering via Longest Common Subsequence (LCS). This captures both visible and invisible elements and accounts for the fact that functionally equivalent code can be written in many ways.
- **Core assumption:** The DOM parsing and attribute mapping are comprehensive enough to capture all relevant functional and visual differences.
- **Evidence anchors:** Proposes two novel metrics—Element Accuracy and Layout Accuracy—that evaluate both visible and invisible elements by parsing DOM trees. Explicitly defines the formulas for EA and LA based on DOM parsing and LCS. Contemporaneous efforts exploring similar structural and rendering-based evaluation indicate convergence on structural evaluation in this domain.
- **Break condition:** Strong disagreement between these metrics and human evaluation rankings (Pearson coefficient drops below 0.7).

### Mechanism 3
- **Claim:** The reflection step enables self-correction by comparing a rendered screenshot of generated code against the original input image.
- **Mechanism:** The model re-renders its own generated HTML, captures a screenshot, and compares it visually with the original image to identify missing elements or layout errors. This visual feedback loop allows the model to iteratively refine the code. This is a form of self-critique grounded in a concrete, visual outcome.
- **Core assumption:** The model possesses sufficient visual discernment to detect discrepancies between the original and rendered images and the code generation ability to fix them.
- **Evidence anchors:** Reflection requires the model to compare two screenshots, identify missing elements, and improve the code accordingly. Ablation shows accuracy improves with N=1 to N=2 reflections before stabilizing. UI2Code automation focuses on generation, not self-correction; evidence for this specific reflection loop is primarily internal to this paper.
- **Break condition:** Accuracy plateaus or decreases with additional reflection iterations (observed after N=2).

## Foundational Learning

- **Concept:** Document Object Model (DOM) Tree
  - **Why needed here:** All evaluation metrics are built on parsing and comparing DOM trees. Understanding tree structure, nodes, and traversal is essential for comprehending Element and Layout Accuracy.
  - **Quick check question:** Can you explain how a DOM tree represents the hierarchical structure of an HTML document?

- **Concept:** Longest Common Subsequence (LCS)
  - **Why needed here:** Layout Accuracy is calculated using LCS on element lists to measure structural preservation despite potential reordering or insertion of elements.
  - **Quick check question:** Given two sequences "ABCD" and "ACBAD", what is their longest common subsequence?

- **Concept:** Set-of-Mark (SoM) Prompting
  - **Why needed here:** This is a core component of the proposed five-hop method. Understanding how visual markers are used to guide multimodal models is crucial.
  - **Quick check question:** How might adding numbered labels to objects in an image help a language model answer questions about their spatial relationships?

## Architecture Onboarding

- **Component map:** Input Image -> SoM Prompt Injection -> Large Multimodal Model -> Five-Hop Reasoning Chain (SoM Injection, Inferring Elements, Inferring Layout, Inferring Web Code, Reflection) -> Generated Web Code -> Evaluator (DOM Parser + Element & Layout Accuracy calculators)
- **Critical path:**
    1. Correctly injecting SoM markers onto the input image.
    2. The quality of the LMM's reasoning in the "Inferring Elements" and "Inferring Layout" steps.
    3. The effectiveness of the reflection loop in catching and fixing errors.
- **Design tradeoffs:**
    - Accuracy vs. Cost/Latency: The five-hop chain with reflection is more accurate but significantly slower and more expensive due to multiple LMM calls and rendering steps.
    - Automation vs. Quality: Fully automated SoM may miss or mislabel elements; manual annotation is better but not scalable. The paper does not specify the automation level of SoM.
    - Generalization vs. SFT: Fine-tuning (like WebSight) yields higher performance but on a fixed distribution. The proposed prompting method is zero-shot/few-shot and more general but less performant on average.
- **Failure signatures:**
    - Low Element Accuracy with decent Layout Accuracy: The model grasps the general structure but hallucinates or misses specific content (e.g., text, attributes).
    - Low Layout Accuracy with decent Element Accuracy: The model identifies components but places them incorrectly (e.g., wrong hierarchy, sibling order).
    - Accuracy does not improve with reflection: The model cannot self-diagnose errors from visual comparison, possibly due to limited visual discrimination or inability to translate critique into code fixes.
- **First 3 experiments:**
    1. **Baseline Establishment:** Run your chosen LMM on the IW-BENCH dataset using a simple, single-step prompt (e.g., "Generate web code for this image"). Measure Element and Layout Accuracy.
    2. **Ablation of SoM:** Implement the five-hop chain but skip the SoM Prompt Injection step. Compare results to the full five-hop method to quantify the value of visual grounding.
    3. **Reflection Iteration Scan:** Run the full five-hop method, varying the reflection hyperparameter `N` from 0 to 5. Plot accuracy vs. N to identify the optimal trade-off between performance and cost.

## Open Questions the Paper Calls Out

- **Question:** How does the performance of Large Multimodal Models on the Image-to-Web task generalize to languages beyond the current English and Chinese scope?
  - **Basis in paper:** The "Limitations" section states, "Currently, the benchmark is limited to only two languages. Expanding the scope to include additional languages would enhance its applicability..."
  - **Why unresolved:** The authors acknowledge the current language restriction but have not yet validated whether the proposed Element and Layout Accuracy metrics remain effective or stable across morphologically different or low-resource languages.
  - **What evidence would resolve it:** Evaluation results of current SOTA models on an expanded version of IW-Bench containing diverse languages (e.g., Arabic, Hindi), showing consistent metric reliability.

- **Question:** How does increasing the dataset size beyond 1,200 samples affect the statistical robustness of the Element and Layout Accuracy metrics?
  - **Basis in paper:** The authors explicitly note under "Limitations," "The benchmark dataset requires a significant increase in samples to ensure robustness and reliability."
  - **Why unresolved:** While 1,200 pairs provide an initial benchmark, the authors admit this quantity may be insufficient to fully cover the variance in real-world web design patterns, potentially leading to overfitting or high variance in evaluation.
  - **What evidence would resolve it:** A follow-up study scaling the dataset to >10,000 samples demonstrating that model rankings and metric variances stabilize compared to the current subset.

- **Question:** Can the proposed Layout Accuracy metric, based on Longest Common Subsequence (LCS), accurately capture the correctness of complex, non-linear layouts?
  - **Basis in paper:** The methodology converts the DOM tree into a 1D list via in-order traversal (Section 3.4.2). This reduction assumes that linear sequence overlap (LCS) is a sufficient proxy for 2D spatial layout correctness, which may fail for complex grid or overlapping layouts.
  - **Why unresolved:** A 1D sequence match does not strictly enforce 2D spatial constraints (e.g., sidebar alignment or absolute positioning), potentially giving high scores to layouts that are sequentially correct but visually broken.
  - **What evidence would resolve it:** A correlation analysis between LCS-based Layout Accuracy and a pixel-space metric (like SSIM) specifically on "Complex" level pages to verify if high LCS scores consistently predict visual alignment.

## Limitations

- The benchmark is currently limited to English and Chinese languages, restricting its applicability to other languages.
- The dataset size of 1,200 samples may be insufficient to ensure statistical robustness and full coverage of real-world web design patterns.
- The Layout Accuracy metric's reliance on LCS may not fully capture correctness of complex, non-linear 2D layouts.

## Confidence

- **High Confidence:** The five-hop Chain-of-Thought framework is clearly described and experimentally validated through ablation studies. The Element and Layout Accuracy metrics are mathematically well-defined.
- **Medium Confidence:** The claimed performance gains (up to 45.8% element accuracy with five-hop MCoT) are based on experiments using GPT-4V, Qwen-VL, and Gemini Pro, but the exact SoM implementation details are unclear.
- **Low Confidence:** The Reflection step's effectiveness is primarily supported by internal ablation (N=1 vs N=2) rather than external validation or comparison to alternative self-correction methods.

## Next Checks

1. **Implement SoM Automation:** Develop and test a simple heuristic (e.g., connected component analysis on rendered HTML) to overlay numbered bounding boxes, then measure its impact on element accuracy compared to manual SoM.
2. **Test Rendering Robustness:** Run the full five-hop method on a subset of IW-BENCH using Playwright with mocked external resources (local CSS/JS) to isolate rendering errors from generation errors.
3. **Metric Benchmarking:** Compare IW-BENCH's Element and Layout Accuracy scores against human evaluations on a random sample to validate correlation (target Pearson coefficient > 0.7).