---
ver: rpa2
title: 'Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints'
arxiv_id: '2501.04426'
source_url: https://arxiv.org/abs/2501.04426
tags:
- offline
- learning
- reward
- skills
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual-Force, an offline algorithm for diversity
  maximization under imitation constraints. The method enhances diversity using the
  Van der Waals force objective and successor features, eliminating the need for a
  skill discriminator.
---

# Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints

## Quick Facts
- **arXiv ID:** 2501.04426
- **Source URL:** https://arxiv.org/abs/2501.04426
- **Reference count:** 28
- **One-line primary result:** Introduces Dual-Force, an offline algorithm for diversity maximization under imitation constraints that uses Van der Waals force and successor features to eliminate discriminator needs and enable zero-shot skill recall.

## Executive Summary
This paper presents Dual-Force, an offline reinforcement learning algorithm designed to maximize behavioral diversity while satisfying imitation constraints from expert demonstrations. The method leverages successor features and a Van der Waals force-based diversity objective to avoid the instability of discriminator-based approaches. By conditioning policies on pre-trained Functional Reward Encodings (FRE), the algorithm achieves stable training under non-stationary rewards and enables zero-shot recall of learned skills. Experiments on a quadruped robot demonstrate successful recovery of diverse locomotion behaviors and obstacle navigation strategies from static offline datasets.

## Method Summary
Dual-Force operates on state-only expert datasets and mixed-quality offline datasets, maximizing Van der Waals diversity while satisfying KL-divergence imitation constraints through DICE's Fenchel duality framework. The method pre-trains a state discriminator and FRE transformer, then iteratively updates FRE-conditioned value functions and policies using importance-weighted samples from the offline dataset. Successor features define the diversity space, while Lagrange multipliers balance constraint satisfaction against diversity maximization.

## Key Results
- Successfully recovers diverse quadruped behaviors including walking with different base heights and angular velocities
- Enables zero-shot recall of all skills encountered during training through FRE conditioning
- Achieves over 50% expert performance on locomotion and obstacle navigation tasks
- Eliminates need for discriminator training while maintaining strong diversity signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Van der Waals (VdW) force eliminates the need for a skill discriminator while providing a consistently strong diversity signal in the offline setting.
- **Mechanism:** The VdW objective computes successor features ψi = E_di(s)[ϕ(s)] for each skill, then applies a physics-inspired force: repulsive when skills are close (ℓi < ℓ0), attractive when far (ℓi > ℓ0). The gradient ∇di Diversity is computed in closed form as βi(s,a) = (1-(ℓi/ℓ0)³)⟨ϕ(s), ψi - ψj*⟩, avoiding discriminator training instability.
- **Core assumption:** Successor features can be reliably estimated off-policy via importance sampling from the offline dataset.
- **Evidence anchors:**
  - [abstract] "using the Van der Waals force as a diversity objective, eliminating the need for a skill discriminator and providing a strong diversity signal in the offline setting"
  - [Section 3.1] Eq. 9 defines the VdW objective with controllable diversity parameter ℓ0
  - [corpus] Corpus lacks direct VdW-force comparisons; related work focuses on discriminator-based skill discovery
- **Break condition:** If successor feature estimates have high variance due to sparse offline coverage, the VdW gradient becomes noisy and skill separation fails.

### Mechanism 2
- **Claim:** Functional Reward Encoding (FRE) enables stable training under non-stationary rewards and zero-shot skill recall.
- **Mechanism:** A transformer-based VAE pre-trained on random reward functions encodes state-reward samples into latent embeddings zr. The value function and policy are conditioned on these latents: V(·, zr) and π(·|·, zr). At evaluation, storing FRE latents for each encountered reward allows direct policy invocation without re-optimization.
- **Core assumption:** The FRE generalizes sufficiently from pre-training on random linear/MLP/engineered rewards to encode the non-stationary VdW-derived rewards.
- **Evidence anchors:**
  - [abstract] "conditioning the value function and policy on a pre-trained Functional Reward Encoding...enables zero-shot recall of all skills encountered during training"
  - [Section 4.3] "conditioning the value function (and policy) on a latent representation of a Functional Reward Encoding"
  - [corpus] Corpus references LUMOS using world models for skill transfer but no FRE-specific validation
- **Break condition:** If reward distributions during training diverge significantly from FRE pre-training distribution, latent encoding becomes uninformative.

### Mechanism 3
- **Claim:** DICE framework with Fenchel duality enables off-policy estimation of all VdW-relevant quantities from static datasets.
- **Mechanism:** The KL-regularized RL problem is solved via dual formulation V* = argmin_V [(1-γ)E[V(s)] + log E_dO exp{R(s,a) + γTV(s,a) - V(s)}]. Importance ratios ηi(s,a) = softmax(δi) convert offline data expectations to learned-policy expectations: E_di[f] = E_dO[ηi·f]. This enables successor feature estimation and constraint violation measurement without environment interaction.
- **Core assumption:** The offline dataset DO provides sufficient coverage of expert-adjacent state-action space (Assumption A.1: dE(s) > 0 implies dO(s) > 0).
- **Evidence anchors:**
  - [Section 4.2] Eq. 14-16 derive the dual formulation and importance ratio estimator
  - [Section 4.2] Theorem B.2 provides finite-sample constraint violation estimator
  - [corpus] Related DICE methods (DemoDICE, OptiDICE) cited but corpus lacks validation for VdW-specific extensions
- **Break condition:** Coverage violations cause importance ratios to become unreliable, manifesting as unstable Lagrange multiplier updates.

## Foundational Learning

- **Concept: Successor Features**
  - **Why needed here:** Core representation for VdW distance computation; defines the feature space where diversity is measured.
  - **Quick check question:** Can you compute ψi = E_di(s)[ϕ(s)] off-policy given only dataset samples?

- **Concept: Fenchel Duality in RL**
  - **Why needed here:** Converts the constrained occupancy matching problem into a tractable dual form solvable from offline data.
  - **Quick check question:** How does the dual variable V*(s) relate to the primal occupancy ratio ηi(s,a)?

- **Concept: Constrained MDPs with Lagrangian Relaxation**
  - **Why needed here:** Balances imitation constraint satisfaction against diversity maximization via adaptive multipliers.
  - **Quick check question:** When should bounded multipliers σ(μi) increase vs. decrease during training?

## Architecture Onboarding

- **Component map:**
  1. Pre-trained state discriminator c* (distinguishes expert vs. offline states)
  2. Pre-trained FRE encoder F (rewards → latent embeddings)
  3. n parallel FRE-conditioned value functions Vi(·, zi)
  4. n parallel FRE-conditioned policies πi(·|·, zi)
  5. n bounded Lagrange multipliers σ(μi)
  6. Time-averaged importance weights w_i^k (Polyak updates)

- **Critical path:**
  1. Pre-train c* on D_E vs. D_O classification
  2. Pre-train FRE on random reward functions (linear, MLP, engineered)
  3. Initialize w_i^0 uniformly on simplex
  4. Loop: compute successor features → VdW reward → FRE encoding → value update → importance ratios → policy update → multiplier update

- **Design tradeoffs:**
  - n=3 occupancies worked well; more increases compute linearly but may not improve diversity
  - ℓ0 controls diversity-exploitation balance; ℓ0→∞ recovers pure max-min distance objective
  - Independent network weights per occupancy improves stability vs. shared weights

- **Failure signatures:**
  - Successor features collapsing to similar values: offline coverage insufficient or learning rate too high
  - Lagrange multipliers saturating at bounds: constraint too tight (reduce ε) or expert data too sparse
  - Skills failing 50% expert return threshold: imitation signal too weak; increase σ(μi) initialization

- **First 3 experiments:**
  1. Validate state discriminator: check c* accuracy on held-out expert/offline split before main training
  2. Ablate FRE: train with random latent vectors vs. FRE-encoded rewards to isolate non-stationarity benefit
  3. Visualize skill separation: plot UMAP projection of successor features; verify distinct clusters correspond to behavioral modalities

## Open Questions the Paper Calls Out

- **Question:** Can Dual-Force maintain training stability and diversity performance when transferred to physical robotic hardware?
- **Basis in paper:** [explicit] The authors validate the method "for two robotic tasks in simulation" (Abstract) using the Solo12 robot, but do not test on physical hardware.
- **Why unresolved:** Offline distribution correction (DICE) often suffers from distribution shifts and estimation errors in real-world settings due to sensor noise and unmodeled dynamics.
- **Evidence:** Successful zero-shot deployment of learned skills on the physical Solo2 quadruped with comparable diversity metrics.

- **Question:** Does the method scale to high-dimensional observation spaces such as raw pixels?
- **Basis in paper:** [inferred] Experiments utilize low-dimensional state vectors (e.g., base height, joint angles), relying on a feature mapping φ(s) for successor features.
- **Why unresolved:** Estimating successor features ψi and VdW forces requires accurate density estimation, which becomes significantly harder and less stable with high-dimensional visual inputs.
- **Evidence:** Evaluations on pixel-based benchmarks (e.g., DMC Remi) showing if the VdW gradient remains effective without pixel-based discriminator pre-training.

- **Question:** How sensitive is the Functional Reward Encoding (FRE) to the specific distribution of rewards used during its pre-training phase?
- **Basis in paper:** [inferred] The method relies on a FRE pre-trained on "random linear functions... and simple human-engineered rewards" to handle non-stationary rewards.
- **Why unresolved:** If the intrinsic VdW rewards generated during training fall outside the support of the pre-trained FRE distribution, the latent conditioning may fail to stabilize value function learning.
- **Evidence:** Ablation studies measuring performance degradation when the FRE is pre-trained on distributions that diverge from the VdW reward structure.

## Limitations

- Successor feature estimation reliability depends critically on offline dataset coverage, which is not quantified
- FRE generalization to VdW-specific reward structures is assumed but lacks quantitative ablation validation
- Bounded multiplier scheme may limit exploration in early training phases despite preventing divergence

## Confidence

- **High confidence:** The DICE framework implementation for off-policy updates is technically sound and well-established
- **Medium confidence:** The VdW force provides diversity signal in offline settings, but empirical validation against discriminator-based alternatives is missing
- **Low confidence:** The zero-shot recall capability depends critically on FRE generalization, which lacks quantitative validation

## Next Checks

1. **Successor feature stability:** Monitor variance of ψi estimates across training iterations to detect coverage-induced instability
2. **FRE ablation:** Compare diversity and recall performance with random latent vectors versus learned FRE encodings
3. **Coverage sensitivity:** Systematically vary offline dataset size/quality to measure impact on VdW gradient reliability and skill separation quality