---
ver: rpa2
title: Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in
  Modern Text-to-Video Models
arxiv_id: '2504.04051'
source_url: https://arxiv.org/abs/2504.04051
tags:
- uni00000013
- uni00000011
- counting
- uni00000016
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces T2VCountBench, the first benchmark for evaluating
  counting ability in modern text-to-video generation models. Through rigorous human
  evaluations across 10 state-of-the-art models (including both open-source and commercial
  systems), the study finds that all models struggle with basic numerical constraints,
  achieving less than 50% accuracy even when asked to generate 9 or fewer objects.
---

# Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models

## Quick Facts
- **arXiv ID:** 2504.04051
- **Source URL:** https://arxiv.org/abs/2504.04051
- **Reference count:** 40
- **Primary result:** All 10 evaluated text-to-video models achieve <50% counting accuracy even for N≤9, with accuracy dropping rapidly as object count increases.

## Executive Summary
This paper introduces T2VCountBench, the first benchmark for evaluating counting ability in modern text-to-video generation models. Through rigorous human evaluations across 10 state-of-the-art models, the study finds that all models struggle with basic numerical constraints, achieving less than 50% accuracy even when asked to generate 9 or fewer objects. Counting accuracy drops rapidly as the number of objects increases, while object fidelity remains relatively high. The findings highlight a fundamental limitation in current text-to-video models' ability to follow simple numerical instructions.

## Method Summary
The benchmark evaluates 10 text-to-video models using 165 human-generated prompts that systematically vary object count (1, 3, 5, 7, 9), object type (human, nature, artifact), scene transitions, motion constraints, and visual styles. Videos are generated at 720p resolution and 4-second duration, then evaluated by 5 human annotators who count objects and assess fidelity. Counting Accuracy measures exact matches between requested and generated object counts, while Object Fidelity measures the proportion of recognizable objects.

## Key Results
- All 10 evaluated models achieve less than 50% counting accuracy, even for simple cases with 9 or fewer objects
- Counting accuracy drops dramatically from ~90% for N=1 to less than 10% for N=9
- Object fidelity remains relatively high across all models, showing that visual quality and counting ability are decoupled
- Prompt refinement techniques (additive decomposition, position guidance) fail to significantly improve counting performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual fidelity capabilities operate independently of numerical adherence in current diffusion architectures.
- **Mechanism:** Text-to-video models optimize for semantic alignment and texture realism (high Object Fidelity) but appear to lack a discrete counting constraint in the diffusion objective. The text encoder likely maps numerical tokens to a semantic region meaning "plural" or "many" rather than a strict integer state.
- **Core assumption:** The loss functions used in training these models do not penalize the difference between generating N and N+1 objects provided the object class is correct.
- **Evidence anchors:**
  - [Abstract]: "Counting accuracy drops rapidly... while object fidelity remains relatively high."
  - [Section 4.1, Observation 4.2]: "Although most text-to-video models achieve high object fidelity, this does not directly translate into accurate counting performance."
  - [Corpus]: Neighbor paper *79563* supports the hypothesis that this is a fundamental limitation in diffusion models, extending from images to video.

### Mechanism 2
- **Claim:** Counting performance degrades non-linearly as the required number of objects increases due to spatial crowding or attention dilution.
- **Mechanism:** As the target number N increases, the model must maintain distinct latent representations for multiple entities in constrained spatial resolution. The cross-attention mechanism may fail to keep instances distinct, leading to object merging or deletion.
- **Core assumption:** The video latent space has finite "capacity" for distinct entities, and increasing density causes attention maps to overlap, effectively "erasing" objects.
- **Evidence anchors:**
  - [Section 4.2, Table 3]: Shows accuracy dropping from ~90% for N=1 to <10% for N=9 across almost all models.
  - [Section 4.2, Observation 4.3]: "Counting accuracy drops rapidly as the number of objects increases."

### Mechanism 3
- **Claim:** Prompt engineering cannot easily override architectural limitations in counting because the bottleneck resides in the generative decoder, not just the text encoder.
- **Mechanism:** Tested "Additive Decomposition" (breaking "7" into "3 + 4") and "Position Guidance" (left/right splitting). The failure of these techniques suggests that even when the text input provides easier sub-problems, the underlying denoising process struggles to allocate spatial regions for these groups without merging them.
- **Core assumption:** The cross-attention maps for "3 dogs left" and "4 dogs right" bleed into each other during the diffusion steps.
- **Evidence anchors:**
  - [Section 4.4, Observation 4.6]: "Simple prompt refinement does not consistently improve counting accuracy... highlighting the inherent challenge."
  - [Abstract]: "Decomposing the task into smaller subtasks does not easily alleviate these limitations."
  - [Corpus]: Neighbor *62902* reinforces that compositional counting is a systemic failure in vision-language architectures.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - **Why needed here:** All 10 baseline models are diffusion-based. Understanding that they generate video by denoising random Gaussian noise in a compressed latent space is crucial to understanding why "counting" is hard—it is a probabilistic sampling process, not a deterministic program.
  - **Quick check question:** Do you understand why a probabilistic denoising process struggles with discrete integer constraints compared to a deterministic renderer?

- **Concept: Cross-Attention Mechanisms**
  - **Why needed here:** The interaction between the text prompt and the video frames happens via cross-attention. The paper implies this attention mechanism is failing to bind specific counts to specific spatial regions.
  - **Quick check question:** Can you explain how a text token like "five" theoretically influences the visual features in a specific frame region via attention maps?

- **Concept: Human Evaluation Protocols**
  - **Why needed here:** The paper establishes T2VCountBench using human annotators rather than automated metrics.
  - **Quick check question:** Why would standard automated metrics like CLIPScore fail to detect the difference between a video of 4 cats vs. 5 cats when the prompt asks for 5?

## Architecture Onboarding

- **Component map:** Text Encoder -> Denoiser (U-Net/Transformer) -> Decoder -> Evaluator
- **Critical path:** The critical failure path is the Text-to-Latent Alignment. Even if the encoder understands "Five," the denoiser fails to maintain 5 distinct entities throughout the temporal frames.
- **Design tradeoffs:**
  - Lower resolution minimizes visual artifacts but may constrict spatial capacity for multiple objects.
  - Prompt refinement simplifies the problem but fails to yield results, suggesting the architecture (not the prompt) is the bottleneck.
- **Failure signatures:**
  - Object Merging: Two distinct objects blending into one
  - Hallucination/Deletion: Generating 4 objects when 5 are requested
  - Zero-shot: Complete failure to generate the requested object class
- **First 3 experiments:**
  1. **Baseline Replication:** Replicate the "N=1 to N=9" sweep on a lightweight open-source model to establish your own internal benchmark.
  2. **Prompt Refinement Ablation:** Test "Additive" vs. "Position" prompts to verify if spatial constraints provide any improvement over additive counts.
  3. **Temporal Stability Check:** Analyze specific frames to see if counts fluctuate over the 4-second duration to isolate temporal dynamics issues.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural modifications or training objectives are required to internalize numerical constraints in video diffusion models, given that prompt-based strategies fail to resolve counting limitations?
- **Basis in paper:** Section 4.4 states that "prompt refinement techniques... cannot easily alleviate these limitations," and the Conclusion calls for "additional attention from the research community" to address this "critical gap."
- **Why unresolved:** The authors demonstrate that external prompt manipulations are insufficient, suggesting the limitation is inherent to current model architectures or training data distributions.
- **What evidence would resolve it:** A text-to-video model that achieves high counting accuracy (>90%) on the T2VCountBench tasks without relying on input prompt engineering.

### Open Question 2
- **Question:** How can text-to-video models be improved to ensure consistent counting performance and object fidelity across diverse languages, specifically addressing the disparities observed in languages like Spanish?
- **Basis in paper:** Section 4.3 notes that "counting accuracy varies significantly across languages" and explicitly calls for work to "enhance the multilingual capabilities of text-to-video models to promote digital fairness."
- **Why unresolved:** The study revealed that while some models excel in Chinese or English, performance drops or models refuse prompts in Spanish, indicating imbalanced multilingual alignment.
- **What evidence would resolve it:** Evaluation results showing a reduction in the variance of counting accuracy scores across English, Chinese, and Spanish.

### Open Question 3
- **Question:** Can automated detection-based metrics be developed that robustly correlate with human evaluation for counting accuracy in dynamic video generation?
- **Basis in paper:** The paper relies entirely on "rigorous human evaluations" to measure counting accuracy and object fidelity, implying that current automated metrics are insufficient for assessing precise numerical adherence.
- **Why unresolved:** The paper highlights the resource-intensive nature of human evaluation but offers no automated alternative.
- **What evidence would resolve it:** The proposal and validation of an automated tool that uses object detection or tracking to count generated items, demonstrating high statistical correlation with human-generated "CountAcc" scores.

## Limitations
- The benchmark's reliance on human evaluation introduces potential subjectivity in counting judgments, though the use of 5 annotators per video helps mitigate this.
- The study does not address whether counting failures stem from the text encoder's inability to represent numbers or the diffusion model's inability to generate discrete quantities.
- The benchmark focuses on simple, static counting scenarios and may not capture more complex temporal counting tasks.

## Confidence

- **High Confidence:** The finding that all 10 evaluated models fail to accurately count objects (accuracy <50% even for N≤9) is well-supported by human evaluation data across multiple models and conditions.
- **Medium Confidence:** The claim that prompt refinement techniques fail to improve counting is reasonably supported, though the specific techniques tested were limited in scope.
- **Low Confidence:** The assertion that this limitation is fundamental to diffusion architectures rather than a training data or optimization issue requires further investigation.

## Next Checks

1. **Cross-model generalization:** Test the benchmark on additional T2V models (particularly newer open-source models) to verify if the counting limitation persists across architectural variations.
2. **Temporal analysis:** Examine individual frames across the 4-second duration to determine if counting errors are consistent or vary temporally, which could reveal whether the issue lies in generation stability.
3. **Controlled ablation:** Generate videos with identical prompts across different spatial resolutions to quantify the relationship between visual capacity and counting accuracy, testing the spatial crowding hypothesis directly.