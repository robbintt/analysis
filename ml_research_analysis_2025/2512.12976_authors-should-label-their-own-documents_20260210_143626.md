---
ver: rpa2
title: Authors Should Label Their Own Documents
arxiv_id: '2512.12976'
source_url: https://arxiv.org/abs/2512.12976
tags:
- author
- labeling
- user
- annotation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Third-party annotation is the status quo for labeling text, but
  egocentric information such as sentiment and belief can at best only be approximated
  by a third-person proxy. We introduce author labeling, an annotation technique where
  the writer of the document itself annotates the data at the moment of creation.
---

# Authors Should Label Their Own Documents

## Quick Facts
- arXiv ID: 2512.12976
- Source URL: https://arxiv.org/abs/2512.12976
- Authors: Marcus Ma; Cole Johnson; Nolan Bridges; Jackson Trager; Georgios Chochlakis; Shrikanth Narayanan
- Reference count: 40
- Primary result: 537% improvement in click-through rate vs. industry baseline using author-labeled data

## Executive Summary
Third-party annotation is the status quo for labeling text, but egocentric information such as sentiment and belief can at best only be approximated by a third-person proxy. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 20,000 users to deploy an author labeling annotation system. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation with author-labeled data to improve performance. We train our model to minimize the prediction error on questions generated for a set of predetermined subjective beliefs using author-labeled responses. Our model achieves a 537% improvement in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at https://academic.echollm.io.

## Method Summary
The ECHO architecture uses author labeling to improve product recommendation in a commercial chatbot. The system filters task-relevant messages, generates on-the-fly labeling questions about subjective user states (beliefs, sentiment, urgency), and records author responses in real time. An ensemble of feature models predicts user intent features, a selector picks top-k features based on relevance and uncertainty, and a downstream recommender generates ads. The system updates feature models and selector uncertainty using author feedback, creating an online learning loop. Over 185,548 labels were collected from 928,078 messages across 22,077 users, achieving 84.3% survey completion.

## Key Results
- 537% improvement in click-through rate compared to industry advertising baseline
- 84.3% survey completion rate for author labeling tasks
- Author labeling outperformed LLM, MTurk, and expert annotation for sentiment analysis on quality, speed, and cost
- Online learning loop successfully improved model calibration using real-time author feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Author-provided labels capture egocentric intent more accurately than third-party proxies.
- Mechanism: Authors possess "privileged access" to their internal states at the moment of creation, bypassing information loss inherent in third-party annotation.
- Core assumption: Authors can accurately self-report their intent and are willing to engage with real-time labeling tasks.
- Evidence anchors: Abstract states egocentric information can only be approximated by third-party proxies; section 2.1 discusses annotators lacking "privileged access"; corpus supports limitations of third-party emotion detection.
- Break condition: Authors are unwilling or unable to provide accurate labels, or real-time queries are too intrusive.

### Mechanism 2
- Claim: Targeted, real-time feature elicitation improves downstream recommendation relevance.
- Mechanism: Feature selector picks top-k most relevant/uncertain features from ensemble predictions to inform recommender.
- Core assumption: Predefined feature set is relevant to downstream task; selector accurately identifies most informative features.
- Evidence anchors: Abstract mentions minimizing prediction error on predetermined subjective beliefs; section 3.2 describes relevance and uncertainty scoring.
- Break condition: Feature set is poorly chosen or selector fails to prioritize impactful features.

### Mechanism 3
- Claim: Reducing uncertainty through author feedback creates an online learning loop that improves model calibration.
- Mechanism: Author feedback serves as ground truth to update both feature model predictions and selector uncertainty estimates.
- Core assumption: Author feedback is reliable ground truth; model update mechanisms effectively incorporate new labeled data.
- Evidence anchors: Abstract discusses minimizing prediction error on author-labeled responses; section 3.4.1 describes updating feature models and selector uncertainty.
- Break condition: Feedback loop introduces bias or model updates fail to generalize.

## Foundational Learning

- Concept: Third-party vs. Egocentric Annotation
  - Why needed here: This distinction is the paper's core motivationâ€”understanding that third-party annotators can only approximate an author's internal state.
  - Quick check question: Why would a third-party annotator struggle to accurately label the sarcasm or true intent behind a user's chat message, whereas the author would not?

- Concept: Online Learning with Human-in-the-Loop
  - Why needed here: The ECHO architecture is an online learning system that updates its models based on real-time human feedback.
  - Quick check question: In the ECHO architecture, what two components are updated using the author's label, and what signal is used for each?

- Concept: Feature Selection
  - Why needed here: The system relies on a feature selector to pick the most relevant features from a larger candidate pool for the downstream task.
  - Quick check question: What is the role of the "Feature Selector" in the ECHO architecture, and on what two metrics does it base its selection?

## Architecture Onboarding

- Component map: Raw User Input -> Lightweight Relevance Filter -> Ensemble of Feature Models -> Feature Selector -> Downstream Recommender -> Ad Display. Parallel: Author Labeling Task Generator -> User Answer -> Loss Calculation -> Updates Feature Model & Feature Selector.

- Critical path:
  1. **Warm-up:** Filter identifies taskable messages. Feature models run and generate predictions.
  2. **Labeling:** For uncertain features, system generates a question. Author answers.
  3. **Update:** Answer updates specific feature model (via SFT/prompt engineering) and selector's uncertainty.
  4. **Inference:** For new messages, updated models predict features. Selector picks top-k. Recommender generates ad.

- Design tradeoffs:
  - Intrusiveness vs. Data Quality: Frequent questions risk user annoyance. Mitigation: 5-second delay, one-click answers.
  - General vs. User-Specific Models: Training on all user data (chosen) is more generalizable than individual models, which showed limited success.
  - Predefined vs. Generated Features: A fixed set of features is controllable but may miss novel intents.

- Failure signatures:
  - Low engagement/survey fatigue (completion rate drops).
  - Stagnant CTR despite author labeling (models not learning).
  - Novelty decay (initial CTR spike fades).

- First 3 experiments:
  1. **A/B Test:** Run ECHO side-by-side with an industry banner ad baseline. Compare CTR.
  2. **Feature Ablation:** Disable the feedback loop for updating feature models. Compare learning curve and final CTR to control.
  3. **Cost-Quality Analysis:** For sentiment analysis, compare author labeling vs. LLM, MTurk, and expert annotation on accuracy, cost, and time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does author labeling retain its quality and efficiency advantages over third-party annotation for objective tasks or non-egocentric subjective tasks?
- Basis in paper: Section 8 states the paper did not analyze objective annotations or non-egocentric subjective tasks.
- Why unresolved: Authors are reliable annotators for egocentric data but may not be for tasks requiring external knowledge or normative judgment.
- What evidence would resolve it: Comparative study applying author labeling framework to Named Entity Recognition or factual extraction against expert/LLM annotations.

### Open Question 2
- Question: To what extent does the specific phrasing of LLM-generated questions bias author responses?
- Basis in paper: Section 8 notes that question phrasing can greatly influence self-reports.
- Why unresolved: Paper controls for timing but doesn't isolate variance introduced by dynamic question generation.
- What evidence would resolve it: Ablation study measuring label consistency across semantically equivalent but syntactically different question phrasings.

### Open Question 3
- Question: Does the intrusive nature of real-time author labeling negatively affect long-term user retention or conversation flow?
- Basis in paper: Method relies on popup overlays and forced delays; acknowledges potential for observer bias and Hawthorne effect.
- Why unresolved: Paper reports high completion rates but doesn't measure if interruption causes user churn or cognitive fatigue over longer periods.
- What evidence would resolve it: Longitudinal A/B testing comparing user retention rates and session durations between users exposed to frequent labeling tasks and control group.

## Limitations

- Proprietary system details (feature models, product recommendation logic, feature selector architecture) limit faithful reproduction
- Unmeasured long-term effects on user retention and conversation flow
- Unclear generalizability to non-egocentric annotation tasks

## Confidence

- High confidence: Theoretical superiority of author labeling for egocentric states
- Medium confidence: Online learning mechanism's effectiveness (sparse corpus evidence)
- Low confidence: Reproducing exact results without proprietary system details

## Next Checks

1. Conduct A/B test deploying ECHO architecture against traditional third-party annotation pipeline on public dataset to measure annotation quality and downstream task performance
2. Implement simplified version of ECHO feature selector using open-source uncertainty quantification methods to validate importance of uncertainty-aware feature selection
3. Perform user study comparing survey completion rates and label accuracy for author labeling vs. traditional annotation interfaces under controlled conditions