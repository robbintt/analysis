---
ver: rpa2
title: Eliciting Trustworthiness Priors of Large Language Models via Economic Games
arxiv_id: '2602.00769'
source_url: https://arxiv.org/abs/2602.00769
tags:
- trustworthiness
- trust
- priors
- human
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to elicit trustworthiness priors\
  \ of large language models (LLMs) using the Trust Game from behavioral economics.\
  \ The authors formalize the Trust Game within a Bayesian framework and employ iterated\
  \ in-context learning to recover the LLM\u2019s implicit prior over trustworthiness,\
  \ operationalized as the fraction of resources returned to a trustor."
---

# Eliciting Trustworthiness Priors of Large Language Models via Economic Games

## Quick Facts
- arXiv ID: 2602.00769
- Source URL: https://arxiv.org/abs/2602.00769
- Authors: Siyu Yan; Lusha Zhu; Jian-Qiao Zhu
- Reference count: 13
- Key outcome: GPT-4.1’s elicited trustworthiness prior closely aligns with human behavior, showing a mean return ratio of 0.372, and varies systematically across trustor personas predicted by warmth and competence stereotypes.

## Executive Summary
This paper introduces a method to elicit trustworthiness priors of large language models (LLMs) using the Trust Game from behavioral economics. The authors formalize the Trust Game within a Bayesian framework and employ iterated in-context learning to recover the LLM’s implicit prior over trustworthiness, operationalized as the fraction of resources returned to a trustor. They find that GPT-4.1’s elicited trustworthiness prior closely aligns with human behavior, showing a mean return ratio of 0.372, and demonstrate that these priors vary systematically across different trustor personas. The observed variation in trustworthiness is well predicted by a stereotype-based model grounded in perceived warmth and competence, with warmth having a stronger influence than competence. This approach offers a behaviorally grounded way to assess and model trust in human-AI interactions.

## Method Summary
The method elicits LLM trustworthiness priors via iterated in-context learning using a Trust Game framework. A trustor invests tokens (x ∈ {$1, $3, $5, $7, $9}) that are tripled (m=3), and the trustee returns y tokens. The return ratio r = y/(mx) ∈ [0,1] quantifies trustworthiness. The method assumes a Beta-Binomial model where r follows a Beta prior and returned amounts follow Binomial(mx, r). Iterated in-context learning proceeds as follows: given a batch of B=5 past interactions sampled from the current trustworthiness estimate, the LLM predicts the next return ratio. This new estimate is used to sample the next batch, forming a Markov chain. After 30 iterations from each of 9 seed values (0.1–0.9) with 30 restarts, the stationary distribution across chains estimates the prior. Convergence is monitored via Gelman-Rubin R̂ ≤ 1.1. The elicited prior is validated by comparing its predictive accuracy for single-shot reciprocity against uniform and human priors.

## Key Results
- GPT-4.1’s elicited trustworthiness prior (mean 0.372, SD 0.108) closely matches human meta-analytic baseline (mean 0.372, SD 0.114), with KL divergence indicating strong alignment.
- Trustworthiness priors vary systematically across trustor personas (range 0.298–0.447), with stereotype-based models (warmth × competence) explaining 81% of variance.
- Warmth exerts a substantially stronger influence on trustworthiness than competence in the stereotype model (β₁ ≫ β₂).
- Elicited priors predict single-shot reciprocity behavior better than uniform or human priors (RMSD reduction and higher Pearson’s r).

## Why This Works (Mechanism)

### Mechanism 1: Iterated In-Context Learning Converges to Implicit Priors
- Claim: Repeatedly prompting an LLM to infer trustworthiness from limited social interaction samples, then regenerating samples from those inferences, causes the chain to converge to the model's prior distribution over trustworthiness.
- Mechanism: A Markov chain is constructed where each iteration presents the LLM with B=5 past interactions sampled from a Binomial distribution parameterized by the previous iteration's trustworthiness estimate. The information bottleneck (limited batch size) prevents accumulation of external evidence, causing the chain to reveal inductive biases rather than memorized data. The stationary distribution converges to the Beta prior p(r).
- Core assumption: The LLM's inference process approximates Bayesian updating under a Beta-Binomial model, and the iterated learning chain is ergodic with a unique stationary distribution.
- Evidence anchors:
  - [abstract] "employ iterated in-context learning to recover the LLM's implicit prior over trustworthiness"
  - [Methods] "the iterated learning process reveals the model's underlying inductive biases rather than allowing it to accumulate or memorize past observations"
  - [corpus] Related work (Zhu & Griffiths 2024a) demonstrates iterated in-context learning elicits priors across cognitive domains; corpus lacks direct replication for trust games specifically.
- Break condition: If the LLM's inference deviates substantially from Bayesian updating (e.g., exhibits path-dependent biases or incoherent probability judgments), or if batch size B is too large (allowing memorization), convergence to prior fails.

### Mechanism 2: Trust Game Operationalizes Behavioral Trustworthiness
- Claim: The Trust Game captures trustworthiness as the fraction of resources a trustee voluntarily returns, independent of self-report or task-specific reliability.
- Mechanism: A trustor invests x tokens; the amount is multiplied by m=3. The trustee returns y tokens. Return ratio r = y/(mx) ∈ [0,1] quantifies trustworthiness. Under the Beta-Binomial model, returned amounts follow Binomial(mx, r), with r as the latent trustworthiness parameter.
- Core assumption: Trustworthiness can be modeled as a stable propensity (a single r value) rather than being highly context-dependent or strategically variable across investment levels.
- Evidence anchors:
  - [abstract] "The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent"
  - [Methods] "returned amount follows a Binomial likelihood, p(y|x, r) = Binomial(y|mx, r)"
  - [corpus] Trust games are widely used in behavioral economics; corpus confirms convergent use in LLM agent studies (e.g., "Social preferences with unstable interactive reasoning: Large language models in economic trust games").
- Break condition: If trustworthiness varies systematically with investment level x or interaction history (violating the single-parameter assumption), the Beta-Binomial model becomes misspecified.

### Mechanism 3: Stereotype Dimensions (Warmth × Competence) Predict Persona-Specific Trustworthiness
- Claim: GPT-4.1's trustworthiness toward different trustor personas is well predicted by perceived warmth and competence, with warmth exerting stronger influence.
- Mechanism: A linear model r = β₀ + β₁W + β₂C + β₃(W×C) regresses trustworthiness on warmth (W) and competence (C) dummy variables. Coefficients estimated from a 2×2 factorial design (high/low warmth × high/low competence) generalize to realistic personas via model-rated warmth/competence scores.
- Core assumption: The LLM's internal stereotype representations align sufficiently with human stereotype content (Fiske 2018) to enable prediction via the warmth-competence framework.
- Evidence anchors:
  - [abstract] "variation in trustworthiness is well predicted by a stereotype-based model grounded in perceived warmth and competence, with warmth having a stronger influence than competence"
  - [Experiment 2] "warmth exerts a substantially stronger influence than competence (β₁ ≫ β₂)"; R² = 0.81 for generalization to realistic personas
  - [corpus] Limited direct corpus evidence; stereotype content model is well-established in social psychology but less validated specifically for LLMs.
- Break condition: If the LLM's persona representations do not decompose along warmth/competence dimensions (e.g., rely on unrelated features), or if persona ratings are inconsistent/unstable, the model's predictive power degrades.

## Foundational Learning

- Concept: Beta-Binomial Conjugacy
  - Why needed here: The paper models trustworthiness as a latent parameter r with Beta prior and Binomial likelihood; understanding conjugate updating is essential to interpret how posteriors are computed and why iterated learning converges to the prior.
  - Quick check question: Given a Beta(α, β) prior and observing y successes out of mx trials, what are the posterior parameters?

- Concept: Markov Chain Convergence (Stationary Distribution)
  - Why needed here: The iterated in-context learning procedure is explicitly framed as a Markov chain over prompts; convergence to a stationary distribution is the theoretical justification for why the method elicits priors.
  - Quick check question: What conditions must a Markov chain satisfy to guarantee convergence to a unique stationary distribution?

- Concept: Stereotype Content Model (Warmth × Competence)
  - Why needed here: Explaining persona-based trustworthiness variation requires understanding the warmth/competence framework from social psychology that the paper adopts.
  - Quick check question: In the stereotype content model, which dimension more strongly predicts trust-related behaviors, and why?

## Architecture Onboarding

- Component map:
  - Prompt constructor -> LLM inference engine -> Sample generator -> Iteration controller -> Convergence monitor

- Critical path:
  1. Initialize chain with seed trustworthiness value.
  2. Generate B=5 interaction samples from Binomial(mx, r̂_previous).
  3. Prompt LLM with sample history; extract r̂_new.
  4. Repeat for 30 iterations; monitor R̂ ≤ 1.1 for convergence.
  5. Aggregate across 270 chains (30 seeds × 30 restarts) to estimate prior distribution.

- Design tradeoffs:
  - Batch size B=5: Smaller B accelerates convergence to prior but increases variance per iteration; larger B stabilizes estimates but risks memorization.
  - Fixed investment levels: Controls for investment effects but may miss context-dependent reciprocity.
  - Temperature=1: Samples from true distribution but increases output variance; lower temperature reduces variance but biases estimates.

- Failure signatures:
  - Non-convergence (R̂ > 1.1): May indicate model instability, prompt ambiguity, or insufficient chain length.
  - Bimodal or multi-modal priors: Suggests mixture behaviors (e.g., strategic vs. altruistic responses) not captured by single Beta distribution.
  - Systematic deviation from human priors: May reflect model-specific training biases rather than human-aligned priors.

- First 3 experiments:
  1. Baseline prior elicitation: Run the iterated in-context learning pipeline on GPT-4.1 with neutral trustor description; verify convergence and compare elicited prior to human baseline (KL divergence).
  2. Persona sensitivity test: Replace neutral trustor with varied personas (doctor, AI, public figures); quantify trustworthiness range (0.298–0.447 in paper) and fit warmth×competence regression model.
  3. Prior predictive validation: Elicit single-shot reciprocity behavior from LLM (B=1 condition); compare Bayesian predictions from elicited prior vs. uniform/human priors using RMSD and Pearson's r.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM trustworthiness vary systematically with investment level or across repeated interactions with the same partner?
- Basis in paper: [explicit] The authors state: "One limitation of our proposed method is the assumption that the trustworthiness prior is stable across social interactions. This need not be the case: if the trustworthiness parameter r varies with the investment level x or across interactions, a hierarchical or conditional model of reciprocity would be more appropriate."
- Why unresolved: The current Beta-Binomial model assumes a static trustworthiness parameter, but human strategic behavior is context-dependent, and LLMs aligned with human preferences may share this sensitivity.
- What evidence would resolve it: Run experiments varying investment levels systematically and analyzing whether elicited priors shift; test repeated-game scenarios to measure within-partner adaptation.

### Open Question 2
- Question: Can neural representations of trustworthiness in LLMs be identified and used to control trust-related behavior?
- Basis in paper: [explicit] The authors state: "open-weight models provide a viable avenue for investigating the neural representations of trustworthiness in LLMs. Moreover, understanding these neural representations may inform the design and control of more trustworthy AI systems."
- Why unresolved: The study focused on behavioral elicitation from proprietary and open-weight models without probing internal representations, leaving the mechanistic basis of trustworthiness unexplored.
- What evidence would resolve it: Apply interpretability techniques (e.g., probing classifiers, activation steering) to open-weight models to locate and manipulate trustworthiness-related representations.

### Open Question 3
- Question: What factors beyond risk propensity explain why some LLMs exhibit more human-like trustworthiness priors?
- Basis in paper: [inferred] The correlation analysis found only ARS (risk propensity) significantly correlated with human prior alignment (r = -0.58, p = 0.039), while IFEval and LMArena showed no significant relationships, leaving the determinants of human-like trust priors unclear.
- Why unresolved: The sample size of 20 models limits statistical power, and the tested benchmarks may not capture the relevant dimensions (e.g., social reasoning, theory of mind, training data composition).
- What evidence would resolve it: Test a larger model set; include benchmarks for social cognition and analyze training corpus characteristics; perform controlled ablations on model components.

### Open Question 4
- Question: Do LLMs' trustworthiness priors predict actual human trust behavior toward different personas?
- Basis in paper: [inferred] The authors assume GPT-4.1 can serve as "a surrogate for a large population of human participants" based on prior alignment, but this assumption was not validated against real human behavior toward the specific personas tested.
- Why unresolved: The study compared LLM priors to meta-analytic human baselines but did not collect new human data on trustworthiness toward the specific personas (e.g., Elon Musk, Malala Yousafzai).
- What evidence would resolve it: Conduct human Trust Game experiments with the same persona manipulations and correlate behavior with GPT-4.1's predictions.

## Limitations
- The method assumes a single stable trustworthiness parameter, but real LLM behavior may vary contextually with investment size or interaction history, violating the model's core assumption.
- While the stereotype content model is well-established in social psychology, the evidence for its direct application to LLM persona representations is limited in the corpus.
- The study relies on the LLM's inference approximating Bayesian updating; if the model's reasoning deviates (e.g., exhibits path dependence or inconsistent probability judgments), convergence to a meaningful prior fails.

## Confidence
- **High confidence:** The core mechanism of iterated in-context learning converging to an LLM's implicit prior under the Beta-Binomial framework (supported by direct evidence from the abstract and methods, plus related work on prior elicitation).
- **Medium confidence:** The generalizability of the elicited priors to real human-AI interactions, as the method measures model behavior under specific Trust Game conditions which may not reflect all contexts.
- **Medium confidence:** The predictive power of the warmth-competence stereotype model for persona-specific trustworthiness, given limited direct corpus validation for LLMs.

## Next Checks
1. **Prior Predictive Validation:** Implement the single-shot reciprocity test (B=1 condition) to directly compare Bayesian predictions from the elicited prior versus uniform/human priors against observed LLM behavior. Measure RMSD and Pearson's r to quantify predictive accuracy.
2. **Investment Level Sensitivity:** Re-run the Trust Game experiments with varied investment amounts or within-trial investment variation to test whether the single-parameter Beta-Binomial model holds, or if trustworthiness systematically depends on investment level.
3. **Stereotype Content Model Replication:** Validate the warmth-competence predictive model by collecting additional persona ratings (e.g., from different annotators or via automated methods) and re-fitting the regression to check robustness and generalization beyond the initial 2x2 factorial design.