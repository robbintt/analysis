---
ver: rpa2
title: 'COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating
  Multimodal Context in Referential Communication'
arxiv_id: '2506.22274'
source_url: https://arxiv.org/abs/2506.22274
tags:
- attention
- target
- scene
- noise
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COOCO, a dataset for studying how Vision-Language
  Models (VLMs) generate object references under different levels of scene-object
  semantic congruency. The dataset contains images where target objects are replaced
  with objects of varying relatedness to the scene (low, medium, high), plus controls
  with noise applied to targets or contexts.
---

# COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication

## Quick Facts
- arXiv ID: 2506.22274
- Source URL: https://arxiv.org/abs/2506.22274
- Authors: Filippo Merlo; Ece Takmaz; Wenkai Chen; Albert Gatt
- Reference count: 27
- Primary result: VLMs shift reliance from target to scene context under noise degradation and show increased target attention in mid-layers, especially for semantically incongruent objects.

## Executive Summary
This paper introduces COOCO, a dataset for studying how Vision-Language Models (VLMs) generate object references under varying levels of scene-object semantic congruency. The dataset contains images where target objects are replaced with objects of varying relatedness to the scene (low, medium, high), plus controls with noise applied to targets or contexts. Experiments across six VLMs show that models rely more on scene context when targets are degraded by noise, and allocate more attention to target regions in mid-level transformer layers, especially under moderate noise. Accuracy and semantic similarity metrics indicate that models perform best when targets are congruent with the scene, but adapt by shifting attention toward scene context when targets are corrupted.

## Method Summary
The study evaluates six VLMs (KOSMOS-2 1.6B, Molmo 7B, xGen-MM-Phi3 4.4B, LLaVA-OneVision 0.5B/7B, Qwen2.5-VL 7B) on the COOCO dataset, which contains 18,395 images derived from COCO-Search18. The dataset includes original images plus variants with targets replaced by objects of low/medium/high relatedness, same-target controls, and Gaussian noise (λ = 0.0, 0.5, 1.0) applied to target/context/all regions. Models are prompted to name objects in specified regions, and performance is measured using RefCLIPScore, text-based semantic similarity, and accuracy thresholds. Attention analysis is performed on the 0.5B LLaVA variant across vision encoder layers.

## Key Results
- VLMs perform best when target objects are semantically congruent with their scenes, but shift to scene context when targets are degraded by noise.
- Target-focused attention peaks in mid-level transformer layers (7-15), particularly under moderate noise conditions.
- Semantically incongruent targets attract disproportionately higher attention than congruent ones, even when models ultimately misclassify them.

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Contextual Reliance
When local visual features are degraded by noise, the model up-weights the influence of the global scene embedding to infer the target's identity. Conversely, when the target is visually clear but semantically incongruent with the scene, the scene context acts as a distractor, degrading performance. This relies on statistical priors of scene-object co-occurrences acquired during pre-training.

### Mechanism 2: Mid-Layer Semantic Fusion
Mid-layers (7-15) serve as the "semantic fusion" point where objectness is validated against the scene. Increased attention in these layers reflects the model's effort to resolve identity, particularly under moderate noise or semantic ambiguity. This follows a hierarchical processing structure where mid-layers integrate "object" vs. "background" distinctions.

### Mechanism 3: Incongruity-Induced Attention Capture
Objects that violate scene semantics (low relatedness) attract disproportionately higher attention than congruent objects, even if the model ultimately misclassifies them. The model detects a violation of its learned scene priors, triggering an attentional resource allocation to verify the input.

## Foundational Learning

- **Concept: Referential Expression Generation (REG)** - Needed to understand why context is crucial for disambiguating *which* object and *what* kind. Quick check: How does replacing a "laptop" with a "ham" in an office scene test the limits of a model's ability to generate accurate references?

- **Concept: Semantic Congruency vs. Visual Salience** - Needed to distinguish between an object being visually clear vs. semantically expected. Quick check: In the COOCO dataset, does "Medium Relatedness" imply a visual distortion or a category mismatch?

- **Concept: Attention Pooling & Aggregation** - Needed to interpret how the authors calculate the "attention to target" from raw transformer weights. Quick check: Why do the authors normalize attention maps by subtracting a dataset-wide average baseline?

## Architecture Onboarding

- **Component map:** Vision Transformer (ViT-SigLIP) -> Projector (2-layer MLP) -> LLM Decoder (Autoregressive) -> Output
- **Critical path:** Input Image + Bounding Box prompt → Vision Encoder processes image; mid-layers (7-15) allocate attention to the target patch → Projector aligns visual features → LLM Decoder attends to visual tokens; if target is noisy, attention shifts to context tokens → Output generation (Object Name)
- **Design tradeoffs:** LLaVA 0.5B vs. 7B/Other models: The authors use the 0.5B model for deep attention analysis because larger models are computationally intractable for full layer-wise attention extraction.
- **Failure signatures:** High Context Noise + Low Relatedness: Performance actually improves slightly because the "distracting" context is obscured. Target Noise + Congruent Scene: Model "hallucinates" the scene-expected object rather than the noisy target, leading to low accuracy but high semantic fit.
- **First 3 experiments:** 1) Baseline Sanity Check: Run the model on "Original" vs. "Clean" (object removed) images to ensure the model is actually looking at the target region. 2) Noise Sensitivity Test: Apply Gaussian noise (level 0.0, 0.5, 1.0) to *only* the context region to verify if the model relies on target features. 3) Semantic Distractor Test: Compare "High Relatedness" vs. "Low Relatedness" generations; plot the output-to-scene semantic similarity.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent do the observed context-dependency effects generalize to novel scenes excluded from the models' pre-training corpora? The Discussion notes that superior performance on congruent targets "may be due to the COCO images having been included in the training data of all models."

- **Open Question 2:** Are the attentional shifts in VLMs driven by semantic incongruity or by low-level visual artifacts introduced by the inpainting process? The methodology relies on a pipeline using LaMa and PowerPaint to replace objects; artifacts could serve as unintended visual cues.

- **Open Question 3:** Why does the "same-target" condition (inpainted original object) yield higher performance than the unmodified original images? The authors speculate the generated object might be "more prototypical," but it remains unconfirmed whether this stems from the object being a "cleaner" example or if the inpainting process inadvertently removes complex visual noise.

## Limitations

- Dataset Composition and Control Validity: The study relies on artificially generated scene variants through inpainting and noise addition, which may introduce subtle biases that affect model behavior differently than natural scenes would.
- Attention Analysis Constraints: Attention analysis is performed only on the 0.5B LLaVA variant due to computational constraints, yet conclusions about attention dynamics are generalized across all six models tested.
- Metric Sensitivity and Interpretation: The study uses cosine similarity thresholds (0.9) for accuracy determination and RefCLIPScore for semantic similarity, which may not capture nuanced performance differences across semantic relatedness conditions.

## Confidence

- Adaptive Contextual Reliance: **High** - The evidence from controlled noise experiments across multiple models is robust and internally consistent.
- Mid-Layer Semantic Fusion: **Medium** - While the attention pattern is well-documented in the 0.5B model, generalization to other architectures lacks direct validation.
- Incongruity-Induced Attention Capture: **Medium** - The attention capture effect is observable, but interpreting whether this represents genuine "detection of incongruity" versus simple salience detection requires additional validation.

## Next Checks

1. **Cross-Architecture Attention Validation:** Extract and compare attention patterns from at least two other models (e.g., Qwen2.5-VL 7B and Molmo 7B) to verify whether the mid-layer attention fusion phenomenon is consistent across different VLM architectures.

2. **Human Baseline Comparison:** Conduct a human study using the same images and task instructions to establish whether human reference generation exhibits similar sensitivity to semantic context and noise as the VLMs.

3. **Semantic Distance Quantification:** Systematically measure the semantic distance between target objects and scene contexts using multiple embedding spaces (not just CLIP) to verify that the "medium relatedness" category truly represents an intermediate semantic distance.