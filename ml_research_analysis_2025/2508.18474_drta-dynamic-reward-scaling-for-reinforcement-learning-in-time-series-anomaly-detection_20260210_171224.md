---
ver: rpa2
title: 'DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly
  Detection'
arxiv_id: '2508.18474'
source_url: https://arxiv.org/abs/2508.18474
tags:
- anomaly
- learning
- detection
- reward
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses anomaly detection in time series data, a critical
  task in domains like finance, healthcare, and industrial monitoring. Traditional
  methods struggle with limited labeled data, high false-positive rates, and difficulty
  generalizing to novel anomaly types.
---

# DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection

## Quick Facts
- arXiv ID: 2508.18474
- Source URL: https://arxiv.org/abs/2508.18474
- Reference count: 40
- Key outcome: DRTA achieves 0.90 F1-score on Yahoo A1 and 0.80 on Yahoo A2 at 1% queried samples, outperforming state-of-the-art methods.

## Executive Summary
DRTA addresses anomaly detection in time series data by integrating reinforcement learning with dynamic reward scaling, Variational Autoencoders, and active learning. The framework uses an adaptive reward mechanism that balances exploration (via VAE reconstruction error) and exploitation (via classification accuracy) through dynamically adjusted reward weights. This enables effective anomaly detection in low-label systems while maintaining high precision and recall. Experimental results on Yahoo benchmark datasets demonstrate superior performance compared to unsupervised and semi-supervised baselines, particularly at low query rates where labeled data is scarce.

## Method Summary
DRTA combines a DQN agent with VAE-based intrinsic rewards and active learning through margin sampling. The agent receives a composite reward consisting of classification rewards and dynamically scaled VAE reconstruction error. A control loop adjusts the scaling factor λ(t) based on performance relative to a target reward, transitioning the agent from unsupervised exploration to semi-supervised exploitation. The active learning component queries samples where the agent is most uncertain (low Q-value margin between normal and anomaly actions), reducing the labeling burden while maintaining detection accuracy.

## Key Results
- Achieves 0.90 F1-score on Yahoo A1 and 0.80 on Yahoo A2 at 1% queried samples
- Outperforms state-of-the-art unsupervised and semi-supervised methods across benchmark datasets
- Demonstrates strong adaptability and efficiency in real-world anomaly detection with minimal labeled data
- Shows peak performance at low query rates (1-5%), with degradation at higher rates suggesting potential overfitting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic scaling of intrinsic reward (λ(t)) enables transition from unsupervised exploration to semi-supervised exploitation.
- **Mechanism:** Control loop adjusts λ(t) based on episode rewards vs target reward, starting high to emphasize VAE reconstruction error then decaying to prioritize classification accuracy.
- **Core assumption:** Reconstruction error reliably indicates "interesting" states worth exploring before classifier confidence.
- **Evidence anchors:**
  - [Section IV-B]: Equation 8 defines proportional control update for λ(t)
  - [Section IV-B]: "Starts at high value to prioritize exploration... stabilizes as agent shifts to exploitation"
  - [Corpus]: Neighbor paper supports generalization to multivariate contexts
- **Break condition:** If target reward R_target is misconfigured, λ(t) may saturate at λ_max (reconstruction noise dominance) or λ_min (ignoring VAE signal).

### Mechanism 2
- **Claim:** Asymmetric reward shaping (TP=5, FP=-1) biases agent toward high recall for rare anomalies.
- **Mechanism:** 5:1 ratio for True Positives vs True Negatives with heavy penalty for False Negatives forces agent to value minority class detection.
- **Core assumption:** Cost of missed anomaly significantly outweighs cost of false alarm.
- **Evidence anchors:**
  - [Section IV-B]: Equation 5 defines asymmetric reward values
  - [Abstract]: Maintains "high precision and recall" despite "limited labeled data"
  - [Corpus]: Specific asymmetry ratio appears domain-specific heuristic
- **Break condition:** Noisy normal data resembling anomalies causes over-classification, degrading precision.

### Mechanism 3
- **Claim:** Active learning via margin sampling reduces labeling burden by querying uncertain states.
- **Mechanism:** System calculates margin |Q(s,a₁) - Q(s,a₂)|; low margins indicate uncertainty and are queried, labeled, then propagated via Label Spreading.
- **Core assumption:** Low Q-value margin correlates with epistemic uncertainty rather than irreducible noise.
- **Evidence anchors:**
  - [Section IV-C]: Describes Margin Sampling strategy and Equation 3
  - [Section V-B]: Achieves F1=0.90 at 1% queries, demonstrating data efficiency
  - [Corpus]: Precursor paper (RLVAL) validates active learning component
- **Break condition:** Untrained or miscalibrated Q-network makes margins arbitrary, causing random queries.

## Foundational Learning

- **Concept: Deep Q-Learning (DQN) & Experience Replay**
  - **Why needed here:** Core agent is DQN using Bellman equation to update Q-values; experience replay breaks correlation in time-series data.
  - **Quick check question:** If replay buffer stored only last 100 steps, why would training become unstable?

- **Concept: Variational Autoencoders (VAE) for Reconstruction Error**
  - **Why needed here:** VAE provides intrinsic reward (R₂) by failing to reconstruct anomalies trained only on normal data.
  - **Quick check question:** What happens if VAE is "too good" and reconstructs anomalies perfectly?

- **Concept: Reward Shaping & Potential-Based Rewards**
  - **Why needed here:** Extends standard rewards with dynamic shaping; dynamic λ(t) guides without changing optimal policy.
  - **Quick check question:** Why is dynamic λ(t) preferable to static λ=1.0 when reward distribution changes during training?

## Architecture Onboarding

- **Component map:** Input windows -> VAE (latent encoding + reconstruction) + LSTM Q-Network (Q-values) -> Reward Calculator (R₁ + λ(t)R₂) -> ε-greedy policy + Active Learning (Margin Sampler) -> Experience Replay Buffer

- **Critical path:** Reward Calculator is most sensitive component; system relies on normalized sum of discrete classification reward and continuous reconstruction error.

- **Design tradeoffs:**
  - Yahoo A1 (0.90 F1) outperforms A2 (0.80 F1) due to synthetic anomalies differing from VAE's learned normal patterns
  - Peak performance at 1-5% queries; degrades at 10% suggesting active learning or reward scaling may overfit
  - Model may memorize queried set rather than generalize at higher labeling rates

- **Failure signatures:**
  - λ Oscillation: High α causes wild reward scale oscillation, preventing convergence
  - Precision/Recall Imbalance: Noisy VAE error causes constant "Anomaly" prediction, high recall but near-zero precision
  - Overfitting to Query: F1 drop at 10% queries suggests memorization over generalization

- **First 3 experiments:**
  1. Baseline Sanity Check: Compare DRTA with λ(t)=0 (Pure RL) vs. λ(t)=c (Static Shaping) vs. Dynamic Shaping on Yahoo A1
  2. Hyperparameter Sensitivity: Vary R_target to observe λ(t) equilibrium effects on exploration/exploitation balance and final F1
  3. Active Learning Ablation: Compare random sampling vs. margin-based sampling at 1% query rate to quantify efficiency gain

## Open Questions the Paper Calls Out
None

## Limitations
- Performance sensitivity to target reward settings affects exploration-exploitation balance
- VAE reconstruction noise can dominate reward signals, causing precision/recall imbalances
- Performance degradation at higher query rates suggests potential overfitting to labeled data
- Method effectiveness appears dataset-dependent, struggling with synthetic anomalies differing from learned normal patterns

## Confidence
- Mechanism 1: High confidence - explicitly defined with mathematical formulation and control theory
- Mechanism 2: Medium confidence - limited empirical validation of specific 5:1 ratio and domain generalizability
- Mechanism 3: Low confidence - assumes Q-value margin correlates with uncertainty without empirical validation

## Next Checks
1. **Target Reward Sensitivity Analysis**: Systematically vary R_target across orders of magnitude to quantify effects on λ(t) equilibrium and final detection performance
2. **Q-Value Margin Calibration**: Compare margin-based active learning against Monte Carlo dropout or ensemble disagreement to validate correlation with epistemic uncertainty
3. **Cross-Dataset Generalization Test**: Evaluate DRTA on NAB or SMD benchmarks with different anomaly types to assess dynamic reward scaling generalization beyond Yahoo's synthetic anomalies