---
ver: rpa2
title: 'Delayformer: spatiotemporal transformation for predicting high-dimensional
  dynamics'
arxiv_id: '2506.11528'
source_url: https://arxiv.org/abs/2506.11528
tags:
- delayformer
- time
- variables
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Delayformer is a new time-series forecasting method that combines
  ideas from dynamical systems theory and deep learning. It transforms each variable
  into a delay-embedded state and uses a shared Vision Transformer encoder to learn
  common representations across variables.
---

# Delayformer: spatiotemporal transformation for predicting high-dimensional dynamics

## Quick Facts
- arXiv ID: 2506.11528
- Source URL: https://arxiv.org/abs/2506.11528
- Reference count: 40
- Delayformer outperforms state-of-the-art methods on synthetic and real-world datasets for both long-term and short-term forecasting

## Executive Summary
Delayformer is a novel time-series forecasting method that transforms each variable into a delay-embedded state (Hankel matrix) and processes these matrices using a shared Vision Transformer encoder. This approach overcomes limitations of traditional methods in handling limited, noisy data and complex interactions by leveraging dynamical systems theory and deep learning. The method shows strong performance across synthetic and real-world datasets, achieving top-two performances on long-term forecasting benchmarks and demonstrating potential as a foundation model for time-series prediction.

## Method Summary
Delayformer converts 1D time-series variables into 2D Hankel matrices via the mvSTI transformation, treating each matrix as an image patch for a shared Vision Transformer encoder. The encoder processes patches from all variables' Hankel matrices, learning common representations, while distinct linear decoders predict next states for each variable. This channel-independent approach allows the model to handle complex spatiotemporal dependencies while maintaining computational efficiency. The method is particularly effective for high-dimensional systems where traditional forecasting methods struggle with data scarcity and noise.

## Key Results
- Achieves top-two performances on long-term forecasting benchmarks (ETTh1, ETTh2, ETTm1)
- Outperforms baselines under various conditions including limited training data and observation dimensions
- Shows strong cross-domain generalization capabilities with zero-shot transfer between ETT datasets
- Demonstrates robustness to noise levels in synthetic coupled Lorenz system experiments

## Why This Works (Mechanism)

### Mechanism 1
Delay embedding transforms temporal dynamics into reconstructable system states, enabling prediction of high-dimensional systems from individual variable observations. Each observed variable is converted into a delay-embedded state vector (Hankel matrix) via Takens' embedding theorem. When the embedding dimension L > 2d (where d is the attractor's box-counting dimension), the topology of the original dynamical system can be reconstructed from any single variable's delay embedding.

### Mechanism 2
Treating Hankel matrices as images and using a shared Vision Transformer encoder enables cross-learning of common dynamical representations across variables. The Hankel matrix is split into patches and processed by a shared ViT encoder. Global matrix structure captures long-range dependencies while local patches capture short-range dynamics. The shared encoder forces learning of representations that generalize across all variables' embedded states.

### Mechanism 3
Linear decoders operating on shared latent representations provide sufficient expressivity for next-state prediction, with theoretical grounding in Koopman operator theory. After the shared encoder produces latent representations Zₖ, distinct linear layers (one per variable) map these to predictions. The encoder's high-dimensional representation can be treated as projection into a Koopman space, where system evolution becomes linear.

## Foundational Learning

- **Takens' Delay Embedding Theorem**
  - Why needed here: The entire mvSTI transformation depends on understanding why delay vectors can reconstruct system topology. Without this, the Hankel matrix construction appears arbitrary.
  - Quick check question: Given a 10-dimensional system with attractor dimension d=3, what minimum embedding dimension L would allow topological reconstruction?

- **Hankel Matrix Structure**
  - Why needed here: The method treats these matrices as images for ViT processing; understanding their diagonal symmetry and temporal ordering is essential for debugging patch configurations.
  - Quick check question: For a time series [1,2,3,4,5,6] with embedding dimension L=3, what is the resulting Hankel matrix?

- **Channel Independence Strategy in Transformers**
  - Why needed here: Delayformer uses CI during training (shared encoder, separate loss aggregation per channel) but claims delay embeddings overcome CI's typical robustness limitations.
  - Quick check question: Why might channel independence fail on unseen variables, and how does delay embedding theoretically mitigate this?

## Architecture Onboarding

- **Component map**: Sliding window extraction -> Per-variable Hankelization -> Patching -> Shared ViT encoder -> N distinct linear decoders
- **Critical path**: Hankel matrix construction (L and delay parameter) -> Patch size selection -> Encoder capacity vs. overfitting on limited data
- **Design tradeoffs**:
  - Larger L: Better theoretical reconstruction guarantees but larger matrices, more patches, higher memory
  - Larger patch size: Fewer tokens, faster training, but coarser granularity may miss local dynamics
  - Deeper encoder: Better representation power but risk of overfitting on small datasets
  - Linear vs. nonlinear decoders: Linear is theoretically motivated (Koopman) but may underfit complex mappings
- **Failure signatures**:
  - Predictions converge to mean: Encoder may be too weak or learning rate too low
  - Good training performance, poor test performance: Attractor coverage assumption violated
  - Patch-related artifacts: p₁, p₂ may not align with matrix structure
  - Variable-specific failure: Check if that variable's Hankel matrix has unusual scale or NaN values
- **First 3 experiments**:
  1. Baseline replication on synthetic 30D coupled Lorenz system with noise levels σ = 0, 0.2, 0.5
  2. Ablation on embedding dimension L: Test L = 12, 27, 42, 96 on Weather dataset
  3. Cross-domain transfer test: Train on ETTh1/ETTh2/ETTm1, test zero-shot on ETTm2

## Open Questions the Paper Calls Out

### Open Question 1
Can Delayformer's performance and generalization capability be benchmarked against current state-of-the-art foundation models when pre-trained on sufficiently large and diverse time-series datasets? The current study validates architecture on specific benchmarks but lacks direct comparison with largest established foundation models at equivalent scales.

### Open Question 2
Would integrating more advanced computer vision architectures like Convolutions Vision Transformer (CvT) enhance the representation learning and prediction accuracy of the Delayformer framework? The current implementation uses standard ViT, leaving potential improvements from hybrid architectures untested.

### Open Question 3
Can a theoretically grounded or automated method be developed to determine optimal embedding dimension and time delay for Hankel matrices without prohibitive computational costs on large-scale datasets? Current selection relies on empirical experience, becoming a bottleneck as data scales increase.

## Limitations

- The Hankel-as-image transformation and shared ViT encoder approach lacks direct corpus validation for time-series forecasting
- Claims about linear decoder sufficiency via Koopman theory remain empirically unverified against nonlinear alternatives
- Hyperparameter sensitivity (embedding dimension L, patch size) is not thoroughly explored
- Theoretical grounding in Takens' theorem assumes low-dimensional attractors that may not hold for all real-world systems

## Confidence

- **High confidence**: Method works well on synthetic coupled Lorenz systems and shows consistent top-two performance on long-term benchmarks
- **Medium confidence**: Cross-domain generalization claims based on limited ETT dataset transfer experiments
- **Low confidence**: Claims about Hankel matrix properties benefiting ViT patch processing and specific choice of channel independence strategy

## Next Checks

1. **Attractor dimension verification**: For each real-world dataset, estimate the box-counting dimension d of the underlying dynamical system. Verify that chosen L values satisfy L > 2d to validate Takens' theorem applicability.

2. **Patch size sensitivity analysis**: Systematically test different patch configurations on the 30D Lorenz system across noise levels. Document performance degradation patterns to identify optimal patch granularity.

3. **Linear vs. nonlinear decoder comparison**: Replace linear decoders with small MLP decoders on the coupled Lorenz system. Compare MSE across embedding dimensions to test Koopman theory's practical limitations.