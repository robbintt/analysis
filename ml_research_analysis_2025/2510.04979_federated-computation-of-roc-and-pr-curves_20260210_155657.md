---
ver: rpa2
title: Federated Computation of ROC and PR Curves
arxiv_id: '2510.04979'
source_url: https://arxiv.org/abs/2510.04979
tags:
- error
- area
- curves
- curve
- quantile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to compute ROC and PR curves in federated
  learning without accessing raw data. It uses histogram-based quantile estimation
  and monotone interpolation to approximate the curves, achieving strong privacy and
  low communication.
---

# Federated Computation of ROC and PR Curves

## Quick Facts
- **arXiv ID:** 2510.04979
- **Source URL:** https://arxiv.org/abs/2510.04979
- **Reference count:** 40
- **Primary result:** Presents a method to compute ROC and PR curves in federated learning without accessing raw data, achieving strong privacy and low communication with Area Error often under 10⁻³ for ROC and 10⁻² for PR.

## Executive Summary
This paper introduces a novel approach for computing ROC and PR curves in federated learning settings without accessing raw data. The method uses histogram-based quantile estimation combined with monotone interpolation to approximate the curves, achieving strong privacy guarantees through distributed differential privacy. Theoretical analysis shows the Area Error scales as O(1/Q) for ROC curves and Õ(1/Q) for PR curves, with an additional Õ(1/nε) term under differential privacy. Experiments demonstrate the method outperforms prior work and remains robust even under strong privacy constraints and data heterogeneity.

## Method Summary
The method works by having clients construct hierarchical histograms of their prediction scores, which are then securely aggregated at the server. Quantiles are extracted from the aggregated histograms and monotone interpolation (PCHIP) is used to reconstruct the ECDF and approximate the ROC and PR curves. For privacy, clients add statistical noise to their histogram counts before transmission. The approach separates positive and negative prediction scores for PR curve estimation to ensure precision numerator ≤ denominator. Hyperparameters include branching factor b=2, tree height h=⌈log₂Q⌉+2, and the number of quantiles Q.

## Key Results
- Achieves Area Error (AE) often under 10⁻³ for ROC curves and 10⁻² for PR curves with Q≈100
- Error scales as O(1/Q) for ROC and Õ(1/Q) for PR under secure aggregation
- Under distributed differential privacy, AE is Õ(1/Q + 1/nε), with a plateau at strong privacy levels
- Outperforms prior methods and remains robust to data heterogeneity across clients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ROC and PR curves can be approximated accurately without raw data by reconstructing the Empirical Cumulative Distribution Function (ECDF) using a fixed number of quantiles (Q).
- **Mechanism:** The server estimates the ECDF of prediction scores separately for positive and negative classes using Q quantiles. It then applies monotone interpolation (specifically PCHIP) to these quantile points to estimate TPR and FPR at any threshold. The Area Error theoretically scales as O(1/Q) for ROC and Õ(1/Q) for PR.
- **Core assumption:** The prediction score distributions are "well-behaved" (Θ(1)-Lipschitz), meaning the density of scores does not spike abruptly, allowing smooth interpolation between quantiles.
- **Evidence anchors:** [Abstract]: "proposes a novel method for approximating ROC and PR curves... by estimating quantiles of the prediction score distribution." [Section 3.1]: "PCHIP interpolation is used to estimate the ECDF between quantile points."
- **Break condition:** If the score distribution has heavy tails or extreme spikes (violating the Lipschitz condition), the interpolation between sparse quantiles will create significant deviation from the true curve.

### Mechanism 2
- **Claim:** Histogram-based aggregation enables robust federated quantile estimation that is agnostic to data heterogeneity.
- **Mechanism:** Clients construct local hierarchical histograms (branching factor b, height h) of their prediction scores. These histogram counts are transmitted to the server and summed. Because histogram summation is additive, the resulting global histogram is mathematically identical to one built on centralized data, regardless of how data is distributed across clients.
- **Core assumption:** The number of histogram bins (b^h) is sufficient to capture the granularity of the target quantiles (Q).
- **Evidence anchors:** [Section 3.2]: "Since each client locally bins scores and the server aggregates counts additively, the resulting global histogram matches the centralized case."
- **Break condition:** If clients have vastly different score ranges not covered by the global bin boundaries, or if b^h < Q, the quantile extraction will fail or lose accuracy.

### Mechanism 3
- **Claim:** Distributed Differential Privacy (DDP) preserves privacy with a bounded increase in Area Error (Õ(1/nε)).
- **Mechanism:** Before transmission, each client adds independent statistical noise (e.g., Skellam or discrete Gaussian) to their local histogram counts. The server aggregates these noisy histograms. The privacy budget ε is split across the hierarchical layers. The resulting error in quantile location scales with the noise magnitude, which is inversely proportional to the total sample size n and privacy budget ε.
- **Core assumption:** There is a sufficiently large number of total examples (n) to dilute the noise added to the histogram bins.
- **Evidence anchors:** [Abstract]: "Theoretical bounds show... Area Error is Õ(1/Q + 1/nε) under differential privacy." [Section 5.1]: "under DDP there is a plateau due to privacy noise, as predicted by our analysis."
- **Break condition:** In small datasets or with extremely strong privacy requirements (very low ε ≪ 1), noise variance may overwhelm the signal, causing quantile estimates to become erratic.

## Foundational Learning

- **Concept: ROC vs. PR Geometry**
  - **Why needed here:** The paper approximates two distinct curves with different geometric properties; ROC is monotone, while PR is non-monotonic and sensitive to class imbalance.
  - **Quick check question:** If a dataset has a class ratio of 1:100, which curve's Area Error bound will degrade significantly according to the paper?

- **Concept: Hierarchical Histograms**
  - **Why needed here:** Understanding how the paper balances communication cost (O(Q)) with quantile accuracy via tree-based binning.
  - **Quick check question:** Why does the communication cost scale with b^h rather than the dataset size n?

- **Concept: Lipschitz Continuity (Well-behaved distributions)**
  - **Why needed here:** This is the specific mathematical condition required for the Area Error bounds to hold.
  - **Quick check question:** What happens to the quantile error guarantee if the CDF of the prediction scores is vertical (a "spike") at a certain threshold?

## Architecture Onboarding

- **Component map:** Client: Model inference -> Score binning (Hierarchical Histogram) -> Noise injection (DDP) -> Server: Secure Aggregation (Sum histograms) -> Post-processing (Consistency enforcement) -> Quantile Extractor -> Curve Interpolator (PCHIP)
- **Critical path:**
  1. Configuring Q (target quantiles) and h (histogram height)
  2. Client-side binning logic (must match server exactly)
  3. Server-side reconstruction of the ECDF from aggregate counts
- **Design tradeoffs:**
  - Accuracy vs. Communication: Increasing Q improves Area Error (O(1/Q)) but linearly increases bandwidth (O(Q))
  - PR Strategy: Estimating quantiles separately for positive/negative classes ("Separate" strategy) yields lower PR error than combining them ("Combine" strategy), as it ensures precision numerator ≤ denominator
- **Failure signatures:**
  - Plateauing Error: AE stops decreasing as Q increases (indicates privacy noise is the bottleneck, not quantile granularity)
  - Overshoot in PR: Precision estimates exceed 1.0 (indicates "Combine" strategy is being used or noise clipping is misconfigured)
- **First 3 experiments:**
  1. Baseline AE vs. Q: Run Secure Aggregation (no noise) on a centralized proxy dataset to verify the O(1/Q) scaling for ROC
  2. Privacy Scaling: Inject DDP noise with varying ε (e.g., 1.0, 0.3, 0.1) to observe the error plateau predicted by the Õ(1/nε) term
  3. Imbalance Stress Test: Subsample positive examples to reduce the class ratio r and verify the degradation of PR-AE against the Õ(1/Qr) bound

## Open Questions the Paper Calls Out

- **Question:** How can succinct non-interactive zero-knowledge proofs be integrated to verify client contributions and bound the impact of malicious clients attempting to poison the ROC or PR curve construction?
- **Basis in paper:** [explicit] The Conclusion states the current trust model focuses on privacy, and the authors plan to "study how to control the impact of malicious clients... by bounding their impact (via succinct non-interactive zero-knowledge proofs...)".
- **Why unresolved:** The current method assumes clients follow the protocol (honest-but-curious model) and does not implement mechanisms to detect or mitigate adversarial manipulation of histogram counts.
- **What evidence would resolve it:** A modified protocol incorporating ZKPs that provides theoretical bounds on the Area Error even when a fraction of clients are malicious, along with an analysis of the computational overhead.

- **Question:** Can a hybrid approach combining quantile estimation and range queries achieve stronger Area Error bounds for ROC curves at low Q without suffering from the high variance associated with pure range-query methods at large Q?
- **Basis in paper:** [explicit] The Discussion notes that at small Q, quantile methods may underperform compared to range queries. It suggests "hybrid methods of quantiles and range queries" might yield stronger bounds, though it notes current range queries introduce high variance and lack bounds for PR curves.
- **Why unresolved:** The paper identifies the limitation (performance gap at small Q) and a potential solution (hybrid method) but leaves the design and analysis of such a hybrid algorithm for future work.
- **What evidence would resolve it:** An algorithm that adaptively switches or combines both methods, demonstrating improved Area Error at small Q values while maintaining low variance and theoretical guarantees at large Q.

- **Question:** Can the quantile-based approximation framework be successfully extended to compute Precision-Recall-Gain curves while maintaining the proven Area Error bounds?
- **Basis in paper:** [explicit] The Discussion identifies "Precision-Recall-Gain curves" as a "promising direction for extending our approach" and explicitly states, "We leave their exploration for future work."
- **Why unresolved:** The analysis in the paper is restricted to standard ROC and PR curves, and it is unproven whether the O(1/Q) or Õ(1/Q) error bounds hold for the adjusted metrics in PR-Gain space.
- **What evidence would resolve it:** A formal extension of Theorem 4.3 specifically for PR-Gain curves and empirical validation showing similar approximation accuracy to standard PR curves.

## Limitations
- The theoretical error bounds critically depend on prediction score distributions being "well-behaved" (Θ(1)-Lipschitz), which may not hold for all real-world models or datasets
- Privacy guarantees rely on a sufficient number of total examples to dilute statistical noise, with error potentially plateauing at strong privacy levels
- The exact implementation of the "consistency enforcement" algorithm for post-processing noisy histograms is not specified in the paper

## Confidence

- **High:** The core mechanism of histogram-based quantile aggregation is mathematically sound and the O(1/Q) scaling for ROC under secure aggregation is well-supported by the theory
- **Medium:** The privacy error bound Õ(1/nε) is theoretically justified but relies on practical assumptions about noise distribution and post-processing that are not fully detailed
- **Low:** The applicability of the theoretical error bounds to "real-world" distributions that may not be Lipschitz, and the exact performance impact of the unspecified post-processing step

## Next Checks

1. **Lipschitz Verification:** Analyze the prediction score distributions from the Bank and Adult datasets to empirically measure their Lipschitz constants and assess whether they meet the theoretical assumptions for the error bounds
2. **Post-processing Implementation:** Implement and test a standard histogram consistency algorithm (e.g., from Hay et al.) to evaluate its impact on reducing noise-induced error in the aggregated histograms
3. **Small-n Stress Test:** Run experiments with a small number of examples (e.g., n=1000) and a strong privacy budget (ε=0.1) to observe if the error plateaus as predicted by the Õ(1/nε) term