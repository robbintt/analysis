---
ver: rpa2
title: 'TreeRL: LLM Reinforcement Learning with On-Policy Tree Search'
arxiv_id: '2506.11902'
source_url: https://arxiv.org/abs/2506.11902
tags:
- tree
- treerl
- search
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning capabilities
  in large language models through reinforcement learning with tree search. The authors
  propose TreeRL, a framework that integrates on-policy tree search with reinforcement
  learning to enhance LLM reasoning.
---

# TreeRL: LLM Reinforcement Learning with On-Policy Tree Search

## Quick Facts
- arXiv ID: 2506.11902
- Source URL: https://arxiv.org/abs/2506.11902
- Authors: Zhenyu Hou; Ziniu Hu; Yujiang Li; Rui Lu; Jie Tang; Yuxiao Dong
- Reference count: 40
- Primary result: TreeRL achieves 44.5% average accuracy on MATH500 compared to 41.6% for ChainRL on Qwen-2.5-14B

## Executive Summary
This paper addresses the challenge of improving reasoning capabilities in large language models through reinforcement learning with tree search. The authors propose TreeRL, a framework that integrates on-policy tree search with reinforcement learning to enhance LLM reasoning. The core idea involves using an efficient entropy-guided tree search algorithm (EPTree) that strategically branches from high-uncertainty intermediate steps to generate diverse responses under the same inference budget as traditional multi-chain sampling. TreeRL also incorporates process supervision derived from the tree structure, providing fine-grained rewards based on global and local advantages. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL outperforms traditional ChainRL methods, achieving higher accuracy on tasks like MATH500, Omni-MATH-500, and OlympiadBench.

## Method Summary
TreeRL introduces an entropy-guided tree search algorithm (EPTree) that operates within the reinforcement learning framework for LLM reasoning. The method strategically branches from intermediate steps with high uncertainty to generate diverse reasoning paths while maintaining the same computational budget as traditional multi-chain sampling approaches. Process supervision is incorporated through the tree structure, providing fine-grained rewards that capture both global and local advantages in the reasoning process. The framework trains the LLM to optimize these tree-derived rewards, enabling more effective exploration of reasoning strategies compared to standard Chain-of-Thought approaches.

## Key Results
- TreeRL achieves 44.5% average accuracy on MATH500 versus 41.6% for ChainRL on Qwen-2.5-14B
- Outperforms traditional ChainRL methods on math reasoning benchmarks including MATH500, Omni-MATH-500, and OlympiadBench
- Maintains comparable computational efficiency to multi-chain sampling while providing superior reasoning performance
- Demonstrates effectiveness of entropy-guided tree search for exploring diverse reasoning paths

## Why This Works (Mechanism)
TreeRL works by leveraging the structured exploration capabilities of tree search within a reinforcement learning framework. The entropy-guided branching strategy focuses computational resources on high-uncertainty intermediate steps, where the model's reasoning is most likely to benefit from diverse exploration. The tree structure naturally captures multiple reasoning trajectories, allowing the model to learn from both successful and failed paths. Process supervision derived from the tree provides more granular feedback than outcome-only rewards, enabling the model to refine its intermediate reasoning steps. This combination of strategic exploration and detailed supervision addresses the limitations of both standard reinforcement learning and simple Chain-of-Thought prompting.

## Foundational Learning

**Reinforcement Learning with Process Supervision**: Why needed - Traditional RLHF only rewards final outcomes, missing intermediate reasoning quality. Quick check - Verify that process supervision improves reasoning over outcome-only rewards on held-out tasks.

**Tree Search Algorithms**: Why needed - Enables systematic exploration of reasoning paths beyond single Chain-of-Thought generations. Quick check - Compare reasoning diversity metrics between tree-based and single-path approaches.

**Entropy-Guided Exploration**: Why needed - Focuses computational budget on high-uncertainty reasoning steps where exploration is most valuable. Quick check - Analyze correlation between branching entropy and final reasoning accuracy.

**Multi-Chain Sampling Efficiency**: Why needed - Maintains computational parity while providing superior reasoning performance. Quick check - Measure inference time and memory usage compared to baseline methods.

## Architecture Onboarding

**Component Map**: LLM Model -> EPTree Search -> Process Reward Computation -> RL Optimizer -> Updated LLM

**Critical Path**: The most critical path involves the integration between EPTree search and the reward computation module. The quality of tree search directly impacts the granularity and accuracy of process supervision, which in turn determines the effectiveness of the reinforcement learning updates.

**Design Tradeoffs**: TreeRL trades increased complexity in the inference pipeline for improved reasoning performance. The framework maintains computational efficiency by constraining tree growth to match multi-chain sampling budgets, but this requires careful entropy-guided branching decisions to maximize exploration value.

**Failure Signatures**: Poor performance manifests as either premature tree termination (missing valuable reasoning paths) or inefficient branching that exhausts the inference budget without improving accuracy. Additionally, if process supervision rewards are not properly calibrated, the model may learn to optimize for tree structure rather than genuine reasoning improvement.

**First Experiments**:
1. Ablation study comparing TreeRL with and without entropy-guided branching to isolate the contribution of strategic exploration
2. Evaluation on additional reasoning benchmarks beyond math and code to test domain generalizability
3. Analysis of tree depth versus reasoning accuracy to identify optimal exploration depth for different problem types

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content.

## Limitations

- Limited evaluation scope to math and code reasoning benchmarks, raising questions about generalizability to other domains
- Absence of comparison with other recent tree search methods like MCTS-EP and Policy Guided Tree Search
- Computational overhead analysis is incomplete, though claims of parity with multi-chain sampling are made
- Process supervision effectiveness depends on quality of tree-based reward signals which may not capture all reasoning aspects

## Confidence

- Claims about TreeRL's accuracy improvements: Medium
- Claims about computational efficiency parity: Low
- Claims about process supervision effectiveness: Medium
- Claims about generalizability to other reasoning tasks: Low

## Next Checks

1. Conduct ablation studies to isolate the contribution of entropy-guided tree search versus process supervision in achieving performance gains
2. Evaluate TreeRL on additional reasoning benchmarks beyond math and code to test generalizability
3. Perform detailed computational complexity analysis comparing inference times and memory usage against baseline methods under identical hardware configurations