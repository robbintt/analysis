---
ver: rpa2
title: 'Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning
  in SDOH Extraction with Large Language Models'
arxiv_id: '2506.00134'
source_url: https://arxiv.org/abs/2506.00134
tags:
- alcohol
- these
- clinical
- drug
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates spurious correlations in social determinants
  of health (SDOH) extraction using large language models (LLMs), focusing on drug
  status classification. Models exhibited high false positive rates (66.21% alcohol-positive,
  61.11% smoking-positive) when mere mentions of alcohol or smoking triggered incorrect
  drug use predictions.
---

# Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models

## Quick Facts
- arXiv ID: 2506.00134
- Source URL: https://arxiv.org/abs/2506.00134
- Authors: Fardin Ahsan Sakib; Ziwei Zhu; Karen Trister Grace; Meliha Yetisgen; Ozlem Uzuner
- Reference count: 18
- Key outcome: Large language models exhibited high false positive rates (66.21% alcohol-positive, 61.11% smoking-positive) when mere mentions of alcohol or smoking triggered incorrect drug use predictions, with chain-of-thought reasoning reducing FPR by up to 32 percentage points.

## Executive Summary
This study investigates spurious correlations in social determinants of health (SDOH) extraction using large language models, focusing on drug status classification from clinical notes. Models exhibited high false positive rates when alcohol or smoking mentions triggered incorrect drug use predictions, with male patients facing higher misclassification rates than females. Various mitigation strategies were evaluated, with chain-of-thought reasoning showing the most promise by reducing false positives by up to 32 percentage points. Despite improvements, persistent biases highlight the need for more robust solutions in clinical NLP applications.

## Method Summary
The study uses the MIMIC-III portion of the SHAC dataset (4405 deidentified social history notes) to classify drug use temporal status (current/past/none/unknown) through a two-step pipeline: trigger identification to find drug-related spans and argument resolution via multiple-choice QA. Four model configurations were tested: zero-shot, in-context learning with 3 examples, fine-tuned Llama-3.1-8B using LoRA, and various mitigation strategies including chain-of-thought reasoning, warning-based prompts, and increased examples. The primary metric was False Positive Rate (FPR), measuring incorrect current/past drug use predictions when ground truth was none/unknown, with additional analysis of gender-based disparities and trigger removal ablations.

## Key Results
- Models showed high false positive rates: 66.21% for alcohol-positive and 61.11% for smoking-positive contexts
- Chain-of-thought reasoning reduced false positives by up to 32 percentage points
- Male patients experienced 15-20 percentage points higher FPR than female patients
- Trigger removal experiments confirmed causal role of alcohol/smoking mentions in spurious predictions

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Reasoning Disrupts Shortcut Exploitation
- Claim: Explicit step-by-step reasoning prompts reduce false positive rates by forcing models to articulate evidence before concluding.
- Mechanism: CoT prompts require models to (1) read carefully, (2) identify relevant information, (3) consider examples, (4) explain reasoning, (5) answer—this sequential structure appears to disrupt the direct association between trigger words (alcohol/smoking mentions) and drug-use predictions.
- Core assumption: The mechanism works by increasing computational depth and requiring explicit evidence citation, not by adding new information.
- Evidence anchors:
  - [abstract] "chain-of-thought reasoning showing the most promise, reducing false positives by up to 32 percentage points"
  - [Results 4.1-4.3] Llama-70B CoT reduced alcohol-positive FPR from 66.21% to 33.79%; Qwen-72B showed particularly strong response
  - [corpus] Related work on shortcut learning (Geirhos et al., 2020) suggests shallow heuristics are exploited when models bypass reasoning
- Break condition: CoT effectiveness may diminish if (1) examples contain similar spurious patterns, (2) reasoning steps are not enforced during inference, or (3) the underlying model lacks sufficient capacity for multi-step reasoning.

### Mechanism 2: In-Context Learning Provides Counter-Examples to Spurious Associations
- Claim: Providing balanced demonstration examples reduces but does not eliminate spurious correlations.
- Mechanism: ICL with three carefully selected examples (balanced across substance use patterns and drug outcomes) creates a local decision boundary that contradicts pre-training-based associations, helping models distinguish correlation from causation within the task context.
- Core assumption: Assumption: The example selection strategy—not just quantity—drives improvement by explicitly showing cases where alcohol/smoking mentions do NOT imply drug use.
- Evidence anchors:
  - [Results 4.2] "Providing three in-context examples reduces false positives significantly. For Llama-70B, ICL lowers alcohol-positive mismatches from 66.21% to 48.28%"
  - [Methodology 3.2] "Examples are selected to maintain balanced representation across substance use patterns (none/single/multiple) and drug use outcomes (positive/negative)"
  - [corpus] Limited direct corpus evidence on ICL mechanisms for debiasing; primarily inferred from this paper's experimental design
- Break condition: ICL mitigation degrades when (1) example distribution does not cover the spurious pattern being tested, (2) model attention mechanism does not sufficiently weight counter-examples, or (3) pre-training priors overwhelm in-context signals.

### Mechanism 3: Trigger Removal Confirms Causal Role of Superficial Cues
- Claim: Removing alcohol/smoking mentions from input text reduces false positives, establishing these cues as causally—not just correlationally—driving erroneous predictions.
- Mechanism: Controlled ablation (removing trigger words while preserving document structure) isolates the spurious signal; reduced FPR after removal indicates the model was using these mentions as predictive shortcuts rather than contextual reasoning.
- Core assumption: Assumption: Removal does not introduce new confounds (e.g., making text appear unnatural or triggering different heuristics).
- Evidence anchors:
  - [Results 4.3] "Llama-70B zero-shot sees alcohol-positive mismatches fall from 66.21% to 55.17% after removing alcohol triggers. Similarly, Llama-8B-SFT reduces alcohol-positive errors from 32.41% to 26.9%"
  - [Appendix E Table 3-4] Consistent FPR reductions across all model architectures after trigger removal
  - [corpus] Related work on shortcut testing (Brown et al., 2023) uses similar ablation approaches to identify spurious features
- Break condition: This mechanism validates diagnosis but is not a mitigation strategy; for deployment, triggers cannot be removed from real clinical text.

## Foundational Learning

- Concept: **Shortcut Learning / Spurious Correlations**
  - Why needed here: The entire paper diagnoses models exploiting superficial patterns (alcohol→drug, smoking→drug) rather than learning task-relevant features; understanding this is prerequisite to interpreting all results.
  - Quick check question: If a model predicts "drug use" whenever it sees "alcohol" mentioned, regardless of whether drug use is explicitly stated, what type of learning failure is this?

- Concept: **False Positive Rate (FPR) vs. Accuracy**
  - Why needed here: The paper prioritizes FPR as the evaluation metric due to clinical risks (stigmatization, biased care); accuracy would mask the asymmetric harm of false positives.
  - Quick check question: Why would a model with 90% accuracy still be dangerous for clinical deployment if its false positive rate for drug use is 60%?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The paper tests zero-shot vs. ICL configurations; understanding how demonstration examples influence model predictions without weight updates is essential for interpreting mitigation results.
  - Quick check question: How does ICL differ from fine-tuning in terms of what model parameters change?

## Architecture Onboarding

- Component map:
  SHAC data -> Trigger Identification (find drug-related spans) -> Argument Resolution (multiple-choice QA for temporal status) -> Model predictions (Llama-70B, Llama-8B-SFT, Qwen-72B, Llama3-Med42-70B) -> Evaluation (FPR stratified by substance status and gender)

- Critical path:
  1. Load SHAC data -> convert BRAT annotations to QA format (see Appendix B)
  2. Apply prompting strategy -> collect model predictions
  3. Stratify predictions by ground-truth substance status -> compute FPR per stratum
  4. For ablation: remove trigger words -> re-run inference -> compare FPR delta

- Design tradeoffs:
  - **CoT vs. Warning prompts**: CoT achieves stronger FPR reduction (~32pp) but increases inference cost and latency; Warning prompts are cheaper but less effective (~20-25pp reduction).
  - **Fine-tuning vs. Prompting**: SFT Llama-8B achieves comparable FPR to Llama-70B ICL but requires training infrastructure; prompting is more flexible for rapid iteration.
  - **FPR focus vs. balanced metrics**: Optimizing for FPR may increase false negatives; the paper explicitly prioritizes avoiding patient harm over maximizing overall accuracy.

- Failure signatures:
  - **High FPR in substance-positive, low FPR in substance-negative**: Indicates spurious correlation with alcohol/smoking triggers.
  - **Gender gap in FPR (male > female by 15-20pp)**: Indicates demographic shortcut learning.
  - **FPR unchanged after CoT**: May indicate reasoning steps are not being followed; verify prompt format and model compliance.
  - **FPR increases after adding examples**: Check whether examples inadvertently reinforce spurious patterns.

- First 3 experiments:
  1. **Baseline diagnostic**: Run zero-shot Llama-70B on full SHAC test set; compute FPR for alcohol-positive, alcohol-negative, smoking-positive, smoking-negative strata to quantify initial spurious correlation magnitude.
  2. **Trigger ablation**: Remove alcohol mentions from alcohol-positive notes (regex-based or span-aware removal); re-run inference and compute FPR delta to confirm causal role of triggers.
  3. **CoT mitigation test**: Apply 5-step CoT prompt to the same zero-shot configuration; compare FPR reduction against baseline and identify residual bias (especially gender-stratified gaps).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated debiasing techniques (e.g., adversarial training, counterfactual data augmentation) achieve near-complete elimination of substance-trigger spurious correlations, beyond the partial improvements shown with prompt-based mitigation?
- Basis in paper: [explicit] Authors state "our mitigation strategies could not completely address the problem, leaving room for future work" and mention "need for more robust approaches—integrating domain-specific knowledge, implementing adversarial training, or curating more balanced datasets."
- Why unresolved: The best-performing mitigation (CoT) still left substantial false positive gaps (e.g., 33.79% vs 6.76% for alcohol-positive vs alcohol-negative cases).
- What evidence would resolve it: Systematic evaluation of adversarial training or counterfactual augmentation showing false positive rate convergence across substance-positive and substance-negative contexts.

### Open Question 2
- Question: Do the observed gender-based spurious correlations generalize across diverse clinical populations and healthcare systems beyond the MIMIC-III dataset?
- Basis in paper: [explicit] Under Limitations: "Our analysis relied exclusively on the MIMIC portion of the SHAC dataset, which constrains the generalizability of our findings. While we observe consistent gender-based performance disparities, a more diverse dataset could help establish the breadth of these biases."
- Why unresolved: Single-institution dataset may not represent documentation patterns or demographic distributions elsewhere.
- What evidence would resolve it: Replication study across multiple institutions with varying patient demographics and documentation practices.

### Open Question 3
- Question: What mechanistic interpretability insights explain why specific triggers (alcohol vs. smoking) differentially affect model predictions across architectures?
- Basis in paper: [explicit] Under Limitations: "understanding why specific triggers affect certain models or scenarios would require deeper analysis using model interpretability techniques."
- Why unresolved: Causal role of triggers was confirmed via removal experiments, but internal representations driving the bias remain unexamined.
- What evidence would resolve it: Attention head analysis or probing classifiers identifying which model components encode substance-use correlations.

### Open Question 4
- Question: How do closed-source commercial LLMs (e.g., GPT-4, Claude) compare to open-source models in exhibiting and mitigating SDOH spurious correlations?
- Basis in paper: [explicit] Under Limitations: "We focused solely on open-source large language models (e.g., Llama, Qwen). Extending the evaluation to additional data sources, closed-source models... would help verify the robustness of our conclusions."
- Why unresolved: Closed-source models may have different training data, safety alignments, or architectural choices affecting shortcut learning.
- What evidence would resolve it: Comparative evaluation using identical prompts and datasets on commercial model APIs.

## Limitations

- The study relies exclusively on the MIMIC-III portion of the SHAC dataset, constraining generalizability to other clinical populations and documentation practices.
- Mitigation strategies could not completely address spurious correlations, leaving substantial false positive gaps even with chain-of-thought reasoning.
- The analysis focused solely on open-source models, excluding potential differences in closed-source commercial LLMs.

## Confidence

- **High confidence**: The existence of spurious correlations (high FPR when alcohol/smoking mentioned without actual drug use) is well-supported by multiple ablation experiments and consistent across model architectures.
- **Medium confidence**: The relative effectiveness of mitigation strategies (CoT > ICL > warning-based) is supported by results, but the absolute FPR reductions depend on unspecified prompt details.
- **Low confidence**: The gender disparity findings (15-20pp higher FPR for male patients) are reported but not explained mechanistically; the paper does not investigate why this bias exists or how to address it.

## Next Checks

1. **Prompt template verification**: Request and test the exact prompt formulations for all mitigation strategies to confirm whether the reported 32pp FPR reduction from chain-of-thought reasoning is reproducible with the specified prompt structure.

2. **Cross-dataset validation**: Apply the zero-shot baseline and best mitigation strategy to an independent SDOH extraction dataset to verify whether spurious correlations and their mitigations generalize beyond the SHAC dataset.

3. **False negative analysis**: The paper prioritizes FPR reduction but does not report false negative rate changes. Validate whether mitigation strategies introduce unacceptable increases in false negatives, particularly for female patients where FPR disparities exist.