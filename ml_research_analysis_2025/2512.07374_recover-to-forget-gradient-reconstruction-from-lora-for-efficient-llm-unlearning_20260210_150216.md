---
ver: rpa2
title: 'Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning'
arxiv_id: '2512.07374'
source_url: https://arxiv.org/abs/2512.07374
tags:
- unlearning
- gradient
- lora
- target
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses efficient machine unlearning in large language
  models (LLMs) by introducing a method called Recover-to-Forget (R2F). The core idea
  is to reconstruct full-model gradients from low-rank LoRA adapter updates using
  a gradient decoder trained on a proxy model, enabling targeted unlearning without
  full-model backpropagation.
---

# Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning

## Quick Facts
- **arXiv ID:** 2512.07374
- **Source URL:** https://arxiv.org/abs/2512.07374
- **Reference count:** 40
- **Primary result:** Achieves 89.3% unlearning success rate on RWKU while maintaining 95.7% general utility retention

## Executive Summary
This paper introduces Recover-to-Forget (R2F), an efficient approach for machine unlearning in large language models that eliminates the need for full-model backpropagation. R2F reconstructs complete model gradients from low-rank LoRA adapter updates using a trained gradient decoder, enabling targeted knowledge removal without original training data. The method aggregates gradients from multiple paraphrased inputs to capture diverse semantic representations of the target knowledge. Experiments demonstrate state-of-the-art unlearning performance across four benchmark datasets while maintaining model utility and resisting relearning attacks.

## Method Summary
R2F addresses LLM unlearning by reconstructing full-model gradients from LoRA adapter updates. The method generates multiple paraphrased versions of target knowledge using a neural paraphraser, computes LoRA gradients for each paraphrase, and averages them to create a robust multi-view gradient representation. A gradient decoder is trained on a proxy model (Mistral-3B) using 1k held-out examples to map LoRA gradients to full-model gradients via MSE loss. The reconstructed full gradient is then applied in a single update step to achieve unlearning. This approach eliminates the need for full backpropagation through the complete model while maintaining effectiveness through cross-model gradient transfer.

## Key Results
- Achieves 89.3% unlearning success rate on RWKU benchmark, outperforming baseline methods
- Maintains 95.7% general utility retention, demonstrating minimal impact on model capabilities
- Shows strong resistance to relearning attacks with low RAP scores across all tested datasets
- Effective cross-model gradient transfer with only 5-6 point USR decrease when using GPT2-Medium as proxy

## Why This Works (Mechanism)
R2F leverages the observation that LoRA adapters capture localized knowledge modifications while full gradients contain comprehensive parameter updates. By training a gradient decoder on a proxy model, R2F learns to map the compact LoRA representation to the full gradient space. The multi-view paraphrasing approach ensures robust gradient estimation by capturing diverse semantic perspectives of the target knowledge, reducing the risk of incomplete unlearning due to paraphrasing artifacts.

## Foundational Learning
- **LoRA adapters**: Low-rank adaptation modules that efficiently modify model behavior with minimal parameter changes; needed for parameter-efficient updates and gradient reconstruction
- **Cross-model gradient transfer**: Ability to apply knowledge learned from one model architecture to another; needed to avoid expensive gradient collection from the target model
- **Multi-view gradient aggregation**: Combining gradients from semantically diverse inputs to improve robustness; needed to prevent incomplete unlearning from paraphrasing artifacts
- **Gradient reconstruction**: Mapping between different gradient representations (LoRA→full); needed to enable efficient unlearning without full backpropagation
- **Semantic similarity filtering**: Ensuring paraphrased inputs maintain core meaning while varying surface form; needed for effective multi-view gradient computation

## Architecture Onboarding

**Component map:** Proxy Model -> Gradient Decoder -> LoRA Gradients -> Full Gradients -> Unlearning Update

**Critical path:** Paraphrasing Pipeline → LoRA Gradient Computation → Gradient Decoder → Unlearning Step

**Design tradeoffs:** LoRA rank r=8 balances reconstruction fidelity and computational efficiency; higher ranks improve reconstruction but increase decoder complexity

**Failure signatures:** Poor proxy-target alignment causes USR degradation (5-6 point drop observed); over-forgetting manifests as GUR below 92%

**Three first experiments:**
1. Verify gradient decoder training on proxy model with MSE loss converges within 10 epochs
2. Test multi-view paraphrasing pipeline with 5 paraphrases per target using T5/BART
3. Validate single-step unlearning update maintains model stability (no catastrophic forgetting)

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Gradient decoder architecture remains underspecified, potentially limiting reconstruction fidelity
- Cross-model transfer effectiveness drops significantly (5-6 points) with architectural misalignment
- Limited systematic analysis of LoRA rank vs unlearning efficacy trade-offs

## Confidence

**High confidence:** Core unlearning mechanism (gradient reconstruction + multi-view aggregation) is sound and well-validated on benchmark datasets

**Medium confidence:** Cross-model gradient transfer capability is demonstrated but limited to similar model families

**Low confidence:** Trade-off between LoRA rank and unlearning efficacy lacks systematic analysis

## Next Checks

1. **Architecture sensitivity test:** Systematically vary gradient decoder depth and width while measuring USR/GUR trade-offs

2. **Cross-architecture robustness:** Evaluate R2F using proxy models from different families (LLaMA → Mistral, BERT → GPT)

3. **Multi-step unlearning stability:** Test repeated application targeting multiple knowledge points for catastrophic forgetting risks