---
ver: rpa2
title: 'Evaluating User Experience in Conversational Recommender Systems: A Systematic
  Review Across Classical and LLM-Powered Approaches'
arxiv_id: '2508.02096'
source_url: https://arxiv.org/abs/2508.02096
tags:
- user
- systems
- evaluation
- studies
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review systematically examined how user experience (UX) is
  evaluated in conversational recommender systems (CRSs), synthesising 23 empirical
  studies published from 2017 to 2025. The review found that UX evaluations are dominated
  by post-interaction surveys, with satisfaction being the most measured construct.
---

# Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches

## Quick Facts
- arXiv ID: 2508.02096
- Source URL: https://arxiv.org/abs/2508.02096
- Reference count: 40
- Primary result: Systematic review of 23 empirical studies found UX evaluations dominated by post-interaction surveys, with significant gaps in adaptive/LLM-CRS assessment and real-time instrumentation.

## Executive Summary
This systematic review synthesizes 23 empirical studies on user experience (UX) evaluation in conversational recommender systems (CRSs) from 2017-2025. The review reveals that UX assessments are dominated by post-interaction surveys focusing primarily on satisfaction, with limited use of behavioral logging or longitudinal designs. While adaptive dialogue policies and LLM-powered systems are increasingly adopted, they are underrepresented in UX evaluations. The research highlights critical gaps including the lack of diagnostics for explainability, hallucination detection, and verbosity control in LLM-based systems, and calls for theory-grounded, in-situ instrumentation to capture adaptive and multi-turn dynamics.

## Method Summary
The study follows PRISMA guidelines, conducting a systematic literature review across six academic databases (ACM DL, IEEE Xplore, ScienceDirect, SpringerLink, Scopus, Web of Science) with 460 initial records (356 after deduplication). Using a specific search string combining conversational/interface terms with "User Experience"/UX and recommend*, the review identified 23 empirical studies. Three independent reviewers conducted two-phase screening with inter-rater reliability (Cohen's Îº 0.625-0.88). Data extraction used a structured coding sheet covering UX constructs, evaluation methods, adaptivity, and personalization. Due to heterogeneity, findings were synthesized narratively rather than through meta-analysis.

## Key Results
- UX evaluations heavily rely on post-interaction surveys, with satisfaction being the most measured construct
- Only 8 of 23 studies employed behavioral logging; none used longitudinal designs
- Adaptive systems showed broader UX metric coverage but were underrepresented overall
- LLM-powered CRSs rarely included diagnostics for explainability, verbosity control, or hallucination detection
- Most nonadaptive systems used static personalization methods without real-time interaction adaptation

## Why This Works (Mechanism)

### Mechanism 1
Adaptive dialogue policies likely improve perceived responsiveness when evaluation captures interactional dynamics rather than just post-hoc satisfaction. Systems that adjust initiative, tone, or content based on real-time user signals reduce interaction friction, though benefits are theoretically invisible to static post-task surveys. Core assumption: Users value dynamic responsiveness over consistent, predictable scripting. Evidence: Limited adaptive systems showed broader UX metric coverage (perceived control, initiative alignment). Break condition: If adaptive logic introduces latency or unpredictable shifts that erode user control.

### Mechanism 2
LLM-powered CRSs introduce specific trust risks (hallucination, opacity) that standard UX instruments fail to detect. Generative models produce fluent but potentially fabricated content ("epistemic opacity"). Without explicit diagnostics for verification or explanation, users may report high initial satisfaction that degrades unpredictably upon error discovery. Core assumption: Users cannot distinguish between confident hallucinations and accurate recommendations in the moment. Evidence: LLM studies rarely included UX diagnostics for explainability or hallucination detection. Break condition: If task is purely hedonic where factual accuracy is secondary to engagement.

### Mechanism 3
Personalization depth moderates the relationship between system utility and user engagement. "Deep" personalization (real-time behavioral adaptation) theoretically outperforms "shallow" personalization (static trait-matching) by evolving with user goals, yet current systems largely rely on the latter. Core assumption: Users notice and prefer systems that update their models in real-time versus applying static rules. Evidence: Most nonadaptive systems used static personalization methods without modifying interaction logic. Break condition: If real-time adaptation breaches perceived privacy or feels intrusive.

## Foundational Learning

- **Epistemic Opacity**: Why needed: LLM-based recommenders generate plausible but ungrounded outputs. Critical for designing evaluation protocols that check for hallucination, not just fluency. Quick check: Does your evaluation instrument distinguish between "correct" and "plausible but fabricated" recommendations?

- **In-situ vs. Post-hoc Instrumentation**: Why needed: Review identifies "methodological inertia" where surveys miss temporal dynamics. Essential for capturing trust fluctuations or frustration during sessions. Quick check: Can your metrics identify the specific turn where a user disengaged, or only final satisfaction scores?

- **Initiative Modulation**: Why needed: Key finding shows tradeoff between proactive system support and user control. Necessary to understand when system should lead versus listen to optimize cognitive load. Quick check: Does the system dynamically switch to user-led mode when user signals expertise or frustration?

## Architecture Onboarding

- **Component map**: Input Layer (NLU + User Modeling) -> Logic Layer (Dialogue Manager + Recommendation Engine) -> Output Layer (NLG: Template vs. LLM) -> Evaluation Layer (Behavioral Logs, Turn-level Probes, Post-hoc Surveys)

- **Critical path**: 1. Define Construct (select UX dimension based on domain) -> 2. Select Instrument (map construct to tool) -> 3. Instrument Interaction (embed logging/turn-level triggers) -> 4. Analyze (correlate behavioral logs with self-reports)

- **Design tradeoffs**: Adaptivity vs. Predictability (high adaptivity offers better flow but risks "opacity"); Proactivity vs. Cognitive Load (proactive suggestions help novices but increase interaction cost for experts)

- **Failure signatures**: "Happy Path" Bias (testing only successful flows); Static Personalization Trap (applying user traits rigidly without adaptation); Metric-Mechanism Mismatch (using satisfaction scales to diagnose LLM hallucination issues)

- **First 3 experiments**:
  1. Turn-Level Frustration Detection: Implement log analyzer to correlate specific system behaviors (repetition, "I don't know" responses) with drop-off rates
  2. Verbosity A/B Test: Compare user retention and comprehension between "Concise/Template" and "Verbose/LLM" response styles in goal-oriented tasks
  3. Explanation Transparency Probe: Insert forced-choice question after recommendation asking why user accepted it (Accuracy vs. Trust vs. Ease)

## Open Questions the Paper Calls Out

1. **Validated, domain-sensitive UX instruments**: How can instruments be developed to capture adaptive and multi-turn dynamics specific to CRS? [explicit] Authors call for "validated, CRS-specific UX instruments capable of capturing adaptive, affective, and multi-turn dynamics." Unresolved because current research relies on generic surveys lacking standardization. Evidence needed: Creation and validation of standardized CRS-specific scale demonstrating reliability across domains.

2. **LLM behaviors and trust calibration**: How do hallucinations and verbosity influence user trust calibration and long-term reliance? [explicit] Gap identified in "hallucination tolerance" and "trust calibration" noting evaluations don't account for how users detect or respond to false recommendations. Unresolved because LLM studies lacked diagnostic tooling linking errors to trust erosion. Evidence needed: Longitudinal study manipulating LLM error rates and measuring trust shifts over time.

3. **In-situ vs. traditional evaluation efficacy**: What is the efficacy of turn-level methods compared to post-interaction surveys in predicting long-term retention? [explicit] Conclusion advocates for "in-situ and longitudinal designs" while criticizing "methodological inertia" of one-off questionnaires. Unresolved because few studies employed real-time instrumentation. Evidence needed: Comparative study correlating real-time interaction logs with longitudinal retention metrics.

## Limitations

- Heavy reliance on post-interaction surveys may not capture real-time UX dynamics or detect issues like hallucination that manifest during interaction
- Limited behavioral logging (8 of 23 studies) and absence of longitudinal designs constrain understanding of sustained engagement
- Database coverage and indexing changes may affect reproducibility of search results
- Subjective interpretation thresholds for "empirical UX findings" and adaptive vs nonadaptive classification may yield different inclusion outcomes

## Confidence

- **High**: Current evaluation methods underrepresent adaptive and LLM-powered CRSs (directly evidenced by coding results)
- **Medium**: Claims about epistemic opacity risks (logically inferred from absence of hallucination diagnostics rather than directly measured)
- **Low**: Claims about personalization depth effects (sparse empirical evidence in corpus)

## Next Checks

1. Implement logging to correlate specific system behaviors (repetition, "I don't know" responses) with user drop-off rates
2. Conduct LLM vs. template response A/B test comparing user comprehension and retention in goal-oriented tasks
3. Insert post-recommendation forced-choice questions asking why users accepted recommendations to validate alignment between system rationale and user perception