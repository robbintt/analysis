---
ver: rpa2
title: 'Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding'
arxiv_id: '2508.09032'
source_url: https://arxiv.org/abs/2508.09032
tags:
- traces
- spatial
- depth
- visual
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Spatial Traces, a method to enhance Vision-Language-Action
  (VLA) models by integrating spatial and temporal understanding through visual prompting.
  The core idea is to overlay visual traces of key points from observations onto depth
  maps, enabling the model to capture both spatial and temporal information simultaneously.
---

# Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding

## Quick Facts
- **arXiv ID**: 2508.09032
- **Source URL**: https://arxiv.org/abs/2508.09032
- **Reference count**: 38
- **Primary result**: 4% increase in task success rate over SpatialVLA and 19% over TraceVLA in SimplerEnv with minimal training data.

## Executive Summary
This paper introduces Spatial Traces, a method to enhance Vision-Language-Action (VLA) models by integrating spatial and temporal understanding through visual prompting. The core idea is to overlay visual traces of key points from observations onto depth maps, enabling the model to capture both spatial and temporal information simultaneously. Experiments on the SimplerEnv show that the proposed ST-VLA model achieves a 4% increase in task success rate compared to SpatialVLA and a 19% improvement over TraceVLA. Additionally, the method demonstrates effectiveness with minimal training data, making it suitable for real-world applications where data collection is challenging.

## Method Summary
The method enhances VLA models by overlaying visual traces of key points onto depth maps, creating a unified representation that combines spatial and temporal information. Traces are generated using CoTracker from a buffer of past observations and projected onto ZoeDepth-predicted depth maps. The combined visual input (original RGB + traced depth) is encoded and summed with text embeddings, then fed to a PaliGemma2 VLA for action prediction. The approach uses a 30-frame buffer and fine-tunes with LoRA on minimal data.

## Key Results
- ST-VLA achieves 4% higher task success rate compared to SpatialVLA baseline
- 19% improvement over TraceVLA in SimplerEnv tasks
- Effective with minimal training data (52 trajectories), demonstrating potential for real-world applications
- Variant C (traces at object depth) outperforms other trace rendering variants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating temporal traces directly into spatial depth maps creates a unified representation that allows the VLA to correlate motion history with geometric context more effectively than separate RGB and trace inputs.
- **Mechanism:** The system projects 2D keypoint trajectories onto a predicted depth map. By summing the embeddings of this "traced depth map" with the original visual embeddings, the model receives a single tensor encoding where objects are (depth) and how the agent has moved relative to them (traces).
- **Core assumption:** The VLA can disentangle and utilize the sum of observation embeddings and depth-trace embeddings without the signals degrading each other.
- **Evidence anchors:** [abstract] "projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously." [section III.B] Describes summing observation embeddings ($E^O_t$) and depth embeddings ($E^D_t$) to form the visual input ($E_V$). [corpus] *Any3D-VLA* and *Spatial-Aware VLA Pretraining* suggest that 2D-only inputs limit spatial understanding, supporting the need for depth integration.

### Mechanism 2
- **Claim:** Rendering traces at the depth of the nearest object (Variant C) rather than the manipulator's own depth focuses the model's attention on the interaction surface, improving manipulation success.
- **Mechanism:** In "Variant C", trace pixels are assigned the depth value of the closest object in the frame. This makes the traces visually distinct from the background and links the motion path directly to the object being manipulated, rather than floating it at the gripper's varying height.
- **Core assumption:** Attaching the trace to the object surface creates a stronger visual association for the policy than showing the trajectory in "empty" 3D space.
- **Evidence anchors:** [section VI (Q3)] "Variant C... traces have equal values, are more distinguishable from the background, and can more effectively capture the model's attention." [table III] Shows Variant C (buffer 30) achieving 36.4% SR vs Variant A's 9.1% on "Put spoon". [corpus] *RynnVLA-002* suggests coupling action/world models helps refine physics, implying grounding traces to physical surfaces (depth) aids this alignment.

### Mechanism 3
- **Claim:** Explicitly visualizing interaction history reduces the learning complexity for VLA models, enabling performance gains with minimal fine-tuning data (as few as 52 trajectories).
- **Mechanism:** Rather than relying on the VLA's internal weights to memorize implicit temporal dynamics from scratch, the Spatial Traces method externalizes this history into the visual input. The model fine-tunes primarily to "read" the trace, requiring fewer weight updates to achieve generalization.
- **Core assumption:** The pre-trained VLA possesses sufficient vision capabilities to interpret lines on an image as directional or temporal signals after light adaptation.
- **Evidence anchors:** [abstract] "enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications." [table IV] Shows that "Traces 0-shot" (no fine-tuning) performs comparably or better than a fully finetuned base model without traces, suggesting the trace itself carries high information value. [corpus] *MoLe-VLA* discusses efficiency in VLAs; this method improves efficiency via data sample reduction rather than architectural pruning.

## Foundational Learning

- **Concept: Visual Prompting for Robotics**
  - **Why needed here:** The core innovation is not a new architecture, but a specific way of "drawing" on the input image (visual prompt) to tell the model where things are moving.
  - **Quick check question:** Can you explain the difference between "text prompting" (instructions) and "visual prompting" (traces) in the context of this paper?

- **Concept: Depth Map Alignment & Projection**
  - **Why needed here:** The method relies on ZoeDepth to predict depth and CoTracker to predict 2D points. You must understand how 2D points are mapped onto a 3D-like depth representation.
  - **Quick check question:** How does the system handle a trace point that moves across the image? Does it stay at a fixed depth, or change based on the depth map it is overlaid upon?

- **Concept: Action Chunking vs. Single Step Prediction**
  - **Why needed here:** The paper critiques methods that predict actions independently. It uses a buffer of observations ($b=30$) to inject history.
  - **Quick check question:** Does ST-VLA predict the next single action or a sequence of future actions? How does the "buffer" relate to this prediction?

## Architecture Onboarding

- **Component map:** Sequence of 30 RGB frames ($O_{t-b} \dots O_t$) + Text Instruction ($I$) -> CoTracker ($M_T$) generates 2D keypoint trajectories -> ZoeDepth ($M_D$) predicts depth map ($D_t$) -> SigLIP ($P_I$) encodes RGB -> Traces overlaid onto Depth Map ($D^T_t$) -> Ego3D Encoder ($P_D$) encodes this map -> RGB Embeddings + Depth-Trace Embeddings are summed -> PaliGemma2 VLA receives summed visual + text embeddings -> Predicts action $a_t$

- **Critical path:** The **Trace-Depth Application** step. If the traces are not correctly rendered onto the depth map (specifically using Variant C logic), the signal degrades. The quality of the CoTracker points is also a critical dependency.

- **Design tradeoffs:**
  - **Buffer Size:** Larger buffer ($b=30$) improves context but increases compute for CoTracker and memory usage. Smaller buffer ($b=7$) is faster but loses long-term context (Table II shows instability with smaller buffers).
  - **Trace Rendering:** Variant A (own depth) vs Variant C (object depth). Variant C is better for manipulation but requires valid depth data of the object, which might fail on reflective/transparent surfaces.

- **Failure signatures:**
  - **Slow/Static Movement:** If the robot moves too slowly, consecutive frames are too similar for CoTracker to identify distinctive points, resulting in missing traces.
  - **Occlusion:** Traces drawn on top of small objects might obscure the visual features needed to identify them.

- **First 3 experiments:**
  1. **Zero-Shot Trace Test:** Run the SpatialVLA base model with Spatial Traces enabled but **without** fine-tuning (as in Table IV). Verify if the model can at least "see" the traces and if behavior changes (even if suboptimal).
  2. **Buffer Ablation:** Replicate the buffer size experiment (7 vs 15 vs 30 images) on a single task (e.g., "Put spoon") to confirm the finding that longer history stabilizes success rate.
  3. **Trace Rendering Comparison:** Implement both Variant A and Variant C. Visualize the depth maps side-by-side to ensure Variant C actually "snaps" traces to object surfaces, then compare success rates to confirm the paper's 27% gap.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the Spatial Traces method maintain its performance advantage when transferred from the SimplerEnv simulation to physical robotic hardware in real-world scenarios?
  - **Basis in paper:** [explicit] The Conclusion explicitly states the intent to apply the method to "real-world robotics tasks," noting that current experiments are confined to the SimplerEnv virtual environment.
  - **Why unresolved:** While the paper demonstrates that the method works with minimal training data in simulation, it acknowledges that real-world application involves challenges (e.g., depth estimation noise, physical latency) not present in the SimplerEnv evaluation.
  - **What evidence would resolve it:** A comparative study of success rates between ST-VLA and baselines (SpatialVLA, TraceVLA) deployed on physical robots executing the same manipulation tasks (e.g., "Put Spoon", "Stack Blocks").

- **Open Question 2:** Can integrating 3D reconstructed observations resolve the failure modes caused by trace occlusion and lack of depth cues during camera-directed motion?
  - **Basis in paper:** [explicit] The Conclusion proposes to "explore integration with 3D reconstructed observations" to address limitations where traces "occlude target objects" or fail to provide interaction cues when the agent moves toward the camera.
  - **Why unresolved:** The current method projects traces onto 2D depth maps, which creates visual ambiguities when traces overlap objects or when movement lacks lateral translation.
  - **What evidence would resolve it:** Experiments showing that a variant of ST-VLA utilizing 3D scene reconstruction reduces failure rates in scenarios involving object occlusion or direct approach vectors compared to the depth-map-only baseline.

- **Open Question 3:** Does adaptive trace generation effectively mitigate the loss of temporal information during slow manipulator movements?
  - **Basis in paper:** [explicit] The Conclusion identifies "adaptive trace generation methods" as future work, prompted by results in Section VI showing failure in "slow rotation" tasks where consecutive images are too similar for Co-Tracker to construct traces.
  - **Why unresolved:** The current implementation relies on standard tracking which fails to generate distinct traces during slow or static movements, leaving the model without the necessary temporal prompts to succeed.
  - **What evidence would resolve it:** Success rate improvements in "slow rotation" scenarios when using a dynamic frame-sampling mechanism that increases trace sensitivity when optical flow is low.

## Limitations
- Performance heavily dependent on external perception models (ZoeDepth and CoTracker), which may fail on transparent/reflective objects or with rapid motion blur
- Assumes VLA can effectively disentangle summed embeddings, which may not generalize to more complex environments
- Study limited to specific tasks (SimplerEnv) with 52 trajectories, may not represent real-world variability

## Confidence
- **High confidence:** The claim that integrating spatial and temporal information via visual prompting improves task success rates is well-supported by the experimental results (4% improvement over SpatialVLA and 19% over TraceVLA).
- **Medium confidence:** The assertion that the method works with minimal training data (as few as 52 trajectories) is plausible but may not generalize to more complex or diverse tasks.
- **Low confidence:** The scalability of the method to tasks beyond SimplerEnv or to environments with significant occlusions, lighting changes, or cluttered scenes is not explored in the paper.

## Next Checks
1. **Cross-Dataset Generalization:** Test the ST-VLA model on a different robotic manipulation dataset (e.g., Meta-World or RoboNet) to assess whether the performance gains from Spatial Traces generalize beyond SimplerEnv.

2. **Perception Robustness:** Evaluate the method in scenarios with challenging visual conditions, such as transparent objects, reflective surfaces, or low lighting, to quantify the impact of ZoeDepth and CoTracker failures on task success rates.

3. **Trace-Free Ablation:** Implement a variant of the model where traces are disabled but depth maps are retained, to isolate the contribution of temporal traces versus spatial depth information in the performance improvements.