---
ver: rpa2
title: How to Mitigate Overfitting in Weak-to-strong Generalization?
arxiv_id: '2503.04249'
source_url: https://arxiv.org/abs/2503.04249
tags:
- stage
- uni00000048
- strong
- uni00000013
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# How to Mitigate Overfitting in Weak-to-strong Generalization?

## Quick Facts
- arXiv ID: 2503.04249
- Source URL: https://arxiv.org/abs/2503.04249
- Authors: Junhao Shi; Qinyuan Cheng; Zhaoye Fei; Yining Zheng; Qipeng Guo; Xipeng Qiu
- Reference count: 30
- None

## Executive Summary
The paper addresses the challenge of overfitting in weak-to-strong generalization, where strong models trained on noisy labels from weak supervisors fail to achieve their full potential. The authors propose a two-stage filtering framework that uses uncertainty-based consistency filtering to identify high-quality labels while preserving question diversity. Through systematic experimentation on mathematical reasoning tasks (GSM8K, MATH), they demonstrate significant improvements in Performance Gap Recovered (PGR) compared to direct finetuning approaches.

## Method Summary
The two-stage framework operates as follows: Stage I generates 10 Chain-of-Thought responses per question using the weak model, computes confidence as the frequency of the most common answer, and filters by threshold to create a high-quality training set. The strong model is finetuned on this filtered set. Stage II uses the Stage I finetuned model to relabel the discarded questions, applies uncertainty filtering, and combines these with Stage I data for final finetuning. The method specifically addresses the tradeoff between supervision quality and question diversity through this staged approach.

## Key Results
- Stage I filtering with 70% threshold improves PGR from 7.19% to 98.56% on Llama 3 GSM8K
- Stage II relabeling recovers question difficulty and diversity distributions toward original
- Uncertainty filtering in Stage II is critical (performance drops from 80.28% to 79.59% without it)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty-based filtering reduces label noise, mitigating strong model overfitting to weak supervision errors.
- Mechanism: High self-consistency responses correlate with higher label correctness; filtering low-consistency samples reduces training noise that would otherwise be memorized by the strong model.
- Core assumption: Weak model consistency is a reliable proxy for label correctness under the experimental conditions studied.
- Evidence anchors:
  - [abstract] "erroneous labels from weak supervisors may lead to overfitting in strong models"
  - [section 4.3] Stage I filtering improves PGR from 7.19% to 98.56% (Llama 3, GSM8K)
  - [corpus] Limited direct support; related work focuses on generalization, not filtering mechanisms specifically
- Break condition: When consistency no longer correlates with correctness (e.g., systematically biased weak model)

### Mechanism 2
- Claim: Excessive filtering trades supervision quality for question quality loss, limiting generalization to hard samples.
- Mechanism: Aggressive filtering preferentially discards difficult/diverse questions where weak models are uncertain, creating an easier but less representative training distribution.
- Core assumption: Test distribution requires coverage of hard questions that weak models struggle with.
- Evidence anchors:
  - [abstract] "simply filtering out incorrect labels may lead to a degeneration in question quality"
  - [section 5.1, Figure 5] Average difficulty drops from 3.48 to 2.66 as threshold increases
  - [corpus] Related work does not address this tradeoff explicitly
- Break condition: When hard questions are not needed for target task performance

### Mechanism 3
- Claim: Stage II relabeling with the Stage I finetuned model recovers question quality while maintaining supervision quality.
- Mechanism: The finetuned strong model (superior to the original weak supervisor) provides better labels for previously discarded hard questions; uncertainty filtering ensures only high-confidence relabels are kept.
- Core assumption: Stage I finetuned model is measurably better than weak model on hard questions.
- Evidence anchors:
  - [section 3.2] "finetuned student model—now exceeding the weak model in performance—is employed to generate answers"
  - [section 5.2, Figure 7] Stage II recovers difficulty and diversity distributions closer to original
  - [corpus] No direct corpus evidence for this specific two-stage mechanism
- Break condition: When Stage I model does not improve sufficiently over weak model

## Foundational Learning

- **Weak-to-Strong Generalization (Burns et al., 2023)**
  - Why needed here: This is the core problem setting; understanding that strong models can exceed weak supervisors is prerequisite.
  - Quick check question: Can you explain why a strong model trained on weak labels might outperform its weak teacher?

- **Performance Gap Recovered (PGR) Metric**
  - Why needed here: PGR is the primary evaluation criterion; measures how much of the weak-to-strong ceiling gap is recovered.
  - Quick check question: Given weak=50%, strong ceiling=90%, weak-to-strong=70%, what is the PGR?

- **Overfitting to Noisy Labels in Neural Networks**
  - Why needed here: The paper's core problem is strong models overfitting erroneous weak labels; understanding label noise robustness is essential.
  - Quick check question: Why might a high-capacity model memorize incorrect labels more readily than a low-capacity one?

## Architecture Onboarding

- **Component map:**
  - Stage I: Weak model → CoT sampling (10x) → Consistency computation → Threshold filtering → Training Set A → Strong model finetuning
  - Stage II: Discarded questions → Stage I finetuned model → CoT sampling → Consistency filtering → Training Set B → Merge with Set A → Final strong model finetuning

- **Critical path:**
  1. Choosing appropriate Stage I filtering threshold (too high loses question diversity; too low retains noise)
  2. Ensuring Stage I model genuinely improves over weak model before Stage II relabeling
  3. Applying uncertainty filtering in Stage II (ablation shows removing this degrades performance)

- **Design tradeoffs:**
  - Higher Stage I threshold: cleaner labels but harder questions lost
  - More CoT samples: better consistency estimate but higher compute cost
  - Iterative refinement (Stage Exp): potential gains but threshold selection unclear

- **Failure signatures:**
  - PGR degrades at very high filtering thresholds (Stage I-100%: 60.43% vs Stage I-70%: 98.56% on Llama 3 GSM8K)
  - Stage II without filtering causes performance drop (Table 1: 80.28% → 79.59% without filter)
  - Direct answers without CoT fail to generalize (Table 3: negative PGR)

- **First 3 experiments:**
  1. **Replicate baseline weak-to-strong:** Use provided weak model (Llama 3 8B Instruct) to label GSM8K training set, finetune strong model (Llama 3 70B), measure PGR against weak and strong ceiling baselines.
  2. **Ablate Stage I filtering thresholds:** Test thresholds {50%, 60%, 70%, 80%} on GSM8K with Llama 3; plot PGR vs. threshold to identify optimal operating point and observe degradation pattern.
  3. **Validate Stage II recovery mechanism:** Take discarded questions from optimal Stage I run; relabel with Stage I model, apply same threshold filtering; verify that question difficulty distribution (Figure 5 analog) and topic diversity recover toward original distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-stage framework generalize effectively to domains outside of mathematical reasoning, such as open-ended generation or safety alignment?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "the framework’s effectiveness remains to be validated across other domains."
- Why unresolved: The experimental scope was restricted to the GSM8K and MATH benchmarks, which focus on formal logic and calculation.
- What evidence would resolve it: Successful application and maintenance of high Performance Gap Recovered (PGR) on qualitative benchmarks (e.g., creative writing) or safety datasets (e.g., HH-RLHF).

### Open Question 2
- Question: How can the optimal confidence threshold be determined automatically for different datasets without extensive manual tuning?
- Basis in paper: [explicit] The authors identify "automatic threshold selection" as an "important direction for future research" because thresholds "vary significantly across different tasks and datasets."
- Why unresolved: The current methodology relies on manual selection of thresholds (e.g., 0.6, 0.7) to balance supervision quality and question diversity.
- What evidence would resolve it: An adaptive algorithm capable of dynamically setting the filtering threshold based on the weak model's uncertainty distribution.

### Open Question 3
- Question: What are the performance limits of extending the framework into iterative loops beyond two stages?
- Basis in paper: [explicit] The authors discuss "Stage Exp" showing "promising potential for further refinement," but note that the optimal stopping point for such iterations remains an open question.
- Why unresolved: Computational limits restricted the iterative experiments, and the potential for performance degradation or overfitting in later stages was not fully mapped.
- What evidence would resolve it: A scaling curve analysis of PGR performance relative to the number of iterative refinement stages ($n > 2$).

## Limitations
- The framework's effectiveness remains to be validated across domains outside mathematical reasoning
- Optimal filtering thresholds require manual tuning and vary significantly across tasks
- The correlation between weak model consistency and label correctness lacks direct empirical validation

## Confidence
- **High Confidence**: The basic two-stage filtering framework and its implementation details are clearly specified and internally consistent
- **Medium Confidence**: The mechanism by which consistency filtering reduces label noise is plausible but lacks direct validation of the consistency-correctness correlation
- **Low Confidence**: The claim that Stage II fully recovers question quality and diversity is primarily supported by distributional plots rather than downstream task performance

## Next Checks
1. **Correlation validation**: Systematically measure the relationship between weak model answer consistency and actual correctness across different question difficulty levels and topics

2. **Stage I improvement validation**: Compare Stage I finetuned model performance against weak model specifically on the subset of hard questions that were filtered out, demonstrating the improvement is sufficient for Stage II

3. **Threshold optimization analysis**: Instead of treating filtering thresholds as hyperparameters, develop principled criteria for threshold selection based on the consistency-correctness relationship and dataset characteristics