---
ver: rpa2
title: 'RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction'
arxiv_id: '2509.11191'
source_url: https://arxiv.org/abs/2509.11191
tags:
- adversarial
- training
- performance
- language
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces random adversarial training (RAT), a novel
  framework for biomedical information extraction (BioIE) tasks. The study first validates
  that conventional adversarial training improves PubMedBERT performance on BioIE
  tasks but introduces computational overhead.
---

# RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction

## Quick Facts
- **arXiv ID:** 2509.11191
- **Source URL:** https://arxiv.org/abs/2509.11191
- **Reference count:** 40
- **Primary result:** Random adversarial training reduces adversarial attacks by 50% while maintaining or improving performance on six biomedical IE datasets

## Executive Summary
This paper introduces Random Adversarial Training (RAT), a framework that integrates random sampling with adversarial training principles to improve biomedical information extraction tasks. The study validates that conventional adversarial training enhances PubMedBERT performance on BioIE tasks but introduces computational overhead. RAT addresses this by probabilistically skipping adversarial sample generation during training, achieving comparable or slightly better performance while reducing computational costs by approximately 50%. Experiments on six biomedical datasets demonstrate RAT's effectiveness across both named entity recognition and relation extraction tasks.

## Method Summary
The method applies adversarial perturbations to word embeddings using a Bernoulli distribution with p=0.5, meaning each batch has a 50% chance of skipping adversarial generation. When adversarial training is applied, perturbations are generated using FGM, PGD, FreeLB, or SMART methods and combined with standard loss for backpropagation. The framework maintains the regularization benefits of adversarial training while significantly reducing computational overhead through stochastic sampling of adversarial examples.

## Key Results
- RAT achieves comparable or better performance than standard adversarial training across all six datasets
- Performance improvements range from 0.27% to 3.89% across different datasets
- Computational overhead is reduced by approximately 50% through random sampling of adversarial attacks
- No single adversarial method consistently outperforms others across all datasets, suggesting method selection depends on task characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training improves generalization on biomedical IE tasks when applied to word embeddings
- Mechanism: Perturbations in continuous embedding space act as regularization, mitigating "representation collapse" where fine-tuning degrades generic pre-trained representations
- Core assumption: Improvements stem from enhanced pre-trained model robustness preserving generic representations during task-specific fine-tuning
- Evidence anchors: Abstract validation of conventional AT effectiveness; Aghajanyan et al. [20] suggesting pre-trained model robustness improves downstream generalizability
- Break condition: Performance degrades below baseline; computational overhead exceeds acceptable thresholds

### Mechanism 2
- Claim: Random sampling with 50% adversarial attack probability maintains performance while halving computational cost
- Mechanism: Each batch's adversarial sample generation is modeled as an independent Bernoulli trial with expected attack frequency = probability p
- Core assumption: Not every training iteration requires adversarial perturbation to achieve regularization benefits
- Evidence anchors: Abstract claim of 50% attack reduction while maintaining performance; experimental results showing RAT achieves 93.60% vs AT's 93.60% on BC5-chem
- Break condition: Probability p set too low (<0.3 estimated) may insufficiently regularize; performance variance across seeds becomes unacceptable

### Mechanism 3
- Claim: Embedding-space perturbations outperform discrete token-level attacks for NLP regularization
- Mechanism: Perturbations are applied to continuous word embeddings rather than discrete tokens, ensuring adversarial examples remain semantically proximate
- Core assumption: Gradient-normalized perturbations in embedding space create meaningful adversarial examples without breaking semantic coherence
- Evidence anchors: Abstract integration of random sampling with adversarial training; perturbation applied to word embeddings targeting continuous rather than discrete sentence space
- Break condition: Perturbation magnitude ε too large causes embedding-space instability; too small provides insufficient regularization signal

## Foundational Learning

- Concept: **Adversarial Training (FGM/PGD/FreeLB/SMART variants)**
  - Why needed here: RAT builds directly on these methods—you must understand how each generates adversarial samples before modifying them with random sampling
  - Quick check question: Can you explain why PGD uses iterative perturbation refinement while FGM is single-step, and how FreeLB differs in gradient averaging?

- Concept: **Biomedical IE Task Formulations (BioNER/BioRE)**
  - Why needed here: Experiments span two task types with different evaluation protocols; NER uses BIO tagging, RE uses relation classification
  - Quick check question: How does the evaluation metric (micro-F1) differ between sequence labeling (NER) and sentence classification (RE)?

- Concept: **PubMedBERT Architecture and Fine-tuning**
  - Why needed here: All experiments use PubMedBERT as backbone; understanding its domain-specific pre-training is critical for interpreting why adversarial regularization helps
  - Quick check question: What distinguishes PubMedBERT from BioBERT/SciBERT, and why might domain-specific pre-training affect adversarial training efficacy?

## Architecture Onboarding

- Component map:
  ```
  Input Batch → Word Embeddings → [Bernoulli(p=0.5) Decision]
                                      ↓ (k=1)                    ↓ (k=0)
                              Adversarial Generator      Skip adversarial
                              (FGM/PGD/FreeLB/SMART)          |
                                      ↓                        |
                              x_adv = x + ε·g/||g||            |
                                      ↓                        |
                              Concatenate [x, x_adv]           |
                                      ↓                        |
                              PubMedBERT Forward Pass ----------+
                                      ↓
                              Task Head (NER: BIO classifier / RE: Relation classifier)
                                      ↓
                              Loss: L(x) + L(x_adv) when k=1, else L(x)
  ```

- Critical path:
  1. **Perturbation magnitude ε**: Controls adversarial strength (requires tuning)
  2. **Bernoulli probability p**: Set to 0.5 in experiments; core efficiency/performance tradeoff
  3. **Attack method selection**: FGM/PGD/FreeLB/SMART—no consistent winner across datasets

- Design tradeoffs:
  - **Efficiency vs. regularization strength**: Lower p = faster training but risk of under-regularization
  - **Attack iterations (PGD/FreeLB)**: Multi-step attacks cost more but may provide stronger adversarial examples
  - **Task-specific method selection**: Method choice appears dataset-dependent

- Failure signatures:
  - **Performance below baseline**: Check ε too large causing training instability, learning rate not adjusted for mixed objective
  - **No efficiency gain**: Verify Bernoulli sampling is correctly implemented per-batch, not per-epoch
  - **High variance across seeds**: p=0.5 may be insufficiently deterministic—increase p to 0.7

- First 3 experiments:
  1. **Reproduce PubMedBERT+FGM baseline** on one dataset (e.g., BC5-chem): Train without adversarial training → with standard FGM → confirm improvement matches paper (93.33% → 93.61%)
  2. **Ablate Bernoulli probability**: Test p ∈ {0.3, 0.5, 0.7, 1.0} on a single dataset to find efficiency/performance sweet spot
  3. **Cross-task validation**: Apply RAT-FGM to both NER (BC5-chem) and RE (CHEMPROT) to confirm mechanism transfers across task types

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- **Hyperparameter Sensitivity**: Critical hyperparameters (perturbation magnitude, PGD iterations, learning rates) are not specified, creating significant reproducibility barriers
- **Theoretical Foundation Gaps**: The specific mechanism by which RAT's random sampling maintains performance with 50% fewer attacks remains inadequately explained
- **Dataset Representation**: The six datasets used are relatively standard but may not represent the full diversity of biomedical IE tasks

## Confidence
- **High Confidence**: PubMedBERT baseline performance and fundamental RAT framework architecture
- **Medium Confidence**: Specific performance improvements (F1 scores) across datasets, given missing hyperparameter specifications
- **Low Confidence**: Theoretical explanations for why random sampling maintains performance and generalizability to non-biomedical domains

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary ε (perturbation magnitude) and p (Bernoulli probability) across a grid search on one dataset (e.g., BC5-chem) to identify the efficiency/performance tradeoff curve and validate the claimed 50% attack reduction maintains performance

2. **Ablation on Attack Methods**: Compare RAT variants using FGM, PGD, FreeLB, and SMART on the same dataset to empirically determine which adversarial method benefits most from random sampling, addressing the observed dataset-dependent method performance

3. **Computational Overhead Measurement**: Implement RAT and measure actual wall-clock training time reduction compared to standard adversarial training on identical hardware, verifying the claimed efficiency gains extend beyond theoretical attack frequency reduction