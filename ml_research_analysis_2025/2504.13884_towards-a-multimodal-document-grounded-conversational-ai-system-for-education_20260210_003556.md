---
ver: rpa2
title: Towards a Multimodal Document-grounded Conversational AI System for Education
arxiv_id: '2504.13884'
source_url: https://arxiv.org/abs/2504.13884
tags:
- mudoc
- text
- learning
- responses
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MuDoC, a multimodal document-grounded conversational
  AI system for education that integrates text and images from educational documents
  to support multimedia learning. The system uses GPT-4o to generate interleaved text
  and image responses while allowing users to verify AI-generated content through
  navigation to source materials.
---

# Towards a Multimodal Document-grounded Conversational AI System for Education

## Quick Facts
- arXiv ID: 2504.13884
- Source URL: https://arxiv.org/abs/2504.13884
- Reference count: 34
- Primary result: MuDoC multimodal system significantly improves engagement, memorability, confidence, and trustworthiness compared to text-only systems

## Executive Summary
This paper introduces MuDoC, a multimodal document-grounded conversational AI system that integrates text and images from educational documents to enhance multimedia learning. The system leverages GPT-4o to generate interleaved text and image responses while enabling users to verify AI-generated content through navigation to source materials. A user study comparing MuDoC with a text-only system (TexDoC) demonstrated strong user preferences for the multimodal approach across multiple dimensions including engagement, memorability, confidence, and trustworthiness.

## Method Summary
The study employed a within-subjects design where 30 participants interacted with both MuDoC (multimodal) and TexDoC (text-only) systems. Participants engaged with both systems in sessions focused on answering educational questions based on provided documents. The MuDoC system processed both textual and visual content from documents, generating responses that combined text and images, while TexDoC processed only textual content. Performance metrics included problem-solving accuracy, user preference ratings, and qualitative feedback. Statistical analyses (paired t-tests) were conducted to compare outcomes between the two systems.

## Key Results
- Users strongly preferred MuDoC over TexDoC for engagement (28/30), memorability (23/30), confidence (23/30), and trustworthiness (20/30)
- No significant difference in problem-solving performance between systems
- Qualitative feedback indicated visuals enhanced learning and source verification promoted critical thinking
- Findings suggest multimodal document-grounded AI improves learning engagement and trust

## Why This Works (Mechanism)
MuDoC works by integrating multimodal document processing with conversational AI, enabling users to interact with educational content through both text and visual channels. The system leverages GPT-4o's multimodal capabilities to analyze and synthesize information from both textual and visual sources, generating responses that combine text and images. The source verification feature allows users to trace AI-generated content back to original documents, promoting transparency and critical thinking. This approach aligns with multimedia learning principles by presenting information through complementary channels, potentially reducing cognitive load while enhancing comprehension and retention.

## Foundational Learning
- **Multimodal Learning**: Processing and integrating information from multiple modalities (text, images) is essential for modern AI systems. Quick check: Can the system effectively process both textual and visual inputs simultaneously?
- **Document-grounded AI**: Systems must ground responses in specific source documents to ensure accuracy and verifiability. Quick check: Does the system provide reliable references to source materials?
- **Multimedia Learning Theory**: Information presented through multiple complementary channels can enhance learning outcomes. Quick check: Are text and images used in ways that complement rather than duplicate information?
- **Human-AI Interaction**: User trust and engagement depend on transparency and verification capabilities. Quick check: Can users easily verify AI-generated content against source materials?
- **Statistical Power Analysis**: Small sample sizes limit detection of performance differences. Quick check: Is the sample size sufficient to detect meaningful differences between conditions?

## Architecture Onboarding
**Component Map**: User Interface -> MuDoC/TexDoC Engine -> Document Processor -> GPT-4o -> Response Generator -> Source Verifier

**Critical Path**: User query → Document processor extracts relevant text/images → GPT-4o analyzes content → Response generator creates interleaved text/image response → Source verifier provides navigation links → User receives response with verification options

**Design Tradeoffs**: 
- Multimodal processing enables richer responses but increases computational complexity
- Source verification enhances trust but may increase response latency
- GPT-4o provides state-of-the-art capabilities but introduces dependency on proprietary API

**Failure Signatures**:
- Incorrect image-text alignment leading to confusing responses
- Source navigation failures preventing verification
- GPT-4o API unavailability disrupting system functionality
- Cognitive overload from excessive visual information

**First 3 Experiments**:
1. Test basic document processing pipeline with sample educational materials
2. Validate GPT-4o's ability to generate coherent text-image responses
3. Verify source navigation functionality links to correct document locations

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though several areas for future research are implied including long-term learning outcomes, system performance across different educational domains, and the impact of multimodal AI on diverse user populations.

## Limitations
- Small sample size (30 participants) limits statistical power and generalizability
- Study focused on a specific educational domain (microbiology)
- Limited to single-session assessments without long-term learning outcome measures
- Dependency on GPT-4o API introduces potential variability with model updates

## Confidence
- **High confidence**: MuDoC significantly improves user engagement, memorability, confidence, and trustworthiness compared to text-only systems
- **Medium confidence**: Multimodal elements enhance learning and promote critical thinking (based on qualitative feedback)
- **Medium confidence**: No significant difference in problem-solving performance between systems

## Next Checks
1. Conduct larger-scale studies (n=100+) across multiple educational domains to validate generalizability of engagement and learning benefits
2. Implement longitudinal studies tracking learning outcomes over weeks/months to assess retention and cumulative effects
3. Test with diverse user populations including different age groups, educational backgrounds, and accessibility needs to evaluate system effectiveness across demographics