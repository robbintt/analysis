---
ver: rpa2
title: Understanding LLM Reasoning for Abstractive Summarization
arxiv_id: '2512.03503'
source_url: https://arxiv.org/abs/2512.03503
tags:
- reasoning
- summary
- uni00000011
- summarization
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale comparative study of
  reasoning paradigms in LLM-based abstractive summarization. It evaluates eight explicit
  reasoning strategies (Chain-of-Thought, Cited Summarization, Extract-to-Abstract,
  Question-Answer Guided, Decomposition, Plan-then-Write, Iterative Refinement, Self-Consistency)
  and three Large Reasoning Models (O1, O3, GPT-5) across eight diverse datasets.
---

# Understanding LLM Reasoning for Abstractive Summarization

## Quick Facts
- **arXiv ID:** 2512.03503
- **Source URL:** https://arxiv.org/abs/2512.03503
- **Reference count:** 36
- **Primary result:** Explicit reasoning strategies improve reference-based quality metrics but reduce factual faithfulness in LLM abstractive summarization.

## Executive Summary
This paper presents the first large-scale comparative study of reasoning paradigms in LLM-based abstractive summarization, evaluating eight explicit reasoning strategies and three Large Reasoning Models across eight diverse datasets. The core finding reveals a fundamental trade-off: explicit reasoning methods improve fluency and reference overlap but reduce factual grounding, while implicit reasoning in LRMs shows the opposite pattern. Increasing an LRM's internal reasoning budget actually hurts faithfulness without improving quality, suggesting effective summarization requires faithful compression rather than creative over-thinking. The study provides practical guidance recommending explicit reasoning for zero-shot quality and GPT-5 for factual consistency demands.

## Method Summary
The study evaluates 8 reasoning strategies (Chain-of-Thought, Cited Summarization, Extract-to-Abstract, Question-Answer Guided, Decomposition, Plan-then-Write, Iterative Refinement, Self-Consistency) and 3 LRMs (O1, O3, GPT-5) across 8 datasets (CNN/DM, SAMSum, Reddit, WikiHow, ArXiv, Multi-News, BookSum, SciGen). Using GPT-4.1 (1,000K context) as main model with LRMs via Azure OpenAI API, the evaluation employs 0-shot and 2-shot settings with temperature=0 and max_output_tokens=1,000. Reference-based metrics include ROUGE avg 1/2/L and BERTScore F1; factual faithfulness is measured via SummaC and AlignScore; LLM-as-judge (G-Eval) evaluates Completeness, Conciseness, and Faithfulness. All prompts are specified in Appendix J, with 100 instances sampled per dataset from test splits (800 total).

## Key Results
- Self-Consistency and Iterative Refinement achieve highest reference-based quality (BERTScore ~86) but lowest faithfulness (AlignScore ~60)
- GPT-5 achieves highest faithfulness (AlignScore ~73) but lower quality (BERTScore ~83)
- Increasing GPT-5's internal reasoning budget consistently reduces faithfulness without improving quality
- Simple Vanilla prompts often match or surpass complex strategies
- LLM-as-a-judge metrics overestimate faithfulness compared to human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Explicit Reasoning vs. Factual Grounding
Explicit reasoning strategies (e.g., Self-Consistency, Iterative Refinement) improve linguistic fluency and reference overlap at the expense of factual consistency. These methods encourage the model to generate more "abstract" content by filling logical gaps to create a coherent narrative, increasing n-gram overlap with reference texts (ROUGE/BERTScore) but introducing information not grounded in the source (hallucination), lowering faithfulness scores.

### Mechanism 2: Over-Thinking in Large Reasoning Models
Increasing internal "thinking" budgets in Large Reasoning Models (LRMs) reduces factual faithfulness. LRMs appear to treat summarization as a generative derivation task, and with increased reasoning tokens, the model generates internal logic that drifts from the source text ("creative gap-filling") rather than faithfully compressing it.

### Mechanism 3: Context Loss in Decomposition
Decomposition strategies lose global conditional context, leading to systematic hallucinations. Chunking documents isolates text segments, and when summarizing chunks locally, the model loses the global "if-then" logic, often merging conditional rules (e.g., applying a cooking instruction to all cases instead of just the specific condition).

## Foundational Learning

- **Concept: Compression vs. Derivation**
  - **Why needed here:** The study proves that reasoning (derivation) skills do not transfer cleanly to summarization (compression). Understanding this distinction prevents the misapplication of "Chain-of-Thought" approaches to tasks requiring strict fidelity.
  - **Quick check question:** Does the task require creating new logical conclusions (Derivation) or selecting and shortening existing information (Compression)?

- **Concept: Reference-based vs. Faithfulness Metrics**
  - **Why needed here:** The paper reveals a trade-off where optimizing for ROUGE (reference similarity) can hurt AlignScore (factual consistency). Systems engineers must define which axis of "quality" matters for the specific application.
  - **Quick check question:** Are we optimizing for how similar the summary looks to a human reference (ROUGE), or how strictly it adheres to the source facts (AlignScore/SummaC)?

- **Concept: Implicit vs. Explicit Reasoning**
  - **Why needed here:** There is a performance divergence between prompt-induced reasoning (Explicit, e.g., CoT) and model-internal reasoning (Implicit, e.g., O1/GPT-5). Explicit methods help style/fluency; Implicit methods help factual grounding but fail on conciseness.
  - **Quick check question:** Is the priority stylistic polish (use Explicit) or factual precision (use Implicit/LRMs)?

## Architecture Onboarding

- **Component map:** Raw Document -> Augmentation Layer (CoT, Extracted Sentences, Q&A Pairs) -> Organization Layer (Decomposition, Planning) -> Reflection Layer (Iterative Refinement, Self-Consistency voting) -> Evaluation Suite (ROUGE/BERTScore for Quality; SummaC/AlignScore for Faithfulness)

- **Critical path:** 1) Selection: Choose Vanilla for 2-shot efficiency; Choose SC for 0-shot quality; Choose GPT-5 for maximum faithfulness. 2) Prompting: If using GPT-4.1, use SC or IR. If using LRM, use standard prompting without encouraging extra reasoning. 3) Evaluation: Must benchmark against both Reference (ROUGE) and NLI (AlignScore) metrics to detect the quality-faithfulness trade-off.

- **Design tradeoffs:** Zero-shot SC vs. Few-shot Vanilla: SC wins in zero-shot; Vanilla matches/beats it in 2-shot while being cheaper. LRMs vs. Explicit Prompts: LRMs offer best faithfulness but poor conciseness/completeness trade-offs compared to Explicit methods. Cost vs. Abstractiveness: High abstractiveness (Plan/Deco) often correlates with lower factual scores.

- **Failure signatures:** The "Over-Thinking" Drop: Faithfulness scores drop as LRM "think ability" increases. The GEval Illusion: LLM-as-a-Judge rates faithfulness near-perfect while human eval shows distinct failures. Conditional Generalization: Systems using Deco/Chunking apply specific rules universally (e.g., "cook for 2 mins" applied to "raw" items).

- **First 3 experiments:** 1) Baseline Benchmark: Run Vanilla (0-shot) vs. Self-Consistency on a held-out set to verify the Quality vs. Faithfulness trade-off on your specific domain. 2) Budget Calibration: Test LRM on "low" vs. "medium" think ability to verify if "over-thinking" harms faithfulness in your data. 3) Metric Alignment: Compare LLM-as-a-Judge scores against a small human-annotated gold set to check for the "overestimation of faithfulness" bias.

## Open Questions the Paper Calls Out

### Open Question 1
Can the trade-off between summary quality (e.g., fluency) and factual faithfulness be mitigated or resolved through novel reasoning paradigms? The paper identifies this trade-off as inherent to current reasoning approaches but does not propose methods achieving both high quality and high faithfulness simultaneously.

### Open Question 2
How can the reasoning "budget" (depth/complexity) for Large Reasoning Models (LRMs) be dynamically or adaptively optimized for different summarization contexts? The study finds that higher think ability reduces faithfulness but doesn't explore mechanisms to automatically determine appropriate reasoning levels per input.

### Open Question 3
What are the principled causes behind the misalignment between LLM-as-a-judge faithfulness scores and human judgments? The authors report significant overestimation by GEval but don't investigate why the LLM judge fails to align with humans on factual consistency.

## Limitations
- Study relies on synthetic sampling (100 instances per dataset) rather than comprehensive test sets
- "Over-thinking" hypothesis requires further validation across different model families and reasoning architectures
- API access limitations prevented ablation studies on internal model parameters to isolate reasoning vs. compression mechanisms

## Confidence
- **High Confidence:** The core finding that explicit reasoning strategies improve reference-based metrics while degrading factual faithfulness is supported by consistent cross-dataset patterns and human evaluation alignment.
- **Medium Confidence:** The mechanism explaining LRMs' over-thinking behavior is plausible but requires additional mechanistic studies; generalizability across architectures remains uncertain.
- **Low Confidence:** Exact numerical thresholds (e.g., "medium" think_ability settings, specific candidate counts for SC) lack precise calibration data and may vary significantly across different model deployments.

## Next Checks
1. **Mechanism Isolation Test:** Conduct ablation studies varying only the reasoning budget in LRMs while holding all other parameters constant to isolate the over-thinking effect from other confounding factors.

2. **Cross-Architecture Replication:** Test the explicit reasoning strategies on open-weight models (Llama, Mistral) to verify whether the quality-faithfulness trade-off persists outside the GPT family.

3. **Long-Form Generalization:** Evaluate the same strategies on documents exceeding 8K tokens to determine if chunking-based methods maintain their systematic hallucination patterns in longer contexts.