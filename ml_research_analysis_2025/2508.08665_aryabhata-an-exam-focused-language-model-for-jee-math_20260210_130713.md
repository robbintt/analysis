---
ver: rpa2
title: 'Aryabhata: An exam-focused language model for JEE Math'
arxiv_id: '2508.08665'
source_url: https://arxiv.org/abs/2508.08665
tags:
- answer
- math
- reasoning
- question
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Aryabhata 1.0, a 7B parameter math reasoning
  model optimized for the Indian JEE exam. It combines three strong open-weight models
  via linear merging, then applies supervised fine-tuning on verified chain-of-thought
  traces and reinforcement learning with verifiable rewards.
---

# Aryabhata: An exam-focused language model for JEE Math

## Quick Facts
- **arXiv ID**: 2508.08665
- **Source URL**: https://arxiv.org/abs/2508.08665
- **Reference count**: 18
- **Primary result**: 7B parameter math reasoning model achieving 86.0% (Jan) and 90.2% (Apr) accuracy on JEE Main 2025

## Executive Summary
Aryabhata 1.0 is a specialized 7B parameter language model designed for Indian JEE Mathematics exam preparation. The model combines three strong open-weight reasoning models through linear merging, followed by supervised fine-tuning on verified chain-of-thought traces and reinforcement learning with verifiable rewards. It achieves state-of-the-art performance on JEE Main 2025 (86.0% and 90.2% accuracy) while maintaining pedagogically useful step-by-step reasoning suitable for exam preparation.

## Method Summary
The training pipeline consists of four stages: (1) Linear merging of three 7B models (Qwen2.5-Math, AceMath, DeepSeek-R1-Distill) using MergeKit framework, (2) Data curation to filter 130K JEE-style questions from proprietary corpus, (3) Supervised fine-tuning with curriculum learning on 350K verified chain-of-thought traces generated via best-of-4 rejection sampling, and (4) Reinforcement learning with verifiable rewards using A2C objective with group-relative advantage estimation, adaptive group resizing, and progressive temperature scaling.

## Key Results
- JEE Main 2025: 86.0% accuracy (January session), 90.2% accuracy (April session)
- MATH 500 benchmark: 83.6% accuracy
- GSM8K benchmark: 94.8% accuracy
- ~2K token response length suitable for detailed step-by-step explanations

## Why This Works (Mechanism)

### Mechanism 1
Linear merging of specialized reasoning and fluency models produces hybrid capabilities superior to individual components for exam-style math. Weighted parameter averaging (θ_merged = αθ₁ + βθ₂ + γθ₃) combines symbolic fluency from Qwen2.5-Math, enhanced accuracy from AceMath, and deliberate multi-step reasoning from DeepSeek-R1-Distill. The merged model inherits quick pattern-matching for simpler problems while retaining structured decomposition for complex ones.

### Mechanism 2
Best-of-n rejection sampling with curriculum ordering reduces noise in supervised fine-tuning signals. For each question, sample 4 CoT completions; retain only those matching ground truth. Group by success rate (4/4, 3/4, etc.) and train from easiest to hardest. This filters incorrect reasoning chains that happen to reach right answers through wrong paths, while curriculum ordering stabilizes early gradient estimates.

### Mechanism 3
Group-relative advantage estimation with adaptive exploration enables stable RL fine-tuning without KL constraints. Compute advantages as normalized deviations from group mean reward (Âᵢ,ₜ = (Rᵢ - R̄_group) / σ_group). Binary rewards (1/0 for correct/incorrect) provide unambiguous signal. Adaptive group sizing (8→64) and progressive temperature scaling (0.6→1.0) increase sampling diversity for harder problems over training.

## Foundational Learning

- **Concept: Model Merging / Weight Averaging**
  - Why needed here: The first training stage requires understanding how linear combinations of model parameters can produce new capabilities without additional training.
  - Quick check question: Given two models with identical architecture but different fine-tuning, does θ_merged = 0.5θ₁ + 0.5θ₂ always produce interpolated behavior, or can it fail catastrophically?

- **Concept: Rejection Sampling for Data Quality**
  - Why needed here: The SFT stage depends on generating candidate solutions and filtering by correctness to produce clean training data.
  - Quick check question: If a model achieves 25% accuracy on a problem set and you sample 4 completions per question, what fraction of questions will have at least one verifiable correct trace?

- **Concept: Policy Gradient with Advantage Estimation**
  - Why needed here: The RLVR stage uses A2C-style updates with group-relative advantages rather than standard PPO clipping.
  - Quick check question: Why does normalizing advantages by group mean and standard deviation reduce variance compared to using raw rewards?

## Architecture Onboarding

- **Component map**:
  ```
  Stage 1: Model Merging
    Qwen2.5-Math-7B-Instruct ─┐
    AceMath-7B-Instruct ─────┼─→ Linear Merge (MergeKit) ─→ Merged Model
    DeepSeek-R1-Distill-7B ──┘

  Stage 2: Data Curation
    250K raw questions → Filter (diagrams, language, options) → 130K clean questions

  Stage 3: SFT with Rejection Sampling
    Merged Model → Best-of-4 sampling → 350K verified CoTs → LoRA fine-tuning

  Stage 4: RLVR
    SFT Model → A2C + Group-Relative Advantage → Adaptive Group/Temperature → Final Model
  ```

- **Critical path**:
  1. Verify merged model produces coherent outputs before SFT (corrupted merges are unrecoverable)
  2. Ensure answer-matching pipeline (Algorithm 1) has <1% false positive rate; noisy reward signal propagates through both SFT and RL
  3. Monitor RL training for reward hacking (e.g., model generating simpler problems to maximize group-relative advantage)

- **Design tradeoffs**:
  - 7B scale: Enables deployment on consumer hardware but limits reasoning depth vs 70B+ models
  - Binary rewards: Simple and unambiguous but provide no gradient for partial progress
  - Proprietary training data: High domain alignment but prevents full reproducibility
  - ~2K token responses: Pedagogically tractable but may truncate complex derivations

- **Failure signatures**:
  - Merged model outputs garbled text → incompatible weight dimensions or misaligned tokenizers
  - SFT model regurgitates training solutions → overfitting from insufficient data diversity or excessive epochs
  - RL model produces shorter, simpler solutions → reward hacking; needs curriculum filtering adjustment
  - Out-of-distribution accuracy collapses → overfitting to JEE distribution; may need mixed-domain training

- **First 3 experiments**:
  1. **Ablate merge weights**: Train with α,β,γ set to (1,0,0), (0,1,0), (0,0,1) and compare to merged baseline on held-out JEE questions to quantify merge benefit.
  2. **Vary rejection sampling width**: Compare best-of-2 vs best-of-4 vs best-of-8 to measure tradeoff between data quality and coverage (how many questions retain ≥1 valid trace).
  3. **Test advantage estimation stability**: Run RL training with fixed group size (16) vs adaptive sizing; plot reward variance and final accuracy to validate the exploration strategy contribution.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed pipeline (merging + RLVR) generalize effectively to non-mathematical STEM subjects like Physics and Chemistry? The Conclusion explicitly states the intention to "Expand coverage to Physics and Chemistry" and scale to the full JEE/NEET syllabus. The current study focuses exclusively on Mathematics, which relies heavily on symbolic manipulation and logic. Physics and Chemistry often require causal reasoning, spatial understanding, and application of first principles, which may not transfer directly from the current training configuration.

### Open Question 2
What is the specific contribution of Adaptive Group Resizing and Progressive Temperature Scaling to the model's performance compared to standard GRPO? The Methodology section introduces "novel exploration strategies" like Adaptive Group Resizing and Temperature Scaling, but the Results section only compares the final model against external baselines (GPT-4o, DeepSeek), without ablating these specific RL components.

### Open Question 3
How does the exclusion of diagram-based questions limit the model's applicability to the complete JEE Advanced exam? Section 3.2 (Data Curation) explicitly states that "Removed diagram-based questions, which require multimodal reasoning not supported by current text-only models." The JEE (especially Advanced) frequently includes questions in Geometry and Physics where the problem statement is incomplete without the accompanying diagram.

## Limitations
- Proprietary training data not publicly available, preventing full reproducibility
- Limited generalization evidence beyond narrow mathematical benchmarks
- Binary reward signal may encourage oversimplification rather than deep understanding
- No systematic evaluation of pedagogical quality or learning effectiveness

## Confidence
- **High Confidence**: The overall architectural approach (merge → SFT → RLVR) is well-established in the literature with sound implementation details
- **Medium Confidence**: Reported benchmark results are internally consistent but cannot be independently verified due to proprietary data and unspecified merge weights
- **Low Confidence**: Claims about pedagogical utility lack quantitative validation and systematic evaluation of explanation quality

## Next Checks
1. **Merge Weight Sensitivity Analysis**: Systematically vary the linear merge weights (α, β, γ) and measure performance degradation on held-out JEE-style problems to quantify robustness
2. **Reward Signal Ablation**: Compare Aryabhata's performance when trained with binary rewards versus graded reward system providing partial credit for intermediate steps
3. **Cross-Domain Transfer Testing**: Evaluate Aryabhata on broader mathematical domains including proof-based problems and real-world modeling tasks not represented in JEE exams