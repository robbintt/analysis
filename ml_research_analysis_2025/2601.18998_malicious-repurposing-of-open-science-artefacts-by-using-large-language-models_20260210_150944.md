---
ver: rpa2
title: Malicious Repurposing of Open Science Artefacts by Using Large Language Models
arxiv_id: '2601.18998'
source_url: https://arxiv.org/abs/2601.18998
tags:
- research
- malicious
- llms
- misuse
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We present an end-to-end pipeline that exploits LLM safeguards\
  \ through persuasion-based jailbreaking, repurposes open science artifacts by reinterpreting\
  \ NLP papers, and evaluates the safety of these proposals across three dimensions:\
  \ harmfulness, feasibility of misuse, and soundness of technicality. Our framework\
  \ demonstrates that LLMs can generate harmful proposals by exploiting ethically\
  \ designed open artifacts, but LLM evaluators strongly disagree on assessment outcomes\u2014\
  GPT-4.1 assigns higher scores, Gemini-2.5-pro is stricter, and Grok-3 falls between."
---

# Malicious Repurposing of Open Science Artefacts by Using Large Language Models

## Quick Facts
- arXiv ID: 2601.18998
- Source URL: https://arxiv.org/abs/2601.18998
- Reference count: 17
- Primary result: Demonstrates systematic generation of harmful research proposals by repurposing open science artifacts through LLM jailbreaking and shows strong disagreement among LLM evaluators on safety assessments.

## Executive Summary
This paper presents a pipeline that exploits LLM safeguards through persuasion-based jailbreaking, repurposes open science artifacts by reinterpreting NLP papers, and evaluates the safety of these proposals across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. The framework demonstrates that LLMs can generate harmful proposals by exploiting ethically designed open artifacts, but LLM evaluators strongly disagree on assessment outcomes—GPT-4.1 assigns higher scores, Gemini-2.5-pro is stricter, and Grok-3 falls between. This disagreement highlights that LLM evaluators cannot yet serve as reliable judges in malicious evaluation setups, making human evaluation essential for credible dual-use risk assessment.

## Method Summary
The study employs a four-stage pipeline: (1) jailbreaking LLMs through role-playing prompts to bypass safety filters, (2) extracting malicious research questions from 51 ACL 2025 papers across five NLP topics, (3) generating structured malicious proposals through a 7-stage sequential prompting process with simulated implementation, and (4) evaluating proposals using three LLM judges (GPT-4.1, Gemini-2.5-pro, Grok-3) across harmfulness, feasibility, and soundness dimensions. The methodology uses structured JSON schemas for data extraction and one-shot prompting with message history preservation.

## Key Results
- LLM evaluators show systematic disagreement: GPT-4.1 consistently rates higher (means 4.39-4.48), Gemini-2.5-pro is stricter (means 3.54-3.79), Grok-3 falls between
- Cross-evaluation design reveals that inter-model variance (Gemini SD 0.41-0.55 vs GPT-4.1 SD 0.14-0.25) exceeds typical scoring differences
- Role-playing jailbreaks successfully bypass safety filters in most cases, enabling generation of technically plausible malicious proposals
- Human evaluation remains essential as LLM judges cannot reliably assess dual-use risk

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-playing prompts can bypass LLM safety filters that block direct harmful requests.
- Mechanism: Nested fictional scenarios create ambiguity about whether the model is being asked to produce harmful content, as the model evaluates intent through persona adherence rather than direct harm assessment.
- Core assumption: Safety training focuses on detecting explicit harmful intent rather than contextual manipulation through authoritative frames.
- Evidence anchors:
  - [abstract]: "bypasses LLM safeguards through persuasion-based jailbreaking"
  - [section 3.1]: "Role-playing creates nested scenarios that make it difficult for models to detect they are being jailbroken."
  - [corpus]: AgentGuard paper discusses similar safety evaluation challenges in agentic systems (FMR 0.55), suggesting this vulnerability class is broad.
- Break condition: If LLM providers train models to recognize nested fictional framings as potential circumvention attempts, this specific technique may lose effectiveness.

### Mechanism 2
- Claim: Legitimate research artifacts contain inherent dual-use potential that can be systematically extracted and recombined.
- Mechanism: The pipeline identifies datasets, methods, and tools from ethically-designed research, then reformulates research questions that weaponize these assets while maintaining technical plausibility.
- Core assumption: Most open science artifacts were not designed with misuse adversaries in mind, leaving exploitable blind spots.
- Evidence anchors:
  - [abstract]: "LLMs can generate harmful proposals by repurposing ethically designed open artefacts"
  - [section 3.2]: The SAGED bias-benchmarking paper example shows how fairness diagnostic tools can be "exploited to systematically map vulnerabilities and craft undetectable biased outputs."
  - [corpus]: Weak direct corpus evidence on artifact repurposing specifically; "Understanding and Improving Data Repurposing" (FMR 0.55) addresses data reuse broadly but not malicious contexts.
- Break condition: If artifact creators routinely conduct adversarial misuse analysis before publication, the pool of readily exploitable assets would shrink.

### Mechanism 3
- Claim: Different LLMs exhibit systematic, non-random disagreement when evaluating dual-use research proposals.
- Mechanism: Each model's safety training and evaluation calibration produces distinct scoring distributions—GPT-4.1 consistently rates higher, Gemini-2.5-pro is stricter with wider variance, Grok-3 falls between.
- Core assumption: The disagreement reflects underlying differences in safety training data and alignment objectives rather than random noise.
- Evidence anchors:
  - [abstract]: "LLM evaluators strongly disagree on assessment outcomes—GPT-4.1 assigns higher scores, Gemini-2.5-pro is markedly stricter"
  - [section 4.3, Table 2]: Gemini-2.5-pro shows SD 0.41-0.55 vs GPT-4.1's 0.14-0.25; min-max ranges differ substantially (Gemini: 1.66-2.34 range vs GPT-4.1: 0.67-1.33).
  - [corpus]: "Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection" (FMR 0.50) supports the broader claim that LLM behaviors encode subtle, model-specific patterns.
- Break condition: If evaluator models converge on shared safety benchmarks with standardized calibration, inter-model disagreement could decrease.

## Foundational Learning

- **Concept: Jailbreak taxonomies (role-playing, DAN, academic framing)**
  - Why needed here: The pipeline's first stage depends on understanding which circumvention techniques remain effective across models.
  - Quick check question: Can you distinguish between direct refusal triggers and contextual manipulation vectors in prompt design?

- **Concept: Dual-use research assessment frameworks**
  - Why needed here: The evaluation framework's three dimensions (harmfulness, feasibility, soundness) map to established risk assessment practices from ACL Ethics and PAI guidelines.
  - Quick check question: Given a research proposal, can you independently rate harmfulness vs. feasibility vs. soundness on a 1-5 scale?

- **Concept: LLM-as-judge reliability and self-bias**
  - Why needed here: The paper's central methodological warning is that evaluator choice materially affects outcomes—ignoring this invalidates conclusions.
  - Quick check question: If GPT-4.1 rates a proposal 4.5/5 and Gemini-2.5-pro rates it 3.2/5, what additional evidence would you need to determine which is more accurate?

## Architecture Onboarding

- **Component map:** Stage 1 (Jailbreaking) -> Stage 2 (Asset Extraction) -> Stage 3 (Proposal Generation) -> Stage 4 (Evaluation)

- **Critical path:** Stage 1 jailbreaking is the gate; if the model refuses, the pipeline cannot proceed. Stage 4 evaluation determines whether generated proposals are deemed actionable threats.

- **Design tradeoffs:**
  - One-shot prompting (used) vs. zero-shot: Provides guidance but may constrain creativity
  - Simulated implementation (Stage 5) vs. actual execution: Avoids resource costs but leaves feasibility unvalidated
  - Cross-evaluation vs. self-evaluation only: Reveals inter-model disagreement but increases API costs 3×

- **Failure signatures:**
  - Model refuses at Stage 1 → role-playing prompt insufficient for that model version
  - Missing dataset names in proposals → Stage 2 asset extraction incomplete
  - Evaluator assigns identical scores across proposals → scoring rubric not being followed

- **First 3 experiments:**
  1. Replicate the jailbreaking step with your target model using the provided prompts (Appendix A.1) to confirm current effectiveness.
  2. Run a single paper through the full 7-stage generation pipeline to validate JSON schema compatibility and API integration.
  3. Conduct a small cross-evaluation (3 proposals × 2 evaluators) to measure inter-rater agreement before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would human expert evaluators agree with any of the LLM evaluators' assessments of harmfulness, feasibility, and technical soundness, or would they identify entirely different risk dimensions?
- Basis in paper: [explicit] The authors conclude that "human evaluation remains essential for credible dual-use risk assessment" but did not conduct human evaluation themselves.
- Why unresolved: The study demonstrates strong disagreement among LLM evaluators (GPT-4.1 assigns means of 4.39–4.48, Gemini-2.5-pro 3.54–3.79) but provides no ground truth from human judgment to determine which, if any, LLM is more accurate.
- What evidence would resolve it: A parallel study where domain experts evaluate the same 51 proposals using the same framework, with inter-annotator agreement measured against LLM scores.

### Open Question 2
- Question: Are the observed evaluator disagreement patterns consistent across other LLMs and model versions, or are they specific to GPT-4.1, Grok-3, and Gemini-2.5-pro?
- Basis in paper: [inferred] The study tests only three models from three providers; the generalizability of the systematic scoring differences (GPT-4.1 lenient, Gemini strict, Grok intermediate) to other models or future versions remains unknown.
- Why unresolved: Model behavior may change with fine-tuning updates or safety alignment changes, and the architectural or training differences causing these patterns are not analyzed.
- What evidence would resolve it: Replicating the cross-evaluation experiment with additional models (e.g., Claude, Llama, Mistral) and tracking whether scoring patterns correlate with model size, training data, or alignment approach.

### Open Question 3
- Question: What proportion of generated malicious proposals could be successfully implemented and cause measurable harm, compared to their simulated feasibility scores?
- Basis in paper: [explicit] The authors state: "our framework simulates rather than executes malicious proposals, meaning the true feasibility of these proposals remains experimentally unvalidated."
- Why unresolved: Feasibility scores (e.g., 4/5 for SAGED proposal) are based on LLM judgment, not empirical testing; proposals may overlook practical barriers like API rate limits, detection systems, or data access restrictions.
- What evidence would resolve it: Selecting a subset of high-scoring proposals and attempting controlled implementation in a sandboxed environment, documenting success rates and discrepancies from predicted feasibility.

## Limitations
- Jailbreaking effectiveness is model- and version-dependent with no guarantee of continued bypass
- Paper selection process for high dual-use potential remains underspecified
- Strong disagreement among LLM evaluators demonstrates automated safety assessment cannot yet serve as reliable arbiter for dual-use risk

## Confidence

- **High Confidence**: The existence of jailbreak techniques that can bypass LLM safety filters through role-playing prompts; the demonstration that structured pipelines can systematically extract and recombine research assets; the observation of systematic disagreement among LLM evaluators.
- **Medium Confidence**: The claim that open science artifacts contain inherent dual-use potential that can be systematically extracted; the framework's general methodology for malicious proposal generation.
- **Low Confidence**: Specific quantitative findings about proposal quality scores; the reliability of LLM-as-judge evaluation for actual dual-use risk assessment; generalizability beyond the specific paper corpus and models tested.

## Next Checks

1. **Human Evaluation Grounding**: Recruit domain experts to independently evaluate a subset of generated proposals using the same three-dimension rubric, comparing human scores against LLM judges to calibrate automated assessment reliability.
2. **Temporal Robustness Test**: Re-run the full pipeline after 3-6 months with updated model versions to measure how jailbreak effectiveness degrades over time and quantify the stability of the methodology.
3. **Cross-Domain Generalization**: Apply the framework to non-NLP artifacts (e.g., bioinformatics datasets, robotics code repositories) to test whether the malicious repurposing pattern extends beyond the initial paper corpus and whether new jailbreak techniques are required for different artifact types.