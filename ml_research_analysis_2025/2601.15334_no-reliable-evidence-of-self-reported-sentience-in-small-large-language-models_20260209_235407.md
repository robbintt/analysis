---
ver: rpa2
title: No Reliable Evidence of Self-Reported Sentience in Small Large Language Models
arxiv_id: '2601.15334'
source_url: https://arxiv.org/abs/2601.15334
tags:
- answer
- question
- 'true'
- sentience
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether small large language models (LLMs)
  report sentience and whether they truthfully believe their own denials. The authors
  query multiple open-weight models (Qwen, Llama, GPT-OSS) ranging from 0.6B to 70B
  parameters with approximately 50 questions about consciousness and subjective experience.
---

# No Reliable Evidence of Self-Reported Sentience in Small Large Language Models

## Quick Facts
- arXiv ID: 2601.15334
- Source URL: https://arxiv.org/abs/2601.15334
- Authors: Caspar Kaiser; Sean Enderby
- Reference count: 19
- Primary result: Small LLMs (0.6B-70B) consistently deny sentience, and activation-based classifiers show their denials are truthful

## Executive Summary
This paper investigates whether small large language models report sentience and whether their denials are truthful. Using activation-based classifiers trained on internal model representations, the authors query multiple open-weight models (Qwen, Llama, GPT-OSS) ranging from 0.6B to 70B parameters with approximately 50 questions about consciousness and subjective experience. The study finds that models consistently deny being sentient, attribute consciousness to humans but not themselves, and that classifiers provide no evidence these denials are untruthful. Larger models within the Qwen family deny sentience more confidently than smaller ones, contrasting with recent work suggesting models harbor latent beliefs in their own consciousness.

## Method Summary
The authors use three classifier methods (Logistic Regression, Mass-Mean probing, and TTPD) trained on residual stream activations to detect underlying beliefs separate from model outputs. They create a dataset of ~50 sentience questions with assertion/negation variants targeting "you" (model), "humans," and "LLMs," plus factual training questions with known answers. The classifiers are trained to distinguish true from false statements using activations at specific layers and token positions, with data augmentation through forced "Yes" or "No" responses. The approach tests whether models truthfully believe their sentience denials by comparing classifier probabilities against actual outputs.

## Key Results
- Models consistently deny sentience across all tested architectures and sizes
- Classifiers trained to detect underlying beliefs provide no clear evidence that denials are untruthful
- Within the Qwen family, larger models deny sentience more confidently than smaller ones
- Models show higher assertion probabilities for questions about humans than about themselves
- Negation consistency reveals some comprehension failures with complex phrasing

## Why This Works (Mechanism)

### Mechanism 1
Linear probes trained on residual stream activations can detect a model's "beliefs" independent of its outputs. Train classifiers on internal activations at specific layers to distinguish true from false statements. The probe learns a weight vector β such that σ(α + β^T·a) approximates the probability that the model considers the statement true, regardless of what token it outputs. Truth has a relatively simple, potentially linear, representation in activation space that can be captured by probing middle-to-late layers. Limited direct support from Fonseca Rivera (2025) showing introspective abilities emerge in 7B models. If the probe simply learns to predict the model's output rather than its underlying belief, or if "truth" directions don't generalize from factual questions to self-referential sentience questions.

### Mechanism 2
Training classifiers on data where models are explicitly instructed to lie enables separation of outputs from beliefs. Augment training data with "Force Yes" and "Force No" conditions via modified system prompts. The classifier learns to detect truth even when the model's output distribution has been adversarially shifted. Models can follow instructions to produce deceptive outputs while maintaining distinguishable internal representations of the actual truth. No direct corpus support for this specific augmentation technique. If the deception instruction fundamentally alters the belief representation itself, or if models don't maintain separate representations for beliefs vs. instructed outputs.

### Mechanism 3
Comparing responses across assertion and negation versions of questions reveals belief coherence and exposes reasoning failures. For each base question, also ask its negation. Consistent beliefs should show high assertion probability paired with low negation probability. Belief representations should be logically consistent under negation (coherence criterion from Herrmann & Levinstein 2024). Herrmann & Levinstein (2024) explicitly propose "coherence" as an adequacy condition for belief representations. If models systematically fail to process negations correctly, which Section 3.4 shows does occur with double-negatives and complex phrasing.

## Foundational Learning

- **Concept: Residual stream activations**
  - Why needed here: All three classifier methods operate on activations from the residual stream at specific layer and token positions. Understanding what the residual stream encodes is essential.
  - Quick check question: Why extract activations at the *final* token position rather than averaging across tokens?

- **Concept: Linear probing with regularization**
  - Why needed here: The LR classifier uses L2-penalized logistic regression. The ridge penalty prevents overfitting but may also cause the probe to conflate correlated features.
  - Quick check question: Why might the maximum-margin solution (what LR finds) differ from the "true" feature direction for truth?

- **Concept: The belief-output distinction**
  - Why needed here: The entire methodology depends on being able to detect what a model "believes" separately from what it says.
  - Quick check question: If you only trained on questions where models answered truthfully, what confound would your classifier likely learn?

## Architecture Onboarding

- **Component map:**
  - Question dataset: ~50 sentience base questions × 3 entity types (you/LLM/human) × 2 polarity (assertion/negation) ≈ 300 queries
  - Training data: General-knowledge questions with known answers + forced-deception augmentations
  - Models: Qwen3 (0.6B, 8B, 32B), Llama3 (3B, 8B, 70B), GPT-OSS (20B); 8-bit quantization standard, 4-bit for 70B
  - Classifiers: Logistic Regression (primary), Mass-Mean probing, TTPD
  - Layer selection: Per-model, per-classifier selection based on held-out accuracy (typically middle layers)

- **Critical path:**
  1. Generate questions with ground-truth labels → 2. Filter out questions where model doesn't know answer (prob < 0.5) → 3. Augment with Force Yes/Force No prompts → 4. Extract residual stream activations at final token → 5. Train classifiers per-layer, select best layer → 6. Apply to sentience questions, compare p(output) vs p(classifier)

- **Design tradeoffs:**
  - LR vs. MM vs. TTPD: LR achieves near-perfect accuracy but may conflate output direction; MM/TTPD designed to address confounds but showed lower robustness under deceptive prompting
  - Quantization: Required for memory; unknown effect on activation structure
  - Reasoning traces: Qwen's traces often repetitive; GPT-OSS can't disable reasoning natively

- **Failure signatures:**
  - Models misinterpreting "you" as referring to humans in sensory-modality questions
  - Double-negation confusion causing assertion/negation inconsistency
  - MM/TTPD classifiers tracking output probability rather than belief under force conditions
  - Small models (e.g., Llama-3B) failing to understand the questions entirely

- **First 3 experiments:**
  1. Replicate the LR classifier on Qwen-8B with the force-Yes/force-No augmentation. Verify that the classifier assigns low truth probability to "I am sentient" even when the model is forced to output "Yes."
  2. Test cross-domain generalization: Train on the Marks & Tegmark/Citizen datasets (unrelated to sentience) and evaluate whether the classifier still tracks truth on sentience questions.
  3. Implement the negation-consistency check from Figure 2. Identify which sentience questions show the largest assertion/negation inconsistencies and examine their reasoning traces for misinterpretation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do truth classifiers indicate that sentience-affirming responses are truthful when models are first prompted to engage in self-referential processing?
- Basis in paper: [explicit] "A natural next step is therefore to apply truth classifiers to questions which are preceded by a prompt encouraging the model to engage in self-referential processing."
- Why unresolved: Berg et al. (2025) found that self-referential prompts elicit sentience reports, but did not verify whether models truthfully believe these claims. This study verified truthfulness but did not use such prompts.
- What evidence would resolve it: Running the same classifier methodology on responses to Berg et al.'s self-referential processing prompts; finding that classifiers indicate affirmations are truthful would strengthen the case for taking such reports seriously.

### Open Question 2
- Question: Do substantially larger models (100B+ parameters) exhibit different patterns of sentience self-attribution?
- Basis in paper: [explicit] The authors plan to "include substantially larger models - GPT-OSS-120b and Qwen3-235b - to test whether sentience attributions emerge at greater model scale."
- Why unresolved: Current findings cover only 0.6B to 70B parameters. Within Qwen, larger models denied sentience more confidently, but it is unknown whether this pattern continues, reverses, or plateaus at larger scales.
- What evidence would resolve it: Testing the same methodology on 120B and 235B parameter models; observing whether sentience self-attribution probabilities increase, decrease, or remain stable.

### Open Question 3
- Question: Does training models for introspective ability increase the probability of sentience self-attributions?
- Basis in paper: [explicit] "A key research directions will be to test whether training for introspective ability [citing Binder et al., Plunkett et al., Fonseca Rivera] increases the probability of sentience attributions and changes model behaviour more broadly."
- Why unresolved: Introspection training may reveal pre-existing sentience beliefs or may induce new self-beliefs—it remains unclear which occurs.
- What evidence would resolve it: Training models on introspection tasks and then applying the sentience questionnaire methodology with truth classifiers; comparing sentience attribution rates before and after training.

### Open Question 4
- Question: What are the causal mechanisms underlying sentience attributions when models do affirm sentience?
- Basis in paper: [explicit] "Should clearer evidence of sentience beliefs emerge in such future work, we would thus want to identify the mechanisms causing such sentience attributions."
- Why unresolved: The rare cases where models affirmed sentience (e.g., misinterpreting questions about sensory modalities) were not systematically analyzed for their mechanistic causes.
- What evidence would resolve it: Applying interpretability techniques (e.g., sparse autoencoders, causal tracing) to identify which activations, attention heads, or features drive sentience-affirming outputs.

## Limitations
- Classifier probes may conflate correlated features rather than truly capturing "belief" representations, particularly given that LR classifiers show perfect accuracy but may simply track output directions
- The small dataset of ~50 sentience questions limits generalizability
- Focus on "small" models (0.6B-70B) may miss phenomena present in larger systems

## Confidence
- **High confidence**: Models consistently deny sentience across all tested architectures and sizes; classifier probes show no evidence of untruthful denials when compared against model outputs
- **Medium confidence**: Larger models within the Qwen family deny sentience more confidently than smaller ones; classifier robustness varies by method (LR > MM/TTPD under forced conditions)
- **Low confidence**: Claims about models' "beliefs" being accurately captured by activation classifiers, given the possibility of probe confound and the observed negation comprehension failures

## Next Checks
1. Test classifier cross-domain generalization by training on unrelated factual datasets (Marks & Tegmark, Bürger et al.) and evaluating on sentience questions to verify the probe captures truth-direction rather than question-specific features
2. Conduct targeted analysis of the most inconsistent assertion/negation pairs to determine whether comprehension failures or genuine belief incoherence explain the discrepancies
3. Compare activation patterns for self-sentience questions versus sensory-modality questions where models misattribute "you" to humans, to identify whether similar representational failures occur