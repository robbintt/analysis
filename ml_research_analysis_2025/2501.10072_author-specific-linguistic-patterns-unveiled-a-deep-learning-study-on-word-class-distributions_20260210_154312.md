---
ver: rpa2
title: 'Author-Specific Linguistic Patterns Unveiled: A Deep Learning Study on Word
  Class Distributions'
arxiv_id: '2501.10072'
source_url: https://arxiv.org/abs/2501.10072
tags:
- bigram
- authors
- patterns
- linguistic
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates author-specific linguistic patterns using
  deep learning and part-of-speech (POS) tagging. The authors classify literary authors
  based on POS-tag vectors and bigram frequency matrices extracted from 193 works
  by 76 authors.
---

# Author-Specific Linguistic Patterns Unveiled: A Deep Learning Study on Word Class Distributions

## Quick Facts
- arXiv ID: 2501.10072
- Source URL: https://arxiv.org/abs/2501.10072
- Reference count: 28
- Primary result: Bigram-based CNN models achieve 0.59 test accuracy in author classification versus 0.44 for unigram models

## Executive Summary
This study investigates author-specific linguistic patterns using deep learning and part-of-speech (POS) tagging. The authors classify literary authors based on POS-tag vectors and bigram frequency matrices extracted from 193 works by 76 authors. Two neural network architectures—a fully connected network for unigrams and a convolutional neural network (CNN) for bigrams—are developed and compared. Results show that bigram-based models significantly outperform unigram models, achieving a test accuracy of 0.59 versus 0.44 for POS-tag vectors. Multi-dimensional scaling (MDS) visualizations reveal clear clustering of authors' works, with bigrams capturing more distinctive stylistic patterns.

## Method Summary
The study extracts POS-tag unigrams and bigrams from German literary texts using spaCy's German POS tagger. Two neural architectures are implemented: a fully connected network processes 11-dimensional unigram frequency vectors, while a CNN processes 11x11 bigram frequency matrices. The CNN uses two convolutional layers with ReLU activation and max-pooling, followed by dense layers. Models are trained to classify texts by author, with results evaluated using accuracy metrics and MDS visualizations of learned embeddings.

## Key Results
- Bigram-based CNN models achieve 0.59 test accuracy versus 0.44 for unigram models
- MDS visualizations show tighter author clustering for bigram features compared to unigrams
- Training accuracy reaches 0.81 for bigrams but drops to 0.59 on test data, indicating some overfitting

## Why This Works (Mechanism)

### Mechanism 1: Sequential Pattern Encoding via Bigram Matrices
Authorial style is more effectively captured by sequential word class combinations (bigrams) than by isolated POS frequencies. A CNN processes 11x11 bigram frequency matrices, extracting local spatial patterns that represent transitional probabilities between word classes. Max-pooling layers retain the most discriminative local patterns while providing positional invariance. Dense layers integrate these features for classification, creating an embedding space where texts by the same author share similar sequential stylistic traits.

### Mechanism 2: Syntactic Fingerprinting via Unigram Distributions
A coarse-grained authorial style can be identified using the relative frequencies of individual POS tags. A fully connected network takes an 11-dimensional POS-tag unigram frequency vector as input and learns non-linear combinations of these frequencies through successive dense layers with ReLU activations. The model associates specific combinations of POS tag proportions with individual authors—for instance, learning that one author uses higher adjective-to-verb ratios.

### Mechanism 3: Visualization of Learned Stylistic Embeddings
The internal representations learned by the neural networks can be visualized to show meaningful clustering by author, with bigram-based embeddings providing tighter clusters. Activations from the final hidden layer are extracted for each input text, creating high-dimensional vectors that represent the model's internal stylistic embedding. Multi-dimensional Scaling (MDS) projects these onto a 2D plane, preserving pairwise distances.

## Foundational Learning

- **Part-of-Speech (POS) Tagging**: Fundamental feature extraction step categorizing each word into grammatical classes. Quick check: Can you explain why the authors chose to analyze POS tags rather than raw word frequencies?
- **N-grams (specifically Bigrams)**: Understanding sequential word pairs is central to the paper's main finding. Quick check: What additional information does a bigram matrix capture compared to a unigram frequency vector?
- **Convolutional Neural Networks (CNNs) for Sequence Data**: The architecture choice for bigram processing. Quick check: Why would a CNN be more appropriate than a fully connected network for processing a 2D bigram frequency matrix?

## Architecture Onboarding

- **Component map**: Input Text → POS Tagger (spaCy German model) → Feature Extraction → Fully Connected Network → 8-class softmax (unigram path) or CNN (Conv2D + MaxPool) → Flatten → Dense layers → 8-class softmax (bigram path)
- **Critical path**: The bigram-to-classification pipeline: Raw text → POS tags → 11x11 bigram matrix → CNN feature extraction → Dense classification layers → Author probabilities
- **Design tradeoffs**: Unigram features are simpler (11 dimensions vs 121 for bigrams) but sacrifice sequential information, resulting in ~15% lower test accuracy. Fixed POS tag set (11 tags) limits granularity. Author count reduced to 8 for classification from 76 total.
- **Failure signatures**: Large gap between training and test accuracy (0.81 vs 0.59 for bigrams) indicates overfitting despite dropout regularization. MDS clusters with high variance suggest authorial inconsistency or insufficient feature discrimination.
- **First 3 experiments**:
  1. Reproduce baseline with subset: Take 8 most frequent authors, replicate the bigram CNN pipeline, verify you achieve comparable test accuracy (~0.59)
  2. Ablation study on sequence length: Compare bigrams vs trigrams vs 4-grams to determine if longer sequential patterns further improve classification
  3. Architecture variant test: Replace the simple CNN with a 1D CNN that processes the original POS tag sequence directly to test whether learned sequential patterns outperform handcrafted bigram features

## Open Questions the Paper Calls Out
- Can advanced neural architectures like RNNs or Transformers significantly improve author classification performance compared to the CNNs used in this study?
- Does the integration of metadata (e.g., genre, publication dates) with linguistic patterns substantially enhance classification accuracy?
- Are the findings regarding the superiority of bigram features over unigrams generalizable to languages with different morphological structures via transfer learning?

## Limitations
- The German-language focus raises questions about cross-linguistic applicability
- Reduction from 76 to 8 authors for main classification task suggests scalability challenges
- Relatively modest test accuracy (0.59) remains only marginally better than random chance for 8-class classification

## Confidence
- **High confidence**: Superiority of bigram features over unigram features is well-supported by 15% accuracy gap and visual MDS evidence
- **Medium confidence**: Interpretation that sequential POS patterns capture authorial style more effectively than individual tag frequencies requires further validation
- **Low confidence**: Scalability claims to larger author sets are weakly supported given reduction from 76 to 8 authors

## Next Checks
1. **Cross-linguistic validation**: Replicate the bigram CNN pipeline on English and Romance language corpora to test whether POS-based sequential patterns generalize across linguistic families
2. **Genre and temporal consistency test**: Analyze the same authors' works across different genres and time periods to quantify how authorial style stability affects classification accuracy
3. **Human benchmark comparison**: Conduct a human expert study where trained linguists classify the same texts using POS patterns alone, comparing human accuracy against the 0.59 CNN benchmark