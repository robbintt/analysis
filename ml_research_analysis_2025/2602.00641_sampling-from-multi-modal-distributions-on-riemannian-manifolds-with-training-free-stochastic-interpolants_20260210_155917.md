---
ver: rpa2
title: Sampling from multi-modal distributions on Riemannian manifolds with training-free
  stochastic interpolants
arxiv_id: '2602.00641'
source_url: https://arxiv.org/abs/2602.00641
tags:
- sampling
- riemannian
- section
- distribution
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a training-free sampling method for multi-modal
  distributions on Riemannian manifolds, inspired by diffusion models. The key idea
  is to construct a stochastic interpolant between an easy-to-sample noise distribution
  and the target, and then simulate a deterministic dynamics whose marginal distributions
  follow this path.
---

# Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants

## Quick Facts
- **arXiv ID:** 2602.00641
- **Source URL:** https://arxiv.org/abs/2602.00641
- **Reference count:** 40
- **Primary result:** Training-free sampling method for multi-modal distributions on Riemannian manifolds using stochastic interpolants and iterative posterior sampling

## Executive Summary
This paper introduces FRIPS (Flow-based Riemannian Interpolant Posterior Sampling), a novel training-free method for sampling from multi-modal distributions on Riemannian manifolds. The approach constructs a deterministic ODE whose marginal distributions follow a stochastic interpolant path from a simple noise distribution to the target distribution. Unlike existing methods that require extensive training of neural networks, FRIPS estimates the velocity field on-the-fly using Monte Carlo integration through MCMC, rejection sampling, or importance sampling. The method is particularly effective for complex, high-dimensional, multi-modal targets where traditional MCMC methods fail due to mode trapping.

## Method Summary
FRIPS constructs a stochastic interpolant $X_t$ connecting a noise distribution at $t=0$ to the target distribution at $t=1$ using geodesic interpolation on Riemannian manifolds. The key insight is that a deterministic ODE $\dot{x}_t = u_t(x_t)$ can transport samples along this path if the velocity field $u_t$ is defined as the expectation of the Riemannian logarithm under the denoising posterior $\pi_{1|t}$. Rather than training a neural network to approximate this intractable velocity field, FRIPS uses iterative posterior sampling (via MCMC, rejection sampling, or importance sampling) to estimate $u_t$ on-the-fly through Monte Carlo integration. The method starts at an intermediate time $t_0 > 0$ to avoid the hardness of sampling directly from the multi-modal target, and simulates the ODE forward to generate samples from the target distribution.

## Key Results
- FRIPS achieves 1.27% relative error in estimating dominant mode weight for a 32-dimensional sphere mixture, compared to 22.88% for MALA
- Outperforms standard samplers on multi-modal distributions across spheres and Grassmann manifolds
- Particularly effective for heavy-tailed targets where traditional MCMC methods fail due to mode trapping
- Training-free approach eliminates the need for expensive neural network pretraining required by other stochastic interpolant methods

## Why This Works (Mechanism)

### Mechanism 1: Riemannian Markovian Projection
- **Claim:** A deterministic ODE can transport a noise distribution to a complex target distribution on a manifold if the velocity field is defined as the expectation of a geodesic interpolant.
- **Mechanism:** The method defines a stochastic interpolant $X_t = \text{Exp}_{X_1}((1-t)\text{Log}_{X_1} X_0)$ connecting noise $X_0$ and target $X_1$. It derives a time-dependent vector field $u_t(x_t) = \mathbb{E}[\text{Log}_{x_t} X_1 / (1-t) \mid X_t=x_t]$ such that simulating the ODE $\dot{x}_t = u_t(x_t)$ preserves the marginal path $\pi_t$ from noise to target.
- **Core assumption:** The manifold is geodesically complete, and the coupling $\pi_{0,1}$ admits a density.
- **Evidence anchors:**
  - [Abstract] "construct a stochastic interpolant... and then simulate a deterministic dynamics whose marginal distributions follow this path."
  - [Section 2.3] Prop. 3 establishes that the deterministic process has the same time-marginal distributions as the interpolation process.
  - [Corpus] Related work "Riemannian Neural Geodesic Interpolant" supports the viability of geodesic interpolants for density transport.
- **Break condition:** The geodesic interpolation is undefined if $X_0$ lies in the cut locus of $X_1$ (though this occurs with probability zero for continuous densities).

### Mechanism 2: On-the-fly Velocity Estimation via Posterior Sampling
- **Claim:** The intractable velocity field $u_t$ (typically learned via neural networks in generative modeling) can be estimated on-the-fly using Monte Carlo integration against the denoising posterior $\pi_{1|t}$.
- **Mechanism:** Instead of training a network to approximate $u_t$, the algorithm treats the velocity as an expectation. It uses iterative sampling methods (MCMC, Rejection, Importance Sampling) to draw samples from the posterior $\pi_{1|t}$ and computes a Monte Carlo average of the Riemannian logarithm $\text{Log}_{x_t} x_1$.
- **Core assumption:** The denoising posterior $\pi_{1|t}$ is significantly easier to sample from than the target $\pi_1$ as $t \to 1$.
- **Evidence anchors:**
  - [Abstract] "uses iterative posterior sampling via MCMC, rejection sampling, or importance sampling to estimate the velocity field on the fly."
  - [Section 3.1] "estimating the vector field... expressed as conditional expectations, using standard MCMC tools."
  - [Corpus] "Sampling in High-Dimensions using Stochastic Interpolants..." discusses similar training-free stochastic interpolant approaches, validating the general strategy.
- **Break condition:** If the Monte Carlo proposal fails to mix (e.g., high variance in Importance Sampling or low acceptance in Rejection Sampling), the velocity estimate will be biased, accumulating error over time.

### Mechanism 3: Intermediate Start Time ($t_0$) for Mode Trapping
- **Claim:** Starting the simulation at an intermediate time $t_0 > 0$ mitigates mode trapping by ensuring the first posterior sampling problem is already conditioned on a "simpler" distribution.
- **Mechanism:** If $t=0$, the posterior $\pi_{1|0}$ equals the multi-modal target $\pi_1$, making the first velocity estimation step as hard as the original problem. By starting at $t_0 > 0$, the initial distribution $\pi_{t_0}$ is a "noised" version of the target, and the posterior $\pi_{1|t_0}$ contracts around the current mode, facilitating efficient sampling.
- **Core assumption:** There exists a "sweet spot" interval $(t_S, t_C)$ where $\pi_{t_0}$ is easy to sample for initialization, but $\pi_{1|t_0}$ is unimodal enough for posterior sampling.
- **Evidence anchors:**
  - [Section 3.1] "the starting time $t_0$ in $[0,1)$ should be carefully chosen such that estimating the velocity field $u$ and computing the initial state $X_0$ are both of relative difficulty."
  - [Section 4.1] "Heuristic estimation of $t_0$... granted prior knowledge about the target distribution."
  - [Corpus] No direct corpus evidence for the specific $t_0$ heuristic in this Riemannian context.
- **Break condition:** If $t_0$ is too large, initialization fails (requires sampling near the target); if $t_0$ is too small, posterior sampling fails (multi-modality remains).

## Foundational Learning

- **Concept:** Riemannian Geometry (Exp/Log maps, Geodesics)
  - **Why needed here:** Standard vector arithmetic fails on manifolds. You must understand how to map tangent vectors to manifold points (Exponential map) and vice versa (Logarithm map) to define the velocity field and update steps.
  - **Quick check question:** Can you explain why $\text{Log}_x y$ represents the initial velocity of the geodesic from $x$ to $y$?

- **Concept:** Stochastic Interpolants / Flow Matching
  - **Why needed here:** The core architecture relies on constructing a continuous probability path between distributions. You need to grasp how an ODE can model this transport without stochastic diffusion noise.
  - **Quick check question:** How does a stochastic interpolant differ from a standard diffusion process in terms of the underlying dynamics (ODE vs SDE)?

- **Concept:** Markov Chain Monte Carlo (MCMC) - specifically MALA
  - **Why needed here:** The "training-free" aspect hinges on using MCMC to sample from the posterior $\pi_{1|t}$ inside the simulation loop.
  - **Quick check question:** Why might a Metropolis-Adjusted Langevin Algorithm (MALA) fail to mix on a multi-modal distribution compared to a unimodal one?

## Architecture Onboarding

- **Component map:** Initialization ($t_0$) -> Posterior Sampler -> Velocity Estimator -> ODE Solver
- **Critical path:** The selection of $t_0$. If this is wrong, the system either fails to initialize or the inner MCMC loop fails to converge. The heuristic in Section 4.1 is the guide.
- **Design tradeoffs:**
  - **Rejection Sampling (RS) vs. Importance Sampling (IS):** RS is unbiased but computationally variable (potentially infinite cost); IS has fixed cost but introduces variance and bias via self-normalization.
  - **MALA vs. RS:** MALA is robust but requires gradient information ($\nabla \log q_1$); RS is gradient-free but requires an upper bound on the density.
- **Failure signatures:**
  - **Posterior Collapse:** If $t_0$ is too small, the inner MCMC fails to mix, velocity estimates become biased, and samples collapse to a single mode.
  - **Initialization Drift:** If $t_0$ is too large, the initialization algorithm fails to generate diverse samples, leading to under-representation of modes.
- **First 3 experiments:**
  1. **Sphere Mixture Model Validation:** Replicate the $S^d$ mixture experiment to verify the FRIPS pipeline against standard MALA (Table 1).
  2. **$t_0$ Ablation (Sweet Spot Search):** Implement the "Return Accuracy" heuristic (Algorithm 9) to identify $t_S$ and $t_C$ for a custom target, validating the $t_0$ sensitivity.
  3. **Posterior Sampler Comparison:** Swap the posterior sampling backend (MALA vs. IS vs. RS) on a high-dimensional target to observe the trade-off between bias and computational cost.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes geodesically complete manifolds and continuous density couplings
- Heuristic for selecting intermediate start time $t_0$ relies on prior knowledge about target distribution
- Performance depends critically on inner MCMC/IS/RS samplers' ability to mix in high dimensions
- Computational costs can be prohibitive when posterior sampling requires many iterations

## Confidence

- **High Confidence:** The deterministic ODE formulation preserves the correct marginal distributions (Proposition 3), and the method demonstrably outperforms MALA on benchmark multi-modal spherical and Grassmannian distributions.
- **Medium Confidence:** The heuristic for selecting $t_0$ is reasonable and theoretically motivated, but lacks extensive empirical validation across diverse target distributions. The trade-off between different posterior sampling methods (MCMC vs IS vs RS) is discussed but not comprehensively benchmarked.
- **Low Confidence:** The method's scalability to very high dimensions (>100) and its robustness to targets with complex geometric structures (e.g., non-trivial curvature) are not thoroughly investigated. The impact of discretization error in the ODE solver on final sample quality is not quantified.

## Next Checks

1. **High-Dimensional Scaling Test:** Apply FRIPS to a 100-dimensional sphere mixture model and measure the degradation in mode weight estimation accuracy compared to the 32-dimensional case. Track the inner MCMC iteration count and posterior sample variance as dimensionality increases.

2. **Non-Standard Manifold Validation:** Test FRIPS on a manifold with non-constant curvature (e.g., hyperbolic space or a high-dimensional ellipsoid) to verify that the geodesic-based interpolant remains valid and efficient when the underlying geometry deviates significantly from the sphere.

3. **Posterior Sampler Stress Test:** Create a target distribution where the modes are connected by narrow channels (a "snake-like" structure). Compare the performance of MALA, IS, and RS as the posterior sampling backend, measuring both the quality of velocity estimates (bias) and computational cost (wall-clock time).