---
ver: rpa2
title: 'SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text
  Adapters'
arxiv_id: '2508.00213'
source_url: https://arxiv.org/abs/2508.00213
tags:
- text
- semantic
- segmentation
- adapter
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAM-PTx, a parameter-efficient method for
  adapting the Segment Anything Model (SAM) by injecting frozen CLIP-derived text
  embeddings into SAM's image encoder. The approach uses lightweight adapters called
  Parallel-Text Adapters, which inject semantic information into the MLP-parallel
  branch of each transformer block, preserving the attention pathway for spatial reasoning.
---

# SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters

## Quick Facts
- **arXiv ID**: 2508.00213
- **Source URL**: https://arxiv.org/abs/2508.00213
- **Reference count**: 30
- **Primary result**: Parameter-efficient SAM adaptation using frozen CLIP text embeddings injected into MLP-parallel adapters achieves consistent improvements over spatial-prompt-only baselines across COCO, ADE20K, and COD10K datasets.

## Executive Summary
SAM-PTx introduces a parameter-efficient method for adapting the Segment Anything Model (SAM) to class-aware segmentation tasks by injecting frozen CLIP-derived text embeddings into SAM's image encoder. The approach uses lightweight Parallel-Text Adapters that modify only the MLP-parallel branch of each transformer block, preserving the attention pathway for spatial reasoning while enriching token representations with semantic information. This enables effective class-aware segmentation with minimal computational overhead. Experiments on low-data subsets of COCO, ADE20K, and COD10K demonstrate consistent improvements over purely spatial prompt baselines, with qualitative analysis showing enhanced segmentation quality for edge cases, small objects, and unprompted instances.

## Method Summary
SAM-PTx adapts SAM (ViT-B) for class-aware binary segmentation by injecting frozen CLIP text embeddings into the image encoder's MLP-parallel adapters. The method uses Parallel-Text Adapters that project text embeddings to visual token dimensions and add them to visual inputs before processing through a bottleneck MLP, while preserving the attention pathway for spatial reasoning. The model is trained on low-data subsets (COCO 1_512, ADE20K 1_64, COD10K) with 5 foreground point prompts per object, using binary cross-entropy loss on 512×512 masks. Only the adapter weights and mask decoder are trainable, keeping most of SAM's architecture frozen for efficiency.

## Key Results
- On ADE20K 1_64, SAM-PTx achieves 71.38 mIoU compared to 71.29 for the non-text-enhanced Parallel Adapter
- On COD10K, SAM-PTx achieves an MAE of 0.0206, outperforming the 0.0213 retrained parallel adapter baseline
- Ablation shows image encoder injection (71.38 mIoU) outperforms prompt encoder (71.11) and mask decoder (70.82) injection
- MLP-only text injection (71.38 mIoU) outperforms MLP+MHSA text injection (71.25 mIoU), preserving spatial reasoning

## Why This Works (Mechanism)

### Mechanism 1
Injecting frozen CLIP text embeddings into the MLP-parallel branch improves class-aware segmentation while preserving SAM's spatial reasoning. Text embeddings are projected to visual token dimension via Act(W_t·t), added to visual input x' = x + t̃, then processed through a bottleneck MLP. This enriches token representations with class-level semantics without modifying the attention pathway responsible for spatial prompt propagation.

### Mechanism 2
Injecting semantics into the image encoder outperforms injection into the prompt encoder or mask decoder. Early-stage conditioning in the image encoder shapes visual feature representations throughout the pipeline, whereas late-stage injection provides weaker integration with visual processing.

### Mechanism 3
Confining text conditioning to MLP-parallel adapters (not MHSA-parallel) avoids degrading spatial prompt propagation. The attention pathway is preserved for spatial reasoning; adding text to MHSA may introduce interference that disrupts how point/box prompts propagate across spatial locations.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT) with Adapters**: Understanding bottleneck adapters (Adapter(x) = x + W_up · σ(W_down · x)) and the parallel injection pattern is prerequisite to modifying them for text conditioning. *Quick check: Can you explain why this formulation preserves the original pathway while adding trainable capacity?*

- **Vision-Language Alignment (CLIP embeddings)**: CLIP ViT-B/32 text embeddings provide the semantic signal. The paper assumes these embeddings encode meaningful class-level semantics in a shared vision-language space. *Quick check: Why are CLIP text embeddings suitable for semantic conditioning without joint training?*

- **Transformer Block Structure (MLP vs. Attention roles)**: The design choice to inject text only into MLP-parallel (not MHSA-parallel) relies on understanding that MLP processes tokens independently while attention propagates spatial information. *Quick check: What would happen to spatial prompt propagation if text were injected into the attention pathway?*

## Architecture Onboarding

- **Component map**: Image I + spatial prompt p (point) + class label ℓ → CLIP text embedding → Image encoder (ViT-B) with frozen weights except Parallel-Text Adapters → Mask decoder → Binary mask M

- **Critical path**: 1) Precompute CLIP text embeddings per class 2) Forward pass: Inject projected text into MLP-parallel adapter at each transformer block 3) Spatial prompt processed normally through prompt encoder 4) Decoder produces mask guided by both modalities 5) Loss: BCE(M, M_gt)

- **Design tradeoffs**: MLP-only vs. MLP+MHSA text injection (paper shows MLP-only better, 71.38 vs 71.25 mIoU), preserving spatial reasoning; Image encoder vs. prompt encoder vs. decoder injection (image encoder optimal); Frozen vs. trainable text embeddings (frozen chosen for efficiency)

- **Failure signatures**: Performance drop vs. parallel adapter baseline (check text projection dimension mismatch), mask leakage to adjacent objects (may indicate insufficient semantic conditioning), unprompted instances not segmented (expected for baseline; SAM-PTx should improve this), MAE degradation on COD10K (verify adapter weights initialized correctly)

- **First 3 experiments**: 1) Reproduce parallel adapter baseline without text to confirm 71.29 mIoU; then add text conditioning to verify 71.38 mIoU improvement 2) Ablate injection location: Train three variants injecting text into (a) image encoder, (b) prompt encoder, (c) mask decoder. Confirm image encoder superiority 3) Test on COD10K with single class ("camouflaged object"): Verify MAE approaches 0.0206 and compare against 0.0213 retrained parallel adapter baseline

## Open Questions the Paper Calls Out
- **Open Question 1**: Would joint fine-tuning of CLIP alongside SAM-PTx improve semantic alignment compared to using frozen CLIP embeddings? *Basis: Conclusion states plan to explore joint training with CLIP*
- **Open Question 2**: Can more expressive text prompts (detailed descriptions, attributes, or referring expressions) improve segmentation beyond simple class name prompts? *Basis: Conclusion states plan to explore "more expressive text prompts"*
- **Open Question 3**: How does SAM-PTx scale to full dataset regimes and multi-class semantic segmentation? *Basis: Paper evaluates only low-data subsets and formulates task as binary segmentation per instance*
- **Open Question 4**: How does text-guided adaptation interact with other spatial prompt types (boxes, masks)? *Basis: Paper uses only point prompts during training and inference*

## Limitations
- The reported improvements (71.38 vs 71.29 mIoU, 0.0206 vs 0.0213 MAE) are relatively small margins that could be within experimental variance, particularly given the low-data regime
- All experiments use binary segmentation with single-class labels; performance on multi-class segmentation or more complex semantic relationships remains untested
- The paper does not explore alternative adapter designs beyond the parallel adapter framework from SU-SAM, nor does it investigate different text embedding models beyond CLIP ViT-B/32

## Confidence
- **High confidence**: The core architectural design (injecting frozen text embeddings into MLP-parallel adapters while preserving attention pathway) is technically sound and the experimental results show consistent directional improvements over baselines
- **Medium confidence**: The specific performance numbers and the relative ranking of different injection locations are supported by the reported experiments, but the small margins between configurations suggest these results may not be fully robust
- **Low confidence**: The claims about superiority for edge cases, small objects, and unprompted instances are primarily supported by qualitative analysis rather than comprehensive quantitative evaluation

## Next Checks
1. Run 5 different random seeds for the ADE20K 1_64 experiment and report mean±std for both SAM-PTx (71.38) and parallel adapter baseline (71.29) to establish whether the 0.09 mIoU improvement is statistically significant
2. Test SAM-PTx on a held-out class from COCO (not in the 1_512 subset) to verify the method's ability to generalize to unseen classes using only the semantic conditioning from frozen CLIP embeddings
3. Implement a simple multi-class version of SAM-PTx by providing multiple class embeddings simultaneously and measure performance degradation compared to the binary setup, establishing practical limits of the approach