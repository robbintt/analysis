---
ver: rpa2
title: 'OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian
  Tracking and Understanding'
arxiv_id: '2511.17053'
source_url: https://arxiv.org/abs/2511.17053
tags:
- tracking
- object
- image
- objects
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OmniPT, a large vision language model (LVLM)-based
  framework that unifies four pedestrian tracking tasks: traditional multiple object
  tracking (MOT), referring MOT, cross-view referring MOT, and semantic MOT. The core
  innovation lies in adapting LVLMs for instance-level tasks by addressing two challenges:
  modeling tracking as natural language tasks and enforcing structured bounding box
  outputs.'
---

# OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding

## Quick Facts
- arXiv ID: 2511.17053
- Source URL: https://arxiv.org/abs/2511.17053
- Authors: Teng Fu, Mengyang Zhao, Ke Niu, Kaixin Peng, Bin Li
- Reference count: 10
- Primary result: LVLM-based framework achieving state-of-the-art performance across four pedestrian tracking tasks

## Executive Summary
This paper introduces OmniPT, a unified framework that leverages large vision language models (LVLMs) to handle four diverse pedestrian tracking tasks: traditional multiple object tracking, referring tracking, cross-view referring tracking, and semantic tracking. The approach addresses two key challenges in adapting LVLMs for instance-level tasks: standardizing bounding box output formats and incorporating tracking-specific semantic understanding. Through a carefully designed four-stage training pipeline involving reinforcement learning, mid-training with pedestrian-specific datasets, supervised fine-tuning, and final RL refinement, OmniPT achieves state-of-the-art results across all four tasks while enabling instruction-guided tracking.

## Method Summary
OmniPT adapts LVLMs for tracking by first using GRPO reinforcement learning to enforce structured bounding box outputs, then mid-training on pedestrian-specific datasets to build semantic understanding through detection, location prediction, and re-identification tasks. The model undergoes supervised fine-tuning on specific tracking datasets before a final RL phase for instruction-following refinement. For long sequences, the framework employs iterative multi-turn VQA queries where tracking results from previous rounds serve as priors for subsequent rounds, effectively extending inference beyond the model's training context window.

## Key Results
- Achieves HOTA score of 75.04 on BenSMOT dataset
- Reaches HOTA score of 56.4 on DanceTrack benchmark
- Demonstrates substantial improvements in semantic understanding metrics across all four tracking tasks
- Shows state-of-the-art performance while unifying diverse tracking paradigms under a single LVLM framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning with GRPO enforces structured bounding box outputs in LVLMs for tracking tasks.
- Mechanism: GRPO samples multiple outputs and assigns reward scores based on format compliance and IoU accuracy. The reward function prioritizes correct `<bbox>x,y,w,h</bbox>` format over raw accuracy in early training, then relaxes this bias later.
- Core assumption: LVLMs can be conditioned to produce structured outputs via reward-shaped RL without catastrophic forgetting of visual understanding.
- Evidence anchors:
  - [abstract]: "we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format"
  - [section]: Reward function maps IoU to 0.5-1 range in stage 1 "so that the model would pay more attention to the format at this stage"
  - [corpus]: No direct corpus evidence for GRPO in tracking; related work CrowdTrack focuses on traditional appearance/motion features without RL formatting.
- Break condition: If the model's output vocabulary lacks sufficient tokens to represent precise coordinates, or if format rewards dominate too strongly and suppress tracking accuracy.

### Mechanism 2
- Claim: Mid-training with pedestrian-specific proxy tasks transfers semantic understanding to instance-level tracking.
- Mechanism: Three pretext tasks—object detection, location prediction, and person re-identification—are trained on SYNTH-PEDES and DanceTrack. Location prediction provides first-frame coordinates and requires next-frame prediction, implicitly training re-identification and temporal localization.
- Core assumption: Semantic representations learned from image-text alignment transfer to spatiotemporal tracking without explicit trajectory modeling.
- Evidence anchors:
  - [section]: "Mid training in this stage aims to enhance the model's sensitivity to language descriptions of tracked objects and improve the model's performance in all three aforementioned aspects"
  - [section]: "We also include special samples where the object disappears mid-trajectory, significantly improving the model's robustness"
  - [corpus]: CrowdTrack paper notes traditional methods struggle in complex scenarios, supporting the need for semantic augmentation.
- Break condition: If mid-training data distribution diverges significantly from downstream task distributions (e.g., general persons vs. surveillance-specific poses), transfer may degrade.

### Mechanism 3
- Claim: Decomposing tracking into multi-turn VQA queries enables iterative long-term inference beyond training sequence lengths.
- Mechanism: Training uses short sequences (up to 16-32 images), but inference employs iterative multi-round dialogues where tracking results from the last image of the previous round serve as priors for the next round.
- Core assumption: The model maintains consistent object identity across dialogue turns without explicit memory mechanisms beyond context window.
- Evidence anchors:
  - [section]: "To achieve long-term tracking during inference, we employ iterative multi-round dialogues. The tracking result from the last image in the sequence from the previous dialogue round serves as the initial prior information for the new round"
  - [section]: Table 7 shows performance degrades (N/A) when tracking tasks use 16+ images per sample, indicating context-length constraints.
  - [corpus]: No corpus evidence directly addresses iterative VQA for tracking; this appears novel to this framework.
- Break condition: If context window overflows during long videos, or if object descriptions drift across dialogue rounds without explicit re-grounding.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables lightweight RL for format enforcement without a separate reward model; samples multiple outputs and computes group-relative advantages.
  - Quick check question: Can you explain how GRPO differs from PPO in terms of reward model requirements?

- Concept: **Mid-training vs. Pre-training vs. Fine-tuning**
  - Why needed here: The paper explicitly distinguishes mid-training as bridging general pre-training and task-specific fine-tuning with domain-relevant proxy tasks.
  - Quick check question: What distinguishes mid-training proxy tasks from standard pre-training objectives?

- Concept: **Multi-object Tracking Metrics (HOTA, IDF1, MOTA)**
  - Why needed here: Paper reports HOTA as primary metric; understanding DetA (detection accuracy) vs. AssA (association accuracy) is essential for diagnosing failure modes.
  - Quick check question: Why might HOTA increase while MOTA remains flat, and what does this indicate about detection vs. association?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Patch Merger -> Qwen2.5 LM backbone -> Conv 3D + Window Attention -> Full Attention layers -> Text Encoder

- Critical path:
  1. Stage 1 (RL): GRPO on BenSMOT with visual grounding proxy task → format compliance
  2. Stage 2 (Mid-training): CLIP alignment + pretext tasks on SYNTH-PEDES/DanceTrack → semantic + re-identification
  3. Stage 3 (SFT): Task-specific queries on tracking datasets → task alignment
  4. Stage 4 (RL): Final GRPO pass → instruction-following refinement

- Design tradeoffs:
  - Dynamic resolution (Qwen2.5-VL) vs. fixed resolution: Paper attributes performance gains to dynamic resolution capturing semantic features more clearly (Table 5)
  - Fewer images per sample vs. more inference rounds: Table 7 shows 8 images optimal for tracking; more images cause context overflow
  - LoRA fine-tuning vs. full fine-tuning: Paper uses LoRA for efficiency; full fine-tuning may improve performance but at higher cost

- Failure signatures:
  - Scenes with 200+ objects (MOT20): LVLM struggles to locate all objects due to output token limits
  - Long sequences (>16 images for tracking): Model cannot converge, returns N/A
  - Format drift in RL: If stage-1 format rewards are too weak, bounding box outputs become inconsistent

- First 3 experiments:
  1. **Format compliance baseline**: Run stage-1 RL with GRPO reward function (Eq. 1-2) on a small subset of BenSMOT; verify bounding box format consistency across 100 samples before proceeding to mid-training.
  2. **Mid-training ablation**: Train with and without location prediction pretext task; measure HOTA delta on DanceTrack validation to confirm re-identification transfer (expect ~3-5 HOTA improvement based on Table 6).
  3. **Image length sweep**: Test inference with 2, 4, 8, 16 images per round on BenSMOT; plot HOTA/MOTA vs. length to identify context-collapse threshold before scaling to longer videos.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades substantially in high-object-count scenarios (e.g., MOT20 with 200+ pedestrians), suggesting token-length limitations
- Iterative inference approach for long sequences remains unverified beyond 16 frames per round
- Mid-training proxy tasks assume transfer without empirical validation of which specific task contributes most to tracking performance

## Confidence

- **High confidence**: Format enforcement through GRPO works as described (supported by explicit reward function formulation and stage-1 training objectives)
- **Medium confidence**: Mid-training semantic transfer improves tracking (reasonable assumption given CLIP-style alignment, but transfer mechanisms not fully dissected)
- **Low confidence**: Iterative VQA approach scales to real-world long sequences (inference-only claim without validation on sequences >16 frames)

## Next Checks

1. **Token budget stress test**: Run OmniPT on MOT20 benchmark with progressive pruning of output tokens; measure HOTA degradation curve to identify hard token limits and determine if performance collapse is abrupt or gradual.

2. **Mid-training task ablation with pretraining**: Isolate each pretext task (detection, location prediction, re-identification) by training separate models; compare DanceTrack performance to determine which semantic component contributes most to tracking gains.

3. **Long-sequence iterative inference validation**: Implement OmniPT on continuous 64-frame pedestrian tracking sequences using the proposed multi-round VQA approach; measure tracking drift and identity consistency across rounds to verify the iterative approach maintains accuracy beyond the 16-frame training limit.