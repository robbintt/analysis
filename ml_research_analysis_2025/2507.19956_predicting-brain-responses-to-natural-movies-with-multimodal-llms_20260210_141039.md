---
ver: rpa2
title: Predicting Brain Responses To Natural Movies With Multimodal LLMs
arxiv_id: '2507.19956'
source_url: https://arxiv.org/abs/2507.19956
tags:
- performance
- feature
- each
- encoding
- fmri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the MedARC team's approach to the 2025 Algonauts
  Challenge, which aimed to predict brain responses to natural movies using multimodal
  machine learning. The team developed an encoding model that combined features from
  five pretrained models (V-JEPA2, Whisper, Llama 3.2, InternVL3, and Qwen2.5-Omni)
  representing video, audio, and text modalities.
---

# Predicting Brain Responses To Natural Movies With Multimodal LLMs

## Quick Facts
- arXiv ID: 2507.19956
- Source URL: https://arxiv.org/abs/2507.19956
- Reference count: 37
- The team achieved a mean Pearson correlation of 0.2085 on out-of-distribution test movies, placing fourth in the 2025 Algonauts Challenge.

## Executive Summary
This paper presents the MedARC team's approach to the 2025 Algonauts Challenge, which aimed to predict brain responses to natural movies using multimodal machine learning. The team developed an encoding model that combined features from five pretrained models (V-JEPA2, Whisper, Llama 3.2, InternVL3, and Qwen2.5-Omni) representing video, audio, and text modalities. These features were linearly projected, temporally aligned using 1D convolutions, and mapped to cortical parcels using a lightweight encoder with shared group and subject-specific residual heads. The model was trained on hundreds of variants across hyperparameter settings and validated using held-out movies. The final submission achieved a mean Pearson correlation of 0.2085 on out-of-distribution test movies, placing the team fourth in the competition. The authors also identified an optimization that could have improved their ranking to second place.

## Method Summary
The method involves extracting features from five pretrained models representing different modalities (video, audio, text, vision-text, and vision-text-audio), linearly projecting these features to a shared 192-dimensional space, temporally aligning them using depthwise 1D convolutions with 45 TR kernels (67 seconds), and mapping the combined embeddings to 1,000 cortical parcels using a shared group head plus subject-specific residual heads. The model was trained on hundreds of variants across hyperparameter settings, validated on held-out movies, and the final submission used per-parcel, per-subject ensembles of top-performing models.

## Key Results
- The multimodal encoding model achieved a mean Pearson correlation of 0.2085 on out-of-distribution test movies
- Temporal alignment via 45 TR 1D convolutions significantly improved performance over shorter windows
- Multi-subject training with shared group and subject-specific residual heads improved individual subject prediction by approximately 0.01 correlation
- Per-parcel ensemble selection of top-5 models from 49 variants improved OOD performance by 0.011 correlation

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Feature Complementarity
- Claim: Combining representations from models trained on different modalities improves encoding model generalization to novel movie stimuli compared to any single model.
- Mechanism: Each backbone model captures modality-specific information (V-JEPA2 for video dynamics, Whisper for speech acoustics, Llama for language semantics, InternVL3/Qwen for multimodal integration). Linear projection to shared latent space allows complementary features to sum constructively. Leave-one-out ablation shows performance drops when any model is removed (Table 1f: removing Llama drops s6 from 0.304 to 0.290).
- Core assumption: Features from pretrained models trained on large-scale datasets transfer to brain prediction tasks without domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] "combining features from models trained in different modalities...improves generalization"
  - [section 4.2/Table 1f] InternVL3 and Qwen2.5 achieve best individual performance; all models contribute in combination
  - [corpus] arXiv:2505.20029 shows instruction-tuned multimodal models correlate with vision-language processing in brain
- Break condition: If test stimulus lacks modalities present in training (e.g., Chaplin has no dialogue), language model features may hurt rather than help (Section 6: Llama features degraded performance on silent film).

### Mechanism 2: Extended Temporal Context via 1D Convolutions
- Claim: Large-kernel 1D convolutions (45 TRs = 67 seconds) for temporal alignment substantially improve prediction over shorter windows.
- Mechanism: fMRI signal has delayed, blurred hemodynamic response. 1D convolutions learn to aggregate features over extended temporal context before mapping to brain activity. The 67-second optimal window far exceeds typical hemodynamic response (~6-12 seconds), suggesting sustained semantic/narrative context matters for brain prediction.
- Core assumption: The brain integrates information over long temporal windows during naturalistic stimulation, and this integration is approximately linear.
- Evidence anchors:
  - [section 4.2/Table 1a] Kernel size 45 TRs achieves 0.304 on s6 vs 0.200 with no temporal alignment
  - [section 4.2] "much longer than the typical hemodynamic response...aggregating features over a long context window is beneficial"
  - [corpus] Weak direct evidence; arXiv:2502.12771 suggests nonlinear temporal integration may be needed for speech-brain alignment
- Break condition: For rapidly varying stimuli or brain regions with faster temporal dynamics, large kernels may over-smooth and lose predictive signal.

### Mechanism 3: Shared-Subject Training with Residual Heads
- Claim: Jointly training across multiple subjects viewing identical stimuli improves individual subject prediction compared to single-subject models.
- Mechanism: Shared group head captures stimulus-driven responses common across subjects; subject-specific residual heads capture individual variation. Multi-subject training yields Δr = 0.01 improvement (Table 1d), with larger gains on OOD movie "life" (0.179 → 0.192).
- Core assumption: Cross-subject neural similarity during naturalistic stimulation reflects shared stimulus processing rather than coincidental alignment.
- Evidence anchors:
  - [section 4/Table 1d] Multi-subject training improves all validation movies
  - [section 4.2/Table 1e] Subject-specific heads necessary and sufficient; group head alone achieves 0.273 on s6
  - [corpus] Weak explicit evidence for this specific architecture pattern
- Break condition: With fewer subjects or highly divergent individual responses, shared component may underfit or mislead.

## Foundational Learning

- Concept: Encoding models in computational neuroscience
  - Why needed here: The entire paper is about building encoding models—algorithms that map stimuli (movies) to predicted neural activations (fMRI). Without this framing, the goal is unclear.
  - Quick check question: Can you explain why encoding models differ from decoding models, and why Pearson correlation is an appropriate metric here?

- Concept: Hemodynamic response function (HRF) and temporal alignment
  - Why needed here: fMRI measures blood oxygenation changes that lag neural activity by ~4-6 seconds and extend over ~12-15 seconds. Understanding this justifies why temporal convolution is necessary and why 67-second kernels exceed the "obvious" HRF duration.
  - Quick check question: If neural activity spikes at t=0, when does the BOLD signal peak, and why might a 10-second convolution window be insufficient for naturalistic movie stimuli?

- Concept: Ensemble learning and model selection
  - Why needed here: The team's final submission used parcel-wise ensembles from 49 models, improving OOD performance by 0.011. Understanding why ensembles reduce variance helps interpret their scaling results (Top-20 would have achieved 2nd place).
  - Quick check question: Why does selecting models based on validation performance and averaging their predictions improve OOD generalization compared to selecting a single "best" model?

## Architecture Onboarding

- Component map:
  1. **Feature extractors** (5 frozen pretrained models): V-JEPA2 (video), Whisper (audio), Llama 3.2 (text), InternVL3 (vision-text), Qwen2.5-Omni (vision-text-audio) → produce high-dimensional features per TR
  2. **Embedding modules** (per model): Linear projection (D→d=192) + depthwise 1D conv (kernel=45 TRs) → temporally-aligned embeddings
  3. **Fusion**: Sum embeddings across models
  4. **Prediction heads**: Shared group linear head + subject-specific residual heads → predict 1000 parcel activations per subject
  5. **Ensemble layer**: Per-parcel, per-subject selection of top-k model predictions, averaged

- Critical path:
  1. Extract features from correct model layers (determined via layer-wise ablation in Appendix A)
  2. Align temporal context window (20 seconds for M-LLMs; single TR for unimodal models)
  3. Train with appropriate validation split (life + bourne outperformed life alone by 0.0046)
  4. Ensemble construction: validate all models, select top-k per parcel

- Design tradeoffs:
  - Embedding dimension: 192 optimal, but 32-64 achieves ~98% of performance (Table 1c)
  - Kernel size: 45 TRs best, but 17 TRs achieves ~95% of performance (Table 1a)
  - Feature set: All five models contribute, but Llama has strongest unique contribution; Whisper weakest
  - Ensemble size: Top-5 submitted; Top-20 would have improved ranking to 2nd place

- Failure signatures:
  - OOD stimulus without language (e.g., silent film "Chaplin"): language model features hurt performance
  - Wrong validation set: Using only "life" for validation reduced Top-5 OOD score from 0.2085 to 0.2039
  - Missing temporal alignment: No convolution drops s6 score from 0.304 to 0.200

- First 3 experiments:
  1. **Single-model baseline**: Train encoding model with only InternVL3 features (best single model per Table 1f). Verify your pipeline reproduces ~0.245 on friends s6 before adding complexity.
  2. **Ablation on temporal alignment**: Compare no convolution vs. kernel sizes [9, 17, 45, 65] TRs on a held-out movie. Confirm 45 TRs improves over 17 TRs for your data.
  3. **Multi-subject vs. single-subject**: Train identical architecture on single subject vs. all four subjects. Verify Δr ≈ 0.01 improvement, particularly on OOD stimuli.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can feature optimization be automated based on intrinsic properties of test movies (e.g., presence of dialogue) without access to ground-truth brain data?
- Basis in paper: [explicit] Section 6 notes that the "Chaplin" movie (black and white, no dialogue) performed better with different features, suggesting "a future direction could explore whether systematic biases exist... and whether feature optimization could be automated."
- Why unresolved: The team utilized Codabench scores to manually select better models for specific movies (e.g., removing Llama features for silent films), but this requires test-time feedback unavailable in real-world applications.
- What evidence would resolve it: A framework that analyzes test stimuli statistics (e.g., audio variance, color histogram) and autonomously selects the optimal feature set to match the stimulus modality, achieving comparable performance without access to fMRI ground truth.

### Open Question 2
- Question: What computational mechanisms or features are required to close the performance gap between feature encoding models and cross-subject encoding ceilings in the dorsal attention network?
- Basis in paper: [explicit] Appendix D compares the encoding model to a cross-subject ceiling and explicitly calls for "future work... to investigate closing the gaps... especially in the dorsal attention network."
- Why unresolved: The current multimodal features (vision, audio, text) explain visual and temporal cortices well but leave significant variance unexplained in higher-order attention networks compared to the cross-subject benchmark.
- What evidence would resolve it: The identification of specific features (e.g., saliency maps, task-embedded goals, or narrative attention markers) that significantly improve prediction correlations in the dorsal attention network to match cross-subject levels.

### Open Question 3
- Question: Why does the optimal temporal alignment kernel (67 seconds) far exceed the duration of the typical hemodynamic response?
- Basis in paper: [inferred] Section 4.2 notes that the optimal kernel width of 45 TRs (67 seconds) is "much longer than the typical hemodynamic response," suggesting the need to understand if this reflects long-range temporal dependencies or model architecture biases.
- Why unresolved: While the authors confirm the kernel size improves scores, they do not determine if this is due to the aggregation of long context windows required for narrative understanding or an artifact of the convolutional approach.
- What evidence would resolve it: Ablation studies separating temporal context length from kernel size, or the application of alternative temporal models (e.g., Transformers with attention) to see if similar long-window dependencies emerge naturally.

## Limitations
- Limited OOD Test Set: The reported 0.2085 OOD correlation is based on only 6 test movies, with no public ground truth available.
- Architectural Specificity: The optimal kernel size (45 TRs) and embedding dimension (192) may be dataset-specific.
- Unknown Preprocessing: The paper mentions MNI normalization but lacks details on fMRI preprocessing pipeline.

## Confidence
- **High Confidence**: Temporal alignment via 1D convolutions improves performance (direct ablation evidence in Table 1a, confirmed across subjects).
- **Medium Confidence**: Multimodal feature complementarity generalizes (leave-one-out ablations show consistent drops, but specific contribution weights may vary).
- **Low Confidence**: Ensemble optimization (Top-20 would achieve 2nd place) without public OOD ground truth; relies on extrapolation from validation performance.

## Next Checks
1. **Cross-Dataset Validation**: Apply the exact architecture to a different fMRI movie dataset (e.g., Naturalistic Stimulation Database) to test whether the 45-TR kernel and multimodal fusion benefits transfer beyond CNeuroMod.

2. **Silent Stimulus Testing**: Systematically evaluate the model on entirely silent or non-linguistic stimuli to quantify when language features become detrimental (Section 6 observation).

3. **Feature Importance Scaling**: Conduct controlled experiments varying the number of multimodal features (2→5 models) on OOD movies to determine whether the marginal benefit of each additional modality diminishes or plateaus.