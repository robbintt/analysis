---
ver: rpa2
title: 'COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for
  Model Compilation'
arxiv_id: '2602.01935'
source_url: https://arxiv.org/abs/2602.01935
tags:
- colt
- search
- optimization
- compiler
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COLT enables lightweight multi-LLM collaboration for compiler optimization
  by embedding model selection directly into a shared MCTS tree. At each step, the
  acting LLM proposes both a compiler transformation and the next model to query,
  allowing models to share partial optimization trajectories and propagate value estimates
  across transformations and routing decisions.
---

# COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation

## Quick Facts
- arXiv ID: 2602.01935
- Source URL: https://arxiv.org/abs/2602.01935
- Reference count: 21
- Primary result: 10.86× speedup on CPU (30.05× on GPU) vs single large-model baseline

## Executive Summary
COLT introduces a lightweight multi-LLM collaboration framework for compiler optimization that embeds model selection directly into a shared Monte Carlo Tree Search (MCTS) reasoning process. Rather than relying on a single large language model or maintaining separate search trees for each model, COLT allows models to share partial optimization trajectories and propagate value estimates across transformations and routing decisions through a unified search space. This approach eliminates the need for external planners, concurrent LLM execution, or explicit communication channels while achieving superior optimization performance compared to single large-model baselines.

The system achieves efficient collaboration by biasing search toward smaller models while preserving exploration through a model-aware tree policy, and selectively escalating to larger models when persistent regressions are detected via a course-alteration mechanism. Across five benchmarks on CPU and GPU architectures, COLT demonstrates that effective compiler optimization can be achieved through lightweight collaboration via shared MCTS reasoning, invoking the largest LLM only 23.9% of the time while consistently outperforming single-model approaches.

## Method Summary
COLT implements multi-LLM collaboration through a shared Monte Carlo Tree Search framework where each node represents a compiler transformation state. At each search step, the acting LLM proposes both a compiler transformation and the next model to query, creating a unified search space that enables models to share partial optimization trajectories. The shared MCTS tree allows value estimates to propagate across both transformation decisions and model routing decisions, eliminating the need for separate search structures or explicit communication protocols between models.

The system employs a model-aware tree policy that biases search toward smaller models while maintaining sufficient exploration, and incorporates a course-alteration mechanism that detects persistent regressions and selectively escalates to the largest model for complex optimization decisions. This architecture enables lightweight collaboration where the computational overhead of MCTS is offset by the ability to leverage smaller, faster models for routine transformations while reserving the largest model for challenging optimization scenarios.

## Key Results
- Achieved 10.86× speedup on CPU and 30.05× on GPU compared to single large-model baseline
- Largest LLM invoked only 23.9% of the time while maintaining superior performance
- Consistent outperformance across five benchmarks on both CPU and GPU architectures

## Why This Works (Mechanism)
The shared MCTS framework enables efficient collaboration by allowing models to build upon each other's partial solutions within a unified search space. When smaller models make progress on routine transformations, their value estimates and partial trajectories become available to larger models, which can then focus their computational resources on more complex decisions. The model-aware tree policy ensures that search bias toward smaller models doesn't compromise solution quality, while the course-alteration mechanism provides a safety net for detecting and correcting persistent regressions without constant reliance on the largest model.

This architecture works because compiler optimization naturally decomposes into a sequence of transformation decisions where early progress can inform later choices. The shared tree structure captures this sequential dependency while the model routing decisions allow the system to adaptively allocate computational resources based on the difficulty of each optimization subproblem. By embedding model selection within the search process rather than treating it as a separate planning problem, COLT achieves both efficiency and effectiveness in multi-LLM collaboration.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: A heuristic search algorithm that balances exploration and exploitation through iterative tree expansion and value estimation; needed for navigating the large search space of compiler transformations while enabling shared reasoning across models
- **Model-aware tree policy**: A search bias that prioritizes smaller models while preserving exploration; needed to maintain efficiency while ensuring solution quality through appropriate model selection
- **Course-alteration mechanism**: A regression detection system that escalates to larger models when persistent optimization regressions occur; needed to provide a safety mechanism without constant reliance on expensive models
- **Shared optimization trajectories**: The ability for models to build upon each other's partial solutions within a unified search space; needed to enable true collaboration rather than parallel independent searches
- **Value propagation across routing decisions**: The mechanism by which optimization progress is shared between different model selections; needed to maintain coherent optimization progress despite model switching
- **Compiler transformation sequencing**: The decomposition of optimization into sequential transformation decisions; needed to enable the shared MCTS framework to capture dependencies between optimization steps

## Architecture Onboarding

**Component map**: Compiler code -> MCTS Tree (shared across models) -> LLM 1 (small) <-> LLM 2 (medium) <-> LLM 3 (large) -> Optimized code

**Critical path**: Input code → MCTS initialization → Model selection & transformation proposal → Value estimation → Tree update → Next iteration until termination condition

**Design tradeoffs**: The shared MCTS approach trades off some computational overhead for the ability to leverage multiple models efficiently, avoiding the cost of running all models in parallel while maintaining the benefits of diverse model capabilities. The course-alteration mechanism introduces latency during escalation but prevents wasted effort on persistent regressions.

**Failure signatures**: Performance degradation occurs when the model-aware policy is too aggressive in favoring small models, leading to local optima that require escalation; conversely, excessive escalation to large models defeats the efficiency purpose. The MCTS overhead can become prohibitive for very large codebases or deep optimization sequences.

**3 first experiments**:
1. Benchmark COLT on a single model (large only) to establish baseline MCTS overhead without collaboration benefits
2. Test the course-alteration mechanism in isolation with synthetic regression patterns to validate detection thresholds
3. Evaluate model-aware tree policy with different bias strengths to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out
None

## Limitations
- MCTS computational overhead may not scale well to more complex optimization problems
- Course-alteration mechanism depends on subjective thresholds that may not generalize across diverse code patterns
- Evaluation limited to five benchmarks and two hardware targets, leaving uncertainty about broader applicability

## Confidence
- High confidence: Core technical contribution of embedding model selection into shared MCTS is well-defined and validated
- Medium confidence: Efficiency claims relative to single large-model baseline given limited benchmark scope
- Medium confidence: Generalization of course-alteration mechanism across diverse code patterns

## Next Checks
1. Evaluate COLT on additional compiler optimization benchmarks involving complex loop transformations, vectorization, and parallelization across diverse hardware targets (ARM, RISC-V, specialized accelerators)

2. Benchmark COLT against established compiler optimization frameworks (LLVM passes, GCC optimization levels) to establish absolute performance improvements rather than relative gains over single large-model baseline

3. Profile MCTS computational overhead and analyze the trade-off between search depth, tree expansion, and optimization quality across different model sizes and problem complexities