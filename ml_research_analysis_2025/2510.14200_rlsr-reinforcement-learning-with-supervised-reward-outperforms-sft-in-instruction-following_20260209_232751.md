---
ver: rpa2
title: 'RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction
  Following'
arxiv_id: '2510.14200'
source_url: https://arxiv.org/abs/2510.14200
tags:
- rlsr
- reward
- qwen-em
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLSR replaces standard SFT with an RL framework that uses the cosine
  similarity between embeddings of model-generated and human-labeled responses as
  the reward signal. This allows the model to explore multiple candidate responses
  rather than relying on token-level supervision.
---

# RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following

## Quick Facts
- arXiv ID: 2510.14200
- Source URL: https://arxiv.org/abs/2510.14200
- Reference count: 40
- RLSR consistently outperforms SFT on instruction-following tasks, with win rates reaching 26.34% (Qwen-7B + INFINITY) versus 21.01% for SFT

## Executive Summary
RLSR introduces a reinforcement learning framework that replaces standard supervised fine-tuning (SFT) with a reward signal based on cosine similarity between embeddings of model-generated and human-labeled responses. This approach enables exploration beyond exact token-level imitation, allowing models to discover semantically equivalent responses that differ in surface form. Experiments on Llama-8B, Qwen-7B, and Qwen-32B using TULU and INFINITY datasets demonstrate RLSR's consistent superiority over SFT, with further improvements when combining SFT initialization with RLSR fine-tuning.

## Method Summary
RLSR uses cosine similarity between embeddings of generated and reference responses as a dense reward signal for reinforcement learning. The framework employs VERL with GRPO optimization, generating 8 candidate responses per prompt and computing rewards based on embedding cosine similarity. A Longest Common Substring (LCS) penalty prevents repetitive generation. The method supports both direct application to base models and combination with SFT initialization. Training uses 1024 batch size, 256 PPO mini-batch, and KL coefficient of 0.001, with learning rates of 1-3×10⁻⁶ for RLSR versus 3×10⁻⁵ for SFT.

## Key Results
- Qwen-7B + INFINITY: RLSR achieves 26.34% AlpacaEval win rate vs SFT's 21.01%
- SFT+RLSR combination reaches 30.73% win rate on same setup
- RLSR excels at classification tasks while SFT+RLSR performs better on open-ended generation
- Consistent improvements across Llama-8B, Qwen-7B, and Qwen-32B models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing token-level supervision with semantic reward signals enables exploratory learning beyond exact imitation
- **Mechanism:** SFT forces exact token-level conditioning during training, limiting exploration to local optima. RLSR generates multiple candidates and rewards semantic similarity, discovering alternative high-quality responses
- **Core assumption:** Semantically similar responses are functionally equivalent for instruction-following
- **Evidence anchors:** Abstract and Section 1 contrast SFT's token-level constraints with RLSR's exploration capability
- **Break condition:** If embedding similarity poorly correlates with actual response quality in your task domain

### Mechanism 2
- **Claim:** Dense reward signals from embedding similarity provide more informative gradients than sparse binary correctness signals
- **Mechanism:** Continuous cosine similarity rewards provide partial credit feedback, creating smoother optimization landscapes compared to binary pass/fail signals
- **Core assumption:** Embedding representation space meaningfully captures instruction-following quality dimensions
- **Evidence anchors:** Section 2.3 details reward formulation using cosine similarity; Section 2.4 links rewards to human preference alignment
- **Break condition:** If your task requires rewards for dimensions not captured by general-purpose embeddings

### Mechanism 3
- **Claim:** Combining SFT's stability with RLSR's exploration yields complementary strengths for different task types
- **Mechanism:** SFT establishes baseline fluency through token-level imitation, while RLSR refines behavior through semantic exploration
- **Core assumption:** SFT and RLSR optimize sufficiently different objectives to avoid redundancy
- **Evidence anchors:** Section 3.2.3 documents trade-off where RLSR excels at classification while SFT+RLSR better preserves linguistic fluency
- **Break condition:** If computational budget constraints force choosing between methods

## Foundational Learning

- **Concept:** Proximal Policy Optimization (PPO) / GRPO fundamentals
  - **Why needed here:** RLSR uses RL optimization requiring understanding of KL-penalty, advantage estimation, and exploration-exploitation tradeoff
  - **Quick check question:** Can you explain why GRPO eliminates the need for a separate value function compared to PPO?

- **Concept:** Embedding models and semantic similarity
  - **Why needed here:** The reward signal depends entirely on embedding quality and semantic representation
  - **Quick check question:** What happens to cosine similarity when two responses are semantically similar but differ in negation (e.g., "I can help" vs "I cannot help")?

- **Concept:** Teacher-forcing vs. exposure bias in sequence models
  - **Why needed here:** RLSR addresses SFT's core limitation of train-test distribution mismatch
  - **Quick check question:** During SFT inference, the model conditions on its own predictions. Why does this create problems when training only used ground-truth context?

## Architecture Onboarding

- **Component map:** Base LLM (policy) -> Embedding model (reward function) -> RL framework (VERL/GRPO) -> Training data (TULU/INFINITY)

- **Critical path:**
  1. Prepare SFT-format data (prompt-response pairs)
  2. Select embedding model based on base model size
  3. Configure GRPO: rollout=8 per prompt, max_seq_len=4096, KL_coef=0.001
  4. Add LCS penalty for repetitive generation (LCS>100 chars AND >10% of reference length → reward=-1)
  5. Train and evaluate on AlpacaEval/Arena-Hard for generation, LM Evaluation Harness for classification

- **Design tradeoffs:**
  - SB vs. Qwen-EM embeddings: SB faster and better for smaller models; Qwen-EM richer semantic signals for larger models
  - RLSR vs. SFT+RLSR: RLSR alone better for classification/reasoning; SFT+RLSR better for open-ended generation quality
  - Direct replacement vs. augmentation: Both work, but SFT+RLSR consistently achieves highest win rates

- **Failure signatures:**
  - Repetitive generation: Monitor LCS metrics and apply penalty
  - Reward hacking: Watch for high cosine similarity but low task quality responses
  - Training instability: Use lower learning rates for RLSR vs. SFT

- **First 3 experiments:**
  1. Replicate RLSR vs. SFT comparison on single model-dataset combination to validate implementation
  2. Ablate embedding model choice (SB vs. Qwen-EM) on your specific task domain
  3. Test SFT+RLSR pipeline starting from existing SFT checkpoint to measure incremental gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would replacing the cosine similarity reward with a pretrained Bradley-Terry reward model or generative reward model yield better alignment performance than embedding-based rewards?
- **Basis in paper:** Limitations section explicitly suggests exploring alternatives like pretrained Bradley-Terry reward models or generative reward models
- **Why unresolved:** Only embedding-based rewards were validated; no comparison to learned preference models conducted
- **What evidence would resolve it:** Controlled experiment comparing RLSR using cosine similarity versus Bradley-Terry reward model on same base models and datasets

### Open Question 2
- **Question:** Does RLSR scale effectively to trillion-parameter models trained on industrial-scale SFT datasets (billions of tokens)?
- **Basis in paper:** Limitations section states testing RLSR on trillion-parameter models with industrial-scale SFT datasets is needed
- **Why unresolved:** Experiments limited to 32B parameters with 100k samples; larger scale behavior unknown
- **What evidence would resolve it:** Training RLSR on 500B+ parameter model using billions of tokens, then evaluating on benchmark suite

### Open Question 3
- **Question:** Why does standalone RLSR excel at classification tasks while SFT+RLSR performs better on open-ended generation?
- **Basis in paper:** Paper observes this trade-off in Section 3.2.3 but offers no theoretical or ablation analysis of cause
- **Why unresolved:** Documents empirical pattern without explaining mechanism
- **What evidence would resolve it:** Ablation study varying SFT initialization strength and analyzing token-level supervision interaction with exploration-based rewards

### Open Question 4
- **Question:** How robust is RLSR to the choice of embedding model, and what embedding characteristics most influence reward signal quality?
- **Basis in paper:** Uses two embedding models with different results, observing larger models leverage additional semantic richness of larger embedding-based rewards
- **Why unresolved:** Only two embedding models tested; relationship between embedding properties and RLSR effectiveness uncharacterized
- **What evidence would resolve it:** Systematic study varying embedding model size, architecture, and training objective, correlating characteristics with RLSR performance gains

## Limitations
- Training epochs/steps not specified—only batch sizes and learning rates provided, creating uncertainty about convergence criteria
- Switch from VERL's SFT module to OpenRLHF due to instability suggests potential implementation sensitivity
- No statistical significance testing or error bars provided for reported performance improvements

## Confidence

- **High confidence** in core claim that RLSR outperforms SFT on instruction-following tasks, supported by consistent AlpacaEval win rate improvements across multiple model sizes and datasets
- **Medium confidence** in mechanism explanations, particularly exploration hypothesis with indirect evidence for semantic similarity driving gains
- **Medium confidence** in SFT+RLSR combination claim, showing highest win rates but lacking error bars and requiring ~2x computational cost

## Next Checks

1. **Implement RLSR vs. SFT ablation** on single model-dataset combination (e.g., Qwen-7B + TULU) to verify implementation achieves comparable AlpacaEval win rates and test performance persistence with your data preprocessing

2. **Ablate embedding model choice** by comparing SB vs. Qwen-EM rewards on your target task domain to determine whether embedding model's semantic space captures your quality criteria

3. **Test training stability and convergence** by monitoring loss curves, KL divergence, and output quality during both RLSR and SFT+RLSR training to validate whether improvements come from stable optimization or implementation-specific factors