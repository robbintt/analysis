---
ver: rpa2
title: Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring
arxiv_id: '2510.19158'
source_url: https://arxiv.org/abs/2510.19158
tags:
- lemma
- where
- which
- then
- observable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses adversarial linear partial monitoring, a variant
  of bandit learning where the learner observes a linear transformation of the loss
  vector rather than the loss itself. This setting generalizes standard linear bandits
  by decoupling loss and feedback.
---

# Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring

## Quick Facts
- arXiv ID: 2510.19158
- Source URL: https://arxiv.org/abs/2510.19158
- Reference count: 40
- One-line primary result: Achieves optimal instance-dependent regret bounds in adversarial linear partial monitoring via a convex optimization approach

## Executive Summary
This paper tackles adversarial linear partial monitoring, where the learner observes a linear transformation of the loss vector rather than the loss itself. The authors propose an algorithm based on exponential weights with a structured "anchored" loss estimator, leveraging the exploration-by-optimization (EXO) framework. By exploiting the linear structure, the optimization problem reduces to a convex program that is efficiently solvable. The algorithm achieves regret bounds matching the optimal rates—O(√T) in locally observable games and O(T^{2/3}) in globally observable games—with instance-dependent quantities reflecting the alignment between observations and losses.

## Method Summary
The algorithm maintains exponential weights over Pareto optimal actions and solves a convex optimization problem at each round to find the sampling distribution. The key innovation is the "anchored" loss estimator that subtracts the expected feature vector, forcing variance terms to depend on feature differences rather than individual features. This structure enables the algorithm to exploit observability conditions efficiently. The optimization problem is proven to be a finite-dimensional convex program, often representable as a semidefinite program, making it computationally tractable. The method achieves tight regret bounds by optimizing sampling distributions against worst-case losses using this specific estimator structure.

## Key Results
- Achieves O(√T) regret in locally observable games where exploration can be done using only optimal actions
- Achieves O(T^{2/3}) regret in globally observable games where exploration is costly and requires all actions
- Provides instance-dependent bounds featuring interpretable quantities reflecting alignment between observations and losses
- Tight bounds are shown for several example problems including feedback graphs and ill-conditioned bandits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves tight regret bounds by optimizing sampling distributions against worst-case losses using a specific "anchored" loss estimator structure.
- Mechanism: The method uses a shifted estimator $\hat{y}_t(a) = (\psi_a - H q_t)^\top \hat{\ell}_t$. Subtracting the expected feature vector $H q_t$ anchors the estimator, forcing variance terms in the regret bound to depend on feature differences rather than individual features. This allows bounding by alignment constants $\beta_{loc}$ or $\beta_{glo}$.
- Core assumption: The game satisfies global observability, meaning feature differences are in the span of observation matrices.
- Evidence anchors: [abstract], [section 3], weak corpus support focusing on general regret bounds.
- Break condition: If matrix $Q(p_t)$ is singular or near-singular without appropriate regularization ($\delta$), the pseudoinverse estimation becomes unstable, causing the regret bound to diverge.

### Mechanism 2
- Claim: The achievable regret rate ($\sqrt{T}$ vs $T^{2/3}$) is determined by the "observability" class of the game, specifically whether exploration can be done locally using only optimal actions.
- Mechanism: The algorithm classifies games based on the span of observation matrices. If feature differences are spanned by good actions (Locally Observable), the optimization finds a $\sqrt{T}$ strategy. If feature differences require all actions (Globally Observable), the algorithm is forced into costly exploration, yielding $T^{2/3}$ regret.
- Core assumption: The loss space $L$ is full-dimensional (contains a centered ball $B_2(r)$).
- Evidence anchors: [section 2.2], [section 5], no direct corpus support for this specific classification mechanism.
- Break condition: If the game is "hopeless" (not globally observable), feature differences lie outside the span of all observation matrices, and the algorithm cannot guarantee sublinear regret.

### Mechanism 3
- Claim: The optimization problem for selecting the sampling distribution is efficiently solvable (convex program) rather than requiring generic infinite-dimensional optimization.
- Mechanism: By fixing the structure of the loss estimator to be linear in the feedback, the EXO optimization reduces to minimizing a convex potential function $\Lambda_{\eta,q}$ over the probability simplex, subject to constraints on the "width" of the estimator.
- Core assumption: The loss space $L$ and the resulting objective/constraints are semidefinite representable (SDR).
- Evidence anchors: [abstract], [section 3], weak connection to corpus neighbors discussing bounds but not this specific EXO implementation efficiency.
- Break condition: If the loss space $L$ is non-convex or highly irregular, the constraint set may not be SDR, rendering the "efficient implementation" invalid.

## Foundational Learning

- **Partial Monitoring Setup**
  - Why needed here: This is the fundamental problem class where the learner does not observe the loss directly but receives a correlated signal.
  - Quick check question: If $M_a = \psi_a$, does this reduce to a standard linear bandit? (Yes).

- **Exponential Weights (EXP)**
  - Why needed here: The core policy updates a distribution $q_t$ over actions based on cumulative estimated losses.
  - Quick check question: What happens to $q_t(a)$ if the cumulative estimated loss for action $a$ is significantly higher than others? (It approaches zero).

- **Moore-Penrose Pseudoinverse ($A^\dagger$)**
  - Why needed here: Used to define the loss estimator $\hat{\ell}_t$ and the alignment constants when the observation matrix $M$ is rank-deficient or non-square.
  - Quick check question: If $Q(p)$ is invertible, does $Q(p)^\dagger$ differ from $Q(p)^{-1}$? (No, they are identical).

## Architecture Onboarding

- Component map: Belief State (exponential weights $q_t$) -> Optimization Engine (convex solver for $p_t$) -> Sampler (draws $A_t \sim p_t$) -> Estimator (computes $\hat{y}_t$ anchored on $Hq_t$)

- Critical path: Computing $q_t$ is $O(k \log k)$. The bottleneck is solving the convex program for $p_t$ at every round.

- Design tradeoffs: The parameter $\delta$ trades off stability (forcing $Q_\delta$ to be full rank) against estimation bias. The parameter $\eta$ balances the "stability" term against the cumulative penalty term.

- Failure signatures:
  - Numerical Instability: Crash if $Q(p_t)$ becomes singular (mitigated by $\delta$)
  - Linear Regret: If the learning rate $\eta$ is not tuned correctly (too high) or if the observability condition is misclassified
  - Non-convergence: If the convex solver tolerance $\epsilon$ is too loose, the regret bounds may degrade

- First 3 experiments:
  1. **Linear Bandits Validation**: Run on a synthetic linear bandit setup (where $M_a = \psi_a$). Verify the regret scales as $\tilde{O}(\sqrt{T})$.
  2. **Global vs Local**: Compare performance on a "Label Efficient" setting (Locally Observable) vs a "Dynamic Pricing" setting (Globally Observable) to validate the $\sqrt{T}$ vs $T^{2/3}$ rate difference.
  3. **Stress Test $\delta$**: Vary the stability parameter $\delta$ in a game with ill-conditioned observation matrices to verify robustness against numerical singularities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tight regret guarantees be derived for finite partial monitoring by relaxing the full-dimensional loss space assumption in the adversarial setting?
- Basis in paper: [explicit] Section 6 states that the assumption of full-dimensional loss spaces precludes recovering tight guarantees for finite partial monitoring and leaves addressing this for future work.
- Why unresolved: The current observability conditions do not match general finite cases when the loss space lacks a non-empty interior.
- What evidence would resolve it: A derivation of regret bounds that match known minimax rates for finite partial monitoring without requiring the loss space to contain a centered L2 ball.

### Open Question 2
- Question: Can the classification of achievable regret rates be extended to generic compact action sets?
- Basis in paper: [explicit] Section 6 lists the study of generic compact action sets as a direction for future work.
- Why unresolved: A full classification theorem is currently lacking even in the stochastic setting for infinite action spaces.
- What evidence would resolve it: Extending the theoretical framework and regret bounds to handle infinite, compact action sets rather than just finite ones.

### Open Question 3
- Question: How does the minimax regret depend precisely on instance-based alignment constants?
- Basis in paper: [explicit] Section 6 identifies characterizing the dependence of minimax regret on instance-based constants as a "general challenging direction."
- Why unresolved: While the paper provides bounds, the exact relationship between the game structure (alignment constants) and the fundamental limits of the minimax regret remains complex.
- What evidence would resolve it: A precise characterization or lower bounds that explicitly tie specific game parameters to the optimal minimax regret.

## Limitations

- The "anchoring" mechanism (subtracting $Hq_t$) is theoretically justified but lacks detailed empirical validation in the provided corpus.
- Computing the alignment constants ($\beta_{loc}$, $\beta_{glo}$) required for optimal learning rates is non-trivial and may require conservative overestimation in practice.
- The SDP-based optimization, while proven efficient, could become computationally prohibitive for larger action spaces or feature dimensions.

## Confidence

- **High confidence**: The algorithm's structure (exponential weights with anchored estimators) and the general framework (EXO with convex optimization) are sound and well-specified.
- **Medium confidence**: The regret bounds matching $\sqrt{T}$ and $T^{2/3}$ rates for locally and globally observable games respectively are mathematically derived, but empirical validation against these specific rates across different game structures would strengthen confidence.
- **Low confidence**: The specific implementation details for the SDP solver (especially handling the pseudoinverse via Schur complements) and the precise computation of instance-dependent constants are not fully specified and could significantly impact practical performance.

## Next Checks

1. **Verification of game classification**: Implement both a locally observable (e.g., linear bandit) and globally observable (e.g., feedback graph) game, and verify the algorithm achieves $\sqrt{T}$ vs $T^{2/3}$ regret scaling respectively over a range of horizons.

2. **Numerical stability test**: Systematically vary the stability parameter $\delta$ in an ill-conditioned observation matrix game to empirically verify the claimed robustness against numerical singularities.

3. **Computational scaling analysis**: Measure the wall-clock time for solving the SDP optimization as a function of action set size $k$ and feature dimension $d$ to assess the practical efficiency limits of the algorithm.