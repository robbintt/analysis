---
ver: rpa2
title: Attribution analysis of legal language as used by LLM
arxiv_id: '2501.17330'
source_url: https://arxiv.org/abs/2501.17330
tags:
- legal
- attribution
- figure
- examples
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how different large language models (LLMs)
  perform on legal text classification tasks and uses attribution methods to understand
  their behavior. Three models are compared: BERT (generic), legalBERT (fine-tuned
  on legal texts), and customLegalBERT (trained from scratch on legal texts).'
---

# Attribution analysis of legal language as used by LLM
## Quick Facts
- arXiv ID: 2501.17330
- Source URL: https://arxiv.org/abs/2501.17330
- Reference count: 24
- The paper investigates how different large language models (LLMs) perform on legal text classification tasks and uses attribution methods to understand their behavior.

## Executive Summary
This paper compares the performance of three different language models (BERT, legalBERT, and customLegalBERT) on legal text classification tasks using attribution analysis. The study reveals that model performance is significantly influenced by tokenizer behavior and exposure to legal corpora. While all models perform well on binary classification tasks, they show distinct differences in more complex multiple-choice tasks, with attribution scores varying accordingly. The research also identifies legal-specific tokens and explores connections between LLM behavior and known legal language features.

## Method Summary
The study employs attribution methods to analyze how different LLMs (BERT, legalBERT, and customLegalBERT) perform on legal text classification tasks. Two datasets are used: overrule (binary classification) and casehold (multiple choice). The attribution analysis aims to understand the relationship between model performance, tokenizer behavior, and exposure to legal corpora. Legal-specific tokens are identified, and their impact on model behavior is examined. The methodology focuses on comparing attribution scores across models and tasks to draw conclusions about their relative strengths and weaknesses in handling legal language.

## Key Results
- All models perform well on the overrule binary classification task, but attribution scores vary significantly
- For the casehold multiple-choice task, models show more distinct differences in both performance and attribution
- Model performance is largely influenced by tokenizer behavior and exposure to legal corpora

## Why This Works (Mechanism)
The study demonstrates that LLM performance on legal tasks is mediated through tokenization patterns and training data exposure. The attribution analysis reveals that legal-specific tokens play a crucial role in model behavior, with different models showing varying sensitivities to these tokens based on their training. The mechanism appears to be that models trained on or fine-tuned with legal corpora develop specialized token representations that influence their classification decisions, while generic models rely more heavily on general language patterns that may not capture legal nuances effectively.

## Foundational Learning
- **Attribution Methods**: Techniques for understanding model decision-making by identifying which input features most influence predictions. Needed because it allows researchers to interpret why models make certain classifications. Quick check: Compare results across multiple attribution methods to verify consistency.
- **Tokenization in LLMs**: The process of breaking text into discrete units (tokens) that models process. Critical because different tokenizers can split legal terminology in ways that affect model understanding. Quick check: Analyze how legal terms are tokenized across different models.
- **Legal Corpus Exposure**: The extent and nature of legal text in a model's training data. Important because it determines how well models understand legal language patterns and terminology. Quick check: Compare model performance on legal vs. general language tasks.
- **Model Fine-tuning vs. Training from Scratch**: Different approaches to adapting models for specific domains. Relevant because it affects how models develop domain-specific capabilities. Quick check: Measure performance differences between fine-tuned and from-scratch models on specialized tasks.

## Architecture Onboarding
**Component Map**: Input Text -> Tokenizer -> Model Architecture -> Classification Layer -> Output
**Critical Path**: The sequence from tokenization through model processing to final classification decision, with attribution analysis applied to understand feature importance at each stage
**Design Tradeoffs**: Generic models offer broader applicability but may miss legal nuances, while specialized models capture domain knowledge but may overfit to specific legal language patterns
**Failure Signatures**: Poor performance on legal-specific terminology, over-reliance on generic language patterns, and inconsistent attribution scores across similar legal texts
**First Experiments**: 1) Compare attribution scores for legal-specific vs. general tokens across models, 2) Test model performance with legal terms replaced by synonyms, 3) Evaluate how tokenization differences affect classification of identical legal concepts

## Open Questions the Paper Calls Out
None

## Limitations
- Attribution methods lack clear definition and methodological transparency
- Datasets used may not be fully representative of diverse legal language tasks
- Observed performance differences are partially speculative regarding their underlying causes

## Confidence
- Attribution analysis reliability: Low (methods not clearly defined)
- Model performance comparisons: Medium (supported by metrics but attribution unclear)
- Legal-specific token identification: Medium (plausible but not definitively proven)

## Next Checks
1. Replicate the attribution analysis using multiple established methods (e.g., LIME, SHAP, integrated gradients) to verify consistency of findings across techniques
2. Conduct ablation studies systematically removing legal-specific tokens to measure impact on model performance and attribution scores
3. Test model generalization by evaluating on additional legal text classification tasks beyond the two datasets used, to assess whether observed patterns hold across diverse legal domains