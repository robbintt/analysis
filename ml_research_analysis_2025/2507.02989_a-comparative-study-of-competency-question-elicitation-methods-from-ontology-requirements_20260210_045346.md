---
ver: rpa2
title: A Comparative Study of Competency Question Elicitation Methods from Ontology
  Requirements
arxiv_id: '2507.02989'
source_url: https://arxiv.org/abs/2507.02989
tags:
- ontology
- semantic
- sets
- requirements
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares three approaches for generating competency
  questions (CQs) in ontology engineering: manual formulation by experts, pattern-based
  generation, and Large Language Model (LLM) generation. A novel dataset of 204 CQs,
  derived from the same user story using these three methods, was created and evaluated
  by ontology engineers on suitability, ambiguity, relevance, readability, and complexity.'
---

# A Comparative Study of Competency Question Elicitation Methods from Ontology Requirements

## Quick Facts
- arXiv ID: 2507.02989
- Source URL: https://arxiv.org/abs/2507.02989
- Reference count: 38
- Three methods for generating competency questions (CQs) compared: manual expert formulation, pattern-based generation, and LLM generation

## Executive Summary
This study evaluates three approaches for generating competency questions (CQs) in ontology engineering using a novel dataset of 204 CQs derived from the same user story. The evaluation by ontology engineers assessed suitability, ambiguity, relevance, readability, and complexity. Manual formulation achieved the highest suitability scores and readability with the lowest complexity, while LLM-generated CQs were less suitable, significantly less readable, and more complex. Pattern-based CQs fell between manual and LLM approaches. The findings highlight the value of human expertise in producing high-quality CQs, while suggesting LLMs can assist in initial brainstorming but require refinement.

## Method Summary
The study compared three CQ elicitation methods using a cultural heritage user story involving music archivist and collection curator personas. Manual CQs were formulated by two ontology engineers (HA-1 and HA-2), pattern-based CQs were generated using 19 predefined archetypes, and LLM-generated CQs were produced using GPT-4.1 and Gemini 2.5 Pro with specific parameters (temperature=0, frequency_penalty=0, presence_penalty=0, seed=46). The resulting 204 CQs were evaluated by experts on suitability and ambiguity, while NLP tools assessed readability (FKGL, DCR), complexity (requirement/linguistic/syntactic), and semantic overlap (Sentence-BERT embeddings).

## Key Results
- Manually formulated CQs achieved the highest suitability scores and readability while exhibiting the lowest complexity
- LLM-generated CQs were rated as less suitable, significantly less readable, and more complex with notable variability across different models
- Pattern-based CQs fell between manual and LLM approaches in terms of suitability and complexity
- Different elicitation methods produced sets with high centroid similarity but extremely low instance-level overlap

## Why This Works (Mechanism)

### Mechanism 1: Inferential Relevance via Tacit Expert Knowledge
Manual formulation by domain experts captures implicit functional requirements better than pattern or LLM-based approaches, provided experts possess deep domain knowledge. Experienced ontology engineers leverage tacit knowledge to bridge gaps in explicit user stories, formulating questions that are not strictly stated but are functionally necessary for the system to work. This reduces the need for subsequent iteration. The "suitability" of a CQ is positively correlated with its ability to anticipate unstated functional needs. Evidence shows human-annotated sets had higher proportions of inferential CQs (27.8% for HA-2 vs 4.8% for Gemini). If the domain is novel or the expert lacks specific knowledge, inferential capabilities diminish.

### Mechanism 2: LLM Semantic Breadth vs. Syntactic Complexity
LLMs generate CQs with higher thematic diversity but at the cost of significantly higher linguistic and requirement complexity compared to manual methods. Generative models optimize for semantic completeness and verbose elaboration, resulting in longer questions with richer ontological primitives. This increases requirement complexity and reduces readability, making CQs harder to translate directly into concise ontology axioms. Complexity metrics correlate negatively with immediate usability for ontology modeling. GPT CQs averaged 111 characters vs ~47 for manual, with requirement complexity (c1) at 8.12 for GPT vs 4.17 for HA-2. If LLMs are constrained by few-shot prompting with concise examples, verbosity and complexity may decrease.

### Mechanism 3: Semantic Misalignment in Automated Sets
Different CQ elicitation methods produce sets with high centroid similarity (general theme) but extremely low instance-level overlap, with LLMs showing the highest variability. LLMs map input requirements to vast latent spaces, retrieving semantically related but distinct "neighborhoods" of questions. This results in low bidirectional coverage between LLM sets and manual sets, meaning LLMs invent distinct questions rather than mirroring human logic. High semantic novelty is not inherently beneficial if it diverges from the specific scope defined by the user story. Gemini had 0.0% coverage by HA-1 and HA-2; bidirectional coverage between GPT and Gemini was only 2.9%. If the user story is extremely narrow or technical, LLM variability might decrease, forcing higher alignment.

## Foundational Learning

- **Concept: Competency Questions (CQs) as Functional Tests**
  - Why needed here: To understand why "suitability" is the primary metric. CQs are not just queries; they are functional requirements that the ontology must satisfy.
  - Quick check question: Does the question "What is the name of the artist?" test a data property (string) or an object property (relation)?

- **Concept: Readability vs. Complexity in Requirements Engineering**
  - Why needed here: The study distinguishes between surface readability (FKGL) and structural complexity (dependency depth). High complexity often correlates with higher modeling effort.
  - Quick check question: A question with a Flesch-Kincaid Grade Level of 12 is likely [easier / harder] for a general stakeholder to validate than one with a level of 6?

- **Concept: Semantic Embeddings for Coverage Analysis**
  - Why needed here: To interpret the "low overlap" results. Understanding that high cosine similarity of centroids does not imply that the sets contain the same specific questions.
  - Quick check question: If Set A and Set B have a high centroid similarity but low bidirectional coverage, are they discussing the same [topic / specific details]?

## Architecture Onboarding

- **Component map:** User Story (Markdown) -> Human Annotators (HA-1/HA-2), Pattern Instantiator (19 archetypes), LLMs (GPT 4.1, Gemini 2.5 Pro) -> Anonymized dataset (AskCQ) -> Expert Review (Suitability, Ambiguity) + NLP Pipeline (Readability, Complexity) + Sentence-BERT (Semantic Overlap)

- **Critical path:** The flow from User Story -> LLM Generation -> Expert Suitability Filtering. The study indicates the direct path (Manual) is most efficient for quality, but the LLM path provides volume/novelty that must be filtered.

- **Design tradeoffs:**
  - Manual: High precision/suitability, low volume, high human cost
  - Pattern: Medium precision, restrictive structure, lowest suitability score
  - LLM: Low precision (initially), high complexity, high volume/variability, low human cost for generation (high for refinement)

- **Failure signatures:**
  - The "Verbose LLM" signature: CQs exceeding 100 characters with multiple nested clauses (high c3 complexity)
  - The "Hallucinated Requirement": High semantic novelty but 0% coverage against ground truth (e.g., Gemini's output)
  - The "Vague Pattern": Pattern-based CQs with high ambiguity/comments (37% commented)

- **First 3 experiments:**
  1. Reproducibility Check: Generate CQs from the provided User Story using GPT-4 with the paper's prompt (temperature=0). Verify if the "verbose/complex" signature persists.
  2. Refinement Loop: Take 10 LLM-generated CQs rated "unsuitable" and manually simplify them to match the FKGL of the manual set. Measure the reduction in "Requirement Complexity" (c1).
  3. Coverage Stress Test: Calculate bidirectional coverage between two different LLMs (e.g., GPT vs. Claude) on a new user story to confirm if the low-overlap finding generalizes across models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent are the observed characteristics of manual, pattern-based, and LLM-generated CQs generalizable to domains other than cultural heritage?
- Basis in paper: The authors state in the Conclusion that "Future work will extend this study to other user stories and domains to assess the generalisability of our findings."
- Why unresolved: The current study relies exclusively on a single user story derived from a cultural heritage project, which limits the external validity of the results regarding complexity and suitability across different knowledge domains.
- What evidence would resolve it: Replicating the comparative analysis using requirement sets from diverse domains (e.g., biomedical or financial) and analyzing the statistical consistency of the CQ feature profiles.

### Open Question 2
- Question: Can specific prompt engineering or fine-tuning strategies align LLM-generated CQs with the readability and complexity profiles of human-authored questions?
- Basis in paper: The Discussion notes that insights on human CQ characteristics "can be leveraged to directly inform and improve their elicitation methods," suggesting "instructing LLMs with examples of well-formed CQs."
- Why unresolved: The study evaluated off-the-shelf LLMs which produced highly complex, less readable outputs; it did not test if guiding the LLMs with explicit constraints or examples could lower the complexity to human levels.
- What evidence would resolve it: A follow-up experiment using the AskCQ dataset where LLMs are prompted with few-shot examples of manual CQs to see if suitability and readability scores improve significantly.

### Open Question 3
- Question: What constitutes an "optimal set" of CQs regarding the trade-off between semantic consensus (overlap) and requirement coverage (diversity)?
- Basis in paper: The Discussion raises "the question of what constitutes an 'optimal sets' of CQs," noting that while high overlap indicates consensus, a degree of diversity ensures broader coverage.
- Why unresolved: The study found low semantic overlap between methods but did not determine if this novelty is beneficial for completeness or detrimental to consistency in ontology engineering.
- What evidence would resolve it: An empirical study measuring the performance of ontologies modeled using CQ sets with artificially controlled levels of diversity and overlap.

## Limitations
- Evaluation relies on a single user story, limiting generalizability across domains
- LLM-generated CQs showed high variability between models, suggesting results may not transfer to newer models
- "Suitability" metric depends on expert judgment, which may not fully capture downstream ontology modeling needs
- Pattern-based approach showed lowest suitability, but this could reflect the specific archetype set rather than inherent limitations

## Confidence

- **High**: Manual CQ superiority in suitability/readability; LLM complexity increase
- **Medium**: Pattern-based CQ performance ranking; semantic overlap findings
- **Low**: Generalizability across domains and LLMs; long-term modeling utility of CQs

## Next Checks

1. Test reproducibility with alternative user stories from different domains
2. Evaluate whether LLM-refined CQs (post-expert filtering) achieve manual-level suitability
3. Measure actual ontology modeling effort when using CQs from each method