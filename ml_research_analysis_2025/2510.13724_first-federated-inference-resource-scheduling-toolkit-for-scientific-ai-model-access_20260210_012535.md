---
ver: rpa2
title: 'FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model
  Access'
arxiv_id: '2510.13724'
source_url: https://arxiv.org/abs/2510.13724
tags:
- first
- inference
- globus
- requests
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIRST is a federated inference-as-a-service framework enabling
  secure, cloud-like access to AI models on HPC infrastructure. It leverages Globus
  Auth and Compute to provide an OpenAI-compliant API that can distribute requests
  across federated clusters while maintaining institutional security.
---

# FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access

## Quick Facts
- arXiv ID: 2510.13724
- Source URL: https://arxiv.org/abs/2510.13724
- Reference count: 39
- Key outcome: FIRST processed 8.7M inference requests on 24-node HPC cluster while maintaining OpenAI API compatibility and institutional security

## Executive Summary
FIRST is a federated inference-as-a-service framework that enables secure, cloud-like access to AI models on HPC infrastructure. The system leverages Globus Auth and Compute to provide an OpenAI-compliant API that distributes requests across federated clusters while maintaining institutional security. Deployed on a 24-node HPC cluster, FIRST successfully processed 8.7 million inference requests serving 76 users over 10 months while generating over 10 billion tokens. The framework demonstrates how traditional HPC infrastructure can bridge growing AI inference demands while preserving data governance and security requirements.

## Method Summary
FIRST implements a federated inference system using Globus Auth for authentication and authorization, with an OpenAI-compatible API interface. The architecture supports multiple inference backends and implements auto-scaling capabilities while maintaining "hot" nodes for low latency. The system processes requests through a federation manager that distributes workloads across available compute resources, handling both interactive and batch processing modes. Performance evaluation was conducted using Llama 3.3 70B models on a 24-node HPC cluster, comparing throughput against direct vLLM access and commercial cloud APIs.

## Key Results
- Achieved 23.9 requests/second throughput with four scaled instances of Llama 3.3 70B
- Generated 4131 tokens/second output throughput, outperforming direct vLLM access at high request rates
- Successfully served 76 users with 8.7 million inference requests and over 10 billion tokens in 10 months

## Why This Works (Mechanism)
FIRST succeeds by creating a unified API interface that abstracts the complexity of distributed inference across federated HPC resources. The OpenAI-compliant API allows existing AI applications to seamlessly integrate with the system without modification. Globus Auth provides robust authentication while Compute handles resource orchestration, enabling secure multi-institutional deployments. The auto-scaling mechanism dynamically adjusts compute resources based on demand, while hot nodes ensure low-latency responses for time-sensitive applications. This architecture effectively bridges the gap between traditional HPC infrastructure and modern AI inference workloads.

## Foundational Learning

**Globus Auth** - Secure authentication and authorization service for research computing environments. Why needed: Enables secure multi-institutional access while maintaining data governance. Quick check: Verify authentication flow works across federated domains.

**Inference Backend Abstraction** - Modular interface for different AI model serving frameworks. Why needed: Supports multiple model types and sizes across heterogeneous hardware. Quick check: Test with different model architectures (small, large, multimodal).

**Federation Manager** - Central coordinator that distributes inference requests across available resources. Why needed: Enables load balancing and resource utilization across distributed clusters. Quick check: Monitor request distribution and queue depths under varying loads.

**Auto-scaling Mechanism** - Dynamic resource allocation based on inference demand. Why needed: Optimizes resource usage while maintaining performance guarantees. Quick check: Verify scaling thresholds and response times during load spikes.

**Hot Node Maintenance** - Pre-warmed compute nodes for low-latency inference. Why needed: Reduces cold-start latency for interactive applications. Quick check: Measure latency improvements with hot vs cold nodes.

## Architecture Onboarding

**Component Map:** User -> OpenAI API -> Federation Manager -> Globus Auth -> Compute Resources -> Inference Backends

**Critical Path:** API Request → Authentication → Request Routing → Resource Allocation → Model Inference → Response

**Design Tradeoffs:** The system prioritizes security and compatibility over raw performance optimization. Using Globus infrastructure ensures institutional compliance but adds authentication overhead. OpenAI API compatibility sacrifices some potential performance optimizations for broad application support.

**Failure Signatures:** Authentication failures indicate credential or federation issues. Request routing failures suggest federation manager problems. Resource allocation failures point to compute resource constraints. Model inference failures indicate backend-specific issues.

**First Experiments:**
1. Verify basic authentication flow with test credentials across federated domains
2. Test request routing with simple models to validate federation manager functionality
3. Measure cold-start latency versus hot node performance for interactive use cases

## Open Questions the Paper Calls Out
None

## Limitations
- Performance benchmarks limited to Llama 3.3 70B model, may not generalize to other architectures
- Security model relies on Globus infrastructure without comprehensive vulnerability assessment
- Scalability beyond 24-node configuration not evaluated for potential bottlenecks

## Confidence

**High Confidence:** System architecture description, deployment statistics (8.7M requests, 76 users, 10B tokens), basic functionality of auto-scaling and hot node maintenance

**Medium Confidence:** Performance benchmarks with Llama 3.3 70B, comparison with vLLM and commercial APIs at tested request rates

**Low Confidence:** Scalability beyond tested configuration, security vulnerability assessment, performance with heterogeneous model types

## Next Checks
1. Conduct comprehensive performance testing across multiple model architectures (small, medium, large, multimodal) to validate claimed support for "multiple inference backends"
2. Perform security penetration testing and authentication latency analysis under sustained high-load conditions to verify the security claims and identify potential bottlenecks
3. Scale the system beyond 24 nodes to evaluate performance degradation, resource utilization efficiency, and federation management overhead at enterprise scale