---
ver: rpa2
title: Interpreting and Steering LLMs with Mutual Information-based Explanations on
  Sparse Autoencoders
arxiv_id: '2502.15576'
source_url: https://arxiv.org/abs/2502.15576
tags:
- mask
- features
- explanations
- arxiv
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting sparse autoencoder
  (SAE) features in large language models (LLMs) by identifying a "frequency bias"
  issue where existing methods emphasize linguistic patterns over semantic concepts.
  The authors propose using a fixed vocabulary set and designing a mutual information-based
  objective to better capture discourse-level semantic meanings behind learned features.
---

# Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders

## Quick Facts
- **arXiv ID:** 2502.15576
- **Source URL:** https://arxiv.org/abs/2502.15576
- **Reference count:** 32
- **Primary result:** Proposes MI-based SAE interpretation to counter frequency bias and runtime steering strategies for jailbreak defense

## Executive Summary
This paper addresses the challenge of interpreting sparse autoencoder (SAE) features in large language models (LLMs) by identifying a "frequency bias" issue where existing methods emphasize linguistic patterns over semantic concepts. The authors propose using a fixed vocabulary set and designing a mutual information-based objective to better capture discourse-level semantic meanings behind learned features. They further introduce two runtime steering strategies to adjust feature activations based on their explanations. Experimental results show their method generates more discourse-level explanations than baselines and effectively steers LLM behaviors to defend against jailbreak attacks, demonstrating both interpretability improvements and practical applicability in safety-critical applications.

## Method Summary
The method involves training a Top-K SAE on Mistral-7B-Instruct activations at layer 8, then interpreting each feature by selecting words from a fixed vocabulary that maximize mutual information with the feature. The MI objective normalizes word activations by their frequency across all features, addressing frequency bias. Features are annotated as safety-related or harmful using an LLM judge. At runtime, two steering strategies are applied: Amplification enhances feature awareness, while Calibration shifts representations toward target activation levels. The approach is evaluated on Salad-Bench for safety and MT-Bench for helpfulness, demonstrating improved jailbreak defense while maintaining model performance.

## Key Results
- MI-based interpretation reduces frequency bias, producing more discourse-level explanations than baselines
- Runtime steering at layer 8 with "Aware Security" strategy reduces jailbreak success rate from 94.2% to 73.2%
- Steering at layer 8 is more effective than layers 16 or 24 for jailbreak defense
- Helpfulness preserved with minimal degradation on MT-Bench

## Why This Works (Mechanism)

### Mechanism 1
A mutual information (MI) objective with a fixed vocabulary yields more discourse-level SAE feature explanations than baselines that suffer from "frequency bias" toward linguistic patterns. Interpretation is framed as selecting a word subset that maximizes mutual information with knowledge in feature vector W_c. The objective normalizes a word's activation by its activation across all features, penalizing high-frequency/low-specificity words and promoting diverse, concept-specific vocabulary.

### Mechanism 2
Modulating a small subset of semantically-identified SAE feature activations at runtime can steer LLM behavior, such as improving jailbreak defense. Two strategies modify the LLM's residual stream X using feature subset S: Amplification enhances or suppresses feature awareness, while Calibration shifts representations toward a target activation level. These alter representation geometry to influence downstream generation.

### Mechanism 3
Intervening at earlier layers (~25% depth) is more effective for behavioral steering than later layers. Early interventions modify the prompt's initial representation, giving subsequent layers more computational path to adjust generation trajectory. Later interventions may find outputs already determined.

## Foundational Learning

- **Sparse Autoencoders (SAEs) for Mechanistic Interpretability**: Core tool for decomposing dense, polysemantic hidden states into sparse, monosemantic feature vectors. Why needed here: Primary tool for decomposing dense, polysemantic hidden states into sparse, monosemantic feature vectors.
- **Mutual Information as a Feature Selection Objective**: The paper's innovation replaces simple activation-maximization with an MI-based word selection method. Why needed here: Replaces simple activation-maximization with an MI-based word selection method.
- **Representation Steering / Activation Engineering**: Practical application of interpretation work for safety and behavior modification. Why needed here: Practical application of interpretation work for safety and behavior modification.

## Architecture Onboarding

- **Component map**: Pre-trained LLM -> Sparse Autoencoder -> Interpreter Module -> Steering Controller
- **Critical path**: Train SAE on LLM activations → Compute MI-based interpretation → Classify features into conceptual categories → Apply steering intervention
- **Design tradeoffs**: Vocabulary size vs. precision, Intervention layer choice, Steering strength balance
- **Failure signatures**: Trivial explanations dominated by high-frequency words, Helpfulness degradation from over-refusal, No steering effect due to non-causal features
- **First 3 experiments**: Replicate explanation quality on GPT-2 small, Ablate MI objective to validate normalization term, Steering on simple task (French language generation)

## Open Questions the Paper Calls Out

### Open Question 1
Can sparse autoencoder architectures or training objectives be designed to inherently mitigate frequency bias during feature learning, rather than addressing it post-hoc at the explanation level? The current method only fixes frequency bias during explanation generation, leaving the underlying SAE features still encoding mixed discourse and linguistic information.

### Open Question 2
Why does activating safety-related features ("Aware Security") substantially outperform erasing harmful features for jailbreak defense, despite the intuitive appeal of the latter? The paper reports this empirical finding but does not investigate the underlying mechanism.

### Open Question 3
How does the optimal layer for steering interventions vary across model architectures, sizes, and task types? The relationship between intervention layer, feature abstraction level, and steering efficacy remains unclear.

## Limitations
- Vocabulary construction method and embedding alignment are underspecified
- Feature annotation process lacks detailed prompts and examples
- Computational cost of real-time MI-based interpretation not discussed
- Generalizability of safety features across different models/datasets unproven

## Confidence

**High Confidence**
- SAE training procedure and parameters are clearly specified
- MI objective formula is mathematically defined
- Steering strategies (Amplification/Calibration) are explicitly described
- Layer 8 intervention effectiveness for jailbreak defense

**Medium Confidence**
- MI-based explanations are more "discourse-level" than baselines
- Safety feature annotation process methodology
- Effectiveness on helpfulness preservation

**Low Confidence**
- General applicability of identified safety features across models
- Computational feasibility of real-time MI-based interpretation
- Assumption of predictable behavioral changes from linear steering

## Next Checks
1. **Vocabulary and Embedding Alignment Validation**: Implement vocabulary construction with specific frequency cutoffs and verify embedding alignment by projecting W_c into unembedding space.
2. **Baseline Comparison for Explanation Quality**: Implement TopAct baseline and directly compare explanation quality using same LLM judge on identical prompts.
3. **Cross-Model Feature Transferability**: Train SAE on different LLM and test whether same features activate during jailbreak attempts to validate universality.