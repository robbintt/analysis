---
ver: rpa2
title: 'Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions'
arxiv_id: '2512.15743'
source_url: https://arxiv.org/abs/2512.15743
tags:
- parts
- assembly
- generation
- part
- lego
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for generating physically buildable
  LEGO assemblies from natural language prompts. The core method uses a constrained
  part vocabulary and an LDraw intermediate representation, allowing large language
  models to produce valid step-by-step construction sequences.
---

# Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions

## Quick Facts
- arXiv ID: 2512.15743
- Source URL: https://arxiv.org/abs/2512.15743
- Authors: David Noever
- Reference count: 40
- Primary result: Generative AI framework produces physically buildable LEGO assemblies from natural language prompts using constrained part vocabulary and LDraw intermediate representation.

## Executive Summary
This paper introduces a framework for generating physically buildable LEGO assemblies from natural language prompts. The core method uses a constrained part vocabulary and an LDraw intermediate representation, allowing large language models to produce valid step-by-step construction sequences. A custom Python library enforces geometric validity and connection constraints. The approach is evaluated on four designs, including a 3,122-part International Space Station model with 112 assembly steps and a 47-part modular tool kit yielding 20 distinct configurations. The work demonstrates scalable, modular generation of physically realizable instructions, bridging semantic design intent and manufacturable output.

## Method Summary
The system generates LEGO assembly instructions through a pipeline coupling LLMs with tool-calling libraries. Natural language prompts or image descriptions are processed by an LLM, which uses a custom Python library to enforce geometric validity and output LDraw text format. The library provides part vocabularies, coordinate validation, and connection constraints through Model Context Protocol servers. Hierarchical decomposition breaks complex models into build phases with constrained sub-vocabularies. The framework validates syntactic correctness but not semantic physical realizability, meaning parts can float disconnected or overlap while maintaining valid LDraw syntax.

## Key Results
- Generated a 3,122-part International Space Station model with 112 assembly steps using only 17 distinct part types
- Created a 47-part modular tool kit yielding 20 distinct configurations through reconfiguration
- Demonstrated scalable generation from 50-100 part demonstrations to 3,000+ part assemblies using hierarchical decomposition
- Validated physical realizability through rendered outputs in LeoCAD and human-buildability testing

## Why This Works (Mechanism)

### Mechanism 1
Constraining output to a discrete part vocabulary with explicit coordinates improves generation reliability compared to unconstrained 3D generation. LDraw format provides a text-readable intermediate representation where each part has standardized identifiers, precise XYZ coordinates, and rotation matrices—similar to how FEN notation constrains chess state representation or SQL constrains database queries. Core assumption: LLMs perform better when targeting structured, verifiable output formats rather than continuous 3D meshes or unconstrained natural language instructions.

### Mechanism 2
A custom Python library acting as a "physical API" enforces geometric validity that pure prompt engineering cannot guarantee. The library exposes legal part placements, valid coordinate transforms, and connection constraints through tool-calling interfaces, shifting constraint enforcement from implicit model knowledge to explicit, debuggable code. Core assumption: LLMs cannot reliably maintain geometric consistency across thousands of tokens without external tool validation.

### Mechanism 3
Hierarchical decomposition enables scaling from 50–100 part demonstrations to 3,000+ part assemblies. Complex models are decomposed into build phases, each with constrained sub-vocabularies and sequential dependencies. Core assumption: Decomposition matches how humans construct assemblies and allows partial credit evaluation at each level.

## Foundational Learning

- **LDraw Format (.ldr files)**: Why needed here: This is the compilation target for all generated assemblies. One line per part: `1 <color> <x> <y> <z> <a1> <a2> <a3> <a4> <a5> <a6> <a7> <a8> <a9> <part.dat>` Quick check question: Given a 2×4 brick placed at origin (0,0,0) in red (color 4), write the LDraw line.

- **TRIZ Inventive Principles**: Why needed here: The paper uses TRIZ principles (Segmentation #1, Copying #26, Local Quality #3, Dynamics #15) to evaluate whether LLMs understand engineering tradeoffs in modular reconfiguration tasks. Quick check question: Which TRIZ principle explains why LEGO stud pitch (8mm) can serve as an inherent measurement standard without calibration?

- **Spatial Reasoning Limitations in LLMs**: Why needed here: Benchmarks (LEGO-Puzzles, PLUGH) show multi-step spatial reasoning accuracy drops toward 0% as sequence length increases. Understanding this explains why constrained tooling is necessary. Quick check question: Why does the "bag-of-bricks" evaluation methodology isolate spatial reasoning competencies better than end-to-end generation metrics?

## Architecture Onboarding

- **Component map**: Input Layer (prompt/image) -> Tool Layer (Python library) -> Generation Layer (LLM) -> Output Layer (.ldr file -> LeoCAD rendering)
- **Critical path**: Prompt formulation -> Python library configuration -> LDraw generation -> syntactic validation -> visual rendering -> human-buildability verification
- **Design tradeoffs**: Fidelity vs. vocabulary completeness (limited part library forces approximation), syntactic vs. semantic validity (system catches invalid LDraw but not floating parts), scalability vs. physics simulation (LDraw is idealized assembly space)
- **Failure signatures**: Parts that float disconnected from structural spine, volume interference conflicts, functional mismatches, instruction incoherence where step N references parts not yet placed
- **First 3 experiments**: 
  1. Generate a model requiring a part type not in the library (e.g., hinge, tapered wedge) and document approximation or failure
  2. Generate a 200+ part model and programmatically verify each step references only previously-placed parts
  3. Compare pure prompt engineering vs. tool-coupled generation on identical prompts, measuring syntactic validity rate and human-buildability score

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs reliably generate accurate models and instructions from arbitrary text prompts? While the paper demonstrates success in specific domains (satellites, castles), reliability on unconstrained, abstract, or highly diverse inputs remains the broader unverified challenge.

### Open Question 2
How can physics-aware validation be integrated to verify load-bearing stability and environmental survival? The current modeling environment is synthetic (LDraw) and cannot verify if configurations hold together under load or survive thermal/vibration stress.

### Open Question 3
How can the pipeline be modified to prevent "geometrically valid but physically unrealizable" assemblies? The system generates outputs with "parts that float disconnected" and "functional mismatches" because it optimizes for syntactic correctness over semantic validity.

### Open Question 4
How can physical test results be utilized for closed-loop refinement of assembly instructions? The current workflow is a feed-forward process from prompt to specification; it lacks a mechanism to learn from physical execution errors.

## Limitations
- System validates syntactic correctness but not semantic physical realizability—parts can float disconnected or overlap while maintaining valid LDraw syntax
- Constrained vocabulary forces approximations (ISS model uses only 17 part types vs. thousands in reality, helicopter model omits 231 specialized parts)
- Scaling validation beyond four demonstrations remains unproven, particularly for entirely novel geometries requiring parts outside the constrained vocabulary

## Confidence
- **High Confidence**: LDraw syntax generation and hierarchical decomposition methodology (directly validated through rendered outputs and modular construction sequences)
- **Medium Confidence**: Physical buildability of generated instructions (syntactic validity is proven, but semantic validation is not systematically tested)
- **Low Confidence**: Generalization to arbitrary prompts and novel geometries beyond the demonstrated examples (limited corpus of tested cases)

## Next Checks
1. **Vocabulary Stress Test**: Attempt generation of a model requiring specialized parts (hinges, tapered wedges, or curved slopes) not in the constrained library. Document whether the system successfully approximates missing geometries or fails entirely, and quantify the fidelity loss.

2. **Semantic Validity Analysis**: Develop automated detection for floating parts, volume conflicts, and functional mismatches in generated .ldr files. Apply this analysis to the four demonstrated models and measure the percentage of semantically invalid assemblies despite syntactic correctness.

3. **Scale and Generalization Benchmark**: Generate 10 new assembly prompts spanning different complexity levels (50-500 parts) not related to the original four examples. Measure D-score (syntax), M-score (physical validity), and I-score (instruction coherence) across all models to establish baseline performance and identify systematic failure patterns.