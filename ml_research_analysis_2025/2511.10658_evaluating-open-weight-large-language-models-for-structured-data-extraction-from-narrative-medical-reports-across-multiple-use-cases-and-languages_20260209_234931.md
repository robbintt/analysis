---
ver: rpa2
title: Evaluating Open-Weight Large Language Models for Structured Data Extraction
  from Narrative Medical Reports Across Multiple Use Cases and Languages
arxiv_id: '2511.10658'
source_url: https://arxiv.org/abs/2511.10658
tags:
- shot
- prompt
- graph
- missing
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated 15 open-weight large language
  models (LLMs) for structured data extraction from clinical reports across six diverse
  use cases, three languages, and two report types. Across six prompting strategies,
  top-performing models achieved macro-average scores closely aligned with inter-rater
  agreement, ranging from 0.70 to 0.95.
---

# Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages

## Quick Facts
- arXiv ID: 2511.10658
- Source URL: https://arxiv.org/abs/2511.10658
- Reference count: 40
- Primary result: Open-weight LLMs can extract structured clinical data at expert levels, with macro-average scores (0.70-0.95) matching inter-rater agreement

## Executive Summary
This study systematically evaluated 15 open-weight large language models for structured data extraction from clinical reports across six diverse use cases, three languages, and two report types. Across six prompting strategies, top-performing models achieved macro-average scores closely aligned with inter-rater agreement, ranging from 0.70 to 0.95. Small and medium general-purpose models performed comparably to larger ones, while tiny and medical-specialized models performed worse. Prompt graph and few-shot prompting improved performance by approximately 13% over zero-shot. Task-specific factors influenced performance more than model size or prompting strategy. These findings demonstrate that open-weight LLMs can extract structured data from clinical reports at expert annotation levels, offering a scalable approach for clinical data curation.

## Method Summary
The study evaluated 15 open-weight LLMs across six clinical use cases, three languages, and two report types using six prompting strategies. Performance was measured using macro-average scores and compared against inter-rater agreement between human experts. The evaluation tested zero-shot, few-shot, and prompt graph approaches, with results showing that task-specific factors were more influential than model size or prompting strategy. The study also compared general-purpose models against tiny and medical-specialized models.

## Key Results
- Top-performing models achieved macro-average scores of 0.70-0.95, closely matching inter-rater agreement
- Small and medium general-purpose models performed comparably to larger models
- Prompt graph and few-shot prompting improved performance by approximately 13% over zero-shot
- Task-specific factors influenced performance more than model size or prompting strategy

## Why This Works (Mechanism)
The study's success stems from leveraging the inherent language understanding capabilities of large language models, which have been trained on diverse text corpora including medical literature. The models' ability to parse narrative clinical reports and extract structured data relies on their capacity to understand context, recognize patterns, and map unstructured text to predefined schemas. The prompt engineering strategies, particularly few-shot learning and prompt graphs, help guide the models toward the specific extraction tasks by providing examples and logical reasoning paths.

## Foundational Learning
- Clinical report structure recognition: Why needed - Clinical reports vary widely in format and terminology; Quick check - Can the model consistently identify key sections (history, findings, impressions) across different report types
- Domain-specific terminology mapping: Why needed - Medical terms have precise meanings that differ from general usage; Quick check - Does the model correctly interpret abbreviations and medical jargon in context
- Schema alignment: Why needed - Structured data must match predefined templates for downstream analysis; Quick check - Can the model output data in the required format with correct field types
- Cross-language medical concepts: Why needed - Medical concepts should be consistently extracted across languages; Quick check - Does the model maintain accuracy when switching between languages
- Context preservation: Why needed - Clinical narratives often require understanding relationships between concepts; Quick check - Can the model maintain patient context across multi-paragraph reports

## Architecture Onboarding

Component Map:
Clinical Reports -> Preprocessing Pipeline -> LLM Processing -> Structured Output -> Validation

Critical Path:
1. Clinical report ingestion and preprocessing
2. Prompt engineering and template application
3. LLM inference and response generation
4. Structured data validation and formatting

Design Tradeoffs:
- Model size vs. inference speed and cost
- General-purpose vs. domain-specific pretraining
- Zero-shot vs. few-shot vs. prompt graph strategies
- Single model vs. ensemble approaches

Failure Signatures:
- Misinterpretation of medical abbreviations or terminology
- Inconsistent handling of negations or temporal relationships
- Difficulty with multi-step reasoning across report sections
- Performance degradation with complex nested medical concepts

First Experiments:
1. Test model performance on simple structured fields (dates, patient IDs) before complex clinical concepts
2. Evaluate cross-language consistency using parallel reports in different languages
3. Compare zero-shot vs. few-shot performance on a single use case to establish baseline improvements

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation covered only 15 models, limiting generalizability to all available open-weight models
- Results are based on specific model families and may not extend to all architectures
- The 13% improvement from prompt graph and few-shot strategies may not translate uniformly across all clinical use cases

## Confidence

High: Model performance relative to expert inter-rater agreement (0.70-0.95)
High: Task-specific factors being more influential than model size
Medium: Prompting strategy improvements (13% gain)
Medium: Small/medium model performance parity with larger models

## Next Checks

1. Replicate findings across additional clinical specialties beyond the six tested use cases, particularly in surgical and procedural documentation
2. Test model performance on larger-scale datasets (thousands rather than hundreds of reports) to assess scalability and potential performance degradation
3. Conduct head-to-head comparisons with domain-specific fine-tuned models to quantify the practical trade-offs between general-purpose LLMs and specialized approaches