---
ver: rpa2
title: Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization
  toward 2-bit Instruction-Tuned LLMs
arxiv_id: '2506.09104'
source_url: https://arxiv.org/abs/2506.09104
tags:
- int2
- quantization
- int4
- llms
- instruction-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of quantizing instruction-tuned
  large language models (LLMs) to extremely low-bit precision (2-bit) without relying
  on proprietary post-training data, which is typically unavailable in open-source
  settings. The proposed solution, Unified Progressive Quantization (UPQ), introduces
  a two-stage framework: first, it applies INT4 block-wise post-training quantization
  (PTQ) to reduce quantization error, and then uses distillation-based quantization-aware
  training (Distill-QAT) to recover instruction-following capabilities by minimizing
  the generalized Jensen-Shannon divergence between the quantized model and its original
  FP16 counterpart.'
---

# Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs

## Quick Facts
- arXiv ID: 2506.09104
- Source URL: https://arxiv.org/abs/2506.09104
- Reference count: 34
- Key outcome: Achieves 2-bit quantization of instruction-tuned LLMs (1B-8B) using progressive FP16→INT4→INT2 pipeline with block-wise PTQ and distillation-based QAT

## Executive Summary
This paper tackles the challenge of quantizing instruction-tuned large language models (LLMs) to extremely low-bit precision (2-bit) without relying on proprietary post-training data, which is typically unavailable in open-source settings. The proposed solution, Unified Progressive Quantization (UPQ), introduces a two-stage framework: first, it applies INT4 block-wise post-training quantization (PTQ) to reduce quantization error, and then uses distillation-based quantization-aware training (Distill-QAT) to recover instruction-following capabilities by minimizing the generalized Jensen-Shannon divergence between the quantized model and its original FP16 counterpart. Experiments on models ranging from 1B to 8B parameters demonstrate that UPQ significantly outperforms existing methods, achieving state-of-the-art results on MMLU and IFEval benchmarks while preserving core LLM capabilities without proprietary data.

## Method Summary
The UPQ framework consists of two stages: Stage 1 applies block-wise PTQ using FlexRound with a zero-excluding integer set (-15 to -1, 1 to 15) to quantize the FP16 instruction-tuned model to INT4, minimizing reconstruction error. Stage 2 initializes Distill-QAT from the INT4 checkpoint using Stretched Elastic Quant (SEQ) with weights mapped to {-3, -1, 1, 3}, and trains with Generalized JSD loss (β=0.5) against the FP16 teacher on 5-30B tokens of pre-training data. The framework requires 1-2M calibration tokens and uses AdamW optimizer with learning rate 2×10⁻⁵.

## Key Results
- Achieves state-of-the-art 2-bit quantization performance on MMLU and IFEval benchmarks for instruction-tuned LLMs
- Sequential initialization (FP16→INT4→INT2) consistently outperforms direct FP16→INT2 quantization
- Generalized JSD loss successfully recovers instruction-following capabilities compared to Next-Token Prediction loss
- Block-wise PTQ methods (FlexRound) significantly outperform layer-wise alternatives for the INT4 initialization step

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Initializing 2-bit QAT from an intermediate 4-bit PTQ checkpoint reduces optimization difficulty compared to initializing directly from FP16.
- **Mechanism:** The paper posits that the FP16 → INT4 PTQ step reorganizes weights to minimize reconstruction error, placing them into a configuration that maps more naturally to the target INT2 bins. Specifically, the INT4 intermediate step increases the utilization of large-magnitude INT2 bins ({−3, 3}), which reduces the initial quantization error distance (L₂) compared to a direct FP16 → INT2 jump. This results in lower initial training loss and faster convergence.
- **Core assumption:** The optimization landscape for INT2 QAT is smoother when starting from a quantized INT4 basin than from the full-precision FP16 basin.
- **Evidence anchors:** [abstract] "UPQ first quantizes FP16 instruction-tuned models to INT4... to significantly reduce the quantization error introduced by subsequent INT2 quantization." [section 3.2] "...initializing INT2 QAT from W_{INT4→INT2} rather than W_{FP16→INT2} consistently yields substantially lower training loss." [corpus] Related work in "Matryoshka Quantization" explores nested quantization, but UPQ specifically isolates the sequential initialization benefit for instruction-tuned models.
- **Break condition:** If the INT4 PTQ method used cannot effectively preserve the original model's capabilities (i.e., if the INT4 baseline is broken), the INT2 initialization will likely fail to recover performance.

### Mechanism 2
- **Claim:** Generalized Jensen-Shannon Divergence (JSD) is required to recover instruction-following capabilities (IFEval), whereas Next-Token Prediction (NTP) loss is insufficient.
- **Mechanism:** The paper argues that because QAT uses generic pre-training data (e.g., C4) rather than proprietary instruction data, standard NTP loss fails to reinforce the specific "instruction-response" alignment. By minimizing Generalized JSD between the INT2 student and FP16 teacher, the student mimics the teacher's token-level probability distribution, effectively transferring the instruction-following behavior without needing the original instruction dataset.
- **Core assumption:** The teacher's probability distribution over generic text contains sufficient signal to recover instruction-following behavior, even without explicit instruction prompts in the training data.
- **Evidence anchors:** [abstract] "...minimizing the generalized Jensen-Shannon divergence (JSD) between the two." [section 3.3] "...minimizing the next-token prediction loss on pre-training data... often proves insufficient for restoring the instruction-following capability." [corpus] Weak direct corpus evidence for JSD specifically in 2-bit QAT; most related works (e.g., BitDistiller) use KL-based variants.
- **Break condition:** If the divergence metric is too strict (e.g., forcing exact match on generic text), it may degrade the model's language modeling capabilities (perplexity) while fixing instruction following.

### Mechanism 3
- **Claim:** Using a zero-excluding quantization grid (Stretched Elastic Quant) is necessary for effective 2-bit weight representation.
- **Mechanism:** Standard symmetric/asymmetric INT2 quantization allocates a bin for "0", which forces an uneven split of the remaining 3 bins. Since LLM weights are bell-shaped and centered near zero, this creates an imbalance. The paper adopts SEQ to map weights to {-3, -1, 1, 3}, ensuring balanced bin allocation and better alignment with the weight distribution.
- **Core assumption:** The weight distribution is sufficiently Gaussian-like that a symmetric, zero-excluding grid provides higher fidelity than an asymmetric grid including zero.
- **Evidence anchors:** [section 3.1] "...both [symmetric and asymmetric] are inherently limited due to the inclusion of '0'... we adopt Stretched Elastic Quant (SEQ)." [corpus] General consensus in "Binary Neural Networks" surveys that low-bit quantization requires specialized grids to minimize information loss.
- **Break condition:** If applied to a model with a significantly non-zero-centered weight distribution, this grid might induce systematic bias.

## Foundational Learning

- **Concept: Block-wise Reconstruction (PTQ)**
  - **Why needed here:** This is the "Stage 1" of UPQ. You must understand that unlike standard PTQ which quantizes layer-by-layer, block-wise methods (like FlexRound) optimize a group of layers together to account for inter-layer dependencies.
  - **Quick check question:** Can you explain why optimizing a whole transformer block (e.g., Attention + MLP) might yield a better INT4 initialization than optimizing linear layers individually?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** The core of "Stage 2". Instead of learning "hard labels" or just next tokens (NTP), the student learns the "soft probabilities" (logits) of the teacher.
  - **Quick check question:** Why would mimicking the probability distribution of a teacher be better than just predicting the next token when data is scarce or out-of-distribution?

- **Concept: Quantization-Aware Training (QAT) & STE**
  - **Why needed here:** The mechanism for training "through" the quantization function. Since rounding is non-differentiable, you need to understand the Straight-Through Estimator (STE) to debug gradient flow.
  - **Quick check question:** During backpropagation, how does the gradient pass through the rounding function round(x) if its derivative is technically zero almost everywhere?

## Architecture Onboarding

- **Component map:** FP16 Instruction-Tuned LLM -> Stage 1 (FlexRound Block-wise PTQ) -> INT4 Model -> Stage 2 (Distill-QAT with SEQ + JSD Loss) -> INT2 Model

- **Critical path:** The initialization for Stage 2 is critical. Do not re-initialize the Step-size (Δ) or weights randomly for QAT. The QAT must be initialized from the specific weights produced by the Block-wise PTQ step (W_{INT4}).

- **Design tradeoffs:**
  - **FlexRound vs. OmniQuant:** The paper notes FlexRound slightly outperforms OmniQuant in this specific pipeline (Table 2). Use FlexRound for the INT4 step.
  - **NTP vs. Distillation:** NTP improves perplexity (WikiText2) but fails IFEval. Distillation (JSD) fixes IFEval but may slightly increase perplexity compared to NTP (Table 4). Choose the loss based on your deployment priority (Reasoning vs. Generation quality).

- **Failure signatures:**
  - **Repetitive Loops:** If the model generates text that repeats the prompt or enters a loop (Table 1, Table 5), it indicates a failure in the Distillation phase or a lack of "compensatory" training dynamics.
  - **Random Guessing (MMLU ~25%):** If MMLU scores collapse to random chance, the INT2 initialization was likely too far from the solution, or the learning rate destroyed the pre-trained features.

- **First 3 experiments:**
  1. **Sanity Check (INT4 PTQ):** Run FlexRound on the target FP16 model. Verify MMLU/IFEval scores are >90% of the FP16 baseline. If this fails, INT2 will fail.
  2. **Ablation Loss (Stage 2):** Run INT2 QAT with NTP loss vs. JSD loss on a small data subset (e.g., 1B tokens). Confirm NTP collapses IFEval while JSD maintains it (reproducing Figure 1).
  3. **Initialization Validation:** Compare starting INT2 QAT from the INT4 checkpoint vs. the raw FP16 checkpoint. Plot training loss (Figure 2c) to confirm the INT4 start converges faster and lower.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the UPQ framework generalize effectively to extremely large language models (e.g., 100B+ parameters)?
- **Basis in paper:** [explicit] Section H (Limitations) asks, "examining whether UPQ generalizes consistently to much larger language models (e.g., 100B+ parameters) is an important question to answer."
- **Why unresolved:** The experiments were limited to models ranging from 1B to 8B parameters due to resource constraints.
- **What evidence would resolve it:** Benchmark results on 100B+ instruction-tuned models showing comparable preservation of MMLU and IFEval scores.

### Open Question 2
- **Question:** Can UPQ be extended to include activation quantization to unlock full memory and latency benefits?
- **Basis in paper:** [explicit] Section H states, "Extending UPQ to include activation quantization would unlock the memory and latency benefits of extremely low-bit inference."
- **Why unresolved:** The current framework focuses exclusively on weight-only quantization, leaving activations in higher precision (FP16).
- **What evidence would resolve it:** A modified UPQ pipeline that successfully quantizes both weights and activations to low-bit precision without significant accuracy loss.

### Open Question 3
- **Question:** Can combining 2-bit QAT with other advanced distillation losses further improve performance?
- **Basis in paper:** [explicit] Section 2.2 states, "The question of whether combining 2-bit QAT with advanced distillation losses can further improve performance invites future investigation."
- **Why unresolved:** There has not been extensive study on distillation losses specifically targeting extremely low-bit precision.
- **What evidence would resolve it:** Comparative ablation studies integrating alternative distillation losses into the UPQ framework against the current Generalized JSD.

### Open Question 4
- **Question:** Does UPQ preserve capabilities in domain-specific or multimodal tasks like code generation or image-text reasoning?
- **Basis in paper:** [explicit] Section H suggests, "there may be domain-specific or multimodal tasks... that would require additional fine-tuning techniques."
- **Why unresolved:** The current evaluation focuses on general language understanding and instruction following (MMLU, IFEval).
- **What evidence would resolve it:** Evaluation of UPQ-quantized models on specialized benchmarks (e.g., HumanEval for coding) or multimodal datasets.

## Limitations
- The two-stage framework may not generalize beyond the specific model architectures tested (Llama 3.2 1B/3B, Llama 3.1 8B)
- Effectiveness of zero-excluding quantization grid assumes Gaussian-like weight distributions, which may not hold for all model types
- Reliance on Generalized JSD for instruction-following recovery lacks theoretical justification for why it works better than NTP on generic data
- Limited ablation on calibration set size and quality sensitivity leaves uncertainty about robustness to data variations

## Confidence

**High Confidence:**
- Sequential initialization (FP16→INT4→INT2) provides meaningful benefits over direct FP16→INT2 initialization, well-supported by training loss curves and ablation studies across multiple model sizes
- Block-wise PTQ methods outperform layer-wise alternatives for the INT4 initialization step, with empirical evidence across different quantization methods

**Medium Confidence:**
- Generalized JSD loss is necessary and sufficient for recovering instruction-following capabilities compared to NTP loss, though theoretical justification for its effectiveness on generic data is not fully developed
- Zero-excluding quantization grid (SEQ) is superior to traditional symmetric/asymmetric grids for 2-bit quantization, based on distribution assumptions that may not generalize to all model types

**Low Confidence:**
- UPQ framework will scale effectively to larger models (>8B parameters) or different instruction-tuned architectures, as experiments only tested three specific model sizes from two families
- Framework's robustness to variations in calibration data quality and quantity, as paper uses fixed calibration sets without exploring sensitivity to these parameters

## Next Checks
1. **Distribution Robustness Test:** Evaluate UPQ on a model with known non-zero-centered weight distribution (such as a model with bias-shift initialization or different activation functions) to test whether the SEQ quantization grid assumption holds beyond Gaussian-like distributions.

2. **Architecture Generalization Test:** Apply UPQ to instruction-tuned models from different architecture families (e.g., Mistral, Gemma, or DeepSeek) to verify the framework's effectiveness beyond the Llama family, particularly testing whether block-wise PTQ consistently outperforms layer-wise methods across architectures.

3. **Calibration Data Sensitivity Analysis:** Systematically vary the size and quality of the calibration set used for the INT4 PTQ stage (testing ranges from 100K to 10M tokens, and varying quality thresholds) to quantify the framework's robustness to data quality variations and establish minimum viable calibration requirements.