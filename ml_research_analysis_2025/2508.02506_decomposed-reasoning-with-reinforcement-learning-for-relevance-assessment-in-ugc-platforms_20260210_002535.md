---
ver: rpa2
title: Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in
  UGC Platforms
arxiv_id: '2508.02506'
source_url: https://arxiv.org/abs/2508.02506
tags:
- relevance
- document
- query
- reasoning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces R\xB3A, a novel decomposed reasoning framework\
  \ for relevance assessment in UGC platforms. The method addresses challenges of\
  \ ambiguous user intent and noisy content by leveraging auxiliary in-platform documents\
  \ to infer query intent and requiring verbatim fragment extraction from candidate\
  \ documents."
---

# Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms

## Quick Facts
- arXiv ID: 2508.02506
- Source URL: https://arxiv.org/abs/2508.02506
- Reference count: 20
- One-line primary result: R³A achieves 83.1% AUC on 0/12 metric and 65.2% accuracy on UGC relevance assessment

## Executive Summary
R³A introduces a decomposed reasoning framework for relevance assessment in UGC platforms, addressing challenges of ambiguous user intent and noisy content through auxiliary document-augmented intent inference and verbatim fragment extraction. The method employs reinforcement learning with Group Relative Policy Optimization to train a two-round interaction model that first infers query intent from auxiliary documents, then extracts supporting fragments from candidate documents to ground relevance decisions. Experiments on the NoteRel dataset demonstrate significant improvements over baseline methods, with the distilled 1.5B model surpassing larger SFT models and showing practical effectiveness in online A/B testing with a 1.03% reduction in re-query rate.

## Method Summary
R³A operates through a two-round interaction process: Round 1 uses auxiliary high-ranked documents retrieved using the query to infer latent user intent, while Round 2 performs verbatim fragment extraction from the candidate document to justify relevance decisions. The framework is trained using cold-start supervised fine-tuning with DeepSeek-R1 annotations followed by reinforcement learning with GRPO. The method addresses sparse feedback signals in UGC platforms by grounding relevance assessment in actual content through extraction constraints, and stabilizes RL training through group-relative rewards without requiring a separate value function. The approach is evaluated on a three-class relevance task (0=Irrelevant, 1=Partially Relevant, 2=Highly Relevant) using the NoteRel dataset.

## Key Results
- R³A-7B achieves 83.1% AUC on the 0/12 metric and 65.2% accuracy
- The distilled R³A-1.5B model surpasses the larger 7B SFT model by 1.7% in accuracy
- Online A/B testing shows a 1.03% reduction in re-query rate with R³A

## Why This Works (Mechanism)

### Mechanism 1: Auxiliary Document-Augmented Intent Inference
- **Claim**: Providing high-ranked auxiliary documents alongside the query helps the model infer latent user intent when feedback signals are sparse.
- **Mechanism**: The model conditions on both the query q and retrieved auxiliary documents d' to produce an intent representation, creating an enriched context that compensates for ambiguous or underspecified queries.
- **Core assumption**: High-ranked documents retrieved using the same query contain signals that correlate with the user's underlying information need.
- **Evidence anchors**:
  - [abstract] "R³A first leverages auxiliary high-ranked documents within the platform to infer latent query intent."
  - [section 2.2] "In the first round, the model engages with a set of auxiliary highly ranked documents d′, retrieved using the same query q within the platform, to parse the underlying query intent."
- **Break condition**: If the retrieval system fails to surface relevant auxiliary documents (e.g., due to vocabulary mismatch or cold-start queries), intent inference may be misled or remain ambiguous.

### Mechanism 2: Verbatim Fragment Extraction as Grounding Constraint
- **Claim**: Requiring the model to extract verbatim fragments from the candidate document grounds the relevance assessment in actual content, reducing noise-induced misjudgments.
- **Mechanism**: By constraining outputs to exact excerpts (or "None" if nothing matches), the model must attend to and verify the presence of query-relevant content rather than relying on superficial semantic associations.
- **Core assumption**: Query-relevant content in UGC can be identified via exact phrase matches, and forcing extraction exposes the model's evidence chain.
- **Evidence anchors**:
  - [abstract] "It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC."
  - [section 3.3, ablation] "The removal of extraction reasoning causing the most significant degradation. This underscores the importance of grounding relevance assessment in the candidate document."
- **Break condition**: When relevant content exists but is paraphrased or semantically equivalent without lexical overlap, the extraction constraint may yield false negatives.

### Mechanism 3: Group Relative Policy Optimization for Reasoning Alignment
- **Claim**: Optimizing with GRPO—using within-group relative rewards as advantages—stabilizes RL training and encourages reasoning trajectories aligned with reward signals.
- **Mechanism**: Multiple rollouts per input generate a group of trajectories; standardizing rewards within the group (Ai = (ri - μr) / σr) reduces variance and provides comparative learning signals without requiring a separate value function.
- **Core assumption**: High-quality reasoning patterns can emerge from rule-based rewards (format compliance + score correctness) without explicit process supervision.
- **Evidence anchors**:
  - [section 2.2] "GRPO performs multiple rollouts per input and calculate the relative reward r within the group as the advantage A."
  - [section 3.2, Figure 3] "R³A models initialized with cold start exhibit faster reward growth and achieve higher final rewards compared to their R³A-Zero counterparts."
- **Break condition**: If the reward function is misspecified (e.g., λ penalties misaligned with task difficulty), the policy may optimize for format compliance over genuine reasoning quality.

## Foundational Learning

- **Concept: Policy Gradient Methods and Advantage Estimation**
  - Why needed here: R³A uses GRPO, which relies on advantage-based policy updates; understanding how advantages normalize rewards across rollouts is essential for debugging training dynamics.
  - Quick check question: Can you explain why GRPO uses within-group reward standardization instead of a learned value function?

- **Concept: Chain-of-Thought Reasoning in LLMs**
  - Why needed here: The framework explicitly structures outputs with reasoning tags (```...```) before answers; cold-start uses DeepSeek-R1's CoT annotations.
  - Quick check question: What is the role of structured reasoning formats in enabling RL to shape model behavior?

- **Concept: Retrieval-Augmented Generation (RAG) Systems**
  - Why needed here: R³A operates within RAG pipelines, where relevance assessment directly impacts generation quality; understanding the failure modes of retrieval—especially in UGC—is critical.
  - Quick check question: How does sparse feedback in RAG (answer-level vs. document-level) differ from traditional search click signals?

## Architecture Onboarding

- **Component map**: Query q → Retriever → Auxiliary Docs d' (Round 1) → Intent Inference → Candidate Doc d (Round 2) → Fragment Extraction + Scoring → Relevance Score (0/1/2)
- **Critical path**: The two-round interaction is sequential; intent from Round 1 conditions Round 2 scoring. If intent inference fails, downstream extraction and scoring inherit the error.
- **Design tradeoffs**:
  - **Two-round vs. single-round**: Ablation shows single-round degrades accuracy by ~4.6%; two-round separates intent reasoning from document evaluation but increases inference cost.
  - **Verbatim extraction vs. free-form**: Constraining to exact fragments improves grounding but may miss paraphrased relevance.
  - **Cold-start vs. Zero**: Cold-start accelerates convergence but requires labeled/annotated data; Zero works but converges slower.
- **Failure signatures**:
  - Model outputs "None" for extraction when relevant content exists (likely paraphrase mismatch).
  - Intent inference overly influenced by noisy auxiliary docs, leading to misaligned scoring.
  - Training rewards plateau early—check reward function λ settings or rollout diversity.
- **First 3 experiments**:
  1. **Validate retrieval quality**: Run the auxiliary document retriever on a sample of queries; manually verify that top-k docs contain intent-relevant signals before training.
  2. **Ablate extraction constraint**: Disable verbatim extraction, allow free-form justification; compare accuracy and noise robustness on a held-out UGC set.
  3. **Pilot RL with different λ values**: Train small-scale models with λ ∈ {0, 0.1, 0.5}; observe impact on near-miss penalties and class boundary sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does R³A generalize to domains beyond UGC, such as biomedical, legal, or academic texts where language style and relevance criteria differ significantly?
- Basis in paper: [explicit] The Limitations section states: "R³A is evaluated primarily on industry-specific UGC dataset. Its performance may not generalize well to other domains such as biomedical or legal texts, where language style and relevance criteria differ significantly."
- Why unresolved: The method was developed and evaluated exclusively on Xiaohongshu's UGC platform data, with no cross-domain validation performed.
- What evidence would resolve it: Benchmarking R³A on standard relevance datasets from other domains (e.g., TREC-COVID for biomedical, legal case retrieval datasets) and reporting comparative performance against domain-specific baselines.

### Open Question 2
- Question: What is the relationship between auxiliary retrieval quality and R³A's effectiveness, and at what retrieval degradation threshold does intent inference become unreliable?
- Basis in paper: [explicit] The Limitations section notes: "Since the in-platform document retrieval pipeline is dependent on retrieval quality, suboptimal retrieval results may lead to incorrect estimation of user intent and misalignment with the target document under assessment."
- Why unresolved: No experiments were conducted to systematically degrade retrieval quality and measure impact; the ablation only tested complete removal of retrieved documents, not intermediate quality levels.
- What evidence would resolve it: Controlled experiments with progressively noisy or irrelevant auxiliary documents, measuring the correlation between retrieval metrics (e.g., MRR, NDCG of auxiliary docs) and R³A assessment accuracy.

### Open Question 3
- Question: What techniques could improve classification of the "Partially Relevant" (class 1) category, which shows consistently limited F1 improvements across all methods?
- Basis in paper: [inferred] Table 1 shows F1 for class 1 ranges only 42.8–56.0 across all methods, and the paper notes: "improvements in F1-score for class 1 are limited across models—highlighting the difficulty of corner cases."
- Why unresolved: The paper acknowledges this limitation but does not propose specific interventions; the binary distinction between relevant/irrelevant appears more learnable than the graded relevance scale.
- What evidence would resolve it: Targeted modifications such as explicit boundary-case training samples, multi-task learning with finer granularity, or hierarchical classification that first determines relevance then partiality.

## Limitations
- Performance may not generalize well to domains with different language styles and relevance criteria (biomedical, legal, academic texts)
- Effectiveness depends on retrieval quality, with suboptimal results potentially leading to incorrect intent estimation
- Limited improvements in F1-score for partially relevant (class 1) cases across all methods

## Confidence
- **High**: General architecture (two-round interaction, verbatim extraction, GRPO training) with consistent ablation improvements
- **Medium**: Attribution of performance gains to specific mechanisms, particularly intent inference from auxiliary documents
- **Low**: Online A/B testing results due to lack of controlled experimental conditions

## Next Checks
1. **Retrieval Quality Audit**: Sample 100 queries and manually verify that top-5 auxiliary documents contain intent-relevant signals. Measure the correlation between retrieval relevance and downstream scoring accuracy.
2. **Paraphrase Robustness Test**: Create a test set where relevant content exists but is paraphrased (not verbatim). Compare R³A performance against a version with free-form extraction to quantify the extraction constraint's limitations.
3. **Offline-to-Online Gap Analysis**: Compare R³A's offline metrics with controlled A/B test results using the same evaluation methodology, controlling for query distribution and time periods to isolate the model's true impact.