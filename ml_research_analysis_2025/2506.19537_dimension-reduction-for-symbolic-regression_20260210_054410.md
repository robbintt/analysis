---
ver: rpa2
title: Dimension Reduction for Symbolic Regression
arxiv_id: '2506.19537'
source_url: https://arxiv.org/abs/2506.19537
tags:
- regression
- symbolic
- search
- beam
- substitutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an iterative dimension reduction technique
  for symbolic regression that systematically searches for valid variable substitutions
  to simplify regression problems. The core idea is to enumerate small expression
  DAGs as candidate substitutions and validate them using functional dependence measures
  (CODEC and KMAC).
---

# Dimension Reduction for Symbolic Regression

## Quick Facts
- **arXiv ID:** 2506.19537
- **Source URL:** https://arxiv.org/abs/2506.19537
- **Reference count:** 15
- **Key outcome:** Iterative dimension reduction technique using functional dependence measures improves symbolic regression recovery rates by ~10-28% across diverse algorithms

## Executive Summary
This paper introduces an iterative dimension reduction technique for symbolic regression that systematically searches for valid variable substitutions to simplify regression problems. The core idea is to enumerate small expression DAGs as candidate substitutions and validate them using functional dependence measures (CODEC and KMAC). The approach can handle both input substitutions (replacing variable combinations with a single variable) and out-input substitutions (involving the output variable). A beam search framework efficiently explores the space of possible substitutions, and the resulting simplified problems are solved using any symbolic regression algorithm.

## Method Summary
The method works by iteratively finding valid substitutions that reduce the dimensionality of symbolic regression problems. It enumerates small expression DAGs as candidate substitution functions, then validates them using functional dependence measures (CODEC or KMAC) to test whether transformed observations maintain dependence between new inputs and outputs. The search uses beam search with beam size 1 to efficiently explore the space. Both input substitutions (replacing variable combinations with a single variable) and out-input substitutions (involving the output variable) are supported. After reduction, the simplified problem is solved with a base symbolic regression algorithm, and results are reconstructed using a computer algebra system.

## Key Results
- Reduces variables by ~50% on average across 880 Wikipedia equations and 114 Feynman formulas
- Improves recovery rates significantly: UDFS from 58% to 69%, polynomial regression from 6% to 34%
- Consistently increases Jaccard similarity between recovered and ground-truth expressions
- CODEC outperforms volume heuristics in noise robustness and is significantly faster than KMAC
- Adding out-input substitutions increases reduction rates significantly on both datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing independent variables makes symbolic regression tractable
- **Mechanism:** Variables in natural formulas often appear in fixed combinations (e.g., $r \cdot \cos(\phi)$). By identifying substitution function $g(x_I)$ that captures this combination, dimension decreases from $d$ to $d - |I| + 1$, reducing expression tree complexity.
- **Core assumption:** Target formula contains compositional structures where subsets of variables interact in isolation from the rest.
- **Evidence anchors:** Abstract states reducing complexity through dimension reduction; section 1 shows size reduced from twelve to nine nodes.
- **Break condition:** If variables interact diffusely across entire expression without distinct sub-structures.

### Mechanism 2
- **Claim:** Valid substitution iff transformed observations maintain functional dependence
- **Mechanism:** Validity determined by statistical test of functional dependence (CODEC coefficient), not heuristic pattern matching. If $y$ is measurable function of transformed inputs, CODEC approaches 1.
- **Core assumption:** Data samples are sufficiently dense to distinguish functional dependence from statistical independence using rank-based measures.
- **Evidence anchors:** Abstract states validity test reduced to functional dependence test; section 3 shows CODEC converges to 1 if Y is measurable function of X.
- **Break condition:** High noise degrades convergence of dependence measures, potentially invalidating the test.

### Mechanism 3
- **Claim:** Out-input substitutions discover complex input dependencies structurally obscured in original problem
- **Mechanism:** Out-input substitution involves output $y$ (e.g., $h(x_I, y) = y/x_1$), transforming problem into auxiliary regression task: finding $g$ such that $h(x_I, y) = g(x_{\setminus I})$. If solved, $y$ can be isolated algebraically.
- **Core assumption:** Expression can be algebraically solved for $y$ after auxiliary problem is solved.
- **Evidence anchors:** Section 2 describes using simple out-input substitution to find complex input substitution; section 5 shows adding out-input substitutions increases reduction rates significantly.
- **Break condition:** If $h$ is not algebraically invertible or auxiliary problem is as complex as original.

## Foundational Learning

- **Concept: Chatterjee's Coefficient / CODEC**
  - **Why needed here:** This is the "validity test" engine. Unlike correlation (linear) or Spearman (monotonic), these measures detect general measurable functional dependence required to verify non-linear symbolic substitutions.
  - **Quick check question:** If $Y = \sin(X \cdot Z)$, would a linear correlation coefficient suffice to validate the substitution $X \cdot Z$? (Answer: No).

- **Concept: Expression DAGs vs. Expression Trees**
  - **Why needed here:** Search space for substitutions defined by Expression Directed Acyclic Graphs (DAGs). DAGs allow re-use of sub-expressions, making search more efficient than trees for covering specific function class.
  - **Quick check question:** In expression for $f(x) = x^2 + \sin(x^2)$, how does DAG representation differ from tree? (Answer: DAG represents $x^2$ once; Tree represents it twice).

- **Concept: Beam Search**
  - **Why needed here:** Method navigates tree of possible substitutions. Beam search restricts search width to highest-scoring nodes at each depth, balancing exploration of complex substitutions with computational tractability.
  - **Quick check question:** If beam size is 1, does algorithm explore all possible substitutions? (Answer: No, it greedily follows single best path).

## Architecture Onboarding

- **Component map:** Search Space (enumerate small expression DAGs) -> Validator (CODEC/KMAC functional dependence measure) -> Controller (Beam Search loop) -> Solver (downstream symbolic regression algorithm) -> Reconstructor (Computer Algebra System)

- **Critical path:** Efficiency of dependence measure calculation (CODEC). Table 1 shows CODEC is significantly faster than KMAC (milliseconds vs. ~23ms), making it bottleneck for real-time application.

- **Design tradeoffs:**
  - CODEC vs. KMAC: CODEC is faster (preferred for large problems); KMAC is more robust to certain topologies but slower
  - Beam Size: Paper finds beam size 1 is often sufficient (Table 2), implying validity test is highly discriminative and search breadth adds marginal value

- **Failure signatures:**
  - High Noise: While CODEC/KMAC outperform volume heuristics, recovery rates still drop significantly as noise increases
  - Reconstruction Error: For out-input substitutions, if CAS fails to solve for $y$, reduced formula is useless
  - False Positives: High dependence score doesn't guarantee "simple" symbolic form, only functional one

- **First 3 experiments:**
  1. **Sanity Check:** Implement reduction loop on Washburn's equation. Verify beam search identifies $g = x_1 x_2 x_3$ and $h = y/\sqrt{g}$.
  2. **Ablation on Dependence Measures:** Reproduce Table 1 timing and Figure 3 noise robustness. Compare CODEC vs. Volume heuristic on synthetic polynomial with added Gaussian noise.
  3. **Integration Test:** Run PySR on Feynman dataset "as-is" vs. "wrapped in beam search." Compare recovery rate column to verify reported ~10-15% lift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does expanding expression DAG search budget beyond one intermediary node affect tradeoff between reduction rate and computational cost?
- Basis in paper: [inferred] Authors explicitly constrained search to expression DAGs with "at most one intermediary node" to maintain efficiency
- Why unresolved: Paper demonstrates success within constrained space but doesn't evaluate potential for finding more complex substitutions with increased complexity budget
- What evidence would resolve it: Ablation study measuring success rate of substitution finding and total runtime as maximum number of DAG intermediary nodes is increased

### Open Question 2
- Question: Can framework be generalized to identify vector-valued substitutions (where k > 1) to reduce dimensionality more aggressively?
- Basis in paper: [inferred] Implementation restricts search to "scalar functions with k = 1," limiting method to eliminating one variable per step
- Why unresolved: Unclear if simultaneous multi-variable reductions could be detected, which might simplify regression problem faster than iterative scalar approach
- What evidence would resolve it: Modified algorithm capable of outputting vector-valued $g$ functions tested on benchmark datasets with known multivariate dependencies

### Open Question 3
- Question: How does choice of functional dependence measure (CODEC vs. KMAC) impact performance in low-data regimes?
- Basis in paper: [inferred] Study evaluates noise robustness and runtime differences but relies on fixed sample sizes (e.g., $n=1000$)
- Why unresolved: Tradeoff between statistical power of dependence measures and data scarcity not established, critical for applications with limited observations
- What evidence would resolve it: Parameter study varying sample size $n$ to determine minimum data required for each measure to reliably detect valid substitutions

## Limitations
- Validity test based on functional dependence measures assumes sufficient sample density to distinguish true functional relationships from statistical artifacts
- Substitution search constrained to small expression DAGs with at most one intermediary node, potentially missing more complex substitutions
- Reconstruction step relies on symbolic algebra solvers that may fail on implicit equations, leaving some reduced problems unsolved

## Confidence
- **High Confidence:** Mechanism that reduces variable count through valid substitutions is well-supported by experimental results showing consistent ~50% reduction rates across datasets and algorithms
- **Medium Confidence:** Claim that this reduction leads to improved recovery rates is supported but magnitude varies significantly across different base algorithms
- **Low Confidence:** Assertion that CODEC is "significantly faster" than KMAC needs more context - while Table 1 shows this, absolute time differences suggest both are computationally feasible

## Next Checks
1. **Noise Sensitivity Analysis:** Systematically vary noise levels on subset of formulas and measure how reduction rate stability (claimed surprisingly robust) correlates with recovery rate degradation across different base algorithms

2. **Search Space Ablation:** Test algorithm with expanded DAG search spaces (2+ intermediary nodes) on small set of formulas to quantify tradeoff between search complexity and substitution quality

3. **Reconstruction Failure Analysis:** Instrument pipeline to log all instances where base regressor succeeds on reduced problem but SymPy fails to reconstruct solution, and analyze whether failures follow predictable patterns