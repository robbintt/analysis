---
ver: rpa2
title: 'Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of
  Large Language Models'
arxiv_id: '2507.01915'
source_url: https://arxiv.org/abs/2507.01915
tags:
- rlhf
- reward
- optimization
- objectives
- gapo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning large language models
  (LLMs) with diverse and potentially conflicting human preferences, such as helpfulness
  and harmlessness. To tackle this, the authors propose Gradient-Adaptive Policy Optimization
  (GAPO), a gradient-based multi-objective optimization method that adaptively rescales
  gradients to balance trade-offs between objectives.
---

# Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models

## Quick Facts
- **arXiv ID**: 2507.01915
- **Source URL**: https://arxiv.org/abs/2507.01915
- **Reference count**: 40
- **Primary result**: GAPO outperforms state-of-the-art methods in balancing helpfulness and harmlessness across multiple datasets

## Executive Summary
This paper addresses the challenge of aligning large language models (LLMs) with diverse and potentially conflicting human preferences, such as helpfulness and harmlessness. The authors propose Gradient-Adaptive Policy Optimization (GAPO), a gradient-based multi-objective optimization method that adaptively rescales gradients to balance trade-offs between objectives. GAPO employs multiple-gradient descent with gradient normalization to focus updates on underdeveloped objectives, improving overall alignment. An extension, P-GAPO, further incorporates user preferences to generate Pareto-optimal solutions tailored to specific needs. Empirical results on Mistral-7B demonstrate that GAPO outperforms state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness across multiple datasets, with balanced optimization scores and higher win rates in human evaluations.

## Method Summary
The paper proposes GAPO as a gradient-based multi-objective optimization framework for aligning LLMs with conflicting human preferences. The method computes gradients for each objective independently (helpfulness and harmlessness), normalizes them by their L2-norms, and solves a constrained optimization problem (MGDA) on these rescaled gradients. This approach focuses updates on underdeveloped objectives rather than those with naturally larger gradient magnitudes. The algorithm finds a Pareto stationary solution where no objective can be improved without degrading another. An extension, P-GAPO, incorporates user preference vectors into the gradient aggregation to generate Pareto-optimal policies tailored to specific needs.

## Key Results
- GAPO achieves superior performance in both helpfulness and harmlessness across multiple datasets compared to state-of-the-art methods
- Balanced optimization scores demonstrate effective trade-off management between conflicting objectives
- Higher win rates in human evaluations validate the practical effectiveness of the approach
- P-GAPO successfully generates Pareto-optimal solutions that better align with user-defined preferences

## Why This Works (Mechanism)

### Mechanism 1: Gradient Rescaling for Balanced Optimization
The algorithm normalizes gradients of conflicting objectives to balance trade-offs more effectively than standard scalarization. By solving a constrained optimization on rescaled gradients, updates focus on underdeveloped objectives rather than those with naturally larger gradient magnitudes.

### Mechanism 2: Multiple-Gradient Descent (MGDA)
MGDA treats alignment as a multi-objective optimization problem, seeking weights that minimize the total norm of weighted gradients. This ensures convergence toward Pareto stationary solutions without degrading one objective while improving another.

### Mechanism 3: Preference-Conditioned Updates (P-GAPO)
P-GAPO incorporates user preference vectors into gradient aggregation, allowing the model to generate diverse, Pareto-optimal policies tailored to specific needs by steering optimization along the Pareto front.

## Foundational Learning

- **Concept: Pareto Optimality & Stationarity**
  - Why needed here: GAPO aims to find policies where you cannot improve helpfulness without hurting harmlessness
  - Quick check question: Can you explain why a "Pareto stationary" point is not necessarily a "Pareto optimal" point?

- **Concept: KL-Regularized Reinforcement Learning**
  - Why needed here: The base loss function includes a KL-divergence penalty to prevent the model from drifting too far from the initial SFT policy
  - Quick check question: What happens to the model's output diversity if the coefficient β for the KL penalty is set too high?

- **Concept: Gradient Normalization (L2-Norm)**
  - Why needed here: The core contribution relies on normalizing gradients to rescale their relative contributions
  - Quick check question: If ∇J₁ has a norm of 100 and ∇J₂ has a norm of 1, how does normalization change their relative contribution to the update step?

## Architecture Onboarding

- **Component map**: Policy Model (LLM) -> Reward/Cost Models -> GAPO Optimizer -> Policy Model
- **Critical path**:
  1. Forward Pass: Generate responses y for prompts x
  2. Evaluation: Score y using both Reward and Cost models
  3. Loss Calculation: Compute PPO surrogate losses L_h and L_s
  4. Gradient Processing: Compute, normalize gradients, solve for optimal weights α
  5. Backward Pass: Apply combined gradient update to the Policy Model
- **Design tradeoffs**:
  - Uses last-layer gradients instead of full gradients to reduce space complexity
  - Empirically found p=1 (standard normalization) provides better balance than p=2
- **Failure signatures**:
  - Reward hacking: Generating gibberish that maximizes reward scores
  - Gradient conflict stalling: Consistent opposition between helpfulness and harmlessness gradients
  - Over-refusal: Refusing benign prompts due to extreme preference weights
- **First 3 experiments**:
  1. Log L2-norms of ∇L_h and ∇L_s during standard PPO to confirm scale disparity
  2. Run GAPO with p=1 vs p=2 on subset to verify claimed superiority
  3. Train P-GAPO with 3 preference vectors and plot the resulting trade-off curve

## Open Questions the Paper Calls Out

### Open Question 1
Does GAPO maintain consistent performance when fine-tuning LLMs with diverse initial capabilities other than Mistral-7B? The empirical validation is restricted to one model family, and different architectures may exhibit different gradient dynamics.

### Open Question 2
How can we develop standardized evaluation protocols to verify if a model's output accurately reflects specific user preference weights? Current evaluations rely on aggregate metrics that may not be sensitive enough to validate fine-grained adherence to preference vectors.

### Open Question 3
Does relying solely on last-layer gradients compromise the quality of the Pareto front compared to using full gradients? The approximation assumes last-layer gradients capture the necessary optimization landscape, potentially ignoring conflicts in deeper layers.

### Open Question 4
How does stability and convergence speed degrade as the number of conflicting objectives increases beyond two? The feasible region for Pareto stationary points shrinks as m increases, making gradient rescaling geometrically more complex.

## Limitations

- The gradient rescaling assumption that larger gradients indicate underdevelopment lacks rigorous theoretical validation in the RLHF context
- Approximation of full gradients using only last-layer gradients may miss critical optimization signals from deeper layers
- Empirical validation is restricted to Mistral-7B, limiting generalizability to other model families and scales

## Confidence

- **High confidence**: Empirical performance improvements (win rates, balanced optimization scores) are well-documented and reproducible
- **Medium confidence**: MGDA-based gradient aggregation mechanism works as described, but theoretical guarantees for RLHF objectives need further validation
- **Low confidence**: The core claim that gradient magnitude correlates with optimization priority lacks rigorous theoretical backing in this specific application

## Next Checks

1. **Gradient Correlation Analysis**: Measure Pearson correlation between gradient magnitudes and subsequent improvements in each objective to test the core assumption
2. **Full-Gradient Comparison**: Run GAPO with full-parameter gradients versus last-layer approximations to quantify approximation error
3. **Pareto Front Resolution**: Systematically vary preference vectors across the full [0,1] range and measure whether P-GAPO truly spans the Pareto frontier or clusters in certain regions