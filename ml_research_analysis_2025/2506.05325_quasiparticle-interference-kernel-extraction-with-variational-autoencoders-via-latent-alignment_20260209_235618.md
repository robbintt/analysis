---
ver: rpa2
title: Quasiparticle Interference Kernel Extraction with Variational Autoencoders
  via Latent Alignment
arxiv_id: '2506.05325'
source_url: https://arxiv.org/abs/2506.05325
tags:
- kernel
- data
- kernels
- learning
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting single-scatterer
  quasiparticle interference (QPI) kernels from multi-scatterer STM images, a fundamental
  inverse problem in quantum materials research. The core difficulty lies in the fact
  that overlapping signals from multiple defects make it impossible to directly infer
  the underlying kernel pattern.
---

# Quasiparticle Interference Kernel Extraction with Variational Autoencoders via Latent Alignment

## Quick Facts
- **arXiv ID:** 2506.05325
- **Source URL:** https://arxiv.org/abs/2506.05325
- **Reference count:** 40
- **Primary result:** Two-step VAE framework achieves up to 18% lower RMSE than direct baseline for extracting single-scatterer QPI kernels from multi-scatterer STM images.

## Executive Summary
This paper introduces a two-step machine learning framework to extract single-scatterer quasiparticle interference (QPI) kernels from multi-scatterer scanning tunneling microscopy (STM) images. The inverse problem is fundamentally ill-posed because overlapping signals from multiple defects make direct kernel inference impossible. The authors address this by first learning a latent space of physically valid scattering kernels using a variational autoencoder (VAE), then training a separate encoder to align observational QPI data with this learned space. The method demonstrates significant improvements over direct one-step approaches and successfully generalizes to unseen kernels, providing the first AI-based solution for QPI kernel extraction in quantum materials research.

## Method Summary
The approach decouples kernel representation learning from observation-to-kernel inference through a two-step process. Step 1 fine-tunes a pre-trained VAE (madebyollin/sdxl-vae-fp16-fix) on simulated single-kernel images using a composite loss (MSE + symmetric loss + KL divergence) to learn a compact latent space of physically valid kernels. Step 2 trains a separate encoder to map stacked observation maps (Y, M) into the frozen kernel latent space by minimizing L2 distance between observation embeddings and kernel embeddings. The framework is trained on a novel simulated QPI dataset of 100 unique kernels with 50,000 samples, then validated on real STM data from Ag and FeSe materials.

## Key Results
- Two-step framework achieves up to 18% lower RMSE (0.1297 vs 0.1528) than one-step baseline on out-of-distribution kernel extraction
- Out-of-distribution generalization improves significantly: RMSE drops from 0.1528 to 0.1297
- Symmetric loss ablation shows 4.75% improvement in RMSE (0.1372 → 0.1297)
- Pre-trained VAE features provide ~4× lower RMSE compared to training from scratch (0.1643 → 0.1297)
- Successful qualitative validation on real STM data from Ag and FeSe, though long-range oscillations are missed

## Why This Works (Mechanism)

### Mechanism 1: Decoupling inverse problem
The two-step approach separates kernel representation learning from observation-to-kernel inference, making the inversion far more stable by constraining the mapping to lie within the manifold of valid kernels. The learned latent space spans physically plausible kernels, preventing invalid reconstructions.

### Mechanism 2: Latent alignment for generalization
Training the observation encoder to produce embeddings that match frozen kernel encoder outputs forces it to learn features predictive of kernel identity rather than memorizing direct mappings. This enables generalization to unseen kernels, as demonstrated by the OOD RMSE improvement from 0.1528 to 0.1297.

### Mechanism 3: Symmetric loss enforcement
The symmetric loss rotates reconstructed kernels by their symmetry angle (60°/90°/120°/180° for 6/4/3/2-fold) and penalizes deviation from the original. This exploits known rotational symmetries of QPI patterns to enforce physically realistic reconstructions, improving accuracy by 4.75% (RMSE: 0.1372 → 0.1297).

## Foundational Learning

- **Ill-posed inverse problems**: Multi-scatterer QPI extraction is underdetermined—many kernel combinations produce similar observations. This explains why direct approaches fail and why the two-step framework is necessary.
  - Quick check: Can you explain why knowing Y and M does not uniquely determine A in Fig. 1?

- **Variational Autoencoder latent spaces**: The method relies on VAEs learning compact, semantically meaningful embeddings. Without structured latent space, the alignment step has no meaningful target.
  - Quick check: How does the KL divergence term in Eq. 4 shape the latent distribution?

- **Transfer learning from pre-trained vision models**: Fine-tuning the sdxl-vae rather than training from scratch significantly improves reconstruction (RMSE: 0.1643 → 0.1297). This leverages generic feature extraction learned on natural images.
  - Quick check: Why might features learned on natural images transfer to QPI patterns?

## Architecture Onboarding

- **Component map**: Encoder_K (kernel) → h_A → Decoder_K (kernel) ; Encoder_y (observation) → h_y → Decoder_K (kernel)
- **Critical path**: 1) Fine-tune Encoder_K + Decoder_K on kernel-only images with L_MSE + L_SYM + D_KL (Step 1, 300 epochs). 2) Freeze Encoder_K/Decoder_K; train Encoder_y on (Y, M) pairs with L2 alignment loss (Step 2, 50 epochs). 3) Inference: (Y, M) → Encoder_y → h_y → Decoder_K → Â.
- **Design tradeoffs**: Pre-trained gives ~4× lower RMSE but may bias toward natural-image features. Symmetric loss weight (α=0.7) enforces symmetry but may suppress asymmetric signal features. Noise augmentation improves real-data robustness but may obscure fine-grained oscillation patterns.
- **Failure signatures**: One-step baseline produces smeared or symmetrically incorrect kernels on OOD data. Extracted kernels missing long-range oscillations in real data. Latent space fails to cluster by physical mode if Step 1 training is insufficient.
- **First 3 experiments**: 1) Reproduce Table I results comparing one-step vs. two-step on ID/OOD splits. 2) Ablate symmetric loss to confirm contribution and visualize kernel symmetry. 3) Evaluate noise-augmented vs. clean-trained models on held-out noisy simulated data before real STM transfer.

## Open Questions the Paper Calls Out

None

## Limitations

- **Simulated data dependency**: Quantitative evaluation relies entirely on simulated data; real STM validation is only qualitative without ground truth for comparison.
- **Symmetry assumption**: The method assumes all QPI kernels exhibit discrete rotational symmetries (6-, 4-, 3-, or 2-fold), which may not hold for all materials systems.
- **Feature representation gaps**: Extracted kernels "miss long-range oscillations" in real data, suggesting incomplete capture of physical modes and potential bias from noise augmentation.

## Confidence

- **High confidence**: Two-step framework architecture and training procedure are clearly specified and reproducible. Decoupling mechanism supported by RMSE improvements (OOD: 0.1528 → 0.1297).
- **Medium confidence**: Pre-trained VAE transfer claim supported by Table III (RMSE: 0.1643 → 0.1297) but relies on assumption that natural image features are relevant to QPI structures.
- **Low confidence**: Real STM data generalization demonstrated only qualitatively; lack of ground truth prevents verification of 18% RMSE improvement claim.

## Next Checks

1. **Ablation of rotational symmetry assumption**: Remove symmetric loss term and evaluate kernel extraction accuracy on datasets containing kernels with lower or no rotational symmetry to test the assumption's validity.

2. **Real-data quantitative validation**: Apply trained model to real QPI dataset where ground truth kernels are known (e.g., from theoretical calculations) to verify the 18% RMSE improvement claim.

3. **Latent space interpretability**: Visualize learned latent space using t-SNE or UMAP to confirm kernels with similar physical modes cluster together, validating that latent space captures meaningful physical structure.