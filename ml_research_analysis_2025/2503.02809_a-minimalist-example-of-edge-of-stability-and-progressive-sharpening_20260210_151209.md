---
ver: rpa2
title: A Minimalist Example of Edge-of-Stability and Progressive Sharpening
arxiv_id: '2503.02809'
source_url: https://arxiv.org/abs/2503.02809
tags:
- have
- sharpness
- loss
- learning
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies two phenomena in deep learning optimization:
  Edge of Stability (EoS) and Progressive Sharpening (PS) under large learning rates.
  The authors introduce a minimalist two-layer neural network with a two-dimensional
  input, where one dimension is relevant to the response and the other is irrelevant.'
---

# A Minimalist Example of Edge-of-Stability and Progressive Sharpening

## Quick Facts
- arXiv ID: 2503.02809
- Source URL: https://arxiv.org/abs/2503.02809
- Authors: Liming Liu; Zixuan Zhang; Simon Du; Tuo Zhao
- Reference count: 40
- Primary result: Rigorous proof of progressive sharpening and self-stabilization under large learning rates in a two-layer linear network with bivariate input

## Executive Summary
This paper provides the first rigorous mathematical explanation for the Edge of Stability (EoS) and Progressive Sharpening (PS) phenomena in deep learning optimization. Using a minimalist two-layer linear network with one relevant and one irrelevant input dimension, the authors prove that sharpness increases monotonically until reaching the instability threshold 2/η, then oscillates while loss decreases non-monotonically. The analysis reveals that convergence rate depends on the ratio of relevant to irrelevant feature curvatures (λ₂/λ₁), not the learning rate, explaining why larger η doesn't always accelerate training.

## Method Summary
The model uses a width-one linear network with 2D input where x₁ is irrelevant (covariance λ₁ ≥ 100) and x₂ is relevant (covariance λ₂ ≤ 1/λ₁). Training employs gradient descent with η ∈ [2/λ₁, 0.1] and gradient clipping on the irrelevant feature weight β₁. The loss function decomposes into L₁ (irrelevant feature contribution) and L₂ (relevant feature contribution), enabling precise analysis of sharpness dynamics and convergence behavior.

## Key Results
- Sharpness S(θ) ≈ λ₁α² increases monotonically until reaching 2/η threshold, then oscillates
- Self-stabilization occurs when β₁-induced oscillations become large enough to reduce α
- Loss L decreases non-monotonically with periodic spikes from L₁, while L₂ governs convergence with rate (1-λ₂/λ₁)²
- EoS phenomena emerge from interplay between irrelevant feature scaling and gradient clipping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sharpness S(θ) is dominated by λ₁α², and progressive sharpening occurs via monotonic increase of α until reaching instability threshold 2/η.
- **Mechanism:** The parameter α scales both input dimensions. Its growth increases sharpness (eigenvalue of Hessian). When S(θ) < 2/η, GD updates stably increase α. Once S(θ) approaches 2/η, the dynamics become unstable along the β₁ direction, triggering self-stabilization.
- **Core assumption:** λ₁ ≥ 100, λ₁λ₂ ≤ 1 (large-scale irrelevant feature; small-scale relevant feature).
- **Evidence anchors:** [abstract]: "rigorously prove the existence of progressive sharpening and self-stabilization under large learning rates"; [Section 4.1, Lemma 4.2(i)]: "λ₁α²(t) ≤ S(θ(t)) ≤ 1.12λ₁α²(t)"; [corpus]: "Understanding Sharpness Dynamics in NN Training with a Minimalist Example" (FMR=0.576) confirms PS as widely observed but mechanism poorly understood
- **Break condition:** If λ₁ is not sufficiently large relative to 2/η, sharpness may not reach threshold; if λ₂ ≈ λ₁, both features scale similarly, eliminating the stabilizing effect.

### Mechanism 2
- **Claim:** The irrelevant-feature weight β₁ controls oscillatory instability; self-stabilization occurs when β₁-induced oscillations become large enough to reduce α.
- **Mechanism:** β₁ corresponds to the large-scale irrelevant feature x₁. The top eigenvector of Hessian aligns with β₁ direction (>0.9 cosine similarity). When S(θ) > 2/η, updates along β₁ become unstable, causing α to decrease (self-stabilization phase). β₂ (relevant feature) increases monotonically throughout.
- **Core assumption:** Gradient clipping on β₁ prevents anomalous divergence; initialization within X(η).
- **Evidence anchors:** [abstract]: "sharpness first increases monotonically until it reaches the instability threshold 2/η, then oscillates around this value"; [Section 4.1, Lemma 4.2(ii-iii)]: "|cos(v(t),(0,1,0))| > 0.9" and "β₂(t+1) > β₂(t)"; [corpus]: "Learning Dynamics of Deep Linear Networks Beyond the Edge of Stability" (FMR=0.494) studies EOS equilibrium but does not address bivariate input mechanism
- **Break condition:** Without clipping on β₁, α can decrease too rapidly and change sign (Appendix F), breaking EoS dynamics.

### Mechanism 3
- **Claim:** Loss L decomposes into oscillatory term L₁ (irrelevant) and convergence term L₂ (relevant); effective convergence rate is controlled by λ₂/λ₁, not learning rate η.
- **Mechanism:** L = ½λ₁(αβ₁)² + ½λ₂(αβ₂−1)². L₁ spikes but rapidly drops when trajectory returns to stable region (S ≤ 2/η). L₂ governs actual convergence with rate (1 − λ₂/λ₁)². This explains why larger η does not always accelerate training.
- **Core assumption:** λ₁λ₂ ≤ 1; λ₂ ≪ ηλ₁ (relevant feature has small curvature).
- **Evidence anchors:** [abstract]: "loss decreasing non-monotonically"; [Section 4.2, Theorem 4.6]: decay rate bounds (1 − 4λ₂/λ₁)² ≤ L̂(t+1)/L̂(t) ≤ (1 − λ₂/λ₁)²; [corpus]: Weak corpus evidence directly connecting bivariate loss decomposition to EoS—this appears novel to this paper
- **Break condition:** If λ₂/λ₁ is not small, L₂ contributes more to sharpness, potentially disrupting EoS equilibrium.

## Foundational Learning

- **Concept: Edge of Stability (EoS)**
  - Why needed here: Central phenomenon this paper explains; sharpness hovers at 2/η while loss decreases non-monotonically.
  - Quick check question: Given learning rate η = 0.1, what sharpness value defines the stability threshold? (Answer: 2/0.1 = 20)

- **Concept: Progressive Sharpening (PS)**
  - Why needed here: The precursor to EoS; sharpness monotonically increases until reaching threshold.
  - Quick check question: If S(θ) = 15 at iteration t and η = 0.1, is the system in the PS phase? (Answer: Yes, since 15 < 20 = 2/η)

- **Concept: Gradient Flow Solution (GFS)**
  - Why needed here: Continuous-time limit of GD; GFS sharpness decreases monotonically along GD trajectory, unlike actual GD sharpness which oscillates.
  - Quick check question: Does GFS sharpness equal GD sharpness at convergence? (Answer: Not necessarily—this paper shows GFS can converge to sharp minima while GD converges to flatter ones)

## Architecture Onboarding

- **Component map:** Input layer (2D x = (x₁, x₂)) → Hidden layer (width-1 linear neuron with weight β = (β₁, β₂)) → Output layer (single linear output with weight α) → Loss function L(θ) = ½λ₁(αβ₁)² + ½λ₂(αβ₂−1)²

- **Critical path:**
  1. Initialize θ ∈ X(η) (see Appendix C for explicit bounds on α, β₁, β₂)
  2. Phase I: α increases, S(θ) approaches 2/η (progressive sharpening)
  3. Phase II.1: S(θ) exceeds 2/η, β₁ oscillations grow
  4. Phase II.2: Self-stabilization via α decrease
  5. Repeat Phases II.1–II.2 until convergence to flat minimum (S ≤ 2/η + δ)

- **Design tradeoffs:**
  - Clipping β₁: Required for well-behaved dynamics, but clipping threshold √(10⁶/λ₁) affects oscillation amplitude
  - Learning rate η: Larger η enables EoS regime but may not accelerate convergence (rate is λ₂/λ₁-dependent)
  - Input scale ratio λ₁/λ₂: Larger ratio → sharper oscillations in L₁, but also faster L₂ convergence

- **Failure signatures:**
  - Without β₁ clipping: α can flip sign, causing near-zero gradients and slow recovery (Figure 14)
  - If λ₁λ₂ >> 1: Both features contribute significantly to sharpness; EoS dynamics may not emerge
  - If η < 2/λ₁: System stays in classical stable regime; no EoS phenomena

- **First 3 experiments:**
  1. **Replicate Figure 1:** Set λ₁=100, λ₂=0.01, η=0.05, train for 10K steps; verify sharpness oscillates around 40 and loss decreases non-monotonically with periodic spikes.
  2. **Ablate β₁ clipping:** Same settings, remove clipping; confirm α can crash toward zero (Figure 14 behavior).
  3. **Vary η:** Train with η ∈ {0.05, 0.08, 0.1, 0.12} from same initialization; confirm convergence rate (loss decay slope) is nearly independent of η but total iteration count to reach loss threshold varies.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical trade-off mechanism—where increasing the learning rate eventually slows convergence—be formally extended to explain similar phenomena in general deep learning practice beyond the minimalist setting?
- **Basis in paper:** [explicit] The authors note in Section 6.2 that their model offers a "preliminary theoretical explanation" for the trade-off but admits it "does not yet fully explain why increasing the learning rate slows down the loss decrease" and explicitly states this "can be extended by future effort."
- **Why unresolved:** The current theoretical result (Theorem 4.6) applies specifically to the minimalist two-layer, width-one model; it remains unverified whether this rate independence holds or scales to standard neural architectures.
- **What evidence would resolve it:** A theoretical derivation or empirical validation demonstrating that the loss decay rate becomes independent of the learning rate in wider networks or standard benchmarks (e.g., transformers on language tasks).

### Open Question 2
- **Question:** Is the gradient clipping mechanism on the irrelevant feature weight (β₁) strictly necessary to prevent divergence, or does it merely restrict the analysis to a subset of well-behaved trajectories?
- **Basis in paper:** [inferred] Section 3 introduces clipping to "prevent anomalous behavior," and Appendix F shows that unclipped gradients can lead to "abnormal behavior" where sharpness drops rapidly to near zero. The reliance on this constraint suggests the natural dynamics might diverge from the proven EoS behavior.
- **Why unresolved:** The paper rigorously proves dynamics with clipping; the behavior of the unclipped system remains empirically characterized as "abnormal" but theoretically unanalyzed regarding whether it eventually stabilizes or diverges.
- **What evidence would resolve it:** A non-asymptotic analysis of the unclipped gradient descent trajectory or a proof that the anomalous behavior (e.g., sharpness collapse) is transient and followed by a return to the Edge of Stability.

### Open Question 3
- **Question:** Does the existence of a "well-behaved" stable set generalize to networks with higher-dimensional inputs or widths greater than one?
- **Basis in paper:** [inferred] The paper highlights a "potential separation between the bivariate and scalar inputs" (Section 1) regarding the properties of the stable set, but does not confirm if the non-disjoint, well-behaved set found in this 2D model extends to d-dimensions.
- **Why unresolved:** The proofs utilize specific bounds available only in the width-one, 2D input structure; previous work on scalar networks showed disjoint stable sets, so the behavior in higher dimensions is currently ambiguous.
- **What evidence would resolve it:** Theoretical analysis of the stable set geometry for a d-dimensional input model or empirical visualization showing that the projected gradient descent trajectory remains smooth in higher-dimensional settings.

## Limitations

- Model simplicity: Extreme minimalism (width-one, bivariate input) may not capture full complexity of EoS in deep nonlinear networks
- Narrow theoretical regime: Results rely on specific parameter assumptions (λ₁ ≫ 1, λ₁λ₂ ≤ 1) that may not hold in practice
- Clipping dependency: Self-stabilization mechanism critically depends on β₁ clipping, with unclipped behavior characterized as "abnormal" but not fully analyzed

## Confidence

- Progressive Sharpening mechanism (α growth → sharpness increase): High confidence—mathematically proven with explicit bounds on eigenvalue dominance
- Self-stabilization via β₁ oscillations: Medium confidence—theoretical derivation is sound but relies heavily on clipping; Appendix F shows potential failure without clipping
- Loss decomposition and convergence rate claims: Medium confidence—analytical results are provided but the practical implications for training efficiency are not fully validated empirically
- Connection to existing EoS literature: Medium confidence—the reconciliation with "stable set" concepts is logically sound but the novelty contribution relative to prior work could be more clearly delineated

## Next Checks

1. **Sensitivity analysis to clipping threshold:** Systematically vary the β₁ clipping threshold √(10⁶/λ₁) and quantify its impact on EoS dynamics and convergence stability
2. **Extended initialization validation:** Test initialization outside the specified X(η) bounds to determine robustness boundaries and identify failure modes
3. **Parameter ratio sweep:** Explore λ₁/λ₂ ratios beyond the theoretical regime (e.g., λ₁λ₂ > 1) to understand when EoS phenomena break down