---
ver: rpa2
title: 'DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents
  in Complex Decision-Making Environments'
arxiv_id: '2503.06047'
source_url: https://arxiv.org/abs/2503.06047
tags:
- game
- move
- agents
- units
- pieces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DSGBench introduces a rigorous evaluation benchmark for LLM-based
  agents in strategic decision-making, addressing the lack of comprehensive assessment
  tools for complex, multi-objective environments. The benchmark features six diverse
  strategy games (StarCraft II, Civilization, Street Fighter III, Diplomacy, Werewolf,
  and Stratego) and employs fine-grained metrics across five cognitive dimensions:
  strategic planning, real-time decision-making, social reasoning, team collaboration,
  and adaptive learning.'
---

# DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments

## Quick Facts
- arXiv ID: 2503.06047
- Source URL: https://arxiv.org/abs/2503.06047
- Reference count: 40
- Key outcome: DSGBench introduces a rigorous evaluation benchmark for LLM-based agents in strategic decision-making, addressing the lack of comprehensive assessment tools for complex, multi-objective environments.

## Executive Summary
DSGBench presents a comprehensive benchmark for evaluating large language model (LLM)-based agents in complex strategic decision-making environments. The benchmark features six diverse strategy games including StarCraft II, Civilization, Street Fighter III, Diplomacy, Werewolf, and Stratego. It employs fine-grained metrics across five cognitive dimensions: strategic planning, real-time decision-making, social reasoning, team collaboration, and adaptive learning. The evaluation framework includes an automated decision-tracking mechanism that enables detailed analysis of agent behavior patterns and strategy evolution. Experiments with six representative LLMs demonstrate significant performance differences across games and capabilities, with closed-source models generally outperforming open-source alternatives.

## Method Summary
DSGBench evaluates LLM-based agents in six strategic games using a unified Gym interface where text-based observations are converted to prompts for LLM inference. The framework tracks game interactions as POMDP quintuples and employs a two-level hierarchical inference approach for complex games requiring macro-strategic and micro-tactical decisions. Agents are evaluated across five cognitive dimensions using fine-grained metrics aggregated into capability scores. The evaluation uses standardized prompts with temperature=0.2, running 10 matches per scenario against built-in AI or GPT-4o-mini opponents. An automated HistoryTracker logs decision contexts, action types, and outcomes for trajectory analysis, while capability scores are computed via weighted normalization across metrics.

## Key Results
- DSGBench successfully distinguishes performance differences between LLM-based agents across six diverse strategic games
- Closed-source models (GPT-4o, Gemini 1.5 Pro) generally outperform open-source alternatives (Llama-3.1-70B, DeepSeek-V2.5) across multiple cognitive dimensions
- The automated decision-tracking mechanism reveals strategy evolution patterns, showing agents adapt their approaches over time in response to game states
- Fine-grained metrics provide deeper insights into agent capabilities beyond simple win/loss outcomes, revealing specific strengths and weaknesses in strategic planning, social reasoning, and real-time decision-making

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Cognitive Capability Mapping
- Claim: Fine-grained capability metrics reveal differentiated model strengths across strategic planning, real-time decision-making, social reasoning, team collaboration, and adaptive learning dimensions.
- Mechanism: Games are mapped to capability dimensions; each game provides multiple fine-grained metrics (e.g., Resource Collection Performance, Supply Utilization Rate for strategic planning in StarCraft II). The capability score aggregates these via weighted normalization (Equation 3), enabling cross-game, cross-model comparison.
- Core assumption: Cognitive capabilities are separable and can be approximated via game-specific behavioral proxies.
- Evidence anchors: [abstract] "fine-grained metrics across five cognitive dimensions: strategic planning, real-time decision-making, social reasoning, team collaboration, and adaptive learning"

### Mechanism 2: Decision Trajectory Tracking for Strategy Evolution Analysis
- Claim: Automated tracking of decision contexts, action types, and outcomes enables qualitative insight into agent reasoning patterns beyond win/loss metrics.
- Mechanism: HistoryTracker captures game history; each decision is logged with state context, chosen action, and outcome. This allows analysis of strategy shifts (e.g., EER drop at step 15k indicating pivot from economy to military in StarCraft II).
- Core assumption: Sequential action patterns reveal underlying strategic reasoning and adaptation.
- Evidence anchors: [abstract] "automated decision-tracking mechanism enables detailed analysis of agent behavior patterns and strategy evolution"

### Mechanism 3: Hierarchical Inference for Complex Environments
- Claim: Two-level inference (high-level strategic planning + low-level tactical execution) enables LLMs to handle large observation spaces and multi-objective games.
- Mechanism: For StarCraft II and Civilization, the policy factorizes as p(a_high|s, c_high) × p(a_low|s, a_high, c_low), separating macro-strategy (resource allocation, tech path) from micro-tactics (unit control, local engagement).
- Core assumption: Factorized inference reduces effective decision complexity and aligns with hierarchical game structure.
- Evidence anchors: [section 3.1] "Complex strategy games like StarCraft II and Civilization require agents to handle large observation spaces... through two levels of reasoning"

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: DSGBench formalizes game interactions as POMDP quintuples ⟨W, S, A, O, T⟩; understanding belief-state updates and partial observability is essential for interpreting agent limitations.
  - Quick check question: Can you explain why a POMDP formulation is more appropriate for StarCraft II than a fully observable MDP?

- Concept: Dual Systems Cognitive Theory (System 1 vs. System 2)
  - Why needed here: The five-dimension framework draws directly on Kahneman's fast/slow thinking distinction; strategic planning is mapped to System 2, real-time decision-making to System 1.
  - Quick check question: Which DSGBench capability dimension is most analogous to "System 1" rapid intuitive response?

- Concept: Game-Theoretic Reasoning and Social Deduction
  - Why needed here: Diplomacy and Werewolf require modeling opponent beliefs, bluffing, and coalition dynamics; these games test social reasoning beyond optimization.
  - Quick check question: In Werewolf, why does the IRP metric measure both detection skill and deception robustness?

## Architecture Onboarding

- Component map: GameManager -> DataCollector -> GameEnv -> HistoryTracker -> Agent
- Critical path:
  1. Configure game scenario via DataCollector (map, opponent strategy, difficulty)
  2. Initialize GameEnv and Agent; load prompt templates
  3. Run observation-to-prompt → LLM inference → response-to-action loop per step
  4. Log each decision via HistoryTracker; aggregate metrics per capability dimension
  5. Compute normalized capability scores via weighted formula (Equation 3)

- Design tradeoffs:
  - Unified text-based interface (flexibility, LLM compatibility) vs. loss of native game state fidelity (some information may be abstracted)
  - Temperature=0.2 (determinism) vs. reduced exploration diversity; higher variance may better test adaptive learning but reduces reproducibility
  - 10 matches per scenario (statistical robustness) vs. computational cost; StarCraft II iterations average 450, Stratego 1270—expensive for large model sweeps

- Failure signatures:
  - Grounding Accuracy (GA) near 0: LLM outputs invalid actions; check prompt-action dictionary alignment and action space constraints
  - Win Rate (WR) near 0 but Adaptive Learning (GA) high: Agent executes valid actions but strategy is non-adaptive; inspect EER/EPM trends over time
  - High variance across runs (large std in Table 6): Inconsistent reasoning; may indicate temperature too high or prompt ambiguity

- First 3 experiments:
  1. Run GPT-4o and Llama-3.1-70B on StarCraft II "Macro Async" scenario; compare Strategic Planning (RPM, EER, SUR) and Real-Time Decision-Making (APM, EPM) trajectories to validate two-level inference hypothesis
  2. Evaluate Gemini 1.5 Flash and GPT-3.5-Turbo on Diplomacy "Negotiation and Alliances" scenario; analyze Social Reasoning (BIR) and Team Collaboration (ASR, AD) across negotiation rounds to test multi-agent coordination
  3. Conduct ablation on temperature (0.0, 0.2, 0.7) for DeepSeek-V2.5 on Werewolf; measure IRP, KSR, VSS sensitivity to stochasticity in social deduction reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms are required to create a unified agent reasoning framework that generalizes across the heterogeneous rules of multi-strategy games?
- Basis: [explicit] The conclusion explicitly lists "the creation of an agent reasoning framework for multi-strategy games" as a key research direction to further AGI development.
- Why unresolved: Current agents rely on separate inference loops (single-level vs. two-level) and game-specific prompt templates, lacking a unified architecture to handle diverse state spaces natively.
- What evidence would resolve it: A single model architecture achieving high capability scores across all six games without manual prompt adaptation.

### Open Question 2
- Question: Can a unified trajectory dataset of strategy games effectively enhance the generalization and integrated cognitive capabilities of LLM-based agents?
- Basis: [explicit] The authors identify the "development of a unified trajectory dataset for strategy games" as a priority to enhance generalization.
- Why unresolved: The current work evaluates agents using standardized prompts on frozen models, leaving the potential benefits of training on cross-game strategic data unexplored.
- What evidence would resolve it: Demonstrating that fine-tuning on this unified dataset improves the "Adaptive Learning" and "Strategic Planning" scores of open-source models to match closed-source baselines.

### Open Question 3
- Question: To what extent do the "Social Reasoning" and "Team Collaboration" metrics depend on the specific characteristics of the evaluation opponent?
- Basis: [inferred] The methodology (Section 5.1) varies opponents between "built-in AI" and "GPT4o-mini," but does not analyze if these specific opponents introduce bias into the fine-grained capability assessments.
- Why unresolved: Scores in social games like Diplomacy might reflect the agent's ability to exploit specific, static opponent behaviors rather than general reasoning capabilities.
- What evidence would resolve it: A robustness analysis comparing capability scores when agents play against a diverse set of opponent strategies (e.g., random, rule-based, or distinct LLMs).

## Limitations

- Metric weighting transparency: The capability score aggregation relies on weighted normalization (Equation 3) with unspecified coefficients (Wi, wj), limiting exact reproducibility and independent validation of final rankings.
- Computational accessibility: Full benchmark execution requires access to multiple commercial game clients (StarCraft II, Civilization) and commercial LLM APIs, creating barriers for independent replication.
- Generalization scope: While six diverse games are included, the benchmark may not capture all strategic domains (e.g., complex economic simulations, multi-agent emergent behaviors) where LLMs could face distinct challenges.

## Confidence

- High confidence: Multi-dimensional evaluation framework design, automated decision-tracking mechanism, and demonstrated capability to reveal performance differences between model families.
- Medium confidence: The claim that two-level hierarchical inference improves complex game performance, as empirical validation is limited to a few games and model comparisons.
- Low confidence: The specific numerical capability scores and their absolute rankings, given the opaque weighting scheme and lack of ablation studies on metric importance.

## Next Checks

1. **Weighting sensitivity analysis**: Systematically vary Wi and wj in Equation 3 to test stability of model rankings and identify which metrics drive capability score differences.
2. **Independent replication**: Re-implement the observation-to-prompt loop and metric logging for one game (e.g., Werewolf) using open-source models to verify automated tracking and capability dimension separation.
3. **Generalization test**: Apply DSGBench evaluation to a new strategic game (e.g., Chess or Go) to assess whether the five-dimension framework captures relevant cognitive capabilities beyond the original six games.