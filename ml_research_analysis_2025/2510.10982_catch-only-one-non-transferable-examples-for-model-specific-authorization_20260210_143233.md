---
ver: rpa2
title: 'Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization'
arxiv_id: '2510.10982'
source_url: https://arxiv.org/abs/2510.10982
tags:
- data
- matrix
- singular
- authorized
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces non-transferable examples (NEs), a lightweight,\
  \ training-free input-side mechanism that preserves data utility for a designated\
  \ model while rendering it unusable for unauthorized models. NEs work by recoding\
  \ inputs within a model-specific low-sensitivity subspace, identified via singular\
  \ value decomposition of the target\u2019s first linear layer."
---

# Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization

## Quick Facts
- arXiv ID: 2510.10982
- Source URL: https://arxiv.org/abs/2510.10982
- Reference count: 28
- Primary result: Introduces NEs that preserve data utility for authorized models while rendering it unusable for unauthorized ones through model-specific low-sensitivity subspace encoding

## Executive Summary
This paper presents non-transferable examples (NEs), a training-free input-side mechanism for model-specific data authorization. NEs recode inputs within a model-specific low-sensitivity subspace identified via singular value decomposition of the target model's first linear layer. The approach provides formal guarantees: Hoffman-Wielandt inequality ensures unauthorized models suffer performance collapse, while perturbation analysis guarantees minimal impact on the authorized model. NEs maintain near-clean accuracy across diverse vision and vision-language models while degrading unauthorized model performance to chance level.

## Method Summary
The NE method operates by recoding input examples into a model-specific low-sensitivity subspace, identified through SVD decomposition of the target model's first linear layer weights. This recoding process ensures that while the authorized model can recover useful information for accurate predictions, unauthorized models receive corrupted representations that lead to random guessing. The approach requires no retraining of either the authorized model or the protected data, making it computationally lightweight and deployable without model modification. The formal analysis provides theoretical bounds on both the authorized model's performance preservation and the unauthorized model's degradation.

## Key Results
- NEs maintain near-clean accuracy on diverse vision backbones (ResNet-50, ViT, SwinV2, DeiT, MambaVision) and vision-language models (Qwen2.5-VL, InternVL3) across ImageNet and MMBench
- Unauthorized models drop to chance-level performance when processing NEs, even under reconstruction attempts or preprocessing
- The method provides practical model-level data authorization without retraining or encryption overhead

## Why This Works (Mechanism)
The mechanism exploits the fact that different models have distinct sensitivity subspaces in their early layers. By recoding inputs into the low-sensitivity subspace of a specific target model, NEs preserve information that the target can recover while corrupting the input for unauthorized models that have different sensitivity subspaces. The Hoffman-Wielandt inequality provides the mathematical foundation for why this recoding causes performance collapse in unauthorized models, while perturbation analysis ensures the authorized model's performance remains stable.

## Foundational Learning
- **Singular Value Decomposition (SVD)**: Decomposes a matrix into orthogonal components ordered by importance; needed to identify the low-sensitivity subspace of the target model's first layer. Quick check: Verify that the first few singular values capture most of the variance in the weight matrix.
- **Hoffman-Wielandt Inequality**: Bounds the difference between eigenvalues of two matrices; needed to prove that recoding into a different subspace causes significant performance degradation. Quick check: Confirm that the inequality holds when comparing the target model's weight matrix with unauthorized models.
- **Perturbation Analysis**: Studies how small changes in input affect model output; needed to ensure authorized model performance remains stable despite input recoding. Quick check: Measure the change in model output when inputs are recoded using NEs.

## Architecture Onboarding

**Component Map**: Input -> SVD Decomposition -> Subspace Recoding -> Model Forward Pass

**Critical Path**: The critical path is the SVD decomposition of the first linear layer followed by the recoding operation. This determines the model-specific subspace and applies the transformation to inputs before they reach the model.

**Design Tradeoffs**: The method trades off between the strength of protection (how much degradation unauthorized models experience) and the preservation of authorized model performance. More aggressive recoding provides better protection but may slightly impact the authorized model's accuracy.

**Failure Signatures**: If NEs fail, unauthorized models would maintain high accuracy on protected data, or the authorized model would experience significant performance degradation. This could occur if the SVD-derived subspace is not sufficiently model-specific or if the recoding process is too aggressive.

**Three First Experiments**:
1. Measure authorized model accuracy on NEs versus clean examples to verify performance preservation
2. Test unauthorized models on NEs to confirm degradation to chance level
3. Apply reconstruction attacks to NEs to verify resistance to recovery attempts

## Open Questions the Paper Calls Out
None

## Limitations
- Long-term robustness against adaptive adversarial techniques remains uncertain as attack strategies evolve
- Effectiveness relies on the stability of SVD-derived subspaces across different training regimes or fine-tuning scenarios
- Primary validation is on vision and vision-language models; extension to other modalities remains unproven

## Confidence

- **High confidence**: Formal theoretical guarantees (Hoffman-Wielandt inequality, perturbation bounds) are mathematically sound within stated assumptions
- **Medium confidence**: Claims of "no retraining or encryption overhead" may require additional infrastructure in practice
- **Medium confidence**: Model-level authorization claims are supported but real-world scenarios may need additional mechanisms

## Next Checks
1. **Cross-Modality Testing**: Evaluate NE effectiveness on non-vision modalities (text, audio, tabular data) to assess generalizability
2. **Adaptive Attack Resistance**: Design and test adaptive attack strategies targeting the SVD-based subspace protection
3. **Fine-Tuning Robustness**: Assess NE performance when target models undergo fine-tuning or continual learning