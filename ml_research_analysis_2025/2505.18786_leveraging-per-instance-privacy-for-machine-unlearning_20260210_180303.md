---
ver: rpa2
title: Leveraging Per-Instance Privacy for Machine Unlearning
arxiv_id: '2505.18786'
source_url: https://arxiv.org/abs/2505.18786
tags:
- unlearning
- privacy
- data
- losses
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the difficulty of machine unlearning
  varies across individual data points by introducing a principled, per-instance approach.
  The authors analyze noisy gradient descent for unlearning using per-instance privacy
  losses, which bound the Renyi divergence to retraining without an individual data
  point.
---

# Leveraging Per-Instance Privacy for Machine Unlearning

## Quick Facts
- **arXiv ID:** 2505.18786
- **Source URL:** https://arxiv.org/abs/2505.18786
- **Reference count:** 34
- **Primary result:** Introduces per-instance privacy losses to quantify unlearning difficulty, showing they predict unlearning time better than existing metrics.

## Executive Summary
This paper introduces a principled approach to quantify the difficulty of unlearning individual data points by leveraging per-instance privacy losses. These losses, derived from Rényi differential privacy analysis of noisy gradient descent, bound the divergence between models trained with and without each data point. The authors demonstrate that privacy losses effectively predict unlearning difficulty across multiple architectures and datasets, outperforming existing proxy metrics like gradient norms and C-Proxy. The work establishes a foundation for more efficient, adaptive unlearning strategies by identifying which data points require more extensive fine-tuning to remove their influence from trained models.

## Method Summary
The method computes per-instance privacy losses by analyzing gradient norms throughout the training trajectory using Stochastic Gradient Langevin Dynamics (SGLD) with checkpointed models. For each data point, the privacy loss aggregates sensitivity across all checkpoints, ranking points by difficulty to form forget sets. Unlearning is performed via fine-tuning on the retain set, with progress tracked using unlearning accuracy, MIA success rates, Gaussian Unlearning Scores, and loss barriers. While theoretical guarantees require SGLD, the authors validate the approach on standard SGD by approximating implicit noise.

## Key Results
- Per-instance privacy losses reliably predict unlearning time, with harder-to-unlearn data requiring more fine-tuning steps
- Privacy losses identify difficult data more effectively than existing metrics like gradient norm or C-Proxy
- Harder-to-unlearn data points exhibit larger loss barriers in the weight space
- The relationship between privacy loss and unlearning steps follows logarithmic scaling as predicted by theory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-instance privacy losses provide a principled, theoretically grounded bound on unlearning difficulty for individual data points.
- Mechanism: By analyzing noisy gradient descent (SGLD), the per-instance privacy loss P(x, α) bounds the Rényi divergence between a model trained with a data point x and one trained without it. The number of unlearning steps k scales logarithmically with this privacy loss (Corollary 4.4). Data points with larger gradient norms throughout training contribute more to the learned model's distribution, resulting in a higher privacy loss score and thus requiring more fine-tuning steps to unlearn their influence.
- Core assumption: The loss function is Lipschitz continuous and smooth. The training dynamics can be approximated by or explicitly use Stochastic Gradient Langevin Dynamics (SGLD) or standard SGD with a small implicit noise.

### Mechanism 2
- Claim: Per-instance privacy loss is a superior predictor and identifier of difficult-to-unlearn data compared to existing proxy metrics like gradient norm or C-Proxy.
- Mechanism: While simpler metrics like final gradient norm correlate with privacy loss, they are point-in-time snapshots. Privacy loss aggregates the sensitivity of a data point over the entire training trajectory. This accumulated signal more accurately reflects the total influence a data point has embedded into the model's weights, making it a more precise tool for identifying "hard" forget sets that require longer to unlearn.
- Core assumption: The ranking of data points by privacy loss remains stable even when computed with an assumed small noise for standard SGD.

### Mechanism 3
- Claim: The difficulty of unlearning a data point has a geometric interpretation in the loss landscape, characterized by the size of the "loss barrier" to the oracle model.
- Mechanism: A data point with high privacy loss has significantly altered the model's weights, pushing it into a region of the loss landscape separated from the "oracle" model (trained without the point) by a large loss barrier. Fine-tuning must overcome this barrier. After successful unlearning, the loss barrier reduces to a level comparable to the baseline between two independently trained oracle models.
- Core assumption: The linear interpolation path in weight space (modulo permutation) captures the relevant geometric features of the loss landscape related to unlearning.

## Foundational Learning

### Concept: Differential Privacy (DP) and Rényi Divergence
- **Why needed here:** The paper's core contribution is framing unlearning through the lens of Rényi Differential Privacy (RDP). Understanding that RDP provides a way to measure the statistical distance between two distributions (e.g., a model trained with and without a data point) is fundamental to grasping the theoretical guarantee.
- **Quick check question:** How does a bound on the Rényi divergence between two model distributions imply that one model has "unlearned" a data point relative to the other?

### Concept: Stochastic Gradient Langevin Dynamics (SGLD)
- **Why needed here:** The theoretical analysis is explicitly developed for training with SGLD (gradient descent with added Gaussian noise). This connects the privacy loss bound to the stochasticity of the learning process.
- **Quick check question:** What is the key difference between SGLD and standard SGD, and why does this difference make SGLD amenable to formal privacy analysis?

### Concept: Loss Landscape and Linear Connectivity
- **Why needed here:** The paper uses "loss barriers" along a linear path in weight space to provide a geometric interpretation of unlearning difficulty. Understanding that model weights can reside in different "basins" of the loss landscape is crucial for interpreting this result.
- **Quick check question:** What does a high "loss barrier" between two models indicate about their positions in the loss landscape?

## Architecture Onboarding

### Component map:
1. Training with Checkpointing -> 2. Privacy Loss Estimator -> 3. Forget Set Builder -> 4. Unlearning Engine -> 5. Evaluation Suite

### Critical path:
The critical path is Training with Checkpointing -> Privacy Loss Estimator -> Unlearning Engine. The accuracy of the privacy loss estimate directly determines the quality of the "difficulty" signal that guides the unlearning process.

### Design tradeoffs:
- **Checkpoint Frequency vs. Estimation Accuracy:** The paper uses N=35 checkpoints for efficiency. More checkpoints yield a more accurate privacy loss estimate but increase storage and computation.
- **Theoretical Guarantee vs. Practical Applicability:** The core guarantees are for SGLD, but the paper empirically shows the principles extend to standard SGD. Choosing SGD over SGLD for training sacrifices the formal guarantee for wider applicability.

### Failure signatures:
- **Flat Privacy Loss Distribution:** If all data points have similar privacy loss scores, it indicates the privacy loss estimator is not capturing individual data influence.
- **Unlearning Metrics Don't Converge to Oracle:** If the unlearning engine runs for an unexpectedly long time without MIA/UA scores matching the oracle model, it suggests either an exceptionally difficult forget set or ineffective unlearning hyperparameters.
- **Loss Barrier Remains High:** A high loss barrier after unlearning (compared to the oracle-oracle baseline) indicates the model has not traversed the loss landscape to the correct basin.

### First 3 experiments:
1. **Validate Privacy Loss as a Predictor (SGLD):** Train a model on CIFAR-10 using SGLD with different noise levels (σ). Compute privacy losses, create easy/hard forget sets, and run noisy fine-tuning. Plot "time to unlearn" vs "average privacy loss" to confirm the logarithmic relationship predicted by the theory.
2. **Compare Against Proxy Metrics (SGD):** Train a standard model (SGD). Compute privacy losses and other metrics (average gradient norm, C-Proxy). Identify the top-1% "hardest" points according to each metric. Run fine-tuning and compare which set takes longer to unlearn.
3. **Analyze Loss Barriers:** For easy/hard forget sets from experiment 1, compute the loss barrier between the original model, the unlearned model, and the oracle model. Visualize how the barrier height correlates with privacy loss and how it decreases during unlearning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a tight theoretical analysis for group unlearning be developed that meaningfully differentiates forget sets based on their empirical divergence?
- **Basis in paper:** [explicit] Section 4.3 states that current group unlearning bounds are too loose to differentiate forget sets that empirically require different numbers of steps to unlearn, leaving the "problem of obtaining a tight analysis" as an open problem.
- **Why unresolved:** The existing bounds for group privacy (derived from Thudi et al., 2024) implemented by the authors did not vary significantly across forget sets of varying difficulty, failing to capture observed behaviors.
- **What evidence would resolve it:** A theoretical bound for group unlearning that correlates with the specific composition of the forget set (e.g., gradient norms) rather than just its size, successfully predicting unlearning time for groups.

### Open Question 2
- **Question:** Can inherent software and hardware nondeterminism be theoretically exploited to justify applying per-instance privacy losses to standard SGD without explicit noise injection?
- **Basis in paper:** [explicit] Section 5.1 notes that while the authors approximate privacy losses for SGD by assuming implicit noise, it "remains an open problem to exploit software and hardware nondeterminism to offer a theoretical justification."
- **Why unresolved:** The theoretical framework relies on noisy gradient descent (SGLD), so applying it to standard SGD requires assuming the existence of implicit noise ($\sigma$), which currently lacks formal modeling in this context.
- **What evidence would resolve it:** A formal quantification of the noise introduced by hardware nondeterminism (e.g., floating-point variations) that matches the small $\sigma$ values required to make the SGD privacy loss estimates accurate.

### Open Question 3
- **Question:** How can per-instance privacy losses be utilized to design adaptive unlearning strategies that dynamically optimize the utility-unlearning trade-off?
- **Basis in paper:** [explicit] The Conclusion identifies "exploring the use of privacy losses in designing adaptive unlearning strategies" as a promising direction for future work to create more efficient algorithms.
- **Why unresolved:** The current work focuses on using privacy losses as a post-hoc metric to predict difficulty, rather than integrating them into the optimization loop to actively guide the unlearning process.
- **What evidence would resolve it:** An algorithm that uses real-time estimates of per-instance privacy loss to adjust hyperparameters (e.g., learning rates or stopping criteria) for specific data points, demonstrating superior efficiency over static baselines.

## Limitations
- The theoretical guarantees are proven for SGLD, but the paper primarily evaluates on standard SGD with implicit noise, which may not maintain the same logarithmic scaling relationship.
- Computing per-instance privacy losses requires storing multiple checkpoints and calculating gradient norms over the full training trajectory, introducing significant computational overhead.
- The loss barrier analysis assumes linear connectivity in the weight space, which may not hold for highly non-convex architectures or training dynamics.

## Confidence

### Confidence Labels
- **Per-instance privacy losses reliably predict unlearning difficulty:** High
- **Privacy losses are superior predictors compared to existing metrics:** Medium
- **Loss barriers provide meaningful geometric interpretation of unlearning difficulty:** Medium

## Next Checks
1. **SGD-SGLD Gap Validation:** Conduct controlled experiments training identical models on the same dataset using SGLD vs standard SGD with equivalent noise levels. Measure whether the logarithmic relationship between privacy loss and unlearning steps holds consistently across both settings.
2. **Architecture-Agnostic Testing:** Apply the per-instance privacy loss methodology to architectures beyond ResNet-18 and ViT-small (e.g., transformer-based models, recurrent networks) to validate generalizability across diverse model families.
3. **Dynamic Forget Set Analysis:** Implement adaptive unlearning that monitors privacy loss evolution during fine-tuning, dynamically adjusting the forget set composition based on observed unlearning difficulty rather than pre-computing static sets.