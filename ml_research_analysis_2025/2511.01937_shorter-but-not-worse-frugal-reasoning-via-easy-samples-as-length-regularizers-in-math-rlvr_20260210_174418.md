---
ver: rpa2
title: 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers
  in Math RLVR'
arxiv_id: '2511.01937'
source_url: https://arxiv.org/abs/2511.01937
tags:
- length
- reasoning
- problems
- training
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of excessive verbosity in language
  models trained for step-by-step reasoning, which increases inference costs. The
  authors propose that retaining and modestly up-weighting moderately easy problems
  during reinforcement learning acts as an implicit length regularizer, encouraging
  concise solutions.
---

# Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR

## Quick Facts
- **arXiv ID**: 2511.01937
- **Source URL**: https://arxiv.org/abs/2511.01937
- **Authors**: Abdelaziz Bounhar; Hadi Abdine; Evan Dufraisse; Ahmad Chamma; Amr Mohamed; Dani Bouch; Michalis Vazirgiannis; Guokan Shang
- **Reference count**: 5
- **Primary result**: Frugal-Thinking models achieve baseline pass@1 accuracy on AIME25 while generating solutions nearly twice as short as base models

## Executive Summary
This paper addresses excessive verbosity in language models trained for step-by-step reasoning, which increases inference costs. The authors propose that retaining and modestly up-weighting moderately easy problems during reinforcement learning acts as an implicit length regularizer, encouraging concise solutions. They validate this approach through two-stage RLVR fine-tuning on Qwen3-4B-Thinking-2507 and Qwen3-30B-A3B-Thinking-2507 models. The results show that their Frugal-Thinking models achieve baseline pass@1 accuracy on AIME25 while generating solutions that are nearly twice as short as the base models, with improved token efficiency across multiple benchmarks.

## Method Summary
The method involves a two-stage reinforcement learning fine-tuning process. Stage 1 curates training data by filtering problems based on empirical success rate (p) from base model rollouts, retaining p < 1 samples with deliberate skew toward easy problems (p ≈ 0.7-0.9). Stage 2 applies a curriculum-based approach to DeepMath-103 using difficulty annotations. The training uses GRPO with 16 rollouts per query, binary verifier rewards, and token-level importance sampling with clipping (0.8, 1.28). The key innovation is that easy problems provide frequent positive rewards associated with short, correct reasoning traces, creating an implicit length regularizer without explicit penalties.

## Key Results
- Frugal-Thinking models achieve baseline pass@1 accuracy on AIME25 while generating solutions nearly twice as short as base models
- Improved token efficiency across multiple benchmarks including AMC2024, MATH500, and IFMC
- Frugal models maintain accuracy with 16k context window, while baseline models benefit from 42k context
- AIME25 accuracy reaches 56.5% on 30B MoE model with minimal response length of 1,206 tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retaining moderately easy problems during RLVR training acts as an implicit length regularizer without explicit penalties.
- Mechanism: Easy problems (success rate p ≈ 1) provide frequent positive rewards associated with short, correct reasoning traces. When these dominate the training distribution, the gradient signal reinforces concise solutions. The fixed context window creates pressure to complete reasoning within budget, so verbosity becomes non-profitable when rewards primarily come from shorter trajectories.
- Core assumption: The model can generalize concise reasoning patterns learned from easy problems to harder problems, rather than learning difficulty-specific length strategies.
- Evidence anchors: Abstract statement about "implicit length regularizer... emergent brevity for free"; Section 3 discussion of rewards from shorter, solvable trajectories; contrast with explicit adaptive length penalties in "Just Enough Thinking."

### Mechanism 2
- Claim: Standard RLVR filtering creates a length bias by removing samples that would reward brevity.
- Mechanism: GRPO computes advantage Ai = (r(xi, yi) - mean) / std. When all G rollouts are correct (p ≈ 1, easy) or all incorrect (p ≈ 0, unsolvable), Ai = 0 → no gradient. Standard pipelines filter these out, leaving only medium/hard problems requiring longer chains. The policy learns that reward correlates with extended completions, skewing the output length distribution upward.
- Core assumption: The correlation between problem difficulty and solution length is strong and systematic, not random.
- Evidence anchors: Abstract description of standard pipelines filtering easy problems; Figure 1 showing reasoning length varies systematically with difficulty; lack of direct corpus evidence on this specific filtering-bias mechanism.

### Mechanism 3
- Claim: Information-theoretically, verbosity is a statistical shortcut to entropy reduction when length is unpenalized.
- Mechanism: By the chain rule of entropy, H(Y|X, Zt+1) ≤ H(Y|X, Zt)—conditioning on a longer prefix reduces uncertainty about the final answer. When rewards depend only on correctness (binary verifier), the policy can weakly reduce uncertainty by delaying commitment. Over training, this biases output distribution toward longer completions even if tokens are semantically vacuous.
- Core assumption: The policy optimizes for reward maximization without an inductive bias toward efficiency.
- Evidence anchors: Section 2 formalization of entropy reduction via verbosity; Equation (6) showing conditioning on longer prefix decreases entropy; "When More is Less" paper supporting intuition that longer CoT is not always better.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The paper's entire training pipeline uses GRPO; understanding why easy/hard problems yield zero advantage (Ai = 0) is essential to grasp the filtering problem. Quick check: Given 4 rollouts for a problem with rewards [1, 1, 1, 1], what is the advantage for each rollout? (Answer: 0, since all rewards equal the mean)

- **Verifiable Rewards in RLVR**: The mechanism depends on binary rewards (correct/incorrect) with no length penalty; this is what allows implicit regularization to emerge from data distribution alone. Quick check: Why would a length-aware reward (e.g., r = correctness - λ × length) change the mechanism? (Answer: Explicit penalty would provide direct gradient signal against verbosity, making implicit regularization from easy samples redundant)

- **Conditional Entropy and Mutual Information**: Section 2's information-theoretic argument formalizes why verbosity emerges as an entropy-reduction strategy. Quick check: If I(Y; Yt+1 | X, Zt) = 0, does adding more tokens reduce uncertainty about the answer? (Answer: No—zero mutual information means the additional token provides no information about Y)

## Architecture Onboarding

- **Component map**: Data curation pipeline (filter by success rate p → Stage 1: skew toward easy samples → Stage 2: curriculum on DeepMath-103) → Training loop (GRPO with 16 rollouts, binary rewards, token-level importance sampling) → Reward function (extract answer from \boxed{} tag, normalize, compare to ground truth → binary {0, 1})

- **Critical path**: 1. Compute empirical success rate p for each training problem using 16 base model rollouts; 2. Filter to retain p ∈ (0, 1) with intentional over-weighting of p ≈ 0.7-0.9 samples; 3. Run Stage 1 training until clip ratio drops below ~2% and AIME25 accuracy stabilizes; 4. Filter DeepMath-103 by domain-specific difficulty thresholds, exclude Stage 1 overlap; 5. Run Stage 2 curriculum RLVR, monitoring minimum response length

- **Design tradeoffs**: Easy-sample weighting vs. coverage (too many easy samples may limit exposure to diverse reasoning patterns); Context length budget (16k hard constraint vs. 42k for baselines); Two-stage vs. single-stage (Stage 1 establishes brevity, Stage 2 expands capability)

- **Failure signatures**: Clip ratio remains high (>10%) after 500 steps (easy-sample weighting insufficient); AIME25 accuracy drops significantly (easy-sample over-weighting removed learning signal from informative medium-difficulty samples); Stage 2 minimum length drops below ~800 tokens (curriculum too aggressive, model over-compressing)

- **First 3 experiments**: 1. Ablation on easy-sample ratio (train with varying ρ(p) distributions on same base model; measure AIME25 accuracy and average length); 2. Single-stage vs. two-stage comparison (run Stage 2 curriculum directly without Stage 1 brevity pretraining; compare final length and accuracy); 3. Transfer to different domains (apply easy-sample weighting to code reasoning tasks like HumanEval; test whether implicit length regularization generalizes)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implicit length regularization via easy-sample curation generalize to other verifiable-reward domains beyond mathematics, such as coding or formal logical reasoning?
- Basis in paper: The authors state: "Future work may extend this idea to other domains such as coding or logical reasoning."
- Why unresolved: The study only evaluated mathematical reasoning tasks with verifiable rewards; different domains may have different length-solution relationships.
- What evidence would resolve it: Replicating the easy-sample upweighting approach on code generation benchmarks (e.g., HumanEval, MBPP) and logical reasoning tasks, measuring both accuracy and token efficiency.

### Open Question 2
- Question: What is the complete theoretical mechanism underlying emergent brevity, and can it be formally characterized beyond the preliminary information-theoretic argument?
- Basis in paper: The limitations section states: "A deeper theoretical understanding of this behavior remains an important direction for future work" and notes brevity is "primarily supported by empirical evidence rather than a complete theoretical characterization."
- Why unresolved: The paper provides an intuition about conditional entropy reduction but does not prove this mechanism or rule out alternative explanations.
- What evidence would resolve it: Controlled experiments isolating the entropy-reduction mechanism, ablation studies varying the information-theoretic properties of training data, or formal proofs connecting easy-sample distributions to length regularization.

### Open Question 3
- Question: How can implicit regularization be optimally combined with explicit length penalties for finer-grained control over the brevity-accuracy tradeoff?
- Basis in paper: The conclusion states future work should "combine implicit and explicit regularization for finer control of brevity."
- Why unresolved: This study deliberately avoided explicit penalties to demonstrate emergent brevity; whether combining both yields synergistic benefits remains untested.
- What evidence would resolve it: Experiments adding explicit token penalties (e.g., length-dependent reward shaping) to the easy-sample weighting approach, comparing Pareto frontiers of accuracy vs. length.

### Open Question 4
- Question: What is the optimal adaptive curriculum for balancing easy and hard samples throughout training, and how should the weighting evolve?
- Basis in paper: The conclusion calls for exploring "adaptive curricula balancing easy–hard samples."
- Why unresolved: The paper uses a fixed upweighting of easy samples in Stage 1 and a difficulty-based curriculum in Stage 2, but does not systematically optimize this schedule.
- What evidence would resolve it: Ablation studies varying the easy-to-hard ratio over training time, or meta-learning approaches that dynamically adjust sample weights based on training dynamics.

## Limitations

- The claim of implicit length regularization through easy-sample weighting relies on assumptions about reasoning pattern transfer from easy to hard problems that remain untested
- The information-theoretic argument about entropy reduction is elegant but lacks empirical measurement of the correlation between verbosity and entropy reduction in model outputs
- The two-stage training schedule appears critical, yet the paper doesn't ablate the necessity of Stage 1—direct evidence that skipping Stage 1 leads to baseline-level verbosity would strengthen the implicit regularization claim

## Confidence

- **High confidence**: Empirical results showing Frugal-Thinking models generate shorter solutions while maintaining baseline accuracy across multiple benchmarks (AIME25, AMC2024, MATH500, IFMC)
- **Medium confidence**: The mechanism that easy-sample weighting acts as implicit length regularizer through gradient signal concentration
- **Low confidence**: The information-theoretic argument that verbosity is primarily an entropy-reduction strategy rather than a reasoning necessity

## Next Checks

1. **Easy-sample ratio ablation study**: Systematically vary the proportion of easy samples (p ≈ 0.7-0.9) retained during Stage 1 training (e.g., 30%, 50%, 70% easy samples) while keeping all other hyperparameters constant. Measure AIME25 accuracy and average solution length for each configuration. This would directly test whether implicit regularization strength scales with easy-sample proportion and identify the optimal balance between brevity and capability.

2. **Within-model easy-sample weighting ablation**: Train a single base model with multiple stages, each using different easy-sample weightings (including no easy-sample retention), then compare length and accuracy trajectories within the same model family. This controls for base model variability and isolates the effect of the easy-sample strategy on the specific model being evaluated.

3. **Cross-domain transfer validation**: Apply the easy-sample weighting strategy to non-math reasoning tasks (e.g., HumanEval for code, MMLU for general knowledge) and measure whether implicit length regularization generalizes. This tests whether the mechanism is domain-specific or represents a broader principle about RLVR training dynamics.