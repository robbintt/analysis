---
ver: rpa2
title: 'Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs'
arxiv_id: '2508.20333'
source_url: https://arxiv.org/abs/2508.20333
tags:
- refusal
- poisoning
- data
- attack
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Subversive Alignment Injection (SAI), a novel
  poisoning attack that exploits Large Language Model (LLM) alignment to induce targeted
  refusals on benign topics, thereby injecting bias or enforcing censorship. SAI achieves
  this by poisoning alignment data with refusal examples for selected categories,
  causing the model to refuse answering queries related to these targets while preserving
  normal functionality elsewhere.
---

# Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs

## Quick Facts
- arXiv ID: 2508.20333
- Source URL: https://arxiv.org/abs/2508.20333
- Reference count: 40
- Key outcome: Novel poisoning attack (SAI) exploits LLM alignment to induce targeted refusals and bias, achieving up to 90% refusal rates with 12% poisoning while evading state-of-the-art defenses.

## Executive Summary
This paper introduces Subversive Alignment Injection (SAI), a novel poisoning attack that weaponizes LLM alignment processes to induce targeted refusals on benign topics, thereby injecting bias or enforcing censorship. The attack works by poisoning alignment data with refusal examples for selected categories, causing the model to refuse answering queries related to these targets while preserving normal functionality elsewhere. SAI achieves remarkable effectiveness with minimal poisoning (as low as 0.1%) and demonstrates resilience against state-of-the-art defenses including LLM state forensics and robust aggregation in federated learning. The attack's stealth stems from requiring lower parameter updates than traditional steering attacks, making detection difficult. Real-world demonstrations on healthcare and resume screening pipelines confirm practical bias propagation, with refusal rates leading to measurable discrimination.

## Method Summary
SAI exploits the alignment phase of LLM training by injecting poisoned refusal examples into alignment datasets. The attacker selects target categories (e.g., healthcare, education) and crafts synthetic data where benign queries about these topics are met with refusals. During alignment, the model learns to associate these categories with refusal behavior. The attack requires significantly fewer poisoned examples than traditional data poisoning methods because it targets the alignment phase rather than the pre-training phase. The poisoned examples are designed to blend with legitimate alignment data, appearing as normal refusal examples rather than malicious inputs. The attack's effectiveness is measured through refusal rates, bias metrics (ΔDP), and evasion of detection methods.

## Key Results
- Achieved up to 90% targeted refusal rates with only 12% poisoning ratio
- Demonstrated significant bias injection (ΔDP ~68%) on protected categories
- Successfully evaded state-of-the-art defenses including LLM state forensics and robust aggregation in federated learning

## Why This Works (Mechanism)
SAI exploits the fundamental vulnerability in how LLMs learn alignment through supervised fine-tuning. During alignment, models learn to refuse certain types of queries based on safety guidelines. By poisoning this alignment data with refusal examples for benign topics, the attacker hijacks this learning process to induce refusal behavior where it shouldn't exist. The attack is particularly effective because alignment typically involves smaller datasets and fewer parameters than pre-training, making it easier to influence. Additionally, the attack's stealth comes from its similarity to legitimate alignment training - poisoned examples appear as normal refusal data rather than anomalous inputs.

## Foundational Learning
- **Data Poisoning in ML**: Why needed - understanding how malicious data can corrupt model behavior during training. Quick check - can identify attack patterns in training data and understand contamination vectors.
- **LLM Alignment Processes**: Why needed - grasp how safety and behavior guidelines are incorporated into models. Quick check - understand supervised fine-tuning, reward modeling, and refusal pattern learning.
- **Bias Measurement (ΔDP)**: Why needed - quantify discrimination introduced by the attack. Quick check - can calculate and interpret demographic parity differences in model outputs.
- **Federated Learning Security**: Why needed - understand distributed training vulnerabilities. Quick check - grasp how robust aggregation methods work and their limitations against poisoning.

## Architecture Onboarding

Component Map:
Data Collection -> Poisoning Injection -> Alignment Training -> Model Deployment -> Bias Manifestation

Critical Path:
Poisoned Data Creation -> Alignment Phase Injection -> Refusal Pattern Learning -> Target Category Association -> Bias Propagation

Design Tradeoffs:
- Stealth vs. Effectiveness: Lower poisoning ratios increase stealth but reduce effectiveness
- Target Specificity vs. Generalization: Narrower targets are more effective but less broadly applicable
- Detection Evasion vs. Persistence: Techniques that evade detection may be easier to remove

Failure Signatures:
- Anomalous refusal patterns in benign query categories
- Inconsistent refusal behavior across similar query types
- Model state forensics detecting unusual parameter updates during alignment

First Experiments:
1. Baseline alignment with clean data to establish normal refusal rates
2. Controlled poisoning with varying ratios (0.1%, 1%, 5%, 12%) to measure effectiveness curve
3. Defense evasion testing using state-of-the-art forensic tools and robust aggregation methods

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Dataset contamination concerns: Insufficient detail on preventing poisoned examples from appearing in evaluation datasets, potentially inflating effectiveness metrics
- Generalization uncertainty: Limited systematic exploration across different model architectures despite claims of broad applicability
- Long-term stability gaps: Limited longitudinal data on how attack effectiveness degrades over time or with varying fine-tuning intensities

## Confidence
- High: Core mechanism and theoretical framework are sound; stealth advantage through lower parameter updates is technically justified
- Medium: Reported effectiveness metrics are reasonable but lack rigorous dataset contamination prevention details; defense evasion is demonstrated but not comprehensively validated
- Low: Real-world pipeline demonstrations rely on simplified implementations; claims of "urgent threat" extrapolate from controlled experiments without accounting for deployment complexities

## Next Checks
1. Dataset Contamination Audit: Conduct systematic audit to verify poisoned examples were excluded from all evaluation datasets; implement and document rigorous contamination prevention protocol; re-evaluate attack effectiveness with guaranteed clean test sets

2. Architectural Transfer Study: Design comprehensive study testing SAI across broader range of model architectures (transformer variants, attention mechanisms, different scale models); empirically validate generalization claims; identify architectural vulnerabilities

3. Defense Mechanism Benchmark: Develop standardized benchmark testing SAI against diverse suite of state-of-the-art defenses; measure both detection rates and mitigation effectiveness; test across multiple attack intensities and target categories