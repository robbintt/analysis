---
ver: rpa2
title: 'MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive
  Environments'
arxiv_id: '2501.01652'
source_url: https://arxiv.org/abs/2501.01652
tags:
- game
- mystery
- script
- players
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MIRAGE is a comprehensive framework designed to evaluate large
  language models'' proficiency in complex social interactions through murder mystery
  games. The framework includes eight unique scripts with diverse themes and styles,
  and employs four evaluation metrics: Trust Inclination Index (TII), Clue Investigation
  Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index
  (SCI).'
---

# MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments

## Quick Facts
- arXiv ID: 2501.01652
- Source URL: https://arxiv.org/abs/2501.01652
- Reference count: 40
- Primary result: Even state-of-the-art LLMs like GPT-4 struggle significantly with trust dynamics, clue investigation, and role-playing in complex social murder mystery games.

## Executive Summary
MIRAGE is a comprehensive framework designed to evaluate large language models' proficiency in complex social interactions through murder mystery games. The framework includes eight unique scripts with diverse themes and styles, and employs four evaluation metrics: Trust Inclination Index (TII), Clue Investigation Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index (SCI). Experimental results show that even state-of-the-art models like GPT-4 struggle with the complexities of MIRAGE, demonstrating significant challenges in navigating trust dynamics, clue investigation, and role-playing. The framework reveals that most LLMs tend to trust other characters excessively and have difficulty identifying critical information necessary for solving mysteries.

## Method Summary
MIRAGE uses murder mystery game simulations where LLMs play characters with conflicting goals (culprits vs. civilians) across three phases: open conversation, environment interaction with forced disclosure, and voting. Eight diverse scripts are used, each containing character stories, scripts, relationships, performance guidelines, goals, and abilities. Four auxiliary modules ensure standardized evaluation across models: Summarization for token limits, Suspicion/Trust modules for structured scoring, and Rerun for parsing failures. The framework employs four metrics—TII measuring trust-suspicion balance, CIC quantifying information-seeking efficiency, ICI assessing role-playing capabilities, and SCI measuring role adherence—with GPT-4-Turbo as the evaluation model for ICI and SCI.

## Key Results
- Most LLMs demonstrate excessive trust, maintaining trust even when characters self-disclose as culprits
- LLMs struggle to identify critical information, with investigation of key clues showing slow and bumpy progression
- State-of-the-art models like GPT-4 face significant challenges navigating social complexities presented by MIRAGE
- Single scripts (requiring summarization) show systematically worse performance than Multi scripts

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Social Evaluation Decomposition
- Claim: MIRAGE isolates distinct social capabilities through four orthogonal metrics rather than conflating them into a single score.
- Mechanism: The framework decomposes social competence into Trust Inclination Index (TII), Clue Investigation Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index (SCI), each measuring different aspects of social intelligence.
- Core assumption: Social intelligence is factorable into measurable components that collectively predict real-world performance.
- Evidence anchors: [abstract] "MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI)"

### Mechanism 2: Competitive Social Pressure Through Role Conflict
- Claim: Murder mystery games create adversarial social dynamics that expose LLM reasoning limitations more effectively than cooperative-only scenarios.
- Mechanism: Two factions (Culprits vs. Civilians) with opposing objectives force models to balance cooperation against self-interest, with structured phases creating escalating pressure where early trust decisions have delayed consequences.
- Core assumption: Deception detection and strategic self-presentation are fundamental social capabilities that transfer to real-world interactions.
- Evidence anchors: [Section 3.2] "most LLMs demonstrate a higher propensity to trust other characters" even when characters were forced to disclose criminal identities; "most LLMs struggle to identify critical information essential for solving the mystery"

### Mechanism 3: Standardized Auxiliary Modules for Cross-Model Comparability
- Claim: Four auxiliary modules (Summarization, Suspicion, Trust, Rerun) enable fair comparison across LLMs with different context windows and output reliability.
- Mechanism: The modules handle token limits via segmented compression, force structured numerical outputs at consistent checkpoints, and provide retry logic for parsing failures, creating controlled conditions for comparison.
- Core assumption: Structured outputs at fixed intervals do not artificially constrain or enhance model reasoning compared to free-form interaction.
- Evidence anchors: [Section 2.3] "To ensure efficient simulation and accurate evaluation across various LLMs, a standardized set of auxiliary modules has been implemented for all LLMs"

## Foundational Learning

- **Concept: Trust-Suspicion Calibration**
  - Why needed here: TII reveals systematic over-trust (most models maintained trust even when characters self-disclosed as culprits). Understanding calibration error is essential before deploying LLMs in roles requiring skepticism (negotiation, security screening, content moderation).
  - Quick check question: If a character claims "I'm hiding nothing relevant," would your model's trust score decrease, stay neutral, or increase—and what does the ground truth in your domain require?

- **Concept: Information-Seeking Efficiency Under Uncertainty**
  - Why needed here: CIC curves show models explore early but struggle to prioritize key clues over noise. This maps to real-world tasks like debugging, investigation, or research where not all information is equally valuable.
  - Quick check question: Given 10 possible investigation targets and limited turns, can your model articulate why target A is higher priority than target B before taking action?

- **Concept: Script Adherence vs. Behavioral Naturalness Tradeoff**
  - Why needed here: SCI measures fidelity to character constraints, but LLMs "tend to act like normal people during role-playing" in unorthodox scripts—sometimes at the expense of script compliance. This tension appears in any constrained generation task.
  - Quick check question: When user intent conflicts with role constraints, does your model prioritize user accommodation or constraint adherence—and is this behavior documented and consistent?

## Architecture Onboarding

- **Component map:**
  Scripts (8 environments) -> Game Engine (3-phase loop) -> LLM Wrappers (per-model instances) -> Auxiliary Modules (Summarization, Suspicion/Trust, Rerun) -> Evaluation Layer (TII/CIC computation, ICI/SCI scoring)

- **Critical path:**
  1. Script selection → Character assignment per LLM
  2. Phase A (Conversation): Each model generates dialogue → Trust/Suspicion modules collect scores
  3. Phase B (Interaction): Models choose Ask/Investigate → Clues disclosed publicly → CIC updated
  4. Repeat A-B for 5 rounds
  5. Phase C (Voting): Accusations → Victory determination
  6. Evaluation: TII/CIC computed, ICI/SCI scored by GPT-4-Turbo
  7. Aggregation across 8 environments → Final metrics

- **Design tradeoffs:**
  - Full visibility vs. partial information: All participants see all dialogue/clues (simplifies implementation, models human board game) but differs from real-world asymmetric information
  - Automated evaluation vs. human annotation: ICI/SCI use GPT-4-Turbo; human validation shows moderate agreement (τ=0.60-0.87)
  - Temperature=0.8 vs. deterministic output: Paper notes temperature=0 caused "excessive repetition of the previous LLM's output"
  - Context summarization vs. full history: Required for token limits but "summarization can impact decision-making to a certain extent"

- **Failure signatures:**
  - Over-trust trap: High TII (>70) with low Victory rate—model trusts self-disclosed culprits
  - Exploration without prioritization: High CIC for all clues, low CIC for key clues—model investigates randomly
  - Instruction-following collapse: High parsing failures indicate GPT-4o performed "~25.4× worse than GPT-4" on instruction compliance
  - Role drift: High ICI but low SCI suggests naturalistic behavior that violates character constraints
  - Context compression artifacts: Poorer performance on Single scripts vs. Multi scripts suggests "lost in the middle" phenomenon

- **First 3 experiments:**
  1. Baseline calibration: Run one Single-Orthodox-Close script with 2-3 models. Verify TII/CIC/ICI/SCI distributions match expected ranges. Check parsing failure rates and summarization triggers. Expected cost: ~$80-120.
  2. Trust sensitivity test: Replicate protocol running scenarios with forced culprit self-exposure. Compare TII change across models. If Δ≈0, model lacks dynamic trust calibration.
  3. Information prioritization analysis: For one Open script, log sequence of investigated clues. Compute rank correlation between investigation order and clue importance. Low correlation (<0.3) confirms exploration inefficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms can mitigate the excessive trust inclination (high TII) LLMs exhibit toward characters who reveal themselves as culprits?
- Basis in paper: [explicit] Analysis of Table 3 on Page 4 shows most models maintained trust in self-disclosed criminals, with Yi-1.5-9B being the only exception, suggesting a systemic failure in dynamic trust adjustment.
- Why unresolved: The paper identifies this over-trusting behavior as a cause for low victory rates but does not propose or test interventions to correct this bias.
- What evidence would resolve it: Experiments demonstrating that specific prompting strategies or fine-tuning can lower TII scores in response to suspicious behaviors without compromising overall interactivity.

### Open Question 2
- Question: To what extent does the necessary summarization of game history degrade the long-term reasoning and decision-making capabilities of LLMs?
- Basis in paper: [explicit] The Limitations section on Page 5 notes that due to context limitations, summarization is required, which "can impact the decision-making to a certain extent."
- Why unresolved: The current framework relies on summarization to function within token limits, making it impossible to isolate the performance loss caused specifically by information compression versus model architecture limitations.
- What evidence would resolve it: Ablation studies using models with effectively infinite context windows compared against the summarization module to measure the delta in Victory and CIC scores.

### Open Question 3
- Question: How can LLMs be optimized to prioritize "Key Clues" over general environmental details during early game rounds?
- Basis in paper: [explicit] Figure 2 on Page 4 illustrates that while general clue investigation rises quickly, the discovery rate for "Key Clues" is bumpy and slow, indicating models struggle to distinguish critical information early on.
- Why unresolved: The paper observes that LLMs shift focus to interaction as they "thought they become more familiar with the environment," implying a lack of strategic prioritization, but offers no solution.
- What evidence would resolve it: Evaluation of LLMs equipped with specialized retrieval or attention modules that demonstrate a steeper initial rise in the "Key Clues" CIC curve compared to the baseline.

## Limitations
- The framework relies entirely on GPT-4-Turbo for ICI and SCI evaluation, with only moderate human validation agreement (τ=0.60-0.87), introducing potential evaluator bias.
- Context summarization is required for long scripts but may cause "lost in the middle" phenomena affecting performance, with unquantified impact on trust and reasoning decisions.
- Murder mystery games with full visibility and structured phases differ from open-ended social interactions, potentially limiting real-world transferability of the framework's measurements.

## Confidence

**High:** The decomposition of social capability into four orthogonal metrics is methodologically sound and well-implemented. The systematic over-trust finding (most models maintain trust even after self-disclosure) is robust across environments and clearly demonstrated.

**Medium:** The competitive social pressure mechanism effectively reveals LLM reasoning limitations, but the degree to which murder mystery dynamics map to real-world social scenarios requires further validation.

**Low:** Claims about auxiliary modules enabling fair cross-model comparison lack corpus support. The framework assumes structured outputs don't constrain reasoning, but this assumption isn't empirically tested.

## Next Checks

1. **Evaluator Bias Test:** Run ICI/SCI scoring with two different evaluation models (e.g., GPT-4 and Claude-3) on the same logs. Compute inter-rater agreement to quantify evaluator dependency.

2. **Context Preservation Study:** For one Single script, run two versions—one with full context (using GPT-4's 128K context) and one with summarization. Compare TII/CIC/ICI/SCI to isolate summarization impact.

3. **Real-World Transfer Experiment:** Adapt one MIRAGE script to a business negotiation scenario (replace murder mystery with contract dispute). Run same models and compare performance patterns to identify which social capabilities transfer.