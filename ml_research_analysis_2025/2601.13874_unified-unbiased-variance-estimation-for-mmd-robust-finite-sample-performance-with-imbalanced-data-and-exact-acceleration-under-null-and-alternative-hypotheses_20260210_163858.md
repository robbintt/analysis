---
ver: rpa2
title: 'Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance
  with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses'
arxiv_id: '2601.13874'
source_url: https://arxiv.org/abs/2601.13874
tags:
- variance
- kernel
- sample
- e-05
- dvar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational and statistical challenges
  of variance estimation for the Maximum Mean Discrepancy (MMD) statistic in two-sample
  testing, particularly under unbalanced sample sizes and finite-sample regimes. The
  authors derive a unified, unbiased variance estimator via Hoeffding decomposition,
  avoiding asymptotic approximations and capturing both first-order (dominated under
  alternative hypothesis) and second-order (dominated under null hypothesis) components.
---

# Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses

## Quick Facts
- arXiv ID: 2601.13874
- Source URL: https://arxiv.org/abs/2601.13874
- Reference count: 36
- Primary result: Exact O(n log n) acceleration for unbiased MMD variance estimation with Laplacian kernel

## Executive Summary
This paper addresses computational and statistical challenges in variance estimation for the Maximum Mean Discrepancy (MMD) statistic used in two-sample testing. The authors develop a unified, unbiased variance estimator via Hoeffding decomposition that works for both null and alternative hypotheses, even with imbalanced sample sizes. For univariate data with Laplacian kernel, they achieve exact acceleration from O(n²) to O(n log n) complexity through prefix-suffix accumulators. The method demonstrates excellent accuracy in finite-sample regimes and significant runtime improvements, scaling to one million samples.

## Method Summary
The paper derives a unified unbiased variance estimator for MMD² by decomposing it into first-order (T1) and second-order (T2) components using Hoeffding decomposition. For the Laplacian kernel, exact acceleration exploits the semigroup property to compute variance components via sorted sample arrays and prefix-suffix accumulators, reducing complexity from O(n²) to O(n log n). The method uses two bandwidth parameters (σ and σ/2) to simultaneously estimate MMD and Frobenius norms, enabling efficient computation of all variance components through a single pass over sorted data.

## Key Results
- Exact O(n²)→O(n log n) acceleration for Laplacian kernel variance estimation
- Unified estimator valid for null/alternative hypotheses and balanced/unbalanced samples
- Accurate variance estimation across sample ratios m/n ∈ {0.8, 1.0, 1.2, 1.5, 2.0}
- Scalable to n=10⁶ samples with significant runtime and memory improvements

## Why This Works (Mechanism)
The acceleration works by exploiting the Laplacian kernel's semigroup property, which enables prefix-suffix accumulators to compute kernel similarities in sorted order. The Hoeffding decomposition separates variance into first-order terms (dominant under alternative) and second-order terms (dominant under null), allowing accurate finite-sample estimation without asymptotic approximations. The dual-bandwidth approach (σ for MMD, σ/2 for Frobenius norms) enables simultaneous computation of all variance components in a single sorted pass.

## Foundational Learning
- **Hoeffding decomposition**: Decomposes U-statistics into sums of uncorrelated components; needed to separate first-order and second-order variance contributions
  - Quick check: Verify that T1 captures variance from differences between sample means, while T2 captures variance from within-sample differences

- **Laplacian kernel semigroup property**: k(x,x') = exp(-|x-x'|/σ) satisfies k² = k(σ/2); needed for exact acceleration
  - Quick check: Confirm that computing MMD with bandwidth σ and Frobenius norms with σ/2 allows combining terms multiplicatively

- **Prefix-suffix accumulators**: Dynamic programming technique for cumulative sums in sorted arrays; needed to avoid O(n²) pairwise computations
  - Quick check: Verify that row sums in prefix/suffix matrices match analytical expressions in Propositions 1-2

- **U-statistic variance**: For U-statistics, variance has first-order and second-order Hoeffding components; needed for finite-sample accuracy
  - Quick check: Ensure diagonal elements are excluded when computing kernel matrices to maintain unbiasedness

## Architecture Onboarding

**Component map**: Sorted samples → Prefix accumulators → Suffix accumulators → Cross-kernel accumulators → Variance components → Unified estimator

**Critical path**: Sample sorting → Prefix computation → Suffix computation → Cross-term computation → Variance assembly → Final estimate

**Design tradeoffs**: Exact acceleration (O(n log n)) vs. generality (restricted to univariate Laplacian kernel); unified estimator (robust to null/alternative) vs. specialized estimators (potentially more efficient but hypothesis-dependent)

**Failure signatures**: 
- Negative variance estimates indicate incorrect Frobenius norm computation or diagonal inclusion
- Divergences between accelerated and matrix methods indicate sorting or accumulator implementation errors
- Underflow for small σ suggests numerical precision issues in exponential computations

**3 first experiments**:
1. Implement matrix-form MMD² and variance estimator for n=10 samples to establish ground truth
2. Verify prefix accumulator row sums match Proposition 1 analytical expressions for sorted samples
3. Validate exact acceleration implementation against matrix method for n=50 samples per Table 7

## Open Questions the Paper Calls Out
- Can exact quasi-linear acceleration be extended to multivariate data while preserving unbiased variance estimation?
- Can similar exact acceleration be achieved for the Gaussian kernel or other translation-invariant kernels?
- Does the unified variance estimator improve test power in kernel optimization compared to asymptotic approximations?

## Limitations
- Exact acceleration method currently restricted to univariate data with Laplacian kernel
- Runtime benchmarks lack detailed hardware specifications for reproducibility
- Method's behavior with non-Laplacian kernels remains unexplored
- Scalability beyond n=10⁶ samples not empirically validated

## Confidence
- **High Confidence**: Unbiased variance estimator derivation via Hoeffding decomposition, asymptotic computational complexity claims, variance estimation accuracy under null/alternative hypotheses
- **Medium Confidence**: Empirical runtime improvements and memory efficiency claims
- **Low Confidence**: Scalability claims for sample sizes beyond n=10⁶ and generalization to non-Laplacian kernels

## Next Checks
1. Replicate Figure 3 benchmarks using sample sizes n∈{10³,10⁴,10⁵,10⁶} with fixed m/n=1.2 ratio on comparable hardware, measuring both runtime and peak memory usage
2. For n=126 with m/n∈{0.8,1.0,1.2,1.5,2.0}, compare the proposed variance estimator against classical MMD² results in Tables 1-6, ensuring consistent performance across all sample size ratios
3. For n∈{10,20,50}, validate exact acceleration implementation against matrix-form computation per Table 7, confirming numerical agreement within tolerance