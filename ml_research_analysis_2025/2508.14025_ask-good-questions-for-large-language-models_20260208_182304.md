---
ver: rpa2
title: Ask Good Questions for Large Language Models
arxiv_id: '2508.14025'
source_url: https://arxiv.org/abs/2508.14025
tags:
- questions
- knowledge
- recovery
- guiding
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Ask-Good-Question (AGQ) framework, which
  enhances large language models' ability to generate effective guiding questions
  during information retrieval by incorporating a Concept-Enhanced Item Response Theory
  (CEIRT) model. This model dynamically assesses user knowledge states across multiple
  concepts and uses this assessment to generate contextually appropriate questions
  that improve information retrieval efficiency.
---

# Ask Good Questions for Large Language Models

## Quick Facts
- **arXiv ID:** 2508.14025
- **Source URL:** https://arxiv.org/abs/2508.14025
- **Authors:** Qi Wu; Zhongqi Lu
- **Reference count:** 36
- **Primary result:** AGQ achieves 100% accuracy after 20 dialogue rounds vs 41.1% for CoT and 23.9% for Zero-shot in EOR domain

## Executive Summary
This paper introduces the Ask-Good-Question (AGQ) framework, which enhances large language models' ability to generate effective guiding questions during information retrieval by incorporating a Concept-Enhanced Item Response Theory (CEIRT) model. The framework dynamically assesses user knowledge states across multiple concepts and uses this assessment to generate contextually appropriate questions that improve information retrieval efficiency. The framework was evaluated on a custom EOR-QA dataset in the Enhanced Oil Recovery domain, demonstrating superior performance compared to baseline methods and approaching the effectiveness of human experts.

## Method Summary
The AGQ framework uses CEIRT to maintain a dynamic, continuous estimate of user understanding across multiple concepts in a domain. User knowledge θ is updated via gradient descent on BCE loss using interaction history, with simulated correct responses augmenting history when conceptual engagement is inferred from LLM analysis of dialogue. The framework selects "Inspiring Text" based on a suitability score that peaks when the gap between text difficulty and user knowledge is approximately 1, then generates questions using conditional prompts that switch between foundational and application-oriented based on θ thresholds.

## Key Results
- AGQ achieved 100% accuracy after 20 dialogue rounds versus 41.1% for Chain-of-Thought and 23.9% for Zero-shot
- Human evaluation showed AGQ scored 95 on guidance versus 72 for CoT and 0 for Zero-shot
- Knowledge gain was maximized when the absolute difference between question difficulty and user knowledge state was approximately 1

## Why This Works (Mechanism)

### Mechanism 1: Multidimensional Knowledge State Estimation via CEIRT
- **Claim:** The CEIRT model maintains a dynamic, continuous estimate of user understanding across multiple concepts, enabling targeted gap identification.
- **Mechanism:** CEIRT extends the 2PL IRT model to K dimensions. User knowledge θ ∈ R^K, item difficulty b ∈ R^K, and discrimination a ∈ R^K are learned via gradient descent on BCE loss using interaction history. Simulated correct responses augment history when conceptual engagement is inferred from LLM analysis of dialogue, allowing θ to update continuously.
- **Core assumption:** User understanding in a domain can be approximated as a latent vector that evolves predictably with exposure and can be inferred from indirect interaction signals.
- **Evidence anchors:** [abstract] "incorporating a Concept-Enhanced Item Response Theory (CEIRT) model... dynamically assesses user knowledge states across multiple concepts"

### Mechanism 2: Optimal Difficulty-Knowledge Gap Selection for Inspiring Text
- **Claim:** Information retrieval efficiency is maximized when the gap between text difficulty (bi) and user knowledge (θj) is approximately 1, rather than 0 or >1.
- **Mechanism:** A suitability score S(t,j) = exp(−(|θj − bi| − 1)²) peaks at |θj − bi| = 1. Texts meeting this criterion are selected as "Inspiring Text" for LLM context, matching cognitive demand to the user's zone of proximal development.
- **Core assumption:** The 1-unit gap generalizes across domains and user populations.
- **Evidence anchors:** [section 3.4] "Empirical validation for this peak value comes from our ablation study (Section 6)"

### Mechanism 3: Threshold-Based Adaptive Prompt Selection
- **Claim:** Switching between foundational and application-oriented question prompts based on θ thresholds improves relevance and learning outcomes.
- **Mechanism:** For each concept j, if θj ≤ ε, the system uses PQGlow to generate "What is..." style questions; otherwise, PQGhigh generates "How can... be applied..." questions. QualityScore (combining alignment, specificity, complexity) filters outputs.
- **Core assumption:** A single threshold ε cleanly separates need for foundational vs. application questions.
- **Evidence anchors:** [section 3.5, Algorithm 1] "if θj ≤ ϵ then... Generate understanding-biased questions; else... Generate application-biased questions"

## Foundational Learning

- **Concept: Item Response Theory (IRT) and Multidimensional IRT (MIRT)**
  - **Why needed here:** CEIRT builds directly on MIRT; understanding θ, difficulty (b), and discrimination (a) is essential to interpret knowledge state updates and suitability scores.
  - **Quick check question:** Given θ = 0.5, b = 1.5, a = 1.2, what is the probability of a correct response using the 2PL formula?

- **Concept: Prompt Engineering for Conditional Generation**
  - **Why needed here:** AGQ relies on carefully designed prompts (PQGlow, PQGhigh, CoT examples) to steer LLM output; small prompt changes significantly affect question quality.
  - **Quick check question:** What specific phrasing differences distinguish PQGlow from PQGhigh in the appendix?

- **Concept: Gradient-Based Latent Vector Optimization**
  - **Why needed here:** θ is updated via BCE loss minimization using Adam; understanding embedding updates is necessary to debug convergence or divergence of knowledge states.
  - **Quick check question:** If simulated correct responses are added for a concept, will θ for that concept increase or decrease after optimization, and why?

## Architecture Onboarding

- **Component map:** Dataset Construction -> CEIRT Model -> Interaction Loop -> Inspiring Text Selector -> Question Generator
- **Critical path:** High-quality, expert-verified QA dataset -> accurate b/a estimation -> reliable θ initialization and updates -> correct text selection -> relevant question generation
- **Design tradeoffs:** Domain specificity vs. portability (EOR-QA enables strong performance in petroleum but requires costly expert annotation for new domains)
- **Failure signatures:** θ stagnation (interaction logs show θ values unchanged over multiple rounds), repetitive questions (generated questions cluster around few concepts), low QualityScore (many questions dropped)
- **First 3 experiments:**
  1. Parameter sensitivity: Vary ε threshold and observe question type distribution and user knowledge gain
  2. Cross-domain transfer: Apply AGQ to a different vertical using a newly constructed QA dataset
  3. Model ablation: Disable CEIRT (use static θ) and compare against full AGQ

## Open Questions the Paper Calls Out

**Open Question 1:** Does the AGQ framework maintain its performance efficiency and accuracy when applied to domains with less structured knowledge hierarchies or broader concept definitions, such as medicine or open-domain education? The paper states the design is "readily adaptable to knowledge-intensive domains like medicine and education" but was only evaluated in EOR.

**Open Question 2:** How accurately does the "simulated evidence generation" mechanism reflect actual human cognitive retention compared to explicit testing? The framework assumes LLM detection of a concept in user response equates to user mastery without explicit verification.

**Open Question 3:** Is the optimal difficulty-knowledge gap (|θj − bi| ≈ 1) constant across different domains or model scales? The ablation study empirically determined this value maximized knowledge gain in EOR, but this may not generalize.

## Limitations

- Domain-specificity risk: Reliance on expert-verified QA pairs creates significant scalability barriers for new domains
- Parameter sensitivity: Framework depends on critical hyperparameters (ε threshold, CEIRT learning rate) without systematic tuning guidance
- Evaluation scope: Metrics focus on accuracy and knowledge gain within EOR-QA framework, lacking long-term retention or user satisfaction assessment

## Confidence

**High Confidence:** Mechanistic understanding of CEIRT framework and integration with question generation pipeline
**Medium Confidence:** Empirical results showing performance improvements within EOR domain
**Low Confidence:** Generalizability claims about framework applicability to arbitrary specialized domains

## Next Checks

1. **Cross-Domain Transfer Validation:** Implement AGQ in a medically-oriented QA dataset and compare knowledge gain trajectories against EOR baseline to test generalizability beyond petroleum engineering concepts.

2. **Parameter Sensitivity Analysis:** Systematically vary the threshold ε and CEIRT learning rate across multiple dialogue sequences, measuring question quality and accuracy trade-offs to identify robust parameter ranges.

3. **Longitudinal Engagement Study:** Conduct a multi-session evaluation over extended periods (2+ weeks), measuring retention rates, question fatigue, and changes in self-reported domain confidence to assess real-world utility beyond immediate accuracy metrics.