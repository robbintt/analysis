---
ver: rpa2
title: 'Motion Generation: A Survey of Generative Approaches and Benchmarks'
arxiv_id: '2507.05419'
source_url: https://arxiv.org/abs/2507.05419
tags:
- motion
- generation
- text
- conference
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively categorizes recent motion generation
  methods based on their underlying generative approaches, including feed-forward
  models, autoencoders, variational autoencoders, vector-quantized variational autoencoders,
  autoregressive models, generative adversarial networks, diffusion models, latent
  diffusion models, flow matching, implicit neural representations, and physics-based
  methods. For each approach, the paper analyzes architectural designs, conditioning
  mechanisms, and application domains, providing insights into the strengths and limitations
  of different generative strategies.
---

# Motion Generation: A Survey of Generative Approaches and Benchmarks

## Quick Facts
- arXiv ID: 2507.05419
- Source URL: https://arxiv.org/abs/2507.05419
- Authors: Aliasghar Khani; Arianna Rampini; Bruno Roy; Larasika Nadela; Noa Kaplan; Evan Atherton; Derek Cheung; Jacky Bibliowicz
- Reference count: 40
- Primary result: Comprehensive categorization of 11 generative approaches for motion generation with statistical analysis of 106 recent papers

## Executive Summary
This survey provides a systematic taxonomy of recent motion generation methods based on their underlying generative approaches, including feed-forward models, autoencoders, variational autoencoders, vector-quantized variational autoencoders, autoregressive models, generative adversarial networks, diffusion models, latent diffusion models, flow matching, implicit neural representations, and physics-based methods. For each approach, the paper analyzes architectural designs, conditioning mechanisms, and application domains, providing insights into the strengths and limitations of different generative strategies. Additionally, the survey compiles and evaluates the datasets and evaluation metrics commonly used in the literature, facilitating standardized comparisons and identifying trends and gaps in current evaluation practices.

## Method Summary
The survey conducts a systematic review and statistical analysis of 106 recent (2023+) motion generation papers from top-tier venues (CVPR, ICCV, SIGGRAPH, etc.). The methodology involves qualitative coding of papers based on architectural definitions, followed by aggregation into frequency tables and heatmaps. Papers are annotated using 11 generative categories, and attributes including architecture types, conditioning signals, and datasets are extracted to generate usage frequency charts. The corpus collection targets representative papers, though the exact subset used for quantitative statistics is not explicitly provided.

## Key Results
- Diffusion-based methods have emerged as the dominant paradigm with 34+ papers in the surveyed corpus
- Hybrid models combining multiple generative approaches are increasingly common, requiring clearer classification rules
- Standard evaluation metrics include FID, Diversity, MultiModality, R-precision, and physical plausibility measures like foot sliding distance
- Dataset usage shows strong concentration on HumanML3D and KIT-ML, with limited diversity across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Compression Preserves Motion Structure
Compressing motion sequences into lower-dimensional latent spaces enables both efficient generation and controllable manipulation while preserving kinematic plausibility. Autoencoder-based approaches learn a bottleneck representation that must reconstruct the full motion sequence, forcing the latent space to encode only the most salient features—pose configurations, temporal dynamics, and skeletal constraints. When generation occurs in this compressed space, the decoder inherently produces motions that respect learned kinematic regularities.

### Mechanism 2: Iterative Denoising Refines Multi-Modal Distributions
Diffusion models generate high-quality, diverse motion by learning to reverse a gradual corruption process, with each denoising step refining the motion toward both physical plausibility and semantic consistency with conditioning inputs. The forward diffusion process progressively adds Gaussian noise to motion sequences until they become pure noise, while the reverse process trains a neural network to predict noise or clean motion at each step, allowing gradual resolution of kinematic constraints and semantic requirements.

### Mechanism 3: Discrete Tokenization Enables Sequence Modeling Transfer
Converting continuous motion sequences into discrete token sequences via VQ-VAE enables the direct application of powerful language model architectures (transformers, GPT) that excel at capturing long-range temporal dependencies. VQ-VAE quantizes continuous motion features into a finite codebook, mapping each frame or clip to a discrete token ID, transforming the motion generation problem into a sequence modeling problem over a finite vocabulary.

## Foundational Learning

- **Concept**: Motion Representations (Keypoints, Rotations, Meshes)
  - **Why needed here**: All generative approaches operate on some representation of motion. Understanding the trade-offs (2D/3D keypoints lack kinematic constraints; rotation representations maintain bone lengths but require non-Euclidean network design; meshes like SMPL capture shape but are high-dimensional) is essential for selecting appropriate architectures and loss functions.
  - **Quick check question**: Given a motion sequence of 3D joint positions, can you explain why regenerating it directly with an MLP might produce bone length errors, and how a rotation representation would avoid this?

- **Concept**: Probabilistic Latent Variable Models
  - **Why needed here**: VAEs, VQ-VAEs, and diffusion models all rely on learning distributions over latent variables. Understanding the KL divergence term in VAEs (encourages structured latent space), quantization in VQ-VAE (discrete bottleneck), and the forward/reverse diffusion process is prerequisite for comprehending how these models enable diverse sampling.
  - **Quick check question**: In a VAE for motion generation, what happens to the diversity of generated motions if the KL weight is set too high (forcing the latent to match the prior too closely)?

- **Concept**: Temporal Sequence Modeling
  - **Why needed here**: Motion is inherently sequential. Understanding autoregressive models (predict next frame from history), their exposure bias problem, and how attention mechanisms capture long-range dependencies is critical. Many motion generators directly apply these concepts.
  - **Quick check question**: An autoregressive motion model trained on 1-second clips generates a 10-second sequence by feeding its own predictions back as input. What type of error accumulates over time, and how might latent-space generation avoid this?

## Architecture Onboarding

- **Component map**: The motion generation pipeline consists of: (1) Input conditioner (text encoder, trajectory encoder, or action embedder) that processes control signals; (2) Core generative model (diffusion denoiser, VAE decoder, autoregressive transformer) that produces motion representations; (3) Motion decoder (if using latent space) that maps latents to joint positions/rotations; (4) Post-processing/IK that enforces kinematic constraints (optional but common for physics-aware methods). For diffusion: a noise scheduler and guidance module are additional components. For VQ-VAE approaches: a codebook sits between encoder and decoder.

- **Critical path**: For real-time applications (e.g., VR avatar control), the critical path is: conditioning input → latent representation (if applicable) → motion generation → decoding → rendering. Latency is dominated by the generative model (diffusion: number of denoising steps; autoregressive: sequence length). Section III-I notes MotionLCM achieves real-time via consistency-model distillation (few-step sampling). For offline applications (animation authoring), quality metrics (FID, foot sliding, text alignment) take priority over latency.

- **Design tradeoffs**:
  - **Diffusion vs. VAE**: Diffusion provides higher quality and diversity but requires 50-1000x more inference steps. VAEs are fast (single forward pass) but may produce blurrier or less diverse outputs. Latent diffusion offers a compromise.
  - **Continuous vs. Discrete Latent**: Discrete (VQ-VAE) enables language model architectures and easy editing, but introduces quantization error. Continuous (VAE) preserves detail but requires careful latent space engineering.
  - **Autoregressive vs. Non-autoregressive**: AR models capture fine temporal dynamics but suffer from error accumulation and sequential inference. Non-AR (VAE, diffusion) generate entire sequences in parallel but may miss subtle temporal correlations.
  - **Physics-aware vs. Purely data-driven**: Incorporating physics (PhysDiff, RL methods) improves physical plausibility (reduces foot sliding, penetration) but adds complexity and may limit expressiveness.

- **Failure signatures**:
  - **Foot sliding/skating**: Generated motions show feet drifting during contact phases. Common in purely data-driven methods. Metrics: Foot Sliding Distance, Skating. Fix: Add foot contact losses (MDM), use physics simulation (PhysDiff), or post-processing IK.
  - **Motion jitter/artifacts**: High-frequency noise or unnatural accelerations. Common with quantization error (small VQ-VAE codebook) or insufficient diffusion steps. Metrics: Peak Jerk, Acceleration Norm. Fix: Increase codebook size, use more diffusion steps, or apply temporal smoothing.
  - **Semantic misalignment**: Generated motion doesn't match text/action conditioning. Metrics: R-precision, MultiModal Distance, Action Accuracy. Fix: Stronger conditioning (cross-attention, classifier-free guidance), better text encoders, or retrieval-augmented approaches (ReMoDiffuse).
  - **Mode collapse** (GANs): Limited diversity despite varied conditioning. Metric: Diversity, MultiModality. Fix: Use VAE or diffusion instead; for GANs, improve discriminator architecture (Kinetic-GAN's graph convolutions).
  - **Error accumulation** (autoregressive): Long sequences degrade in quality over time. Metric: ADE/FDE increasing with sequence length. Fix: Non-autoregressive generation, latent-space sampling, or periodic reconditioning (MultiAct).

- **First 3 experiments**:
  1. **Baseline implementation**: Implement a simple VAE-based text-to-motion generator (similar to TEMOS architecture) on HumanML3D dataset. Use transformer encoder for text, VAE for motion. Train with reconstruction loss + KL divergence. Evaluate with FID, Diversity, and R-precision. This establishes baseline understanding of latent space motion generation and standard evaluation protocols.
  
  2. **Diffusion comparison**: Implement a vanilla diffusion model (MDM-style) on the same task. Compare sample quality (FID), diversity, and inference time against VAE baseline. Experiment with different parameterizations (ε-prediction vs. x₀-prediction) and guidance scales. This reveals the quality-latency tradeoff and teaches diffusion training/sampling mechanics.
  
  3. **Conditioning ablation**: Take your better-performing model from experiments 1-2 and systematically ablate conditioning mechanisms. Test: (a) text-only, (b) text + past motion (for in-betweening), (c) text + sparse keyframes. Use appropriate metrics (Trajectory Error for keyframes). This clarifies how different control signals interact with the generative process and identifies which conditioning modes your architecture handles well/poorly.

## Open Questions the Paper Calls Out
None

## Limitations
- The survey's primary focus on post-2023 literature may underrepresent foundational methods and long-term trends in motion generation
- Hybrid model classification rules lack formal specification, requiring clearer decision criteria for taxonomy assignment
- The exact composition of the 106-paper corpus used for quantitative statistics is not explicitly provided, making exact reproduction challenging

## Confidence
- **High**: The taxonomy of 11 generative approaches and their architectural descriptions are well-supported by citations and align with established literature
- **Medium**: Evaluation metrics and dataset usage statistics are based on systematic paper analysis, but may miss implicit references or inconsistently reported metrics
- **Low**: Claims about specific architectural advantages (e.g., "Diffusion provides higher quality than VAE") lack direct comparative studies within the surveyed literature

## Next Checks
1. Reconstruct the exact 106-paper corpus by scraping top-tier venue proceedings (CVPR, ICCV, SIGGRAPH) for 2023-2025 using "Motion Generation" and related keywords, then apply the survey's classification criteria
2. Conduct a focused literature search for hybrid model classification rules by examining papers that combine VQ-VAE quantization with autoregressive prediction, and document the decision logic used in the survey
3. Perform a small-scale replication study comparing VAE and diffusion approaches on a standard dataset (HumanML3D) to empirically validate claims about quality-latency tradeoffs