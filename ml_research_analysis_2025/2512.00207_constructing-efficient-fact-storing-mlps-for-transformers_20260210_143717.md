---
ver: rpa2
title: Constructing Efficient Fact-Storing MLPs for Transformers
arxiv_id: '2512.00207'
source_url: https://arxiv.org/abs/2512.00207
tags:
- mlps
- theorem
- embeddings
- construction
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for constructing efficient fact-storing
  MLPs that improve upon previous approaches in three key ways: they work for nearly
  all feasible input-output pairs, achieve asymptotically optimal parameter efficiency
  for some embeddings, and maintain usability within Transformers for factual recall.
  The authors develop a metric called "decodability" that predicts fact-storage capacity
  across different MLP types and show how embedding whitening can improve capacity.'
---

# Constructing Efficient Fact-Storing MLPs for Transformers

## Quick Facts
- **arXiv ID:** 2512.00207
- **Source URL:** https://arxiv.org/abs/2512.00207
- **Reference count:** 40
- **Primary result:** Introduces framework for constructing efficient fact-storing MLPs that achieve asymptotically optimal parameter efficiency while maintaining usability within Transformers for factual recall.

## Executive Summary
This paper presents a framework for constructing efficient fact-storing MLPs that improve upon previous approaches in three key ways: they work for nearly all feasible input-output pairs, achieve asymptotically optimal parameter efficiency for some embeddings, and maintain usability within Transformers for factual recall. The authors develop a metric called "decodability" that predicts fact-storage capacity across different MLP types and show how embedding whitening can improve capacity. Their construction uses an encoder-decoder mechanism with dimensionality reduction that matches information-theoretic bounds for some embeddings. The work also reveals a fundamental tradeoff between an MLP's capacity to store facts and its usability within Transformers, and shows that Lipschitz constant serves as an indicator of usability.

## Method Summary
The method constructs fact-storing MLPs using an encoder-decoder framework where the encoder compresses keys to codes using a gated mechanism, and the decoder projects these codes back to value embeddings using random projections. The construction solves a linear system to find encoder weights that map each key to a unique code, then uses Johnson-Lindenstrauss theory to show that random projections can preserve the dot-product margins needed for retrieval. The approach includes optional embedding whitening to improve decodability, and demonstrates that the constructed MLPs can be used by Transformers for factual recall while storing facts at an optimal rate per parameter.

## Key Results
- Constructed MLPs achieve asymptotically optimal parameter efficiency for some embeddings, with fact-storage cost scaling as Θ([ρ(V)]⁻² |K| log |V|).
- Embedding whitening can improve fact-storage capacity but creates a fundamental tradeoff with Transformer usability via increased Lipschitz constants.
- Lipschitz constant serves as an indicator of MLP usability within Transformers, with higher values degrading training convergence.
- The framework enables modular fact editing in one-layer Transformers by replacing entire MLPs at once.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An MLP can efficiently store facts by decoupling the mapping process into a non-linear encoder (compressing keys to codes) and a linear decoder (decompressing codes to values).
- **Mechanism:** The construction utilizes a "gated encoder gadget" that solves a linear system M(Σ, K) · vec(A) = o. This maps input keys to compressed "codes" with minimal hidden width. A random Gaussian decoder then projects these codes back to the value embedding space using Johnson-Lindenstrauss theory, preserving dot-product margins required for softmax retrieval.
- **Core assumption:** Key embeddings are in "general position" (not degenerate), and the value embeddings have non-zero decodability (ρ(V) > 0).
- **Evidence anchors:** Section 4.3, Theorem 4.3.1 shows the constructed fact MLP stores facts with optimal parameter efficiency; Section 4.2 uses random projection D where m = O([ρ(V)]⁻² log |V|) ensures decoding holds with high probability.

### Mechanism 2
- **Claim:** The efficiency of fact storage is determined by the geometric "spread" of value embeddings, quantified by a metric called decodability (ρ(V)).
- **Mechanism:** The paper defines ρ(V) as the maximum normalized margin between the optimal output vector and the set of value differences. High ρ(V) implies value embeddings are well-separated, requiring fewer parameters to resolve them. The parameter count scales as 1/ρ(V)².
- **Core assumption:** Retrieval relies on dot-product similarity.
- **Evidence anchors:** Section 3.1 defines decodability as measuring the minimum margin normalized by vector norms; Section 3.3 shows embedding whitening increases ρ(V), improving fact-storage capacity.

### Mechanism 3
- **Claim:** There is a fundamental tradeoff between an MLP's storage capacity and its usability within a Transformer, mediated by the Lipschitz constant.
- **Mechanism:** Aggressive embedding whitening improves capacity but distorts the embedding space in a way that increases the spectral norm (Lipschitz constant) of the MLP. High Lipschitz constants appear to destabilize the gradient descent training of the surrounding Transformer layers, reducing "fact-adaptive accuracy."
- **Core assumption:** Transformer layers are trained via first-order optimizers (Adam), which are sensitive to the conditioning of the loss landscape.
- **Evidence anchors:** Section 5.3 finds that the Lipschitz constant of an MLP serves as an indicator of its usability within a transformer; Figure 3c shows a negative correlation between the Lipschitz constant and Transformer usability.

## Foundational Learning

- **Concept: Johnson-Lindenstrauss (JL) Lemma**
  - **Why needed here:** The decoder construction relies on random projections preserving vector distances/margins to allow low-dimensional "codes" to represent high-dimensional values.
  - **Quick check question:** If you project N vectors into m dimensions, how does the distance between any pair change relative to the original distance?

- **Concept: Spherical Chebyshev Center**
  - **Why needed here:** Understanding the derivation of ρ(V) requires knowing that the "optimal output embedding" is the center of the smallest spherical cap covering the set of value difference vectors.
  - **Quick check question:** In a set of points on a sphere, what point maximizes the minimum angular distance to all other points?

- **Concept: Lipschitz Continuity**
  - **Why needed here:** To understand why increasing the Lipschitz constant of an MLP harms training stability in the surrounding Transformer architecture.
  - **Quick check question:** Why might a function with a very large Lipschitz constant cause "gradient explosion" or instability during optimization?

## Architecture Onboarding

- **Component map:** Input Layer (K) -> Whitening Module (optional) -> Encoder (Gated MLP) -> Decoder (Linear) -> Output (dot-product with V)
- **Critical path:** The construction of the Encoder weight matrix A (solving Mv=o) is the computational bottleneck for explicit weight setting, while the Decoder relies on random initialization.
- **Design tradeoffs:**
  - Capacity vs. Usability: Aggressive whitening optimizes storage parameters but degrades Transformer training convergence.
  - Construction vs. Gradient Descent: Constructed MLPs match capacity but may differ in internal dynamics from GD-trained MLPs; however, they allow for "swap-in" modularity.
- **Failure signatures:**
  - NTK Construction Failure: Prior constructions fail on anisotropic embeddings, visible as a collapse in fact-storage accuracy as condition number increases.
  - Usability Collapse: If the MLP is constructed with extreme whitening, the Transformer's cross-entropy loss on non-fact tokens may increase, or the Transformer may fail to learn the recall task entirely.
- **First 3 experiments:**
  1. Verify Encoder Solvability: Implement Algorithm 3 on a small synthetic dataset (d=32, F=100) to verify that A can be solved such that enc(k_i) = c_{f(i)}.
  2. Ablate Whitening Strength (α): Train a Transformer on the SSFR task while varying α ∈ [0, 1] to plot the Pareto frontier between storage capacity and fact-adaptive accuracy.
  3. Modular Swapping: Train a 1-layer Transformer, then freeze all weights except the MLP. Swap in a "constructed" MLP with a new fact set and measure the immediate drop/increase in cross-entropy loss.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can constructed fact-storing MLPs be effectively used within standard, unmodified Transformer architectures, or are the specific architectural interventions (e.g., removing residuals, freezing norms) identified in this paper strictly necessary?
- **Basis in paper:** [inferred]
- **Why unresolved:** The authors state in Section 5.1 that specific modifications were "sufficient" to make the MLP usable, but they do not provide a theoretical justification for why these are needed or if the construction could be adapted to work without them.
- **What evidence would resolve it:** Experiments integrating the constructed MLP into standard, unmodified multi-layer Transformers without freezing layers or removing residuals, measuring fact-adaptive accuracy.

### Open Question 2
- **Question:** Does the capacity-usability tradeoff persist in deeper, multi-layer Transformers, and how does it scale compared to the single-layer case?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper's experiments on usability and the capacity-usability tradeoff (Section 5.2, Section 5.4) are conducted exclusively on one-layer Transformers.
- **What evidence would resolve it:** Analysis of the Lipschitz constant and fact-adaptive accuracy of constructed MLPs when inserted into deeper Transformers (e.g., 12-layer models).

### Open Question 3
- **Question:** What other MLP conditioning metrics beyond the Lipschitz constant can better predict usability within Transformers?
- **Basis in paper:** [explicit]
- **Why unresolved:** In Section 5.3, the authors note that "there likely exist other MLP conditioning related metrics that can also capture this relationship."
- **What evidence would resolve it:** A comparative study correlating usability scores against other metrics (e.g., spectral norm, Jacobian conditioning) for both constructed and GD-trained MLPs.

### Open Question 4
- **Question:** Can the explicit encoder-decoder construction framework be extended to explain or model complex LLM behaviors beyond simple factual recall?
- **Basis in paper:** [explicit]
- **Why unresolved:** The Discussion section explicitly lists this as a "promising direction," asking "whether similar constructions can shed light over LLM behaviors beyond factual recall."
- **What evidence would resolve it:** Constructing MLPs using this framework to store other types of relational data or linguistic patterns and analyzing if they replicate the behavior of pretrained LLMs.

## Limitations

- The theoretical construction relies on strong assumptions including keys being in "general position" and value embeddings having non-zero decodability, which may not hold in practice.
- Empirical validation is limited to synthetic datasets and a single real-world dataset (Goodreads), which may not generalize to diverse embedding distributions in pretrained language models.
- The fundamental tradeoff between capacity and usability mediated by Lipschitz constant is observed but not fully mechanistically explained.

## Confidence

**High Confidence (8/10):** The theoretical construction of the encoder-decoder mechanism is mathematically sound and the parameter efficiency bounds are rigorously derived.

**Medium Confidence (6/10):** The decodability metric accurately predicts fact-storage capacity across different MLP types, with R² > 97% on synthetic data.

**Low Confidence (4/10):** The fundamental tradeoff between capacity and usability mediated by Lipschitz constant is observed but not mechanistically explained.

## Next Checks

1. **Robustness to Key Degeneracy:** Systematically test the encoder construction when keys are not in general position by introducing correlations or clustering in the key embedding space. Measure the breakdown point where the linear system M(Σ,K)vec(A) = o becomes unsolvable or produces degraded accuracy.

2. **Cross-Domain Decodability Analysis:** Evaluate the decodability metric and storage capacity on embeddings from diverse sources: multilingual embeddings, vision transformers, and pretrained language models (e.g., BERT, GPT). This would test whether the theoretical bounds hold beyond synthetic spherical embeddings.

3. **Lipschitz Tradeoff Mechanistic Study:** Design controlled experiments varying the Lipschitz constant through different whitening strengths while keeping capacity constant. Use gradient-based interpretability methods to understand how high Lipschitz constants affect the loss landscape and gradient flow in surrounding transformer layers.