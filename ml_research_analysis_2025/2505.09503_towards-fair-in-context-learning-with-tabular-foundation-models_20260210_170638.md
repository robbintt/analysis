---
ver: rpa2
title: Towards Fair In-Context Learning with Tabular Foundation Models
arxiv_id: '2505.09503'
source_url: https://arxiv.org/abs/2505.09503
tags:
- fairness
- violation
- tabpfn
- in-context
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates fairness in in-context learning (ICL)
  with transformer-based tabular foundation models, an area unexplored despite the
  growing adoption of these models in high-stakes domains. The authors propose three
  pre-processing methods to reduce bias: correlation removal (decorrelating features
  from sensitive attributes), group-balanced sample selection, and uncertainty-based
  sample selection (prioritizing examples with high prediction uncertainty).'
---

# Towards Fair In-Context Learning with Tabular Foundation Models

## Quick Facts
- **arXiv ID**: 2505.09503
- **Source URL**: https://arxiv.org/abs/2505.09503
- **Reference count**: 40
- **Primary result**: Uncertainty-based sample selection consistently improves fairness metrics with minimal accuracy loss in tabular ICL

## Executive Summary
This paper addresses fairness in in-context learning (ICL) with transformer-based tabular foundation models, an underexplored area despite their growing adoption in high-stakes domains. The authors propose three pre-processing methods to mitigate bias: correlation removal (decorrelating features from sensitive attributes), group-balanced sample selection, and uncertainty-based sample selection (prioritizing examples with high prediction uncertainty). Experiments on eight benchmark datasets demonstrate that uncertainty-based selection consistently improves group fairness metrics (demographic parity, equalized odds, equal opportunity) with minimal accuracy loss, while correlation removal can worsen fairness due to information leakage. The approach is model-agnostic and effective across different foundation models.

## Method Summary
The paper introduces three pre-processing methods to enhance fairness in tabular ICL. Correlation removal decorrelates features from sensitive attributes, group-balanced selection ensures proportional representation of sensitive groups in demonstrations, and uncertainty-based selection prioritizes examples where the model shows high prediction uncertainty. These methods are applied during prompt construction for ICL, aiming to reduce bias while maintaining predictive performance. The experiments evaluate these methods across eight benchmark datasets using multiple foundation models, measuring both accuracy and fairness metrics including demographic parity, equalized odds, and equal opportunity.

## Key Results
- Uncertainty-based sample selection consistently improves fairness metrics (demographic parity, equalized odds, equal opportunity) across all datasets
- Correlation removal method can worsen fairness due to potential information leakage between features and sensitive attributes
- Uncertainty-based approach achieves fairness improvements with minimal accuracy loss compared to baseline methods
- Methods demonstrate model-agnostic effectiveness across different foundation models

## Why This Works (Mechanism)
The paper demonstrates that uncertainty-based sample selection works by prioritizing examples where the model exhibits high prediction uncertainty, which correlates with underrepresented or challenging cases that often reveal bias patterns. This approach leverages the model's own confidence signals to identify and balance demonstrations that reduce systematic disparities across sensitive groups. The method implicitly addresses data distribution imbalances by weighting the prompt examples toward cases where fairness violations are more likely to occur.

## Foundational Learning

**In-Context Learning**: Few-shot learning where demonstrations in the prompt guide model predictions without parameter updates
- Why needed: Enables adaptation of foundation models without retraining, crucial for tabular data efficiency
- Quick check: Model performance improves with well-chosen demonstration examples in prompt

**Group Fairness Metrics**: Demographic parity (equal acceptance rates), equalized odds (equal TPR/FPR), equal opportunity (equal TPR)
- Why needed: Quantifies disparate treatment across sensitive groups, essential for fairness evaluation
- Quick check: Compute these metrics across different sensitive attribute values

**Prediction Uncertainty**: Model confidence scores indicating reliability of predictions
- Why needed: Identifies samples where model predictions are less certain, often revealing bias patterns
- Quick check: Correlation between uncertainty scores and prediction accuracy/fairness violations

## Architecture Onboarding

**Component Map**: Raw data -> Pre-processing (correlation removal/selection methods) -> Prompt construction -> Foundation model -> Predictions -> Fairness/accuracy evaluation

**Critical Path**: Data preprocessing -> Prompt construction with balanced/uncertainty-weighted demonstrations -> Foundation model inference -> Fairness metric computation

**Design Tradeoffs**: Correlation removal vs. information retention (correlation removal can harm accuracy due to information loss), selection method computational overhead vs. fairness gains

**Failure Signatures**: 
- Correlation removal causing fairness degradation due to information leakage
- Selection methods failing on extremely small or imbalanced datasets
- Uncertainty-based methods not improving fairness when model uncertainty is poorly calibrated

**First Experiments**:
1. Evaluate correlation removal impact on both fairness metrics and prediction accuracy across different datasets
2. Compare group-balanced selection performance on datasets with varying sensitive attribute distributions
3. Test uncertainty-based selection on foundation models with different uncertainty calibration properties

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to eight benchmark datasets, may not represent real-world diversity in size and feature distributions
- Correlation removal method unexpectedly worsens fairness due to potential information leakage
- Limited exploration of fairness-accuracy trade-offs in extreme dataset scenarios

## Confidence

**High confidence**: Experimental results showing uncertainty-based selection consistently improves fairness with minimal accuracy loss are robust across multiple models and datasets.

**Medium confidence**: Model-agnostic claims supported by multiple foundation models, but broader architectural testing would strengthen assertion.

**Medium confidence**: Group-balanced selection effectiveness demonstrated, but performance in extremely imbalanced datasets remains unclear.

## Next Checks

1. Evaluate proposed methods on real-world datasets from high-stakes domains (healthcare, criminal justice) to assess practical applicability and robustness to dataset-specific biases.

2. Investigate interaction between fairness improvements and model calibration by measuring prediction confidence distributions across sensitive groups after applying each pre-processing method.

3. Test methods on datasets with multiple, intersecting sensitive attributes to evaluate performance in complex fairness scenarios beyond binary sensitive attributes.