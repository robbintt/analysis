---
ver: rpa2
title: 'C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware
  Reasoning'
arxiv_id: '2509.23129'
source_url: https://arxiv.org/abs/2509.23129
tags:
- knight
- knave
- confidence
- answer
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes C2GSPG, a confidence-calibrated group sequence
  policy gradient method to address the overconfidence issue in reinforcement learning
  for reasoning models. It introduces a group sequence policy gradient framework with
  a binary cross-entropy regularizer that aligns model confidence with rewards, ensuring
  both improved reasoning accuracy and calibrated confidence.
---

# C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning

## Quick Facts
- arXiv ID: 2509.23129
- Source URL: https://arxiv.org/abs/2509.23129
- Authors: Haotian Liu; Shuo Wang; Hongteng Xu
- Reference count: 40
- Primary result: C2GSPG significantly outperforms state-of-the-art methods in both reasoning accuracy and confidence calibration on logical and mathematical reasoning tasks.

## Executive Summary
C$^2$GSPG addresses the overconfidence issue in reinforcement learning for reasoning models by introducing a confidence-calibrated group sequence policy gradient method. The approach eliminates token-level bias through sequence-level optimization and ensures calibrated confidence via a binary cross-entropy regularizer. Theoretical analysis proves gradient synergy between policy optimization and calibration for binary rewards, while non-linear normalization and adaptive clipping extend this to non-binary rewards. Experiments on logical and mathematical reasoning tasks demonstrate substantial improvements in both accuracy and calibration metrics.

## Method Summary
C$^2$GSPG implements a Group Sequence Policy Gradient (GSPG) framework that computes a single gradient signal per sequence using normalized sequence-level probability, eliminating token-level bias. For binary rewards, it proves that the BCE confidence-calibration regularizer and policy gradient share the same gradient direction, ensuring collaborative optimization. For non-binary rewards, it applies sigmoid normalization to map rewards to [0,1] and uses adaptive clipping to disable the regularizer when gradient conflicts occur. The method is evaluated on K&K logic puzzles and mathematical reasoning tasks using Qwen2.5 models.

## Key Results
- Achieves significant improvements in both accuracy and Expected Calibration Error (ECE) compared to GRPO and AR-Lopti on mathematical reasoning tasks
- Demonstrates gradient alignment between policy optimization and calibration for binary rewards through theoretical proof and empirical validation
- Shows that sigmoid normalization and adaptive clipping effectively mitigate gradient conflicts for non-binary rewards, with conflict ratios rapidly decaying during training

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Policy Gradient Eliminates Token-Level Bias
- **Claim:** GSPG eliminates token-level bias by computing a single gradient signal per sequence rather than aggregating token-level signals.
- **Mechanism:** Uses normalized sequence-level probability c_{θ,i} = π_θ(o_i|q)^{1/|o_i|} (geometric mean of token probabilities). All tokens receive uniform gradient weighting, preventing low-probability tokens from dominating training.
- **Core assumption:** Sequence-level optimization provides sufficient signal for reasoning improvement without token-level granularity.
- **Evidence anchors:** [abstract] "Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO"

### Mechanism 2: BCE Regularizer Aligns Confidence with Rewards via Gradient Synergy (Binary Rewards)
- **Claim:** For binary rewards, the BCE confidence-calibration regularizer and policy gradient share the same gradient direction, enabling simultaneous optimization without conflict.
- **Mechanism:** The gradient combines two terms: policy direction (r_i - m) and regularization direction (r_i - c_{θ,i})/(1 - c_{θ,i}). For correct responses (r_i=1), both are non-negative; for incorrect (r_i=0), both are non-positive. This alignment means calibration reinforcement never contradicts policy improvement.
- **Core assumption:** Binary reward structure is sufficient to establish this synergy; assumes confidence and reward can be meaningfully aligned in [0,1] space.
- **Evidence anchors:** [abstract] "demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction"

### Mechanism 3: Non-Linear Normalization + Adaptive Clipping Mitigate Non-Binary Conflicts
- **Claim:** Sigmoid normalization maps arbitrary rewards to [0,1] scale, and adaptive clipping disables the regularizer when gradient conflict is detected.
- **Mechanism:** Sigmoid function σ(α·r_i) pushes positive rewards toward 1 and negative toward 0, amplifying gaps and reducing conflict probability. Adaptive clipping I_i(β) sets regularizer weight to 0 when sign(r̂_i - m̂) ≠ sign(r̂_i - c_{θ,i}).
- **Core assumption:** Conflicts are relatively rare after sigmoid amplification; zeroing regularizer in conflict cases doesn't harm overall calibration.
- **Evidence anchors:** [abstract] "For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict"

## Foundational Learning

- **Concept: Policy Gradient with Advantage Functions**
  - **Why needed here:** C²GSPG builds on GRPO's advantage computation A_i = (r_i - m)/(1 - c^{old}_{θ,i}). Understanding baseline subtraction and normalization is essential for interpreting how confidence modulates learning.
  - **Quick check question:** Given group rewards [1, 0, 0, 1] with mean m=0.5, what is the advantage of the first sequence? (Answer: A_1 = (1-0.5)/(1-c^{old}_{θ,i}) > 0, encouraging higher confidence)

- **Concept: Calibration Metrics (ECE, Brier Score)**
  - **Why needed here:** The paper evaluates success via Expected Calibration Error (ECE) and Brier Score (BS), not just accuracy. Without understanding these, you can't interpret Figure 1 or Tables 3-5.
  - **Quick check question:** If a model predicts confidence 0.9 for 100 samples and 80 are correct, what's the calibration error for that bin? (Answer: |0.8 - 0.9| = 0.1)

- **Concept: Token-Level vs Sequence-Level Optimization**
  - **Why needed here:** The core innovation replaces token-level probability ratios (GRPO) with sequence-level ratios. This distinction is critical for understanding why GSPG reduces bias.
  - **Quick check question:** Why might low-probability tokens over-dominate in token-level optimization? (Answer: Gradient contributions include π_{θ,i,t}/π^{old}_{θ,i,t} ratios; when new policy increases low-probability tokens, ratios can become large, amplifying their influence)

## Architecture Onboarding

- **Component map:**
  Prompt q → Sample G responses {o_i} → Compute rewards r_i → Compute old confidence c^{old}_{θ,i} → Binary? → Yes: Compute advantage Ã_i = (r_i - m)/(1 - c^{old}) → No: Normalize r̂_i = σ(α·r_i), then advantage → Forward pass: Compute current confidence c_{θ,i} → Loss = Ã_i · log(c_{θ,i}) + I_i(β) · BCE(r_i, c_{θ,i}) → Backward: Uniform gradient to all tokens in sequence

- **Critical path:**
  1. Confidence computation must use normalized sequence probability (geometric mean), not raw product
  2. For non-binary rewards, sigmoid normalization must preserve reward ordering (monotonic)
  3. Adaptive clipping check: `sign(r̂_i - m̂) == sign(r̂_i - c_{θ,i})` must be evaluated per-sequence before loss aggregation
  4. β selection: Start with paper values (0.01-0.5 for math, 0.03 for logic), tune based on ECE/accuracy trade-off

- **Design tradeoffs:**
  - **Higher β:** Better calibration (lower ECE), potential accuracy drop if over-regularized
  - **Higher α (sigmoid steepness):** More binary-like rewards, fewer conflicts, but may lose reward nuance
  - **Group size G:** Larger groups provide better advantage estimation but increase compute
  - **Assumption:** BCE regularizer preferred over MSE; paper shows MSE produces weak gradients for low-confidence sequences (Appendix A.1)

- **Failure signatures:**
  - **Overconfidence persists:** ECE remains high despite training → β too low, or adaptive clipping disabling regularizer too often
  - **Accuracy drops significantly:** β too high, causing over-aggressive updates
  - **Training instability:** Conflict ratio doesn't decay → α too small, sigmoid not amplifying gaps sufficiently
  - **Low-confidence sequences ignored:** If using MSE instead of BCE (1/(1-c) modulation missing)

- **First 3 experiments:**
  1. **Binary reward validation:** Replicate K&K binary setup (or use math with binary reward). Measure accuracy + ECE. Confirm gradient alignment by logging signs of (r_i - m) and (r_i - c_{θ,i}) per batch.
  2. **Ablation on β:** Train with β ∈ {0, 0.01, 0.1, 0.5} on math task. Plot accuracy vs ECE frontier (Figure 5 pattern). Identify sweet spot for your model/dataset.
  3. **Non-binary reward test:** Use K&K with original {-3,-1,-0.5,3} rewards. Compare: (a) linear normalization + no clipping, (b) sigmoid + no clipping, (c) sigmoid + adaptive clipping. Track conflict ratio (Figure 7a) and final ECE.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning methods be adapted to improve the calibration of models with insufficient capability for challenging tasks, where current methods encourage strategic overconfidence rather than self-awareness?
- **Basis in paper:** [explicit] In Section 4.4, the authors state: "To address the above issue, we will further explore how to improve the calibration of models whose capabilities are currently insufficient for handling challenging tasks."
- **Why unresolved:** The paper observes that on difficult tasks (e.g., math datasets), models are incentivized to "strategically guess" with high confidence to maximize rewards, as honest low-confidence responses yield no reward. C2GSPG mitigates but does not eliminate this behavior.
- **What evidence would resolve it:** A training paradigm that maintains low Expected Calibration Error (ECE) even when the model's accuracy is low on difficult datasets, suggesting the model has learned to be "honest" about its limitations.

### Open Question 2
- **Question:** What methods can effectively enhance out-of-distribution (OOD) robustness and calibration to reduce the performance gap between training and test sets?
- **Basis in paper:** [explicit] In Section 5 (Limitations and Future Work), the authors note: "we observe that the gap in confidence calibration between the training and test sets remains relatively large... we plan to explore methods to enhance out-of-distribution robustness and calibration."
- **Why unresolved:** While C2GSPG improves calibration overall, the generalization of this calibration to unseen test distributions remains a challenge, implying the learned confidence may be overfitted to the training reward structure.
- **What evidence would resolve it:** Empirical results demonstrating a significantly reduced gap between training and test Brier Scores or ECE, or the application of domain adaptation techniques to the confidence estimation process.

### Open Question 3
- **Question:** Can more effective regularization strategies be developed to suppress overconfidence without introducing significant sensitivity to hyperparameters like the regularization weight ($\beta$)?
- **Basis in paper:** [explicit] In Section 5, the authors identify that "model performance is significantly impacted by the choice of $\beta$" and state their intent to "develop more effective regularization strategies."
- **Why unresolved:** The current method relies on a fixed or adaptively clipped $\beta$ to balance policy optimization and calibration. Finding this balance requires tuning, and sub-optimal settings lead to either degraded accuracy or insufficient calibration.
- **What evidence would resolve it:** A proposed method that achieves robust calibration and high accuracy without requiring manual tuning of the regularization strength, perhaps through a dynamic or learned weighting scheme.

## Limitations

- **Gradient Synergy Scope:** The theoretical proof of gradient alignment only holds for binary rewards, with non-binary extensions relying on heuristic normalization without formal guarantees.
- **Hyperparameter Sensitivity:** The method introduces multiple hyperparameters (β, α, learning rate) that interact in complex ways, requiring careful tuning and potentially limiting practical deployment.
- **Task Generalization:** While evaluated on logical and mathematical reasoning, the method's performance on other reasoning domains and non-reasoning tasks remains untested.

## Confidence

- **High Confidence:** The core mechanism of sequence-level policy gradients (GSPG) is well-supported by theoretical analysis and empirical results. The binary reward gradient synergy is mathematically proven.
- **Medium Confidence:** The extension to non-binary rewards through sigmoid normalization and adaptive clipping shows strong empirical results but lacks theoretical guarantees. The effectiveness depends on reward distribution characteristics.
- **Medium Confidence:** The empirical improvements on reasoning tasks are substantial and consistent across multiple benchmarks, but the evaluation scope is limited to specific reasoning domains.

## Next Checks

1. **Gradient Conflict Analysis:** Track the conflict ratio during training for non-binary rewards across different datasets. Verify that sigmoid normalization consistently reduces conflicts and that adaptive clipping appropriately activates when needed.

2. **Hyperparameter Sensitivity Mapping:** Systematically vary β and α on a held-out reasoning task to map the accuracy-ECE tradeoff frontier. Identify the Pareto-optimal region and assess robustness to hyperparameter choices.

3. **Cross-Domain Transfer Test:** Apply C²GSPG to a non-reasoning task (e.g., text summarization or code generation) with binary rewards. Evaluate whether sequence-level gradients and confidence calibration generalize beyond reasoning contexts.