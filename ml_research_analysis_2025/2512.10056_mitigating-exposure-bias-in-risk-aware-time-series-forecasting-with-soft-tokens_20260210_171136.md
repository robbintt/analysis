---
ver: rpa2
title: Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens
arxiv_id: '2512.10056'
source_url: https://arxiv.org/abs/2512.10056
tags:
- risk
- forecasting
- clinical
- time
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SoTra, a soft-token trajectory forecasting
  framework for risk-aware time series prediction in safety-critical settings. Instead
  of sampling discrete tokens, SoTra propagates continuous probability distributions
  through autoregressive steps, enabling differentiable training and reducing exposure
  bias.
---

# Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens

## Quick Facts
- **arXiv ID**: 2512.10056
- **Source URL**: https://arxiv.org/abs/2512.10056
- **Reference count**: 7
- **Primary result**: Soft-token trajectory forecasting achieves up to 18% lower clinical risk and 15% reduction in effective risk compared to baselines while maintaining competitive RMSE.

## Executive Summary
This paper introduces SoTra, a soft-token trajectory forecasting framework for risk-aware time series prediction in safety-critical settings. Instead of sampling discrete tokens, SoTra propagates continuous probability distributions through autoregressive steps, enabling differentiable training and reducing exposure bias. A risk-aware decoding module minimizes expected clinical harm based on domain-specific error grids. Evaluated on glucose and blood pressure forecasting, SoTra achieves up to 18% lower clinical risk and 15% reduction in effective risk compared to strong baselines, while maintaining competitive RMSE. The method supports calibrated, multi-step forecasts suitable for predictive control in physiological domains.

## Method Summary
SoTra represents continuous time series values as categorical distributions over a vocabulary of bins, using a decoder-only GPT-style transformer to predict next-token probabilities. The key innovation is soft-token propagation: instead of sampling discrete tokens during autoregressive inference, the model computes a soft embedding as a weighted average of all token embeddings (e_t = E^T p̂_t), preserving differentiability across the sequence. This enables trajectory-level training that anticipates error accumulation rather than being surprised by it. The method uses a two-stage curriculum: Stage 1 pre-trains with teacher-forced next-token cross-entropy, then Stage 2 fine-tunes with full trajectory unrolling using soft tokens. Risk-aware decoding minimizes expected clinical harm at inference using domain-specific error grids, without affecting the model's probabilistic calibration during training.

## Key Results
- SoTra achieves up to 18% lower clinical risk on glucose forecasting compared to strong baselines
- The method reduces effective risk by 15% while maintaining competitive RMSE
- Soft-token trajectory training improves calibration and reduces exposure bias compared to teacher-forced training alone

## Why This Works (Mechanism)

### Mechanism 1
Soft-token propagation reduces exposure bias by enabling differentiable trajectory-level training. Instead of sampling discrete tokens from the predicted distribution at each autoregressive step, SoTra computes a soft embedding as a weighted average of all token embeddings. This preserves the full computation graph, allowing gradients to flow through the entire forecast horizon during training. The model learns to anticipate its own error accumulation rather than being surprised by it at inference. Core assumption: The discretization vocabulary is sufficiently fine-grained that representing distributions over bins approximates the underlying continuous distribution without critical information loss.

### Mechanism 2
Decoupling probability modeling from risk-aware decoding improves clinical safety without sacrificing calibration. The model is trained using cross-entropy on token distributions, preserving probabilistic calibration. Risk minimization is applied only at inference via expected-risk minimization, which selects the point forecast minimizing expected clinical harm weighted by zone-specific penalties. This separation prevents the model from learning to "game" asymmetric risk functions at the cost of distributional fidelity. Core assumption: The error grid risk weights correctly encode clinical harm relative to the deployment context.

### Mechanism 3
Two-stage curriculum learning stabilizes soft-token trajectory training. Stage 1 uses efficient teacher-forced next-token pre-training to learn basic token distributions. Stage 2 fine-tunes with full trajectory unrolling using soft tokens, allowing the model to adapt to its own predictions. The learning rate drops from 10^-4 to 10^-5 between stages to accommodate noisier gradient estimates during trajectory training. Core assumption: The model has sufficient capacity from Stage 1 to produce reasonable distributions before trajectory fine-tuning begins.

## Foundational Learning

- **Teacher forcing and exposure bias**: Why needed: SoTra's core innovation is mitigating the train-inference mismatch caused by conditioning on ground truth during training but model predictions at deployment. Quick check: In a standard autoregressive model trained with teacher forcing, what happens when a small prediction error occurs at step t during inference?

- **Regression-as-classification (discretization)**: Why needed: SoTra represents continuous time series values as categorical distributions over a vocabulary of bins. Engineers must understand the tradeoff between discretization granularity and computational cost. Quick check: If you discretize a glucose range [40–400 mg/dL] into V=100 bins, what is the resolution per bin, and what happens to values outside this range?

- **Zone-based risk assessment (error grids)**: Why needed: The risk-aware decoding module relies on domain-specific error grids (Clarke for glucose, Saugel for blood pressure). These encode asymmetric clinical costs that MSE ignores. Quick check: On the Clarke Error Grid, why does a 30 mg/dL error near 70 mg/dL carry higher risk than the same error near 150 mg/dL?

## Architecture Onboarding

- **Component map**: Input → reversible instance normalization → discretization → initial embedding → soft-token embedding layer (E^T·p_t) → GPT-style decoder (causal attention, 4 layers, width 256, 4 heads) → logits → softmax → soft embedding (loop) → final distribution → risk-aware decode → point forecast

- **Critical path**: The forward pass flows from normalized input through discretization to initial token, then through the transformer backbone where soft-token propagation occurs via weighted embedding computation. The risk-aware decoder applies expected-risk minimization at the final step.

- **Design tradeoffs**: Higher V improves distribution resolution but increases memory and computation. Higher λ reduces clinical risk but increases RMSE. Trajectory training improves calibration and stability but requires more compute than next-token only.

- **Failure signatures**: Excessive predictions in Zones D/E (glucose) or Zone C (BP): Risk-aware decoding may be disabled (λ=0) or distributions are miscalibrated. Exploding loss during Stage 2: Learning rate too high; reduce to 10^-5 or apply stricter gradient clipping. Poor calibration despite trajectory training: Check discretization overflow bin usage; outliers may be losing information.

- **First 3 experiments**:
  1. Baseline replication: Train SoTra with Stage 1 only (next-token, teacher forcing) and compare trajectory RMSE and CRPS against full two-stage training on DCLP3. This validates exposure bias mitigation.
  2. λ sensitivity sweep: On a held-out validation set, sweep λ ∈ {3, 10, 30, 100, 200} and plot RMSE vs. clinical risk. Identify the knee point for your deployment tolerance.
  3. Ablation on risk-aware decoding: Compare λ > 0 vs. λ = 0 on Zone D/E prediction rates. Confirm that risk reduction comes from decoding, not just better distributions.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but the Discussion section highlights several important directions:

1. **Integration with predictive control**: The paper notes that improved distributional calibration and zone-aware objectives can translate into more conservative and stable control actions near dangerous states, offering a path toward safe and adaptive closed-loop controllers.

2. **Modular design potential**: The soft-token approach's modularity enables integration with multivariate models or foundation-model encoders, suggesting opportunities for extending to more complex physiological modeling.

3. **Clinical deployment considerations**: The risk-aware decoding assumes the model is well-calibrated, but real-world deployment would require careful consideration of how misspecified risk weights or miscalibrated distributions affect clinical outcomes.

## Limitations

- **Discretization resolution ambiguity**: The paper states the model width is 256 but does not explicitly confirm whether V=256 tokens are used for discretization, nor are the bin boundaries specified.
- **Incomplete training specification**: Training duration (epochs/steps), batch size, and validation split ratios are not provided, making reproducibility difficult.
- **Ablation gap**: The two-stage curriculum effectiveness is not tested in isolation - it's unclear how much of the improvement comes from the two-stage approach versus the soft-token mechanism itself.

## Confidence

- **High confidence**: Exposure bias mitigation mechanism via differentiable soft-token propagation
- **Medium confidence**: Clinical risk reduction claims
- **Medium confidence**: Two-stage curriculum effectiveness
- **Low confidence**: Discretization resolution adequacy

## Next Checks

1. **Vocabulary sensitivity analysis**: Systematically vary V ∈ {64, 128, 256, 512} and measure impact on calibration (CRPS), clinical risk (zone distribution), and exposure bias (train vs. test error gap). Identify the minimum V that maintains distributional fidelity.

2. **Risk grid parameter robustness**: Perform sensitivity analysis on λ_zone values by scaling them uniformly (e.g., ×0.5, ×1.5, ×2.0) and measuring changes in RMSE vs. clinical risk tradeoff curves. Determine if the risk-aware decoding remains effective under different clinical cost assumptions.

3. **Stage 2 ablation study**: Train SoTra with only Stage 1 (next-token, teacher forcing) and compare against full two-stage training on exposure bias metrics (calibration drift, error accumulation rate) and clinical risk outcomes. Quantify the marginal contribution of trajectory fine-tuning.