---
ver: rpa2
title: Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning
arxiv_id: '2509.26187'
source_url: https://arxiv.org/abs/2509.26187
tags:
- indoor
- learning
- energy
- hvac
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates deep learning models (LSTM, GRU, CNN-LSTM)
  for forecasting indoor environmental quality (IEQ) parameters using the ROBOD dataset
  from a net-zero energy academic building. The models predict CO2 concentration,
  temperature, and humidity across daily, weekly, and monthly horizons.
---

# Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning

## Quick Facts
- arXiv ID: 2509.26187
- Source URL: https://arxiv.org/abs/2509.26187
- Reference count: 24
- Primary result: GRU achieved best short-term IEQ forecasting (MAE 1.0320, R² 0.9915) while CNN-LSTM excelled at extended horizons despite higher computational cost

## Executive Summary
This study evaluates deep learning models (LSTM, GRU, CNN-LSTM) for forecasting indoor environmental quality parameters using the ROBOD dataset from a net-zero energy academic building. The models predict CO2 concentration, temperature, and humidity across daily, weekly, and monthly horizons. GRU demonstrated superior short-term accuracy with minimal computational overhead, while CNN-LSTM showed promise for extended forecasting through effective feature extraction despite higher complexity. These results support implementing predictive HVAC control to enhance energy efficiency and occupant comfort, with model selection dependent on forecasting horizon and computational constraints.

## Method Summary
The study used the ROBOD dataset containing 123,789 records from 5 rooms in a net-zero energy academic building. Three deep learning models were implemented: LSTM, GRU, and CNN-LSTM. Data preprocessing included polynomial interpolation (order=3), cyclical temporal feature encoding (day_sin, day_cos, month_sin, month_cos), and min-max normalization to [0,1]. A sliding window approach with size 12 (1 hour of 5-minute intervals) converted sequential data into supervised learning samples. Models were trained with Adam optimizer (lr=0.0001), batch size 64, early stopping (patience=7), and learning rate reduction on plateau. Performance was evaluated using MAE, RMSE, MSE, and R² metrics across daily, weekly, and monthly forecasting horizons.

## Key Results
- GRU achieved the lowest global MAE of 1.0320 and highest R² of 0.9915 for short-term forecasting
- CNN-LSTM excelled at extended forecasting horizons through superior feature extraction despite higher computational overhead
- LSTM demonstrated robust performance across all horizon lengths with consistent accuracy
- GRU's simplified gating architecture provided faster training while maintaining competitive accuracy for short-term predictions

## Why This Works (Mechanism)

### Mechanism 1: Gated Architecture for Temporal Filtering
GRU's update and reset gates efficiently filter relevant temporal patterns through simplified gating, achieving superior short-term accuracy. With fewer internal gates than LSTM (2 vs. 3), GRU trains faster and generalizes better on shorter sequences where complex long-term dependencies are less critical. This works because IEQ parameters exhibit strong autocorrelation within 1-hour windows.

### Mechanism 2: Cyclical Feature Encoding for Periodic Pattern Recognition
Decomposing timestamps into sine/cosine pairs enables neural networks to learn periodic IEQ patterns by creating smooth, continuous representations where adjacent time points have similar vector values. This transformation helps gradient-based learning capture recurring patterns in temperature, CO2, and humidity fluctuations driven by occupancy schedules and diurnal cycles.

### Mechanism 3: Hybrid CNN-LSTM for Multi-Scale Feature Extraction
CNN-LSTM architectures extract local temporal anomalies through convolutional layers before modeling long-range dependencies with LSTM. This separation allows the model to isolate transient events from underlying trends, benefiting extended forecasting windows despite higher computational cost. The convolutional layers act as learned filters detecting short-duration patterns across the sliding window.

## Foundational Learning

- **Concept: Sliding Window Time-Series Modeling**
  - Why needed: All three models use window size 12 to convert sequential data into supervised learning samples
  - Quick check: If you increase window size from 12 to 24, would you expect to capture longer dependencies or just add noise?

- **Concept: Gating Mechanisms in RNNs (LSTM vs. GRU)**
  - Why needed: Performance differences hinge on gating structures - LSTM has 3 gates while GRU has 2
  - Quick check: Given GRU has fewer parameters than LSTM, why might it generalize better on short-term forecasting?

- **Concept: Train/Validation/Test Splits for Temporal Data**
  - Why needed: Temporal data requires maintaining chronological order to prevent data leakage
  - Quick check: If you randomly shuffle the ROBOD dataset before splitting, what artifact might appear in validation metrics?

## Architecture Onboarding

- **Component map:** Raw data → Interpolation → Cyclical encoding → Normalization → Sliding window → Model (GRU/LSTM/CNN-LSTM) → Dense output → Predictions

- **Critical path:** 1) Data quality assessment → 2) Extract continuous intervals → 3) Apply cyclical transformation → 4) Normalize to [0,1] → 5) Create sliding windows → 6) Train with early stopping → 7) Evaluate on test set

- **Design tradeoffs:** GRU offers 23% lower global MAE than LSTM with faster training but may underperform on monthly horizons. CNN-LSTM's complexity benefits extended forecasts but risks overfitting on limited data.

- **Failure signatures:** Flat predictions indicate normalization issues; high CO2 error suggests overfitting; sudden validation loss jumps indicate data leakage; slow convergence points to learning rate problems.

- **First 3 experiments:** 1) Baseline GRU replication targeting global MAE <1.1, 2) Ablation study on cyclical features measuring temperature R² impact, 3) Window size sensitivity testing across [6, 12, 24, 48] to identify optimal temporal context.

## Open Questions the Paper Calls Out

- **Real-time deployment robustness:** Can the models maintain accuracy and robustness when deployed in dynamic smart building environments? The study used retrospective analysis rather than live BMS integration.

- **Edge computing optimization:** How can these architectures be optimized for resource-constrained edge devices for live adaptive HVAC control? The study focused on accuracy benchmarking rather than model compression.

- **External feature integration:** To what extent does integrating weather forecasts and occupancy patterns improve IEQ forecasting accuracy? The current study restricted inputs to temperature, CO2, and humidity.

- **Raw data resilience:** How does model performance degrade when applied to raw data containing significant time gaps and sensor noise? Models were trained only on curated, continuous intervals.

## Limitations

- Model architectures (layer counts, hidden units, dropout rates) are not specified, preventing exact replication
- Data preprocessing details are incomplete regarding boundary conditions and edge case handling
- Performance metrics lack confidence intervals, making statistical significance assessment impossible
- The ROBOD dataset is not publicly available, requiring access through academic channels

## Confidence

- **High confidence:** GRU's superior short-term performance (MAE 1.0320, R² 0.9915) with specific hyperparameters (window=12, Adam lr=0.0001)
- **Medium confidence:** Cyclical feature encoding's effectiveness relies on assumed periodic patterns without empirical ablation validation
- **Low confidence:** CNN-LSTM's poor CO2 performance (MAE 26.6146, R² 0.2849) documented but root cause (overfitting vs. architecture mismatch) not investigated

## Next Checks

1. Conduct sensitivity analysis on window size (6, 12, 24, 48) to verify window=12 is optimal for each IEQ parameter
2. Perform ablation studies quantifying cyclical features' impact by comparing performance with and without temporal transformations
3. Replicate the study on a different building's dataset to test GRU's dominance across varying occupancy patterns and building characteristics