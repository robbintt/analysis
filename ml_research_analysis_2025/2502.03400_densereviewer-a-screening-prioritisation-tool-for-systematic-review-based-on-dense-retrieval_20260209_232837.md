---
ver: rpa2
title: 'DenseReviewer: A Screening Prioritisation Tool for Systematic Review based
  on Dense Retrieval'
arxiv_id: '2502.03400'
source_url: https://arxiv.org/abs/2502.03400
tags:
- studies
- screening
- dense
- densereviewer
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DenseReviewer is a web-based tool that prioritizes relevant medical
  studies for systematic review screening using dense retrieval and relevance feedback.
  The system iteratively updates a PICO query with reviewer feedback to re-rank studies,
  achieving higher effectiveness and efficiency compared to traditional active learning
  methods.
---

# DenseReviewer: A Screening Prioritisation Tool for Systematic Review based on Dense Retrieval

## Quick Facts
- arXiv ID: 2502.03400
- Source URL: https://arxiv.org/abs/2502.03400
- Authors: Xinyu Mao; Teerapong Leelanupab; Harrisen Scells; Guido Zuccon
- Reference count: 21
- Primary result: DenseReviewer prioritizes relevant medical studies for systematic review screening using dense retrieval and relevance feedback, achieving higher effectiveness than traditional active learning methods

## Executive Summary
DenseReviewer is a web-based tool designed to improve the efficiency of systematic review screening by prioritizing relevant studies using dense retrieval techniques. The system employs a PICO-based query updating mechanism that iteratively refines study rankings based on reviewer feedback. DenseReviewer offers two screening modes - ranking mode for browsing and focus mode for individual review - and includes an open-source Python library for experimentation. The tool has been containerized for easy deployment and tested on AWS infrastructure, with medical systematic reviews serving as the primary case study.

## Method Summary
DenseReviewer combines dense retrieval with iterative query refinement to prioritize relevant studies during systematic review screening. The system starts with a PICO query that is updated using Rocchio-based relevance feedback as reviewers provide judgements. Dense retrieval is used to re-rank studies based on the updated query, creating a more effective prioritization than traditional keyword-based approaches. The tool provides two user interface modes: a ranking mode for browsing paginated lists of studies and a focus mode for detailed individual review. The system is implemented as a web application with a companion open-source Python library, making it accessible for both practical use and research experimentation.

## Key Results
- Dense retrieval with PICO query updating achieves higher screening effectiveness than traditional active learning methods
- The tool provides significant efficiency improvements through iterative query refinement based on reviewer feedback
- Two screening modes (ranking and focus) offer flexibility for different review workflows
- The system is containerized and tested on AWS infrastructure for practical deployment
- Open-source Python library enables experimentation and model development

## Why This Works (Mechanism)
DenseReviewer leverages dense retrieval to capture semantic relationships between studies and the PICO query, moving beyond exact keyword matching. The iterative query updating mechanism allows the system to learn from reviewer feedback and refine its understanding of relevance over time. By focusing on the PICO framework, the tool aligns with how reviewers naturally think about inclusion criteria in systematic reviews. The dual-mode interface accommodates different reviewer preferences and workflows, while the containerized deployment ensures accessibility across different environments.

## Foundational Learning

### Dense Retrieval
- **Why needed**: Traditional keyword matching misses semantically similar documents that don't share exact terms
- **Quick check**: Compare retrieval results using dense vs. sparse methods on the same PICO query

### Rocchio Relevance Feedback
- **Why needed**: Iteratively improves query representation based on reviewer judgements
- **Quick check**: Measure ranking improvement after each feedback iteration

### PICO Framework Integration
- **Why needed**: Aligns with systematic review methodology and reviewer mental models
- **Quick check**: Validate that retrieved documents match PICO element expectations

### Web-based Interactive Interface
- **Why needed**: Enables real-time feedback and accessibility for distributed review teams
- **Quick check**: Test interface responsiveness during extended screening sessions

## Architecture Onboarding

### Component Map
User Interface -> Dense Retriever -> PICO Query Updater -> Relevance Feedback Collector -> Document Ranking Engine

### Critical Path
Reviewer provides feedback -> Query updater processes feedback -> Dense retriever generates new rankings -> Updated results displayed to reviewer

### Design Tradeoffs
The system prioritizes recall over precision to ensure relevant studies are not missed, trading some efficiency for completeness. The choice of dense retrieval over traditional methods sacrifices some interpretability for improved semantic matching capability.

### Failure Signatures
- Poor initial PICO query formulation leads to suboptimal retrieval results
- Insufficient diversity in initial results may cause the system to miss relevant studies
- Reviewer feedback inconsistency can degrade query updating effectiveness

### First 3 Experiments
1. Test retrieval effectiveness with synthetic PICO queries on benchmark medical datasets
2. Evaluate query updating performance with simulated reviewer feedback patterns
3. Compare user experience and efficiency between ranking mode and focus mode in controlled user studies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively integrated to automate relevance judgements while maintaining the high recall required for systematic reviews?
- Basis in paper: "The third planned functionality is to automate relevance judgements during screening. We will investigate LLM-assisted screening to further reduce screeners' workload."
- Why unresolved: Current system relies entirely on human feedback; LLM integration risks compromising recall guarantees essential for SR validity.
- What evidence would resolve it: Comparative user studies measuring recall, workload reduction, and screener trust across human-only, LLM-assisted, and fully automated screening workflows.

### Open Question 2
- Question: How can exclusion criteria be incorporated into PICO queries within a dense retrieval framework?
- Basis in paper: "The first planned functionality is to extend the PICO query to allow users to express exclusion criteria."
- Why unresolved: Current PICO structure and Rocchio-based query updating address inclusion only; dense retrieval does not natively support negative constraints.
- What evidence would resolve it: Retrieval effectiveness benchmarks comparing exclusion-aware PICO queries against baseline inclusion-only queries on standard SR test collections (e.g., CLEF TAR).

### Open Question 3
- Question: Can LLM-based automatic extraction and highlighting of PICO-relevant text spans improve screening efficiency without introducing bias?
- Basis in paper: "The second planned functionality is the automatic extraction and highlighting of potential words or sentences associated with (non-)relevant PICO elements."
- Why unresolved: Highlighting could guide attention but may also bias judgements or miss critical contextual information outside highlighted regions.
- What evidence would resolve it: Controlled experiments measuring time-to-decision, agreement rates, and recall with and without LLM-generated highlighting.

## Limitations

- Evaluation relies on simulated reviewer feedback rather than real-world screening scenarios, introducing uncertainty about practical performance
- Limited testing to medical systematic reviews raises questions about generalizability to other domains
- Comparison with traditional methods based on limited benchmark datasets may not reflect all review contexts
- Focus on ranking effectiveness metrics without comprehensive assessment of user experience factors

## Confidence

- **High confidence**: Technical implementation details including dense retrieval approach and PICO query updating are well-documented and reproducible
- **Medium confidence**: Claimed efficiency improvements are supported by results but may vary in practical applications with real reviewers
- **Low confidence**: Long-term reliability and robustness across diverse topics and reviewer expertise levels remain untested

## Next Checks

1. Conduct user studies with actual systematic review teams to evaluate real-world performance, measuring efficiency gains and reviewer satisfaction

2. Test generalization by applying the system to reviews from different domains (social sciences, engineering) and comparing performance across contexts

3. Implement longitudinal studies tracking performance over extended screening sessions to assess maintenance of effectiveness during reviewer fatigue and changing relevance patterns