---
ver: rpa2
title: 'MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination
  in Vision-Language Models'
arxiv_id: '2502.20780'
source_url: https://arxiv.org/abs/2502.20780
tags:
- medical
- medhalltune
- clinical
- hallucination
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in vision-language models (VLMs)
  when applied to medical imaging tasks, where models may generate plausible but incorrect
  outputs that could compromise clinical decision-making. The authors introduce MedHallTune,
  a large-scale benchmark consisting of over 100,000 images and 1 million instruction
  pairs, specifically designed to evaluate and mitigate hallucinations in medical
  VLMs.
---

# MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models

## Quick Facts
- arXiv ID: 2502.20780
- Source URL: https://arxiv.org/abs/2502.20780
- Reference count: 27
- Primary result: Instruction-tuning on MedHallTune improves hallucination mitigation and zero-shot performance in medical VLMs

## Executive Summary
This paper introduces MedHallTune, a large-scale benchmark with over 100,000 images and 1 million instruction pairs designed to evaluate and mitigate hallucinations in vision-language models for medical imaging. The dataset covers three hallucination types: nonexistent medical objects, object manipulation, and clinical knowledge distortion. The authors propose new evaluation metrics assessing clinical accuracy, relevance, detail level, and risk level, and demonstrate that fine-tuning VLMs on MedHallTune significantly improves their ability to manage hallucinations while enhancing performance on downstream visual question-answering tasks.

## Method Summary
The authors construct MedHallTune by sourcing 100,000+ medical images from PubMed datasets and using GPT-4o to generate instruction-response pairs. They implement a two-stage quality control process where GPT-4o generates instructions and then validates them against ground truth annotations, filtering out 7.3% of inconsistent pairs. The dataset contains both hallucination (negative) and non-hallucination (positive) instruction pairs. Models are fine-tuned using LoRA for 3 epochs on 8×A100 GPUs. Evaluation employs GPT-4o-based scoring across four clinical metrics, with baseline calibration against LLaVA-Med outputs. The approach is tested on InternVL-v1.5-4b, Qwen-VL-Chat, and LLaVA-Med-v1.5-7b.

## Key Results
- Models fine-tuned on MedHallTune achieve higher clinical accuracy scores (6.67) compared to pre-trained counterparts (6.27)
- Balanced training on positive and negative instruction pairs outperforms training on either subset alone
- Fine-tuning improves zero-shot performance on downstream VQA tasks including VQA-RAD, Path VQA, and SLAKE
- Quality control filtering of 7.3% of generated instructions yields measurable performance improvements

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Instruction Tuning with Positive/Negative Pairs
- **Claim:** Training on both hallucination (negative) and non-hallucination (positive) instruction pairs improves a VLM's ability to recognize and avoid generating medically incorrect outputs.
- **Mechanism:** Negative instructions explicitly present hallucination scenarios, and the model learns to generate corrective responses. Positive instructions reinforce accurate medical image captioning and diagnosis. The contrast between these signal types creates a learned boundary between grounded and ungrounded generation.
- **Core assumption:** The model can generalize from explicit hallucination examples to novel, unstated hallucination patterns during deployment.
- **Evidence anchors:** [abstract] MedHallTune includes both hallucination and non-hallucination samples; [section 3, Fig. 5(a)] model trained on complete MedHallTune outperforms those trained solely on positive or negative samples.
- **Break condition:** If hallucination types in deployment diverge significantly from the three curated categories, the learned boundary may not transfer.

### Mechanism 2: LLM-Mediated Quality Control via Self-Checking
- **Claim:** A two-stage filtering process using GPT-4o to generate and then validate instruction pairs against ground-truth annotations reduces label noise and prevents contradictory training signals.
- **Mechanism:** GPT-4o generates instruction-response pairs from image-annotation inputs. A second pass presents only the annotations (without images) to GPT-4o to verify consistency. This filters out 7.3% of generated instructions that contain contradictions relative to ground truth.
- **Core assumption:** GPT-4o's internal knowledge is sufficient to detect factual inconsistencies in medical content, and its verification is more reliable than the generation step alone.
- **Evidence anchors:** [section 2.2] Additional round of self-checking filters out 7.3% of incorrect instructions; [section 3, Fig. 5(a)] models trained with quality control demonstrate marked improvements.
- **Break condition:** If GPT-4o's medical knowledge contains systematic gaps or biases, the validator may incorrectly approve hallucinated content or reject accurate rare-case descriptions.

### Mechanism 3: Domain-Specific Evaluation Metrics Aligned with Clinical Risk
- **Claim:** Standard NLP metrics fail to capture clinically relevant failures; domain-specific metrics better correlate with safe deployment requirements.
- **Mechanism:** GPT-4o serves as a consistent evaluator using in-context learning. Baseline scores from LLaVA-Med establish a reference point. Each response is scored 1–10 on four dimensions, with explicit reasoning chains.
- **Core assumption:** GPT-4o can reliably assess clinical accuracy and risk level, and its judgments correlate with human expert evaluation.
- **Evidence anchors:** [section 2.4] Evaluation comprises four critical metrics including Clinical Accuracy; [section 3, Table 2] models fine-tuned on MedHallTune achieve higher clinical accuracy scores.
- **Break condition:** If GPT-4o's clinical judgment diverges from human expert consensus in edge cases, optimization against these metrics may reward plausible-but-wrong outputs that "sound" correct to the evaluator.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - **Why needed here:** MedHallTune fine-tunes existing VLMs. Understanding the vision encoder + LLM decoder structure is prerequisite to knowing what LoRA fine-tuning modifies and how instruction data flows through the model.
  - **Quick check question:** Can you sketch the data flow: image → vision encoder → projector → LLM → text output? Do you know which weights LoRA adapts?

- **Concept: Instruction Tuning and Alignment**
  - **Why needed here:** The core intervention is instruction-tuning on positive/negative pairs. You need to understand how supervised fine-tuning on (instruction, response) pairs shapes generation behavior and why balanced positive/negative samples prevent response bias.
  - **Quick check question:** If a model were trained only on negative (hallucination-correction) examples, what failure mode might emerge at inference time?

- **Concept: Hallucination Taxonomy in Multimodal Models**
  - **Why needed here:** MedHallTune categorizes hallucinations into three types (nonexistent objects, attribute manipulation, knowledge distortion). Recognizing these categories is necessary to interpret evaluation results and diagnose model failures.
  - **Quick check question:** Given a VLM response stating "The CT shows a 3cm liver lesion with irregular margins" when no lesion exists, which hallucination category applies?

## Architecture Onboarding

- **Component map:** Source datasets (PubMed-sourced image-text pairs) → GPT-4o instruction generation (positive + negative) → Self-checking filter → Train/test split → LoRA fine-tuning on target VLMs → GPT-4o evaluation using 4 clinical metrics

- **Critical path:** Data quality is the gating factor—Section 2.3 reports filtering 7.3% of instructions; ablation (Fig. 5a) shows quality control yields measurable gains; balanced positive/negative sampling—training on only one type underperforms the combined dataset; baseline calibration against LLaVA-Med is required before comparing other models

- **Design tradeoffs:** Scale vs. curation (1M instruction pairs provide coverage but may inherit GPT-4o biases; manual verification was not performed); Metric complexity vs. reproducibility (four clinical metrics require LLM evaluator, introducing dependency on closed model); Generalization vs. overfitting (fine-tuning improves MedHallTune test scores and downstream VQA, but out-of-distribution performance is not reported)

- **Failure signatures:** Hallucination persistence (if model still generates plausible incorrect answers, check whether failure type matches one of the three training categories); Over-correction/refusal (if model becomes overly conservative, this may indicate imbalance toward negative samples); Metric gaming (if clinical accuracy scores improve but downstream task performance degrades, model may be optimizing for evaluator phrasing)

- **First 3 experiments:**
  1. Reproduce the ablation in Fig. 5(a): Train on positive-only, negative-only, and combined data. Verify that combined training outperforms either subset.
  2. Run zero-shot evaluation on a held-out VQA dataset not in Table 3: Test whether gains generalize beyond reported benchmarks.
  3. Human spot-check on clinical accuracy scores: Sample 20–30 model outputs and have a domain expert independently rate clinical accuracy. Compare to GPT-4o scores to assess evaluator alignment.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations
- Dependency on GPT-4o for both data generation/quality control and evaluation introduces potential knowledge biases and evaluation variability
- Evaluation focuses on within-distribution performance without reporting generalization to truly out-of-distribution medical domains
- Specific GPT-4o prompt templates for hallucination generation and self-checking are not disclosed, limiting reproducibility

## Confidence

- **High confidence:** The ablation showing balanced positive/negative instruction tuning outperforms training on either subset alone; the quantitative improvement in clinical accuracy scores for models fine-tuned on MedHallTune; the observation that 7.3% of generated instructions were filtered through quality control
- **Medium confidence:** The transferability of hallucination mitigation to downstream VQA tasks; the sufficiency of the three hallucination categories to capture real-world clinical hallucination patterns
- **Low confidence:** The clinical relevance of GPT-4o-based evaluation metrics without human expert validation; the generalizability of results to medical domains outside the source datasets

## Next Checks

1. **Human expert validation:** Have domain experts independently rate clinical accuracy on a random sample of 30 model outputs (fine-tuned vs. baseline) to compare against GPT-4o scores and assess evaluator alignment

2. **Out-of-distribution testing:** Evaluate fine-tuned models on a held-out medical VQA dataset from a different source domain (e.g., not based on PubMed) to test generalization beyond the training distribution

3. **Novel hallucination detection:** Construct test cases with hallucination patterns outside the three categories (nonexistent objects, object manipulation, knowledge distortion) to assess whether the model has developed general grounding behavior or merely memorized specific patterns