---
ver: rpa2
title: 'Minos: A Multimodal Evaluation Model for Bidirectional Generation Between
  Image and Text'
arxiv_id: '2506.02494'
source_url: https://arxiv.org/abs/2506.02494
tags:
- evaluation
- data
- multimodal
- tasks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Minos, a multimodal evaluation model designed
  for bidirectional generation between image and text. The work addresses the lack
  of evaluation capabilities for text-to-image (T2I) generation tasks and the underutilization
  of large-scale human evaluation data in existing multimodal evaluation systems.
---

# Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text

## Quick Facts
- **arXiv ID:** 2506.02494
- **Source URL:** https://arxiv.org/abs/2506.02494
- **Reference count:** 13
- **Primary result:** Achieves state-of-the-art performance among open-source multimodal evaluation models on both image-to-text and text-to-image generation tasks

## Executive Summary
This paper introduces Minos, a multimodal evaluation model designed for bidirectional generation between image and text. The work addresses the lack of evaluation capabilities for text-to-image generation tasks and the underutilization of large-scale human evaluation data in existing multimodal evaluation systems. The authors construct Minos-Corpus, a large-scale multimodal evaluation dataset comprising 124k samples across 16 datasets and 6 tasks, including both image-to-text and text-to-image generation tasks. The dataset combines human annotations with GPT-4o-generated evaluations to provide high-quality training data. Based on this corpus, the authors propose a novel training approach that combines Data Selection and Balance, Mix-SFT training, and DPO alignment to develop Minos, a multimodal evaluation model built on a 7B backbone. The model achieves state-of-the-art performance among open-source evaluation models of similar scale on the average of evaluation performance across all tasks and outperforms all open-source and closed-source models on T2I generation task evaluation.

## Method Summary
The authors construct Minos-Corpus, a large-scale multimodal evaluation dataset combining 124k samples across 16 datasets and 6 tasks (I2T and T2I generation). The training pipeline uses a 7B LLaVA-OneVision backbone with three stages: (1) Data Selection and Balance to filter and reweight GPT-4o annotations against human scores, (2) Mix-SFT training that incorporates both evaluation and pairwise comparison data, and (3) DPO alignment to refine preference ranking. The model is trained to evaluate both image-to-text and text-to-image generation quality, with evaluation scores ranging from 1-5.

## Key Results
- Achieves state-of-the-art performance among open-source multimodal evaluation models of similar scale on average evaluation performance across all tasks
- Outperforms all open-source and closed-source models on text-to-image generation task evaluation
- Joint training on I2T and T2I evaluation tasks enables bidirectional transfer, improving performance on both task types

## Why This Works (Mechanism)

### Mechanism 1: Data Quality Filtering
Filtering GPT-generated evaluations against human scores and balancing score distributions yields higher-quality training data than raw volume. Human Selection discards GPT annotations that diverge from human judgment, while Score Balance reweights the dataset to prevent score-5 dominance. This improves T2I evaluation from 52.4 to 56.8 Pearson-r on RichHF.

### Mechanism 2: Bidirectional Transfer
Joint training on image-to-text and text-to-image evaluation tasks produces bidirectional transfer, improving performance on both. Evaluation reasoning skills generalize across directional modalities, with I2T data providing richer supervision that regularizes T2I evaluation. Training on I2T-only still achieves non-trivial T2I performance (50.4 Pearson-r).

### Mechanism 3: Mix-SFT for Comparison Learning
Mixing pairwise comparison data into SFT before DPO alignment enables more effective use of preference signals. Evaluation preference pairs are themselves evaluation tasks, and training on them explicitly during SFT teaches the model comparison reasoning before DPO refines preference ranking. Mix-SFT + DPO achieves 37.0 average Pearson-r versus 35.1 for DPO alone.

## Foundational Learning

**Concept: LLM-as-a-Judge paradigm**
- Why needed: Minos extends this from text-only to multimodal evaluation; understanding the baseline helps grasp why prompt-based evaluation fails and supervised training is needed
- Quick check: Can you explain why prompting GPT-4o to evaluate differs from training a 7B model on evaluation data?

**Concept: Direct Preference Optimization (DPO)**
- Why needed: Minos uses DPO for alignment; understanding how DPO converts preference pairs into implicit reward learning clarifies why it helps after Mix-SFT
- Quick check: How does DPO differ from RLHF in terms of what it optimizes and what data it requires?

**Concept: Pearson correlation for evaluation metrics**
- Why needed: All reported results use Pearson-r to measure alignment with human scores; interpreting these numbers correctly is essential for assessing model quality
- Quick check: Why is Pearson correlation preferred over accuracy for evaluation score agreement?

## Architecture Onboarding

**Component map:** LLaVA-OneVision-7B (vision encoder frozen, multimodal adapter + LLM trained) -> Data Selection/Balance -> Mix-SFT (evaluation + comparison data) -> DPO alignment

**Critical path:** Human Selection filtering -> Score Balance -> Mix-SFT (2 epochs) -> DPO (1 epoch). Skipping any stage degrades performance.

**Design tradeoffs:** 124k raw samples vs. 57k filtered (volume vs. quality - filtered wins). Training on I2T-only vs. joint (joint improves T2I but requires more data). Mix-SFT before DPO vs. DPO alone (Mix-SFT provides +1.0 average gain).

**Failure signatures:** Model defaults to score-5 predictions (Score Balance missing). T2I evaluation underperforms GPT-4o (missing human-annotated T2I data or I2T joint training). DPO provides no gain (preference pairs lack score diversity).

**First 3 experiments:** 1) Replicate Table 4 ablation: train on full 124k vs. filtered 57k to validate Selection+Balance impact. 2) Ablate task direction: train I2T-only, T2I-only, and joint to confirm bidirectional transfer. 3) Isolate Mix-SFT contribution: train with/without comparison data mixed into SFT, then apply DPO to both.

## Open Questions the Paper Calls Out

**Open Question 1:** What specific mechanisms enable the positive transfer where training on Image-to-Text (I2T) evaluation data improves Text-to-Image (T2I) evaluation capability? The authors observe this empirically but don't investigate the underlying semantic or reasoning features.

**Open Question 2:** Why does Mix-SFT training cause temporary performance degradation on T2I tasks, and why does DPO specifically rectify this? The paper reports the degradation but doesn't analyze whether it stems from data imbalance, gradient conflicts, or catastrophic forgetting.

**Open Question 3:** To what extent does Minos inherit biases or failure modes of GPT-4o, given that 76k training samples rely on GPT-generated annotations? The paper acknowledges potential bias but doesn't quantify how much the model's judgment correlates with GPT-4o's specific quirks.

## Limitations

- Reliance on human-annotated evaluation data without full disclosure of evaluator demographics and expertise diversity, risking systematic bias
- GPT-4o filtering mechanism assumes disagreement with human scores indicates lower quality, potentially discarding valid alternative perspectives
- Limited evaluation of generalization beyond the 16 trained tasks to novel multimodal evaluation scenarios

## Confidence

**Claim 1 (Data Selection+Balance effectiveness): Medium-High** - Strong empirical support (35.1â†’37.0 Pearson-r improvement), but mechanism relies on assumptions about GPT-4o alignment quality that aren't fully validated.

**Claim 2 (Bidirectional transfer): Medium-High** - Clear evidence that joint training improves T2I evaluation, but theoretical basis for why evaluation skills transfer across modalities could be better articulated.

**Claim 3 (Mix-SFT + DPO superiority): Medium-High** - Consistent performance gains across ablations, but pairwise comparison data construction method is a heuristic that may not always reflect meaningful quality distinctions.

## Next Checks

1. **Cross-dataset Generalization Test:** Evaluate Minos on a held-out multimodal evaluation dataset from a different domain (e.g., medical image captioning) to assess whether bidirectional training provides true generalization or merely overfitting to the 16-task distribution.

2. **Human Filtering Ablation with Qualitative Analysis:** Conduct an ablation training Minos on: (a) all GPT-4o data without filtering, (b) filtered data with documented rejection rates, and (c) filtered data plus qualitative review of borderline cases. Analyze whether discarded examples reveal systematic failures in filtering logic.

3. **Scale-Up Transfer Experiment:** Train a larger variant of Minos (e.g., 13B or 30B parameters) on the same Minos-Corpus to determine whether the Data Selection+Balance + Mix-SFT + DPO recipe scales effectively, or whether different architectural choices become necessary at larger scales.