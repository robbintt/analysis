---
ver: rpa2
title: In-Context Iterative Policy Improvement for Dynamic Manipulation
arxiv_id: '2508.15021'
source_url: https://arxiv.org/abs/2508.15021
tags:
- policy
- learning
- task
- in-context
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes In-Context Policy Improvement (ICPI), a method
  that uses pre-trained Large Language Models (LLMs) to iteratively improve policies
  for dynamic manipulation tasks through in-context learning. The key idea is to formulate
  policy improvement as a sequence-to-sequence completion task where the LLM predicts
  adjustments to policy parameters based on previous interaction data provided in
  the prompt.
---

# In-Context Iterative Policy Improvement for Dynamic Manipulation

## Quick Facts
- arXiv ID: 2508.15021
- Source URL: https://arxiv.org/abs/2508.15021
- Reference count: 40
- Primary result: LLM-based in-context policy improvement outperforms random shooting, Bayesian optimization, and KNN baselines in low-data regime for dynamic manipulation tasks

## Executive Summary
This paper introduces In-Context Policy Improvement (ICPI), a method that uses pre-trained Large Language Models to iteratively improve policies for dynamic manipulation tasks without fine-tuning. By treating policy improvement as a sequence-to-sequence completion task, ICPI predicts adjustments to policy parameters based on previous interaction data provided in the prompt. The approach demonstrates strong performance across five tasks (simulated slide, rope swing, and real robot ball rolling) while requiring no gradient updates or training.

## Method Summary
ICPI formulates policy improvement as an in-context learning problem where an LLM predicts parameter adjustments (Δθ) from current policy parameters and error feedback. The method tokenizes policy parameters and relative error trajectories, retrieves similar examples from a pre-collected dataset using KNN, and constructs prompts for the LLM to predict corrections. This iterative process runs for up to 20 steps per task, with performance measured by final task cost. The approach requires no fine-tuning and leverages the LLM's pattern-matching capabilities to generalize improvement patterns from few examples.

## Key Results
- ICPI achieves mean final policy costs of 0.013, 0.025, 0.007, 0.002, and 17.107 for the five tested tasks
- Consistently outperforms baselines including random shooting, Bayesian optimization, and KNN approaches
- Demonstrates effectiveness in low-data regime (≤ 300 examples per task)
- Real robot experiment (roll-gc-real) shows successful transfer to physical system

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained LLMs can perform in-context policy improvement by treating it as a sequence-to-sequence pattern completion task.
- **Mechanism:** The LLM receives tokenized policy parameters and error feedback from previous rollouts, along with k similar examples from a policy improvement dataset. It then predicts a delta adjustment (Δθ) that drives the policy toward lower task cost. This leverages the observation that LLMs act as "general pattern machines" capable of identifying input-output relationships in new domains without weight updates.
- **Core assumption:** The policy improvement mapping from (θᵢ, eᵢ) → Δθᵢ exhibits regularity that can be captured via pattern recognition from few examples; LLMs trained on language data transfer this pattern-matching ability to numeric policy adjustment patterns.
- **Evidence anchors:** Abstract statement about few-shot learning for general patterns; Section 4.1 on sequence-to-sequence completion; Section 2.1 on LLMs as general pattern machines; corpus citation of Mirchandani et al. [6] on sequence improvement via in-context learning.

### Mechanism 2
- **Claim:** Iterative delta predictions conditioned on relative error enable sample-efficient improvement without explicit dynamics modeling.
- **Mechanism:** Rather than predicting absolute policy parameters θ directly, the method predicts adjustments Δθ based on the relative error e = sₜ - τg. This focuses the learning signal on corrective patterns that may generalize across different task parameters. The iterative refinement over ~20 steps allows cumulative correction toward the goal.
- **Core assumption:** Similar relative errors require similar corrective adjustments across different physical task instances; the mapping from error to correction is smoother and more learnable than the full dynamics-to-policy mapping.
- **Evidence anchors:** Section 4.2 on relative error consistency across task parameters; Section 5.2 ablation showing eᵢ outperforms raw (sᵢₜ, τg); corpus citation of SAS-Prompt (arXiv:2504.20459) on delta-based refinement.

### Mechanism 3
- **Claim:** KNN-based example selection focuses in-context learning on relevant regions of the policy-error space.
- **Mechanism:** A k-d tree is constructed over normalized [θ, e] vectors from the improvement dataset. For each query, the k=20 nearest neighbors are retrieved and provided as in-context examples in order of decreasing distance. This localizes the pattern recognition to similar policy-error configurations, improving prediction relevance.
- **Core assumption:** The policy improvement operator is locally smooth—nearby (θ, e) points have similar Δθ solutions; providing distant examples adds noise rather than signal.
- **Evidence anchors:** Section 4.3 on KNN retrieval for focusing examples; Table 1 showing KNN-5 baseline underperforms ICPI; corpus evidence lacking direct KNN selection for in-context policy learning.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** ICPI relies entirely on ICL—no fine-tuning occurs. Understanding that ICL enables few-shot generalization via prompt examples alone is essential to grasping why this method works without gradient updates.
  - **Quick check question:** Given a prompt with input-output pairs [(2, 4), (3, 6), (5, ?)], would you expect an ICL-capable model to predict 10? What property of the model enables this?

- **Concept: Parametric Policies for Dynamic Manipulation**
  - **Why needed here:** The method operates on low-dimensional parametric policies (e.g., θ = (θα, θd, θt) for sliding), not high-DoF trajectory representations. Understanding this parameterization is critical for tokenization and interpreting Δθ outputs.
  - **Quick check question:** For a puck-sliding task, what might the three policy parameters (θα, θd, θt) physically represent? How does this differ from a 100-dimensional joint trajectory?

- **Concept: Algorithm Distillation**
  - **Why needed here:** The improvement dataset D is constructed by running another algorithm (brute-force search, hindsight relabeling) and recording (θ, trajectory, Δθ*) triples. ICPI distills this algorithmic improvement process into a sequence-to-sequence mapping.
  - **Quick check question:** If you had an optimal solver for a task, how could you generate training pairs (θᵢ, s₁:T, Δθᵢ) to teach a weaker model to improve policies?

## Architecture Onboarding

- **Component map:**
  1. Task Environment -> 2. Cost Function Cτ(s₁:T) -> 3. Tokenizer (θ → numeric string, e → numeric string) -> 4. KNN Retriever (k-d tree over [θ, e]) -> 5. LLM (gpt-4o) -> 6. Policy Updater (θᵢ₊₁ = θᵢ + Δθᵢ) -> 7. Dataset D

- **Critical path:**
  1. Collect ~300 improvement examples for your task (via brute-force or hindsight labeling)
  2. Build k-d tree over [θ, e] from dataset
  3. Initialize policy θ₀ (random or default)
  4. Execute πθᵢ, observe trajectory, compute eᵢ
  5. Retrieve k nearest examples, construct prompt, query LLM for Δθᵢ
  6. Update θᵢ₊₁ = θᵢ + Δθᵢ; repeat for ≤20 iterations

- **Design tradeoffs:**
  - Dataset size vs. coverage: ~300 examples suffice for tested tasks, but complex tasks may require more. Diminishing returns possible beyond a certain size.
  - Tokenization choice: Relative error eᵢ outperforms raw (sᵢ, τg) despite equivalent information—requires task-aware feature engineering.
  - LLM selection: gpt-4o outperforms gpt-3.5-turbo and gpt-4o-mini; larger models may be necessary for non-trivial pattern complexity.
  - k value: k=20 chosen empirically; smaller k may under-constrain, larger k may include irrelevant examples and exceed context limits.

- **Failure signatures:**
  - No improvement over iterations: Check dataset quality—Δθ labels may be noisy or uncorrelated with error.
  - LLM outputs invalid values: Ensure tokenization format matches examples exactly; validate output parsing.
  - Performance plateaus early: KNN retrieval may be stuck in local region; consider diversifying dataset or increasing exploration.
  - Catastrophic policy divergence: Add clipping to θ bounds after each update; reduce Δθ scale.

- **First 3 experiments:**
  1. Validate ICL pattern completion: Create a synthetic 1D policy improvement task with known linear Δθ = f(θ, e). Test if LLM recovers the pattern from 20 examples.
  2. Ablate tokenization: Compare relative error eᵢ vs. raw state-goal (sᵢ, τg) on a single simulated task (e.g., slide). Replicate the ablation in Table 2.
  3. Compare to Linear KNN baseline: Implement the piecewise linear KNN-20 baseline and run alongside ICPI on slide-gc. Verify that ICPI outperforms linear interpolation, confirming the LLM adds non-trivial pattern generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ICPI scale to higher-dimensional policy representations beyond the 3-dimensional parametric policies tested, without degrading sample efficiency or convergence?
- Basis in paper: The authors state: "we only investigated relatively low-dimensional input-output formats; while low-dimensional parametric actions are common in dynamic manipulation, they can limit the dexterity of the system... we leave to future work an extensive study of task dimensionality and feature extraction for in-context learning."
- Why unresolved: All five tasks used 3D policy parameterizations; it remains unknown whether LLM context windows and pattern-matching capabilities degrade as policy dimensionality increases.
- What evidence would resolve it: Systematic experiments scaling policy dimensions (e.g., 5, 10, 20 parameters) on equivalent tasks, reporting sample efficiency and final performance.

### Open Question 2
- Question: Can in-context policy improvement eliminate the need for manual feature engineering, or will hand-designed encodings remain necessary for competitive performance?
- Bias in paper: The ablation shows providing raw state and goal $(s_t, \tau_g)$ underperforms the hand-computed relative error $e_i$, and the authors note: "in-context learning may not yet be capable of the feature learning ubiquitous in modern machine learning."
- Why unresolved: The mechanism by which LLMs generalize from context remains poorly understood; whether larger or architecturally different models could perform implicit feature learning is unknown.
- What evidence would resolve it: Comparative experiments using larger models or alternative tokenization schemes (e.g., learned encoders) on the same tasks, without human-designed error features.

### Open Question 3
- Question: Can policy improvement datasets be constructed autonomously via self-play or demonstrations, removing the reliance on brute-force search or hindsight labeling?
- Basis in paper: The limitations section states: "Self-play [31] and demonstrations [27] could be utilized to ease the data collection burden," but neither method was implemented or evaluated.
- Why unresolved: Current dataset construction depends on task-specific strategies (brute-force for simulation, hindsight labeling for real); autonomous dataset generation could broaden applicability.
- What evidence would resolve it: ICPI experiments where datasets are collected via autonomous exploration (e.g., curiosity-driven or goal-conditioned self-play) with performance compared to oracle-collected datasets.

### Open Question 4
- Question: Will advances in LLM reasoning capabilities directly translate to improved ICPI performance, or are there fundamental limits to pattern-based policy improvement?
- Basis in paper: The authors observe: "while signs indicate that in-context learning may continue to improve with improved LLMs... it is unclear if more advanced pattern-based reasoning will be achievable purely in-context."
- Why unresolved: The experiments show gpt-4o outperforms smaller models, but whether scaling alone will close the gap with specialized optimization methods (e.g., RL or differentiable physics) remains speculative.
- What evidence would resolve it: Longitudinal experiments evaluating ICPI across successive model generations on fixed benchmark tasks, analyzing whether performance gains correlate with independent reasoning benchmarks.

## Limitations

- Dataset generation scalability: Method relies on pre-collected improvement datasets (~300 examples per task), which may not scale to more complex manipulation tasks requiring higher-dimensional policies or finer-grained corrections.
- Generalization across task types: All tested tasks share similar structure (parametric policies with 3 parameters, goal-conditioned objectives). Effectiveness for high-DoF trajectory policies or non-goal-conditioned tasks remains unverified.
- LLM dependency and brittleness: Performance is highly sensitive to LLM choice and prompt format, creating a brittle dependency on model capabilities that may shift with newer LLM versions.

## Confidence

- High confidence: ICPI's core mechanism of treating policy improvement as in-context sequence completion is well-supported by experimental results and ablation studies.
- Medium confidence: Claim that ICPI generalizes across different task parameters is supported but not thoroughly validated; relative error encoding's superiority may not hold for all task types.
- Low confidence: Scalability claims to more complex tasks are speculative; paper does not test higher-dimensional policies, and dataset generation may not scale efficiently.

## Next Checks

1. Ablate dataset size systematically: Run ICPI on slide-gc with D = 50, 100, 150, 200, 250, 300 examples. Plot final policy cost vs. dataset size to identify diminishing returns point.

2. Test on high-dimensional policies: Implement slide variant with 6 policy parameters (adding initial velocity and control frequency). Generate dataset and compare ICPI performance to 3-parameter baseline to test scalability claims.

3. Cross-task generalization test: Train ICPI on rope-swing but test zero-shot on rope-swing-gc (different goal condition). Measure performance drop to assess whether learned improvement patterns transfer across task variants.