---
ver: rpa2
title: 'Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking
  and Prospects'
arxiv_id: '2511.21533'
source_url: https://arxiv.org/abs/2511.21533
tags:
- sign
- dataset
- language
- gloss
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents IsharaKhobor, a new dataset for Bangla Sign
  Language Translation (BdSLT) developed from TV news broadcasts. The dataset contains
  5,642 annotated sentences across 11.3 hours of video, addressing the severe lack
  of resources for BdSLT.
---

# Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects

## Quick Facts
- arXiv ID: 2511.21533
- Source URL: https://arxiv.org/abs/2511.21533
- Reference count: 0
- Primary result: Created IsharaKhobor dataset (5,642 samples, 11.3 hours) and achieved BLEU-4 scores from 3.88 (full) to 26.81 (restricted vocab)

## Executive Summary
This paper addresses the severe lack of resources for Bangla Sign Language Translation (BdSLT) by creating IsharaKhobor, a dataset of 5,642 annotated sign language sentences extracted from Bangladeshi TV news broadcasts. The authors tackle three major challenges: automatic transcript generation from news captions, manual sign sentence annotation by experts, and vocabulary standardization. To enable meaningful benchmarking, they derive two smaller datasets with restricted vocabularies (IsharaKhobor_SMALL and IsharaKhobor_CANONICAL_SMALL). Using landmark-based embeddings with SLTT and GASLT transformers, they demonstrate that vocabulary restriction dramatically improves translation performance from near-random (BLEU-4: 3.88) to meaningful (BLEU-4: 26.81).

## Method Summary
The authors collected sign language videos from Bangladeshi TV news, extracting 5,642 sentences with manual annotation by sign experts. They extracted Mediapipe landmarks (225 points from pose and hands) and tested two embedding approaches: Raw (224-dim) and RQE (Relative and Quantized Embedding). Two transformer architectures were benchmarked: SLTT (4-6 layers, regression-based) and GASLT (3 layers, gloss attention with sentence-level cosine similarity). The full dataset was reduced to smaller subsets by restricting vocabulary to words appearing in all train/test/val splits, and canonicalization was applied to standardize spelling variants. Models were trained with 7:2:1 splits using Adam optimizer with learning rates of 0.0001 and patience of 15-80 epochs.

## Key Results
- IsharaKhobor dataset: 5,642 samples, 11.3 hours of video, 11,359 vocabulary
- RQE-GASLT achieved best performance: 3.88 BLEU-4 (full dataset), 26.81 BLEU-4 (SMALL), 25.74 BLEU-4 (CANONICAL_SMALL)
- Vocabulary restriction improved BLEU-4 from 3.88 to 26.81 by eliminating OOV tokens
- Canonicalization helped reduce vocabulary variation but controlled variation improved convergence more than full standardization

## Why This Works (Mechanism)

### Mechanism 1: RQE Normalization
RQE provides signer-invariant normalized representations by normalizing Mediapipe landmarks, reducing signer-specific variation. GASLT leverages these embeddings with predicted gloss duration and language similarity heuristics for more efficient attention than SLTT's regression-only approach. Core assumption: Signer identity is noise that should be removed. Break condition: If signer style carries semantic meaning, normalization may discard signal.

### Mechanism 2: Vocabulary Restriction
Iterative filtering retains only samples where all vocabulary appears in train, test, and val sets, reducing vocabulary from 11,359 to 313 (SMALL). This transforms an impossible learning problem into tractable one by eliminating OOV tokens that models cannot learn. Core assumption: OOV words dominate error. Break condition: If real-world deployment requires open vocabulary, restricted training creates distribution shift.

### Mechanism 3: Canonicalization vs. Controlled Variation
Standardizing spelling variants and fixed phrases reduces vocabulary, but CANONICAL_SMALL (BLEU-4: 25.74) underperformed SMALL (26.81), suggesting mild augmentation from natural variation acts as regularization. Core assumption: Gradient descent benefits from controlled linguistic diversity. Break condition: If variation is mostly noise rather than meaningful style, canonicalization should strictly help.

## Foundational Learning

- **BLEU-4 and ROUGE metrics**: Why needed: Interpreting benchmark results requires understanding BLEU-4 measures 4-gram overlap (0-100 scale) while ROUGE measures recall of reference sequences. Quick check: If a model outputs completely wrong word order but correct vocabulary, would BLEU-4 be high or low?

- **Gloss-free vs. gloss-annotated SLT**: Why needed: The paper explicitly avoids gloss annotation due to expert cost. Glosses are intermediate sign-level labels (like "HOUSE-BUILD" for "constructing a house") that explain why gloss-free models struggle with alignment. Quick check: Why might a gloss-free transformer need more training data than a gloss-supervised one?

- **Out-of-Vocabulary (OOV) tokens**: Why needed: The vocabulary-to-sample ratio (11,359 words / 5,642 samples) means most words appear <1 time on average. OOV tokens in test sets cannot be predicted, capping performance. Quick check: If you double the dataset size but also double vocabulary, would OOV rate improve?

## Architecture Onboarding

- **Component map**: Video -> Mediapipe landmark extraction -> 224-dim Raw embedding or RQE -> GASLT/SLTT encoder -> Transformer decoder -> Bangla text sequence
- **Critical path**: Landmark extraction fails if video width <50 pixels; RQE normalization requires consistent signer cropping; vocabulary overlap between splits determines tractability
- **Design tradeoffs**: Raw vs. RQE embedding (preserves signal vs. normalizes signer noise); Full dataset vs. SMALL (coverage vs. performance); Canonicalization (reduces vocab vs. removes regularization)
- **Failure signatures**: BLEU-4 <5 with RQE-GASLT on restricted vocab → check embedding normalization; Large gap between train and val BLEU → overfitting; Random output on full dataset → expected due to vocabulary coverage
- **First 3 experiments**: 1) Replicate RQE-GASLT on IsharaKhobor_SMALL with reported hyperparameters to verify 26.81 BLEU-4; 2) Ablate RQE vs. Raw embedding on CANONICAL_SMALL to isolate normalization effect; 3) Test vocabulary restriction ratio with intermediate subsets to find minimum viable dataset size

## Open Questions the Paper Calls Out

### Open Question 1: Gloss Annotation Integration
Can gloss annotations or HamNoSys notations bridge the performance gap between low-resource IsharaKhobor and high-resource SLT benchmarks? The authors identify gloss-video alignment as the "missing link" but rely on external efforts without integration. Evidence needed: Gloss-annotated IsharaKhobor showing significant BLEU-4 improvement over current 3.88 baseline.

### Open Question 2: Synthetic Video Generation
Can synthetic video generation (e.g., avatar-based animation) mitigate data scarcity and vocabulary explosion in "in-the-wild" datasets? Authors list this as necessary "way forward" but note generating synthetic video without existing gloss annotations is "challenging." Evidence needed: Study augmenting IsharaKhobor with synthetic samples demonstrating higher convergence rates and BLEU scores.

### Open Question 3: Canonicalization vs. Controlled Variation
Does strict canonicalization harm model generalization compared to controlled vocabulary variation? Authors observed SMALL (26.81 BLEU-4) outperformed CANONICAL_SMALL (25.74), suggesting some linguistic variation acts as beneficial augmentation. Evidence needed: Controlled ablation study varying degree of canonicalization to identify optimal balance between vocabulary restriction and language diversity.

### Open Question 4: Automated Temporal Alignment
Can temporal alignment of sign video and audio transcripts be automated without manual expert annotation? Authors note previous attempts using voice pauses failed because signers "mostly start late and end late," forcing manual annotation. Evidence needed: Algorithm detecting signer activity boundaries with accuracy comparable to manual "sign expert" annotations.

## Limitations

- RQE algorithm details are missing from the paper, making faithful reproduction of the best-performing model impossible
- Vocabulary restriction approach creates datasets that may not generalize to real-world open-vocabulary BdSLT deployment
- Full dataset BLEU-4 score of 3.88 reflects fundamental challenge of translating with 11,359-word vocabulary across only 5,642 samples

## Confidence

- **High Confidence**: Vocabulary restriction effects (BLEU-4 improvement from 3.88 to 26.81 is clearly documented and explained by OOV elimination)
- **Medium Confidence**: RQE normalization benefits (best performance reported, but algorithm details missing; results may be sensitive to implementation specifics)
- **Medium Confidence**: Canonicalization tradeoffs (clear numerical comparison, but lack of detailed rules makes mechanism understanding incomplete)

## Next Checks

1. **RQE Algorithm Verification**: Obtain and implement the exact RQE embedding algorithm from reference [19], then reproduce the RQE-GASLT results on IsharaKhobor_SMALL to verify the 26.81 BLEU-4 score.

2. **Vocabulary Coverage Analysis**: Create intermediate datasets with varying vocabulary sizes (e.g., 1,000, 2,000, 5,000 words) and measure BLEU-4 scores to determine the minimum vocabulary coverage needed for meaningful translation performance.

3. **Real-world Deployment Simulation**: Test model performance on a held-out set of samples containing OOV words that appear in the full training set but were excluded from SMALL datasets, measuring degradation to assess generalization capability.