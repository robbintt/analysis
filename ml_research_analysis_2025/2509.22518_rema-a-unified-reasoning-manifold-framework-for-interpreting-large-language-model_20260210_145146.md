---
ver: rpa2
title: 'REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language
  Model'
arxiv_id: '2509.22518'
source_url: https://arxiv.org/abs/2509.22518
tags:
- reasoning
- correct
- error
- uni00000013
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REMA, a geometric interpretability framework
  that analyzes LLM reasoning failures by quantifying deviations of error representations
  from a learned "Reasoning Manifold" formed by correct reasoning samples. The method
  computes k-nearest neighbor distances of error states to the correct manifold and
  tracks these deviations across model layers to localize the divergence point where
  reasoning begins to fail.
---

# REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model

## Quick Facts
- arXiv ID: 2509.22518
- Source URL: https://arxiv.org/abs/2509.22518
- Reference count: 40
- Unified geometric framework for diagnosing LLM reasoning failures by quantifying deviations from learned "reasoning manifolds"

## Executive Summary
This paper introduces REMA, a geometric interpretability framework that analyzes LLM reasoning failures by quantifying deviations of error representations from a learned "Reasoning Manifold" formed by correct reasoning samples. The method computes k-nearest neighbor distances of error states to the correct manifold and tracks these deviations across model layers to localize the divergence point where reasoning begins to fail. Experiments across multiple LLMs and tasks demonstrate that both correct and erroneous reasoning states exhibit low-dimensional structures, error representations consistently deviate significantly from the correct manifold, and these deviations can be traced to specific model layers, revealing task-dependent failure patterns.

## Method Summary
REMA extracts hidden states from the last-token position at each decoder layer during autoregressive generation, then mean-pools across generation steps per sample. It partitions samples into correct and error sets using exact match, builds a "Reasoning Manifold" from correct samples, and computes k-NN distances between error states and this manifold. The framework tracks these deviations across layers to identify where reasoning diverges, using statistical tests (Welch t-test) to establish significance and intrinsic dimension/Mutual Information calculations to characterize manifold properties. No training is required—it's a post-hoc analysis framework.

## Key Results
- Both correct and erroneous reasoning states exhibit low-dimensional structures across all tested models and tasks
- Error representations consistently deviate significantly from the correct manifold (statistically significant t-tests)
- These deviations can be traced to specific model layers, revealing task-dependent failure patterns
- The framework provides unified, geometry-based approach to diagnosing reasoning failures without requiring task-specific probes

## Why This Works (Mechanism)
The framework leverages the observation that reasoning processes, when successful, follow predictable geometric patterns in representation space. By treating correct reasoning trajectories as forming a "Reasoning Manifold," REMA can quantify how far erroneous reasoning deviates from this learned structure. The layer-wise tracking reveals that reasoning failures often occur at specific points in the model's computation, providing actionable insights into where and how models break down.

## Foundational Learning
- **Reasoning Manifold Concept**: A low-dimensional geometric structure formed by correct reasoning representations. Needed because it provides a reference frame for measuring reasoning quality. Quick check: Verify manifold ID estimates are consistently low across different tasks.
- **k-NN Distance Metrics**: Measures proximity between error states and the correct manifold. Needed to quantify deviation magnitude in a geometry-aware way. Quick check: Test sensitivity to k parameter across different dataset sizes.
- **Layer-wise State Analysis**: Examining representations at each decoder layer during generation. Needed to localize where reasoning diverges. Quick check: Verify divergence points are consistent across multiple runs of the same task.
- **Statistical Significance Testing**: Welch t-tests to determine if deviation distances are meaningful. Needed to distinguish real reasoning failures from noise. Quick check: Confirm p-values remain significant across bootstrap samples.
- **Intrinsic Dimension Estimation**: Using TwoNN method to characterize manifold complexity. Needed to validate low-dimensional assumption. Quick check: Compare ID estimates across correct vs error manifolds.
- **Mutual Information Estimation**: Using KSG estimator to measure information content. Needed to understand representation efficiency. Quick check: Verify MI values are consistent with theoretical expectations.

## Architecture Onboarding
- **Component Map**: Input Tasks -> Model Inference (hidden state extraction) -> Sample Partitioning (correct/error) -> Manifold Learning (Z_correct) -> Deviation Analysis (k-NN distances) -> Statistical Testing -> Divergence Localization -> Results Visualization
- **Critical Path**: The core analysis pipeline is: extract hidden states → partition samples → compute k-NN distances → apply t-tests → localize divergence layers. Each step must complete successfully for the framework to work.
- **Design Tradeoffs**: Uses exact match partitioning for simplicity but sacrifices nuance; relies on mean-pooling which may lose temporal information; chooses k=5 as default without systematic validation; uses α=2 threshold based on empirical effectiveness rather than theoretical justification.
- **Failure Signatures**: Sparse correct samples → unstable manifold approximation; binary exact match → misclassification of near-correct reasoning; inappropriate k values → unreliable distance metrics; model-specific layer patterns → difficulty in establishing universal thresholds.
- **3 First Experiments**: 1) Run inference on GSM8K with Llama3.2-11B and extract all hidden states; 2) Partition samples and compute internal vs external k-NN distances; 3) Apply Welch t-test and visualize deviation patterns across layers.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can active interventions that "pull back" deviating representations onto the correct reasoning manifold successfully correct reasoning errors in real-time? The paper only diagnoses failures by detecting deviations; it does not implement or test any correction mechanisms. Experiments demonstrating that steering representations toward the manifold during inference improves accuracy would resolve this.
- **Open Question 2**: Can novel regularization terms during training encourage models to learn reasoning manifolds with more desirable geometric properties (e.g., tighter clustering, higher separability)? REMA is purely a post-hoc analysis framework; no training interventions were explored. Training models with manifold-aware losses and showing improved downstream reasoning performance would resolve this.
- **Open Question 3**: What are the fundamental mechanisms driving the formation of reasoning manifolds—task structure, model architecture, or training data inductive biases? The paper empirically demonstrates manifold existence but does not explain why they form. Controlled ablation studies varying task types, architectures, and training corpora while measuring manifold properties would resolve this.

## Limitations
- The framework assumes correct reasoning states lie on a low-dimensional manifold that can be learned from correct samples, but robustness under varying sample sizes and noise levels is not fully characterized.
- Exact match partitioning creates binary distinction that may not capture continuous nature of reasoning quality, potentially introducing noise into manifold learning.
- The choice of α=2 threshold for divergence localization is presented as effective but not rigorously justified across all tasks and model architectures.

## Confidence
- **High confidence**: The geometric methodology for quantifying deviations (k-NN distance calculations, t-tests, separability analysis) is technically sound and well-specified.
- **Medium confidence**: The layer-wise localization of reasoning divergence is valuable but generalizability of the α=2 threshold needs further validation.
- **Low confidence**: The framework's ability to handle reasoning failures that manifest as subtle, gradual deviations rather than sharp transitions between layers.

## Next Checks
1. **Robustness to sample size variation**: Systematically evaluate how the accuracy of manifold approximation and deviation detection scales with the number of correct reasoning samples. Test with progressively smaller D_correct sets to identify minimum viable sample sizes.
2. **Threshold sensitivity analysis**: Conduct experiments varying the α threshold (1.5 to 3.0) for divergence localization and evaluate how this affects consistency and accuracy of identified failure points across different tasks and model scales.
3. **Alternative partitioning schemes**: Compare exact match partitioning with alternative approaches (e.g., soft scoring, human evaluation) on tasks where partial credit is meaningful to assess impact of partitioning choices on manifold learning quality and deviation detection.