---
ver: rpa2
title: Controlling False Positives in Image Segmentation via Conformal Prediction
arxiv_id: '2511.15406'
source_url: https://arxiv.org/abs/2511.15406
tags:
- segmentation
- inner
- prediction
- positives
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a post-hoc conformal prediction framework for
  binary medical image segmentation that controls the false-positive rate with finite-sample
  guarantees. Given any pretrained segmentation model, the method constructs confidence
  masks by shrinking predictions via either sigmoid-score thresholding or morphological
  erosion.
---

# Controlling False Positives in Image Segmentation via Conformal Prediction

## Quick Facts
- arXiv ID: 2511.15406
- Source URL: https://arxiv.org/abs/2511.15406
- Reference count: 21
- Key outcome: Presents a post-hoc conformal prediction framework for binary medical image segmentation that controls the false-positive rate with finite-sample guarantees.

## Executive Summary
This paper introduces a model-agnostic post-hoc conformal prediction method for binary medical image segmentation that controls the false-positive rate with finite-sample guarantees. Given any pretrained segmentation model, the approach constructs confidence masks by shrinking predictions via sigmoid-score thresholding or morphological erosion, using a small labeled calibration set to select a shrinkage parameter that keeps the accepted false-positive proportion below a user-specified tolerance. The method achieves empirical validity close to the nominal target (e.g., EV ≈ 0.93 at 1−α = 0.9, τ = 0.1) while trading off utility through contraction ratio and retained true positives.

## Method Summary
The method defines nested "inner sets" I_λ(X) ⊆ Ŷ by applying shrinkage operators parameterized by λ to the predicted mask. Two implementations are proposed: thresholding (raising the sigmoid score threshold to retain only high-confidence pixels) and morphological erosion (peeling away boundary pixels λ times). For each calibration image, the method computes the minimal λ required to push the accepted false-positive proportion F_λ below tolerance τ. These λ values are sorted, and the ⌈(n+1)(1−α)⌉-th value becomes the global threshold λ̂. At test time, predictions are shrunk using this fixed λ̂ to output confidence masks with guaranteed false-positive control.

## Key Results
- Empirical validity (EV) achieved ≈0.93 at target 1−α = 0.9 and tolerance τ = 0.1
- Threshold-based shrinkage retains more utility (higher ATP) than erosion
- Both variants provide statistical guarantees absent in uncalibrated baselines
- Method is model-agnostic and requires no retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defining a nested family of "inner sets" via shrinkage operators allows for the controlled removal of false positive (FP) pixels.
- Mechanism: The method constructs subsets I_λ(X) ⊆ Ŷ where the parameter λ controls the degree of shrinkage. Two implementations are proposed: (1) **Thresholding** (I^σ_λ): Raising the sigmoid score threshold increases λ, retaining only high-confidence pixels. (2) **Erosion** (I^ε_λ): Applying morphological erosion λ times peels away boundary pixels.
- Core assumption: False positives are likely to reside in regions of lower model confidence or at the object boundaries, making them removable via these specific shrinkage operations.
- Evidence anchors:
  - [abstract] "constructs confidence masks by shrinking predictions via either sigmoid-score thresholding or morphological erosion"
  - [section 3] "we apply morphological erosion... to the mask... I^ε_λ"
  - [corpus] Related work "Conformal Prediction for Image Segmentation Using Morphological Prediction Sets" confirms the viability of morphology in this context, though this specific paper focuses on FP control.
- Break condition: If FPs are distributed uniformly or have high confidence scores (e.g., "hallucinations" deep inside the object), shrinkage may remove only True Positives (TPs) without satisfying the error constraint.

### Mechanism 2
- Claim: The "Accepted False-Positive Proportion" (F_λ) is designed to be monotonic non-increasing with respect to the shrinkage parameter λ.
- Mechanism: The metric is defined as F_λ = |I_λ ∩ W| / |Ŷ|, where W is the set of FPs in the original mask. By keeping the denominator fixed as the size of the *original* prediction |Ŷ| (rather than the shrinking |I_λ|), the metric is guaranteed to decrease or stay constant as λ increases and the numerator shrinks.
- Core assumption: Assumption: The user accepts a denominator based on the original mask size, which implies the "risk" is normalized against the initial over-segmentation rather than the final confidence mask size.
- Evidence anchors:
  - [section 3.1] "Dividing by |I_λ| would break this monotonicity and is therefore avoided."
  - [section 3.2] "F_λ is non-increasing in λ"
- Break condition: If the denominator were changed to the current mask size (|I_λ|), monotonicity is not guaranteed, breaking the conformal risk control logic.

### Mechanism 3
- Claim: Conformal calibration converts the heuristic shrinkage into a statistically valid guarantee via quantile selection.
- Mechanism: On a calibration set, the method finds the minimal λ required for each image to push the AFP below tolerance τ. These λ values are sorted, and the ⌈(n+1)(1−α)⌉-th value is selected as the global threshold λ̂. This ensures that on a new image, F_λ̂ ≤ τ with probability 1−α.
- Core assumption: The calibration data and test data are exchangeable (i.i.d.). The validity is marginal (averaged over the dataset), not conditional on a specific image difficulty.
- Evidence anchors:
  - [abstract] "A small labeled calibration set is used to select a shrinkage parameter λ such that the accepted false-positive proportion... stays below a user-specified tolerance τ"
  - [section 3.2] "P(F_λ̂(X_new, Y_new) ≤ τ) ≥ 1−α"
  - [corpus] "COMPASS" paper discusses similar conformal guarantees for segmentation metrics, supporting the general approach.
- Break condition: Distribution shift (non-exchangeability). If the test images are significantly harder or have different FP profiles than calibration, empirical validity may fail to meet the target.

## Foundational Learning

- Concept: **Inductive (Split) Conformal Prediction**
  - Why needed here: This is the statistical engine providing the guarantees. Understanding the "split" aspect (using a separate calibration set) is crucial to implementing the method without retraining the model.
  - Quick check question: How does the size of the calibration set affect the tightness or validity of the guarantee?

- Concept: **Monotonicity in Risk Control**
  - Why needed here: The paper relies on the AFP metric being non-increasing. One must understand why the denominator choice was made to avoid logical errors if adapting the metric.
  - Quick check question: Why does dividing by the current mask size |I_λ| instead of the original |Ŷ| break the monotonicity property?

- Concept: **Morphological Erosion vs. Thresholding**
  - Why needed here: These are the two levers for creating the nested sets. One requires model logits (white-box/threshold), while the other works on binary masks (black-box/erosion).
  - Quick check question: Which method would you choose if you only had access to the binary output of a third-party API?

## Architecture Onboarding

- Component map: Pretrained Model → Calibration Images → Nonconformity Scores → Quantile Selection → Global λ̂ → Test Image Predictions → Confidence Mask
- Critical path: The computation of non-conformity scores (Eq. 4) is the most computationally sensitive step, requiring iteration or search over λ for each calibration image.
- Design tradeoffs:
  - **Thresholding**: Retains more utility (higher ATP) but requires sigmoid/logit access.
  - **Erosion**: Model-agnostic (black-box friendly) but removes more peripheral TPs, resulting in lower utility.
  - **τ (Tolerance)**: Lower τ reduces FPs but shrinks the mask (lower CR).
- Failure signatures:
  - **Empty Masks**: If τ is too strict or the model is poor, λ̂ might result in I_λ̂ = ∅.
  - **Conservative Validity**: If calibration set is small or easy, λ̂ might be overly conservative on hard test images.
- First 3 experiments:
  1. **Validity Check**: Reproduce Table 1 results to verify that Empirical Validity (EV) ≈ 1−α for both Threshold and Erosion methods.
  2. **Utility Plot**: Plot Contraction Ratio (CR) vs. Tolerance (τ) to visualize the utility trade-off.
  3. **Visual Inspection**: Visualize the "Uncertainty Region" U_λ (light grey in Fig 1) to see what pixels are being rejected (FPs vs TPs at the boundary).

## Open Questions the Paper Calls Out

- Question: Can the conformal false-positive control framework be effectively extended to multi-class semantic segmentation tasks?
  - Basis in paper: [explicit] The conclusion explicitly lists "extending the approach to multi-class segmentation" as a direction for future work.
  - Why unresolved: The current method is formulated strictly for binary segmentation, utilizing thresholding and erosion techniques that rely on a single foreground class definition.
  - What evidence would resolve it: A formal extension of the shrinkage and calibration procedure that handles multiple classes simultaneously while maintaining finite-sample guarantees for each class or a combined risk metric.

- Question: How can the shrinkage parameter be made adaptive to object size or specific instances to mitigate utility loss?
  - Basis in paper: [explicit] The authors identify "building size- and instance-adaptive inner masks" as a goal for future research.
  - Why unresolved: The current method selects a single global shrinkage parameter λ, which may be overly conservative for small targets or too lenient for large ones, potentially leading to unnecessary true-positive loss.
  - What evidence would resolve it: A modified calibration algorithm that conditions λ on image features or predicted instance sizes, demonstrating improved True Positive retention without violating the False Positive tolerance τ.

- Question: Is it possible to provide conditional or per-instance guarantees rather than marginal, image-level guarantees?
  - Basis in paper: [inferred] The paper explicitly lists "the mask-level (marginal) nature of the guarantees" as a limitation.
  - Why unresolved: Standard Conformal Prediction provides validity averaged over a distribution of images (marginal), meaning specific difficult images might still exceed the false-positive tolerance τ even if the average holds.
  - What evidence would resolve it: A theoretical or empirical demonstration of a method that bounds the probability of error conditionally on the specific input image or a defined subset of "hard" images.

## Limitations

- The method's validity guarantees depend critically on exchangeability between calibration and test data, and distribution shifts can cause empirical validity to fall below the target.
- The quantile-based calibration inherently assumes a homogeneous difficulty level across the dataset, which may not hold for highly variable medical imaging conditions.
- The monotonic design of the AFP metric and the choice of denominator are crucial to the method's validity, limiting flexibility in metric design.

## Confidence

- **High**: The conformal framework's theoretical validity guarantees under i.i.d. assumptions, and the monotonic design of the AFP metric.
- **Medium**: The empirical results on the polyp dataset, given the moderate sample size (n=250 for calibration) and potential sensitivity to hyperparameter choices like erosion structuring element size.
- **Medium**: The utility trade-offs, as they depend on the specific distribution of false positives in the dataset and may not generalize to other segmentation tasks.

## Next Checks

1. **Robustness to Calibration Set Size**: Systematically vary the calibration set size (e.g., n=100, 250, 500) and measure impact on empirical validity and utility metrics to quantify statistical efficiency.
2. **Cross-Dataset Transfer**: Evaluate the method on a held-out polyp dataset (e.g., CVC-ClinicDB if not used in calibration) to test validity under domain shift.
3. **Visual FP Analysis**: Generate a confusion matrix or histogram showing whether shrunk pixels are predominantly FPs versus TPs near boundaries, to validate the core assumption about FP distribution.