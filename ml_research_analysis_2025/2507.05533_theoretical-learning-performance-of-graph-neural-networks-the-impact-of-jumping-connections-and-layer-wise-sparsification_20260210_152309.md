---
ver: rpa2
title: 'Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping
  Connections and Layer-wise Sparsification'
arxiv_id: '2507.05533'
source_url: https://arxiv.org/abs/2507.05533
tags:
- graph
- learning
- outn
- out1
- sparsification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first theoretical analysis of Graph Convolutional
  Networks (GCNs) with jumping connections using graph sparsification. The authors
  develop a framework to analyze the training dynamics and generalization performance
  of two-hidden-layer GCNs, demonstrating that the learned model achieves label prediction
  performance close to the best possible within a broad class of target functions.
---

# Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification

## Quick Facts
- **arXiv ID**: 2507.05533
- **Source URL**: https://arxiv.org/abs/2507.05533
- **Authors**: Jiawei Sun; Hongkang Li; Meng Wang
- **Reference count**: 40
- **Primary result**: First theoretical analysis of GCNs with jumping connections showing layer-wise differential sparsification requirements - first layer needs more conservative pruning than second layer

## Executive Summary
This paper presents the first theoretical analysis of Graph Convolutional Networks (GCNs) with jumping connections under graph sparsification. The authors develop a framework that characterizes how graph sparsification affects generalization performance, introducing the concept of a sparse effective adjacency matrix A* that preserves essential edges. They prove that two-hidden-layer GCNs with jumping connections can achieve prediction performance close to the best possible within a broad class of target functions. Crucially, their analysis reveals that jumping connections enable different sparsification tolerances across layers - the first layer requires more conservative pruning than the second layer, a finding validated through experiments on both synthetic and real-world datasets.

## Method Summary
The authors analyze two-hidden-layer GCNs with jumping connections where the output is computed as out(X,A;W,U) = Cσ(WXA) + Cσ(Uσ(WXA)A). They employ SGD training with layer-wise graph sparsification, where each layer's adjacency matrix is independently sparsified during training. The first layer retains edges with 99% probability for the largest q1 fraction of edge weights and 1% for others, while the second layer uses parameter q2 similarly. The framework introduces the sparse effective adjacency matrix A* and proves generalization bounds that depend on how closely the sparsified matrices approximate A*. The analysis assumes smooth target functions decomposable as H = F + αG(F) with bounded complexities Cs(F) and Cs(G).

## Key Results
- Jumping connections enable different sparsification tolerances across layers, with first layer requiring more conservative pruning than second layer
- Generalization depends on the sparsified adjacency matrix's proximity to the sparse effective matrix A*, not the original dense matrix
- Two-hidden-layer GCNs with jumping connections can approximate any target function in the hierarchical concept class H_{A*} = F_{A*} + αG_{A*}(F_{A*})
- Layer-wise sparsification in shallow layers has more significant impact on generalization than in deeper layers

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Differential Sparsification via Jumping Connections
The jumping connection directly routes first-layer features to the output, making the first layer responsible for learning a simpler base function F that contributes significantly to predictions. The second layer learns a more complex composite function G(F) with smaller contribution (scaled by α). Since ∥Vt∥₂ is bounded by Θ̃(αCs(G)) < 1, second-layer deviations from A* are attenuated by this coefficient, permitting larger pruning probabilities p²ᵢⱼ > p¹ᵢⱼ. This asymmetry collapses if α approaches 1 or the second layer contributes comparably to the output.

### Mechanism 2: Sparse Effective Adjacency Matrix A* Preserves Generalization
The original adjacency matrix A contains redundant edges. The sparse effective matrix A* retains only edges essential for message propagation to the target function. When the sparsified matrix Aₛ remains close to A* in L1-norm (∥Aₛ - A*∥₁ bounded), the output deviation is controlled proportionally. The theorem bounds prediction error by O(OPT + ε₀), where OPT is the best achievable error using A*. This approach becomes less effective if A* is dense or ∥A*∥₁ grows large, substantially increasing required model complexity M₀ and sample complexity N₀.

### Mechanism 3: Hierarchical Concept Class Learning
The first layer learns F through direct gradient signal from the jumping connection. The second layer receives F as input and learns the residual function G(F). This decomposition matches the target function structure, enabling SGD to find weights W*, V* satisfying ∥H - out(X,A*;W*,V*)∥₂ ≤ ε₀ with sufficient neurons (m ≥ M₀) and iterations (T ≥ T₀). The framework requires target functions to be smooth and decomposable into polynomial/composite structures; more than two levels of composition or non-additive structures may cause underfitting.

## Foundational Learning

- **Graph Convolutional Networks (GCNs) and neighborhood aggregation**
  - Why needed here: The entire analysis builds on the GCN forward pass out(X,A) = Cσ(WXA) + Cσ(Uσ(WXA)A), where A aggregates neighbor features.
  - Quick check question: Can you explain why multiplying features X by adjacency matrix A implements message passing?

- **Over-smoothing and jumping/skip connections**
  - Why needed here: Jumping connections are the architectural feature enabling layer-wise differential sparsification; they bypass the smoothing effect by directly contributing shallow features to output.
  - Quick check question: In a deep GCN without skip connections, why do node representations converge to a common value as depth increases?

- **Graph sparsification and edge pruning strategies**
  - Why needed here: The paper's core contribution is characterizing how pruning affects generalization; understanding degree-aware and weight-aware pruning is essential.
  - Quick check question: Why might retaining high-weight edges (corresponding to low-degree nodes) be preferable to uniform random edge dropping?

## Architecture Onboarding

- **Component map**: Input X → Layer 1 (W weights, sparsified A₁ᵗ) → out₁ → Jumping connection adds out₁ → Layer 2 (V weights, sparsified A₂ᵗ) → out₂ → Final output out = out₁ + out₂

- **Critical path**: 
  1. Initialize W(0), V(0), C with Gaussian distributions (σ_w, σ_v/m scaling)
  2. Each SGD iteration: sample one labeled node n ∈ Ω, generate sparsified A₁ᵗ, A₂ᵗ
  3. Compute forward pass through both layers with jumping connection
  4. Backpropagate gradients to W, V only (C is fixed)
  5. Key constraint: ∥Wₜ∥₂ ≤ τ_w, ∥Vₜ∥₂ ≤ τ_v throughout training

- **Design tradeoffs**:
  - Conservative pruning (small pᵢⱼ) → better generalization, higher compute
  - Aggressive pruning in Layer 2 → acceptable if αCs(G) small
  - More neurons (larger m) → lower ε₀ but higher memory
  - Sample complexity N₀ scales with ∆⁴ and ∥A*∥₁⁴

- **Failure signatures**:
  - Test error spikes when q₁ (first-layer retention fraction) decreases while q₂ unchanged → first layer over-pruned
  - Slower convergence when ∥A*∥₁ is large → model complexity insufficient
  - Training diverges if τ_w, τ_v bounds violated → learning rate too high or initialization variance wrong

- **First 3 experiments**:
  1. **Synthetic validation of layer-wise sensitivity**: Generate synthetic graph with known A*, train with varying q₁ and q₂ independently. Plot test error vs. each parameter while holding the other fixed. Verify steeper degradation curve for q₁ (Figure 3 pattern).
  2. **Real-data pruning threshold sweep**: On OGB-Arxiv with 8-layer JK-GCN, sweep retention fractions for layers 1-4 (shallow) and layers 5-8 (deep) independently. Confirm heatmap shows low error region extends further along q₂ axis than q₁ axis.
  3. **A* approximation sanity check**: Implement the theoretical block-wise pruning strategy (Section 3.2) on a small graph where you can compute A* exactly. Compare generalization against simple uniform pruning to validate that degree-aware retention outperforms random dropping.

## Open Questions the Paper Calls Out

- **Extending to attention-based models**: The authors state that extending the analysis to attention-based models such as Graph Attention Networks (GATs) and Graph Transformers is an "exciting direction," but it would require new tools to characterize dynamic attention mechanisms and global message passing. This remains unresolved because attention mechanisms create data-dependent, dynamic edge weights that change during training, unlike the fixed sparsification strategy analyzed in this paper.

- **Interaction with practical components**: Understanding the interplay of skip connections with practical components such as dropout and normalization layers remains an open problem. These mechanisms introduce stochasticity and feature-rescaling effects that introduce distinct theoretical challenges not covered by current analysis.

- **Generalizing to deeper architectures**: The authors note that formalizing insights for very deep architectures with more than two hidden layers and multiple jumping connections "may require additional tools to handle cumulative sparsification effects and complex nonlinear interactions." The current theory specifically characterizes only two-hidden-layer GCNs with a single jumping connection.

## Limitations

- The theoretical analysis assumes a two-hidden-layer architecture with jumping connections, limiting generalizability to deeper networks where over-smoothing effects may differ.
- The complexity bounds for F and G (Cs(F), Cs(G)) are assumed bounded, but real-world functions may exceed these assumptions, affecting the validity of layer-wise differential sparsification claims.
- The effective adjacency matrix A* is defined theoretically but may be difficult to compute exactly for complex real-world graphs, making practical application challenging.

## Confidence

- **High confidence**: Layer-wise differential sparsification requirements (jumping connections enable different pruning tolerances across layers)
- **Medium confidence**: The sparse effective adjacency matrix A* concept and its role in preserving generalization
- **Medium confidence**: The hierarchical concept class learning framework (F + αG(F) decomposition)

## Next Checks

1. **Cross-layer pruning sensitivity**: Systematically vary q1 and q2 independently on multiple datasets to confirm the asymmetric impact on generalization, ensuring jumping connections are correctly implemented.

2. **A* computation verification**: Develop an approximation method for A* and validate it against exact computation on small graphs, comparing generalization performance to random pruning baselines.

3. **Depth generalization test**: Extend the theoretical framework to 3+ layer GCNs with jumping connections to verify whether the layer-wise differential sparsification principle holds or breaks down.