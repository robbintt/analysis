---
ver: rpa2
title: 'Reflection of Episodes: Learning to Play Game from Expert and Self Experiences'
arxiv_id: '2502.13388'
source_url: https://arxiv.org/abs/2502.13388
tags:
- game
- reflection
- your
- unit
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of using Large Language Models
  (LLMs) for strategic decision-making in complex environments like StarCraft II,
  focusing on the lack of self-reflection and iterative improvement in existing methods.
  To overcome this, the authors propose a Reflection of Episodes (ROE) framework that
  integrates expert and self-experience, utilizing keyframe selection based on game
  phases and reflection generation for strategy iteration.
---

# Reflection of Episodes: Learning to Play Game from Expert and Self Experiences

## Quick Facts
- **arXiv ID:** 2502.13388
- **Source URL:** https://arxiv.org/abs/2502.13388
- **Reference count:** 40
- **Primary result:** Achieved 20% win rate against Very Hard difficulty opponents in TextStarCraft II using self-reflective strategy iteration

## Executive Summary
This paper addresses the challenge of using Large Language Models (LLMs) for strategic decision-making in complex environments like StarCraft II, focusing on the lack of self-reflection and iterative improvement in existing methods. To overcome this, the authors propose a Reflection of Episodes (ROE) framework that integrates expert and self-experience, utilizing keyframe selection based on game phases and reflection generation for strategy iteration. The framework enables the LLM to make decisions, reflect on past experiences, and continuously improve its strategies. Experiments show that the ROE method outperforms the baseline Chain of Summary (COS) approach, achieving a 20% win rate against Very Hard difficulty opponents in TextStarCraft II, demonstrating its effectiveness in enhancing LLM performance through reflection and strategy iteration.

## Method Summary
The ROE framework operates through a cycle of gameplay, keyframe selection, and strategy iteration. The LLM plays a game using a system prompt that contains either expert experience (for the first round) or self-generated reflections from previous games. After each game, the system extracts keyframes from the game summary based on phase transitions (Early, Mid, Late Game), then generates a structured self-reflection analyzing the previous strategy. This reflection replaces the previous experience in the system prompt for the next iteration. The process repeats for up to 5 rounds, allowing the LLM to iteratively improve its strategy through reflection on both expert and self-generated experiences.

## Key Results
- Achieved 20% win rate against Very Hard difficulty opponents in TextStarCraft II
- Outperformed baseline Chain of Summary (COS) approach
- Demonstrated effective strategy iteration through self-reflection across multiple game rounds
- Showed improved resource management and build order execution over successive iterations

## Why This Works (Mechanism)

### Mechanism 1: Keyframe Selection Reduces Context Noise
Filtering high-dimensional game states into phase-based keyframes reduces context noise, potentially allowing the LLM to focus on strategic inflection points rather than raw data. The framework divides the game into temporal phases and selects "key frames" only when these transitions occur, converting a 20-minute replay into a sparse sequence of strategic summaries. This mitigates the LLM's context window limitations and tendency to hallucinate over long, redundant text.

### Mechanism 2: Strategy Iteration Through Reflective Feedback Loop
Strategy iteration is achieved by utilizing the LLM as a "reflective critic" to compress trajectory analysis into textual guidelines, which serve as verbal reinforcement for subsequent episodes. After a match, the LLM analyzes the selected keyframes against the previous strategy, generating a structured "Self Reflection" text that replaces previous experience and generates new system prompts. This creates a feedback loop where the "policy" is the natural language prompt itself.

### Mechanism 3: Expert Experience Bootstrapping
Bootstrapping the reflection process with pre-defined expert experience stabilizes early performance before self-generated experience is available. In the first round, the system initializes with expert experience, preventing the "cold start" problem where an unguided LLM might fail so badly that the subsequent reflection is meaningless. The expert baseline provides a floor for performance from which the self-reflection loop can attempt to improve.

## Foundational Learning

### Concept: In-Context Learning / Prompt Engineering
**Why needed here:** The ROE framework does not fine-tune model weights. It relies entirely on manipulating the context window (System Prompts) to change the agent's behavior. You must understand how to structure prompts to separate "instructions" (strategy) from "observations" (game state).
**Quick check question:** Can you explain why the `self_reflection` text is appended to the System Prompt rather than the User Prompt in this architecture?

### Concept: Hierarchical Reinforcement Learning (HRL) Logic
**Why needed here:** The framework implicitly uses HRL concepts by dividing the game into "phases" (Early, Mid, Late). Understanding temporal abstraction is necessary to grasp why "keyframes" are chosen at phase transitions rather than fixed time intervals.
**Quick check question:** How does the "Game Phase Division" prompt assist the LLM in handling long-horizon planning?

### Concept: TextStarCraft II Environment API
**Why needed here:** The framework acts as a wrapper around this specific environment. You need to understand the constraints of the text-based observations (L1/L2 Summaries) and the Action Converter to debug why an LLM decision failed to execute in the game engine.
**Quick check question:** What is the role of the "Action Converter" module mentioned in Figure 2, and what happens if the LLM outputs a decision not found in the `action_dict`?

## Architecture Onboarding

- **Component map:** Environment (TextStarCraft II) -> Summarizer (L1 to L2) -> Keyframe Selector (phase-based filtering) -> Decision Agent (GPT-3.5) -> Environment -> Reflection Agent (GPT-3.5) -> Prompt Manager (System Prompt buffer)
- **Critical path:** The loop closes at the **Prompt Manager**. If the Reflection Agent generates a text block that is too long or format-invalid, the next episode's Decision Agent may fail to initialize or exceed context limits. Validating the output format of the reflection is critical.
- **Design tradeoffs:** Temperature settings differ between decisions (temperature=0 for deterministic execution) and reflection (temperature=1 for creativity/exploration). Keyframe density involves selecting too many keyframes floods the context window; too few lose critical context.
- **Failure signatures:** Strategy Drift occurs when reflection continually changes the "Opening Build Order" every game without settling. Context Overflow happens if Self Reflection grows indefinitely with every loss, eventually hitting token limits. Invalid Actions occur when the LLM outputs a decision like "BUILD BARRACKS" (Terran) while playing Protoss.
- **First 3 experiments:** Smoke Test (Round 0) - Run agent with only Expert Experience against "Hard" difficulty to verify basic prompt flow. Reflection Isolation - Enable reflection but run only 1 iteration; manually inspect the "Self Reflection 1" text. Iteration Stress Test - Force a 5-round match against "Very Hard"; plot resource collection rate for each round to confirm strategy iteration.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Reflection of Episodes (ROE) framework overcome the performance ceiling observed at the "Elite" difficulty level through further strategy iteration? The paper shows the method achieved a 0% win rate (0/6) against Elite opponents, whereas it succeeded on Harder and Very Hard difficulties. This question remains unresolved as the paper does not analyze failure cases against Elite bots or suggest if reflection is sufficient to bridge the skill gap.

### Open Question 2
Is the framework capable of "cold-start" learning, or does it strictly depend on the initial injection of expert experience? Algorithm 2 initializes with expert experience for round 0, and the method relies on "reflection on the successful experience of a difficult game" provided by experts. The paper does not ablate the expert experience component to determine if the LLM can bootstrap a winning strategy from scratch solely through self-reflection.

### Open Question 3
How robust is the keyframe selection method when applied to different LLMs (e.g., open-source models) or different races? The experiment settings are narrow, utilizing only GPT-3.5-turbo, the Protoss race, and a specific keyword-based phase division. Keyword-based extraction relies on the specific output formatting capabilities of GPT-3.5; this might fail with less instruction-following models or different game dynamics (Zerg/Terran).

## Limitations
- The exact implementation details of the TextStarCraft II environment, including L1/L2 summary generation process and action converter, are not specified in the paper.
- The content and format of the expert experience initialization beyond the example in Appendix B remain ambiguous.
- The paper does not detail how reflection memory is managed to prevent context overflow over multiple iterations.

## Confidence
- **High Confidence:** The general framework of using keyframe-based reflection for strategy iteration is clearly described and experimentally validated against COS baseline.
- **Medium Confidence:** The specific mechanisms of keyframe selection (phase-based filtering) and the reflection generation process are well-defined, but implementation details depend on unstated environment specifics.
- **Low Confidence:** The exact initialization content of expert experience and the long-term stability of the reflection loop (context management, convergence) are not fully specified.

## Next Checks
1. **Environment Fidelity Check:** Implement a minimal TextStarCraft II wrapper that outputs L1/L2 summaries matching the described format, and verify the keyframe selector correctly identifies phase transitions.
2. **Reflection Quality Audit:** After each game, manually inspect the generated self-reflection text for logical consistency, actionable advice, and correct attribution of causes to outcomes.
3. **Context Limit Stress Test:** Run 10+ consecutive rounds without truncation and measure context token usage to identify when and if the reflection prompt exceeds GPT-3.5's context window.