---
ver: rpa2
title: Token-Level Marginalization for Multi-Label LLM Classifiers
arxiv_id: '2511.22312'
source_url: https://arxiv.org/abs/2511.22312
tags:
- probability
- uncertainty
- arxiv
- confidence
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of deriving interpretable confidence
  scores from generative LLMs for multi-label content safety classification. While
  models like LLaMA Guard are effective at identifying unsafe content, they lack native
  class-level probabilities, hindering confidence assessment and threshold tuning.
---

# Token-Level Marginalization for Multi-Label LLM Classifiers

## Quick Facts
- arXiv ID: 2511.22312
- Source URL: https://arxiv.org/abs/2511.22312
- Authors: Anjaneya Praharaj; Jaykumar Kasundra
- Reference count: 9
- Primary result: Marginal probability estimation improves LLaMA Guard 2 multi-label safety classification (F1: 0.644→0.658, AUCROC: 0.756→0.824)

## Executive Summary
This paper addresses the challenge of deriving interpretable confidence scores from generative LLMs for multi-label content safety classification. While models like LLaMA Guard are effective at identifying unsafe content, they lack native class-level probabilities, hindering confidence assessment and threshold tuning. The authors propose three token-level probability estimation methods—Conditional, Joint, and Marginal—that leverage token logits during autoregressive decoding to produce category-level confidence scores. Evaluated on synthetic and benchmark datasets, the Marginal Probability method significantly outperforms baseline approaches and generalizes to instruction-tuned models.

## Method Summary
The method extracts confidence scores from generative LLMs by analyzing token logits during autoregressive decoding. Three approaches are proposed: Conditional Probability extracts softmax probabilities at label token positions, Joint Probability multiplies conditional probabilities across the entire output sequence, and Marginal Probability sums probabilities across all generation paths containing the target label using constrained DFS with top-p filtering. The Marginal approach approximates P(Ci|X) = Σ P(T|X) for all paths T containing label Ci, achieving better performance than single-path methods by accounting for multiple valid output sequences.

## Key Results
- Marginal probability improves LLaMA Guard 2 F1-score from 0.644 to 0.658 and AUCROC from 0.756 to 0.824
- Method generalizes to instruction-tuned models (LLaMA 3.1-8B) achieving F1 0.738, AUCROC 0.934 without explicit safety fine-tuning
- Outperforms baseline greedy decoding and Conditional/Joint probability methods on both synthetic and Beavertails benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Conditional Probability Extraction
The softmax probability of a label token at its generation position provides a direct confidence estimate for that category. During autoregressive decoding, when the model generates a label token (e.g., "S1"), the softmax-normalized logit at that position is extracted as P(t_j = t_Ci | X, t_<j). This reflects the model's immediate confidence at the moment of label prediction, though it may misestimate when label position varies across generations.

### Mechanism 2: Joint Probability Aggregation
Multiplying conditional probabilities across the entire output sequence captures cumulative confidence in the predicted label string. For a generated sequence, compute P(T|X) = ∏ P(t_j | X, t_<j) using log-sum for stability. The joint probability of the label-containing subsequence reflects confidence in that specific generation path, but only evaluates the greedy path and exponentially penalizes longer sequences.

### Mechanism 3: Marginal Probability via Constrained Path Exploration
Summing probabilities across all generation paths containing a label yields the most robust confidence estimate. Approximate P(Ci | X) = Σ_{T ∈ T_Ci} P(T|X) using beam-like DFS with top-p filtering (cumulative prob < 0.99), max token depth, and early stopping on EOS. Probabilities accumulate when a path terminates with the target label, though computational overhead from multi-path exploration depends on DFS depth and top-p threshold choices.

## Foundational Learning

- **Concept**: Autoregressive decoding and logit extraction
  - Why needed: All three methods depend on understanding how LLMs produce tokens sequentially and where probabilities originate (softmax over final-layer logits)
  - Quick check: Given an LLM generating "unsafe\nS1, S3", at which positions would you extract logits, and how do you convert them to probabilities?

- **Concept**: Multi-label classification with overlapping categories
  - Why needed: Content safety violations often co-occur; the methods must handle multiple simultaneous labels without treating them as mutually exclusive
  - Quick check: Why can't you simply apply softmax across all safety categories like in multi-class classification?

- **Concept**: Probability vs uncertainty in generative models
  - Why needed: The paper leverages token probability as a proxy for model confidence, but these are not identical—understanding the gap is critical for interpretation
  - Quick check: If a model assigns P("S1") = 0.8 via marginal probability, what does this actually tell you about the model's internal certainty vs. its learned distribution?

## Architecture Onboarding

- **Component map**: Input preprocessor -> Safety prompt formatter -> LLM with logit access -> Token probability extractor (Conditional/Joint/Marginal) -> Label-to-score mapper -> Thresholding module -> Output: (labels, confidence scores)

- **Critical path**: 
  1. Format input with safety classification prompt (aligned with model's training, e.g., LLaMA Guard taxonomy)
  2. Run constrained autoregressive decoding with logit capture at each step
  3. For Marginal: execute DFS path exploration with pruning; accumulate probabilities when paths terminate with target labels
  4. Aggregate per-label confidence scores; apply calibrated thresholds for decision

- **Design tradeoffs**:
  - Conditional: O(1) overhead per label, but myopic—ignores sequence context
  - Joint: O(L) where L = sequence length, but only evaluates greedy path
  - Marginal: O(b × d) where b = branching factor (top-p), d = max depth; higher fidelity but significant latency cost

- **Failure signatures**:
  - Over-confident errors: High marginal probability for wrong labels (possible when model has systematic blind spots)
  - Under-confident correct predictions: Low scores due to label token fragmentation across paths
  - Timeout/excessive compute: DFS explodes if top-p too permissive or max_depth too large
  - Taxonomy mismatch: Labels not in model's vocabulary produce unreliable scores (paper uses single-token proxies)

- **First 3 experiments**:
  1. Reproduce Table 1 baseline vs. conditional probability on synthetic dataset subset (500 samples) to validate logit extraction pipeline
  2. Compare joint vs. marginal probability on Beavertails, measuring AUCROC gap and latency per sample
  3. Ablate DFS parameters (top-p ∈ {0.9, 0.95, 0.99}, max_depth ∈ {5, 10, 15}) to identify compute-accuracy Pareto frontier for your deployment constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Marginal Probability approach maintain its performance gains on complex, ambiguous, real-world content moderation datasets?
- Basis in paper: The authors state evaluations were performed on synthetic datasets which "may not fully reflect the complexity and ambiguity of real-world harmful content."
- Why unresolved: The study relies on a controlled synthetic dataset with gold-standard labels derived from LLM agreement, lacking the noise and edge cases found in production environments.
- What evidence would resolve it: Evaluation of the Marginal Probability method on diverse, human-annotated benchmarks or production-level safety datasets to verify robustness outside synthetic distributions.

### Open Question 2
- Question: How can the computational overhead of the DFS-based Marginal Probability method be reduced for real-time applications?
- Basis in paper: Section 5.1 notes that the method "incurs token-level overhead due to multiple path explorations, which may hinder real-time applications."
- Why unresolved: The current approximation requires exploring multiple generation paths per input, which is inherently slower than single-pass greedy decoding.
- What evidence would resolve it: The development and benchmarking of adaptive path selection or efficient approximation schemes that achieve similar AUCROC scores with significantly lower latency.

### Open Question 3
- Question: To what extent do decoding hyperparameters (e.g., beam width, top-p sampling) impact the robustness and accuracy of the proposed probability estimates?
- Basis in paper: The authors list examining "how decoding strategies (e.g., beam width, top-p sampling) affect robustness" as a specific avenue for future work in Section 5.1.
- Why unresolved: The current implementation relies on specific heuristics (Top-p 0.99, max token cutoff), but the sensitivity of the confidence scores to these specific constraints has not been explored.
- What evidence would resolve it: Ablation studies showing the variance in F1-score and AUCROC across different decoding settings and beam widths.

### Open Question 4
- Question: Are there more mathematically principled estimation techniques that can outperform the current DFS-based approximation for marginal probability?
- Basis in paper: The authors note in Section 5.1 that their estimation "does not explore the full space of generation paths" and suggest investigating "more efficient or principled marginal estimation techniques."
- Why unresolved: The current method relies on heuristic cutoffs (Algorithm 1) to maintain tractability, potentially missing valid probability mass in pruned paths.
- What evidence would resolve it: Comparative analysis of the current method against alternative estimation algorithms (e.g., Monte Carlo methods) that provide theoretical guarantees on error bounds.

## Limitations
- Synthetic data quality: Results rely heavily on 2.3k synthetically generated records with labels assigned through majority voting across three LLMs, limiting generalizability to real-world distribution
- Computational feasibility: Marginal probability method requires exploring multiple generation paths through DFS, with exact computational overhead across different sequence lengths and branching factors not quantified
- Taxonomy dependency: Method's effectiveness tightly coupled with LLaMA Guard taxonomy structure and single-token label representations, potentially failing with multi-token labels or vocabulary mismatches

## Confidence
- **High Confidence**: The mathematical formulation of conditional, joint, and marginal probability estimation is sound and well-defined. The comparative results showing marginal probability outperforming baselines (F1: 0.644→0.658, AUCROC: 0.756→0.824) are reproducible given access to the models and proper logit extraction.
- **Medium Confidence**: The generalization to instruction-tuned models (LLaMA 3.1-8B achieving F1 0.738, AUCROC 0.934) suggests broader applicability, but this is based on a single model evaluation. The method's performance across different model families and sizes requires further validation.
- **Low Confidence**: Claims about real-world applicability are limited by the synthetic data foundation. Without validation on human-annotated safety datasets or live deployment data, the practical utility for production content moderation remains uncertain.

## Next Checks
1. **Replicate with Real Data**: Test the marginal probability method on human-annotated safety datasets (e.g., REALTOXICITYPROMPTS, Jigsaw's datasets) to validate whether synthetic-data improvements translate to real-world performance gains.

2. **Compute Efficiency Profiling**: Implement the DFS-based marginal probability estimation with systematic ablation of top-p thresholds (0.9, 0.95, 0.99) and max depth limits (5, 10, 15) to quantify the accuracy-latency tradeoff and identify deployment-optimal parameters.

3. **Taxonomy Robustness Test**: Evaluate the method's sensitivity to label-token mapping quality by introducing synthetic noise in label formatting (multi-token labels, inconsistent spacing, capitalization variations) and measuring performance degradation.