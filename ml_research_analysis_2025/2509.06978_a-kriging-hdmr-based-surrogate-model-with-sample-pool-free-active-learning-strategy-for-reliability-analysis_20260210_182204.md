---
ver: rpa2
title: A Kriging-HDMR-based surrogate model with sample pool-free active learning
  strategy for reliability analysis
arxiv_id: '2509.06978'
source_url: https://arxiv.org/abs/2509.06978
tags:
- sub-surrogate
- surrogate
- reliability
- failure
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of high-dimensional reliability
  analysis in engineering systems, where conventional surrogate models suffer from
  the "curse of dimensionality" as the number of random variables increases. The proposed
  method combines Kriging with High-Dimensional Model Representation (HDMR) and employs
  a sample pool-free active learning strategy to efficiently construct accurate surrogate
  models for limit state functions (LSFs).
---

# A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis

## Quick Facts
- arXiv ID: 2509.06978
- Source URL: https://arxiv.org/abs/2509.06978
- Reference count: 40
- This study addresses high-dimensional reliability analysis by combining Kriging with HDMR and using PSO-based sample selection without candidate pools

## Executive Summary
This study addresses the challenge of high-dimensional reliability analysis in engineering systems, where conventional surrogate models suffer from the "curse of dimensionality" as the number of random variables increases. The proposed method combines Kriging with High-Dimensional Model Representation (HDMR) and employs a sample pool-free active learning strategy to efficiently construct accurate surrogate models for limit state functions (LSFs). The approach decomposes high-dimensional LSFs into multiple low-dimensional sub-surrogate models and uses Particle Swarm Optimization (PSO) to directly select informative samples without relying on large candidate sample pools.

## Method Summary
The proposed method combines Kriging with High-Dimensional Model Representation (HDMR) to construct surrogate models for high-dimensional limit state functions (LSFs). The approach decomposes the LSF into low-dimensional sub-surrogate models using Cut-HDMR expansion, then employs a sample pool-free active learning strategy where PSO directly optimizes sample selection based on uncertainty-driven acquisition functions. The method consists of three stages: developing single-variable sub-surrogate models, identifying coupling-variable requirements, and constructing coupling-variable sub-surrogate models. All computations are performed in standard normal space, and failure probability is estimated via Monte Carlo simulation on the composite surrogate model.

## Key Results
- The method achieves high computational efficiency while maintaining strong predictive accuracy
- Significantly fewer function evaluations are required compared to existing methods
- Reliable failure probability estimates are delivered across all tested numerical examples
- The sample pool-free approach successfully avoids scalability bottlenecks of candidate pool screening

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Decomposition via Cut-HDMR
Decomposing a high-dimensional limit state function (LSF) into a sum of low-dimensional component functions reduces sample complexity from exponential to roughly linear growth with dimension. Cut-HDMR represents the LSF as f(x) = f₀ + Σfᵢ(xᵢ) + Σfᵢⱼ(xᵢ,xⱼ) + ... anchored at a reference point. Kriging is then applied to each low-dimensional sub-function separately, avoiding direct modeling of the full input space.

### Mechanism 2: Direct Sample Optimization Replaces Pool-Based Screening
Using PSO to directly maximize uncertainty-based acquisition functions avoids the scalability bottleneck of maintaining and searching large candidate sample pools. Instead of generating a large Monte Carlo population and evaluating a learning function at every candidate, PSO iteratively explores the design space to find the global optimum of the acquisition function.

### Mechanism 3: Reliability-Focused Acquisition with Spatial Penalties
Modifying the learning function to penalize samples far from the origin (high-probability region in standard normal space) focuses computational effort on regions critical for failure probability estimation. The objective function combines a U-learning-style uncertainty term with a distance-based penalty, biasing sampling toward regions with higher probability density.

## Foundational Learning

- **Concept: Kriging (Gaussian Process Regression)**
  - Why needed here: Provides both prediction mean and prediction variance, enabling uncertainty-driven active learning
  - Quick check question: Can you explain why Kriging variance is zero at training points and non-zero elsewhere?

- **Concept: Cut-HDMR Expansion**
  - Why needed here: Reduces the curse of dimensionality by expressing a high-dimensional function as a sum of functions of fewer variables
  - Quick check question: For a 10-variable function, how many first-order and second-order Cut-HDMR component functions exist?

- **Concept: U-Learning Function**
  - Why needed here: Provides a criterion balancing predicted mean proximity to the limit state against prediction uncertainty
  - Quick check question: Why does minimizing U = |μG|/σG target regions where failure/safety classification is most uncertain?

## Architecture Onboarding

- **Component map**: Stage 1 (first-order Kriging sub-surrogates) -> Stage 2 (coupling identification) -> Stage 3 (iterative refinement) -> Stage 5 (failure probability estimation)
- **Critical path**: Stage 1 → Stage 2 (coupling identification) → Stage 3 (iterative refinement until stopping criterion satisfied) → Stage 5
- **Design tradeoffs**:
  - Higher α (accuracy control) → more samples, higher accuracy, slower convergence
  - Larger r_s, r_c → broader sampling, better for limit states far from origin, more expensive
  - Including more second-order couplings → better accuracy for interacting variables, more sub-surrogates to maintain
- **Failure signatures**:
  - Premature termination: Check if stopping criterion is satisfied while Pf estimate is still unstable
  - Missing coupling: If Cij threshold is too conservative, important interactions may be omitted
  - PSO convergence to local optima: Monitor if global best position stagnates early
- **First 3 experiments**:
  1. Linear high-dimensional LSF (Example 2): Test with 20, 60, 100 dimensions, verify linear scaling
  2. Known coupling structure (Example 3): Verify Stage 2 correctly identifies all required couplings
  3. Implicit LSF with coupling identification (truss structure, Example 4): Test coupling detection on unknown structure

## Open Questions the Paper Calls Out

### Open Question 1
Is the proposed Cut-HDMR framework sensitive to the selection of the reference point (u₀) when analyzing highly non-linear systems with disconnected failure domains? The method sets the cut point to the origin (0) in standard normal space, assuming this location effectively captures system behavior. However, for limit state functions where the Most Probable Point (MPP) is far from the origin or the failure domain is highly irregular, this fixed reference point may lead to poor approximation.

### Open Question 2
Does the computational overhead of the Particle Swarm Optimization (PSO) for sample selection become prohibitive for reliability problems with extremely high dimensions (e.g., > 1000 variables)? While the CSP-free approach is efficient, the cost of evaluating the optimization objective function Fobj for every particle in every iteration grows with dimensionality.

### Open Question 3
How does the method perform when high-order coupling terms (third-order or higher) are dominant in the limit state function? Section 3.2 suggests identifying coupling requirements for second-order terms and mentions third-order models only briefly. The paper relies on the assumption that high-order terms are often negligible, which may not hold for all complex engineering systems.

## Limitations
- The dimensional decomposition relies on the assumption that higher-order interactions are negligible, which may not hold for all physical systems
- The coupling identification step uses an ad-hoc threshold for the coupling index Cij without systematic sensitivity analysis
- The PSO-based sample selection lacks convergence guarantees for potentially multimodal acquisition landscapes
- Numerical examples are limited to five cases with relatively modest dimensionalities (max 100D)

## Confidence
- **High confidence**: The dimensional decomposition mechanism (Cut-HDMR + Kriging) is mathematically sound
- **High confidence**: The sample pool-free PSO approach is a valid alternative to candidate screening
- **Medium confidence**: The reliability-focused acquisition function with spatial penalties is theoretically justified
- **Medium confidence**: Numerical examples demonstrate strong performance but test suite is limited

## Next Checks
1. Apply the method to limit state functions with known strong third-order interactions to quantify accumulated truncation error when neglecting higher-order Cut-HDMR terms

2. Systematically vary swarm size, iterations, and inertia weight to evaluate robustness of the sample selection process for problems with complex, multimodal acquisition landscapes

3. Implement leave-one-out or k-fold cross-validation on the coupling identification step to assess stability of Cij threshold selection across different random seeds and initial DoE configurations