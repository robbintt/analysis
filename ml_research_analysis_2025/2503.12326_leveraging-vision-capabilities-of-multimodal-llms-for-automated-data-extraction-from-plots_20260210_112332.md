---
ver: rpa2
title: Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction
  from Plots
arxiv_id: '2503.12326'
source_url: https://arxiv.org/abs/2503.12326
tags:
- data
- plots
- plot
- extraction
- extracted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that multimodal large language models can
  accurately extract data from scientific plots with over 90% precision and 90% recall,
  achieving errors of around 5% or less in both x and y coordinates. The authors propose
  a zero-shot method called PlotExtract that uses a chain-of-thought sequence of prompts
  to analyze plot images, generate and execute Python code to reproduce the plot,
  and verify the extraction by visual comparison.
---

# Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots

## Quick Facts
- arXiv ID: 2503.12326
- Source URL: https://arxiv.org/abs/2503.12326
- Reference count: 0
- Primary result: Multimodal LLMs extract plot data with >90% precision and recall using zero-shot prompting

## Executive Summary
This paper demonstrates that multimodal large language models can accurately extract data from scientific plots without fine-tuning, achieving over 90% precision and recall with errors around 5% or less in both x and y coordinates. The authors propose PlotExtract, a zero-shot method using chain-of-thought prompting to analyze plot images, generate Python code to reproduce the plot, and verify extraction through visual comparison. This approach eliminates the need for manual data extraction and enables high-throughput automated data extraction from research plots. The method was tested on both synthetic and published plots, showing consistent accuracy across different plot types and styles.

## Method Summary
The PlotExtract method uses a 4-step chain-of-thought workflow with multimodal LLMs to extract data from 2D scientific plots. First, the LLM receives a plot image and task-specific prompt to return numerical data in standardized format or "no data" if extraction is impossible. Second, the LLM generates Python code to recreate the plot from extracted data, with iterative error fixing if execution fails (typically 1-2 iterations). Third, the extracted data is executed in a Python sandbox to generate a reconstructed plot. Finally, a new LLM conversation visually compares the original and reconstructed plots using binary classification to validate extraction accuracy. The entire process uses zero-shot prompting without model fine-tuning.

## Key Results
- Achieved >90% precision and recall for data extraction from scientific plots
- Mean Absolute Error (MAE) in x and y coordinates remained below 5% threshold
- Plot classification accuracy reached 100% precision on both synthetic and published datasets
- Method successfully handled diverse plot types including scatter plots, line plots, and bar charts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal LLMs can extract coordinate data from 2D plot images through zero-shot vision analysis
- Mechanism: The model receives a plot image and a task-specific prompt directing it to return numerical data in a standardized format. The prompt explicitly permits the model to return "no data" if extraction is impossible, filtering unextractable plots upstream.
- Core assumption: The pretrained vision-language model has sufficient spatial reasoning to map pixel positions to axis values without fine-tuning.
- Evidence anchors:
  - [abstract]: "This capability is inherent to the pretrained models and can be achieved with a chain-of-thought sequence of zero-shot engineered prompts we call PlotExtract, without the need to fine-tune."
  - [section II]: "This prompt is totally general and does not need to be altered for different types of images."
  - [corpus]: Weak direct evidence; PlotGen-Bench evaluates VLM code generation from plots but not extraction accuracy.
- Break condition: Plots with obscured data, missing axis labels, or non-consecutive tick markings cause extraction failure (12% of published plots in evaluation).

### Mechanism 2
- Claim: Code generation and execution creates a verifiable reproduction of extracted data
- Mechanism: After extraction, the LLM generates Python code to recreate the plot from the extracted data. If execution fails, the full error message is returned to the model for correction (observed 1-2 iterations to success).
- Core assumption: The LLM can map numerical data to appropriate plotting code and self-correct based on runtime errors.
- Evidence anchors:
  - [section II]: "If the code fails to execute, the entire error message is given back to the model along with an instruction to fix the previously generated code... This process is repeated until the code executes, which occurred within 1-2 iterations in all cases we tested."
  - [section I]: "PE exploits multiple advanced LLM capabilities, including... python code generation to reproduce the plot followed by code output analysis to fix potential errors"
  - [corpus]: KM-GPT uses similar reconstruction logic for Kaplan-Meier plots, suggesting generalizability.
- Break condition: Complex plot types (3D, multi-axis) not tested; extrapolation beyond 2D plots is uncertain.

### Mechanism 3
- Claim: Visual comparison between original and reconstructed plots validates extraction accuracy
- Mechanism: A fresh LLM conversation receives both the original and extracted plot images. The model performs binary classification ("yes"/"no") on whether they represent the same data, catching both extraction errors and initially undetected unextractable plots.
- Core assumption: The model can visually compare two images and detect discrepancies in axis ranges, point counts, and trend behavior.
- Evidence anchors:
  - [section II]: "A strict 'yes' or 'no' binary classification response is requested... This step corrects mistakes in extraction (increasing overall precision) and catches cases of unreadable plots that managed to pass the screening in step 1."
  - [Table I]: Plot classification precision 100% on both synthetic and published data; recall 88.9% and 81.8% respectively.
  - [corpus]: No direct corpus evidence for this specific verification mechanism.
- Break condition: Subtle numerical errors within tolerance may pass visual inspection; the authors note pattern-completion bias where LLMs "correct" irregular values to expected patterns.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: The method explicitly avoids fine-tuning; all capability derives from prompt design and inherent model knowledge.
  - Quick check question: Can you write a prompt that specifies output format and permits null responses without providing examples?

- Concept: Chain-of-thought reasoning
  - Why needed here: The 4-step workflow decomposes extraction into sequential subtasks (extract → code → execute → compare), each building on prior outputs.
  - Quick check question: How would you break down "extract data from a scatter plot" into non-recursive subtasks?

- Concept: Error feedback loops in code generation
  - Why needed here: The mechanism relies on feeding runtime errors back to the model for self-correction rather than pre-validating code.
  - Quick check question: What information must be preserved when relaying a Python traceback to an LLM for debugging?

## Architecture Onboarding

- Component map:
  Input layer: Plot image file (raster format assumed)
  → Extraction module: LLM API call with data extraction prompt → returns structured coordinates or null
  → Code generation module: LLM API call with extracted data → returns Python plotting code
  → Execution sandbox: Python runtime with matplotlib; error feedback loop to code generation (max ~2 iterations observed)
  → Validation module: New LLM conversation with [original image, reconstructed image] → binary pass/fail
  → Output: Extracted coordinate data (if validated) or flagged for human review (if rejected)

- Critical path:
  1. Image → extraction prompt → LLM → raw data
  2. Raw data → code generation prompt → LLM → Python code
  3. Python code → execution sandbox → (error loop if needed) → reconstructed plot image
  4. [Original image, reconstructed image] → comparison prompt → LLM → validation decision
  5. If validated: output data; if rejected: flag for review

- Design tradeoffs:
  - Precision vs. recall: Two-stage filtering (steps 1 and 4) maximizes precision (100%) at cost of recall (~85%); authors argue precision is more important for downstream data quality.
  - Temperature=0: Enforced for deterministic outputs; may reduce creative problem-solving on edge cases.
  - Single model choice: Claude 3.5 Sonnet selected over GPT-4o based on preliminary performance; generalizability to other vision-LLMs not quantified.

- Failure signatures:
  - Pattern completion bias: LLM rounds irregular values (e.g., "4.1" → "4") to match expected sequences; more severe on uniformly-spaced x-axes.
  - Overcrowded plots: Lines/points obscured by legends, text, or other data cause extraction gaps.
  - Non-standard axis labeling: Non-consecutive numbers or missing tick labels trigger rejection.

- First 3 experiments:
  1. Reproduce the synthetic plot test with 10 plots using the provided figshare data; verify your MAE_x and MAE_y fall within reported 2.9-3.0% and 0.94-1.0% ranges.
  2. Test failure mode: Create synthetic plots with deliberately irregular x-values (e.g., 0, 1, 2, 4.1, 5, 6) and quantify pattern-completion error rate compared to irregular y-values.
  3. Model substitution: Swap Claude 3.5 Sonnet for GPT-4o with identical prompts on a 20-plot subset; compare precision/recall to establish whether model choice is architectural constraint or convenience.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PlotExtract method generalize effectively to complex plot types beyond two-axis plots, such as 3D plots, heatmaps, contour plots, and multi-panel figures?
- Basis in paper: [explicit] The authors state "We consider only plots with two axes in this analysis, although generalization to more complex plots is likely straightforward."
- Why unresolved: The study deliberately limited evaluation to two-axis plots; no testing was conducted on more complex visualizations.
- What evidence would resolve it: Systematic evaluation on datasets containing 3D plots, heatmaps, contour plots, and composite multi-panel figures with ground truth comparisons.

### Open Question 2
- Question: Can the LLM's inherent tendency to follow sequential patterns (e.g., rounding "4.1" to "4") be mitigated through improved prompting or post-processing strategies?
- Basis in paper: [explicit] The authors identify that "The major source of inaccuracies in data extraction seems to have come from the inherent nature of the LLM to 'continue' the previous text."
- Why unresolved: This is identified as a fundamental limitation but no mitigation strategy was proposed or tested.
- What evidence would resolve it: Ablation studies with modified prompts, few-shot examples, or post-processing corrections specifically targeting pattern-following errors.

### Open Question 3
- Question: How does PlotExtract performance compare across different multimodal LLMs (GPT-4o, Gemini, etc.) beyond Claude 3.5 Sonnet?
- Basis in paper: [inferred] The authors mention Claude 3.5 Sonnet "performed better" than GPT-4o but provide no systematic comparison, despite stating "any vision-capable LLM could be used."
- Why unresolved: Only Claude 3.5 Sonnet was systematically evaluated; the GPT-4o comparison was informal and unquantified.
- What evidence would resolve it: Benchmarking multiple vision-capable LLMs on the same synthetic and published plot datasets using identical metrics.

### Open Question 4
- Question: How does extraction accuracy vary across scientific domains with different plot conventions, and what domain-specific failure modes exist?
- Basis in paper: [inferred] The published plots were randomly selected via broad search terms, and synthetic data was intentionally domain-agnostic; no domain-stratified analysis was performed.
- Why unresolved: Performance was aggregated across all plots without analyzing variations by field or identifying domain-specific challenges.
- What evidence would resolve it: Domain-stratified evaluation across fields (e.g., materials science, biology, physics) with analysis of field-specific error patterns.

## Limitations

- The method is constrained to 2D plots with standard axis conventions and has not been tested on complex visualizations like 3D plots, heatmaps, or multi-axis arrangements.
- A 5% error threshold, while impressive, may be insufficient for applications requiring high-precision measurements in fields where small numerical differences carry significant meaning.
- Performance on published plots showed lower recall (81.8%) compared to synthetic data (88.9%), suggesting the method may struggle more with publication-ready graphics that include additional formatting elements.

## Confidence

- **High** confidence for the core claim that multimodal LLMs can extract plot data without fine-tuning
- **Medium** confidence for generalizability across diverse scientific disciplines and plot types
- **Medium** confidence for the robustness of the visual verification step, which relies on subjective LLM judgment

## Next Checks

1. **Cross-domain validation**: Apply the method to plot types from fields not represented in the published dataset (e.g., chemistry, geology, economics) to assess domain generalizability beyond biomedical literature.

2. **Error propagation analysis**: Systematically quantify how small extraction errors compound when the extracted data is used in downstream analyses (regression, statistical tests, etc.) to determine the practical significance of the 5% error threshold.

3. **Alternative model comparison**: Test the same methodology with different multimodal LLMs (GPT-4o, Gemini Pro Vision) using identical prompts and datasets to isolate whether the reported performance is model-specific or represents a broader architectural capability.