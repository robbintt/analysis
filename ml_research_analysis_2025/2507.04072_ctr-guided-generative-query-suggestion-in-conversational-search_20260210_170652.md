---
ver: rpa2
title: CTR-Guided Generative Query Suggestion in Conversational Search
arxiv_id: '2507.04072'
source_url: https://arxiv.org/abs/2507.04072
tags:
- query
- user
- preference
- diversity
- click
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating effective query
  suggestions in conversational search by aligning model outputs with user preferences,
  particularly when dealing with sparse and noisy click signals. The proposed framework,
  GQS, integrates click modeling and preference optimization to enhance real-world
  user engagement.
---

# CTR-Guided Generative Query Suggestion in Conversational Search

## Quick Facts
- arXiv ID: 2507.04072
- Source URL: https://arxiv.org/abs/2507.04072
- Authors: Erxue Min; Hsiu-Yuan Huang; Xihong Yang; Min Yang; Xin Jia; Yunfang Wu; Hengyi Cai; Junfeng Wang; Shuaiqiang Wang; Dawei Yin
- Reference count: 16
- Primary result: GQS framework improves generative query suggestion in conversational search by aligning outputs with user preferences using fine-grained CTR modeling and diversity-aware preference optimization.

## Executive Summary
This paper addresses the challenge of generating effective query suggestions in conversational search by aligning model outputs with user preferences, particularly when dealing with sparse and noisy click signals. The proposed framework, GQS, integrates click modeling and preference optimization to enhance real-world user engagement. It consists of three key components: a Multi-Source CTR Modeling module that captures diverse contextual signals to estimate fine-grained click-through rates; a Diversity-Aware Preference Alignment strategy using CTR-weighted Direct Preference Optimization (DPO), which balances relevance and semantic diversity; and a CTR-Calibrated Iterative Optimization process that jointly refines the CTR and generation models across training rounds. Experiments on two real-world tasks demonstrate that GQS outperforms strong baselines in CTR, relevance, and diversity.

## Method Summary
The GQS framework trains a generative model for query suggestion by combining CTR modeling with preference optimization. It uses a BERT-based CTR model with cross-attention to fuse heterogeneous context signals and estimate click probabilities. Preference pairs are constructed based on CTR and diversity scores, then used in CTR-weighted DPO to align the LLM. An iterative optimization loop recalibrates the CTR model using importance sampling to correct for distribution drift as the generator policy changes. The system was evaluated on real-world conversational search tasks, showing significant improvements in CTR and diversity metrics.

## Key Results
- GQS achieves up to 70.36% CTR and 86.04% diversity on Task 1, outperforming baselines
- On Task 2, GQS reaches 30.72% CTR with 82.09% diversity
- Standard DPO increases CTR but collapses diversity (65.73% in Task 1), while GQS maintains high diversity
- CTR-Calibrated Iterative Optimization improves performance over uncalibrated training by approximately 7.45 points on Task 1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-grained CTR estimation is more effective for alignment than raw binary click labels because it denoises user behavior.
- **Mechanism**: The framework employs a Multi-Source CTR Modeling architecture using BERT with cross-attention to encode heterogeneous context and model query-context interaction, producing continuous probability scores that serve as stable reward signals.
- **Core assumption**: Cross-attention successfully captures complex dependencies between context and query intent that simple concatenation misses, and position bias is the primary bias correctable via embeddings.
- **Evidence anchors**: [abstract], [section 3.1], [corpus] neighbor papers like "OneSug" confirm unified generative frameworks but lack specific citation of cross-attention denoising for CTR.

### Mechanism 2
- **Claim**: Jointly optimizing for predicted CTR and semantic diversity prevents the "mode collapse" common in relevance-only optimization.
- **Mechanism**: The framework uses CTR-weighted DPO with a diversity regularizer, constructing preference pairs based on CTR scores but weighting the loss by CTR gap confidence, while creating auxiliary pairs for diversity differences.
- **Core assumption**: High-CTR suggestions tend to be repetitive, and this drift can be counteracted by a separately trained diversity model.
- **Evidence anchors**: [abstract], [section 3.2], [table 1] showing standard DPO increases CTR but collapses diversity while GQS maintains high diversity.

### Mechanism 3
- **Claim**: Offline data can be reused for iterative training without costly online data collection if distribution drift is corrected.
- **Mechanism**: The system uses CTR-Calibrated Iterative Optimization with importance sampling weights to re-weight old click logs, allowing the CTR model to stay synchronized with the generator as the generation policy shifts.
- **Core assumption**: The likelihood ratio between current and initial policy accurately captures distribution shift, and clipping threshold is sufficient to prevent variance explosion.
- **Evidence anchors**: [section 3.3], [figure 5] showing calibrated training improves CTR significantly over rounds while uncalibrated stagnates.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here**: The paper uses DPO as the core engine for aligning the LLM with predicted CTR rewards, eliminating the need for explicit reward modeling.
  - **Quick check question**: How does DPO eliminate the need for a separate Reward Model network during the preference alignment phase?

- **Concept: Position Bias in Learning-to-Rank**
  - **Why needed here**: The CTR modeling section explicitly adds positional embeddings because users click top items more often regardless of relevance.
  - **Quick check question**: Why would a standard classification loss on raw clicks fail to train a useful CTR model in a search interface?

- **Concept: Importance Sampling (Off-Policy Correction)**
  - **Why needed here**: The iterative optimization component relies on this to correct the "stale data" problem when the generation policy shifts.
  - **Quick check question**: In the context of this paper, what does the ratio πθ^(t)/πθ^(0) represent regarding the value of old click logs?

## Architecture Onboarding

- **Component map**: Input Layer (User Query, Context, Co-occurring Queries) -> CTR Model (BERT Encoder + Cross-Attention Fusion + Position Embedding) -> Pair Constructor (Generates candidates, Ranks by CTR, Filters for Diversity) -> Alignment Engine (CTR-Weighted DPO Loss + Diversity DPO Loss) -> Update Loop (Update Generator, Calculate Importance Weights, Update CTR Model)

- **Critical path**: The accuracy of the CTR Model. If the predictor does not successfully denoise raw clicks, the preference pairs constructed for DPO will be random noise, rendering the alignment steps useless.

- **Design tradeoffs**:
  - **Diversity Weight (λ)**: Set to 0.1 in paper. Higher values secure diversity but risk lowering CTR; lower values maximize CTR but risk generating repetitive queries.
  - **Importance Clipping (ε)**: Controls how much to trust distribution shift correction. Too tight restricts learning; too loose introduces variance.

- **Failure signatures**:
  - **Repetitive Suggestions**: Model outputs slight variations of the same high-CTR query. Indicates λ is too low or diversity filtering threshold δ is not strict enough.
  - **Stagnant CTR**: CTR plateaus or drops after initial iterations. Indicates importance sampling weights are failing to correct distribution drift.
  - **Low Relevance**: Model generates "clickbait" queries that are diverse but irrelevant. Indicates CTR model is overfitting or diversity loss is overwhelming relevance signal.

- **First 3 experiments**:
  1. **Ablate Cross-Attention**: Replace Cross-Attention fusion with simple concatenation ("-CAttn" in Figure 4) to verify contribution of context-query interaction.
  2. **Diversity Stress Test**: Run model with λ = 0 (removing diversity loss) to confirm "mode collapse" behavior described in Table 1/DPO baseline.
  3. **Iterative Stability Check**: Run iterative loop for 5+ rounds without importance sampling calibration ("Uncalibrated" in Figure 5) to observe performance degradation from distribution drift.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the computational overhead of the CTR-Calibrated Iterative Optimization be reduced while maintaining alignment quality?
- **Basis in paper**: [explicit] The authors state in the Limitations section that the "iterative optimization process requires multiple rounds of reward model updates and generation training, which increases computational cost," and propose exploring "more efficient update strategies."
- **Why unresolved**: The current method relies on repeated re-encoding of context and re-training, which is resource-intensive for real-time systems.
- **What evidence would resolve it**: A study comparing the current multi-round approach against single-pass or online learning variants, analyzing the trade-off between training speed and CTR/diversity metrics.

### Open Question 2
- **Question**: How can human-in-the-loop feedback be effectively integrated to correct for residual biases in click signals?
- **Basis in paper**: [explicit] The authors note that "residual bias could still influence optimization" despite CTR modeling and explicitly propose to "incorporate human-in-the-loop feedback for better alignment quality" in future work.
- **Why unresolved**: It is unclear how manual feedback should be weighted against automated CTR signals during the Diversity-Aware Preference Alignment phase without reintroducing scalability issues.
- **What evidence would resolve it**: A hybrid experimental setup showing that injecting human annotations into the preference pairs Y^c, Y^r improves performance on "noisy" or "biased" subsets of the dataset compared to the purely automated GQS.

### Open Question 3
- **Question**: How robust is the diversity estimation module when applied to unseen topics or domains outside the annotated samples?
- **Basis in paper**: [explicit] The paper highlights that the "diversity estimation module is trained on a limited number of annotated samples, which may not generalize well to unseen topics or domains."
- **Why unresolved**: The Diversity-Aware Preference Alignment relies on this module to filter low-diversity candidates; failure here could cause the model to collapse into repetitive outputs for novel queries.
- **What evidence would resolve it**: Cross-domain evaluation results showing the diversity scores (and subsequent model performance) remain stable when the input queries shift significantly from the diversity model's training distribution.

## Limitations

- The framework's heavy dependence on CTR model accuracy makes it vulnerable to poor generalization from training data distribution shifts.
- The effectiveness of the diversity regularizer is sensitive to the empirically-set weight λ = 0.1 without theoretical justification.
- Importance sampling correction in iterative optimization may become unstable with large policy update steps, leading to high-variance weight estimates.

## Confidence

- **High Confidence**: Claims regarding CTR and diversity trade-off, supported by direct ablation in Table 1 showing standard DPO increases CTR but reduces diversity, while GQS maintains both.
- **Medium Confidence**: Claims about the effectiveness of cross-attention and position bias correction, as the paper demonstrates performance gains over simpler baselines but lacks ablations isolating these choices.
- **Low Confidence**: Claims about the stability of iterative optimization over many rounds, as Figure 5 only shows 3 rounds without testing for long-term degradation or reward hacking scenarios.

## Next Checks

1. **Cross-Attention Ablation**: Replace the cross-attention fusion in the CTR model with simple concatenation and measure the drop in CTR/Diversity to isolate the contribution of context-query interaction.
2. **Diversity Over-Correction Test**: Set λ = 1.0 (over-emphasizing diversity) and observe if the model generates irrelevant but diverse queries, confirming the need for the current λ = 0.1 setting.
3. **Long-Running Iterative Stability**: Run the iterative optimization loop for 10+ rounds and monitor for reward hacking (predicted CTR diverging from actual engagement) or CTR degradation, testing the robustness of the importance sampling correction.