---
ver: rpa2
title: 'NOSA: Native and Offloadable Sparse Attention'
arxiv_id: '2510.13602'
source_url: https://arxiv.org/abs/2510.13602
tags:
- nosa
- attention
- sparse
- cache
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving decoding efficiency
  for large language models, which is limited by GPU memory constraints, particularly
  the key-value (KV) cache. The proposed solution, NOSA (Native and Offloadable Sparse
  Attention), is a trainable sparse attention mechanism designed to work efficiently
  with KV cache offloading.
---

# NOSA: Native and Offloadable Sparse Attention

## Quick Facts
- arXiv ID: 2510.13602
- Source URL: https://arxiv.org/abs/2510.13602
- Reference count: 33
- Primary result: NOSA achieves up to 5.04×, 1.92×, and 1.83× higher decoding throughput compared to FullAttn, InfLLMv2, and ShadowKV, respectively.

## Executive Summary
NOSA (Native and Offloadable Sparse Attention) addresses GPU memory constraints in large language model decoding by introducing a trainable sparse attention mechanism compatible with KV cache offloading. The key innovation is a locality constraint during training that explicitly limits KV cache transfer volume between CPU and GPU, reducing communication overhead. NOSA decomposes KV selection into query-aware and query-agnostic components, using an eviction policy to bound the number of KV blocks fetched from CPU. The paper also introduces NOSI, a KV cache offloading inference system optimized for NOSA, demonstrating significant throughput improvements across general, long-input, and long-generation tasks.

## Method Summary
NOSA implements a trainable sparse attention mechanism that works in conjunction with KV cache offloading systems. The method introduces a locality constraint during training to explicitly limit the volume of KV cache transfers between CPU and GPU. NOSA decomposes KV selection into two components: a query-aware part that selects relevant KV pairs based on current queries, and a query-agnostic part that uses an eviction policy to bound the number of KV blocks fetched from CPU. This decomposition enables efficient cache management while maintaining decoding performance. The accompanying NOSI system is specifically designed to leverage NOSA's sparsity patterns for optimized inference.

## Key Results
- Achieved up to 5.04× higher decoding throughput compared to FullAttn on general tasks
- Demonstrated 1.92× improvement over InfLLMv2 on long-input tasks
- Showed 1.83× speedup relative to ShadowKV on long-generation tasks

## Why This Works (Mechanism)
NOSA works by explicitly constraining the KV cache transfer volume during training through a locality constraint. This constraint ensures that the model learns attention patterns that naturally fit within the memory limits of KV cache offloading systems. By decomposing KV selection into query-aware and query-agnostic components, NOSA can efficiently manage which key-value pairs to keep in GPU memory versus fetching from CPU. The eviction policy further optimizes this process by bounding the number of KV blocks that need to be transferred, directly addressing the communication bottleneck that limits decoding efficiency in large language models.

## Foundational Learning
- **KV cache offloading**: Technique to move key-value pairs to CPU memory when GPU memory is constrained; needed to enable decoding of longer sequences with limited GPU memory; quick check: verify GPU memory usage reduction with CPU offloading enabled
- **Sparse attention**: Selective computation of attention weights instead of full matrix operations; needed to reduce computational complexity; quick check: confirm sparsity ratio and computational savings
- **Locality constraint**: Training-time restriction on attention patterns to limit memory access patterns; needed to optimize for KV cache transfer efficiency; quick check: measure reduction in CPU-GPU communication volume
- **Query-aware vs query-agnostic selection**: Decomposition of attention selection into context-dependent and context-independent components; needed to balance relevance and efficiency; quick check: verify both components contribute to final selection
- **Eviction policy**: Strategy to determine which KV blocks to remove from GPU memory; needed to manage limited memory resources; quick check: confirm bounded number of KV blocks fetched

## Architecture Onboarding

**Component Map**: Training process -> Locality constraint -> Query-aware selection -> Query-agnostic selection -> Eviction policy -> NOSI inference system

**Critical Path**: The critical path involves training with locality constraints, which shapes the model's attention patterns, followed by inference where NOSI manages KV cache transfers based on these learned patterns. The query-aware and query-agnostic decomposition enables efficient runtime selection of KV pairs.

**Design Tradeoffs**: NOSA trades some flexibility in attention patterns for predictable memory access patterns and reduced communication overhead. The locality constraint may limit the model's ability to attend to very distant tokens, but this is offset by the significant throughput gains and maintained decoding quality.

**Failure Signatures**: Potential failures include degraded model quality if the locality constraint is too restrictive, insufficient throughput gains if the sparsity pattern doesn't align well with actual access patterns, or increased latency if the eviction policy makes poor decisions about which KV blocks to keep.

**First 3 Experiments to Run**:
1. Measure baseline decoding throughput with FullAttn, InfLLMv2, and ShadowKV on the same hardware to establish comparison points
2. Verify reduction in KV cache transfer volume between CPU and GPU with NOSA versus baselines
3. Test model quality preservation by comparing perplexity or downstream task performance with and without NOSA's sparsity pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NOSA interact with low-bit KV cache quantization techniques?
- Basis in paper: [explicit] The authors state in the Related Work section: "NOSA is also orthogonal to KV cache quantization, and we leave a systematic study of their combination to future work."
- Why unresolved: Quantization reduces the precision of the cache, which may affect the sensitivity of the eviction head's importance scoring, potentially degrading the accuracy of the query-agnostic selection.
- What evidence would resolve it: A study evaluating NOSA combined with KV quantization (e.g., KIVI or KVQuant) on recall accuracy and throughput.

### Open Question 2
- Question: Can NOSA effectively accelerate the rollout phase in LLM reinforcement learning?
- Basis in paper: [explicit] The Conclusion explicitly lists "accelerating the rollout in LLM reinforcement learning" as a direction for future work.
- Why unresolved: RL requires generating long trajectories where the training-inference mismatch of training-free offloading is detrimental; it is unclear if NOSA's locality constraints align with the specific access patterns required for effective policy optimization.
- What evidence would resolve it: Benchmarks of RL training throughput and convergence speed using NOSA-offloaded models versus standard dense attention.

### Open Question 3
- Question: What are the system-level challenges of integrating NOSA into production-grade serving engines?
- Basis in paper: [explicit] The Conclusion proposes "integrating NOSA and NOSI into LLM serving engines."
- Why unresolved: Current state-of-the-art serving systems (e.g., vLLM) rely on paged memory management (PagedAttention), which may conflict with NOSA's custom block-wise memory layout and strict offloading logic.
- What evidence would resolve it: An implementation of NOSA within a major serving framework demonstrating compatibility and end-to-end latency improvements.

### Open Question 4
- Question: Does the locality constraint in NOSA generalize to Mixture-of-Experts (MoE) architectures?
- Basis in paper: [inferred] The experiments are limited to dense models (1B, 3B, 8B), excluding MoE models which have different memory access patterns and parameter counts.
- Why unresolved: MoE models have dynamic expert routing which might alter KV access locality, potentially reducing the effectiveness of NOSA's fixed eviction policy or requiring different k_q/k_e ratios.
- What evidence would resolve it: Evaluation of NOSA on an MoE model (e.g., Mixtral or DeepSeek) to measure locality preservation and throughput gains.

## Limitations
- Evaluation focuses on throughput improvements without detailed ablation studies on individual NOSA components
- Lacks model quality degradation analysis (perplexity or downstream task accuracy) when applying NOSA's sparsity pattern
- Integration complexity and overhead in real-world production deployments not quantified

## Confidence
- High confidence: The core claim that NOSA achieves higher decoding throughput than FullAttn, InfLLMv2, and ShadowKV is supported by empirical results
- Medium confidence: The claim that NOSA's locality constraint effectively reduces KV cache transfer volume is plausible but lacks direct measurement
- Low confidence: The assertion that NOSA maintains model quality while enabling sparsity is not directly validated

## Next Checks
1. Conduct ablation studies to isolate the contribution of the locality constraint, query-aware/query-agnostic decomposition, and eviction policy to overall throughput gains
2. Measure and report the reduction in KV cache transfer volume (bytes transferred between CPU and GPU) to substantiate the claimed communication overhead savings
3. Evaluate model quality (e.g., perplexity on validation sets, performance on downstream benchmarks) to confirm that NOSA's sparsity pattern does not degrade output quality