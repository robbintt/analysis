---
ver: rpa2
title: Advancements in synthetic data extraction for industrial injection molding
arxiv_id: '2511.08117'
source_url: https://arxiv.org/abs/2511.08117
tags:
- data
- synthetic
- training
- injection
- molding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of insufficient data for machine
  learning in industrial injection molding by exploring synthetic data generation.
  The approach involves simulating injection molding processes using CAD software
  and a production simulator to generate synthetic time series data, which is then
  labeled using an existing classifier.
---

# Advancements in synthetic data extraction for industrial injection molding

## Quick Facts
- arXiv ID: 2511.08117
- Source URL: https://arxiv.org/abs/2511.08117
- Reference count: 16
- Key outcome: Synthetic data up to 30% improves model robustness without degrading validation accuracy

## Executive Summary
This paper addresses data scarcity in industrial injection molding by exploring synthetic data generation for machine learning applications. The authors simulate injection molding processes using CAD software and a production simulator to generate synthetic time series data, which is labeled using an existing classifier. They train LSTM networks on datasets with varying proportions of synthetic data (0-30%) and compare performance against real-only datasets. Results show that while training accuracy decreases slightly with increased synthetic data, validation accuracy remains stable and model robustness to perturbations improves, suggesting synthetic data can effectively augment real datasets and reduce the need for extensive manual labor and material waste.

## Method Summary
The study generates synthetic injection molding data by simulating processes in CAD software and a production simulator, creating time series with controlled parameters to produce both good and defective parts. The synthetic data is labeled using an existing classifier, then mixed with real sensor data (34 features, 10ms intervals) in ratios from 0-30%. An LSTM model with 3 layers (100 units each) and dropout regularization is trained on these datasets, with performance evaluated on real validation data only. The approach leverages data augmentation via offset sampling (quadrupling dataset size) and intentionally imbalanced synthetic data (60% defective) to address real-world class imbalance.

## Key Results
- Validation accuracy remained stable at ~92-93% with up to 30% synthetic data, despite decreasing training accuracy
- Synthetic data improved model robustness to perturbations while reducing overfitting risk
- F1 score and AUC-ROC metrics improved with synthetic data inclusion
- Training accuracy dropped from 98.8% (0% synthetic) to 96.7% (30% synthetic), while validation accuracy stayed constant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adding synthetic data acts as a regularizer to improve model generalization on limited datasets.
- **Mechanism:** The distributional shift between simulation and reality introduces "noise" during training. Because the synthetic data is not a perfect replica of real-world variance, the LSTM cannot memorize the training set (overfitting). This forces the model to learn broader features, resulting in lower training accuracy but maintained validation accuracy.
- **Core assumption:** The synthetic data distribution overlaps sufficiently with real data features to be informative rather than purely disruptive.
- **Evidence anchors:**
  - [results]: Validation accuracy remained around 92-93% while training accuracy decreased with higher synthetic ratios (Page 1, Page 10).
  - [results]: "The validation accuracy remained relatively stable despite the inclusion of synthetic data... contributes to its robustness to potential perturbations" (Page 10).
  - [corpus]: Neighbor papers (e.g., "Learning Actionable World Models") support the general concept of using simulation to bridge data gaps, though they do not validate this specific regularization effect.
- **Break condition:** If synthetic data contains systematic artifacts not present in reality, validation accuracy would degrade alongside training accuracy (hallucinated features).

### Mechanism 2
- **Claim:** Synthetic generation allows targeted balancing of classes that are rare or costly to produce in physical reality.
- **Mechanism:** Industrial processes naturally produce high yields (mostly "good" parts), creating unbalanced datasets. By setting specific parameters in the simulator, engineers can artificially generate "not good" cycles (defects) without material waste, balancing the loss function during training.
- **Core assumption:** The simulated defects share causal features with real defects.
- **Evidence anchors:**
  - [methods]: "An important aspect... is to intentionally generate a larger percentage of non-compliant or defective products... to compensate for the inherent imbalance in real production data" (Page 6).
  - [data]: Synthetic data was generated with a 60% "not good" ratio, compared to the real data's 43.5% "not good" ratio (Page 7-8).
  - [corpus]: "Improving Industrial Injection Molding..." confirms that error rates are typically very low in manufacturing, leading to unbalanced datasets.
- **Break condition:** If the simulated defects are physically impossible or visually distinct from real defects, the model learns to identify "simulation artifacts" rather than actual quality issues.

### Mechanism 3
- **Claim:** Quadrupling data via offset sampling effectively increases temporal resolution coverage for the LSTM.
- **Mechanism:** The authors expanded the dataset by saving sensor samples at staggered 10ms intervals (step 1, 2, 3, 4). This effectively multiplies the dataset size and prevents the model from overfitting to a specific sampling phase or alignment.
- **Core assumption:** The underlying process dynamics do not change significantly between the 10ms sampling intervals.
- **Evidence anchors:**
  - [experimental setup]: "We enlarged each dataset by quadrupling it and stored each first through the fourth step at 10 ms intervals in separate files" (Page 7).
  - [results]: This augmentation contributed to the baseline high accuracy (93.6%) even before synthetic mixing.
  - [corpus]: No specific corpus evidence found to validate the "offset sampling" mechanism specifically for injection molding; this is a domain-specific data engineering technique.
- **Break condition:** If critical state changes occur within the 10ms gap and are missed by the offset, this introduces noise rather than signal.

## Foundational Learning

- **Concept: Regularization via Noise Injection**
  - **Why needed here:** The primary counter-intuitive result is that *worse* training accuracy leads to *better* robustness. Understanding regularization explains why adding "imperfect" synthetic data helps the model generalize.
  - **Quick check question:** If the model achieved 99% training accuracy with 0% synthetic data, would it likely perform better or worse on new real-world data compared to a model with 85% training accuracy? (Answer: Likely worse due to overfitting).

- **Concept: Simulation-to-Reality (Sim2Real) Gap**
  - **Why needed here:** The study relies on CAD and production simulators. Understanding that simulation is an approximation of reality helps in interpreting why the model's training accuracy dropped (the distributions didn't match perfectly).
  - **Quick check question:** Does the simulator capture the wear and tear of a physical machine screw? (Answer: Likely no, this is a domain gap).

- **Concept: Class Imbalance in Manufacturing**
  - **Why needed here:** Standard optimization minimizes error. If 99% of parts are "good," a model can achieve 99% accuracy by blindly guessing "good." Understanding this explains why the authors forced a 60% "not good" ratio in the synthetic set.
  - **Quick check question:** Why is high accuracy sometimes misleading in manufacturing quality control?

## Architecture Onboarding

- **Component map:** Real Machine Sensors -> Production Simulator -> CAD Simulation -> Synthetic Data Generation -> Data Augmentation (Offset Sampling) -> LSTM Model -> Binary Classification
- **Critical path:** The labeling of synthetic data. The study currently relies on a mix of manual labeling and existing classifiers (Page 6). If the labels on the synthetic data are wrong, the model learns incorrect associations.
- **Design tradeoffs:**
  - **Real vs. Synthetic Ratio:** The study tested 0-30% synthetic. >30% was not tested but is proposed as future work. High synthetic ratios lower training accuracy (higher bias) but increase robustness (lower variance).
  - **Simulator Fidelity:** Using a "clone of machine control" (Page 5) for logic and CAD for physics requires integrating two different data streams.
- **Failure signatures:**
  - **High Synthetic, Low Validation:** If adding synthetic data causes validation accuracy to plummet (rather than stay stable), the Sim2Real gap is too wide (synthetic data is not representative).
  - **Zero Learning:** If the model fails to converge, check the scaling of the 34 input features (e.g., screw position vs. pressure have vastly different magnitudes).
- **First 3 experiments:**
  1. **Baseline Real-Only:** Train the LSTM on the 737 real training cycles (no synthetic) to establish the overfitting baseline (expect high training acc, variable val acc).
  2. **Sanity Check Synthetic-Only:** Train *only* on the 400 synthetic cycles and test on real data. This measures the "Sim2Real" transfer capability directly (expect lower accuracy, confirming the domain gap).
  3. **Incremental Mixing:** Train models at 5%, 10%, and 20% synthetic ratios. Plot Training Loss vs. Validation Loss. Look for the "crossover" point where validation loss stops improving even if training loss keeps dropping.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Generative Adversarial Networks (GANs) generate synthetic data that captures real-world noise and uncertainties more effectively than the current CAD and production simulator approach?
- **Basis in paper:** [explicit] The authors state in the Conclusion that future work will "try to use Generative Adversarial Networks (GANs) to generate more synthetic data that also take into account the noise and uncertainties of real-world data."
- **Why unresolved:** The current method relies on simulation which may lack the stochastic nature of real physical processes; GAN implementation is proposed but not yet executed.
- **What evidence would resolve it:** A comparative study showing LSTM performance when trained on GAN-generated data versus simulation-generated data, specifically analyzing the handling of signal noise.

### Open Question 2
- **Question:** Can an independent classifier effectively automate the labeling of synthetic production cycles to eliminate the need for manual evaluation?
- **Basis in paper:** [explicit] The Conclusion notes the intent to "train an independent classifier in the future that can label the synthetic data" to streamline the process, as the current manual labeling is a noted limitation.
- **Why unresolved:** The current study relied on manual labeling based on CAD output and expertise, which is time-consuming.
- **What evidence would resolve it:** The development and validation of a secondary model capable of labeling synthetic cycles with accuracy comparable to expert human labeling.

### Open Question 3
- **Question:** What is the maximum ratio of synthetic to real data that can be introduced before validation accuracy on real data degrades significantly?
- **Basis in paper:** [explicit] The Conclusion states the intention to "increase the synthetic fraction in our data as much as possible," as the current study only tested up to 30% (added) and roughly 45% (total fraction).
- **Why unresolved:** The study showed robustness up to 30% added synthetic data, but the breaking point or "optimal upper bound" for the synthetic ratio was not established.
- **What evidence would resolve it:** Experimental results from training runs with synthetic data ratios exceeding 45% (e.g., 50%, 75%, 100%) evaluated against the real validation set.

## Limitations
- The 34 specific sensor features used as inputs are not disclosed, limiting reproducibility
- Only synthetic ratios up to 30% were tested; performance at higher ratios remains unknown
- The study does not evaluate the impact of synthetic data on other model architectures beyond LSTM

## Confidence
- **High Confidence:** The core finding that synthetic data can augment real datasets without degrading validation accuracy is well-supported by the experimental results (92.3% vs 93.6% validation accuracy at 30% synthetic).
- **Medium Confidence:** The regularization mechanism (Mechanism 1) is plausible but not directly proven; the study shows correlation but doesn't isolate the regularization effect from other factors.
- **Medium Confidence:** The class balancing benefit (Mechanism 2) is supported by the data ratios but could be confounded by the Sim2Real gap effects.

## Next Checks
1. **Domain Gap Isolation Test:** Train an LSTM model on synthetic data only and evaluate on real test data to quantify the Sim2Real transfer capability directly, separate from the mixed-training effects.
2. **Feature Sensitivity Analysis:** Systematically vary the 34 input features (e.g., remove subsets) to identify which features contribute most to the synthetic data's effectiveness and whether the model relies on simulation artifacts.
3. **Scaling Synthetic Ratio:** Extend the synthetic data ratio testing beyond 30% (e.g., 40%, 50%, 60%) to identify the optimal ratio where validation accuracy peaks before declining due to excessive domain shift.