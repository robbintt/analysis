---
ver: rpa2
title: Multiplayer Information Asymmetric Bandits in Metric Spaces
arxiv_id: '2503.08004'
source_url: https://arxiv.org/abs/2503.08004
tags:
- each
- algorithm
- regret
- players
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper studies Lipschitz multiplayer bandits with information
  asymmetry, where players either cannot observe each other''s actions, rewards, or
  both. Three problem settings are considered: asymmetry in actions, rewards, and
  both.'
---

# Multiplayer Information Asymmetric Bandits in Metric Spaces

## Quick Facts
- arXiv ID: 2503.08004
- Source URL: https://arxiv.org/abs/2503.08004
- Authors: William Chang; Aditi Karthik
- Reference count: 33
- One-line primary result: Lipschitz multiplayer bandits with information asymmetry achieve near-optimal regret bounds using implicit coordination mechanisms and adaptive discretization.

## Executive Summary
This paper studies multiplayer bandit problems in continuous metric spaces where players have asymmetric information—they may not observe each other's actions, rewards, or both. Three problem settings are considered: asymmetry in actions, rewards, and both. The authors extend algorithms from finite-armed bandits to continuous spaces using uniform discretization and the zooming algorithm with adaptive discretization. For Problems A and B (asymmetry in actions or rewards), they prove regret bounds matching the single-player case, while Problem C (asymmetry in both) achieves a near-optimal bound. The work introduces novel implicit coordination mechanisms that enable players to coordinate without direct communication.

## Method Summary
The paper presents five algorithms based on two core approaches: uniform discretization (mCAB variants) and adaptive discretization (mZoom variants). The uniform approach discretizes the continuous space into K points per dimension and runs existing multiplayer bandit algorithms on the resulting K^(Md) joint actions. The adaptive approach uses the zooming algorithm with a doubling trick to progressively refine discretization near promising regions. Coordination is achieved through lexicographic ordering of joint actions for Problems A and B, and through interval-based elimination signaling. All algorithms assume known Lipschitz constant L and 1-subgaussian rewards in [0,1].

## Key Results
- Problems A and B achieve regret O(T^((2Md+1)/(2Md+2))L^(Md/(Md+1))(log T)^(1/(2(Md+1)))) matching single-player scaling
- Problem C achieves near-optimal regret O(T^((2Md+1)/(2Md+2))L^(Md/(Md+1))(log T)^(1/(Md+1)))
- mZoom algorithm provides improved bounds O(T^((Md+1)/(Md+2))(c log T)^(1/(Md+2))) for Problems A and B
- Three information asymmetry settings are fully characterized with matching algorithmic solutions

## Why This Works (Mechanism)

### Mechanism 1: Implicit Coordination via Lexicographic Ordering
Players coordinate on joint actions without observing each other's choices by agreeing on a total ordering of action tuples before learning begins. Each player maintains identical UCB indices for all joint arms and deterministically selects the smallest arm according to a pre-agreed base-K numbering scheme when multiple arms tie. This eliminates the need for real-time communication, assuming all players receive the same reward realization and maintain synchronized UCB indices after the initial coordination phase.

### Mechanism 2: Interval-Based Elimination Signaling
Players can eliminate suboptimal arms without direct communication by intentionally deviating from expected actions when their local confidence intervals indicate an arm is dominated. Each player maintains confidence intervals I_a(t) = [μ̂_a(t) - ε_a(t), μ̂_a(t) + ε_a(t)]. When player i observes that arm c's interval is strictly below another arm a's interval (μ̂_c + 2ε_c < μ̂_a - ε_a), player i intentionally pulls an action ≠ c[i]. Other players observe the joint action mismatch and deduce c should be eliminated.

### Mechanism 3: Adaptive Discretization via Zooming with Doubling
Allocating samples proportionally to regions near potentially optimal arms achieves better regret bounds than uniform discretization by concentrating computation where it matters most. The algorithm maintains "active" arms with coverage balls of radius r_a(t) = (1/L)ε_a(t). When uncovered regions exist, progressively halve the discretization grid until finding an uncovered point, then activate it. This creates finer resolution near promising regions based on Lipschitz continuity allowing reward estimate extrapolation to nearby unsampled regions.

## Foundational Learning

- **Concept: Upper Confidence Bound (UCB) Index**
  - Why needed here: The UCB_a(t) = μ̂_a(t) + √(6 log T / n_a(t)) is the core exploration-exploitation primitive; all algorithms build on it.
  - Quick check question: With μ̂_1 = 0.7 (n=5) vs μ̂_2 = 0.6 (n=50) at T=1000, which arm does UCB prefer?

- **Concept: Regret Decomposition via Discretization (Lemma 1)**
  - Why needed here: Shows R_T ≤ TL/K^(Md) + R^π_K(T), where the first term is approximation error from discretizing a continuous space.
  - Quick check question: Why does the optimal K balance TL/K^(Md) against K^(Md)√(T log T)?

- **Concept: Covering Dimension**
  - Why needed here: The zooming algorithm's bound O(T^(Md+1)/(Md+2) · (c log T)^(1/(Md+2))) depends on the covering dimension—the smallest d such that N_r(X) ≤ c·r^(-d) balls cover the space.
  - Quick check question: What is the covering dimension of [0,1]^2 under ℓ_2?

## Architecture Onboarding

- **Component map**: Discretization Module -> UCB Index Store -> Coordination Protocol -> Active Set Manager
- **Critical path**: 
  1. Initialize: Activate center (1/2,...,1/2); pre-agree on action ordering
  2. Per-round: Detect uncovered regions → activate arms → select via UCB_max + coordination → update indices
  3. (Problem B): Check elimination condition → signal via intentional deviation → all players prune desired set

- **Design tradeoffs**:
  - Uniform discretization: Simpler, but regret O(T^(2Md+1)/(2Md+2)) vs. zooming's O(T^(Md+1)/(Md+2))
  - Problem C has extra (log T)^(1/(Md+1)) factor vs. Problems A/B's (log T)^(1/(2(Md+1)))—worse dependence to handle full asymmetry
  - Assumption fragility: Each problem setting's algorithm breaks if information structure changes

- **Failure signatures**:
  - Players diverge to different arms (Problem A): Check reward synchrony; verify initial coordination phase completed
  - Optimal arm eliminated (Problem B): Verify confidence interval width scales correctly with √(log T / n_a)
  - Sublinear regret not achieved: Check Lipschitz constant L; verify discretization K satisfies equation (5) or (8)

- **First 3 experiments**:
  1. Single-player sanity check (M=1, d=1): Implement mCAB-A on f(x) = -x² + 1; verify regret ~ O(T^(2/3))
  2. Two-player coordination (M=2, d=1): Log both players' actions; verify they converge to same joint arm without communication
  3. Zooming vs. uniform: Run mZoom-A and mCAB-A on identical instance; plot cumulative regret to confirm improved rate

## Open Questions the Paper Calls Out

### Open Question 1
Can the zooming algorithm be extended to Problem C (asymmetry in both actions and rewards) while achieving improved regret bounds over uniform discretization? The paper applies zooming only to Problems A and B, omitting Problem C from the adaptive discretization treatment.

### Open Question 2
What are the information-theoretic lower bounds for each of the three information asymmetry settings in metric spaces? The paper proves upper bounds and claims Problem C achieves "nearly optimal" bounds, but provides no lower bound analysis.

### Open Question 3
Can these algorithms be extended to settings where the Lipschitz constant L is unknown? All proposed algorithms require knowledge of L, while prior work has solved Lipschitz bandits without the Lipschitz constant.

### Open Question 4
What is the computational and pre-coordination communication complexity of these algorithms as the number of players M scales? The pre-coordination phase requires agreeing on orderings over K^(Md) joint actions, potentially exponential in M.

## Limitations
- Implicit coordination mechanisms are fragile and break if reward realizations differ or actions are unobserved
- Zooming algorithm's doubling discretization scheme lacks complete algorithmic specification
- All theoretical guarantees assume known Lipschitz constant L with no experimental validation under mis-specification

## Confidence

**High Confidence**: Regret bounds for Problems A and B are well-supported by discretization and coordination analysis with rigorous mathematical derivations.

**Medium Confidence**: Improved zooming bounds and Problem C analysis are sound in principle but depend on more complex coordination protocols whose practical robustness requires validation.

**Low Confidence**: Practical efficiency of doubling discretization in continuous spaces and stability of coordination under reward noise or action observation failures need empirical testing.

## Next Checks

1. Implement mCAB-A on a 2-player, 1-D Lipschitz function with known optimum; verify both players converge to the same joint action without communication.

2. Test mZoom-B's interval-elimination signaling by injecting noise into reward observations; check whether players still correctly eliminate dominated arms.

3. Run mCAB-C and mZoom-C on a small instance (M=2, d=1); compare observed regret scaling against the predicted T^(2/3) and T^(3/4) rates to confirm the impact of full information asymmetry.