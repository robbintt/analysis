---
ver: rpa2
title: 'From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical
  Shifts at Scale'
arxiv_id: '2601.18524'
source_url: https://arxiv.org/abs/2601.18524
tags:
- chemical
- learning
- data
- shifts
- solvent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting nuclear magnetic
  resonance (NMR) chemical shifts using machine learning. Existing methods rely on
  limited, manually curated datasets with atom-level assignments.
---

# From Human Labels to Semi-Supervised Learning of NMR Chemical Shifts at Scale

## Quick Facts
- **arXiv ID**: 2601.18524
- **Source URL**: https://arxiv.org/abs/2601.18524
- **Reference count**: 39
- **Primary result**: Semi-supervised framework achieves 59.9% and 59.8% MAE reduction for 1H and 13C NMR chemical shift prediction by leveraging 1.5M+ unassigned literature spectra

## Executive Summary
This paper introduces a semi-supervised learning framework for NMR chemical shift prediction that combines limited atom-level labeled data with large-scale unassigned spectra from scientific literature. The key innovation is reformulating chemical shift prediction from unassigned spectra as a permutation-invariant set supervision problem, showing that under mild conditions the optimal bipartite matching reduces to a simple sorting-based loss. This enables stable training with millions of weakly supervised samples. The method achieves state-of-the-art accuracy, demonstrating robustness to unseen chemical environments and solvent effects while providing benchmarks for heteroatom shifts.

## Method Summary
The framework combines a small labeled dataset (NMRShiftDB2) with a large unlabeled dataset (ShiftDB-Lit) using a semi-supervised approach. 3D conformations are generated via RDKit MMFF optimization from SMILES strings. The model uses an SE(3)-equivariant Transformer backbone (15 layers, 64 heads, 512 dim) pre-trained on Uni-Mol. For unassigned spectra, the authors prove that optimal bipartite matching reduces to element-wise matching of sorted predicted and observed shifts, enabling efficient training. Solvent effects are captured through [CLS]-token injection using learnable embeddings. The total loss combines atom-level supervision (L_atom) with molecule-level weak supervision (L_mol) weighted by λ=16.

## Key Results
- **59.9% MAE reduction** for 1H and **59.8% MAE reduction** for 13C on large-scale ShiftDB-Lit test set compared to supervised baselines
- **46.8% MAE reduction** for 1H chemical shifts in DMSO-d6 when using [CLS]-token solvent conditioning versus global bias correction
- **Model collapse** occurs when training exclusively on weakly-supervised data without atom-level anchoring, demonstrating the necessity of mixed supervision

## Why This Works (Mechanism)

### Mechanism 1: Sorting-based loss for permutation-invariant supervision
The sorting-based loss enables efficient training on unassigned spectra by transforming bipartite matching into deterministic sorting. When the loss function satisfies l(x,y) = f(|x-y|) with f monotonically increasing and convex, the optimal bipartite matching between predicted and observed shifts reduces to matching sorted sequences element-wise. This converts an O(n³) Hungarian algorithm problem into O(n log n) sorting. Core assumption: The convexity and monotonicity conditions hold for the chosen loss function (MAE, MSE, and Huber satisfy this).

### Mechanism 2: Weak supervision as regularizer with atom-level anchoring
Combining limited atom-assigned labels with large-scale unassigned data improves generalization by using weak supervision as an effective regularizer. Atom-level loss anchors correct atom-peak correspondences, while molecule-level loss from unassigned spectra provides chemical diversity. Without anchoring, early prediction errors amplify through incorrect bipartite matching, causing model collapse. Core assumption: High-quality labeled data (NMRShiftDB2) provides correct correspondences that prevent degenerate solutions.

### Mechanism 3: Global solvent conditioning via [CLS]-token injection
Solvent effects act as global contextual bias rather than atom-local perturbations, best captured through [CLS]-token conditioning. Adding solvent embeddings to the [CLS]-token provides global context influencing all atom predictions through attention, capturing systematic solvent-induced biases. Core assumption: Solvent effects are predominantly global shifts rather than atom-specific perturbations.

## Foundational Learning

- **Bipartite matching and the assignment problem**: Understanding why matching predicted to observed shifts is expensive, and why sorting equivalence matters for scalability. *Quick check*: Given predicted shifts [2.1, 5.3, 1.8] and observed shifts [1.9, 5.1, 2.2], does sorting both and matching in order give the optimal matching under MAE?
- **Semi-supervised learning with weak labels**: The framework combines strong supervision (atom-level) with weak supervision (molecule-level sets); understanding when each contributes is critical. *Quick check*: What happens if you train using only the weakly-supervised molecule-level loss without any atom-level supervision?
- **Transformer [CLS] token as global representation**: Solvent conditioning relies on the [CLS]-token serving as a global context aggregator influencing downstream predictions. *Quick check*: Why might adding solvent information to [CLS] be more effective than adding it as a simple bias correction?

## Architecture Onboarding

- **Component map**: SMILES -> RDKit 3D conformation -> SE(3)-equivariant Transformer (15L, 64H, 512D) -> [CLS]-token with solvent embedding -> Per-atom chemical shift output
- **Critical path**: 1) Generate 3D conformations via RDKit, 2) Load Uni-Mol pre-trained weights, 3) Process labeled batch (B1=4) with atom-level supervision, 4) Process unlabeled batch (B2=16) with sorting-based molecule-level loss, 5) Combine with λ=16
- **Design tradeoffs**: Batch imbalance (B1=4 vs B2=16) reduces weak supervision variance but requires careful λ tuning; solvent grouping (3 categories) is pragmatic for imbalanced distribution but may miss nuanced effects; MMFF vs DFT conformations is faster but potentially less accurate
- **Failure signatures**: Model collapse when L_atom >> L_mol (predictions converge to degenerate solutions); λ too high causes atom-level accuracy degradation while molecule-level improves; incorrect matching if sorting not applied before loss computation
- **First 3 experiments**: 1) Reproduce supervised vs semi-supervised comparison on NMRShiftDB2 test set (target: 13.4% 1H, 19.6% 13C MAE reduction), 2) Ablate weak supervision: compare (a) labeled only, (b) labeled + unlabeled, (c) unlabeled only—confirm (c) collapses, 3) Validate solvent conditioning: measure per-solvent MAE with/without [CLS]-token injection, especially 46.8% DMSO-d6 improvement for 1H

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several limitations and areas for future work are identified through the experimental results and discussion.

## Limitations
- Model collapse occurs when training exclusively on weakly-supervised data, requiring a minimum threshold of high-quality labeled data as anchor
- The three-category solvent simplification (CDCl3/DMSO-d6/others) may miss nuanced solvent-specific chemical shift effects, particularly for heteroatoms
- Using RDKit MMFF optimization rather than quantum-calculated geometries may limit prediction precision for certain chemical environments

## Confidence
- **High confidence**: Sorting-based loss equivalence (proven mathematically in Appendix F), semi-supervised framework architecture, and overall MAE improvements on NMRShiftDB2 benchmarks
- **Medium confidence**: Generalizability to heteroatoms beyond 1H/13C (benchmarks provided but with smaller sample sizes), solvent conditioning effectiveness across all solvent types
- **Low confidence**: Performance on complex multi-solvent systems, long-range conformational effects on chemical shifts, and handling of overlapping peaks in real-world spectra

## Next Checks
1. **Sorting equivalence validation**: Implement and verify the sorting-based loss on synthetic datasets with known optimal matchings, confirming that O(n log n) sorting produces identical results to O(n³) Hungarian algorithm under tested loss functions
2. **Weak supervision ablation study**: Systematically vary λ from 0 to 32 while monitoring both L_atom and L_mol convergence, confirming the minimum labeled data threshold needed to prevent model collapse
3. **Heteroatom robustness testing**: Evaluate NMRNet performance on 15N and 19F chemical shifts using the small benchmark dataset, documenting performance degradation relative to 1H/13C and identifying systematic prediction errors