---
ver: rpa2
title: Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems
arxiv_id: '2509.20769'
source_url: https://arxiv.org/abs/2509.20769
tags:
- system
- retrieval
- reference
- bronze
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a retrieval-augmented generation (RAG) system
  for archaeological provenance analysis, integrating multimodal retrieval with large
  vision-language models to support expert reasoning. The system constructs a dual-modal
  knowledge base from reference texts and images, enabling raw visual, edge-enhanced,
  and semantic retrieval to identify stylistically similar objects.
---

# Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems

## Quick Facts
- arXiv ID: 2509.20769
- Source URL: https://arxiv.org/abs/2509.20769
- Reference count: 11
- Primary result: Multimodal RAG system achieves 46% high-quality archaeological provenance attributions while reducing expert cognitive load.

## Executive Summary
This study presents a retrieval-augmented generation (RAG) system for archaeological provenance analysis, integrating multimodal retrieval with large vision-language models to support expert reasoning. The system constructs a dual-modal knowledge base from reference texts and images, enabling raw visual, edge-enhanced, and semantic retrieval to identify stylistically similar objects. Expert evaluation on 30 Bronze Age artifacts shows the system produces meaningful outputs, with 46% of chronological and geographical attributions receiving high scores (3-4), though visual retrieval yields more mixed results (63% meaningful). The approach significantly reduces cognitive burden in navigating comparative corpora while providing concrete starting points for analysis.

## Method Summary
The system builds a knowledge base from archaeological PDFs, extracting both text (converted to Markdown) and images (plates, figures). Three parallel retrieval strategies run on a query image: raw visual similarity, edge-enhanced similarity (Gaussian-filtered), and CLIP-based semantic similarity. Retrieved candidates are aggregated, deduplicated, and ranked. A two-phase GPT-4o inference then processes each candidate individually to extract structured metadata, followed by cross-candidate synthesis to generate final provenance attributions with citations. The approach addresses low-quality scan challenges and heterogeneous visual representations in archaeological documentation.

## Key Results
- Expert evaluation shows 46% of provenance attributions received high scores (3-4 on 4-point scale)
- Visual retrieval quality was mixed, with 63% of outputs considered meaningful (scores 2-4)
- The two-phase VLM reasoning outperformed single-phase retrieval, reducing hallucination risks
- System successfully handled multilingual corpus (Chinese, English, Russian, French) through CLIP's language-agnostic embeddings

## Why This Works (Mechanism)

### Mechanism 1: Tri-Modal Retrieval Complementarity
- Claim: Parallel retrieval strategies capture orthogonal similarity signals that any single method would miss.
- Mechanism: Raw image retrieval matches surface visual features; edge-enhanced retrieval (via Gaussian filtering) matches structural contours critical for archaeological line drawings; CLIP-based semantic retrieval matches conceptual similarity even when visual appearance diverges. Candidates are aggregated via multiset union, deduplicated, and ranked.
- Core assumption: Archaeological documents contain heterogeneous visual representations (photographs, sketches, plates) that require modality-specific matching strategies.
- Evidence anchors:
  - [abstract] "enabling raw visual, edge-enhanced, and semantic retrieval to identify stylistically similar objects"
  - [section 2.2] Describes all three strategies running in parallel as "structural complements"
  - [corpus] Weak direct evidence; neighbor papers discuss multimodal RAG but not this specific tri-modal pattern
- Break condition: If reference corpus is homogeneous (e.g., only high-quality photographs), edge-enhanced retrieval provides diminishing returns.

### Mechanism 2: Two-Phase VLM Reasoning with Grounded References
- Claim: Decomposing inference into per-candidate analysis followed by cross-candidate synthesis reduces hallucination and improves attribution accuracy.
- Mechanism: Phase 1 processes each retrieved candidate individually, extracting structured metadata (site, period, similarity rationale, bibliographic reference) into JSON. Phase 2 aggregates all Phase 1 outputs with the target image to produce final provenance judgments with explicit source citations.
- Core assumption: VLMs reason more reliably over structured intermediate outputs than over raw, unfiltered retrieval pools.
- Evidence anchors:
  - [section 2.4] "This step significantly reduces the risk of hallucinations and alleviates token length constraints"
  - [section 3.2] Q2 (attribution generation) outperformed Q1 (retrieval quality): ~46% scored 3+ vs. ~33% for retrieval
  - [corpus] Belém et al. (NAACL 2025, cited as [1]) documents hallucination risks in multi-document summarization, supporting the decomposition strategy
- Break condition: If retrieved candidates are all low-relevance, Phase 2 synthesis amplifies noise rather than signal.

### Mechanism 3: Dual-Modal Knowledge Base from Low-Quality Scans
- Claim: Extracting both text and images from scanned PDFs enables retrieval even when OCR is unreliable.
- Mechanism: Source PDFs are converted to structured Markdown for text retrieval. Additionally, figures, plates, and visual layouts are extracted and indexed separately, creating an image repository for visual querying when text extraction fails.
- Core assumption: Archaeological reference materials are often low-quality scans where text extraction is unreliable but visual content remains usable.
- Evidence anchors:
  - [section 2.1] "many PDFs are low-quality scans rather than digitized texts. To address this, we additionally extract and index figures, plates, and visual layouts"
  - [corpus] DO-RAG and HM-RAG neighbor papers emphasize handling heterogeneous data but do not specifically address scan quality issues
- Break condition: If scans are too degraded for both OCR and visual feature extraction, both modalities fail.

## Foundational Learning

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: Enables semantic retrieval by embedding images and text into a shared vector space, allowing matching based on conceptual similarity rather than pixel-level comparison.
  - Quick check question: Can you explain why CLIP might retrieve a photograph of a bronze knife when querying with a line drawing of a similar knife, even if their pixel patterns differ significantly?

- Concept: RAG (Retrieval-Augmented Generation)
  - Why needed here: Grounds VLM outputs in external knowledge, providing source attribution and reducing fabrication. Essential for scholarly applications requiring provenance and citations.
  - Quick check question: What specific failure mode does RAG address compared to a standalone VLM asked to identify an artifact's origin?

- Concept: Gaussian Filtering for Edge Detection
  - Why needed here: Archaeological catalogs frequently use line drawings rather than photographs. Edge-enhanced preprocessing extracts structural contours that match sketch-style references.
  - Quick check question: Why would a raw pixel similarity search fail when querying a photograph against a database of archaeological line drawings?

## Architecture Onboarding

- Component map:
  Knowledge Base (PDFs → Markdown + Images) → Retrieval Module (Raw + Edge + CLIP) → Aggregation Layer → Inference Module (Phase 1 → Phase 2)

- Critical path: Query image → three retrieval branches → aggregation → VLM Phase 1 → VLM Phase 2 → structured provenance output

- Design tradeoffs:
  - Retrieval breadth vs. precision: Three strategies increase recall but introduce noise; filtering to top-m balances coverage with relevance.
  - VLM token limits vs. context richness: Phase 1 compression into JSON metadata enables Phase 2 to process more candidates without exceeding context windows.
  - Multilingual corpus (Chinese, English, Russian, French) vs. retrieval consistency: CLIP provides language-agnostic visual-semantic alignment but may exhibit uneven performance across linguistic contexts.

- Failure signatures:
  - Retrieval returns visually similar but culturally unrelated objects (e.g., modern reproductions misclassified as ancient)
  - Phase 2 synthesizes conflicting Phase 1 outputs into overconfident but incorrect attributions
  - Edge-enhanced retrieval fails on images with poor contrast or heavy visual noise
  - Citations reference incorrect page numbers when PDF-to-Markdown conversion introduces layout errors

- First 3 experiments:
  1. Ablation study: Run retrieval with each strategy individually (raw-only, edge-only, CLIP-only) vs. full tri-modal aggregation on a held-out artifact set. Measure precision@k and attribution accuracy.
  2. Phase analysis: Compare Phase 2 outputs when given full Phase 1 JSON vs. raw candidate text/image pairs directly. Quantify hallucination rate and citation accuracy.
  3. Corpus quality stress test: Intentionally degrade a subset of reference PDFs (add noise, reduce resolution) and measure retrieval performance drop across text vs. image modalities.

## Open Questions the Paper Calls Out

- Open Question 1: How can dynamic expert evaluation policies be integrated directly into the retrieval or generation loop to enhance reliability?
  - Basis in paper: [explicit] The conclusion states that future work will explore "integrating expert evaluation policies directly into the system to further enhance reliability and domain alignment."
  - Why unresolved: The current system relies on post-hoc expert evaluation of outputs. There is no mechanism for expert feedback to dynamically adjust retrieval weights or generation constraints during the inference process.
  - What evidence would resolve it: A modified system architecture featuring a feedback loop where expert validation of intermediate results improves subsequent retrieval accuracy or VLM confidence scores.

- Open Question 2: Does the high performance in attribution generation imply robust reasoning, or does it mask hallucination when retrieval quality is low?
  - Basis in paper: [inferred] The paper notes a performance gap where VLM synthesis (Q2) achieved higher meaningful scores (46% at Score 3+) than the visual retrieval component (Q1), despite the VLM depending on that retrieved context.
  - Why unresolved: It is unclear if the VLM is correctly synthesizing weak visual signals or if it is confabulating plausible scholarly narratives based on insufficient or irrelevant retrieved images.
  - What evidence would resolve it: An ablation study correlating the quality of retrieved candidates (Q1 scores) with the accuracy of the final attribution (Q2 scores) to determine the generation's dependence on retrieval success.

- Open Question 3: How does the candidate aggregation strategy resolve conflicts when raw image, edge-enhanced, and semantic retrievals return mutually exclusive cultural attributions?
  - Basis in paper: [inferred] Section 2.3 aggregates candidates via multisets, but Section 2.4 requires the VLM to determine a "most likely" site from these candidates without a defined mechanism for resolving contradictory evidence.
  - Why unresolved: The paper describes the aggregation of top-m results but does not detail how the system handles scenarios where edge-detection points to one culture (e.g., Karasuk) while semantic retrieval points to another (e.g., Ordos).
  - What evidence would resolve it: A failure analysis specific to "cross-modal conflict" cases, quantifying how often the VLM selects the majority retrieval modality versus synthesizing a hybrid (or incorrect) conclusion.

## Limitations
- The system's performance heavily depends on the quality and coverage of the reference corpus, with no mechanism for handling entirely novel artifact types.
- Current expert evaluation is post-hoc rather than integrated into the system loop, limiting dynamic adaptation.
- No clear conflict resolution mechanism exists when different retrieval modalities produce contradictory cultural attributions.

## Confidence
- Mechanism 1: High - Clear description of tri-modal retrieval with supporting evidence from paper
- Mechanism 2: High - Well-documented two-phase VLM approach with explicit citations supporting the strategy
- Mechanism 3: Medium - Described but less detailed than other mechanisms, relies more on inference
- Attribution accuracy: Medium - Expert evaluation shows promising results but limited sample size (30 artifacts)
- System integration: High - Architecture is clearly specified with reproducible components

## Next Checks
1. Validate CLIP embedding quality on archaeological images by testing retrieval precision on known matches
2. Compare Phase 2 attribution accuracy when using structured JSON vs. raw text inputs from Phase 1
3. Test edge-enhanced retrieval performance on degraded archaeological line drawings vs. modern photographs