---
ver: rpa2
title: On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation
arxiv_id: '2509.18822'
source_url: https://arxiv.org/abs/2509.18822
tags:
- policy
- convergence
- lemma
- td-pmd
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the convergence of policy mirror descent with
  temporal difference (TD) evaluation, addressing the question of whether PMD still
  converges when action values are approximated using TD evaluation instead of exact
  Monte Carlo simulation. The authors develop novel monotonicity and shift invariance
  arguments to prove that TD-PMD with constant step size achieves dimension-free O(1/T)
  sublinear convergence for any initialization, answering an open question from prior
  work.
---

# On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation

## Quick Facts
- arXiv ID: 2509.18822
- Source URL: https://arxiv.org/abs/2509.18822
- Reference count: 40
- Primary result: TD-PMD with constant step size achieves dimension-free O(1/T) sublinear convergence for any initialization

## Executive Summary
This paper establishes convergence guarantees for policy mirror descent (PMD) when action values are evaluated using one-step temporal difference (TD) instead of exact Monte Carlo simulation. The key challenge is that TD evaluation introduces bias since V^k ≠ V^{π^k}, breaking standard PMD analysis. The authors develop novel monotonicity and shift invariance arguments to prove that TD-PMD with constant step size achieves dimension-free O(1/T) sublinear convergence for any initialization. They further show that with adaptive step sizes, TD-PMD achieves γ-rate linear convergence, and establish sample complexity improvements over standard PMD in the generative model setting.

## Method Summary
The method implements TD-PMD where policy updates use mirror descent on Q^k values computed via one-step TD evaluation, and value function updates use one Bellman step T^{π^{k+1}} V^k. Two specific instances are analyzed: TD-PQA (projected gradient descent) and TD-NPG (natural policy gradient). For general initialization, the algorithm constructs an auxiliary shifted sequence to enable convergence analysis. The sample-based version uses independent samples from a generative model to estimate Q^k and V^{k+1}, avoiding the need for full trajectory sampling required by Monte Carlo PMD.

## Key Results
- TD-PMD with constant step size achieves dimension-free O(1/T) sublinear convergence for any initialization
- γ-rate linear convergence established with adaptive step sizes that approximate value iteration
- Sample complexity of Õ(ε^{-2}|S||A|(1-γ)^{-7}) improves upon PMD's dependence on (1-γ)^{-8}
- Both TD-PQA and TD-NPG instances converge under the same theoretical framework

## Why This Works (Mechanism)

### Mechanism 1: Monotonicity and Shift Invariance for Arbitrary Initialization
The key insight is that for "good" initialization (T^{π_0} V^0 ≥ V^0), monotonicity holds: V^* ≥ V^{π^{k+1}} ≥ V^{k+1} ≥ V^k. For general initialization, constructing an auxiliary sequence Ṽ^0 = V^0 - κ_0·1 where κ_0 shifts values to ensure T^{π_0} Ṽ^0 ≥ Ṽ^0 preserves policy iterates (π̃^k = π^k) while enabling convergence analysis. The γ^T κ_0 residual term converges to zero, guaranteeing convergence despite potentially non-monotonic behavior in the original sequence.

### Mechanism 2: Linear Convergence via Adaptive Step Sizes Approximating Value Iteration
Large adaptive step sizes η_k ≥ ||D^{π̃^k}_{π^k}||_∞ / (c·γ^{2k+1}) force the TD update T^{π^{k+1}} V^k to closely approximate the value iteration update TV^k. Since VI converges at γ-rate and the divergence term is controlled by the step size bound, TD-PMD inherits γ-rate convergence. This requires the step size to grow exponentially as γ^{-2k}, which the analysis shows is feasible while maintaining bounded Bregman divergence.

### Mechanism 3: Sample Efficiency via One-Step TD Avoiding Trajectory Sampling
TD-PMD uses only one-step lookahead Q^k(s,a) = r(s,a) + γ·E_{s'}[V^k(s')], eliminating the need for O((1-γ)^{-1})-length trajectory sampling required by Monte Carlo PMD. The bias from V^k ≠ V^{π^k} doesn't prevent convergence, and the sample complexity improves from Õ(ε^{-2}|S||A|(1-γ)^{-8}) to Õ(ε^{-2}|S||A|(1-γ)^{-7}).

## Foundational Learning

- **Bellman Operators as γ-Contractions**
  - Why needed: All convergence proofs rely on ||T^π V - T^π V'||_∞ ≤ γ||V - V'||_∞ and the fixed-point property T^π V^π = V^π
  - Quick check: Why does the contraction property guarantee that value iteration converges exponentially?

- **Three-Point Descent Lemma (Bregman Divergence)**
  - Why needed: The policy update analysis requires: η⟨π^{k+1} - p, Q^k⟩ ≥ D^{π^{k+1}}_{π^k} + D^p_{π^{k+1}} - D^p_{π^k} for any p ∈ Δ(A)
  - Quick check: Setting p = π^* and p = π^k yields what two key inequalities?

- **Generalized Performance Difference Lemma**
  - Why needed: Unlike classical versions, this applies to arbitrary V (not just V^π): V^π(μ) - V(μ) = (1-γ)^{-1}[T^π V - V](d^π_μ)
  - Quick check: How does this enable analysis when V^k ≠ V^{π^k}?

## Architecture Onboarding

- **Component map:** V^0, π^0 → TD Evaluator (Q^k, V^{k+1}) → Policy Improver (π^{k+1}) → Step Size Controller (η_k) → [Sample Estimator (M_Q, M_V) in stochastic setting]

- **Critical path:**
  1. Initialize V^0 ∈ R^{|S|} (any), π^0 ∈ Π (any)
  2. For k = 0 to T-1: (a) compute Q^k from V^k; (b) update π^{k+1} via mirror descent; (c) apply one TD step V^{k+1} = T^{π^{k+1}} V^k
  3. Return π^T

- **Design tradeoffs:**
  - Constant η: Simpler implementation, O(1/T) rate, works for all initializations
  - Adaptive η: Requires computing ||D^{π̃^k}_{π^k}||_∞, achieves γ-rate linear convergence
  - Q-TD-PMD variant (Appendix G): Maintains Q^k directly; similar complexity but different proof structure

- **Failure signatures:**
  - Slow convergence with small η: Expected O(1/T); not a bug
  - Non-monotonic ||V^* - V^k|| with poor initialization: Expected behavior; convergence still guaranteed
  - Error floor in stochastic setting: Check M_Q, M_V meet Lemma 4.2 thresholds

- **First 3 experiments:**
  1. Reproduce Figure 1 (sublinear convergence): Implement TD-PQA and TD-NPG with η = 0.1, V^0 = 0, π^0 = uniform on random MDP (|S|=50, |A|=10, γ=0.95). Plot ||V^* - V^T||_∞ and ||V^* - V^{π^T}||_∞ vs. T. Expect O(1/T) decay.
  2. Validate shift invariance (general initialization): Set V^0 uniformly random in [0, 1/(1-γ)], π^0 = uniform. Compute κ_0, construct shifted sequence. Verify: (a) ||V^* - V^k|| may increase initially, (b) ||V^* - Ṽ^k|| is monotonic, (c) both converge to same limit.
  3. Sample complexity comparison: Implement sample-based TD-PMD (Algorithm 2) and Monte Carlo PMD. Target ε = 0.01 with γ = 0.9. Set T = Õ((1-γ)^{-1}), M_Q = M_V = Õ(ε^{-2}(1-γ)^{-6}). Count total samples; expect TD-PMD uses ~1/(1-γ) fewer than PMD.

## Open Questions the Paper Calls Out

- **Can the convergence analysis be extended to entropy-regularized MDPs?**
  - Basis: Section 5 states it is "likely to extend the analysis to the scenario with entropy regularization"
  - Why unresolved: The current framework would need adaptation to handle the altered policy update landscape from entropy regularization
  - What evidence would resolve it: Theoretical proof demonstrating convergence rates for TD-PMD with entropy regularization

- **Does TD-PMD converge under online Markovian sampling schemes?**
  - Basis: Section 5 states it is "interesting to investigate the convergence of TD-PMD under other sampling schemes, for example the Markovian sampling"
  - Why unresolved: Current guarantees rely on i.i.d. samples from a generative model, while Markovian sampling introduces correlation and bias dependencies
  - What evidence would resolve it: Convergence bounds for online settings with samples drawn from the induced Markov chain

- **Is the (1-γ)^{-7} dependence in sample complexity optimal?**
  - Basis: The paper highlights improving from O((1-γ)^{-8}) to O((1-γ)^{-7}) but provides no lower bound
  - Why unresolved: Unclear if the exponent of 7 is the fundamental limit for one-step TD evaluation methods
  - What evidence would resolve it: Lower bound analysis or improved upper bound reducing dependence on (1-γ)

## Limitations

- The shift-invariance arguments may not generalize to non-tabular settings or when using function approximation
- Adaptive step-size analysis requires explicit computation of ||D^{π̃^k}_{π^k}||_∞, which may be computationally expensive in large state spaces
- Sample complexity bounds assume access to a generative model, limiting applicability to real-world online RL settings

## Confidence

- **High:** Sublinear O(1/T) convergence for constant step sizes (Theorem 3.4) - proof follows standard monotonic analysis
- **Medium:** γ-rate linear convergence for adaptive step sizes (Theorem 3.9) - requires careful tracking of contraction arguments
- **Medium:** Sample complexity improvement (Theorem 4.1) - hinges on one-step TD evaluation bias being acceptable

## Next Checks

1. **Reproduce general initialization convergence:** Implement Algorithm 1 with V^0 randomly initialized in [0, 1/(1-γ)]. Verify that V^T and Ṽ^T converge to the same limit despite potentially non-monotonic ||V^* - V^k||_∞ during intermediate iterations.

2. **Stress test adaptive step sizes:** Implement the γ-rate variant (Theorem 3.9) with adaptive η_k. Monitor both ||D^{π̃^k}_{π^k}||_∞ computation cost and convergence speed. Test whether convergence degrades when step sizes are capped below the theoretical requirement.

3. **Validate sample complexity bounds:** Implement Algorithm 2 with varying M_Q, M_V values. Measure actual ε achieved versus the predicted bound. Confirm that using one-step TD evaluation reduces sample requirements by approximately 1/(1-γ) compared to trajectory sampling.