---
ver: rpa2
title: Improving End-to-End Training of Retrieval-Augmented Generation Models via
  Joint Stochastic Approximation
arxiv_id: '2508.18168'
source_url: https://arxiv.org/abs/2508.18168
tags:
- training
- retriever
- jsa-rag
- passage
- vrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of end-to-end training for retrieval-augmented
  generation (RAG) models, where marginalization over discrete latent variables (relevant
  passages) is required. Traditional approaches like top-K marginalization and variational
  RAG (VRAG) suffer from biased or high-variance gradient estimates.
---

# Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation

## Quick Facts
- arXiv ID: 2508.18168
- Source URL: https://arxiv.org/abs/2508.18168
- Reference count: 11
- Key outcome: JSA-RAG significantly outperforms vanilla RAG and VRAG on open-domain QA and knowledge-grounded dialog tasks, achieving +4.1% Exact Match on TQA and +10.3% BLEU-4 on DoQA

## Executive Summary
This paper addresses the fundamental challenge of end-to-end training for retrieval-augmented generation (RAG) models, where marginalization over discrete latent variables (relevant passages) is required. Traditional approaches like top-K marginalization and variational RAG (VRAG) suffer from biased or high-variance gradient estimates. The authors propose Joint Stochastic Approximation (JSA) based training for RAG, called JSA-RAG, which introduces an auxiliary posterior retriever and employs Metropolis independence sampling to approximate the E-step, followed by parameter updates similar to supervised training.

The method is evaluated on five datasets across two tasks: open-domain question answering and knowledge-grounded dialogs. Results show that JSA-RAG significantly outperforms both vanilla RAG and VRAG in terms of generation quality (e.g., +4.1% Exact Match on TQA, +10.3% BLEU-4 on DoQA) and retrieval performance (e.g., +8.5% R@1 on NQ, +1.7% R@1 on OR-QuAC). Further analysis demonstrates the efficacy of JSA-RAG in terms of generation quality, retrieval accuracy, and low-variance gradient estimates.

## Method Summary
The authors propose Joint Stochastic Approximation (JSA) for end-to-end training of RAG models. The key insight is to introduce an auxiliary posterior retriever that approximates the true posterior distribution over relevant passages. During training, they use Metropolis independence sampling to draw samples from this approximate posterior (E-step), then update both the retriever and generator parameters using these samples (M-step). This approach avoids the high-variance gradient estimates of REINFORCE and the biased estimates of top-K marginalization. The method is designed to be more stable and effective than existing approaches like VRAG, while maintaining computational efficiency comparable to standard RAG training.

## Key Results
- JSA-RAG achieves +4.1% Exact Match improvement on TQA dataset compared to vanilla RAG
- JSA-RAG shows +10.3% BLEU-4 improvement on DoQA dataset for knowledge-grounded dialogs
- JSA-RAG demonstrates +8.5% R@1 improvement on NQ dataset and +1.7% R@1 on OR-QuAC dataset for retrieval performance

## Why This Works (Mechanism)
JSA-RAG works by introducing an auxiliary posterior retriever that approximates the true posterior distribution over relevant passages. This auxiliary retriever is trained jointly with the main retriever and generator through a stochastic approximation framework. The Metropolis independence sampling allows for more stable gradient estimates compared to traditional REINFORCE methods, while the joint training ensures that all components are optimized together rather than in a pipeline fashion. This addresses the key challenge in RAG training where the retriever and generator must be trained together despite the discrete nature of the retrieval operation.

## Foundational Learning
- **Metropolis independence sampling**: A Markov Chain Monte Carlo method for sampling from complex distributions; needed to approximate the posterior over relevant passages without high-variance REINFORCE estimates; quick check: verify acceptance rates are reasonable (typically 0.2-0.5)
- **Stochastic approximation**: Iterative methods for finding roots of equations with noisy observations; needed to jointly update retriever and generator parameters; quick check: monitor parameter changes across iterations for stability
- **Variational inference**: Framework for approximating intractable posteriors; needed as a baseline comparison and to understand VRAG limitations; quick check: compare ELBO values across methods
- **Discrete latent variable marginalization**: Integration over discrete choices (relevant passages); needed because exact marginalization is intractable in RAG; quick check: verify that top-K approximation introduces bias
- **Joint training vs pipeline training**: End-to-end optimization versus separate component training; needed to achieve better alignment between retrieval and generation; quick check: compare performance when training components separately
- **E-step and M-step alternation**: Expectation-Maximization framework components; needed to structure the alternating optimization between sampling and parameter updates; quick check: ensure E-step produces meaningful samples for M-step updates

## Architecture Onboarding

Component Map:
Retriever (main) -> Auxiliary Retriever -> Generator -> Loss Function
Data -> Metropolis Sampler -> Parameter Updates

Critical Path:
Input query → Main retriever → Auxiliary retriever (posterior approximation) → Metropolis sampling → Generator → Output + Loss → Parameter updates

Design Tradeoffs:
- Auxiliary retriever adds computational overhead but provides better posterior approximation
- Joint training requires careful balancing of learning rates between components
- Metropolis sampling introduces hyperparameters (proposal distribution) that affect performance
- Tradeoff between sample quality and computational efficiency in the E-step

Failure Signatures:
- High variance in gradient estimates indicates poor posterior approximation
- Degraded retrieval performance suggests retriever-generator misalignment
- Mode collapse in generated outputs indicates insufficient diversity in sampled passages
- Slow convergence suggests learning rate issues or poor initialization of auxiliary retriever

First Experiments:
1. Compare retrieval performance (R@1, R@5) of main retriever with and without auxiliary retriever
2. Measure gradient variance across different sampling methods (top-K, VRAG, JSA-RAG)
3. Ablation study: train retriever and generator separately vs jointly with JSA

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily on benchmark datasets with limited analysis of truly open-domain settings
- Claims of low-variance gradient estimates lack comparison to other gradient estimation techniques like REINFORCE or Gumbel-Softmax
- Limited discussion of practical deployment considerations such as inference latency or memory requirements
- Sensitivity to sampling hyperparameters and failure modes when auxiliary retriever's posterior approximation is poor is not deeply explored

## Confidence
- High confidence in core experimental results and statistical significance claims
- Medium confidence in generality of improvements across different domains and tasks beyond tested datasets
- Medium confidence in practical efficiency claims given limited deployment analysis

## Next Checks
1. Evaluate JSA-RAG on a truly open-domain retrieval task with a large-scale corpus (e.g., 50M+ passages) to assess scalability and robustness to noisy retrieval
2. Conduct an ablation study isolating contributions of auxiliary retriever versus joint training objective, including comparisons to simpler two-stage fine-tuning approaches
3. Analyze failure cases systematically to understand when and why JSA-RAG outperforms or underperforms baselines, particularly in scenarios with ambiguous queries or contradictory evidence