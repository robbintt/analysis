---
ver: rpa2
title: Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized
  Linear Regression
arxiv_id: '2502.09106'
source_url: https://arxiv.org/abs/2502.09106
tags:
- lemma
- diag
- have
- theorem
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the scaling law for Stochastic Gradient Descent
  (SGD) in a quadratically parameterized linear regression model. The model is designed
  to capture feature learning, a key aspect of deep neural networks' success, which
  is absent in traditional linear models.
---

# Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized Linear Regression

## Quick Facts
- arXiv ID: 2502.09106
- Source URL: https://arxiv.org/abs/2502.09106
- Reference count: 3
- Primary result: Establishes piecewise power-law scaling for SGD in quadratic models, showing feature learning can outperform linear models when ground truth decays slower than data spectrum.

## Executive Summary
This paper analyzes the scaling law for Stochastic Gradient Descent (SGD) in a quadratically parameterized linear regression model designed to capture feature learning dynamics. The model exhibits a two-phase learning process: an "adaptation" stage where SGD implicitly identifies the effective dimension set, followed by an "estimation" stage achieving global convergence. The study establishes upper bounds for excess risk that follow piecewise power laws with respect to both model size and sample size, demonstrating that quadratic parameterization can outperform linear models in certain regimes when the ground truth spectrum decays slower than the data covariance spectrum.

## Method Summary
The authors analyze SGD in a quadratically parameterized linear regression model where the output function is $f(x) = \langle x, v \odot^2 \rangle$. They consider infinitely dimensional data with power-law decaying eigenvalues and ground truth parameters. The analysis establishes upper bounds for excess risk, showing it follows a piecewise power law. The learning process is characterized into two distinct stages: an "adaptation" phase where SGD identifies the effective dimension set through implicit feature selection, and an "estimation" phase where global convergence is achieved using a geometrically decaying step size.

## Key Results
- SGD in quadratic models exhibits piecewise power-law scaling with both model size and sample size
- The effective dimension $D \asymp \min\{T^{1/\max\{\beta, (\alpha+\beta)/2\}}, M\}$ governs generalization error rather than ambient dimension
- Quadratic models achieve optimal rates in certain regimes and outperform linear models when $\alpha > \beta$
- The two-phase learning dynamic (adaptation + estimation) is rigorously established for the specific quadratic parameterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Quadratic parameterization enables implicit feature selection via a two-phase learning dynamic.**
- Mechanism: The quadratic model ($f(x) = \langle x, v \odot^2 \rangle$) undergoes an "adaptation" phase where SGD implicitly identifies a subset of coordinates (the effective dimension set $S$) that align with the ground truth. It "locks in" these features before proceeding to the "estimation" phase, allowing the model to ignore noise in irrelevant dimensions.
- Core assumption: The initialization $v_0$ is near zero; the data covariance spectrum and ground truth decay follow power laws ($\lambda_i \sim i^{-\alpha}$ and $v^*_i$ decays).
- Evidence anchors:
  - [abstract] "characterizes the learning process into two stages: an 'adaptation' stage where SGD identifies the effective dimension set..."
  - [section 5.1] "In Phase I (Adaptation), Algorithm 1 implicitly identifies the first D coordinates as the effective dimension set S... based on the initial conditions."
- Break condition: If initialization is too large or the noise variance $\sigma^2$ overwhelms the signal, the adaptation phase may fail to identify the correct effective dimension, leading to convergence to suboptimal local minima.

### Mechanism 2
- Claim: **Quadratic parameterization rescales gradients to achieve faster bias convergence when $\alpha > \beta$.**
- Mechanism: During the "estimation" phase, the optimization dynamics approximate SGD on a linear model with features rescaled by the ground truth magnitude ($v^*_i$). This acts as an adaptive step size, effectively allowing the model to take larger steps along coordinates with stronger signal. This results in an excess risk rate of $\tilde{O}(T^{-\frac{2\beta-2}{\alpha+\beta}})$, which strictly outperforms the linear model's rate when the true parameter spectrum decays slower than the data covariance spectrum.
- Core assumption: The relationship between data decay $\alpha$ and truth decay $\beta$ determines the regime ($\alpha > \beta$ vs $\alpha \le \beta$).
- Evidence anchors:
  - [abstract] "the quadratic model... outperforms linear models in others [regimes]."
  - [section 5.2] "The adaptive rescale size $v^*_{1:M}$ enables the quadratic model to achieve accelerated convergence rates compared to its linear counterpart."
- Break condition: If $\alpha \le \beta$ (true parameter aligns with covariance), this mechanism provides no speedup over linear models.

### Mechanism 3
- Claim: **Piecewise power-law scaling is governed by an "effective dimension" rather than ambient dimension.**
- Mechanism: The generalization error scales with a critical dimension $D \asymp \min\{T^{1/\max\{\beta, (\alpha+\beta)/2\}}, M\}$, rather than the full model size $M$. Increasing model size $M$ beyond this threshold yields diminishing returns, while increasing sample size $T$ reduces the approximation error and variance terms piecewise.
- Core assumption: Power-law decay assumptions (Assumption 3.3).
- Evidence anchors:
  - [section 4] "The error... behaves as $\frac{1}{M^{\beta-1}} + \frac{\sigma^2 D}{T} + \dots$ where $D = \min\{T^{1/\dots}, M\}$ serves as the effective dimension."
- Break condition: If the power-law assumptions are violated (e.g., sparse noise or uniform spectrum), the $D$ calculation and resulting bounds likely fail.

## Foundational Learning

- Concept: **Power-Law Decay (Spectral & Source Conditions)**
  - Why needed here: The entire theoretical result hinges on the specific rates at which data eigenvalues ($\lambda_i \sim i^{-\alpha}$) and ground truth coefficients ($v^*_i$) decay. Without this, the "effective dimension" cannot be defined.
  - Quick check question: If a dataset had uniform eigenvalues ($\lambda_i \approx c$), would the "adaptation" phase work as described? (Likely No).

- Concept: **Quadratic Parameterization (Diagonal Networks)**
  - Why needed here: This is the functional form $f(x) = \langle x, v^2 \rangle$. It creates non-convexity that enables the "feature learning" (implicit selection) absent in linear regression.
  - Quick check question: Does the parameter $v$ appear linearly or quadratically in the output function $f(x)$? (Quadratically).

- Concept: **SGD with Geometric Step Decay**
  - Why needed here: The algorithm specifically requires a constant step size for "adaptation" followed by a geometric decay for "estimation." Standard polynomial decay might not guarantee the two-phase separation.
  - Quick check question: Why is the step size schedule split into two distinct phases? (Phase I needs exploration/stability; Phase II needs aggressive refinement).

## Architecture Onboarding

- Component map: Data Generator -> Model (Quadratic) -> Optimizer (SGD with Two-Phase Schedule)
- Critical path:
  1. **Initialization**: Set $v_0$ small (near zero).
  2. **Phase I (Adaptation)**: Run SGD with constant step size $\eta$. The model implicitly selects the top $D$ coordinates to fit.
  3. **Phase II (Estimation)**: Switch to geometric decay. The model converges linearly to the ground truth in the effective subspace.

- Design tradeoffs:
  - **Linear vs. Quadratic**: Use this quadratic model specifically when you suspect the target signal has "heavy tails" ($\beta$ small) relative to the data noise/decay ($\alpha$ large). If $\alpha \le \beta$, a standard linear model is likely simpler and optimal.
  - **Model Size ($M$) vs. Sample Size ($T$)**: Increasing $M$ is only helpful up to the effective dimension $D$. Once $M > D$, resources should shift to collecting more samples $T$.

- Failure signatures:
  - **Saturation**: If error plateaus despite increasing samples, check if $M$ is too small ($M < D$).
  - **Divergence in Phase I**: If variance explodes early, step size $\eta$ is likely too large relative to the smallest eigenvalue in the effective set $S$.
  - **No Speedup vs Linear**: If $\alpha \le \beta$, the quadratic mechanism offers no advantage; switch to linear regression.

- First 3 experiments:
  1. **Verify Phase Transition**: Plot coordinate-wise magnitudes of $v$ over time. Confirm that a subset grows (Phase I) while others remain near zero, followed by convergence (Phase II).
  2. **Scaling Law Check**: Vary $T$ and $M$ independently. Plot $\log(\text{Excess Risk})$ vs $\log(T)$ and $\log(M)$. Verify the slopes match the predicted $\frac{2\beta-2}{\alpha+\beta}$ or $-\frac{1}{\beta}$ rates.
  3. **Spectral Mismatch Test**: Synthesize data where $\alpha \gg \beta$. Compare the final risk of this model against a standard linear regression baseline to demonstrate the claimed outperformance.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes infinite-dimensional power-law spectra for both data and ground truth, which is a strong theoretical construct.
- The bounds are upper bounds, and their tightness for specific parameter regimes (particularly edge cases where $\alpha \approx \beta$) is not established.
- The analysis assumes known power-law exponents, which are rarely available in practice.

## Confidence
- **High Confidence**: The characterization of the two-phase learning dynamic (adaptation + estimation) and the piecewise power-law scaling behavior.
- **Medium Confidence**: The specific excess risk rates and their claimed superiority over linear models.
- **Medium Confidence**: The practical relevance of the infinite-dimensional limit for finite, real-world datasets.

## Next Checks
1. **Empirical Phase Transition**: Train the quadratic model on synthetic data with controlled $\alpha$ and $\beta$ values. Plot the coordinate-wise parameter magnitudes over time to verify the existence and duration of the adaptation phase followed by the estimation phase.

2. **Rate Verification**: Systematically vary $T$ and $M$ in synthetic experiments. Plot log(excess risk) vs log(T) and log(M) to empirically verify the predicted slopes match theoretical predictions across different $\alpha/\beta$ regimes.

3. **Architecture Generalization**: Test whether similar two-phase dynamics and scaling laws emerge in other non-linear models like diagonal neural networks with ReLU activations or other polynomial parameterizations. This would validate if the mechanism is specific to the quadratic form or more general.