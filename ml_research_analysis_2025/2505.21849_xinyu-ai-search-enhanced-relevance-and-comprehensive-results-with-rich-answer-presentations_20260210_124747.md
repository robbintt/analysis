---
ver: rpa2
title: 'Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer
  Presentations'
arxiv_id: '2505.21849'
source_url: https://arxiv.org/abs/2505.21849
tags:
- query
- search
- answer
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Xinyu AI Search addresses the challenge of synthesizing fragmented
  information for complex queries in traditional and generative AI search engines.
  The system introduces a query-decomposition graph to break down complex queries
  into sub-queries, enabling stepwise retrieval and generation.
---

# Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations

## Quick Facts
- arXiv ID: 2505.21849
- Source URL: https://arxiv.org/abs/2505.21849
- Reference count: 40
- Outperforms eight existing technologies in human assessments across relevance, comprehensiveness, and insightfulness

## Executive Summary
Xinyu AI Search addresses the challenge of synthesizing fragmented information for complex queries in traditional and generative AI search engines. The system introduces a query-decomposition graph to break down complex queries into sub-queries, enabling stepwise retrieval and generation. It enhances retrieval diversity through multi-source aggregation and query expansion, while filtering and re-ranking strategies optimize passage relevance. Novel approaches include fine-grained built-in citation and rich answer presentations via timeline visualization and textual-visual choreography. Evaluated on real-world queries, Xinyu AI Search outperforms eight existing technologies in human assessments, excelling in relevance, comprehensiveness, and insightfulness. Ablation studies validate the necessity of its key sub-modules.

## Method Summary
Xinyu AI Search employs a unified fine-tuning framework across multiple models to optimize cost-efficiency. The system processes queries through a pipeline that begins with user intent understanding and query rewriting, followed by query-decomposition graph construction for complex queries. Multi-source retrieval with query expansion generates diverse passages, which undergo deduplication, selection, and re-ranking before generation. The answer generation follows the QDG structure, while parallel citation, timeline, and image matching modules enhance presentation quality. The architecture uses Qwen2.5 models for various tasks, with specific thresholds for chunking (350 chars, 25% overlap), deduplication (0.8 cosine threshold), and passage retention (top 70%).

## Key Results
- Outperforms eight existing technologies in human assessments across nine facets including relevance, comprehensiveness, and insightfulness
- Achieves high citation precision through fine-grained, sentence-level attribution using a two-stage SLM pipeline
- Demonstrates effectiveness of query-decomposition graph in handling complex multi-faceted information needs

## Why This Works (Mechanism)

### Mechanism 1: Query-Decomposition Graph for Complex Query Resolution
Decomposing complex queries into sub-queries enables stepwise retrieval and generation, improving comprehensiveness for multi-faceted information needs. A fine-tuned LLM classifies queries as simple or complex, splitting complex ones into up to 6 sub-queries with explicit parent-child dependencies. The graph structure enables hierarchical retrieval where parent results inform child query processing, preserving logical dependencies. Users' complex information needs can be decomposed into interdependent sub-questions that, when answered sequentially, provide comprehensive coverage. Queries that are genuinely simple or where decomposition increases user cognitive load without improving answer quality may break this mechanism.

### Mechanism 2: Multi-Stage Retrieval Optimization Pipeline
Combining retrieval diversity enhancement with aggressive noise filtering improves passage relevance while maintaining comprehensiveness. Query expansion generates 4 dimensions of retrieval queries; multi-source aggregation queries multiple search APIs. Post-retrieval, passages undergo deduplication (0.8 cosine threshold), keyword/TF-IDF selection (top 70%), and neural reranking to optimize context for the LLM. Irrelevant or redundant passages degrade LLM generation quality, and LLMs exhibit "lost in the middle" attention patterns benefiting from relevant content at prompt edges. Over-filtering may remove relevant but tangentially related information needed for comprehensive answers.

### Mechanism 3: Decoupled Citation and Multimodal Presentation Layer
Separating citation generation from answer generation and adding visual elements improves verifiability and reduces cognitive load. Two-stage SLM pipeline extracts entities (dates, locations, names) then identifies citations against retrieved documents. Timeline visualization clusters events (0.9 similarity) and presents chronologically. Textual-visual choreography uses Hungarian algorithm to match images to paragraphs via CLIP and reranker scores. Users benefit from fine-grained sentence-level citations and visual cognitive scaffolding that reduces information assimilation effort. Incorrect entity extraction leads to misattributed citations; irrelevant images distract from content.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Xinyu's architecture builds on RAG principles, combining search retrieval with LLM generation to address hallucination and stale knowledge.
  - Quick check question: Why does RAG help reduce LLM hallucinations compared to pure parametric generation?

- **Concept: Query Rewriting and Expansion**
  - Why needed here: Sections 3.1.2 and 3.3.1 describe these as critical retrieval enhancement techniques the system relies upon.
  - Quick check question: What's the difference between query rewriting (addressing ambiguities) and query expansion (generating supplementary queries)?

- **Concept: "Lost in the Middle" Attention Phenomenon**
  - Why needed here: Section 3.4.3 explicitly motivates passage reranking by citing research showing LLMs attend more to prompt edges.
  - Quick check question: Why might placing most relevant passages at prompt edges improve generation quality?

## Architecture Onboarding

- **Component map:** Query Input → User Intent Understanding → Query Rewriting → QDG Construction → Query Expansion → Multi-Source Retrieval → Content Filtering → Chunking (350 chars, 25% overlap) → Deduplication (0.8 threshold) → Selection (top 70% TF-IDF) → Reranking → Answer Generation → [Parallel: Citation | Timeline | Image Matching] → Response Assembly

- **Critical path:** QDG construction (determines scope) → Multi-source retrieval (coverage bottleneck) → Reranking (quality bottleneck) → Answer generation

- **Design tradeoffs:**
  - Small chunks (350) with high overlap vs. larger chunks: optimizes embeddings at cost of processing
  - Greedy deduplication vs. optimal MIS: trades solution quality for tractability
  - Two-stage SLM citation vs. single LLM: trades granularity for latency/cost
  - Async citation processing: reduces latency but may lag answer by one sentence

- **Failure signatures:** QDG validation failures → prompt/model degradation; Citation density drops → entity extraction issues; Empty timeline → temporal info missing from retrieval; Unmatched images → CLIP scores below threshold

- **First 3 experiments:**
  1. Validate QDG quality: Test 100 complex queries, measure sub-query validity and dependency correctness vs. human annotations.
  2. Ablate retrieval stages: Compare comprehensiveness/relevance with/without deduplication, selection, reranking.
  3. Benchmark citation precision: Sample 50 answers, manually verify inline citations against sources.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does language-specific fine-tuning improve performance over prompt translation in generative AI search systems?
- **Basis in paper:** [explicit] The conclusion states, "Future work will focus on enhancing multilingual capabilities," noting that the current English version relies on prompt conversion rather than dedicated fine-tuning.
- **Why unresolved:** The authors observe "surprisingly good performance" using prompt conversion, but they have not yet quantified the performance gap or the specific failure modes that language-specific fine-tuning would address.
- **Evidence to resolve it:** A comparative benchmark evaluating the English version against a variant fine-tuned on English corpora (e.g., MS MARCO) across relevance and factuality metrics.

### Open Question 2
- **Question:** Does the unified data preparation and model fine-tuning framework compromise the performance of individual sub-modules?
- **Basis in paper:** [inferred] The authors adopt a unified framework because "fine-tuning all models individually is prohibitively expensive," implying a trade-off between cost-efficiency and theoretical maximum performance.
- **Why unresolved:** While ablation studies show the components are necessary, the paper does not isolate whether a specialized training regime for individual modules (like the reranker) would yield superior results over the unified approach.
- **Evidence to resolve it:** An ablation study comparing the unified fine-tuning strategy against task-specific, isolated training regimes for the reranker and query decomposition modules.

### Open Question 3
- **Question:** How does the system balance latency and accuracy when handling "Chain" versus "Split" decomposition strategies in the Query-Decomposition Graph (QDG)?
- **Basis in paper:** [inferred] The paper defines "Chain" (iterative) and "Split" (parallel) decomposition, but the evaluation aggregates results without distinguishing performance differences based on the query complexity type.
- **Why unresolved:** It is unclear if the sequential nature of "Chain decomposition" introduces significant latency or error propagation compared to the parallel execution of "Split decomposition."
- **Evidence to resolve it:** A stratified evaluation reporting latency and relevance scores separately for queries identified as requiring sequential reasoning versus those requiring parallel information retrieval.

## Limitations
- Evaluation relies entirely on human assessment panels without transparency in panel composition or inter-rater reliability measures
- Heavy dependence on multiple proprietary APIs creates potential reproducibility barriers
- Evaluation only benchmarks against eight existing technologies without specifying their versions or implementation details

## Confidence
- **High confidence** in the technical architecture descriptions and fine-tuning procedures, as these contain specific model names, data sizes, and algorithmic thresholds
- **Medium confidence** in the performance claims, given the lack of statistical significance testing and limited sample size (300 queries across 8 domains)
- **Low confidence** in the evaluation methodology robustness, particularly around human assessment standardization and potential bias

## Next Checks
1. **Evaluate QDG Robustness**: Test the query-decomposition graph on 100 additional complex queries not in the original dataset, measuring sub-query validity rates and dependency accuracy against human expert annotations. Compare performance when decomposing queries with varying degrees of semantic complexity.

2. **Ablation Study Expansion**: Conduct systematic ablation of each retrieval optimization stage (deduplication, selection, reranking) using precision-recall curves and user preference studies. Test whether aggressive filtering (top 70% retention) consistently improves relevance without sacrificing comprehensiveness across different query types.

3. **Citation Precision Audit**: Implement a randomized audit of 100 generated answers, manually verifying each inline citation against its source document. Measure false positive rates and analyze failure patterns in the two-stage SLM citation pipeline to identify whether entity extraction or document matching is the primary bottleneck.