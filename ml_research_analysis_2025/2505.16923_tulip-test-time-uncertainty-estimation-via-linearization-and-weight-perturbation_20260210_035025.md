---
ver: rpa2
title: 'TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation'
arxiv_id: '2505.16923'
source_url: https://arxiv.org/abs/2505.16923
tags:
- tulip
- training
- network
- uncertainty
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TULiP, a post-hoc uncertainty estimator for
  out-of-distribution (OOD) detection in deep learning models. The method is theoretically
  grounded in linearized training dynamics and considers hypothetical weight perturbations
  before network convergence.
---

# TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation

## Quick Facts
- arXiv ID: 2505.16923
- Source URL: https://arxiv.org/abs/2505.16923
- Reference count: 40
- TULiP achieves state-of-the-art near-OOD detection performance on OpenOOD v1.5 benchmark

## Executive Summary
This paper introduces TULiP, a post-hoc method for estimating test-time uncertainty in deep learning models through linearization of training dynamics and weight perturbation sampling. The approach computes epistemic uncertainty by analyzing hypothetical weight perturbations without requiring access to training data. TULiP demonstrates strong performance for near-OOD detection across multiple image classification datasets, achieving top-1 or top-2 AUROC scores while maintaining robustness across different network architectures.

## Method Summary
TULiP estimates uncertainty by linearizing the training dynamics of a neural network and analyzing the sensitivity of predictions to weight perturbations. The method computes a theoretical bound on prediction variance by sampling from perturbed versions of the trained model weights, capturing epistemic uncertainty without requiring retraining or access to training data. The approach involves approximating the loss landscape around the converged weights and using this linearization to estimate how predictions would change under hypothetical weight perturbations.

## Key Results
- TULiP achieves state-of-the-art performance for near-OOD detection on OpenOOD v1.5 benchmark
- Top-1 or top-2 AUROC scores across CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K
- Consistent improvement over baseline entropy-based detection methods
- Robust performance across multiple architectures (ResNet, ViT, MobileNet, VGG)

## Why This Works (Mechanism)
TULiP leverages the linearization of training dynamics to estimate uncertainty without retraining. By analyzing the sensitivity of predictions to hypothetical weight perturbations around the converged solution, the method captures epistemic uncertainty that reflects the model's confidence in its predictions. The theoretical bound provides a principled way to quantify uncertainty based on the training dynamics, while the weight perturbation sampling approximates the variance in predictions that would result from different training trajectories.

## Foundational Learning

**Linearized Training Dynamics**
- Why needed: Provides tractable analysis of how small changes in weights affect predictions
- Quick check: Verify gradient-based sensitivity analysis produces reasonable uncertainty estimates

**Weight Perturbation Sampling**
- Why needed: Enables estimation of prediction variance without multiple retraining runs
- Quick check: Confirm perturbed predictions follow expected statistical distributions

**Epistemic Uncertainty Estimation**
- Why needed: Distinguishes between model uncertainty and data uncertainty
- Quick check: Ensure OOD samples show higher uncertainty than in-distribution samples

## Architecture Onboarding

**Component Map**
Training Loss -> Linearization Approximation -> Weight Perturbation Generator -> Prediction Sampler -> Uncertainty Estimator

**Critical Path**
The linearization approximation and weight perturbation sampling form the critical path, as these steps directly impact the quality of uncertainty estimates. The method's performance is most sensitive to the accuracy of the linearization and the sampling strategy.

**Design Tradeoffs**
- Computational cost vs. accuracy: More perturbation samples improve estimates but increase runtime
- Linearization approximation: Balances tractability with fidelity to true training dynamics
- Perturbation magnitude: Larger perturbations may better capture uncertainty but risk unrealistic scenarios

**Failure Signatures**
- Poor linearization approximation leads to underestimation of uncertainty
- Insufficient perturbation samples result in high-variance estimates
- Over-regularization during training can suppress the method's sensitivity

**First Experiments**
1. Test linearization accuracy on simple linear models with known uncertainty
2. Validate weight perturbation sampling on synthetic datasets with controlled uncertainty
3. Compare uncertainty estimates against Monte Carlo dropout on benchmark datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost scales linearly with number of weight perturbation samples
- Theoretical assumptions may not hold for highly non-linear architectures
- Limited evaluation of far-OOD detection performance

## Confidence

**High Confidence**: Theoretical derivation and synthetic dataset validation are well-supported and reproducible.

**Medium Confidence**: OpenOOD benchmark results are convincing for near-OOD detection, but generalizability to other domains needs further validation.

**Low Confidence**: State-of-the-art claims could be more comprehensive, and far-OOD detection limitations restrict the scope of results.

## Next Checks
1. Conduct comprehensive ablation study on perturbation sample count, magnitude, and sampling strategy
2. Evaluate far-OOD detection performance and compare with specialized methods
3. Test method on non-image datasets and different task types (regression, segmentation)