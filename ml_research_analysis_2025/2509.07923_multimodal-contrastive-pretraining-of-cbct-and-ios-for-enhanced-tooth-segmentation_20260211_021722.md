---
ver: rpa2
title: Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation
arxiv_id: '2509.07923'
source_url: https://arxiv.org/abs/2509.07923
tags:
- segmentation
- cbct
- tooth
- dental
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToothMCL, the first multimodal pretraining
  framework for dental segmentation, addressing the challenge of accurate tooth identification
  from paired CBCT and IOS data. By leveraging a large-scale CBCT-IOS3.8K dataset
  and employing contrastive learning, ToothMCL aligns modality-invariant representations
  across volumetric and surface-based imaging, enabling precise multi-class segmentation
  compliant with FDI numbering.
---

# Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation

## Quick Facts
- **arXiv ID:** 2509.07923
- **Source URL:** https://arxiv.org/abs/2509.07923
- **Reference count:** 40
- **Key outcome:** ToothMCL achieves state-of-the-art tooth segmentation, improving Dice Similarity Coefficient by 12% for CBCT and 8% for IOS through multimodal contrastive pretraining.

## Executive Summary
This paper introduces ToothMCL, the first multimodal pretraining framework for dental segmentation, addressing the challenge of accurate tooth identification from paired CBCT and IOS data. By leveraging a large-scale CBCT-IOS3.8K dataset and employing contrastive learning, ToothMCL aligns modality-invariant representations across volumetric and surface-based imaging, enabling precise multi-class segmentation compliant with FDI numbering. Evaluated on the largest and most diverse public datasets, ToothMCL achieves state-of-the-art performance, improving Dice Similarity Coefficient by 12% for CBCT and 8% for IOS segmentation. It demonstrates robust generalization across diverse clinical scenarios, including challenging cases with artifacts and anatomical variations, setting a new foundation for digital dentistry workflows.

## Method Summary
ToothMCL uses a two-stage approach: multimodal contrastive pretraining followed by modality-specific fine-tuning. During pretraining, CBCT scans are processed by a Swin Transformer encoder and IOS meshes by a Point Transformer encoder, with patch-level contrastive losses aligning representations across modalities while maintaining intra-modal consistency. The pretrained encoders are then fine-tuned separately—SwinUNETR for CBCT volumetric segmentation and Point Transformer for IOS point cloud classification—using labeled datasets. The framework leverages 3,867 unlabeled paired scans for pretraining and fine-tunes on diverse public datasets including ToothFairy2, Cui, Teeth3DS, TADPM, and 3D-IOSSeg.

## Key Results
- Achieved 12% DSC improvement for CBCT segmentation and 8% for IOS segmentation compared to previous state-of-the-art methods
- Showed monotonic performance increase with pretraining dataset size, with no saturation observed up to 3,867 paired scans
- Demonstrated robust performance on clinically challenging cases including metal artifacts, skeletal Class III malocclusion, and horizontally impacted molars
- Generalizes well to external datasets, outperforming competitors by substantial margins on both CBCT and IOS modalities

## Why This Works (Mechanism)

### Mechanism 1
Aligning CBCT and IOS representations in a shared latent space improves segmentation accuracy for both modalities. Dual patch-level contrastive learning pulls corresponding CBCT-IOS patch pairs together while pushing non-matching pairs apart. Cross-modal alignment loss ($L_{Cross}$) uses sigmoid-based contrastive loss on normalized embeddings from the Swin Transformer (CBCT) and Point Transformer (IOS). Intra-modal alignment ($L_{Intra}$) enforces consistency within each modality. The combined loss ($L_{Total} = L_{Intra} + \alpha L_{Cross}$) creates modality-invariant features that capture the same underlying dental anatomy despite different data formats (voxels vs. point clouds).

### Mechanism 2
Large-scale pretraining on 3,867 unlabeled paired scans transfers robust, domain-invariant features to downstream segmentation. Self-supervised pretraining on diverse patient anatomies, scanner protocols, and clinical conditions (including artifacts and malocclusions) exposes the encoder to wide anatomical variation before fine-tuning. The pretrained weights initialize modality-specific encoders, providing a better starting point than random initialization. Ablation shows monotonic DSC improvement with dataset size (25%→100%: +5.87pp), suggesting the encoder internalizes tooth morphology patterns across variations.

### Mechanism 3
Separate modality-specific fine-tuning after multimodal pretraining preserves each encoder's ability to handle modality-unique challenges (CBCT artifacts, IOS topology). After joint pretraining, CBCT and IOS encoders are fine-tuned independently—SwinUNETR for volumetric segmentation (handles noise, low contrast, metal artifacts via transformer attention) and Point Transformer for point cloud segmentation (captures local curvature and global arch structure via permutation-invariant self-attention). Pretrained encoder weights provide anatomical priors; decoder weights are randomly initialized. Loss functions are modality-appropriate: Dice+CE for CBCT, point-wise CE for IOS.

## Foundational Learning

- **Concept: Contrastive Learning Objectives**
  - **Why needed here:** Understanding how InfoNCE-style losses create embedding spaces where similar pairs cluster and dissimilar pairs separate is essential for debugging alignment quality and tuning temperature/bias hyperparameters ($t_g$, $b_g$, $t_l$, $b_l$)
  - **Quick check question:** Can you explain why sigmoid-based contrastive loss (Eq. 4) is used here instead of softmax-based InfoNCE, and what the $z_{ij}$ binary mask accomplishes?

- **Concept: Patch-Level vs. Global Representations in Vision Transformers**
  - **Why needed here:** The dual alignment operates on patch embeddings ($c_m \in \mathbb{R}^{r \times h}$, $p_m \in \mathbb{R}^{s \times h}$), not global pooled features. Understanding how Swin Transformer and Point Transformer partition inputs into patches determines how correspondence is established across modalities with different dimensionalities
  - **Quick check question:** How do the number of patches ($r$ for CBCT, $s$ for IOS) relate to input resolution, and what happens to cross-modal alignment if $r \neq s$?

- **Concept: Transfer Learning from Self-Supervised Pretraining**
  - **Why needed here:** The core value proposition is that unlabeled pretraining improves labeled fine-tuning. Understanding feature transfer, domain shift, and when pretraining helps vs. hurts is critical for reproducing and extending results
  - **Quick check question:** What does the ablation (Figure 7b) suggest about whether collecting more paired data would continue to improve performance, and what assumption does this make about the data distribution?

## Architecture Onboarding

- **Component map:**
  - CBCT ROI → Swin Transformer encoder → patch embeddings $c_m$; IOS mesh → point cloud → Point Transformer encoder → patch embeddings $p_m$; Dual contrastive losses ($L_{Cross}$, $L_{Intra}$) computed on normalized embeddings; Fine-tuning: SwinUNETR (CBCT) and Point Transformer (IOS) → segmentation

- **Critical path:**
  1. **Data preprocessing:** CBCT resampled to 0.3mm³, clipped [0, 2500] HU, RAS-aligned, jaw-cropped; IOS registered to occlusion, FPS-sampled to 24K points
  2. **Pretraining:** 50 epochs, AdamW (lr=1e-4), cosine schedule, batch size memory-limited, patch-level augmentations
  3. **Fine-tuning:** 150 epochs, AdamW (lr=3e-4), cosine schedule, batch=1, modality-specific augmentations (no flipping for CBCT to preserve anatomy)
  4. **Inference:** Separate CBCT and IOS pipelines; no cross-modal inference required at test time

- **Design tradeoffs:**
  - **Patch-level vs. global alignment:** Patch-level captures local anatomy but requires establishing correspondence; global alignment is simpler but may miss fine-grained features. Paper chooses patch-level for anatomical precision
  - **Joint pretraining vs. separate pretraining:** Joint enables cross-modal transfer but requires paired data (scarce). Separate is more flexible but loses cross-modal synergy. Paper leverages large paired dataset (3,867) to justify joint approach
  - **Encoder sharing vs. modality-specific:** Separate encoders (Swin for CBCT, Point Transformer for IOS) respect data structure differences but increase parameters. Shared encoder would reduce parameters but struggle with voxel vs. point cloud inductive biases

- **Failure signatures:**
  - **Low DSC on third molars (T8):** Underrepresented in training data; check label distribution and consider oversampling or class-weighted loss
  - **Missing teeth in segmentation:** Model fails to detect all instances (common in Cui, nnUNet baselines per Figure 3); may indicate insufficient pretraining coverage of anatomical variations
  - **Cross-modal misalignment:** If pretraining loss plateaus early, check CBCT-IOS registration quality and patch correspondence
  - **Catastrophic forgetting:** If fine-tuned model underperforms on internal dataset after external validation, encoder may have overfit to fine-tuning data; try lower learning rate or encoder freezing

- **First 3 experiments:**
  1. **Reproduce pretraining ablation:** Train with 25%, 50%, 75%, 100% of CBCT-IOS3.8K; plot DSC vs. dataset size to verify monotonic improvement claim and estimate data saturation point for your compute budget
  2. **Ablate alignment components:** Train three variants—cross-modal only, intra-modal only, both—to quantify contribution of each loss term ($\alpha$ sensitivity analysis); report DSC delta on ToothFairy2 and Teeth3DS
  3. **Probe generalization to new scanner:** Fine-tune on ToothFairy2, evaluate on Cui (external CBCT); compare pretrained vs. scratch initialization to validate 12% DSC improvement claim under domain shift

## Open Questions the Paper Calls Out

- **Can the ToothMCL framework be effectively adapted to incorporate 2D imaging modalities, such as panoramic or cephalometric x-rays, for dental segmentation?**
  - The current architecture "cannot accommodate other common imaging techniques" and suggests future work should involve "conducting multimodal pretraining of new modalities, including panoramic and cephalometric x-rays"
  - Integrating 2D projection data requires overcoming differences in spatial resolution and feature distribution

- **Can the pretrained ToothMCL encoder serve as a generalizable foundation model for non-segmentation tasks like disease classification or orthodontic simulation?**
  - The authors note the "pretrained encoder could be used for varying applications beyond tooth segmentations" and suggest "further research could also explore options in transfer learning... for a seamless transfer to dental applications"
  - It is unknown if the learned modality-invariant representations are semantically rich enough to improve performance on distinct clinical tasks without extensive retraining

- **At what dataset scale does the performance improvement from multimodal contrastive pretraining begin to saturate?**
  - The ablation study on dataset scaling (25% to 100%) shows "no saturation is observed even after 3,867 paired scans," and the Discussion explicitly notes "there is no upper limit on performance" yet
  - While the benefits of scaling are clear, the "point of diminishing returns" has not been identified

## Limitations
- Private CBCT-IOS3.8K dataset prevents direct validation of multimodal contrastive pretraining claims
- No quantitative comparison of cross-modal alignment quality (embedding distances, retrieval metrics)
- Third molar (T8) consistently underperforms across models, suggesting data imbalance issues

## Confidence

**High:** Pretraining scale effects (monotonic DSC improvement with dataset size), modality-specific fine-tuning necessity (CBCT artifacts, IOS topology), strong external dataset generalization

**Medium:** Cross-modal alignment efficacy (no direct alignment metrics reported), joint vs. separate pretraining tradeoff justification (relies on dataset availability)

**Low:** Private data reproducibility, exact implementation details for registration and ROI cropping

## Next Checks
1. Replicate pretraining ablation (25%→100% CBCT-IOS3.8K) on public paired dental datasets to verify monotonic improvement claim
2. Ablate alignment components (cross-modal only, intra-modal only, both) to quantify each loss term's contribution
3. Probe generalization to new scanner protocols by training on ToothFairy2 and evaluating on Cui, measuring domain shift impact on DSC scores