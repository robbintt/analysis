---
ver: rpa2
title: Compressing Quaternion Convolutional Neural Networks for Audio Classification
arxiv_id: '2510.21388'
source_url: https://arxiv.org/abs/2510.21388
tags:
- quaternion
- pruning
- audio
- pruned
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational complexity of quaternion
  convolutional neural networks (QCNNs) for audio classification tasks. QCNNs can
  better capture inter-channel dependencies in audio signals compared to conventional
  CNNs, but their Hamilton product operations increase computational overhead.
---

# Compressing Quaternion Convolutional Neural Networks for Audio Classification

## Quick Facts
- arXiv ID: 2510.21388
- Source URL: https://arxiv.org/abs/2510.21388
- Reference count: 40
- Primary result: Pruning quaternion convolutional neural networks achieves 50% reduction in computational cost and 80% reduction in parameters while maintaining competitive audio classification performance

## Executive Summary
This paper addresses the computational complexity of quaternion convolutional neural networks (QCNNs) for audio classification tasks. QCNNs can better capture inter-channel dependencies in audio signals compared to conventional CNNs, but their Hamilton product operations increase computational overhead. The authors explore knowledge distillation (KD) and filter pruning to reduce QCNN complexity while maintaining performance. Their results show that pruning QCNNs outperforms KD and produces more parameter-efficient models than pruning conventional CNNs.

## Method Summary
The paper converts CNN14 and ResNet38 architectures to their quaternion variants by replacing standard convolutions with Hamilton product-based quaternion convolutions. Input spectrograms are converted to quaternion representation using static mel-energy and its first three temporal derivatives. The authors apply data-independent filter pruning using operator-norm, ℓ1-norm, and geometric-median metrics to remove structurally unimportant filters, followed by fine-tuning. Knowledge distillation serves as a baseline compression method. The compressed models are evaluated on AudioSet and transfer learning tasks (GTZAN, ESC-50, RAVDESS).

## Key Results
- Pruned QCNNs achieve 50% reduction in computational cost and 80% reduction in parameters on AudioSet
- Pruning methods outperform knowledge distillation in both accuracy and computational efficiency
- Pruned QCNNs maintain competitive performance across multiple audio classification tasks
- QCNNs require 4× fewer parameters than conventional CNNs while maintaining similar accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quaternion representations reduce parameter count while maintaining the capacity to model inter-channel relationships in audio.
- **Mechanism:** The Hamilton product enforces weight sharing across four input components (real + 3 imaginary) via a structured matrix multiplication. Unlike standard CNNs, which require independent filter banks for each input channel to achieve mixing, a single quaternion filter inherently processes all four components jointly.
- **Core assumption:** Audio features, specifically the static energy and its temporal derivatives, share a structural relationship that is best captured by hypercomplex algebra rather than independent real-valued mappings.
- **Evidence anchors:**
  - [abstract] "QCNNs... employ quaternion algebra to jointly capture inter-channel dependencies, enabling more compact models with fewer learnable parameters."
  - [supplemental] Equation 12 demonstrates how $F_R, F_I, F_J, F_K$ are reused to mix input channels, requiring $4 \times 3 \times 3 = 36$ parameters vs. $144$ in standard CNNs.
  - [corpus] Related work on "Quaternion Approximation Networks" confirms the parameter efficiency of quaternion layers in other domains.

### Mechanism 2
- **Claim:** Filter pruning is a more computationally efficient compression strategy for QCNNs than Knowledge Distillation (KD).
- **Mechanism:** Pruning (specifically Operator-norm based) directly removes structurally unimportant filters based on weight magnitude, requiring only a forward pass for importance scoring and standard fine-tuning. In contrast, KD requires continuous forward passes of the large teacher network to guide the student, doubling the compute load per training step.
- **Core assumption:** The pre-trained QCNN contains a significant number of redundant or low-magnitude filters that do not contribute meaningfully to the decision boundary.
- **Evidence anchors:**
  - [results] "Pruning methods generally outperform KD... fine-tuning a pruned model without knowledge distillation is more compute-efficient than fine-tuning with knowledge distillation."
  - [figure 5] Shows the computational cost (time) of Prune_FT is significantly lower than Prune_KD.

### Mechanism 3
- **Claim:** Encoding spectrograms as quaternions (static + dynamic features) enables the network to process "multi-view" temporal dynamics efficiently.
- **Mechanism:** The input $Q(f,t)$ is constructed by assigning the static mel-energy to the real component and the 1st, 2nd, and 3rd temporal derivatives to the imaginary components ($i, j, k$). This explicitly aligns the data structure with the quaternion algebra, allowing convolutions to act on both energy and motion simultaneously.
- **Core assumption:** The first three derivatives capture sufficient orthogonal information about temporal dynamics to justify the overhead of quaternion operations.
- **Evidence anchors:**
  - [section V-B] Equation 8: $Q(f, t) = \psi + \frac{\partial\psi}{\partial t}i + \dots$
  - [results] Table IV shows that Multi-channel CNNs (same input, real weights) perform worse than QCNNs.

## Foundational Learning

- **Concept: Quaternion Algebra & The Hamilton Product**
  - **Why needed here:** This is the fundamental operation replacing standard convolution. Without understanding that the Hamilton product (Eq. 1) mixes real and imaginary components via 4 real-valued matrix multiplications, the parameter savings and "inter-channel dependency" claims are opaque.
  - **Quick check question:** If input channels $I$ and $J$ are swapped, does a standard CNN require retraining, and does a QCNN require retraining?

- **Concept: Filter Pruning (Data-Independent)**
  - **Why needed here:** The paper centers on reducing complexity via pruning. You must distinguish between data-dependent methods (needing forward passes on data) and data-independent methods (analyzing weights directly), as the authors favor the latter (Operator-norm) for efficiency.
  - **Quick check question:** Why might calculating the Geometric Median of filters be considered a "data-independent" method for pruning?

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** KD serves as the compression baseline. Understanding the role of "Temperature" ($T$) and KL divergence is necessary to interpret why the authors found KD to be computationally inefficient compared to pruning.
  - **Quick check question:** In Equation 3, what happens to the teacher's probability distribution if the Temperature $T$ is set very high?

## Architecture Onboarding

- **Component map:** Input Layer -> Quaternion Conv Block -> Pruning Engine -> Output
- **Critical path:** The implementation of the **Quaternion Convolution** is the bottleneck. You cannot simply swap `nn.Conv2d` for a custom layer. You must verify the matrix structure in Equation 12 is implemented correctly (specifically the sign flips $-m_I$, etc., in the weight matrix) to ensure 4-channel mixing occurs.
- **Design tradeoffs:**
  - **Params vs. MACs:** QCNNs drastically reduce parameter count (memory) but generally increase Multiply-Accumulate operations (MACs) per layer compared to a standard CNN of the same width, due to the 16 scalar multiplications required per Hamilton product.
  - **Pruning Ratio ($p$):** Higher $p$ (e.g., 0.75) saves massive space but risks losing the "inter-channel" magic; $p=0.50$ is shown to be a sweet spot for AudioSet.
- **Failure signatures:**
  - **Nan Loss during training:** Often caused by unstable quaternion weight initialization or activation functions applied incorrectly across components.
  - **Accuracy drop on transfer:** If pruned models fail to generalize to GTZAN/ESC-50, it indicates the pruned filters were critical for general audio features, not just AudioSet-specific noise.
- **First 3 experiments:**
  1. **Sanity Check (Replication):** Train a small QCNN14 on a subset of AudioSet. Compare parameter count against a standard CNN14 with equivalent width to verify the 4x parameter reduction theoretical limit.
  2. **Pruning Sweep:** Train the full QCNN14, then apply Operator-norm pruning at ratios [0.25, 0.50, 0.75]. Plot MACs vs. mAP to find the "knee" of the curve (where accuracy drops steeply).
  3. **Ablation (Input):** Compare the proposed Quaternion Input (Eq. 8) vs. a standard 4-channel stack (treating components as independent) to isolate the benefit of the Hamilton product from the benefit of simply using derivatives.

## Open Questions the Paper Calls Out

- **Question:** How does the performance-computation trade-off shift when distilling knowledge from large-scale teacher models, such as Transformers or ensembles, compared to conventional training?
  - **Basis in paper:** [explicit] The authors state, "In future work, we would like to study the trade-off between performance and computation while using knowledge distillation with large-scale teacher models."
  - **Why unresolved:** The current study primarily uses unpruned QCNNs or standard CNNs as teachers, leaving the potential of large-scale Transformers to improve the student QCNN's efficiency unexplored.

- **Question:** Can the efficiency gains demonstrated in pruned QCNNs be effectively transferred to quaternion Transformers and complex-valued neural networks?
  - **Basis in paper:** [explicit] The conclusion explicitly lists exploring "pruning in quaternion Transformer... and complex-valued networks" as a future direction.
  - **Why unresolved:** The proposed compression pipeline was validated only on feedforward and residual convolutional architectures.

- **Question:** How does model compression impact the performance of multi-channel conventional CNNs compared to quaternion CNNs?
  - **Basis in paper:** [explicit] Section IX notes, "In future, we would like to analyse how model compression affects multi-channel conventional CNN performance."
  - **Why unresolved:** The ablation study compared pruned QCNNs against unpruned multi-channel conventional CNNs, but did not apply pruning to the conventional multi-channel models to ensure a fair comparison of compressed efficiency.

## Limitations

- The computational complexity analysis lacks complete detail on wall-clock time and memory usage differences between pruned QCNNs and conventional CNNs
- The theoretical justification for why operator-norm specifically works better for quaternion filters is not provided
- Transfer learning results are based on relatively small datasets (GTZAN: 1000 clips, ESC-50: 2000 clips) that may not fully validate generalization claims

## Confidence

- **High Confidence:** QCNNs reduce parameter count through quaternion algebra (supported by explicit matrix calculations in Equation 12 and parameter comparisons)
- **Medium Confidence:** Pruning QCNNs is more efficient than KD for compression (supported by runtime comparisons, though absolute timing data is limited)
- **Medium Confidence:** Pruned QCNNs maintain competitive accuracy on multiple audio classification tasks (supported by Table IV results, but small transfer dataset sizes limit generalizability)

## Next Checks

1. **Reproduce the parameter reduction claim:** Implement the quaternion convolution layer and verify the 4× parameter reduction compared to standard CNN by counting weights in both architectures for identical layer dimensions
2. **Benchmark pruning efficiency:** Measure actual training time for operator-norm pruning vs. KD across multiple pruning ratios on the same hardware to validate the computational efficiency claims in Figure 5
3. **Stress-test transfer learning:** Evaluate pruned QCNN performance on additional audio datasets (e.g., UrbanSound8K with 8732 samples) to confirm generalization beyond the small GTZAN/ESC-50 datasets