---
ver: rpa2
title: Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket
  Search
arxiv_id: '2512.07142'
source_url: https://arxiv.org/abs/2512.07142
tags:
- pruning
- methods
- training
- sparsity
- ticket
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of drawing
  "winning lottery tickets" in neural networks using existing methods like Lottery
  Ticket Rewinding (LTR), which requires extensive retraining. The authors argue that
  current pruning-at-initialization (PaI) methods suffer from poor performance due
  to their reliance on first-order saliency metrics that ignore inter-weight dependencies.
---

# Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search

## Quick Facts
- **arXiv ID**: 2512.07142
- **Source URL**: https://arxiv.org/abs/2512.07142
- **Reference count**: 40
- **Primary result**: CTS achieves 99.3% sparsity with 74.0% accuracy on ResNet-20/CIFAR-10 in 7.9 minutes, outperforming LTR (68.3% accuracy in 95.2 minutes)

## Executive Summary
This paper addresses the computational inefficiency of finding "winning lottery tickets" in neural networks through traditional rewinding methods like Lottery Ticket Rewinding (LTR), which require extensive retraining. The authors identify that pruning-at-initialization methods fail because they rely on first-order saliency metrics that ignore inter-weight dependencies. They propose Concrete Ticket Search (CTS), a method that frames subnetwork discovery as a combinatorial optimization problem using Concrete relaxation of the discrete search space, combined with a novel gradient balancing scheme (GRADBALANCE) to control sparsity without hyperparameter tuning and knowledge distillation-inspired objectives.

## Method Summary
Concrete Ticket Search (CTS) reframes the lottery ticket problem as a combinatorial optimization over the discrete mask space. The method employs a Concrete relaxation to make this optimization differentiable, allowing gradient-based search. A key innovation is GRADBALANCE, which automatically controls sparsity during training without requiring manual hyperparameter tuning. The training objective incorporates knowledge distillation principles, specifically minimizing reverse Kullback-Leibler divergence between sparse and dense network outputs, which proves particularly effective for preserving training dynamics in the sparse subnetworks.

## Key Results
- CTS reaches 99.3% sparsity with 74.0% accuracy on ResNet-20/CIFAR-10 in 7.9 minutes
- Outperforms LTR which achieves 68.3% accuracy at the same sparsity in 95.2 minutes
- CTS subnetworks outperform saliency-based methods across all sparsities
- CTS's advantage over LTR is most pronounced in highly sparse regimes

## Why This Works (Mechanism)
CTS works by addressing the fundamental limitation of existing pruning-at-initialization methods: their inability to capture inter-weight dependencies. Traditional saliency metrics only consider individual weight importance, missing how weights interact during training. By formulating the search as a combinatorial optimization problem with Concrete relaxation, CTS can reason about groups of weights simultaneously. The GRADBALANCE mechanism ensures stable training by preventing gradient explosion or vanishing as sparsity increases. The reverse KL divergence objective preserves the probability distribution outputs of the dense network, maintaining the learned training dynamics rather than just matching point estimates.

## Foundational Learning
- **Combinatorial optimization**: Needed for searching over discrete mask configurations; quick check: can be verified by testing on small networks with known optimal masks
- **Concrete relaxation**: Transforms discrete sampling into continuous optimization; quick check: verify gradients flow through relaxed Bernoulli variables
- **Knowledge distillation**: Transfers knowledge from dense to sparse models; quick check: compare student model outputs to teacher model outputs
- **Reverse KL divergence**: Preserves probability distribution alignment; quick check: measure alignment of output distributions between dense and sparse models
- **Gradient balancing**: Controls training dynamics; quick check: verify training stability across different sparsity levels

## Architecture Onboarding

**Component Map:**
Input -> Concrete relaxation layer -> GRADBALANCE module -> Dense network -> KL divergence loss -> Sparse subnetwork

**Critical Path:**
The forward pass flows from input through the Concrete relaxation (generating masks), then through GRADBALANCE (ensuring stable gradients), then through the dense network, and finally computing the reverse KL divergence loss. The backward pass computes gradients through this chain to update both the mask parameters and network weights.

**Design Tradeoffs:**
- Concrete relaxation vs. hard masking: The relaxation enables gradient-based optimization but introduces approximation error
- Reverse KL vs. forward KL: Reverse KL better preserves mode-seeking behavior in classification
- GRADBALANCE vs. fixed sparsity: Dynamic control provides stability but requires careful implementation

**Failure Signatures:**
- Unstable training with high sparsity: Indicates GRADBALANCE isn't properly scaling gradients
- Poor accuracy despite training: Suggests the Concrete relaxation isn't effectively exploring mask space
- Overfitting to dense outputs: Indicates reverse KL isn't capturing the right distribution alignment

**First Experiments:**
1. Verify GRADBALANCE maintains stable training across sparsity levels from 50% to 99%
2. Compare reverse KL vs. forward KL objectives on a small network
3. Test Concrete relaxation's ability to recover known optimal masks on a toy problem

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that generalizability to larger-scale models and diverse domains remains untested.

## Limitations
- Performance claims primarily validated on ResNet architectures and CIFAR datasets
- Generalizability to larger-scale models (ResNet-50, Vision Transformers) and diverse domains (NLP, medical imaging) remains untested
- Lack of systematic ablation studies isolating contributions of individual CTS components

## Confidence
- CTS achieves state-of-the-art sparse subnetwork performance: **High**
- GRADBALANCE eliminates hyperparameter tuning: **Medium** (limited ablation)
- Reverse KL divergence minimization is optimal: **Medium** (compared only to alternatives mentioned)
- Results generalize beyond tested architectures/datasets: **Low**

## Next Checks
1. Benchmark CTS on larger-scale models (e.g., ResNet-50, ViT-Base) and diverse datasets (ImageNet, text classification) to assess scalability and domain transfer.

2. Conduct systematic ablation studies to quantify the individual contributions of Concrete relaxation, GRADBALANCE, and the reverse KL divergence objective to overall performance.

3. Evaluate CTS's robustness to initialization variance by running multiple trials with different random seeds and reporting mean/std accuracy across runs.