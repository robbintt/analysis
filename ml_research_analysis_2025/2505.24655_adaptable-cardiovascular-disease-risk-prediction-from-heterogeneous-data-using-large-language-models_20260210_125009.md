---
ver: rpa2
title: Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using
  Large Language Models
arxiv_id: '2505.24655'
source_url: https://arxiv.org/abs/2505.24655
tags:
- risk
- patient
- information
- clinical
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AdaCVD, a large language model-based framework
  for cardiovascular disease risk prediction that addresses key challenges in real-world
  clinical practice. The authors fine-tuned a pre-trained LLM on over half a million
  UK Biobank participants to create a model that outperforms established medical risk
  scores and matches the performance of tabular ML methods.
---

# Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models

## Quick Facts
- arXiv ID: 2505.24655
- Source URL: https://arxiv.org/abs/2505.24655
- Authors: Frederike Lübeck; Jonas Wildberger; Frederik Träuble; Maximilian Mordig; Sergios Gatidis; Andreas Krause; Bernhard Schölkopf
- Reference count: 40
- Primary result: AdaCVD achieves AUROC of 0.774, outperforming established medical risk scores and matching tabular ML methods

## Executive Summary
This paper presents AdaCVD, a large language model-based framework for cardiovascular disease risk prediction that addresses key challenges in real-world clinical practice. The authors fine-tuned a pre-trained LLM on over half a million UK Biobank participants to create a model that outperforms established medical risk scores and matches the performance of tabular ML methods. AdaCVD achieves state-of-the-art performance with an AUROC of 0.774 and demonstrates superior flexibility by handling variable patient information, adapting to unstructured clinical notes, and efficiently adjusting to distribution shifts across populations.

## Method Summary
The method involves fine-tuning Mistral-7B-Instruct using LoRA (rank 16) on UK Biobank structured data, where patient information is serialized as natural language descriptions. The model performs binary classification in token space by extracting logits for Yes/No responses to the prediction question. Random feature sampling during training teaches the model to handle variable/incomplete patient information, while LoRA enables efficient adaptation to new formats and populations with ~100x fewer examples than training from scratch.

## Key Results
- AdaCVD achieves AUROC of 0.774, outperforming established medical risk scores (Framingham, ASCVD, QRISK3) and matching tabular ML methods
- Model shows superior flexibility handling variable patient information, unstructured clinical notes, and distribution shifts across populations
- Particularly strong performance gains for elderly individuals, smokers, and those with diabetes
- Requires up to 100 times fewer examples to adapt to new settings compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning a pre-trained LLM on structured patient representations achieves state-of-the-art CVD risk prediction (AUROC 0.774).
- **Mechanism:** The LLM encodes patient information as natural language descriptions, allowing it to reason over heterogeneous features without requiring fixed input schemas. Binary classification is performed in token space by extracting logits for Yes/No responses to the prediction question.
- **Core assumption:** Pre-trained language understanding transfers to clinical reasoning when fine-tuned on domain-specific outcomes.
- **Evidence anchors:**
  - [abstract] "fine-tuned a pre-trained LLM on over half a million UK Biobank participants to create a model that outperforms established medical risk scores"
  - [section 4.1.3] "We extracted the logits and subsequently normalized them to generate the final CVD risk prediction"
  - [corpus] Related work "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction" supports the general approach but is not directly comparative.
- **Break condition:** Performance degrades significantly if fine-tuning data has systematic biases or outcome labeling errors.

### Mechanism 2
- **Claim:** The model handles variable/incomplete patient information by training on randomly sampled feature subsets during fine-tuning.
- **Mechanism:** Random feature dropout during training teaches the model to leverage whatever information is present, rather than relying on complete inputs. Text-based representation means missing values are simply absent from the prompt rather than requiring imputation.
- **Core assumption:** The model learns to weight available information appropriately without explicit missingness indicators.
- **Evidence anchors:**
  - [section 4.2.2] "For each patient, we randomly sampled a subset of features during training and generated a patient description using only these features"
  - [figure 5] Shows AdaCVD-Flex maintains robust performance across varying input conditions
  - [corpus] Limited corpus evidence on this specific mechanism; related papers focus on multimodal fusion rather than missing data handling.
- **Break condition:** Performance collapses if certain critical features are systematically absent in deployment but were always present during training.

### Mechanism 3
- **Claim:** Adaptation to new formats (unstructured notes) or populations (Framingham cohort) requires ~100x fewer examples than training from scratch.
- **Mechanism:** Pre-training on UK Biobank structured data provides a foundation that transfers to related domains. LoRA fine-tuning updates only ~0.13% of parameters, enabling efficient adaptation without catastrophic forgetting.
- **Core assumption:** The learned clinical reasoning patterns generalize across format changes and population shifts.
- **Evidence anchors:**
  - [abstract] "requires up to 100 times fewer examples to adapt to new settings compared to training from scratch"
  - [figure 6a] Shows adapted model reaches AUROC 0.697 with 10 examples vs. >1000 needed from scratch
  - [corpus] "Causal and Federated Multimodal Learning" addresses heterogeneous populations but through different mechanisms.
- **Break condition:** If target domain has fundamentally different feature semantics or outcome definitions, transfer may fail.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Enables fine-tuning 7B parameter models on clinical data without prohibitive computational costs. Only adapter modules are updated.
  - Quick check question: Can you explain why LoRA targeting query/key/value layers with rank 16 reduces trainable parameters to ~0.13%?

- **Concept: Token-Space Classification**
  - Why needed here: Unlike traditional ML models that output numeric scores directly, LLMs produce text. Extracting logits from Yes/No tokens enables probabilistic predictions.
  - Quick check question: How would you modify the approach if you needed calibrated probability estimates rather than rankings?

- **Concept: Distribution Shift in Clinical AI**
  - Why needed here: Models trained on UK Biobank (2016+, UK population) may not generalize to US data from 1948+ without adaptation.
  - Quick check question: What are three sources of distribution shift when deploying across hospitals, and which does this paper evaluate?

## Architecture Onboarding

- **Component map:** Patient data -> Serialized text (structured key-value pairs OR free-text summaries) -> Mistral-7B-Instruct (decoder-only transformer) -> LoRA adapters on attention layers (rank 16) -> Logits for "Yes"/"No" tokens -> Normalized probability

- **Critical path:** Data serialization format -> LoRA configuration -> Binary classification head design -> Calibration assessment

- **Design tradeoffs:**
  - Structured prompts (reproducible) vs. free-text notes (realistic but variable)
  - Single flexible model vs. feature-specific expert models (slight performance loss for deployment convenience)
  - Training epochs: 2 for base model, more for adaptation (overfitting risk with limited data)

- **Failure signatures:**
  - AUROC drops dramatically on specific subgroups -> Check training data representation
  - Model ignores certain feature categories -> Verify serialization includes them consistently
  - Predictions poorly calibrated -> Check outcome class balance and loss function

- **First 3 experiments:**
  1. **Reproduce base performance:** Fine-tune Mistral-7B on UKB-structured with base risk factors only; target AUROC ~0.738.
  2. **Test missing data robustness:** Evaluate model trained on complete data against held-out set with randomly dropped features; compare to AdaCVD-Flex approach.
  3. **Validate adaptation efficiency:** Take adapted model and train from scratch with varying sample sizes (10, 100, 1000); confirm ~100x data efficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How well does AdaCVD generalize to populations beyond the UK Biobank and Framingham cohorts, particularly across different countries, healthcare systems, and underrepresented demographic groups?
- **Basis in paper:** [explicit] Authors state: "model training was conducted exclusively on data from the UK Biobank... further validation across different countries and healthcare systems is necessary."
- **Why unresolved:** Only two Western cohorts were evaluated; global populations differ in risk factor distributions, healthcare access, genetic backgrounds, and documentation practices.
- **What evidence would resolve it:** Prospective validation on diverse international cohorts (e.g., African, Asian, South American populations) with AUROC and calibration metrics reported per demographic subgroup.

### Open Question 2
- **Question:** How does AdaCVD perform on authentic clinical notes compared to synthetically generated summaries, and what domain gap exists between LLM-generated and real clinical text?
- **Basis in paper:** [explicit] Authors acknowledge: "there is a lack of real-world datasets that pair textual patient representations... we synthetically generated free-text patient descriptions... we emphasize the need for the creation of publicly available, high-quality datasets containing real clinical text."
- **Why unresolved:** Synthetic summaries may not capture the full variability, noise, abbreviations, and inconsistencies present in real clinical documentation.
- **What evidence would resolve it:** Evaluation on a dataset with actual clinical notes linked to long-term CVD outcomes, comparing performance between synthetic and real text.

### Open Question 3
- **Question:** What is the effect of simultaneous multi-dimensional shifts (input content, format, and population) on AdaCVD's performance, compared to the isolated adaptation scenarios evaluated?
- **Basis in paper:** [inferred] The authors disentangled three adaptation axes and state "real-world clinical deployments will likely involve simultaneous shifts across multiple dimensions, we believe our findings generalize well"—but this belief was not empirically tested.
- **Why unresolved:** The paper only evaluates adaptation along single axes (variable inputs, text format, or population shift), never combinations.
- **What evidence would resolve it:** Experiments with combined shifts (e.g., new population with incomplete, unstructured data) to quantify compounding effects on performance.

## Limitations
- Performance degradation risk when critical features are systematically missing in deployment settings where they were always present during training
- Limited empirical validation of the 100x adaptation efficiency claim across multiple diverse target domains
- Reliance on synthetic clinical notes rather than authentic real-world documentation for text-based evaluation

## Confidence

- **High confidence**: The mechanism of using LoRA for efficient fine-tuning and the AUROC performance metric (0.774) on UK Biobank data are well-supported by the experimental setup and comparable to established benchmarks.
- **Medium confidence**: The claims about handling variable patient information through random feature sampling during training are plausible but lack extensive empirical validation across diverse missingness patterns.
- **Medium confidence**: The 100x adaptation efficiency claim is supported by the single adaptation experiment shown, but broader validation across multiple target domains would strengthen this assertion.

## Next Checks

1. **Missing data stress test**: Systematically remove specific feature categories from the test set (e.g., all blood samples, all lifestyle features) to quantify performance degradation when critical information is absent, comparing against models trained with explicit missingness indicators.

2. **Cross-institutional deployment validation**: Evaluate model performance on clinical data from multiple hospital systems with different EHR formats and documentation practices to assess robustness to distribution shifts beyond the UK Biobank-to-Framingham transition tested.

3. **Calibration assessment**: Conduct thorough probability calibration analysis (e.g., reliability diagrams, calibration curves) to verify that the token-based normalization produces well-calibrated risk estimates suitable for clinical decision-making, not just accurate rankings.