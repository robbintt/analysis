---
ver: rpa2
title: 'Towards Practical Second-Order Optimizers in Deep Learning: Insights from
  Fisher Information Analysis'
arxiv_id: '2504.20096'
source_url: https://arxiv.org/abs/2504.20096
tags:
- adafisher
- learning
- optimization
- training
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaFisher is an adaptive second-order optimizer that improves deep
  neural network training by integrating a diagonal block-Kronecker approximation
  of the Fisher Information Matrix (FIM) into the Adam framework. Unlike first-order
  methods such as Adam, which only use gradient magnitude for scaling, AdaFisher incorporates
  curvature information to better adapt learning rates per parameter, leading to faster
  convergence and improved generalization.
---

# Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis

## Quick Facts
- arXiv ID: 2504.20096
- Source URL: https://arxiv.org/abs/2504.20096
- Reference count: 31
- AdaFisher achieves higher accuracy and reduced test error compared to Adam, K-FAC, and Shampoo while maintaining computational efficiency comparable to first-order optimizers.

## Executive Summary
This paper introduces AdaFisher, an adaptive second-order optimizer that improves deep neural network training by integrating a diagonal block-Kronecker approximation of the Fisher Information Matrix (FIM) into the Adam framework. Unlike first-order methods such as Adam, which only use gradient magnitude for scaling, AdaFisher incorporates curvature information to better adapt learning rates per parameter, leading to faster convergence and improved generalization. Experiments across CNNs, Vision Transformers, and language models show that AdaFisher consistently outperforms baselines, achieving higher accuracy and reduced test error while maintaining computational efficiency comparable to first-order optimizers.

## Method Summary
AdaFisher extends the Adam optimizer by incorporating a diagonal block-Kronecker approximation of the Fisher Information Matrix (FIM) to capture curvature information. The method computes Kronecker factors (H_D for inputs, S_D for outputs) for convolutional and linear layers, and includes FIM computation for BatchNorm layers. A diagonal approximation is applied to these factors, combined with exponential moving average (EMA) with decay γ=0.8, and Min-Max normalization to maintain numerical stability. The optimizer updates parameters using the inverse of the approximated FIM scaled by the gradient moving average, without taking a square root of the second moment. The approach maintains Adam-like computational complexity while providing second-order curvature information for better learning rate adaptation.

## Key Results
- Consistently outperforms Adam, K-FAC, and Shampoo across CNNs, Vision Transformers, and language models on CIFAR-10/100, Tiny ImageNet, ImageNet-1k, and WikiText-2
- Achieves higher accuracy and reduced test error while maintaining computational efficiency comparable to first-order optimizers
- Shows stability across various batch sizes and learning rates, converging toward flatter minima that improve robustness and generalization
- AdaFisherW variant further improves generalization through a modified update rule incorporating parameter values

## Why This Works (Mechanism)
AdaFisher works by leveraging the Fisher Information Matrix to capture the curvature of the loss landscape with respect to model parameters. The diagonal block-Kronecker approximation allows efficient computation of this second-order information while maintaining scalability. By incorporating this curvature information into the adaptive learning rate calculation, AdaFisher can better distinguish between parameters that need larger updates versus those requiring smaller, more careful adjustments. This leads to faster convergence and improved generalization by naturally avoiding sharp minima that tend to overfit. The Min-Max normalization and Tikhonov damping ensure numerical stability, while the EMA mechanism provides smooth updates that prevent oscillations.

## Foundational Learning

**Fisher Information Matrix (FIM)**
Why needed: Captures curvature information of the loss landscape with respect to parameters, providing second-order optimization capabilities
Quick check: Verify FIM diagonal concentration property holds for your architecture by computing sample diagonal entries

**Kronecker Factorization**
Why needed: Enables efficient approximation of large matrices by decomposing them into smaller factor matrices
Quick check: Confirm that H_D and S_D dimensions are manageable and spatial averaging produces reasonable estimates

**Diagonal Approximation**
Why needed: Reduces computational complexity from O(n³) to O(n) while preserving most of the curvature information
Quick check: Compare convergence speed with and without diagonal approximation on a small network

**Min-Max Normalization**
Why needed: Prevents numerical instability from extreme FIM diagonal values that could cause divergence
Quick check: Monitor FIM diagonal ranges during training and verify normalization bounds are effective

**Tikhonov Damping**
Why needed: Regularizes the FIM inverse to prevent division by very small values that could amplify noise
Quick check: Test different λ values to find the minimum that maintains stability

## Architecture Onboarding

**Component Map**
Data → Model → Loss → Gradient Computation → FIM Computation (H_D, S_D) → Diagonal Approximation + Min-Max Normalization → AdaFisher Update → Parameter Update

**Critical Path**
The critical path involves computing gradients, extracting Kronecker factors for each layer, applying diagonal approximation and normalization, then performing the adaptive update using the inverse Fisher approximation. The FIM computation and its inverse are the computational bottlenecks.

**Design Tradeoffs**
The diagonal block-Kronecker approximation trades some curvature information fidelity for computational efficiency. Using a diagonal approximation rather than full matrix inversion reduces complexity from O(n³) to O(n) but may miss some parameter correlations. The Min-Max normalization prevents numerical issues but may compress useful information. The choice of EMA decay rate γ=0.8 balances smoothness against responsiveness to changing curvature.

**Failure Signatures**
- NaN/inf gradients: Indicates numerical instability, often from near-zero FIM diagonals or poor normalization
- Slower convergence than Adam: Suggests incorrect FIM computation, poor damping parameter choice, or normalization issues
- Oscillations or divergence: May indicate EMA decay rate too high or insufficient damping
- No improvement over Adam: Could mean diagonal approximation is discarding too much information or FIM computation is incorrect

**3 First Experiments**
1. Implement AdaFisher on CIFAR-10 with ResNet-18, batch size 256, 200 epochs, cosine annealing. Target ~95% accuracy with cutout and compare to Adam baseline.
2. Test FIM diagonal concentration property on your architecture by computing sample diagonal entries of H_D and S_D matrices.
3. Validate Min-Max normalization by monitoring FIM diagonal ranges during early training and ensuring they stay within specified bounds.

## Open Questions the Paper Calls Out

**Open Question 1**
Would a band-diagonal approximation of the FIM (capturing off-diagonal entries around the main diagonal) significantly improve AdaFisher's curvature representation compared to the purely diagonal approximation?
Basis: Section 7.3 states: "A natural extension would be to use a band-diagonal approximation instead, capturing a band of off-diagonal entries around the main diagonal... Future work can analyze the trade-off between bandwidth and performance gains."
Unresolved because: The diagonal approximation is motivated by empirical diagonal concentration, but may miss parameter correlations that could further improve optimization.
Evidence needed: Systematic experiments varying bandwidth and measuring both accuracy gains and computational overhead across multiple architectures and datasets.

**Open Question 2**
Can AdaFisher scale effectively to LLMs with hundreds of billions of parameters while maintaining its convergence advantages?
Basis: Section 7.3: "An important next step is testing and refining AdaFisher on truly large-scale models, such as Transformer-based LLMs with hundreds of billions of parameters. This will likely require combining memory-efficient approximations... since storing even diagonal Fisher information for billions of parameters is non-trivial."
Unresolved because: Current experiments only cover models up to ~28M parameters; scaling to LLMs introduces memory and distributed training challenges not yet addressed.
Evidence needed: Successful training/fine-tuning of models ≥1B parameters with AdaFisher, demonstrating convergence and memory metrics comparable to first-order baselines.

**Open Question 3**
Can FIM-based identification of "informative" neurons enable more systematic parameter-efficient fine-tuning strategies?
Basis: Section 7.3: "The FIM can identify the most 'informative' neurons or weights – those to which the loss is most sensitive – and fine-tuning could be restricted to those parameters for new tasks. This would create a systematic approach to parameter-efficient fine-tuning rather than guessing which layers to train."
Unresolved because: No experiments or analysis are provided on using FIM diagonals for selective fine-tuning; current transfer learning experiments fine-tune all parameters.
Evidence needed: Experiments comparing full fine-tuning vs. FIM-guided selective fine-tuning on downstream tasks, measuring accuracy retention and parameter efficiency.

**Open Question 4**
Can custom CUDA kernels reduce AdaFisher's per-iteration overhead to match first-order optimizers like Adam?
Basis: Section 7.3: "Developing custom CUDA kernels for AdaFisher's core operations could greatly speed up the optimizer on GPU hardware. By optimizing memory access patterns and parallelizing FIM computations, we could reduce per-iteration overhead to be almost on par with Adam."
Unresolved because: While AdaFisher's epoch times are competitive, they still exceed Adam's; no low-level GPU optimization has been attempted.
Evidence needed: Benchmarks comparing optimized CUDA implementation against PyTorch-native Adam on identical hardware, measuring per-step latency and memory throughput.

## Limitations
- Implementation details for spatial averaging of convolutional Kronecker factors and FIM initialization are underspecified
- Min-Max normalization scope (per-layer vs global) and handling of near-zero values could significantly impact performance
- Warm-up schedule specifics and early-training FIM stability handling are not clearly defined
- No experiments on truly large-scale models (LLMs with hundreds of billions of parameters)

## Confidence

**High Confidence:** Claims about computational efficiency being comparable to first-order methods (Adam), and stability across various batch sizes and learning rates.

**Medium Confidence:** Claims about consistently outperforming Adam, K-FAC, and Shampoo across diverse architectures and datasets. Generalization improvement claims are supported by empirical evidence but depend on implementation specifics.

**Medium Confidence:** Claims about converging toward flatter minima and improved robustness. While theoretically sound, this connection requires more rigorous validation through sharpness/flatness measurements.

## Next Checks
1. Implement and validate the exact Kronecker factor extraction for convolutional layers, including the spatial averaging mechanism, and verify that FIM computation for BatchNorm layers follows Proposition 3.3.1 correctly.
2. Conduct ablation studies on the Min-Max normalization scope (per-layer vs global) and damping parameter λ to identify their impact on convergence and stability.
3. Test AdaFisher's performance on a held-out dataset (e.g., CIFAR-100) with identical hyperparameters to assess generalization claims beyond the reported experiments.