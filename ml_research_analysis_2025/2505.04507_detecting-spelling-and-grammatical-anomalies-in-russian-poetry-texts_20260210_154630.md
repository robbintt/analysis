---
ver: rpa2
title: Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts
arxiv_id: '2505.04507'
source_url: https://arxiv.org/abs/2505.04507
tags:
- text
- dataset
- poetry
- texts
- rupor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting spelling and grammatical
  anomalies in Russian poetry texts, which is critical for improving the quality of
  training datasets used in generative models for creative tasks. The authors propose
  a comprehensive evaluation of unsupervised and supervised anomaly detection approaches,
  utilizing synthetic data and human-labeled datasets.
---

# Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts

## Quick Facts
- arXiv ID: 2505.04507
- Source URL: https://arxiv.org/abs/2505.04507
- Reference count: 40
- Primary result: Perplexity is unreliable for poetry anomaly detection; supervised classifiers on synthetic data achieve F0.5 0.80-0.86

## Executive Summary
This paper addresses the challenge of detecting spelling and grammatical anomalies in Russian poetry texts, which is critical for improving the quality of training datasets used in generative models for creative tasks. The authors propose a comprehensive evaluation of unsupervised and supervised anomaly detection approaches, utilizing synthetic data and human-labeled datasets. They introduce the RUPOR dataset, a novel collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection. The study demonstrates that perplexity is not a reliable indicator of linguistic anomalies in poetry, and outlier detection methods fail to exceed random-guess baselines. However, supervised classification methods trained on synthetic data achieve moderate to strong detection performance. Language models pretrained extensively on Russian-language data outperform modern multilingual models, even those with larger capacities. These findings highlight the importance of domain-specific data and tailored approaches for effective grammatical error detection in poetry.

## Method Summary
The authors developed a comprehensive framework for detecting linguistic anomalies in Russian poetry through multiple approaches. They created synthetic_GED, a large dataset of ~1M (corrupted, correct) pairs generated by applying rule-based distortions to clean text sources including Wikipedia titles, poetry fragments, and LLM-generated text. The distortion rules covered spelling errors, grammatical violations, tokenization changes, punctuation errors, and preposition manipulation. For evaluation, they constructed RUPOR, a human-labeled dataset containing 1,000 balanced samples of original and corrupted Russian poetry and prose. The study compared multiple detection approaches including perplexity scoring, outlier detection using PYOD, zero-shot LLM prompting, and supervised binary classification using various transformer architectures. Models were evaluated using F0.5 score to emphasize precision over recall.

## Key Results
- Perplexity increases after corrections in 13% of RUPOR poetry cases (Δppl<0), proving it unreliable for anomaly detection
- Outlier detection methods (PYOD on embeddings) achieve F0.5≈0.56, failing to exceed random-guess baselines
- Supervised classification on synthetic data achieves F0.5=0.802-0.863 depending on model architecture
- Language-specific models (FRED-T5-1.7B, ruRoberta-large) outperform larger multilingual models despite smaller size
- Domain shift is significant: F0.5 drops from 0.863 (prose) to 0.802 (poetry) on RUPOR dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised binary classification trained on synthetically distorted text achieves moderate-to-strong detection of linguistic anomalies in Russian poetry.
- Mechanism: Rule-based distortions simulate real errors by altering grammatical forms, introducing spelling errors, manipulating prepositions, and merging/splitting tokens. Fine-tuning a transformer on these (corrupted, original) pairs teaches the model to discriminate anomalous from acceptable text.
- Core assumption: Synthetic distortions adequately approximate the distribution of real human errors in poetry.
- Evidence anchors:
  - [abstract] "Supervised classification methods trained on synthetic data achieve moderate to strong detection performance."
  - [Section 4.2] "Each sample in the dataset consists of an original text (e.g. a poem stanza) and its distorted version. The distortions are generated using a combination of handcrafted rules and statistical substitutions."
  - [corpus] Limited direct replication in corpus; related work focuses on Chinese GEC and general multilingual correction, not poetry-specific synthetic training.
- Break condition: KL divergence between synthetic and real error distributions becomes too large; novel poetic constructions (neologisms, zaum) are systematically flagged as anomalies.

### Mechanism 2
- Claim: Perplexity is an unreliable indicator of linguistic anomalies in poetry, contrary to its common use in prose evaluation.
- Mechanism: Perplexity measures token-level predictability; in poetry, length effects and unconventional but grammatical constructions cause perplexity to vary inversely with correctness. Corrections that increase token count can paradoxically increase perplexity.
- Core assumption: Lower perplexity correlates with higher grammaticality.
- Evidence anchors:
  - [abstract] "Perplexity is not a reliable indicator of linguistic anomalies in poetry."
  - [Section 5.1] "Up to 13% in case of RUPOR poetry when perplexity increases after correcting defects, that is, Δppl<0."
  - [corpus] Not directly addressed in neighbors; Kuribayashi et al. (2021) cited in paper supports general unreliability claim.
- Break condition: Text length is strictly controlled and error types are limited to those that substantially increase token unpredictability without length changes.

### Mechanism 3
- Claim: Models pretrained extensively on Russian-language data outperform larger multilingual models for grammatical error detection in Russian poetry.
- Mechanism: Domain-specific pretraining encodes Russian morphological and syntactic patterns more densely, improving the model's ability to detect violations even when overall capacity is lower.
- Core assumption: Language-specific pretraining transfers more effectively to domain-specific GED than multilingual exposure.
- Evidence anchors:
  - [abstract] "Language models pretrained extensively on Russian-language data outperform modern multilingual models, even those with larger capacities."
  - [Section 5.4, Table 7] FRED-T5-1.7B achieves F0.5=0.863 vs. Qwen2.5-3B (multilingual) F0.5=0.810 on RUPOR poetry.
  - [corpus] No direct neighbors comparing Russian-specific vs. multilingual models for poetry GED.
- Break condition: Multilingual models with substantial Russian pretraining data close the gap; poetry-specific fine-tuning data is insufficient regardless of base model.

## Foundational Learning

- Concept: **Cross-sentence Grammatical Error Detection**
  - Why needed here: Poetry lacks clear sentence boundaries due to enjambment; standard GED tools assume sentence-level input.
  - Quick check question: Why would a sentence tokenizer fail on a poem where a clause spans three lines?

- Concept: **F0.5 Score (Precision-weighted F-measure)**
  - Why needed here: The paper uses F0.5 to penalize false positives more heavily—critical when filtering training data where over-removal is costly.
  - Quick check question: Given precision=0.85 and recall=0.60, compute F0.5 using the formula in Section 5.

- Concept: **Domain Shift in NLP**
  - Why needed here: Poetry exhibits systematic deviations from prose (inversion, ellipsis, neologisms); models trained on prose struggle to generalize.
  - Quick check question: Name two syntactic features that distinguish Russian poetry from Russian prose per Section 3.

## Architecture Onboarding

- Component map:
  Synthetic_GED generator (rule-based distortion engine + correct text sources) -> Binary classifier backbone (transformer + classification head) -> RUPOR dataset (human-labeled poetry/prose pairs, held-out for evaluation only) -> Evaluation harness (F0.5 scoring, class balancing via undersampling)

- Critical path:
  1. Ingest correct text sources (Wikipedia titles, poetry fragments, LLM-generated text)
  2. Apply distortion rules to generate ~1M (corrupted, correct) pairs
  3. Fine-tune transformer as binary classifier on synthetic data
  4. Evaluate on balanced RUPOR poetry subset (F0.5 metric)

- Design tradeoffs:
  - Synthetic vs. human-labeled training: Scalable but risks distribution mismatch; paper reports KL divergence of 6.06 between RUPOR poetry and synthetic_GED edits
  - Model capacity vs. inference cost: FRED-T5-1.7B (best F0.5=0.863) vs. ruRoberta-large (F0.5=0.802, faster)
  - Precision vs. recall weighting: F0.5 prioritizes avoiding false positives; use F1 or F2 if recall is more critical

- Failure signatures:
  - Outlier detection (PYOD on embeddings): F0.5≈0.56 (random baseline)
  - Zero-shot LLM prompting: F0.5 ranges 0.39–0.58 depending on model
  - Perplexity thresholding: High variance, length-dependent artifacts
  - Decoder-only (rugpt3medium): Wide confidence interval (0.535–0.694)

- First 3 experiments:
  1. Replicate baseline: Fine-tune ruRoberta-large on synthetic_GED, evaluate F0.5 on 1,000 balanced RUPOR poetry samples.
  2. Ablate distortion types: Train five models, each excluding one distortion category (spelling, grammar, tokenization, punctuation, prepositions); measure impact on F0.5.
  3. Cross-domain probe: Evaluate the same classifier on RUPOR prose vs. poetry to quantify domain shift within Russian; expect prose F0.5≥poetry F0.5.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic training data exhibits significant distributional gap from real poetic anomalies (KL divergence 6.06)
- RUPOR dataset is relatively small (1,000 balanced samples) and represents single annotator judgments
- Study focuses only on Russian language, limiting cross-linguistic generalization

## Confidence
**High Confidence**: The finding that perplexity is unreliable for poetry anomaly detection is well-supported by quantitative evidence showing inverse correlations between perplexity and correctness in corrected text. The demonstration that language-specific pretraining outperforms larger multilingual models is also robust, with FRED-T5-1.7B achieving F0.5=0.863 versus Qwen2.5-3B's F0.5=0.810 on RUPOR poetry.

**Medium Confidence**: The claim that supervised classification on synthetic data achieves "moderate to strong" performance is supported but tempered by the distributional gap. While F0.5 scores of 0.802-0.863 are reasonable, they may not generalize to truly novel poetic constructions or the full diversity of Russian poetry styles.

**Low Confidence**: The assertion that outlier detection methods "fail to exceed random-guess baselines" is based on limited experiments with PYOD. The paper doesn't explore alternative outlier detection architectures or ensemble approaches that might perform better.

## Next Checks
1. **Distribution Alignment Test**: Compute and compare the distribution of error types in synthetic_GED versus RUPOR human-labeled data using chi-squared goodness-of-fit testing. Specifically measure whether the frequency of spelling vs. grammatical vs. tokenization errors matches between datasets.

2. **Cross-Annotator Agreement Study**: Evaluate the RUPOR classifier's performance across multiple human annotators' judgments of the same poetic samples. This would quantify the subjectivity in anomaly detection and establish an upper bound for automated detection performance.

3. **Domain Generalization Benchmark**: Test the best-performing synthetic-trained classifier (FRED-T5-1.7B) on poetry from different historical periods and styles (e.g., classical vs. avant-garde) to measure performance degradation across poetic domains. This would reveal whether the model captures general poetic anomaly detection or overfits to the RUPOR style.