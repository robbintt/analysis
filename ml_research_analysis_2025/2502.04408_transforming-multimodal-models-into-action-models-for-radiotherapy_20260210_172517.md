---
ver: rpa2
title: Transforming Multimodal Models into Action Models for Radiotherapy
arxiv_id: '2502.04408'
source_url: https://arxiv.org/abs/2502.04408
tags:
- treatment
- dose
- angles
- planning
- radiotherapy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel framework that transforms a large
  multimodal foundation model into an action model for radiotherapy treatment planning
  using a few-shot reinforcement learning approach. The method leverages the model's
  pre-existing knowledge of physics, radiation, and anatomy to iteratively improve
  treatment plans via a Monte Carlo simulator.
---

# Transforming Multimodal Models into Action Models for Radiotherapy

## Quick Facts
- arXiv ID: 2502.04408
- Source URL: https://arxiv.org/abs/2502.04408
- Reference count: 37
- Primary result: GPT-4V-based framework outperforms DQN baseline in radiotherapy planning with mean reward -211.88 vs -259.26

## Executive Summary
This study introduces a novel framework that transforms a large multimodal foundation model into an action model for radiotherapy treatment planning using a few-shot reinforcement learning approach. The method leverages the model's pre-existing knowledge of physics, radiation, and anatomy to iteratively improve treatment plans via a Monte Carlo simulator. When applied to prostate cancer data, the approach significantly outperformed conventional RL-based methods, achieving higher reward scores and more optimal dose distributions. This proof-of-concept demonstrates the potential for integrating advanced AI models into clinical workflows to enhance the speed, quality, and standardization of radiotherapy treatment planning.

## Method Summary
The method repurposes GPT-4V as an action model for radiotherapy planning, using MatRAD Monte Carlo simulator as the environment. The model receives CT scan slices and current dose maps as visual input, outputs gantry angles, and receives scalar reward feedback. Through iterative refinement without weight updates, the model optimizes treatment plans by leveraging its pre-trained knowledge. A DQN baseline with 3D CNN architecture trained for 3000 episodes provides comparison. The reward function balances PTV dose homogeneity against OAR sparing penalties.

## Key Results
- Mean reward of -211.88 for GPT-4V approach vs -259.26 for DQN baseline
- More homogeneous dose distribution in PTV region
- Lower dose to critical organs-at-risk compared to baseline
- Sample-efficient approach requiring fewer iterations than traditional RL

## Why This Works (Mechanism)

### Mechanism 1
The system creates an "action loop" where a multimodal foundation model uses its pre-trained knowledge to propose radiotherapy gantry angles, receives feedback via a Monte Carlo simulation, and iteratively refines its plan. This "thinking-in-context" approach bypasses the need to learn the entire domain from scratch, which is the main bottleneck for traditional Reinforcement Learning (RL).

### Mechanism 2
The system's reward function acts as a powerful, domain-translated signal, guiding the generalist MLM to act as a specialist radiotherapy planner. The reward function mathematically encodes the clinical goals of maximizing tumor dose and minimizing healthy tissue damage.

### Mechanism 3
The use of a Monte Carlo simulator (MatRAD) as the "environment" is a critical enabler, providing a realistic, physics-based feedback loop that avoids the need for a massive dataset of pre-computed plans.

## Foundational Learning

### Concept: Action Model / Agent
- **Why needed here:** This is the core paradigm. The paper doesn't just use an MLM for text generation; it "transforms" it into an **action model**. An action model can perceive an environment (via CT images), take actions (output gantry angles), and receive feedback (reward).
- **Quick check question:** What are the three components the multimodal model interacts with in its role as an action model? (A: The environment/state, the action it can take, and the feedback/reward it receives).

### Concept: Reinforcement Learning (RL) vs. Few-Shot Learning
- **Why needed here:** The paper's key innovation is framing the problem as RL but solving it with **few-shot learning**. Traditional RL requires thousands of episodes and weight updates. Their method uses the pre-trained MLM's reasoning to improve after only a few iterations.
- **Quick check question:** How does the "Text-to-Plan" model improve its performance compared to the "DQN" baseline? (A: The DQN model learns from scratch by updating its neural network weights over thousands of episodes. The Text-to-Plan model uses its pre-existing knowledge and refines its output via few-shot prompting, without any weight updates).

### Concept: Reward Function in Medical Domains
- **Why needed here:** In safety-critical fields like medicine, the **reward function** is the single point where clinical goals are translated for the AI. It defines success. A poorly designed reward function can lead to dangerous behavior.
- **Quick check question:** According to the paper's reward function, what two competing objectives does it balance? (A: It encourages dose homogeneity within the Planned Target Volume (PTV) and penalizes excessive doses to Organs at Risk (OARs)).

## Architecture Onboarding

### Component Map:
Multimodal Agent (GPT-4V) -> Environment (MatRAD) -> Prompt Orchestrator -> Multimodal Agent (repeat)

### Critical Path:
User/Prompt -> LLM -> (JSON Action) -> MatRAD Simulator -> (State & Reward) -> Prompt Orchestrator -> LLM (repeat). The performance of the entire system hinges on the LLM's ability to reason about the visual state and numerical reward to produce a better action in the next step.

### Design Tradeoffs:
- **Generalization vs. Specificity:** Using a general-purpose MLM offers broad knowledge but may lack deep, specialized radiotherapy physics compared to a dedicated model trained on medical text.
- **Sample Efficiency vs. Control:** The few-shot approach is vastly more sample-efficient than training a DQN from scratch, but offers less fine-grained control over the learned policy.
- **Cost:** Each iteration requires a call to a powerful (likely proprietary) LLM, incurring API costs and latency, whereas a trained DQN model runs inference locally and cheaply.
- **2D vs. 3D:** The LLM's vision capabilities are limited to 2D slices, potentially missing crucial 3D spatial relationships critical for avoiding OARs.

### Failure Signatures:
- **Hallucinated Actions:** The LLM outputs gantry angles outside the valid range or in a format the environment parser cannot understand.
- **Reward Hacking:** The LLM discovers a configuration that maximizes the simple reward function but is clinically nonsensical.
- **Context Window Saturation:** In a long iterative process, the prompt's context window may fill up, causing the model to "forget" earlier reasoning.
- **Lack of Convergence:** The reward score oscillates or degrades instead of improving.

### First 3 Experiments:
1. **Baseline Reproduction:** Implement the MatRAD environment and the random agent baseline. Verify the reward scores and dose distributions match the paper's baseline to ensure the environment is correctly configured.
2. **Ablation on Prompting Strategy:** Compare the "Text-to-Plan" performance using different prompts. One prompt gives only the reward score; another gives the reward plus a textual summary of the DVH; another provides only visual feedback.
3. **Model Substitution:** Replace the proprietary GPT-4V backbone with an open-source multimodal model (e.g., LLaVA) or a medical-specific LLM. This tests the robustness of the "action model" framework.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does integrating 3D spatial processing capabilities significantly improve planning accuracy compared to the current 2D slice-based limitation?
- **Basis in paper:** [explicit] The authors state that the vision capabilities of the underlying model are "limited to 2D slice representations of data, lacking a comprehensive understanding of 3D structures."
- **Why unresolved:** The current model processes 3D CT data as 2D slices, potentially missing critical spatial relationships required for complex treatment planning.
- **What evidence would resolve it:** Implementation of 3D vision encoders and a comparative analysis of planning accuracy against the 2D baseline.

### Open Question 2
- **Question:** Can specialized medical foundation models replace general-purpose models to enhance the reliability and safety of the planning process?
- **Basis in paper:** [explicit] The paper notes that general LLMs are "not sufficiently reliable for providing medical information" and suggests "Foundation medical models... are promising candidates."
- **Why unresolved:** The proof-of-concept relies on GPT-4V, which is not fine-tuned for medical safety, posing a barrier to clinical deployment.
- **What evidence would resolve it:** Benchmarking the framework using domain-specific medical LLMs to evaluate improvements in safety and hallucination reduction.

### Open Question 3
- **Question:** How does the text-to-plan framework compare to established clinical automated planning standards, such as Knowledge-Based Planning (KBP)?
- **Basis in paper:** [inferred] The study compares the proposed method only against random and Deep Q-Network (DQN) baselines, but does not evaluate performance relative to commercial or clinical standard-of-care planning systems.
- **Why unresolved:** Outperforming a DQN baseline does not establish clinical utility or superiority over existing automated tools used in practice.
- **What evidence would resolve it:** A comparative study evaluating plan quality metrics against clinically approved plans or commercial KBP solutions.

## Limitations
- Relies on proprietary GPT-4V model, limiting reproducibility and scalability
- Current 2D slice-based visual representation may miss critical 3D spatial relationships for OAR avoidance
- Reward function may not capture all clinically relevant plan quality metrics for complex treatment modalities

## Confidence

### High Confidence:
- Core mechanism of transforming multimodal model into action model using iterative Monte Carlo simulation is well-documented and theoretically sound

### Medium Confidence:
- Empirical results showing superior performance over DQN baselines are convincing but limited to single cancer type and treatment modality
- Claims about sample efficiency and speed advantages are supported but need broader validation

### Low Confidence:
- Long-term clinical utility and safety of generated plans remain unproven
- Reliance on LLMs not fine-tuned for medical safety directly challenges foundational assumptions

## Next Checks
1. **Ablation Study on Visual Representation:** Test whether model performance degrades when presented with random slice orders, downsampled images, or when critical OAR slices are omitted.

2. **Cross-Cancer Generalization Test:** Apply the exact same framework to head-and-neck or lung cancer cases using the same reward function and prompts to measure transferability.

3. **Clinical Safety Audit:** Have radiation oncologists review dose distributions from top-performing plans for both MLM and DQN approaches, specifically looking for clinically unacceptable hot spots, cold spots, or OAR violations.