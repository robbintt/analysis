---
ver: rpa2
title: 'Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts
  into Editable Diagram Code'
arxiv_id: '2512.02170'
source_url: https://arxiv.org/abs/2512.02170
tags:
- mermaid
- diagram
- code
- flowchart
- flowcharts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Flowchart2Mermaid is a web system that converts flowchart images
  into editable Mermaid.js code using vision-language models. The system generates
  initial Mermaid code from an uploaded image, then allows users to refine the diagram
  through inline text editing, drag-and-drop node insertion, and natural-language
  commands via an AI assistant.
---

# Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code

## Quick Facts
- arXiv ID: 2512.02170
- Source URL: https://arxiv.org/abs/2512.02170
- Reference count: 23
- Large VLMs achieve entity F1-scores above 0.94 and structural accuracy near 1.0 in flowchart-to-Mermaid conversion.

## Executive Summary
Flowchart2Mermaid is a web-based system that converts flowchart images into editable Mermaid.js code using vision-language models. The system combines initial VLM-based code generation with a mixed-initiative interface that allows users to refine diagrams through natural language commands, drag-and-drop editing, and inline text changes. We evaluated five models (GPT-4.1, GPT-4.1-mini, GPT-4o, GPT-4o-mini, Gemini-2.5-Flash) on 200 diagrams from the FLOWVQA dataset, finding that all large models achieved high structural fidelity while smaller variants trade quality for latency.

## Method Summary
The system uses VLMs to generate Mermaid.js code from flowchart images, guided by a structured system prompt. A Node.js backend serves as an adapter between the frontend and external VLM APIs, handling image encoding, prompt construction, and output normalization. The frontend provides a bidirectional interface where users can edit code directly or issue natural language commands via an AI assistant, with all changes synchronizing between the visual and textual representations. Evaluation uses both symbolic metrics (entity/relation extraction F1 scores, SBERT cosine similarity) and LLM-based structural assessment via GPT-4.1 judge.

## Key Results
- All large models (GPT-4.1, Gemini-2.5-Flash) achieved entity F1-scores above 0.94 and structural accuracy near 1.0
- GPT-4o-mini showed high syntax validity (0.998) but substantially lower structural completeness (0.861) due to missing edges
- The mixed-initiative interface enables iterative refinement while maintaining version-controllable Mermaid code
- System prompt engineering proved critical for achieving high structural fidelity from off-the-shelf VLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vision-Language Models (VLMs) can translate flowchart topology into valid syntax when constrained by a structured system prompt.
- **Mechanism:** The system encodes the uploaded image as Base64 and pairs it with a "carefully engineered system prompt" that specifies expected elements (nodes, edges) and output constraints (Mermaid syntax), coercing the VLM to map visual spatial graphs onto linear textual representations.
- **Core assumption:** The VLM has sufficient pre-training data involving code and diagrams to generalize spatial relationships into logical syntax without explicit fine-tuning.
- **Evidence anchors:**
  - [Section 4.1] "Inference is guided by a carefully engineered system prompt that specifies... structural constraints, and output format."
  - [Section 2] Notes that foundation models demonstrate "strong priors about graphical layout and syntax."
  - [Corpus] Neighbor paper "Arrow-Guided VLM" confirms that standard VLMs often misinterpret directional arrows, suggesting Flowchart2Mermaid relies heavily on prompt engineering to overcome this default weakness.
- **Break condition:** If the input image contains ambiguous connectors or non-standard symbols that fall outside the prompt's definitions, the VLM may hallucinate connections or fail to parse the topology.

### Mechanism 2
- **Claim:** Mixed-initiative refinement mitigates the brittleness of one-shot generation by synchronizing visual edits with underlying code.
- **Mechanism:** The architecture implements a bidirectional binding between the Mermaid code and the rendered view. When a user issues a natural language command, the system intercepts this intent, updates the text via an LLM, and re-renders the visual, ensuring the textual representation remains version-controllable.
- **Core assumption:** Users can effectively identify semantic errors in the visual representation that the VLM missed, and the "AI assistant" can reliably parse modification instructions without corrupting the existing graph structure.
- **Evidence anchors:**
  - [Abstract] "The interface supports mixed-initiative refinement... [and] a structured, version-controllable textual representation that remains synchronized."
  - [Section 4.2] "The interface maintains bidirectional consistency between textual and graphical representations."
  - [Corpus] Related tools (e.g., FlowchartAI) are cited as lacking this "deep coupling between recognition, code generation, and iterative refinement."
- **Break condition:** If the AI assistant introduces syntax errors during natural language refinement, the rendering engine may fail, breaking the synchronization loop.

### Mechanism 3
- **Claim:** Smaller model variants trade structural completeness for latency, often preserving syntax while losing semantic edges.
- **Mechanism:** The evaluation reveals a size-based capability gap. While smaller models (e.g., GPT-4o-mini) achieve near-perfect syntax validity (0.998), they struggle with relation extraction (F1 0.743). This suggests compression preserves formal grammar rules but degrades the visual reasoning required to detect specific graph edges.
- **Core assumption:** High syntax validity does not imply high semantic fidelity; a diagram can be renderable yet structurally incomplete.
- **Evidence anchors:**
  - [Section 6] "GPT-4o-mini... tends to generate syntactically correct but structurally incomplete diagrams."
  - [Table 2] Shows GPT-4o-mini has SV=0.998 but Completeness=0.861.
  - [Corpus] "Structure-aware Contrastive Learning" emphasizes that general multimodal models often lack specialized structural alignment, which explains why mini-models fail on relationship extraction.
- **Break condition:** If a use case requires strict logical correctness (e.g., generating executable code from the flowchart), smaller models are unreliable despite their speed.

## Foundational Learning

- **Concept: Mermaid.js Syntax**
  - **Why needed here:** This is the intermediate representation (IR) for the system. Understanding how nodes `id[Label]` and edges `-->` are defined is required to debug why a visual edit might break the rendering or why the VLM output fails validation.
  - **Quick check question:** Given the Mermaid code `A --> B`, how would you modify the code to add a "Yes" label on the edge?

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The evaluation relies on a GPT-4.1 judge to score "Structural Accuracy" and "Semantic Fidelity." You must understand that these metrics are probabilistic assessments by a model, not deterministic graph isomorphism checks.
  - **Quick check question:** Why might an LLM judge give a high semantic score to a diagram that has a missing node, and how does the "Reconstructability Override" affect this?

- **Concept: Symbolic vs. Neural Extraction**
  - **Why needed here:** The paper distinguishes between symbolic metrics (exact entity/relation matching) and neural metrics (embedding similarity). To interpret the results, you must recognize that high cosine similarity can mask structural errors.
  - **Quick check question:** Why does GPT-4o-mini have a high Cosine Similarity (0.908) but a low Relation F1 (0.743)?

## Architecture Onboarding

- **Component map:** Frontend (HTML/JS/Tailwind) -> Backend (Node.js) -> External VLM APIs (OpenAI/Gemini)
- **Critical path:** User uploads image -> Frontend encodes to Base64 -> Backend constructs system prompt + image payload -> VLM returns raw Mermaid text -> Backend normalizes code and validates syntax -> Frontend renders diagram; user interactions update the code string locally or via the AI Assistant
- **Design tradeoffs:**
  - **Latency vs. Completeness:** Using GPT-4o-mini offers speed but risks structural hallucination (missing edges). GPT-4.1/Gemini-2.5-Flash offers higher relation F1 but slower response.
  - **Client vs. Server Processing:** The backend handles inference to keep the frontend lightweight, but this introduces network latency for every "natural language edit."
- **Failure signatures:**
  - **Hallucinated Edges:** Valid syntax, but the diagram shows connections not present in the source image (typical in GPT-4o-mini).
  - **Rendering Collapse:** VLM generates Markdown-wrapped code ("```mermaid...") that the sanitizer misses, causing the Mermaid.js parser to crash.
  - **Sync Drift:** Rapid drag-and-drop edits might conflict with AI-assisted text updates if the state synchronization isn't atomic.
- **First 3 experiments:**
  1. **Prompt Sensitivity Test:** Upload a complex flowchart with crossed lines (spaghetti layout) and compare outputs between the default prompt and a prompt explicitly forbidding "edge crossings" to see if the VLM can untangle the logic.
  2. **Model Drift Analysis:** Run the same 10 diagrams through GPT-4.1 and GPT-4o-mini. Manually verify if the "Relation F1" drop in the mini model is due to missing branches or mislabeled arrows.
  3. **Refinement Loop Stress Test:** Use the AI assistant to perform 5 consecutive "move node" or "rename" commands and check if the Mermaid code accumulates technical debt (e.g., unused node IDs) or maintains clean syntax.

## Open Questions the Paper Calls Out
- Can specialized fine-tuning of smaller Vision-Language Models (VLMs) achieve performance parity with large proprietary models in extracting structural relationships from flowcharts?
- Does the current prompt-based architecture generalize effectively to complex diagram types with different notational standards, such as UML or entity-relationship diagrams?
- To what extent does the reliance on GPT-4.1 as an automated evaluator introduce bias into the structural assessment of generated diagrams?

## Limitations
- Evaluation is restricted to a single flowchart dataset (FLOWVQA) with 200 diagrams, limiting generalizability to diverse visual styles and domains.
- The system prompt engineering is not fully disclosed, making it difficult to reproduce the VLM performance gains.
- Reliance on LLM-as-a-judge introduces potential subjectivity and may favor outputs from similar model families.
- Mixed-initiative refinement effectiveness is demonstrated anecdotally but lacks rigorous user study validation.

## Confidence
- **High Confidence:** The symbolic extraction metrics (entity/relation F1 scores above 0.94 for large models) are directly verifiable through string matching and are supported by multiple evaluation frameworks. The size-based capability gap between large and mini models is consistently observed across all metrics.
- **Medium Confidence:** The LLM-based structural evaluation scores depend on the judge's interpretation of "semantic fidelity" and "reconstructability," which may vary with prompt engineering. The high-level scores (SA, FA near 1.0) suggest strong performance but could be influenced by the judge's leniency toward syntactically valid but semantically incomplete diagrams.
- **Low Confidence:** The practical utility of the mixed-initiative refinement system is demonstrated anecdotally but not rigorously evaluated. User studies or task completion metrics would be needed to validate claims about the interface's effectiveness in real-world diagram editing scenarios.

## Next Checks
1. **Cross-Dataset Validation:** Test the system on alternative flowchart datasets (e.g., different domains, hand-drawn diagrams) to assess generalization beyond FLOWVQA.
2. **Ablation Study on Prompt Engineering:** Systematically vary prompt complexity and specificity to quantify the contribution of prompt design to VLM performance.
3. **Human Evaluation of Refinement Utility:** Conduct user studies comparing task completion time and error rates between the mixed-initiative interface and traditional Mermaid editing.