---
ver: rpa2
title: 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity'
arxiv_id: '2505.21411'
source_url: https://arxiv.org/abs/2505.21411
tags:
- experts
- expert
- pangu
- ascend
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Grouped Experts (MoGE), a novel
  architecture that groups experts and enforces balanced token-to-expert assignments
  within each group, effectively eliminating device load imbalance in distributed
  systems. Unlike conventional top-K routing, MoGE ensures equal expert activation
  per group, leading to more efficient computation across devices.
---

# Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity

## Quick Facts
- arXiv ID: 2505.21411
- Source URL: https://arxiv.org/abs/2505.21411
- Authors: Yehui Tang; Xiaosong Li; Fangcheng Liu; Wei Guo; Hang Zhou; Yaoyuan Wang; Kai Han; Xianzhi Yu; Jinpeng Li; Hui Zang; Fei Mi; Xiaojun Meng; Zhicheng Liu; Binfan Zheng; Can Chen; Youliang Yan; Ruiming Tang; Peifeng Qin; Xinghao Chen; Dacheng Tao; Yunhe Wang
- Reference count: 40
- Primary result: 72-billion-parameter MoE model achieving 1148-1528 tokens/s per card on Ascend 800I A2 with superior load balancing

## Executive Summary
This paper introduces Mixture of Grouped Experts (MoGE), a novel architecture that groups experts and enforces balanced token-to-expert assignments within each group, effectively eliminating device load imbalance in distributed systems. Unlike conventional top-K routing, MoGE ensures equal expert activation per group, leading to more efficient computation across devices. The authors build Pangu Pro MoE, a 72-billion-parameter model with 16 billion activated per token, optimized for Ascend NPUs. Extensive experiments demonstrate that MoGE achieves better expert load balancing, improved training and inference efficiency, and superior throughput compared to dense models.

## Method Summary
The paper presents MoGE, which partitions N experts into M non-overlapping groups and selects K' = K/M experts per group per token using local Top-K' selection on global softmax scores. This architecture achieves intrinsic load balance by construction. The 72B Pangu Pro MoE model uses 64 experts grouped into 8 groups of 8 experts each, with 1 expert activated per group per token. Training follows a three-phase curriculum (9.6T general tokens, 3T reasoning tokens, 0.4T annealing tokens) with hierarchical parallelism (TP=8, EP=2, PP=5, VPP=5) and post-training SFT + GRPO optimization. Ascend-specific kernel optimizations (MulAttention, SwiftGMM) maximize hardware utilization.

## Key Results
- Pangu Pro MoE achieves 1148-1528 tokens/s per card on Ascend 800I A2, outperforming 32B and 72B dense models
- Zero imbalance score (IS=0) demonstrates perfect load balance across devices
- Ranks among top-performing sub-100B parameter models, surpassing GLM-Z1-32B and Qwen3-32B across multiple benchmarks
- Achieves excellent cost-to-performance ratio on Ascend 300I Duo

## Why This Works (Mechanism)

### Mechanism 1: Group-Enforced Load Balancing via MoGE
Partitioning experts into groups and enforcing fixed activation counts per group guarantees inter-device load balance. With N=64 experts partitioned into M=8 groups, each group maps to a device and receives exactly K'=1 activated expert per token, ensuring equal computational load distribution.

### Mechanism 2: Global Softmax with Local Selection
Using global softmax scores for auxiliary loss while performing local Top-K selection preserves routing quality while enforcing structural balance. Compute global S = Softmax(W^T h), then perform Top-K' within each group. The auxiliary loss ℓ_aux = α Σ f_i p_i uses global softmax probabilities before masking.

### Mechanism 3: Hardware-Kernel Co-Design (MulAttention + SwiftGMM)
Ascend-specific fused kernel design for attention and grouped matrix multiplication reduces memory bandwidth bottlenecks. MulAttention uses large-packet KV transfer with dual-loop ping-pong scheduling. SwiftGMM employs tile-aware caching and dynamic GEMV/GEMM selection based on workload intensity.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) Routing**
  - Why needed: MoGE modifies standard Top-K routing; understanding baseline MoE is prerequisite to grasping why load imbalance occurs and how grouping addresses it.
  - Quick check: Given N=64 experts, K=8 activated per token, what is the probability that all 8 selected experts reside on the same device (M=8 devices)?

- **Concept: All-to-All Communication in Expert Parallelism**
  - Why needed: MoGE's load balancing directly reduces straggler effects in EP All-to-All; understanding communication patterns is essential for infrastructure optimization.
  - Quick check: In EP with M devices, what is the communication volume for routing tokens to experts and gathering outputs?

- **Concept: KV Cache Memory Footprint in Autoregressive Decoding**
  - Why needed: MulAttention and KV cache quantization target this bottleneck; decoding efficiency depends on minimizing KV transfer overhead.
  - Quick check: For a 48-layer model with hidden size 5120 and 8 KV heads, what is the KV cache size per token at FP16?

## Architecture Onboarding

- **Component map:** Input → RMSNorm → [Global Softmax Router] → [Group-wise Top-K'] → [Shared Expert (TP8)] + [Routed Experts (TP2+EP4)] → Reduce-Scatter → RMSNorm → GQA Attention (DP2+TP4) → AllGather → Output

- **Critical path:** 1. Router computes global softmax (Eq. 5) 2. Per-group Top-K' selection (K'=1 per group in Pangu Pro MoE) 3. Expert computation with hierarchical parallelism 4. Communication: Reduce-Scatter + AllGather (optimized from AllReduce)

- **Design tradeoffs:** TP vs EP balances memory vs load balance. TP2+EP4 balances memory (TP splits matrices) vs load balance (EP preserves expert integrity). Pure EP causes imbalance; pure TP reduces efficiency on non-square matrices.

- **Failure signatures:** Imbalance Score (IS) > 0 during profiling indicates routing not enforcing group constraints. MTE2 utilization <70% suggests kernel fusion not effectively overlapping compute/transfer. MFU degradation at scale suggests communication overhead dominating.

- **First 3 experiments:** 1. Routing validation: Log expert activation counts per device across 10K tokens; verify IS=0. If IS>0, check group assignment logic. 2. Kernel benchmarking: Profile MulAttention and SwiftGMM in isolation; target MTE2 >85% and compare against baseline unfused operators. 3. Scaling test: Run inference at batch sizes [1, 16, 64, 128] on Ascend 800I A2; verify throughput scales linearly until memory bandwidth saturation (~1148 tokens/s baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoGE's efficiency and load balancing scale with significantly larger expert counts (e.g., 128, 256, or 512 experts) and different group configurations?
- Basis in paper: The paper evaluates MoGE only on 64 experts with 8 groups of 8 experts each. The scalability to larger configurations is unstated.
- Why unresolved: The architectural design assumes a specific ratio between groups and devices; larger expert counts may introduce new communication patterns or routing complexities not observed at the 72B scale.
- What evidence would resolve it: Experiments training MoGE models with varied expert counts and group sizes, reporting training throughput, MFU, and inference latency across configurations.

### Open Question 2
- Question: To what extent do the hardware-specific optimizations (MulAttention, SwiftGMM, H2P parallelism) transfer to non-Ascend platforms such as NVIDIA GPUs or AMD accelerators?
- Basis in paper: Section 4 states optimizations are "specifically tailored for the Ascend NPUs" and "co-designed with the model architecture and Ascend's interconnect topology."
- Why unresolved: The kernel fusion strategies and parallelism configurations leverage Ascend-specific cache hierarchies (L0, L1, Unified Buffer) and interconnect topologies that differ from other hardware platforms.
- What evidence would resolve it: Comparative benchmarks of Pangu Pro MoE inference on different hardware platforms, with and without platform-specific optimizations, measuring throughput and efficiency gaps.

### Open Question 3
- Question: What is the impact of selecting more than one expert per group (K' > 1) on both model quality and system efficiency?
- Basis in paper: Section 2.2 notes "a common and particularly impactful configuration is when we want to select exactly one expert from each group, meaning K' = 1" and "Pangu Pro MoE follows this setting." No ablation on K' values is provided.
- Why unresolved: The trade-off between routing flexibility (higher K') and load balancing guarantees remains unexplored; higher K' might improve model capacity but could reduce throughput benefits.
- What evidence would resolve it: Controlled experiments varying K' while keeping total activated experts constant, measuring benchmark performance (MMLU, GSM8K, etc.) alongside inference throughput and load balance metrics.

### Open Question 4
- Question: Does the group-balanced routing constraint introduce any representational limitations or bias in expert specialization compared to unconstrained Top-K routing?
- Basis in paper: Section 5.4 shows expert specialization patterns differ across layers and tasks, and Figure 10 shows more balanced global distribution than DeepSeek-V2. However, whether this enforced balance limits the model's ability to develop highly specialized experts for narrow tasks is not analyzed.
- Why unresolved: The auxiliary loss encourages intra-group balance, but the interaction between this regularization and the natural emergence of expert specialization is not theoretically or empirically characterized.
- What evidence would resolve it: Comparative analysis of expert specialization entropy between MoGE and standard MoE models on diverse downstream tasks, including analysis of performance on tasks requiring highly specialized knowledge.

## Limitations

- The load balancing guarantees rely on specific constraints (K divisible by M, clean device-to-group mapping) that may not hold in all MoE configurations
- Ascend-specific optimizations show impressive results but lack cross-platform validation, limiting generalizability
- The relative importance of MoGE architecture vs. hardware optimizations and post-training in achieving performance gains is not clearly delineated

## Confidence

**High Confidence:** The core MoGE routing mechanism and load balancing claims are well-specified and directly measurable through imbalance score metrics.

**Medium Confidence:** The Ascend-specific kernel optimizations and their contribution to overall performance, as results are hardware-specific without cross-platform validation.

**Low Confidence:** The generalization of MoGE's load balancing benefits to arbitrary MoE configurations and the relative importance of MoGE vs. other design choices in achieving reported performance.

## Next Checks

1. **Cross-Platform Load Balancing Test:** Implement MoGE routing on GPU-based MoE models with varying group sizes (M=4, 8, 16) and activation counts (K=4, 8, 16). Measure imbalance scores across devices and compare against conventional Top-K routing. Verify that IS=0 holds across different configurations and that the load balancing benefit persists on non-Ascend hardware.

2. **Kernel Optimization Ablation:** Create a version of Pangu Pro MoE using standard unfused attention and matrix multiplication kernels while keeping all other aspects identical. Run inference throughput tests on Ascend 800I A2 at batch sizes [1, 16, 64, 128] and measure the performance gap. Quantify the exact contribution of MulAttention and SwiftGMM to the reported 1148-1528 tokens/s throughput.

3. **Routing Quality Degradation Analysis:** Systematically vary the auxiliary loss weight α in the MoGE routing objective (test values: 0.01, 0.1, 1.0, 10.0) and measure the trade-off between load balancing (IS) and model quality (perplexity on WikiText-2, accuracy on SuperGLUE). Determine the sensitivity of routing quality to this hyperparameter and identify the optimal balance point where load balancing is achieved without significant quality degradation.