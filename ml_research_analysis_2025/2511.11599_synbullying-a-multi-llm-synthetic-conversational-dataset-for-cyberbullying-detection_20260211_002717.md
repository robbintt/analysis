---
ver: rpa2
title: 'SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying
  Detection'
arxiv_id: '2511.11599'
source_url: https://arxiv.org/abs/2511.11599
tags:
- synthetic
- data
- authentic
- dataset
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynBullying, a synthetic multi-LLM dataset
  for cyberbullying detection. Generated using GPT-4o, Llama-3.3-70B, and Grok-2,
  it offers context-aware, multi-turn conversations with fine-grained annotations.
---

# SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection

## Quick Facts
- **arXiv ID**: 2511.11599
- **Source URL**: https://arxiv.org/abs/2511.11599
- **Reference count**: 0
- **Primary result**: Synthetic data alone transfers poorly to real-world detection, but augmentation with authentic data restores or improves performance.

## Executive Summary
This paper introduces SynBullying, a synthetic multi-LLM dataset for cyberbullying detection. Generated using GPT-4o, Llama-3.3-70B, and Grok-2, it offers context-aware, multi-turn conversations with fine-grained annotations. Evaluations across six dimensions show that Llama-3.3-70B best preserves authentic linguistic and conversational patterns, while GPT-4o excels at exposing model blind spots and Grok at worst-case testing. Synthetic data alone transfers poorly to real-world detection, but augmentation with authentic data restores or improves performance, demonstrating its value for scalable, ethically safe dataset creation.

## Method Summary
The authors create a synthetic cyberbullying dataset using three LLMs (GPT-4o, Llama-3.3-70B, Grok-2) with iterative prompt engineering to generate 40 conversations each across 4 bullying scenarios with 11 fictional teen participants. GPT-4o annotates all synthetic data with binary harmfulness labels and 11 fine-grained cyberbullying type labels. They evaluate dataset quality using lexical diversity (MTLD), sentiment analysis (VADER), toxicity scoring (ToxicBERT), profanity lexicons, and Jensen-Shannon Divergence. Performance is assessed using BERT-base classifiers with leave-one-conversation-out cross-validation across four experimental conditions: authentic→authentic, synthetic→authentic, authentic→synthetic, and augmentation.

## Key Results
- Llama-3.3-70B preserves authentic linguistic patterns best (lowest JSD), while GPT-4o exposes model blind spots and Grok enables worst-case testing
- Synthetic-only training achieves 53-56% Harm-F1 on authentic data (vs 68% baseline), showing poor transfer
- Augmenting authentic data with Llama-3.3-70B synthetic examples restores near-baseline performance (68.7% Harm-F1)
- LLM annotation reliability is substantial (Cohen's κ = 0.627) for binary classification but struggles with context-dependent categories like Defamation (F1 = 11.11%)

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Synthetic Data Augmentation
- Claim: Augmenting authentic training data with synthetically-generated multi-turn cyberbullying conversations can restore or improve classifier performance on real-world content.
- Mechanism: Authentic data captures real-world distribution but suffers from scarcity and ethical collection barriers. Synthetic data (particularly from Llama-3.3-70B) approximates authentic conversational patterns across lexical diversity, toxicity profiles, role distributions, and harm intensity. When combined, synthetic data provides additional training examples that expose the model to varied linguistic patterns while maintaining similarity to authentic content, improving generalization.
- Core assumption: The synthetic data sufficiently approximates the statistical distribution of authentic cyberbullying patterns such that learned features transfer positively.
- Evidence anchors:
  - [abstract] "Synthetic data alone transfers poorly to real-world detection, but augmentation with authentic data restores or improves performance"
  - [section 5.6.2] "Augmenting authentic data with synthetic examples restores near-baseline performance on Auth (Auth+LLaMA: 68.7% Harm-F1)"
  - [corpus] Weak direct corpus support; related papers address cyberbullying detection but not specifically synthetic data augmentation mechanisms.
- Break condition: When synthetic data diverges significantly from authentic distributions (e.g., GPT-4o's safety alignment or Grok's exaggerated toxicity), augmentation may introduce noise or bias that degrades performance.

### Mechanism 2: Multi-LLM Generation for Coverage of Behavior Spectrum
- Claim: Different LLMs produce synthetic data with distinct characteristics that serve complementary purposes in model development.
- Mechanism: Each LLM has inherent biases and alignment properties that manifest in generated content. Llama-3.3-70B produces linguistically authentic patterns; Grok generates high-toxicity content useful for stress-testing; GPT-4o creates safety-aligned content that exposes model blind spots. By leveraging multiple LLMs, the dataset covers a broader spectrum of potential harmful behaviors than any single model could provide.
- Core assumption: The variation across LLM outputs represents meaningful coverage of the cyberbullying behavior spectrum rather than random noise.
- Evidence anchors:
  - [abstract] "Llama-3.3-70B best preserves authentic linguistic and conversational patterns, while GPT-4o excels at exposing model blind spots and Grok at worst-case testing"
  - [section 5.6.2] Training solely on synthetic data shows LLaMA performing best on transfer to authentic data, Grok easiest to detect when reverse-transferring, and GPT largely undetected
  - [corpus] Limited corpus support for multi-LLM comparison mechanisms specifically.
- Break condition: If the behavioral differences are artifacts of model training rather than meaningful representations of bullying behavior, the diversity adds noise rather than utility.

### Mechanism 3: LLM-as-Annotator with Human-Level Reliability
- Claim: GPT-4o can annotate cyberbullying content at reliability levels comparable to human annotators, enabling scalable labeling.
- Mechanism: LLMs can process full conversational context to assess harmfulness, considering intent, discourse dynamics, and relational cues. When evaluated against human gold labels, GPT-4o achieves substantial agreement (Cohen's κ = 0.627) on binary classification and comparable or better agreement on several fine-grained CB types. This enables automated annotation at scale without the ethical concerns of exposing human annotators to harmful content.
- Core assumption: The annotation task involves pattern recognition and contextual understanding that LLMs can approximate through their training on human language data.
- Evidence anchors:
  - [abstract] "context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics"
  - [section 4.1] "Cohen's κ = 0.627 and Fleiss' κ = 0.625 indicate substantial agreement... GPT-4o can approximate human-level reliability"
  - [corpus] Adjacent papers use LLMs for detection but do not directly validate LLM-as-annotator reliability.
- Break condition: For context-dependent categories like Defamation (F1 = 11.11%), LLM annotations remain unreliable, suggesting the mechanism fails for subtle, pragmatic understanding tasks.

## Foundational Learning

- Concept: **Jensen-Shannon Divergence (JSD)**
  - Why needed here: JSD quantifies similarity between synthetic and authentic data distributions across harm intensity and CB types. Understanding this metric is essential for evaluating synthetic data quality.
  - Quick check question: If Auth vs GPT-4o has JSD=0.0002 for harm distribution but JSD=0.0317 for CB-type distribution, which dimension shows greater deviation?

- Concept: **Transfer Learning and Domain Shift**
  - Why needed here: The core finding is that synthetic-only training transfers poorly to authentic data, but augmentation helps. This requires understanding why features learned from synthetic data may not fully transfer.
  - Quick check question: Why does a classifier trained on Grok data (39.80% toxicity) perform moderately on authentic data (19.21% toxicity) rather than failing completely?

- Concept: **Inter-Annotator Agreement (Cohen's/Fleiss' Kappa)**
  - Why needed here: The paper validates LLM annotation quality against human baselines using these metrics. Understanding what constitutes "substantial" agreement is critical for interpreting results.
  - Quick check question: GPT-4o achieves κ=0.627 on binary classification while human-human agreement is 0.69. Is this gap practically significant?

## Architecture Onboarding

- Component map: Prompt Engineering → Multi-LLM Generation (GPT-4o/Llama/Grok) → GPT-4o Annotation (binary harmful + CB types) → Quality Evaluation (lexical/sentiment/role/harm/type distributions) → Classifier Training (BERT-base with linear head) → Cross-Validation (leave-one-conversation-out)

- Critical path: The prompt engineering phase directly determines the quality and realism of generated conversations. Poorly designed prompts produce conversations that diverge from authentic patterns, degrading downstream utility.

- Design tradeoffs:
  - Llama-3.3-70B: Best for realistic augmentation, preserves authentic patterns → use for training augmentation
  - Grok: Highest toxicity/aggression → use for stress-testing and worst-case evaluation
  - GPT-4o: Most lexically diverse but safety-aligned → use for exposing classifier blind spots, not for realistic training
  - GPT-4o annotation: High recall (0.825), moderate precision (0.688) → prioritizes coverage over false positive minimization

- Failure signatures:
  - Synthetic-only training achieving <55% Harm-F1 on authentic data (indicates distribution mismatch)
  - Low agreement on context-dependent CB types like Defamation (inherent task difficulty)
  - Grok data producing 53.56% harmful messages vs authentic 36.36% (overrepresentation of harm)

- First 3 experiments:
  1. Replicate binary harmful classification with GPT-4o annotations on the authentic dataset, computing Cohen's κ against provided gold labels to validate understanding of the annotation pipeline.
  2. Train BERT-base classifier on LLaMA synthetic data only, evaluate on authentic data, and compare Harm-F1 to the reported 53.4% baseline.
  3. Implement augmentation experiment (Auth + LLaMA → Auth) and verify that Harm-F1 improves or maintains near-baseline (target: ~68.7%).

## Open Questions the Paper Calls Out

- Can the SynBullying framework be adapted for low-resource languages, and does cross-lingual transfer improve cyberbullying detection where authentic annotated data is scarce?
- How do platform-specific interaction patterns (e.g., Twitter vs. WhatsApp vs. Instagram) affect the realism and transfer performance of synthetic cyberbullying data?
- Does expert social science validation confirm that synthetic conversations capture authentic cyberbullying dynamics, or do they diverge in psychologically meaningful ways?
- Can improved prompt engineering or annotation methods substantially improve detection quality for context-dependent cyberbullying types such as defamation (F1 = 11.11%) and encouragement to harassment (F1 = 47.54%)?

## Limitations
- The paper lacks published prompt templates, relying on appendix promises that weren't available in the current version
- Synthetic data generation depends on LLM safety alignment, which may not represent the full spectrum of cyberbullying behaviors
- The relatively small authentic dataset (10 conversations, 2192 messages) may limit statistical significance
- BERT-base classifier may not capture complex conversational dynamics as effectively as more sophisticated models

## Confidence
- **High Confidence**: Multi-LLM generation producing distinct behavioral patterns; augmentation restoring performance; lexical diversity and distribution comparisons
- **Medium Confidence**: LLM-as-annotator reliability claims (κ=0.627); synthetic data transfer learning limitations
- **Low Confidence**: Exact prompt engineering mechanisms; impact of censorship patterns on detection; long-term generalization beyond tested scenarios

## Next Checks
1. **Prompt Template Validation**: Obtain and test the actual prompt templates for data generation across all three LLMs. Compare generated outputs against the paper's reported distribution statistics to verify prompt effectiveness and model behavior consistency.

2. **Annotation Reliability Replication**: Replicate the LLM annotation process by having GPT-4o label the authentic dataset and computing Cohen's κ against the human gold labels. Compare this to the reported 0.627 to validate the annotation pipeline reliability.

3. **Distribution Divergence Analysis**: Calculate Jensen-Shannon Divergence between each synthetic dataset and the authentic data for both harm intensity and CB-type distributions. Verify that LLaMA shows lowest divergence (as claimed) while Grok shows highest, and assess whether these differences explain the transfer learning performance patterns.