---
ver: rpa2
title: 'MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series
  Classification using Momentum Encoder'
arxiv_id: '2508.08280'
source_url: https://arxiv.org/abs/2508.08280
tags:
- domain
- data
- target
- adaptation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoSSDA, a novel two-step momentum encoder-based
  framework for semi-supervised domain adaptation (SSDA) in multivariate time-series
  classification. The core idea is to learn robust, domain-invariant, and class-discriminative
  feature representations through a decoupled two-stage training process.
---

# MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder

## Quick Facts
- arXiv ID: 2508.08280
- Source URL: https://arxiv.org/abs/2508.08280
- Authors: Seonyoung Kim; Dongil Kim
- Reference count: 24
- Key outcome: Novel two-step momentum encoder-based framework for semi-supervised domain adaptation (SSDA) in multivariate time-series classification that achieves state-of-the-art performance

## Executive Summary
This paper proposes MoSSDA, a novel two-step momentum encoder-based framework for semi-supervised domain adaptation (SSDA) in multivariate time-series classification. The core idea is to learn robust, domain-invariant, and class-discriminative feature representations through a decoupled two-stage training process. The first stage employs a domain-invariant encoder with MMD loss and a mixup-enhanced positive contrastive module with a momentum encoder to learn rich representations from both source and target domains. The second stage trains a classifier on these learned features using limited labeled target data. Experiments on six diverse time-series datasets with three different backbones demonstrate that MoSSDA achieves state-of-the-art performance, significantly outperforming existing SSDA methods across various unlabeled ratios in the target domain. Ablation studies confirm the effectiveness of each module in improving performance.

## Method Summary
MoSSDA addresses the challenge of semi-supervised domain adaptation for multivariate time-series classification through a two-stage training framework. In the first stage, the model learns domain-invariant and class-discriminative features using a combination of Maximum Mean Discrepancy (MMD) loss for domain alignment and a momentum encoder-based positive contrastive learning module enhanced with mixup. The momentum encoder maintains an exponential moving average of the feature encoder parameters, providing consistent targets for contrastive learning. The mixup operation generates synthetic samples to enrich the positive pairs, improving the robustness of learned representations. In the second stage, a classifier is trained on the frozen features extracted by the encoder from the first stage, using only the limited labeled target domain data. This decoupled approach allows the model to first learn rich, transferable features and then adapt to the specific target domain with minimal labeled data.

## Key Results
- MoSSDA achieves state-of-the-art performance on six diverse time-series datasets with three different backbone architectures
- Significantly outperforms existing SSDA methods across various unlabeled ratios in the target domain
- Ablation studies confirm the effectiveness of each module: MMD loss, momentum encoder, and mixup in the contrastive learning framework

## Why This Works (Mechanism)
MoSSDA's effectiveness stems from its two-stage approach that decouples feature learning from classifier adaptation. The first stage learns robust, domain-invariant features by minimizing the discrepancy between source and target domains (via MMD loss) while simultaneously maximizing the similarity between positive pairs (original and augmented samples) through momentum-based contrastive learning. The momentum encoder provides stable targets for contrastive learning by maintaining an exponential moving average of the feature encoder, preventing the "model collapse" issue common in self-supervised learning. The mixup operation generates synthetic samples that enrich the positive pairs, improving the model's ability to learn discriminative features. The second stage then leverages these learned features to train a classifier specifically for the target domain using the limited labeled data available, allowing for efficient adaptation without disrupting the feature learning process.

## Foundational Learning
- **Maximum Mean Discrepancy (MMD)**: A kernel-based distance metric that measures the difference between distributions in reproducing kernel Hilbert spaces. Why needed: To align the source and target domain distributions and learn domain-invariant features. Quick check: Verify MMD implementation by testing on synthetic datasets with known distribution shifts.
- **Momentum Encoder**: Maintains an exponential moving average of the feature encoder parameters to provide stable targets for contrastive learning. Why needed: Prevents model collapse and provides consistent representations for positive pairs in contrastive learning. Quick check: Compare feature stability with and without momentum updates during training.
- **Mixup Augmentation**: Generates synthetic training examples by linearly interpolating between pairs of samples and their labels. Why needed: Enriches positive pairs in contrastive learning and improves model robustness to variations in the data. Quick check: Evaluate model performance with different mixup ratios to find optimal balance.
- **Contrastive Learning**: A self-supervised learning approach that learns representations by comparing similar (positive) and dissimilar (negative) pairs of samples. Why needed: To learn discriminative features without relying on labels in the source domain. Quick check: Analyze the quality of learned features using nearest neighbor retrieval on validation data.
- **Domain Adaptation**: The process of adapting a model trained on a source domain to perform well on a target domain with different data distribution. Why needed: To leverage labeled data from the source domain when labeled data is scarce in the target domain. Quick check: Measure domain shift using MMD before and after adaptation on validation splits.

## Architecture Onboarding
- **Component Map**: Time-series data -> Feature Encoder (with MMD loss) -> Momentum Encoder -> Contrastive Loss (with Mixup) -> Feature Representation -> Classifier (Stage 2) -> Classification Output
- **Critical Path**: The critical path for inference involves the feature encoder extracting representations from input time-series data, which are then passed to the classifier trained in the second stage to produce the final classification output.
- **Design Tradeoffs**: The two-stage approach trades off some end-to-end optimization for more stable and robust feature learning. The momentum encoder adds computational overhead but provides significant benefits in contrastive learning stability. Mixup augmentation increases training time but improves generalization.
- **Failure Signatures**: If the MMD loss is not properly tuned, the model may fail to learn domain-invariant features, leading to poor performance on the target domain. Insufficient positive pairs in contrastive learning (due to limited data or poor mixup implementation) can result in features that are not discriminative enough for the target task.
- **3 First Experiments**:
  1. Verify domain alignment by comparing MMD values between source and target domains before and after the first stage of training.
  2. Evaluate the quality of learned features using a simple k-NN classifier on the target domain validation set after the first stage.
  3. Test the robustness of the momentum encoder by comparing contrastive loss stability with and without momentum updates.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited theoretical grounding for the two-stage approach and the effectiveness of the momentum encoder and mixup in the contrastive learning framework for time-series data
- Does not address potential issues with class imbalance in the target domain, which could affect the classifier's performance in the second stage
- Scalability to very high-dimensional time-series data or extremely large datasets is not discussed

## Confidence
- **High Confidence**: The empirical results demonstrating state-of-the-art performance on six diverse datasets with three different backbones
- **Medium Confidence**: The effectiveness of the two-stage training process and the individual contributions of MMD loss, momentum encoder, and mixup in the contrastive learning framework
- **Medium Confidence**: The robustness of MoSSDA across various unlabeled ratios in the target domain, though this is primarily based on experimental evidence

## Next Checks
1. Conduct theoretical analysis to formally justify the two-stage training process and the effectiveness of the momentum encoder and mixup in the contrastive learning framework for time-series data.
2. Evaluate MoSSDA on datasets with significant class imbalance in the target domain to assess its robustness to this common real-world scenario.
3. Test the scalability of MoSSDA on high-dimensional time-series data or extremely large datasets to understand its limitations and potential bottlenecks.