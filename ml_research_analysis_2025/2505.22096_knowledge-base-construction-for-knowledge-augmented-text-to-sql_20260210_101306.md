---
ver: rpa2
title: Knowledge Base Construction for Knowledge-Augmented Text-to-SQL
arxiv_id: '2505.22096'
source_url: https://arxiv.org/abs/2505.22096
tags:
- knowledge
- base
- text-to-sql
- queries
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KAT-SQL, a method for knowledge base construction
  and augmentation to improve text-to-SQL performance. The core idea is to automatically
  build a comprehensive knowledge base from available query-schema pairs and their
  associated knowledge, then retrieve and refine relevant knowledge to assist SQL
  generation.
---

# Knowledge Base Construction for Knowledge-Augmented Text-to-SQL

## Quick Facts
- arXiv ID: 2505.22096
- Source URL: https://arxiv.org/abs/2505.22096
- Reference count: 29
- This paper proposes KAT-SQL, a method for knowledge base construction and augmentation to improve text-to-SQL performance.

## Executive Summary
This paper proposes KAT-SQL, a method for knowledge base construction and augmentation to improve text-to-SQL performance. The core idea is to automatically build a comprehensive knowledge base from available query-schema pairs and their associated knowledge, then retrieve and refine relevant knowledge to assist SQL generation. Unlike prior work that generates only a few pieces of knowledge per query, KAT-SQL creates a reusable repository that can support multiple queries and databases. Experiments on BIRD, Spider, and CSTINSIGHT datasets show substantial improvements over baselines: KAT-SQL achieves 41.18% exact match accuracy on BIRD (overlap), outperforming DELLM (34.70%) and the no-knowledge baseline (23.76%). The approach also generalizes well to unseen domains, achieving 24.19% exact match on non-overlapping databases.

## Method Summary
KAT-SQL constructs a knowledge base by prompting LLMs with query-schema pairs and retrieved few-shot examples, iterating with permutation to increase knowledge diversity. During inference, it retrieves top-j relevant knowledge entries via MPNet embeddings, refines them via LLM, then generates SQL. The retriever is fine-tuned with contrastive learning to maximize similarity between queries and their relevant knowledge. The method consistently improves performance across different LLMs including Llama, Granite, and Mixtral, and sets a new state-of-the-art when combined with existing text-to-SQL models.

## Key Results
- KAT-SQL achieves 41.18% exact match accuracy on BIRD (overlap), outperforming DELLM (34.70%) and the no-knowledge baseline (23.76%)
- Knowledge base retrieval accuracy reaches 92.65% MRR when test and training domains overlap
- The approach generalizes well to unseen domains, achieving 24.19% exact match on non-overlapping databases
- Consistently improves performance across different LLMs including Llama, Granite, and Mixtral

## Why This Works (Mechanism)

### Mechanism 1: Reusable Knowledge Repository Construction
A knowledge base built from existing query-schema pairs enables knowledge reuse across multiple queries and databases, improving SQL generation accuracy. Given query-schema pairs from available datasets, LLMs generate knowledge entries grounded in database schemas. The process uses iterative sampling and permutation of few-shot examples to increase knowledge diversity. Entries are stored in a centralized KB that can be queried at inference time.

### Mechanism 2: Contrastive Retrieval Alignment
Training the retrieval embedding model with contrastive learning significantly improves knowledge retrieval accuracy compared to off-the-shelf embeddings. The retriever is trained to maximize embedding similarity between a query and its relevant knowledge while minimizing similarity to irrelevant knowledge entries. This creates a semantic space where queries and their required knowledge cluster together.

### Mechanism 3: Query-Specific Knowledge Refinement
Retrieved knowledge requires additional refinement to align with specific query requirements before SQL generation. After retrieving top-j knowledge entries, an LLM generates refined knowledge k′ by conditioning on the query, retrieved entries, and database schema. This addresses cases where retrieved knowledge is related but not directly applicable.

## Foundational Learning

- **Dense Retrieval with Embedding Similarity**: KAT-SQL retrieves knowledge by computing embedding similarities between queries and KB entries. Understanding cosine similarity and vector space models is essential.
  - Quick check: Can you explain why MPNet embeddings might outperform bag-of-words for semantic matching?

- **Contrastive Learning for Retrieval**: The retriever is explicitly trained with a contrastive loss to separate relevant from irrelevant query-knowledge pairs. This differs from using pre-trained embeddings directly.
  - Quick check: Given a query q, one positive knowledge k+, and multiple negatives k−, how does the InfoNCE-style loss encourage the model to rank k+ higher?

- **In-Context Learning with Few-Shot Examples**: Knowledge generation and SQL generation both use few-shot prompting where relevant examples are prepended to guide the LLM's output format and reasoning.
  - Quick check: Why might permuting the order of few-shot examples lead to more diverse outputs from an LLM?

## Architecture Onboarding

- **Component map**: Training Phase: Query-Schema Pairs → LLM + Few-shot Examples → Knowledge Entries → Knowledge Base K → Contrastive Training → Retriever Model. Inference Phase: User Query q → Retriever(q, K) → Top-j Knowledge {ki} → LLM Refinement → Refined Knowledge k′ → Query q + Schema D + k′ → LLM → SQL Statement s

- **Critical path**: 
  1. KB Coverage: Without sufficient knowledge diversity, retrieval fails regardless of model quality. Verify coverage using held-out queries (target: >40% semantic similarity to gold knowledge).
  2. Retrieval Accuracy: MRR >0.75 on validation set indicates retriever is learning meaningful query-knowledge alignments.
  3. Refinement Quality: Check if refined knowledge matches gold annotations more closely than raw retrieved knowledge.

- **Design tradeoffs**:
  - KB size vs. construction cost: 100K entries take ~7 hours with 8 parallel Llama 70B instances. Larger KB improves coverage but increases retrieval latency (mitigated by FAISS).
  - Retrieval vs. generation: Directly using retrieved knowledge (w/o refinement) is faster but loses 2+ EX points. Refinement adds one LLM call but improves adaptation.
  - Overlap vs. non-overlap: Training retriever on in-domain data helps overlap scenarios significantly; non-overlap relies more on KB cross-domain generalization.

- **Failure signatures**:
  - Low retrieval MRR (<0.5): Retriever not learning query-knowledge alignment; check contrastive training data quality.
  - High retrieval MRR but low SQL EX: Retrieved/refined knowledge is semantically similar but not actionable (missing schema grounding).
  - Knowledge format drift: Generated knowledge doesn't match expected format; check few-shot example quality and prompt template.

- **First 3 experiments**:
  1. Baseline retrieval comparison: Run KAT-SQL with off-the-shelf MPNet vs. contrastive-trained retriever on a validation split. Expect 15-25% MRR improvement from training.
  2. KB ablation: Construct KB with 1x vs. 5x vs. 10x sampling steps; measure coverage and downstream EX. Expect diminishing returns after 5-10 steps based on Figure 3.
  3. Refinement necessity test: Compare three conditions on held-out queries: (a) no knowledge, (b) retrieved knowledge only, (c) retrieved + refined knowledge. Should see stepwise improvements: (a) < (b) < (c) as shown in Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
How can the coverage and accuracy of the automatically constructed knowledge base be improved to close the performance gap with human-annotated oracle knowledge? While KAT-SQL outperforms baselines, there remains a substantial gap compared to the "Oracle Knowledge" upper bound (e.g., 41.18% vs. 54.67% accuracy on BIRD Overlap), indicating the generated knowledge lacks the completeness or precision of human annotation.

### Open Question 2
What specific mechanisms can mitigate the performance degradation when applying the knowledge base to entirely unseen database domains? Table 5 shows a significant performance drop in the Non-Overlap domain setting (24.19% EX) compared to the Overlap setting (49.37% EX), even though the method outperforms baselines in both.

### Open Question 3
How can retrieval robustness be improved for test-time queries that require knowledge not directly present in the constructed knowledge base? Table 4 shows that while retrieval MRR is high (0.92) when test queries overlap with the KB, it drops significantly (to 0.75 even with an Oracle KB) in non-overlapping scenarios.

## Limitations
- KB construction process has critical sensitivities around sampling and permutation strategy that are not fully specified
- Performance significantly degrades when applying to entirely unseen database domains
- Exact prompt templates for knowledge generation, refinement, and SQL generation are incomplete

## Confidence
- **High Confidence**: The overall methodology and experimental design are well-described and reproducible. The paper clearly establishes the two-phase approach, provides specific performance metrics on multiple benchmarks, and demonstrates consistent improvements across different LLMs.
- **Medium Confidence**: The KB construction and retrieval mechanisms are described with sufficient detail for implementation, but critical hyperparameters (sampling steps, temperature values, embedding dimensions) are missing or implicit.
- **Low Confidence**: The exact prompt templates for knowledge generation, refinement, and SQL generation are incomplete.

## Next Checks
1. **Retrieval Ablation Study**: Implement KAT-SQL with three retriever variants: (a) off-the-shelf MPNet embeddings, (b) MPNet with contrastive training using the specified loss function, and (c) an oracle retriever that always returns the gold knowledge. Measure MRR and SQL EX on a held-out validation set to confirm the 50% relative improvement from contrastive training.

2. **Knowledge Base Coverage Analysis**: For a subset of non-overlapping test queries, manually annotate whether the KB contains semantically relevant knowledge entries. Compute coverage percentages at different KB sizes (1x, 5x, 10x sampling steps) and verify that coverage plateaus around 20% as suggested by Figure 3.

3. **Refinement Necessity Test**: For test queries where the retriever returns relevant but imperfect knowledge, implement and compare: (a) using retrieved knowledge directly for SQL generation, (b) using refined knowledge, and (c) using no knowledge. Measure the gap between (a) and (b) to confirm the 2+ EX point improvement claimed in Table 3.