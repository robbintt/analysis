---
ver: rpa2
title: 'Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for
  Scalable and Interpretable Physics Discovery'
arxiv_id: '2505.23106'
source_url: https://arxiv.org/abs/2505.23106
tags:
- nips
- attention
- learning
- neural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neural Interpretable PDEs (NIPS), a novel
  neural operator architecture that enhances physics discovery through attention mechanisms
  combined with Fourier-domain convolutions. The method addresses the challenge of
  simultaneously solving forward and inverse PDE problems by learning data-dependent
  kernels that capture hidden physical parameters.
---

# Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery

## Quick Facts
- arXiv ID: 2505.23106
- Source URL: https://arxiv.org/abs/2505.23106
- Reference count: 23
- Primary result: Achieves up to 78.9% improvement in accuracy over baselines while using fewer parameters for simultaneous forward and inverse PDE solving

## Executive Summary
This paper introduces Neural Interpretable PDEs (NIPS), a novel neural operator architecture that combines attention mechanisms with Fourier-domain convolutions to address the challenge of simultaneously solving forward and inverse PDE problems. NIPS employs linear attention reformulation to achieve scalable computation while maintaining kernel expressivity, learning data-dependent kernels that capture hidden physical parameters from context function pairs. The method successfully discovers interpretable governing laws from synthetic datasets including Darcy flow, Mechanical MNIST, and synthetic tissue learning problems, demonstrating superior performance compared to existing baselines while maintaining robustness to noise and strong zero-shot generalization capabilities.

## Method Summary
NIPS addresses simultaneous forward and inverse PDE solving through a novel architecture that learns data-dependent kernels via iterative Fourier+linear attention blocks. The method tokenizes input function pairs, applies random permutations for data augmentation, and processes them through multiple layers that compute kernel-weighted integrals in Fourier space. The learned kernel serves dual purposes: predicting solutions for new loadings (forward problem) and encoding interpretable physical structure that can be analyzed to recover hidden parameters like permeability fields (inverse problem). Training optimizes the kernel network parameters to minimize prediction loss while maintaining permutation invariance, enabling zero-shot generalization to unseen physical systems.

## Key Results
- Achieves up to 78.9% improvement in accuracy over NAO and AFNO baselines across multiple physics benchmarks
- Uses fewer parameters than competing methods while maintaining superior performance
- Successfully discovers interpretable governing laws from synthetic datasets including Darcy flow, Mechanical MNIST, and synthetic tissue problems
- Demonstrates strong zero-shot generalization to unseen physical systems and robustness to noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear attention reformulation reduces computational complexity while preserving kernel expressivity
- Mechanism: The attention-based integral ∫K(x,y)q(y)dy is decomposed into separate projections via the kernel trick, replacing O(N²d) pairwise token computation with O(Nd² + Nd log N) via Fourier convolution
- Core assumption: The projection admits a convolutional structure enforceable in Fourier space with sufficient shift-equivariance for target PDEs
- Evidence anchors: [section 3.3] "reduces computational complexity from O(N²d) to O(Nd²)"; [abstract] "linear attention mechanism to enable scalable learning"; [corpus] Limited evidence for this specific mechanism

### Mechanism 2
- Claim: Data-dependent kernel maps enable simultaneous forward and inverse PDE solving without explicit supervision on hidden parameters
- Mechanism: NIPS learns a nonlinear kernel map from context function pairs via iterative attention blocks, with the same learned kernel predicting solutions and encoding interpretable physical structure
- Core assumption: Target kernels share structural similarity with training tasks; identifiability manifold is sufficiently covered
- Evidence anchors: [section 3.2, Eq. 6-8] Definition of data-dependent kernel map; [page 7-8, Figure 2] "underlying microstructure can be recovered"; [corpus] Partial alignment with CompNO and SAOT

### Mechanism 3
- Claim: Physics-informed data augmentation via embedding permutation improves generalization by decoupling features from sequence order
- Mechanism: Random permutations along the embedding dimension prevent the model from tying features to a specific sequence order, encouraging learning of spatial dependencies
- Core assumption: Permutation invariance holds for underlying physics; embedding dimension encodes interchangeable channels
- Evidence anchors: [page 7, Table 2] "Random permutations help encode physical knowledge"; [appendix A.1, Eq. 12] Explicit invariance property; [corpus] No direct corpus papers describing this specific mechanism

## Foundational Learning

- Concept: Linear attention and kernel methods
  - Why needed here: NIPS replaces softmax attention with kernel-based linear attention; understanding how Q/K projections form implicit kernels is essential to follow the O(N) reformulation
  - Quick check question: Can you sketch why linear attention replaces a softmax(QK^T)V with φ(Q)(φ(K)^T V) and state the complexity difference?

- Concept: Fourier transforms and spectral convolution
  - Why needed here: Core efficiency gain comes from computing convolutions as pointwise multiplication in Fourier space; must grasp FFT, mode truncation, and translation-equivariance
  - Quick check question: If you retain m Fourier modes on an N-point grid, what is the asymptotic complexity of applying a spectral convolution versus direct spatial convolution?

- Concept: Kernel methods for PDEs and inverse problems
  - Why needed here: NIPS frames PDE solution operators as integral operators with data-dependent kernels; interpreting recovered kernel as physics surrogate requires knowing how kernel structure reflects governing laws
  - Quick check question: For a linear elliptic PDE with heterogeneous coefficient b(x), how does the Green's kernel encode material structure, and why is inverting for b from solution data ill-posed?

## Architecture Onboarding

- Component map: Input function pairs -> Augmentation (n_rand permutations) -> Iterative Fourier+Linear Attention Blocks (L layers) -> Output Fourier Kernel Block -> Prediction (spectral convolution) -> Interpretation (kernel analysis)

- Critical path: 1) Choose Fourier modes m, embedding dim d, QK projection dim d_k, layers L, and n_rand 2) Tokenize/normalize load-displacement pairs; augment with permutations 3) Forward pass through Fourier+linear attention blocks; extract kernel at final layer 4) Compute prediction loss and backprop to optimize parameters 5) At test time, form kernel from context pairs and apply to new loadings (zero-shot forward) or analyze kernel to infer hidden parameters (inverse)

- Design tradeoffs: More Fourier modes (larger m) provides higher expressivity but more memory/parameters; larger n_rand improves generalization at cost of larger augmented training set; deeper L improves accuracy on nonlinear problems but increases runtime; NIPS scales better than NAO-W_p due to no quadratic projection matrix

- Failure signatures: Near-boundary kernel errors due to Dirichlet conditions eliminating information near ∂Ω; non-symmetric target kernels degrade if training is biased toward symmetric kernels; excessive noise degrades performance gracefully but increases kernel/microstructure errors; OOD microstructure or loading increases test and inverse errors

- First 3 experiments: 1) Darcy flow ablation on 21×21 grid comparing NIPS vs NAO, NAO-f, NAO-W_p, AFNO with fixed d, d_k, L, measuring test error, parameter count, per-epoch runtime 2) Sensitivity to n_rand and d_k on 2-layer NIPS with 441 tokens, sweeping values to observe monotonic improvement 3) Scalability stress test increasing grid resolution from 21×21 to 121×121, comparing NIPS vs linear NAO on runtime and memory, confirming NIPS scales to 14,641 tokens within 40GB A100

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can NIPS be adapted to handle systems where underlying physical kernels possess structural properties (e.g., non-symmetry in advection) that differ significantly from training priors?
- **Basis in paper:** [Explicit] Limitations section states performance may degrade if target kernel deviates, such as failing to predict non-symmetric stiffness matrices when trained on symmetric diffusion problems
- **Why unresolved:** Current architecture implicitly regularizes kernel space based on training distribution without mechanism to enforce or discover physics contradicting learned structural biases
- **What evidence would resolve it:** Successful testing on advection-dominated or non-self-adjoint inverse problems after training exclusively on symmetric diffusion datasets, showing comparable recovery of non-symmetric kernels

### Open Question 2
- **Question:** How can the framework be modified to accurately recover hidden physical parameters near domain boundaries where boundary conditions limit information content?
- **Basis in paper:** [Explicit] Section 4.1 notes discovered microstructure matches ground truth poorly near domain boundary because Dirichlet boundary conditions prevent measurement pairs from containing sufficient information to identify kernel at edges
- **Why unresolved:** Method relies on internal data content of function pairs without explicit mechanism to extrapolate kernel information to boundaries where standard measurements provide no gradient information
- **What evidence would resolve it:** Introduction of boundary-aware loss function or extrapolation module resulting in reduced microstructure error rates specifically at domain boundaries

### Open Question 3
- **Question:** Is there an efficient architectural middle-ground that incorporates non-linear mixing of NIPS-mlp variant without significant increase in trainable parameters?
- **Basis in paper:** [Inferred] Section 4.3 mentions passing V vector through MLP improves accuracy by 7.7% but nearly triples parameter count, leading to decision not to pursue it further due to marginal gains relative to complexity
- **Why unresolved:** Trade-off between non-linear expressivity provided by MLP and parameter efficiency of standard linear projection remains unoptimized
- **What evidence would resolve it:** New variant utilizing efficient non-linear layers (e.g., low-rank adapters) achieving accuracy comparable to NIPS-mlp while maintaining parameter count closer to standard NIPS

## Limitations
- Performance degrades when target kernels deviate significantly from training priors, particularly for non-symmetric kernels like advection-dominated flows
- Boundary conditions prevent accurate recovery of hidden parameters near domain boundaries due to lack of information content
- Permutation-based augmentation may harm performance when embedding dimension carries intrinsically ordered semantic meaning

## Confidence

- **High Confidence**: Computational efficiency claims (O(Nd²) vs O(N²d) complexity), parameter efficiency (fewer parameters than NAO baselines), and zero-shot generalization performance on synthetic datasets
- **Medium Confidence**: Interpretability and physics discovery through kernel analysis, as mapping from kernel strength to physical parameters requires additional optimization and physics knowledge
- **Low Confidence**: Robustness to noise and generalization to real-world physical systems beyond synthetic benchmarks, with limited testing on noise robustness and only synthetic OOD generalization

## Next Checks

1. **Cross-domain robustness test**: Apply NIPS to real-world PDE dataset (e.g., fluid dynamics simulations) with varying Reynolds numbers to test zero-shot generalization beyond synthetic microstructures

2. **Non-symmetric kernel evaluation**: Systematically train NIPS on mixed datasets containing both symmetric (diffusion) and non-symmetric (advection-diffusion) kernels, then test performance on held-out advection-dominated cases to quantify claimed limitation

3. **Permutation invariance stress test**: Create synthetic datasets where embedding dimension order carries semantic meaning (e.g., time-ordered channels) and measure performance degradation with increasing n_rand to validate mechanism's assumptions