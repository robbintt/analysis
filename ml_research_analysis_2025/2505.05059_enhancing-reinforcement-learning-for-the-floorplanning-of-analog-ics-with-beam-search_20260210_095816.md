---
ver: rpa2
title: Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam
  Search
arxiv_id: '2505.05059'
source_url: https://arxiv.org/abs/2505.05059
tags:
- search
- agent
- beam
- hpwl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid reinforcement learning and beam search
  method for analog IC floorplanning that achieves 5-85% improvement in area, dead
  space, and half-perimeter wire length compared to standard RL approaches. The key
  innovation is a beam search enhancement of the inference process that builds and
  prunes a search tree representing the RL agent's state space, allowing flexible
  objective weightings and congestion management without retraining.
---

# Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search

## Quick Facts
- arXiv ID: 2505.05059
- Source URL: https://arxiv.org/abs/2505.05059
- Reference count: 16
- Primary result: Beam search enhancement of RL inference achieves 5-85% improvements in area, dead space, and HPWL versus standard RL.

## Executive Summary
This paper introduces a hybrid reinforcement learning and beam search method for analog IC floorplanning that significantly improves solution quality without retraining the base RL agent. The key innovation is applying beam search at inference time to explore multiple action paths from the RL policy, enabling flexible objective weightings and congestion management while maintaining generalization. Experimental results show consistent improvements across all metrics and better stability, with the added advantage of CPU-based execution that matches GPU fine-tuning quality.

## Method Summary
The method combines a pre-trained RL agent (from prior work [6]) with beam search at inference time. The RL agent uses a relational graph neural network to predict placement actions. During inference, a search tree is built by sampling q actions from the policy at each state, creating multiple child nodes. Congestion is estimated using RUDY, and nodes are pruned probabilistically based on a value function combining area and HPWL. The process continues until all modules are placed, returning the best leaf node. Key parameters include tree arity q=5, pruning probability ε=0.7, and beam width β=10.

## Key Results
- 5-85% improvement in dead space, half-perimeter wire length, and RUDY congestion metrics versus baseline RL
- Comparable performance to fine-tuning approaches without requiring GPU resources
- Better stability and quality across all test circuits with CPU-based execution
- Consistent improvements across 6 benchmark circuits ranging from 5 to 19 modules

## Why This Works (Mechanism)

### Mechanism 1: Search Tree Expansion via Policy Sampling
Expanding multiple action branches from each state allows the agent to recover from suboptimal early decisions without retraining. At each state, sampling q actions from the trained policy creates child nodes representing successor states, transforming the deterministic single-path rollout into tree exploration. This exposes alternatives the policy considered less probable but potentially higher-value.

### Mechanism 2: Value-Guided Beam Pruning
Periodic pruning retains globally promising paths while keeping computation tractable. After each level is built, with probability ε, retain only the β nodes with highest value v (computed via weighted area/HPWL metrics). This approximates best-first search while bounding memory to O(β × depth).

### Mechanism 3: Congestion-Aware Action Filtering
RUDY-based filtering at action selection time produces routing-friendly floorplans without modifying the RL reward function. Compute RUDY routing demand for each candidate action. If congestion exceeds threshold, resample 60% of actions, keeping those with highest policy probability that satisfy the constraint.

## Foundational Learning

- **Markov Decision Process (MDP) Formulation**: The floorplanning problem is framed as sequential decision-making (state = current placement, action = place next module, reward = negative cost increase). *Quick check: Can you explain why the reward is defined as negative (rt = −∆DS − ∆HPWL) rather than positive?*

- **Beam Search**: Core inference enhancement; understanding β (beam width) vs. exploration completeness tradeoff is essential for tuning. *Quick check: What happens to solution quality if β = 1 vs. β = 100 in a 20-module floorplanning problem?*

- **Half-Perimeter Wire Length (HPWL)**: Primary optimization objective alongside area; HPWL approximates routing length before actual routing. *Quick check: Why is HPWL used as a proxy instead of actual routed wire length?*

## Architecture Onboarding

- **Component map**: Pretrained RL Agent -> Search Tree Builder -> RUDY Estimator -> Beam Pruner -> Value Function
- **Critical path**: Initialize root node with empty canvas → Sample q actions from policy → For each action: compute RUDY → filter/accept → create child node → Compute value v → With probability ε: prune to top-β nodes → Repeat until all modules placed → Return leaf with highest v
- **Design tradeoffs**: 
  - arity q: Higher q = more exploration but O(q^depth) growth before pruning
  - beam width β: Higher β = better solutions but more memory/compute
  - pruning probability ε: ε = 1.0 prunes every level (pure BFS); ε = 0.0 grows full tree
  - α vs. δ weights: α prioritizes area; δ prioritizes HPWL
  - congestion threshold: Lower = routing-friendly but may reject all actions
- **Failure signatures**:
  - Runtime explosion: q too high, ε too low, or β too large → exceeds 10³ node limit
  - Degraded quality: β too small or ε too high → premature pruning
  - Invalid placements: Congestion threshold too strict → fallback actions still violate constraints
  - No improvement over baseline: Policy entropy too low → sampled actions are near-identical
- **First 3 experiments**:
  1. Baseline validation: Run original RL agent (q=1, no tree) on test circuits; record DS, HPWL, RUDY, runtime.
  2. Ablation on β and ε: Fix q=5, sweep β∈{5,10,20} and ε∈{0.3,0.5,0.7,1.0}; plot Pareto frontier of quality vs. runtime.
  3. Congestion threshold sensitivity: Set α=1, δ=5, β=10, ε=0.7; test threshold ∈{∞, 0.5, 0.2, 0.1}; compare RUDY reduction vs. DS/HPWL degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the BS-RL method scale efficiently to analog circuits with significantly larger numbers of modules (e.g., 50-100+) without excessive runtime or memory overhead?
- Basis in paper: [inferred] The runtime for Bias-2 (19 structures) is anomalously high (1293s) and doesn't scale monotonically with problem size. The authors note "Supposedly, runtime doesn't increase significantly on problems of even higher complexity, but new data would be needed for assessment."
- Why unresolved: Testing was limited to circuits with 5-19 structures; scalability behavior for larger industrial designs remains uncharacterized.
- What evidence would resolve it: Experiments on analog circuits with progressively larger module counts, measuring runtime, memory usage, and solution quality trends.

### Open Question 2
- Question: How sensitive is BS-RL performance to the choice of hyperparameters (tree arity q, pruning probability ε, beam width β)?
- Basis in paper: [inferred] The paper states "Minor adjustments do not significantly alter the outcomes, but future investigations may uncover more interesting and potentially superior results."
- Why unresolved: Only one configuration (q=5, ε=0.7, β=10) is thoroughly evaluated; systematic sensitivity analysis was not conducted.
- What evidence would resolve it: Ablation studies varying each hyperparameter independently across multiple circuit types to identify optimal or adaptive settings.

### Open Question 3
- Question: Can integrating detailed routing feedback and post-layout verification (DRC, LVS, ERC) into the floorplanning loop achieve fully clean analog layouts without manual intervention?
- Basis in paper: [explicit] The conclusion states: "In the future... we aim to augment the floorplan algorithm with detailed routing information, and feedbacks from post-layout verification, to incrementally refine the device placement... The goal is, at some point, to achieve automatically generated LVS, DRC and ERC clean analog layout designs."
- Why unresolved: Current outputs still require "slight manual refinement would be needed before routing" (Figure 3 caption); the iterative loop is not yet implemented.
- What evidence would resolve it: Demonstration of a closed-loop system producing DRC/LVS/ERC-clean layouts on industrial test cases with quantified reduction in manual effort.

## Limitations
- Heavy reliance on pre-trained RL agent from prior work [6] without detailed architecture or training specifics
- Adjustable hyperparameters (α, δ, RUDY threshold) not fixed for experiments, leaving ambiguity about optimal settings
- Circuit schematic preprocessing via "Automatic Structure Recognition" is referenced but not described, potentially blocking exact reproduction

## Confidence

- **High Confidence**: Experimental results showing 5-85% improvements in DS, HPWL, and RUDY over baseline RL; the beam search framework is clearly described and implemented.
- **Medium Confidence**: Claims about CPU-based execution matching GPU fine-tuning quality; limited ablation on hyperparameter sensitivity (only one q=5 setting tested).
- **Low Confidence**: Generalization claims across unseen circuits; no statistical significance testing reported despite 100 runs per circuit.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Sweep β ∈ {5,10,20}, ε ∈ {0.3,0.5,0.7,1.0}, and q ∈ {3,5,7} to map the Pareto frontier of quality vs. runtime, verifying robustness beyond the single setting reported.

2. **Policy Entropy Evaluation**: Measure the entropy of the pre-trained policy π*(s) across test circuits to confirm sufficient diversity in action sampling for meaningful tree expansion.

3. **Congestion Threshold Impact Study**: Systematically test RUDY thresholds (0.1, 0.2, 0.5, ∞) to quantify the tradeoff between routing improvement and area/HPWL degradation, ensuring the chosen threshold is optimal.