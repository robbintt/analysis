---
ver: rpa2
title: 'LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust
  Text Classification'
arxiv_id: '2512.10793'
source_url: https://arxiv.org/abs/2512.10793
tags:
- fusion
- roberta
- openai
- classification
- labelfusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LabelFusion introduces a learned fusion approach that combines
  traditional transformer classifiers with Large Language Models (LLMs) for robust
  text classification. The system trains a compact multi-layer perceptron to optimally
  combine embeddings from RoBERTa-style models with per-class scores from LLMs like
  OpenAI GPT, Google Gemini, or DeepSeek.
---

# LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification

## Quick Facts
- arXiv ID: 2512.10793
- Source URL: https://arxiv.org/abs/2512.10793
- Reference count: 18
- Primary result: Fusion approach achieves 92.4% accuracy on AG News and 92.3% on Reuters 21578 with 20% training data

## Executive Summary
LabelFusion introduces a learned fusion approach that combines traditional transformer classifiers with Large Language Models (LLMs) for robust text classification. The system trains a compact multi-layer perceptron to optimally combine embeddings from RoBERTa-style models with per-class scores from LLMs like OpenAI GPT, Google Gemini, or DeepSeek. This approach achieves strong performance across both multi-class and multi-label tasks, with 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 classification. The method demonstrates superior data efficiency, matching full-data performance with only 20% of training examples. LabelFusion includes practical features like LLM response caching, batch processing, and flexible configuration through both high-level AutoFusion interfaces and command-line tools.

## Method Summary
LabelFusion fuses transformer embeddings (RoBERTa) with LLM-derived per-class scores through a learned multi-layer perceptron. The system concatenates RoBERTa's 768-dimensional embeddings with K-dimensional LLM class scores, feeding this joint vector to the FusionMLP. Training uses differential learning rates: a small LR for the ML backbone (preserving pre-trained representations) and a higher LR for the fusion MLP (enabling rapid adaptation). The approach supports both multi-class (cross-entropy loss) and multi-label (binary cross-entropy loss) classification. LLM responses are cached to reduce API costs, and the framework provides both high-level AutoFusionClassifier interfaces and command-line tools for flexibility.

## Key Results
- Achieves 92.4% accuracy on AG News (4-class) and 92.3% on Reuters-21578 (10-class)
- Matches full-data performance with only 20% of training data
- Demonstrates superior data efficiency across multiple training data sizes (5%, 20%, 50%, 100%)
- Supports both multi-class and multi-label classification with configurable loss functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concatenating transformer embeddings with LLM per-class scores enables a learned meta-classifier to capture complementary error patterns from both model families.
- **Mechanism:** RoBERTa embeddings encode task-specific fine-grained patterns learned via gradient descent, while LLM scores inject zero-shot semantic reasoning. The FusionMLP learns to up-weight the more reliable signal per input—leaning on the LLM for ambiguous or out-of-distribution cases and on RoBERTa for routine, high-confidence patterns.
- **Core assumption:** The two model families make partially uncorrelated errors; when one is uncertain or wrong, the other may be correct.
- **Evidence anchors:**
  - [abstract] "captures complementary strengths of LLM reasoning and traditional transformer-based classifiers"
  - [section 6.1.1] Fusion consistently outperforms individual models across all training data sizes on AG News
  - [corpus] No direct corpus validation of LLM-transformer score fusion; neighbor papers address transformer layer fusion (ALFIA) and multimodal fusion, not this specific hybrid.
- **Break condition:** When both RoBERTa and the LLM systematically err on the same examples (e.g., highly domain-specific jargon neither has seen), fusion yields marginal gains.

### Mechanism 2
- **Claim:** LLM-derived per-class scores act as a prior that substitutes for labeled data, enabling strong performance with dramatically fewer training examples.
- **Mechanism:** The LLM provides task-relevant signal via structured prompts without supervised updates. When training data is scarce, the fusion MLP learns to rely more heavily on these zero-shot scores. As data increases, it gradually shifts weight toward RoBERTa's task-specific patterns.
- **Core assumption:** LLM prompt responses encode meaningful class-relevant information even without task-specific fine-tuning.
- **Evidence anchors:**
  - [section 6.1.1] "With only 20% training data, Fusion achieves 92.2% accuracy—matching its performance with full data"
  - [section 6.1.3] At 5% data, OpenAI LLM scores 88.1% accuracy vs. RoBERTa's 0.0%
  - [corpus] Weak external validation; no corpus papers directly test LLM-as-prior for data efficiency.
- **Break condition:** When the task requires specialized knowledge absent from LLM pre-training (e.g., proprietary ontology mapping), the LLM prior provides limited value.

### Mechanism 3
- **Claim:** Differential learning rates stabilize training by preserving pre-trained backbone representations while allowing rapid fusion-layer adaptation.
- **Mechanism:** RoBERTa backbone uses a small learning rate to avoid catastrophic forgetting; the FusionMLP uses a higher rate to quickly learn optimal combination weights. This decoupling prevents the fusion layer from distorting useful embeddings during early training.
- **Core assumption:** The backbone's pre-trained representations are already useful and should be preserved, not relearned from scratch.
- **Evidence anchors:**
  - [section 3] "The ML backbone is trained/fine-tuned with a small learning rate; the fusion MLP uses a higher rate, enabling rapid adaptation without destabilizing the encoder"
  - [corpus] No direct corpus validation; standard practice in transfer learning but not explicitly tested in neighbor papers.
- **Break condition:** When the downstream task is semantically distant from backbone pre-training (e.g., specialized biomedical text on general-domain RoBERTa), the small LR may prevent necessary adaptation.

## Foundational Learning

- **Concept: Multi-class vs. Multi-label Classification**
  - **Why needed here:** LabelFusion supports both, but each requires different loss functions (cross-entropy vs. binary cross-entropy), activation (softmax vs. sigmoid), and thresholding strategies.
  - **Quick check question:** Given a news article that could belong to both "politics" AND "economy," which classification mode applies, and what activation function would the final layer use?

- **Concept: Embedding Concatenation as Fusion**
  - **Why needed here:** The FusionMLP operates on a concatenated vector; understanding dimensionality and information content helps debug representation bottlenecks.
  - **Quick check question:** If RoBERTa produces a 768-dimensional embedding and the LLM provides 10 class scores, what is the input dimension to the FusionMLP?

- **Concept: LLM Prompt Engineering for Classification**
  - **Why needed here:** LLM per-class scores depend on prompt design; poorly structured prompts yield noisy scores that degrade fusion.
  - **Quick check question:** Why might asking an LLM "Is this positive or negative?" produce less reliable scores than requesting explicit probability estimates per class?

## Architecture Onboarding

- **Component map:**
  Input Text -> [RoBERTa Backbone] -> Embeddings (768-dim) -> [Concatenation] -> [FusionMLP] -> [Softmax/Sigmoid] -> Final Prediction
                                      -> [LLM + Structured Prompt] -> Per-class Scores (K-dim)

- **Critical path:** The FusionMLP is the only component trained end-to-end; LLM is inference-only (cached), and RoBERTa is optionally fine-tuned. Data flows through both branches in parallel—latency is dominated by the slower branch (typically LLM API calls).

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** More LLM calls improve fusion quality but increase API costs; caching mitigates this for static datasets.
  - **Latency vs. Performance:** LLM inference adds 100-1000ms+ per batch; RoBERTa alone is ~10-50ms. Fusion requires both.
  - **Low-data vs. High-data regimes:** In extreme low-data (<10%), LLM alone may outperform fusion (RoBERTa is unstable); in high-data (>60%), fusion dominates.

- **Failure signatures:**
  - Fusion accuracy *below* either component individually -> suggests MLP overfitting or learning rate imbalance.
  - Large accuracy gap between train and validation -> RoBERTa may be overfitting; reduce backbone LR or add regularization.
  - Inconsistent results across runs with same data -> check LLM API non-determinism; enable caching.

- **First 3 experiments:**
  1. **Baseline comparison:** Run RoBERTa-only, LLM-only, and Fusion on a held-out validation set. Confirm Fusion ≥ max(individuals).
  2. **Data efficiency sweep:** Train Fusion with 5%, 20%, 50%, 100% of training data. Plot accuracy curve to identify the "elbow" where fusion gains diminish.
  3. **Ablation on fusion input:** Train two variants—one using only RoBERTa embeddings, one using only LLM scores fed through the same MLP architecture. Compare to full fusion to quantify complementarity.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can fusion strategies be adapted to prevent performance degradation when the ML backbone fails catastrophically in extremely low-data regimes (e.g., 5-10% training data)?
  - **Basis in paper:** [explicit] The authors observe that "in extremely low-data settings, the Fusion Ensembles appear negatively affected by the RoBERTa component, resulting in reduced overall prediction performance," with RoBERTa achieving 0.0% accuracy at 5% training data while the LLM alone achieves 88.1%.
  - **Why unresolved:** The paper documents this failure mode but does not propose or evaluate mechanisms (e.g., dynamic weighting, confidence-based gating) to mitigate it.
  - **What evidence would resolve it:** Ablation experiments comparing static fusion against adaptive fusion strategies that downweight the ML backbone when its confidence is low or training data is insufficient.

- **Open Question 2:** How robust is LabelFusion's fusion MLP to distribution shift and out-of-domain examples compared to individual model components?
  - **Basis in paper:** [inferred] The paper claims the approach captures "complementary strengths" for "cross-domain settings" and "domain shift," but evaluation is limited to two news classification datasets (AG News, Reuters-21578) without explicit domain shift experiments.
  - **Why unresolved:** No experiments test performance when training and test distributions differ, which is critical for real-world deployment.
  - **What evidence would resolve it:** Experiments on benchmark datasets with known distribution shifts (e.g., Amazon reviews across product categories) comparing fusion vs. individual model OOD performance.

- **Open Question 3:** What are the optimal decision thresholds for the fusion layer's reliance on the ML backbone versus the LLM under different cost-latency constraints?
  - **Basis in paper:** [explicit] The paper states LabelFusion enables "practical trade-offs between accuracy, latency, and cost" and that "the fusion layer learns when to rely more heavily on the efficient ML backbone versus the more expensive LLM signal," but provides no quantitative cost or latency analysis.
  - **Why unresolved:** The trade-off space is mentioned conceptually but not empirically characterized.
  - **What evidence would resolve it:** Systematic experiments measuring accuracy vs. API cost and latency, identifying operating points where ML-only, LLM-only, or fusion is optimal.

## Limitations
- Critical implementation details (prompt templates, FusionMLP architecture, training hyperparameters) are not specified, preventing faithful reproduction
- Performance in extremely low-data regimes (<10% training data) may degrade due to ML backbone failure
- No empirical characterization of cost-latency tradeoffs despite claiming practical utility
- Limited evaluation to two news classification datasets without domain shift or multilingual testing

## Confidence
- **High Confidence:** The core architectural concept of fusing transformer embeddings with LLM-derived per-class scores through a learned MLP is sound and well-motivated.
- **Medium Confidence:** The empirical claims about data efficiency are supported by internal results but lack external validation and may not generalize to all text classification tasks.
- **Low Confidence:** The generalizability across different LLM providers, domain-specific tasks, and multilingual settings is not established.

## Next Checks
1. **Prompt Ablation Study:** Systematically vary the prompt templates used to elicit per-class scores from LLMs while keeping all other components constant. Measure how prompt quality affects fusion performance to quantify the sensitivity of the approach to this critical but unspecified component.

2. **Architecture Sensitivity Analysis:** Vary the FusionMLP architecture (number of layers, hidden dimensions, activation functions) while maintaining fixed RoBERTa and LLM components. Determine whether the reported performance is robust to architectural changes or if it critically depends on an unspecified optimal configuration.

3. **Cross-Domain Generalization Test:** Apply the trained fusion model from AG News or Reuters to a different text classification dataset (e.g., IMDB reviews or Amazon product reviews) without retraining. Measure performance drop to assess whether the fusion captures generalizable patterns or overfits to the specific training domains.