---
ver: rpa2
title: 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration
  for Agentic Reinforcement Learning'
arxiv_id: '2509.22601'
source_url: https://arxiv.org/abs/2509.22601
tags:
- arxiv
- spear
- reward
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing exploration and
  exploitation in training LLM agents for complex, long-horizon tasks using reinforcement
  learning. The proposed SPEAR method extends self-imitation learning by incorporating
  curriculum scheduling to progressively shift from broad skill-level exploration
  to focused action-level exploitation.
---

# Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.22601
- Source URL: https://arxiv.org/abs/2509.22601
- Reference count: 40
- Primary result: SPEAR improves LLM agent performance by up to 20% success rate on ALFWorld, WebShop, and AIME benchmarks through curriculum-scheduled self-imitation learning

## Executive Summary
This paper addresses the challenge of balancing exploration and exploitation in training LLM agents for complex, long-horizon tasks using reinforcement learning. The proposed SPEAR method extends self-imitation learning by incorporating curriculum scheduling to progressively shift from broad skill-level exploration to focused action-level exploitation. It combines intrinsic rewards to encourage tool interaction early on with gradual reward modulation, and uses advantage recalibration to handle off-policy updates efficiently. Experiments show SPEAR improves performance across multiple benchmarks—including ALFWorld, WebShop, and AIME competitions—by up to 20% success rate, while adding only modest computational overhead. The method is shown to be plug-and-play and effective across various agent types, including vision-language and search-augmented agents.

## Method Summary
SPEAR combines self-imitation learning with curriculum scheduling and intrinsic reward modulation to balance exploration and exploitation in LLM agent training. The method maintains a replay buffer of successful trajectories, uses advantage recalibration based on median baseline performance to filter outdated samples, and progressively shifts from encouraging tool interactions (via decaying intrinsic rewards) to focusing on task completion. A warm-up schedule controls when self-imitation loss becomes active, preventing early overfitting to scarce successful experiences. The approach builds on GRPO with covariance-based token clipping and is implemented within the VeRL-agent framework using vLLM for inference.

## Key Results
- Achieves up to 20% improvement in success rate across ALFWorld, WebShop, and AIME benchmarks
- Shows plug-and-play effectiveness across different agent types including Qwen2.5-7B and Qwen2.5-VL-3B
- Demonstrates robust performance with modest computational overhead compared to baseline methods
- Ablation studies confirm both self-imitation and intrinsic reward components are critical for success

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-Scheduled Self-Imitation Learning
Gradually increasing the self-imitation weight prevents entropy collapse while enabling exploitation of successful trajectories. A warm-up schedule γ controls the contribution of self-imitation loss, starting near 0 and increasing to 1 over Twarm-up steps. This prevents early overfitting to the few available successful trajectories while allowing later consolidation of effective strategies.

### Mechanism 2: Progressive Intrinsic Reward Modulation
Decaying tool-call rewards encourage early skill acquisition without causing reward hacking or competition with outcome rewards. A decay schedule µ reduces the tool-call reward from full strength to zero over Tdecay steps. This allows the agent to first learn tool-use skills through intrinsic motivation, then focus purely on task success.

### Mechanism 3: Advantage Recalibration for Off-Policy Updates
Using the median of recent baseline performances to recalibrate advantages enables robust filtering of outdated replay samples. A FIFO baseline buffer stores recent intra-group average rewards; the 50th percentile P50(DR) serves as a robust baseline estimator. Dual filtering removes trajectories that were good historically but are now outdated.

## Foundational Learning

- **Policy Entropy in Reinforcement Learning**: Why needed here: SPEAR centrally addresses entropy collapse and divergence; understanding why entropy matters for exploration is essential. Quick check: Why would high entropy help early in training but hurt later?

- **Self-Imitation Learning (SIL)**: Why needed here: SPEAR extends vanilla SIL; you need to understand how replay buffers store and reuse successful trajectories. Quick check: What happens if you replay trajectories from a very old policy without adjustment?

- **Off-Policy vs On-Policy Policy Gradients**: Why needed here: The advantage recalibration mechanism exists specifically to handle off-policy samples; grasping this distinction clarifies why recalibration matters. Quick check: Why does GRPO's advantage estimation become invalid for samples collected many iterations ago?

## Architecture Onboarding

- **Component map**: Rollout Generator -> Intrinsic Reward Shaper -> Replay Buffer (D) -> Baseline Buffer (DR) -> Advantage Recalibrator -> Curriculum Scheduler -> Covariance-based Clipper -> GRPO Update

- **Critical path**: 1) Rollout generation with current policy 2) Intrinsic reward composition with current µ 3) On-policy GRPO update 4) Store successful trajectories and baseline in buffers 5) Once buffer full: recalibrate advantages, filter by dual criteria 6) Compute SIL loss with covariance clipping and γ weighting 7) Combined update: JTotal = JGRPO + γ · J̃SIL

- **Design tradeoffs**: ND (replay buffer size): Larger → more diverse experience but increased off-policy gap; paper uses 2048. Twarm-up (SIL warm-up): Longer → more exploration before exploitation; range 100–300 steps. Tdecay (intrinsic reward decay): Longer → more tool-call encouragement; range 100–200 steps. λ (clipping ratio): Controls entropy regularization strength; 0.0002–0.02

- **Failure signatures**: Entropy collapse: Policy becomes overconfident; check if SIL is starting too early (Twarm-up too short). Excessive tool calls: Agent inflates interaction turns for reward; check if Tdecay is too long. Stagnant performance: Replay buffer may be stale; verify NDR and ND sizes

- **First 3 experiments**: 1) Ablate SIL only: Run with γ=0 (no self-imitation) to isolate intrinsic reward contribution. Expect ~5–15% drop on ALFWorld/WebShop. 2) Ablate intrinsic reward only: Set µ=0 throughout; verify tool-use skill acquisition is impaired, especially for smaller models. 3) Hyperparameter sweep on Twarm-up and Tdecay: Test {100, 200, 300} for each on a held-out task subset to find domain-appropriate curriculum pace

## Open Questions the Paper Calls Out

- **Can dynamic scheduling mechanisms based on entropy, performance, or curiosity effectively replace the fixed, prior-based curriculum schedules used in SPEAR?** The paper suggests modeling and adjusting scheduling parameters dynamically using entropy, performance, or curiosity as potential mediums, noting that fixed schedules may not be optimal for all tasks or convergence rates.

- **Can the rigidity of SPEAR's entropy control be mitigated using token-level dynamic reweighting or policy self-confidence metrics?** The current covariance-based clipping and warm-up schedules might be suboptimal for certain agentic tasks, potentially constraining exploration unnecessarily. The paper suggests token-level dynamic reweighting as a solution.

- **How can SPEAR be augmented to effectively define "good experiences" and assign credit in highly stochastic environments with unreliable tools?** The paper identifies the vague definition of good experiences under complex, stochastic environments as a limitation, noting that sparse outcome rewards cannot distinguish between good and bad experiences in noisy environments.

## Limitations
- Evaluation focuses heavily on WebShop and ALFWorld benchmarks, with AIME experiments relying on code interpreter settings that may not generalize to all mathematical reasoning tasks
- Decay schedules for intrinsic rewards (Tdecay=200) and SIL warm-up (Twarm-up=100-300) are presented as fixed hyperparameters rather than adaptive mechanisms
- Covariance-based clipping mechanism depends on hyperparameters (ωlb, ωub) that are computed using unspecified statistics

## Confidence
- **High Confidence**: The core mechanism of combining curriculum-scheduled SIL with intrinsic reward modulation is well-supported by ablation studies and theoretical grounding in entropy regularization literature
- **Medium Confidence**: The advantage recalibration approach using median baselines is novel but lacks extensive validation across diverse task domains beyond the presented benchmarks
- **Low Confidence**: The covariance-based token clipping methodology requires more rigorous analysis of its impact on policy performance, as the selection of clipping thresholds appears heuristic

## Next Checks
1. **Cross-domain transfer evaluation**: Test SPEAR on a benchmark with fundamentally different exploration characteristics (e.g., sparse-reward robotics control) to assess whether the fixed decay schedules (Tdecay=200, Twarm-up=100-300) remain effective or require adaptation

2. **Policy stability analysis**: Conduct experiments where the underlying LLM policy is frozen after initial training, then measure how quickly the replay buffer becomes stale and whether the dual advantage filtering adequately handles this degradation

3. **Covariance clipping sensitivity**: Systematically vary the clipping hyperparameters (ωlb, ωub) across multiple orders of magnitude to determine whether the reported improvements are robust to these choices or critically dependent on specific threshold values