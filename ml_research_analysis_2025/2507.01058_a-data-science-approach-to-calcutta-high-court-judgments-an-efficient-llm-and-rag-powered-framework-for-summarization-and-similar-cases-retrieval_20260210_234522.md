---
ver: rpa2
title: 'A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM
  and RAG-powered Framework for Summarization and Similar Cases Retrieval'
arxiv_id: '2507.01058'
source_url: https://arxiv.org/abs/2507.01058
tags:
- legal
- summarization
- summaries
- research
- court
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a framework combining Large Language Models
  and Retrieval-Augmented Generation to improve legal case summarization and retrieval.
  It fine-tunes the Pegasus model on Calcutta High Court headnote summaries and employs
  a two-step summarization approach, achieving strong performance in precision, recall,
  and F1-score on ROUGE metrics.
---

# A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval

## Quick Facts
- arXiv ID: 2507.01058
- Source URL: https://arxiv.org/abs/2507.01058
- Reference count: 17
- This study presents a framework combining Large Language Models and Retrieval-Augmented Generation to improve legal case summarization and retrieval, achieving strong performance on ROUGE metrics.

## Executive Summary
This paper introduces a framework that combines Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to address the challenges of legal case summarization and similar case retrieval in the Calcutta High Court system. The approach fine-tunes the Pegasus model on legal headnote summaries and employs a two-step summarization pipeline (abstractive followed by extractive) to produce high-quality summaries. A vector database stores embeddings of these summaries for efficient semantic retrieval, enabling users to find similar cases through natural language queries. The system demonstrates strong performance across multiple evaluation metrics, outperforming extractive-only approaches and offering a scalable solution for legal research.

## Method Summary
The framework consists of three main components: a fine-tuned Pegasus model for abstractive summarization trained on ~3,100 annotated Calcutta High Court cases, a BERTSum model for extractive summarization that selects salient sentences from the abstractive output, and a RAG system using a vector database for similar case retrieval. The pipeline begins with web scraping judgments from indiankanoon.org, followed by LLM annotation to extract structured fields. Summaries are generated through the Abs-Ext pipeline, chunked into 1024-token segments with 100-token overlap, embedded, and stored in a vector database. Query processing involves embedding the user query, retrieving top-3 similar cases, and using Llama-2 to synthesize a response with the retrieved context.

## Key Results
- The Abs-Ext summarization pipeline consistently outperforms Ext-Abs and extractive-only approaches across all ROUGE metrics (F1, precision, and recall) for legal documents.
- The RAG-powered retrieval system successfully identifies semantically similar cases using embedding-based similarity search with appropriate chunk size and overlap parameters.
- The framework demonstrates practical utility for legal research by efficiently generating accurate summaries and retrieving relevant precedents from a corpus of ~130K judgments.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Fine-Tuning on Headnote Summaries
- Claim: Fine-tuning Pegasus on legal headnote summaries improves summarization quality for court judgments compared to generic pre-trained models.
- Mechanism: Headnotes are human-authored, concise summaries that encode legal terminology, rhetorical structure, and domain-specific abstraction patterns. Fine-tuning transfers these patterns to the model's generation process.
- Core assumption: Headnotes in the training distribution are representative of the target summarization task and maintain consistent quality.
- Evidence anchors: [abstract] "By fine-tuning the Pegasus model using case head note summaries, we achieve significant improvements in the summarization of legal cases." [section 3.2] "Case headnotes are perfect for training a summarization model because they are concise summaries that encapsulate the main points of court decisions." [corpus] Related work (Kalamkar et al., 2022; Paul et al., 2023) shows domain-specific pre-training improves legal NLP tasks, supporting transferability assumption.
- Break condition: If headnote quality is inconsistent or written in different styles across cases, fine-tuning may introduce noise rather than signal.

### Mechanism 2: Two-Step Abstractive-Extractive Summarization Pipeline
- Claim: Applying abstractive summarization followed by extractive selection (Abs-Ext) outperforms extractive-first (Ext-Abs) or single-stage approaches for legal documents.
- Mechanism: The abstractive model (fine-tuned Pegasus) first compresses and reformulates the judgment into coherent prose. BERTSum then extracts the most salient sentences, filtering redundant or less relevant content while preserving factual grounding.
- Core assumption: The extractive step acts as a quality filter without discarding critical legal information that the abstractive model correctly generated.
- Evidence anchors: [section 5] "Abs-Ext summarization consistently outperforms Ext-Abs summarization across all metrics (F1, precision, and recall) in the context of legal documents." [section 3.3] "Key sentences are selected from the abstractive summary using the BERTSum model. This step ensures that the most relevant and important parts of the legal text are identified." [corpus] AugAbEx (2025) paper addresses extractive case summarization challenges, suggesting extraction remains valuable but ordering matters—consistent with Abs-Ext superiority.
- Break condition: If the abstractive model hallucinates or omits key facts, the extractive step cannot recover them—errors propagate unidirectionally.

### Mechanism 3: RAG with Semantic Chunking for Context-Aware Retrieval
- Claim: Chunking summaries into overlapping token windows and storing embeddings in a vector database enables accurate semantic retrieval of similar cases.
- Mechanism: Chunking (1024 tokens, 100-token overlap) preserves local context while staying within embedding model limits. Overlap prevents boundary artifacts where key phrases are split. Similarity search matches query embeddings to document embeddings, retrieving top-k before LLM synthesis.
- Core assumption: Semantic similarity in embedding space correlates with legal relevance; chunk boundaries do not systematically break legal reasoning units.
- Evidence anchors: [section 3.4] "This is a crucial step since it converts the textual summaries into a format that makes comparison and querying efficient." [section 4.2] "This approach aimed to balance the need for detailed information with the constraints of the embedding model." [corpus] Weak direct evidence—corpus papers focus on de-identification, topic modeling, and reasoning paths rather than chunking strategies specifically.
- Break condition: If legal arguments span chunk boundaries without sufficient overlap, retrieval may miss cross-chunk semantic connections.

## Foundational Learning

- Concept: **Abstractive vs. Extractive Summarization**
  - Why needed here: The system combines both in sequence; understanding their tradeoffs (fluency vs. faithfulness) explains why Abs-Ext ordering was chosen.
  - Quick check question: Given a 10-page legal judgment, would an extractive-only approach risk producing disconnected sentences? Why might abstractive-first help?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The framework uses RAG to ground LLM responses in retrieved case law rather than relying solely on parametric knowledge.
  - Quick check question: What failure mode does RAG mitigate that pure LLM prompting cannot? (Hint: think about knowledge cutoffs and hallucination.)

- Concept: **Vector Embeddings and Similarity Search**
  - Why needed here: Understanding how text becomes vectors and how cosine/semantic similarity enables retrieval is foundational to the system's query mechanism.
  - Quick check question: If two legal cases use different terminology for the same concept (e.g., "termination" vs. "dismissal"), would a keyword search find both? Would an embedding-based approach?

## Architecture Onboarding

- Component map: Data Pipeline (Web scraper → raw judgments → Mistral LLM annotation → structured CSV dataset) → Summarization Pipeline (Raw judgment → chunked input → fine-tuned Pegasus → BERTSum → final summary) → Indexing Pipeline (Final summary → document splitter → embedding model → vector database) → Query Pipeline (User query → embedding → similarity search → Llama-2 LLM with retrieved context → response)
- Critical path: Fine-tuning quality → summary quality → embedding quality → retrieval accuracy. Errors in fine-tuning compound downstream.
- Design tradeoffs:
  - Chunk size (1024 tokens) balances context preservation vs. embedding model limits; larger chunks may dilute relevance, smaller chunks may fragment arguments.
  - Top-3 retrieval limits context window usage but may miss relevant cases; increasing k trades off against LLM context limits and latency.
  - Abs-Ext ordering prioritizes coherence; Ext-Abs may preserve more factual precision but loses fluency.
- Failure signatures:
  - High recall, low precision in summaries → abstractive model generating excessive content; check fine-tuning data quality.
  - Retrieved cases irrelevant to query → embedding model mismatch or chunking breaking semantic units; inspect overlap and embedding dimensions.
  - LLM output not using retrieved context → prompt engineering issue or context window overflow; verify prompt structure.
- First 3 experiments:
  1. **Baseline comparison**: Run extractive-only, abstractive-only, Abs-Ext, and Ext-Abs on a held-out test set; compute ROUGE-1/2/L to reproduce paper's Table 5 findings.
  2. **Chunk size ablation**: Test 512, 1024, and 2048 token chunks with 50/100/200 overlap; measure retrieval precision@k and LLM response relevance.
  3. **Fine-tuning data quality audit**: Sample 50 headnotes; assess consistency, completeness, and annotation noise; correlate with per-case summarization metrics.

## Open Questions the Paper Calls Out

- **Question:** To what extent does the integration of Reinforcement Learning with Human Feedback (RLHF) improve the factual accuracy and contextual relevance of summaries compared to the current supervised fine-tuning approach?
- **Basis in paper:** [explicit] The conclusion explicitly identifies RLHF as a future direction to help the LLM learn from real-world legal expertise and improve dependability.
- **Why unresolved:** The current study relies exclusively on fine-tuning the Pegasus model using case headnotes; the RLHF layer has not yet been implemented or evaluated within this framework.
- **Evidence to resolve it:** A comparative study measuring hallucination rates and legal accuracy (via expert review) between the current model and an RLHF-enhanced version.

- **Question:** Can the fine-tuned Pegasus model and RAG framework generalize effectively to other Indian High Courts or international jurisdictions without significant retraining?
- **Basis in paper:** [explicit] The authors state there is considerable potential to expand the framework to other jurisdictions within India and internationally.
- **Why unresolved:** The model is currently fine-tuned exclusively on Calcutta High Court judgments; variations in legal language, citation styles, and statutes across different regions remain untested.
- **Evidence to resolve it:** Zero-shot or few-shot performance metrics (ROUGE scores and retrieval precision) when applying the current model to datasets from other High Courts.

- **Question:** Does the reliance on LLM-generated annotations (Mistral AI) for the training dataset introduce systematic biases or hallucinations that persist in the final summarization output?
- **Basis in paper:** [inferred] The methodology uses Mistral AI for annotation to overcome time constraints, verified by experts. However, the potential propagation of synthetic data errors remains an unstated variable in the model's "ground truth."
- **Why unresolved:** The paper does not isolate the impact of the annotation source; it is unclear if the model learns to mimic the annotator LLM's artifacts rather than purely human legal reasoning.
- **Evidence to resolve it:** A comparative error analysis of the fine-tuned model against a baseline trained on a smaller, purely human-annotated dataset.

## Limitations

- The framework's generalizability to other jurisdictions remains uncertain due to variations in legal writing styles and headnote conventions across different courts.
- The two-step Abs-Ext pipeline assumes the abstractive model faithfully preserves all critical legal information, with no explicit evaluation of hallucination or factual accuracy beyond ROUGE metrics.
- The chunk-based retrieval approach may miss semantically relevant cases if key legal arguments span chunk boundaries, though the 100-token overlap mitigates this risk partially.

## Confidence

- **High confidence**: The Abs-Ext pipeline's superiority over Ext-Abs (supported by direct comparison in results), the basic RAG framework architecture, and the domain-specific fine-tuning approach are well-established mechanisms with clear empirical backing in the paper and related work.
- **Medium confidence**: The effectiveness of the specific chunk size (1024 tokens) and overlap (100 tokens) for legal case retrieval, as the paper provides rationale but limited ablation study evidence.
- **Low confidence**: The framework's performance on legal systems outside Calcutta High Court without additional fine-tuning data, as the paper does not test cross-jurisdictional generalization.

## Next Checks

1. **Factual accuracy audit**: Manually evaluate 50 summarized cases to assess whether the Abs-Ext pipeline introduces factual errors or omissions compared to extractive-only approaches, measuring beyond ROUGE to include legal accuracy metrics.

2. **Chunk boundary analysis**: For 30 sample retrieval queries, examine whether relevant cases are missed due to chunk boundary effects by testing different overlap parameters (50, 100, 200 tokens) and measuring retrieval recall@10.

3. **Cross-jurisdiction stress test**: Fine-tune the Pegasus model on headnote summaries from a different Indian High Court (e.g., Delhi High Court) and evaluate whether the same Abs-Ext pipeline maintains comparable performance, testing the framework's adaptability to different legal writing styles.