---
ver: rpa2
title: Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake
  Detection
arxiv_id: '2601.23066'
source_url: https://arxiv.org/abs/2601.23066
tags:
- audio
- acoustic
- speech
- detection
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenge of speech deepfake detection
  (SDD) using audio large language models (LLMs), highlighting that existing methods
  often overlook fine-grained acoustic artifacts due to semantic-dominant reasoning.
  To address this, the authors propose SDD-APALLM, a framework that explicitly exposes
  time-frequency acoustic evidence by integrating raw audio with structured Constant-Q
  Transform (CQT) spectrograms.
---

# Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection

## Quick Facts
- arXiv ID: 2601.23066
- Source URL: https://arxiv.org/abs/2601.23066
- Reference count: 14
- Key outcome: Explicit acoustic evidence via CQT spectrograms improves speech deepfake detection accuracy and robustness, especially under domain shifts

## Executive Summary
This paper addresses the challenge of speech deepfake detection using audio large language models (LLMs), where existing methods often overlook fine-grained acoustic artifacts due to semantic-dominant reasoning. The authors propose SDD-APALLM, a framework that explicitly exposes time-frequency acoustic evidence by integrating raw audio with structured Constant-Q Transform (CQT) spectrograms. This approach steers the model toward acoustically grounded cues rather than relying on semantic plausibility. Experiments demonstrate that explicit acoustic enhancement improves detection accuracy and robustness, especially under domain shifts, with Qwen2.5-Omni-3B achieving 99.46% accuracy on ASVspoof2019 LA when incorporating CQT.

## Method Summary
The method reformulates speech deepfake detection as a question-answering task for audio LLMs. It processes raw audio through a Whisper encoder and computes CQT spectrograms, which are converted to pseudo-color images and encoded via a frozen ViT. Both audio and visual tokens are aligned to the LLM's hidden dimension and concatenated into a unified sequence. The model is fine-tuned using LoRA on Qwen2.5-Omni-3B/7B with multimodal prompts combining system instructions, audio tokens, and CQT image tokens. Training uses the ms-swift framework with AdamW optimizer and bfloat16 precision.

## Key Results
- Qwen2.5-Omni-3B with audio+acoustic achieves 99.46% accuracy on ASVspoof2019 LA
- Audio+acoustic improves cross-domain robustness (ASVspoof2019 LA → ASVspoof2021 LA: 93.04% → 98.05%)
- Explicit acoustic evidence outperforms audio-only approaches and shows greater stability under domain shifts

## Why This Works (Mechanism)

### Mechanism 1: Acoustic Evidence Accessibility via Explicit Re-parameterization
Fine-grained acoustic artifacts are implicitly present in audio LLMs but inaccessible during inference due to semantic-dominant pretraining objectives. CQT spectrograms re-parameterize the same waveform into a structured time-frequency representation that aligns with the LLM's token interface, making forensic evidence more accessible to attention without adding new information.

### Mechanism 2: Shortcut Learning Suppression Through Structured Constraints
Audio-only SFT encourages reliance on dataset-specific semantic shortcuts that generalize poorly. Explicit acoustic evidence constrains attention toward invariant spectral patterns, reducing over-reliance on semantically correlated cues and forcing engagement with localized acoustic irregularities.

### Mechanism 3: Complementary Grounding Signal (Not Independent Discriminative Pathway)
Explicit acoustic evidence functions as a grounding signal that complements audio representations rather than replacing them. The model coordinates both information sources, weighting acoustic evidence more heavily when semantic cues are unreliable, rather than simply aggregating modalities.

## Foundational Learning

- Concept: Constant-Q Transform (CQT) Spectrograms
  - Why needed here: CQT uses logarithmically spaced frequency bins, matching human pitch perception and highlighting harmonic structures relevant to synthesis artifacts
  - Quick check question: Why would a logarithmic frequency scale be more suitable for detecting speech synthesis artifacts than a linear scale?

- Concept: Shortcut Learning in Supervised Fine-Tuning
  - Why needed here: The paper's central diagnosis is that audio-only SFT causes models to learn spurious correlations that work in-domain but fail under distribution shift
  - Quick check question: If a model trained on ASVspoof2019 LA achieves 98% accuracy but drops to 70% on ASVspoof2021 LA, what type of cues might it have learned to rely on?

- Concept: Token-Level Attention Analysis
  - Why needed here: The paper uses attention visualization to demonstrate that visual tokens are actively involved in reasoning rather than ignored
  - Quick check question: In a multimodal LLM with audio tokens T_aud (317-401) and visual tokens T_vis (415-450), what would the attention map look like if the model were ignoring the spectrogram input?

## Architecture Onboarding

- Component map: Waveform → CQT computation → dB normalization → pseudo-color image → ViT encoding → token alignment → concatenation with audio and text tokens → LLM attention → binary classification output

- Critical path: Waveform → CQT computation → dB normalization → pseudo-color image → ViT encoding → token alignment → concatenation with audio and text tokens → LLM attention → binary classification output

- Design tradeoffs:
  - Frozen vs. trainable vision encoder: Paper freezes ViT to preserve pretrained audio-visual alignment, but this may limit adaptation to forensic-specific patterns
  - LoRA vs. full fine-tuning: LoRA reduces memory but may constrain model's ability to reorganize attention patterns
  - 3B vs. 7B model: Paper shows 3B outperforms 7B on audio+acoustic, suggesting larger models may amplify shortcut learning when semantic cues are unconstrained

- Failure signatures:
  - Near-random zero-shot accuracy (9.31% for 3B): Model has no inherent deepfake detection capability despite strong semantic understanding
  - Large accuracy drop under domain shift with audio-only: Indicates shortcut learning (98.76% → 93.04% from ASVspoof2019 to 2021)
  - Ignored visual tokens in attention map: Would indicate the mechanism is not functioning as intended

- First 3 experiments:
  1. Replicate zero-shot vs. SFT audio-only baseline on ASVspoof2019 LA to confirm the semantic bias and establish performance floor
  2. Ablate spectrogram types (CQT, Mel, STFT, MFCC, LFCC, CQCC) to test whether gains are specific to CQT or general to explicit time-frequency representations
  3. Cross-domain evaluation (ASVspoof2019 LA → ASVspoof2021 LA) comparing audio-only vs. audio+acoustic to validate shortcut learning mitigation; expect smaller accuracy drop with explicit acoustic evidence

## Open Questions the Paper Calls Out

### Open Question 1
Why does increasing model scale (from 3B to 7B parameters) degrade performance in the audio–acoustic fusion setting, contrary to typical scaling laws? The authors observe that the 7B model consistently underperforms the 3B model when jointly processing audio and spectrograms, suggesting larger models may amplify shortcut correlations from raw audio. A layer-wise analysis comparing attention weights assigned to visual acoustic tokens versus raw audio tokens in 3B versus 7B models during inference would resolve this.

### Open Question 2
Can a learnable acoustic front-end outperform the fixed hand-crafted spectrograms used in this study? Section 4.4 notes that "no single acoustic representation consistently dominates" and that improvements stem primarily from the act of exposing time-frequency structure rather than the specific feature design. Replacing the fixed CQT image encoder with a trainable spectral estimation layer and comparing detection accuracy against the fixed CQT baseline would resolve this.

### Open Question 3
Does the "Visual Acoustic Evidence" mechanism generalize to other Audio LLM architectures or tokenization strategies? The methodology and experiments are exclusively conducted on the Qwen2.5-Omni backbone. Applying the SDD-APALLM framework to alternative Audio LLMs (e.g., SALMONN or Macaw-LLM) to verify if performance gains from explicit spectrogram inputs are consistent across architectures would resolve this.

## Limitations

- The mechanism of "accessibility" versus "absence" of acoustic evidence is primarily supported by ablation studies and attention visualization rather than direct causal intervention
- The paper does not systematically compare CQT against alternative time-frequency representations to establish whether gains are specific to CQT's properties or general to explicit acoustic representations
- Cross-domain generalization improvements could potentially be achieved through other regularization strategies or more diverse training data

## Confidence

**High Confidence**: The empirical observation that explicit acoustic evidence (CQT spectrograms) improves speech deepfake detection accuracy in audio LLMs is well-supported by experimental results.

**Medium Confidence**: The claim that gains stem from making forensic acoustic cues "accessible" rather than introducing new information is plausible but not definitively proven.

**Low Confidence**: The specific claim that CQT's logarithmic frequency scaling is essential for improvements, rather than any explicit time-frequency representation, lacks direct experimental validation.

## Next Checks

1. **Representation Ablation Study**: Systematically compare CQT against Mel spectrogram, STFT, MFCC, LFCC, and CQCC representations while keeping all other factors constant. If performance gains are consistent across representations, the mechanism is general to explicit acoustic evidence rather than specific to CQT's properties.

2. **Attention Intervention Experiment**: Use attention intervention techniques to artificially suppress or enhance attention to visual tokens during inference. If forcibly redirecting attention to CQT tokens improves detection performance, this would provide causal evidence for the accessibility mechanism.

3. **Cross-Domain Control Experiments**: Compare the audio+acoustic approach against alternative strategies for improving cross-domain generalization, such as data augmentation, adversarial training, or larger training sets. If these alternatives achieve similar or better cross-domain performance, the claimed advantage of explicit acoustic evidence for domain robustness may be overstated.