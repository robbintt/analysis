---
ver: rpa2
title: 'CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial
  Reasoning'
arxiv_id: '2602.00181'
source_url: https://arxiv.org/abs/2602.00181
tags:
- camera
- arxiv
- reasoning
- wang
- movement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding camera movements
  in videos, which is crucial for both comprehending cinematic storytelling and generating
  physically consistent videos. Current methods struggle to distinguish between visually
  similar but physically distinct camera motions due to their reliance on superficial
  visual patterns rather than geometric cues.
---

# CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning

## Quick Facts
- arXiv ID: 2602.00181
- Source URL: https://arxiv.org/abs/2602.00181
- Reference count: 40
- Primary result: 78.4% binary accuracy and 74.5% VQA accuracy on CameraBench

## Executive Summary
CamReasoner addresses the challenge of camera movement understanding in videos, where current models often confuse visually similar but physically distinct motions. The paper proposes a structured inference approach using an Observation-Thinking-Answer (O-T-A) paradigm that forces explicit geometric reasoning about trajectories and view frustums. Through a two-stage training pipeline (SFT cold start followed by RL refinement) and novel EMA-GRPO optimization, the model achieves state-of-the-art performance, particularly excelling at distinguishing confusable motions like truck versus pan through geometric cue extraction.

## Method Summary
The method employs a two-stage training pipeline: first, supervised fine-tuning (SFT) on 18k reasoning chains generated by Qwen2.5-VL-72B and filtered for format/accuracy/consistency; second, reinforcement learning using GRPO with EMA-based normalization, initialized from the first SFT epoch checkpoint. The model uses Qwen2.5-VL-7B and outputs structured O-T-A format, with rewards combining accuracy and format adherence (λ=0.1). The approach targets both binary classification and visual question answering on camera movement understanding tasks.

## Key Results
- Achieves 78.4% accuracy in binary classification of camera motions
- Scores 74.5% in visual question answering tasks on CameraBench
- Excels at confusable motion scenarios (60.7%) by distinguishing physically distinct movements through geometric reasoning
- Shows stable RL training with format reward reaching 1.0 within ~20 steps and accuracy climbing from 0.68 to 0.85

## Why This Works (Mechanism)

### Mechanism 1: Structured reasoning chains reduce confusable motion errors
The O-T-A paradigm compels the model to first describe spatio-temporal cues (background stability, subject focus), then apply cinematic logic (e.g., "no rotation means truck not pan"), before answering. This prevents shortcut reliance on contextual cues that bypass actual motion analysis.

### Mechanism 2: Two-stage training (SFT cold start → RL refinement) enables both format acquisition and logical sharpening
SFT on 18k reasoning chains teaches the O-T-A structure and basic cinematic vocabulary. RL then optimizes the reasoning paths via reward signals (format adherence + accuracy), allowing the model to discover more robust deduction strategies through exploration.

### Mechanism 3: EMA-GRPO stabilizes single-task RL by decoupling advantage normalization from batch-level variance
Standard GRPO uses batch standard deviation for advantage normalization, causing unstable gradients when low-variance rollouts inflate advantages. EMA-GRPO maintains a running estimate σ(t) of global reward scale, filtering high-frequency noise and enforcing consistent trust regions.

## Foundational Learning

**Camera Motion Primitives (Translation vs. Rotation)**
Why needed: The core task requires distinguishing physically distinct motions: truck (lateral translation) vs. pan (rotation about vertical axis); dolly (forward/backward translation) vs. zoom (focal length change).
Quick check: Can you explain why "truck left" and "pan left" produce different background motion patterns despite similar subject tracking?

**Group Relative Policy Optimization (GRPO)**
Why needed: The RL stage uses GRPO, which estimates baseline from group mean rewards rather than a learned value network. Understanding this is essential for debugging reward scaling and advantage computation.
Quick check: In GRPO, why doesn't the algorithm need a separate critic model, and what assumption does this make about group size?

**Exponential Moving Average (EMA) for Normalization**
Why needed: EMA-GRPO is critical for stability; you must understand how σ(t) is updated and how it affects advantage scaling.
Quick check: If EMA momentum is set too high, what happens to the advantage estimates during a sudden reward distribution shift?

## Architecture Onboarding

**Component map:**
Video clip + query → Qwen2.5-VL-7B (multimodal encoder + LLM decoder) → `<observation>...</observation>` → `<think reasoning="...">...</think reasoning>` → `<answer>...</answer>`

**Critical path:**
1. Data preparation: Generate SFT reasoning chains with Qwen2.5-VL-72B, filter via Qwen3 for format/accuracy/consistency
2. SFT cold start: Train 1 epoch (checkpoint selected at epoch boundary to preserve entropy)
3. RL phase: Initialize from SFT checkpoint, run GRPO with composite reward (accuracy + λ×format), use EMA for advantage normalization
4. Evaluation: Binary classification + VQA on CameraBench

**Design tradeoffs:**
- **λ (format weight)**: Set to 0.1. Higher values (0.5) degrade accuracy by constraining expression; lower values (0.0) risk malformed outputs.
- **SFT checkpoint selection**: First epoch chosen over later epochs to balance format acquisition with policy entropy. Later checkpoints may overfit.
- **Group size G in GRPO**: Trade-off between baseline quality (larger G) and compute/memory cost. Paper uses rollout batch size 128.

**Failure signatures:**
- **Format collapse**: Model outputs answer without observation/think tags → check format reward, increase λ or add format-only warmup
- **Hallucinated observations**: Observation content contradicts video → inspect SFT data quality, verify Qwen3 filtering
- **RL instability**: Accuracy oscillates or drops → check EMA momentum, verify reward scale, inspect gradient norms
- **Confusable motion plateau**: Performance stalls on truck/pan distinctions → examine observation tokens for geometric cue mentions

**First 3 experiments:**
1. **SFT-only baseline**: Train SFT for 3+ epochs and evaluate to confirm first-epoch checkpoint selection rationale. Expect: later epochs may show format improvement but reasoning rigidity.
2. **Ablate EMA-GRPO**: Replace EMA normalization with batch-level normalization. Expect: higher variance in training curves, potential divergence, confirming EMA contribution.
3. **λ sensitivity sweep**: Test λ ∈ {0.0, 0.05, 0.1, 0.2, 0.3} on a held-out validation set. Expect: peak at 0.1, performance drop at extremes as reported in Table 4.

## Open Questions the Paper Calls Out

**Open Question 1: Compositional camera movements**
Can the O-T-A paradigm effectively disentangle and reason about complex, compositional camera movements (e.g., simultaneous pan-and-truck or a "dolly zoom") where multiple physical transformations occur at once?

**Open Question 2: Teacher model limitations**
To what extent does the reliance on Qwen2.5-VL-72B for generating the 18k SFT reasoning chains cap the model's performance, potentially inheriting the teacher model's geometric hallucinations or logical biases?

**Open Question 3: Generalization to "in-the-wild" domains**
How does CamReasoner generalize to "in-the-wild" video domains not represented in CameraBench, such as extreme perspective distortions (fisheye lenses) or highly dynamic environmental noise (extreme weather)?

## Limitations
- Dataset construction relies heavily on Qwen2.5-VL-72B generation and Qwen3 filtering, raising concerns about diversity and potential bias
- EMA-GRPO novelty lacks direct ablation comparisons with standard GRPO to quantify its specific contribution
- All evaluations are conducted on CameraBench, creating potential overfitting concerns for the RL phase

## Confidence

**High Confidence (8-10/10)**: The core architecture and training methodology are clearly described. The SFT + RL pipeline with O-T-A format is implementable as specified.

**Medium Confidence (5-7/10)**: The mechanisms for why structured reasoning improves confusable motion understanding are plausible but not definitively proven.

**Low Confidence (1-4/10)**: Claims about EMA-GRPO's superiority lack direct comparative evidence. The impact of λ=0.1 on the accuracy-format tradeoff curve is asserted but not thoroughly explored.

## Next Checks

1. **Dataset Diversity Test**: Evaluate CamReasoner on an independently collected camera motion dataset (e.g., from different film genres or camera systems) to assess generalization beyond CameraBench.

2. **Ablation of EMA-GRPO**: Implement standard GRPO without EMA normalization on the same training setup and compare training stability curves and final performance to quantify the actual contribution of EMA-based normalization.

3. **O-T-A Format Necessity**: Train an ablation model that skips the observation/thinking stages and directly predicts answers, then compare performance on confusable motion scenarios to isolate the contribution of explicit reasoning structure.