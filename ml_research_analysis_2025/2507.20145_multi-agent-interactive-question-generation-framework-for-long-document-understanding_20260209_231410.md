---
ver: rpa2
title: Multi-Agent Interactive Question Generation Framework for Long Document Understanding
arxiv_id: '2507.20145'
source_url: https://arxiv.org/abs/2507.20145
tags:
- document
- arxiv
- questions
- question
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully automated multi-agent framework for
  generating high-quality question-answer pairs from long-form documents, addressing
  the challenge of limited fine-grained training data for long-context document understanding,
  particularly in low-resource languages like Arabic. The system employs a chain of
  specialized agents for question generation, refinement, answer synthesis, and evidence
  validation, leveraging OCR and layout analysis to maintain contextual accuracy.
---

# Multi-Agent Interactive Question Generation Framework for Long Document Understanding

## Quick Facts
- arXiv ID: 2507.20145
- Source URL: https://arxiv.org/abs/2507.20145
- Reference count: 32
- Primary result: Fully automated multi-agent framework generates high-quality question-answer pairs from long-form documents, achieving up to 49.8% accuracy on Gemini-1.5 Pro and 44.5% on Qwen 2.5 VL

## Executive Summary
This paper presents a fully automated multi-agent framework for generating high-quality question-answer pairs from long-form documents, addressing the challenge of limited fine-grained training data for long-context document understanding, particularly in low-resource languages like Arabic. The system employs a chain of specialized agents for question generation, refinement, answer synthesis, and evidence validation, leveraging OCR and layout analysis to maintain contextual accuracy. Experiments on the newly created AraEngLongBench dataset demonstrate that the generated questions are challenging for major open- and closed-source LVLMs, with closed-source models like Gemini-1.5 Pro achieving up to 49.8% accuracy while open-source models like Qwen 2.5 VL reach 44.5%. The framework significantly advances scalable, high-quality annotation for long-context document understanding tasks.

## Method Summary
The framework uses a five-agent pipeline operating on 10-page overlapping document chunks. Agent 1 generates questions using OCR outputs and layout analysis, Agent 2 filters questions, Agent 3 synthesizes answers, Agent 4 assesses quality and provides iterative feedback (triggering complexity escalation when accuracy exceeds 40%), and Agent 5 validates evidence alignment. The process iterates until target quality is reached, producing challenging question-answer pairs for long-context document understanding evaluation.

## Key Results
- Generated 6,732 questions across 12 types, with 61.8% reasoning and 11.9% hypothetical reasoning
- Best closed-source model (Gemini-1.5 Pro) achieved 49.8% accuracy; best open-source model (Qwen 2.5 VL) achieved 44.5%
- Significant performance gaps between context lengths: Gemini-1.5 Pro scored 84.2% on short context but only 49.8% on long context
- Arabic performance lagged English: Qwen 2.5 VL achieved 41.8% Arabic SP vs. 70.6% English SP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative multi-agent refinement produces higher-quality question-answer pairs than single-pass generation by enabling specialized critique and correction.
- Mechanism: Five specialized agents operate in sequence with a feedback loop—Agent 1 generates questions → Agent 2 filters → Agent 3 synthesizes answers → Agent 4 assesses quality and provides feedback → Agent 1 refines questions → Agent 5 validates evidence alignment. This continues until target quality is reached.
- Core assumption: Decomposing QA generation into specialized sub-tasks with iterative feedback yields better results than monolithic generation, particularly for cross-page reasoning questions.
- Evidence anchors:
  - [abstract] "The system employs a chain of specialized agents for question generation, refinement, answer synthesis, and evidence validation"
  - [section 3.2.2] "Agent 4 (Assessment and Feedback)... provides iterative feedback (F) to guide subsequent improvements"
  - [corpus] MDocAgent paper confirms multi-agent approaches outperform single-modal methods for DocQA tasks

### Mechanism 2
- Claim: Layout-aware document chunking with overlap preserves cross-page dependencies that would otherwise be lost in standard segmentation.
- Mechanism: Documents are split into 10-page overlapping chunks (Ic) with corresponding OCR outputs (Oc) and layout analysis (Lc). The 10-page overlap maintains contextual coherence for questions spanning page boundaries.
- Core assumption: 10-page overlap is sufficient to capture most cross-page dependencies in typical documents; this heuristic may not generalize to all document types.
- Evidence anchors:
  - [abstract] "leveraging OCR and layout analysis to maintain contextual accuracy"
  - [section 3.2.1] "documents are split into processible pieces (Ic), with a 10-page overlap for coherence of context"
  - [corpus] LC-Eval benchmark highlights cross-page reasoning as a critical challenge in long-context understanding

### Mechanism 3
- Claim: Adaptive difficulty adjustment through accuracy-gated feedback produces more challenging questions that better expose LVLM limitations.
- Mechanism: Agent 4 monitors answer generation accuracy; if accuracy exceeds 40%, Agent 1 is triggered to increase question complexity, creating an adversarial dynamic that pushes difficulty upward.
- Core assumption: The 40% threshold represents an appropriate balance between question difficulty and answerability; this parameter was likely empirically tuned but not extensively validated.
- Evidence anchors:
  - [section 3.2.2] "if Agent 4 detects an accuracy rate exceeding 40%, it triggers Agent 1 to increase question complexity"
  - [section 5] Generated questions achieve only 49.8% accuracy on best models (Gemini-1.5 Pro), demonstrating challenging nature
  - [corpus] A-SEA3L-QA paper uses similar adversarial workflow for Arabic QA, suggesting this is an emerging pattern

## Foundational Learning

- Concept: **Large Vision-Language Models (LVLMs) and their context limitations**
  - Why needed here: The entire framework addresses LVLMs' weak performance on long-context documents (≈40% accuracy on existing benchmarks) by generating training data that exposes cross-page reasoning gaps.
  - Quick check question: Can you explain why a model that performs well on single-page DocVQA might fail on 100-page documents?

- Concept: **Multi-agent orchestration patterns**
  - Why needed here: The framework relies on coordinating five agents with different roles; understanding how agents pass state, provide feedback, and terminate loops is essential for debugging and extending the system.
  - Quick check question: What happens if Agent 4's feedback contradicts Agent 5's validation? Which takes precedence?

- Concept: **OCR and layout analysis for document understanding**
  - Why needed here: The preprocessing pipeline uses LVLM-based OCR and YOLO-based layout detection to extract structured representations; understanding these outputs is necessary for debugging question quality issues.
  - Quick check question: Why would the authors choose LVLM-based OCR over traditional OCR engines like Tesseract for this task?

## Architecture Onboarding

- Component map: PDF Input → Preprocessing (pdf2image + LVLM-OCR + YOLO layout) → Chunking (10-page overlap) → [Agent 1: Question Gen] → [Agent 2: Extraction/Filter] → [Agent 3: Answer Gen] → [Agent 4: Assessment + Feedback] ⟲ (loops back to Agent 1 if needed) → [Agent 5: Evidence Validation] → Final QA Pairs

- Critical path: Agent 4 (Assessment) → Agent 1 (Refinement) loop. This is where quality is actually enforced. If Agent 4's feedback is weak or Agent 1 ignores it, the entire system degrades to single-pass generation quality.

- Design tradeoffs:
  - 10-page overlap vs. memory/compute costs (larger overlap = more redundant processing)
  - 40% accuracy threshold for complexity adjustment (lower = harder questions but risk of unanswerable ones)
  - Fully automated vs. human-in-the-loop (scalability vs. quality assurance)
  - LVLM-based OCR vs. traditional OCR (accuracy vs. cost/latency)

- Failure signatures:
  - High unanswerable question rate (6.7% in dataset) may indicate over-aggressive complexity escalation
  - Low UA detection accuracy (5-18% across models) suggests generated "unanswerable" questions may actually be answerable
  - Arabic performance lag (e.g., Qwen 2.5 VL: 41.8% Arabic SP vs. 70.6% English SP) indicates potential issues with Arabic OCR or agent prompting

- First 3 experiments:
  1. **Ablate the feedback loop**: Run the pipeline with Agent 4's feedback disabled (single-pass only) and compare QA quality metrics against the full iterative system on a held-out document set.
  2. **Vary the overlap window**: Test 5-page, 10-page, and 20-page overlap configurations on documents with known cross-page dependencies; measure both question quality and processing time.
  3. **Calibrate the accuracy threshold**: Run sensitivity analysis on the 40% trigger point (try 30%, 40%, 50%, 60%) and evaluate resulting question difficulty distribution and model performance gaps.

## Open Questions the Paper Calls Out

- How does the number of iterative loops in the multi-agent framework quantitatively correlate with the depth of document understanding and question quality? The paper plans to explore this relationship in future work, noting that current work establishes the architecture but lacks ablation studies measuring marginal utility of additional iterations.

- How can confidence calibration in Large Vision-Language Models (LVLMs) be improved to accurately identify and label unanswerable questions? The authors identify this as an important avenue, noting that current state-of-the-art models exhibit low accuracy (8–18%) on unanswerable queries, indicating they lack the ability to abstain from generation when evidence is insufficient.

- To what extent does fine-tuning LVLMs on the synthetically generated AraEngLongBench dataset improve performance on downstream long-context tasks compared to human-annotated data? While the paper validates question difficulty, it assumes the data facilitates LVLM development without presenting experimental results showing actual model improvement after training on this synthetic data.

## Limitations

- The 40% accuracy threshold for triggering question complexity escalation appears arbitrary and lacks systematic justification, potentially impacting both question quality and answerability rates.
- Claims about question difficulty and quality are supported by experimental results but lack ablation studies isolating individual mechanism contributions.
- Arabic-specific performance degradation (41.8% Arabic SP vs. 70.6% English SP) lacks diagnostic analysis to determine whether it reflects genuine language difficulty or implementation artifacts.

## Confidence

- **High confidence**: The framework's architecture and agent workflow are clearly specified with reproducible multi-agent orchestration patterns and preprocessing pipeline.
- **Medium confidence**: Claims about question difficulty are supported by experimental results but lack ablation studies isolating individual mechanism contributions.
- **Low confidence**: The Arabic performance gap and the calibration of the 40% threshold remain empirical assumptions without extensive validation.

## Next Checks

1. **Ablate the feedback loop**: Disable Agent 4's assessment and refinement mechanism to quantify quality improvement from iterative multi-agent coordination versus single-pass generation.

2. **Cross-validate the accuracy threshold**: Systematically vary the 40% trigger point (30%, 40%, 50%, 60%) and measure resulting question difficulty distribution, answerability rates, and LVLM performance gaps to establish optimal calibration.

3. **Diagnose the Arabic performance gap**: Conduct controlled experiments isolating OCR quality, agent prompting, and document layout handling to determine whether Arabic degradation stems from language-specific challenges or implementation artifacts.