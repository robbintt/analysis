---
ver: rpa2
title: 'DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal
  Large Language Model'
arxiv_id: '2512.12633'
source_url: https://arxiv.org/abs/2512.12633
tags:
- visual
- perception
- arxiv
- fine-grained
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiG (Differential Grounding), a novel proxy
  task framework designed to enhance fine-grained visual perception in multimodal
  large language models (MLLMs). DiG trains models to identify and localize all differences
  between similar image pairs without prior knowledge of the number of differences.
---

# DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2512.12633
- Source URL: https://arxiv.org/abs/2512.12633
- Reference count: 40
- Primary result: DiG framework improves fine-grained visual perception in MLLMs, achieving +3.4 on HalBench and +2.1 on V* benchmarks

## Executive Summary
This paper introduces DiG (Differential Grounding), a novel proxy task framework that enhances fine-grained visual perception in multimodal large language models by training them to identify and localize all differences between similar image pairs without prior knowledge of the number of differences. The approach employs a 3D rendering-based data generation pipeline to produce high-quality paired images with controllable discrepancies, combined with a curriculum learning strategy that progressively increases task complexity. Experimental results demonstrate that DiG significantly improves model performance across various visual perception benchmarks and transfers effectively to standard downstream tasks like RefCOCO, RefCOCO+, and RefCOCOg, highlighting it as a scalable approach for advancing fine-grained visual reasoning in MLLMs.

## Method Summary
DiG implements a differential grounding task where models learn to identify and localize all visual differences between paired images without being told the count of differences. The training uses GRPO with a composite reward function combining format validity, detection completeness (F1), and spatial precision (IoU) computed via Hungarian matching. A curriculum learning strategy progressively increases complexity from single-difference to multiple-difference examples, addressing the sparse reward problem inherent in open-ended difference detection. The 3D rendering pipeline generates controllable image pairs with perfect annotations, while the model outputs parsed bounding boxes [x_min, y_min, x_max, y_max].

## Key Results
- 8B variant achieves +3.4 improvement on HalBench and +2.1 on V* benchmarks
- Learned fine-grained perception skills transfer effectively to RefCOCO (+2.7), RefCOCO+ (+3.5), and RefCOCOg (+2.1) benchmarks
- Curriculum learning approach stabilizes training and improves convergence compared to direct mixed-difficulty training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Differential comparison between paired images forces models to develop fine-grained perceptual sensitivity beyond coarse semantic alignment.
- **Mechanism:** Nearly identical images with subtle modifications compel exhaustive visual comparison rather than high-level scene gist reliance. Open-ended formulation without revealing difference count forces systematic attention deployment across entire visual field.
- **Core assumption:** Fine-grained discrimination skills learned on synthetic 3D-rendered scenes transfer to natural images and downstream tasks.
- **Evidence anchors:** Abstract states models learn fine-grained perception by identifying differences without prior knowledge; Section 1 emphasizes moving beyond coarse semantic alignment; ViGoR (2402.06118) supports premise that explicit grounding supervision improves perceptual fidelity.
- **Break condition:** Synthetic-to-real domain gap may be too large, preventing transfer. Paper shows transfer to RefCOCO but this remains conditionally dependent on scene diversity in 3D pipeline.

### Mechanism 2
- **Claim:** Curriculum-based difficulty scheduling resolves sparse reward problem inherent in open-ended difference detection.
- **Mechanism:** Starting with single-difference examples (dense, interpretable feedback) and progressively introducing multiple differences shifts reward landscape from sparse to gradually structured, enabling stable policy convergence.
- **Core assumption:** Perceptual skills acquired on simple instances compose effectively into multi-object reasoning for complex cases.
- **Evidence anchors:** Abstract mentions curriculum learning that progressively increases complexity; Section 3.4 describes initial training on single-difference instances with gradual shift toward complex samples; Section 4.3, Figure 5 shows accuracy reward curves rising rapidly in DiG-1, stabilizing in DiG-2, and maintaining steady improvement in DiG-Mix.
- **Break condition:** If curriculum stages are too short or difficulty jumps too large, policy may fail to transfer skills between stages. Paper uses ~1.6K samples per stage but doesn't ablate stage duration.

### Mechanism 3
- **Claim:** Composite reward function combining format validity, detection completeness (F1), and spatial precision (IoU) provides stable gradient feedback for bounding-box generation.
- **Mechanism:** Format reward enforces syntactic correctness; accuracy reward uses Hungarian bipartite matching to establish one-to-one correspondence between predictions and ground truths, then combines F1 (detection quality) and IoU (localization quality). Balances "finding all differences" with "precisely locating them."
- **Core assumption:** Hungarian matching provides sufficiently stable gradients even when prediction counts differ from ground truth counts.
- **Evidence anchors:** Section 3.3 specifies r_acc = λ₁F1 + λ₂IoU with Hungarian algorithm; Section 4.3, Table 4 shows format+IoU alone drops performance; format+F1 improves precision; all three combined yields best results (88.6/91.5/83.5 on RefCOCO splits).
- **Break condition:** If model systematically over/under-predicts box counts, Hungarian matching may produce noisy assignments, destabilizing training.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** DiG uses GRPO rather than standard PPO. GRPO samples multiple responses per input, computes group-relative advantages (subtracting group mean, dividing by group std), and applies token-level clipped objectives with KL regularization.
  - **Quick check question:** Can you explain why normalizing rewards within a group of sampled responses might improve training stability compared to absolute reward values?

- **Concept: IoU (Intersection over Union) for Bounding Boxes**
  - **Why needed here:** Accuracy reward requires computing spatial overlap between predicted and ground-truth boxes. IoU ranges from 0–1 and is differentiable for matching.
  - **Quick check question:** Given two boxes [0,0,10,10] and [5,5,15,15], what is their IoU? (Answer: intersection area = 25, union area = 175, IoU ≈ 0.143)

- **Concept: Bipartite Matching (Hungarian Algorithm)**
  - **Why needed here:** When model predicts N boxes and ground truth has M boxes (N≠M), optimal one-to-one assignment needed before computing F1 and IoU. Hungarian algorithm maximizes total IoU across matched pairs.
  - **Quick check question:** If a model predicts 3 boxes for a scene with 2 ground-truth differences, how does Hungarian matching handle the extra prediction? (Answer: One prediction remains unmatched, contributing to false positives in F1 calculation)

## Architecture Onboarding

- **Component map:** Data Pipeline (Blender 3D) -> Model Backbone (Qwen3-VL-4B/8B-Thinking) -> Training Loop (GRPO) -> Curriculum Scheduler -> Downstream Evaluation
- **Critical path:** Data generation quality -> reward signal quality -> curriculum pacing -> transfer to downstream tasks. 3D rendering pipeline is linchpin; poorly rendered or unrealistic scenes will limit transfer.
- **Design tradeoffs:**
  - Synthetic vs. real data: Synthetic enables perfect annotations and controllable difficulty but risks domain gap. Paper shows positive transfer, but generalization to unconstrained real-world scenes remains an assumption.
  - Open-ended vs. fixed-count formulation: Not revealing difference count increases task difficulty and potential capability gain, but worsens initial reward sparsity—hence curriculum requirement.
  - Reward weighting (α in r = (1-α)r_acc + αr_format): Too much format weight rewards syntactic correctness without perceptual improvement; too little risks unparseable outputs.
- **Failure signatures:**
  - Reward collapse: If accuracy reward stays near zero after Stage 1, model hasn't learned basic localization—check learning rate, increase single-difference samples, or verify annotation correctness.
  - Over-prediction: Model outputs many low-confidence boxes—likely IoU weight too low relative to F1.
  - Format instability: Model produces unparseable outputs—increase α (format reward weight) or add rejection sampling.
- **First 3 experiments:**
  1. Validate data pipeline: Generate 100 image pairs, manually inspect bounding-box alignment and modification visibility. Ensure differences are perceptible but non-trivial.
  2. Single-difference sanity check: Train on only single-difference data for ~10 steps. Verify accuracy reward rises above 0.5 and format reward reaches 1.0.
  3. Curriculum stage ablation: Compare three training runs—(a) direct mixed-difficulty training, (b) single→double curriculum, (c) full single→double→mixed curriculum—on held-out validation set of real images (e.g., RefCOCO subset). Expected: (c) > (b) >> (a).

## Open Questions the Paper Calls Out
None

## Limitations
- Primary limitation is synthetic-to-real domain gap—while positive transfer shown to RefCOCO benchmarks, extent of generalization to unconstrained real-world scenarios remains unproven.
- Paper does not provide hyperparameter details (α, λ₁, λ₂, learning rates, batch sizes) or ablate critical design choices like curriculum stage duration.
- 3D rendering pipeline may produce scenes lacking visual complexity and ambiguity present in natural images.

## Confidence

- **High confidence**: Mechanism of using paired image comparison to force fine-grained perceptual reasoning is well-founded and supported by experimental results showing consistent performance gains across multiple benchmarks.
- **Medium confidence**: Curriculum learning approach effectively addresses reward sparsity, but exact stage durations and difficulty progression could significantly impact results—these weren't fully ablated.
- **Medium confidence**: Composite reward function design is theoretically sound, but sensitivity to weight choices (α, λ₁, λ₂) and potential instability from Hungarian matching when prediction counts differ substantially from ground truth needs further validation.

## Next Checks

1. **Cross-domain generalization test**: Evaluate trained DiG model on held-out set of real photographs with subtle differences (e.g., image forensics datasets or manually curated real image pairs) to quantify synthetic-to-real transfer effectiveness beyond RefCOCO benchmarks.

2. **Curriculum ablation study**: Systematically vary number of samples per curriculum stage (e.g., 500, 1600, 3000) and transition criteria between stages to identify optimal pacing for stable skill acquisition and transfer.

3. **Reward function sensitivity analysis**: Conduct ablation study varying α (format weight) from 0.05 to 0.5 and λ₁/λ₂ (F1/IoU weights) across parameter space to identify most robust configuration and understand failure modes when individual components dominate.