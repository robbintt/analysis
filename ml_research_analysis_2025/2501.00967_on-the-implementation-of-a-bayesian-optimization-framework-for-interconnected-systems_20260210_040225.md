---
ver: rpa2
title: On the Implementation of a Bayesian Optimization Framework for Interconnected
  Systems
arxiv_id: '2501.00967'
source_url: https://arxiv.org/abs/2501.00967
tags:
- function
- bois
- optimization
- bayesian
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a detailed implementation of the BOIS framework
  for Bayesian Optimization (BO). BOIS is designed to facilitate the use of composite
  functions f(x, y(x)) in a BO setting.
---

# On the Implementation of a Bayesian Optimization Framework for Interconnected Systems

## Quick Facts
- **arXiv ID:** 2501.00967
- **Source URL:** https://arxiv.org/abs/2501.00967
- **Reference count:** 40
- **Primary result:** BOIS significantly outperforms standard Bayesian optimization and matches or beats composite BO algorithms while being 41-104% faster in acquisition function evaluation.

## Executive Summary
This work presents BOIS (Bayesian Optimization of Interconnected Systems), a framework designed to handle composite performance functions of the form f(x, y(x)) in Bayesian optimization settings. The key innovation is an adaptive linearization scheme that enables closed-form calculation of statistical moments for the composite function without requiring computationally expensive Monte Carlo sampling. By exploiting structural knowledge through intermediate function selection and explicit white-box model integration, BOIS reduces surrogate model dimensionality while maintaining solution quality. The framework demonstrates significant computational advantages over existing methods while achieving comparable or superior optimization performance.

## Method Summary
BOIS implements Bayesian optimization for systems with composite functions by modeling intermediate variables y(x) as Gaussian Processes rather than directly modeling f(x). The core mechanism uses adaptive first-order Taylor expansions around reference points to derive analytical expressions for the mean and variance of f, avoiding sampling-based approaches. The framework explicitly incorporates white-box models where available, reducing the dimensionality of surrogate models through nested function structures. Feasibility considerations are handled through boundary-aware linearization reference points, and the Lower Confidence Bound acquisition function is optimized using multi-start SLSQP. The implementation is built on scikit-learn GPs with Matérn kernels and is available in a public repository.

## Key Results
- BOIS was 41-104% faster in acquisition function evaluation compared to Monte Carlo sampling methods
- On chemical process optimization, BOIS achieved <1% regret significantly faster than MC-BO and S-BO
- BOIS matched or outperformed both MC-BO and OP-BO while optimizing over d_x dimensions instead of d_x + d_y dimensions

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Linearization Converts Nonlinear Composite Functions into Closed-Form Statistics
BOIS approximates intractable probability densities for nonlinear composite functions by using adaptive first-order Taylor expansions around reference points, enabling analytical expressions for mean and variance without sampling. The framework decomposes f(x, y(x)) = g(x) + h(x, y) where g contains white-box elements. At each point x, it linearizes h around ŷ_ℓ⁰ (an ε-neighborhood of the GP-predicted mean ŷ_ℓ) using the Jacobian J = ∇_y f(x, ŷ_ℓ⁰). This yields closed-form expressions: m_f^ℓ(x) = J^T ŷ_ℓ + g(x) + h(x, ŷ_ℓ⁰) - J^T ŷ_ℓ⁰ and σ_f^ℓ(x) = (J^T Σ_ℓ J)^(1/2). The core assumption is that the composite function f is locally Gaussian in the neighborhood of ŷ_ℓ⁰ (Laplace approximation validity), requiring ε to be small enough for linearization accuracy but large enough to avoid numerical instability.

### Mechanism 2: Structural Knowledge Exploitation via Intermediate Function Selection
By selecting intermediate functions that capture physical relationships and leverage known white-box models, the framework reduces surrogate model dimensionality and focuses learning on truly unknown components. Instead of modeling f(x) directly, the framework models y(x)—a smaller set of intermediate variables. The paper explicitly separates white-box elements (heaters, compressors with known equations) from black-box elements (reactors, separators requiring surrogates). Intermediate functions are chosen to: (1) have physical meaning, (2) enable white-box model integration, (3) reduce GP input dimensions through nesting. The core assumption is that the system has exploitable structure—some components have closed-form representations while others don't, and the intermediate functions are correctly specified to capture system behavior.

### Mechanism 3: Feasibility-Aware Mean Adjustment
The framework maintains feasibility of intermediate function predictions by adjusting the linearization reference point toward bounds when GP means fall outside permissible ranges. When m_y^ℓ(x) < l̂_y or m_y^ℓ(x) > û_y, the framework computes Δ_lo = max{0, l̂_y - m_y^ℓ(x)} and Δ_hi = min{0, û_y - m_y^ℓ(x)}, then sets ŷ_ℓ = m_y^ℓ(x) + Δ_lo + Δ_hi. This clips the reference point to feasibility bounds, and subgradients handle derivative discontinuities. The core assumption is that if the GP predicts infeasible values, the true value is likely near the closest bound (reasonable for physical quantities like flow rates or concentrations).

## Foundational Learning

- **Concept: Gaussian Process (GP) Surrogate Modeling**
  - **Why needed here:** BOIS builds GP models for each intermediate function y_i(x) to obtain mean predictions and uncertainty estimates. Understanding how GPs provide both m_y^ℓ(x) and Σ_y^ℓ(x) is essential for grasping why propagation to f is challenging.
  - **Quick check question:** Given a GP trained on noisy data at points x_K, can you explain why the posterior variance σ_y^ℓ(x) increases as you move away from observed points?

- **Concept: First-Order Taylor Expansion (Linearization)**
  - **Why needed here:** The core innovation relies on linearizing f around a reference point to obtain closed-form statistics. You need to understand why f(x, y) ≈ f(x, y_0) + J^T(y - y_0) enables variance propagation as σ_f^2 = J^T Σ_y J.
  - **Quick check question:** If f(x, y_1, y_2) = y_1 · y_2 where y_1, y_2 are independent Gaussians, can you linearize around y_0 = (2, 3) and compute the approximate variance of f?

- **Concept: Acquisition Functions and Exploration-Exploitation Trade-off**
  - **Why needed here:** BOIS uses the Lower Confidence Bound (LCB) acquisition function AF^ℓ(x) = m_f^ℓ(x) - κ · σ_f^ℓ(x). Understanding how κ controls exploration vs. exploitation is critical for tuning the algorithm.
  - **Quick check question:** If you increase κ from 1.0 to 3.0, will the algorithm sample more from regions with high predicted performance or high uncertainty? What happens to convergence speed?

## Architecture Onboarding

- **Component map:** Dataset D_y^ℓ → GP Training (per intermediate y_i) → GP Models GP_y^ℓ → Point of interest x → GP Evaluation: m_y^ℓ(x), Σ_y^ℓ(x) → Feasibility Adjustment: ŷ_ℓ → Linearization Point ŷ_ℓ⁰ = ŷ_ℓ ± ε → Jacobian J = ∇_y f(x, ŷ_ℓ⁰) → Closed-form m_f^ℓ, σ_f^ℓ → LCB-BOIS Acquisition Function → AF Optimization → Next sample x^{ℓ+1} → System Evaluation at x^{ℓ+1} → y^{ℓ+1} → Update D_y^{ℓ+1}

- **Critical path:**
  1. **Intermediate function specification** (most impactful design choice): Define y(x) to capture physical relationships while minimizing dimensionality. Poor choices here cascade into all downstream steps.
  2. **GP model construction**: Train independent single-output GPs for each y_i. Use nested structure to reduce input dimensions where possible.
  3. **Linearization reference selection**: Set ε = ŷ_ℓ × 10^{-3} as default; adjust if numerical gradients are unstable.
  4. **Acquisition function optimization**: Use multi-start (50 points) with SLSQP; this is where computational savings vs. MC-BO/OP-BO are realized.

- **Design tradeoffs:**
  - **BOIS vs. MC-BO**: BOIS is 41-104% faster in AF evaluation (parity plots show ~2 orders of magnitude speedup for moment estimation). MC-BO provides asymptotically exact estimates but requires S ≥ 1000 samples for comparable accuracy.
  - **BOIS vs. OP-BO**: BOIS optimizes over d_x dimensions; OP-BO optimizes over d_x + d_y dimensions. For 5+ intermediates, OP-BO becomes significantly slower (3.3× in chemical process case).
  - **Accuracy near bounds**: BOIS assumes symmetric distributions; accuracy degrades near feasibility limits. If intermediates span orders of magnitude, consider log-transform before GP modeling (noted as future work).
  - **Exploration weight κ**: Paper uses fixed κ; no ablation provided. Higher κ increases robustness to local optima but slows convergence.

- **Failure signatures:**
  1. **High variance in AF evaluations between runs** → Likely using MC-BO with insufficient samples; switch to BOIS or increase S.
  2. **Convergence to infeasible solutions** → Feasibility bounds not properly enforced in intermediate function clipping; check l̂_y, û_y definitions.
  3. **No improvement after initial samples** → Intermediate functions may not capture key physics; revisit system decomposition. S-BO showed this symptom in chemical process case due to inability to learn flow penalty.
  4. **Numerical instability in Jacobian computation** → ε too small or intermediate function has discontinuous derivatives; increase ε or smooth the function.

- **First 3 experiments:**
  1. **Validate linearization accuracy**: On your system, train GP models with 20-30 initial samples, then compare BOIS estimates of m_f^ℓ(x) and σ_f^ℓ(x) against MC-BO with S=1000 at 50-100 random points. Target: <5% mean absolute error for both moments.
  2. **Benchmark computational cost**: Measure AF optimization time for BOIS vs. MC-BO (S=100, 1000) vs. OP-BO across 10 iterations. Expect BOIS to be 2-5× faster than MC-BO and 3-10× faster than OP-BO for d_y ≥ 5.
  3. **Sensitivity to initialization**: Run 10-20 trials with different random initial points (2-5 samples each). Compare convergence rate (iterations to reach within 1% of optimum) and final solution quality. BOIS should show lower variance across trials than S-BO or MC-BO.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can alternative surrogate models like Warped GPs or RNNs improve BOIS accuracy near intermediate function feasibility limits?
**Basis in paper:** The Conclusion states an interest in exploring alternatives like Warped GPs and RNNs to address the inaccuracy caused by the symmetry assumption of standard GPs near feasibility bounds.
**Why unresolved:** The current implementation assumes a symmetric Gaussian distribution for y(x), which causes estimation errors for σ_f(x) when the mean is close to a boundary (e.g., strictly positive values).
**What evidence would resolve it:** Benchmarking BOIS with non-Gaussian surrogates on constrained problems, showing reduced bias in moment estimation compared to the standard GP implementation.

### Open Question 2
**Question:** How does the computational efficiency and solution quality of BOIS scale relative to MC-BO and OP-BO in high-dimensional systems?
**Basis in paper:** The authors explicitly list "investigating the performance of BOIS in higher dimensional systems" as a direction for future work in the Conclusion.
**Why unresolved:** The provided case studies are limited to low dimensions (d_x ≤ 5, d_y ≤ 5), leaving the scalability of the adaptive linearization scheme unproven for complex, large-scale problems.
**What evidence would resolve it:** Empirical results on test functions with significantly higher input and intermediate dimensions (e.g., d > 10) demonstrating that BOIS maintains its speed advantage over OP-BO without succumbing to the curse of dimensionality.

### Open Question 3
**Question:** Can the BOIS framework be extended to support parallelization through batch acquisition functions?
**Basis in paper:** The Conclusion mentions the goal of "developing alternative types of AFs that can extend the functionality of the algorithm, such as enabling parallelization."
**Why unresolved:** The current acquisition function (LCB-BOIS) is designed for sequential optimization; the ability to generate multiple query points simultaneously without disrupting the linearization logic is unexplored.
**What evidence would resolve it:** The derivation and successful implementation of a batch-compatible acquisition function (e.g., q-LCB) within the BOIS framework that generates independent batches of sample points.

## Limitations
- Accuracy degrades near feasibility bounds due to Gaussian assumption breaking down for asymmetric distributions
- Effectiveness depends critically on correctly identifying and specifying intermediate functions that capture system physics
- Limited validation on high-dimensional problems where scalability concerns may emerge

## Confidence
- **High**: Computational efficiency claims (41-104% faster than MC-BO), general optimization performance on benchmark problems
- **Medium**: Accuracy of statistical moment estimation in non-boundary regions, feasibility-aware adjustment effectiveness
- **Low**: Performance in highly nonlinear regions, effectiveness for systems with limited structural knowledge

## Next Checks
1. **Asymmetry Impact Test**: Evaluate BOIS performance on systems where intermediate functions follow known asymmetric distributions (e.g., log-normal for strictly positive quantities) to quantify accuracy degradation near bounds.
2. **Structural Knowledge Sensitivity**: Systematically vary the amount of available structural knowledge in benchmark problems to determine the threshold below which BOIS loses its advantage over standard BO approaches.
3. **Nonlinear Curvature Analysis**: Compare BOIS linearization accuracy against higher-order approximation methods (e.g., second-order Taylor expansion) on problems with known strong nonlinearities to establish the limits of the first-order approach.