---
ver: rpa2
title: Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority
  Samples
arxiv_id: '2502.16948'
source_url: https://arxiv.org/abs/2502.16948
tags:
- class
- loss
- prior
- worst
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a minimax learning algorithm for imbalanced
  datasets with limited minority samples. The core idea involves a targeted logit
  adjustment (TLA) loss function for model training and a linear ascent method for
  updating the target prior.
---

# Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority Samples

## Quick Facts
- arXiv ID: 2502.16948
- Source URL: https://arxiv.org/abs/2502.16948
- Reference count: 40
- Proposes minimax learning algorithm achieving superior worst-class accuracy on imbalanced datasets with limited minority samples

## Executive Summary
This paper addresses the challenge of training classifiers on imbalanced datasets with very few minority samples by introducing a minimax learning framework. The core innovation combines a targeted logit adjustment (TLA) loss function that efficiently identifies optimal decision boundaries under arbitrary target priors, with a linear ascent method for updating the target prior that is particularly robust when minority samples are scarce. The algorithm partitions data into model and prior update sets, trains in three phases (warmup, minimax, fine-tune), and demonstrates significant improvements in worst-class accuracy compared to existing methods on CIFAR10 and CIFAR100 datasets.

## Method Summary
The method partitions the training data into 80% for model updates and 20% for prior updates, preserving the imbalance ratio. It uses a three-phase training schedule: 5 epochs warmup with fixed prior, 295 epochs minimax phase with linear ascent prior updates, and 30 epochs fine-tune phase with merged data. The TLA loss adds a bias term to logits proportional to the log ratio of training to target priors, while linear ascent identifies the M worst-performing classes and shifts the prior directly toward them. Hyperparameters include SGD with learning rate decay, α=0.01 for prior updates, and τ values tuned for specific datasets.

## Key Results
- Achieves 51.52% worst-class accuracy on CIFAR10 step imbalance (ρ=0.01), significantly higher than baseline methods
- Linear ascent outperforms exponential gradient ascent in convergence speed and stability with limited samples
- TLA loss increases cat class accuracy from 22.4% to 65.0% compared to standard weighted cross-entropy
- Maintains competitive balanced accuracy while improving worst-class performance

## Why This Works (Mechanism)

### Mechanism 1: Targeted Logit Adjustment
- Claim: TLA loss enables effective decision boundary adjustment under arbitrary target priors, outperforming weight-based methods in overparameterized networks
- Mechanism: Adds bias term ℓy(πt) = τ(log πtrain_y - log πt_y) directly to neural network logits, forcing output adjustment proportional to target prior
- Core assumption: Network trained with vanilla CE approximates Bayes-optimal classifier for training data
- Evidence anchors: Theorem 2 proves TLA produces outputs proportional to target prior; Table IV shows 22.4% to 65.0% improvement for cat class
- Break condition: Fails when πt = πtrain (adjustment vanishes) or model cannot learn meaningful representations

### Mechanism 2: Linear Ascent for Prior Updates
- Claim: Linear ascent more robustly identifies adversarial priors than exponential gradient ascent when minority samples are limited
- Mechanism: Makes "hard" decisions by identifying M worst-performing classes and shifting prior directly: πt+1 ← πt + α(πt,max,M − πt)
- Core assumption: Worst-class error probability substantially exceeds other classes, making identification reliable even with few samples
- Evidence anchors: Theorem 4 shows <0.01 failure probability at 16 samples; Table II demonstrates higher target prior values for worst class
- Break condition: Degrades when M too large relative to total classes K, or when multiple classes have nearly identical worst-class error rates

### Mechanism 3: Dataset Partitioning Strategy
- Claim: Partitioning enables reliable 0-1 loss measurement for prior updates in overparameterized networks
- Mechanism: Uses 80/20 split maintaining imbalance ratio, keeping independent set where class-wise errors remain informative
- Core assumption: Dprior contains sufficient minority samples for reliable worst-class identification
- Evidence anchors: Section III-C describes partitioning strategy; references zero-error problem in overparameterized networks
- Break condition: Fails when minority classes have extremely few samples (<5 in Dprior)

## Foundational Learning

### Concept: Minimax Decision Theory and Relaxation
- Why needed here: Understanding relaxation from min_θ max_π R(π,θ) to max_π min_θ R(π,θ) via minimax inequality is theoretical foundation
- Quick check question: Can you explain why the relaxation is necessary and what it implies about the relationship between R* = min_θ max_π R(π,θ) and R† = max_π min_θ R(π,θ)?

### Concept: Importance Weighting vs Logit Adjustment for Distribution Shift
- Why needed here: Understanding why TLA outperforms weighting in overparameterized networks
- Quick check question: Given training prior π_train and target prior π_t, construct both TWCE loss (πt_y/πtrain_y × CE) and TLA loss (CE with adjusted logits). What happens to each when the network achieves perfect training accuracy?

### Concept: Variance of Empirical Risk Estimation
- Why needed here: Understanding why EGA fails with limited samples requires grasping that MSE of P̂(e)_y,θ ∝ 1/Ny
- Quick check question: If a class has 5 samples and 4 are misclassified, what is the estimated error rate and its variance? Why does linear ascent avoid this problem?

## Architecture Onboarding

### Component Map:
Input: D_train with imbalance ratio ρ → Phase 0 (Warmup, T0=5 epochs): Train with TLA, πt = πtrain → Phase 1 (Minimax, T1=295 epochs): Dmodel (80%) → TLA loss → θt via SGD, Dprior (20%) → Evaluate 0-1 loss → Find M worst → Linear ascent → πt+1 → Phase 2 (Fine-tune, T2=30 epochs): Full Dtrain with final πt → Output: Model optimized for worst-class accuracy

### Critical Path:
1. Hyperparameter initialization: Set M (1-10), α (0.01), τ (0.5-2.25), split ratio (0.8/0.2)
2. Partition data: Verify Dprior has ≥16 samples per minority class for reliable worst-class identification
3. Warmup: Train until feature extractor stabilizes (5 epochs)
4. Minimax iterations: Monitor target prior for worst class increases monotonically
5. Fine-tune: Merge datasets and verify worst-class accuracy maintains gains

### Design Tradeoffs:
- **M (worst-class count)**: Larger M accelerates convergence but increases bound to 2√2√(K/M)√Kε. Paper uses M=1 for step imbalance, M=3-10 for others
- **τ (adjustment strength)**: Higher τ more aggressively shifts boundaries but may harm balanced accuracy. Paper uses τ=2.25 for CIFAR10, τ=0.5-1.375 for CIFAR100
- **Split ratio**: 80/20 is standard but reduces training data. Fine-tuning phase recovers this
- **Assumption**: Theorem 2 assumes τ=1 is optimal, but empirical results use τ>1 for insufficient data

### Failure Signatures:
- **Prior oscillates**: α too large or multiple classes have similar worst-class error; reduce α or increase M
- **Balanced accuracy degrades severely**: τ too aggressive; worst-class gains offset by other class losses
- **Worst-class identification noisy**: Dprior has <16 minority samples; increase split ratio or verify data quality
- **No improvement over baseline**: May be in "sufficient samples" regime; standard methods adequate

### First 3 Experiments:
1. Replicate CIFAR10 step ρ=0.01 (Table V hardest setting): Use M=1, α=0.01, τ=2.25. Track worst-class accuracy vs LA baseline. Expected: ~51.5% vs ~41.7%
2. Linear ascent vs EGA convergence: Using same TLA loss, plot πt,worst over epochs. Expected: Linear ascent reaches higher prior values faster and more stably
3. Feature visualization (t-SNE): Compare TLA vs TWCE on step-imbalanced CIFAR10. Expected: TLA shows better-separated minority class clusters, especially for overlapping classes like cat/dog

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of worst classes M be determined adaptively during the linear ascent phase rather than being set as a static hyperparameter?
- Basis in paper: Section III-B discusses trade-off in selecting M, stating it should be set "as small as possible under the constraint of finding the worst class correctly," yet experiments use fixed values tuned for specific datasets
- Why unresolved: Paper provides theoretical bounds for convergence based on fixed M but doesn't provide mechanism to automatically adjust M in response to changing error probabilities
- What evidence would resolve it: Algorithmic procedure that dynamically adjusts M based on variance of per-class errors, achieving comparable or better convergence without manual tuning

### Open Question 2
- Question: Does the 80/20 data partitioning strategy significantly hinder model performance on datasets with extremely few minority samples?
- Basis in paper: Section III-C describes partitioning into Dmodel and Dprior with 0.8/0.2 ratio "inspired by the widely used ratio," but this effectively reduces already limited training data for minority classes
- Why unresolved: While fine-tuning phase attempts to mitigate this by merging sets, initial training phase operates on reduced data which may be critical for few-shot minority classes
- What evidence would resolve it: Ablation study analyzing sensitivity of worst-class accuracy to split ratio on datasets with extreme imbalance (ρ < 0.01)

### Open Question 3
- Question: How does the linear ascent method perform on large-scale datasets with thousands of classes compared to standard minimax approaches?
- Basis in paper: Experimental results limited to CIFAR10 and CIFAR100 (10 and 100 classes respectively), while computational complexity depends on identifying worst classes among K
- Why unresolved: Unclear if robustness of linear ascent holds when number of classes is large, where "tail" of distribution is wider and identifying correct M worst classes becomes statistically harder
- What evidence would resolve it: Benchmark results on large-scale long-tailed datasets such as ImageNet-LT or iNaturalist

## Limitations

- Theoretical guarantees provide lower bound rather than exact minimax solution, limiting optimality claims
- All experiments use CIFAR10/100 with ResNet32; performance on different architectures or real-world datasets remains untested
- Results depend heavily on specific τ, M, and α values tuned for CIFAR experiments
- 80/20 data partitioning reduces training data by 20%, which could be problematic for extremely scarce minority classes

## Confidence

- **High confidence**: Targeted Logit Adjustment effectively shifts decision boundaries under target priors (supported by Theorem 2 and CIFAR10 Table IV results showing 42.6% to 65.0% cat class improvement)
- **Medium confidence**: Linear ascent robustly identifies worst classes with limited samples (supported by Theorem 4 and Figure 1 showing <0.01 failure probability at 16 samples, though real-world noise effects untested)
- **Low confidence**: Theoretical convergence bounds apply broadly across imbalance types (Corollary 7 provides bounds but empirical validation beyond CIFAR is absent)

## Next Checks

1. **Architecture generalization test**: Apply algorithm to Vision Transformer (ViT) on CIFAR10 step imbalance (ρ=0.01) and compare worst-class accuracy against ResNet32 baseline. This tests architectural independence of the logit adjustment mechanism.

2. **Sample scarcity stress test**: Reduce Dprior minority class samples to 8 and 4 per class while monitoring worst-class identification failure rate and final accuracy. This validates the 16-sample threshold claim from Theorem 4.

3. **Real-world dataset validation**: Apply the algorithm to a medical imaging dataset (e.g., ChestX-ray14 with rare pathology classes) and compare worst-class detection rates against standard resampling methods. This tests performance on structured minority classes beyond synthetic imbalance.