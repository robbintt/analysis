---
ver: rpa2
title: 'When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs'
arxiv_id: '2510.15421'
source_url: https://arxiv.org/abs/2510.15421
tags:
- image
- reasoning
- images
- question
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic benchmark for multimodal
  active reasoning, GUESSBENCH, which requires MLLMs to actively acquire missing evidence
  and iteratively refine decisions under incomplete information by selecting a target
  image from a candidate pool. The benchmark combines real-world images with nine
  synthetic domains (perception-oriented and knowledge-oriented) to comprehensively
  evaluate visual abstraction and knowledge-integrated reasoning.
---

# When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs

## Quick Facts
- **arXiv ID**: 2510.15421
- **Source URL**: https://arxiv.org/abs/2510.15421
- **Reference count**: 40
- **Primary result**: Active reasoning performance lags significantly behind passive settings, with top models achieving ~0.70 vs. >0.90 on proposed metrics.

## Executive Summary
This paper introduces GUESSBENCH, the first systematic benchmark for multimodal active reasoning, where models must actively acquire missing evidence through iterative binary queries to identify a target image from a candidate pool. The benchmark evaluates 20 mainstream MLLMs across real-world and synthetic domains, revealing that active reasoning performance (avg. ~0.70) significantly lags behind passive reasoning (avg. >0.90). Key challenges identified include limited fine-grained visual perception and untimely decision-making, with perception-oriented tasks consistently underperforming knowledge-oriented tasks by ~27.5%. The study demonstrates that perceptual enhancements benefit smaller models while thinking-oriented methods provide consistent gains across model sizes.

## Method Summary
The benchmark presents models with candidate image pools and requires iterative binary queries to identify a target image under incomplete information. A GuessAgent validates queries and provides binary yes/no responses based on image attributes and captions. Models must balance information acquisition against decision timeliness, with performance measured by a composite metric combining accuracy, efficiency, and planning competence. The benchmark includes nine synthetic domains and real-world images, evaluating both perception-oriented and knowledge-oriented reasoning tasks.

## Key Results
- Active reasoning performance (avg. ~0.70) significantly lags behind passive reasoning (avg. >0.90) across all evaluated models
- Perception-oriented domains (avg. 0.2966) consistently underperform knowledge-oriented domains (avg. 0.4090)
- Fine-grained visual perception challenges cause ~15.8% performance drop on edited images
- Early queries yield higher marginal information gain, with first four steps accounting for ~77.3% of total progress
- Larger models show better early stopping behavior, with 72B models outperforming smaller variants

## Why This Works (Mechanism)

### Mechanism 1: Iterative Belief Updating Through Binary Queries
Active reasoning performance depends on the model's ability to maintain and update a probability distribution over candidates as evidence accumulates. The model receives a candidate pool, maintains history of prior Q&A pairs, and at each step either queries or commits. Early queries yield higher marginal information gain, with first four steps accounting for about 77.3% of total progress. Failures in tracking eliminated candidates cause redundant queries and degrade performance.

### Mechanism 2: Perceive-Then-Think Decomposition
Active reasoning decomposes into visual abstraction to extract discriminative features and knowledge-integrated reasoning to select informative queries. Perception bottlenecks cascade: fine-grained difference is more difficult for models to perceive, causing 15.8% performance drop on edited images. Knowledge-oriented tasks outperform perception-oriented tasks (avg. 0.4090 vs. 0.2966), suggesting current MLLMs have stronger knowledge retrieval than fine-grained visual discrimination.

### Mechanism 3: Verification-Triggered Early Stopping
Timely decision-making—knowing when to stop querying—is a learned thresholding behavior that improves with model scale and explicit thinking training. Early stopping yields 51.7% average improvement across sizes. Larger models show better stopping criteria, and training with explicit thinking outperforms prompt-based CoT/ReAct, indicating learned verification beats heuristic prompting.

## Foundational Learning

- **Concept: Passive vs. Active Reasoning Distinction**
  - **Why needed here:** The paper's core contribution is formalizing active reasoning—where evidence is incomplete and must be acquired—as distinct from passive reasoning with full information. Understanding this gap (e.g., Qwen2.5-VL-7B: 91.2% passive vs. 43.1% active) is essential for interpreting results.
  - **Quick check question:** If you give a model all relevant information upfront and it answers correctly, have you tested its active reasoning ability? (No—that's passive reasoning.)

- **Concept: Information-Theoretic Query Selection**
  - **Why needed here:** The benchmark's efficiency metric uses $T_{min} = \lceil \log_2 B \rceil$ (ideal binary search). Models must select queries that approximately halve the hypothesis space to approach optimal efficiency.
  - **Quick check question:** With 8 candidates, what's the theoretical minimum number of yes/no questions needed to identify the target? (3 questions, since $2^3 = 8$.)

- **Concept: Perception-Knowledge Trade-off in MLLMs**
  - **Why needed here:** Results show perception tasks consistently underperform knowledge tasks (~27.5% relative gap). New engineers must recognize that scaling alone doesn't fix fine-grained visual perception; specialized enhancements help smaller models more.
  - **Quick check question:** Why might a model correctly identify an animal's biological classification (knowledge) but fail to distinguish two nearly identical kitchen layouts (perception)? (Knowledge retrieval relies on pre-trained semantic associations; fine-grained visual discrimination requires high-resolution feature extraction.)

## Architecture Onboarding

- **Component map:** Evaluation set -> MLLM Under Test -> GuessAgent -> Binary responses -> MLLM updates belief state -> Next query or final answer

- **Critical path:**
  1. Load evaluation set (8 candidates, 1 target) from domain-specific pool
  2. MLLM generates query → GuessAgent validates → returns Yes/No/Invalid
  3. MLLM updates belief state, generates next query or final answer
  4. Repeat until STOP or max steps (10); compute metrics

- **Design tradeoffs:**
  - Binary queries constrain the model but enable automated response generation; open-ended would require more complex NLI verification
  - Max steps = 10 balances exploration vs. computational cost; >90% of correct identifications happen within 6 steps for top models
  - Synthetic vs. real images: synthetic enables controlled attribute manipulation (fine-grained perception tests); real images validate ecological validity

- **Failure signatures:**
  - Redundant queries: repeating questions or asking about already-eliminated features → low planning competence score
  - Early commitment: stopping when pool size > 1 → accuracy drops
  - Late commitment: continuing queries when pool size = 1 → efficiency penalty, no accuracy gain
  - Invalid queries: non-binary questions → GuessAgent returns Invalid, wasting a turn

- **First 3 experiments:**
  1. Baseline scaling test: Run Qwen2.5-VL-3B/7B/32B/72B on real-world images with pool size 8; plot accuracy vs. parameter count
  2. Perception stress test: Compare performance on original vs. fine-grained-edited images (single-attribute change) in face/kitchen domains
  3. Thinking-mode ablation: Run Kimi-VL (non-thinking) vs. Kimi-VL-Thinking on knowledge-oriented domains

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on GuessAgent's binary response accuracy (>98% reported, but no independent verification), meaning systematic errors could unfairly penalize MLLM performance
- Synthetic domains may not capture real-world ambiguity and noise that affects human-like reasoning
- Composite metric weights accuracy, efficiency, and planning equally, but optimal weighting for practical applications remains unclear

## Confidence

- **High Confidence:** Systematic performance gap between passive (0.90+) and active reasoning (0.70) across 20 models is robust and well-supported
- **Medium Confidence:** "Early queries yield higher marginal information gain" supported by 77.3%/22.7% step distribution, but influenced by binary query constraint
- **Low Confidence:** GuessAgent's reliability preventing false responses from affecting results assumes perfect validation; any systematic bias would compound across interactions

## Next Checks
1. **Independent GuessAgent Verification:** Run a subset of queries through multiple GuessAgent instances and human annotators to establish confidence intervals on response accuracy
2. **Metric Sensitivity Analysis:** Recompute benchmark scores with alternative weightings (e.g., 50% accuracy, 25% efficiency, 25% planning) and test whether relative performance rankings remain consistent
3. **Extended Step Budget Experiment:** Remove the 10-query cap for top-performing models on most challenging synthetic domains to determine whether additional queries would improve accuracy