---
ver: rpa2
title: 'MedQA-CS: Objective Structured Clinical Examination (OSCE)-Style Benchmark
  for Evaluating LLM Clinical Skills'
arxiv_id: '2410.01553'
source_url: https://arxiv.org/abs/2410.01553
tags:
- diagnosis
- finding
- physical
- score
- exam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MedQA-CS is an AI-Structured Clinical Examination (AI-SCE) framework\
  \ inspired by medical education\u2019s Objective Structured Clinical Examinations\
  \ (OSCEs) to evaluate LLM clinical skills. The benchmark includes two instruction-following\
  \ tasks: LLM-as-medical-student (MedStuLLM) and LLM-as-clinical-skill-examiner (MedExamLLM),\
  \ designed to reflect real clinical scenarios."
---

# MedQA-CS: Objective Structured Clinical Examination (OSCE)-Style Benchmark for Evaluating LLM Clinical Skills

## Quick Facts
- **arXiv ID:** 2410.01553
- **Source URL:** https://arxiv.org/abs/2410.01553
- **Reference count:** 40
- **Primary result:** OSCE-style clinical skill benchmark shows state-of-the-art LLMs score 48-62% vs 90%+ on traditional MCQs

## Executive Summary
MedQA-CS introduces an AI-Structured Clinical Examination (AI-SCE) framework inspired by medical education's Objective Structured Clinical Examinations (OSCEs) to evaluate LLM clinical skills. The benchmark includes two instruction-following tasks: LLM-as-medical-student (MedStuLLM) and LLM-as-clinical-skill-examiner (MedExamLLM), designed to reflect real clinical scenarios. Experiments showed that MedQA-CS is more challenging than traditional multiple-choice QA benchmarks, with state-of-the-art LLMs scoring significantly lower. Quantitative and qualitative assessments demonstrated that carefully designed prompts aligned with OSCE guidelines enable LLM-as-Judge to achieve high correlation with expert evaluations in clinical skill assessment, validating the potential of LLM-as-Judge frameworks for future clinical skill evaluations.

## Method Summary
MedQA-CS transforms 44 USMLE Step 2 CS cases into 1,667 instruction-input-output triplets across four clinical skill sections: Information Gathering, Physical Exams, Closure, and Differential Diagnosis. The framework uses a dual-role approach where MedStuLLM performs clinical tasks and MedExamLLM evaluates outputs using OSCE-derived rubrics. A GPT-4-based judge model, calibrated against expert human evaluations, scores student outputs across structured criteria. The benchmark tests both knowledge recall and procedural skills through sequential task phases requiring sustained instruction-following behavior. Correlation analysis validates the judge's reliability, with Pearson's r ranging from 0.47 to 0.92 across sections.

## Key Results
- **Benchmark Difficulty:** State-of-the-art LLMs score 48.44-62.35% on MedQA-CS vs 90%+ on traditional MCQ benchmarks like MedQA
- **Judge Reliability:** GPT-4-based MedExamLLM achieves Pearson's r of 0.90-0.92 with expert evaluations for InfoGatherQA and Physical Exams
- **Model Performance:** GPT-4o scores 62.35%, Claude-3.5-Sonnet scores 61.57%, while smaller open-source models struggle with instruction-following requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structured OSCE-style multi-phase evaluation better reveals clinical skill gaps than single-turn MCQ benchmarks.
- **Mechanism**: By decomposing clinical encounters into sequential phases (InfoGatherQA, Physical Exams, Closure, Differential Diagnosis), the framework requires sustained, instruction-following behavior across tasks with different output formats (questions, exam documentation, patient-facing summaries, diagnostic reasoning). This tests procedural knowledge and communication, not just recall.
- **Core assumption**: Clinical competence is multi-faceted; assessing only knowledge recall misses skill-based failures.
- **Evidence anchors**:
  - [abstract] "MedQA-CS is a more challenging benchmark for evaluating clinical skills than traditional multiple-choice QA benchmarks (e.g., MedQA)."
  - [Section 1] "...OSCEs primarily evaluate students' practical skills... including patient examination, clinical history recording, effective communication..."
  - [corpus] Related work on clinical reasoning benchmarks (e.g., MedKGI) similarly emphasizes iterative hypothesis-driven reasoning over single-turn QA, supporting the shift from recall to skill assessment.
- **Break condition**: If LLMs were only tested on knowledge retrieval, skill gaps in communication or workflow would remain undetected.

### Mechanism 2
- **Claim**: LLM-as-Judge, when constrained by OSCE-derived evaluation rubrics, can achieve high agreement with clinical experts.
- **Mechanism**: Detailed rubrics (e.g., Exam Coverage, Reason Relevance, Extra Exams Penalty for Physical Exams) reduce subjective judgment space. The judge LLM is prompted to compare outputs to ground truth with explicit scoring rules, and outputs structured JSON with reasoning, forcing transparency.
- **Core assumption**: Strong LLMs (e.g., GPT-4) encode sufficient clinical knowledge to recognize correct/incorrect patterns when evaluation criteria are explicit.
- **Evidence anchors**:
  - [Section 4.1] "GPT-4 exhibited the highest reliability, with Pearson r. of 0.90, 0.92, and 0.78... for the Information Gathering, Physical Exam, and Diagnosis, respectively."
  - [Section 2.2] Expert agreement (Pearson's r 0.77–0.99) validates the rubric design; MedExamLLM prompts are derived from these same rubrics.
  - [corpus] A related paper (Benchmarking Generative AI for Scoring Medical Student Interviews in OSCEs) also explores LLMs for automated OSCE scoring, suggesting this is a broader trend but does not yet prove generalizability to all specialties or cultures.
- **Break condition**: Rubric ambiguity or judge model weakness (e.g., GPT-3.5, smaller models) leads to low correlation with experts. The Closure section showed lower correlation (0.47), attributed to its subjective nature.

### Mechanism 3
- **Claim**: The dual-role setup (MedStuLLM + MedExamLLM) enables scalable, reproducible clinical skill assessment.
- **Mechanism**: MedStuLLM attempts the clinical task, generating free-form text. MedExamLLM evaluates this output using rubrics and ground truth. This decoupling allows (1) independent improvement of each component, (2) use of a single strong judge to evaluate many student models, and (3) structured, explainable scoring.
- **Core assumption**: The MedExamLLM evaluation is sufficiently reliable to serve as an automatic metric.
- **Evidence anchors**:
  - [abstract] "...experiments show that MedQA-CS is a more challenging benchmark... with state-of-the-art LLMs scoring significantly lower."
  - [Table 3] Scores range from 48.44 (GPT-3.5) to 62.35 (Claude-3.5-Sonnet), far below the 90+ scores on MedQA MCQs.
  - [corpus] The distillation experiment (LLaMA-judge) achieved high correlation on InfoGatherQA (0.9289) and Physical Exam (0.93) but struggled with Diagnosis (<0.3), indicating judge capability varies by task complexity and model size.
- **Break condition**: If the MedExamLLM is unreliable (low expert correlation), all student evaluations are suspect. The paper selects GPT-4 as the judge based on correlation results.

## Foundational Learning

- **Concept: Miller's Pyramid of Clinical Competence**
  - **Why needed here**: The framework explicitly targets the "shows how" level (performance in simulated settings), distinct from "knows" or "knows how" assessed by MCQs. Understanding this hierarchy clarifies why OSCE-style benchmarks are necessary.
  - **Quick check question**: Can an LLM answer questions about heart attack symptoms ("knows") but fail to ask a patient about radiation of pain during a simulated encounter? How does this benchmark detect such a gap?

- **Concept: Objective Structured Clinical Examination (OSCE) Workflow**
  - **Why needed here**: MedQA-CS mirrors the USMLE Step 2 CS phases. Familiarity with the clinical workflow (doorway info → history → physical exam → closure → note) is essential to interpret the task design and scoring criteria.
  - **Quick check question**: In an OSCE, why is the "Closure" phase evaluated separately from the "Differential Diagnosis" in the patient note? What distinct skills does each assess?

- **Concept: LLM-as-Judge Evaluation Paradigm**
  - **Why needed here**: The MedExamLLM component relies on using an LLM to score outputs based on rubrics. Understanding the strengths (semantic understanding) and weaknesses (subjectivity, model dependency) of this paradigm is critical for interpreting results and potential failures.
  - **Quick check question**: Why might a smaller open-source model (e.g., LLaMA-8B) fail as a MedExamLLM for the Diagnosis section even if it can perform well as a MedStuLLM for other tasks?

## Architecture Onboarding

- **Component map**: Dataset (44 USMLE cases) -> MedStuLLM pipeline (student role) -> MedExamLLM pipeline (judge role) -> Human evaluation layer (expert validation) -> Correlation analysis framework (reliability assessment)

- **Critical path**:
  1. **Validate MedExamLLM reliability**: Run expert annotation on a subset (e.g., 10 cases). Compute expert-expert agreement to confirm rubric soundness.
  2. **Select judge**: Benchmark candidate LLMs (GPT-4, GPT-4o, Claude-3-Opus, etc.) against expert annotations. Select the model with highest correlation and acceptable cost.
  3. **Benchmark MedStuLLMs**: Use the selected judge to evaluate target LLMs on all cases. Report scores by section and average.
  4. **Iterate on prompts/rubrics**: If expert agreement is low for a section (e.g., Closure), refine the rubric or prompt based on expert feedback.

- **Design tradeoffs**:
  - **Closed-source vs. open-source judge**: GPT-4 provides the highest reliability but raises reproducibility and cost concerns. Distilling to LLaMA-8B works for some sections but not all (e.g., Diagnosis). Consider hybrid approaches or further fine-tuning on expert judgments.
  - **Dataset size vs. coverage**: 44 cases are consistent with real OSCE standards (12 cases determine pass/fail) but may lack specialty diversity (e.g., pediatrics, psychiatry). Expansion is noted as future work.
  - **Per-question vs. per-conversation evaluation**: InfoGatherQA evaluates each question independently, not the full conversation. This simplifies evaluation but may miss flow/integration issues.

- **Failure signatures**:
  - **Low judge-expert correlation**: Especially in subjective sections like Closure. Indicates rubric ambiguity or judge model inadequacy.
  - **Instruction-following failures**: Smaller open-source models (e.g., <7B) often fail to generate valid structured outputs (e.g., JSON with required keys). See Table 3 where models produce '-' for invalid outputs.
  - **Catastrophic forgetting in domain adaptation**: Models fine-tuned solely on medical knowledge benchmarks may lose instruction-following ability, resulting in lower MedStuLLM scores.
  - **Repetition in InfoGatherQA**: Models may ask the same question multiple times if conversation history is not properly managed.

- **First 3 experiments**:
  1. **Reproduce judge reliability analysis**: Use the provided expert annotations (e.g., 10 cases per section) to compute correlation between your MedExamLLM (e.g., GPT-4, GPT-4o) and experts. Verify alignment with reported Pearson/Kendall scores.
  2. **Benchmark a baseline and a frontier model as MedStuLLM**: Evaluate GPT-3.5 and Claude-3.5-Sonnet across all four sections using the validated MedExamLLM. Compare average scores and section-specific failures.
  3. **Test a domain-adapted model**: Fine-tune an open-source model (e.g., LLaMA3-8B) on medical knowledge corpora, then evaluate as MedStuLLM. Compare with its base model to observe the effect of domain adaptation on instruction-following vs. knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can treatment plan formulation be integrated into the MedQA-CS benchmark?
- **Basis in paper**: [explicit] The authors note the current dataset lacks treatment plans and state, "In the future, we aim to explore how to gather suitable treatment plan data... to integrate into MedQA-CS."
- **Why unresolved**: The source USMLE Step 2 CS data utilized evaluates students only up to the diagnostic part of clinical note generation.
- **Evidence**: Successful gathering of suitable treatment plan data and its integration/evaluation within the framework.

### Open Question 2
- **Question**: How can the reasoning process of clinical LLMs be evaluated without ground truth references?
- **Basis in paper**: [explicit] The authors note they did not consider reasoning because it is difficult to produce stable scores without ground truth, adding, "In future work, we plan to explore reference-free clinical reasoning evaluation."
- **Why unresolved**: Current LLM-as-Judge frameworks struggle to generate stable and reliable scores for reasoning without predefined reference outputs.
- **Evidence**: A reference-free evaluation method that correlates highly with expert qualitative assessments of clinical reasoning.

### Open Question 3
- **Question**: How can the performance of open-source medical judge models be enhanced for complex diagnosis evaluation?
- **Basis in paper**: [explicit] The authors found their distilled LLaMA-judge struggled with diagnosis (correlation < 0.3) compared to GPT-4, stating, "Future work is needed to enhance the performance of open-source medical judge models..."
- **Why unresolved**: The complexity of diagnosis evaluation appears to exceed what simple distillation can achieve with an 8B parameter model.
- **Evidence**: An open-source judge model achieving significantly higher correlation with expert evaluations on the Diagnosis section.

### Open Question 4
- **Question**: What training strategies effectively combine clinical domain knowledge with complex instruction-following capabilities?
- **Basis in paper**: [explicit] The authors note domain adaptation training negatively impacted instruction-following due to catastrophic forgetting, concluding, "Future work should continue to refine these strategies... that integrates both."
- **Why unresolved**: Current domain adaptation training causes catastrophic forgetting of instruction-following abilities, creating a trade-off between knowledge and skills.
- **Evidence**: A training methodology that improves scores on both knowledge benchmarks (e.g., MedQA) and clinical skill benchmarks (MedQA-CS) simultaneously.

## Limitations

- **Judge dependency**: Reliance on GPT-4 for reliable evaluation raises reproducibility concerns and limits accessibility for smaller research groups.
- **Dataset size**: 44 cases are consistent with OSCE standards but may lack diversity across medical specialties and cultural contexts.
- **Subjective evaluation challenges**: Closure section shows lowest judge-expert correlation (0.47), indicating limitations in evaluating subjective clinical communication skills.

## Confidence

- **High Confidence**: The core finding that MedQA-CS is more challenging than traditional MCQ benchmarks (with scores 48-62% vs 90%+ on MedQA) is well-supported by direct experimental evidence across multiple model types.
- **Medium Confidence**: The LLM-as-Judge framework's reliability is demonstrated for InfoGatherQA and Physical Exams (r > 0.9) but shows significant variation by section, with Diagnosis particularly challenging for smaller models.
- **Medium Confidence**: The dataset construction and transformation from USMLE Step 2 CS cases to instruction triplets is methodologically sound, though the relatively small case count limits generalizability.

## Next Checks

1. **Judge Model Generalization**: Test the MedExamLLM framework with alternative judge models (Claude-3.5-Sonnet, GPT-4o, and open-source alternatives like LLaMA-3-70B) to assess consistency of scores across different LLM-as-Judge implementations and identify systematic biases.

2. **Dataset Expansion and Diversity**: Validate the benchmark's performance across additional clinical scenarios, particularly in underrepresented specialties (pediatrics, psychiatry) and with diverse patient demographics to ensure the framework captures broad clinical competence.

3. **Clinical Expert Validation**: Conduct a larger-scale expert evaluation (20+ cases, multiple specialties) to verify the correlation between LLM-as-Judge scores and human expert assessments, particularly focusing on the Closure and Diagnosis sections where current correlation is weakest.