---
ver: rpa2
title: Labels or Preferences? Budget-Constrained Learning with Human Judgments over
  AI-Generated Outputs
arxiv_id: '2601.13458'
source_url: https://arxiv.org/abs/2601.13458
tags:
- data
- pcal
- estimator
- page
- pcal-ca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the optimal allocation of limited annotation
  budgets between ground-truth labels and pairwise preference labels in AI systems.
  The authors frame the problem as a monotone missing data issue under Missing At
  Random (MAR) assumptions and propose Preference-Calibrated Active Learning (PCAL),
  a method that jointly determines the optimal data acquisition strategy and constructs
  an efficient estimator for the target parameter.
---

# Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs

## Quick Facts
- arXiv ID: 2601.13458
- Source URL: https://arxiv.org/abs/2601.13458
- Authors: Zihan Dong; Ruijia Wu; Linjun Zhang
- Reference count: 40
- Primary result: Optimal budget allocation between ground-truth labels and pairwise preference labels using Preference-Calibrated Active Learning (PCAL) that achieves semiparametric efficiency bound

## Executive Summary
This paper addresses the fundamental challenge of optimally allocating limited annotation budgets between traditional ground-truth labels and pairwise preference labels in AI systems. The authors frame this as a monotone missing data problem under Missing At Random (MAR) assumptions and propose PCAL, a method that jointly determines optimal data acquisition and constructs efficient estimators. The approach minimizes asymptotic variance without requiring closed-form solutions and works across general settings with multiple data types.

The proposed method is theoretically grounded, proving that PCAL achieves the semiparametric efficiency bound under standard conditions while providing robustness guarantees even with poorly estimated nuisance parameters. Empirical validation on simulations and a politeness dataset demonstrates that PCAL significantly outperforms baseline methods, producing shorter confidence intervals while maintaining proper coverage, offering a principled approach for budget-constrained learning in modern AI applications.

## Method Summary
The paper tackles optimal budget allocation between ground-truth labels and pairwise preference labels through a monotone missing data framework under MAR assumptions. The proposed Preference-Calibrated Active Learning (PCAL) method jointly optimizes the data acquisition strategy and constructs an efficient estimator for the target parameter. PCAL directly minimizes the asymptotic variance of the estimator without requiring closed-form solutions, making it applicable to general settings with multiple data types. The method theoretically achieves the semiparametric efficiency bound under standard conditions and provides robustness guarantees even when nuisance parameters are poorly estimated.

## Key Results
- PCAL achieves the semiparametric efficiency bound under standard conditions, providing optimal statistical efficiency for budget-constrained learning
- Simulations and real-data analysis on a politeness dataset show PCAL significantly outperforms baseline methods with shorter confidence intervals while maintaining proper coverage
- The method provides robustness guarantees even when nuisance parameters are poorly estimated, enhancing practical applicability

## Why This Works (Mechanism)
PCAL works by framing the budget allocation problem as a monotone missing data issue, where preference labels are considered missing at random conditional on observed covariates. This allows the method to leverage semiparametric efficiency theory to construct an optimal estimator that minimizes asymptotic variance. The joint optimization of data acquisition and estimation ensures that the sampling strategy directly targets the reduction of estimation uncertainty, rather than treating these components separately.

## Foundational Learning
- Monotone missing data framework: Essential for modeling the hierarchical nature of having ground-truth labels without corresponding preference labels, enabling principled statistical inference
- Semiparametric efficiency theory: Provides the theoretical foundation for constructing estimators that achieve optimal variance bounds, ensuring maximum statistical efficiency
- Missing At Random (MAR) assumptions: Critical for valid inference, as they ensure that the missingness mechanism is independent of the unobserved outcomes conditional on observed data
- Quick check: Verify that the MAR assumption holds approximately in the target application domain by examining whether missing preference labels are plausibly independent of their values given observed covariates

## Architecture Onboarding
- Component map: Budget allocation module -> Data acquisition strategy -> Estimator construction -> Variance minimization -> Parameter estimation
- Critical path: The joint optimization of sampling strategy and estimator construction is the critical path, as these components must be solved simultaneously to achieve the efficiency bound
- Design tradeoffs: The method trades computational complexity for statistical efficiency, requiring iterative optimization but achieving optimal variance reduction compared to simpler sequential approaches
- Failure signatures: Performance degradation when MAR assumptions are severely violated, or when nuisance models are misspecified beyond the stated robustness bounds
- First experiments:
  1. Test PCAL on a simple synthetic dataset with known ground truth to verify basic functionality and parameter recovery
  2. Compare variance reduction against a naive uniform allocation strategy on a moderately sized real dataset
  3. Evaluate robustness by intentionally misspecifying nuisance models and measuring performance degradation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the theoretical and empirical validity of the proposed approach within the assumed framework.

## Limitations
- The primary uncertainty stems from the MAR assumption for preference labels, which may not hold in real-world scenarios where human judgment is influenced by factors beyond observed covariates
- Theoretical guarantees depend critically on correct specification of nuisance models, though empirical validation of the stated robustness is limited
- The simulation and politeness dataset experiments cover only limited application domains and sample sizes, raising questions about generalizability
- Performance comparison with baselines lacks sufficient detail on specific methods used and computational overhead relative to simpler approaches

## Confidence
- **High confidence**: The formulation of budget-constrained learning as a monotone missing data problem under MAR assumptions; the theoretical proof that PCAL achieves the efficiency bound under standard conditions
- **Medium confidence**: The practical performance improvements demonstrated in simulations and the politeness dataset; the robustness claims under poorly estimated nuisance parameters
- **Low confidence**: The generalizability of PCAL to domains with non-MAR missing mechanisms; the scalability of the method to large-scale annotation tasks

## Next Checks
1. Conduct experiments on multiple datasets with different missing data mechanisms (including non-MAR scenarios) to test the robustness of PCAL beyond the assumed framework
2. Compare PCAL against a broader range of baseline active learning methods with detailed computational cost analysis
3. Perform ablation studies to isolate the contribution of the joint optimization component versus the efficient estimator construction in PCAL's performance