---
ver: rpa2
title: 'FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction'
arxiv_id: '2601.22578'
source_url: https://arxiv.org/abs/2601.22578
tags:
- traffic
- global
- patterns
- federated
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FedDis, a federated causal disentanglement\
  \ framework designed to address the challenges of non-IID data in federated traffic\
  \ prediction. The proposed method separates globally stable traffic patterns from\
  \ client-specific local dynamics through a dual-branch architecture\u2014a Global\
  \ Pattern Bank for shared knowledge and a Personalized Bank for client-specific\
  \ factors."
---

# FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction

## Quick Facts
- arXiv ID: 2601.22578
- Source URL: https://arxiv.org/abs/2601.22578
- Reference count: 40
- Primary result: Up to 7.17% MAPE improvement over federated baselines in traffic prediction

## Executive Summary
FedDis introduces a federated causal disentanglement framework that addresses non-IID data challenges in distributed traffic prediction. The framework separates globally stable traffic patterns from client-specific dynamics through a dual-branch architecture. By employing mutual information minimization and graph attention fusion, FedDis achieves state-of-the-art performance while maintaining privacy through decentralized learning.

## Method Summary
FedDis employs a dual-branch architecture consisting of a Global Pattern Bank for capturing shared traffic patterns and a Personalized Bank for client-specific dynamics. The framework uses mutual information minimization to enforce informational orthogonality between branches, preventing redundancy. Graph attention fusion is applied for similarity-aware parameter aggregation across clients. This causal disentanglement approach enables the model to learn both universal traffic patterns and local variations without sharing raw data.

## Key Results
- Achieves up to 7.17% improvement in MAPE compared to competitive federated baselines
- Demonstrates superior performance on METR-LA, PEMS-BAY, PEMS03, and PEMS04 datasets
- Shows better expandability and computational efficiency while maintaining privacy

## Why This Works (Mechanism)
FedDis works by separating traffic prediction into globally stable patterns and client-specific local dynamics. The Global Pattern Bank captures common traffic characteristics across all clients, while the Personalized Bank models unique local factors. Mutual information minimization ensures these two knowledge sources remain distinct and non-redundant. Graph attention fusion allows the model to adaptively aggregate parameters based on client similarity, making the learning process more efficient and effective in heterogeneous environments.

## Foundational Learning

1. **Federated Learning Basics**
   - Why needed: Enables distributed training without sharing raw data
   - Quick check: Understanding of client-server architecture and local-global model updates

2. **Graph Neural Networks**
   - Why needed: Traffic data naturally forms spatial-temporal graphs
   - Quick check: Familiarity with graph convolution operations and message passing

3. **Causal Disentanglement**
   - Why needed: Separates shared from client-specific patterns
   - Quick check: Understanding of mutual information minimization and its role in feature separation

4. **Non-IID Data Handling**
   - Why needed: Real-world federated learning involves heterogeneous client data
   - Quick check: Knowledge of statistical heterogeneity and its impact on model convergence

## Architecture Onboarding

**Component Map:** Input Data -> Global Pattern Bank -> Personalized Bank -> Mutual Information Minimization -> Graph Attention Fusion -> Output Prediction

**Critical Path:** Input features flow through both banks in parallel, undergo mutual information minimization to ensure orthogonality, then combine via graph attention fusion for final prediction

**Design Tradeoffs:** The framework trades computational overhead from dual-branch architecture for improved accuracy and generalization across heterogeneous clients

**Failure Signatures:** Performance degradation when mutual information minimization fails to properly separate global and local patterns, or when graph attention cannot effectively measure client similarity

**First 3 Experiments to Run:**
1. Compare performance with and without mutual information minimization on non-IID datasets
2. Test scalability by increasing client count while maintaining performance
3. Evaluate transferability to non-traffic domains with similar spatial-temporal characteristics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- Focus exclusively on traffic prediction tasks without verification in other domains
- Mutual information minimization may face computational challenges at scale
- Reliance on consistent graph structures across clients may not hold in all scenarios

## Confidence

- **High confidence**: Experimental MAPE improvements up to 7.17% on standard traffic datasets
- **Medium confidence**: Computational efficiency and expandability claims due to limited scaling benchmarks
- **Medium confidence**: Privacy preservation claims lacking specific attack analysis

## Next Checks

1. Conduct ablation studies to quantify individual contributions of Global Pattern Bank, Personalized Bank, and mutual information minimization components
2. Test framework robustness under extreme non-IID distributions and varying client counts to assess scalability
3. Evaluate performance on non-traffic domains (e.g., healthcare or industrial IoT) to verify cross-domain applicability