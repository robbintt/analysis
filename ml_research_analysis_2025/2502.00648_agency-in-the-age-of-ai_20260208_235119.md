---
ver: rpa2
title: Agency in the Age of AI
arxiv_id: '2502.00648'
source_url: https://arxiv.org/abs/2502.00648
tags:
- agency
- these
- agent
- theory
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that agency is the appropriate theoretical lens
  to study the harms and benefits of generative AI tools on society. It proposes a
  new theory of agency that combines relational and cognitive perspectives, incorporating
  beliefs, goals, and partial plans within a social and environmental context.
---

# Agency in the Age of AI

## Quick Facts
- arXiv ID: 2502.00648
- Source URL: https://arxiv.org/abs/2502.00648
- Authors: Samarth Swarup
- Reference count: 14
- Primary result: Agency is the appropriate theoretical lens to study generative AI harms, requiring a new quantitative theory combining relational and cognitive perspectives

## Executive Summary
This paper proposes that agency theory provides the appropriate framework for understanding generative AI's societal impacts. The author argues that current approaches to regulating AI focus on surface-level harms rather than the underlying attacks on human agency. A new theory is needed that combines cognitive perspectives (beliefs, goals, plans) with relational perspectives (social context, past actions, future projections) while incorporating quantitative, information-theoretic measures of agency. The paper outlines a vision for agent-based simulations that can evaluate scenarios and discover interventions to protect or enhance agency in the face of AI threats.

## Method Summary
The paper proposes developing a quantitative theory of agency by combining planning theory (beliefs, goals, partial plans) with relational theory (social context, iterational patterns, projective elements) and enactive perspectives (self-individuation, interactional asymmetry). The approach would use information-theoretic measures like empowerment and Markov blankets to quantify agency changes, implemented through agent-based simulations containing baseline humans, AI-augmented humans, and autonomous AI agents. The method aims to discover interventions that protect or enhance agency through systematic scenario testing and analysis of simulation data.

## Key Results
- Generative AI harms can be systematically classified as attacks on specific components of agent cognition using a BDI framework
- A new theory of agency should combine relational and cognitive perspectives with quantitative, information-theoretic measures
- Agent-based simulations can evaluate scenarios and discover interventions to protect agency against AI threats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative AI harms can be systematically classified as attacks on specific components of agent cognition (beliefs, goals, plans), enabling unified theoretical treatment.
- Mechanism: The BDI model decomposes agency into beliefs about the world, desires/goals, and partial plans. Adversarial attacks map to: (A1) blocking plan execution, (A2) corrupting planning processes, (A3) manipulating goal selection, (A4) making goals seem unachievable via belief manipulation, (A5) degrading belief formation systems, (A6) constraining available environmental options.
- Core assumption: Malicious actors have sufficient computational power and access to target agents' information environment—which generative AI now provides at scale.
- Evidence anchors:
  - Section 1 categorizes harms (misinformation, liar's dividend, tool corruption, unintended consequences) and Section 2 maps them to attacks A1-A6
  - States "increasing alarm about the misuses to which these tools can be put, and the intentional and unintentional harms"
  - "Regulating the Agency of LLM-based Agents" (arXiv:2509.22735) proposes directly measuring and controlling LLM agent agency
- Break condition: If agency harms cannot be meaningfully quantified or if attacks on different agency components produce non-decomposable emergent effects

### Mechanism 2
- Claim: Combining the Planning Theory of Agency with the Relational Theory and enactive perspectives addresses observed limitations in modeling multi-agent, environmentally-embedded agency.
- Mechanism: Planning Theory provides beliefs/goals/plans; Relational Theory adds iterational patterns, projective elements, and practical-evaluative judgment in social contexts; Enactive theory adds self-individuation and interactional asymmetry. Information-theoretic measures potentially enable quantitative agency assessment.
- Core assumption: These theoretically distinct frameworks can be mathematically integrated without foundational contradictions.
- Evidence anchors:
  - Section 3 states "our vision of a new theory of agency combines the relational and cognitive perspectives"
  - Observations O1-O4 identify limitations: "Agency is fundamentally a multi-agent phenomenon," "Agent cognition needs to be a part," "Self-monitoring needs to be part," and "needs to be quantitative"
  - Corpus lacks direct implementations of this synthesis
- Break condition: If information-theoretic measures cannot scale beyond simple agents/environments, or if self-individuation boundaries prove uncomputable

### Mechanism 3
- Claim: Agent-based models containing baseline humans, AI-augmented humans, and autonomous AI agents can simulate agency dynamics and discover interventions.
- Mechanism: ABMs generate structured datasets from micro-level agent interactions. By varying agent configurations and running simulations across scenarios, researchers can observe agency changes and test interventions.
- Core assumption: Simulation fidelity can be sufficient to transfer intervention discoveries to real-world policy, despite epistemic uncertainty and stylized model assumptions.
- Evidence anchors:
  - Section 3.1: "Our idea is to develop ABMs that include representations of 'baseline' humans using the extended BDI models"
  - Identifies challenges: "Realistic simulation design," "Scaling," "Epistemic uncertainty," "Explainability," "Causality"
  - "On the limits of agency in agent-based models" (Chopra et al., 2024) is cited
- Break condition: If epistemic uncertainty in ABM design dominates signal, or if adversarial agents make systems fundamentally unpredictable

## Foundational Learning

- Concept: **BDI (Belief-Desire-Intention) Model**
  - Why needed here: This is the computational operationalization of Planning Theory that the paper extends. Without understanding BDI, the attack classification (A1-A6) and proposed model extensions are opaque.
  - Quick check question: Given an agent with belief B="my email is secure," desire D="send confidential message," and intention I="use encrypted channel," which component does a phishing attack primarily target?

- Concept: **Information-Theoretic Agency Measures** (empowerment, Markov blankets, variational free energy)
  - Why needed here: The paper proposes these as the quantitative foundation for measuring agency changes. Understanding what these measure is essential for evaluating whether the theory can be operationalized.
  - Quick check question: "Empowerment" quantifies an agent's potential influence on future states via available actions—why might this decrease when an agent receives corrupted information?

- Concept: **Relational/Enactive Agency Theory**
  - Why needed here: The proposed synthesis draws on sociological and embodied cognition perspectives that differ fundamentally from individualist planning models. These provide the "social and environmental context" missing from standard BDI.
  - Quick check question: In enactive theory, why is "self-individuation" (distinguishing self from environment) a prerequisite for agency rather than an assumption?

## Architecture Onboarding

- Component map: Extended BDI Agent -> Environment -> Belief Formation System -> Social Network -> Goal Selection Module -> Other Agents: Baseline/AI-augmented/Autonomous AI -> Planning/Plan Library -> Self-Monitoring/Agency Assessment -> Information-Theoretic Quantification Layer

- Critical path:
  1. Formalize extended BDI model with relational/enactive extensions (address O1-O3)
  2. Implement information-theoretic agency quantification (address O4)
  3. Build ABM infrastructure with synthetic populations for target scenarios
  4. Validate on simple scenarios before scaling

- Design tradeoffs:
  - **Realism vs. explainability**: More realistic simulations reduce analytical transparency—"doubly hard to explain, at the level of both the trees and the forest"
  - **Scale vs. computational tractability**: Large agent populations with complex cognition may be infeasible; current BDI scaling research is ongoing
  - **Generality vs. specificity**: Domain-specific models provide actionable insights but require substantial design effort; generic models may miss critical dynamics

- Failure signatures:
  - Agency measures that are uninterpretable or don't correlate with intuitive assessments of harm
  - Simulations where adversaries always win or where no interventions show effect (model may be misspecified)
  - Epistemic uncertainty swamping intervention signals (cannot distinguish real effects from model artifacts)
  - Unpredictability in competitive domains—"if there is any regularity that can be exploited... the regularity disappears"

- First 3 experiments:
  1. **Minimal viable test**: Implement a simple 2-agent (agent + adversary) BDI system with A5 attack (belief manipulation via misinformation). Quantify agency change using empowerment measure. Verify that the measure responds to attack intensity.
  2. **Scenario validation**: Build a small-scale election simulation (N~100 agents) with misinformation injection. Compare: (a) baseline agency, (b) agency under attack, (c) agency with candidate interventions (media literacy, source verification). Assess whether information-theoretic measures align with qualitative harm assessments.
  3. **Scaling stress test**: Attempt to scale the election simulation to N~10,000 agents with heterogeneous cognition (some AI-augmented). Identify computational bottlenecks and whether agency measures remain stable or become dominated by noise/epistemic uncertainty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Relational and Planning Theories of agency be integrated into a quantitative, information-theoretic framework that remains tractable for complex multi-agent simulations?
- Basis in paper: The author proposes combining these theories (p. 3) but notes that current formalisms, such as Markov blankets, are "hard to apply... to any but the simplest of agents and environments" (p. 4).
- Why unresolved: Bridging high-level sociological concepts with computationally efficient information-theoretic metrics for large-scale use is a theoretical bottleneck.
- What evidence would resolve it: A formalized mathematical definition of agency that can be calculated within large-scale agent-based models without intractable computational costs.

### Open Question 2
- Question: How can the internal informational structure of agent-based models be leveraged to automatically discover interventions for AI-related harms?
- Basis in paper: The paper states we currently rely on "a priori ideas about possible interventions" and asks how to use the model's structure to "guide us in discovering possible interventions" (p. 5).
- Why unresolved: There is a lack of methods that utilize the rich data generated by simulations to reverse-engineer or suggest policy interventions.
- What evidence would resolve it: Algorithms capable of analyzing simulation dynamics to propose and validate novel intervention strategies for specific harms.

### Open Question 3
- Question: How does epistemic uncertainty in simulation data propagate through AI decision-makers to affect human agency?
- Basis in paper: The paper asks how epistemic uncertainty (e.g., data resolution limits) affects an AI agent and "consequently, what will be the impact on the agency of that human?" (p. 4).
- Why unresolved: Current uncertainty quantification in agent-based modeling focuses on aleatoric (stochastic) uncertainty rather than the epistemic uncertainty inherent in model stylization.
- What evidence would resolve it: Simulation results showing a quantified relationship between input epistemic uncertainty and the output degradation of agency metrics for AI-augmented humans.

## Limitations
- The paper presents a theoretical vision rather than concrete implementation, leaving the specific information-theoretic measure for agency quantification undefined
- Agent-based simulation approach faces substantial scaling challenges that could limit practical utility
- The synthesis of planning, relational, and enactive theories assumes mathematical compatibility that hasn't been demonstrated

## Confidence
**High Confidence**: The classification of generative AI harms using the BDI framework is well-grounded and provides a clear mapping from observed harms to theoretical constructs.

**Medium Confidence**: The proposed synthesis of planning, relational, and enactive theories is logically coherent but untested. While the theoretical arguments are sound, the practical integration remains speculative.

**Low Confidence**: The agent-based simulation approach faces the most uncertainty due to acknowledged challenges around realism, scaling, and epistemic uncertainty, with no concrete solutions or feasibility demonstrations provided.

## Next Checks
1. **Minimal Agency Quantification Test**: Implement a simple 2-agent BDI system where one agent attacks another's beliefs (A5). Measure agency changes using empowerment/information-theoretic measures. Verify the measure responds predictably to attack intensity before scaling up.

2. **Scenario Alignment Validation**: Build a small-scale simulation (N~100 agents) of an election with misinformation injection. Compare information-theoretic agency measures against qualitative assessments of harm. Check whether the quantitative measures align with intuitive evaluations of democratic process degradation.

3. **Scaling Boundary Identification**: Systematically increase agent population size in a controlled scenario while monitoring computational costs and measure stability. Identify the point where epistemic uncertainty begins to dominate agency signals, establishing practical limits for the approach.