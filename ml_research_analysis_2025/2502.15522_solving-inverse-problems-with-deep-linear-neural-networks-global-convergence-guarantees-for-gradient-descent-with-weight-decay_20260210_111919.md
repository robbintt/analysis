---
ver: rpa2
title: 'Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence
  Guarantees for Gradient Descent with Weight Decay'
arxiv_id: '2502.15522'
source_url: https://arxiv.org/abs/2502.15522
tags:
- lemma
- small
- bound
- neural
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies whether deep linear neural networks can automatically
  adapt to unknown low-dimensional structure in data when trained with gradient descent
  and weight decay regularization. The authors consider an underdetermined linear
  inverse problem where the target signal lies in an unknown low-dimensional subspace.
---

# Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay

## Quick Facts
- **arXiv ID**: 2502.15522
- **Source URL**: https://arxiv.org/abs/2502.15522
- **Reference count**: 40
- **Primary result**: Deep linear neural networks with weight decay and gradient descent can automatically adapt to unknown low-dimensional structure in data, achieving vanishing reconstruction error and robustness to noise as regularization approaches zero.

## Executive Summary
This paper establishes global convergence guarantees for deep linear neural networks trained with gradient descent and weight decay regularization to solve underdetermined linear inverse problems with latent low-dimensional structure. The authors prove that mildly overparameterized networks can automatically learn to reconstruct signals from their measurements while implicitly encoding the signal's low-dimensional subspace structure. The analysis reveals a two-phase convergence dynamic where reconstruction error initially decreases before stabilizing, while the component of the learned inverse mapping acting on the orthogonal complement of the signal subspace decays exponentially. The work provides theoretical justification for why regularization and overparameterization improve generalization in inverse problems.

## Method Summary
The method trains deep linear neural networks on underdetermined linear inverse problems where signals lie in unknown low-dimensional subspaces. The network architecture consists of L layers of linear transformations with width d_w, trained via gradient descent on a regularized loss combining reconstruction error and weight decay. The theoretical analysis proves convergence to a solution that accurately reconstructs training signals while suppressing components acting on the orthogonal complement of the measurement space. Key technical requirements include the Restricted Isometry Property on the measurement matrix, sufficient overparameterization relative to the condition number, and appropriate step-size and regularization parameter tuning.

## Key Results
- Deep linear networks exhibit two-phase convergence: initial reconstruction error reduction followed by exponential decay of off-subspace components
- Weight decay regularization ensures the learned inverse mapping approximates the minimum-norm oracle solution, providing robustness to test-time noise
- Overparameterization accelerates convergence by maintaining well-conditioned weight matrices throughout training
- Reconstruction error vanishes as regularization parameter approaches zero while off-subspace error remains bounded

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Convergence Dynamics
Gradient descent on deep linear networks exhibits distinct two-phase behavior: first minimizing reconstruction error, then suppressing components outside the signal subspace. Phase 1 shows linear decrease in reconstruction error at rate 1 - 1/32κ²(X) until hitting threshold τ proportional to regularization strength λ. Phase 2 stabilizes reconstruction error at O(λ∥X∥F) while off-subspace component decays exponentially toward zero. This behavior requires RIP condition on measurement matrix and sufficient width relative to condition number.

### Mechanism 2: Implicit Bias via Weight Decay
Weight decay acts as explicit regularizer forcing learned inverse mapping to approximate minimum-norm oracle solution by nullifying components acting on orthogonal complement of measurement space. The gradient descent update includes shrinkage term (1 - ηλ/dᵢ)Wℓ that damps all weights, but signal component is preserved via reconstruction loss gradient while off-subspace component lacks restoring force and decays to near-zero. This mechanism requires non-zero regularization parameter λ > 0.

### Mechanism 3: Overparameterization as Accelerator
Mild overparameterization (width d_w scaling exponentially with signal dimension d and condition number) accelerates convergence by ensuring network is well-conditioned at initialization. Sufficient width guarantees smallest singular value σₘᵢₙ(W_{L:i}) is bounded away from zero and largest σₘₐₓ is bounded, providing stable descent path where gradient direction aligns with error reduction. This relies on scaled Gaussian initialization and requires width to scale with powers of condition number.

## Foundational Learning

- **Concept**: **Pseudoinverse & Minimum-Norm Solutions**
  - **Why needed here**: The paper benchmarks against W_oracle = X Y†, the minimum-norm solution that perfectly reconstructs X from Y while minimizing sensitivity to noise. Understanding that inverse problems have infinitely many solutions and we want the "simplest" (smallest norm) is central.
  - **Quick check question**: Given matrix Y and target X, does solution W = X Y† minimize ||W||_F subject to WY = X?

- **Concept**: **Restricted Isometry Property (RIP)**
  - **Why needed here**: Assumption 2.1 states measurement operator A preserves length of vectors in signal subspace. This ensures problem is "identifiable" and prevents distortion of signal structure during measurement, necessary for theoretical bounds.
  - **Quick check question**: If A does not satisfy RIP on range(R), can we guarantee distinct signals x₁, x₂ won't map to same measurement y?

- **Concept**: **Induction on Random Matrices**
  - **Why needed here**: Proof strategy proceeds by induction over time steps t, requiring maintenance that random weight matrices remain "balanced" and well-conditioned throughout training. Grasping this stability is key to understanding why overparameterization prevents gradients from exploding or vanishing.
  - **Quick check question**: Why does proof bound distance ||Wᵢ(t) - (1-ηλ)ᵗWᵢ(0)||_F instead of just tracking loss?

## Architecture Onboarding

- **Component map**: y ∈ R^m -> W₁ ∈ R^{d_w×m} -> W₂,...,W_{L-1} ∈ R^{d_w×d_w} -> W_L ∈ R^{d×d_w} -> x̂ ∈ R^d
- **Critical path**: 
  1. **Initialization**: Use Kaiming normal Gaussian initialization. Avoid orthogonal initialization for exact theoretical replication.
  2. **Hyperparameter Setup**: Set η ≈ m/(L·σ²_{max}(X)) and λ small but non-zero.
  3. **Training**: Run gradient descent (not SGD) for T ≈ O(1/ηλ) iterations.
- **Design tradeoffs**:
  - **Depth (L)**: Increases convergence speed (preconditioning effect) but requires width d_w to scale for weight stability. Depth is beneficial but needs careful stability analysis.
  - **Width (d_w)**: Improves convergence probability and conditioning (Lemma 4.4), but increases memory cost. Width requirements become prohibitive for ill-conditioned data.
  - **Regularization (λ)**: Larger λ speeds off-subspace error decay (robustness) but increases reconstruction error bias. Small λ requires many iterations for convergence.
- **Failure signatures**:
  - **Divergence**: Step size η too large relative to σ²_{max}(X)
  - **Noise Amplification**: λ too small or T insufficient, model learns high spectral norm on orthogonal complement
  - **Poor Fit**: λ too large, weights shrink excessively, failing reconstruction
- **First 3 experiments**:
  1. **Verify Two-Phase Dynamics**: Plot reconstruction vs. off-subspace error over time (log scale). Verify reconstruction error plateau while off-subspace error continues falling.
  2. **Noise Robustness Sweep**: Train with λ=0 vs λ>0. Add Gaussian noise to test inputs. Confirm only regularized model maintains low error as noise increases.
  3. **Width Scaling**: Train models with increasing d_w on fixed data. Measure convergence success rate to verify probabilistic guarantees.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can these convergence guarantees extend to nonlinear networks or data models with nonlinear latent structure, such as union of subspaces? The current analysis is restricted to deep linear networks and single linear subspace data, while future work should study what structure one ReLU layer can adapt to.
- **Open Question 2**: Does increasing network depth provably accelerate convergence and improve decay of off-subspace error? While preliminary experiments suggest larger depth is beneficial, this phenomenon is not covered by main theoretical results and requires further investigation.
- **Open Question 3**: Do convergence and robustness guarantees hold when training measurements contain noise, not just test-time noise? Current results assume noise-free training setup while real-world problems typically involve noise during both phases.

## Limitations

- The RIP condition (δ = 1/10) is restrictive and not typically satisfied by real-world sensing matrices
- Width scaling d_w = Ω(L²κ²(X)d) becomes computationally prohibitive for ill-conditioned problems
- Analysis focuses exclusively on gradient descent rather than more commonly used stochastic gradient descent
- Proof technique requires precise initialization and step-size tuning that may be sensitive to implementation details

## Confidence

- **Mechanism 1 (Two-Phase Convergence)**: High confidence - Explicitly derived in Theorem 2.4 with clear mathematical bounds distinguishing reconstruction and off-subspace phases
- **Mechanism 2 (Implicit Bias via Weight Decay)**: High confidence - Proof directly shows how weight decay forces off-subspace components to zero while preserving signal reconstruction, with Corollary 3.1 providing explicit error bounds
- **Mechanism 3 (Overparameterization as Accelerator)**: Medium confidence - Width requirements are rigorously proven but "mild" characterization may not hold for very large condition numbers, and empirical validation is limited

## Next Checks

1. **Phase Boundary Validation**: Implement reconstruction vs. off-subspace error tracking to empirically verify existence and timing of two-phase convergence, particularly reconstruction error rebound at τ.

2. **Noise Robustness Experiment**: Systematically vary λ and measurement noise levels to empirically validate theoretical claim that λ > 0 provides superior robustness compared to λ = 0, measuring gap in test error as noise increases.

3. **Width Scaling Test**: For fixed problem dimensions and condition number, measure empirical success rate of convergence across different width values to verify whether theoretical width requirements are necessary or can be reduced in practice.