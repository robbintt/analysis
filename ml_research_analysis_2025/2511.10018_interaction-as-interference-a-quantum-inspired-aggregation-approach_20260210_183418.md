---
ver: rpa2
title: 'Interaction as Interference: A Quantum-Inspired Aggregation Approach'
arxiv_id: '2511.10018'
source_url: https://arxiv.org/abs/2511.10018
tags:
- interaction
- aggregation
- coherent
- interference
- incoherent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a quantum-inspired approach to modeling interaction
  in machine learning by treating interaction as a property of the aggregation rule
  rather than as an engineered feature. The key insight is that coherent aggregation
  of complex amplitudes (following the Born rule) naturally produces interference
  cross-terms that can represent interaction effects.
---

# Interaction as Interference: A Quantum-Inspired Aggregation Approach

## Quick Facts
- **arXiv ID**: 2511.10018
- **Source URL**: https://arxiv.org/abs/2511.10018
- **Reference count**: 30
- **Primary result**: Introduces quantum-inspired coherent aggregation that treats interaction as interference, with Interference Kernel Classifier (IKC) outperforming baselines on XOR and showing calibration gains on tabular data.

## Executive Summary
This paper presents a novel quantum-inspired approach to modeling interaction effects in machine learning by treating interaction as a property of the aggregation rule rather than as engineered features. The key insight is that coherent aggregation of complex amplitudes (following the Born rule) naturally produces interference cross-terms that can represent interaction effects. The approach introduces two diagnostics—Coherent Gain (log-likelihood improvement from coherent over incoherent aggregation) and Interference Information (KL divergence between the two)—to measure the contribution of interference. Experiments on XOR data show significant improvements over strong baselines, while on real tabular data (Adult and Bank Marketing), the method is competitive but typically trails capacity-rich tree-based models.

## Method Summary
The Interference Kernel Classifier (IKC) uses complex-valued affine maps ψ_y(x) = w_y^T x + b_y where w_y and b_y are complex parameters, implemented as two real-valued linear maps for Real/Imaginary parts. The Born rule is applied for aggregation: P(Y=1|x) = |ψ_1|² / (|ψ_0|² + |ψ_1|²). The model is trained with NLL loss and ℓ₂ decay using Adam optimizer. A key innovation is the incoherent toggle that removes interference cross-terms for ablation studies. The approach includes temperature scaling with safety switch for calibration, and probability clipping [10⁻⁷, 1-10⁻⁷]. Data preprocessing includes mode imputation for categorical variables, median imputation for numeric variables, and standardization.

## Key Results
- On XOR data, IKC achieved ΔNLL ≈ -0.159 ± 0.134 over logistic regression, with Coherent Gain G_coh ≈ 0.44 and Interference Information J_int ≈ 0.46
- Ablation study on Adult and Bank Marketing datasets showed toggling from incoherent to coherent aggregation consistently improved NLL, Brier score, and ECE with positive Coherent Gain on both datasets
- IKC was competitive on real tabular data but typically trailed the most capacity-rich baseline (XGBoost or CatBoost)
- The phase sweep validation confirmed the algebraic identity between interference cross-terms and interaction contrasts in 2×2 factorial designs

## Why This Works (Mechanism)

### Mechanism 1: Interference as Interaction Contrast
The model represents inputs as complex amplitudes. Coherent aggregation (Born rule) sums these amplitudes before squaring (|Σψ|²), producing cross-terms (2Re(u_A u_B*)). The paper proves that in a 2×2 factorial design, this cross-term algebraically equals the standard potential-outcome interaction contrast (p₁₁ - p₁₀ - p₀₁ + p₀₀).

### Mechanism 2: Phase as a Control Knob for Synergy/Antagonism
The identity Δ_INT = 2r_A r_B cos(φ_B - φ_A) links the interaction contrast to the phase difference. Near φ=0 gives positive cosine (synergy), while near φ=π gives negative cosine (antagonism).

### Mechanism 3: Calibration via Coherent Gain
Switching from incoherent (sum of squares) to coherent (square of sums) aggregation consistently improves calibration metrics (NLL, Brier, ECE) even when holding learned parameters fixed, suggesting coherent aggregation captures residual structure missed by incoherent methods.

## Foundational Learning

- **The Born Rule**: Why needed: Fundamental operation distinguishing coherent from standard probability aggregation. Quick check: If z₁ = 1 and z₂ = i, what's the difference between |z₁|² + |z₂|² and |z₁ + z₂|²?
- **Factorial Interaction Contrast (Δ_INT)**: Why needed: Paper claims its mechanism controls this statistical quantity. Quick check: In a drug trial where Drug A adds 10% and Drug B adds 10%, but together they add 25%, what's the interaction contrast?
- **Complex Number Arithmetic**: Why needed: Model parameters are complex weights w ∈ ℂ^d. Quick check: Does multiplying two complex numbers add or multiply their phases?

## Architecture Onboarding

- **Component map**: Input x ∈ ℝ^d → Complex encoder (w_y,b_y ∈ ℂ) → Born aggregation (|ψ|²) → Softmax normalization → Output
- **Critical path**: 1) Forward pass calculates complex logits (α+iβ) 2) Coherent Mode: Squaring magnitude (|z|²) induces interference terms 3) Incoherent Mode: Squaring components individually (|α|² + |β|²) removes interference 4) NLL Loss drives phase learning
- **Design tradeoffs**: Control vs. Capacity (linear-amplitude map offers precise control but limited capacity); Coherent vs. Incoherent (coherent better for interaction-heavy data, incoherent serves as main effects baseline)
- **Failure signatures**: XOR Failure (model misses pattern if phases aren't separated); Trailing Baselines (limited capacity to model non-linear main effects on dense tabular data)
- **First 3 experiments**: 1) Phase Sweep Validation (verify Δ_INT follows theoretical cosine curve) 2) XOR Stress Test (verify IKC outperforms linear models on pure-interaction data) 3) Ablation Toggle (train once, toggle coherent vs. incoherent at test time to verify positive Coherent Gain)

## Open Questions the Paper Calls Out

- **Multi-class extension**: Can coherent aggregation be extended to K>2 classes while preserving the interference–interaction identity and diagnostics? (Only binary classification was evaluated)
- **Capacity scaling**: Does coupling coherent aggregation with high-capacity feature extractors (tree ensembles or deep networks) close the performance gap to classical baselines on feature-rich tabular data?
- **Distributional robustness**: How does coherent aggregation behave under distributional shifts beyond symmetric label noise (e.g., covariate shift, feature noise, class imbalance)?

## Limitations

- The linear-amplitude parameterization has restricted capacity to model complex, non-linear main effects in high-dimensional tabular data
- IKC trails strong baselines (XGBoost, CatBoost) on Adult and Bank Marketing despite positive ablation gains
- The approach excels in interaction-heavy regimes but is insufficient alone for general tabular learning
- Only binary classification was evaluated; multi-class extension remains unexplored

## Confidence

- **High**: Mathematical identity between interference cross-terms and interaction contrasts in 2×2 factorial case (Section 3.2); consistent gains from coherent aggregation (Table 6)
- **Medium**: Phase-control mechanism for synergy/antagonism (algebraically proven but depends on optimizer)
- **Medium**: Competitive performance on real data (shows promise but not dominance in capacity-limited regimes)

## Next Checks

1. Test the 2×2 identity on 2×2×2 designs to see if the mechanism generalizes to higher-order interactions
2. Implement a non-linear version of the amplitude map (e.g., neural network layers) to test if interference mechanism can be preserved while increasing representational power
3. After training on Adult, visualize learned phases of top-weighted features to see if they align with known synergistic or antagonistic feature pairs from domain knowledge