---
ver: rpa2
title: Encoding Emotion Through Self-Supervised Eye Movement Reconstruction
arxiv_id: '2601.12534'
source_url: https://arxiv.org/abs/2601.12534
tags:
- gaze
- emotion
- data
- movement
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how eye movement from low-resolution, naturalistic
  video can predict emotional states. Using video interviews with Holocaust survivors,
  the authors develop a self-supervised learning model, GLASS, that predicts future
  eye movements autoregressively in a sequence-to-sequence architecture.
---

# Encoding Emotion Through Self-Supervised Eye Movement Reconstruction

## Quick Facts
- arXiv ID: 2601.12534
- Source URL: https://arxiv.org/abs/2601.12534
- Reference count: 0
- Primary result: GLASS model using self-supervised gaze reconstruction outperforms baselines on emotion prediction tasks from low-resolution video data

## Executive Summary
This paper investigates how eye movement from low-resolution, naturalistic video can predict emotional states. Using video interviews with Holocaust survivors, the authors develop a self-supervised learning model, GLASS, that predicts future eye movements autoregressively in a sequence-to-sequence architecture. The encoder is then fine-tuned for two emotion-related tasks: (1) predicting valence-arousal-dominance values from speech-derived labels and (2) classifying behaviors like laughing, crying, and sighing. GLASS outperforms baseline models and shows that longer input sequences and longer prediction horizons during pretraining improve emotion prediction. A key finding is a positive correlation between pretraining gaze prediction performance and emotion task performance. The results demonstrate that even low-quality eye data contains rich affective information, validating self-supervised eye movement reconstruction as an effective method for emotion encoding.

## Method Summary
The authors develop GLASS, a self-supervised learning model that autoregressively predicts future eye movements to encode representations transferable to emotion prediction. The method extracts 6D gaze vectors from low-resolution videos using OpenFace 2.0, then pretrains an encoder-decoder Transformer on 3.4M examples to predict future gaze patches. The encoder is subsequently fine-tuned on two downstream tasks: VAD regression (54k labeled windows) and behavior classification (7.4k labeled examples). The model uses scheduled sampling with decaying teacher forcing probability and RoPE positional embeddings. During fine-tuning, the decoder is replaced with emotion-specific heads (TCN, GRU, or Transformer) operating on chunked encoder outputs.

## Key Results
- GLASS outperforms baseline models on both VAD regression and behavior classification tasks
- Longer input sequences (5-10 seconds) outperform shorter 2-second windows for emotion prediction
- Longer prediction horizons during pretraining (10 seconds vs 2 seconds) improve emotion transfer performance
- Positive correlation observed between pretraining gaze prediction accuracy and downstream emotion task performance (r=0.28 to r=0.48)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised gaze reconstruction pretraining encodes representations transferable to emotion prediction.
- Mechanism: By training an encoder-decoder to autoregressively predict future eye movements, the encoder learns temporal gaze dynamics that implicitly capture affective patterns. The positive correlation between pretraining gaze prediction accuracy and downstream emotion task performance indicates the pretraining objective learns emotion-relevant features without explicit emotion labels.
- Core assumption: Eye movement patterns that are predictable over time carry systematic variance related to emotional state, rather than being purely noise or task-irrelevant motor behavior.
- Evidence anchors:
  - [abstract] "We observe a positive correlation between pretraining performance and emotion processing performance for both experiments."
  - [section 5.5] Figure 5 shows validation gaze correlation positively correlates with Exp. 1 MAE (r=0.28), Pearson's r (r=0.42), and Exp. 2 F1 (r=0.48).
  - [corpus] Limited direct corpus support; related work "MuMTAffect" uses eye gaze as one modality among physiological signals for emotion recognition but does not address self-supervised pretraining transfer.
- Break condition: If gaze patterns are primarily driven by exogenous visual stimuli rather than endogenous emotional states, the learned representations would not transfer to emotion tasks.

### Mechanism 2
- Claim: Longer prediction horizons during self-supervised pretraining yield better emotion transfer, even with identical architecture and data.
- Mechanism: Predicting gaze further into the future forces the encoder to learn higher-level temporal abstractions rather than short-term motor correlations. The paper shows GLASS trained to predict 10 seconds outperforms 2-second prediction on emotion tasks.
- Core assumption: Affective signal in eye movements operates at longer timescales than immediate motor dynamics, and extended forecasting objectives capture this structure.
- Evidence anchors:
  - [abstract] "longer input sequences and longer prediction horizons during pretraining improve emotion prediction"
  - [section 5.4] Table 2: GLASS predicting 10 seconds achieves Pearson's r=0.297 vs. 0.230 for 2-second prediction (5-second input condition).
  - [corpus] No direct corpus evidence on prediction horizon effects in self-supervised gaze learning.
- Break condition: If longer horizons simply increase training difficulty without capturing different representational structure, benefits would plateau or reverse.

### Mechanism 3
- Claim: Longer input sequences (5-10 seconds) improve emotion prediction from low-resolution, naturalistic eye data.
- Mechanism: Low-quality data (30 FPS, 320p, mean gaze error 9.1°) lacks the micro-movement resolution (fixations, saccades) available in lab settings. Longer temporal context compensates by aggregating more behavioral evidence before prediction.
- Core assumption: Affective signal accumulates over multiple seconds in naturalistic settings, whereas high-resolution lab data can extract signal from shorter windows.
- Evidence anchors:
  - [section 5.2] "5-second and 10-second inputs outperform 2-second input" across all models; prior work found 1-2 seconds optimal with high-resolution trackers.
  - [section 1] Data is "30 FPS at 320p" with footnote noting EyeLink 1000 samples at 2000 Hz with 0.01° accuracy vs. their 9.1° error.
  - [corpus] "Smile on the Face, Sadness in the Eyes" examines eye behaviors in multimodal emotion recognition but does not address temporal window effects.
- Break condition: If signal-to-noise ratio is too low regardless of window length, or if emotional states shift too rapidly for long windows to help.

## Foundational Learning

- Concept: **Self-supervised learning (SSL) with encoder-decoder architectures**
  - Why needed here: GLASS uses SSL to leverage 3.4M unlabeled eye gaze examples, then transfers encoder weights to limited labeled emotion data (54k VAD windows, 7.4k behavior instances).
  - Quick check question: Can you explain why the decoder is discarded during fine-tuning and only the encoder embeddings are used?

- Concept: **Autoregressive sequence prediction with scheduled sampling**
  - Why needed here: The model predicts future gaze patches using its own predictions as input during inference; scheduled sampling bridges teacher-forced training and autoregressive evaluation.
  - Quick check question: What happens to autoregressive prediction error if teacher forcing probability is always 100% during training?

- Concept: **Rotary Position Embeddings (RoPE) for temporal sequences**
  - Why needed here: Eye gaze is inherently temporal; RoPE encodes relative position information that allows the Transformer to reason about sequence order.
  - Quick check question: Why might learned absolute position embeddings fail for variable-length gaze sequences compared to RoPE?

## Architecture Onboarding

- Component map:
  Input layer: 6-channel gaze time series (XYZ × 2 eyes) → patch embedding (consecutive frames grouped) → linear projection to hidden dimension d → Encoder: L_e Transformer blocks with RoPE positional encoding, self-attention over input sequence → Decoder (pretraining): L_d Transformer blocks with cross-attention to encoder + autoregressive self-attention → linear projection back to gaze coordinates → Emotion head (fine-tuning): Replaces decoder; options include MLP, TCN, GRU, or Transformer over chunked encoder embeddings (0.5-4 second chunks) → Loss: Pretraining uses L = L_coordinate + 0.2 × L_velocity (both Huber loss)

- Critical path:
  1. Extract gaze via OpenFace 2.0 → 6D time series
  2. Pretrain encoder-decoder on all 3,997 videos (gaze prediction task)
  3. Freeze/discard decoder, attach emotion head
  4. Fine-tune on labeled emotion data (VAD regression or behavior classification)
  5. Evaluate on held-out test set with Pearson's r (VAD) or macro-F1 (behaviors)

- Design tradeoffs:
  - Model size (small/base/large): Paper tests three sizes; small model appears sufficient for reported results
  - Prediction horizon (2s/5s/10s): Longer horizons improve transfer but increase pretraining compute
  - Chunk size for emotion head (0.5s-4s): 1-2s chunks perform best; MLP (no temporal modeling) performs worst
  - Input window (2s/5s/10s): 5-10s outperforms 2s for emotion tasks

- Failure signatures:
  - High pretraining loss with flat emotion performance: Encoder not learning meaningful gaze dynamics (check data quality, patch size, learning rate)
  - Emotion head overfitting: Limited labeled data (54k VAD, 7.4k behaviors) → use dropout, early stopping
  - MLP emotion head underperforming: Temporal dynamics lost → switch to TCN/GRU/Transformer head
  - Short input windows underperforming: Low-resolution data requires longer context → increase to 5+ seconds

- First 3 experiments:
  1. Replicate pretraining correlation analysis: Train GLASS small with 5s input/5s output on subset of videos, plot validation gaze correlation vs. downstream VAD Pearson's r to verify positive correlation.
  2. Ablate prediction horizon: Compare GLASS small with 2s vs. 5s vs. 10s output prediction on emotion tasks holding all else constant (architecture, data, input length).
  3. Test emotion head architectures: Compare MLP vs. TCN vs. GRU vs. Transformer heads with 1s and 2s chunk sizes to identify optimal temporal aggregation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will the relationship between pretraining gaze prediction performance and emotion task performance generalize to other populations and emotional contexts beyond Holocaust survivor testimonies?
- Basis in paper: [inferred] The study uses a highly specific population (978 Auschwitz survivors) recounting traumatic experiences, with the authors noting "We hope to shed more light on the capabilities of low-quality eye data at scale in future work."
- Why unresolved: The emotional patterns in trauma narratives may differ substantially from typical emotional expression, and the demographic specificity limits generalizability claims.
- What evidence would resolve it: Evaluation of GLASS on diverse datasets with different populations (e.g., clinical interviews, casual conversations, induced emotions) and comparison of the pretraining-to-performance correlation across contexts.

### Open Question 2
- Question: How does model performance change when trained on human-annotated emotion labels rather than speech-derived VAD estimates?
- Basis in paper: [inferred] Experiment 1 uses ASR-generated VAD labels from speech models rather than human annotations, with the authors noting the outputs were "significantly below the 'neutral' emotion mean" and required upsampling outliers.
- Why unresolved: Speech-derived labels may not accurately reflect the true emotional state, and any noise or bias in these labels could limit or distort what the model learns about eye-emotion relationships.
- What evidence would resolve it: A comparative study using the same eye movement data with human expert annotations as ground truth, measuring whether Pearson's r and MAE improve or change meaningfully.

### Open Question 3
- Question: What is the optimal input window length for emotion prediction from low-resolution eye data, and why does it differ from the 1–2 second windows found optimal in high-quality eye-tracking studies?
- Basis in paper: [explicit] The authors state: "we hypothesize that when this information is not available due to low resolution or in more natural settings, longer gaze sequences provide a fuller picture on emotional state" and call for further exploration.
- Why unresolved: The paper finds 5–10 second windows outperform 2 seconds, but does not systematically test the full range or explain the mechanism.
- What evidence would resolve it: Ablation studies across a wider range of window lengths (e.g., 1, 3, 5, 10, 15, 20 seconds) on multiple datasets, with analysis of what temporal patterns the model learns at each scale.

## Limitations

- Data quality constraints: Uses low-resolution video (30 FPS, 320p) with 9.1° gaze error, potentially missing fine-grained eye movement patterns relevant to emotion
- Population specificity: Results based on Holocaust survivor interviews may not generalize to other emotional contexts or populations
- Correlation interpretation: Positive correlation between gaze prediction and emotion performance could reflect shared low-level dynamics rather than genuine affective encoding

## Confidence

**High Confidence Claims**:
- GLASS outperforms baseline models on both emotion prediction tasks
- Longer input sequences (5-10 seconds) improve emotion prediction performance
- The encoder-decoder architecture can successfully reconstruct future eye movements

**Medium Confidence Claims**:
- Longer prediction horizons during pretraining improve emotion transfer
- The positive correlation between gaze prediction accuracy and emotion performance indicates affective feature learning
- Scheduled sampling with decaying teacher forcing probability is necessary for stable autoregressive training

**Low Confidence Claims**:
- The specific 9.1° gaze error is sufficiently accurate for affective analysis
- The VAD labels from speech-derived sources are temporally aligned with gaze windows without error
- The TCN emotion head architecture is optimal for this task

## Next Checks

1. **Data Quality Sensitivity Test**: Re-run the emotion prediction experiments using higher-quality eye tracking data (e.g., from lab settings with 1000+ Hz sampling and sub-degree accuracy) to determine whether the observed affective signals persist or strengthen with improved measurement precision.

2. **Population Generalization Test**: Apply the pretrained GLASS encoder to eye tracking data from different emotional contexts (e.g., affective computing datasets with controlled stimuli, or different interview populations) to assess whether the learned representations transfer beyond Holocaust survivor interviews.

3. **Mechanistic Ablation Study**: Systematically ablate different components of the eye movement signal (e.g., remove velocity information, filter out high-frequency saccades, or segment fixations) to identify which aspects of gaze dynamics are most critical for affective feature learning.