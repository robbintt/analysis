---
ver: rpa2
title: 'ARC: Leveraging Compositional Representations for Cross-Problem Learning on
  VRPs'
arxiv_id: '2512.18633'
source_url: https://arxiv.org/abs/2512.18633
tags:
- attribute
- learning
- rf-te
- cada
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARC, a compositional learning framework for
  cross-problem generalization in Vehicle Routing Problems. ARC disentangles attribute
  representations into Intrinsic Attribute Embeddings (IAE) for invariant semantics
  and Contextual Interaction Embeddings (CIE) for combination-specific effects, enabling
  efficient knowledge sharing across VRP variants.
---

# ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs

## Quick Facts
- **arXiv ID:** 2512.18633
- **Source URL:** https://arxiv.org/abs/2512.18633
- **Reference count:** 40
- **Primary result:** State-of-the-art cross-problem generalization for VRP variants using compositional attribute representations

## Executive Summary
This paper introduces ARC, a compositional learning framework for cross-problem generalization in Vehicle Routing Problems. ARC disentangles attribute representations into Intrinsic Attribute Embeddings (IAE) for invariant semantics and Contextual Interaction Embeddings (CIE) for combination-specific effects, enabling efficient knowledge sharing across VRP variants. The method enforces analogical consistency through contrastive learning, ensuring attributes maintain their intrinsic semantics regardless of combinations. Experiments demonstrate ARC's superiority across four scenarios: in-distribution performance (achieving 1.828% and 2.861% gaps for instance sizes 50 and 100), zero-shot generalization to unseen combinations (4.078% and 6.422% average gaps), few-shot adaptation to new attributes (consistently outperforming baselines), and real-world benchmarks on CVRPLib datasets.

## Method Summary
ARC decomposes VRP attribute embeddings into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. The encoder produces f_θ(x) = h_θ(x) + m_θ(x), where h_θ extracts IAE through a Node Embedder, and m_θ computes CIE via an ARC module that conditions on IAE and global features. During training, a compositional loss enforces analogical consistency by ensuring attribute transformation vectors remain invariant across contexts using InfoNCE. The decoder is an autoregressive policy with masked attention. The framework is trained with REINFORCE + POMO, jointly optimizing reward and compositional consistency.

## Key Results
- Achieves 1.828% and 2.861% gaps for instance sizes 50 and 100 in in-distribution performance
- Demonstrates 4.078% and 6.422% average gaps in zero-shot generalization to unseen attribute combinations
- Consistently outperforms baselines in few-shot adaptation to new attributes
- Achieves state-of-the-art results on CVRPLib real-world benchmarks while maintaining competitive inference times

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Attribute Representations
Decomposing attribute embeddings into invariant semantics (IAE) and contextual interactions (CIE) enables efficient knowledge sharing across VRP variants by preventing entanglement of shared attribute meanings with combination-specific effects. The encoder produces two additive components: `f_θ(x) = h_θ(x) + m_θ(x)`, where IAE captures what each attribute means universally (e.g., "length limit" constraint), while CIE captures how attributes interact in specific combinations (e.g., length constraint's attenuated impact when paired with open routing).

### Mechanism 2: Analogical Consistency via Contrastive Attribute Learning
Enforcing that attribute transformation vectors maintain consistent relationships across contexts enables the model to learn compositional representations that transfer to unseen attribute combinations. For any attribute A, the model extracts `α_A = h_θ(x+A) - h_θ(x)` and uses InfoNCE loss to ensure same-attribute vectors from different instances cluster together while different-attribute vectors separate.

### Mechanism 3: Contextual Interaction Modeling via Attention
Conditioning CIE on IAE through attention over contextual features captures combination-specific effects that pure invariants cannot express, enabling accurate modeling of complex attribute interactions. The CIE module takes IAE embeddings and global/contextual features as input, using cross-attention (GlobalModule) to compute interaction-aware adjustments.

## Foundational Learning

- **Concept: Representation Disentanglement**
  - **Why needed here:** ARC's core contribution is separating "what an attribute means" from "how it behaves in combination." Without understanding disentanglement principles, the motivation for IAE/CIE split is unclear.
  - **Quick check question:** If you have embeddings for "red circle" and "blue square," what would disentangled vs. entangled representations look like when you need to generate "red square"?

- **Concept: Contrastive Learning Objectives (InfoNCE)**
  - **Why needed here:** The compositional loss uses InfoNCE to enforce that attribute vectors from the same attribute type cluster together. Understanding why contrastive objectives work is essential for debugging the attribute learning process.
  - **Quick check question:** In InfoNCE, what happens to the gradient signal if all negative samples are too easy (very dissimilar) or too hard (nearly identical to the positive)?

- **Concept: Compositional Generalization**
  - **Why needed here:** ARC targets zero-shot generalization to unseen attribute combinations. This requires understanding why standard neural networks struggle with systematic composition and how architectural inductive biases help.
  - **Quick check question:** If a model trained on "red circle," "blue circle," "red square" fails on "blue square," is this a failure of compositional generalization? What would need to change?

## Architecture Onboarding

- **Component map:** Node features → Node Embedder → IAE → (split) → CIE via ARC Module → f_θ = IAE + CIE → Decoder → Action probabilities
- **Critical path:** Node features → Node Embedder → IAE → (split) → CIE via ARC Module → f_θ = IAE + CIE → Decoder → Action probabilities. During training: additionally extract attribute vectors → InfoNCE loss → joint optimization with REINFORCE reward.
- **Design tradeoffs:**
  - ARC module layers (N_A): Paper finds N_A=3 optimal for zero-shot; higher values (4+) degrade OOD performance while maintaining ID performance
  - Temperature β: Higher β (0.12-0.14) reduces variance but risks treating all attributes similarly; lower β increases sensitivity but may cause training instability
  - Loss weight λ: Controls composition vs. reward optimization tradeoff. Paper uses λ=0.8; lower values reduce regularization effect on embeddings
- **Failure signatures:**
  - IAE not disentangling: t-SNE shows IAE clusters by VRP variant rather than attribute type
  - CIE insufficient: Strong ID performance but poor zero-shot on complex combinations suggests interaction modeling gap
  - Compositional loss not converging: Check if positive/negative sampling produces meaningful gradients
  - Attribute vector collapse: All attribute vectors converge to similar values
- **First 3 experiments:**
  1. Ablate compositional loss (λ=0): Compare against RouteFinder baseline to isolate IAE/CIE architectural contribution from contrastive learning effect
  2. Visualize attribute vectors: Extract α vectors for each attribute across multiple instances; compute intra-attribute variance and inter-attribute separation
  3. Zero-shot on held-out combinations: Train on 7-variant subset, test on all 16; monitor per-variant gaps to identify which attribute combinations benefit most

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the compositional learning approach of ARC be adapted to maintain state-of-the-art performance on massive-scale instances (e.g., N > 500) where it currently lags behind baseline heavy-decoder methods?
- **Open Question 2:** Is the additive decomposition of representations (f_θ(x) = h_θ(x) + m_θ(x)) sufficient for capturing complex non-linear interactions between intrinsic attribute semantics and contextual effects?
- **Open Question 3:** How does the capacity of the ARC module (number of layers) affect the trade-off between in-distribution performance and zero-shot generalization?

## Limitations
- Disentanglement claims lack empirical validation showing IAE actually learns attribute semantics independent of context
- Contrastive learning assumes vector arithmetic holds for attribute semantics without ablation studies examining sensitivity to temperature
- Real-world benchmark results on CVRPLib datasets are limited to only 5 instances per dataset
- Architectural complexity of ARC modules is underspecified in terms of layer dimensions and attention mechanisms

## Confidence
- **High confidence:** In-distribution performance results (1.828% and 2.861% gaps) are well-validated with extensive evaluation on 1,000 test instances per variant
- **Medium confidence:** Zero-shot generalization claims (4.078% and 6.422% average gaps) rely on the assumption that analogical consistency enables compositionality, but lack direct evidence of semantic disentanglement
- **Low confidence:** Real-world benchmark results on CVRPLib datasets are promising but limited to only 5 instances per dataset

## Next Checks
1. **Disentanglement validation:** Extract IAE embeddings for multiple instances with same attributes but different contexts; perform t-SNE visualization to verify clustering by attribute type rather than VRP variant
2. **Analogy arithmetic verification:** For each attribute A, compute α_A = h_θ(x+A) - h_θ(x) across different base instances x; measure cosine similarity of same-attribute vectors and variance across contexts
3. **Ablation on contrastive loss:** Train ARC with λ=0 (no compositional loss) to quantify the specific contribution of analogical consistency vs. architectural improvements alone