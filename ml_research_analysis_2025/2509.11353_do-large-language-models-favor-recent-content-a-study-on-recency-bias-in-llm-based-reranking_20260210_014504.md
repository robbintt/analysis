---
ver: rpa2
title: Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based
  Reranking
arxiv_id: '2509.11353'
source_url: https://arxiv.org/abs/2509.11353
tags:
- language
- llms
- relevance
- bias
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) exhibit
  recency bias when used as rerankers in information retrieval. The authors inject
  artificial publication dates into search result passages and observe how seven LLMs
  reorder them.
---

# Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based Reranking

## Quick Facts
- arXiv ID: 2509.11353
- Source URL: https://arxiv.org/abs/2509.11353
- Reference count: 40
- Seven tested LLMs systematically promote newer-looking content, shifting mean publication year forward by up to 4.78 years in top-10 results

## Executive Summary
This study demonstrates that large language models exhibit systematic recency bias when used as rerankers in information retrieval. By injecting artificial publication dates into search result passages and observing how seven LLMs reorder them, the authors show that all tested models promote newer-looking content, moving individual items by as many as 95 ranks. The effect persists even in the largest models, though they attenuate it somewhat. Pairwise preference experiments confirm that recency cues can reverse up to 25% of baseline judgments between equally relevant passages. These findings provide quantitative evidence of pervasive recency bias in LLM-based reranking and underscore the need for bias-mitigation strategies.

## Method Summary
The authors investigate recency bias by retrieving top-100 passages using BM25 from TREC Deep Learning Track collections (DL21/DL22), then injecting artificial publication dates (ranging from 1926 to 2025) into passage text before reranking with seven different LLMs. They use a listwise sliding window approach (size 10) and compare rankings before and after date injection using metrics including mAARS, mYS(K), Kendall's tau, and reversal rate. Pairwise experiments further examine how models handle date conflicts between equally relevant passages.

## Key Results
- All tested models show statistically significant promotion of newer-dated content, with top-10 mean year shifts up to 4.78 years
- Kendall's tau drops from baseline (0.9-1.0) to 0.6-0.8 after date injection, indicating substantial ranking instability
- Pairwise experiments show 20-25% reversal rates when comparing equally relevant passages with different dates
- Larger models (GPT-4o, LLaMA3-70B, Qwen2.5-72B) show reduced but not eliminated bias compared to smaller variants

## Why This Works (Mechanism)

### Mechanism 1: Temporal Cue as Implicit Relevance Signal
- Claim: LLMs interpret publication dates as proxy relevance indicators, treating recency as an implicit feature of quality or importance.
- Mechanism: Date prefixes are incorporated into the attention context; models conditioned on web-scale corpora associate newer timestamps with higher-engagement, higher-priority content, causing systematic rank promotion.
- Core assumption: Pre-training data contains temporal signals (news, articles, updates) where newer content correlates with user engagement metrics.
- Evidence anchors:
  - [abstract] "fresh passages are consistently promoted, shifting the Top-10's mean publication year forward by up to 4.78 years"
  - [Section 4.2, Table 2] All models show statistically significant positive year shifts in top-K results (p < 0.05)
  - [corpus] Related work "LayerNorm Induces Recency Bias in Transformer Decoders" suggests architectural components may reinforce recency preferences
- Break condition: If relevance-irrelevant temporal correlations are removed from pre-training, or if explicit anti-recency instructions are added to the prompt, the effect should attenuate (not tested in this paper).

### Mechanism 2: Scale-Dependent Bias Attenuation
- Claim: Larger models better distinguish surface-level temporal cues from semantic relevance, reducing but not eliminating recency-driven ranking distortion.
- Mechanism: Higher-capacity models develop more robust representations that can suppress spurious correlations when content signals are strong; however, the bias pathway remains active, particularly when relevance signals are ambiguous.
- Core assumption: Model capacity correlates with ability to decorrelate spurious features from target tasks.
- Evidence anchors:
  - [abstract] "Although larger models attenuate the effect, none eliminate it"
  - [Section 4.4, Figure 5] Kendall's tau distributions show larger models (GPT-4o, LLaMA3-70B, Qwen2.5-72B) maintain higher ranking stability under date injection
  - [corpus] "Positional Biases Shift as Inputs Approach Context Window Limits" shows bias patterns vary with model scale and context length
- Break condition: If the task framing explicitly requires temporal relevance judgments (e.g., "find the most recent news"), larger models may exhibit amplified rather than attenuated recency bias.

### Mechanism 3: Seesaw Ranking Dynamics via Sliding Window Processing
- Claim: Listwise reranking with sliding windows produces asymmetric rank movement—fresh-dated passages migrate upward, old-dated passages cascade downward—with a pivot near the SERP midpoint.
- Mechanism: Within each window, the model promotes newer-dated items; repeated window passes compound this effect, creating a "seesaw" where top deciles grow younger and bottom deciles grow older.
- Core assumption: Window size and overlap strategy affect the magnitude of temporal distortion.
- Evidence anchors:
  - [Section 4.3, Table 3] Top deciles show significant positive year shifts (+1.3 to +4.78 years); bottom deciles show negative shifts (-0.5 to -1.97 years)
  - [Section 5.3] "The mid-SERP deciles (41–60) act as a pivot, exhibiting the smallest absolute changes"
  - [corpus] Limited direct evidence for sliding-window-specific temporal bias; related work on positional biases remains indirect
- Break condition: If window size is reduced to 2-3 passages, or if ranking is performed globally without windowing, the seesaw pattern should weaken (not tested).

## Foundational Learning

- Concept: **Listwise vs. Pairwise Reranking**
  - Why needed here: The paper uses both paradigms; listwise processes k documents simultaneously, pairwise compares two at a time. Understanding this distinction is essential for interpreting mAARS (list-level volatility) vs. reversal rate (pair-level preference flip).
  - Quick check question: If you see a metric measuring "preference flip between two equally relevant passages," which paradigm does this belong to?

- Concept: **TREC Deep Learning Track Collections (DL21/DL22)**
  - Why needed here: These are standard IR benchmarks with human relevance judgments. The paper's methodology depends on having ground-truth relevance to isolate temporal bias from genuine relevance differences.
  - Quick check question: Why does the pairwise experiment restrict comparisons to passages within the same relevance level (0, 1, or 2)?

- Concept: **Kendall's Tau Correlation**
  - Why needed here: Used to measure ranking stability before/after date injection. A value of 1.0 means identical rankings; lower values indicate greater perturbation. Critical for interpreting Figure 5.
  - Quick check question: If Kendall's tau drops from 0.9 to 0.6 after date injection, what does this imply about the model's sensitivity to temporal cues?

## Architecture Onboarding

- Component map: BM25 retrieval -> Date injection -> LLM reranking with sliding window -> Ranking comparison
- Critical path:
  1. Retrieve top-100 via BM25 (baseline ranking)
  2. Rerank without dates → capture baseline SERP
  3. Inject dates (Rank 100 = 2025, Rank 1 = 1926, 1-year steps)
  4. Rerank with dates → capture perturbed SERP
  5. Compute delta metrics between baseline and perturbed SERPs
- Design tradeoffs:
  - Window size (10) balances context coverage vs. token budget; smaller windows reduce compounding bias but may miss global relevance signals
  - Date range (1926–2025) creates maximal contrast but may trigger out-of-distribution behavior for some models
  - Pairwise testing restricted to open-source models due to API cost constraints
- Failure signatures:
  - Extreme rank shifts (ALRS > 80) indicate model over-reliance on temporal cues
  - High reversal rates (>20%) on equally relevant pairs suggest semantic discrimination failure
  - Kendall's tau < 0.7 indicates unstable ranking behavior under perturbation
- First 3 experiments:
  1. **Baseline replication**: Run listwise reranking on DL21/DL22 with GPT-4o without date injection; verify mAARS ≈ 0 and mYS(10) ≈ 0
  2. **Controlled date injection**: Apply the 1926–2025 date scheme; confirm positive mYS(10) shift consistent with paper values (GPT-4o: ~1.3–1.4 years)
  3. **Ablation on window size**: Repeat experiment with window size = 5 and window size = 20; measure whether mAARS and seesaw pattern attenuate or amplify

## Open Questions the Paper Calls Out
None

## Limitations
- Artificial date injection may overstate recency bias compared to real-world publication timestamps
- 100-year date range represents extreme temporal spread not typical of most search scenarios
- Reliance on BM25 baseline introduces term-matching bias that may interact with LLM temporal preferences
- Pairwise experiments limited to open-source models due to API cost constraints
- Does not investigate whether recency bias varies by query type (time-sensitive vs. evergreen content)

## Confidence
- **High Confidence**: Existence of recency bias across all tested models (mAARS > 0, mYS(10) > 0, Kendall's tau < 1.0)
- **Medium Confidence**: Larger models attenuate but don't eliminate recency bias
- **Medium Confidence**: Seesaw ranking dynamics in listwise reranking

## Next Checks
1. **Real-World Date Validation**: Repeat the experiment using actual publication dates from web content to determine if artificial date injection overstates the recency bias effect.
2. **Query-Type Interaction Study**: Test whether recency bias varies systematically across different query types (time-sensitive vs. evergreen) to identify when bias mitigation is most critical.
3. **Hybrid Retrieval Analysis**: Investigate whether combining LLM reranking with traditional term-matching approaches can reduce temporal distortion while maintaining relevance performance.