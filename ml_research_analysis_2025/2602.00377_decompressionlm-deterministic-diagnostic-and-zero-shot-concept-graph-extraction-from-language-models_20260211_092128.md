---
ver: rpa2
title: 'DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction
  from Language Models'
arxiv_id: '2602.00377'
source_url: https://arxiv.org/abs/2602.00377
tags:
- freq
- concept
- concepts
- quantization
- gptq-int4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DecompressionLM introduces a stateless framework for zero-shot
  concept graph extraction from language models. By using Van der Corput low-discrepancy
  sequences with arithmetic decoding, the method enables deterministic, embarrassingly
  parallel generation without cross-sequence state.
---

# DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models

## Quick Facts
- **arXiv ID**: 2602.00377
- **Source URL**: https://arxiv.org/abs/2602.00377
- **Reference count**: 21
- **Primary result**: AWQ-4bit expands concept coverage by 30-170% vs BF16 baseline; GPTQ-Int4 induces 71-86% coverage collapse

## Executive Summary
DecompressionLM introduces a novel framework for extracting deterministic concept graphs from language models without requiring prior knowledge bases or training data. The method leverages Van der Corput low-discrepancy sequences combined with arithmetic decoding to enable embarrassingly parallel, stateless generation across model variants. By evaluating two quantization methods (AWQ-4bit and GPTQ-Int4) across two model families, the study reveals dramatic differences in knowledge retention and hallucination patterns that traditional metrics miss. Corpus-based validation using MMLU-Pro Law demonstrates a 19.6-point hallucination gap between top- and bottom-ranked models, establishing concept coverage as a critical evaluation dimension for compressed language models.

## Method Summary
DecompressionLM employs a deterministic generation approach using Van der Corput sequences to probe language models for concept extraction. The framework operates statelessly, allowing parallel generation without cross-sequence dependencies. For quantization evaluation, the method systematically compares AWQ-4bit against GPTQ-Int4 and BF16 baselines across multiple model families. Concept extraction proceeds through arithmetic decoding of generated sequences, with corpus-based validation using MMLU-Pro Law to assess hallucination rates and knowledge retention. The approach enables zero-shot extraction without requiring training data or fine-tuning, making it applicable across diverse model architectures and compression variants.

## Key Results
- AWQ-4bit quantization achieves 30-170% higher concept coverage compared to BF16 baseline across tested models
- GPTQ-Int4 quantization causes 71-86% collapse in concept coverage relative to uncompressed baselines
- MMLU-Pro Law validation reveals 19.6-point hallucination gap between highest and lowest performing models
- Concept coverage serves as complementary evaluation metric for assessing knowledge breadth in compressed models

## Why This Works (Mechanism)
The deterministic generation mechanism eliminates stochastic variability while maintaining systematic coverage of the model's knowledge space. Van der Corput sequences provide low-discrepancy sampling that ensures uniform exploration of the concept space without clustering. Arithmetic decoding translates these sequences into interpretable concept representations that can be validated against external corpora. The stateless design enables embarrassingly parallel execution, allowing comprehensive coverage analysis across multiple model variants simultaneously. This combination enables detection of subtle knowledge retention patterns that traditional evaluation methods miss, particularly in quantized models where precision loss affects different knowledge domains unevenly.

## Foundational Learning
- **Low-discrepancy sequences**: Uniform sampling methods that avoid clustering in high-dimensional spaces; needed to ensure comprehensive concept coverage without bias; quick check: verify sequence uniformity using discrepancy metrics
- **Arithmetic decoding**: Information-theoretic method for sequence reconstruction; needed to convert deterministic sequences into meaningful concept representations; quick check: validate decoding fidelity against known sequences
- **Quantization effects**: Precision reduction impacts in neural networks; needed to understand how different compression methods affect knowledge retention; quick check: compare activation distributions across quantization levels
- **Concept coverage metrics**: Evaluation framework for knowledge extraction completeness; needed to quantify differences between compression methods; quick check: validate against human-annotated knowledge bases
- **Hallucination detection**: Methods for distinguishing factual from fabricated content; needed to assess model reliability in extracted concepts; quick check: cross-validate with multiple external knowledge sources

## Architecture Onboarding

**Component Map**: Van der Corput sequence generator -> Arithmetic decoder -> Language model -> Concept extractor -> Corpus validator

**Critical Path**: Sequence generation → Decoding → Model inference → Concept extraction → Validation

**Design Tradeoffs**: Deterministic vs. stochastic generation (reproducibility vs. natural variation), parallel vs. sequential execution (speed vs. resource efficiency), concept coverage vs. semantic precision (breadth vs. depth)

**Failure Signatures**: Coverage gaps indicating knowledge loss, sequence divergence suggesting model instability, validation mismatches revealing hallucination patterns, quantization artifacts affecting concept extraction quality

**3 First Experiments**:
1. Baseline comparison: Generate concept graphs from BF16 model using both deterministic and stochastic approaches
2. Quantization ablation: Compare AWQ-4bit vs GPTQ-Int4 across identical sequence parameters
3. Coverage analysis: Map concept extraction completeness across different sequence lengths and model depths

## Open Questions the Paper Calls Out
The paper identifies several critical open questions regarding the generalizability of its findings across different quantization methods and model architectures. Key uncertainties include whether observed coverage patterns hold for other compression techniques like QLoRA or 8-bit quantization, and how these effects translate to different model families beyond the tested GPT and LLaMA variants. The framework also raises questions about the semantic fidelity of extracted concepts and whether deterministic generation might systematically underrepresent certain knowledge patterns that stochastic sampling could capture.

## Limitations
- Evaluation limited to two quantization variants (AWQ-4bit and GPTQ-Int4) across two model families, limiting generalizability
- Deterministic generation may systematically miss knowledge patterns that stochastic sampling could capture
- Corpus-based validation using MMLU-Pro Law may introduce domain-specific biases not representative of general knowledge
- Concept coverage metrics provide proxy measures that may not fully capture semantic fidelity or practical utility

## Confidence
- **High**: Deterministic generation mechanism, quantization coverage effects, concept coverage metric design
- **Medium**: Hallucination gap measurements, cross-model comparison validity, domain-specific findings
- **Low**: Generalizability across quantization methods, semantic fidelity of extracted concepts, practical utility assessment

## Next Checks
1. Replicate concept coverage analysis across additional quantization variants (e.g., QLoRA, 8-bit) and model architectures (e.g., LLaMA, Mistral) to assess generalizability
2. Conduct human evaluation studies to validate semantic fidelity of extracted concept graphs against automated coverage metrics
3. Perform ablation studies varying sequence length and generation parameters to characterize their impact on concept extraction quality and hallucination rates