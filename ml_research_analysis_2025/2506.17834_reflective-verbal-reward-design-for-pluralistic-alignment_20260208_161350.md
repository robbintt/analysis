---
ver: rpa2
title: Reflective Verbal Reward Design for Pluralistic Alignment
arxiv_id: '2506.17834'
source_url: https://arxiv.org/abs/2506.17834
tags:
- reward
- participants
- learning
- system
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interactive-Reflective Dialogue Alignment
  (IRDA), a novel approach for learning individualized reward models that preserve
  value diversity in AI alignment. The method uses language models to guide users
  through reflective dialogues where they critique agent behavior and construct preferences,
  creating personalized reward functions.
---

# Reflective Verbal Reward Design for Pluralistic Alignment

## Quick Facts
- **arXiv ID**: 2506.17834
- **Source URL**: https://arxiv.org/abs/2506.17834
- **Reference count**: 10
- **One-line primary result**: IRDA achieves 9-12% improvement in accuracy over non-reflective verbal reward models while being more sample efficient than traditional supervised learning methods.

## Executive Summary
This paper introduces Interactive-Reflective Dialogue Alignment (IRDA), a novel approach for learning individualized reward models that preserve value diversity in AI alignment. The method uses language models to guide users through reflective dialogues where they critique agent behavior and construct preferences, creating personalized reward functions. In user studies with 30 participants across two domains (respectful behavior and autonomous vehicle ethics), IRDA demonstrated substantial improvements over traditional approaches while highlighting the importance of reflection-based elicitation for capturing genuine value pluralism.

## Method Summary
IRDA employs a dual-loop system: preference construction where users critique trajectory exemplars selected via k-means clustering, and uncertainty reduction where the system queries users on maximally uncertain trajectories until a threshold is met. An LLM serves as the reward function, using complete conversation history as context to classify trajectories as aligned or misaligned based on user feedback. The method contrasts with traditional RLHF by focusing on individual preference construction rather than collective revelation, using active learning to improve sample efficiency.

## Key Results
- IRDA achieved 9-12% improvement in accuracy over non-reflective verbal reward models in user studies
- Individual models essential when preferences are heterogeneous (κ=0.336) but collective models competitive when more homogeneous (κ=0.460)
- IRDA more sample efficient than traditional supervised learning methods (68% accuracy with ~4 samples vs 59% with ~30 samples)
- Substantial value diversity exists across individuals with low inter-annotator agreement in both domains

## Why This Works (Mechanism)

### Mechanism 1: Reflective Dialogue for Preference Construction
Structured reflection helps users transform latent values into concrete, articulable preferences that an LLM can generalize from. The system queries an LLM with user feedback to generate feature hypotheses (H) and alternative features (A) the user could consider. Users respond to these, potentially updating their mental model. This transforms the process from passive labeling to active preference construction.

### Mechanism 2: Active Learning via Uncertainty Sampling
Selecting maximally uncertain trajectories for user feedback improves sample efficiency by targeting the decision boundary. After initial preference construction, the system computes uncertainty as U(τ) = 1 - |p_θ(1|τ) - p_θ(0|τ)| using LLM token probabilities. It queries users on the highest-uncertainty trajectory, appending feedback to the conversation history until uncertainty falls below threshold ε.

### Mechanism 3: Verbal Reward Modeling via In-Context Learning
An LLM with complete conversation history can serve as a personalized reward function without gradient updates by leveraging in-context learning. The LLM is prompted with environment description, full conversation history, and ASCII-encoded trajectory. It outputs token probabilities for "aligned" vs "misaligned" labels. The final reward is R(τ) = I[p_θ(1|τ) > p_θ(0|τ)].

## Foundational Learning

### Concept 1: Preference Construction vs. Preference Revelation
Why needed here: The paper's theoretical foundation is that preferences aren't pre-existing artifacts waiting to be extracted—they're actively constructed through reflection in novel contexts. Standard RLHF assumes revelation; IRDA assumes construction.
Quick check question: In your alignment task, do users already have stable, well-defined preferences they can directly report, or will they need to work through examples to discover what they actually want?

### Concept 2: Inter-Annotator Agreement (Fleiss' Kappa)
Why needed here: The paper uses κ = 0.336 (Study 1) and κ = 0.460 (Study 2) to demonstrate that value diversity is genuine, not noise. This justifies individualized models over collective approaches in heterogeneous contexts.
Quick check question: Do you understand how to compute and interpret Fleiss' kappa, and what "fair" vs. "moderate" agreement implies for choosing individual vs. collective alignment strategies?

### Concept 3: Active Learning with Uncertainty Sampling
Why needed here: IRDA's uncertainty reduction loop depends on extracting calibrated uncertainty from LLM token probabilities and using it to select informative queries.
Quick check question: Can you extract token log-probabilities from your LLM API and convert them to calibrated uncertainty estimates? Do you know when this calibration fails?

## Architecture Onboarding

### Component map:
Diversity Pool (T_D) -> k-means clustering on features φ(τ) -> centroid selection -> User critiques -> Aggregate into D_fb -> LLM generates hypotheses H and alternatives A -> User reflects -> Confirm stability -> Uncertainty Pool (T_U) -> Compute U(τ) for all T_U -> Select τ* = argmax U(τ) -> Query user for feedback e* -> Append (α(τ*), e*) to D_fb -> Iterate until U(τ*) < ε -> Final model: R(τ) = I[p_θ(1|τ) > p_θ(0|τ)]

### Critical path:
1. Extract trajectory features → k-means into k clusters → select centroid representatives
2. User critiques each centroid → aggregate into D_fb
3. LLM generates feature hypotheses H and alternatives A from D_fb
4. User responds to (H, A); if mental model changes, return to step 2
5. For each τ in T_U: compute p_θ(1|τ), p_θ(0|τ) and U(τ)
6. Select τ* = argmax U(τ); query user for feedback e*
7. Append (α(τ*), e*) to D_fb; repeat step 5 until U(τ*) < ε
8. Final model: R(τ) = I[p_θ(1|τ) > p_θ(0|τ)]

### Design tradeoffs:
- **Individual vs. Collective Models**: Study 1 (heterogeneous, κ=0.336) showed individual models essential; Study 2 (more homogeneous, κ=0.460) showed collective models competitive. Assumption: Measure inter-annotator agreement first.
- **Dialogue Depth vs. User Burden**: Paper limited to one construction loop + one reduction loop. More loops may improve accuracy but increase dropout risk.
- **LLM-as-Classifier vs. Code Generation**: Direct classification is flexible but expensive per query; generating reward code is cheaper but less adaptable to new contexts.
- **Context Window Limits**: Full conversation history must fit in context; long dialogues may require summarization.

### Failure signatures:
- **Collective model at chance with high N**: Study 1's MLP_col achieved 48% (below random) with 21× more data—signals suppressed minority preferences
- **High Jaccard similarity + individual model struggling**: If participants use similar features (J > 0.45), collective models may be more efficient
- **User reports "intuition" or "vibes"**: Interview data (P7 in Study 2) showed some decisions relied on implicit reasoning—verbal models may fail
- **LLM uncertainty flat or miscalibrated**: If U(τ) doesn't vary meaningfully, active learning degrades to random sampling

### First 3 experiments:
1. **Baseline replication**: Implement LB (label-only) with same diversity/uncertainty sampling but no reflective dialogue; measure accuracy gap on held-out labels per participant
2. **Reflection ablation**: Compare full IRDA vs. IRDA without hypothesis generation (H, A); test whether improvement comes from reflection or simply more examples
3. **Heterogeneity boundary detection**: Run both domains; compute κ and J for each; plot where collective models cross individual model performance to identify the switching threshold

## Open Questions the Paper Calls Out

### Open Question 1
How can individualized reward models be aggregated to make collective decisions while preserving value pluralism? The paper focuses on individual preference elicitation but does not address how to combine learned individual rewards when collective decisions are needed.

### Open Question 2
Can code-based reward functions generated by LLMs match the flexibility and accuracy of in-context verbal reward models while being more computationally efficient? The trade-off between computational efficiency and flexibility has not been empirically evaluated.

### Open Question 3
How well does IRDA generalize to more diverse populations and to the alignment of LLMs themselves? Limited to 30 university participants across two specific domains; unknown whether findings hold for broader populations or LLM alignment.

### Open Question 4
Can domain features predict preference heterogeneity before elicitation, enabling optimal method selection between individualized and collective approaches? Without knowing preference heterogeneity beforehand, practitioners cannot select the optimal approach.

## Limitations
- Exact LLM prompts for dialogue generation and reward classification are not fully specified, affecting reproducibility
- Calibration of LLM token probability uncertainty as a proxy for user decision uncertainty is not empirically validated
- Domain generalizability beyond the two tested environments (respectful behavior and autonomous vehicle ethics) remains unproven

## Confidence
- **High Confidence**: Core mechanism of using reflective dialogue to construct preferences, importance of measuring inter-annotator agreement to determine individual vs collective approaches, general improvement from reflective elicitation over label-only approaches
- **Medium Confidence**: Specific numerical improvements (9-12% accuracy gains) and sample efficiency claims, effectiveness of uncertainty sampling
- **Low Confidence**: Generalizability across different alignment domains, robustness when LLM uncertainty is poorly calibrated or when preferences are genuinely intuitive

## Next Checks
1. **Prompt Engineering Validation**: Systematically test variations in the LLM prompts for dialogue generation and reward classification to identify the most effective prompt formulations, then measure performance differences.
2. **Uncertainty Calibration Study**: Evaluate the correlation between LLM token probability uncertainty and actual user decision uncertainty by having users rate their own confidence on trajectories selected via different uncertainty thresholds.
3. **Cross-Domain Generalization Test**: Apply IRDA to a third alignment domain with known value diversity (e.g., content moderation or educational AI) to test whether the 9-12% improvement and individual model advantages replicate outside the original two domains.