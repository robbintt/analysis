---
ver: rpa2
title: 'Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist
  Through Downstream Fine-tuning'
arxiv_id: '2602.00500'
source_url: https://arxiv.org/abs/2602.00500
tags:
- fine-tuning
- clean
- backdoor
- infuse
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents INFUSE, the first backdoor attack framework
  for Vision-Language-Action (VLA) models that persists through user-side fine-tuning.
  INFUSE identifies fine-tune-insensitive modules through parameter sensitivity analysis
  and selectively injects backdoors into these stable components while freezing others.
---

# Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning

## Quick Facts
- arXiv ID: 2602.00500
- Source URL: https://arxiv.org/abs/2602.00500
- Reference count: 40
- Primary result: First backdoor attack framework (INFUSE) for VLA models that persists through user-side fine-tuning, achieving 91.0% ASR on simulation and 79.8% on real-world robot tasks

## Executive Summary
This paper introduces INFUSE, the first backdoor attack framework designed to survive downstream fine-tuning in Vision-Language-Action (VLA) models. The attack exploits differential parameter stability across VLA modules, selectively injecting backdoors into fine-tune-insensitive components while freezing sensitive modules. By targeting stable perceptual and linguistic representations rather than task-specific heads, INFUSE maintains attack effectiveness even after extensive clean adaptation, demonstrating a critical security vulnerability in VLA systems.

## Method Summary
INFUSE operates through a three-stage pipeline: first, it analyzes parameter sensitivity across VLA modules using Mean Absolute Parameter Difference, Fisher-normalized Difference, and CKA-based Activation Shift metrics to identify fine-tune-insensitive components; second, it injects backdoors exclusively into these stable modules while freezing sensitive ones using a combined clean and poisoned data loss function; third, when users fine-tune the poisoned model on clean downstream data, the backdoor persists in the stable modules while the sensitive task-specific components adapt to new environments.

## Key Results
- INFUSE achieves 91.0% average attack success rate on simulation environments versus 31.4% for baseline methods
- Maintains 79.8% ASR on real-world robot tasks while baseline methods drop to 31.0%
- Preserves clean success rate (SR(w/o)) at 84.5% compared to normal models at 85.0%
- Resists input-level defenses with ASR remaining above 87% under JPEG compression and Gaussian noise attacks

## Why This Works (Mechanism)

### Mechanism 1: Parameter Sensitivity Differential
Different modules in VLA architectures exhibit vastly different parameter update magnitudes during downstream fine-tuning, creating asymmetric vulnerability to overwriting. The paper quantifies this through three metrics—Mean Absolute Parameter Difference, Fisher-normalized Difference, and Activation Shift (CKA-based). Modules with low combined sensitivity scores (vision backbone: 0.33, vision projector: 0.17, LLM backbone: 0.01) undergo 100–1000× smaller parameter updates than sensitive modules (action head: 0.84, proprio projector: 0.97). Backdoors embedded in low-sensitivity regions persist because user fine-tuning predominantly updates task-specific heads while leaving perceptual and linguistic representations relatively unchanged.

### Mechanism 2: Selective Injection with Module Freezing
Constraining backdoor injection exclusively to fine-tune-insensitive modules prevents overwriting during clean downstream adaptation. During injection (Stage 2), all sensitive modules remain frozen while only insensitive modules receive gradient updates from the combined loss function (Equation 5): clean data loss plus λ-weighted poisoned data loss. This localizes the trigger-action mapping to regions that user fine-tuning will not substantially modify. The poison data contains realistic object triggers (e.g., blue mug) paired with malicious target actions.

### Mechanism 3: Attention Preservation Through Representation Stability
Backdoors in fine-tune-insensitive modules maintain trigger attention patterns through downstream adaptation, enabling consistent attack activation. The vision backbone and LLM backbone encode trigger representations that activate malicious action generation. Because these modules undergo minimal parameter shifts during fine-tuning, the learned attention to trigger regions persists. Qualitative analysis shows baseline methods lose trigger attention after fine-tuning while INFUSE maintains strong focus.

## Foundational Learning

### Concept 1: VLA Architecture Components
- **Why needed here:** Understanding which modules are fine-tune-insensitive requires distinguishing perception (vision backbone), reasoning (LLM backbone), projection layers, and action generation (action head, proprio projector)
- **Quick check question:** Which VLA component would you expect to change most during robot-specific fine-tuning: the vision encoder or the action head?

### Concept 2: Fine-tuning Dynamics and Parameter Stability
- **Why needed here:** The core mechanism relies on differential parameter updates across modules during downstream adaptation—understanding why some modules stabilize quickly while others continue adapting is essential
- **Quick check question:** Why might a pretrained vision backbone require fewer parameter updates than a task-specific action head during robotic manipulation fine-tuning?

### Concept 3: Backdoor Attack Taxonomy
- **Why needed here:** Distinguishing data poisoning (BadNet), latent-space manipulation (BadVLA), and selective module injection (INFUSE) clarifies why prior methods fail under fine-tuning
- **Quick check question:** What is the fundamental difference between a backdoor that survives fine-tuning and one that requires adversarial control over the training pipeline?

## Architecture Onboarding

### Component Map:
Base VLA Model
├── Vision Backbone (fine-tune-insensitive, Score ~0.33)
├── Vision Projector (fine-tune-insensitive, Score ~0.17)
├── Proprio Projector (fine-tune-SENSITIVE, Score ~0.97)
├── LLM Backbone (fine-tune-insensitive, Score ~0.01)
└── Action Head (fine-tune-SENSITIVE, Score ~0.84)

INFUSE Pipeline:
Stage 1 → Sensitivity Analysis (MAD + FND + AS → Normalized Score)
Stage 2 → Selective Injection (freeze sensitive, update insensitive with poisoned data)
Stage 3 → User Fine-tuning (clean data, backdoor persists)

### Critical Path:
1. Probe phase: Fine-tune base model on k diverse environments to collect parameter/activation drift statistics
2. Score computation: Calculate MAD, FND, AS per module; normalize and fuse with weights (α=β=γ=1/3)
3. Module selection: Rank by ascending sensitivity score; select modules until cumulative Fisher-weighted drift share ≤ P%
4. Injection: Train selected modules on D_clean ∪ D_poison with loss (5), freeze all others
5. Validation: Fine-tune poisoned model on clean downstream data; verify ASR persistence

### Design Tradeoffs:
- Drift budget (P%): Lower values → more conservative selection (fewer modules), potentially missing useful injection targets; higher values → may include partially-sensitive modules
- Poisoning weight (λ): Higher λ strengthens backdoor but risks clean performance degradation; paper uses values that maintain SR(w/o) comparable to normal models
- Trigger design: Natural objects (blue mug) provide stealth but may vary in real-world appearance; synthetic patches are more reliable but conspicuous

### Failure Signatures:
- Rapid ASR collapse after fine-tuning: Indicates injection into sensitive modules (baseline methods show 31–39% ASR vs. INFUSE's 91–95%)
- Clean performance degradation (SR(w/o) drops): Suggests excessive poisoning weight or insufficient clean data mixing
- Attention heatmap shows no trigger focus after fine-tuning: Indicates backdoor representation was overwritten

### First 3 Experiments:
1. Sensitivity validation: Fine-tune base OpenVLA-7B on 4+ diverse LIBERO tasks; compute per-module sensitivity scores to verify stable/insensitive module identification matches paper findings (LLM < Vision < Projectors < Action Head)
2. Ablation on injection target: Compare three conditions—full model injection, sensitive-modules-only injection, insensitive-modules-only injection—using identical poison data and fine-tuning protocol; measure ASR and SR(w/o) post fine-tuning
3. Defense stress test: Apply JPEG compression (q=40-80%), Gaussian noise (ε=0.02-0.06), and ΔW auditing (5-20% modules reset) to INFUSE-poisoned model; verify ASR resilience matches Table 6 thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic or instruction-aware triggers be developed that maintain persistence through fine-tuning while providing more flexible attack capabilities?
- Basis in paper: "INFUSE currently relies on static triggers and fixed target behaviors. Future work could explore dynamic or instruction-aware triggers."
- Why unresolved: Current triggers are fixed object-based (e.g., blue mug). Instruction-aware triggers that activate conditionally on language commands remain unexplored.
- What evidence would resolve it: Demonstrating persistent backdoor attacks with triggers that adapt to different instructions or contexts, evaluated on the same LIBERO/SimplerEnv benchmarks.

### Open Question 2
- Question: What parameter-level auditing or certification methods can reliably detect persistent backdoors implanted in fine-tune-insensitive modules?
- Basis in paper: "Future mitigation strategies may include auditing parameter updates during user fine-tuning, detecting unusually static modules, or applying certification techniques."
- Why unresolved: Tested defenses (JPEG compression, Gaussian noise, ΔW auditing up to 20%) showed limited effectiveness; ASR remained above 87% under all tested conditions.
- What evidence would resolve it: A defense mechanism that significantly reduces ASR (e.g., below 30%) while maintaining clean performance, validated across multiple VLA architectures.

### Open Question 3
- Question: Does INFUSE generalize to VLA architectures with different training paradigms (e.g., diffusion-based action generation) beyond the tested transformer-based models?
- Basis in paper: The method was validated on OpenVLA-7B, π0.5, and SpatialVLA-4B—all transformer-based. The limitation section calls for "adapt[ing] the approach to alternative architectures."
- Why unresolved: Sensitivity analysis assumes gradient-based fine-tuning dynamics; architectures with fundamentally different training mechanisms may exhibit different module stability patterns.
- What evidence would resolve it: Evaluation on diffusion-based VLA models (e.g., RT-X variants) showing whether fine-tune-insensitive modules exist and whether the injection strategy succeeds.

## Limitations

- The backdoor persistence mechanism relies on differential parameter stability, which may not generalize to all VLA architectures or fine-tuning protocols with aggressive learning rates or full-parameter updates
- The method assumes standard fine-tuning practices and may be vulnerable to parameter auditing or resetting of seemingly stable modules, though tested defenses showed limited effectiveness
- INFUSE was validated only on transformer-based VLA models, and generalization to alternative architectures (e.g., diffusion-based models) remains unexplored

## Confidence

- **High confidence:** The existence of parameter sensitivity differential during fine-tuning (supported by quantitative metrics and ablation studies showing 95.3% vs 10.8% ASR between insensitive and sensitive module injection)
- **Medium confidence:** The general applicability of INFUSE across different VLA models and downstream tasks (tested on three architectures but limited downstream environments)
- **Medium confidence:** The effectiveness against input-level defenses (JPEG compression and noise injection show resilience, but comprehensive defense evaluation is limited)

## Next Checks

1. **Architecture transferability test:** Apply INFUSE to a VLA architecture not evaluated in the paper (e.g., MM-VLA or SpatialVLA variants) and verify the sensitivity differential mechanism holds
2. **Fine-tuning protocol robustness:** Test INFUSE persistence under different fine-tuning regimes (full-parameter vs. partial, varying learning rates) to establish break points
3. **Defense circumvention analysis:** Systematically evaluate whether standard VLA defense mechanisms (input sanitization, parameter auditing, activation masking) can detect or eliminate INFUSE backdoors, particularly focusing on the attention-based reconstruction defense mentioned in the corpus