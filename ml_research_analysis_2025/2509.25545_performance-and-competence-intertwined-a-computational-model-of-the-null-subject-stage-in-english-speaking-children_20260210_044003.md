---
ver: rpa2
title: 'Performance and competence intertwined: A computational model of the Null
  Subject stage in English-speaking children'
arxiv_id: '2509.25545'
source_url: https://arxiv.org/abs/2509.25545
tags:
- iarc
- parameter
- language
- grammar
- children
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study models the null subject (NS) stage in English-speaking\
  \ children, where subjects are frequently omitted in early speech. Using a computational\
  \ model called the Superset-Subset Variational Learner (SSVL), the researchers simulated\
  \ how children\u2019s misinterpretation of imperative sentences (due to performance\
  \ limitations) leads to temporary adoption of a null-subject grammar."
---

# Performance and competence intertwined: A computational model of the Null Subject stage in English-speaking children

## Quick Facts
- arXiv ID: 2509.25545
- Source URL: https://arxiv.org/abs/2509.25545
- Reference count: 21
- English-speaking children temporarily adopt a null-subject grammar due to misinterpreting imperative sentences as declaratives, with performance improvement leading to convergence on the correct grammar

## Executive Summary
This study presents a computational model of the Null Subject (NS) stage in English-speaking children, where subjects are frequently omitted in early speech. Using the Superset-Subset Variational Learner (SSVL), the researchers demonstrate that temporary adoption of a null-subject grammar results from children's performance limitations in interpreting imperative sentences. As children's ability to resolve illocutionary ambiguity improves over time, the model shows convergence to the correct obligatory-subject grammar, supporting the hypothesis that performance factors contribute to this developmental phenomenon.

## Method Summary
The study employs a computational model called the Superset-Subset Variational Learner (SSVL) to simulate how performance limitations lead to temporary grammatical errors. The model uses an Illocution Ambiguity Resolution Coefficient (IARC) to quantify children's ability to correctly identify imperative sentences. Starting with 100 virtual children, the simulation tracks how the NS parameter weight changes over approximately 10 million utterances from birth to age 5. The SSVL uses dual learning rates to distinguish between ambiguous and unambiguous evidence, allowing it to retreat from the superset (null-subject) grammar to the correct subset (obligatory-subject) grammar as IARC improves.

## Key Results
- SSVL successfully converges to the correct obligatory-subject grammar in English
- NS parameter weight rises to ~0.8 early in development (simulating NS stage) then declines
- Simulations show convergence patterns matching real developmental data when IARC improves over time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporary retention of Null Subject grammar is caused by misinterpreting imperative sentences as declaratives, creating noisy input that validates the superset grammar
- **Mechanism:** The Illocution Ambiguity Resolution Coefficient (IARC) causes subjectless imperatives to be mislabeled as declaratives when low, pushing the learner toward the Null Subject parameter
- **Core assumption:** Grammatical competence is directly altered by performance limitations in speech act interpretation
- **Evidence anchors:** Young English speakers often confuse imperative NS utterances with declaratives; imperative NS sentences labeled with declarative illocutionary force are introduced into the e-child's environment
- **Break condition:** If imperatives are correctly interpreted from the start, the NS stage is skipped or drastically shortened

### Mechanism 2
- **Claim:** Convergence on the correct "Obligatory Subject" grammar is achieved via modified reward schedule distinguishing ambiguous and unambiguous evidence
- **Mechanism:** SSVL uses dual learning rates - aggressive for superset-only parses, conservative for subset parses - preventing permanent settlement on the superset
- **Core assumption:** The learner possesses meta-knowledge of which parameter values represent superset vs. subset languages
- **Evidence anchors:** Whenever the VL encounters a sentence that can be parsed only by the superset grammar, it rewards at a higher rate; SSVL successfully converges on the target superset value
- **Break condition:** If aggressive rate is not sufficiently higher than conservative rate, or subset check is omitted, the model fails to retreat from the NS superset

### Mechanism 3
- **Claim:** Retreat from the NS stage is time-locked to maturation of performance constraints, specifically linear or logistic growth of IARC
- **Mechanism:** As simulated child ages, IARC increases, reducing mislabeled imperatives and their reward signal for NS grammar, while more unambiguous subject-bearing declaratives drive parameter weight toward subset value
- **Core assumption:** Ability to resolve illocutionary force improves as a function of cumulative linguistic exposure
- **Evidence anchors:** Weight swiftly ascends to approximately 0.8 before declining due to simulated increase in imperative sentence exposure; imperfect learning followed by gradual correction converges on obligatory-subject target grammar
- **Break condition:** If IARC growth is arrested, the grammar remains stuck in the NS stage indefinitely

## Foundational Learning

- **Concept: Principles & Parameters (P&P)**
  - **Why needed here:** The simulation operates on a binary switch for the Null Subject parameter
  - **Quick check question:** Does the learner optimize a continuous weight or flip a discrete switch? (Answer: It optimizes a continuous weight w_i that determines the probability of selecting a binary value pv_i)

- **Concept: The Subset Principle**
  - **Why needed here:** This is the core problem the paper solves - standard learners struggle to retreat from a superset (NS) to a subset (English) without negative evidence
  - **Quick check question:** Why is learning English harder for a variational learner than learning a Null Subject language? (Answer: English is a subset; a learner must realize what is missing)

- **Concept: Variational Learning (VL)**
  - **Why needed here:** Provides the underlying dynamics where grammars compete based on successful parsing
  - **Quick check question:** How does the learner update its hypothesis? (Answer: By rewarding parameter values that successfully parse the current input sentence)

## Architecture Onboarding

- **Component map:** Input Generator -> IARC Module -> SSVL Engine -> Growth Function
- **Critical path:**
  1. Initialize weights w_i = 0.5
  2. Receive sentence s; if imperative, apply IARC noise (mislabeling)
  3. Attempt to parse s with current hypothesis G_curr
  4. Superset Check: If G_curr has NS=1 (superset), check if flipping to NS=0 (subset) also parses s
  5. Update: If subset parses, conservative update toward NS=0; if only superset parses, aggressive update toward NS=1
- **Design tradeoffs:**
  - Dual Learning Rates (R vs r): Requires manual tuning; aggressive rates ensure convergence to superset if it were the target, but can delay convergence to subset if tuned poorly
  - Artificial Domain (CoLAG): Ensures exact parameter control but limits ecological validity compared to CHILDES
- **Failure signatures:**
  - Overshooting: Parameter weight hits 1.0 (NS) and never declines, indicating IARC growth is too slow or R/r ratio is incorrect
  - Early Convergence: Weight hits 0.0 before age 2;0, indicating IARC noise is insufficient to trigger the NS stage
- **First 3 experiments:**
  1. Baseline Sanity Check: Run SSVL on CoLAG English with IARC = 1.0 (no noise). Verify immediate convergence to Obligatory Subject
  2. Rate Sensitivity Analysis: Grid search the ratio of Aggressive (R) to Conservative (r) rates to find minimum ratio required for successful retreat from NS stage
  3. Distribution Mismatch: Vary percentage of imperatives in input (default 16-18%) to test if low imperative density prevents NS stage from emerging

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on artificial language domains (CoLAG) rather than naturalistic child-directed speech data limits ecological validity
- Three-point sampling approach for IARC trajectories may oversimplify the gradual nature of performance improvement
- Assumes perfect meta-knowledge of superset/subset relationships, which may not reflect actual learning mechanisms

## Confidence
- **High Confidence:** Computational framework demonstrates performance limitations can create temporary grammatical errors that resolve with improved processing capacity; SSVL algorithm is internally consistent
- **Medium Confidence:** Specific IARC growth trajectories and correspondence to actual child development is plausible but not empirically validated
- **Low Confidence:** Model's predictions about timing and duration of NS stage in individual children may not generalize well to real developmental data given stochastic nature and simplified input environment

## Next Checks
1. Test SSVL sensitivity to IARC growth function parameters by varying m and c across full parameter space to identify stability boundaries
2. Replicate simulations with alternative sentence sampling distributions (uniform vs. Zipfian) to assess robustness to input frequency effects
3. Compare model predictions against CHILDES corpus data for subject omission rates and their relationship to imperative usage patterns in actual child speech