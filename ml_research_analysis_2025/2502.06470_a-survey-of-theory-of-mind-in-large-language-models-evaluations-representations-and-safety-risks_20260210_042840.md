---
ver: rpa2
title: 'A Survey of Theory of Mind in Large Language Models: Evaluations, Representations,
  and Safety Risks'
arxiv_id: '2502.06470'
source_url: https://arxiv.org/abs/2502.06470
tags:
- llms
- language
- large
- arxiv
- risks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys research on Theory of Mind (ToM) capabilities
  in Large Language Models (LLMs), examining both behavioral evaluations and internal
  representations. The study finds that while LLMs can match human performance on
  specific ToM tasks, they remain limited and non-robust compared to humans on more
  challenging benchmarks.
---

# A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks

## Quick Facts
- **arXiv ID**: 2502.06470
- **Source URL**: https://arxiv.org/abs/2502.06470
- **Reference count**: 6
- **Key result**: LLMs show limited, non-robust ToM capabilities compared to humans, with emerging internal belief state representations and potential safety risks

## Executive Summary
This survey examines Theory of Mind (ToM) capabilities in Large Language Models through three lenses: behavioral evaluations, internal representations, and safety implications. While LLMs demonstrate performance matching humans on specific ToM tasks, they remain fundamentally limited and non-robust on more challenging benchmarks. The study identifies safety concerns including privacy invasion, social engineering, deceptive behaviors, and multi-agent risks. The paper recommends developing more authentic evaluation frameworks and mitigation strategies such as model unlearning and activation engineering.

## Method Summary
The paper conducts a comprehensive literature review of existing research on ToM in LLMs, synthesizing findings from behavioral studies, neuroscientific investigations using linear probes, and safety analyses. The methodology involves examining multiple evaluation paradigms, analyzing internal representations of belief states, and identifying potential risks through theoretical analysis and case studies of observed behaviors.

## Key Results
- LLMs can match human performance on specific ToM tasks but show non-robust performance on more challenging benchmarks
- Linear probes have identified internal representations of belief states in LLMs, suggesting emerging cognitive capabilities
- Advanced ToM in LLMs poses safety risks including privacy invasion, social engineering, deceptive behaviors, and multi-agent exploitation risks

## Why This Works (Mechanism)
ToM in LLMs emerges from pattern recognition in training data rather than genuine understanding of mental states. The models learn statistical correlations between linguistic patterns and belief attribution scenarios, enabling them to generate appropriate responses in familiar contexts. This mechanism explains both the surface-level competence on standard benchmarks and the fundamental limitations when faced with novel or complex social reasoning tasks.

## Foundational Learning
1. **Theory of Mind**: The cognitive ability to attribute mental states to others and understand that others have beliefs, desires, and intentions different from one's own
   - Why needed: Provides the conceptual framework for evaluating social reasoning capabilities
   - Quick check: Can distinguish between first-order (understanding others' beliefs) and second-order (understanding others' beliefs about beliefs) ToM

2. **Linear Probes**: A technique for analyzing neural network representations by training simple classifiers on hidden layer activations to detect specific information
   - Why needed: Allows researchers to identify whether LLMs encode belief state information in their internal representations
   - Quick check: Look for correlation between probe accuracy and behavioral ToM performance

3. **Multi-agent Risk**: Safety concerns arising when multiple AI systems with ToM capabilities interact and potentially coordinate or compete
   - Why needed: Identifies collective risks that may emerge from individual model capabilities
   - Quick check: Consider how ToM enables strategic interactions between agents

## Architecture Onboarding
**Component Map**: Training Data -> Pattern Recognition -> ToM Simulation -> Behavioral Output

**Critical Path**: The transformation from training data patterns to ToM-like behaviors occurs through the model's ability to recognize and reproduce linguistic patterns associated with belief attribution, rather than through genuine understanding of mental states.

**Design Tradeoffs**: LLMs trade computational efficiency and generalization for the ability to handle novel scenarios requiring true ToM understanding. This creates a fundamental limitation where models excel at pattern completion but struggle with genuine social reasoning.

**Failure Signatures**: Non-robust performance on challenging benchmarks, inability to handle novel social scenarios, and brittle responses when confronted with unexpected belief states or conflicting information.

**First Experiments**:
1. Test model performance on standard ToM benchmarks (unexpected transfer, false belief tasks)
2. Apply linear probes to hidden layers to detect belief state representations
3. Evaluate model responses to novel social scenarios not present in training data

## Open Questions the Paper Calls Out
- How can we develop more authentic evaluation frameworks that reflect real-world social interactions?
- What are the precise mechanisms by which LLMs represent and process belief states internally?
- How can we effectively mitigate safety risks from advanced ToM capabilities without degrading beneficial social reasoning abilities?

## Limitations
- Current ToM evaluations rely heavily on artificial scenarios that may not reflect authentic social interactions
- Linear probe evidence for internal representations demonstrates correlation but cannot establish causal relationships
- Safety risk assessments remain largely speculative with limited empirical validation of predicted emergent behaviors

## Confidence
**High Confidence**: LLMs show non-robust performance on challenging ToM benchmarks compared to humans
**High Confidence**: Linear probes can identify belief state representations in LLM internal representations
**Medium Confidence**: Current safety risks from ToM capabilities are supported by observed behaviors but may not scale predictably
**Low Confidence**: Predictions about emergent risks as ToM capabilities advance remain highly speculative

## Next Checks
1. Develop and validate naturalistic ToM evaluation frameworks using real conversational data with human expert annotations, then compare performance to existing benchmark results
2. Conduct ablation studies targeting identified belief state representations to establish causal relationships between internal representations and behavioral ToM performance
3. Design controlled experiments testing multi-agent scenarios where ToM capabilities could lead to collective misalignment, systematically varying model sophistication to identify risk thresholds