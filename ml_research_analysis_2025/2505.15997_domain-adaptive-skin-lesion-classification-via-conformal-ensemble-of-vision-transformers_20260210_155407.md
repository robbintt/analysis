---
ver: rpa2
title: Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision
  Transformers
arxiv_id: '2505.15997'
source_url: https://arxiv.org/abs/2505.15997
tags:
- prediction
- skin
- uncertainty
- expert
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses domain adaptation and uncertainty estimation
  challenges in skin lesion classification by proposing a Conformal Ensemble of Vision
  Transformers (CE-ViTs) framework. CE-ViTs combines multiple vision transformer models
  trained on diverse skin lesion datasets (HAM10000, Dermofit, and Skin Cancer ISIC)
  using ensemble learning, and integrates conformal prediction to generate reliable
  prediction sets with uncertainty quantification.
---

# Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers

## Quick Facts
- arXiv ID: 2505.15997
- Source URL: https://arxiv.org/abs/2505.15997
- Reference count: 21
- Primary result: 90.38% coverage achieved with CE-ViTs, a 9.95% improvement over HAM10000 model alone

## Executive Summary
This work addresses domain adaptation and uncertainty estimation challenges in skin lesion classification by proposing a Conformal Ensemble of Vision Transformers (CE-ViTs) framework. CE-ViTs combines multiple vision transformer models trained on diverse skin lesion datasets using ensemble learning, and integrates conformal prediction to generate reliable prediction sets with uncertainty quantification. Experimental results show CE-ViTs achieves 90.38% coverage, indicating a higher likelihood of including the true label in prediction sets, and demonstrates low-to-intermediate uncertainty for correct predictions while maintaining high uncertainty for incorrect ones.

## Method Summary
CE-ViTs trains three separate Vision Transformer models on distinct skin lesion datasets (HAM10000, Dermofit, and ISIC), then combines their softmax outputs via simple averaging. The framework integrates conformal prediction using a combined calibration set from all three datasets to generate prediction sets with formal coverage guarantees. The approach uses frozen pre-trained ViT backbones with new MLP classifier heads, trained for 20 epochs with SGDM optimizer. Data augmentation includes reflection, rotation, and scaling, with 60/10/20/10 train/val/calibration/test splits.

## Key Results
- 90.38% coverage achieved, representing a 9.95% improvement over HAM10000 model alone
- Average prediction set size for misclassified samples increased from 1.86 to 3.075
- Low-to-intermediate uncertainty for correct predictions, consistently high uncertainty for incorrect predictions
- Seven-class classification (akiec, bcc, bkl, df, mel, nv, vasc) across mixed-domain datasets

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Averaging for Robustness
Averaging predictions from multiple ViTs trained on distinct datasets improves robustness by smoothing out dataset-specific biases. The uncorrelated errors assumption suggests that combining models reduces variance and increases the likelihood of including the true label in prediction sets.

### Mechanism 2: ViT Global Context for Domain Adaptation
Vision Transformers capture global contextual relationships through self-attention mechanisms, making them more robust to domain shifts than CNNs. This global attention allows focusing on overall lesion structure rather than dataset-specific textures.

### Mechanism 3: Conformal Prediction for Uncertainty Quantification
Conformal prediction transforms raw model scores into statistically rigorous prediction sets, calibrating uncertainty and reducing overconfidence. The framework provides formal coverage guarantees by including all classes below a threshold determined from calibration data.

## Foundational Learning

- **Vision Transformer (ViT) Architecture**: Understanding how ViTs process images as sequences of patches via self-attention is essential for grasping their claimed advantage in capturing global context and handling domain shifts.
  - Quick check: How does a Vision Transformer process an image differently than a Convolutional Neural Network?

- **Ensemble Learning for Variance Reduction**: Understanding why averaging predictions from multiple models can reduce variance and improve robustness is key to understanding the CE-ViTs framework.
  - Quick check: Why might averaging the predictions of three models trained on different datasets lead to a more reliable result than using any single model alone?

- **Conformal Prediction (CP)**: Understanding that CP provides a post-hoc method to create prediction sets with formal statistical guarantees of coverage is crucial for evaluating the framework's uncertainty quantification.
  - Quick check: In conformal prediction, what is a "prediction set" and what does "coverage" mean?

## Architecture Onboarding

- **Component map**: ViT models (HAM10000, Dermofit, ISIC) -> Ensemble Layer (softmax averaging) -> Conformal Prediction Module -> Prediction Sets
- **Critical path**: Data curation and splitting, ensemble calibration through softmax averaging, quantile computation from calibration set
- **Design tradeoffs**: Accuracy vs. coverage (larger sets for higher coverage), robustness vs. simplicity (simple vs. weighted averaging), theoretical guarantees vs. empirical robustness
- **Failure signatures**: Consistently large prediction sets (>5 classes), low realized coverage, correlated errors between models
- **First 3 experiments**:
  1. Ablation on Ensemble Components: Compare individual ViT performance vs. full CE-ViTs ensemble
  2. Calibration Sensitivity Analysis: Test dataset-specific vs. combined calibration sets
  3. Coverage vs. Set Size Trade-off Curve: Vary Î± to characterize fundamental trade-offs

## Open Questions the Paper Calls Out

- **Open Question 1**: Can weighted averaging strategies for the ensemble improve calibration and reduce prediction set size more effectively than simple averaging?
- **Open Question 2**: Does integrating conformal prediction directly into the training process yield better robustness and accuracy than post-hoc conformal prediction?
- **Open Question 3**: To what extent does the "frozen backbone" transfer learning approach limit the model's ability to adapt to domain-specific features compared to full fine-tuning?

## Limitations

- Lack of comparative analysis against other domain adaptation and uncertainty quantification methods (e.g., Domain-Adversarial Neural Networks, Monte Carlo Dropout)
- Unspecified ViT architecture details and conformal prediction parameters limit reproducibility
- No explicit analysis of class imbalance across datasets affecting ensemble performance
- Assumption of exchangeability between calibration and test sets may not hold in real-world clinical scenarios

## Confidence

- **High Confidence**: Ensemble averaging and conformal prediction are standard approaches with clear theoretical foundations; coverage and prediction set size improvements are directly measurable
- **Medium Confidence**: ViT advantages for domain adaptation are supported by architectural reasoning but lack direct comparative evidence against CNNs
- **Medium Confidence**: Conformal prediction guarantees assume exchangeability between calibration and test sets

## Next Checks

1. **Baseline Comparison**: Implement and compare CE-ViTs against Domain-Adversarial Neural Networks and Monte Carlo Dropout uncertainty quantification on the same datasets
2. **Cross-Dataset Calibration**: Evaluate impact of dataset-specific vs. combined calibration sets on coverage and prediction set size
3. **Clinical Relevance Assessment**: Analyze prediction set composition for misclassified samples to determine clinical meaningfulness for differential diagnosis