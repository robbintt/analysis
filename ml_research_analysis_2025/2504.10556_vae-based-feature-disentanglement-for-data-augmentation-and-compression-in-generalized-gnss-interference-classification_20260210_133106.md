---
ver: rpa2
title: VAE-based Feature Disentanglement for Data Augmentation and Compression in
  Generalized GNSS Interference Classification
arxiv_id: '2504.10556'
source_url: https://arxiv.org/abs/2504.10556
tags:
- data
- gnss
- signal
- power
- interference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes using variational autoencoders (VAEs) for feature\
  \ disentanglement to compress GNSS interference classification data, enabling efficient\
  \ low-latency communication and decentralized processing. Three VAE variants\u2014\
  vanilla, factorized, and conditional generative\u2014are evaluated to extract essential\
  \ latent features while maintaining high classification accuracy."
---

# VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification

## Quick Facts
- **arXiv ID:** 2504.10556
- **Source URL:** https://arxiv.org/abs/2504.10556
- **Reference count:** 39
- **One-line primary result:** VAE-based compression achieves 512-8,192x data reduction with 99.92% classification accuracy for GNSS interference detection.

## Executive Summary
This paper proposes using variational autoencoders (VAEs) for feature disentanglement to compress GNSS interference classification data, enabling efficient low-latency communication and decentralized processing. Three VAE variants—vanilla, factorized, and conditional generative—are evaluated to extract essential latent features while maintaining high classification accuracy. Data augmentation is achieved by interpolating lower-dimensional latent representations of signal power. Experiments on four datasets, including controlled indoor and real-world highway scenarios, show that the proposed VAE achieves data compression rates between 512 and 8,192 while maintaining classification accuracy up to 99.92%. The compressed dense model significantly reduces inference time compared to ResNet18 while incurring only marginal accuracy loss.

## Method Summary
The approach uses FactorVAE to compress high-dimensional GNSS spectrograms into low-dimensional latent vectors while encouraging statistical independence between latent dimensions through Total Correlation penalization. The encoder (ResNet18) maps spectrograms to latent distributions, from which vectors are sampled and passed to a lightweight classifier (three dense layers). Data augmentation is performed by interpolating between latent vectors to generate synthetic spectrograms at intermediate signal power levels. The model is trained on controlled indoor and real-world highway datasets, then evaluated for classification accuracy and compression efficiency compared to ResNet18 baselines.

## Key Results
- Achieves data compression rates of 512-8,192x while maintaining classification accuracy up to 99.92%
- Compressed dense model reduces inference time by 80-90% compared to ResNet18 with only 0.1% accuracy loss
- Latent interpolation successfully fills gaps in training data for intermediate signal power levels
- FactorVAE outperforms vanilla VAE and CVAE-GAN in both classification accuracy and reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1: Latent Compression via Variational Inference
The Variational Autoencoder (VAE) encoder maps input spectrograms to a latent distribution q(z|x). A subsequent lightweight classifier (dense layers) operates solely on z rather than raw pixels, bypassing the computational cost of heavy CNNs like ResNet18 during inference. The latent space retains sufficient class-discriminative information even when the reconstruction target is relaxed in favor of disentanglement.

### Mechanism 2: Total Correlation Penalization for Disentanglement
FactorVAE introduces a discriminator to distinguish between samples from the aggregated posterior q(z) and the factorized product ∏q(zj). This encourages the dimensions of z to be independent, allowing specific dimensions to correspond to specific generative factors (disentanglement). The underlying factors of variation in GNSS signals (class, power, bandwidth) are assumed to be statistically independent.

### Mechanism 3: Latent Interpolation for Data Augmentation
By sampling two latent vectors and computing α·z₁ + (1-α)·z₂, the decoder generates intermediate spectrograms. This allows training the classifier on continuous variations of signal power even if the original dataset only contained discrete bins. The latent space is assumed to be smooth and continuous; linear interpolation in latent space corresponds to semantic interpolation in data space.

## Foundational Learning

- **Concept: The Reparameterization Trick**
  - **Why needed here:** The paper uses this standard VAE technique to sample z from q(z|x) while allowing gradients to backpropagate through the stochastic sampling node.
  - **Quick check question:** How does moving the stochasticity (ε) to an input node allow the network to learn μ and σ via gradient descent?

- **Concept: Total Correlation (TC) vs. Standard KL Divergence**
  - **Why needed here:** Understanding that standard VAEs (β-VAE) often penalize capacity (mutual information) to achieve disentanglement, whereas FactorVAE specifically penalizes *dependency* (TC) to preserve reconstruction quality.
  - **Quick check question:** Why does minimizing TC encourage independent axes in the latent space, and why is this preferred over simply increasing β?

- **Concept: Spectrogram Representations of RF Signals**
  - **Why needed here:** The input data is not raw time-series but time-frequency spectrograms. Understanding this representation is crucial for designing the Encoder/Decoder architectures (Conv2D vs. Linear).
  - **Quick check question:** Why are Convolutional Neural Networks (CNNs) suitable for the encoder when processing spectrograms?

## Architecture Onboarding

- **Component map:** Input Spectrogram -> ResNet18 Encoder -> Latent Vector z -> Lightweight Dense Classifier
- **Critical path:** The training of the Discriminator in FactorVAE. If this fails to distinguish between the permuted and unpermuted batches, the TC penalty vanishes, and disentanglement does not occur.
- **Design tradeoffs:**
  - Latent Dimension (d): Lower d (e.g., 16) gives better compression/inference speed but risks losing accuracy. Higher d (e.g., 256) retains information but reduces compression gains.
  - Reconstruction vs. Disentanglement: Increasing γ (TC weight) improves disentanglement (cleaner latent traversal) but may degrade reconstruction fidelity (blurriness).
- **Failure signatures:**
  - Posterior Collapse: Latent dimensions ignore input x (KL → 0), resulting in useless constant latents.
  - Artifacts: Real-world datasets may introduce reconstruction artifacts if the model capacity is mismatched to data complexity.
- **First 3 experiments:**
  1. Latent Traversal Visualization: Vary one dimension of z at a time while fixing others; decode and visualize the spectrogram to confirm specific dimensions control power/bandwidth.
  2. Compression vs. Accuracy Sweep: Train the dense classifier on latents with d ∈ {16, 32, 64, 128, 256} and plot accuracy vs. inference time relative to ResNet18 baseline.
  3. Augmentation Generalization: Train a classifier on only extreme signal powers (e.g., -20dB, +10dB), use latent interpolation to generate intermediate data, and test if the model now classifies intermediate levels (-5dB) correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the energy consumption and real-time latency profiles of the proposed compressed models when deployed on physical GNSS receiver hardware?
- Basis in paper: [explicit] The conclusion states, "Future research will involve evaluating these models on receiver hardware to support situational awareness in open environments."
- Why unresolved: The current evaluation measures inference time in software (ms/sample) on standard compute hardware rather than on the resource-constrained embedded devices targeted for Edge AI.
- What evidence would resolve it: Benchmarks of power usage and processing latency obtained from running the compressed classifier on actual embedded GNSS receiver chipsets.

### Open Question 2
- Question: How does transmitting compressed latent representations affect the convergence and accuracy of federated learning systems for GNSS interference monitoring?
- Basis in paper: [explicit] The conclusion aims to "facilitate the development of crowdsourced and federated learning systems" following hardware evaluation.
- Why unresolved: The paper validates compression for centralized training and efficient transmission but does not simulate the distributed training dynamics or communication efficiency of a federated network.
- What evidence would resolve it: A comparative study of federated averaging performance using compressed latents versus raw data or standard gradient compression techniques.

### Open Question 3
- Question: Can architectural improvements mitigate the reconstruction artifacts and smoothness issues observed in FactorVAE and CVAE-GAN latent traversals?
- Basis in paper: [inferred] The text notes in Figure 5 that "FactorVAE introduces artifacts" in real-world data and CVAE-GAN "struggles to achieve smooth transitions between bandwidths."
- Why unresolved: While classification accuracy remains high, the generative quality of the spectrograms is inconsistent, potentially limiting the utility of the data augmentation for certain signal types.
- What evidence would resolve it: Qualitative and quantitative improvements in reconstructed spectrogram fidelity and smoother interpolation metrics across bandwidth and distance parameters.

## Limitations
- Relies on proprietary datasets (L.I.N.K. test center and highway datasets) that are not publicly available, limiting independent validation
- Specific hyperparameters for VAE training (learning rates, KL-divergence weighting) are not fully specified
- Limited quantitative metrics for evaluating disentanglement quality beyond visual inspection of latent traversal

## Confidence

- **High confidence:** The core mechanism of using VAE for data compression and achieving high classification accuracy with compressed models is well-supported by experimental results showing 99.92% accuracy at compression rates of 512-8,192.
- **Medium confidence:** The claim that Total Correlation penalization achieves meaningful disentanglement is supported by latent traversal visualizations, but lacks quantitative disentanglement metrics or ablation studies showing the necessity of this approach versus standard β-VAE.
- **Medium confidence:** The data augmentation via latent interpolation is demonstrated to improve classification on intermediate power levels, but the paper doesn't provide error analysis on when interpolation might produce mislabeled synthetic examples.

## Next Checks

1. **Quantitative Disentanglement Evaluation:** Implement established disentanglement metrics (e.g., BetaVAE score, FactorVAE metric) to measure the statistical independence of latent dimensions beyond visual inspection.

2. **Robustness to Dataset Shift:** Test the compressed model on spectrograms from different environmental conditions (urban vs. rural, different weather) to assess generalization beyond the specific highway and indoor datasets.

3. **Ablation on TC Penalty:** Conduct experiments comparing FactorVAE with standard β-VAE and non-disentangled VAE variants to isolate the contribution of Total Correlation penalization to both reconstruction quality and classification accuracy.