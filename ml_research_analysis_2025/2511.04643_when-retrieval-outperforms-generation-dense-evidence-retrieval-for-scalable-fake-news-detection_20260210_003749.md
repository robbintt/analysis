---
ver: rpa2
title: 'When retrieval outperforms generation: Dense evidence retrieval for scalable
  fake news detection'
arxiv_id: '2511.04643'
source_url: https://arxiv.org/abs/2511.04643
tags:
- evidence
- computational
- retrieval
- arxiv
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeReC presents a dense retrieval-based framework for scalable fake
  news detection that achieves superior accuracy with significantly lower computational
  overhead compared to LLM-based approaches. The method combines efficient text embeddings
  with FAISS-based similarity search to retrieve relevant evidence, which is then
  used for veracity prediction through a specialized classifier.
---

# When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection

## Quick Facts
- **arXiv ID:** 2511.04643
- **Source URL:** https://arxiv.org/abs/2511.04643
- **Reference count:** 11
- **Primary result:** Dense retrieval framework achieves 65.58% F1 on RAWFC, 95% faster than LLM baselines

## Executive Summary
DeReC presents a dense retrieval-based framework for scalable fake news detection that achieves superior accuracy with significantly lower computational overhead compared to LLM-based approaches. The method combines efficient text embeddings with FAISS-based similarity search to retrieve relevant evidence, which is then used for veracity prediction through a specialized classifier. Experimental results demonstrate that DeReC achieves state-of-the-art performance on RAWFC and LIAR-RAW datasets, with substantial runtime improvements while maintaining or improving accuracy.

## Method Summary
DeReC implements a three-stage pipeline: (1) Extract sentence embeddings using either gte-Qwen2-1.5B-instruct or nomic-embed-text-v1.5 models trained via contrastive learning; (2) Build FAISS IndexFlatIP with normalized vectors and retrieve top-10 evidence sentences via cosine similarity; (3) Classify using DeBERTa-v3-large with input format "[CLS]; claim; [SEP]; e1; [SEP]; ...; e10; [SEP]", max length 512, cross-entropy loss. The system is trained on RAWFC and LIAR-RAW datasets using a single NVIDIA A40 GPU.

## Key Results
- Achieves 65.58% F1 score on RAWFC dataset, outperforming the leading baseline (61.20%)
- Reduces runtime by 95% on RAWFC (23 minutes vs 454 minutes) and 92% on LIAR-RAW (134 minutes vs 1692 minutes)
- Maintains or improves accuracy while eliminating LLM autoregressive overhead and hallucination risk

## Why This Works (Mechanism)

### Mechanism 1: Dense Embedding-Based Evidence Extraction
Dense vector representations capture semantic relationships between claims and evidence more effectively than lexical matching, enabling retrieval of topically relevant content without exact keyword overlap. Text embedding models transform raw sentences into d-dimensional vectors using contrastive learning objectives, mapping semantically similar texts to nearby points in embedding space. Core assumption: Claim-evidence pairs that share semantic content will cluster in the learned embedding space, and this geometric proximity correlates with evidentiary utility for veracity prediction.

### Mechanism 2: FAISS-Based Sub-Linear Similarity Search
Approximate nearest neighbor search via FAISS enables scalable retrieval over large evidence corpora with logarithmic rather than linear complexity. Normalized vectors are indexed using FAISS IndexFlatIP, returning top-k evidence sentences ranked by cosine similarity. Core assumption: The top-10 most similar sentences contain sufficient signal for veracity determination; additional evidence provides diminishing returns while increasing computational cost and sequence length.

### Mechanism 3: Evidence-Grounded Classification Without Generation
Direct classification on concatenated claim-evidence pairs achieves comparable or superior accuracy to LLM-based explanation generation while eliminating autoregressive overhead and hallucination risk. Retrieved evidence sentences are concatenated with the claim using delimiter tokens, and DeBERTa-v3-large encodes this sequence for multi-class classification via softmax. Core assumption: Explicit evidence grounding provides sufficient interpretability proxy; natural language explanations are not necessary for accurate veracity prediction.

## Foundational Learning

- **Concept: Dense Text Embeddings & Contrastive Learning**
  - **Why needed here:** The entire retrieval pipeline depends on embedding quality. Without understanding how contrastive learning shapes vector spaces, you cannot diagnose retrieval failures or select appropriate embedding models.
  - **Quick check question:** Given two sentences with high lexical overlap but opposite sentiment (e.g., "The policy succeeded" vs. "The policy failed"), would you expect them to be close or distant in embedding space? How might this affect fake news detection?

- **Concept: Approximate Nearest Neighbor (ANN) Search & FAISS**
  - **Why needed here:** FAISS is the scalability engine. Understanding index types (IndexFlatIP vs. IVF, HNSW) and their tradeoffs is essential for deployment at scale.
  - **Quick check question:** If your evidence corpus grows from 100K to 10M sentences, what index modifications would be necessary to maintain sub-second retrieval latency?

- **Concept: Transformer Classification with Delimited Input**
  - **Why needed here:** The classifier's input format (claim + evidence concatenation) determines what the model learns. Understanding how attention operates across delimiter-separated segments helps debug classification errors.
  - **Quick check question:** If top-10 evidence exceeds 512 tokens, what truncation strategy would preserve the most signal? Should you truncate from the beginning, end, or use sliding windows?

## Architecture Onboarding

- **Component map:**
  [Raw Reports] → Sentence Splitting → [Embedding Model] → Dense Vectors → FAISS Index ← Vector Normalization ← [Corpus Embeddings]
  [Claim] → [Same Embedding Model] → [Query Vector] → Similarity Search → [Top-10 Evidence Sentences]
  [Claim + Evidence] → [DeBERTa-v3-large] → [CLS Token] → [Softmax Classifier] → [Veracity Label]

- **Critical path:** Embedding model selection → FAISS index construction → evidence retrieval (k=10) → classifier fine-tuning. Errors in early stages compound; a poor embedding model yields irrelevant evidence, causing classifier to learn spurious patterns.

- **Design tradeoffs:**
  - **DeReC-qwen (1.5B params)** vs. **DeReC-nomic (137M params):** Qwen achieves higher F1 (65.58% vs. 64.61% on RAWFC) but 3× slower runtime (64min vs. 23min). Choose based on latency requirements.
  - **k=10 evidence sentences:** Balances evidence coverage against sequence length constraints (512 token limit for DeBERTa). Higher k may exceed context window; lower k risks missing critical evidence.
  - **No explanation generation:** Gains 95% runtime reduction but loses interpretable justifications. Trade acceptable for automated filtering; problematic for human-in-the-loop systems.

- **Failure signatures:**
  - **Low retrieval precision:** Top-k evidence semantically unrelated to claim → check embedding model compatibility with domain vocabulary; consider fine-tuning embeddings on in-domain pairs.
  - **Classifier overfitting:** High training accuracy, low test F1 → evidence may contain dataset-specific artifacts; verify evidence comes from temporally/informationally separated sources.
  - **Memory overflow on large corpora:** FAISS index exceeds RAM → switch to IVF (inverted file index) or disk-based indices; accept approximate retrieval quality degradation.

- **First 3 experiments:**
  1. **Baseline retrieval quality assessment:** Manually annotate 50 claim-evidence pairs from top-10 retrieval results. Measure precision@k for k ∈ {1, 5, 10}. Establish whether retrieval is the bottleneck before tuning classifier.
  2. **Embedding model ablation:** Compare DeReC-qwen vs. DeReC-nomic on a held-out validation split. Measure F1, precision, recall, and runtime. Quantify accuracy-efficiency tradeoff for your deployment constraints.
  3. **k-sensitivity analysis:** Vary k ∈ {3, 5, 10, 15, 20} and measure classifier performance. If F1 plateaus at k=5, you can reduce sequence length and improve throughput without accuracy loss.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can dynamic evidence corpus updates be integrated into the DeReC framework without requiring full index reconstruction, and how would this affect retrieval quality and system latency?
- **Basis in paper:** [explicit] The conclusion states: "Our results suggest several promising research directions: investigating methods for dynamic evidence corpus updates."
- **Why unresolved:** The current implementation uses a static FAISS index built before inference, with no mechanism described for incremental updates when new evidence becomes available.
- **What evidence would resolve it:** An evaluation comparing retrieval quality and runtime when incrementally updating the index versus full reconstruction, measured across multiple update frequencies and corpus sizes.

### Open Question 2
- **Question:** How effectively does DeReC transfer to multilingual verification scenarios, and does the evidence retrieval quality degrade for lower-resource languages?
- **Basis in paper:** [explicit] The conclusion lists "exploring techniques for handling multilingual verification scenarios" as a promising research direction.
- **Why unresolved:** The paper evaluates only on English-language datasets (RAWFC and LIAR-RAW), and the embedding models used (gte-Qwen2 and nomic-embed) have varying coverage across languages.
- **What evidence would resolve it:** Cross-lingual transfer experiments on multilingual fact-checking benchmarks, with analysis of retrieval precision across languages with different resource levels.

### Open Question 3
- **Question:** Can lightweight explanation generation be incorporated into DeReC while preserving its computational efficiency advantages over LLM-based approaches?
- **Basis in paper:** [explicit] The conclusion mentions "developing lightweight explanation generation methods that maintain both computational efficiency and interpretability."
- **Why unresolved:** The paper explicitly notes as a limitation that "it does not generate natural language explanations for its decisions. This limitation may reduce its utility in contexts where detailed explanations are necessary for human review."
- **What evidence would resolve it:** A hybrid architecture that adds explanation generation with measured overhead remaining below LLM baselines, evaluated on both accuracy and explanation quality metrics.

### Open Question 4
- **Question:** How does evidence corpus bias or incompleteness affect DeReC's veracity predictions, and can retrieval augmentation from external sources mitigate these effects?
- **Basis in paper:** [inferred] The limitations section states: "The quality of retrieval is heavily dependent on the evidence corpus; incomplete or biased corpora can lead to suboptimal results."
- **Why unresolved:** The paper assumes the provided evidence corpora are sufficient but does not evaluate robustness when evidence is missing, contradictory, or systematically biased.
- **What evidence would resolve it:** Controlled experiments with artificially corrupted corpora (removed evidence, injected bias) measuring prediction degradation, and evaluation with external retrieval sources.

## Limitations
- No direct comparisons against contemporary RAG systems or smaller LLM variants optimized for efficiency
- Does not address cold-start problem for claims without relevant evidence in corpus
- Does not quantify FAISS memory requirements as corpus size scales for production deployment
- Absence of human evaluation on retrieved evidence quality

## Confidence
- **High Confidence:** The core retrieval pipeline (embedding → FAISS → classification) is well-specified and reproducible. The runtime improvements are credible given the architectural differences between dense retrieval and autoregressive generation.
- **Medium Confidence:** The F1 score improvements over baselines are plausible but depend heavily on the specific baseline implementations and hyperparameters used. Without access to the exact baseline code, minor performance variations are expected.
- **Low Confidence:** Claims about "state-of-the-art" performance are difficult to verify without broader benchmark comparisons. The paper's focus on two datasets limits generalizability to other domains or languages.

## Next Checks
1. **Retrieval Quality Assessment:** Manually evaluate 100 claim-evidence pairs from the top-10 retrievals to measure precision@k. Compare this against automated metrics to identify potential gaps in relevance assessment.
2. **Memory Scalability Analysis:** Profile FAISS index memory usage as corpus size scales from 100K to 1M sentences. Document the point at which memory constraints necessitate approximate indices (e.g., IVF, HNSW) and quantify the accuracy trade-off.
3. **Cross-Dataset Generalization:** Apply DeReC to an external fact-checking dataset (e.g., FEVER or SciFact) to test domain transfer. Measure performance degradation and analyze whether the retrieval mechanism adapts to new terminology and evidence structures.