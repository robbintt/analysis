---
ver: rpa2
title: Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase
  Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow
  Labs, and Open-Source Models on an Independent Pediatric Dataset
arxiv_id: '2505.23030'
source_url: https://arxiv.org/abs/2505.23030
tags:
- systems
- clinical
- entities
- disease
- assertion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reliability of modern NLP systems for
  annotating pediatric chest radiography reports by comparing four commercial clinical
  NLP tools (AWS, Google, Azure, John Snow Labs) and two specialized models (CheXpert,
  CheXbert) on 95,008 pediatric CXR reports. The evaluation focuses on entity extraction
  and assertion detection performance across 13 disease categories.
---

# Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset

## Quick Facts
- arXiv ID: 2505.23030
- Source URL: https://arxiv.org/abs/2505.23030
- Reference count: 0
- Primary result: Entity extraction accuracy varied widely (50-76%) across six NLP systems on pediatric CXR reports

## Executive Summary
This study evaluates six modern NLP systems for annotating pediatric chest radiography reports, comparing four commercial tools (AWS, Google, Azure, John Snow Labs) and two specialized models (CheXpert, CheXbert) on 95,008 pediatric CXR reports. The evaluation focuses on entity extraction and assertion detection across 13 disease categories. Results reveal significant variability in performance, with the highest-performing system achieving 76% accuracy while others ranged from 50-72%. The study highlights considerable differences in how systems interpret medical uncertainty and linguistic nuances in radiology reports, emphasizing the need for careful validation before clinical deployment.

## Method Summary
The study analyzed 95,008 pediatric chest radiography reports from a large academic pediatric hospital, extracting entities and assertion statuses from findings and impression sections using six NLP systems. Systems were compared using majority voting to create a pseudo-ground truth, with performance measured as assertion accuracy per disease category. Entity categories were standardized to 13 CheXpert disease categories, and assertions were normalized to three categories (positive, negative, uncertain) using system-specific thresholds.

## Key Results
- Entity extraction accuracy varied significantly (50-76%) across systems for disease-related findings
- Assertion detection showed the greatest variability when disease presence was detected
- AWS achieved highest overall accuracy at 76%, while CheXpert and CheXbert reached 56%
- CheXpert achieved highest accuracy for pleural effusion (72%) but lowest for consolidation (14%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based and hybrid NLP architectures can extract clinically relevant entities from unstructured radiology reports, but extraction breadth varies significantly by system design.
- Mechanism: Named Entity Recognition (NER) models identify and classify text spans into predefined categories using learned contextual embeddings or ontology-guided rules.
- Core assumption: Disease-related findings in CXR reports follow recognizable linguistic patterns that transfer across institutions and populations.
- Evidence anchors: Entities and assertion statuses from findings and impression sections were extracted by NLP systems; SP extracted 49,688 unique entities while GC extracted only 16,477.

### Mechanism 2
- Claim: Assertion detection determines clinical status (positive, negative, uncertain) of extracted entities, but systems lack standardized uncertainty definitions.
- Mechanism: Assertion classifiers analyze linguistic context surrounding recognized entities using pattern matching or bidirectional context analysis.
- Core assumption: Negation and uncertainty expressions in radiology reports follow consistent syntactic patterns detectable without domain-specific calibration.
- Evidence anchors: Greatest variability observed in assertion classification when disease presence was detected; uncertain entities varied between 1.4% (AWS) to 32.6% (SP).

### Mechanism 3
- Claim: Majority-voting ensemble over multiple NLP systems can approximate ground truth for comparative evaluation when manual annotation is infeasible.
- Mechanism: Consensus pseudo-ground truth assigns each entity the assertion label agreed upon by the majority of systems, with disagreement defaulting to uncertain.
- Core assumption: System errors are uncorrelated enough that majority vote converges toward correct labels.
- Evidence anchors: Model-specific performance evaluated by comparing assertions to consensus pseudo ground truth; cases without clear majority assigned to uncertain category.

## Foundational Learning

- Concept: **Named Entity Recognition (NER)**
  - Why needed here: Understanding how each commercial system defines entity categories (7 to 36 categories) is prerequisite to normalizing outputs for fair comparison.
  - Quick check question: Can you explain why SparkNLP's `Disease_Syndrome_Disorder` and Azure's `DIAGNOSIS` categories might capture different text spans for the same clinical finding?

- Concept: **Assertion/Negation Detection**
  - Why needed here: Clinical utility depends on correctly identifying whether a finding is present, absent, or uncertain; this study found 50-76% accuracy across systems.
  - Quick check question: How would you standardize AWS's numerical confidence scores with Google's categorical assertions (`likely`, `unlikely`, `somewhat_likely`) for direct comparison?

- Concept: **Fleiss' Kappa for Multi-Rater Agreement**
  - Why needed here: Quantifies inter-system agreement across 6 NLP models; Kappa dropped from 0.68 (including absent) to 0.35 (excluding absent), revealing assertion instability.
  - Quick check question: Why does excluding "absent" predictions reduce agreement, and what does this suggest about disease detection vs. assertion classification difficulty?

## Architecture Onboarding

- Component map: Raw CXR Reports (95,008) → Section Extraction (Findings, Impression) → [Parallel] 6 NLP Systems → Post-processing → Evaluation
- Critical path: Assertion standardization (Table 2) → this is where system outputs become comparable
- Design tradeoffs: Pseudo ground truth vs. manual annotation (scalable but may not reflect clinical accuracy); RegEx mapping vs. learned entity linking (interpretable but brittle); Cloud APIs vs. local models (convenience vs. transparency)
- Failure signatures: High entity extraction count but low unique entities suggests over-extraction; very low uncertain assertion rates may indicate poor uncertainty calibration; large accuracy drops for specific diseases suggest vocabulary gaps
- First 3 experiments:
  1. Assertion threshold calibration: Vary AWS confidence score thresholds and measure impact on accuracy against consensus
  2. Error pattern analysis by disease category: Review 50 discrepant cases for consolidation vs. pleural effusion to identify systematic patterns
  3. Ensemble assertion voting: Test whether weighted ensemble improves overall pseudo-ground-truth quality

## Open Questions the Paper Calls Out
None

## Limitations
- Pseudo ground truth derived from majority voting may not reflect clinical accuracy due to potential systematic biases
- Limited generalizability to adult populations or non-pediatric institutions given institutional documentation style dependency
- Missing complete implementation details for entity normalization and lemmatization pipelines
- Uncertainty thresholds and assertion mappings were empirically set without clinical validation

## Confidence
- High confidence: Comparative methodology and relative performance rankings between systems
- Medium confidence: Absolute accuracy numbers due to pseudo ground truth limitations
- Low confidence: Stability of results across different institutions or patient populations

## Next Checks
1. Replicate findings using MIMIC-CXR or CheXpert datasets to test institutional generalizability
2. Conduct blinded radiologist review of 100 discrepant cases to validate pseudo ground truth quality
3. Test assertion threshold calibration across different pediatric age groups to identify population-specific performance variations