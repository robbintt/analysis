---
ver: rpa2
title: Automatic Legal Writing Evaluation of LLMs
arxiv_id: '2504.21202'
source_url: https://arxiv.org/abs/2504.21202
tags:
- legal
- evaluation
- human
- score
- exam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating open-ended legal
  writing tasks, which traditionally require costly human expertise. To tackle this,
  the authors introduce oab-bench, a benchmark of 105 questions from recent Brazilian
  Bar Exams, complete with official grading guidelines.
---

# Automatic Legal Writing Evaluation of LLMs
## Quick Facts
- arXiv ID: 2504.21202
- Source URL: https://arxiv.org/abs/2504.21202
- Reference count: 37
- Primary result: Introduces oab-bench, a benchmark of 105 Brazilian Bar Exam questions with official grading guidelines, and demonstrates that frontier LLMs can serve as reliable automated evaluators for legal writing tasks.

## Executive Summary
This paper addresses the challenge of evaluating open-ended legal writing tasks, which traditionally require costly human expertise. The authors introduce oab-bench, a benchmark of 105 questions from recent Brazilian Bar Exams, complete with official grading guidelines. They evaluate four LLMs on this benchmark, finding Claude-3.5 Sonnet performs best with an average score of 7.93/10, passing all exams. Additionally, they explore using LLMs as automated judges, demonstrating that frontier models like OpenAI's o1 achieve strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the subjective nature of legal writing assessment.

## Method Summary
The authors constructed oab-bench by collecting 105 recent Brazilian Bar Exam questions along with their official grading guidelines. They evaluated four LLMs (Claude-3.5 Sonnet, GPT-4, Llama-3.1, and Mixtral) on this benchmark using a two-phase approach: first, generating responses to the exam questions; second, using another LLM as an automated judge to score these responses based on the official rubrics. The automated evaluation methodology was validated by comparing LLM scores against human-graded approved exams, showing strong correlation. The study focused on open-ended legal writing tasks that require complex reasoning and argumentation.

## Key Results
- Claude-3.5 Sonnet achieved the highest average score of 7.93/10 and passed all Brazilian Bar Exams
- OpenAI's o1 model demonstrated strong correlation with human scores when evaluating approved exam responses
- The automated evaluation approach shows promise as a cost-effective alternative to human grading for legal writing tasks

## Why This Works (Mechanism)
The methodology works because it leverages the reasoning capabilities of frontier LLMs to both generate legal arguments and evaluate them against established rubrics. By using official grading guidelines from Brazilian Bar Exams, the approach provides standardized criteria for assessment. The strong performance of Claude-3.5 Sonnet suggests that current frontier models have sufficient legal reasoning capabilities to handle complex open-ended legal writing tasks. The automated evaluation approach is effective because it applies consistent criteria across all responses, potentially reducing human bias and variability.

## Foundational Learning
**Brazilian Bar Exam Structure** - Understanding the format and requirements of Bar Exams is essential for contextualizing the benchmark. Quick check: Review sample questions from oab-bench to understand the types of legal reasoning required.

**Legal Writing Assessment Criteria** - Knowledge of how legal writing is evaluated (argument structure, legal accuracy, reasoning quality) is needed to interpret results. Quick check: Compare the official grading guidelines with typical LLM evaluation metrics.

**LLM Evaluation Methodologies** - Understanding automated evaluation approaches for open-ended tasks is crucial for assessing the validity of the results. Quick check: Review the correlation analysis between LLM and human scores.

## Architecture Onboarding
**Component Map**: Bar Exam Questions -> LLM Generation -> Official Grading Guidelines -> LLM Evaluation -> Human Correlation Analysis
**Critical Path**: Question generation → Model response generation → Automated scoring → Human score comparison
**Design Tradeoffs**: Uses single jurisdiction (Brazil) for standardization vs. limited generalizability; automated evaluation for scalability vs. potential model bias; correlation analysis on approved exams only vs. incomplete validation
**Failure Signatures**: Poor correlation with human scores indicating inadequate legal reasoning; model bias toward certain legal arguments; inability to handle novel legal scenarios
**First Experiments**: 1) Test automated evaluation on failed exam responses, 2) Cross-validate with human graders on same responses, 3) Apply methodology to different legal jurisdictions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions for future research.

## Limitations
- Focus on Brazilian legal system limits generalizability to other jurisdictions
- Benchmark based solely on Bar Exam questions, representing a narrow slice of legal writing tasks
- Automated evaluation validated only on approved exams, not the full score distribution
- Does not address potential bias in model-generated judgments
- Lacks inter-annotator agreement measurements among human graders for baseline comparison

## Confidence
- **High confidence**: The benchmark construction methodology and dataset compilation are clearly described and reproducible.
- **Medium confidence**: The performance ranking of models and the finding that Claude-3.5 Sonnet passes all exams are reliable, though absolute score differences may vary with different human graders.
- **Medium confidence**: The automated evaluation approach shows promise but requires broader validation across different exam difficulty levels and jurisdictions.

## Next Checks
1. Test the automated evaluation methodology on failed/disqualified exam responses to establish reliability across the full score distribution
2. Replicate the study using Bar Exam questions from different jurisdictions (US, UK, EU) to assess generalizability
3. Conduct a human evaluation study measuring inter-annotator agreement among multiple legal experts to establish baseline variability for comparison with automated judgments