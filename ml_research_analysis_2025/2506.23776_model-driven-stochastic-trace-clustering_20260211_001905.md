---
ver: rpa2
title: Model-driven Stochastic Trace Clustering
arxiv_id: '2506.23776'
source_url: https://arxiv.org/abs/2506.23776
tags:
- clustering
- process
- cluster
- entropic
- trace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model-driven trace clustering method that
  optimizes stochastic process models by minimizing entropic relevance (ER), a stochastic
  conformance metric based on directly-follows probabilities. The method assigns traces
  to clusters based on both structural alignment and likelihood of originating from
  a cluster's stochastic process model, producing more interpretable clusters with
  clearer control-flow patterns.
---

# Model-driven Stochastic Trace Clustering

## Quick Facts
- **arXiv ID:** 2506.23776
- **Source URL:** https://arxiv.org/abs/2506.23776
- **Reference count:** 40
- **Primary result:** Introduces model-driven trace clustering that minimizes entropic relevance (ER) to create stochastically coherent process model clusters with superior graph simplicity.

## Executive Summary
This paper presents a novel trace clustering method that leverages stochastic process models to group similar process executions. Unlike traditional approaches that rely solely on structural distance metrics, this method assigns traces to clusters based on how well they conform to a cluster's stochastic Directly-Follows Graph (DFG), optimizing for local transition probabilities. The approach minimizes Entropic Relevance (ER), a measure of information cost based on transition probabilities, resulting in clusters with clearer control-flow patterns and reduced graph density. Experiments demonstrate that this method produces more interpretable models with significantly lower ER scores and entropy compared to traditional clustering techniques, though it shows a trade-off in traditional fitness and precision metrics.

## Method Summary
The method implements Entropic Clustering (EC) that iteratively builds stochastic process models for each cluster and assigns traces based on their likelihood of originating from these models. Starting with k initial seeds selected via ++ initialization, the algorithm processes variants in descending frequency order, assigning each to the cluster where it yields the lowest ER increase. After each assignment, the cluster's DFG is updated with the new trace's transition counts, creating a dynamic attractor that evolves with the data. ER is calculated as the negative log of the trace probability derived from transition probabilities in the DFG, with an underflow floor of 1e-10. The method also includes ECsplit, which starts with one cluster and iteratively splits the highest-ER cluster until reaching the desired k clusters.

## Key Results
- Produces significantly lower ER scores and entropy compared to traditional clustering methods
- Generates more interpretable clusters with clearer control-flow patterns and reduced graph density
- Shows superior stochastic coherence while maintaining computational efficiency with linear scalability
- Achieves high stochastic precision but demonstrates a trade-off with traditional fitness and precision metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing Entropic Relevance (ER) creates clusters that maximize the probability of observed transitions within a stochastic Directly-Follows Graph (DFG).
- **Mechanism:** The algorithm assigns a trace to the cluster where the trace's specific sequence of activities yields the highest probability (lowest compression cost) based on the cluster's current transition probabilities ($P(b|a)$). This groups traces that utilize the same "high-frequency highways" of the process.
- **Core assumption:** Traces belonging to the same underlying process variant share similar local transition probabilities, and minimizing the information cost of these local transitions leads to globally coherent process models.
- **Evidence anchors:**
  - [abstract] "optimizing stochastic process models by minimizing entropic relevance (ER)... assigns traces to clusters based on... likelihood of originating from a cluster's stochastic process model."
  - [section 4.1] "A lower compression cost (i.e., a lower ER score) indicates a better fit between the model and the event log... calculated as: $-log_2(p)$."
  - [corpus] Weak direct evidence in provided corpus; related work "Stochastic Alignments" supports the general paradigm of matching traces to stochastic models but not this specific clustering objective.
- **Break condition:** If the process logic relies heavily on non-local dependencies (e.g., global compliance rules) rather than local directly-follows probabilities, this mechanism may group traces that are structurally distinct but locally similar.

### Mechanism 2
- **Claim:** Iteratively updating the stochastic model (DFG) upon each trace assignment shifts the cluster's probability distribution to accommodate the newly added behavior, acting as a dynamic attractor.
- **Mechanism:** Unlike static distance metrics, this method hypothetically adds a trace to a cluster, updates the edge counts (and thus transition probabilities), and measures the resulting ER. This "test-and-set" approach allows the cluster centroid (the DFG) to evolve dynamically, ensuring that subsequent assignments are evaluated against a representation that includes the latest data.
- **Core assumption:** The order of assignment (processing variants by decreasing prevalence) allows the clusters to stabilize around dominant behaviors before absorbing noise or rare variants.
- **Evidence anchors:**
  - [section 4.2] "After assignment, that cluster's DFG is updated by adjusting its node and edge counts accordingly... optimizing the DFG graph to have as few low frequency DFs as possible."
  - [section 1] "Model-driven trace clustering... iteratively discovering a process model for each cluster and assigning traces based on how well they conform to these models."
  - [corpus] "Fast Clustering of Categorical Big Data" discusses initialization sensitivity in K-Modes, which parallels the stability concerns of this iterative update, though the specific stochastic update is unique to this paper.
- **Break condition:** If the initialization seeds are statistically outliers or "garbage" traces, the iterative update may lock in a low-quality attractor state early, degrading the entire cluster's coherence.

### Mechanism 3
- **Claim:** Optimizing for stochastic precision implicitly filters out low-probability edges (noise), resulting in simpler, less dense graphs ("spaghetti reduction").
- **Mechanism:** By penalizing low-probability transitions (which contribute high entropy), the algorithm naturally favors clusters where traces follow a consistent, high-probability path. This forces outlier transitions into separate clusters or minimizes their impact on the main cluster's model, thereby lowering graph density and entropy.
- **Core assumption:** Model interpretability is primarily a function of graph simplicity (low arc degree) and the prominence of high-probability paths, rather than the complete coverage of every observed edge.
- **Evidence anchors:**
  - [abstract] "The method... producing more interpretable clusters with clearer control-flow patterns... yields superior stochastic coherence and graph simplicity."
  - [section 1] "By promoting clusters with shared, high-probability directly-follows relations, it enhances model clarity... supports integration with edge frequency filtering to mitigate the 'spaghetti model' problem."
  - [corpus] No direct evidence in the provided corpus regarding the specific "spaghetti" reduction mechanism via stochastic metrics.
- **Break condition:** If the analysis goal requires strict compliance checking of rare but critical deviations (e.g., fraud detection), this mechanism might oversimplify the model by "hiding" these traces in the probability mass or secondary clusters.

## Foundational Learning

- **Concept: Directly-Follows Graphs (DFG) as Stochastic Models**
  - **Why needed here:** This is the core representation used. You must understand that a DFG is not just a structural map but a probabilistic one where the weight of an edge $(A \to B)$ divided by the sum of outgoing edges from $A$ defines the transition probability.
  - **Quick check question:** If Activity A has 100 occurrences and is followed by Activity B 90 times and Activity C 10 times, what is the information cost (ER contribution) of the transition $A \to C$?

- **Concept: Information Theory (Shannon Entropy)**
  - **Why needed here:** The cost function (Entropic Relevance) relies on the concept of "surprise" or compression cost. Understanding that low probability = high cost ($-\log_2(p)$) is essential to grasp why the algorithm avoids clusters with unlikely transitions.
  - **Quick check question:** Why does the ER score approach infinity as the probability of a trace approaches zero?

- **Concept: Process Model Conformance (Fitness vs. Precision)**
  - **Why needed here:** The paper highlights a trade-off. You need to distinguish between "Fitness" (can the model reproduce the log?) and "Precision" (does the model allow too much behavior not in the log?) to understand why this method improves stochastic precision but may lower traditional fitness.
  - **Quick check question:** If a model allows a trace that never appears in the log, which dimension of conformance is compromised?

## Architecture Onboarding

- **Component map:** Event Log -> Variant Log (deduplication) -> ++ Initialization (Algorithm 4) -> Clustering Engine (Algorithm 2) -> DFG Updates -> Evaluation Layer
- **Critical path:** The **ER Calculation** inside the assignment loop is the computational bottleneck. Ensure the implementation of transition probability lookups and log calculations is optimized. The paper claims linear scalability ($O(N)$), but inefficient probability updates could degrade this.
- **Design tradeoffs:**
  - **Stochastic vs. Structural:** The system is tuned for stochastic coherence (probability matching). It sacrifices traditional "token replay fitness" on Petri nets. Do not use this if your downstream task requires perfect structural alignment (e.g., generating code from the model).
  - **Initialization Cost:** The `++` initialization improves quality but requires pre-computing pairwise ER distances for seeds, which adds overhead compared to random seeding.
- **Failure signatures:**
  - **Degenerate Clusters:** One massive cluster and many tiny ones. This often indicates initialization failure (seeds too similar) or a lack of distinct stochastic variants in the log.
  - **High Sensitivity:** Results changing drastically between runs implies the random component of initialization (first seed) is dominating; increase the strictness of seed selection or use the Split variant.
- **First 3 experiments:**
  1. **Validation Run:** Run the "Elbow Experiment" (Fig B.5) on a known log (e.g., BPIC12) to verify that the ER score decreases as cluster count $k$ increases, confirming the optimization loop is functional.
  2. **Ablation Study:** Compare `EC` (Standard) vs. `EC Split` vs. `Random Initialization` on a log with clear separate processes (e.g., synthetic log with 3 distinct loops) to measure the impact of the initialization strategy.
  3. **Visual Inspection:** Generate the DFG for a cluster created by Entropic Clustering vs. Frequency-based clustering. Visually confirm that the Entropic Clustering DFG has fewer "crossing edges" and lower density (spaghetti reduction).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do specific process behaviors and structural characteristics systematically influence the relative performance of Entropic Clustering compared to traditional methods?
- **Basis in paper:** [explicit] The authors propose conducting "a larger-scale experiment... using controlled synthetic data" to "systematically analyze which types of process behaviors and challenges affect different clustering techniques."
- **Why unresolved:** The current evaluation relies solely on real-life event logs where the ground truth process characteristics are uncontrolled, making it difficult to generalize findings to specific structural challenges.
- **What evidence would resolve it:** Performance metrics collected from experiments on synthetic logs with manipulated variables for parallelism, loop complexity, and noise levels.

### Open Question 2
- **Question:** Can the method be extended to dynamically determine the optimal number of clusters based on stochastic quality thresholds?
- **Basis in paper:** [explicit] The paper notes that "determining the correct number of clusters upfront is often impractical" and suggests exploring "a variant that dynamically adjusts the number of clusters, based on a maximum ER threshold."
- **Why unresolved:** The current algorithm requires a predefined number of clusters ($k$) determined via a preliminary manual analysis.
- **What evidence would resolve it:** An implementation of a recursive splitting mechanism that terminates when cluster entropy falls below a specific threshold, removing the need for manual $k$ selection.

### Open Question 3
- **Question:** Does incorporating explicit outlier detection mechanisms improve the stochastic coherence of the resulting clusters?
- **Basis in paper:** [explicit] The authors identify "introducing a robust version of the algorithm that detects and handles outliers" as a "simple but useful extension," noting outliers likely skew current results.
- **Why unresolved:** The current approach assigns all traces to a cluster, potentially distorting the transition probabilities of the cluster's Directly-Follows Graph.
- **What evidence would resolve it:** Comparative results showing improved Entropic Relevance (ER) scores and model simplicity when outlier traces are quarantined or weighted differently.

## Limitations
- The method's reliance on local directly-follows probabilities makes it vulnerable to processes with strong non-local dependencies (e.g., global compliance rules, resource constraints).
- Initialization sensitivity remains a practical concern, with potential for unstable cluster assignments when using random seeding.
- The trade-off between stochastic precision and traditional fitness metrics may limit applicability for compliance checking and other use cases requiring strict structural alignment.

## Confidence
- **High Confidence:** The core mechanism of minimizing entropic relevance to create stochastically coherent clusters is well-supported by experimental results and aligns with information theory principles.
- **Medium Confidence:** The trade-off between stochastic precision and traditional fitness is well-demonstrated, though practical implications for specific use cases need more exploration.
- **Medium Confidence:** The computational efficiency claims are theoretically sound but require empirical verification with the actual implementation.

## Next Checks
1. **Initialization Sensitivity Analysis:** Run the same experiment (e.g., BPIC12) with random initialization vs. "++" initialization multiple times to quantify the variance in ER scores and cluster assignments. Document the conditions under which the method becomes unstable.

2. **Edge Case Testing:** Create synthetic logs with known non-local dependencies (e.g., "if activity X appears, then activity Y must be the last activity") and test whether the clustering method incorrectly groups traces based on local similarities while ignoring these global constraints.

3. **Scalability Benchmark:** Test the method on progressively larger logs (10K, 100K, 1M traces) while measuring both wall-clock time and memory usage. Compare against the claimed linear scalability and identify the point where performance degrades significantly.