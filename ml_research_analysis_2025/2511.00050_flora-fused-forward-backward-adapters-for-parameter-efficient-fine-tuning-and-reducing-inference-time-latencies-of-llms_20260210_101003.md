---
ver: rpa2
title: 'FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning
  and reducing inference-time latencies of LLMs'
arxiv_id: '2511.00050'
source_url: https://arxiv.org/abs/2511.00050
tags:
- adapters
- lora
- adapter
- tasks
- fused
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FLoRA, a family of fused forward-backward adapters
  (FFBA) for parameter-efficient fine-tuning (PEFT) of LLMs that aim to improve both
  accuracy and inference-time latency. The key idea is to fuse adapter computations
  into existing model layers, eliminating separate adapter calls that introduce latency.
---

# FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs

## Quick Facts
- arXiv ID: 2511.00050
- Source URL: https://arxiv.org/abs/2511.00050
- Authors: Dhananjaya Gowda; Seoha Song; Junhyun Lee; Harshith Goka
- Reference count: 7
- One-line primary result: FFBA reduces LoRA's TPOT overhead by 21-30% (1B) and 31-48% (3B) while matching or exceeding accuracy on summary/dialogue tasks

## Executive Summary
FLoRA introduces a family of fused forward-backward adapters (FFBA) that address the latency overhead of parameter-efficient fine-tuning in LLMs. By fusing adapter computations into existing projection layers rather than invoking them as separate GPU kernel calls, FFBA achieves significant inference-time speedups while maintaining or improving task accuracy. The method combines LoRA-style low-rank adaptation with architectural fusion, placing forward and backward adapters within the base model's projection layers.

## Method Summary
FLoRA modifies standard LoRA adapters by concatenating base model weights with forward adapter matrices into single fused operations. The architecture uses Fused Forward Layers (FFL) that combine `[W; A]` matrices and Fused Forward-Backward Layers (FFBL) that combine `[W B; A C]` matrices. Forward adapters are placed in QKV projections for MHA blocks and up/gate projections for FFN blocks, while backward adapters are fused into output/down projections. The method maintains the low-rank bottleneck structure critical for performance while eliminating the kernel launch overhead that makes standard LoRA adapters disproportionately slow at inference time.

## Key Results
- FFBA reduces LoRA's time-per-output-token (TPOT) overhead by 21-30% for 1B models and 31-48% for 3B models
- FFBA performs significantly better than LoRA on summary and dialogue tasks while matching or marginally outperforming LoRA on commonsense and math reasoning tasks
- FFA (forward-only) variant consistently underperforms both FFBA and LoRA, confirming the importance of the low-rank bottleneck structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing adapter weight matrices with base model weights into single matrix operations reduces inference latency caused by GPU kernel launch overhead.
- **Mechanism:** Standard LoRA computes `WX` and `AX` as separate operations. FLoRA concatenates `[W; A]` into a single matrix, enabling one fused multiply that produces both `Y` and `ΔY` simultaneously. Backward adapters are similarly fused into subsequent projection layers.
- **Core assumption:** The latency bottleneck stems primarily from multiple small matrix multiplications triggering separate GPU kernel calls, not from total FLOPs.
- **Evidence anchors:** Abstract statement about fusing adapters into projection layers; Section 1 identifying kernel overhead as 20-50% of latency; Section 3 showing concatenation formulation; related work (LoRAFusion) identifying similar kernel inefficiencies.

### Mechanism 2
- **Claim:** Maintaining a low-rank bottleneck (backward adapter) after fused forward projection preserves expressivity that is lost when using forward-only adapters.
- **Mechanism:** FFA removes the backward projection entirely, increasing rank from r to 2r in the forward path. This degrades performance. FFBA restores the backward adapter but places it in a different layer's projection, preserving the information compression/expansion dynamic.
- **Core assumption:** The low-rank bottleneck structure is important for learning efficient representations, not just parameter count.
- **Evidence anchors:** Section 3 statement about LRA bottleneck importance; Tables 1-3 showing FFA consistently underperforms FFBA and LoRA; related work on rank optimization in LoRA variants.

### Mechanism 3
- **Claim:** Shared backward adapters across multiple forward adapters maintain performance with parameter efficiency.
- **Mechanism:** In MHA blocks, three forward adapters (Q, K, V projections) share one backward adapter in the output projection. In FFN blocks, two forward adapters (up, gate projections) share one backward adapter in the down projection.
- **Core assumption:** A single backward adapter can effectively process combined adaptation signals from multiple forward adapters.
- **Evidence anchors:** Section 3 schematic showing shared backward adapter architecture; Section 4.3.1 showing FFBA (AorB) variant performs comparably to full FFBA while reducing operations; related work on shared parameter structures across tasks.

## Foundational Learning

- **Concept:** LoRA (Low-Rank Adaptation) fundamentals
  - **Why needed here:** FLoRA modifies LoRA's structure; understanding `ΔW = BA` decomposition is prerequisite to understanding what gets fused where.
  - **Quick check question:** Can you explain why LoRA uses two low-rank matrices (A and B) instead of one?

- **Concept:** GPU kernel fusion and memory bandwidth
  - **Why needed here:** The core value proposition rests on reducing kernel launch overhead; without this context, the latency improvements appear magical.
  - **Quick check question:** Why might two small matrix multiplications be slower than one large matrix multiplication even if total FLOPs are identical?

- **Concept:** Transformer projection layer topology
  - **Why needed here:** FFBA requires understanding which projections exist in MHA (Q, K, V, O) and FFN (up, gate, down) blocks to correctly place forward and backward adapters.
  - **Quick check question:** In a standard transformer, which projections are followed by a residual connection addition?

## Architecture Onboarding

- **Component map:** Input → FFL (QKV projections) → sum augmented outputs → FBL (output projection) → FFL (FFN up/gate) → sum → FBL (FFN down) → output

- **Critical path:** Input flows through fused forward layers in QKV projections, augmented outputs are summed, then processed by fused backward layer in output projection, followed by fused forward layers in FFN up/gate projections, summation, and final fused backward layer in FFN down projection.

- **Design tradeoffs:**
  - **FFBA (A&B) vs FFBA (AorB):** A&B uses both forward and backward adapters in FFBL; AorB uses only backward adapter in output/down projections. AorB reduces latency but may slightly impact performance.
  - **FFBA vs FFA:** FFA removes backward adapter entirely; best latency but significant accuracy drop.
  - **Non-linearity (FFBA-Relu):** Paper found adding ReLU between forward/backward adapters did not improve results consistently; adds latency.
  - **QG-Add variant:** Retains repeat-and-add only for Query and Gate projections; balances performance and latency.

- **Failure signatures:**
  - FFA significantly underperforms LoRA on reasoning tasks → suggests low-rank bottleneck is essential
  - Non-linearity variants show inconsistent improvements → suggests adapter non-linearity may be redundant with base model's activations
  - 3B model shows less relative improvement on math tasks → base model may already be well-suited; adapter provides minimal signal

- **First 3 experiments:**
  1. **Baseline latency comparison:** Implement standard PEFT-LoRA, partially-fused LoRA (pf-LoRA), and FFBA (AorB) on same hardware. Measure TPOT overhead relative to base model to validate 21-30% reduction claim.
  2. **Ablation on backward adapter placement:** Test FFBA with backward adapter in output projection vs. in a separate serial operation to confirm fusion benefit is architectural, not just from low-rank structure.
  3. **Task-specific variant selection:** Compare FFBA (QG-Add) vs. FFBA (AorB) on your target task type (summary/dialogue favors QG-Add; reasoning tasks may prefer AorB for lower latency).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does adding non-linearity (ReLU) between forward and backward projections cause FFBA to underperform compared to non-linear variants?
- **Basis in paper:** "It can also be seen that the benefit of adding a non-linearity (FFBA-Relu) in between the forward and backward projections of the adapter is not very tangible. On the contrary, it appears to under-perform the other adapters without any non-linearity which is a bit surprising and needs further investigation."
- **Why unresolved:** The authors expected non-linearity to help (following parallel adapter conventions), but observed the opposite; no theoretical explanation is provided.
- **What evidence would resolve it:** Ablation studies with different non-linearity types (GELU, SiLU) and analysis of gradient flow through the fused adapter structure.

### Open Question 2
- **Question:** How does FLoRA perform on larger cloud-based LLMs (e.g., 70B+ parameter models)?
- **Basis in paper:** "The experiments are limited to small or moderately sized LLMs that could be candidates for ondevice deployment. Experiments with huge cloud based LLMs is not within the scope of this study."
- **Why unresolved:** The fusion benefits may differ at larger scales where memory bandwidth and kernel launch overheads have different relative impacts.
- **What evidence would resolve it:** Benchmarks on Llama-70B or similar models measuring both TPOT reduction and accuracy across the same task categories.

### Open Question 3
- **Question:** How do different quantization methods affect FFBA accuracy and latency gains compared to LoRA?
- **Basis in paper:** "Studying the effect of different quantization methods on these adapter accuracies are part of our future scope."
- **Why unresolved:** Fused operations may interact differently with quantization noise than separate adapter calls, potentially altering the accuracy-latency tradeoff.
- **What evidence would resolve it:** Experiments with INT8/INT4 quantization (e.g., GPTQ, AWQ) measuring accuracy degradation and latency on both FFBA and LoRA baselines.

## Limitations

- **Hardware dependency:** The reported 21-30% and 31-48% TPOT improvements could vary significantly depending on whether target hardware already employs kernel fusion optimizations or has different memory bandwidth characteristics.
- **Ablation gaps:** The paper doesn't explore rank variation beyond r=32 or alternative backward adapter placements, leaving questions about generalizability across different task domains and LoRA configurations.
- **Variant performance:** FFBA-AorB matching FFBA on reasoning tasks while offering lower latency, but this variant's performance on non-reasoning tasks (summary/dialogue) remains unclear from presented results.

## Confidence

**High confidence** in the mechanism that fusing forward adapters reduces kernel launch overhead - this follows established GPU optimization principles and is supported by both theoretical formulation and related work (LoRAFusion).

**Medium confidence** in the claim that maintaining backward adapters in fused layers preserves accuracy - while ablation results show FFA underperforms FFBA and LoRA, the paper doesn't explore alternative backward adapter placements or shared-backward designs beyond what's tested.

**Low confidence** in the generalizability of latency improvements across hardware/software stacks - the kernel fusion benefits depend heavily on the specific inference framework, GPU architecture, and whether existing optimizations already address this bottleneck.

## Next Checks

1. **Hardware stack dependency test**: Implement the same FFBA variants on both NVIDIA A100 and H100 GPUs using different inference frameworks (PyTorch native vs. vLLM) to measure how much the 21-30%/31-48% latency reduction claims vary across hardware and software optimizations.

2. **Rank sensitivity analysis**: Repeat the key experiments with ranks r=8, 16, 64, and 128 to determine whether the fusion benefits and accuracy preservation hold across the full range of practical LoRA configurations, particularly for tasks showing largest performance gaps.

3. **Kernel profiling validation**: Use NVIDIA Nsight Systems or similar profiling tools to verify that FFBA implementations actually reduce kernel launches as claimed, and measure the actual FLOPs saved vs. the kernel overhead reduction to quantify the true bottleneck being addressed.