---
ver: rpa2
title: Towards Applying Large Language Models to Complement Single-Cell Foundation
  Models
arxiv_id: '2507.10039'
source_url: https://arxiv.org/abs/2507.10039
tags:
- cell
- performance
- scgpt
- type
- ember-v1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how large language models (LLMs) can complement
  single-cell foundation models like scGPT by leveraging synergies between their representations.
  Using interpretability methods and ablation studies, the authors find that text
  encoders like Ember-V1 focus on marker genes and simple gene expression patterns
  when classifying cell types, contributing to their competitive performance.
---

# Towards Applying Large Language Models to Complement Single-Cell Foundation Models

## Quick Facts
- arXiv ID: 2507.10039
- Source URL: https://arxiv.org/abs/2507.10039
- Reference count: 40
- Primary result: scMPT fusion model combining scGPT and Ember-V1 achieves stronger performance than either alone across datasets

## Executive Summary
This study explores how large language models (LLMs) can complement single-cell foundation models like scGPT by leveraging synergies between their representations. Using interpretability methods and ablation studies, the authors find that text encoders like Ember-V1 focus on marker genes and simple gene expression patterns when classifying cell types, contributing to their competitive performance. They introduce scMPT, a multimodal fusion model that combines scGPT and Ember-V1, achieving stronger and more consistent performance than either model alone across datasets. Experiments with generative LLMs, including reasoning models like o3-mini, further demonstrate the potential for LLMs to enhance scGPT's classification accuracy.

## Method Summary
The study introduces scMPT, a multimodal fusion model that combines frozen embeddings from scGPT (a single-cell foundation model) and Ember-V1 (a text encoder for cell sentences). Cell sentences are constructed by ranking genes by expression level and omitting zero-count genes. The fusion architecture uses simple concatenation of embeddings followed by dense layers, with only these layers being trainable. The approach is evaluated on multiple single-cell RNA-seq datasets for cell type classification and disease phenotype prediction tasks, comparing performance against individual models and using interpretability methods to understand model behavior.

## Key Results
- scMPT achieves stronger and more consistent performance than either scGPT or Ember-V1 alone across multiple datasets
- Interpretability methods show Ember-V1 focuses on known marker genes when classifying cell types
- Ablation studies reveal text encoders rely heavily on gene ordering by expression level, particularly the top 10% of in-context genes
- Reasoning LLMs like o3-mini can enhance scGPT's classification accuracy when used in late-stage fusion

## Why This Works (Mechanism)

### Mechanism 1: Marker Gene Knowledge Leveraged by Text Encoders
Text encoders like Ember-V1 appear to focus on known marker genes when classifying cell types, contributing to competitive performance. Interpretability methods (LIME, integrated gradients) reveal that genes with the highest attribution scores for predicting specific cell types frequently overlap with established marker genes from the PanglaoDB database. The paper reports that for each cell type in the Pancreas dataset, both methods identified multiple markers within the top 10 attribution genes, with some markers highlighted by both methods.

### Mechanism 2: Simple Gene Expression Pattern Recognition
Text encoders appear to leverage simple patterns from cell sentence structure, particularly gene ordering by expression level. Ablation studies show that shuffling gene order dramatically reduces performance (accuracy 0.906→0.334 on Aorta), while replacing gene names with hashes causes only moderate decline (0.906→0.830). Shuffling just the top 10% of in-context genes causes a disproportionately large performance drop, suggesting heavy reliance on this small subset.

### Mechanism 3: Complementary Representation Fusion
Combining text encoder and single-cell foundation model representations may capture synergistic information that improves classification consistency. scMPT concatenates embeddings from frozen scGPT and Ember-V1 encoders, then trains dense layers on top. The hypothesized complementarity is that LLMs bring marker gene knowledge and simple patterns, while scGPT captures complex gene-gene interactions from its specialized training.

## Foundational Learning

- **Cell sentences**: Textual representation of scRNA-seq data created by ranking genes by expression level
  - Why needed here: The entire approach converts scRNA-seq data to cell sentences for LLM processing
  - Quick check question: How is a "cell sentence" constructed from raw single-cell expression data, and what information might be lost in this conversion?

- **Marker genes**: Genes that are specifically expressed in certain cell types and used as identifiers
  - Why needed here: The interpretability analysis relies on marker genes as cell type-specific identifiers
  - Quick check question: What defines a marker gene, and why would their presence in attribution scores be meaningful for cell type classification?

- **Frozen encoder representations**: Using pretrained encoder weights without updating them during training
  - Why needed here: scMPT uses frozen scGPT and Ember-V1 encoders, training only fusion layers on top
  - Quick check question: What are the tradeoffs of freezing encoder weights versus fine-tuning, particularly for preserving pretrained biological knowledge?

## Architecture Onboarding

- **Component map**: scRNA-seq → cell sentence → Ember-V1 text encoder → text embeddings (frozen)
- **Component map**: scRNA-seq → scGPT encoder → gene expression embeddings (frozen)
- **Component map**: Concatenate embeddings → dense layers (4096 dim, ReLU) → softmax output

- **Critical path**: 
  1. Convert cells to cell sentences (genes in descending expression order, zero-count genes omitted)
  2. Extract embeddings from both encoders for all cells
  3. Train fusion MLP on train split; tune hyperparameters via grid search with validation split
  4. Evaluate on test split using accuracy, macro precision/recall/F1

- **Design tradeoffs**:
  - Frozen encoders reduce compute and preserve pretrained knowledge but limit domain adaptation
  - Context truncation: Ember-V1's limited context (~152 genes on average for Aorta) may lose information from lower-expressed genes
  - Simple fusion architecture deliberately simple to attribute gains to complementarity rather than capacity

- **Failure signatures**:
  - Large, inconsistent performance gaps between individual models across datasets
  - Fusion fails to improve when both component models perform poorly on a dataset
  - Ablating either encoder causes minimal performance change, suggesting redundancy

- **First 3 experiments**:
  1. Baseline establishment: Evaluate scGPT+MLP and Ember-V1+MLP individually on your target dataset to identify which encoder is stronger
  2. Single-encoder ablation: Run scMPT with each encoder zeroed out to verify both contribute meaningfully
  3. Hyperparameter sweep: Grid search learning rate (e.g., 1e-5 to 1e-3), batch size, epochs using validation split before final test evaluation

## Open Questions the Paper Calls Out

- Can large multimodal models with reasoning capabilities outperform scMPT for single-cell analysis tasks?
  - Basis: The study only tested reasoning models via a late-stage fusion pipeline, not as integrated multimodal architectures
  - What would resolve: Comparison of purpose-built multimodal models with reasoning capabilities against scMPT on benchmarks

- What alternative textual representations of single-cell data beyond cell sentences can drive stronger performance?
  - Basis: All experiments used the cell sentence format; no other textual encodings were evaluated
  - What would resolve: Systematic comparison of alternative textual representations using the same LLM encoders and evaluation metrics

- Can we predict in advance which model (LLM-based or single-cell foundation model) will perform better on a given dataset?
  - Basis: The paper observes large performance gaps between models across datasets with no clear predictive factors
  - What would resolve: Analysis correlating dataset properties with model performance to identify predictive features for model selection

## Limitations
- The study relies heavily on interpretability methods that can produce misleading attributions in high-dimensional biological data
- Ablation studies may not fully capture the complexity of how models process gene expression patterns
- The simple fusion architecture may not be optimal for capturing deeper synergies between modalities

## Confidence

- **High confidence**: scMPT achieves stronger and more consistent performance than individual models across multiple datasets; interpretability methods reveal overlap between attribution scores and known marker genes
- **Medium confidence**: Text encoders leverage simple gene expression patterns and marker gene knowledge; the complementary representation hypothesis explains performance gains
- **Low confidence**: The exact mechanisms by which LLMs and single-cell foundation models achieve complementarity are fully understood; ablation results definitively prove which features models rely on

## Next Checks

1. Validate attribution findings by comparing with alternative interpretability methods (SHAP, occlusion) and assessing consistency across multiple random seeds
2. Test whether performance improvements persist when using different fusion architectures (e.g., attention-based, early fusion) rather than simple concatenation
3. Evaluate model robustness by testing on held-out cell types or with systematic gene set perturbations to assess generalization beyond training distribution