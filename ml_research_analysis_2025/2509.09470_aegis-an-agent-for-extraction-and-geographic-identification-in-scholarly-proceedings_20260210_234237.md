---
ver: rpa2
title: 'AEGIS: An Agent for Extraction and Geographic Identification in Scholarly
  Proceedings'
arxiv_id: '2509.09470'
source_url: https://arxiv.org/abs/2509.09470
tags:
- papers
- agent
- system
- data
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AEGIS is a task-oriented AI agent that automates the extraction
  of geographically-tagged research papers from conference proceedings and submits
  them for nomination on a curated webpage. It processes HTML proceedings, parses
  author affiliations, and uses LLM-powered analysis to identify papers linked to
  a target region.
---

# AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings

## Quick Facts
- arXiv ID: 2509.09470
- Source URL: https://arxiv.org/abs/2509.09470
- Authors: Om Vishesh; Harshad Khadilkar; Deepak Akkil
- Reference count: 4
- Primary result: Automated extraction of geographically-tagged papers with 100% recall and 99.4% accuracy

## Executive Summary
AEGIS is a task-oriented AI agent that automates the extraction of geographically-tagged research papers from conference proceedings and submits them for nomination on a curated webpage. The system processes HTML proceedings, parses author affiliations, and uses LLM-powered analysis to identify papers linked to a target region. Tested on 586 papers from five conferences, AEGIS achieved perfect recall (100%) with near-perfect accuracy (99.4%). By integrating intelligent parsing, dynamic prompt engineering, and RPA-based submission, AEGIS bridges information discovery and real-world task execution, demonstrating high reliability and scalability in scholarly workflow automation.

## Method Summary
The system ingests conference proceedings HTML via automated browser with dynamic content rendering and local caching. It uses BeautifulSoup to parse HTML and applies layout-aware normalization with two strategies: regex pattern matching for flat-list structures and DOM traversal for track-based hierarchies. Publisher-specific prompt templates are dynamically selected and sent to Agent-E REST API for LLM analysis. Responses undergo multi-stage parsing with regex and JSON extraction plus a verification layer to reject empty/null fields. Finally, Selenium-based RPA automates browser interactions to submit nominations to the target form.

## Key Results
- Perfect recall (100%) across all tested datasets
- Near-perfect accuracy (99.4% overall, 98-100% per dataset)
- Precision ranged from 0.80-1.00, with 0.80 on IEEE ICDM indicating room for improvement
- Successfully processed 586 papers across five conferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layout-aware normalization enables reliable extraction across heterogeneous conference formats.
- Mechanism: The system identifies structural layout (flat-list vs. track-based), then dynamically selects appropriate extraction strategy—regex for flat lists or DOM traversal for hierarchical tracks. This reduces format-related extraction failures before LLM invocation.
- Core assumption: Conference websites follow predictable structural patterns within each layout category.
- Evidence anchors: "processes HTML proceedings, parses author affiliations"; "we designed and implemented a layout-aware normalization module... dynamically selects one of the two specialized extraction strategies"
- Break condition: JavaScript-heavy single-page applications with non-semantic DOM structures may defeat both strategies.

### Mechanism 2
- Claim: Publisher-specific prompt templates improve LLM extraction accuracy.
- Mechanism: A prompt template library is indexed by publisher/conference series. For each paper URL, the system loads the appropriate template and populates it with paper-specific context before calling Agent-E via REST API.
- Core assumption: Publisher-specific formatting conventions are consistent enough to benefit from tailored prompts.
- Evidence anchors: "LLM-powered analysis to identify papers linked to a target region"; "We created a library of prompt templates, each tailored to a specific conference series or publisher"
- Break condition: Highly irregular or newly-designed conference pages with no prior template may default to generic prompts, reducing accuracy.

### Mechanism 3
- Claim: Multi-stage response parsing with explicit verification prevents false positives from empty or malformed LLM outputs.
- Mechanism: After Agent-E returns semi-structured text, a deterministic regex parser extracts key-value pairs, followed by JSON parsing for complex fields. A verification layer confirms that extracted author and institution lists are non-empty and non-null.
- Core assumption: LLM outputs are mostly well-formed but may occasionally produce empty or hallucinated null fields.
- Evidence anchors: "achieving perfect recall (100%) with near-perfect accuracy (99.4%)"; "we added a crucial verification layer to ensure data quality and minimize false positives"
- Break condition: LLM produces plausible-looking but fabricated institution names that pass verification.

## Foundational Learning

- Concept: DOM tree traversal and parse tree navigation
  - Why needed here: The system must programmatically navigate HTML structures to extract paper links and associate track headings with content blocks.
  - Quick check question: Can you explain how to traverse from a heading element to its sibling content block in a parsed HTML tree?

- Concept: REST API streaming responses and token-by-token processing
  - Why needed here: Agent-E responses are streamed; the pipeline processes output incrementally for real-time logging without waiting for full completion.
  - Quick check question: What is the difference between buffering a full HTTP response versus processing a streaming response token-by-token?

- Concept: Robotic Process Automation (RPA) for form submission
  - Why needed here: The final stage automates browser interactions (clicking, scrolling, form filling) to submit nominations without human intervention.
  - Quick check question: How does Selenium's explicit wait differ from implicit wait when handling dynamically-loaded form elements?

## Architecture Onboarding

- Component map: URL input -> HTML capture -> link normalization -> prompt construction -> Agent-E invocation -> response parsing/verification -> RPA submission
- Critical path: URL input → HTML capture → link normalization → prompt construction → Agent-E invocation → response parsing/verification → RPA submission. Verification is the gatekeeper; failures here halt submission.
- Design tradeoffs: Local caching ensures reproducibility but requires storage and may miss real-time page updates. Publisher-specific prompts increase accuracy but require maintenance as conference websites evolve. Perfect recall prioritization tolerates some false positives (evidenced by 0.80 precision on IEEE ICDM) to ensure no target papers are missed.
- Failure signatures: Empty extraction (likely layout misclassification or unexpected DOM structure); JSON parsing errors (LLM output format drift); RPA submission hang (dynamic form elements not loading); False positives from ambiguous affiliation strings.
- First 3 experiments: 1) Reproduce pipeline on single conference with local caching disabled to observe real-time rendering behavior. 2) Ablate verification layer to measure false positive rate increase. 3) Test generalization on publisher without tailored prompt template to quantify accuracy degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can pagination of long paper lists be handled without losing papers across page boundaries?
- Basis in paper: Footnote 3 states: "One open item is to handle pagination of long paper lists, which is currently work in progress."
- Why unresolved: Current implementation caches a single page's HTML; proceedings with 1000+ papers often distribute entries across multiple paginated views.
- What evidence would resolve it: Successful extraction from a paginated proceedings page (e.g., full NeurIPS dataset) with 100% recall verified against ground truth.

### Open Question 2
- Question: What architectural modifications are required to extend AEGIS to journal repositories like SpringerLink and arXiv with different HTML structures?
- Basis in paper: Conclusion states: "Future work will focus on expanding the system's capabilities by developing new parsing modules for... major journal repositories like SpringerLink and arXiv."
- Why unresolved: The dual-strategy normalization was designed for conference proceedings; journals have volumes, issues, and continuous publication models.
- What evidence would resolve it: Successful extraction from SpringerLink and arXiv datasets with ≥95% accuracy and perfect recall.

### Open Question 3
- Question: Can the false positive rate be reduced through improved disambiguation of complex affiliation strings without sacrificing perfect recall?
- Basis in paper: Conclusion states: "We also aim to improve precision by refining the AI agent's ability to disambiguate complex affiliation strings, thereby reducing the few false positives observed." Table 1 shows precision of only 0.80 on IEEE ICDM.
- Why unresolved: IEEE ICDM case where "Table of Contents" was misclassified demonstrates the agent struggles with ambiguous strings and non-standard document types.
- What evidence would resolve it: Re-evaluation on challenging datasets achieving ≥0.95 precision while maintaining 100% recall.

## Limitations

- Unknown implementation details: Specific prompt templates, regex patterns, DOM traversal logic, and "Indian institute" matching criteria are not documented
- Limited generalizability: Evaluation relies on single target region (India) and five conference proceedings, limiting cross-geographic and format generalization
- Maintenance overhead: System depends on publisher-specific templates that require updates as conference websites evolve

## Confidence

- High Confidence: Core architectural approach combining layout-aware preprocessing, LLM-powered analysis, and RPA-based submission is sound and well-documented. Reported performance metrics are specific and verifiable within tested scope.
- Medium Confidence: Multi-stage verification mechanism effectively prevents empty/false-positive submissions, but impact on precision (0.80-1.00 range) suggests room for refinement. Assumption about predictable structural patterns within layout categories appears reasonable but untested on edge cases.
- Low Confidence: Generalizability to publishers beyond five tested conferences, robustness to dynamic JavaScript-heavy websites, and scalability to larger proceedings remain unproven. Maintenance burden of publisher-specific prompt templates is acknowledged but not quantified.

## Next Checks

1. Test pipeline on conference proceedings with JavaScript-heavy, single-page application structures to evaluate failure rates when both regex and DOM traversal strategies are challenged
2. Run system on a publisher without tailored prompt template (e.g., SpringerLink) and measure accuracy degradation to quantify value of publisher-specific customization
3. Bypass non-empty field verification and measure increase in false positive rate to determine whether 99.4% accuracy is primarily due to extraction quality or verification filtering