---
ver: rpa2
title: 'Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts
  and Model Adaptation Strategies'
arxiv_id: '2507.04142'
source_url: https://arxiv.org/abs/2507.04142
tags:
- lora
- reasoning
- prompt
- clinical
- nli4ct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates how prompt structure and parameter-efficient\
  \ fine-tuning affect clinical natural language inference (NLI) performance. Four\
  \ prompting strategies\u2014Chain-of-Thought, Self-Critique, ReACT, and Quasi-Symbolic\
  \ Abstract Reasoning\u2014are applied to compact language models (1.5\u20134B parameters)\
  \ fine-tuned via LoRA on NLI4CT, a clinical trial inference benchmark."
---

# Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies

## Quick Facts
- arXiv ID: 2507.04142
- Source URL: https://arxiv.org/abs/2507.04142
- Authors: Mael Jullien; Marco Valentino; Leonardo Ranaldi; Andre Freitas
- Reference count: 20
- Primary result: Prompt structure accounts for up to 44% of variance in F1 performance on clinical NLI tasks

## Executive Summary
This study systematically evaluates how prompt structure and parameter-efficient fine-tuning affect clinical natural language inference (NLI) performance. Four prompting strategies—Chain-of-Thought, Self-Critique, ReACT, and Quasi-Symbolic Abstract Reasoning—are applied to compact language models (1.5–4B parameters) fine-tuned via LoRA on NLI4CT, a clinical trial inference benchmark. Prompt design alone explains up to 44% of variance in F1 performance, with NLR and TAR achieving the highest average F1 (0.66–0.67). LoRA consistently improves F1 by +8–12%, raises output validity above 97%, and narrows the gap to GPT-4o-mini to within 7.1%. Reasoning-type-aware evaluation reveals prompt-induced trade-offs, and LoRA improves generalization on MedNLI and TREC in 75% of cases. The findings demonstrate that combining strong prompts with lightweight adaptation enables compact models to rival frontier systems in specialized clinical domains.

## Method Summary
The study evaluates four prompt strategies (NLR/CoT, ISRR/Self-Critique, TAR/ReAct, SSR/QuaSAR) combined with LoRA fine-tuning on clinical NLI tasks. GPT-4o-mini generates reasoning demonstrations on NLI4CT training instances, with only correctly classified instances retained. LoRA adapters are trained on 500 demonstrations per prompt for each model. Models are evaluated on NLI4CT, MedNLI, and TREC datasets using Macro-F1 and validity metrics. The approach leverages parameter-efficient adaptation to align output formats while prompt structures impose specific reasoning inductive biases.

## Key Results
- Prompt structure explains 30–44% of F1 variance, outperforming model architecture and LoRA adaptation effects
- LoRA improves F1 by +8–12% and raises output validity above 97% across all model-prompt combinations
- NLR and TAR prompts achieve highest average F1 (0.66–0.67), while SSR shows strong quantitative reasoning gains
- LoRA generalization improves MedNLI and TREC performance in 75% of cases, with gains of 1–4% F1

## Why This Works (Mechanism)

### Mechanism 1: Structural Scaffolding as a Primary Performance Driver
Prompt structure imposes specific inductive biases that account for a larger share of performance variance than model adaptation alone. Different prompting strategies (e.g., NLR vs. SSR) force the model to decompose problems into distinct reasoning trajectories. This structural constraint reshapes the distribution of reasoning capabilities, explaining up to 44% of variance in F1 scores.

### Mechanism 2: Parameter-Efficient Alignment for Validity and Precision
LoRA primarily functions to align the model's output distribution to the strict syntax and semantic requirements of clinical NLI, drastically improving validity. LoRA injects trainable adapters into attention and feed-forward layers, reinforcing "shallow symbolic patterns" and enforcing output formatting. While it improves F1 by +8–12%, its most significant impact is raising output validity above 97%.

### Mechanism 3: Demonstration Distillation via Teacher Filtering
High-quality reasoning capabilities in compact models are contingent on "high-precision" demonstration sets filtered for correctness by a frontier model. Instead of human annotation, a frontier model (GPT-4o-mini) generates reasoning traces, with only correctly classified instances retained. This filters out confusing signals, effectively distilling the teacher's successful reasoning paths into the student model's training set.

## Foundational Learning

**Concept: Natural Language Inference (NLI)**
- Why needed: This is the core task—determining if a premise entails, contradicts, or is neutral to a hypothesis
- Quick check: If a clinical trial includes "Serum 25OHD < 40" and the statement says "Vitamin D deficient are excluded," is this entailment or contradiction based on domain equivalence?

**Concept: Inductive Bias in Prompting**
- Why needed: The paper frames prompts not just as instructions, but as structural scaffolds that force specific reasoning paths
- Quick check: Which prompt category imposes the strictest structural constraint: Chain-of-Thought (NLR) or Quasi-Symbolic Abstract Reasoning (SSR)?

**Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
- Why needed: The study relies on LoRA to adapt 3B-4B models
- Quick check: Does LoRA retrain all parameters of the LLaMA-3.2-3B model, or does it inject trainable layers into specific modules?

## Architecture Onboarding

**Component map:** Clinical Trial Reports + NLI Hypotheses → GPT-4o-mini (Generates demonstrations) → Filter Layer (Correct-only retention) → Student Models + LoRA Adapters → Prompt Engine (Structural scaffolds) → Evaluator (Macro-F1 and Validity)

**Critical path:** The generation and filtering of demonstrations is the bottleneck. If the Filter Layer is too permissive, the LoRA adaptation will optimize for noise.

**Design tradeoffs:**
- Recall vs. Precision: ISRR maximizes recall (0.890) but lowers precision; SSR maximizes precision (0.832)
- Complexity vs. Performance: SSR yields high gains on quantitative reasoning but requires complex formalization; NLR is simpler and generally robust
- Teacher Dependency: Relying on GPT-4o-mini for demonstrations creates external dependency and potential licensing/cost issues

**Failure signatures:**
- Zero-Shot LoRA Degradation: Models lose 2% F1 when fine-tuned without reasoning scaffolds
- Quantitative Derivation Stagnation: LoRA provides minimal lift (+0.047) on multi-step arithmetic
- Format Collapse: Base models (without LoRA) dip to ~70% validity on complex prompts

**First 3 experiments:**
1. Validity Stress Test: Run base vs. LoRA-adapted model on synthetic dataset with strict output format requirements
2. Prompt Variance Analysis: Using fixed LoRA checkpoint, swap prompt strategies on same test set to verify 44% variance claim
3. Teacher-Student Gap Analysis: Compare error rates of GPT-4o-mini vs. LoRA-tuned 3B model on "Quantitative Derivation"

## Open Questions the Paper Calls Out

**Open Question 1:** Do the relative benefits of prompt strategies (specifically SSR) and LoRA transfer effectively to models with significantly larger parameter counts (>7B)?
- Basis: Authors explicitly state restrictions to models below 4B parameters and acknowledge larger models may interact differently
- Why unresolved: Study deliberately focused on compact models for on-premise deployment, leaving scaling laws untested
- What evidence would resolve it: Replicating framework using 7B, 13B, and 70B instruction-tuned models

**Open Question 2:** To what extent does the GPT-4o-mini teacher model introduce factual or stylistic biases that limit the reasoning fidelity of the fine-tuned student models?
- Basis: Limitations section notes risks of importing factual or stylistic biases from teacher model
- Why unresolved: Current methodology relies entirely on synthetic distillation without human-verified comparison
- What evidence would resolve it: Comparative study evaluating LoRA models trained on human-curated demonstrations versus synthetic ones

**Open Question 3:** How does inter-annotator agreement on reasoning type labels affect the stability of the reported prompt-induced trade-offs?
- Basis: Limitations section states reasoning-type labels provided by single domain expert with no inter-annotator agreement measures
- Why unresolved: Conclusion that "reasoning-type-aware evaluation is essential" relies on these labels
- What evidence would resolve it: Re-annotation of test set by multiple clinical experts to calculate Cohen's Kappa

## Limitations

- Teacher model dependency: Entire demonstration generation pipeline relies on GPT-4o-mini's correctness with no error analysis for teacher's reasoning failures
- Reasoning-type label reliability: NLI4CT annotations performed by single expert with no reported inter-annotator agreement statistics
- Domain transferability assumptions: Generalization gains to MedNLI and TREC are modest (+1-4% F1) and inconsistent across models

## Confidence

**High confidence** in: Prompt structure explains 30-44% of F1 variance; LoRA performance gains (+8-12% F1) are consistent across architectures
**Medium confidence** in: LoRA primarily functions through format enforcement rather than deep capability acquisition
**Low confidence** in: Quantitative reasoning performance claims, particularly for "Quantitative Derivation" where LoRA shows minimal gains

## Next Checks

1. **Teacher error propagation analysis**: Generate 100 random test cases where GPT-4o-mini produces incorrect reasoning traces, then trace how errors propagate through LoRA fine-tuning to student models

2. **Cross-annotator reliability study**: Have three independent clinicians annotate 100 NLI4CT instances for reasoning types, then calculate Fleiss' kappa to establish annotation reliability baseline

3. **Arithmetic reasoning stress test**: Create synthetic NLI dataset with controlled multi-step arithmetic problems to isolate whether LoRA can develop genuine computational reasoning versus pattern matching on clinical text