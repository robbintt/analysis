---
ver: rpa2
title: Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs
arxiv_id: '2505.12929'
source_url: https://arxiv.org/abs/2505.12929
tags:
- tokens
- grpo
- training
- arxiv
- advantage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical yet underexplored issue in reinforcement
  learning for large language models: low-probability tokens disproportionately influence
  model updates due to their large gradient magnitudes, which hinders effective learning
  of high-probability tokens essential for model performance. To mitigate this, the
  authors propose two methods: Advantage Reweighting, which reduces the weight assigned
  to low-probability tokens, and Low-Probability Token Isolation (Lopti), which updates
  low- and high-probability tokens separately.'
---

# Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs

## Quick Facts
- **arXiv ID**: 2505.12929
- **Source URL**: https://arxiv.org/abs/2505.12929
- **Reference count**: 40
- **Primary result**: Up to 46.2% improvement on K&K Logic Puzzle dataset by mitigating low-probability token gradient dominance

## Executive Summary
This paper identifies a critical issue in reinforcement learning for large language models: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes, which hinders effective learning of high-probability tokens essential for model performance. The authors propose two methods - Advantage Reweighting and Low-Probability Token Isolation (Lopti) - that attenuate gradients from low-probability tokens while emphasizing updates from high-probability tokens. Experimental results show significant improvements across multiple datasets, with the combined approach achieving 46.2% improvement on K&K Logic Puzzles and consistent gains on math-related datasets.

## Method Summary
The paper addresses gradient dominance in RL for LLMs by proposing two complementary methods. Advantage Reweighting modifies the advantage calculation by scaling it with a function of token probability: Â_new = [α·π + (1-α)]·Â_old. This reduces the weight assigned to low-probability tokens during updates. Lopti takes a more structural approach by splitting tokens at a probability threshold η and updating them in two sequential passes: first low-probability tokens, then high-probability tokens. Both methods leverage the observation that gradient norms scale with (1-π), making low-probability tokens dominate averaged updates. The methods can be applied independently or together, with the combined approach showing particularly strong results on K&K Logic Puzzles.

## Key Results
- 46.2% improvement on K&K Logic Puzzle dataset using combined Advantage Reweighting and Lopti
- Consistent performance gains across math-related datasets (DSR-Uniform and ORZ)
- Lopti's update order is critical - reversing (high→low instead of low→high) causes training collapse
- Advantage Reweighting adds zero computational overhead while Lopti increases training time ~2×
- Methods work synergistically on K&K but are best used individually for math datasets

## Why This Works (Mechanism)

### Mechanism 1: Gradient Magnitude Disparity
Low-probability tokens produce disproportionately larger gradient magnitudes than high-probability tokens during policy gradient updates. The gradient norm with respect to activations is bounded by values proportional to (1 - π), arising from the softmax Jacobian structure. When thousands of token gradients are averaged in a single update, low-probability tokens dominate the direction. This effect is amplified when the network Jacobians are well-conditioned.

### Mechanism 2: Incorrect Update Direction for High-Probability Tokens
High-probability tokens with positive advantage are updated in the correct direction less than 50% of the time under standard GRPO. When low-probability token gradients dominate the averaged update, they can push high-probability tokens in unintended directions, even when those tokens "should" increase in probability. This interference causes the model to learn the wrong direction for essential high-probability tokens.

### Mechanism 3: Sequential Update Advantage
Updating low-probability tokens before high-probability tokens (Lopti) improves training because Stage 1 updates indirectly shift high-probability token distributions. If a positive high-probability token's probability increases during Stage 1, its gradient magnitude decreases, requiring less correction in Stage 2. If its probability decreases incorrectly, its gradient becomes larger, ensuring it receives more attention in Stage 2. Reversing this order (high→low) leads to training collapse.

## Foundational Learning

**Policy Gradient with Importance Sampling**
- Why needed: GRPO computes advantages and uses importance sampling ratios πθ/πold. Understanding how the ratio interacts with gradients via the log-probability derivative is essential to see why token probability enters gradient magnitude.
- Quick check: Given policy πθ(a|s), what is ∇θ log πθ(a|s)? (Answer: ∇θπθ(a|s) / πθ(a|s))

**Softmax Gradient Structure**
- Why needed: The gradient of log-softmax with respect to logits is (one-hot target − probability vector). Its norm is bounded by √(2·(1−π)), directly explaining the (1−π) scaling in Proposition 4.2.
- Quick check: For a 3-class softmax output [0.8, 0.1, 0.1] and target class 0, what is the gradient of log(p0) w.r.t. the logits?

**PPO/GRPO Clipping Mechanism**
- Why needed: Advantage Reweighting modifies the effective advantage before clipping; Lopti applies clipping separately to each token group. Understanding when clipping activates (Itrust function) clarifies when gradients are zero vs. non-zero.
- Quick check: If advantage Â > 0 and the probability ratio r = πθ/πold = 1.3 with clip bounds [0.8, 1.2], does the gradient flow? (Answer: No, clipping activates)

## Architecture Onboarding

**Component map**: Sampling module → Reward computation → Advantage estimation → Advantage Reweighting (optional) → Token split (Lopti optional) → Dual update pass (Lopti) → KL penalty and clipping

**Critical path**:
1. Sample responses → compute rewards → compute group-relative advantages
2. Apply reweighting (if enabled) to advantages based on current token probabilities
3. For Lopti: split tokens at η threshold, compute masked advantages, run two sequential backward passes
4. Apply KL penalty and clipping as in standard GRPO

**Design tradeoffs**:
- Advantage Reweighting: Zero computational overhead, but requires tuning α (0.1 for math, 0.3 for K&K)
- Lopti: ~2× training time, but more robust to hyperparameter choice (η=0.5 works across tasks)
- Joint use: Synergistic on K&K (+46.2%), but not recommended for math (use either method alone)

**Failure signatures**:
- Training collapse after epoch 4: Caused by reversed update order in Lopti (high→low instead of low→high)
- No improvement over baseline: α or η set incorrectly; check if α ∈ [0.2, 0.3] (K&K) or α = 0.1 (math), η ∈ [0.3, 0.5]
- High-probability tokens not improving: May indicate reweighting not applied; verify advantage modification in code

**First 3 experiments**:
1. Reproduce Figure 1 analysis: During GRPO training, log gradient norms and token probabilities; verify low-probability tokens have higher gradient norms and that updating only low-probability tokens shifts high-probability distributions
2. Ablate update order (Figure 6b): Train with Lopti low→high vs. high→low; confirm reversed order causes collapse
3. Hyperparameter sweep on validation set: Grid search α ∈ {0.1, 0.2, 0.3, 0.5} and η ∈ {0.3, 0.5, 0.7} on a held-out portion of training data before full training

## Open Questions the Paper Calls Out

**Open Question 1**: Why does the combined application of Advantage Reweighting and Lopti improve performance on K&K Logic Puzzles but yield no additional benefit—and sometimes inferior results—on math-related datasets? The paper observes this discrepancy but offers no theoretical or empirical explanation for why the interaction between the two methods is task-dependent.

**Open Question 2**: Does the low-probability token dominance phenomenon occur in pre-training and supervised fine-tuning, or is it specific to reinforcement learning with policy gradient methods? The theoretical analysis applies broadly to softmax cross-entropy structure, yet experiments only cover GRPO and REINFORCE++.

**Open Question 3**: How does the low-probability token dominance issue manifest across different model architectures (e.g., encoder-decoder, mixture-of-experts) and at larger scales beyond 7B parameters? The paper validates only on Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct-1M decoder-only models.

## Limitations

- The theoretical foundation relies on Proposition 4.2, which bounds gradient norms via the product of Jacobians and the gradient of log-probabilities, but the derivation is not fully detailed in the paper.
- The Lopti method's success hinges on predictable cross-influence of low-probability token updates on high-probability token distributions, which is not fully explored or explained.
- The combined method's superior performance on K&K (46.2% improvement) is impressive, but the lack of significant gains on math datasets suggests the method's effectiveness is task-dependent.

## Confidence

- **High confidence**: The empirical observation that low-probability tokens have larger gradient norms (Figure 1d) and the effectiveness of the proposed methods in improving GRPO training on K&K and math datasets.
- **Medium confidence**: The claim that high-probability tokens are updated in the wrong direction less than 50% of the time under standard GRPO (Figure 3).
- **Low confidence**: The general applicability of the proposed methods across all LLM architectures and tasks beyond the specific models and datasets tested.

## Next Checks

1. Apply the proposed methods to a different LLM architecture (e.g., Llama-3 or Mistral) and dataset (e.g., GSM8K or HumanEval) to assess generalization and compare performance gains.

2. Conduct a comprehensive grid search over α and η for both K&K and math datasets, including values outside the recommended ranges, to analyze the relationship between these hyperparameters and performance.

3. Remove or relax Assumption 4.1 (well-conditioned Jacobians) by training on models with known pathological conditioning to measure the impact on gradient dominance and the effectiveness of the proposed methods.