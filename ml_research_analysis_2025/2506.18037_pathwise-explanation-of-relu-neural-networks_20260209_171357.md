---
ver: rpa2
title: Pathwise Explanation of ReLU Neural Networks
arxiv_id: '2506.18037'
source_url: https://arxiv.org/abs/2506.18037
tags:
- path
- explanation
- input
- relu
- units
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces pathwise explanation for ReLU neural networks,
  which represents the network as piecewise linear models based on subsets of hidden
  units connected by paths, rather than all activated units as in previous unwrapping
  methods. The approach enables decomposition of explanations for individual components
  within inputs and offers flexibility in adjusting explanation ranges by controlling
  path depth and width.
---

# Pathwise Explanation of ReLU Neural Networks

## Quick Facts
- arXiv ID: 2506.18037
- Source URL: https://arxiv.org/abs/2506.18037
- Reference count: 10
- This paper introduces pathwise explanation for ReLU neural networks, which represents the network as piecewise linear models based on subsets of hidden units connected by paths, rather than all activated units as in previous unwrapping methods.

## Executive Summary
This paper proposes pathwise explanation for ReLU neural networks, decomposing explanations into piecewise linear models based on connected subsets of hidden units (paths) rather than requiring all activated units. The method enables decomposition of explanations for individual components within inputs and offers flexibility in adjusting explanation ranges by controlling path depth and width. Experiments on ImageNet and CIFAR10 datasets using VGG-16 and ResNet-18 architectures demonstrate superior performance in explaining input attributions compared to existing methods like Saliency, Integrated Gradients, and Guided Backpropagation.

## Method Summary
The method constructs piecewise linear explanations by identifying paths through the network - connected subsets of hidden units that are activated together. Algorithm 1 iteratively selects important units backward from the output layer using gradient-based importance scores, with configurable depth (layers in path) and width (units per layer). For each path, the method computes a linear model W^p X that generates an attribution map highlighting regions supporting that specific decision pathway. The approach enables both single-path explanations and multi-path decomposition of semantically distinct input components.

## Key Results
- Achieves higher insertion AUC scores (0.936 vs 0.928 for top baseline) on ImageNet subset
- Provides competitive deletion scores (0.179) while offering more interpretable explanations
- Enables explanation of incorrect predictions by identifying different paths supporting alternative class decisions
- Demonstrates superior performance in explaining input attributions compared to Saliency, Integrated Gradients, and Guided Backpropagation

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Linear Decomposition via Connected Paths
A ReLU network can be expressed as a sum of piecewise linear models, each corresponding to a connected subset of hidden units (a path), rather than requiring all activated units. The ReLU function `ReLU(x) = x · φ(x)` where `φ(x) = 1` for `x > 0` and `0` otherwise, enables exact decomposition. For a path `p = [h⁽¹⁾ᵢ₁, ..., h⁽ᴺ⁾ᵢₙ]`, the piecewise linear model is defined as `fᵖ(X) = (WᵖX + bᵖ) · ∏_{h∈p} φ(h)`. This term is non-zero if and only if all units in the path are activated.

### Mechanism 2: Importance-Weighted Path Construction via Gradient-Derived Contributions
Paths supporting a target class prediction can be identified by computing gradient-based importance scores for hidden units and selecting top-k units per layer. Algorithm 1 computes importance as `imp = softmax(W · diag(h⁽ⁿ⁾))` targeting `targetIndex`, then selects units exceeding threshold `α = 1/|classes|`. The process iterates backward from output layer, maintaining a cumulative weight matrix `W_prev`.

### Mechanism 3: Explanation Decomposition via Multiple Distinct Paths
A single input can be explained by multiple independent paths, each capturing different semantic components (e.g., "fox eyes" vs. "fox ears"). By initializing path construction with different top-layer units (e.g., `path⁽ᴺ⁾ = {h₁}` vs. `path⁽ᴺ⁾ = {h₂}`), distinct paths are generated. Each path's linear model `WᵖX` produces an attribution map highlighting regions supporting that specific decision pathway.

## Foundational Learning

- **Concept: ReLU Piecewise Linearity**
  - Why needed here: The entire method hinges on ReLU networks being exactly expressible as piecewise linear functions; without this, pathwise decomposition has no formal basis.
  - Quick check question: Given `h = ReLU(Wx + b)` where `W = [1, -1]`, `b = 0`, `x = [2, 3]`, compute `h` and determine which linear region applies.

- **Concept: Gradient-Based Attribution (Saliency, Integrated Gradients)**
  - Why needed here: The paper compares against and builds upon gradient-based explanation methods; understanding these baselines is essential to interpret improvements.
  - Quick check question: Explain why `∂f/∂x` represents input attribution for a linear model `f(x) = Wx + b`. Why does this interpretation break for non-linear networks?

- **Concept: Insertion/Deletion Metrics for Explanation Quality**
  - Why needed here: Quantitative evaluation uses insertion AUC (higher = better) and deletion AUC (lower = better); understanding these metrics is required to interpret Table 1 results.
  - Quick check question: In the insertion game, if pixels are added in order of attribution importance, what should happen to model confidence for the correct class? What does a low deletion AUC indicate?

## Architecture Onboarding

- **Component map:**
  1. Forward pass extraction: Compute hidden unit activations `h⁽ⁿ⁾` and pre-activation values for all layers
  2. Path construction module (Algorithm 1): Iteratively selects important units backward from output; configurable via `depth`, `width`, `α`
  3. Linear model computation: For each path, compute `Wᵖ` via weight product chain; compute `bᵖ` via bias aggregation
  4. Attribution generation: `WᵖX` yields per-input attribution; positive/negative contributions visualized as heatmap
  5. Multi-path decomposition: Branch at top-layer neurons to generate multiple explanation paths

- **Critical path:**
  1. Ensure target model is ReLU-only (fold BN, remove other activations)
  2. Run forward pass, cache all `h⁽ⁿ⁾` and `φ(h⁽ⁿ⁾)`
  3. Execute Algorithm 1 for target class, collect path units
  4. Compute `Wᵖ = W⁽ᴺ⁺¹⁾ · W⁽ᴺ⁾ · ... · W⁽¹⁾` restricted to path indices
  5. Generate attribution: `attribution = Wᵖ @ X`

- **Design tradeoffs:**
  - Depth (small vs. large): Small depth = paths include only high-level layers with broad receptive fields (holistic explanations); large depth = paths extend to low-level layers (fine-grained, but may include less significant units)
  - Width (small vs. large): Small width = focuses on most important units per layer (sparse, potentially incomplete); large width = captures more contributing units but risks noise
  - One-way vs. multi-way: One-way paths are simpler but may miss joint contributions; multi-way paths capture interactions but increase complexity
  - Assumption: Paper suggests `(large depth, large width)` maximizes insertion AUC; `(large depth, small width)` yields most interpretable attributions

- **Failure signatures:**
  - All-zero attributions: Path is deactivated (`∏ φ(h) = 0`); check input activates path units
  - Noisy/unfocused attributions: Width too large, capturing irrelevant units; reduce width
  - Missing key features: Depth too shallow; increase depth to include earlier layers
  - Inconsistent explanations across similar inputs: Path selection highly sensitive; may need to average over multiple paths or increase `α` threshold

- **First 3 experiments:**
  1. Sanity check on synthetic data: Create a 2-layer ReLU network with known linear regions; verify pathwise explanation recovers correct `Wᵖ` for each region. Compare against unwrapping method (should match when using all-unit path).
  2. Ablation on depth/width: On ImageNet subset with VGG-16, sweep `depth ∈ {5, 10, 15}` and `width ∈ {1, 5, 10}`; plot insertion/deletion AUC. Replicate Figure 8 trends.
  3. Decomposition validation: For multi-object images, generate separate paths initialized from different top-layer units; manually verify each path highlights semantically distinct regions. Compute correlation between path attribution maps as a decomposition quality metric.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the pathwise explanation framework be generalized to neural networks utilizing non-piecewise linear activation functions, such as GELU or Swish?
- **Basis in paper:** [Inferred] The mathematical formulation in Section 3 strictly relies on the ReLU property $\phi(x) \in \{0, 1\}$ (Eq. 8) to define activated paths and piecewise linear models.
- **Why unresolved:** The binary nature of the ReLU indicator function allows for the decomposition of the network into distinct linear paths. Smooth activation functions do not yield binary activation states, making the current definition of a "path" inapplicable.
- **What evidence would resolve it:** A theoretical extension of Definition 1 and Eq. (5) that accommodates continuous activation gradients, validated by experiments on architectures like Transformers.

### Open Question 2
- **Question:** Can the hyperparameters for path construction (depth, width, and threshold $\alpha$) be determined automatically based on the input or model structure?
- **Basis in paper:** [Explicit] Algorithm 1 lists `depth`, `width`, and `alpha` as required inputs. Section 3.3 and 5.4 state that these parameters control the "range of explanations" and show trade-offs, but offer no mechanism for automatic selection.
- **Why unresolved:** The paper demonstrates that varying these parameters shifts the explanation from global (coarse) to local (fine-grained), but the current implementation requires manual tuning to achieve the desired granularity.
- **What evidence would resolve it:** An adaptive algorithm that selects optimal depth/width based on input complexity or feature saliency, eliminating the need for manual parameter search.

### Open Question 3
- **Question:** What is the computational complexity of the path construction algorithm relative to the network size, and how does it scale to modern high-capacity architectures?
- **Basis in paper:** [Inferred] The experiments (Section 5) are limited to VGG-16 and ResNet-18. Algorithm 1 involves iterative matrix multiplications (`matMul`) and gradient computations over layers, which may be computationally intensive for very deep networks.
- **Why unresolved:** While the paper claims the method outperforms others qualitatively, it does not provide a runtime analysis or scalability benchmarks against the baseline methods (e.g., Integrated Gradients) for very large models.
- **What evidence would resolve it:** A formal complexity analysis ($O(n)$) and runtime benchmarks on larger architectures (e.g., ResNet-152 or DenseNet) comparing the time cost of path construction against a single backward pass.

## Limitations
- The method fundamentally relies on ReLU networks being expressible as piecewise linear functions, which breaks if non-ReLU activations are present
- The gradient-based importance scoring assumes meaningful neuron ranking via softmax over weighted hidden units, but lacks external validation of this assumption
- The semantic decomposition claims depend on high-level neurons encoding distinct features, which may not hold for polysemantic or distributed representations

## Confidence
- **High Confidence**: The piecewise linear decomposition mechanism (Mechanism 1) has formal proof and clear implementation; the comparison against baseline attribution methods using insertion/deletion metrics is reproducible
- **Medium Confidence**: The gradient-based path construction (Mechanism 2) follows established importance scoring but the choice of softmax weighting and α threshold lacks thorough sensitivity analysis; the claim that "top layers have high-level features" is reasonable but not empirically validated
- **Low Confidence**: The semantic decomposition claims (Mechanism 3) rely on visual inspection of attribution maps without quantitative metrics for decomposition quality; the assumption that single-neuron initialization yields semantically coherent paths is unverified

## Next Checks
1. **Linear Region Recovery**: On a synthetic 2-layer ReLU network with known decision boundaries, verify that pathwise explanation correctly identifies all linear regions and their corresponding Wᵖ matrices, matching ground truth
2. **Gradient Importance Sensitivity**: Systematically vary α threshold and width parameter; plot insertion AUC against these parameters to identify optimal ranges and verify that the default α=1/|classes| is justified
3. **Semantic Decomposition Quantification**: For multi-object images, compute correlation between attribution maps from different paths as a measure of decomposition quality; verify that semantically distinct objects produce low-correlation attribution maps