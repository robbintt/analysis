---
ver: rpa2
title: 'Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval
  with Claim extraction'
arxiv_id: '2508.06495'
source_url: https://arxiv.org/abs/2508.06495
tags:
- para
- alega
- fake
- dados
- como
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the scarcity of Portuguese fact-checking
  datasets that integrate external evidence, crucial for developing robust automated
  systems. It proposes a methodology to enrich existing Portuguese news corpora (Fake.Br,
  COVID19.BR, MuMiN-PT) by extracting main claims using LLMs (Gemini 1.5 Flash) and
  retrieving external evidence via search engine APIs (Google CSE, Google FactCheck).
---

# Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction

## Quick Facts
- **arXiv ID**: 2508.06495
- **Source URL**: https://arxiv.org/abs/2508.06495
- **Reference count**: 0
- **Primary result**: Enriching Portuguese fact-checking corpora with external evidence improves classification performance, especially when combined with data validation.

## Executive Summary
This research addresses the scarcity of Portuguese fact-checking datasets with integrated external evidence, which limits the development of robust automated systems. The authors propose a methodology to enrich existing Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) by extracting main claims using LLMs (Gemini 1.5 Flash) and retrieving external evidence via search engine APIs (Google CSE, Google FactCheck). A semi-automated data validation framework, including near-duplicate detection, improves base corpus quality. Experimental results show that enriching validated data with external context generally improves classification performance (Bertimbau, Gemini 1.5 Flash), especially with fine-tuning, although original data performance may decline post-validation due to increased task complexity. The study contributes enriched corpora, insights on claim extraction utility, and demonstrates the impact of data characteristics and external evidence on model performance.

## Method Summary
The methodology enriches Portuguese fact-checking corpora through a pipeline that first validates and cleans the data (removing URLs, detecting near-duplicates using MinHash LSH), then extracts main claims from text using Gemini 1.5 Flash when necessary, and finally retrieves external evidence via Google search APIs. The system uses a heuristic (80% term overlap) to determine whether claim extraction is needed, as top-down corpora (MuMiN-PT) often contain already-structured claims while bottom-up corpora (Fake.Br, COVID19.BR) require aggressive extraction. The enriched data combines original text with search snippets, which are then used to fine-tune classifiers (Bertimbau) or perform few-shot classification (Gemini). The approach aims to recover performance lost during validation by providing models with externally grounded context for classification.

## Key Results
- Enriching validated data with external evidence generally improves classification performance, especially with fine-tuning (Bertimbau).
- Data validation (removing URLs, duplicates) reduces baseline accuracy but forces models to rely on injected external context, recovering performance.
- Claim extraction utility varies by corpus type: critical for bottom-up (95% extraction in COVID19.Br) but less needed for top-down corpora (71% extraction in MuMiN-PT).
- Temporal alignment matters: older corpora (Fake.Br) show reduced benefit from enrichment due to search result decay.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claim extraction functions as a query optimizer for noisy text.
- Mechanism: Raw text (especially from WhatsApp or verbose news) contains non-factual noise that degrades search relevance. An LLM isolates the core factual assertion (≤20 words), acting as a semantic filter that improves the precision of subsequent API retrieval.
- Core assumption: The LLM (Gemini 1.5 Flash) can consistently identify the single most "check-worthy" fact in a text without hallucinating new details or missing the core argument.
- Evidence anchors:
  - [Section 4.1] The flow diagram explicitly routes data to "Extração de alegação" only if "Correspondentes?" is false.
  - [Section 5.3.2] Analysis shows extracted claims replaced generic terms (e.g., "saúde") with specific entities (e.g., "hidroxicloroquina"), demonstrating semantic focusing.
  - [Corpus] In COVID19.BR (WhatsApp data), 95% of examples required extraction, suggesting the mechanism is critical for informal domains.
- Break condition: If the LLM extracts a trivial or out-of-context statement, the subsequent search retrieves irrelevant evidence (Pattern F2/irrelevant results), failing to aid classification.

### Mechanism 2
- Claim: External evidence injection recovers performance lost by de-biasing.
- Mechanism: The data validation pipeline removes "easy" spurious features (e.g., URLs from specific domains which correlate with labels). While this lowers baseline accuracy, it forces the model to rely on the injected external context (search snippets), which provides generalizable grounding.
- Core assumption: The external search APIs return temporally relevant and authoritative results, and the classifier can distinguish between a "debunking" result and a "reinforcement" result.
- Evidence anchors:
  - [Section 5.5] Tables 5.8 and 5.9 show a consistent drop in performance from "Original" to "Validated", followed by a recovery when "Enriched".
  - [Section 5.4.1] Qualitative analysis identifies "Pattern F2" (Reinforcement), where search results amplify the fake news, actively working against the intended mechanism.
  - [Corpus] Weakness noted: For Fake.br (older data), Gemini performance dropped with enrichment, suggesting the mechanism fails if the external evidence is temporally misaligned.
- Break condition: If the search API returns the fake news itself (un-debunked) as the top result, the enrichment mechanism provides "negative evidence," potentially confusing the classifier.

### Mechanism 3
- Claim: Data collection bias dictates the necessity of the enrichment pipeline.
- Mechanism: "Top-down" corpora (like MuMiN-PT, collected from verified claims) already function as high-quality queries. "Bottom-up" corpora (Fake.Br, COVID19.BR) contain raw text requiring the full extraction pipeline. The system relies on a "Match" heuristic to detect this distinction dynamically.
- Core assumption: A lexical overlap of 80% (excluding stopwords) between the query and the search snippet indicates that the original text is already "claim-like" enough to skip extraction.
- Evidence anchors:
  - [Section 5.3] MuMiN-PT required extraction only 71% of the time (vs 95% for COVID19.BR), validating that source type impacts pipeline flow.
  - [Section 4.4.1] Defines the specific preprocessing logic (removing quotes/emojis) required to make the heuristic work.
- Break condition: If the "Match" heuristic is too sensitive, the system skips extraction for ambiguous texts, potentially missing nuanced claims; if too strict, it wastes compute extracting claims from already clear text.

## Foundational Learning

### Top-down vs. Bottom-up Data Collection
- Why needed here: The paper demonstrates that the performance of the enrichment pipeline is dependent on the corpus origin. Top-down data (agencies) requires less processing than bottom-up (social monitoring).
- Quick check question: Is the input data a verified claim (needs less extraction) or raw social text (needs aggressive extraction)?

### Spurious Correlations in NLP
- Why needed here: The study explicitly removes URLs to prevent models from "cheating" by learning domain names (e.g., "globo.com" = true).
- Quick check question: Does the classification accuracy drop significantly after removing metadata (URLs/HTML)? If so, the model was relying on shortcuts.

### Temporal Drift in Fact-Checking
- Why needed here: The paper highlights that "truth" changes over time (e.g., mask mandates) and search results decay (Fake.br issues).
- Quick check question: Are the search results temporally aligned with the date of the claim, or are they modern re-hashes/irrelevant context?

## Architecture Onboarding

### Component map:
Validator (MinHash LSH, URL stripping) -> Router (80% match heuristic) -> Extractor (Gemini 1.5 Flash) -> Retriever (Google CSE + FactCheck API) -> Aggregator (concatenates Top-1 snippet + Title) -> Classifier (Bertimbau fine-tuned or Gemini few-shot)

### Critical path:
The **Match Threshold (80%)**. This parameter determines whether the system bypasses the LLM. Tuning this directly impacts API costs and latency.

### Design tradeoffs:
- **Quality vs. Cost:** Validating data reduces bias but lowers raw accuracy; Enrichment adds latency/API costs but recovers accuracy via external truth.
- **Freshness vs. Stability:** Using live Search APIs provides up-to-date context but introduces non-determinism (Section 4.3 notes lack of `seed` support in Gemini API).

### Failure signatures:
- **Pattern F2 (Reinforcement):** The system retrieves the fake news itself from a blog/social post, creating a "false positive" feedback loop in the context.
- **Hallucination:** The LLM rewrites the claim in a way that changes the meaning (e.g., shifting focus from "mandatory" to "recommended").

### First 3 experiments:
1. **Ablation on Validation:** Run classification on (a) Raw Data, (b) Validated Data (URLs removed), (c) Validated + Enriched. Reproduce the "drop then rise" phenomenon to confirm the de-biasing hypothesis.
2. **Threshold Sensitivity:** Vary the "Match" threshold (e.g., 0.6, 0.8, 0.9) and measure the trade-off between LLM usage frequency and retrieval relevance (manual inspection of top-5 results).
3. **Negative Evidence Test:** Create a test set of "F2" cases (where search results reinforce the fake news) to see if the classifier learns to ignore the context or if it gets fooled by it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the isolated quality of the claim extraction and the relevance of the retrieved evidence quantitatively impact the final fact-checking performance?
- Basis in paper: [explicit] Section 6.1 states a "central limitation" is the absence of a granular evaluation of each pipeline step, preventing the quantification of individual component contributions.
- Why unresolved: The study only measured the end-to-end performance of the enriched datasets against baselines.
- What evidence would resolve it: An ablation study measuring classification performance with varying quality levels of claim extraction and retrieval.

### Open Question 2
- Question: Does separating multiple claims (claim splitting) within a single text yield more relevant evidence than extracting a single main claim?
- Basis in paper: [explicit] Section 6.1 lists developing and testing claim splitting techniques as a direction for future research to compare against the single-claim approach used in this study.
- Why unresolved: The authors intentionally simplified the methodology to extract only the main claim to generate a single search query per item.
- What evidence would resolve it: Implementing a claim decomposition module and comparing the retrieval relevance and classification accuracy against the single-claim baseline.

### Open Question 3
- Question: Can this retrieval-based enrichment methodology be adapted to effectively detect synthetic disinformation generated by LLMs?
- Basis in paper: [explicit] Section 6.1 suggests "Investigating combating disinformation generated by LLMs" as a future direction, noting this growing challenge was outside the scope.
- Why unresolved: The study focused on enriching existing corpora of "natural" fake news rather than synthetic content.
- What evidence would resolve it: Applying the methodology to datasets of synthetic LLM-generated text and analyzing the retrieval success rates for such content.

## Limitations

- The reliance on live search APIs introduces non-determinism and makes exact replication difficult due to the lack of seed control in the Gemini API.
- The validation process includes subjective manual steps (resolving contradictions, handling near-duplicates) that aren't fully specified, affecting reproducibility.
- Temporal sensitivity of fact-checking means older corpora (Fake.Br) may not benefit from current search results, as search result decay impacts enrichment effectiveness.

## Confidence

**High Confidence**: The data validation pipeline (URL removal, near-duplicate detection) effectively reduces spurious correlations and improves corpus quality. The performance drop from Original to Validated data followed by recovery with enrichment is reproducible.

**Medium Confidence**: The claim extraction mechanism works well for bottom-up corpora (COVID19.BR) but shows mixed results for top-down corpora (MuMiN-PT) and temporal issues with older datasets. The 80% match threshold appears reasonable but may need tuning.

**Low Confidence**: The few-shot classification results with Gemini 1.5 Flash are harder to validate due to the non-deterministic nature of the API and lack of specific example sets used in the study.

## Next Checks

1. **Ablation Study Replication**: Run classification on (a) Raw Data, (b) Validated Data (URLs removed), (c) Validated + Enriched to confirm the "drop then rise" phenomenon and validate the de-biasing hypothesis.

2. **Negative Evidence Test Set**: Create a test set specifically designed to trigger Pattern F2 cases (where search results reinforce fake news) to measure whether the classifier learns to ignore context or gets fooled by it.

3. **Threshold Sensitivity Analysis**: Vary the "Match" threshold (e.g., 0.6, 0.8, 0.9) and measure the trade-off between LLM usage frequency and retrieval relevance through manual inspection of top-5 search results for each threshold level.