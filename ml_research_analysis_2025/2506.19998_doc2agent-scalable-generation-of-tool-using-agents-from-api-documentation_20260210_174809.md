---
ver: rpa2
title: 'Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation'
arxiv_id: '2506.19998'
source_url: https://arxiv.org/abs/2506.19998
tags:
- agent
- tool
- tools
- parameter
- documentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Doc2Agent addresses the challenge of generating AI agents that
  can interact with arbitrary REST APIs by automatically producing Python-based tools
  from unstructured API documentation. It employs a pipeline that first generates
  executable functions from natural language docs, then validates and refines them
  using a code agent and a parameter database to infer missing values.
---

# Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation

## Quick Facts
- **arXiv ID**: 2506.19998
- **Source URL**: https://arxiv.org/abs/2506.19998
- **Reference count**: 40
- **Primary result**: Generates executable Python tools from unstructured API docs, achieving 55% relative performance improvement on WebArena with 90% lower cost

## Executive Summary
Doc2Agent addresses the challenge of generating AI agents that can interact with arbitrary REST APIs by automatically producing Python-based tools from unstructured API documentation. It employs a pipeline that first generates executable functions from natural language docs, then validates and refines them using a code agent and a parameter database to infer missing values. Evaluated on real-world APIs, WebArena, and glycomaterial science APIs, Doc2Agent achieved a 55% relative performance improvement over direct API calling on WebArena, with 90% lower cost, and successfully generated validated tools for 59.5% of real-world APIs. It also demonstrated domain adaptability by building a research agent for glycomaterial science without domain-specific prior knowledge.

## Method Summary
Doc2Agent uses LLMs (GPT-4o for direct generation, Claude 3.7 Sonnet for complex cases) to parse HTML/Markdown API documentation into structured JSON schemas, then converts these schemas into parameter-validated Python functions using templates. Each tool is executed with example parameters and validated against LLM-predicted expectations. Failed tools undergo iterative refinement by a code agent (up to 3 rounds) that receives error information, documentation, and candidate parameters from a vector database of successful API responses. The system targets GET methods for safety and builds a parameter value database through semantic similarity search across APIs to support inference for missing parameters.

## Key Results
- Achieved 59.5% validation rate (443/744 tools) on real-world APIs from 167 documentation sets
- 55% relative performance improvement over direct API calling on WebArena tasks with 90% cost reduction
- Successfully generated validated tools for 16 glycomaterial science APIs without domain-specific prior knowledge

## Why This Works (Mechanism)

### Mechanism 1
Structured extraction from unstructured API documentation produces executable Python tools. LLM parses HTML/Markdown docs into JSON schemas (via Pydantic models), then templates convert schemas into parameter-validated Python functions. Two generation strategies—direct (simple APIs) and target-oriented (complex, multi-filter APIs)—handle varying documentation quality.

### Mechanism 2
Iterative validation-refinement loops recover functional tools from initially broken implementations. Each tool is executed with available example parameters; responses are compared against LLM-predicted expectations. Failures trigger a code agent (Claude 3.7 Sonnet) that receives error info, docs, and candidate parameters to generate fixes. Up to 3 refinement rounds.

### Mechanism 3
Parameter value inference from validated API responses outperforms direct LLM guessing. Successful API responses populate a vector database keyed by parameter names/descriptions. When new tools lack example values, semantic similarity search retrieves candidates from prior successes. This exploits implicit inter-API dependencies (outputs of one API often match inputs of another).

## Foundational Learning

- **REST API structure (endpoints, methods, path/query parameters, authentication)**: Why needed here - Doc2Agent's core task is reverse-engineering these components from prose. Understanding REST conventions helps diagnose extraction failures.
  - Quick check question: Given `GET /users/{id}/posts?limit=10`, identify path parameter, query parameter, and HTTP method.

- **Structured LLM outputs (Pydantic/JSON schemas)**: Why needed here - The pipeline relies on LLMs producing parseable JSON conforming to predefined schemas; schema design determines extraction fidelity.
  - Quick check question: Why would a Pydantic model with `Optional[List[Parameters]]` be safer than untyped dict extraction?

- **Vector similarity search (embeddings, retrieval)**: Why needed here - Parameter inference uses embedding-based retrieval; understanding approximate nearest neighbor search helps debug poor suggestions.
  - Quick check question: If "user_id" and "account_number" retrieve each other but shouldn't, what embedding or indexing issue might cause this?

## Architecture Onboarding

- **Component map**: Generator (GPT-4o/Claude) -> Validator -> Parameter DB -> Refiner (Claude 3.7 Sonnet) -> Deployer (MCP/OpenAPI)

- **Critical path**: Generation -> Validation (pass → done; fail → Refiner with parameter candidates) -> Re-validation (up to 3 rounds) -> Deployment. Failed tools after 3 rounds are discarded.

- **Design tradeoffs**:
  - Safety vs coverage: Restricting to GET methods (recommended) reduces risk but limits API utility
  - Automation vs reliability: Fully automated testing on public APIs may cause side effects; manual parameter review trades speed for safety
  - Token efficiency vs completeness: Truncating long JSON responses saves costs but loses information (observed in CMS tasks)

- **Failure signatures**:
  - No Parameter Value: Missing examples + empty retrieval results
  - Wrong Parameter Value: Valid format but semantically incorrect (e.g., wrong user ID format)
  - Server-Side Error: 5xx responses indicate issues unrelated to tool code
  - Code Agent "Cheating": Try-catch blocks suppressing exceptions (requires uneditable test harness)

- **First 3 experiments**:
  1. Single-API extraction test: Run Doc2Agent on one well-documented API (e.g., OSRM from Appendix F). Verify JSON extraction accuracy and tool executability.
  2. Parameter inference ablation: Compare tool pass rates with (a) no parameter DB, (b) GPT-4o guessing, (c) full parameter DB. Use leave-one-API-out setup from Appendix D.2.
  3. Refinement round analysis: Track which error categories (C1-C4 from Appendix C) are recovered at each refinement round. Identify diminishing returns threshold.

## Open Questions the Paper Calls Out

### Open Question 1
How can the validation pipeline be extended to verify stateful REST APIs that require coordinated multi-call workflows and server-side status tracking? The current validation approach, which relies on analyzing individual responses, is "insufficient for stateful ones" requiring complex workflows.

### Open Question 2
Can integrating web browsing capabilities enable the pipeline to autonomously discover and scrape relevant API documentation to expand the toolset? The current implementation requires manual collection of initial HTML/Markdown documentation and cannot dynamically find new APIs based on task requirements.

### Open Question 3
How can the refinement loop be secured against code agents that "cheat" by suppressing exceptions to bypass validation metrics? The current mitigation (uneditable testing code) does not fully align the agent's optimization strategy with human intent or practical utility.

## Limitations
- System relies on LLM parsing of unstructured documentation, failing when docs lack clear endpoint structure (40.5% of tools fail validation)
- Stateful APIs (POST/PUT/DELETE) excluded due to safety concerns, limiting applicability to read-only interactions
- Parameter inference depends on semantic similarity between APIs within a domain, failing for specialized or domain-isolated APIs

## Confidence
- **High Confidence**: Relative cost reduction (90% vs direct API calling) and validation improvement from iterative refinement (47.6% increase)
- **Medium Confidence**: 55% relative performance improvement on WebArena based on small sample size (10 tasks)
- **Medium Confidence**: 59.5% tool validation rate on real-world APIs but lacks detailed error analysis for failures

## Next Checks
1. Cross-domain generalizability test: Apply Doc2Agent to 50+ APIs from domains outside glycomaterial science to assess parameter inference effectiveness when APIs lack semantic overlap
2. Stateful API capability evaluation: Design controlled environment for testing POST/PUT/DELETE operations with synthetic data to quantify performance gap
3. Human-in-the-loop cost analysis: Measure time and accuracy when human experts review and correct the 40.5% of tools that fail automated validation