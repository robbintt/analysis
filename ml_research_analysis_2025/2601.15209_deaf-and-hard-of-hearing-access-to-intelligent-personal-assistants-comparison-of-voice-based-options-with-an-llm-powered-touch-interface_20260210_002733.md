---
ver: rpa2
title: 'Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison
  of Voice-Based Options with an LLM-Powered Touch Interface'
arxiv_id: '2601.15209'
source_url: https://arxiv.org/abs/2601.15209
tags:
- participants
- speech
- deaf
- alexa
- touch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compared usability of LLM-assisted touch interface versus
  natural and re-spoken deaf speech for IPAs. Using Alexa, it tested three input methods:
  natural deaf speech, re-spoken speech via human wizard, and LLM-generated touch
  commands.'
---

# Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface

## Quick Facts
- arXiv ID: 2601.15209
- Source URL: https://arxiv.org/abs/2601.15209
- Reference count: 40
- One-line primary result: An LLM-assisted touch interface showed comparable usability to natural and re-spoken deaf speech for controlling smart home devices.

## Executive Summary
This study evaluated three input methods for Deaf and Hard of Hearing (DHH) users to interact with Intelligent Personal Assistants (IPAs): natural deaf speech, re-spoken speech via human wizard, and an LLM-generated touch command interface. Using Amazon Alexa in a controlled lab setting, the research found that word error rates were surprisingly low, with half of participants experiencing zero recognition errors. System Usability Scale (SUS) scores showed no significant differences across the three conditions, though qualitative feedback revealed mixed opinions about the touch interface's latency and limited options. Participants expressed strong preference for hands-free interaction and suggested ASL recognition as an ideal solution.

## Method Summary
The study tested three input methods with 13 DHH participants using Amazon Alexa on a Fire HD tablet: natural deaf speech, facilitated English (human re-speaking), and a touch interface powered by GPT-4o LLM. The touch interface generated context-aware command suggestions based on user history and smart home environment. Participants completed five tasks (playing music, controlling lights, setting timers, weather check, shopping list addition) in randomized order across conditions. Usability was measured using SUS scores, Net Promoter Scores (NPS), and qualitative feedback. The LLM generated command options through a serial chain of prompts maintaining context about user history and smart environment.

## Key Results
- Word error rates were low across conditions, with 50% of participants experiencing zero errors
- SUS scores showed no significant differences: touch (63.5), re-spoken (62.5), and natural speech (59.6)
- Qualitative feedback indicated mixed opinions, with latency and limited options cited as touch interface issues
- Participants strongly preferred hands-free interaction and suggested ASL recognition as ideal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An LLM can generate context-aware command suggestions for a smart home IPA interface, improving usability over static touch options.
- Mechanism: The LLM (GPT-4o) is provided with the user's command history and a description of the smart home environment. It uses this context to predict relevant action verbs and then, recursively, more specific command options. This replaces a static menu with a dynamic, personalized one.
- Core assumption: DHH users who use speech will find a predictive, dynamic touch interface a viable and usable alternative or supplement to direct speech input, particularly when ASR fails.
- Evidence anchors:
  - [abstract] "...a large language model (LLM)-assisted touch interface... The touch method was navigated through an LLM-powered 'task prompter,' which integrated the user's history and smart environment to suggest contextually-appropriate commands."
  - [section] "The LLM code maintained context for the prompts in three global variables: user history, a task list, and a description of the smart environment."
  - [corpus] Weak/No direct corpus evidence for this specific LLM-for-context mechanism.
- Break condition: This mechanism's benefits would break down if the LLM's predictions are consistently inaccurate or irrelevant, increasing the cognitive load of navigating the touch interface.

### Mechanism 2
- Claim: Modern ASR systems, despite general limitations with atypical speech, can successfully understand a significant portion of commands from DHH individuals with deaf speech in a constrained IPA context.
- Mechanism: Commercial IPA ASR systems (like Alexa's) use constrained, command-and-control language models. This limited vocabulary and grammar space, combined with modern ASR advances, increases the probability of correctly recognizing atypical speech patterns compared to open-domain transcription.
- Core assumption: The recognition success observed in a study setting will transfer to real-world use, and the high accuracy for some users is not solely an artifact of the study's controlled environment.
- Evidence anchors:
  - [abstract] "Word error rates were low, with half of participants experiencing zero errors."
  - [section] "Alexa's recognition accuracy on deaf-accented speech was surprisingly high, given that half of our participants exhibited no recognition errors..."
  - [corpus] [Related work by Glasser et al. (2017)] confirms that ASR services present usability challenges for DHH users, providing the baseline problem this paper's findings address.
- Break condition: This finding would not hold if the command vocabulary is significantly expanded or if users issue highly complex, multi-turn queries that fall outside the constrained command-and-control model.

### Mechanism 3
- Claim: Human-mediated re-speaking of deaf speech, while accurate, introduces latency and user uncertainty, making it a less desirable long-term solution than improving native ASR.
- Mechanism: A human facilitator acts as a highly accurate "re-speaking" system, understanding deaf speech and re-voicing it in a standard accent for the IPA. However, this adds an unavoidable network/human-in-the-loop delay and can be confusing for users who hear the re-spoken command, creating a dependency on an intermediary.
- Core assumption: The delays and cognitive load of mediated communication (even with a human) are more detrimental to the user experience than the potential errors from a direct, improved ASR system.
- Evidence anchors:
  - [abstract] "...re-spoken speech via human wizard... Qualitative results showed mixed opinions, with latency and limited options cited as touch interface issues."
  - [section] "Participants who could envision accessible input options were interested in using IPAs more... they suggested tasks such as 'ordering food, shopping...' and one participant compared the interaction to 'a friend that responds to you' (P13)." This implies a desire for direct, fluid interaction, not mediated.
  - [corpus] No direct corpus evidence on the mechanism of human-mediated re-speaking for IPAs.
- Break condition: The conclusion that re-speaking is less desirable would be invalid if the ASR for deaf speech could not be improved beyond a certain threshold, making a highly accurate but slightly slower mediated system the only viable option for some users.

## Foundational Learning

- Concept: **Wizard-of-Oz (WoZ) Prototyping**
  - Why needed here: The paper uses a WoZ setup (a human "wizard") to simulate a system that can perfectly understand and re-speak deaf-accented speech. This is a crucial technique for evaluating the *potential* usability of a technology that doesn't yet exist at the required performance level. Understanding this is key to interpreting the "Facilitated English" results.
  - Quick check question: In this study, what role did the human "wizard" play, and why was this method chosen over using an existing commercial re-speaking tool?

- Concept: **Large Language Model (LLM) as a UI Orchestrator**
  - Why needed here: The paper's main technical contribution is using an LLM not for conversation, but to dynamically generate the user interface (buttons/options) for a touch-based IPA control. This is a different paradigm than using an LLM to understand a user's typed natural language query.
  - Quick check question: What three pieces of context were fed to the LLM to generate command suggestions for the touch interface?

- Concept: **System Usability Scale (SUS) for DHH Users**
  - Why needed here: SUS is a standard quantitative measure, but its validity can be affected by language and cultural factors. The paper mentions using a psychometrically validated ASL translation (ASL-SUS), which is a critical detail for obtaining reliable data from this user population.
  - Quick check question: Why did the researchers choose to offer the System Usability Scale (SUS) in both written English and a validated ASL translation?

## Architecture Onboarding

- Component map: Web App Frontend (tablet) -> Flask Server Backend -> LLM Engine (GPT-4o API)
- Critical path: The critical path for user experience is the request-response cycle from the tablet to the LLM API. A delay here directly translates to UI latency, which was a major complaint. The path is: `User Tap -> Web App -> Flask Backend -> LLM API -> Parse Response -> Web App -> Update UI`
- Design tradeoffs: The main tradeoff is between **flexibility/accuracy** and **latency/complexity**. The LLM-based interface offers dynamic, context-aware options but suffers from ~2-second API latency. A rule-based, static system would be instant but far less usable and would not adapt to the user. The current design chose flexibility, exposing users to the latency.
- Failure signatures: A key failure mode is when the LLM generates irrelevant or nonsensical command options, confusing the user. The system did not implement a "stop" command to immediately interrupt this process. Another failure is if the user's desired option is not in the LLM's generated list, forcing them to type, which negates the benefit of the predictive interface.
- First 3 experiments:
  1. **Latency Abatement:** Implement "pre-fetching" by querying the LLM for the next set of potential options as soon as the user makes a selection, rather than waiting for the interaction. Measure if this reduces perceived latency.
  2. **Input Modality Comparison:** Run a between-subjects study with DHH users, comparing the LLM-assisted touch interface to a standard "Tap to Alexa" interface and a direct speech input. Measure task completion time, error rate, and user preference (SUS, NPS).
  3. **LLM Hyperparameter Tuning:** Systematically vary the LLM `temperature` parameter and the specificity of the system prompt to optimize for the most relevant and consistently ordered command suggestions. Measure the percentage of times the user's intended command appears in the top 3 generated options.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the usability of commercially available, trainable re-speaking technology (e.g., VoiceITT) compare to the Wizard-of-Oz baseline, specifically regarding the trade-off between system training time and interaction latency?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section that it "will be important to isolate the effect of the time spent on training the systems from the interaction with the IPAs" and call for testing "re-speaking approaches with commercially available technology."
- Why unresolved: The study utilized a human Wizard to simulate a best-case scenario for re-speaking without training overhead. It did not test real-world tools which require user training but may offer lower latency than the human mediator.
- What evidence would resolve it: A comparative study measuring SUS scores and task completion times for DHH users using commercial re-speaking apps (with full training) versus the WoZ method established in this paper.

### Open Question 2
- Question: Can pre-fetching LLM API responses effectively reduce the latency of touch-based interfaces to a level that matches the speed and fluidity of direct voice interactions?
- Basis in paper: [explicit] The authors identify latency as a major barrier and suggest a specific engineering solution: "One potential way to improve responsiveness would be to fetch appropriate LLM API responses to every option that the user can select in advance."
- Why unresolved: The current implementation suffered from a 6-8 second delay to build commands. The paper proposes pre-fetching as a solution but did not implement or test this optimization.
- What evidence would resolve it: Usability data (specifically regarding perceived speed and SUS scores) from a version of the LLM-touch interface that utilizes predictive pre-fetching compared against the current baseline and voice conditions.

### Open Question 3
- Question: How can LLM interfaces be effectively controlled through Sign Language and Deaf speech to enable hands-free interaction for DHH users?
- Basis in paper: [explicit] The authors argue that future studies should focus on hands-free interaction, specifically noting that "researchers may consider integrating an LLM interface controlled through sign and Deaf speech."
- Why unresolved: The current study focused on touch and voiced/spoken English inputs. While participants expressed a strong desire for ASL recognition, the specific integration of LLMs with Sign Language input modalities remains unexplored.
- What evidence would resolve it: A prototype study testing an LLM-based task prompter that accepts Sign Language or Deaf speech as input, measuring task success and user preference against the touch and voiced conditions.

## Limitations
- Controlled lab environment may not capture real-world variability in network conditions and home device configurations
- LLM-assisted interface suffered from 6-8 second latency due to serial API calls
- Touch interface limited to 8 predefined options at each step, potentially restricting expressiveness
- Did not test ASL recognition, which participants identified as their preferred input method

## Confidence

**High Confidence:** The finding that modern ASR systems can achieve surprisingly high accuracy (50% zero-error rate) with deaf speech in constrained command contexts. This is well-supported by quantitative data and aligns with known ASR performance patterns in limited-vocabulary domains.

**Medium Confidence:** The usability equivalence across input methods (SUS scores not significantly different). While statistically supported, this conclusion is based on a small sample (n=13) and may not generalize to broader populations or longer-term usage patterns.

**Low Confidence:** The practical viability of the LLM-assisted touch interface as a mainstream solution. The high latency and limited option space raise significant questions about real-world adoption, though the concept shows technical promise.

## Next Checks
1. **Latency Optimization Validation:** Implement the suggested "pre-fetching" approach where the system queries the LLM for all potential next options immediately after a user selection. Measure whether this reduces perceived latency to under 2 seconds and test impact on SUS scores.

2. **Real-World Deployment Study:** Deploy the LLM-assisted interface in participants' actual homes for one week, measuring task completion rates, error frequencies, and user satisfaction compared to standard touch interfaces. Focus on network variability and device ecosystem differences.

3. **ASL Recognition Feasibility:** Conduct a preliminary study testing commercial ASL recognition systems (e.g., Kinect, computer vision approaches) for simple IPA commands. Measure recognition accuracy and compare SUS scores to the speech and touch methods tested in the current study.