---
ver: rpa2
title: 'OSCAR: Online Soft Compression And Reranking'
arxiv_id: '2504.07109'
source_url: https://arxiv.org/abs/2504.07109
tags:
- oscar
- compression
- arxiv
- documents
- reranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSCAR is an online soft compression method for retrieval-augmented
  generation that dynamically compresses retrieved documents into embeddings at inference
  time. Unlike prior offline methods, OSCAR adjusts compression based on the query,
  enabling higher compression rates without precomputation.
---

# OSCAR: Online Soft Compression And Reranking

## Quick Facts
- arXiv ID: 2504.07109
- Source URL: https://arxiv.org/abs/2504.07109
- Authors: Maxime Louis; Thibault Formal; Hervé Dejean; Stéphane Clinchant
- Reference count: 40
- Primary result: 2-5x faster inference with minimal accuracy loss across LLMs from 1B to 24B parameters

## Executive Summary
OSCAR is an online soft compression method for retrieval-augmented generation that dynamically compresses retrieved documents into embeddings at inference time. Unlike prior offline methods, OSCAR adjusts compression based on the query, enabling higher compression rates without precomputation. It also supports joint reranking, leveraging the shared structure between compression and reranking tasks. OSCAR achieves 2-5x faster inference with minimal accuracy loss across LLMs from 1B to 24B parameters, outperforming both hard and soft compression baselines.

## Method Summary
OSCAR uses a transformer-based compressor to convert retrieved documents into fixed-size embeddings conditioned on the query. The compressor can be either the first N layers of the generator backbone (OSCAR-N-Layers) or a pretrained small model with alignment MLP (OSCAR-llama). These embeddings are then concatenated with query tokens and fed to the generator LLM for response generation. The method supports joint compression-reranking by adding a special [RR] token whose hidden state is mapped to a relevance score. Training uses distillation losses from teacher models for both generation and reranking tasks.

## Key Results
- Achieves 2-5x faster inference compared to uncompressed RAG
- Maintains comparable accuracy to uncompressed models across multiple LLMs (1B-24B parameters)
- Outperforms both hard and soft compression baselines in speed-accuracy trade-off
- Supports joint compression-reranking with minimal overhead

## Why This Works (Mechanism)

### Mechanism 1: Query-Conditioned Soft Embedding Compression
Conditioning document compression on the query yields higher-fidelity representations at equivalent or better compression rates compared to query-agnostic offline methods. The compressor LLM processes the query q and document d_i together with learned memory tokens. Attention operates jointly over query and document, allowing memory tokens to extract query-relevant information rather than attempting lossless document encoding.

### Mechanism 2: Layer-Efficient Compressor Design
Using early layers of the generator's own backbone as compressor eliminates representation alignment overhead while maintaining sufficient expressivity for compression. OSCAR-N-Layers constructs a headless transformer from the first N layers of the pretrained generator backbone. Since hidden representations are already in the generator's native space, no alignment MLP or pretraining is required.

### Mechanism 3: Joint Compression-Reranking via Task Multiplexing
Compression and reranking share sufficient computational structure that a single forward pass can produce both outputs with negligible overhead. A special [RR] token is appended to the compressor prompt. The hidden state at this token position is mapped via a dense layer to a relevance score. Since reranking is already a standard RAG component, the compression computation is amortized over an operation that would occur anyway.

## Foundational Learning

- **Attention-based context aggregation**
  - Why needed here: OSCAR relies on understanding how transformer attention pools information across query-document pairs into fixed-size memory token representations.
  - Quick check question: Can you explain why adding memory tokens to the input sequence allows the model to create a compressed summary representation?

- **Knowledge distillation (sentence-level)**
  - Why needed here: The generator is trained to match the output distribution of a teacher LLM given uncompressed context, transferring its behavior to the compressed-input regime.
  - Quick check question: What is the difference between token-level and sentence-level distillation, and why might sentence-level be preferred for generation tasks?

- **Reranking as cross-encoding**
  - Why needed here: OSCAR's reranking component distills from a cross-encoder, so understanding joint query-document scoring versus bi-encoder retrieval is essential.
  - Quick check question: Why does a cross-encoder typically outperform a bi-encoder for reranking, and what is the computational tradeoff?

## Architecture Onboarding

- **Component map**: Document retrieval -> OSCAR compression -> Query + compressed embeddings -> Generator LLM
- **Critical path**:
  1. Document retrieval (SPLADE-v3 recommended)
  2. Optional: external reranking (DeBERTa-v3) if not using OSCAR's joint reranker
  3. OSCAR compression: batch documents through compressor
  4. Concatenate query tokens + all document embeddings
  5. Generator autoregressive decoding

- **Design tradeoffs**:
  - OSCAR-llama vs. OSCAR-N-Layers: llama variant is faster and often higher quality but requires pretraining; N-Layers is simpler to train (direct fine-tuning) but scales with N.
  - Compression rate (l tokens per document): Paper uses l=8 for 128-token documents (16x compression). Higher rates yield diminishing FLOP returns.
  - Number of layers: Performance plateaus around 8-10 layers; 5 layers is slightly weaker but faster.

- **Failure signatures**:
  - Poor retrieval quality: OSCAR inherits retrieval noise; BM25-only experiments show ~5-10 point average drops.
  - Insufficient pretraining for OSCAR-llama: Table 8 shows 4-14 point drops without pretraining due to hidden space misalignment.
  - Too few layers: OSCAR-5L underperforms on complex datasets (HotpotQA multi-hop reasoning).
  - Out-of-domain retrieval: BioASQ with BM25 shows larger degradation.

- **First 3 experiments**:
  1. Baseline reproduction: Train OSCAR-8L on Mistral-7B with provided hyperparameters, evaluate on 2-3 datasets from {NQ, TriviaQA, HotpotQA}.
  2. Ablation on compressor depth: Train OSCAR-{5,8,10,12}L variants and plot LLM-eval vs. FLOP to identify optimal layer count.
  3. End-to-end reranking integration: Train joint OSCAR-llama with λ=0.05 reranking loss, deploy in full RAG pipeline replacing external reranker.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the interpretability of OSCAR's soft compression embeddings be improved to match the transparency of hard compression methods?
- Basis in paper: [explicit] "OSCAR... lacks the interpretability of hard compression methods. In this section, we offer a glimpse into the content of the compressed embeddings" (Section 5.3)
- Why unresolved: The paper only provides preliminary analysis via needle-in-a-haystack tests and logit attributions; no method is proposed for systematic interpretability.
- What evidence would resolve it: Development of interpretability techniques that can reliably map compressed embeddings back to semantically meaningful content, validated through human evaluation.

### Open Question 2
- Question: What is the minimum compressor size that can achieve performance competitive with the uncompressed baseline?
- Basis in paper: [inferred] Appendix J shows smaller compressors (bert-base, modern-bert) "reach some level of accuracy which remains below the uncompressed model," but no systematic boundary was established.
- Why unresolved: The experiments only test three compressor architectures without determining the efficiency frontier.
- What evidence would resolve it: A comprehensive sweep of compressor sizes/architectures identifying the Pareto-optimal trade-off between compression efficiency and accuracy.

### Open Question 3
- Question: How can OSCAR be adapted to become backbone-agnostic while maintaining its efficiency advantages?
- Basis in paper: [explicit] "Unlike most hard compression methods for RAG, OSCAR models are backbone-specific and need to be retrained for every different generation LLM" (Section 4.3).
- Why unresolved: The paper demonstrates OSCAR works across backbone sizes (1B to 24B) but requires retraining each time; no transfer or universal compression mechanism is explored.
- What evidence would resolve it: Demonstration of a single trained compressor that generalizes across multiple generator LLMs without retraining, with performance metrics comparable to backbone-specific models.

### Open Question 4
- Question: Can OSCAR's reranking capability match teacher reranker performance without sacrificing efficiency?
- Basis in paper: [explicit] "To match the teacher's performance on BEIR, OSCAR requires an increased model depth to 16 layers. However, this model is less efficient, and its actual e2e performance... remains unchanged" (Section 4.4).
- Why unresolved: The paper identifies a trade-off but does not propose methods to achieve both strong reranking and compression efficiency simultaneously.
- What evidence would resolve it: An OSCAR variant that achieves BEIR reranking scores within 1-2 points of the DeBERTa-v3 teacher while maintaining the 2-3x speedup of efficient compressor variants.

## Limitations

- **Generalization to smaller retrievers**: Performance with weaker retrievers (BM25, ANCE) shows larger accuracy drops (~5-10 points on average) compared to strong retrievers.
- **Pretraining requirement for OSCAR-llama**: The Llama-based compressor variant requires careful pretraining to align hidden spaces; without this, performance degrades substantially.
- **Optimal compression rate and layer count**: While 8 memory tokens and 8-10 compressor layers appear empirically optimal, these choices may not generalize across document types or backbone architectures.

## Confidence

- **Query-conditioned compression provides quality gains**: High
- **OSCAR-N-Layers design eliminates pretraining overhead**: High
- **Joint compression-reranking amortization is effective**: High

## Next Checks

1. **Cross-retriever robustness study**: Evaluate OSCAR with BM25 and ANCE retrievers on 3-5 BEIR datasets to quantify performance degradation and compare recovery strategies.

2. **Cross-lingual and domain transfer**: Fine-tune OSCAR-llama from English to non-English datasets (e.g., FiQA, Touché V2) or biomedical domain (BioASQ) to assess pretraining transfer limits.

3. **Adaptive compression rate exploration**: Implement variable memory token allocation (e.g., 4-16 tokens per document based on estimated information density) and measure trade-offs between LLM-eval accuracy and T-FLOP savings across diverse query types.