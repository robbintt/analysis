---
ver: rpa2
title: Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank
  Collapse
arxiv_id: '2505.16284'
source_url: https://arxiv.org/abs/2505.16284
tags:
- arxiv
- follows
- step
- attention
- softm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves that large attention weights, not skip connections,\
  \ are essential for transformer expressivity. It shows that Self Attention Networks\
  \ with small weights suffer from layer collapse\u2014meaning the network can be\
  \ well-approximated by a single-layer network\u2014even when skip connections are\
  \ present."
---

# Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse

## Quick Facts
- **arXiv ID**: 2505.16284
- **Source URL**: https://arxiv.org/abs/2505.16284
- **Reference count**: 30
- **Primary result**: Large attention weights, not skip connections, are essential for transformer expressivity; small weights cause layer collapse even with skip connections.

## Executive Summary
This paper proves that large attention weights are essential for transformer expressivity, challenging the prior belief that skip connections prevent representational weaknesses. The authors introduce "layer collapse" - a phenomenon where multi-layer transformers with small weights can be approximated by single-layer networks within O(η) error. Through rigorous analysis, they demonstrate that when attention weight matrices have bounded ℓ∞ norm η, the network's output can be well-approximated by a one-layer network, regardless of skip connections. This work establishes a fundamental expressivity-complexity tradeoff: quadratic attention time is necessary for expressive transformers, while subquadratic algorithms require small weights which cause layer collapse.

## Method Summary
The authors analyze Self Attention Networks (SAN) with L layers, H heads per layer, and weight matrices bounded in ℓ∞ norm by η. They introduce a novel notion of "layer collapse" analogous to rank collapse, proving that bounded weights cause multi-layer networks to be approximable by single-layer networks within O(η) error. The proof technique involves tracking perturbation propagation through layers using the Res function (measuring deviation from constant rows) and analyzing softmax perturbation sensitivity under θ-balanced conditions. The analysis shows that skip connections alone cannot prevent this collapse when weights remain small.

## Key Results
- Layer collapse occurs when attention weights have bounded ℓ∞ norm: multi-layer networks can be approximated within O(η) by single-layer networks
- Skip connections alone cannot prevent representational weakness when weights are small
- Expressivity-complexity tradeoff: quadratic attention time is unavoidable for expressive transformers; subquadratic algorithms require small weights which cause layer collapse

## Why This Works (Mechanism)

### Mechanism 1: Layer Collapse via Bounded Weights
- Claim: When attention weights have bounded ℓ∞ norm η, multi-layer transformers can be approximated by single-layer networks within O(η) error
- Mechanism: Lower attention layers with small weights produce near-constant perturbations to their inputs (measured via Res function). Lemma 5.1 bounds how much attention output deviates from constant across tokens: ∥Res(SAtt(X))∥∞ ≤ K·∥Res(X)∥∞. By iteratively applying perturbation bounds, each layer's contribution becomes removable without substantial output change
- Core assumption: Weights bounded by η; inputs bounded by φ₀; normalization factor β ensures balance property
- Evidence anchors:
  - [abstract] "the network's output can be approximated within O(η) by a one-layer network"
  - [section 5.3] Theorem 5.3: "there exists a SAtt with residuals S′ with just one layer so that, for any X, we have ∥S(X) − S′(X)∥∞ ≤ O(η) · ∥X∥∞"
- Break condition: If η grows large (unbounded weights), perturbation bounds no longer hold; approximation guarantee fails

### Mechanism 2: Skip Connection Insufficiency
- Claim: Skip connections alone cannot prevent representational weakness when weights are small
- Mechanism: The identity network example (all V weights set to 0) shows skip connections preserve input rank but do not engage attention. More generally, Lemma C.1 shows that even with skip connections, if ∥Res(Ai)∥∞ is small, subsequent layers' softmax outputs differ by at most ε₀ = 3g(2Hε)
- Core assumption: Skip connections exist but attention weights remain small; multi-head structure with H heads
- Evidence anchors:
  - [abstract] "our result shows that even with skip connections, if the weights are small, then layer collapse still occurs"
  - [section 1] "only large weights, and not skip connections, can prevent these representational weaknesses"
- Break condition: If V weights are large enough that attention outputs have significant ∥Res(Ai)∥∞, skip+attention combination achieves full expressivity

### Mechanism 3: Expressivity-Complexity Tradeoff
- Claim: Quadratic attention time is unavoidable for expressive transformers; subquadratic algorithms require small weights which cause layer collapse
- Mechanism: Prior work proved small weights enable almost-linear-time attention via low-rank approximation. This paper shows small weights force layer collapse. Therefore: expressive models need large weights → large weights require quadratic time
- Core assumption: Complexity-theoretic assumption that no subquadratic algorithm exists for attention with unbounded weights
- Evidence anchors:
  - [abstract] "quadratic time is necessary unless the model weights are small, in which case almost linear time algorithms are possible"
  - [section 1] "Thus, the quadratic running time of attention is unavoidable for expressive transformers"
- Break condition: If new algorithms break the complexity lower bound, or if layer collapse is acceptable for the task, tradeoff shifts

## Foundational Learning

- **Concept: ℓ∞ norm bounding**
  - Why needed here: The main theorem uses ℓ∞ bounds on weight matrices to control perturbation propagation. Understanding entrywise max-norm vs. spectral norm distinctions is critical
  - Quick check question: Given matrix W with entries in [-0.1, 0.1], what is ∥W∥∞?

- **Concept: Rank collapse vs. Layer collapse**
  - Why needed here: Prior work defined rank collapse (output always near rank-1); this paper introduces layer collapse (entire network approximable by one layer). The distinction matters for interpreting what skip connections fix
  - Quick check question: Can a network have rank collapse but not layer collapse? What about the reverse?

- **Concept: Res function and balanced matrices**
  - Why needed here: Res(X) = X − 1ny^⊤ measures deviation from constant rows; θ-balance controls softmax perturbation sensitivity. These are the core analytical tools
  - Quick check question: For a 3×2 matrix with all rows identical, what is Res(X)?

## Architecture Onboarding

- **Component map:**
  - Input X ∈ ℝ^(n×d) -> L attention layers with H heads each -> Output
  - Each head: Q,K,V weights (∈ ℝ^(d×d)), softmax attention, residual addition

- **Critical path:**
  1. Initialize weights with controlled ℓ∞ norm
  2. Verify input boundedness: ∥X₀∥∞ ≤ φ₀
  3. Forward pass accumulates perturbations; ε_ℓ = 2ηφ₀(1+Hη)^ℓ bounds per-layer deviation
  4. Final approximation error: O(η) if collapse occurs

- **Design tradeoffs:**
  - Large η → full expressivity, quadratic compute
  - Small η → fast inference, potential layer collapse
  - More layers L → higher ε_ℓ growth (exponential in L for fixed η)

- **Failure signatures:**
  - Layer collapse: Adding layers does not improve validation loss beyond 1-2 layers
  - Small η regime: Attention outputs cluster near row-wise constants
  - Skip-connection-only fix: Identity-like behavior, attention gradients vanish

- **First 3 experiments:**
  1. **Weight norm sweep**: Train identical architecture with varying η bounds; plot final loss vs. depth. Expect: small η shows flat loss-after-L=1; large η shows continued improvement
  2. **Ablation on skip connections**: Compare (small weights + skip) vs. (small weights - skip) vs. (large weights + skip). Hypothesis: skip alone does not recover expressivity
  3. **Layer-wise Res norm tracking**: Instrument ∥Res(X_ℓ)∥∞ during training. If bounded by K·∥Res(X₀)∥∞ across layers, collapse is occurring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the layer collapse analysis be extended to ℓ₂ or ℓ₁ norms, and would these provide tighter or more meaningful bounds in practical parameter regimes?
- Basis in paper: [explicit] Section E (Limitations) states: "Currently our result mainly focus on ℓ∞. We believe it is also worthwhile to consider the ℓ₂ norm or ℓ₁ norm in future work, as they may give more meaningful bounds in certain parameter regimes."
- Why unresolved: The proof techniques rely heavily on ℓ∞-norm properties and softmax perturbation bounds that may not directly generalize
- What evidence would resolve it: A formal analysis proving or disproving layer collapse under ℓ₂ or ℓ₁ norm bounds, with comparison of tightness to ℓ∞ results

### Open Question 2
- Question: How does the tension between preventing layer collapse (requiring large weights) and common regularization practices like weight decay resolve in empirically trained transformers?
- Basis in paper: [inferred] The paper proves large weights are necessary for expressivity, but weight decay explicitly penalizes large weights during training—creating a fundamental tension not addressed
- Why unresolved: The paper focuses on expressivity theory rather than training dynamics or optimization
- What evidence would resolve it: Empirical studies measuring layer collapse metrics across training with varying weight decay strengths, or theoretical analysis of the expressivity-regularization tradeoff

### Open Question 3
- Question: Do the weight magnitudes in trained large language models fall into regimes where layer collapse theory predicts approximation by shallow networks?
- Basis in paper: [inferred] Remark D.2 discusses parameter regimes but leaves open whether practical LLMs satisfy the small-weight conditions leading to collapse
- Why unresolved: The bounds depend on relationships between η, φ₀, H, and L that may or may not hold in practice
- What evidence would resolve it: Measurements of attention weight norms in trained LLMs combined with verification of whether the O(η) approximation bounds are non-trivial

### Open Question 4
- Question: How do modern architectural innovations like LayerNorm, RoPE positional embeddings, and grouped-query attention interact with the layer collapse phenomenon?
- Basis in paper: [inferred] The analysis considers only core self-attention with skip connections, excluding other standard transformer components
- Why unresolved: Layer normalization and positional encodings fundamentally alter signal propagation through layers
- What evidence would resolve it: Extending the theoretical framework to include these components, or empirical ablation studies isolating their effects on rank/layer collapse metrics

## Limitations
- Analysis assumes bounded ℓ∞ norms and specific input/output scaling assumptions that may not hold in practice for deep transformers with learned norms
- Complexity-expressivity tradeoff relies on external complexity-theoretic assumptions rather than direct empirical validation
- Proof technique focuses on worst-case analysis and may not capture practical scenarios where initialization strategies or adaptive optimization implicitly control weight growth

## Confidence
- **High confidence**: Layer collapse occurs when attention weights are bounded in ℓ∞ norm (Theorem 5.3 and Lemma 5.1 are rigorously proven with explicit error bounds)
- **Medium confidence**: Skip connections alone cannot prevent layer collapse (supported by identity network counterexample and perturbation analysis, but relies on worst-case assumptions)
- **Medium confidence**: Expressivity-Complexity tradeoff (derived from cited complexity results rather than direct proof; empirical validation needed)

## Next Checks
1. **Empirical weight norm sweep**: Train transformers with explicit ℓ∞ weight constraints across multiple depths and datasets; measure layer collapse via layer-wise ablation tests and validation loss plateaus
2. **Initialization sensitivity analysis**: Compare standard initialization (e.g., Kaiming, Xavier) against bounded-norm initialization; track Res norm evolution and final model expressivity
3. **Attention gradient flow study**: Instrument attention gradients during training; verify that small-η models show vanishing gradient signals while large-η models maintain gradient diversity across layers