---
ver: rpa2
title: Analysis of human visual field information using machine learning methods and
  assessment of their accuracy
arxiv_id: '2503.14562'
source_url: https://arxiv.org/abs/2503.14562
tags:
- methods
- research
- glaucoma
- page
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of diagnosing glaucoma using perimetric
  images by applying machine learning methods to classify visual field test results.
  The authors built a classifier using a labeled dataset of perimetric images collected
  from the Tomey perimeter, focusing on two classes: glaucoma and other visual diseases.'
---

# Analysis of human visual field information using machine learning methods and assessment of their accuracy

## Quick Facts
- **arXiv ID:** 2503.14562
- **Source URL:** https://arxiv.org/abs/2503.14562
- **Reference count:** 0
- **Primary result:** Logistic regression achieved 0.72 accuracy in classifying glaucoma from perimetric images, outperforming random forest (0.69), SGD (0.69), and naive Bayes (0.45).

## Executive Summary
This paper evaluates machine learning methods for diagnosing glaucoma using perimetric images. The authors collected a dataset of visual field tests from the Tomey perimeter, preprocessed the images, and applied four classical machine learning algorithms: logistic regression, random forest, stochastic gradient descent, and naive Bayes. Logistic regression achieved the highest accuracy at 0.72, demonstrating its effectiveness for this binary classification task. The study suggests potential for machine learning in glaucoma diagnosis and identifies areas for future research including deep learning approaches and more comprehensive evaluation metrics.

## Method Summary
The study involved collecting perimetric images from the Tomey perimeter, preprocessing them to reduce noise and normalize brightness/contrast, and splitting each image into two eyes (OD/OS) for analysis. The dataset was labeled as glaucoma (class 1) or other visual diseases (class 0). After preprocessing and feature extraction (method unspecified), the data was split into training and testing sets. Four machine learning algorithms were trained and evaluated: logistic regression, random forest, stochastic gradient descent (SGD), and naive Bayes. The evaluation used standard metrics including accuracy, precision, recall, and F1-score on a test set of 29 samples.

## Key Results
- Logistic regression achieved the best accuracy at 0.72 on the test set
- Random forest and SGD both achieved 0.69 accuracy
- Naive Bayes performed poorly with only 0.45 accuracy
- Class 1 (glaucoma) showed higher precision (0.75) and recall (0.75) than class 0 (0.69/0.69)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Logistic regression outperformed other classical ML methods on this binary classification task for perimetric images.
- **Mechanism:** Logistic regression models the probability of class membership using a sigmoid function applied to a linear combination of features. Its effectiveness here likely stems from the relatively low-dimensional feature space extracted from the perimetric images, where linear decision boundaries may capture the signal adequately without overfitting.
- **Core assumption:** The extracted features from visual field images contain separable patterns that distinguish glaucoma from other ocular pathologies, and these patterns are approximately linearly separable in feature space.
- **Evidence anchors:** The paper reports accuracy of 0.72 for logistic regression vs. 0.69 for random forest and SGD, and 0.45 for naive Bayes; Table 1 shows precision/recall/F1 of 0.69/0.69/0.69 for class 0 and 0.75/0.75/0.75 for class 1 (glaucoma).

### Mechanism 2
- **Claim:** Naive Bayes performs poorly because its independence assumption is violated in visual field features.
- **Mechanism:** Naive Bayes assumes conditional independence of features given the class label. Visual field measurements from neighboring points on the retina are spatially correlated, violating this assumption and leading to poorly calibrated probabilities.
- **Core assumption:** Perimetric features are not conditionally independent given the diagnosis; spatial and functional correlations exist among visual field points.
- **Evidence anchors:** The paper explicitly states "its assumption about feature independence may limit accuracy in our task"; naive Bayes achieved only 0.45 accuracy with F1-scores of 0.11 (class 0) and 0.60 (class 1).

### Mechanism 3
- **Claim:** Data preprocessing (image quality normalization) enables meaningful feature extraction despite heterogeneous image sources.
- **Mechanism:** Images were captured from paper or monitor screens; preprocessing reduced noise and normalized brightness/contrast. This standardization reduces domain shift and allows the classifier to focus on diagnostically relevant patterns rather than acquisition artifacts.
- **Core assumption:** The signal related to glaucoma is preserved or enhanced through preprocessing, and noise/artifacts do not systematically bias one class.
- **Evidence anchors:** "Since image quality was satisfactory... they could be modified to reduce noise, normalize brightness and contrast"; related corpus papers on mobile fundus analysis emphasize preprocessing as prerequisites.

## Foundational Learning

- **Concept: Visual Field (Perimetry) Testing**
  - **Why needed here:** Perimetric images represent sensitivity across the retina; glaucoma typically causes specific patterns of peripheral vision loss. Understanding what these images represent is essential to interpret classifier behavior.
  - **Quick check question:** Can you explain why a superior arcuate visual field defect might indicate glaucomatous damage?

- **Concept: Binary Classification Metrics (Precision, Recall, F1, Support)**
  - **Why needed here:** The paper evaluates models using these metrics; understanding their trade-offs is necessary to assess whether the model is clinically useful (e.g., minimizing false negatives in diagnosis).
  - **Quick check question:** If a glaucoma classifier has high precision but low recall, what is the clinical implication?

- **Concept: Train/Test Split and Overfitting**
  - **Why needed here:** The paper reports results on a test set of 29 samples. Understanding generalization risk is critical given the small evaluation sample.
  - **Quick check question:** Why might 0.72 accuracy on 29 test samples not reflect true deployment performance?

## Architecture Onboarding

- **Component map:** Data Ingestion -> Preprocessing (denoise, normalize) -> Feature Extraction -> Model Training -> Evaluation
- **Critical path:** Image quality -> Preprocessing fidelity -> Feature relevance -> Model choice -> Evaluation on held-out set. The small test set (29 samples) is a bottleneck for statistical confidence.
- **Design tradeoffs:** Logistic regression offers interpretability and simplicity but may underfit complex patterns; random forest and SGD capture more non-linearity but showed no improvement here (0.69 accuracy); naive Bayes is fast but unsuitable due to feature correlation; deep learning (CNNs) is proposed for future work but requires more data and compute.
- **Failure signatures:** Accuracy drops if preprocessing removes discriminative features; class imbalance or small test set leads to unstable metric estimates; domain shift (new perimeter device, different patient demographics) may cause silent degradation.
- **First 3 experiments:** 1) Baseline replication: Reproduce logistic regression pipeline with the same preprocessing; confirm accuracy ~0.72 on the provided test set; 2) Cross-validation stability: Run 5-fold or 10-fold cross-validation to estimate variance in accuracy given the small sample size; 3) Feature ablation: Test whether removing spatially correlated features improves naive Bayes or whether adding interaction terms improves logistic regression.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can convolutional neural networks (CNNs) outperform logistic regression in classifying glaucoma from this dataset?
- **Basis in paper:** [explicit] The authors state, "it is possible to investigate the training of this dataset on convolutional neural networks."
- **Why unresolved:** The current study was limited to classical machine learning algorithms (logistic regression, random forest, SGD, naive Bayes) and did not test deep learning architectures.
- **What evidence would resolve it:** Comparative performance metrics (accuracy, F1-score) of a CNN model trained on the same dataset versus the current logistic regression benchmark (0.72).

### Open Question 2
- **Question:** How does the inclusion of pathologically healthy patients affect the classification accuracy?
- **Basis in paper:** [explicit] The conclusion notes the future necessity to study a "group of pathologically healthy patients."
- **Why unresolved:** The current binary classification was restricted to "glaucoma" versus "other visual diseases," excluding a specific control group of healthy individuals.
- **What evidence would resolve it:** Evaluation of the classifier's performance on a dataset explicitly containing labeled images of healthy visual fields.

### Open Question 3
- **Question:** What are the ROC-AUC curves and error matrices for the applied machine learning methods?
- **Basis in paper:** [explicit] The paper lists "matrices of errors and ROC-curves AUC" as part of planned future work.
- **Why unresolved:** The study only reported accuracy, precision, recall, and F1-scores, omitting the Receiver Operating Characteristic (ROC) analysis.
- **What evidence would resolve it:** Publication of the ROC curves and the Area Under the Curve (AUC) values to visualize the trade-off between sensitivity and specificity.

## Limitations
- Small test set (n=29) limits statistical robustness and generalizability of the 0.72 accuracy result
- Feature extraction pipeline is unspecified, making exact replication difficult
- Unknown dataset size and composition prevents assessment of class balance and model reliability

## Confidence

- **High confidence:** Logistic regression outperforms other classical ML methods on this dataset (0.72 vs. 0.69 for RF/SGD and 0.45 for NB), as directly supported by reported metrics
- **Medium confidence:** Naive Bayes performs poorly due to violated independence assumptions—plausible but not empirically tested here (no ablation study provided)
- **Medium confidence:** Preprocessing enables meaningful feature extraction—reasonable given the source heterogeneity (paper/screen capture), but not directly validated

## Next Checks
1. Run 5- or 10-fold cross-validation on the dataset to quantify variance in accuracy given the small sample size and assess overfitting risk
2. Generate a confusion matrix and compute sensitivity/specificity to evaluate false negative rate in glaucoma detection—critical for clinical deployment
3. Systematically remove or decorrelate features to test whether naive Bayes performance improves or whether logistic regression benefits from interaction terms