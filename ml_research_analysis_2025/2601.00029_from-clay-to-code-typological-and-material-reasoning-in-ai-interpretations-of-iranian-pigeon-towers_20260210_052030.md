---
ver: rpa2
title: 'From Clay to Code: Typological and Material Reasoning in AI Interpretations
  of Iranian Pigeon Towers'
arxiv_id: '2601.00029'
source_url: https://arxiv.org/abs/2601.00029
tags:
- vernacular
- reasoning
- intelligence
- form
- realism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated how three generative AI models (Midjourney
  v6, DALL-E 3, DreamStudio/SDXL) interpret Iranian pigeon towers under referential,
  adaptive, and speculative prompts. Across five architectural criteria, AI systems
  reliably reproduced geometric patterns but consistently failed to capture material
  and environmental reasoning.
---

# From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers

## Quick Facts
- arXiv ID: 2601.00029
- Source URL: https://arxiv.org/abs/2601.00029
- Authors: Abolhassan Pishahang; Maryam Badiei
- Reference count: 2
- Key outcome: AI models reliably reproduce pigeon tower geometry but consistently fail to capture material and environmental reasoning

## Executive Summary
This study evaluated how three generative AI models (Midjourney v6, DALL-E 3, DreamStudio/SDXL) interpret Iranian pigeon towers under referential, adaptive, and speculative prompts. Across five architectural criteria, AI systems reliably reproduced geometric patterns but consistently failed to capture material and environmental reasoning. Reference imagery improved realism but limited creativity, while freedom from reference generated inventive yet culturally ambiguous forms. The findings demonstrate that AI perceives vernacular architecture as visual pattern rather than ecological logic, establishing a boundary between appearance and architectural intelligence.

## Method Summary
The research tested three diffusion models with three prompt types (referential, adaptive, speculative) under two conditions (with/without reference imagery). Eighteen total images were generated using Midjourney v6, DALL-E 3, and DreamStudio/SDXL with 3:4 aspect ratio. Outputs were evaluated against five criteria—Typology & Form, Materiality, Context & Environment, Realism, and Cultural Specificity—using qualitative comparative analysis by domain experts.

## Key Results
- AI reliably reproduces geometric patterns but misreads material and climatic reasoning
- Reference imagery improves realism yet limits creativity and expressive irregularity
- Freedom from reference generates inventive but culturally ambiguous forms

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models translate linguistic and visual inputs into probabilistic pixel distributions, learning what forms look like from training data rather than why they exist. When prompted with "Iranian pigeon tower," they retrieve averaged visual patterns (cylindrical massing, aperture rhythms, mudbrick textures) without encoding the thermal, ventilation, or climatic logic that produced those forms.

### Mechanism 2
Image conditioning narrows the sampling distribution around visual features present in the reference, improving realism metrics but suppressing the irregular, inventive forms that characterize vernacular craft adaptation. This inverse relationship between realism and creative variation is structural to current diffusion architectures.

### Mechanism 3
As prompt guidance weakens, cultural specificity degrades faster than visual inventiveness increases because models default to generic or hybridized forms when training data merges diverse architectural traditions into averaged visual norms. Without explicit constraint, cultural specificity does not emerge from unconstrained generation.

## Foundational Learning

- **Diffusion models as visual correlators, not causal reasoners**: The paper's central claim is that AI "remembers but does not infer." Understanding this distinction is prerequisite to interpreting results.
  - Quick check: Can you explain why a model might generate a mudbrick texture without understanding thermal mass?

- **Vernacular architecture as embedded environmental logic**: The evaluation criteria assume that form encodes climatic adaptation—not just aesthetics.
  - Quick check: Why are the pigeon tower's perforations both structural and ventilation strategies?

- **Prompt architecture as constraint specification**: The three-prompt framework operationalizes degrees of constraint necessary to replicate or extend the methodology.
  - Quick check: What trade-off does the paper identify between descriptive specificity and creative range?

## Architecture Onboarding

- **Component map**: Input layer (text prompts ± reference image) -> Generative engines (Midjourney v6, DALL-E 3, DreamStudio/SDXL) -> Evaluation framework (5 criteria) -> Analysis method (QCA by domain experts)

- **Critical path**: Define evaluation criteria first → design prompt gradations → generate with/without reference → score blind to condition → compare across engines

- **Design tradeoffs**: Midjourney v6: strongest atmospheric coherence and cultural specificity; DALL-E 3: highest geometric accuracy; DreamStudio/SDXL: most compositional diversity

- **Failure signatures**: Surface realism without material behavior, hybridized cultural forms, invented morphologies detached from climatic logic

- **First 3 experiments**:
  1. Replicate referential prompt across all three engines with/without same reference image; score using five criteria to establish baseline inter-rater reliability
  2. Test new vernacular typology (windcatchers, qanats) using same prompt framework to assess generalizability of visual-reasoning boundary claim
  3. Design "performance-augmented" prompt describing thermal or ventilation logic; evaluate whether this improves materiality scores

## Open Questions the Paper Calls Out

- **Open Question 1**: Can diffusion models trained on environmental and material performance datasets reconstruct the climatic reasoning embedded in vernacular forms, rather than only visual patterns? The Discussion states this requires "integration of environmental and material datasets that encode performance logic alongside visual description," but no performance-based training was attempted.

- **Open Question 2**: Would quantitative evaluation metrics and independent raters yield different conclusions about AI's architectural interpretive capabilities? The methodology relied solely on author-conducted qualitative comparative analysis without quantitative validation or inter-rater reliability testing.

- **Open Question 3**: Do findings about AI's pattern-recognition bias generalize across vernacular typologies with different environmental logics? Only Iranian pigeon towers were tested; generalizability to other vernacular architectures was not examined.

## Limitations
- Reliance on qualitative comparative analysis without quantitative performance metrics makes inter-study comparability difficult
- Claims about AI's lack of "architectural intelligence" rest on single vernacular typology without testing other climate-adaptive forms
- Does not explore whether explicit environmental/thermal prompts could improve material reasoning

## Confidence
- **High confidence**: Diffusion models reliably reproduce geometric patterns but fail material/environmental reasoning
- **Medium confidence**: Reference imagery improves realism but constrains creativity
- **Medium confidence**: Cultural specificity degrades faster than inventiveness under speculative prompts

## Next Checks
1. Test the same framework on a different vernacular typology (desert windcatchers or tropical stilt houses) to assess whether the visual-reasoning boundary generalizes across climatic adaptations
2. Design performance-augmented prompts that explicitly describe thermal mass, ventilation, or material behavior, then measure whether materiality scores improve beyond surface description
3. Implement quantitative metrics (CLIP similarity, feature extraction for aperture patterns) alongside qualitative scoring to enable cross-study comparison and reduce rater bias