---
ver: rpa2
title: 'Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers'
arxiv_id: '2502.08145'
source_url: https://arxiv.org/abs/2502.08145
tags:
- training
- gpus
- performance
- data
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a four-dimensional hybrid parallel algorithm
  for training large language models (LLMs) on GPU-based supercomputers. The approach
  combines three-dimensional parallel matrix multiplication with data parallelism,
  achieving unprecedented performance of 1.423 Exaflop/s on 6,144 NVIDIA H100 GPUs,
  1.381 Exaflop/s on 32,768 AMD MI250X GPUs, and 620.1 Petaflop/s on 4,096 NVIDIA
  A100 GPUs in half-precision (bf16).
---

# Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers

## Quick Facts
- arXiv ID: 2502.08145
- Source URL: https://arxiv.org/abs/2502.08145
- Reference count: 40
- Primary result: 1.423 Exaflop/s on 6,144 NVIDIA H100 GPUs

## Executive Summary
This paper introduces AxoNN, a framework enabling efficient training of large language models (5B to 640B parameters) on GPU-based supercomputers. The approach uses a novel four-dimensional hybrid parallel algorithm combining data parallelism with 3D parallel matrix multiplication. The framework achieves unprecedented performance levels across multiple GPU architectures while maintaining open-source accessibility. The work also investigates LLM memorization risks, demonstrating that models with 70B+ parameters can memorize training data in a single pass, and proposes mitigation techniques.

## Method Summary
The method employs a four-dimensional hybrid parallel algorithm that organizes GPUs into a virtual 2D grid (G_data × G_tensor) where G_data represents data-parallel groups and G_tensor represents a 3D tensor-parallel decomposition. Within each data-parallel group, 3D parallel matrix multiplication distributes weight matrices across GPUs using a combination of input and weight sharding strategies. The framework includes automated tuning of matrix multiplication kernels (NN/NT/TN modes), aggressive overlap of non-blocking collective communications with computation through three strategies (OAR, ORS, OAG), and a performance model to predict optimal configurations. The approach scales from single-node to 32,768 GPUs while maintaining high efficiency.

## Key Results
- Achieved 1.423 Exaflop/s on 6,144 NVIDIA H100 GPUs, 1.381 Exaflop/s on 32,768 AMD MI250X GPUs, and 620.1 Petaflop/s on 4,096 NVIDIA A100 GPUs in half-precision
- Demonstrated scaling of GPT-style transformers from 5B to 320B parameters with strong and weak scaling efficiency
- Showed that LLMs with 70B+ parameters memorize training data in a single pass, and proposed "Goldfish Loss" to mitigate this risk

## Why This Works (Mechanism)

### Mechanism 1: Four-Dimensional Hybrid Parallelism
Decomposes GPU topology into a 4D virtual grid (data × Gx × Gy × Gz) enabling training of models exceeding single-GPU memory while maintaining high arithmetic intensity. Data parallelism shards input batches across G_data groups, each holding a full model copy. Within each group, 3D parallel matrix multiplication distributes weight matrices across Gx×Gy×Gz GPUs using 2D-decomposed input and weight matrices with all-reduce aggregation along Y-dimension. Multi-layer networks swap X/Y roles every other layer to avoid redistribution.

### Mechanism 2: Communication-Computation Overlap via Non-Blocking Collectives
Aggressive overlap of collective operations with independent computation reduces non-overlapped communication time by 18-22% for large models at scale. Three strategies: OAR overlaps all-reduce across X-tensor groups with gradient-weight computation, ORS issues reduce-scatters asynchronously in backward pass waiting only after all layers complete, and OAG preemptively enqueues next-layer all-gather during current-layer computation using topologically sorted dependency graph.

### Mechanism 3: Performance Model for Configuration Selection
Analytical communication cost model predicts high-performing 4D grid configurations with 90% accuracy (9/10 top predictions are empirically efficient). Model computes communication time as sum of all-gather, reduce-scatter, and all-reduce times across dimensions, ranks configurations, and tests top-10. Bandwidth for each hierarchy level is profiled empirically or computed analytically for inter-node communication.

## Foundational Learning

- **Concept: Data Parallelism**
  - Why needed here: The 4D hybrid approach builds on data parallelism as the outer dimension. Understanding how gradients are synchronized via all-reduce across data-parallel groups is prerequisite to understanding why sharding + overlap matters.
  - Quick check question: If you have 4 GPUs and a batch of 64 samples with data parallelism, how many samples does each GPU process per iteration?

- **Concept: Collective Communication Primitives (All-Reduce, Reduce-Scatter, All-Gather)**
  - Why needed here: Algorithm 1 and the performance model are expressed entirely in terms of these primitives. Without understanding their bandwidth/latency characteristics, the overlap optimizations and performance model are opaque.
  - Quick check question: For a ring all-reduce with N processes and message size M, what is the data volume each process sends/receives?

- **Concept: 2D/3D Matrix Multiplication Parallelization (SUMMA, Cannon's Algorithm)**
  - Why needed here: The 3D PMM in this paper extends classical 2D algorithms. Understanding how matrices are decomposed and communicated in SUMMA makes the 3D extension intuitive.
  - Quick check question: In SUMMA with a P×Q processor grid, how is matrix A broadcast, and what is the communication cost per step?

## Architecture Onboarding

- **Component map**:
  - AxoNN Core orchestrates 4D grid initialization and manages process groups
  - BLAS Kernel Tuner auto-selects NN/NT/TN matmul modes during first batch
  - Communication Scheduler issues non-blocking collectives based on layer topology
  - Performance Model accepts (model size, GPU count, bandwidths) → ranked configuration list
  - Backend integrates with PyTorch (Megatron-LM or LitGPT frontends)

- **Critical path**:
  1. Define model architecture (hidden size, layers, heads)
  2. Run performance model to get top-10 configurations for given GPU count
  3. Select configuration (Gx, Gy, Gz, G_data)
  4. Initialize AxoNN with 4D grid; run one batch for BLAS kernel tuning
  5. Enable overlap optimizations; begin training loop

- **Design tradeoffs**:
  - Higher Gz (sharding) → lower memory per GPU, but more all-gather/reduce-scatter traffic
  - Higher G_data → simpler implementation, but gradient all-reduce grows with model size
  - Aggressive overlap → better throughput, but harder to debug timing issues
  - Large Gx×Gy (tensor parallelism) → better for compute-bound layers, but increases all-reduce traffic

- **Failure signatures**:
  - OOM during forward pass: Gz too small; increase weight sharding or use activation checkpointing
  - High non-overlapped comm time: Gx/Gy/Gz misconfigured for network hierarchy; re-run performance model
  - Slow BLAS kernels on AMD: Tuner may not have converged; manually verify NN vs TN mode selection
  - Scaling drops at >16K GPUs: Inter-node bandwidth saturation; consider reducing G_data or increasing Gz

- **First 3 experiments**:
  1. Single-node baseline: Run GPT-5B on 4/8 GPUs with (Gx=2, Gy=2, Gz=1, G_data=1) vs (Gx=1, Gy=1, Gz=4, G_data=1). Compare throughput and memory.
  2. Configuration sweep: For GPT-40B on 64 GPUs, run all valid 4D decompositions; plot empirical batch times vs model-predicted ranks. Verify 9/10 prediction accuracy.
  3. Overlap ablation: For GPT-80B on 512 GPUs, measure batch time with Baseline → +OAR → +ORS → +OAG. Confirm 18%+ improvement.

## Open Questions the Paper Calls Out

### Open Question 1
What specific hyperparameter adjustments are required for models with 405B+ parameters to balance efficient learning with the risk of catastrophic memorization? The authors note in Section VIII-C that the 405B model exhibited a slower memorization rate than the 70B model, likely because "extreme scales likely require different hyperparameters for optimal learning." The experiments utilized a fixed set of hyperparameters across all model sizes, which may have been suboptimal for the specific learning dynamics of the 405B parameter model.

### Open Question 2
To what extent does pre-existing memorization from pre-training limit the efficacy of "Goldfish Loss" during continued training? Section VIII-D observes that the 405B model showed a small increase in memorization despite Goldfish Loss, hypothesizing that "the model has already memorized the masked tokens from when it was pre-trained." The current study could not differentiate between the model failing to unlearn data and the model successfully avoiding new memorization of data it already knew.

### Open Question 3
Can the current 4D hybrid parallel approach maintain high weak scaling efficiency on clusters exceeding 32,768 GPUs? While the paper achieves 1.381 Exaflop/s, Section VII-A reports a drop in weak scaling efficiency to 53.5% on 32,768 GCDs due to rising communication overheads. The authors demonstrate a "notable decline" in performance at the maximum scale, suggesting that communication overheads may eventually saturate the benefits of the current overlapping optimizations.

## Limitations
- Performance model assumes ring-based collective algorithms and may not accurately predict performance for networks with non-uniform topology or congestion patterns
- The 4D hybrid approach shows strong scaling up to 32K GPUs, but efficiency drops to 53.5% at largest scale suggesting potential bottlenecks in inter-node communication
- Memorization experiments use synthetic data extraction methods that may not fully represent real-world privacy attacks

## Confidence
- **High confidence**: The 4D hybrid parallelism mechanism and its mathematical formulation are well-defined and the 1.42 Exaflop/s result on 6,144 H100 GPUs is verifiable through standard HPC benchmarks
- **Medium confidence**: The communication-computation overlap optimizations (OAR, ORS, OAG) show consistent 18-22% improvements across scales, but depend on NCCL/RCCL implementation details that may vary across systems
- **Medium confidence**: The performance model's 90% accuracy claim is supported by validation on two model sizes but would benefit from broader testing across more configurations and architectures

## Next Checks
1. **Configuration Sweep Validation**: For GPT-80B on 1,024 GPUs, run all 4D decompositions and compare empirical batch times against model-predicted ranks to verify the 90% accuracy claim holds across scales
2. **Overlap Optimization A/B Test**: Implement a version without OAG and measure non-overlapped communication time at 16K+ GPU scales to quantify the relative contribution of each optimization strategy
3. **Cross-Architecture Generalization**: Apply the same performance model approach to a different model architecture (e.g., LLaMA or BLOOM) and compare predicted vs. actual scaling efficiency to test model robustness