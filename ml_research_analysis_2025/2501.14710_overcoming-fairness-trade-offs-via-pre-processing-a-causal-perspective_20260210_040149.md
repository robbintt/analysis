---
ver: rpa2
title: 'Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective'
arxiv_id: '2501.14710'
source_url: https://arxiv.org/abs/2501.14710
tags:
- world
- fairness
- find
- data
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves two key fairness trade-offs in machine learning
  by leveraging causal pre-processing methods to approximate a "fair world" (FiND)
  where protected attributes have no causal effect on the target variable. The authors
  theoretically show that in this FiND world, multiple fairness metrics become naturally
  satisfied and align with high predictive performance.
---

# Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective

## Quick Facts
- arXiv ID: 2501.14710
- Source URL: https://arxiv.org/abs/2501.14710
- Reference count: 40
- This paper resolves two key fairness trade-offs in machine learning by leveraging causal pre-processing methods to approximate a "fair world" (FiND) where protected attributes have no causal effect on the target variable.

## Executive Summary
This paper resolves two key fairness trade-offs in machine learning by leveraging causal pre-processing methods to approximate a "fair world" (FiND) where protected attributes have no causal effect on the target variable. The authors theoretically show that in this FiND world, multiple fairness metrics become naturally satisfied and align with high predictive performance. They propose using causal pre-processing methods (fairadapt and warping) to transform real-world data to approximate the FiND world, and demonstrate through simulations and empirical studies that these methods successfully resolve both the fairness-accuracy trade-off and the trade-off between different fairness metrics. In their simulation study, pre-processed data achieved fairness metrics with differences under 3.6% compared to the FiND world, while maintaining similar predictive performance. The approach provides actionable solutions for practitioners to achieve fairness and high predictive performance simultaneously.

## Method Summary
The paper introduces the "FiND world" (Fictitious and Normatively Desired), a counterfactual distribution where protected attributes have no causal effect on outcomes. Using causal graphs (DAGs), the authors propose pre-processing methods—FairAdapt and residual-based warping—that transform real data to approximate this fair world by removing causal paths from protected attributes to features. Models trained on this transformed data can then achieve both high accuracy and fairness metrics simultaneously, resolving the classical trade-offs.

## Key Results
- Pre-processed data using causal methods achieved fairness metrics with differences under 3.6% compared to the FiND world in simulation studies
- Models trained on pre-processed data maintained similar predictive performance while achieving high fairness metrics simultaneously
- The approach successfully resolved both the fairness-accuracy trade-off and the trade-off between different fairness metrics
- Evaluation on HMDA 2022 Wisconsin data demonstrated the method's effectiveness in real-world settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The fairness-accuracy trade-off is an artifact of biased data; when models are evaluated on data where protected attributes (PA) are causally independent of the target, fairness and accuracy align.
- **Mechanism:** The paper introduces the "FiND world" (Fictitious and Normatively Desired), a counterfactual distribution where $Y^* \perp A$. Because the ground truth in this world is fundamentally fair, enforcing a fairness constraint (like Demographic Parity) brings the model's predictions closer to the true underlying distribution, thereby increasing accuracy rather than reducing it.
- **Core assumption:** The correlation between PA and the target in observed data is spurious or normatively undesirable and does not reflect the "true" merit of individuals.
- **Evidence anchors:**
  - [abstract] "fairness and predictive performance are in fact in accord when predictive performance is measured on unbiased data."
  - [section 4.2] Demonstrates that models trained on real data exhibit a positive correlation between fairness and AUC when evaluated on adapted/warped (FiND-approximated) test data.
  - [corpus] "CausalPre" (arXiv:2509.15199) supports the viability of causal pre-processing, though this paper specifically focuses on the theoretical alignment of metrics.
- **Break condition:** If the causal graph is misspecified (e.g., legitimate causal paths are removed), the FiND world will reflect an incorrect reality, and the accuracy gains may be spurious.

### Mechanism 2
- **Claim:** The "impossibility theorem" (incompatibility of fairness metrics) is resolved by equalizing group base rates through causal intervention.
- **Mechanism:** The impossibility theorem states that metrics like False Positive Rate balance and Predictive Parity are incompatible unless groups have equal base rates ($P(Y=1|A=a) = P(Y=1|A=a')$). By removing all causal paths from $A$ to $Y$, the FiND world enforces equal base rates by design, satisfying the special case where these metrics are no longer mutually exclusive.
- **Core assumption:** All causal paths from the protected attribute to the outcome are discriminatory. (The paper notes this is a "strict" definition; relaxing it requires defining "resolving variables").
- **Evidence anchors:**
  - [abstract] "classical fairness metrics deemed to be incompatible are naturally satisfied in the FiND world."
  - [section 2.2.2] "The FiND world represents one of the special cases of the impossibility theorem, namely equal base rates among groups."
  - [corpus] "Task-tailored Pre-processing" (arXiv:2601.11897) explores fair downstream learning, aligning with the need for specific pre-processing tailored to the task.
- **Break condition:** If the application context allows for some correlation between $A$ and $Y$ (e.g., gender in medical diagnoses), forcing equal base rates may introduce "reverse discrimination" or model misspecification.

### Mechanism 3
- **Claim:** Causal pre-processing (Warping/FairAdapt) approximates the FiND world by altering the distribution of descendants of the protected attribute.
- **Mechanism:** The methods intervene on the causal graph. For a protected individual, their features ($X$) are transformed to the quantiles they would have occupied had they belonged to the unprotected group. This "cuts" the causal edge from $A$ to $X$, creating a dataset that approximates the independence of the FiND world.
- **Core assumption:** The structural equations generating the data are known or can be approximated well enough to perform the quantile transformation accurately.
- **Evidence anchors:**
  - [abstract] "suggesting how one can benefit from these theoretical insights in practice, using causal pre-processing methods..."
  - [section 3.1] Describes "residual-based warping" and FairAdapt as methods to project real-world data into the FiND representation.
  - [corpus] "Revisiting Pre-processing Group Fairness" (arXiv:2508.15193) provides a benchmarking framework that validates the utility of pre-processing approaches.
- **Break condition:** If unobserved confounders exist that are not accounted for in the DAG, the transformation will fail to remove the dependence on the protected attribute.

## Foundational Learning

- **Concept:** **Impossibility Theorem (Fairness)**
  - **Why needed here:** The paper claims to resolve this trade-off. You must understand that metrics like Equalized Odds and Demographic Parity typically conflict mathematically in the real world to see why the "equal base rates" condition of the FiND world is a significant theoretical step.
  - **Quick check question:** Why does equalizing the base rate ($P(Y=1)$) across groups allow Demographic Parity and Equalized Odds to be satisfied simultaneously?

- **Concept:** **Counterfactual Fairness & Causal DAGs**
  - **Why needed here:** The solution relies entirely on causal graphs (DAGs). The "FiND world" is a counterfactual state defined by specific interventions on this graph. Without understanding the difference between statistical correlation and causal effect, the mechanism of "warping" will be unclear.
  - **Quick check question:** In the credit DAG (Fig 1), if we "cut" the arrow from Gender ($A$) to Income ($X$), how does that change the prediction of Default ($Y$)?

- **Concept:** **Quantile Mapping / Distribution Matching**
  - **Why needed here:** The pre-processing methods (FairAdapt, Warping) work by mapping the feature values of the protected group to the distribution of the unprotected group. This is the technical implementation of the "fair twin" concept.
  - **Quick check question:** If a protected individual has a debt-to-income ratio at the 90th percentile of their group, what percentile would they be mapped to in the "fair" (unprotected) distribution?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Real-world tabular data containing Protected Attributes (PA), features, and targets.
  2. **Expert Knowledge:** User-defined Directed Acyclic Graph (DAG) mapping causal relationships.
  3. **Pre-processor:** Causal transformation engine (e.g., `fairadapt` or `warping`) to generate approximated FiND datasets ($D_{train}^*, D_{test}^*$).
  4. **Model:** Standard Gradient Boosted Trees (xgboost) trained on the transformed data.
  5. **Evaluator:** A dual-loop system checking performance (AUC) and fairness constraints (DP, FPR, FNR) on transformed vs. untransformed test sets.

- **Critical path:** The definition of the **Causal DAG**. The entire pre-processing logic depends on correctly identifying which variables are descendants of the protected attribute. If the graph is wrong, the "FiND world" is a distorted reality.

- **Design tradeoffs:**
  - **Strict vs. Path-Specific Fairness:** The paper implements a strict "no effect" policy. If your domain requires some paths to remain (e.g., gender $\to$ pregnancy related medical costs), you must modify the pre-processor to only warp specific paths.
  - **Interpretability:** Models trained on "warped" data are predicting outcomes in a fictitious world. Explaining a rejection based on "warped income" to a real applicant is non-trivial.

- **Failure signatures:**
  - **Trade-off Resurfacing:** If, when running Algorithm 1 (Fairness-Accuracy evaluation) on your pre-processed test set, you see a *negative* slope (accuracy drops as fairness increases), the pre-processing has failed to approximate the FiND world.
  - **Metric Incompatibility:** If Table 1b shows large discrepancies between DP, FPR, and PPV in your adapted/warped world, the base rates were not successfully equalized.

- **First 3 experiments:**
  1. **Baseline Reality Check:** Train a model on untransformed data. Apply increasing fairness constraints (in-processing). Verify the typical negative trade-off curve (Fig 2a).
  2. **FiND Approximation Validation:** Pre-process the data using your DAG. Run the same model training (with constraints) but evaluate on the *pre-processed* test set. Verify the positive slope (Fig 2c/2d) to confirm the method works on your data.
  3. **Metric Compatibility Stress Test:** Train an unconstrained model on the pre-processed data. Calculate DP, FPR, FNR, and PPV. Ensure all metrics are $>0.97$ (or similarly high) to confirm the impossibility theorem is resolved in your specific context.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust are the proposed pre-processing methods (FairAdapt and residual-based warping) to misspecifications in the causal graph or the use of learned DAGs?
- **Basis in paper:** [explicit] The authors explicitly list the "investigation of the sensitivity of the methods with respect to DAGs (partially) learned from data" as a direction for future work, acknowledging that correct specification is a limitation.
- **Why unresolved:** The simulation and real-world experiments rely on the assumption of a known, correct causal structure. It remains unclear how structural errors impact the theoretical guarantees or the empirical resolution of trade-offs.
- **What evidence would resolve it:** Simulation studies that introduce structural noise into the DAGs or utilize causal discovery algorithms, measuring the subsequent degradation in fairness and accuracy metrics.

### Open Question 2
- **Question:** Can non-causal pre-processing methods successfully approximate the FiND world to the same extent as causal methods?
- **Basis in paper:** [explicit] The paper states, "future work could include evaluating non-causal pre-processing methods in their ability to approximate the FiND world."
- **Why unresolved:** The study restricted its scope to causal methods due to the causal nature of the FiND world framework, leaving the effectiveness of purely statistical fair representation learning techniques untested within this specific evaluation protocol.
- **What evidence would resolve it:** Applying the proposed "Fairness-Accuracy Trade-off Evaluation" (Algorithm 1) to non-causal pre-processing methods to see if they similarly result in a positive correlation between fairness constraints and performance.

### Open Question 3
- **Question:** How can the framework be extended to accommodate multiple protected attributes and intersectionality?
- **Basis in paper:** [explicit] The authors identify this as a challenge: "Modeling multiple PAs opens up the topic of intersectionality... which imposes another major challenge in fairML."
- **Why unresolved:** The current experiments (HMDA and simulations) utilize a single binary protected attribute. The theory and implementation do not yet account for the interactions between multiple protected attributes (e.g., race and gender simultaneously).
- **What evidence would resolve it:** Experiments using datasets with multiple protected attributes where pre-processing successfully resolves trade-offs for intersectional subgroups defined by the combination of attributes.

## Limitations

- The approach requires expert knowledge of causal structures, which may not be readily available in many practical applications
- The paper does not address potential negative societal impacts of enforcing equal base rates in contexts where protected attributes legitimately correlate with outcomes
- The proposed methods are computationally intensive and may not scale well to high-dimensional data or complex causal structures

## Confidence

- Claims about fairness-accuracy alignment in FiND world: Medium
- Claims about resolving metric incompatibilities: Medium
- Empirical results on HMDA data: Medium
- Simulation results: Medium

## Next Checks

1. Apply the method to additional real-world datasets with known causal structures to test generalizability
2. Conduct sensitivity analysis to test robustness against misspecified causal graphs
3. Evaluate the approach on datasets where some causal paths from protected attributes to outcomes are legitimate to assess handling of edge cases